Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 947?951,
Prague, June 2007. c?2007 Association for Computational Linguistics
Fast and Robust Multilingual Dependency Parsing
with a Generative Latent Variable Model
Ivan Titov
University of Geneva
24, rue Ge?ne?ral Dufour
CH-1211 Gene`ve 4, Switzerland
ivan.titov@cui.unige.ch
James Henderson
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, United Kingdom
james.henderson@ed.ac.uk
Abstract
We use a generative history-based model to
predict the most likely derivation of a de-
pendency parse. Our probabilistic model is
based on Incremental Sigmoid Belief Net-
works, a recently proposed class of la-
tent variable models for structure predic-
tion. Their ability to automatically in-
duce features results in multilingual pars-
ing which is robust enough to achieve accu-
racy well above the average for each indi-
vidual language in the multilingual track of
the CoNLL-2007 shared task. This robust-
ness led to the third best overall average la-
beled attachment score in the task, despite
using no discriminative methods. We also
demonstrate that the parser is quite fast, and
can provide even faster parsing times with-
out much loss of accuracy.
1 Introduction
The multilingual track of the CoNLL-2007 shared
task (Nivre et al, 2007) considers dependency pars-
ing of texts written in different languages. It re-
quires use of a single dependency parsing model
for the entire set of languages; model parameters
are estimated individually for each language on the
basis of provided training sets. We use a recently
proposed dependency parser (Titov and Hender-
son, 2007b)1 which has demonstrated state-of-the-
art performance on a selection of languages from the
1The ISBN parser will be soon made downloadable from the
authors? web-page.
CoNLL-X shared task (Buchholz and Marsi, 2006).
This parser employs a latent variable model, Incre-
mental Sigmoid Belief Networks (ISBNs), to de-
fine a generative history-based model of projective
parsing. We used the pseudo-projective transforma-
tion introduced in (Nivre and Nilsson, 2005) to cast
non-projective parsing tasks as projective. Follow-
ing (Nivre et al, 2006), the encoding scheme called
HEAD in (Nivre and Nilsson, 2005) was used to en-
code the original non-projective dependencies in the
labels of the projectivized dependency tree. In the
following sections we will briefly discuss our modi-
fications to the ISBN parser, experimental setup, and
achieved results.
2 The Probability Model
Our probability model uses the parsing order pro-
posed in (Nivre et al, 2004), but instead of perform-
ing deterministic parsing as in (Nivre et al, 2004),
this ordering is used to define a generative history-
based model, by adding word prediction to the Shift
parser action. We also decomposed some parser ac-
tions into sub-sequences of decisions. We split arc
prediction decisions (Left-Arcr and Right-Arcr) each
into two elementary decisions: first the parser cre-
ates the corresponding arc, then it assigns a relation
r to the arc. Similarly, we decompose the decision
to shift a word into a decision to shift and a pre-
diction of the word. We used part-of-speech tags
and fine-grain word features, which are given in the
data, to further decompose word predictions. First
we predict the fine-grain part-of-speech tag for the
word, then the set of word features (treating each
set as an atomic value), and only then the particu-
947
lar word form. This approach allows us to both de-
crease the effect of sparsity and to avoid normaliza-
tion across all the words in the vocabulary, signifi-
cantly reducing the computational expense of word
prediction. When conditioning on words, we treated
each word feature individually, as this proved to be
useful in (Titov and Henderson, 2007b).
The probability of each parser decision, condi-
tioned on the complete parse history, is modeled
using a form a graphical model called Incremental
Sigmoid Belief Networks. ISBNs, originally pro-
posed for constituent parsing in (Titov and Hender-
son, 2007a), use vectors of binary latent variables to
encode information about the parse history. These
history variables are similar to the hidden state of
a Hidden Markov Model. But unlike the graphi-
cal model for an HMM, which would specify con-
ditional dependency edges only between adjacent
states in the parse history, the ISBN graphical model
can specify conditional dependency edges between
latent variables which are arbitrarily far apart in the
parse history. The source state of such an edge is
determined by the partial parse structure built at the
time of the destination state, thereby allowing the
conditional dependency edges to be appropriate for
the structural nature of the parsing problem. In par-
ticular, they allow conditional dependencies to be
local in the parse structure, not just local in the his-
tory sequence. In this they are similar to the class
of neural networks proposed in (Henderson, 2003)
for constituent parsing. In fact, in (Titov and Hen-
derson, 2007a) it was shown that this neural network
can be viewed as a coarse approximation to the cor-
responding ISBN model.
Traditional statistical parsing models also condi-
tion on features which are local in the parse struc-
ture, but these features need to be explicitly defined
before learning, and require careful feature selec-
tion. This is especially difficult for languages un-
known to the parser developer, since the number of
possible features grows exponentially with the struc-
tural distance considered.
The ISBN model uses an alternative approach,
where latent variables are used to induce features
during learning. The most important problem in de-
signing an ISBN is to define an appropriate struc-
tural locality for each parser decision. This is done
by choosing a fixed set of relationships between
parser states, where the information which is needed
to make the decision at the earlier state is also use-
ful in making the decision at the later state. The la-
tent variables for these related states are then con-
nected with conditional dependency edges in the
ISBN graphical model. Longer conditional depen-
dencies are then possible through chains of these im-
mediate conditional dependencies, but there is an in-
ductive bias toward shorter chains. This bias makes
it important that the set of chosen relationships de-
fines an appropriate notion of locality. However,
as long as there exists some chain of relationships
between any two states, then any statistical depen-
dency which is clearly manifested in the data can be
learned, even if it was not foreseen by the designer.
This provides a potentially powerful form of feature
induction, which is nonetheless biased toward a no-
tion of locality appropriate for the nature of the prob-
lem.
In our experiments we use the same definition of
structural locality as was proposed for the ISBN de-
pendency parser in (Titov and Henderson, 2007b).
The current state is connected to previous states us-
ing a set of 7 distinct relationships defined in terms
of each state?s parser configuration, which includes
of a stack and a queue. Specifically, the current state
is related to the last previous state whose parser con-
figuration has: the same queue, the same stack, a
stack top which is the rightmost right child of the
current stack top, a stack top which is the leftmost
left child of the current stack top, a front of the queue
which is the leftmost child of the front of the cur-
rent queue, a stack top which is the head word of
the current stack top, a front of the queue which is
the current stack top. Different model parameters
are trained for each of these 7 types of relationship,
but the same parameters are used everywhere in the
graphical model where the relationship holds.
Each latent variable in the ISBN parser is also
conditionally dependent on a set of explicit features
of the parsing history. As long as these explicit fea-
tures include all the new information from the last
parser decision, the performance of the model is not
very sensitive to this design choice. We used the
base feature model defined in (Nivre et al, 2006)
for all the languages but Arabic, Chinese, Czech,
and Turkish. For Arabic, Chinese, and Czech, we
used the same feature models used in the CoNLL-X
948
shared task by (Nivre et al, 2006), and for Turkish
we used again the base feature model but extended
it with a single feature: the part-of-speech tag of the
token preceding the current top of the stack.
3 Parsing
Exact inference in ISBN models is not tractable, but
effective approximations were proposed in (Titov
and Henderson, 2007a). Unlike (Titov and Hender-
son, 2007b), in the shared task we used only the
simplest feed-forward approximation, which repli-
cates the computation of a neural network of the type
proposed in (Henderson, 2003). We would expect
better performance with the more accurate approxi-
mation based on variational inference proposed and
evaluated in (Titov and Henderson, 2007a). We did
not try this because, on larger treebanks it would
have taken too long to tune the model with this bet-
ter approximation, and using different approxima-
tion methods for different languages would not be
compatible with the shared task rules.
To search for the most probable parse, we use the
heuristic search algorithm described in (Titov and
Henderson, 2007b), which is a form of beam search.
In section 4 we show that this search leads to quite
efficient parsing.
To overcome a minor shortcoming of the pars-
ing algorithm of (Nivre et al, 2004) we introduce a
simple language independent post-processing step.
Nivre?s parsing algorithm allows unattached nodes
to stay on the stack at the end of parsing, which is
reasonable for treebanks with unlabeled attachment
to root. However, this sometimes happens with lan-
guages where only labeled attachment to root is al-
lowed. In these cases (only 35 tokens in Greek, 17
in Czech, 1 in Arabic, on the final testing set) we
attached them using a simple rule: if there are no
tokens in the sentence attached to root, then the con-
sidered token is attached to root with the most fre-
quent root-attachment relation used for its part-of-
speech tag. If there are other root-attached tokens in
the sentence, it is attached to the next root-attached
token with the most frequent relation. Preference is
given to the most frequent attachment direction for
its part-of-speech tag. This rule guarantees that no
loops are introduced by the post-processing.
4 Experiments
We evaluated the ISBN parser on all the languages
considered in the shared task (Hajic? et al, 2004;
Aduriz et al, 2003; Mart?? et al, 2007; Chen et
al., 2003; Bo?hmova? et al, 2003; Marcus et al,
1993; Johansson and Nugues, 2007; Prokopidis et
al., 2005; Csendes et al, 2005; Montemagni et al,
2003; Oflazer et al, 2003). ISBN models were
trained using a small development set taken out from
the training set, which was used for tuning learn-
ing and decoding parameters, for early stopping and
very coarse feature engineering.2 The sizes of the
development sets were different: starting from less
than 2,000 tokens for smaller treebanks to 5,000 to-
kens for the largest one. The relatively small sizes
of the development sets limited our ability to per-
form careful feature selection, but this should not
have significantly affected the model performance,
as discussed in section 2.3 We used frequency cut-
offs: we ignored any property (word form, lemma,
feature) which occurs in the training set less than
a given threshold. We used a threshold of 20 for
Greek and Chinese and a threshold of 5 for the rest.
Because cardinalities of each of these sets (sets of
word forms, lemmas and features) effect the model
efficiency, we selected the larger threshold when val-
idation results with the smaller threshold were com-
parable. For the ISBN latent variables, we used vec-
tors of length 80, based on our previous experience.
Results on the final testing set are presented in ta-
ble 1. The model achieves relatively high scores on
each individual language, significantly better than
each average result in the shared task. This leads
to the third best overall average results in the shared
task, both in average labeled attachment score and
in average unlabeled attachment score. The absolute
error increase in labeled attachment score over the
best system is only 0.4%. We attribute ISBN?s suc-
cess mainly to its ability to automatically induce fea-
tures, as this significantly reduces the risk of omit-
ting any important highly predictive features. This
makes an ISBN parser a particularly good baseline
when considering a new treebank or language, be-
2We plan to make all the learning and decoding parameters
available on our web-page.
3Use of cross-validation with our model is relatively time-
consuming and, thus, not quite feasible for the shared task.
949
Ara Bas Cat Chi Cze Eng Gre Hun Ita Tur Ave
LAS 74.1 75.5 87.4 82.1 77.9 88.4 73.5 77.9 82.3 79.8 79.90
UAS 83.2 81.9 93.4 87.9 84.2 89.7 81.2 82.2 86.3 86.2 85.62
Table 1: Labeled attachment score (LAS) and unlabeled attachment score (UAS) on the final testing sets
 78.5
 79
 79.5
 80
 80.5
 81
 0  20  40  60  80  100  120  140
Av
er
ag
e 
LA
S
Parsing Time per Token, ms
Figure 1: Average labeled attachment score on
Basque, Chinese, English, and Turkish development
sets as a function of parsing time per token
cause it does not require much effort in feature en-
gineering. As was demonstrated in (Titov and Hen-
derson, 2007b), even a minimal set of local explicit
features achieves results which are non-significantly
different from a carefully chosen set of explicit fea-
tures, given the language independent definition of
locality described in section 2.
It is also important to note that the model is
quite efficient. Figure 1 shows the tradeoff be-
tween accuracy and parsing time as the width of the
search beam is varied, on the development set. This
curve plots the average labeled attachment score
over Basque, Chinese, English, and Turkish as a
function of parsing time per token.4 Accuracy of
only 1% below the maximum can be achieved with
average processing time of 17 ms per token, or 60
tokens per second.5
We also refer the reader to (Titov and Henderson,
2007b) for more detailed analysis of the ISBN de-
pendency parser results, where, among other things,
it was shown that the ISBN model is especially ac-
curate at modeling long dependencies.
4A piecewise-linear approximation for each individual lan-
guage was used to compute the average. Experiments were run
on a standard 2.4 GHz desktop PC.
5For Basque, Chinese, and Turkish this time is below 7 ms,
but for English it is 38 ms. English, along with Catalan, required
the largest beam across all 10 languages. Note that accuracy in
the lowest part of the curve can probably be improved by vary-
ing latent vector size and frequency cut-offs. Also, efficiency
was not the main goal during the implementation of the parser,
and it is likely that a much faster implementation is possible.
5 Conclusion
We evaluated the ISBN dependency parser in the
multilingual shared task setup and achieved com-
petitive accuracy on every language, and the third
best average score overall. The proposed model re-
quires minimal design effort because it relies mostly
on automatic feature induction, which is highly de-
sirable when using new treebanks or languages. The
parsing time needed to achieve high accuracy is also
quite small, making this model a good candidate for
use in practical applications.
The fact that our model defines a probability
model over parse trees, unlike the previous state-
of-the-art methods (Nivre et al, 2006; McDonald et
al., 2006), makes it easier to use this model in ap-
plications which require probability estimates, such
as in language processing pipelines or for language
modeling. Also, as with any generative model,
it should be easy to improve the parser?s accu-
racy with discriminative reranking, such as discrim-
inative retraining techniques (Henderson, 2004) or
data-defined kernels (Henderson and Titov, 2005),
with or even without the introduction of any addi-
tional linguistic features.
Acknowledgments
This work was funded by Swiss NSF grant 200020-
109685, UK EPSRC grant EP/E019501/1, and EU
FP6 grant 507802 for project TALK.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 201?204.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(Abeille?, 2003), chapter 7, pages 103?127.
950
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of the Tenth Conference on Computational Nat-
ural Language Learning, New York, USA.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(Abeille?, 2003), chapter 13, pages 231?248.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
J. Hajic?, O. Smrz?, P. Zema?nek, J. ?Snaidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
James Henderson and Ivan Titov. 2005. Data-defined
kernels for parse reranking derived from probabilis-
tic models. In Proc. 43rd Meeting of Association for
Computational Linguistics, Ann Arbor, MI.
James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Proc.
joint meeting of North American Chapter of the Asso-
ciation for Computational Linguistics and the Human
Language Technology Conf., pages 103?110, Edmon-
ton, Canada.
James Henderson. 2004. Discriminative training of
a neural network statistical parser. In Proc. 42nd
Meeting of Association for Computational Linguistics,
Barcelona, Spain.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proc. of the Tenth Con-
ference on Computational Natural Language Learn-
ing, New York, USA.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille? (Abeille?, 2003), chap-
ter 11, pages 189?210.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proc. 43rd Meeting of Asso-
ciation for Computational Linguistics, pages 99?106,
Ann Arbor, MI.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proc. of the
Eighth Conference on Computational Natural Lan-
guage Learning, pages 49?56, Boston, USA.
Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit,
and Svetoslav Marinov. 2006. Pseudo-projective de-
pendency parsing with support vector machines. In
Proc. of the Tenth Conference on Computational Nat-
ural Language Learning, pages 221?225, New York,
USA.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL).
K. Oflazer, B. Say, D. Zeynep Hakkani-T u?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille?
(Abeille?, 2003), chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT), pages 149?160.
Ivan Titov and James Henderson. 2007a. Constituent
parsing with incremental sigmoid belief networks. In
Proc. 45th Meeting of Association for Computational
Linguistics (ACL), Prague, Czech Republic.
Ivan Titov and James Henderson. 2007b. A latent vari-
able model for generative dependency parsing. In
Proc. 10th Int. Conference on Parsing Technologies
(IWPT), Prague, Czech Republic.
951
An ISU Dialogue System Exhibiting Reinforcement Learning of Dialogue
Policies: Generic Slot-filling in the TALK In-car System
Oliver Lemon, Kallirroi Georgila, and James Henderson
School of Informatics
University of Edinburgh
olemon@inf.ed.ac.uk
Matthew Stuttle
Dept. of Engineering
University of Cambridge
mns25@cam.ac.uk
Abstract
We demonstrate a multimodal dialogue system
using reinforcement learning for in-car sce-
narios, developed at Edinburgh University and
Cambridge University for the TALK project1.
This prototype is the first ?Information State
Update? (ISU) dialogue system to exhibit rein-
forcement learning of dialogue strategies, and
also has a fragmentary clarification feature.
This paper describes the main components and
functionality of the system, as well as the pur-
poses and future use of the system, and surveys
the research issues involved in its construction.
Evaluation of this system (i.e. comparing the
baseline system with handcoded vs. learnt dia-
logue policies) is ongoing, and the demonstra-
tion will show both.
1 Introduction
The in-car system described below has been con-
structed primarily in order to be able to collect data
for Reinforcement Learning (RL) approaches to mul-
timodal dialogue management, and also to test and fur-
ther develop learnt dialogue strategies in a realistic ap-
plication scenario. For these reasons we have built a
system which:
  contains an interface to a dialogue strategy learner
module,
  covers a realistic domain of useful ?in-car? con-
versation and a wide range of dialogue phenom-
ena (e.g. confirmation, initiative, clarification, in-
formation presentation),
  can be used to complete measurable tasks (i.e.
there is a measure of successful and unsuccessful
dialogues usable as a reward signal for Reinforce-
ment Learning),
  logs all interactions in the TALK data collection
format (Georgila et al, 2005).
1This research is supported by the TALK project (Euro-
pean Community IST project no. 507802), http://www.talk-
project.org
In this demonstration we will exhibit the software
system that we have developed to meet these require-
ments. First we describe the domain in which the di-
alogue system operates (an ?in-car? information sys-
tem). Then we describe the major components of the
system and give examples of their use. We then discuss
the important features of the system in respect to the
dialogue phenomena that they support.
1.1 A System Exhibiting Reinforcement Learning
The central motivation for building this dialogue sys-
tem is as a platform for Reinforcement Learning (RL)
experiments. The system exhibits RL in 2 ways:
  It can be run in online learning mode with real
users. Here the RL agent is able to learn from suc-
cessful and unsuccessful dialogueswith real users.
Learning will be much slower than with simulated
users, but can start from an already learnt policy,
and slowly improve upon that.
  It can be run using an already learnt policy (e.g.
the one reported in (Henderson et al, 2005;
Lemon et al, 2005), learnt from COMMUNICA-
TOR data (Georgila et al, 2005)). This mode can
be used to test the learnt policies in interactions
with real users.
Please see (Henderson et al, 2005) for an expla-
nation of the techniques developed for Reinforcement
Learning with ISU dialogue systems.
2 System Overview
The baseline dialogue system is built around the DIP-
PER dialogue manager (Bos et al, 2003). This sys-
tem is initially used to conduct information-seeking di-
alogues with a user (e.g. find a particular hotel and
restaurant), using hand-coded dialogue strategies (e.g.
always use implicit confirmation, except when ASR
confidence is below 50%, then use explicit confirma-
tion). We have then modified the DIPPER dialogue
manager so that it can consult learnt strategies (for ex-
ample strategies learnt from the 2000 and 2001 COM-
MUNICATOR data (Lemon et al, 2005)), based on its
119
current information state, and then execute dialogue ac-
tions from those strategies. This allows us to compare
hand-coded against learnt strategies within the same
system (i.e. the other components such as the speech-
synthesiser, recogniser, GUI, etc. all remain fixed).
2.1 Overview of System Features
The following features are currently implemented:
  use of Reinforcement Learning policies or dia-
logue plans,
  multiple tasks: information seeking for hotels,
bars, and restaurants,
  overanswering/ question accommodation/ user-
initiative,
  open speech recognition using n-grams,
  confirmations - explicit and implicit based on
ASR confidence,
  fragmentary clarifications based on word confi-
dence scores,
  multimodal output - highlighting and naming en-
tities on GUI,
  simple user commands (e.g. ?Show me all the in-
dian restaurants?),
  dialogue context logging in ISU format (Georgila
et al, 2005).
3 Research Issues
The work presented here explores a number of research
themes, in particular: using learnt dialogue policies,
learning dialogue policies in online interaction with
users, fragmentary clarification, and reconfigurability.
3.1 Moving between Domains:
COMMUNICATOR and In-car Dialogues
The learnt policies in (Henderson et al, 2005) focussed
on the COMMUNICATOR system for flight-booking di-
alogues. There we reported learning a promising initial
policy for COMMUNICATOR dialogues, but the issue
arises of how we could transfer this policy to new do-
mains ? for example the in-car domain.
In the in-car scenarios the genre of ?information
seeking? is central. For example the SACTI corpora
(Stuttle et al, 2004) have driver information requests
(e.g. searching for hotels) as a major component.
One question we address here is to what extent di-
alogue policies learnt from data gathered for one sys-
tem, or family of systems, can be re-used or adapted
for use in other systems. We conjecture that the slot-
filling policies learnt from our experiments with COM-
MUNICATOR will also be good policies for other slot-
filling tasks ? that is, that we are learning ?generic?
slot-filling or information seeking dialogue policies. In
section 5 we describe how the dialogue policies learnt
for slot filling on the COMMUNICATOR data set can be
abstracted and used in the in-car scenarios.
3.2 Fragmentary Clarifications
Another research issue we have been able to explore
in constructing this system is the issue of generating
fragmentary clarifications. The system can be run with
this feature switched on or off (off for comparison with
COMMUNICATOR systems). Instead of a system sim-
ply saying ?Sorry, please repeat that? or some such sim-
ilar simple clarification request when there is a speech
recognition failure, we were able to use the word con-
fidence scores output by the ATK speech recogniser to
generate more intelligent fragmentary clarification re-
quests such as ?Did you say a cheap chinese restau-
rant??. This works by obtaining an ASR confidence
score for each recognised word. We are then able to
try various techniques for clarifying the user utterance.
Many possibilities arise, for example: explicitly clarify
only the highest scoring content word below the rejec-
tion threshold, or, implicitly clarify all content words
and explicitly clarify the lowest scoring content word.
The current platform enables us to test alternative
strategies, and develop more complex ones.
4 The ?In-car? Scenario
The scenario we have designed the system to cover is
that of information seeking about a town, for example
its hotels, restaurants, and bars. We imagine a driver
who is travelling towards this town, or is already there,
who wishes to accomplish relatively complex tasks,
such as finding an italian restaurant near their hotel,
or finding all the wine bars in town, and so on. The
driver/user should be able to specify queries using nat-
ural dialogue, and will receive system output that is a
mixture of spoken and graphical information (e.g. a de-
scription of an item and a map showing its location).
The example town is taken from the (Stuttle et al,
2004) corpus collection materials, and contains a num-
ber of hotels, bars, restaurants, and tourist attractions.
The user should be able to get information on a range
of locations in the town, and the dialogue system will
be used to specify and refine the user queries, as well
as to present information to the user. See the example
dialogue in table 1.
We now describe the dialogue system components.
5 Component-level Description
This section describes the components of the baseline
in-car dialogue system. Communication between com-
ponents is handled by OAA?s asynchronous hub archi-
tecture (Cheyer and Martin, 2001). The major compo-
nents are:
  DIPPER (Bos et al, 2003) ISU dialogue manager,
  Dialogue Policy Reinforcement Learner Agent2,
  ATK for speech recognition,
  Festival speech synthesiser,
2This is written in Python and has an OAA wrapper in C.
120
Utterance Feature
S How can I help you? Open initiative ini-
tial question
U I?m looking for a cheap chinese
restaurant.
User initiative
S Would you like something cen-
tral?
Intention recogni-
tion
U Yes.
S The Dragon Way is an af-
fordable cantonese restaurant
located on Main Street [dis-
played on map].
Multimodal presen-
tation
S Are you happy with those op-
tions?
U Yes, and I also want a double
room.
S Would you like something ex-
pensive, cheap, or reasonably
priced?
Intention recogni-
tion
U A luxury hotel, in the center of
town.
S There are 2 hotel options meet-
ing your requirements. The
Royal Hotel [displayed on
map] is a renovated nineteenth
century palace     The Hotel
Primus [displayed on map] has
big, bright, clean rooms    
Which hotel option would you
like?
Multimodal presen-
tation
U The second one please.
S The phone number of the Hotel
Primus is 2094-227.
S Would you like any further in-
formation?
U I want to find a jazz bar.
S I?m sorry, what kind of bar are
you looking for?
Intention recogni-
tion, Fragmentary
clarification
U A jazz bar.
S A jazz bar. Would you like
something central?
Implicit confirma-
tion
Table 1: Example dialogue, showing system features
  Multimodal Map interface (a java OAA agent),
  Database agent (java OAA wrapper to MySQL).
5.1 Dialogue Policy Learner Agent
This agent acts as an interface between the DIPPER
dialogue manager and the system simulation based on
RL. In particular it has the following solvable:
callRLsimulation(IS file name,
conversational domain, speech act, task,
result).
The first argument is the name of the file that contains
all information about the current information state,
which is required by the RL algorithm to produce
an action. The action returned by the RL agent is
a combination of conversational domain,
speech act, and task. The last argument shows
whether the learnt policy will continue to produce
more actions or release the turn. When run in online
learning mode the agent not only produces an action
when supplied with a state, but at the end of every
dialogue it uses the reward signal to update its learnt
policy. The reward signal is defined in the RL agent,
and is currently a linear combination of task success
metrics combined with a fixed penalty for dialogue
length (see (Henderson et al, 2005)).
This agent can be called whenever the system has
to decide on the next dialogue move. In the original
hand-coded system this decision is made by way of a
dialogue plan (using the ?deliberate? solvable). The
RL agent can be used to drive the entire dialogue pol-
icy, or can be called only in certain circumstances. This
makes it usable for whole dialogue strategies, but also,
if desired, it can be targetted only on specific dialogue
management decisions (e.g. implicit vs. explicit confir-
mation, as was done by (Litman et al, 2000)).
One important research issue is that of tranferring
learnt strategies between domains. We learnt a strat-
egy for the COMMUNICATOR flight booking dialogues
(Lemon et al, 2005; Henderson et al, 2005), but
this is generated by rather different scenarios than the
in-car dialogues. However, both are ?slot-filling? or
information-seeking applications. We defined a map-
ping (described below) between the states and actions
of both systems, in order to construct an interface be-
tween the learnt policies for COMMUNICATOR and the
in-car baseline system.
5.2 Mapping between COMMUNICATOR and
the In-car Domains
There are 2 main problems to be dealt with here:
  mapping between in-car system information states
and COMMUNICATOR information states,
  mapping between learnt COMMUNICATOR sys-
tem actions and in-car system actions.
The learnt COMMUNICATOR policy tells us, based
on a current IS, what the optimal system action
is (for example request info(dest city) or
acknowledgement). Obviously, in the in-car sce-
nario we have no use for task types such as ?destina-
tion city? and ?departure date?. Our method therefore
is to abstract away from the particular details of the
task type, but to maintain the information about dia-
loguemoves and the slot numbers that are under discus-
sion. That is, we construe the learnt COMMUNICATOR
policy as a policy concerning how to fill up to 4 (or-
dered) informational slots, and then access a database
and present results to the user. We also note that some
slots are more essential than others. For example, in
COMMUNICATOR it is essential to have a destination
city, otherwise no results can be found for the user.
Likewise, for the in-car tasks, we consider the food-
type, bar-type, and hotel-location to be more important
to fill than the other slots. This suggests a partial order-
ing on slots via their importance for an application.
In order to do this we define the mappings shown
in table 2 between COMMUNICATOR dialogue actions
and in-car dialogue actions, for each sub-task type of
the in-car system.
121
COMMUNICATOR action In-car action
dest-city food-type
depart-date food-price
depart-time food-location
dest-city hotel-location
depart-date room-type
depart-time hotel-price
dest-city bar-type
depart-date bar-price
depart-time bar-location
Table 2: Action mappings
Note that we treat each of the 3 in-car sub-tasks (ho-
tels, restaurants, bars) as a separate slot-filling dialogue
thread, governed by COMMUNICATOR actions. This
means that the very top level of the dialogue (?How
may I help you?) is not governed by the learnt policy.
Only when we are in a recognised task do we ask the
COMMUNICATOR policy for the next action. Since the
COMMUNICATOR policy is learnt for 4 slots, we ?pre-
fill? a slot3 in the IS when we send it to the Dialogue
Policy Learner Agent in order to retrieve an action.
As for the state mappings, these follow the same
principles. That is, we abstract from the in-car states to
form states that are usable by COMMUNICATOR . This
means that, for example, an in-car state where food-
type and food-price are filled with high confidence is
mapped to a COMMUNICATOR state where dest-city
and depart-date are filled with high confidence, and
all other state information is identical (modulo the task
names). Note that in a future version of the in-car sys-
tem where task switching is allowed we will have to
maintain a separate view of the state for each task.
In terms of the integration of the learnt policies with
the DIPPER system update rules, we have a system flag
which states whether or not to use a learnt policy. If
this flag is present, a different update rule fires when
the system determines what action to take next. For
example, instead of using the deliberate predicate
to access a dialogue plan, we instead call the Dialogue
Policy Learner Agent via OAA, using the current Infor-
mation State of the system. This will return a dialogue
action to the DIPPER update rule.
In current work we are evaluating howwell the learnt
policies work for real users of the in-car system.
6 Conclusions and Future Work
This report has described work done in the TALK
project in building a software prototype baseline ?In-
formation State Update? (ISU)-based dialogue system
in the in-car domain, with the ability to use dialogue
policies derived from machine learning and also to per-
form online learning through interaction. We described
the scenarios, gave a component level description of
the software, and a feature level description and exam-
3We choose ?orig city? because it is the least important
and is already filled at the start of many COMMUNICATOR
dialogues.
ple dialogue.
Evaluation of this system (i.e. comparing the sys-
tem with hand-coded vs. learnt dialogue policies) is
ongoing. Initial evaluation of learnt dialogue policies
(Lemon et al, 2005; Henderson et al, 2005) suggests
that the learnt policy performs at least as well as a rea-
sonable hand-coded system (the TALK policy learnt for
COMMUNICATOR dialogue management outperforms
all the individual hand-coded COMMUNICATOR sys-
tems).
The main achievements made in designing and con-
structing this baseline system have been:
  Combining learnt dialogue policies with an ISU
dialogue manager. This has been done for online
learning, as well as for strategies learnt offline.
  Mapping learnt policies between domains, i.e.
mapping Information States and system actions
between DARPA COMMUNICATOR and in-car in-
formation seeking tasks.
  Fragmentary clarification strategies: the combina-
tion of ATK word confidence scoring with ISU-
based dialogue management rules allows us to ex-
plore word-based clarification techniques.
References
J. Bos, E. Klein, O. Lemon, and T. Oka. 2003.
DIPPER: Description and Formalisation of an
Information-State Update Dialogue System Archi-
tecture. In 4th SIGdial Workshop on Discourse and
Dialogue, Sapporo.
A. Cheyer and D. Martin. 2001. The open agent archi-
tecture. Journal of Autonomous Agents and Multi-
Agent Systems, 4(1):143?148.
K. Georgila, O. Lemon, and J. Henderson. 2005. Au-
tomatic annotation of COMMUNICATOR dialogue
data for learning dialogue strategies and user sim-
ulations. In Ninth Workshop on the Semantics and
Pragmatics of Dialogue (SEMDIAL), DIALOR?05.
J. Henderson, O. Lemon, and K. Georgila. 2005.
Hybrid Reinforcement/Supervised Learning for Di-
alogue Policies from COMMUNICATOR data. In
IJCAI workshop on Knowledge and Reasoning in
Practical Dialogue Systems.
O. Lemon, K. Georgila, J. Henderson, M. Gabsdil,
I. Meza-Ruiz, and S. Young. 2005. D4.1: Inte-
gration of Learning and Adaptivity with the ISU ap-
proach. Technical report, TALK Project.
D. Litman, M. Kearns, S. Singh, and M. Walker. 2000.
Automatic optimization of dialoguemanagement. In
Proc. COLING.
M. Stuttle, J. Williams, and S. Young. 2004. A frame-
work for dialog systems data collection using a sim-
ulated ASR channel. In ICSLP 2004, Jeju, Korea.
122
Discriminative Training of a Neural Network Statistical Parser
James HENDERSON
School of Informatics, University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW
United Kingdom
james.henderson@ed.ac.uk
Abstract
Discriminative methods have shown significant
improvements over traditional generative meth-
ods in many machine learning applications, but
there has been difficulty in extending them to
natural language parsing. One problem is that
much of the work on discriminative methods
conflates changes to the learning method with
changes to the parameterization of the problem.
We show how a parser can be trained with a dis-
criminative learning method while still param-
eterizing the problem according to a generative
probability model. We present three methods
for training a neural network to estimate the
probabilities for a statistical parser, one gen-
erative, one discriminative, and one where the
probability model is generative but the training
criteria is discriminative. The latter model out-
performs the previous two, achieving state-of-
the-art levels of performance (90.1% F-measure
on constituents).
1 Introduction
Much recent work has investigated the applica-
tion of discriminative methods to NLP tasks,
with mixed results. Klein and Manning (2002)
argue that these results show a pattern where
discriminative probability models are inferior
to generative probability models, but that im-
provements can be achieved by keeping a gener-
ative probability model and training according
to a discriminative optimization criteria. We
show how this approach can be applied to broad
coverage natural language parsing. Our estima-
tion and training methods successfully balance
the conflicting requirements that the training
method be both computationally tractable for
large datasets and a good approximation to the
theoretically optimal method. The parser which
uses this approach outperforms both a genera-
tive model and a discriminative model, achiev-
ing state-of-the-art levels of performance (90.1%
F-measure on constituents).
To compare these different approaches, we
use a neural network architecture called Sim-
ple Synchrony Networks (SSNs) (Lane and Hen-
derson, 2001) to estimate the parameters of the
probability models. SSNs have the advantage
that they avoid the need to impose hand-crafted
independence assumptions on the learning pro-
cess. Training an SSN simultaneously trains a
finite representations of the unbounded parse
history and a mapping from this history repre-
sentation to the parameter estimates. The his-
tory representations are automatically tuned to
optimize the parameter estimates. This avoids
the problem that any choice of hand-crafted in-
dependence assumptions may bias our results
towards one approach or another. The indepen-
dence assumptions would have to be different
for the generative and discriminative probabil-
ity models, and even for the parsers which use
the generative probability model, the same set
of independence assumptions may be more ap-
propriate for maximizing one training criteria
over another. By inducing the history represen-
tations specifically to fit the chosen model and
training criteria, we avoid having to choose in-
dependence assumptions which might bias our
results.
Each complete parsing system we propose
consists of three components, a probability
model for sequences of parser decisions, a Sim-
ple Synchrony Network which estimates the pa-
rameters of the probability model, and a proce-
dure which searches for the most probable parse
given these parameter estimates. This paper
outlines each of these components, but more de-
tails can be found in (Henderson, 2003b), and,
for the discriminative model, in (Henderson,
2003a). We also present the training methods,
and experiments on the proposed parsing mod-
els.
2 Two History-Based Probability
Models
As with many previous statistical parsers (Rat-
naparkhi, 1999; Collins, 1999; Charniak, 2000),
we use a history-based model of parsing. De-
signing a history-based model of parsing in-
volves two steps, first choosing a mapping from
the set of phrase structure trees to the set of
parses, and then choosing a probability model
in which the probability of each parser decision
is conditioned on the history of previous deci-
sions in the parse. We use the same mapping
for both our probability models, but we use two
different ways of conditioning the probabilities,
one generative and one discriminative. As we
will show in section 6, these two different ways
of parameterizing the probability model have a
big impact on the ease with which the parame-
ters can be estimated.
To define the mapping from phrase structure
trees to parses, we use a form of left-corner pars-
ing strategy (Rosenkrantz and Lewis, 1970). In
a left-corner parse, each node is introduced after
the subtree rooted at the node?s first child has
been fully parsed. Then the subtrees for the
node?s remaining children are parsed in their
left-to-right order. Parsing a constituent starts
by pushing the leftmost word w of the con-
stituent onto the stack with a shift(w) action.
Parsing a constituent ends by either introducing
the constituent?s parent nonterminal (labeled
Y ) with a project(Y) action, or attaching to the
parent with an attach action.1 A complete parse
consists of a sequence of these actions, d1,..., dm,
such that performing d1,..., dm results in a com-
plete phrase structure tree.
Because this mapping from phrase structure
trees to sequences of decisions about parser
actions is one-to-one, finding the most prob-
able phrase structure tree is equivalent to
finding the parse d1,..., dm which maximizes
P (d1,..., dm|w1,..., wn). This probability is only
nonzero if yield(d1,..., dm) = w1,..., wn, so we
can restrict attention to only those parses
which actually yield the given sentence. With
this restriction, it is equivalent to maximize
P (d1,..., dm), as is done with our first probability
model.
The first probability model is generative, be-
cause it specifies the joint probability of the in-
put sentence and the output tree. This joint
probability is simply P (d1,..., dm), since the
1More details on the mapping to parses can be found
in (Henderson, 2003b).
probability of the input sentence is included in
the probabilities for the shift(wi) decisions in-
cluded in d1,..., dm. The probability model is
then defined by using the chain rule for con-
ditional probabilities to derive the probability
of a parse as the multiplication of the proba-
bilities of each decision di conditioned on that
decision?s prior parse history d1,..., di?1.
P (d1,..., dm) = ?iP (di|d1,..., di?1)
The parameters of this probability model are
the P (di|d1,..., di?1). Generative models are the
standard way to transform a parsing strategy
into a probability model, but note that we are
not assuming any bound on the amount of in-
formation from the parse history which might
be relevant to each parameter.
The second probability model is discrimina-
tive, because it specifies the conditional proba-
bility of the output tree given the input sen-
tence. More generally, discriminative models
try to maximize this conditional probability, but
often do not actually calculate the probabil-
ity, as with Support Vector Machines (Vapnik,
1995). We take the approach of actually calcu-
lating an estimate of the conditional probability
because it differs minimally from the generative
probability model. In this form, the distinc-
tion between our two models is sometimes re-
ferred to as ?joint versus conditional? (John-
son, 2001; Klein and Manning, 2002) rather
than ?generative versus discriminative? (Ng and
Jordan, 2002). As with the generative model,
we use the chain rule to decompose the entire
conditional probability into a sequence of prob-
abilities for individual parser decisions, where
yield(dj ,..., dk) is the sequence of words wi from
the shift(wi) actions in dj ,..., dk.
P (d1,..., dm|yield(d1,..., dm)) =
?iP (di|d1,..., di?1, yield(di,..., dm))
Note that d1,..., di?1 specifies yield(d1,..., di?1),
so it is sufficient to only add yield(di,..., dm) to
the conditional in order for the entire input sen-
tence to be included in the conditional. We
will refer to the string yield(di,..., dm) as the
lookahead string, because it represents all those
words which have not yet been reached by the
parse at the time when decision di is chosen.
The parameters of this model differ from those
of the generative model only in that they in-
clude the lookahead string in the conditional.
Although maximizing the joint probability is
the same as maximizing the conditional proba-
bility, the fact that they have different param-
eters means that estimating one can be much
harder than estimating the other. In general we
would expect that estimating the joint probabil-
ity would be harder than estimating the condi-
tional probability, because the joint probability
contains more information than the conditional
probability. In particular, the probability distri-
bution over sentences can be derived from the
joint probability distribution, but not from the
conditional one. However, the unbounded na-
ture of the parsing problem means that the in-
dividual parameters of the discriminative model
are much harder to estimate than those of the
generative model.
The parameters of the discriminative model
include an unbounded lookahead string in the
conditional. Because these words have not yet
been reached by the parse, we cannot assign
them any structure, and thus the estimation
process has no way of knowing what words in
this string will end up being relevant to the next
decision it needs to make. The estimation pro-
cess has to guess about the future role of an
unbounded number of words, which makes the
estimate quite difficult. In contrast, the param-
eters of the generative model only include words
which are either already incorporated into the
structure, or are the immediate next word to
be incorporated. Thus it is relatively easy to
determine the significance of each word.
3 Estimating the Parameters with a
Neural Network
The most challenging problem in estimat-
ing P (di|d1,..., di?1, yield(di,..., dm)) and
P (di|d1,..., di?1) is that the conditionals
include an unbounded amount of information.
Both the parse history d1,..., di?1 and the
lookahead string yield(di,..., dm) grow with
the length of the sentence. In order to apply
standard probability estimation methods, we
use neural networks to induce finite repre-
sentations of both these sequences, which we
will denote h(d1,..., di?1) and l(yield(di,..., dm)),
respectively. The neural network training
methods we use try to find representations
which preserve all the information about the
sequences which are relevant to estimating the
desired probabilities.
P (di|d1,..., di?1) ? P (di|h(d1,..., di?1))
P (di|d1,..., di?1, yield(di,..., dm)) ?
P (di|h(d1,..., di?1), l(yield(di,..., dm)))
Of the previous work on using neural net-
works for parsing natural language, by far the
most empirically successful has been the work
using Simple Synchrony Networks. Like other
recurrent network architectures, SSNs compute
a representation of an unbounded sequence by
incrementally computing a representation of
each prefix of the sequence. At each position i,
representations from earlier in the sequence are
combined with features of the new position i to
produce a vector of real valued features which
represent the prefix ending at i. This repre-
sentation is called a hidden representation. It
is analogous to the hidden state of a Hidden
Markov Model. As long as the hidden repre-
sentation for position i ? 1 is always used to
compute the hidden representation for position
i, any information about the entire sequence
could be passed from hidden representation to
hidden representation and be included in the
hidden representation of that sequence. When
these representations are then used to estimate
probabilities, this property means that we are
not making any a priori hard independence as-
sumptions (although some independence may
be learned from the data).
The difference between SSNs and most other
recurrent neural network architectures is that
SSNs are specifically designed for process-
ing structures. When computing the his-
tory representation h(d1,..., di?1), the SSN uses
not only the previous history representation
h(d1,..., di?2), but also uses history representa-
tions for earlier positions which are particularly
relevant to choosing the next parser decision di.
This relevance is determined by first assigning
each position to a node in the parse tree, namely
the node which is on the top of the parser?s
stack when that decision is made. Then the
relevant earlier positions are chosen based on
the structural locality of the current decision?s
node to the earlier decisions? nodes. In this way,
the number of representations which informa-
tion needs to pass through in order to flow from
history representation i to history representa-
tion j is determined by the structural distance
between i?s node and j?s node, and not just the
distance between i and j in the parse sequence.
This provides the neural network with a lin-
guistically appropriate inductive bias when it
learns the history representations, as explained
in more detail in (Henderson, 2003b).
When computing the lookahead representa-
tion l(yield(di,..., dm)), there is no structural in-
formation available to tell us which positions are
most relevant to choosing the decision di. Prox-
imity in the string is our only indication of rele-
vance. Therefore we compute l(yield(di,..., dm))
by running a recurrent neural network backward
over the string, so that the most recent input is
the first word in the lookahead string, as dis-
cussed in more detail in (Henderson, 2003a).
Once it has computed h(d1,..., di?1) and (for
the discriminative model) l(yield(di,..., dm)), the
SSN uses standard methods (Bishop, 1995) to
estimate a probability distribution over the set
of possible next decisions di given these repre-
sentations. This involves further decomposing
the distribution over all possible next parser ac-
tions into a small hierarchy of conditional prob-
abilities, and then using log-linear models to
estimate each of these conditional probability
distributions. The input features for these log-
linear models are the real-valued vectors com-
puted by h(d1,..., di?1) and l(yield(di,..., dm)), as
explained in more detail in (Henderson, 2003b).
Thus the full neural network consists of a recur-
rent hidden layer for h(d1,..., di?1), (for the dis-
criminative model) a recurrent hidden layer for
l(yield(di,..., dm)), and an output layer for the
log-linear model. Training is applied to this full
neural network, as described in the next section.
4 Three Optimization Criteria and
their Training Methods
As with many other machine learning methods,
training a Simple Synchrony Network involves
first defining an appropriate learning criteria
and then performing some form of gradient de-
scent learning to search for the optimum values
of the network?s parameters according to this
criteria. In all the parsing models investigated
here, we use the on-line version of Backprop-
agation to perform the gradient descent. This
learning simultaneously tries to optimize the pa-
rameters of the output computation and the
parameters of the mappings h(d1,..., di?1) and
l(yield(di,..., dm)). With multi-layered networks
such as SSNs, this training is not guaranteed to
converge to a global optimum, but in practice
a network whose criteria value is close to the
optimum can be found.
The three parsing models differ in the crite-
ria the neural networks are trained to optimize.
Two of the neural networks are trained using the
standard maximum likelihood approach of opti-
mizing the same probability which they are esti-
mating, one generative and one discriminative.
For the generative model, this means maximiz-
ing the total joint probability of the parses and
the sentences in the training corpus. For the
discriminative model, this means maximizing
the conditional probability of the parses in the
training corpus given the sentences in the train-
ing corpus. To make the computations easier,
we actually minimize the negative log of these
probabilities, which is called cross-entropy er-
ror. Minimizing this error ensures that training
will converge to a neural network whose outputs
are estimates of the desired probabilities.2 For
each parse in the training corpus, Backpropaga-
tion training involves first computing the proba-
bility which the current network assigns to that
parse, then computing the first derivative of
(the negative log of) this probability with re-
spect to each of the network?s parameters, and
then updating the parameters proportionately
to this derivative.3
The third neural network combines the ad-
vantages of the generative probability model
with the advantages of the discriminative opti-
mization criteria. The structure of the network
and the set of outputs which it computes are
exactly the same as the above network for the
generative model. But the training procedure
is designed to maximize the conditional proba-
bility of the parses in the training corpus given
the sentences in the training corpus. The con-
ditional probability for a sentence can be com-
puted from the joint probability of the gener-
ative model by normalizing over the set of all
parses d?1,..., d?m? for the sentence.
P (d1,..., dm|w1,..., wn) = P (d1,...,dm)?
d?1,...,d?m?
P (d?1,...,d?m? )
So, with this approach, we need to maximize
this normalized probability, and not the proba-
bility computed by the network.
The difficulty with this approach is that there
are exponentially many parses for the sentence,
so it is not computationally feasible to com-
pute them all. We address this problem by
only computing a small set of the most prob-
able parses. The remainder of the sum is es-
timated using a combination of the probabili-
ties from the best parses and the probabilities
2Cross-entropy error ensures that the minimum of the
error function converges to the desired probabilities as
the amount of training data increases (Bishop, 1995),
so the minimum for any given dataset is considered an
estimate of the true probabilities.
3A number of additional training techniques, such as
regularization, are added to this basic procedure, as will
be specified in section 6.
from the partial parses which were pruned when
searching for the best parses. The probabilities
of pruned parses are estimated in such a way
as to minimize their effect on the training pro-
cess. For each decision which is part of some un-
pruned parses, we calculate the average proba-
bility of generating the remainder of the sen-
tence by these un-pruned parses, and use this
as the estimate for generating the remainder of
the sentence by the pruned parses. With this
estimate we can calculate the sum of the prob-
abilities for all the pruned parses which origi-
nate from that decision. This approach gives us
a slight overestimate of the total sum, but be-
cause this total sum acts simply as a weighting
factor, it has little effect on learning. What is
important is that this estimate minimizes the
effect of the pruned parses? probabilities on the
part of the training process which occurs after
the probabilities of the best parses have been
calculated.
After estimating P (d1,..., dm|w1,..., wn), train-
ing requires that we estimate the first derivative
of (the negative log of) this probability with re-
spect to each of the network?s parameters. The
contribution to this derivative of the numera-
tor in the above equation is the same as in the
generative case, just scaled by the denominator.
The difference between the two learning meth-
ods is that we also need to account for the con-
tribution to this derivative of the denominator.
Here again we are faced with the problem that
there are an exponential number of derivations
in the denominator, so here again we approxi-
mate this calculation using the most probable
parses.
To increase the conditional probability of the
correct parse, we want to decrease the total joint
probabilities of the incorrect parses. Probability
mass is only lost from the sum over all parses be-
cause shift(wi) actions are only allowed for the
correct wi. Thus we can decrease the total joint
probability of the incorrect parses by making
these parses be worse predictors of the words in
the sentence.4 The combination of training the
correct parses to be good predictors of the words
and training the incorrect parses to be bad pre-
dictors of the words results in prediction prob-
4Non-prediction probability estimates for incorrect
parses can make a small contribution to the derivative,
but because pruning makes the calculation of this con-
tribution inaccurate, we treat this contribution as zero
when training. This means that non-prediction outputs
are trained to maximize the same criteria as in the gen-
erative case.
abilities which are not accurate estimates, but
which are good at discriminating correct parses
from incorrect parses. It is this feature which
gives discriminative training an advantage over
generative training. The network does not need
to learn an accurate model of the distribution
of words. The network only needs to learn an
accurate model of how words disambiguate pre-
vious parsing decisions.
When we apply discriminative training only
to the most probable incorrect parses, we train
the network to discriminate between the correct
parse and those incorrect parses which are the
most likely to be mistaken for the correct parse.
In this sense our approximate training method
results in optimizing the decision boundary be-
tween correct and incorrect parses, rather than
optimizing the match to the conditional prob-
ability. Modifying the training method to sys-
tematically optimize the decision boundary (as
in large margin methods such as Support Vector
Machines) is an area of future research.
5 Searching for the most probable
parse
The complete parsing system uses the probabil-
ity estimates computed by the SSN to search for
the most probable parse. The search incremen-
tally constructs partial parses d1,..., di by taking
a parse it has already constructed d1,..., di?1 and
using the SSN to estimate a probability distri-
bution P (di|d1,..., di?1, ...) over possible next de-
cisions di. These probabilities are then used to
compute the probabilities for d1,..., di. In gen-
eral, the partial parse with the highest proba-
bility is chosen as the next one to be extended,
but to perform the search efficiently it is nec-
essary to prune the search space. The main
pruning is that only a fixed number of the
most probable derivations are allowed to con-
tinue past the shifting of each word. Setting
this post-word beam width to 5 achieves fast
parsing with reasonable performance in all mod-
els. For the parsers with generative probability
models, maximum accuracy is achieved with a
post-word beam width of 100.
6 The Experiments
We used the Penn Treebank (Marcus et al,
1993) to perform empirical experiments on the
proposed parsing models. In each case the input
to the network is a sequence of tag-word pairs.5
5We used a publicly available tagger (Ratnaparkhi,
1996) to provide the tags. For each tag, there is an
We report results for three different vocabulary
sizes, varying in the frequency with which tag-
word pairs must occur in the training set in or-
der to be included explicitly in the vocabulary.
A frequency threshold of 200 resulted in a vo-
cabulary of 508 tag-word pairs, a threshold of 20
resulted in 4215 tag-word pairs, and a threshold
of 5 resulted in 11,993 tag-word pairs
For the generative model we trained net-
works for the 508 (?GSSN-Freq?200?) and 4215
(?GSSN-Freq?20?) word vocabularies. The
need to calculate word predictions makes train-
ing times for the 11,993 word vocabulary very
long, and as of this writing no such network
training has been completed. The discrimina-
tive model does not need to calculate word pre-
dictions, so it was feasible to train networks for
the 11,993 word vocabulary (?DSSN-Freq?5?).
Previous results (Henderson, 2003a) indicate
that this vocabulary size performs better than
the smaller ones, as would be expected.
For the networks trained with the discrimi-
native optimization criteria and the generative
probability model, we trained networks for the
508 (?DGSSN-Freq?200?) and 4215 (?DGSSN-
Freq?20?) word vocabularies. For this train-
ing, we need to select a small set of the most
probable incorrect parses. When we tried using
only the network being trained to choose these
top parses, training times were very long and
the resulting networks did not outperform their
generative counterparts. In the experiments re-
ported here, we provided the training with a
list of the top 20 parses found by a network of
the same type which had been trained with the
generative criteria. The network being trained
was then used to choose its top 10 parses from
this list, and training was performed on these
10 parses and the correct parse.6 This reduced
the time necessary to choose the top parses dur-
ing training, and helped focus the early stages
of training on learning relevant discriminations.
Once the training of these networks was com-
plete, we tested both their ability to parse on
their own and their ability to re-rank the top
unknown-word vocabulary item which is used for all
those words which are not sufficiently frequent with that
tag to be included individually in the vocabulary (as
well as other words if the unknown-word case itself does
not have at least 5 instances). We did no morphological
analysis of unknown words.
6The 20 candidate parses and the 10 training parses
were found with post-word beam widths of 20 and 10,
respectively, so these are only approximations to the top
parses.
20 parses of their associated generative model
(?DGSSN-. . ., rerank?).
We determined appropriate training param-
eters and network size based on intermediate
validation results and our previous experience.7
We trained several networks for each of the
GSSN models and chose the best ones based on
their validation performance. We then trained
one network for each of the DGSSN models
and for the DSSN model. The best post-word
beam width was determined on the validation
set, which was 5 for the DSSN model and 100
for the other models.
To avoid repeated testing on the standard
testing set, we first compare the different mod-
els with their performance on the validation set.
Standard measures of accuracy are shown in ta-
ble 1.8 The largest accuracy difference is be-
tween the parser with the discriminative proba-
bility model (DSSN-Freq?5) and those with the
generative probability model, despite the larger
vocabulary of the former. This demonstrates
the difficulty of estimating the parameters of a
discriminative probability model. There is also
a clear effect of vocabulary size, but there is a
slightly larger effect of training method. When
tested in the same way as they were trained
(for reranking), the parsers which were trained
with a discriminative criteria achieve a 7% and
8% reduction in error rate over their respec-
tive parsers with the same generative probabil-
ity model. When tested alone, these DGSSN
parsers perform only slightly better than their
respective GSSN parsers. Initial experiments on
giving these networks exposure to parses out-
side the top 20 parses of the GSSN parsers at
the very end of training did not result in any im-
provement on this task. This suggests that at
least some of the advantage of the DSSN mod-
els is due to the fact that re-ranking is a simpler
task than parsing from scratch. But additional
experimental work would be necessary to make
any definite conclusions about this issue.
7All the best networks had 80 hidden units for the
history representation (and 80 hidden units in the looka-
head representation). Weight decay regularization was
applied at the beginning of training but reduced to near
0 by the end of training. Training was stopped when
maximum performance was reached on the validation
set, using a post-word beam width of 5.
8All our results are computed with the evalb pro-
gram following the standard criteria in (Collins, 1999),
and using the standard training (sections 2?22, 39,832
sentences, 910,196 words), validation (section 24, 1346
sentence, 31507 words), and testing (section 23, 2416
sentences, 54268 words) sets (Collins, 1999).
LR LP F?=1?
DSSN-Freq?5 84.9 86.0 85.5
GSSN-Freq?200 87.6 88.9 88.2
DGSSN-Freq?200 87.8 88.8 88.3
GSSN-Freq?20 88.2 89.3 88.8
DGSSN-Freq?200, rerank 88.5 89.6 89.0
DGSSN-Freq?20 88.5 89.7 89.1
DGSSN-Freq?20, rerank 89.0 90.3 89.6
Table 1: Percentage labeled constituent recall
(LR), precision (LP), and a combination of both
(F?=1) on validation set sentences of length at
most 100.
LR LP F?=1?
Ratnaparkhi99 86.3 87.5 86.9
Collins99 88.1 88.3 88.2
Collins&Duffy02 88.6 88.9 88.7
Charniak00 89.6 89.5 89.5
Collins00 89.6 89.9 89.7
DGSSN-Freq?20, rerank 89.8 90.4 90.1
Bod03 90.7 90.8 90.7
* F?=1 for previous models may have rounding errors.
Table 2: Percentage labeled constituent recall
(LR), precision (LP), and a combination of both
(F?=1) on the entire testing set.
For comparison to previous results, table 2
lists the results for our best model (DGSSN-
Freq?20, rerank)9 and several other statisti-
cal parsers (Ratnaparkhi, 1999; Collins, 1999;
Collins and Duffy, 2002; Charniak, 2000;
Collins, 2000; Bod, 2003) on the entire testing
set. Our best performing model is more accu-
rate than all these previous models except (Bod,
2003). This DGSSN parser achieves this result
using much less lexical knowledge than other ap-
proaches, which mostly use at least the words
which occur at least 5 times, plus morphological
features of the remaining words. However, the
fact that the DGSSN uses a large-vocabulary
tagger (Ratnaparkhi, 1996) as a preprocessing
stage may compensate for its smaller vocabu-
lary. Also, the main reason for using a smaller
vocabulary is the computational complexity of
computing probabilities for the shift(wi) actions
on-line, which other models do not require.
9On sentences of length at most 40, the DGSSN-
Freq?20-rerank model gets 90.1% recall and 90.7% pre-
cision.
7 Related Work
Johnson (2001) investigated similar issues for
parsing and tagging. His maximal conditional
likelihood estimate for a PCFG takes the same
approach as our generative model trained with
a discriminative criteria. While he shows a
non-significant increase in performance over the
standard maximal joint likelihood estimate on
a small dataset, because he did not have a com-
putationally efficient way to train this model,
he was not able to test it on the standard
datasets. The other models he investigates con-
flate changes in the probability models with
changes in the training criteria, and the discrim-
inative probability models do worse.
In the context of part-of-speech tagging,
Klein and Manning (2002) argue for the same
distinctions made here between discriminative
models and discriminative training criteria, and
come to the same conclusions. However, their
arguments are made in terms of independence
assumptions. Our results show that these gen-
eralizations also apply to methods which do not
rely on independence assumptions.
While both (Johnson, 2001) and (Klein and
Manning, 2002) propose models which use the
parameters of the generative model but train
to optimize a discriminative criteria, neither
proposes training algorithms which are com-
putationally tractable enough to be used for
broad coverage parsing. Our proposed training
method succeeds in being both tractable and
effective, demonstrating both a significant im-
provement over the equivalent generative model
and state-of-the-art accuracy.
Collins (2000) and Collins and Duffy (2002)
also succeed in finding algorithms for training
discriminative models which balance tractabil-
ity with effectiveness, showing improvements
over a generative model. Both these methods
are limited to reranking the output of another
parser, while our trained parser can be used
alone. Neither of these methods use the param-
eters of a generative probability model, which
might explain our better performance (see ta-
ble 2).
8 Conclusions
This article has investigated the application of
discriminative methods to broad coverage nat-
ural language parsing. We distinguish between
two different ways to apply discriminative meth-
ods, one where the probability model is changed
to a discriminative one, and one where the
probability model remains generative but the
training method optimizes a discriminative cri-
teria. We find that the discriminative proba-
bility model is much worse than the generative
one, but that training to optimize the discrimi-
native criteria results in improved performance.
Performance of the latter model on the stan-
dard test set achieves 90.1% F-measure on con-
stituents, which is the second best current ac-
curacy level, and only 0.6% below the current
best (Bod, 2003).
This paper has also proposed a neural net-
work training method which optimizes a dis-
criminative criteria even when the parameters
being estimated are those of a generative prob-
ability model. This training method success-
fully satisfies the conflicting constraints that it
be computationally tractable and that it be a
good approximation to the theoretically optimal
method. This approach contrasts with previous
approaches to scaling up discriminative meth-
ods to broad coverage natural language pars-
ing, which have parameterizations which depart
substantially from the successful previous gen-
erative models of parsing.
References
Christopher M. Bishop. 1995. Neural Networks
for Pattern Recognition. Oxford University
Press, Oxford, UK.
Rens Bod. 2003. An efficient implementation of
a new DOP model. In Proc. 10th Conf. of Eu-
ropean Chapter of the Association for Com-
putational Linguistics, Budapest, Hungary.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proc. 1st Meeting of North
American Chapter of Association for Compu-
tational Linguistics, pages 132?139, Seattle,
Washington.
Michael Collins and Nigel Duffy. 2002. New
ranking algorithms for parsing and tagging:
Kernels over discrete structures and the voted
perceptron. In Proc. 35th Meeting of Asso-
ciation for Computational Linguistics, pages
263?270.
Michael Collins. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania, Philadel-
phia, PA.
Michael Collins. 2000. Discriminative rerank-
ing for natural language parsing. In Proc.
17th Int. Conf. on Machine Learning, pages
175?182, Stanford, CA.
James Henderson. 2003a. Generative ver-
sus discriminative models for statistical left-
corner parsing. In Proc. 8th Int. Workshop on
Parsing Technologies, pages 115?126, Nancy,
France.
James Henderson. 2003b. Inducing history
representations for broad coverage statisti-
cal parsing. In Proc. joint meeting of North
American Chapter of the Association for
Computational Linguistics and the Human
Language Technology Conf., pages 103?110,
Edmonton, Canada.
Mark Johnson. 2001. Joint and conditional es-
timation of tagging and parsing models. In
Proc. 39th Meeting of Association for Compu-
tational Linguistics, pages 314?321, Toulouse,
France.
Dan Klein and Christopher D. Manning. 2002.
Conditional structure versus conditional es-
timation in NLP models. In Proc. Conf. on
Empirical Methods in Natural Language Pro-
cessing, pages 9?16, Univ. of Pennsylvania,
PA.
Peter Lane and James Henderson. 2001. In-
cremental syntactic parsing of natural lan-
guage corpora with Simple Synchrony Net-
works. IEEE Transactions on Knowledge and
Data Engineering, 13(2):219?231.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313?330.
A. Y. Ng and M. I. Jordan. 2002. On discrim-
inative vs. generative classifiers: A compari-
son of logistic regression and naive bayes. In
T. G. Dietterich, S. Becker, and Z. Ghahra-
mani, editors, Advances in Neural Informa-
tion Processing Systems 14, Cambridge, MA.
MIT Press.
Adwait Ratnaparkhi. 1996. A maximum en-
tropy model for part-of-speech tagging. In
Proc. Conf. on Empirical Methods in Natural
Language Processing, pages 133?142, Univ. of
Pennsylvania, PA.
Adwait Ratnaparkhi. 1999. Learning to parse
natural language with maximum entropy
models. Machine Learning, 34:151?175.
D.J. Rosenkrantz and P.M. Lewis. 1970. De-
terministic left corner parsing. In Proc. 11th
Symposium on Switching and Automata The-
ory, pages 139?152.
Vladimir N. Vapnik. 1995. The Nature of
Statistical Learning Theory. Springer-Verlag,
New York.
Proceedings of the 43rd Annual Meeting of the ACL, pages 181?188,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Data-Defined Kernels for Parse Reranking
Derived from Probabilistic Models
James Henderson
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, United Kingdom
james.henderson@ed.ac.uk
Ivan Titov
Department of Computer Science
University of Geneva
24, rue Ge?ne?ral Dufour
CH-1211 Gene`ve 4, Switzerland
ivan.titov@cui.unige.ch
Abstract
Previous research applying kernel meth-
ods to natural language parsing have fo-
cussed on proposing kernels over parse
trees, which are hand-crafted based on do-
main knowledge and computational con-
siderations. In this paper we propose a
method for defining kernels in terms of
a probabilistic model of parsing. This
model is then trained, so that the param-
eters of the probabilistic model reflect the
generalizations in the training data. The
method we propose then uses these trained
parameters to define a kernel for rerank-
ing parse trees. In experiments, we use
a neural network based statistical parser
as the probabilistic model, and use the
resulting kernel with the Voted Percep-
tron algorithm to rerank the top 20 parses
from the probabilistic model. This method
achieves a significant improvement over
the accuracy of the probabilistic model.
1 Introduction
Kernel methods have been shown to be very ef-
fective in many machine learning problems. They
have the advantage that learning can try to optimize
measures related directly to expected testing perfor-
mance (i.e. ?large margin? methods), rather than
the probabilistic measures used in statistical models,
which are only indirectly related to expected test-
ing performance. Work on kernel methods in natural
language has focussed on the definition of appropri-
ate kernels for natural language tasks. In particu-
lar, most of the work on parsing with kernel meth-
ods has focussed on kernels over parse trees (Collins
and Duffy, 2002; Shen and Joshi, 2003; Shen et
al., 2003; Collins and Roark, 2004). These kernels
have all been hand-crafted to try reflect properties
of parse trees which are relevant to discriminating
correct parse trees from incorrect ones, while at the
same time maintaining the tractability of learning.
Some work in machine learning has taken an al-
ternative approach to defining kernels, where the
kernel is derived from a probabilistic model of the
task (Jaakkola and Haussler, 1998; Tsuda et al,
2002). This way of defining kernels has two ad-
vantages. First, linguistic knowledge about parsing
is reflected in the design of the probabilistic model,
not directly in the kernel. Designing probabilistic
models to reflect linguistic knowledge is a process
which is currently well understood, both in terms of
reflecting generalizations and controlling computa-
tional cost. Because many NLP problems are un-
bounded in size and complexity, it is hard to specify
all possible relevant kernel features without having
so many features that the computations become in-
tractable and/or the data becomes too sparse.1 Sec-
ond, the kernel is defined using the trained param-
eters of the probabilistic model. Thus the kernel is
in part determined by the training data, and is auto-
matically tailored to reflect properties of parse trees
which are relevant to parsing.
1For example, see (Henderson, 2004) for a discussion of
why generative models are better than models parameterized to
estimate the a posteriori probability directly.
181
In this paper, we propose a new method for de-
riving a kernel from a probabilistic model which is
specifically tailored to reranking tasks, and we ap-
ply this method to natural language parsing. For the
probabilistic model, we use a state-of-the-art neural
network based statistical parser (Henderson, 2003).
The resulting kernel is then used with the Voted Per-
ceptron algorithm (Freund and Schapire, 1998) to
reranking the top 20 parses from the probabilistic
model. This method achieves a significant improve-
ment over the accuracy of the probabilistic model
alone.
2 Kernels Derived from Probabilistic
Models
In recent years, several methods have been proposed
for constructing kernels from trained probabilistic
models. As usual, these kernels are then used with
linear classifiers to learn the desired task. As well as
some empirical successes, these methods are moti-
vated by theoretical results which suggest we should
expect some improvement with these classifiers over
the classifier which chooses the most probable an-
swer according to the probabilistic model (i.e. the
maximum a posteriori (MAP) classifier). There is
guaranteed to be a linear classifier for the derived
kernel which performs at least as well as the MAP
classifier for the probabilistic model. So, assuming
a large-margin classifier can optimize a more ap-
propriate criteria than the posterior probability, we
should expect the derived kernel?s classifier to per-
form better than the probabilistic model?s classifier,
although empirical results on a given task are never
guaranteed.
In this section, we first present two previous ker-
nels and then propose a new kernel specifically for
reranking tasks. In each of these discussions we
need to characterize the parsing problem as a classi-
fication task. Parsing can be regarded as a mapping
from an input space of sentences x?X to a struc-
tured output space of parse trees y?Y . On the basis
of training sentences, we learn a discriminant func-
tion F : X ? Y ? R. The parse tree y with the
largest value for this discriminant function F (x, y)
is the output parse tree for the sentence x. We focus
on the linear discriminant functions:
Fw(x, y) = <w,?(x, y)>,
where ?(x, y) is a feature vector for the sentence-
tree pair, w is a parameter vector for the discrim-
inant function, and <a, b> is the inner product of
vectors a and b. In the remainder of this section, we
will characterize the kernel methods we consider in
terms of the feature extractor ?(x, y).
2.1 Fisher Kernels
The Fisher kernel (Jaakkola and Haussler, 1998) is
one of the best known kernels belonging to the class
of probability model based kernels. Given a genera-
tive model of P (z|??) with smooth parameterization,
the Fisher score of an example z is a vector of partial
derivatives of the log-likelihood of the example with
respect to the model parameters:
???(z) = (
?logP (z|??)
??1
, . . . , ?logP (z|??)??l ).
This score can be regarded as specifying how the
model should be changed in order to maximize the
likelihood of the example z. Then we can define the
similarity between data points as the inner product
of the corresponding Fisher scores. This kernel is
often referred to as the practical Fisher kernel. The
theoretical Fisher kernel depends on the Fisher in-
formation matrix, which is not feasible to compute
for most practical tasks and is usually omitted.
The Fisher kernel is only directly applicable to
binary classification tasks. We can apply it to our
task by considering an example z to be a sentence-
tree pair (x, y), and classifying the pairs into cor-
rect parses versus incorrect parses. When we use the
Fisher score ???(x, y) in the discriminant function F ,
we can interpret the value as the confidence that the
tree y is correct, and choose the y in which we are
the most confident.
2.2 TOP Kernels
Tsuda (2002) proposed another kernel constructed
from a probabilistic model, called the Tangent vec-
tors Of Posterior log-odds (TOP) kernel. Their TOP
kernel is also only for binary classification tasks, so,
as above, we treat the input z as a sentence-tree pair
and the output category c ? {?1,+1} as incor-
rect/correct. It is assumed that the true probability
distribution is included in the class of probabilis-
tic models and that the true parameter vector ?? is
unique. The feature extractor of the TOP kernel for
182
the input z is defined by:
???(z) = (v(z, ??),
?v(z,??)
??1
, . . . , ?v(z,??)??l ),
where v(z, ??) = logP (c=+1|z, ??) ?
logP (c=?1|z, ??).
In addition to being at least as good as the
MAP classifier, the choice of the TOP kernel fea-
ture extractor is motivated by the minimization of
the binary classification error of a linear classifier
<w,???(z)> + b. Tsuda (2002) demonstrates that
this error is closely related to the estimation error of
the posterior probability P (c=+1|z, ??) by the esti-
mator g(<w,???(z)> + b), where g is the sigmoid
function g(t) = 1/(1 + exp (?t)).
The TOP kernel isn?t quite appropriate for struc-
tured classification tasks because ???(z) is motivated
by binary classificaton error minimization. In the
next subsection, we will adapt it to structured classi-
fication.
2.3 A TOP Kernel for Reranking
We define the reranking task as selecting a parse tree
from the list of candidate trees suggested by a proba-
bilistic model. Furthermore, we only consider learn-
ing to rerank the output of a particular probabilistic
model, without requiring the classifier to have good
performance when applied to a candidate list pro-
vided by a different model. In this case, it is natural
to model the probability that a parse tree is the best
candidate given the list of candidate trees:
P (yk|x, y1, . . . , ys) =
P (x,yk)?
t
P (x,yt)
,
where y1, . . . , ys is the list of candidate parse trees.
To construct a new TOP kernel for reranking, we
apply an approach similar to that used for the TOP
kernel (Tsuda et al, 2002), but we consider the prob-
ability P (yk|x, y1, . . . , ys, ??) instead of the proba-
bility P (c=+1|z, ??) considered by Tsuda. The re-
sulting feature extractor is given by:
???(x, yk) = (v(x, yk, ??),
?v(x,yk,??)
??1
, . . . , ?v(x,yk,??)??l ),
where v(x, yk, ??) = logP (yk|y1, . . . , ys, ??) ?
log
?
t6=k P (yt|y1, . . . , ys, ??). We will call this ker-
nel the TOP reranking kernel.
3 The Probabilistic Model
To complete the definition of the kernel, we need
to choose a probabilistic model of parsing. For
this we use a statistical parser which has previously
been shown to achieve state-of-the-art performance,
namely that proposed in (Henderson, 2003). This
parser has two levels of parameterization. The first
level of parameterization is in terms of a history-
based generative probability model, but this level is
not appropriate for our purposes because it defines
an infinite number of parameters (one for every pos-
sible partial parse history). When parsing a given
sentence, the bounded set of parameters which are
relevant to a given parse are estimated using a neural
network. The weights of this neural network form
the second level of parameterization. There is a fi-
nite number of these parameters. Neural network
training is applied to determine the values of these
parameters, which in turn determine the values of
the probability model?s parameters, which in turn
determine the probabilistic model of parse trees.
We do not use the complete set of neural network
weights to define our kernels, but instead we define a
third level of parameterization which only includes
the network?s output layer weights. These weights
define a normalized exponential model, with the net-
work?s hidden layer as the input features. When we
tried using the complete set of weights in some small
scale experiments, training the classifier was more
computationally expensive, and actually performed
slightly worse than just using the output weights.
Using just the output weights also allows us to make
some approximations in the TOP reranking kernel
which makes the classifier learning algorithm more
efficient.
3.1 A History-Based Probability Model
As with many other statistical parsers (Ratnaparkhi,
1999; Collins, 1999; Charniak, 2000), Henderson
(2003) uses a history-based model of parsing. He
defines the mapping from phrase structure trees to
parse sequences using a form of left-corner parsing
strategy (see (Henderson, 2003) for more details).
The parser actions include: introducing a new con-
stituent with a specified label, attaching one con-
stituent to another, and predicting the next word of
the sentence. A complete parse consists of a se-
quence of these actions, d1,..., dm, such that per-
forming d1,..., dm results in a complete phrase struc-
ture tree.
Because this mapping to parse sequences is
183
one-to-one, and the word prediction actions in
a complete parse d1,..., dm specify the sentence,
P (d1,..., dm) is equivalent to the joint probability of
the output phrase structure tree and the input sen-
tence. This probability can be then be decomposed
into the multiplication of the probabilities of each
action decision di conditioned on that decision?s
prior parse history d1,..., di?1.
P (d1,..., dm) = ?iP (di|d1,..., di?1)
3.2 Estimating Decision Probabilities with a
Neural Network
The parameters of the above probability model are
the P (di|d1,..., di?1). There are an infinite num-
ber of these parameters, since the parse history
d1,..., di?1 grows with the length of the sentence. In
other work on history-based parsing, independence
assumptions are applied so that only a finite amount
of information from the parse history can be treated
as relevant to each parameter, thereby reducing the
number of parameters to a finite set which can be
estimated directly. Instead, Henderson (2003) uses
a neural network to induce a finite representation
of this unbounded history, which we will denote
h(d1,..., di?1). Neural network training tries to find
such a history representation which preserves all the
information about the history which is relevant to es-
timating the desired probability.
P (di|d1,..., di?1) ? P (di|h(d1,..., di?1))
Using a neural network architecture called Simple
Synchrony Networks (SSNs), the history representa-
tion h(d1,..., di?1) is incrementally computed from
features of the previous decision di?1 plus a finite
set of previous history representations h(d1,..., dj),
j < i ? 1. Each history representation is a finite
vector of real numbers, called the network?s hidden
layer. As long as the history representation for po-
sition i ? 1 is always included in the inputs to the
history representation for position i, any information
about the entire sequence could be passed from his-
tory representation to history representation and be
used to estimate the desired probability. However,
learning is biased towards paying more attention to
information which passes through fewer history rep-
resentations.
To exploit this learning bias, structural locality is
used to determine which history representations are
input to which others. First, each history representa-
tion is assigned to the constituent which is on the top
of the parser?s stack when it is computed. Then ear-
lier history representations whose constituents are
structurally local to the current representation?s con-
stituent are input to the computation of the correct
representation. In this way, the number of represen-
tations which information needs to pass through in
order to flow from history representation i to his-
tory representation j is determined by the structural
distance between i?s constituent and j?s constituent,
and not just the distance between i and j in the
parse sequence. This provides the neural network
with a linguistically appropriate inductive bias when
it learns the history representations, as explained in
more detail in (Henderson, 2003).
Once it has computed h(d1,..., di?1), the SSN
uses a normalized exponential to estimate a proba-
bility distribution over the set of possible next deci-
sions di given the history:
P (di|d1,..., di?1, ?) ?
exp(<?di ,h(d1,...,di?1)>)?
t?N(di?1)
exp(<?t,h(d1,...,di?1)>)
,
where by ?t we denote the set of output layer
weights, corresponding to the parser action t,
N(di?1) defines a set of possible next parser actions
after the step di?1 and ? denotes the full set of model
parameters.
We trained SSN parsing models, using the on-line
version of Backpropagation to perform the gradient
descent with a maximum likelihood objective func-
tion. This learning simultaneously tries to optimize
the parameters of the output computation and the pa-
rameters of the mappings h(d1,..., di?1). With multi-
layered networks such as SSNs, this training is not
guaranteed to converge to a global optimum, but in
practice a network whose criteria value is close to
the optimum can be found.
4 Large-Margin Optimization
Once we have defined a kernel over parse trees, gen-
eral techniques for linear classifier optimization can
be used to learn the given task. The most sophis-
ticated of these techniques (such as Support Vec-
tor Machines) are unfortunately too computationally
expensive to be used on large datasets like the Penn
Treebank (Marcus et al, 1993). Instead we use a
184
method which has often been shown to be virtu-
ally as good, the Voted Perceptron (VP) (Freund and
Schapire, 1998) algorithm. The VP algorithm was
originally applied to parse reranking in (Collins and
Duffy, 2002) with the Tree kernel. We modify the
perceptron training algorithm to make it more suit-
able for parsing, where zero-one classification loss
is not the evaluation measure usually employed. We
also develop a variant of the kernel defined in sec-
tion 2.3, which is more efficient when used with the
VP algorithm.
Given a list of candidate trees, we train the clas-
sifier to select the tree with largest constituent F1
score. The F1 score is a measure of the similarity
between the tree in question and the gold standard
parse, and is the standard way to evaluate the accu-
racy of a parser. We denote the k?th candidate tree
for the j?th sentence xj by yjk. Without loss of gener-
ality, let us assume that yj1 is the candidate tree with
the largest F1 score.
The Voted Perceptron algorithm is an ensem-
ble method for combining the various intermediate
models which are produced during training a per-
ceptron. It demonstrates more stable generalization
performance than the normal perceptron algorithm
when the problem is not linearly separable (Freund
and Schapire, 1998), as is usually the case.
We modify the perceptron algorithm by introduc-
ing a new classification loss function. This modifi-
cation enables us to treat differently the cases where
the perceptron predicts a tree with an F1 score much
smaller than that of the top candidate and the cases
where the predicted and the top candidates have sim-
ilar score values. The natural choice for the loss
function would be ?(yjk, y
j
1) = F1(y
j
1) ? F1(y
j
k),
where F1(yjk) denotes the F1 score value for the
parse tree yjk. This approach is very similar to slack
variable rescaling for Support Vector Machines pro-
posed in (Tsochantaridis et al, 2004). The learning
algorithm we employed is presented in figure 1.
When applying kernels with a large training cor-
pus, we face efficiency issues because of the large
number of the neural network weights. Even though
we use only the output layer weights, this vector
grows with the size of the vocabulary, and thus can
be large. The kernels presented in section 2 all lead
to feature vectors without many zero values. This
w = 0
for j = 1 .. n
for k = 2 .. s
if <w,?(xj , yjk)> > <w, ?(xj , y
j
1)>
w = w + ?(yjk, y
j
1)(?(x
j , yj1)? ?(x
j , yjk))
Figure 1: The modified perceptron algorithm
happens because we compute the derivative of the
normalization factor used in the network?s estima-
tion of P (di|d1,..., di?1). This normalization factor
depends on the output layer weights corresponding
to all the possible next decisions (see section 3.2).
This makes an application of the VP algorithm in-
feasible in the case of a large vocabulary.
We can address this problem by freezing the
normalization factor when computing the feature
vector. Note that we can rewrite the model log-
probability of the tree as:
logP (y|?) =
?
i log (
exp(<?di ,h(d1,...,di?1)>)?
t?N(di?1)
exp(<?t,h(d1,...,di?1)>)
) =
?
i(<?di , h(d1,..., di?1)>)??
i log
?
t?N(di?1) exp(<?t, h(d1,..., di?1)>).
We treat the parameters used to compute the first
term as different from the parameters used to com-
pute the second term, and we define our kernel only
using the parameters in the first term. This means
that the second term does not effect the derivatives
in the formula for the feature vector ?(x, y). Thus
the feature vector for the kernel will contain non-
zero entries only in the components corresponding
to the parser actions which are present in the candi-
date derivation for the sentence, and thus in the first
vector component. We have applied this technique
to the TOP reranking kernel, the result of which we
will call the efficient TOP reranking kernel.
5 The Experimental Results
We used the Penn Treebank WSJ corpus (Marcus et
al., 1993) to perform empirical experiments on the
proposed parsing models. In each case the input to
the network is a sequence of tag-word pairs.2 We re-
port results for two different vocabulary sizes, vary-
ing in the frequency with which tag-word pairs must
2We used a publicly available tagger (Ratnaparkhi, 1996) to
provide the tags.
185
occur in the training set in order to be included ex-
plicitly in the vocabulary. A frequency threshold of
200 resulted in a vocabulary of 508 tag-word pairs
(including tag-unknown word pairs) and a threshold
of 20 resulted in 4215 tag-word pairs. We denote
the probabilistic model trained with the vocabulary
of 508 by the SSN-Freq?200, the model trained with
the vocabulary of 4215 by the SSN-Freq?20.
Testing the probabilistic parser requires using a
beam search through the space of possible parses.
We used a form of beam search which prunes the
search after the prediction of each word. We set the
width of this post-word beam to 40 for both testing
of the probabilistic model and generating the candi-
date list for reranking. For training and testing of
the kernel models, we provided a candidate list con-
sisting of the top 20 parses found by the generative
probabilistic model. When using the Fisher kernel,
we added the log-probability of the tree given by the
probabilistic model as the feature. This was not nec-
essary for the TOP kernels because they already con-
tain a feature corresponding to the probability esti-
mated by the probabilistic model (see section 2.3).
We trained the VP model with all three kernels
using the 508 word vocabulary (Fisher-Freq?200,
TOP-Freq?200, TOP-Eff-Freq?200) but only the ef-
ficient TOP reranking kernel model was trained with
the vocabulary of 4215 words (TOP-Eff-Freq?20).
The non-sparsity of the feature vectors for other ker-
nels led to the excessive memory requirements and
larger testing time. In each case, the VP model was
run for only one epoch. We would expect some im-
provement if running it for more epochs, as has been
empirically demonstrated in other domains (Freund
and Schapire, 1998).
To avoid repeated testing on the standard testing
set, we first compare the different models with their
performance on the validation set. Note that the val-
idation set wasn?t used during learning of the kernel
models or for adjustment of any parameters.
Standard measures of accuracy are shown in ta-
ble 1.3 Both the Fisher kernel and the TOP kernels
show better accuracy than the baseline probabilistic
3All our results are computed with the evalb program fol-
lowing the standard criteria in (Collins, 1999), and using the
standard training (sections 2?22, 39,832 sentences, 910,196
words), validation (section 24, 1346 sentence, 31507 words),
and testing (section 23, 2416 sentences, 54268 words) sets
(Collins, 1999).
LR LP F?=1
SSN-Freq?200 87.2 88.5 87.8
Fisher-Freq?200 87.2 88.8 87.9
TOP-Freq?200 87.3 88.9 88.1
TOP-Eff-Freq?200 87.3 88.9 88.1
SSN-Freq?20 88.1 89.2 88.6
TOP-Eff-Freq?20 88.2 89.7 88.9
Table 1: Percentage labeled constituent recall (LR),
precision (LP), and a combination of both (F?=1) on
validation set sentences of length at most 100.
model, but only the improvement of the TOP kernels
is statistically significant.4 For the TOP kernel, the
improvement over baseline is about the same with
both vocabulary sizes. Also note that the perfor-
mance of the efficient TOP reranking kernel is the
same as that of the original TOP reranking kernel,
for the smaller vocabulary.
For comparison to previous results, table 2 lists
the results on the testing set for our best model
(TOP-Efficient-Freq?20) and several other statisti-
cal parsers (Collins, 1999; Collins and Duffy, 2002;
Collins and Roark, 2004; Henderson, 2003; Char-
niak, 2000; Collins, 2000; Shen and Joshi, 2004;
Shen et al, 2003; Henderson, 2004; Bod, 2003).
First note that the parser based on the TOP efficient
kernel has better accuracy than (Henderson, 2003),
which used the same parsing method as our base-
line model, although the trained network parameters
were not the same. When compared to other kernel
methods, our approach performs better than those
based on the Tree kernel (Collins and Duffy, 2002;
Collins and Roark, 2004), and is only 0.2% worse
than the best results achieved by a kernel method for
parsing (Shen et al, 2003; Shen and Joshi, 2004).
6 Related Work
The first application of kernel methods to parsing
was proposed by Collins and Duffy (2002). They
used the Tree kernel, where the features of a tree are
all its connected tree fragments. The VP algorithm
was applied to rerank the output of a probabilistic
model and demonstrated an improvement over the
baseline.
4We measured significance with the randomized signifi-
cance test of (Yeh, 2000).
186
LR LP F?=1?
Collins99 88.1 88.3 88.2
Collins&Duffy02 88.6 88.9 88.7
Collins&Roark04 88.4 89.1 88.8
Henderson03 88.8 89.5 89.1
Charniak00 89.6 89.5 89.5
TOP-Eff-Freq?20 89.1 90.1 89.6
Collins00 89.6 89.9 89.7
Shen&Joshi04 89.5 90.0 89.8
Shen et al03 89.7 90.0 89.8
Henderson04 89.8 90.4 90.1
Bod03 90.7 90.8 90.7
* F?=1 for previous models may have rounding errors.
Table 2: Percentage labeled constituent recall (LR),
precision (LP), and a combination of both (F?=1) on
the entire testing set.
Shen and Joshi (2003) applied an SVM based
voting algorithm with the Preference kernel defined
over pairs for reranking. To define the Preference
kernel they used the Tree kernel and the Linear ker-
nel as its underlying kernels and achieved state-of-
the-art results with the Linear kernel.
In (Shen et al, 2003) it was pointed out that
most of the arbitrary tree fragments allowed by the
Tree kernel are linguistically meaningless. The au-
thors suggested the use of Lexical Tree Adjoining
Grammar (LTAG) based features as a more linguis-
tically appropriate set of features. They empiri-
cally demonstrated that incorporation of these fea-
tures helps to improve reranking performance.
Shen and Joshi (2004) proposed to improve mar-
gin based methods for reranking by defining the
margin not only between the top tree and all the
other trees in the candidate list but between all the
pairs of parses in the ordered candidate list for the
given sentence. They achieved the best results when
training with an uneven margin scaled by the heuris-
tic function of the candidates positions in the list.
One potential drawback of this method is that it
doesn?t take into account the actual F1 score of the
candidate and considers only the position in the list
ordered by the F1 score. We expect that an im-
provement could be achieved by combining our ap-
proach of scaling updates by the F1 loss with the
all pairs approach of (Shen and Joshi, 2004). Use
of the F1 loss function during training demonstrated
better performance comparing to the 0-1 loss func-
tion when applied to a structured classification task
(Tsochantaridis et al, 2004).
All the described kernel methods are limited to
the reranking of candidates from an existing parser
due to the complexity of finding the best parse given
a kernel (i.e. the decoding problem). (Taskar et
al., 2004) suggested a method for maximal mar-
gin parsing which employs the dynamic program-
ming approach to decoding and parameter estima-
tion problems. The efficiency of dynamic program-
ming means that the entire space of parses can be
considered, not just a candidate list. However, not
all kernels are suitable for this method. The dy-
namic programming approach requires the feature
vector of a tree to be decomposable into a sum over
parts of the tree. In particular, this is impossible with
the TOP and Fisher kernels derived from the SSN
model. Also, it isn?t clear whether the algorithm
remains tractable for a large training set with long
sentences, since the authors only present results for
sentences of length less than or equal to 15.
7 Conclusions
This paper proposes a method for deriving a ker-
nel for reranking from a probabilistic model, and
demonstrates state-of-the-art accuracy when this
method is applied to parse reranking. Contrary to
most of the previous research on kernel methods in
parsing, linguistic knowledge does not have to be ex-
pressed through a list of features, but instead can be
expressed through the design of a probability model.
The parameters of this probability model are then
trained, so that they reflect what features of trees are
relevant to parsing. The kernel is then derived from
this trained model in such a way as to maximize its
usefulness for reranking.
We performed experiments on parse reranking us-
ing a neural network based statistical parser as both
the probabilistic model and the source of the list
of candidate parses. We used a modification of
the Voted Perceptron algorithm to perform reranking
with the kernel. The results were amongst the best
current statistical parsers, and only 0.2% worse than
the best current parsing methods which use kernels.
We would expect further improvement if we used
different models to derive the kernel and to gener-
187
ate the candidates, thereby exploiting the advantages
of combining multiple models, as do the better per-
forming methods using kernels.
In recent years, probabilistic models have become
commonplace in natural language processing. We
believe that this approach to defining kernels would
simplify the problem of defining kernels for these
tasks, and could be very useful for many of them.
In particular, maximum entropy models also use a
normalized exponential function to estimate proba-
bilities, so all the methods discussed in this paper
would be applicable to maximum entropy models.
This approach would be particularly useful for tasks
where there is less data available than in parsing, for
which large-margin methods work particularly well.
References
Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proc. 10th Conf. of European Chap-
ter of the Association for Computational Linguistics,
Budapest, Hungary.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. 1st Meeting of North American
Chapter of Association for Computational Linguistics,
pages 132?139, Seattle, Washington.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures and the voted perceptron. In Proc.
40th Meeting of Association for Computational Lin-
guistics, pages 263?270.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proc. 42th
Meeting of Association for Computational Linguistics,
Barcelona, Spain.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia, PA.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proc. 17th Int. Conf. on Ma-
chine Learning, pages 175?182, Stanford, CA.
Yoav Freund and Robert E. Schapire. 1998. Large
margin classification using the perceptron algorithm.
In Proc. of the 11th Annual Conf. on Computational
Learning Theory, pages 209?217, Madisson WI.
James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Proc.
joint meeting of North American Chapter of the Asso-
ciation for Computational Linguistics and the Human
Language Technology Conf., pages 103?110, Edmon-
ton, Canada.
James Henderson. 2004. Discriminative training of
a neural network statistical parser. In Proc. 42nd
Meeting of Association for Computational Linguistics,
Barcelona, Spain.
Tommi S. Jaakkola and David Haussler. 1998. Ex-
ploiting generative models in discriminative classi-
fiers. Advances in Neural Information Processes Sys-
tems 11.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proc. Conf. on Empir-
ical Methods in Natural Language Processing, pages
133?142, Univ. of Pennsylvania, PA.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34:151?175.
Libin Shen and Aravind K. Joshi. 2003. An SVM based
voting algorithm with application to parse reranking.
In Proc. of the 7th Conf. on Computational Natural
Language Learning, pages 9?16, Edmonton, Canada.
Libin Shen and Aravind K. Joshi. 2004. Flexible margin
selection for reranking with full pairwise samples. In
Proc. of the 1st Int. Joint Conf. on Natural Language
Processing, Hainan Island, China.
Libin Shen, Anoop Sarkar, and Aravind K. Joshi. 2003.
Using LTAG based features in parse reranking. In
Proc. of Conf. on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan.
Ben Taskar, Dan Klein, Michael Collins, Daphne Koller,
and Christopher Manning. 2004. Max-margin pars-
ing. In Proc. Conf. on Empirical Methods in Natural
Language Processing, Barcelona, Spain.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proc. 21st Int. Conf. on Machine
Learning, pages 823?830, Banff, Alberta, Canada.
K. Tsuda, M. Kawanabe, G. Ratsch, S. Sonnenburg,
and K. Muller. 2002. A new discriminative ker-
nel from probabilistic models. Neural Computation,
14(10):2397?2414.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of the result differences. In Proc.
17th International Conf. on Computational Linguis-
tics, pages 947?953, Saarbruken, Germany.
188
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 632?639,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Constituent Parsing with Incremental Sigmoid Belief Networks
Ivan Titov
Department of Computer Science
University of Geneva
24, rue Ge?ne?ral Dufour
CH-1211 Gene`ve 4, Switzerland
ivan.titov@cui.unige.ch
James Henderson
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, United Kingdom
james.henderson@ed.ac.uk
Abstract
We introduce a framework for syntactic
parsing with latent variables based on a form
of dynamic Sigmoid Belief Networks called
Incremental Sigmoid Belief Networks. We
demonstrate that a previous feed-forward
neural network parsing model can be viewed
as a coarse approximation to inference with
this class of graphical model. By construct-
ing a more accurate but still tractable ap-
proximation, we significantly improve pars-
ing accuracy, suggesting that ISBNs provide
a good idealization for parsing. This gener-
ative model of parsing achieves state-of-the-
art results on WSJ text and 8% error reduc-
tion over the baseline neural network parser.
1 Introduction
Latent variable models have recently been of in-
creasing interest in Natural Language Processing,
and in parsing in particular (e.g. (Koo and Collins,
2005; Matsuzaki et al, 2005; Riezler et al, 2002)).
Latent variables provide a principled way to in-
clude features in a probability model without need-
ing to have data labeled with those features in ad-
vance. Instead, a labeling with these features can
be induced as part of the training process. The
difficulty with latent variable models is that even
small numbers of latent variables can lead to com-
putationally intractable inference (a.k.a. decoding,
parsing). In this paper we propose a solution to
this problem based on dynamic Sigmoid Belief Net-
works (SBNs) (Neal, 1992). The dynamic SBNs
which we peopose, called Incremental Sigmoid Be-
lief Networks (ISBNs) have large numbers of latent
variables, which makes exact inference intractable.
However, they can be approximated sufficiently well
to build fast and accurate statistical parsers which in-
duce features during training.
We use SBNs in a generative history-based model
of constituent structure parsing. The probability of
an unbounded structure is decomposed into a se-
quence of probabilities for individual derivation de-
cisions, each decision conditioned on the unbounded
history of previous decisions. The most common ap-
proach to handling the unbounded nature of the his-
tories is to choose a pre-defined set of features which
can be unambiguously derived from the history (e.g.
(Charniak, 2000; Collins, 1999)). Decision prob-
abilities are then assumed to be independent of all
information not represented by this finite set of fea-
tures. Another previous approach is to use neural
networks to compute a compressed representation of
the history and condition decisions on this represen-
tation (Henderson, 2003; Henderson, 2004). It is
possible that an unbounded amount of information
is encoded in the compressed representation via its
continuous values, but it is not clear whether this is
actually happening due to the lack of any principled
interpretation for these continuous values.
Like the former approach, we assume that there
are a finite set of features which encode the relevant
information about the parse history. But unlike that
approach, we allow feature values to be ambiguous,
and represent each feature as a distribution over (bi-
nary) values. In other words, these history features
are treated as latent variables. Unfortunately, inter-
632
preting the history representations as distributions
over discrete values of latent variables makes the ex-
act computation of decision probabilities intractable.
Exact computation requires marginalizing out the la-
tent variables, which involves summing over all pos-
sible vectors of discrete values, which is exponential
in the length of the vector.
We propose two forms of approximation for dy-
namic SBNs, a neural network approximation and
a form of mean field approximation (Saul and Jor-
dan, 1999). We first show that the previous neural
network model of (Henderson, 2003) can be viewed
as a coarse approximation to inference with ISBNs.
We then propose an incremental mean field method,
which results in an improved approximation over
the neural network but remains tractable. The re-
sulting parser achieves significantly higher accuracy
than the neural network parser (90.0% F-measure vs
89.1%). We argue that this correlation between bet-
ter approximation and better accuracy suggests that
dynamic SBNs are a good abstract model for natural
language parsing.
2 Sigmoid Belief Networks
A belief network, or a Bayesian network, is a di-
rected acyclic graph which encodes statistical de-
pendencies between variables. Each variable Si in
the graph has an associated conditional probability
distributions P (Si|Par(Si)) over its values given
the values of its parents Par(Si) in the graph. A
Sigmoid Belief Network (Neal, 1992) is a particu-
lar type of belief networks with binary variables and
conditional probability distributions in the form of
the logistic sigmoid function:
P (Si =1|Par(Si)) =
1
1+exp(??Sj?Par(Si) JijSj)
,
where Jij is the weight for the edge from variable
Sj to variable Si. In this paper we consider a gen-
eralized version of SBNs where we allow variables
with any range of discrete values. We thus general-
ize the logistic sigmoid function to the normalized
exponential (a.k.a. softmax) function to define the
conditional probabilities for non-binary variables.
Exact inference with all but very small SBNs
is not tractable. Initially sampling methods were
used (Neal, 1992), but this is also not feasible for
large networks, especially for the dynamic models
of the type described in section 2.2. Variational
methods have also been proposed for approximat-
ing SBNs (Saul and Jordan, 1999). The main idea of
variational methods (Jordan et al, 1999) is, roughly,
to construct a tractable approximate model with a
number of free parameters. The free parameters are
set so that the resulting approximate model is as
close as possible to the original graphical model for
a given inference problem.
2.1 Mean Field Approximation Methods
The simplest example of a variation method is the
mean field method, originally introduced in statis-
tical mechanics and later applied to unsupervised
neural networks in (Hinton et al, 1995). Let us de-
note the set of visible variables in the model (i.e. the
inputs and outputs) by V and hidden variables by
H = h1, . . . , hl. The mean field method uses a fully
factorized distribution Q as the approximate model:
Q(H|V ) =
?
i
Qi(hi|V ).
where each Qi is the distribution of an individual
latent variable. The independence between the vari-
ables hi in this approximate distribution Q does not
imply independence of the free parameters which
define the Qi. These parameters are set to min-
imize the Kullback-Leibler divergence (Cover and
Thomas, 1991) between the approximate distribu-
tion Q(H|V ) and the true distribution P (H|V ):
KL(Q?P ) =
?
H
Q(H|V ) ln Q(H|V )P (H|V ) , (1)
or, equivalently, to maximize the expression:
LV =
?
H
Q(H|V ) ln P (H, V )Q(H|V ) . (2)
The expression LV is a lower bound on the log-
likelihood ln P (V ). It is used in the mean field
theory (Saul and Jordan, 1999) to approximate the
likelihood. However, in our case of dynamic graph-
ical models, we have to use a different approach
which allows us to construct an incremental parsing
method without needing to introduce the additional
parameters proposed in (Saul and Jordan, 1999).
We will describe our modification of the mean field
method in section 3.3.
633
2.2 Dynamics
Dynamic Bayesian networks are Bayesian networks
applied to arbitrarily long sequences. A new set of
variables is instantiated for each position in the se-
quence, but the edges and weights for these variables
are the same as in other positions. The edges which
connect variables instantiated for different positions
must be directed forward in the sequence, thereby
allowing a temporal interpretation of the sequence.
Typically a dynamic Bayesian Network will only in-
volve edges between adjacent positions in the se-
quence (i.e. they are Markovian), but in our parsing
models the pattern of interconnection is determined
by structural locality, rather than sequence locality,
as in the neural networks of (Henderson, 2003).
Using structural locality to define the graph in a
dynamic SBN means that the subgraph of edges with
destinations at a given position cannot be determined
until all the parser decisions for previous positions
have been chosen. We therefore call these models
Incremental SBNs, because, at any given position
in the parse, we only know the graph of edges for
that position and previous positions in the parse. For
example in figure 1, discussed below, it would not
be possible to draw the portion of the graph after t,
because we do not yet know the decision dtk.
The incremental specification of model structure
means that we cannot use an undirected graphical
model, such as Conditional Random Fields. With
a directed dynamic model, all edges connecting the
known portion of the graph to the unknown portion
of the graph are directed toward the unknown por-
tion. Also there are no variables in the unknown
portion of the graph whose values are known (i.e. no
visible variables), because at each step in a history-
based model the decision probability is conditioned
only on the parsing history. Only visible variables
can result in information being reflected backward
through a directed edge, so it is impossible for any-
thing in the unknown portion of the graph to affect
the probabilities in the known portion of the graph.
Therefore inference can be performed by simply ig-
noring the unknown portion of the graph, and there
is no need to sum over all possible structures for the
unknown portion of the graph, as would be neces-
sary for an undirected graphical model.
Figure 1: Illustration of an ISBN.
3 The Probabilistic Model of Parsing
In this section we present our framework for syn-
tactic parsing with dynamic Sigmoid Belief Net-
works. We first specify the form of SBN we propose,
namely ISBNs, and then two methods for approx-
imating the inference problems required for pars-
ing. We only consider generative models of pars-
ing, since generative probability models are simpler
and we are focused on probability estimation, not
decision making. Although the most accurate pars-
ing models (Charniak and Johnson, 2005; Hender-
son, 2004; Collins, 2000) are discriminative, all the
most accurate discriminative models make use of a
generative model. More accurate generative models
should make the discriminative models which use
them more accurate as well. Also, there are some
applications, such as language modeling, which re-
quire generative models.
3.1 The Graphical Model
In ISBNs, we use a history-based model, which de-
composes the probability of the parse as:
P (T ) = P (D1, ..., Dm) =
?
t
P (Dt|D1, . . . , Dt?1),
where T is the parse tree and D1, . . . , Dm is its
equivalent sequence of parser decisions. Instead of
treating each Dt as atomic decisions, it is convenient
to further split them into a sequence of elementary
decisions Dt = dt1, . . . , dtn:
P (Dt|D1, . . . , Dt?1) =
?
k
P (dtk|h(t, k)),
where h(t, k) denotes the parsing history
D1, . . . , Dt?1, dt1, . . . , dtk?1. For example, a
634
decision to create a new constituent can be divided
in two elementary decisions: deciding to create a
constituent and deciding which label to assign to it.
We use a graphical model to define our proposed
class of probability models. An example graphical
model for the computation of P (dtk|h(t, k)) is
illustrated in figure 1.
The graphical model is organized into vectors
of variables: latent state variable vectors St? =
st?1 , . . . , st
?
n , representing an intermediate state of the
parser at derivation step t?, and decision variable
vectors Dt? = dt?1 , . . . , dt
?
l , representing a parser de-
cision at derivation step t?, where t? ? t. Variables
whose value are given at the current decision (t, k)
are shaded in figure 1, latent and output variables are
left unshaded.
As illustrated by the arrows in figure 1, the prob-
ability of each state variable st?i depends on all the
variables in a finite set of relevant previous state and
decision vectors, but there are no direct dependen-
cies between the different variables in a single state
vector. Which previous state and decision vectors
are connected to the current state vector is deter-
mined by a set of structural relations specified by
the parser designer. For example, we could select
the most recent state where the same constituent was
on the top of the stack, and a decision variable rep-
resenting the constituent?s label. Each such selected
relation has its own distinct weight matrix for the
resulting edges in the graph, but the same weight
matrix is used at each derivation position where the
relation is relevant.
As indicated in figure 1, the probability of each
elementary decision dt?k depends both on the current
state vector St? and on the previously chosen ele-
mentary action dt?k?1 from Dt
?
. This probability dis-
tribution has the form of a normalized exponential:
P (dt?k = d|St
?, dt?k?1)=
?h(t?,k)(d) e
?
j Wdjs
t?
j
?
d??h(t?,k)(d?) e
?
jWd?js
t?
j
, (3)
where ?h(t?,k) is the indicator function of a set of
elementary decisions that may possibly follow the
parsing history h(t?, k), and the Wdj are the weights.
For our experiments, we replicated the same pat-
tern of interconnection between state variables as
described in (Henderson, 2003).1 We also used the
1In the neural network of (Henderson, 2003), our variables
same left-corner parsing strategy, and the same set of
decisions, features, and states. We refer the reader to
(Henderson, 2003) for details.
Exact computation with this model is not
tractable. Sampling of parse trees from the model
is not feasible, because a generative model defines a
joint model of both a sentence and a tree, thereby re-
quiring sampling over the space of sentences. Gibbs
sampling (Geman and Geman, 1984) is also impos-
sible, because of the huge space of variables and
need to resample after making each new decision in
the sequence. Thus, we know of no reasonable alter-
natives to the use of variational methods.
3.2 A Feed-Forward Approximation
The first model we consider is a strictly incremental
computation of a variational approximation, which
we will call the feed-forward approximation. It can
be viewed as the simplest form of mean field approx-
imation. As in any mean field approximation, each
of the latent variables is independently distributed.
But unlike the general case of mean field approxi-
mation, in the feed-forward approximation we only
allow the parameters of the distributions Qi to de-
pend on the distributions of their parents. This addi-
tional constraint increases the potential for a large
Kullback-Leibler divergence with the true model,
defined in expression (1), but it significantly simpli-
fies the computations.
The set of hidden variables H in our graphical
model consists of all the state vectors St? , t? ? t,
and the last decision dtk. All the previously observed
decisions h(t, k) comprise the set of visible vari-
ables V . The approximate fully factorisable distri-
bution Q(H|V ) can be written as:
Q(H|V ) = qtk(dtk)
?
t?,i
(
?t?i
)st?i (1 ? ?t?i
)1?st?i .
where ?t?i is the free parameter which determines the
distribution of state variable i at position t?, namely
its mean, and qtk(dtk) is the free parameter which de-
termines the distribution over decisions dtk.
Because we are only allowed to use information
about the distributions of the parent variables to
map to their ?units?, and our dependencies/edges map to their
?links?.
635
compute the free parameters ?t?i , the optimal assign-
ment of values to the ?t?i is:
?t?i = ?
(
?t?i
)
,
where ? denotes the logistic sigmoid function and
?t?i is a weighted sum of the parent variables? means:
?t?i =
?
t???RS(t?)
?
j
J?(t
?,t??)
ij ?t
??
j +
?
t???RD(t?)
?
k
B?(t
?,t??)
idt??k
, (4)
where RS(t?) is the set of previous positions with
edges from their state vectors to the state vector at t?,
RD(t?) is the set of previous positions with edges
from their decision vectors to the state vector at t?,
?(t?, t??) is the relevant relation between the position
t?? and the position t?, and J?ij and B?id are weight
matrices.
In order to maximize (2), the approximate distri-
bution of the next decisions qtk(d) should be set to
qtk(d) =
?h(t,k) (d) e
?
j Wdj?
t
j
?
d? ?h(t,k) (d?) e
?
j Wd?j?
t
j
, (5)
as follows from expression (3). The resulting esti-
mate of the tree probability is given by:
P (T ) ?
?
t,k
qtk(dtk).
This approximation method replicates exactly the
computation of the feed-forward neural network
in (Henderson, 2003), where the above means ?t?i
are equivalent to the neural network hidden unit acti-
vations. Thus, that neural network probability model
can be regarded as a simple approximation to the
graphical model introduced in section 3.1.
In addition to the drawbacks shared by any mean
field approximation method, this feed-forward ap-
proximation cannot capture backward reasoning.
By backward (a.k.a. top-down) reasoning we mean
the need to update the state vector means ?t?i after
observing a decision dtk, for t? ? t. The next section
discusses how backward reasoning can be incorpo-
rated in the approximate model.
3.3 A Mean Field Approximation
This section proposes a more accurate way to ap-
proximate ISBNs with mean field methods, which
we will call the mean field approximation. Again,
we are interested in finding the distribution Q which
maximizes the quantity LV in expression (2). The
decision distribution qtk(dtk) maximizes LV when it
has the same dependence on the state vector means
?tk as in the feed-forward approximation, namely ex-
pression (5). However, as we mentioned above, the
feed-forward computation does not allow us to com-
pute the optimal values of state means ?t?i .
Optimally, after each new decision dtk, we should
recompute all the means ?t?i for all the state vec-
tors St? , t? ? t. However, this would make the
method intractable, due to the length of derivations
in constituent parsing and the interdependence be-
tween these means. Instead, after making each deci-
sion dtk and adding it to the set of visible variables V ,
we recompute only means of the current state vector
St.
The denominator of the normalized exponential
function in (3) does not allow us to compute LV ex-
actly. Instead, we use a simple first order approxi-
mation:
EQ[ln
?
d
?h(t,k) (d) exp(
?
j
Wdjstj)]
? ln
?
d
?h(t,k)(d) exp(
?
j
Wdj?tj), (6)
where the expectation EQ[. . .] is taken over the state
vector St distributed according to the approximate
distribution Q.
Unfortunately, even with this assumption there is
no analytic way to maximize LV with respect to the
means ?tk, so we need to use numerical methods.
Assuming (6), we can rewrite the expression (2) as
follows, substituting the true P (H, V ) defined by
the graphical model and the approximate distribu-
tion Q(H|V ), omitting parts independent of ?tk:
Lt,kV =
?
i
??ti ln ?ti ? (1 ? ?ti) ln
(
1 ? ?ti
)
+?ti?ti +
?
k?<k
?h(t,k?)(dtk?)
?
j
Wdtk?j?
t
j
?
?
k?<k
ln
?
?
?
d
?h(t,k?)(d) exp(
?
j
Wdj?tj)
?
?, (7)
here, ?ti is computed from the previous relevant state
means and decisions as in (4). This expression is
636
concave with respect to the parameters ?ti, so the
global maximum can be found. We use coordinate-
wise ascent, where each ?ti is selected by an efficient
line search (Press et al, 1996), while keeping other
?ti? fixed.
3.4 Parameter Estimation
We train these models to maximize the fit of the
approximate model to the data. We use gradient
descent and a maximum likelihood objective func-
tion. This requires computation of the gradient of
the approximate log-likelihood with respect to the
model parameters. In order to compute these deriva-
tives, the error should be propagated all the way
back through the structure of the graphical model.
For the feed-forward approximation, computation of
the derivatives is straightforward, as in neural net-
works. But for the mean field approximation, it re-
quires computation of the derivatives of the means
?ti with respect to the other parameters in expres-
sion (7). The use of a numerical search in the mean
field approximation makes the analytical computa-
tion of these derivatives impossible, so a different
method needs to be used to compute their values. If
maximization of Lt,kV is done until convergence, then
the derivatives of Lt,kV with respect to ?ti are close to
zero:
F t,ki =
?Lt,kV
??ti
? 0 for all i.
This system of equations allows us to use implicit
differentiation to compute the needed derivatives.
4 Experimental Evaluation
In this section we evaluate the two approximations
to dynamic SBNs discussed in the previous section,
the feed-forward method equivalent to the neural
network of (Henderson, 2003) (NN method) and the
mean field method (MF method). The hypothesis
we wish to test is that the more accurate approxima-
tion of dynamic SBNs will result in a more accurate
model of constituent structure parsing. If this is true,
then it suggests that dynamic SBNs of the form pro-
posed here are a good abstract model of the nature
of natural language parsing.
We used the Penn Treebank WSJ corpus (Marcus
et al, 1993) to perform the empirical evaluation of
the considered approaches. It is expensive to train
R P F1
Bikel, 2004 87.9 88.8 88.3
Taskar et al, 2004 89.1 89.1 89.1
NN method 89.1 89.2 89.1
Turian and Melamed, 2006 89.3 89.6 89.4
MF method 89.3 90.7 90.0
Charniak, 2000 90.0 90.2 90.1
Table 1: Percentage labeled constituent recall (R),
precision (P), combination of both (F1) on the test-
ing set.
the MF approximation on the whole WSJ corpus, so
instead we use only sentences of length at most 15,
as in (Taskar et al, 2004) and (Turian and Melamed,
2006). The standard split of the corpus into training
(sections 2?22, 9,753 sentences), validation (section
24, 321 sentences), and testing (section 23, 603 sen-
tences) was performed.2
As in (Henderson, 2003; Turian and Melamed,
2006) we used a publicly available tagger (Ratna-
parkhi, 1996) to provide the part-of-speech tag for
each word in the sentence. For each tag, there is an
unknown-word vocabulary item which is used for all
those words which are not sufficiently frequent with
that tag to be included individually in the vocabu-
lary. We only included a specific tag-word pair in the
vocabulary if it occurred at least 20 time in the train-
ing set, which (with tag-unknown-word pairs) led to
the very small vocabulary of 567 tag-word pairs.
During parsing with both the NN method and the
MF method, we used beam search with a post-word
beam of 10. Increasing the beam size beyond this
value did not significantly effect parsing accuracy.
For both of the models, the state vector size of 40
was used. All the parameters for both the NN and
MF models were tuned on the validation set. A sin-
gle best model of each type was then applied to the
final testing set.
Table 1 lists the results of the NN approximation
and the MF approximation, along with results of dif-
2Training of our MF method on this subset of WSJ took less
than 6 days on a standard desktop PC. We would expect that
a model for the entire WSJ corpus can be trained in about 3
months time. The training time is about linear with the num-
ber of words, but a larger state vector is needed to accommo-
date all the information. The long training times on the entire
WSJ would not allow us to tune the model parameters properly,
which would have increased the randomness of the empirical
comparison, although it would be feasible for building a sys-
tem.
637
ferent generative and discriminative parsing meth-
ods (Bikel, 2004; Taskar et al, 2004; Turian and
Melamed, 2006; Charniak, 2000) evaluated in the
same experimental setup. The MF model improves
over the baseline NN approximation, with an error
reduction in F-measure exceeding 8%. This im-
provement is statically significant.3 The MF model
achieves results which do not appear to be signifi-
cantly different from the results of the best model
in the list (Charniak, 2000). It should also be noted
that the model (Charniak, 2000) is the most accu-
rate generative model on the standard WSJ parsing
benchmark, which confirms the viability of our gen-
erative model.
These experimental results suggest that Incre-
mental Sigmoid Belief Networks are an appropriate
model for natural language parsing. Even approxi-
mations such as those tested here, with a very strong
factorisability assumption, allow us to build quite
accurate parsing models. The main drawback of our
proposed mean field approach is the relative compu-
tational complexity of the numerical procedure used
to maximize Lt,kV . But this approximation has suc-
ceeded in showing that a more accurate approxima-
tion of ISBNs results in a more accurate parser. We
believe this provides strong justification for more ac-
curate approximations of ISBNs for parsing.
5 Related Work
There has not been much previous work on graph-
ical models for full parsing, although recently sev-
eral latent variable models for parsing have been
proposed (Koo and Collins, 2005; Matsuzaki et al,
2005; Riezler et al, 2002). In (Koo and Collins,
2005), an undirected graphical model is used for
parse reranking. Dependency parsing with dynamic
Bayesian networks was considered in (Peshkin and
Savova, 2005), with limited success. Their model
is very different from ours. Roughly, it considered
the whole sentence at a time, with the graphical
model being used to decide which words correspond
to leaves of the tree. The chosen words are then
removed from the sentence and the model is recur-
sively applied to the reduced sentence.
Undirected graphical models, in particular Condi-
3We measured significance of all the experiments in this pa-
per with the randomized significance test (Yeh, 2000).
tional Random Fields, are the standard tools for shal-
low parsing (Sha and Pereira, 2003). However, shal-
low parsing is effectively a sequence labeling prob-
lem and therefore differs significantly from full pars-
ing. As discussed in section 2.2, undirected graph-
ical models do not seem to be suitable for history-
based full parsing models.
Sigmoid Belief Networks were used originally
for character recognition tasks, but later a dynamic
modification of this model was applied to the rein-
forcement learning task (Sallans, 2002). However,
their graphical model, approximation method, and
learning method differ significantly from those of
this paper.
6 Conclusions
This paper proposes a new generative framework
for constituent parsing based on dynamic Sigmoid
Belief Networks with vectors of latent variables.
Exact inference with the proposed graphical model
(called Incremental Sigmoid Belief Networks) is
not tractable, but two approximations are consid-
ered. First, it is shown that the neural network
parser of (Henderson, 2003) can be considered as a
simple feed-forward approximation to the graphical
model. Second, a more accurate but still tractable
approximation based on mean field theory is pro-
posed. Both methods are empirically compared, and
the mean field approach achieves significantly better
results, which are non-significantly different from
the results of the most accurate generative parsing
model (Charniak, 2000) on our testing set. The fact
that a more accurate approximation leads to a more
accurate parser suggests that ISBNs are a good ab-
stract model for constituent structure parsing. This
empirical result motivates research into more accu-
rate approximations of dynamic SBNs.
We focused in this paper on generative models
of parsing. The results of such a generative model
can be easily improved by a discriminative rerank-
ing model, even without any additional feature en-
gineering. For example, the discriminative train-
ing techniques successfully applied in (Henderson,
2004) to the feed-forward neural network model can
be directly applied to the mean field model pro-
posed in this paper. The same is true for rerank-
ing with data-defined kernels, with which we would
638
expect similar improvements as were achieved with
the neural network parser (Henderson and Titov,
2005). Such improvements should situate the result-
ing model among the best current parsing models.
References
Dan M. Bikel. 2004. Intricacies of Collins? parsing
model. Computational Linguistics, 30(4).
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. ACL, pages 173?180, Ann Arbor, MI.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. ACL, pages 132?139, Seattle, Wash-
ington.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia, PA.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proc. ICML, pages 175?182,
Stanford, CA.
Thomas M. Cover and Joy A. Thomas. 1991. Elements
of Information Theory. John Wiley, New York, NY.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6:721?741.
James Henderson and Ivan Titov. 2005. Data-defined
kernels for parse reranking derived from probabilistic
models. In Proc. ACL, Ann Arbor, MI.
James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Proc.
HLT-NAACL, pages 103?110, Edmonton, Canada.
James Henderson. 2004. Discriminative training of
a neural network statistical parser. In Proc. ACL,
Barcelona, Spain.
G. Hinton, P. Dayan, B. Frey, and R. Neal. 1995.
The wake-sleep algorithm for unsupervised neural net-
works. Science, 268:1158?1161.
M. I. Jordan, Z.Ghahramani, T. S. Jaakkola, and L. K.
Saul. 1999. An introduction to variational methods for
graphical models. In Michael I. Jordan, editor, Learn-
ing in Graphical Models. MIT Press, Cambridge, MA.
Terry Koo and Michael Collins. 2005. Hidden-variable
models for discriminative reranking. In Proc. EMNLP,
Vancouver, B.C., Canada.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proc. ACL, Ann Arbor, MI.
Radford Neal. 1992. Connectionist learning of belief
networks. Artificial Intelligence, 56:71?113.
Leon Peshkin and Virginia Savova. 2005. Dependency
parsing with dynamic bayesian network. In AAAI,
20th National Conference on Artificial Intelligence,
Pittsburgh, Pennsylvania.
W. Press, B. Flannery, S. Teukolsky, and W. Vetterling.
1996. Numerical Recipes. Cambridge University
Press, Cambridge, UK.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proc. EMNLP, pages
133?142, Univ. of Pennsylvania, PA.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proc. ACL, Philadelphia, PA.
Brian Sallans. 2002. Reinforcement Learning for Fac-
tored Markov Decision Processes. Ph.D. thesis, Uni-
versity of Toronto, Toronto, Canada.
Lawrence K. Saul and Michael I. Jordan. 1999. A
mean field learning algorithm for unsupervised neu-
ral networks. In Michael I. Jordan, editor, Learning in
Graphical Models, pages 541?554. MIT Press, Cam-
bridge, MA.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proc. HLT-NAACL,
Edmonton, Canada.
Ben Taskar, Dan Klein, Michael Collins, Daphne Koller,
and Christopher Manning. 2004. Max-margin pars-
ing. In Proc. EMNLP, Barcelona, Spain.
Joseph Turian and Dan Melamed. 2006. Advances in
discriminative parsing. In Proc. COLING-ACL, Syd-
ney, Australia.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of the result differences. In Proc.
COLING, pages 947?953, Saarbruken, Germany.
639
Lookahead in Deterministic Left-Corner Parsing?
James HENDERSON
School of Informatics, University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW
United Kingdom
james.henderson@ed.ac.uk
Abstract
To support incremental interpretation, any
model of human sentence processing must not
only process the sentence incrementally, it must
to some degree restrict the number of analyses
which it produces for any sentence prefix. De-
terministic parsing takes the extreme position
that there can only be one analysis for any sen-
tence prefix. Experiments with an incremen-
tal statistical parser show that performance is
severely degraded when the search for the most
probable parse is pruned to only the most prob-
able analysis after each prefix. One method
which has been extensively used to address the
difficulty of deterministic parsing is lookahead,
where information about a bounded number
of subsequent words is used to decide which
analyses to pursue. We simulate the effects of
lookahead by summing probabilities over pos-
sible parses for the lookahead words and using
this sum to choose which parse to pursue. We
find that a large improvement is achieved with
one word lookahead, but that more lookahead
results in relatively small additional improve-
ments. This suggests that one word lookahead
is sufficient, but that other modifications to our
left-corner parsing model could make determin-
istic parsing more effective.
1 Introduction
Incremental interpretation is a fundamental
property of the human parsing mechanism. To
support incremental interpretation, any model
of sentence processing must not only process the
sentence incrementally, it must to some degree
restrict the number of analyses which it pro-
duces for any sentence prefix. Otherwise the
ambiguity of natural language would make the
number of possible interpretations at any point
in the parse completely overwhelming. Deter-
? This work has been supported by the Department of
Computer Science, University of Geneva.
ministic parsing takes the extreme position that
there can only be one analysis for any sentence
prefix. We investigate methods which make
such a strong constraint feasible, in particular
the use of lookahead.
In this paper we do not try to construct a sin-
gle deterministic parser, but instead consider a
family of deterministic parsers and empirically
measure the optimal performance of a determin-
istic parser in this family. As has been previ-
ously proposed by Brants and Crocker (2000),
we take a corpus-based approach to this em-
pirical investigation, using a previously defined
statistical parser (Henderson, 2003). The sta-
tistical parser uses an incremental history-based
probability model based on left-corner parsing,
and the parameters of this model are estimated
using a neural network. Performance of this
basic model is state-of-the-art, making these re-
sults likely to generalize beyond this specific sys-
tem.
We specify the family of deterministic parsers
in terms of pruning the search for the most prob-
able parse. Both deterministic parsing and the
use of k-word lookahead are characterized as
constraints on pruning this search. We then de-
rive the optimal pruning strategy given these
constraints and the probabilities provided by
the statistical parser?s left-corner probability
model. Empirical experiments on the accuracy
of a parser which uses this pruning method in-
dicate the best accuracy we could expect from
a deterministic parser of this kind. This al-
lows us to compare different deterministic pars-
ing methods, in particular the use of different
amounts of lookahead.
In the remainder of this paper, we first dis-
cuss how the principles of deterministic parsing
can be expressed in terms of constraints on the
search strategy used by a statistical parser. We
then present the probability model used by the
statistical parser, the way a neural network is
used to estimate the parameters of this proba-
bility model, and the methods used to search for
the most probable parse according these param-
eters. Finally, we present the empirical experi-
ments on deterministic parsing with lookahead,
and discuss the implications of these results.
2 Approximating Optimal
Deterministic Parsing
The general principles of deterministic parsing,
as proposed by Marcus (1980), are that parsing
proceeds incrementally from left to right, and
that once a parsing decision has been made, it
cannot be revoked or overridden by an alter-
native analysis. We translate the first principle
into the design of a statistical parser by using an
incremental generative probability model. Such
a model provides us with probabilities for par-
tial parses which generate prefixes of the sen-
tence and which do not depend on the words
not in this prefix. We can then translate the
second principle into constraints on how a sta-
tistical parser chooses which partial parses to
pursue further as it searches for the most prob-
able complete parse.
The principle that decisions cannot be re-
voked or overridden means that, given a se-
quence of parser actions a1,..., ai?1 which we
have already chosen, we need to choose a sin-
gle parser action ai before considering any sub-
sequent parser action ai+1. However, this con-
straint does not prevent considering the effects
of multiple alternative parser actions for ai be-
fore choosing between them. This leaves a great
deal of flexibility for the design of a determin-
istic parser, because the set of actions defined
by a deterministic parser does not have to be
the same as the basic decisions defined by our
probability model. We can combine any fi-
nite sequence of decisions dj ,..., dj+l from our
probability model into a single parser action ai.
This combination allows a deterministic parser
to consider the effects of the entire sequence of
decisions dj ,..., dj+l before deciding whether to
choose it. Different deterministic parser designs
will combine the basic decisions into parser ac-
tions in different ways, thereby imposing differ-
ent constraints on how long a sequence of fu-
ture decisions dj ,..., dj+l can be considered be-
fore choosing a parser action.
Once we have made a distinction between the
basic decisions of the probability model dj and
the parser actions ai = dj ,..., dj+l, it is conve-
nient to express the choice of the parse a1,..., an
as a search for the most probable d1,..., dm,
where a1,..., an = d1,..., dm. The search incre-
mentally constructs partial parses and prunes
this search down to a single partial parse after
each complete parser action. In other words,
given that the search has so far chosen the par-
tial parse a1,..., ai?1 = d1,..., dj?1, the search
first considers all the possible partial parses
d1,..., dj?1, dj ,..., dj+l where there exists an ai =
dj ,..., dj+l. The search is then pruned down to
only the best d1,..., dj?1, dj ,..., dj+l from this set,
and the search continues with all partial parses
containing this prefix. Thus the search is al-
lowed to delay pruning for as many basic deci-
sions as are combined into a single parser action.
Rather than considering one single determin-
istic parser design, in this paper we consider a
family of deterministic parser designs. We then
determine tight upper bounds on the perfor-
mance of any deterministic parser in this fam-
ily. We define a family of deterministic parsers
by starting with a particular incremental gen-
erative probability model, and consider a range
of ways to define parser action ai as finite se-
quences dj ,..., dj+l of these basic decisions.
We define the family of parser designs as al-
lowing the combination of any sequence of de-
cisions which occur between the parsing of two
words. After a word has been incorporated into
the parse, this constraint allows the search to
consider all the possible decision sequences lead-
ing up to the incorporation of the next word,
but not beyond. When the next word is reached,
the search must again be pruned down to a sin-
gle analysis. This is a natural point to prune,
because it is the position where new informa-
tion about the sentence is available. Given this
definition of the family of deterministic parsers
and the fact that we are only concerned with
an upper bound on a deterministic parser?s per-
formance, there is no need to consider parser
designs which require more pruning than this,
since they will never perform as well as a parser
which requires less pruning.
Unfortunately, allowing the combination of
any sequence of decisions which occur between
the parsing of two words does not exactly corre-
spond to the constraints on deterministic pars-
ing. This is because we cannot put a finite upper
bound on the number of actions which occur be-
tween two words. Thus this class of parsers in-
cludes non-deterministic parsers, and therefore
our performance results represent only an up-
per bound on the performance which could be
achieved by a deterministic parser in the class.
However, there is good reason to believe this
is a tight upper bound. Lexicalized theories of
syntax all assume that the amount of informa-
tion about the syntactic structure contributed
by each word is finite, and that all the informa-
tion in the syntactic structure is contributed by
some word. Thus it should possible to distribute
all the information about the structure across
the parse in such a way that a finite amount
falls in between each word. The parsing order
we use (a form of left-corner parsing) seems to
achieve this fairly well, except for the fact that
it uses a stack. Parsing right-branching struc-
tures, such as are found in English, results in
the stack growing arbitrarily large, and then the
whole stack needs to be popped at the end of
the sentence. With the exception of these se-
quences of popping actions, the number of ac-
tions which occur between any two words could
be bounded. In our training set, the bound on
the number of non-popping actions between any
two words could be set at just 4.
In addition to designing parser actions to
make deterministic parsing easier, another
mechanism which is commonly used in deter-
ministic parser designs is lookahead. With
lookahead, information about words which have
not yet been incorporated into the parse can be
used to decide what action to choose next. We
consider models where the lookahead consists of
some small fixed-length prefix of the un-parsed
portion of the sentence, which we call k-word
lookahead. This mechanisms is constrained by
the requirement that the parser be incremental,
since a deterministic parser with k-word looka-
head can only provide an interpretation for the
portion of the sentence which is k words be-
hind what has been input so far. Thus it is not
possible to include the entire unboundedly-long
sentence in the lookahead. The family of deter-
ministic parsers with k-word lookahead would
include parsers which sometimes choose parser
actions without waiting to see all k words (and
thus on average allow interpretation sooner),
but because here we are only concerned with
the optimal performance achievable with a given
lookahead, we do not have to consider these al-
ternatives.
The optimal deterministic parser with looka-
head will choose the partial parse which is the
most likely to lead to the correct complete parse
given the previous partial parse plus the k words
of lookahead. In other words, we are trying
to maximize P (at+1|a1,..., at, wt+1,..., wt+k),
which is the same as maximizing
P (at+1, wt+1,..., wt+k|a1,..., at) for the given
wt+1,..., wt+k. (Note that any partial parse
a1,..., at generates the words w1,..., wt, because
the optimal deterministic parser designs we
are considering all have parser actions which
combine the entire portion of a parse between
one word and another.) We can compute this
probability by summing over all parses which
include the partial parse a1,..., at+1 and which
generate the lookahead string wt+1,..., wt+k.
P (at+1, wt+1,..., wt+k|a1,..., at) =
?
(at+2,...,at+k) P (at+1, at+2,..., at+k|a1,..., at)
where at+1,..., at+k generates wt+1,..., wt+k .
Because the parser actions are defined in terms
of basic decisions in the probability model, we
can compute this sum directly using the prob-
ability model. A real deterministic parser can-
not actually perform this computation explic-
itly, because it involves pursuing multiple anal-
yses which are then discarded. But ideally a
deterministic parser should compute an esti-
mate which approximates this sum. Thus we
can compute the performance of a deterministic
parser which makes the ideal use of lookahead
by explicitly computing this sum. Again, this
will be an upper bound on the performance of
a real deterministic parser, but we can reason-
ably expect that a real deterministic parser can
reach performance quite close to this ideal for a
small amount of lookahead.
This approach to lookahead can also be ex-
pressed in terms of pruning the search for the
best parse. After pruning to a single par-
tial parse a1,..., at which ends by generating
wt, the search is allowed to pursue multiple
parses in parallel until they generate the word
wt+k. The probabilities for these new partial
parses are then summed to get estimates of
P (at+1, wt+1,..., wt+k|a1,..., at) for each possible
at+1, and these sums are used to choose a single
at+1. The search is then pruned by removing all
partial parses which do not start with a1,..., at+1.
The remaining partial parses are then contin-
ued until they generate the word wt+k+1, and
their probabilities are summed to decide how to
prune to a single choice of at+2.
By expressing the family of deterministic
parsers with lookahead in terms of a pruning
strategy on a basic parsing model, we are able to
easily investigate the effects of different looka-
head lengths on the maximum performance of
a deterministic parser in this family. To com-
plete the specification of the family of determin-
istic parsers, we simple have to specify the basic
parsing model, as done in the next section.
3 A Generative Left-Corner
Probability Model
As with several previous statistical parsers
(Collins, 1999; Charniak, 2000), we use a gen-
erative history-based probability model of pars-
ing. Designing a history-based model of pars-
ing involves two steps, first choosing a mapping
from the set of phrase structure trees to the set
of parses, and then choosing a probability model
in which the probability of each parser decision
is conditioned on the history of previous deci-
sions in the parse. For the model to be genera-
tive, these decisions must include predicting the
words of the sentence. To support incremental
parsing, we want to map phrase structure trees
to parses which predict the words of the sen-
tence in their left-to-right order. To support de-
terministic parsing, we want our parses to spec-
ify information about the phrase structure tree
at appropriate points in the sentence. For these
reasons, we choose a form of left-corner parsing
(Rosenkrantz and Lewis, 1970).
In a left-corner parse, each node is introduced
after the subtree rooted at the node?s first child
has been fully parsed. Then the subtrees for the
node?s remaining children are parsed in their
left-to-right order. In the form of left-corner
parsing we use, parsing a constituent starts by
pushing the leftmost word w of the constituent
onto the stack with a shift(w) action. Parsing a
constituent ends by either introducing the con-
stituent?s parent nonterminal (labeled Y ) with
a project(Y) action, or attaching to the parent
with a attach action.
More precisely, this parsing strategy is a ver-
sion of left-corner parsing which first applies
right-binarization to the grammar, as is done
in (Manning and Carpenter, 1997) except that
we binarize down to nullary rules rather than
to binary rules. This means that choosing the
children for a node is done one child at a time,
and that ending the sequence of children is a
separate choice. We also extended the parsing
strategy slightly to handle Chomsky adjunction
structures (i.e. structures of the form [X [X . . .]
[Y . . .]]) as a special case. The Chomsky ad-
junction is removed and replaced with a special
?modifier? link in the tree (becoming [X . . . [mod
Y . . .]]). This means that the parser?s set of
basic actions includes modify, as well as attach,
shift(w), and project(Y). We also compiled some
frequent chains of non-branching nodes (such as
[S [VP . . .]]) into a single node with a new la-
bel (becoming [S-VP . . .]). All these grammar
transforms are undone before any evaluation of
the output trees is performed.
Because this mapping from phrase structure
trees to sequences of parser decisions is one-to-
one, finding the most probable phrase structure
tree is equivalent to finding the parse d1,..., dm
which maximizes P (d1,..., dm), as is done in gen-
erative models. Because this probability in-
cludes the probabilities of the shift(wi) deci-
sions, this is the joint probability of the phrase
structure tree and the sentence. The probabil-
ity model is then defined by using the chain rule
for conditional probabilities to derive the prob-
ability of a parse as the multiplication of the
probabilities of each decision di conditioned on
that decision?s prior parse history d1,..., di?1.
P (d1,..., dm) = ?iP (di|d1,..., di?1)
The parameters of this probability model are
the P (di|d1,..., di?1). Generative models are the
standard way to transform a parsing strategy
into a probability model, but note that we are
not assuming any bound on the amount of in-
formation from the parse history which might
be relevant to each parameter.
4 Estimating the Parameters with a
Neural Network
The most challenging problem in estimating
P (di|d1,..., di?1) is that the conditional includes
an unbounded amount of information. The
parse history d1,..., di?1 grows with the length of
the sentence. In order to apply standard prob-
ability estimation methods, we use neural net-
works to induce finite representations of this se-
quence, which we will denote h(d1,..., di?1). The
neural network training methods we use try to
find representations which preserve all the infor-
mation about the sequences which are relevant
to estimating the desired probabilities.
P (di|d1,..., di?1) ? P (di|h(d1,..., di?1))
Of the previous work on using neural net-
works for parsing natural language, by far the
most empirically successful has been the work
using Simple Synchrony Networks (Henderson,
2003). Like other recurrent network architec-
tures, SSNs compute a representation of an un-
bounded sequence by incrementally computing
a representation of each prefix of the sequence.
At each position i, representations from earlier
in the sequence are combined with features of
the new position i to produce a vector of real
valued features which represent the prefix end-
ing at i. This representation is called a hidden
representation. It is analogous to the hidden
state of a Hidden Markov Model. As long as
the hidden representation for position i?1 is al-
ways used to compute the hidden representation
for position i, any information about the entire
sequence could be passed from hidden represen-
tation to hidden representation and be included
in the hidden representation of that sequence.
When these representations are then used to es-
timate probabilities, this property means that
we are not making any a priori hard indepen-
dence assumptions.
The difference between SSNs and most other
recurrent neural network architectures is that
SSNs are specifically designed for process-
ing structures. When computing the his-
tory representation h(d1,..., di?1), the SSN uses
not only the previous history representation
h(d1,..., di?2), but also uses history representa-
tions for earlier positions which are particularly
relevant to choosing the next parser decision di.
This relevance is determined by first assigning
each position to a node in the parse tree, namely
the node which is on the top of the parser?s
stack when that decision is made. Then the
relevant earlier positions are chosen based on
the structural locality of the current decision?s
node to the earlier decisions? nodes. In this way,
the number of representations which informa-
tion needs to pass through in order to flow from
history representation i to history representa-
tion j is determined by the structural distance
between i?s node and j?s node, and not just the
distance between i and j in the parse sequence.
This provides the neural network with a lin-
guistically appropriate inductive bias when it
learns the history representations, as explained
in more detail in (Henderson, 2003). The fact
that this bias is both structurally defined and
linguistically appropriate is the reason that this
parser performs so much better than previous
attempts at using neural networks for parsing,
such as (Costa et al, 2001).
Once it has computed h(d1,..., di?1), the SSN
uses standard methods (Bishop, 1995) to esti-
mate a probability distribution over the set of
possible next decisions di given these represen-
tations. This involves further decomposing the
distribution over all possible next parser actions
into a small hierarchy of conditional probabili-
ties, and then using log-linear models to esti-
mate each of these conditional probability dis-
tributions. The input features for these log-
linear models are the real-valued vectors com-
puted by h(d1,..., di?1), as explained in more de-
tail in (Henderson, 2003).
As with many other machine learning meth-
ods, training a Simple Synchrony Network in-
volves first defining an appropriate learning cri-
teria and then performing some form of gra-
dient descent learning to search for the opti-
mum values of the network?s parameters accord-
ing to this criteria. We use the on-line ver-
sion of Backpropagation to perform the gradi-
ent descent. This learning simultaneously tries
to optimize the parameters of the output com-
putation and the parameters of the mapping
h(d1,..., di?1). With multi-layered networks such
as SSNs, this training is not guaranteed to con-
verge to a global optimum, but in practice a
network whose criteria value is close to the op-
timum can be found.
5 Searching for the most probable
parse
As discussed in section 2, we investigate de-
terministic parsing by translating the princi-
ples of deterministic parsing into properties of
the pruning strategy used to search the space
of possible parses. The complete parsing sys-
tem alternates between using the search strat-
egy to decide what partial parse d1,..., di?1 to
pursue further and using the SSN to estimate
a probability distribution P (di|d1,..., di?1) over
possible next decisions di. The probabilities
P (d1,..., di) for the new partial parses are then
just P (d1,..., di?1) ? P (di|d1,..., di?1). When no
pruning applies, the partial parse with the high-
est probability is chosen as the next one to be
extended.
Even in the non-deterministic version of the
parser, we need to prune the search space. This
is because the number of possible parses is ex-
ponential in the length of the sentence, and
we cannot use dynamic programming to com-
pute the best parse efficiently because we do not
make any independence assumptions. However,
we have found that the search can be drasti-
cally pruned without loss in accuracy, using a
similar approach to that used here to model de-
terministic parsing. After the prediction of each
word, we prune all partial parses except a fixed
beam of the most probable partial parses. Due
to the use of the above left-corner parsing order,
we have found that the beam can be as little as
100 parses without having any measurable effect
on accuracy. Below we will refer to this beam
width as the post-word search beam width.
In addition to pruning after the prediction of
each word, we also prune the search space in be-
tween two words by limiting its branching fac-
tor to at most 5. This, in effect, just limits the
number of labels considered for each new non-
terminal. We found that increasing the branch-
ing factor had no effect on accuracy and little
effect on speed.
For the simulations of deterministic parsers,
we always applied both the above pruning
strategies, in addition to the deterministic prun-
ing. This non-deterministic pruning reduces
the number of partial parses a1,..., at+1,..., at+k
whose probabilities are included in the sum used
to choose at+1 for the deterministic pruning.
This approximation is not likely to have any
significant effect on the choice of at+1, because
the probabilities of the partial parses which are
pruned by the non-deterministic pruning tend
to be very small compared to the most prob-
able alternatives. The non-deterministic prun-
ing also reduces the set of partial parses which
are chosen between during the subsequent de-
terministic pruning. But this undoubtedly has
no significant effect, since experimental results
have shown that the level of non-deterministic
pruning discussed above does not effect perfor-
mance even without deterministic pruning.
6 The Experiments
To investigate the effects of lookahead on our
family of deterministic parsers, we ran empirical
experiments on the standard the Penn Treebank
(Marcus et al, 1993) datasets. The input to
the network is a sequence of tag-word pairs.1
We report results for a vocabulary size of 508
tag-word pairs (a frequency threshold of 200).
We first trained a network to estimate the pa-
rameters of the basic probability model. We de-
termined appropriate training parameters and
network size based on intermediate validation
1We used a publicly available tagger (Ratnaparkhi,
1996) to provide the tags. This tagger is run before the
parser, so there may be some information about future
words which is available in the disambiguated tag which
is not available in the word itself. We don?t think this has
had a significant impact on the results reported here, but
currently we are working on doing the tagging internally
to the parser to avoid this problem.
80
82
84
86
88
90
0 2 4 6 8 10 12 14 16
deterministic recall
deterministic precision
non-deterministic recall
non-deterministic precision
Figure 1: Labeled constituent recall and pre-
cision as a function of the number of words
of lookahead used by a deterministic parser.
Curves reach their non-deterministic perfor-
mance with large lookahead.
results and our previous experience.2 We
trained several networks and chose the best ones
based on their validation performance. The
best post-word search beam width for the non-
deterministic parser was determined on the val-
idation set, which was 100.
To avoid repeated testing on the standard
testing set, we measured the performance of
the different models on section 0 of the Penn
Treebank (which is not included in either the
training or validation sets). Standard measures
of accuracy for different lookahead lengths are
plotted in figure 1.3 First we should note that
the non-deterministic parser has state-of-the-
art accuracy (89.0% F-measure), considering its
vocabulary size. A moderately larger vocabu-
lary version (4215 tag-word pairs) of this parser
achieves 89.8% F-measure on section 0, where
the best current result on the testing set is
90.7% (Bod, 2003).
As expected, the deterministic parsers do
worse than the non-deterministic one, and this
difference becomes less as the lookahead is
lengthened. What is surprising about the curves
in figure 1 is that there is a very large increase
in performance from zero words of lookahead
2The best network had 80 hidden units for the history
representation. Weight decay regularization was applied
at the beginning of training but reduced to near 0 by the
end of training. Training was stopped when maximum
performance was reached on the validation set, using a
post-word beam width of 5.
3All our results are computed with the evalb program
following the standard criteria in (Collins, 1999). We
used the standard training (sections 2?22, 39,832 sen-
tences, 910,196 words) and validation (section 24, 1346
sentence, 31507 words) sets (Collins, 1999). Results of
the nondeterministic parser average 0.2% worse on the
standard testing set, and average 0.8% better when a
larger vocabulary (4215 tag-word pairs) is used.
(i.e. pruning the search to 1 alternative directly
after every word) to one word of lookahead. Af-
ter one word of lookahead the curves show rel-
atively moderate improvements with each addi-
tional word of lookahead, converging to the non-
deterministic level, as would be expected.4 But
between zero words of lookahead and one word
of lookahead there is a 5.6% absolute improve-
ment in F-measure (versus a 0.9% absolute im-
provement between one and two words of looka-
head). In other words, adding the first word
of lookahead results in a 2/3 reduction in the
difference between the deterministic and non-
deterministic parser?s F-measure, while adding
subsequent words results in at most a 1/3 re-
duction per word.
7 Discussion
The large improvement in performance which
results from adding the first word of lookahead,
as compared to adding the subsequent words,
indicates that the first word of lookahead has
a qualitatively different effect on deterministic
parsing. We believe that one word of lookahead
is both necessary and sufficient for a model of
deterministic parsing.
The large gain provided by the first word of
lookahead indicates that this lookahead is nec-
essary for deterministic parsing. Given the fact
the with one word of lookahead the F-measure
of the deterministic parser is only 2.7% below
the maximum possible, it is unlikely that the
family of deterministic parsers assumed here is
so sub-optimal that the entire 5.6% improve-
ment gained with one word lookahead is simply
the result of compensating for limitations in the
choice of this family.
The performance curves in figure 1 also sug-
gest that one word of lookahead is sufficient. We
believe the gain provided by more than one word
of lookahead is the result of compensating for
limitations in the family of deterministic parsers
assumed here. Any limitations in this family
will result in the deterministic search making
choices before the necessary disambiguating in-
formation is available, thereby leading to addi-
tional errors. As the lookahead increases, some
previously mistaken choices will become disam-
biguated by the additional lookahead informa-
tion, thereby improving performance. In the
limit as lookahead increases, the performance of
4Note that when the lookahead length is longer
than the longest sentence, the deterministic and non-
deterministic parsers become equivalent.
the deterministic and non-deterministic parsers
will become the same, no matter what family
of deterministic parsers has been specified. The
smooth curve of increasing performance as the
lookahead is increased above one word is the
type of results we would expect if the lookahead
were simply correcting mistakes in this way.
Examples of possible limitations to the fam-
ily of deterministic parsers assumed here include
the choice of the left-corner ordering of parser
decisions. The left-corner ordering completely
determines when each decision about the phrase
structure tree must be made. If the family of de-
terministic parsers had more flexibility in this
ordering, then the optimal deterministic parser
could use an ordering which was tailored to the
statistics of the data, thereby avoiding being
forced to make decisions before sufficient infor-
mation is available.
8 Conclusions
In this paper we have investigated issues in de-
terministic parsing by characterizing these is-
sues in terms of the search procedure used by
a statistical parser. We use a neural network
to estimate the probabilities for an incremental
history-based probability model based on left-
corner parsing. Using an unconstrained search
procedure to try to find the most probable parse
according to this probability model (i.e. non-
deterministic parsing) results in state-of-the-art
accuracy. Deterministic parsing is simulated
by allowing the sequence of decisions between
two words to be combined into a single parser
action, and choosing the best single combined
action based on the probability calculated us-
ing the basic left-corner probability model. All
parses which do not use this chosen action are
then pruned from the search. When this prun-
ing is applied directly after each word, there is
a large reduction in accuracy (8.3% F-measure)
as compared to the non-deterministic search.
Given the pervasive ambiguity in natural lan-
guage, it is not surprising that this drastic prun-
ing strategy results in a large reduction in ac-
curacy. For this reason, deterministic parsers
usually use some form of lookahead. Looka-
head gives the parser more information about
the sentence at the point when the choice of
the next parser action takes place. We sim-
ulate the optimal use of k-word lookahead by
summing over all partial parses which continue
the given partial parse to the point where all
k words in the lookahead have been generated.
When expressed in terms of search, this means
that the deterministic pruning is done k words
behind a non-deterministic search for the best
parse, based on a sum over the partial parses
found by the non-deterministic search. When
accuracy is plotted as a function of k (figure 1),
we found that there is a large increase in accu-
racy when the first word of lookahead is added
(only 2.7% F-measure below non-deterministic
search). Further increases in the lookahead
length have much less of an impact.
We conclude that the first word of lookahead
is necessary for the success of any deterministic
parser, but that additional lookahead is proba-
bly not necessary. The remaining error created
by this model of deterministic parsing is proba-
bly best dealt with by investigating other aspect
of the model of deterministic parsing assumed
here, in particular the strict adherence to the
left-corner parsing order.
Despite the need to consider alternatives to
the left-corner parsing order, these results do
demonstrate that the left-corner parsing strat-
egy proposed is surprisingly good at supporting
deterministic parsing. This fact is important
in making the non-deterministic search strat-
egy used with this parser tractable. The obser-
vations made in this paper could lead to more
sophisticated search strategies which further in-
crease the speed of this or similar parsers with-
out significant reductions in accuracy.
References
Christopher M. Bishop. 1995. Neural Networks
for Pattern Recognition. Oxford University
Press, Oxford, UK.
Rens Bod. 2003. An efficient implementation of
a new DOP model. In Proc. 10th Conf. of Eu-
ropean Chapter of the Association for Com-
putational Linguistics, Budapest, Hungary.
Thorsten Brants and Matthew Crocker. 2000.
Probabilistic parsing and psychological plau-
sibility. In Proceedings of the Eighteenth
Conference on Computational Linguistics
(COLING-2000), Saarbru?cken / Luxemburg
/ Nancy.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proc. 1st Meeting of North
American Chapter of Association for Compu-
tational Linguistics, pages 132?139, Seattle,
Washington.
Michael Collins. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania, Philadel-
phia, PA.
F. Costa, V. Lombardo, P. Frasconi, and
G. Soda. 2001. Wide coverage incremental
parsing by learning attachment preferences.
In Proc. of the Conf. of the Italian Associa-
tion for Artificial Intelligence.
James Henderson. 2003. Inducing history rep-
resentations for broad coverage statistical
parsing. In Proc. joint meeting of North
American Chapter of the Association for
Computational Linguistics and the Human
Language Technology Conf., pages 103?110,
Edmonton, Canada.
Christopher D. Manning and Bob Carpenter.
1997. Probabilistic parsing using left corner
language models. In Proc. Int. Workshop on
Parsing Technologies, pages 147?158.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313?330.
Mitchell Marcus. 1980. A Theory of Syntac-
tic Recognition for Natural Language. MIT
Press, Cambridge, MA.
Adwait Ratnaparkhi. 1996. A maximum en-
tropy model for part-of-speech tagging. In
Proc. Conf. on Empirical Methods in Natural
Language Processing, pages 133?142, Univ. of
Pennsylvania, PA.
D.J. Rosenkrantz and P.M. Lewis. 1970. De-
terministic left corner parsing. In Proc. 11th
Symposium on Switching and Automata The-
ory, pages 139?152.
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 560?567,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Loss Minimization in Parse Reranking
Ivan Titov
Department of Computer Science
University of Geneva
24, rue Ge?ne?ral Dufour
CH-1211 Gene`ve 4, Switzerland
ivan.titov@cui.unige.ch
James Henderson
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, United Kingdom
james.henderson@ed.ac.uk
Abstract
We propose a general method for reranker
construction which targets choosing the
candidate with the least expected loss,
rather than the most probable candidate.
Different approaches to expected loss ap-
proximation are considered, including es-
timating from the probabilistic model used
to generate the candidates, estimating
from a discriminative model trained to
rerank the candidates, and learning to ap-
proximate the expected loss. The pro-
posed methods are applied to the parse
reranking task, with various baseline mod-
els, achieving significant improvement
both over the probabilistic models and the
discriminative rerankers. When a neural
network parser is used as the probabilistic
model and the Voted Perceptron algorithm
with data-defined kernels as the learning
algorithm, the loss minimization model
achieves 90.0% labeled constituents F1
score on the standard WSJ parsing task.
1 Introduction
The reranking approach is widely used in pars-
ing (Collins and Koo, 2005; Koo and Collins,
2005; Henderson and Titov, 2005; Shen and Joshi,
2003) as well as in other structured classifica-
tion problems. For structured classification tasks,
where labels are complex and have an internal
structure of interdependency, the 0-1 loss consid-
ered in classical formulation of classification al-
gorithms is not a natural choice and different loss
functions are normally employed. To tackle this
problem, several approaches have been proposed
to accommodate loss functions in learning algo-
rithms (Tsochantaridis et al, 2004; Taskar et al,
2004; Henderson and Titov, 2005). A very differ-
ent use of loss functions was considered in the ar-
eas of signal processing and machine translation,
where direct minimization of expected loss (Min-
imum Bayes Risk decoding) on word sequences
was considered (Kumar and Byrne, 2004; Stol-
cke et al, 1997). The only attempt to use Mini-
mum Bayes Risk (MBR) decoding in parsing was
made in (Goodman, 1996), where a parsing al-
gorithm for constituent recall minimization was
constructed. However, their approach is limited
to binarized PCFG models and, consequently, is
not applicable to state-of-the-art parsing meth-
ods (Charniak and Johnson, 2005; Henderson,
2004; Collins, 2000). In this paper we consider
several approaches to loss approximation on the
basis of a candidate list provided by a baseline
probabilistic model.
The intuitive motivation for expected loss mini-
mization can be seen from the following example.
Consider the situation where there are a group of
several very similar candidates and one very dif-
ferent candidate whose probability is just slightly
larger than the probability of any individual candi-
date in the group, but much smaller than their total
probability. A method which chooses the maxi-
mum probability candidate will choose this outlier
candidate, which is correct if you are only inter-
ested in getting the label exactly correct (i.e. 0-1
loss), and you think the estimates are accurate. But
if you are interested in a loss function where the
loss is small when you choose a candidate which
is similar to the correct candidate, then it is better
to choose one of the candidates in the group. With
this choice the loss will only be large if the outlier
turns out to be correct, while if the outlier is cho-
sen then the loss will be large if any of the group
are correct. In other words, the expected loss of
560
choosing a member of the group will be smaller
than that for the outlier.
More formally, the Bayes risk of a model y =
h(x) is defined as
R(h) = Ex,y?(y, h(x)), (1)
where the expectation is taken over all the possi-
ble inputs x and labels y and ?(y, y?) denotes a
loss incurred by assigning x to y? when the correct
label is y. We assume that the loss function pos-
sesses values within the range from 0 to 1, which
is equivalent to the requirement that the loss func-
tion is bounded in (Tsochantaridis et al, 2004). It
follows that an optimal reranker h? is one which
chooses the label y that minimizes the expected
loss:
h?(x) = arg min
y??G(x)
?
y
P (y|x)?(y, y?), (2)
where G(x) denotes a candidate list provided by
a baseline probabilistic model for the input x.
In this paper we propose different approaches to
loss approximation. We apply them to the parse
reranking problem where the baseline probabilis-
tic model is a neural network parser (Henderson,
2003), and to parse reranking of candidates pro-
vided by the (Collins, 1999) model. The result-
ing reranking method achieves very significant im-
provement in the considered loss function and im-
provement in most other standard measures of ac-
curacy.
In the following three sections we will discuss
three approaches to learning such a classifier. The
first two derive a classification criteria for use with
a predefined probability model (the first genera-
tive, the second discriminative). The third de-
fines a kernel for use with a classification method
for minimizing loss. All use previously proposed
learning algorithms and optimization criteria.
2 Loss Approximation with a
Probabilistic Model
In this section we discuss approximating the ex-
pected loss using probability estimates given by
a baseline probabilistic model. Use of probabil-
ity estimates is not a serious limitation of this
approach because in practice candidates are nor-
mally provided by some probabilistic model and
its probability estimates are used as additional fea-
tures in the reranker (Collins and Koo, 2005; Shen
and Joshi, 2003; Henderson and Titov, 2005).
In order to estimate the expected loss on the ba-
sis of a candidate list, we make the assumption that
the total probability of the labels not in the can-
didate list is sufficiently small that the difference
?(x, y?) of expected loss between the labels in the
candidate list and the labels not in the candidate
list does not have an impact on the loss defined
in (1):
?(x, y?) =
?
y/?G(x) P (y|x)?(y, y?)
?
y/?G(x) P (y|x)
? (3)
?
y?G(x) P (y|x)?(y, y?)
?
y?G(x) P (y|x)
This gives us the following approximation to the
expected loss for the label:
l(x, y?) =
?
y?G(x) P (y|x)?(y, y?)
?
y?G(x) P (y|x)
. (4)
For the reranking case, often the probabilistic
model only estimates the joint probability P (x, y).
However, neither this difference nor the denomi-
nator in (4) affects the classification. Thus, replac-
ing the true probabilities with their estimates, we
can define the classifier
h?(x) = arg min
y??G(x)
?
y?G(x)
P (x, y|??)?(y, y?), (5)
where ?? denotes the parameters of the probabilis-
tic model learned from the training data. This ap-
proach for expected loss approximation was con-
sidered in the context of word error rate minimiza-
tion in speech recognition, see for example (Stol-
cke et al, 1997).
3 Estimating Expected Loss with
Discriminative Classifiers
In this section we propose a method to improve on
the loss approximation used in (5) by constructing
the probability estimates using a trained discrimi-
native classifier. Special emphasis is placed on lin-
ear classifiers with data-defined kernels for rerank-
ing (Henderson and Titov, 2005), because they do
not require any additional domain knowledge not
already encoded in the probabilistic model, and
they have demonstrated significant improvement
over the baseline probabilistic model for the parse
reranking task. This kernel construction can be
motivated by the existence of a function which
maps a linear function in the feature space of the
kernel to probability estimates which are superior
to the estimates of the original probabilistic model.
561
3.1 Estimation with Fisher Kernels
The Fisher kernel for structured classification
is a trivial generalization of one of the best
known data-defined kernels for binary classifica-
tion (Jaakkola and Haussler, 1998). The Fisher
score of an example input-label pair (x, y) is a
vector of partial derivatives of the log-likelihood
of the example with respect to the model parame-
ters1:
?FK?? (x, y) = (6)
(logP (x, y|??), ?logP (x,y|??)??1
,..., ?logP (x,y|??)??l
).
This kernel defines a feature space which is appro-
priate for estimating the discriminative probability
in the candidate list in the form of a normalized
exponential
P (x, y)
?
y??G(x) P (x, y?)
? (7)
exp(w?T ?FK?? (x, y))
?
y??G(x) exp(w?T ?FK?? (x, y
?))
for some choice of the decision vector w = w?
with the first component equal to one.
It follows that it is natural to use an estimator
of the discriminative probability P (y|x) in expo-
nential form and, therefore, the appropriate form
of the loss minimizing classifier is the following:
h?FK(x) = (8)
arg min
y??G(x)
?
y?G(x)
exp(Aw?T ?FK?? (x, y
?))?(y, y?),
where w? is learned during classifier training and
the scalar parameter A can be tuned on the devel-
opment set. From the construction of the Fisher
kernel, it follows that the optimal value A is ex-
pected to be close to inverse of the first component
of w?, 1/w?1.
If an SVM is used to learn the classifier, then
the form (7) is the same as that proposed by (Platt,
1999), where it is proposed to use the logistic sig-
moid of the SVM output as the probability estima-
tor for binary classification problems.
1The first component logP (x, y|??) is not in the strict
sense part of the Fisher score, but usually added to kernel
features in practice (Henderson and Titov, 2005).
3.2 Estimation with TOP Kernels for
Reranking
The TOP Reranking kernel was defined in (Hen-
derson and Titov, 2005), as a generalization of the
TOP kernel (Tsuda et al, 2002) proposed for bi-
nary classification tasks. The feature extractor for
the TOP reranking kernel is given by:
?TK?? (x, y) = (9)
(v(x, y, ??), ?v(x, y, ??)??1
,..., ?v(x, y, ??)??l
),
where
v(x, y, ??) = log P (x, y|??)? log
?
y??G(x)?{y}
P (x, y?|??).
The TOP reranking kernel has been demon-
strated to perform better than the Fisher kernel
for the parse reranking task (Henderson and Titov,
2005). The construction of this kernel is moti-
vated by the minimization of the classification er-
ror of a linear classifier wT ???(x, y). This linear
classifier has been shown to converge, assuming
estimation of the discriminative probability in the
candidate list can be in the form of the logistic sig-
moid (Titov and Henderson, 2005):
P (x, y)
?
y??G(x) P (x, y?)
? (10)
1
1 + exp(?w?T ?TK?? (x, y))
for some choice of the decision vector w = w?
with the first component equal to one. From this
fact, the form of the loss minimizing classifier fol-
lows:
h?TK(x) = (11)
arg min
y??G(x)
?
y?G(x)
g(Aw?T ?TK?? (x, y
?))?(y, y?),
where g is the logistic sigmoid and the scalar pa-
rameter A should be selected on the development
set. As for the Fisher kernel, the optimal value of
A should be close to 1/w?1.
3.3 Estimates from Arbitrary Classifiers
Although in this paper we focus on approaches
which do not require additional domain knowl-
edge, the output of most classifiers can be used
to estimate the discriminative probability in equa-
tion (7). As mentioned above, the form of (7)
562
is appropriate for the SVM learning task with
arbitrary kernels, as follows from (Platt, 1999).
Also, for models which combine classifiers using
votes (e.g. the Voted Perceptron), the number of
votes cast for each candidate can be used to de-
fine this discriminative probability. The discrim-
inative probability of a candidate is simply the
number of votes cast for that candidate normalized
across candidates. Intuitively, we can think of this
method as treating the votes as a sample from the
discriminative distribution.
4 Expected Loss Learning
In this section, another approach to loss approx-
imation is proposed. We consider learning a lin-
ear classifier to choose the least loss candidate,
and propose two constructions of data-defined loss
kernels which define different feature spaces for
the classification. In addition to the kernel, this
approach differs from the previous one in that the
classifier is assumed to be linear, rather than the
nonlinear functions in equations (8) and (11).
4.1 Loss Kernel
The Loss Kernel feature extractor is composed of
the logarithm of the loss estimated by the proba-
bilistic model and its first derivatives with respect
to each model parameter:
?LK?? (x, y) = (12)
(v(x, y, ??), ?v(x, y, ??)??1
,..., ?v(x, y, ??)??l
),
where
v(x, y, ??) = log(
?
y??G(x)
P (y?, x|??)?(y?, y)).
The motivation for this kernel is very similar to
that for the Fisher kernel for structured classifica-
tion. The feature space of the kernel guarantees
convergence of an estimator for the expected loss
if the estimator is in normalized exponential form.
The standard Fisher kernel for structured classifi-
cation is a special case of this Loss Kernel when
?(y, y?) is 0-1 loss.
4.2 Loss Logit Kernel
As the Loss kernel was a generalization of the
Fisher kernel to arbitrary loss function, so the Loss
Logit Kernel is a generalization of the TOP kernel
for reranking. The construction of the Loss Logit
Kernel, like the TOP kernel for reranking, can be
motivated by the minimization of the classification
error of a linear classifier wT ?LLK?? (x, y), where
?LLK?? (x, y) is the feature extractor of the kernel
given by:
?LLK?? (x, y) = (13)
(v(x, y, ??), ?v(x, y, ??)??1
,..., ?v(x, y, ??)??l
),
where
v(x, y, ??) = log(
?
y??G(x)
P (y?|x, ??)(1??(y?, y)))?
log(
?
y??G(x)
P (y?|x, ??)?(y?, y)).
5 Experimental Evaluation
To perform empirical evaluations of the proposed
methods, we considered the task of parsing the
Penn Treebank Wall Street Journal corpus (Mar-
cus et al, 1993). First, we perform experiments
with SVM Struct (Tsochantaridis et al, 2004) as
the learner. Since SVM Struct already uses the
loss function during training to rescale the margin
or slack variables, this learner allows us to test the
hypothesis that loss functions are useful in pars-
ing not only to define the optimization criteria but
also to define the classifier and to define the feature
space. However, SVM Struct training for large
scale parsing experiments is computationally ex-
pensive2, so here we use only a small portion of
the available training data to perform evaluations
of the different approaches. In the other two sets
of experiments, described below, we test our best
model on the standard Wall Street Journal parsing
benchmark (Collins, 1999) with the Voted Percep-
tron algorithm as the learner.
5.1 The Probabilistic Models of Parsing
To perform the experiments with data-defined ker-
nels, we need to select a probabilistic model of
parsing. Data-defined kernels can be applied to
any kind of parameterized probabilistic model.
For our first set of experiments, we choose
to use a publicly available neural network based
probabilistic model of parsing (Henderson, 2003).
2In (Shen and Joshi, 2003) it was proposed to use an
ensemble of SVMs trained the Wall Street Journal corpus,
but the generalization performance of the resulting classifier
might be compromised in this approach.
563
This parsing model is a good candidate for our ex-
periments because it achieves state-of-the-art re-
sults on the standard Wall Street Journal (WSJ)
parsing problem (Henderson, 2003), and data-
defined kernels derived from this parsing model
have recently been used with the Voted Percep-
tron algorithm on the WSJ parsing task, achiev-
ing a significant improvement in accuracy over the
neural network parser alone (Henderson and Titov,
2005). This gives us a baseline which is hard to
beat, and allows us to compare results of our new
approaches with the results of the original data-
defined kernels for reranking.
The probabilistic model of parsing in (Hender-
son, 2003) has two levels of parameterization. The
first level of parameterization is in terms of a
history-based generative probability model. These
parameters are estimated using a neural network,
the weights of which form the second level of pa-
rameterization. This approach allows the prob-
ability model to have an infinite number of pa-
rameters; the neural network only estimates the
bounded number of parameters which are relevant
to a given partial parse. We define data-defined
kernels in terms of the second level of parameteri-
zation (the network weights).
For the last set of experiments, we used the
probabilistic model described in (Collins, 1999)
(model 2), and the Tree Kernel (Collins and Duffy,
2002). However, in these experiments we only
used the estimates from the discriminative classi-
fier, so the details of the probabilistic model are
not relevant.
5.2 Experiments with SVM Struct
Both the neural network probabilistic model and
the kernel based classifiers were trained on sec-
tion 0 (1,921 sentences, 40,930 words). Section 24
(1,346 sentences, 29,125 words) was used as the
validation set during the neural network learning
and for choosing parameters of the models. Sec-
tion 23 (2,416 sentences, 54,268 words) was used
for the final testing of the models.
We used a publicly available tagger (Ratna-
parkhi, 1996) to provide the part-of-speech tags
for each word in the sentence. For each tag, there
is an unknown-word vocabulary item which is
used for all those words which are not sufficiently
frequent with that tag to be included individually
in the vocabulary. For these experiments, we only
included a specific tag-word pair in the vocabu-
R P F1 CM
SSN 80.9 81.7 81.3 18.3
TRK 81.1 82.4 81.7 18.2
SSN-Estim 81.4 82.3 81.8 18.3
LLK-Learn 81.2 82.4 81.8 17.6
LK-Learn 81.5 82.2 81.8 17.8
FK-Estim 81.4 82.6 82.0 18.3
TRK-Estim 81.5 82.8 82.1 18.6
Table 1: Percentage labeled constituent recall (R),
precision (P), combination of both (F1) and per-
centage complete match (CM) on the testing set.
lary if it occurred at least 20 time in the training
set, which (with tag-unknown-word pairs) led to
the very small vocabulary of 271 tag-word pairs.
The same model was used both for choosing the
list of candidate parses and for the probabilistic
model used for loss estimation and kernel feature
extraction. For training and testing of the kernel
models, we provided a candidate list consisting of
the top 20 parses found by the probabilistic model.
For the testing set, selecting the candidate with an
oracle results in an F1 score of 89.1%.
We used the SVM Struct software pack-
age (Tsochantaridis et al, 2004) to train the SVM
for all the approaches based on discriminative
classifier learning, with slack rescaling and lin-
ear slack penalty. The loss function is defined as
?(y, y?) = 1 ? F1(y, y?), where F1 denotes F1
measure on bracketed constituents. This loss was
used both for rescaling the slacks in the SVM and
for defining our classification models and kernels.
We performed initial testing of the models on
the validation set and preselected the best model
for each of the approaches before testing it on
the final testing set. Standard measures of pars-
ing accuracy, plus complete match accuracy, are
shown in table 1.3 As the baselines, the table in-
cludes the results of the standard TOP reranking
kernel (TRK) (Henderson and Titov, 2005) and
the baseline probabilistic model (SSN) (Hender-
son, 2003). SSN-Estim is the model using loss
estimation on the basic probabilistic model, as ex-
plained in section 2. LLK-Learn and LK-Learn are
the models which define the kernel based on loss,
using the Loss Logit Kernel (equation (13)) and
the Loss Kernel (equation (12)), respectively. FK-
Estim and TRK-Estim are the models which esti-
3All our results are computed with the evalb pro-
gram (Collins, 1999).
564
mate the loss with data-defined kernels, using the
Fisher Kernel (equation (8)) and the TOP Rerank-
ing kernel (equation (11)), respectively.
All our proposed models show better F1 accu-
racy than the baseline probabilistic model SSN,
and all these differences are statistically signifi-
cant.4 The difference in F1 between TRK-Estim
and FK-Estim is not statistically significant, but
otherwise TRK-Estim demonstrates a statistically
significant improvement over all other models. It
should also be noted that exact match measures for
TRK-Estim and SSN-Estim are not negatively af-
fected, even though the F1 loss function was opti-
mized. It is important to point out that SSN-Estim,
which improves significantly over SSN, does not
require the learning of a discriminative classifier,
and differs from the SSN only by use of the dif-
ferent classification model (equation (5)), which
means that it is extremely easy to apply in prac-
tice.
One surprising aspect of these results is the fail-
ure of LLK-Learn and LK-Learn to achieve im-
provement over SSN-Estim. This might be ex-
plained by the difficulty of learning a linear ap-
proximation to (4). Under this explanation, the
performance of LLK-Learn and LK-Learn could
be explained by the fact that the first component of
their kernels is a monotonic function of the SSN-
Estim estimation. To test this hypothesis, we did
an additional experiment where we removed the
first component of Loss Logit Kernel (13) from
the feature vector and performed learning. Sur-
prisingly, the model achieved virtually the same
results, rather than the predicted worse perfor-
mance. This result might indicate that the LLK-
Learn model still can be useful for different prob-
lems where discriminative learning gives more ad-
vantage over generative approaches.
These experimental results demonstrate that
the loss approximation reranking approaches pro-
posed in this paper demonstrate significant im-
provement over the baseline models, achieving
about the same relative error reduction as previ-
ously achieved with data-defined kernels (Hender-
son and Titov, 2005). This improvement is despite
the fact that the loss function is already used in the
definition of the training criteria for all the mod-
els except SSN. It is also interesting to note that
the best result on the validation set for estimation
4We measured significance of all the experiments in this
paper with the randomized significance test (Yeh, 2000).
of the loss with data-defined kernels (12) and (13)
was achieved when the parameter A is close to the
inverse of the first component of the learned de-
cision vector, which confirms the motivation for
these kernels.
5.3 Experiments with Voted Perceptron and
Data-Defined Kernels
The above experiments with the SVM Struct
demonstrate empirically the viability of our ap-
proaches. The aim of experiments on the entire
WSJ is to test whether our approaches still achieve
significant improvement when more accurate gen-
erative models are used, and also to show that
they generalize well to learning methods different
from SVMs. We perform experiments on the stan-
dard WSJ parsing data using the standard split into
training, validation and testing sets. We replicate
completely the setup of experiments in (Hender-
son and Titov, 2005). For a detailed description of
the experiment setup, we refer the reader to (Hen-
derson and Titov, 2005). We only note here that
the candidate list has 20 candidates, and, for the
testing set, selecting the candidate with an oracle
results in an F1 score of 95.4%.
We selected the TRK-Estim approach for these
experiments because it demonstrated the best re-
sults in the previous set of experiments (5.2). We
trained the Voted Perceptron (VP) modification
described in (Henderson and Titov, 2005) with the
TOP Reranking kernel. VP is not a linear classi-
fier, so we were not able to use a classifier in the
form (11). Instead the normalized counts of votes
given to the candidate parses were used as proba-
bility estimates, as discussed in section 3.3.
The resulting accuracies of this model are pre-
sented in table 2, together with results of the
TOP Reranking kernel VP (Henderson and Titov,
2005) and the SSN probabilistic model (Hender-
son, 2003). Model TRK-Estim achieves signifi-
cantly better results than the previously proposed
models, which were evaluated in the same exper-
imental setup. Again, the relative error reduction
is about the same as that of TRK. The resulting
system, consisting of the generative model and
the reranker, achieves results at the state-of-the-art
level. We believe that this method can be applied
to most parsing models to achieve a significant im-
provement.
565
R P F1
Henderson, 2003 88.8 89.5 89.1
Henderson&Titov, 2005 89.1 90.1 89.6
TRK-Estim 89.5 90.5 90.0
Table 2: Percentage labeled constituent recall (R),
precision (P), combination of both (F1) on the test-
ing set.
5.4 Experiments with Voted Perceptron and
Tree Kernel
In this series of experiments we validate the state-
ment in section 3.3, where we suggested that loss
approximation from a discriminative classifier is
not limited only to models with data-defined ker-
nels. We apply the same method as used in
the TRK-Estim model above to the Tree Ker-
nel (Collins and Duffy, 2002), which we call the
TK-Estim model.
We replicated the parse reranking experimen-
tal setup used for the evaluation of the Tree Ker-
nel in (Collins and Duffy, 2002), where the can-
didate list was provided by the generative proba-
bilistic model (Collins, 1999) (model 2). A list of
on average 29 candidates was used, with an oracle
F1 score on the testing set of 95.0%. We trained
VP using the same parameters for the Tree Ker-
nel and probability feature weighting as described
in (Collins and Duffy, 2002). A publicly avail-
able efficient implementation of the Tree Kernel
was utilized to speed up computations (Moschitti,
2004). As in the previous section, votes of the per-
ceptron were used to define the probability esti-
mate used in the classifier.
The results for the MBR decoding method (TK-
Estim), defined in section 3.3, along with the stan-
dard Tree Kernel VP results (Collins and Duffy,
2002) (TK) and the probabilistic baseline (Collins,
1999) (CO99) are presented in table 3. The pro-
posed model improves in F1 score over the stan-
dard VP results. Differences between all the mod-
els are statistically significant. The error reduction
of TK-Estim is again about the same as the error
reduction of TK. This improvement is achieved
without adding any additional linguistic features.
It is important to note that the model improves
in other accuracy measures as well. We would
expect even better results with MBR-decoding if
larger n-best lists are used. The n-best parsing al-
gorithm (Huang and Chiang, 2005) can be used to
efficiently produce candidate lists as large as 106
R P F1? CB 0C 2C
CO99 88.1 88.3 88.2 1.06 64.0 85.1
TK 88.6 88.9 88.7 0.99 66.5 86.3
TK-Estim 89.0 89.5 89.2 0.91 66.6 87.4
* F1 for previous models may have rounding errors.
Table 3: Result on the testing set. Percentage la-
beled constituent recall (R), precision (P), combi-
nation of both (F1), an average number of cross-
ing brackets per sentence (CB), percentage of sen-
tences with 0 and ? 2 crossing brackets (0C and
2C, respectively).
parse trees with the model of (Collins, 1999).
6 Conclusions
This paper considers methods for the estimation of
expected loss for parse reranking tasks. The pro-
posed methods include estimation of the loss from
a probabilistic model, estimation from a discrim-
inative classifier, and learning of the loss using a
specialized kernel. An empirical comparison of
these approaches on parse reranking tasks is pre-
sented. Special emphasis is given to data-defined
kernels for reranking, as they do not require the
introduction of any additional domain knowledge
not already encoded in the probabilistic model.
The best approach, estimation of the loss on the
basis of a discriminative classifier, achieves very
significant improvements over the baseline gener-
ative probabilistic models and the discriminative
classifier itself. Though the largest improvement is
demonstrated in the measure which corresponds to
the considered loss functional, other measures of
accuracy are also improved. The proposed method
achieves 90.0% F1 score on the standard Wall
Street Journal parsing task when the SSN neural
network is used as the probabilistic model and VP
with a TOP Reranking kernel as the discriminative
classifier.
Acknowledgments
We would like to thank Michael Collins and
Terry Koo for providing us their data and use-
ful comments on experimental setup, and Alessan-
dro Moschitti for providing us the source code for
his Tree Kernel implementation. We also thank
anonymous reviewers for their constructive com-
ments.
566
References
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proc. 43rd Meeting of Association for
Computational Linguistics, pages 173?180, Ann Ar-
bor, MI.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels
over discrete structures and the voted perceptron.
In Proc. 40th Meeting of Association for Computa-
tional Linguistics, pages 263?270, Philadelphia, PA.
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25?69.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proc. 17th Int. Conf. on
Machine Learning, pages 175?182, Stanford, CA.
Joshua Goodman. 1996. Parsing algorithms and meth-
ods. In Proc. 34th Meeting of the Association for
Computational Linguistics, pages 177?183, Santa
Cruz, CA.
James Henderson and Ivan Titov. 2005. Data-defined
kernels for parse reranking derived from probabilis-
tic models. In Proc. 43rd Meeting of Association for
Computational Linguistics, Ann Arbor, MI.
James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Proc.
joint meeting of North American Chapter of the As-
sociation for Computational Linguistics and the Hu-
man Language Technology Conf., pages 103?110,
Edmonton, Canada.
James Henderson. 2004. Discriminative training of
a neural network statistical parser. In Proc. 42nd
Meeting of Association for Computational Linguis-
tics, Barcelona, Spain.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. 9th Int. Workshop on Parsing Tech-
nologies, Vancouver, Canada.
Tommi S. Jaakkola and David Haussler. 1998. Ex-
ploiting generative models in discriminative classi-
fiers. Advances in Neural Information Processes
Systems 11.
Terry Koo and Michael Collins. 2005. Hidden-
variable models for discriminative reranking. In
Proc. Conf. on Empirical Methods in Natural Lan-
guage Processing, Vancouver, B.C., Canada.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the Human Language Tech-
nology Conference and Meeting of the North Amer-
ican Chapter of the Association for Computational
Linguistics, Boston, MA.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Alessandro Moschitti. 2004. A study on convolutional
kernels for shallow semantic parsing. In Proc. 42nd
Meeting of the Association for Computational Lin-
guistics, Barcelona, Spain.
John C. Platt. 1999. Probabilistic outputs for sup-
port vector machines and comparision to regular-
ized likelihood methods. In A. Smola, P. Bartlett,
B. Scholkopf, and D. Schuurmans, editors, Ad-
vances in Large Margin Classifiers, pages 61?74.
MIT Press.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proc. Conf. on
Empirical Methods in Natural Language Process-
ing, pages 133?142, Univ. of Pennsylvania, PA.
Libin Shen and Aravind K. Joshi. 2003. An SVM
based voting algorithm with application to parse
reranking. In Proc. of the 7th Conf. on Computa-
tional Natural Language Learning, pages 9?16, Ed-
monton, Canada.
Andreas Stolcke, Yochai Konig, and Mitchel Wein-
traub. 1997. Explicit word error minimization in
n-best list rescoring. In Proc. of 5th European Con-
ference on Speech Communication and Technology,
pages 163?165, Rhodes, Greece.
Ben Taskar, Dan Klein, Michael Collins, Daphne
Koller, and Christopher Manning. 2004. Max-
margin parsing. In Proc. Conf. on Empirical Meth-
ods in Natural Language Processing, Barcelona,
Spain.
Ivan Titov and James Henderson. 2005. Deriving ker-
nels from MLP probability estimators for large cate-
gorization problems. In International Joint Confer-
ence on Neural Networks, Montreal, Canada.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vector
machine learning for interdependent and structured
output spaces. In Proc. 21st Int. Conf. on Machine
Learning, pages 823?830, Banff, Alberta, Canada.
K. Tsuda, M. Kawanabe, G. Ratsch, S. Sonnenburg,
and K. Muller. 2002. A new discriminative ker-
nel from probabilistic models. Neural Computation,
14(10):2397?2414.
Alexander Yeh. 2000. More accurate tests for the
statistical significance of the result differences. In
Proc. 17th International Conf. on Computational
Linguistics, pages 947?953, Saarbruken, Germany.
567
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 6?13, New York City, June 2006. c?2006 Association for Computational Linguistics
Porting Statistical Parsers with Data-Defined Kernels
Ivan Titov
University of Geneva
24, rue Ge?ne?ral Dufour
CH-1211 Gene`ve 4, Switzerland
ivan.titov@cui.unige.ch
James Henderson
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, United Kingdom
james.henderson@ed.ac.uk
Abstract
Previous results have shown disappointing
performance when porting a parser trained
on one domain to another domain where
only a small amount of data is available.
We propose the use of data-defined ker-
nels as a way to exploit statistics from a
source domain while still specializing a
parser to a target domain. A probabilistic
model trained on the source domain (and
possibly also the target domain) is used to
define a kernel, which is then used in a
large margin classifier trained only on the
target domain. With a SVM classifier and
a neural network probabilistic model, this
method achieves improved performance
over the probabilistic model alone.
1 Introduction
In recent years, significant progress has been made
in the area of natural language parsing. This re-
search has focused mostly on the development of
statistical parsers trained on large annotated corpora,
in particular the Penn Treebank WSJ corpus (Marcus
et al, 1993). The best statistical parsers have shown
good results on this benchmark, but these statistical
parsers demonstrate far worse results when they are
applied to data from a different domain (Roark and
Bacchiani, 2003; Gildea, 2001; Ratnaparkhi, 1999).
This is an important problem because we cannot ex-
pect to have large annotated corpora available for
most domains. While identifying this problem, pre-
vious work has not proposed parsing methods which
are specifically designed for porting parsers. Instead
they propose methods for training a standard parser
with a large amount of out-of-domain data and a
small amount of in-domain data.
In this paper, we propose using data-defined ker-
nels and large margin methods to specifically ad-
dress porting a parser to a new domain. Data-defined
kernels are used to construct a new parser which ex-
ploits information from a parser trained on a large
out-of-domain corpus. Large margin methods are
used to train this parser to optimize performance on
a small in-domain corpus.
Large margin methods have demonstrated sub-
stantial success in applications to many machine
learning problems, because they optimize a mea-
sure which is directly related to the expected test-
ing performance. They achieve especially good per-
formance compared to other classifiers when only
a small amount of training data is available. Most
of the large margin methods need the definition of a
kernel. Work on kernels for natural language parsing
has been mostly focused on the definition of kernels
over parse trees (e.g. (Collins and Duffy, 2002)),
which are chosen on the basis of domain knowledge.
In (Henderson and Titov, 2005) it was proposed to
apply a class of kernels derived from probabilistic
models to the natural language parsing problem.
In (Henderson and Titov, 2005), the kernel is con-
structed using the parameters of a trained proba-
bilistic model. This type of kernel is called a data-
defined kernel, because the kernel incorporates in-
formation from the data used to train the probabilis-
tic model. We propose to exploit this property to
transfer information from a large corpus to a statis-
6
tical parser for a different domain. Specifically, we
propose to train a statistical parser on data including
the large corpus, and to derive the kernel from this
trained model. Then this derived kernel is used in a
large margin classifier trained on the small amount
of training data available for the target domain.
In our experiments, we consider two different
scenarios for porting parsers. The first scenario is
the pure porting case, which we call ?transferring?.
Here we only require a probabilistic model trained
on the large corpus. This model is then reparameter-
ized so as to extend the vocabulary to better suit the
target domain. The kernel is derived from this repa-
rameterized model. The second scenario is a mixture
of parser training and porting, which we call ?focus-
ing?. Here we train a probabilistic model on both
the large corpus and the target corpus. The kernel
is derived from this trained model. In both scenar-
ios, the kernel is used in a SVM classifier (Tsochan-
taridis et al, 2004) trained on a small amount of data
from the target domain. This classifier is trained to
rerank the candidate parses selected by the associ-
ated probabilistic model. We use the Penn Treebank
Wall Street Journal corpus as the large corpus and
individual sections of the Brown corpus as the tar-
get corpora (Marcus et al, 1993). The probabilis-
tic model is a neural network statistical parser (Hen-
derson, 2003), and the data-defined kernel is a TOP
reranking kernel (Henderson and Titov, 2005).
With both scenarios, the resulting parser demon-
strates improved accuracy on the target domain over
the probabilistic model alone. In additional experi-
ments, we evaluate the hypothesis that the primary
issue for porting parsers between domains is differ-
ences in the distributions of words in structures, and
not in the distributions of the structures themselves.
We partition the parameters of the probability model
into those which define the distributions of words
and those that only involve structural decisions, and
derive separate kernels for these two subsets of pa-
rameters. The former model achieves virtually iden-
tical accuracy to the full model, but the later model
does worse, confirming the hypothesis.
2 Data-Defined Kernels for Parsing
Previous work has shown how data-defined kernels
can be applied to the parsing task (Henderson and
Titov, 2005). Given the trained parameters of a prob-
abilistic model of parsing, the method defines a ker-
nel over sentence-tree pairs, which is then used to
rerank a list of candidate parses.
In this paper, we focus on the TOP reranking ker-
nel defined in (Henderson and Titov, 2005), which
are closely related to Fisher kernels. The rerank-
ing task is defined as selecting a parse tree from the
list of candidate trees (y1, . . . , ys) suggested by a
probabilistic model P (x, y|??), where ?? is a vector of
model parameters learned during training the prob-
abilistic model. The motivation for the TOP rerank-
ing kernel is given in (Henderson and Titov, 2005),
but for completeness we note that the its feature ex-
tractor is given by:
???(x, yk) =
(v(x, yk, ??), ?v(x,yk,??)??1 , . . . ,
?v(x,yk,??)
??l ),
(1)
where v(x, yk, ??) = log P (x, yk|??) ?
log ?t6=k P (x, yt|??). The first feature reflects
the score given to (x, yk) by the probabilistic
model (relative to the other candidates for x), and
the remaining features reflect how changing the
parameters of the probabilistic model would change
this score for (x, yk).
The parameters ?? used in this feature extractor do
not have to be exactly the same as the parameters
trained in the probabilistic model. In general, we
can first reparameterize the probabilistic model, pro-
ducing a new model which defines exactly the same
probability distribution as the old model, but with a
different set of adjustable parameters. For example,
we may want to freeze the values of some parame-
ters (thereby removing them from ??), or split some
parameters into multiple cases (thereby duplicating
their values in ??). This flexibility allows the features
used in the kernel method to be different from those
used in training the probabilistic model. This can be
useful for computational reasons, or when the kernel
method is not solving exactly the same problem as
the probabilistic model was trained for.
3 Porting with Data-Defined Kernels
In this paper, we consider porting a parser trained on
a large amount of annotated data to a different do-
main where only a small amount of annotated data
is available. We validate our method in two different
7
scenarios, transferring and focusing. Also we verify
the hypothesis that addressing differences between
the vocabularies of domains is more important than
addressing differences between their syntactic struc-
tures.
3.1 Transferring to a Different Domain
In the transferring scenario, we are given just a prob-
abilistic model which has been trained on a large
corpus from a source domain. The large corpus is
not available during porting, and the small corpus
for the target domain is not available during training
of the probabilistic model. This is the case of pure
parser porting, because it only requires the source
domain parser, not the source domain corpus. Be-
sides this theoretical significance, this scenario has
the advantage that we only need to train a single
probabilistic parser, thereby saving on training time
and removing the need for access to the large cor-
pus once this training is done. Then any number of
parsers for new domains can be trained, using only
the small amount of annotated data available for the
new domain.
Our proposed porting method first constructs a
data-defined kernel using the parameters of the
trained probabilistic model. A large margin clas-
sifier with this kernel is then trained to rerank the
top candidate parses produced by the probabilistic
model. Only the small target corpus is used during
training of this classifier. The resulting parser con-
sists of the original parser plus a very computation-
ally cheap procedure to rerank its best parses.
Whereas training of standard large margin meth-
ods, like SVMs, isn?t feasible on a large corpus, it
is quite tractable to train them on a small target cor-
pus.1 Also, the choice of the large margin classifier
is motivated by their good generalization properties
on small datasets, on which accurate probabilistic
models are usually difficult to learn.
We hypothesize that differences in vocabulary
across domains is one of the main difficulties with
parser portability. To address this problem, we pro-
pose constructing the kernel from a probabilistic
model which has been reparameterized to better suit
1In (Shen and Joshi, 2003) it was proposed to use an en-
semble of SVMs trained the Wall Street Journal corpus, but we
believe that the generalization performance of the resulting clas-
sifier is compromised in this approach.
the target domain vocabulary. As in other lexicalized
statistical parsers, the probabilistic model we use
treats words which are not frequent enough in the
training set as ?unknown? words (Henderson, 2003).
Thus there are no parameters in this model which
are specifically for these words. When we consider
a different target domain, a substantial proportion
of the words in the target domain are treated as un-
known words, which makes the parser only weakly
lexicalized for this domain.
To address this problem, we reparameterize the
probability model so as to add specific parameters
for the words which have high enough frequency
in the target domain training set but are treated as
unknown words by the original probabilistic model.
These new parameters all have the same values as
their associated unknown words, so the probability
distribution specified by the model does not change.
However, when a kernel is defined with this repa-
rameterized model, the kernel?s feature extractor in-
cludes features specific to these words, so the train-
ing of a large margin classifier can exploit differ-
ences between these words in the target domain. Ex-
panding the vocabulary in this way is also justified
for computational reasons; the speed of the proba-
bilistic model we use is greatly effected by vocabu-
lary size, but the large-margin method is not.
3.2 Focusing on a Subdomain
In the focusing scenario, we are given the large cor-
pus from the source domain. We may also be given
a parsing model, but as with other approaches to this
problem we simply throw this parsing model away
and train a new one on the combination of the source
and target domain data. Previous work (Roark and
Bacchiani, 2003) has shown that better accuracy can
be achieved by finding the optimal re-weighting be-
tween these two datasets, but this issue is orthogonal
to our method, so we only consider equal weighting.
After this training phase, we still want to optimize
the parser for only the target domain.
Once we have the trained parsing model, our pro-
posed porting method proceeds the same way in this
scenario as in transferring. However, because the
original training set aleady includes the vocabulary
from the target domain, the reparameterization ap-
proach defined in the preceding section is not nec-
essary so we do not perform it. This reparameter-
8
ization could be applied here, thereby allowing us
to use a statistical parser with a smaller vocabulary,
which can be more computationally efficient both
during training and testing. However, we would ex-
pect better accuracy of the combined system if the
same large vocabulary is used both by the proba-
bilistic parser and the kernel method.
3.3 Vocabulary versus Structure
It is commonly believed that differences in vo-
cabulary distributions between domains effects the
ported parser performance more significantly than
the differences in syntactic structure distributions.
We would like to test this hypothesis in our frame-
work. The probabilistic model (Henderson, 2003)
allows us to distinguish between those parameters
responsible for the distributions of individual vocab-
ulary items, and those parameters responsible for the
distributions of structural decisions, as described in
more details in section 4.2. We train two additional
models, one which uses a kernel defined in terms of
only vocabulary parameters, and one which uses a
kernel defined in terms of only structure parameters.
By comparing the performance of these models and
the model with the combined kernel, we can draw
conclusion on the relative importance of vocabulary
and syntactic structures for parser portability.
4 An Application to a Neural Network
Statistical Parser
Data-defined kernels can be applied to any kind
of parameterized probabilistic model, but they are
particularly interesting for latent variable models.
Without latent variables (e.g. for PCFG models), the
features of the data-defined kernel (except for the
first feature) are a function of the counts used to esti-
mate the model. For a PCFG, each such feature is a
function of one rule?s counts, where the counts from
different candidates are weighted using the probabil-
ity estimates from the model. With latent variables,
the meaning of the variable (not just its value) is
learned from the data, and the associated features of
the data-defined kernel capture this induced mean-
ing. There has been much recent work on latent
variable models (e.g. (Matsuzaki et al, 2005; Koo
and Collins, 2005)). We choose to use an earlier
neural network based probabilistic model of pars-
ing (Henderson, 2003), whose hidden units can be
viewed as approximations to latent variables. This
parsing model is also a good candidate for our exper-
iments because it achieves state-of-the-art results on
the standard Wall Street Journal (WSJ) parsing prob-
lem (Henderson, 2003), and data-defined kernels de-
rived from this parsing model have recently been
used with the Voted Perceptron algorithm on the
WSJ parsing task, achieving a significant improve-
ment in accuracy over the neural network parser
alone (Henderson and Titov, 2005).
4.1 The Probabilistic Model of Parsing
The probabilistic model of parsing in (Henderson,
2003) has two levels of parameterization. The first
level of parameterization is in terms of a history-
based generative probability model. These param-
eters are estimated using a neural network, the
weights of which form the second level of param-
eterization. This approach allows the probability
model to have an infinite number of parameters; the
neural network only estimates the bounded number
of parameters which are relevant to a given partial
parse. We define our kernels in terms of the second
level of parameterization (the network weights).
A history-based model of parsing first defines a
one-to-one mapping from parse trees to sequences
of parser decisions, d1,..., dm (i.e. derivations). Hen-
derson (2003) uses a form of left-corner parsing
strategy, and the decisions include generating the
words of the sentence (i.e. it is generative). The
probability of a sequence P (d1,..., dm) is then de-
composed into the multiplication of the probabilities
of each parser decision conditioned on its history of
previous decisions ?iP (di|d1,..., di?1).
4.2 Deriving the Kernel
The complete set of neural network weights isn?t
used to define the kernel, but instead reparameteriza-
tion is applied to define a third level of parameteriza-
tion which only includes the network?s output layer
weights. As suggested in (Henderson and Titov,
2005) use of the complete set of weights doesn?t
lead to any improvement of the resulting reranker
and makes the reranker training more computation-
ally expensive.
Furthermore, to assess the contribution of vocab-
ulary and syntactic structure differences (see sec-
9
tion 3.3), we divide the set of the parameters into vo-
cabulary parameters and structural parameters. We
consider the parameters used in the estimation of the
probability of the next word given the history repre-
sentation as vocabulary parameters, and the param-
eters used in the estimation of structural decision
probabilities as structural parameters. We define the
kernel with structural features as using only struc-
tural parameters, and the kernel with vocabulary fea-
tures as using only vocabulary parameters.
5 Experimental Results
We used the Penn Treebank WSJ corpus and the
Brown corpus to evaluate our approach. We used
the standard division of the WSJ corpus into train-
ing, validation, and testing sets. In the Brown corpus
we ran separate experiments for sections F (informa-
tive prose: popular lore), K (imaginative prose: gen-
eral fiction), N (imaginative prose: adventure and
western fiction), and P (imaginative prose: romance
and love story). These sections were selected be-
cause they are sufficiently large, and because they
appeared to be maximally different from each other
and from WSJ text. In each Brown corpus section,
we selected every third sentence for testing. From
the remaining sentences, we used 1 sentence out of
20 for the validation set, and the remainder for train-
ing. The resulting datasets sizes are presented in ta-
ble 1.
For the large margin classifier, we used the SVM-
Struct (Tsochantaridis et al, 2004) implementation
of SVM, which rescales the margin with F1 mea-
sure of bracketed constituents (see (Tsochantaridis
et al, 2004) for details). Linear slack penalty was
employed.2
5.1 Experiments on Transferring across
Domains
To evaluate the pure porting scenario (transferring),
described in section 3.1, we trained the SSN pars-
ing model on the WSJ corpus. For each tag, there is
an unknown-word vocabulary item which is used for
all those words not sufficiently frequent with that tag
to be included individually in the vocabulary. In the
2Training of the SVM takes about 3 hours on a standard
desktop PC. Running the SVM is very fast, once the probabilis-
tic model has finished computing the probabilities needed to
select the candidate parses.
testing training validation
WSJ 2,416 39,832 1,346
(54,268) (910,196) (31,507)
Brown F 1,054 2,005 105
(23,722) (44,928) (2,300)
Brown K 1,293 2,459 129
(21,215) (39,823) (1,971)
Brown N 1,471 2,797 137
(22,142) (42,071) (2,025)
Brown P 1,314 2,503 125
(21,763) (41,112) (1,943)
Table 1: Number of sentences (words) for each
dataset.
vocabulary of the parser, we included the unknown-
word items and the words which occurred in the
training set at least 20 times. This led to the vo-
cabulary of 4,215 tag-word pairs.
We derived the kernel from the trained model for
each target section (F, K, N, P) using reparameteriza-
tion discussed in section 3.1: we included in the vo-
cabulary all the words which occurred at least twice
in the training set of the corresponding section. This
approach led to a smaller vocabulary than that of the
initial parser but specifically tied to the target do-
main (3,613, 2,789, 2,820 and 2,553 tag-word pairs
for sections F, K, N and P respectively). There is no
sense in including the words from the WSJ which do
not appear in the Brown section training set because
the classifier won?t be able to learn the correspond-
ing components of its decision vector. The results
for the original probabilistic model (SSN-WSJ) and
for the kernel method (TOP-Transfer) on the testing
set of each section are presented in table 2.3
To evaluate the relative contribution of our porting
technique versus the use of the TOP kernel alone,
we also used this TOP kernel to train an SVM on the
WSJ corpus. We trained the SVM on data from the
development set and section 0, so that the size of this
dataset (3,267 sentences) was about the same as for
each Brown section.4 This gave us a ?TOP-WSJ?
3All our results are computed with the evalb program fol-
lowing the standard criteria in (Collins, 1999).
4We think that using an equivalently sized dataset provides
a fair test of the contribution of the TOP kernel alone. It would
also not be computationally tractable to train an SVM on the full
WSJ dataset without using different training techniques, which
would then compromise the comparison.
10
model, which we tested on each of the four Brown
sections. In each case, the TOP-WSJ model did
worse than the original SSN-WSJ model, as shown
in table 2. This makes it clear that we are getting no
improvement from simply using a TOP kernel alone
or simply using more data, and all our improvement
is from the proposed porting method.
5.2 Experiments on Focusing on a Subdomain
To perform the experiments on the approach sug-
gested in section 3.2 (focusing), we trained the SSN
parser on the WSJ training set joined with the train-
ing set of the corresponding section. We included
in the vocabulary only words which appeared in the
joint training set at least 20 times. Resulting vocab-
ularies comprised 4,386, 4,365, 4,367 and 4,348 for
sections F, K, N and P, respectively.5 Experiments
were done in the same way as for the parser transfer-
ring approach, but reparameterization was not per-
formed. Standard measures of accuracy for the orig-
inal probabilistic model (SSN-WSJ+Br) and the ker-
nel method (TOP-Focus) are also shown in table 2.
For the sake of comparison, we also trained the
SSN parser on only training data from one of the
Brown corpus sections (section P), producing a
?SSN-Brown? model. This model achieved an F1
measure of only 81.0% for the P section testing
set, which is worse than all the other models and
is 3% lower than our best results on this testing set
(TOP-Focus). This result underlines the need to port
parsers from domains in which there are large anno-
tated datasets.
5.3 Experiments Comparing Vocabulary to
Structure
We conducted the same set of experiments with the
kernel with vocabulary features (TOP-Voc-Transfer
and TOP-Voc-Focus) and with the kernel with the
structural features (TOP-Str-Transfer and TOP-Str-
Focus). Average results for classifiers with these
kernels, as well as for the original kernel and the
baseline, are presented in table 3.
5We would expect some improvement if we used a smaller
threshold on the target domain, but preliminary results suggest
that this improvement would be small.
section LR LP F?=1
TOP-WSJ F 83.9 84.9 84.4
SSN-WSJ F 84.4 85.2 84.8
TOP-Transfer F 84.5 85.6 85.0
SSN-WSJ+Br F 84.2 85.2 84.7
TOP-Focus F 84.6 86.0 85.3
TOP-WSJ K 81.8 82.3 82.1
SSN-WSJ K 82.2 82.6 82.4
TOP-Transfer K 82.4 83.5 83.0
SSN-WSJ+Br K 83.1 84.2 83.6
TOP-Focus K 83.6 85.0 84.3
TOP-WSJ N 83.3 84.5 83.9
SSN-WSJ N 83.5 84.6 84.1
TOP-Transfer N 84.3 85.7 85.0
SSN-WSJ+Br N 85.0 86.5 85.7
TOP-Focus N 85.0 86.7 85.8
TOP-WSJ P 81.3 82.1 81.7
SSN-WSJ P 82.3 83.0 82.6
TOP-Transfer P 82.7 83.8 83.2
SSN-WSJ+Br P 83.1 84.3 83.7
TOP-Focus P 83.3 84.8 84.0
Table 2: Percentage labeled constituent recall (LR),
precision (LP), and a combination of both (F?=1) on
the individual test sets.
5.4 Discussion of Results
For the experiments which directly test the useful-
ness of our proposed porting technique (SSN-WSJ
versus TOP-Transfer), our technique demonstrated
improvement for each of the Brown sections (ta-
ble 2), and this improvement was significant for
three out of four of the sections (K, N, and P).6 This
demonstrates that data-defined kernels are an effec-
tive way to port parsers to a new domain.
For the experiments which combine training a
new probability model with our porting technique
(SSN-WSJ+Br versus TOP-Focus), our technique
still demonstrated improvement over training alone.
There was improvement for each of the Brown sec-
tions, and this improvement was significant for two
6We measured significance in F1 measure at the 5% level
with the randomized significance test of (Yeh, 2000). We think
that the reason the improvement on section F was only signif-
icant at the 10% level was that the baseline model (SSN-WSJ)
was particularly lucky, as indicated by the fact that it did even
better than the model trained on the combination of datasets
(SSN-WSJ+Br).
11
LR LP F?=1
SSN-WSJ 83.1 83.8 83.5
TOP-Transfer 83.5 84.7 84.1
TOP-Voc-Transfer 83.5 84.7 84.1
TOP-Str-Transfer 83.1 84.3 83.7
SSN-WSJ+Br 83.8 85.0 84.4
TOP-Focus 84.1 85.6 84.9
TOP-Voc-Focus 84.1 85.6 84.8
TOP-Str-Focus 83.9 85.4 84.7
Table 3: Average accuracy of the models on chapters
F, K, N and P of the Brown corpus.
out of four of the sections (F and K). This demon-
strates that, even when the probability model is well
suited to the target domain, there is still room for
improvement from using data-defined kernels to op-
timize the parser specifically to the target domain
without losing information about the source domain.
One potential criticism of these conclusions is that
the improvement could be the result of reranking
with the TOP kernel, and have nothing to do with
porting. The lack of an improvement in the TOP-
WSJ results discussed in section 5.1 clearly shows
that this cannot be the explanation. The opposite
criticism is that the improvement could be the result
of optimizing to the target domain alone. The poor
performance of the SSN-Brown model discussed in
section 5.2 makes it clear that this also cannot be
the explanation. Therefore reranking with data de-
fined kernels must be both effective at preserving
information about the source domain and effective
at specializing to the target domain.
The experiments which test the hypothesis that
differences in vocabulary distributions are more im-
portant than difference in syntactic structure distri-
butions confirm this belief. Results for the classi-
fier which uses the kernel with only vocabulary fea-
tures are better than those for structural features in
each of the four sections with both the Transfer and
Focus scenarios. In addition, comparing the results
of TOP-Transfer with TOP-Voc-Transfer and TOP-
Focus with TOP-Voc-Focus, we can see that adding
structural features in TOP-Focus and TOP-Transfer
leads to virtually no improvement. This suggest that
differences in vocabulary distributions are the only
issue we need to address, although this result could
possibly also be an indication that our method did
not sufficiently exploit structural differences.
In this paper we concentrate on the situation
where a parser is needed for a restricted target do-
main, for which only a small amount of data is avail-
able. We believe that this is the task which is of
greatest practical interest. For this reason we do not
run experiments on the task considered in (Gildea,
2001) and (Roark and Bacchiani, 2003), where they
are porting from the restricted domain of the WSJ
corpus to the more varied domain of the Brown cor-
pus as a whole. However, to help emphasize the
success of our proposed porting method, it is rele-
vant to show that even our baseline models are per-
forming better than this previous work on parser
portability. We trained and tested the SSN parser in
their ?de-focusing? scenario using the same datasets
as (Roark and Bacchiani, 2003). When trained
only on the WSJ data (analogously to the SSN-
WSJ baseline for TOP-Transfer) it achieves results
of 82.9%/83.4% LR/LP and 83.2% F1, and when
trained on data from both domains (analogously
to the SSN-WSJ+Br baselines for TOP-Focus) it
achieves results of 86.3%/87.6% LR/LP and 87.0%
F1. These results represent a 2.2% and 1.3% in-
crease in F1 over the best previous results, respec-
tively (see the discussion of (Roark and Bacchiani,
2003) below).
6 Related Work
Most research in the field of parsing has focused on
the Wall Street Journal corpus. Several researchers
have addressed the portability of these WSJ parsers
to other domains, but mostly without addressing the
issue of how a parser can be designed specifically
for porting to another domain. Unfortunately, no di-
rect empirical comparison is possible between our
results and results with other parsers, because there
is no standard portability benchmark to date where a
small amount of data from a target domain is used.
(Ratnaparkhi, 1999) performed portability exper-
iments with a Maximum Entropy parser and demon-
strated that the parser trained on WSJ achieves far
worse results on the Brown corpus sections. Adding
a small amount of data from the target domain im-
proves the results, but accuracy is still much lower
than the results on the WSJ. They reported results
when their parser was trained on the WSJ training
12
set plus a portion of 2,000 sentences from a Brown
corpus section. They achieved 80.9%/80.3% re-
call/precision for section K, and 80.6%/81.3% for
section N.7 Our analogous method (TOP-Focus)
achieved much better accuracy (3.7% and 4.9% bet-
ter F1, respectively).
In addition to portability experiments with the
parsing model of (Collins, 1997), (Gildea, 2001)
provided a comprehensive analysis of parser porta-
bility. On the basis of this analysis, a tech-
nique for parameter pruning was proposed leading
to a significant reduction in the model size with-
out a large decrease of accuracy. Gildea (2001)
only reports results on sentences of 40 or less
words on all the Brown corpus sections combined,
for which he reports 80.3%/81.0% recall/precision
when training only on data from the WSJ corpus,
and 83.9%/84.8% when training on data from the
WSJ corpus and all sections of the Brown corpus.
(Roark and Bacchiani, 2003) performed experi-
ments on supervised and unsupervised PCFG adap-
tation to the target domain. They propose to use
the statistics from a source domain to define pri-
ors over weights. However, in their experiments
they used only trivial sub-cases of this approach,
namely, count merging and model interpolation.
They achieved very good improvement over their
baseline and over (Gildea, 2001), but the absolute
accuracies were still relatively low (as discussed
above). They report results with combined Brown
data (on sentences of 100 words or less), achieving
81.3%/80.9% when training only on the WSJ cor-
pus and 85.4%/85.9% with their best method using
the data from both domains.
7 Conclusions
This paper proposes a novel technique for improv-
ing parser portability, applying parse reranking with
data-defined kernels. First a probabilistic model of
parsing is trained on all the available data, including
a large set of data from the source domain. This
model is used to define a kernel over parse trees.
Then this kernel is used in a large margin classifier
7The sizes of Brown sections reported in (Ratnaparkhi,
1999) do not match the sizes of sections distributed in the Penn
Treebank 3.0 package, so we couldn?t replicate their split. We
suspect that a preliminary version of the corpus was used for
their experiments.
trained on a small set of data only from the target do-
main. This classifier is used to rerank the top parses
produced by the probabilistic model on the target do-
main. Experiments with a neural network statistical
parser demonstrate that this approach leads to im-
proved parser accuracy on the target domain, with-
out any significant increase in computational cost.
References
Michael Collins and Nigel Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over discrete struc-
tures and the voted perceptron. In Proc. ACL 2002 , pages
263?270, Philadelphia, PA.
Michael Collins. 1997. Three generative, lexicalized models
for statistical parsing. In Proc. ACL/EACL 1997 , pages 16?
23, Somerset, New Jersey.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania, Philadelphia, PA.
Daniel Gildea. 2001. Corpus variation and parser performance.
In Proc. EMNLP 2001 , Pittsburgh, PA.
James Henderson and Ivan Titov. 2005. Data-defined kernels
for parse reranking derived from probabilistic models. In
Proc. ACL 2005 , Ann Arbor, MI.
James Henderson. 2003. Inducing history representations for
broad coverage statistical parsing. In Proc. NAACL/HLT
2003 , pages 103?110, Edmonton, Canada.
Terry Koo and Michael Collins. 2005. Hidden-variable models
for discriminative reranking. In Proc. EMNLP 2005 , Van-
couver, B.C., Canada.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: The Penn Treebank. Computational Linguistics,
19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2005.
Probabilistic CFG with latent annotations. In Proc. ACL
2005 , Ann Arbor, MI.
Adwait Ratnaparkhi. 1999. Learning to parse natural language
with maximum entropy models. Machine Learning, 34:151?
175.
Brian Roark and Michiel Bacchiani. 2003. Supervised and
unsuperised PCFG adaptation to novel domains. In Proc.
HLT/ACL 2003 , Edmionton, Canada.
Libin Shen and Aravind K. Joshi. 2003. An SVM based voting
algorithm with application to parse reranking. In Proc. 7th
Conf. on Computational Natural Language Learning, pages
9?16, Edmonton, Canada.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims,
and Yasemin Altun. 2004. Support vector machine learning
for interdependent and structured output spaces. In Proc.
21st Int. Conf. on Machine Learning, pages 823?830, Banff,
Alberta, Canada.
Alexander Yeh. 2000. More accurate tests for the statistical
significance of the result differences. In Proc. 17th Int. Conf.
on Computational Linguistics, pages 947?953, Saarbruken,
Germany.
13
Proceedings of the 10th Conference on Parsing Technologies, pages 144?155,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Latent Variable Model for Generative Dependency Parsing
Ivan Titov
University of Geneva
24, rue Ge?ne?ral Dufour
CH-1211 Gene`ve 4, Switzerland
ivan.titov@cui.unige.ch
James Henderson
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, United Kingdom
james.henderson@ed.ac.uk
Abstract
We propose a generative dependency pars-
ing model which uses binary latent variables
to induce conditioning features. To define
this model we use a recently proposed class
of Bayesian Networks for structured predic-
tion, Incremental Sigmoid Belief Networks.
We demonstrate that the proposed model
achieves state-of-the-art results on three dif-
ferent languages. We also demonstrate that
the features induced by the ISBN?s latent
variables are crucial to this success, and
show that the proposed model is particularly
good on long dependencies.
1 Introduction
Dependency parsing has been a topic of active re-
search in natural language processing during the last
several years. The CoNLL-X shared task (Buch-
holz and Marsi, 2006) made a wide selection of
standardized treebanks for different languages avail-
able for the research community and allowed for
easy comparison between various statistical meth-
ods on a standardized benchmark. One of the sur-
prising things discovered by this evaluation is that
the best results are achieved by methods which
are quite different from state-of-the-art models for
constituent parsing, e.g. the deterministic parsing
method of (Nivre et al, 2006) and the minimum
spanning tree parser of (McDonald et al, 2006).
All the most accurate dependency parsing models
are fully discriminative, unlike constituent parsing
where all the state of the art methods have a genera-
tive component (Charniak and Johnson, 2005; Hen-
derson, 2004; Collins, 2000). Another surprising
thing is the lack of latent variable models among
the methods used in the shared task. Latent vari-
able models would allow complex features to be in-
duced automatically, which would be highly desir-
able in multilingual parsing, where manual feature
selection might be very difficult and time consum-
ing, especially for languages unknown to the parser
developer.
In this paper we propose a generative latent vari-
able model for dependency parsing. It is based on
Incremental Sigmoid Belief Networks (ISBNs), a
class of directed graphical model for structure pre-
diction problems recently proposed in (Titov and
Henderson, 2007), where they were demonstrated
to achieve competitive results on the constituent
parsing task. As discussed in (Titov and Hender-
son, 2007), computing the conditional probabili-
ties which we need for parsing is in general in-
tractable with ISBNs, but they can be approximated
efficiently in several ways. In particular, the neu-
ral network constituent parsers in (Henderson, 2003)
and (Henderson, 2004) can be regarded as coarse ap-
proximations to their corresponding ISBN model.
ISBNs use history-based probability models. The
most common approach to handling the unbounded
nature of the parse histories in these models is to
choose a pre-defined set of features which can be
unambiguously derived from the history (e.g. (Char-
niak, 2000; Collins, 1999; Nivre et al, 2004)). De-
cision probabilities are then assumed to be indepen-
dent of all information not represented by this finite
set of features. ISBNs instead use a vector of binary
144
latent variables to encode the information about the
parser history. This history vector is similar to the
hidden state of a Hidden Markov Model. But un-
like the graphical model for an HMM, which speci-
fies conditional dependency edges only between ad-
jacent states in the sequence, the ISBN graphical
model can specify conditional dependency edges be-
tween states which are arbitrarily far apart in the
parse history. The source state of such an edge is de-
termined by the partial output structure built at the
time of the destination state, thereby allowing the
conditional dependency edges to be appropriate for
the structural nature of the problem being modeled.
This structure sensitivity is possible because ISBNs
are a constrained form of switching model (Mur-
phy, 2002), where each output decision switches the
model structure used for the remaining decisions.
We build an ISBN model of dependency parsing
using the parsing order proposed in (Nivre et al,
2004). However, instead of performing determin-
istic parsing as in (Nivre et al, 2004), we use this
ordering to define a generative history-based model,
by integrating word prediction operations into the
set of parser actions. Then we propose a simple, lan-
guage independent set of relations which determine
how latent variable vectors are interconnected by
conditional dependency edges in the ISBN model.
ISBNs also condition the latent variable vectors on a
set of explicit features, which we vary in the experi-
ments.
In experiments we evaluate both the performance
of the ISBN dependency parser compared to previ-
ous work, and the ability of the ISBN model to in-
duce complex history features. Our model achieves
state-of-the-art performance on the languages we
test, significantly outperforming the model of (Nivre
et al, 2006) on two languages out of three and
demonstrating about the same results on the third.
In order to test the model?s feature induction abili-
ties, we train models with two different sets of ex-
plicit conditioning features: the feature set individu-
ally tuned by (Nivre et al, 2006) for each considered
language, and a minimal set of local features. These
models achieve comparable accuracy, unlike with
the discriminative SVM-based approach of (Nivre et
al., 2006), where careful feature selection appears to
be crucial. We also conduct a controlled experiment
where we used the tuned features of (Nivre et al,
2006) but disable the feature induction abilities of
our model by elimination of the edges connecting
latent state vectors. This restricted model achieves
far worse results, showing that it is exactly the ca-
pacity of ISBNs to induce history features which is
the key to its success. It also motivates further re-
search into how feature induction techniques can be
exploited in discriminative parsing methods.
We analyze how the relation accuracy changes
with the length of the head-dependent relation,
demonstrating that our model very significantly out-
performs the state-of-the-art baseline of (Nivre et
al., 2006) on long dependencies. Additional exper-
iments suggest that both feature induction abilities
and use of the beam search contribute to this im-
provement.
The fact that our model defines a probability
model over parse trees, unlike the previous state-of-
the-art methods (Nivre et al, 2006; McDonald et al,
2006), makes it easier to use this model in appli-
cations which require probability estimates, e.g. in
language processing pipelines. Also, as with any
generative model, it may be easy to improve the
parser?s accuracy by using discriminative retraining
techniques (Henderson, 2004) or data-defined ker-
nels (Henderson and Titov, 2005), with or even with-
out introduction of any additional linguistic features.
In addition, there are some applications, such as lan-
guage modeling, which require generative models.
Another advantage of generative models is that they
do not suffer from the label bias problems (Bot-
tou, 1991), which is a potential problem for con-
ditional or deterministic history-based models, such
as (Nivre et al, 2004).
In the remainder of this paper, we will first review
general ISBNs and how they can be approximated.
Then we will define the generative parsing model,
based on the algorithm of (Nivre et al, 2004), and
propose an ISBN for this model. The empirical part
of the paper then evaluates both the overall accuracy
of this method and the importance of the model?s
capacity to induce features. Additional related work
will be discussed in the last section before conclud-
ing.
145
2 The Latent Variable Architecture
In this section we will begin by briefly introduc-
ing the class of graphical models we will be us-
ing, Incremental Sigmoid Belief Networks (Titov
and Henderson, 2007). ISBNs are designed specif-
ically for modeling structured data. They are latent
variable models which are not tractable to compute
exactly, but two approximations exist which have
been shown to be effective for constituent parsing
(Titov and Henderson, 2007). Finally, we present
how these approximations can be trained.
2.1 Incremental Sigmoid Belief Networks
An ISBN is a form of Sigmoid Belief Network
(SBN) (Neal, 1992). SBNs are Bayesian Networks
with binary variables and conditional probability
distributions in the form:
P (Si = 1|Par(Si)) = ?(
?
Sj?Par(Si)
JijSj),
where Si are the variables, Par(Si) are the variables
which Si depends on (its parents), ? denotes the lo-
gistic sigmoid function, and Jij is the weight for the
edge from variable Sj to variable Si in the graphi-
cal model. SBNs are similar to feed-forward neural
networks, but unlike neural networks, SBNs have a
precise probabilistic semantics for their hidden vari-
ables. ISBNs are based on a generalized version of
SBNs where variables with any range of discrete val-
ues are allowed. The normalized exponential func-
tion (?soft-max?) is used to define the conditional
probability distributions at these nodes.
To extend SBNs for processing arbitrarily long se-
quences, such as a parser?s sequence of decisions
D1, ..., Dm, SBNs are extended to a form of Dy-
namic Bayesian Network (DBN). In DBNs, a new
set of variables is instantiated for each position in
the sequence, but the edges and weights are the same
for each position in the sequence. The edges which
connect variables instantiated for different positions
must be directed forward in the sequence, thereby
allowing a temporal interpretation of the sequence.
Incremental Sigmoid Belief Networks (Titov and
Henderson, 2007) differ from simple dynamic SBNs
in that they allow the model structure to depend on
the output variable values. Specifically, a decision is
allowed to effect the placement of any edge whose
destination is after the decision. This results in a
form of switching model (Murphy, 2002), where
each decision switches the model structure used for
the remaining decisions. The incoming edges for
a given position are a discrete function of the se-
quence of decisions which precede that position.
This makes the ISBN an ?incremental? model, not
just a dynamic model. The structure of the model is
determined incrementally as the decision sequence
proceeds.
ISBNs are designed to allow the model structure
to depend on the output values without overly com-
plicating the inference of the desired conditional
probabilities P (Dt|D1, . . . , Dt?1), the probability
of the next decision given the history of previous de-
cisions. In particular, it is never necessary to sum
over all possible model structures, which in general
would make inference intractable.
2.2 Modeling Structures with ISBNs
ISBNs are designed for modeling structured data
where the output structure is not given as part of
the input. In dependency parsing, this means they
can model the probability of an output dependency
structure when the input only specifies the sequence
of words (i.e. parsing). The difficulty with such
problems is that the statistical dependencies in the
dependency structure are local in the structure, and
not necessarily local in the word sequence. ISBNs
allow us to capture these statistical dependencies in
the model structure by having model edges depend
on the output variables which specify the depen-
dency structure. For example, if an output specifies
that there is a dependency arc from word wi to word
wj , then any future decision involving wj can di-
rectly depend on its head wi. This allows the head
wi to be treated as local to the dependent wj even if
they are far apart in the sentence.
This structurally-defined notion of locality is par-
ticularly important for the model?s latent variables.
When the structurally-defined model edges connect
latent variables, information can be propagated be-
tween latent variables, thereby providing an even
larger structural domain of locality than that pro-
vided by single edges. This provides a poten-
tially powerful form of feature induction, which is
nonetheless biased toward a notion of locality which
is appropriate for the structure of the problem.
146
2.3 Approximating ISBNs
(Titov and Henderson, 2007) proposes two approxi-
mations for inference in ISBNs, both based on vari-
ational methods. The main idea of variational meth-
ods (Jordan et al, 1999) is, roughly, to construct a
tractable approximate model with a number of free
parameters. The values of the free parameters are set
so that the resulting approximate model is as close as
possible to the original graphical model for a given
inference problem.
The simplest example of a variation method is the
mean field method, which uses a fully factorized dis-
tribution Q(H|V ) = ?i Qi(hi|V ) as the approxi-
mate model, where V are the visible (i.e. known)
variables, H = h1, . . . , hl are the hidden (i.e. la-
tent) variables, and each Qi is the distribution of an
individual latent variable hi. The free parameters of
this approximate model are the means ?i of the dis-
tributions Qi.
(Titov and Henderson, 2007) proposes two ap-
proximate models based on the variational approach.
First, they show that the neural network of (Hen-
derson, 2003) can be viewed as a coarse mean field
approximation of ISBNs, which they call the feed-
forward approximation. This approximation im-
poses the constraint that the free parameters ?i of
the approximate model are only allowed to depend
on the distributions of their parent variables. This
constraint increases the potential for a large approx-
imation error, but it significantly simplifies the com-
putations by allowing all the free parameters to be
set in a single pass over the model.
The second approximation proposed in (Titov and
Henderson, 2007) takes into consideration the fact
that, after each decision is made, all the preceding
latent variables should have their means ?i updated.
This approximation extends the feed-forward ap-
proximation to account for the most important com-
ponents of this update. They call this approxima-
tion the mean field approximation, because a mean
field approximation is applied to handle the statisti-
cal dependencies introduced by the new decisions.
This approximation was shown to be a more accu-
rate approximation of ISBNs than the feed-forward
approximation, but remain tractable. It was also
shown to achieve significantly better accuracy on
constituent parsing.
2.4 Learning
Training these approximations of ISBNs is done to
maximize the fit of the approximate models to the
data. We use gradient descent, and a regularized
maximum likelihood objective function. Gaussian
regularization is applied, which is equivalent to the
weight decay standardly used in neural networks.
Regularization was reduced through the course of
learning.
Gradient descent requires computing the deriva-
tives of the objective function with respect to the
model parameters. In the feed-forward approxima-
tion, this can be done with the standard Backpropa-
gation learning used with neural networks. For the
mean field approximation, propagating the error all
the way back through the structure of the graphical
model requires a more complicated calculation, but
it can still be done efficiently (see (Titov and Hen-
derson, 2007) for details).
3 The Dependency Parsing Algorithm
The sequences of decisions D1, ..., Dm which we
will be modeling with ISBNs are the sequences of
decisions made by a dependency parser. For this we
use the parsing strategy for projective dependency
parsing introduced in (Nivre et al, 2004), which
is similar to a standard shift-reduce algorithm for
context-free grammars (Aho et al, 1986). It can
be viewed as a mixture of bottom-up and top-down
parsing strategies, where left dependencies are con-
structed in a bottom-up fashion and right dependen-
cies are constructed top-down. For details we refer
the reader to (Nivre et al, 2004). In this section we
briefly describe the algorithm and explain how we
use it to define our history-based probability model.
In this paper, as in the CoNLL-X shared task,
we consider labeled dependency parsing. The state
of the parser is defined by the current stack S, the
queue I of remaining input words and the partial la-
beled dependency structure constructed by previous
parser decisions. The parser starts with an empty
stack S and terminates when it reaches a configura-
tion with an empty queue I . The algorithm uses 4
types of decisions:
1. The decision Left-Arcr adds a dependency arc
from the next input word wj to the word wi on
top of the stack and selects the label r for the
147
relation between wi and wj . Word wi is then
popped from the stack.
2. The decision Right-Arcr adds an arc from the
word wi on top of the stack to the next input
word wj and selects the label r for the relation
between wi and wj .
3. The decision Reduce pops the word wi from
the stack.
4. The decision Shiftwj shifts the word wj from
the queue to the stack.
Unlike the original definition in (Nivre et al, 2004)
the Right-Arcr decision does not shift wj to the
stack. However, the only thing the parser can do
after a Right-Arcr decision is to choose the Shiftwj
decision. This subtle modification does not change
the actual parsing order, but it does simplify the def-
inition of our graphical model, as explained in sec-
tion 4.
We use a history-based probability model, which
decomposes the probability of the parse according
to the parser decisions:
P (T ) = P (D1, ..., Dm) =
?
t
P (Dt|D1, . . . , Dt?1),
where T is the parse tree and D1, . . . , Dm is its
equivalent sequence of parser decisions. Since we
need a generative model, the action Shiftwj also pre-
dicts the next word in the queue I , wj+1, thus the
P (Shiftwi |D1, . . . , Dt?1) is a probability both of
the shift operation and the word wj+1 conditioned
on current parsing history.1
Instead of treating each Dt as an atomic decision,
it is convenient to split it into a sequence of elemen-
tary decisions Dt = dt1, . . . , dtn:
P (Dt|D1, . . . , Dt?1) =
?
k
P (dtk|h(t, k)),
1In preliminary experiments, we also considered look-
ahead, where the word is predicted earlier than it appears at the
head of the queue I , and ?anti-look-ahead?, where the word is
predicted only when it is shifted to the stack S. Early predic-
tion allows conditioning decision probabilities on the words in
the look-ahead and, thus, speeds up the search for an optimal
decision sequence. However, the loss of accuracy with look-
ahead was quite significant. The described method, where a
new word is predicted when it appears at the head of the queue,
led to the most accurate model and quite efficient search. The
anti-look-ahead model was both less accurate and slower.
Figure 1: An ISBN for estimating P (dtk|h(t, k)).
where h(t, k) denotes the parsing history
D1, . . . , Dt?1, dt1, . . . , dtk?1. We split Left-Arcr
and Right-Arcr each into two elementary decisions:
first, the parser decides to create the corresponding
arc, then, it decides to assign a relation r to the
arc. Similarly, we decompose the decision Shiftwj
into an elementary decision to shift a word and a
prediction of the word wj+1. In our experiments we
use datasets from the CoNLL-X shared task, which
provide additional properties for each word token,
such as its part-of-speech tag and some fine-grain
features. This information implicitly induces word
clustering, which we use in our model: first we
predict a part-of-speech tag for the word, then a set
of word features, treating feature combination as an
atomic value, and only then a particular word form.
This approach allows us to both decrease the effect
of sparsity and to avoid normalization across all the
words in the vocabulary, significantly reducing the
computational expense of word prediction.
4 An ISBN for Dependency Parsing
In this section we define the ISBN model we use for
dependency parsing. An example of this ISBN for
estimating P (dtk|h(t, k)) is illustrated in figure 1. It
is organized into vectors of variables: latent state
variable vectors St? = st?1 , . . . , st
?
n , representing an
intermediate state at position t?, and decision vari-
able vectors Dt? , representing a decision at position
t?, where t? ? t. Variables whose value are given at
the current decision (t, k) are shaded in figure 1, la-
tent and current decision variables are left unshaded.
As illustrated by the edges in figure 1, the prob-
ability of each state variable st?i (the individual cir-
cles in St?) depends on all the variables in a finite
set of relevant previous state and decision vectors,
148
but there are no direct dependencies between the dif-
ferent variables in a single state vector. For each
relevant decision vector, the precise set of decision
variables which are connected in this way can be
adapted to a particular language. As long as these
connected decisions include all the new information
about the parse, the performance of the model is not
very sensitive to this choice. This is because ISBNs
have the ability to induce their own complex features
of the parse history, as demonstrated in the experi-
ments in section 6.
The most important design decision in building
an ISBN model is choosing the finite set of relevant
previous state vectors for the current decision. By
connecting to a previous state, we place that state in
the local context of the current decision. This speci-
fication of the domain of locality determines the in-
ductive bias of learning with ISBNs. When deciding
what information to store in its latent variables, an
ISBN is more likely to choose information which
is immediately local to the current decision. This
stored information then becomes local to any fol-
lowing connected decision, where it again has some
chance of being chosen as relevant to that decision.
In this way, the information available to a given deci-
sion can come from arbitrarily far away in the chain
of interconnected states, but it is much more likely
to come from a state which is relatively local. Thus,
we need to choose the set of local (i.e. connected)
states in accordance with our prior knowledge about
which previous decisions are likely to be particularly
relevant to the current decision.
To choose which previous decisions are particu-
larly relevant to the current decision, we make use
of the partial dependency structure which has been
decided so far in the parse. Specifically, the current
latent state vector is connected to a set of 7 previous
latent state vectors (if they exist) according to the
following relationships:
1. Input Context: the last previous state with the
same queue I .
2. Stack Context: the last previous state with the
same stack S.
3. Right Child of Top of S: the last previous state
where the rightmost right child of the current
stack top was on top of the stack.
4. Left Child of Top of S: the last previous state
where the leftmost left child of the current stack
top was on top of the stack.
5. Left Child of Front of I2 : the last previous
state where the leftmost child of the front ele-
ment of I was on top of the stack.
6. Head of Top: the last previous state where the
head word of the current stack top was on top
of the stack.
7. Top of S at Front of I: the last previous state
where the current stack top was at the front of
the queue.
Each of these 7 relations has its own distinct weight
matrix for the resulting edges in the ISBN, but the
same weight matrix is used at each position where
the relation is relevant.
All these relations but the last one are motivated
by linguistic considerations. The current decision is
primarily about what to do with the current word on
the top of the stack and the current word on the front
of the queue. The Input Context and Stack Context
relationships connect to the most recent states used
for making decisions about each of these words. The
Right Child of Top of S relationship connects to a
state used for making decisions about the most re-
cently attached dependent of the stack top. Simi-
larly, the Left Child of Front of I relationship con-
nects to a state for the most recently attached depen-
dent of the queue front. The Left Child of Top of S
is the first dependent of the stack top, which is a par-
ticularly informative dependent for many languages.
Likewise, the Head of Top can tell us a lot about the
stack top, if it has been chosen already.
A second motivation for including a state in the
local context of a decision is that it might contain in-
formation which has no other route for reaching the
current decision. In particular, it is generally a good
idea to ensure that the immediately preceding state is
always included somewhere in the set of connected
states. This requirement ensures that information, at
least theoretically, can pass between any two states
in the decision sequence, thereby avoiding any hard
2We refer to the head of the queue as the front, to avoid
unnecessary ambiguity of the word head in the context of de-
pendency parsing.
149
independence assumptions. The last relation, Top of
S at Front of I , is included mainly to fulfill this re-
quirement. Otherwise, after a Shiftwj operation, the
preceding state would not be selected by any of the
relationships.
As indicated in figure 1, the probability of each
elementary decision dt?k depends both on the current
state vector St? and on the previously chosen ele-
mentary action dt?k?1 from Dt
?
. This probability dis-
tribution has the form of a normalized exponential:
P (dt?k = d|St
? , dt?k?1)=
?h(t?,k) (d) e
?
j Wdjs
t?
j
?
d??h(t?,k) (d?) e
?
j Wd?js
t?
j
,
where ?h(t?,k) is the indicator function of the set of
elementary decisions that may possibly follow the
last decision in the history h(t?, k), and the Wdj are
the weights. Now it is easy to see why the origi-
nal decision Right-Arcr (Nivre et al, 2004) had to
be decomposed into two distinct decisions: the de-
cision to construct a labeled arc and the decision to
shift the word. Use of this composite Right-Arcr
would have required the introduction of individual
parameters for each pair (w, r), where w is an arbi-
trary word in the lexicon and r - an arbitrary depen-
dency relation.
5 Searching for the Best Tree
ISBNs define a probability model which does not
make any a-priori assumptions of independence be-
tween any decision variables. As we discussed in
section 4 use of relations based on partial output
structure makes it possible to take into account sta-
tistical interdependencies between decisions closely
related in the output structure, but separated by mul-
tiple decisions in the input structure. This property
leads to exponential complexity of complete search.
However, the success of the deterministic parsing
strategy which uses the same parsing order (Nivre et
al., 2006), suggests that it should be relatively easy
to find an accurate approximation to the best parse
with heuristic search methods. Unlike (Nivre et al,
2006), we can not use a lookahead in our generative
model, as was discussed in section 3, so a greedy
method is unlikely to lead to a good approximation.
Instead we use a pruning strategy similar to that de-
scribed in (Henderson, 2003), where it was applied
to a considerably harder search problem: constituent
parsing with a left-corner parsing order.
We apply fixed beam pruning after each deci-
sion Shiftwj , because knowledge of the next word
in the queue I helps distinguish unlikely decision
sequences. We could have used best-first search be-
tween Shiftwj operations, but this still leads to rela-
tively expensive computations, especially when the
set of dependency relations is large. However, most
of the word pairs can possibly participate only in a
very limited number of distinct relations. Thus, we
pursue only a fixed number of relations r after each
Left-Arcr and Right-Arcr operation.
Experiments with a variety of post-shift beam
widths confirmed that very small validation perfor-
mance gains are achieved with widths larger than 30,
and sometimes even a beam of 5 was sufficient. We
found also that allowing 5 different relations after
each dependency prediction operation was enough
that it had virtually no effect on the validation accu-
racy.
6 Empirical Evaluation
In this section we evaluate the ISBN model for
dependency parsing on three treebanks from the
CoNLL-X shared task. We compare our genera-
tive models with the best parsers from the CoNLL-
X task, including the SVM-based parser of (Nivre et
al., 2006) (the MALT parser), which uses the same
parsing algorithm. To test the feature induction abil-
ities of our model we compare results with two fea-
ture sets, the feature set tuned individually for each
language by (Nivre et al, 2006), and another fea-
ture set which includes only obvious local features.
This simple feature set comprises only features of
the word on top of the stack S and the front word
of the queue I . We compare the gain from using
tuned features with the similar gain obtained by the
MALT parser. To obtain these results we train the
MALT parser with the same two feature sets.3
In order to distinguish the contribution of ISBN?s
feature induction abilities from the contribution of
3The tuned feature sets were obtained from
http://w3.msi.vxu.se/?nivre/research/MaltParser.html. We
removed lookahead features for ISBN experiments but
preserved them for experiments with the MALT parser. Anal-
ogously, we extended simple features with 3 words lookahead
for the MALT parser experiments.
150
our estimation method and search, we perform an-
other experiment. We use the tuned feature set and
disable the feature induction abilities of the model
by removing all the edges between latent variables
vectors. Comparison of this restricted model with
the full ISBN model shows how important the fea-
ture induction is. Also, comparison of this restricted
model with the MALT parser, which uses the same
set of features, indicates whether our generative esti-
mation method and use of beam search is beneficial.
6.1 Experimental Setup
We used the CoNLL-X distributions of Danish
DDT treebank (Kromann, 2003), Dutch Alpino tree-
bank (van der Beek et al, 2002) and Slovene SDT
treebank (Dzeroski et al, 2006). The choice of these
treebanks was motivated by the fact that they all
are freely distributed and have very different sizes
of their training sets: 195,069 tokens for Dutch,
94,386 tokens for Danish and only 28,750 tokens for
Slovene. As it is generally believed that discrimina-
tive models win over generative models with a large
amount of training data, so we expected to see simi-
lar trend in our results. Test sets are about equal and
contain about 5,000 scoring tokens.
We followed the experimental setup of the shared
task and used all the information provided for the
languages: gold standard part-of-speech tags and
coarse part-of-speech tags, word form, word lemma
(lemma information was not available for Danish)
and a set of fine-grain word features. As we ex-
plained in section 3, we treated these sets of fine-
grain features as an atomic value when predicting
a word. However, when conditioning on words, we
treated each component of this composite feature in-
dividually, as it proved to be useful on the develop-
ment set. We used frequency cutoffs: we ignored
any property (e.g., word form, feature or even part-
of-speech tag4) which occurs in the training set less
than 5 times. Following (Nivre et al, 2006), we used
pseudo-projective transformation they proposed to
cast non-projective parsing tasks as projective.
ISBN models were trained using a small devel-
opment set taken out from the training set, which
was used for tuning learning parameters and for
4Part-of-speech tags for multi-word units in the Danish tree-
bank were formed as concatenation of tags of the words, which
led to quite sparse set of part-of-speech tags.
early stopping. The sizes of the development sets
were: 4,988 tokens for larger Dutch corpus, 2,504
tokens for Danish and 2,033 tokens for Slovene.
The MALT parser was trained always using the en-
tire training set. We expect that the mean field ap-
proximation should demonstrate better results than
feed-forward approximation on this task as it is the-
oretically expected and confirmed on the constituent
parsing task (Titov and Henderson, 2007). How-
ever, the sizes of testing sets would not allow us
to perform any conclusive analysis, so we decided
not to perform these comparisons here. Instead we
used the mean field approximation for the smaller
two corpora and used the feed-forward approxima-
tion for the larger one. Training the mean field ap-
proximations on the larger Dutch treebank is feasi-
ble, but would significantly reduce the possibilities
for tuning the learning parameters on the develop-
ment set and, thus, would increase the randomness
of model comparisons.
All model selection was performed on the devel-
opment set and a single model of each type was
applied to the testing set. We used a state vari-
able vector consisting of 80 binary variables, as it
proved sufficient on the preliminary experiments.
For the MALT parser we replicated the parameters
from (Nivre et al, 2006) as described in detail on
their web site.
The labeled attachment scores for the ISBN with
tuned features (TF) and local features (LF) and
ISBN with tuned features and no edges connect-
ing latent variable vectors (TF-NA) are presented
in table 1, along with results for the MALT parser
both with tuned and local feature, the MST parser
(McDonald et al, 2006), and the average score
(Aver) across all systems in the CoNLL-X shared
task. The MST parser is included because it demon-
strated the best overall result in the task, non signif-
icantly outperforming the MALT parser, which, in
turn, achieved the second best overall result. The la-
beled attachment score is computed using the same
method as in the CoNLL-X shared task, i.e. ignor-
ing punctuation. Note, that though we tried to com-
pletely replicate training of the MALT parser with
the tuned features, we obtained slightly different re-
sults. The original published results for the MALT
parser with tuned features were 84.8% for Danish,
78.6% for Dutch and 70.3% for Slovene. The im-
151
Danish Dutch Slovene
ISBN TF 85.0 79.6 72.9
LF 84.5 79.5 72.4
TF-NA 83.5 76.4 71.7
MALT TF 85.1 78.2 70.5
LF 79.8 74.5 66.8
MST 84.8 79.2 73.4
Aver 78.3 70.7 65.2
Table 1: Labeled attachment score on the testing sets
of Danish, Dutch and Slovene treebanks.
provement of the ISBN models (TF and LF) over
the MALT parser is statistically significant for Dutch
and Slovene. Differences between their results on
Danish are not statistically significant.
6.2 Discussion of Results
The ISBN with tuned features (TF) achieved signif-
icantly better accuracy than the MALT parser on 2
languages (Dutch and Slovene), and demonstrated
essentially the same accuracy on Danish. The results
of the ISBN are among the two top published results
on all three languages, including the best published
results on Dutch. All three models, MST, MALT and
ISBN, demonstrate much better results than the av-
erage result in the CoNLL-X shared task. These re-
sults suggest that our generative model is quite com-
petitive with respect to the best models, which are
both discriminative.5 We would expect further im-
provement of ISBN results if we applied discrimina-
tive retraining (Henderson, 2004) or reranking with
data-defined kernels (Henderson and Titov, 2005),
even without introduction of any additional features.
We can see that the ISBN parser achieves about
the same results with local features (LF). Local fea-
tures by themselves are definitely not sufficient for
the construction of accurate models, as seen from
the results of the MALT parser with local features
(and look-ahead). This result demonstrates that IS-
BNs are a powerful model for feature induction.
The results of the ISBN without edges connecting
latent state vectors is slightly surprising and suggest
that without feature induction the ISBN is signifi-
cantly worse than the best models. This shows that
5Note that the development set accuracy predicted correctly
the testing set ranking of ISBN TF, LF and TF-NA models on
each of the datasets, so it is fair to compare the best ISBN result
among the three with other parsers.
to root 1 2 3 - 6 > 6
Da ISBN 95.1 95.7 90.1 84.1 74.7
MALT 95.4 96.0 90.8 84.0 71.6
Du ISBN 79.8 92.4 86.2 81.4 71.1
MALT 73.1 91.9 85.0 76.2 64.3
Sl ISBN 76.1 92.5 85.6 79.6 54.3
MALT 59.9 92.1 85.0 78.4 47.1
Av ISBN 83.6 93.5 87.3 81.7 66.7
MALT 76.2 93.3 87.0 79.5 61.0
Improv 7.5 0.2 0.4 2.2 5.7
Table 2: F1 score of labeled attachment as a function
of dependency length on the testing sets of Danish,
Dutch and Slovene.
the improvement is coming mostly from the abil-
ity of the ISBN to induce complex features and not
from either using beam search or from the estima-
tion procedure. It might also suggest that genera-
tive models are probably worse for the dependency
parsing task than discriminative approaches (at least
for larger datasets). This motivates further research
into methods which combine powerful feature in-
duction properties with the advantage of discrimina-
tive training. Although discriminative reranking of
the generative model is likely to help, the derivation
of fully discriminative feature induction methods is
certainly more challenging.
In order to better understand differences in per-
formance between ISBN and MALT, we analyzed
how relation accuracy changes with the length of
the head-dependent relation. The harmonic mean
between precision and recall of labeled attachment,
F1 measure, for the ISBN and MALT parsers with
tuned features is presented in table 2. F1 score is
computed for four different ranges of lengths and
for attachments directly to root. Along with the re-
sults for each of the languages, the table includes
their mean (Av) and the absolute improvement of
the ISBN model over MALT (Improv). It is easy
to see that accuracy of both models is generally sim-
ilar for small distances (1 and 2), but as the distance
grows the ISBN parser starts to significantly outper-
form MALT, achieving 5.7% average improvement
on dependencies longer than 6 word tokens. When
the MALT parser does not manage to recover a long
dependency, the highest scoring action it can choose
is to reduce the dependent from the stack without
specifying its head, thereby attaching the dependent
152
to the root by default. This explains the relatively
low F1 scores for attachments to root (evident for
Dutch and Slovene): though recall of attachment to
root is comparable to that of the ISBN parser (82.4%
for MALT against 84.2% for ISBN, on average over
3 languages), precision for the MALT parser is much
worse (71.5% for MALT against 83.1% for ISBN,
on average).
The considerably worse accuracy of the MALT
parser on longer dependencies might be explained
both by use of a non-greedy search method in the
ISBN and the ability of ISBNs to induce history fea-
tures. To capture a long dependency, the MALT
parser should keep a word on the stack during a
long sequence of decision. If at any point during
the intermediate steps this choice seems not to be
locally optimal, then the MALT parser will choose
the alternative and lose the possibility of the long
dependency.6 By using a beam search, the ISBN
parser can maintain the possibility of the long de-
pendency in its beam even when other alternatives
seem locally preferable. Also, long dependences are
often more difficult, and may be systematically dif-
ferent from local dependencies. The designer of a
MALT parser needs to discover predictive features
for long dependencies by hand, whereas the ISBN
model can automatically discover them. Thus we
expect that the feature induction abilities of ISBNs
have a strong effect on the accuracy of long depen-
dences. This prediction is confirmed by the differ-
ences between the results of the normal ISBN (TF)
and the restricted ISBN (TF-NA) model. The TF-
NA model, like the MALT parser, is biased toward
attachment to root; it attaches to root 12.0% more
words on average than the normal ISBN, without
any improvement of recall and with a great loss of
precision. The F1 score on long dependences for the
TF-NA model is also negatively effected in the same
way as for the MALT parser. This confirms that the
ability of the ISBN model to induce features is a ma-
jor factor in improving accuracy of long dependen-
cies.
6The MALT parser is trained to keep the word as long as
possible: if both Shift and Reduce decisions are possible during
training, it always prefers to shift. Though this strategy should
generally reduce the described problem, it is evident from the
low precision score for attachment to root, that it can not com-
pletely eliminate it.
7 Related Work
There has not been much previous work on latent
variable models for dependency parsing. Depen-
dency parsing with Dynamic Bayesian Networks
was considered in (Peshkin and Savova, 2005), with
limited success. Roughly, the model considered
the whole sentence at a time, with the DBN being
used to decide which words correspond to leaves
of the tree. The chosen words are then removed
from the sentence and the model is recursively ap-
plied to the reduced sentence. Recently several la-
tent variable models for constituent parsing have
been proposed (Koo and Collins, 2005; Matsuzaki
et al, 2005; Prescher, 2005; Riezler et al, 2002).
In (Matsuzaki et al, 2005) non-terminals in a stan-
dard PCFG model are augmented with latent vari-
ables. A similar model of (Prescher, 2005) uses a
head-driven PCFG with latent heads, thus restrict-
ing the flexibility of the latent-variable model by us-
ing explicit linguistic constraints. While the model
of (Matsuzaki et al, 2005) significantly outperforms
the constrained model of (Prescher, 2005), they both
are well below the state-of-the-art in constituent
parsing. In (Koo and Collins, 2005), an undirected
graphical model for constituent parse reranking uses
dependency relations to define the edges. Thus, it
should be easy to apply a similar method to rerank-
ing dependency trees.
Undirected graphical models, in particular Condi-
tional Random Fields, are the standard tools for shal-
low parsing (Sha and Pereira, 2003). However, shal-
low parsing is effectively a sequence labeling prob-
lem and therefore differs significantly from full pars-
ing. As discussed in (Titov and Henderson, 2007),
undirected graphical models do not seem to be suit-
able for history-based parsing models.
Sigmoid Belief Networks (SBNs) were used orig-
inally for character recognition tasks, but later a dy-
namic modification of this model was applied to the
reinforcement learning task (Sallans, 2002). How-
ever, their graphical model, approximation method,
and learning method differ significantly from those
of this paper. The extension of dynamic SBNs with
incrementally specified model structure (i.e. Incre-
mental Sigmoid Belief Networks, used in this pa-
per) was proposed and applied to constituent parsing
in (Titov and Henderson, 2007).
153
8 Conclusions
We proposed a latent variable dependency parsing
model based on Incremental Sigmoid Belief Net-
works. Unlike state-of-the-art dependency parsers,
it uses a generative history-based model. We demon-
strated that it achieves state-of-the-art results on a
selection of languages from the CoNLL-X shared
task. The parser uses a vector of latent variables
to represent an intermediate state and uses rela-
tions defined on the output structure to construct the
edges between latent state vectors. These proper-
ties make it a powerful feature induction method
for dependency parsing, and it achieves competi-
tive results even with very simple explicit features.
The ISBN model is especially accurate at modeling
long dependences, achieving average improvement
of 5.7% over the state-of-the-art baseline on depen-
dences longer than 6 words. Empirical evaluation
demonstrates that competitive results are achieved
mostly because of the ability of the model to in-
duce complex features and not because of the use of
a generative probability model or a specific search
method. As with other generative models, it can be
further improved by the application of discrimina-
tive reranking techniques. Discriminative methods
are likely to allow it to significantly improve over
the current state-of-the-art in dependency parsing.7
Acknowledgments
This work was funded by Swiss NSF grant 200020-
109685, UK EPSRC grant EP/E019501/1, and EU
FP6 grant 507802 for project TALK. We thank
Joakim Nivre and Sandra Ku?bler for an excellent
tutorial on dependency parsing given at COLING-
ACL 2006.
References
Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. 1986.
Compilers: Principles, Techniques and Tools. Addi-
son Wesley.
Leon Bottou. 1991. Une approche the?oretique de
l?apprentissage connexionniste: Applications a` la re-
connaissance de la parole. Ph.D. thesis, Universite? de
Paris XI, Paris, France.
7The ISBN dependency parser will be soon made download-
able from the authors? web-page.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of the Tenth Conference on Computational Nat-
ural Language Learning, New York, USA.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. 43rd Meeting of Association for Compu-
tational Linguistics, pages 173?180, Ann Arbor, MI.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. 1st Meeting of North American
Chapter of Association for Computational Linguistics,
pages 132?139, Seattle, Washington.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia, PA.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proc. 17th Int. Conf. on Ma-
chine Learning, pages 175?182, Stanford, CA.
S. Dzeroski, T. Erjavec, N. Ledinek, P. Pajas, Z. Zabokrt-
sky, and A. Zele. 2006. Towards a Slovene depen-
dency treebank. In Proc. Int. Conf. on Language Re-
sources and Evaluation (LREC), Genoa, Italy.
James Henderson and Ivan Titov. 2005. Data-defined
kernels for parse reranking derived from probabilis-
tic models. In Proc. 43rd Meeting of Association for
Computational Linguistics, Ann Arbor, MI.
James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Proc.
joint meeting of North American Chapter of the Asso-
ciation for Computational Linguistics and the Human
Language Technology Conf., pages 103?110, Edmon-
ton, Canada.
James Henderson. 2004. Discriminative training of
a neural network statistical parser. In Proc. 42nd
Meeting of Association for Computational Linguistics,
Barcelona, Spain.
M. I. Jordan, Z.Ghahramani, T. S. Jaakkola, and L. K.
Saul. 1999. An introduction to variational methods for
graphical models. In Michael I. Jordan, editor, Learn-
ing in Graphical Models. MIT Press, Cambridge, MA.
Terry Koo and Michael Collins. 2005. Hidden-variable
models for discriminative reranking. In Proc. Conf. on
Empirical Methods in Natural Language Processing,
Vancouver, B.C., Canada.
Matthias T. Kromann. 2003. The Danish dependency
treebank and the underlying linguistic theory. In Pro-
ceedings of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), Vaxjo, Sweden.
154
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting of the ACL,
Ann Arbor, MI.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proc. of the Tenth Con-
ference on Computational Natural Language Learn-
ing, New York, USA.
Kevin P. Murphy. 2002. Dynamic Belief Networks:
Representation, Inference and Learning. Ph.D. thesis,
University of California, Berkeley, CA.
Radford Neal. 1992. Connectionist learning of belief
networks. Artificial Intelligence, 56:71?113.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proc. of the
Eighth Conference on Computational Natural Lan-
guage Learning, pages 49?56, Boston, USA.
Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit,
and Svetoslav Marinov. 2006. Pseudo-projective de-
pendency parsing with support vector machines. In
Proc. of the Tenth Conference on Computational Nat-
ural Language Learning, pages 221?225, New York,
USA.
Leon Peshkin and Virginia Savova. 2005. Dependency
parsing with dynamic Bayesian network. In AAAI,
20th National Conference on Artificial Intelligence,
Pittsburgh, Pennsylvania.
Detlef Prescher. 2005. Head-driven PCFGs with latent-
head statistics. In Proc. 9th Int. Workshop on Parsing
Technologies, Vancouver, Canada.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proc. 40th Meeting of Associa-
tion for Computational Linguistics, Philadelphia, PA.
Brian Sallans. 2002. Reinforcement Learning for Fac-
tored Markov Decision Processes. Ph.D. thesis, Uni-
versity of Toronto, Toronto, Canada.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proc. joint meet-
ing of North American Chapter of the Association for
Computational Linguistics and the Human Language
Technology Conf., Edmonton, Canada.
Ivan Titov and James Henderson. 2007. Constituent
parsing with incremental sigmoid belief networks. In
Proc. 45th Meeting of Association for Computational
Linguistics, Prague, Czech Republic.
L. van der Beek, G. Bouma, J. Daciuk, T. Gaustad,
R. Malouf, G van Noord, R. Prins, and B. Villada.
2002. The Alpino dependency treebank. Computa-
tional Linguistic in the Netherlands (CLIN).
155
 
	131
132
133
134
135
136
137
138
Hybrid Reinforcement/Supervised Learning
of Dialogue Policies from Fixed Data Sets
James Henderson?
University of Geneva
Oliver Lemon??
University of Edinburgh
Kallirroi Georgila??
University of Edinburgh
We propose a method for learning dialogue management policies from a fixed data set. The method
addresses the challenges posed by Information State Update (ISU)-based dialogue systems, which
represent the state of a dialogue as a large set of features, resulting in a very large state space
and a huge policy space. To address the problem that any fixed data set will only provide
information about small portions of these state and policy spaces, we propose a hybrid model that
combines reinforcement learning with supervised learning. The reinforcement learning is used
to optimize a measure of dialogue reward, while the supervised learning is used to restrict the
learned policy to the portions of these spaces for which we have data. We also use linear function
approximation to address the need to generalize from a fixed amount of data to large state spaces.
To demonstrate the effectiveness of this method on this challenging task, we trained this model
on the COMMUNICATOR corpus, to which we have added annotations for user actions and In-
formation States. When tested with a user simulation trained on a different part of the same data
set, our hybrid model outperforms a pure supervised learning model and a pure reinforcement
learning model. It also outperforms the hand-crafted systems on the COMMUNICATOR data,
according to automatic evaluation measures, improving over the average COMMUNICATOR
system policy by 10%. The proposed method will improve techniques for bootstrapping and
automatic optimization of dialogue management policies from limited initial data sets.
1. Introduction
In the practical development of dialogue systems it is often the case that an initial
corpus of task-oriented dialogues is collected, either using ?Wizard of Oz? methods
or a prototype system deployment. This data is usually used to motivate and inspire
a new hand-built dialogue system or to modify an existing one. However, given the
? Universite? de Gene`ve, De?partement d?Informatique, Battelle-ba?timent A, 7 route de Drize, 1227 Carouge,
Switzerland. E-mail: james.henderson@cui.unige.ch.
?? University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: {olemon, kgeorgil}@
inf.ed.ac.uk.
Submission received: 18 November 2005; revised submission received: 18 October 2006; accepted for
publication: 21 September 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 4
existence of such data, it should be possible to exploit machine learning methods to
automatically build and optimize a new dialogue system. This objective poses two
questions: what machine learning methods are effective for this problem? and how
can we encode the task in a way which is appropriate for these methods? For the
latter challenge, we exploit the Information State Update (ISU) approach to dialogue
systems (Bohlin et al 1999; Larsson and Traum 2000), which provides the kind of rich
and flexible feature-based representations of context that are used with many recent
machine learning methods, including the linear function approximation method we
use here. For the former challenge, we propose a novel hybrid method that combines
reinforcement learning (RL) with supervised learning (SL).
The focus of this article is to establish effective methods for using fixed corpora
of dialogues to automatically optimize complex dialogue systems. To avoid the need
for extensive hand-crafting, we allow rich representations of context that include all
the features that might be relevant to dialogue management decisions, and we allow
a broad set of dialogue management decisions with very few constraints on when a
decision is applicable. This flexibility simplifies system design, but it leads to a huge
space of possible dialogue management policies, which poses severe difficulties for
existing approaches to machine learning for dialogue systems (see Section 1.1). Our pro-
posed method addresses these difficulties without the use of user simulations, feature
engineering, or further data collections.
We demonstrate the effectiveness of the proposed method on the COMMUNICA-
TOR corpora of flight-booking dialogues. Our method (?hybrid learning? with linear
function approximation) can learn dialogue strategies that are better than those learned
by standard learning methods, and that are better than the (in this case hand-coded)
strategies present in the original corpora, according to a variety of metrics. To evaluate
learned strategies we run them with simulated users that are also trained on (differ-
ent parts of) the COMMUNICATOR corpora, and automatically score the simulated
dialogues based on how many information ?slots? they manage to collect from users
(?filled slots?), whether those slots were confirmed (?confirmed slots?), and how many
dialogue turns were required to do so. Later work has shown these metrics to correlate
strongly with task completion for real users of the different policies (Lemon, Georgila,
and Henderson 2006).
The main contributions of the work are therefore in empirically demonstrating that:
 limited initial data sets can be used to train complex dialogue policies,
using a novel combination of supervised and reinforcement learning; and
 large, feature-based representations of dialogue context can be used
in tractable learning of dialogue policies, using linear function
approximation.
In this article, after a discussion of related work, we outline the annotations we have
added to the COMMUNICATOR data, then present the proposed learning method, and
describe our evaluation method. Finally, we present the evaluation results and discuss
their implications.
1.1 Related Work
As in previous work on learning for dialogue systems, in this article we focus on
learning dialogue management policies. Formally, a dialogue management policy is a
488
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
mapping from a dialogue context (a.k.a. a state) to an action that the system should
take in that context. Because most previous work on dialogue systems has been done in
the context of hand-crafted systems, we use representations of the dialogue context and
the action set based on previous work on hand-crafted dialogue systems. Our main
novel contribution is in the area of learning, where we build on previous work on
automatically learning dialogue management policies, discussed subsequently.
The ISU approach to dialogue (Bohlin et al 1999; Larsson and Traum 2000) employs
rich representations of dialogue context for flexible dialogue management. Information
States are feature structures intended to record all the information about the preceding
portion of the dialogue that is relevant to making dialogue management decisions. An
example of some of the types of information recorded in our Information States is shown
in Figure 1, including filled slots, confirmed slots, and previous speech acts. Previous
work has raised the question of whether dialogue management policies can be learned
(Levin and Pieraccini 1997) for systems that have only a limited view of the dialogue
context, for example, not including prior speech act history (see the following).
One prominent representation of the set of possible system actions is the DATE
scheme (Walker and Passonneau 2001). In particular, this representation is used in the
COMMUNICATOR corpus annotation (Walker, Passonneau, and Boland 2001), discussed
herein. The DATE scheme classifies system actions in terms of their Conversational
Domain, Speech Act, and Task. For example, one possible system action is ?about task,
Figure 1
Example fields from an Information State annotation. User-provided information is in
square brackets.
489
Computational Linguistics Volume 34, Number 4
request info, dest city?, which corresponds to a system utterance such as What is
your destination city? The specific instantiation of this scheme, and our extensions to it,
are discussed in Section 1.2.
Machine-learning approaches to dialogue management attempt to learn optimal
dialogue policies from corpora of simulated or real dialogues, or by generating such
data during automatic trial-and-error exploration of possible policies. Automatic op-
timization is desirable because of the high cost of developing and maintaining hand-
coded dialogue managers, and because there is no guarantee that hand-coded dialogue
management strategies are good. Several research groups have developed reinforce-
ment learning approaches to dialogue management, starting with Levin and Pieraccini
(1997) and Walker, Fromer, and Narayanan (1998). Previous work has been restricted
to limited dialogue context representations and limited sets of actions to choose among
(Walker, Fromer, and Narayanan 1998; Goddeau and Pineau 2000; Levin, Pieraccini, and
Eckert 2000; Roy, Pineau, and Thrun 2000; Scheffler and Young 2002; Singh et al 2002;
Williams and Young 2005; Williams, Poupart, and Young 2005a).
Much of the prior work in RL for dialogue management focuses on the problem
of choosing among a particular limited set of actions (e.g., confirm, don?t confirm)
in specific problematic states (see, e.g., Singh et al 2000a). This approach augments,
rather than replaces, hand-crafted dialogue systems, because the vast majority of deci-
sions, which are not learned, need to be specified by hand. In contrast, we tackle the
problem of learning to choose among any possible dialogue actions for almost every
possible state.
In addition, all prior work has used only a limited representation of the dialogue
context, often consisting only of the states of information slots (e.g., destination city
filled with high confidence) in the application (Goddeau and Pineau 2000; Levin,
Pieraccini, and Eckert 2000; Singh et al 2000a, 2000b, 2002; Young 2000; Scheffler and
Young 2002; Williams, Poupart, and Young 2005a, 2005b; Williams and Young 2005;
Pietquin and Dutoit 2006b), with perhaps some additional low-level information (such
as acoustic features [Pietquin 2004]). Only recently have researchers experimented with
using enriched representations of dialogue context (Gabsdil and Lemon 2004; Lemon
et al 2005; Frampton and Lemon 2006; Rieser and Lemon 2006c), as we do in this
article. From this work it is known that adding context features leads to better dialogue
strategies, compared to, for example, simply using the status of filled or confirmed
information slots as has been studied in all prior work (Frampton and Lemon 2006).
In this article we explore methods for scalable, tractable learning when using all the
available context features.
Reinforcement Learning requires estimating how good different actions will be
in different dialogue contexts. Because most previous work has only differentiated
between a small number of possible dialogue contexts, they have been able to per-
form these estimates for each state independently (e.g., Singh et al 2002; Pietquin
2004). In contrast, we use function approximation to allow generalization to states
that were not in the training data. Function approximation was also applied to RL by
Denecke, Dohsaka, and Nakano (2005), but they still use a relatively small state space
(6 features, 972 possible states). They also only exploit data for the 50 most frequent
states, using what is in effect a Gaussian kernel to compute estimates for the remaining
states from these 50 states. This is a serious limitation to their method, because a large
percentage of the data is likely to be from less frequent states, and thus would be
ignored. In our data set, we found that state frequencies followed a Zipfian (i.e., large-
tailed) distribution, with 61% of the system turns having states that only occurred once
in the data.
490
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
Another source of variation between learning approaches is the extent to which they
train on data from simulated users of different kinds, rather than train on data gathered
from real user interactions (as is done in this article). Simulated users are generally
preferred due to the much smaller development effort involved, and the fact that trial-
and-error training with humans is tedious for the users. However, the issues of how
to construct and then evaluate simulated users are open problems. Clearly there is a
dependency between the accuracy of the simulation used for training and the eventual
dialogue policy that is learned (Schatzmann et al 2005). Current research attempts to
develop metrics for user simulation that are predictive of the overall quality of the
final learned dialogue policy (Schatzmann, Georgila, and Young 2005; Schatzmann
et al 2005; Georgila, Henderson, and Lemon 2005; Georgila, Henderson, and Lemon
2006; Rieser and Lemon 2006a; Schatzmann et al 2006; Williams 2007). Furthermore,
several approaches use simple probabilistic simulations encoded by hand, using intu-
itions about reasonable user behaviors (e.g., Pietquin 2004; Frampton and Lemon 2005;
Pietquin and Dutoit 2006a), whereas other work (e.g., Scheffler and Young 2001, 2002;
Georgila, Henderson, and Lemon 2005; Georgila, Henderson, and Lemon 2006; Rieser
and Lemon 2006a) builds simulated users from dialogue corpora. We use the latter
approach, but only in the evaluation of our learned policies.
No matter which method is chosen for user simulation, a simulated user is still
clearly different from a human user. Therefore, it is important to learn as much as possi-
ble from the data we have from human users. In addition, the huge policy space makes
policy exploration with simulated users intractable, unless we can initialize the system
with a good policy and constrain the policy exploration. This also requires learning as
much as possible from the initial set of data. Therefore, in this article we investigate
using a fixed corpus of dialogues to automatically optimize dialogue systems. No user
simulation is involved in training, thus avoiding the issue of dependency on the quality
and availability of user simulations.
Previous work on RL has made use of policy exploration (Sutton and Barto 1998),
where new data is generated for each policy that is considered during the course
of learning (for example using simulated users). Indeed, this is often considered an
integral part of RL. In contrast, we choose to learn from a fixed data set, without
policy exploration. This is motivated by the fact that real dialogue corpora are very
expensive to produce, and it is often not practical to produce new real dialogues during
the course of learning. Singh et al (2002) manage to perform one iteration of policy
exploration with real data, but most work on RL requires many thousands of iterations.
As discussed previously, this motivates using simulated data for training, but even if
accurate dialogues can be automatically generated with simulated users, training on
simulated dialogues does not replace the need to fully exploit the real data, and does
not solve the sparse data problems that we address here. With a very large state space,
it will never be tractable for policy exploration to test a new policy on even a reasonable
proportion of the states. Thus we will inevitably need to stop policy exploration with
a policy that has not been sufficiently tested. In this sense, we will be in a very similar
situation to learning from a fixed data set, where we don?t have the option of generating
new data for new states. For this reason, the solution we propose for learning from fixed
data sets is also useful for learning with policy exploration.
There have been some proposals in RL for learning a policy that is different from
that used to generate the data (called ?off-policy? learning), but these methods have
been found not to work well with linear function approximation (Sutton and Barto
1998). They also do not solve the problem of straying from the region of state space
that has been observed in the data, discussed subsequently.
491
Computational Linguistics Volume 34, Number 4
1.2 The COMMUNICATOR Domain and Data Annotation
To empirically evaluate our proposed learning method, we apply it to the COMMU-
NICATOR domain using the COMMUNICATOR corpora. The COMMUNICATOR corpora
(2000 [Walker et al 2001] and 2001 [Walker et al 2002b]) consist of human?machine
dialogues (approximately 2,300 dialogues in total). The users always try to book a flight,
but they may also try to select a hotel or car rental. The dialogues are primarily ?slot-
filling? dialogues, with some information being presented to the user after the system
thinks it has filled (or confirmed) the relevant slots. These corpora have been previously
annotated using the DATE scheme, for the Conversational Domain, Speech Act, and
Task of each system utterance (Walker and Passonneau 2001; Walker, Passonneau, and
Boland 2001). In addition, the results of user questionnaires are available, but only for
the 2001 corpus.
Table 1 shows some statistics for the two collections. In the 2000 collection each turn
contains only one utterance but in the 2001 corpus a turn may contain more than one
utterance. More details about the COMMUNICATOR corpora can be found in Walker,
Passonneau, and Boland (2001) and Walker et al (2001, 2002a).
We used a hand-crafted automatic system (Georgila, Lemon, and Henderson 2005;
Georgila et al, submitted) to assign Speech Acts and Tasks to the user utterances, and to
compute state representations for each point in the dialogue (i.e., after every utterance).
Although we annotated the whole 2000 and 2001 corpora, because we need the results
of user questionnaires (as discussed subsequently), we only make use of the 2001 data
for the experiments reported here. The 2001 data has eight systems, 1,683 dialogues, and
125,388 total states, two thirds of which result from system actions and one third from
user actions. The annotation system is implemented using DIPPER (Bos et al 2003) and
OAA (Cheyer and Martin 2001), using several OAA agents (see Georgila, Lemon, and
Henderson, 2005, and Georgila et al, submitted, for more details). Following the ISU
approach, we represented states using Information States, which are feature structures
intended to record all the information about the preceding portion of the dialogue that
is relevant to making dialogue management decisions. An example of some of the types
of information recorded in an Information State is shown in Figure 1, including filled
slots, confirmed slots, and previous speech acts.
Given this corpus, we need to learn a dialogue management policy that maps these
state representations to effective system actions. As the example in Figure 1 illustrates,
there are a large number of features in dialogue states that are potentially relevant to
Table 1
Statistics for the 2000 and 2001 COMMUNICATOR data.
Year
2000 2001 Total
Number of dialogues 648 1683 2331
Number of turns 24,728 78,718 103,446
Number of system turns 13,013 39,419 52,432
Number of user turns 11,715 39,299 51,014
Number of utterances 24,728 89,666 114,394
Number of system utterances 13,013 50,159 63,172
Number of user utterances 11,715 39,507 51,222
Number of system dialogue acts 22,752 85,881 108,633
492
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
dialogue management, and thus should not be excluded from the state representations
we use in learning. This leads to a very large space of possible states (over 10386 states are
theoretically possible in our model), with a very high chance that a state encountered
in testing will not be exactly the same as any state encountered in training. This fact
motivates, if not requires, the use of approximation methods.
The complexity of the COMMUNICATOR domain is alsomanifested in the large num-
ber of system actions that the dialogue management policy needs to choose between.
The DATE scheme representation of system actions implies that each possible triple of
values for the Conversational Domain, Speech Act, and Task is a different action. In
addition, we have added release turn and end dialogue actions. There are a total of
74 system actions that occur in the annotated COMMUNICATOR data.
2. Reinforcement Learning with a Fixed Data Set
We use the annotated COMMUNICATOR data to train a Reinforcement Learning system.
In RL, the objective of the system is to maximize the reward it gets during entire dia-
logues. Rewards are defined to reflect how successful a dialogue was, so by maximizing
the total reward the system optimizes the quality of dialogues. The difficulty is that, at
any point in the dialogue, the system cannot be sure what will happen in the remainder
of the dialogue, and thus cannot be sure what effect its actions will have on the total
reward at the end of the dialogue. Thus the system must choose an action based on the
average reward it has observed previously after it has performed that action in states
similar to the current one. This average is the expected future reward.
The core component of any RL system is the estimation of the expected future
reward (called the Q-function). Given a state and an action that could be taken in that
state, the Q-function tells us what total reward, on average, we can expect between
taking that action and the end of the dialogue.1 Once we have this function, the optimal
dialogue management policy reduces to simply choosing the action that maximizes the
expected future reward for the current state.
Our proposal for RL with fixed data sets uses two main techniques. The first is the
use of function approximation to estimate the expected future reward. We claim that
linear function approximation is an effective way to generalize from a limited data set
to a large space of state?action pairs. The second technique is a novel hybrid learning
method that combines RL with supervised learning (SL). SL is used to characterize how
much data we have for each area of the state?action space (also using linear function
approximation). Our hybrid policy uses SL to avoid state?action pairs for which we do
not have enough data, while using RL to maximize reward within the parts of the space
where we do have enough data. We claim that this is an effective solution to the problem
of learning complex tasks from fixed data sets.
2.1 Defining Dialogue Reward
To apply RL to the COMMUNICATOR data, we first have to define a mapping r(d, i) from
a dialogue d and a position in that dialogue i to a reward value. This reward function is
computed using the reward level of annotation in the COMMUNICATOR data, which was
1 The expected future reward also depends on the dialogue management policy that the system will use in
the future. This self-referential nature of RL is the topic of much RL research, and will be discussed more
in the following.
493
Computational Linguistics Volume 34, Number 4
extracted from user questionnaires and task completion measures. For all states other
than the final state, we provide a reward of ?1 if the state follows a system action, and
0 otherwise. This encodes the idea that, all other things being equal, short dialogues
are better than long ones. For the final state we provide a reward that is the sum of
the rewards for each feature in the reward annotation. ?Actual Task Completion? and
?Perceived Task Completion? are both worth a reward of 100 if they are non-zero (i.e.,
true), and 0 otherwise. The remaining reward features have values ranging from 1 to 5
in the annotation (where 5 is the best), which we rescale to the range 0 to 1 (1 converts
to 0, 5 converts to 1). Their reward is their rescaled value times the weight shown in
Table 2. The relative values of these later weights were determined by the empirical
analysis reported in Walker et al (2001) within the PARADISE evaluation framework
(Walker, Kamm, and Litman 2000).
2.2 Estimating the Expected Future Reward
Given this definition of reward, we want to find an estimate Q(si, a) of the expected
future reward, which is the expected value (?E[ ]? in Equation 1) of the total reward
between taking action a in state si and the end of the dialogue. This expectation is a sum
over all possible future dialogues d, weighted by the probability of the dialogue given
that we have performed action a in state si.
Q(si, a) ? Ed|si,a[
?
j>i
r(d, j)] =
?
d
(P(d|si, a)
?
j>i
r(d, j)) (1)
Given that the number of possible future dialogues d = ?si+1, . . . , snd? is exponential in
the length of the sequences, it is not surprising that estimating the expected reward over
these sequences can be very difficult.
The ISU framework is significantly different from the frameworks used in previous
work on reinforcement learning for dialogue management, in that the rich context
representation makes the number of possible states extremely large. Having a large
number of states is a more realistic scenario for practical, flexible, and generic dialogue
systems, but it also makes many RL approaches intractable. In particular, with a large
number of states it is not possible to learn estimates of the expected future reward for
every state, unless we can exploit commonalities between different states. The feature-
based nature of ISU state representations expresses exactly these commonalities be-
tween states through the features that the states share. There are a number of techniques
that could be used for RL with feature-based representations of states, but the simplest
and most efficient is linear function approximation.
Table 2
The weights used to compute a dialogue?s final reward value, multiplied by values between 0
and 1 computed from user responses.
Actual task completion 100
Perceived task completion 100
Task ease 36
Comprehension ease 28
System behaved as expected 32
Future use 36
494
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
We use linear function approximation to map from a vector of real valued features
f (s) for the state s to a vector of estimates Q(s, a), one estimate for each a. The trained
parameters of the linear function are a vector of weights wa for each action a. Given
weights trained on a given data set, an estimate Qdata(s, a) of the expected future reward
given a state s and an action a is the inner product of the state vector f (s) and the weight
vector wa.
2
Qdata(s, a) = f (s)
Twa =
?
i
fi(s)wai (2)
This approximation method has the effect of treating two states as similar if they
share features. During learning, updating the estimate Qdata(s, a) for one observed state
s will also update the estimate Qdata(s
?, a) for all other states s? to the extent that s?
shares features with s. This updating happens via the weights wa; if s has feature i then
updating the estimate Qdata(s, a) will change wai, which will in turn change Qdata(s
?, a) for
any s? that also has feature i. Thus each feature represents a dimension with respect to
which two states can be similar or different. This similarity measure is known as a linear
kernel.
This is the first time that linear function approximation has been used for learning
dialogue strategies. Denecke, Dohsaka, and Nakano (2005) also use function approxi-
mation, but there the notion of similarity used during learning is Euclidean distance,
rather than shared features. In effect, Denecke, Dohsaka, and Nakano use a Gaussian
kernel, whereas we use a linear kernel.
To train the weights of the linear approximation Qdata(s, a), we employed a standard
RL learning method called SARSA(?) (Sutton and Barto 1998). This method learns
based on two criteria, with a parameter ? used to weight their relative influence. The
first criterion comes from temporal-difference learning: the current estimate for the
Q-function should (on average) equal the reward from the next state plus the estimate
for the expected future reward at the next state. The second criterion comes directly
from the observed reward: The current estimate for the Q-function should (on average)
equal the reward observed for the remainder of the dialogue. The combination of these
two criteria makes learning faster than using either one alone. Gradient descent learning
is applied to the weights; at each step of learning, the weights are updated so as to make
the Q-function better fit this combined criterion.
Whereas the weights wa are learned from data, the mapping f (s) from states to vec-
tors must be specified beforehand. Because each value fi(s) in these vectors represents a
possible commonality between states, it is through the definition of f (s) that we control
the notion of similarity that will be used by the linear function approximation. The
definition of f (s) we are currently using is a straightforward mapping from attribute?
value pairs in the Information State s to values in the vector f (s).
The state vector mapping f (s) was computed using the first four levels of our state
annotations for the COMMUNICATOR data (i.e., the Dialogue, Task, Low, and History
levels shown in Figure 1). The values of the attributes in these annotations were con-
verted to features of three types. For attributes that take numbers as values, we used
a simple function to map these numbers to a real number between 0 and 1, with the
absence of any value being mapped to 0 (resulting in six features, e.g., StateNumber).
2 We will use the notation xTy to denote the inner product between vectors x and y (i.e., ?x transpose
times y?). wai is the ith element of the vector wa.
495
Computational Linguistics Volume 34, Number 4
For attributes that can have arbitrary text as their values, we used 1 to represent the
presence of text and 0 to represent no value (resulting in two features, e.g., AsrInput).
The remaining attributes all have either a finite set of possible values, or a list of such
values.
The vast majority of our features are constructed from this third set of attributes.
First, to reflect the importance of speech act?task pairs (which we use to define both
system and user actions), we construct a new SpeechAct-Task attribute whose value
is the concatenation of the values for the SpeechAct and Task attributes. The same is
done for the SpeechActsHist and TasksHist attributes. Second, attributes with a list
value (i.e., the . . .Hist and . . .Status attributes, plus user actions3) are converted to a
set of attribute?value pairs consisting of the attribute and each value in the list (result-
ing in 509 features, e.g., FilledSlotsStatus:[orig city]). Note that this conversion
loses the ordering between the values in the list. In the case of SpeechAct, Task, and
SpeechAct-Task attributes that have list values (which result from turns in which a
user performs more than one action), we also include the whole list as a value for
the attribute4 (resulting in 364 features, e.g., SpeechAct:[no answer,provide info]).
Finally, attributes with single values are assigned features (which result in 401 features,
e.g., Speaker:user).
From this set of potential features, we only use those that occur in the data at least
five times.5 (Only these features are included in the feature counts given previously.)
Each feature is assigned an element of the vector f (s) that is 1 if that feature is present in
the state and 0 if it is not. In total there are 1,282 features.
One advantage of using linear function approximation is that the learning method
can be kept fairly simple, while still incorporating domain knowledge in the design
of the mapping to feature vectors. One area of future research is to investigate more
complicated mappings to feature vectors f (s). This would involve making use of kernel-
based methods. Kernels are used to compensate for the oversimplicity of linear func-
tions, and can be used to express more complicated notions of commonality between
states (Shawe-Taylor and Cristianini 2004).
2.3 Pure RL and SL Policies
Given the estimate of the expected future reward Qdata(s, a) discussed in the previous
section, one obvious approach would use this estimate to define the dialogue policy.
This ?pure RL? policy simply selects the action a with the highest Qdata(s, a) given the
state s. As demonstrated by the evaluation in Section 3, this policy performs very badly.
Inspection of the actions chosen by the pure RL policy indicates that this policy is
very different from the policy observed in the COMMUNICATOR data; the pure RL policy
almost never chose the same action as was in the data. This means that the actions that
have been learned to have the best future reward for a state are not the ones that were
3 Because in the 2001 COMMUNICATOR data users may perform more than one action in a single turn, a
user?s action is potentially a list of speech act?task pairs. These are annotated as lists of speech acts plus
lists of tasks, to which we add lists of speech act?task pairs. Histories of these lists (i.e., lists of lists) are
first flattened and then treated like other lists.
4 These ?list? values are more accurately described as set values, because we do not encode the ordering
of the values in the list.
5 We also do not include the . . .Value. . . attributes, such as FilledSlotValue, which specify the actual
fillers for slots.
496
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
typically chosen by the COMMUNICATOR systems in that state. This difference results in
two problems:
 such atypical actions then lead to states unlike anything observed in the
data,
 the policy that the system will use for future actions is different from that
observed in the data, and
The first problem makes the Qdata(s, a) estimates for the visited states highly unreliable,
because we don?t have data for these states. Because the future reward depends on the
policy that the systemwill use in the future, the second problemmeans that the estimate
Qdata(s, a) is not even relevant to the expected future reward of the pure RL policy.Wewill
return to these problems when we develop our proposed method in Section 2.4.
These problems are a result of the fact that we are training on a fixed data set,
and therefore cannot generate new data that is appropriate for the new policy. The
solution to these problems that is typically used in RL research is to generate new
data as learning progresses and the policy changes, as discussed in Section 1.1. The RL
system can thus explore the space of possible policies and states, generating new data
that is relevant to each explored policy and its states. The problem with learning with
policy exploration, even when using simulated users, is that it is not tractable with a
large state space and action set. Consider that with 10386 states and 74 actions, there are
7410
386
possible policies. If we were able to explore policies at a rate of 1 policy a second,
after 1 year we would have visited only one policy in every 7410
385.6
policies. Policy
exploration algorithms are only partially random, so to some extent they can make
accurate choices about which parts of the policy space to explore and which to ignore,
but these numbers are indicative of the scale of the problem faced by policy exploration.
In addition, experiments with a random policy achieved an average score of ?66,
showing that the vast majority of policies are very bad. This indicates that starting
policy exploration with a random policy would require an extremely large amount of
exploration to move from there to a policy which is as good as the policy found with
the proposal discussed herein (which achieved a score of 140, out of a maximum 197).
Therefore it is crucial that exploratory learning at least be initialized with a policy that
we already know to be good. The method proposed in this article for learning a policy
from a pre-existing corpus of dialogues can be used to find such an initial policy.
Given these problems with using RL with a fixed data set, an obvious alternative
would be to simply train a policy to mimic the policies of the systems used to generate
the data. One reason for training a policy, rather than using one of the original policies,
is that learning allows us to merge the policies from all the different systems, which
can lead to a better policy than any one system (as we will show in Section 3). Another
reason is that learning results in a policy that generalizes from the original policies in
interesting ways. Most notably, our learning method can be used to define a probabilis-
tic policy, not just the (presumably) deterministic policies used to generate the data. A
third reason could be (as in our case) that we do not have access to any of the original
systems that generated the data. In some sense we can use learning to reverse engineer
the systems.
We train a policy to mimic the policy observed in the data using supervised learning
with linear function approximation. This ?pure SL? policy simply selects the action
a with the highest probability P(a|s) of being chosen given the state s. We estimate
P(a|s) with linear function approximation, just as for Qdata(s, a), except that a normalized
497
Computational Linguistics Volume 34, Number 4
exponential function (a.k.a. ?softmax?) is used so that the result is a probability distri-
bution over actions a.
P(a|s) ? Sdata(s, a) =
exp( f (s)Tw?a)
?
a? exp( f (s)
Tw?
a?
)
(3)
This gives us a log-linear model, also known as a maximum entropy model. The
parameters of this model (thew?a) are trained using supervised learning on the COMMU-
NICATOR data. As with the Q-function, the use of linear function approximation means
that we have estimates for P(a|s) even for states s that have never occurred in the data,
based on similar states that did occur.
2.4 A Hybrid Approach to RL
In this work we focus on solving the first of the two problems we have discussed,
namely, preventing the system from straying into portions of the state space for which
we do not have sufficient data. To do this, we propose a novel hybrid approach that
combines RL with supervised learning. SL is used to model which actions will take the
system into a portion of the state space for which we don?t have sufficient data. RL
is used to choose between the remaining actions. A discriminant function Qhybrid(s, a) is
derived that combines these two criteria in a principled way. The resulting policy can be
adjusted to be as similar as necessary to the policy in the data, thereby also addressing
the second problem discussed previously.
As with the pure SL policy, supervised learning is used to model the policy that
the systems in the data actually use. Because in general multiple policies were used,
we model the data?s policy as a probabilistic policy, using the estimate Sdata(s, a) of P(a|s)
presented in the previous section. Sdata(s, a) is an estimate of the probability that a random
system selected from those that generated the data would choose action a given that it is
in state s. Because we are using function approximation to learn Sdata(s, a) from the data,
it will generalize (or ?smooth?) the policies actually used to generate the data so that
similar states will allow similar sets of actions.6
The hybrid approach we have investigated is based on the assumption that the Q-
function trained on the data is a poor model of the expected future reward for states
in the portion of the state space not covered by the data. Thus we need an alternative
method for estimating the future reward for these unobserved states. We have exper-
imented with two such methods. The first method simply specifies a fixed reward U
for these states. By setting this fixed reward to a low value, it amounts to a penalty for
straying from the observed portion of the state space.
The second method estimated the reward for unobserved states by adding a fixed
reward offset UO to the reward estimates for ending the dialogue immediately. This
method compensates for the use of a dialogue-final reward scheme, where many things
that the dialogue has already accomplished aren?t reflected in the reward given so far.
For example, in our reward scheme, filling a slot does not result in immediate reward,
but instead results in reward at the end of the dialogue if it leads to a successful
dialogue. The estimated reward for ending the dialogue immediately reflects howmuch
6 For this reason, we will get a probabilistic policy even if only a single deterministic policy is used
to generate the data. This makes this method applicable even for data sets generated with a single
deterministic prototype system.
498
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
reward is stored up in the state in this way. If the fixed reward added to this estimate is
set to negative, then we can be sure that the reward estimated for unobserved states is
always less than that for the best observed state, so this method also results in a penalty
for straying from the observed portion of the state space.
Given an estimated reward u for unobserved states, the expected future reward is
then the average between u for the cases where performing a in s leads to an unobserved
state and the expected reward Qdata(s, a) for the cases where it leads to an observed state.
Formally, this average is a mixture of the estimate u with the estimate Qdata(s, a), where
the mixture coefficient is the probability Pobserved(s, a) that performing a in swill lead to an
observed state.
Ed|si,a[
?
j>i r(d, j)]
? Qdata(s, a)Pobserved(s, a)+ u(1? Pobserved(s, a))
(4)
Because this estimate of the expected future reward is only needed for choosing
the next action given the current state s, we only need to estimate a function that dis-
criminates between different actions in the same way as this estimate. To derive such a
discriminant function, we first approximate Pobserved(s, a) with a first-order approximation
in terms of the probability distribution in the data P(s, a) and the size of the data set N,
under the assumption that the number of possible state?action pairs is much larger than
the size of the data set (so P(s, a)N 	 1).
Pobserved(s, a) = 1? (1? P(s, a))N
? P(s, a)N ? Sdata(s, a)P(s)N
(5)
Given this approximation, the discriminant function needs to order two actions a1, a2 in
the same way as this estimate of the expected future reward.
Qdata(s, a1)Sdata(s, a1)P(s)N + u(1? Sdata(s, a1)P(s)N)
? Qdata(s, a2)Sdata(s, a2)P(s)N + u(1? Sdata(s, a2)P(s)N)
if and only if
Sdata(s, a1)(Qdata(s, a1)? u) ? Sdata(s, a2)(Qdata(s, a2)? u)
(6)
We call this discriminant function Qhybrid(s, a).
Qhybrid(s, a) = Sdata(s, a)(Qdata(s, a)? u) (7)
We use thisQhybrid(s, a) function to choose the actions for our hybrid policy. By adjust-
ing the value of the unobserved state penalty u, we can adjust the extent to which this
model follows the supervised policy defined by Sdata(s, a) or the reinforcement learning
policy defined by Qdata(s, a). In particular, if u is very low, then maximizing Qhybrid(s, a) is
equivalent to maximizing Sdata(s, a). Thus a very low u is equivalent to the policy that
always chooses the most probable action, which we will call the ?SL policy.?
The procedure for trainingQhybrid(s, a) is simply to trainQdata(s, a) with RL and Sdata(s, a)
with SL. These two models are then combined using Equation (7), given a value for u
computed with one of the two methods presented previously. Both of these methods
involve setting a constant that determines the relative importance of RL versus SL. In
the next section we will empirically investigate good values for these constants.
499
Computational Linguistics Volume 34, Number 4
3. Empirical Evaluation
We evaluate the trained dialogue management policies by running them against trained
user simulations. The policies and the user simulations were trained using different
parts of the annotated COMMUNICATOR data (using two-fold and five-fold cross val-
idation). We compare our results against each other and against the performance of
the eight COMMUNICATOR systems, using an evaluation metric discussed subsequently.
The Information States for the simulated dialogues were computed with the same rules
used to compute the Information States for the annotated data.
3.1 The Testing Setup
For these experiments, we restrict our attention to users who only want single-leg
and return flight bookings. This allows us to do the evaluation using only the four
essential slots included in both these types of bookings: origin city, destination city,
departure date, and departure time. To achieve this restriction, we first selected all those
COMMUNICATOR dialogues that consisted only of single-leg or return flight bookings.
This subset contained 217 ATT dialogues, 116 BBN dialogues, 126 CMU dialogues, 159
Colorado dialogues, 77 IBM dialogues, 192 Lucent dialogues, 180 MIT dialogues, and
185 SRI dialogues, for a total of 1,252 dialogues (out of 1,683). This subset was used for
evaluating the COMMUNICATOR systems and for training the user models. The system
models were trained on the full set of dialogues, because they should not know the
user?s goals in advance. So, for each fold of the data, the user model was trained on only
the single-leg and return dialogues from that fold and the system model was trained on
the full set of dialogues from a subset of the remaining folds (one fold for the two-fold
experiments and three folds for the five-fold experiment, as discussed subsequently).
The user models were trained in the same way as the Sdata(s, a) function for the
pure SL model discussed in Section 2.3, using linear function approximation and a
normalized exponential output function. The states that precede user actions are input
as vectors of features virtually identical to those used for the system. However, unlike
the action set for the system, the user only chooses one action per turn, and that action
can include multiple ?Speech Act, Task? pairs. The output of the model is a probability
distribution over these actions. The user simulation selects an action randomly accord-
ing to this distribution. We also trained a user model based on n-grams of user and
system actions, which produced similar results in our testing (Georgila, Henderson,
and Lemon 2006).
In our initial experiments with the hybrid policy, we found that it never closed
the dialogue. We think that this was due to the system action (annotated in DATE)
meta greeting goodbye, which is used both as the first action and as the last action
of a dialogue. The hybrid policy expects this action to be chosen before it will close
the dialogue, but the system never chooses this action at the end of a dialogue because
it is so strongly associated with the beginning of the dialogue. This is an example of
the limitations of linear function approximation, and our dependence on the previous
COMMUNICATOR annotations. We could address this problem by splitting this action
into two actions, one for ?greeting? and one for ?goodbye.? But because we do not want
to embark on the task of feature engineering at this stage, we have instead augmented
the hybrid policy with a rule that closes the dialogue after the system chooses the action
offer, to offer the user a flight. After this first flight offer, the user has one turn to
reply, and then the dialogue is ended. For practical reasons we have also added rules
500
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
that close the dialogue after 100 states (i.e., total of user and system actions), and that
release the turn if the system has done 10 actions in a row without releasing the turn.
3.2 The Evaluation Metrics
To evaluate the success of a dialogue, we take the final state of the dialogue and use
it to compute a scoring function. We want the scoring function to be similar to the
reward we compute from the quality measures provided with the COMMUNICATOR
data (e.g., the user questionnaires), but because we do not have these quality measures
for the simulated dialogues, we cannot use the exact same reward function. When we
compare the hybrid policy against the COMMUNICATOR systems, we apply the same
scoring function to both types of dialogues so that we have a comparable evaluation
metric for both.
Because currently we are only considering users who only want single-leg or return
flight bookings, the scoring function only looks at the four essential slots for these
bookings: origin city, destination city, departure date, and departure time. We give
25 points for each slot that is filled, plus another 25 points for each slot that is also
confirmed. We also deduct 1 point for each action performed by the system, to penalize
longer dialogues. Thus the maximum possible score is 197 (i.e., 200 minus 3 system
actions: ask for all the user information in one action, then confirm all the four slots in
one action and offer a flight).
The motivation behind this evaluation metric is that confirmed slots are more likely
to be correct than slots that are just filled. If we view the score as proportional to the
probability that a slot is filled correctly, then this scoring assumes that confirmed slots
are twice as likely to be correct. Although other scoring metrics are clearly possible, this
one is a simple and reasonable approximation of the relative expected correctness of
confirmed versus non-confirmed information in dialogue systems. On the other hand,
none of our conclusions depend on this exact scoring function, as indicated by results for
the ?no-conf? version of our scoring function (discussed subsequently), which ignores
confirmations.
When combining the scores for different slots, we do not try to model the all-or-
nothing nature of the COMMUNICATOR task-completion quality measures, but instead
sum the scores for the individual slots. This summakes our scoring metric value partial
completions more highly, but inspection of the distributions of scores indicates that
this difference does not favor either the hybrid policy or the original COMMUNICATOR
systems.
Although this evaluation metric could reflect the relative quality of individual
dialogues more accurately, we believe it provides a good measure of the relative quality
of the systems we wish to compare. First, the exact same metric is applied to every
system. Additional information that we have for some systems, but not all, is not used
(e.g., the COMMUNICATOR user questionnaires, which we do not have for simulated
dialogues). Second, the systems are being run against approximately equivalent users.
The user simulation is trained on exactly the same user actions that are used to evaluate
the COMMUNICATOR systems, so the user simulations mimic exactly these users. In
particular, the simulation is able to mimic the effects of speech recognition errors,
because it is just as likely as the real users to disagree with a confirmation or provide a
new value for a previously filled slot. The nature of the simulation model may make it
systematically different from real users in some way, but we know of no argument for
why this would bias our results in favor of one system or another.
501
Computational Linguistics Volume 34, Number 4
One concern about this evaluation metric is that it does not reflect the quality of the
speech recognizer being used by the system. If a system has a good speech recognizer,
then it may not be necessary for it to confirm a slot value, but our scoring function will
still penalize it for not confirming. This would certainly be a problem if this metric were
to be used to compare different systems within the COMMUNICATOR data set. However,
the intention of the metric is simply to facilitate comparisons between different versions
of our proposed system, and between our proposed systems and those in the data.
Because the user simulations are trained on the COMMUNICATOR data, they simulate
speech recognition errors at the same rate as the data, thereby controlling for the quality
of the speech recognizer.
Nonetheless, it is worth considering another evaluation metric that does not penal-
ize for missing confirmations. For this reason we also evaluate the different systems
based on their scores for only filled slots and length, which we call the ?no-conf? score.
3.3 The Influence of Reinforcement Learning
In our first set of experiments, we evaluated the success of our hybrid policy relative
to the performance of the pure reinforcement learning policy and the pure supervised
learning policy. We also investigated how to best set the parameters for combining the
supervised and reinforcement learning policies in a hybrid policy.
We first compared the two proposed hybrid methods using two-fold cross valida-
tion. We trained models of both Qdata(s, a) and Sdata(s, a), and then used them to define
policies. We trained both models for 100 iterations through the training portion of the
data, at which point there was little change in the training error. We trained Qdata(s, a)
using SARSA(?) with ? = 0.9. This training was repeated twice, once for each fold of the
complete data set. The reinforcement learning policy uses only Qdata(s, a), the SL policy
uses only Sdata(s, a), and the hybrid policies combine the two using Equation (7). For the
hybrid policies, we used the two methods for estimating the unobserved state penalty
u and various values for the fixed reward U or reward offset UO.
During testing, each policy was run for 2,000 dialogues against a linear function
approximation user model trained on the opposite half of the data. The final state for
each one of these dialogues was then fed through the scoring function and averaged
across dialogues and across data halves. The results are plotted in Figure 2. To allow
direct comparisons between the different values of U and UO, these scores are plotted
against the proportion of decisions that are different from that which the pure SL policy
would choose. Thus the SL policy (average reward 139.8) is plotted at 0 (which is
Figure 2
Average dialogue score plotted against the proportion of decisions that diverge from the SL
policy, for different values of the unobservable state reward U and reward offset UO. Averages
over two folds, 2,000 dialogues per fold.
502
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
Figure 3
Average dialogue score plotted against the proportion of decisions that diverge from the SL
policy, for different values of the unobservable state reward offset UO. Averages over five folds,
2,000 dialogues per fold.
equivalent to a large negative U or UO). Additional points for the hybrid policies are
shown for (from left to right, respectively) U = 0, 40, 80, and 100, and UO = ?300, ?100,
?60, ?40, ?20, ?10, and 0. The pure reinforcement learning policy is not shown because
its average score falls well below the bottom of the graph, at 44.4.
Figure 2 indicates that, for both hybrid methods, adding some influence from RL
increases performance over pure SL, but too much RL results in degradation. Using a
reward offsetUO for u generally does better than a fixed rewardU, and allows a greater
influence from RL before degradation.
We found that the results for our two folds were very different,7 so we repeated
the experiments using five-fold cross validation, where the dialogues from each system
were split randomly (rather than chronologically).8 For each fold, we trained models
of both Qdata(s, a) and Sdata(s, a) on three of the folds, using a fourth fold to decide when
to stop training. The fifth fold was then used to train a linear function approximation
user model, which was used to generate 2,000 simulated dialogues. Combining the five
folds, this gave us 10,000 dialogues per model. Because, in the previous experiments,
using a reward offset UO performed better than using a fixed reward U, we only tested
models using different values of the reward offset UO.
The validation performance of the trained models for Qdata(s, a) and Sdata(s, a) per-
formed similarly across the different splits. Taken together, the models of Sdata(s, a) had a
perplexity of 4.4. Intuitively, this means that the supervised models were able to narrow
down the list of possible actions from 74 to about 4 choices, on average. This suggests
that the ISU representation of state is doing a good job of representing the information
being used by the systems to make dialogue management decisions, but that there is
still a good amount of uncaptured variability. Presumably most of this variability is due
to differences between the policies for the different systems. The models of Qdata(s, a)
had a mean squared error of 8,242, whose square root is 91. This measure is harder to
interpret because it is dominated by large errors, but suggests that the expected future
reward is rather hard to predict, as is to be expected.
Figure 3 shows the average scores for the pure SL policy (at 0) and for hybrid
policies (from left to right) with UO = ?300, ?100, ?60, ?40, ?20, ?10, ?5 and 0. The
7 For the two-fold experiments, the data were split by putting the first half of the dialogues for each system
in one fold and the second half in the other, under the constraint that no user had dialogues in more than
one fold. It appears that the users that were run at the beginning of the 2001 COMMUNICATOR data
collection were very different from those run at the end.
8 To be more precise, for each system we split the set of users randomly into five groups. Then all the
dialogues for a given group of users were put in the same fold.
503
Computational Linguistics Volume 34, Number 4
hybrid policies perform consistently better than the SL policy. The difference between
the hybrid policy and the SL policy is statistically significant at the 5% level for the
three best hybrid policies tested (p < 0.01 for UO = ?40, p < 0.001 for UO = ?10, and
p < 0.007 for UO = ?5). If we combine all the tested hybrid policies together, then their
average score (139.4) is also significantly better than the SL policy (p < 0.014). All these
results are significantly better than the average score of the pure RL policy (34.9).
3.4 Comparisons with COMMUNICATOR Systems
In our second set of experiments, we evaluated the success of our learned policies
relative to the performance of the COMMUNICATOR systems that they were trained on.
To evaluate the performance of the COMMUNICATOR systems, we extracted final states
from all the dialogues that only contain single-leg or return flight bookings and fed
them through the scoring function. The average scores are shown in Tables 3 and 4,
along with the average scores for the pure SL policy, the pure RL policy, and the best
hybrid policy (UO = ?10). The total score, the score excluding confirmations, and the
three components of the total score are shown.
Table 3 shows the results computed from the complete dialogues. These results
show a clear advantage for the hybrid policy over the average across the COMMUNI-
CATOR systems, as well as over each individual COMMUNICATOR system. In particular,
the hybrid policy uses fewer steps. Because the number of steps is doubtless affected
by the hybrid policy?s built-in strategy of stopping the dialogue after the first flight
offer, we also evaluated the performance of the COMMUNICATOR systems if we also
stopped these dialogues after the first flight offer, shown in Table 4. The COMMUNI-
CATOR systems do better when stopped at the first flight offer, but still their average
(?all COMMUNICATOR?) is not nearly as good as the hybrid or SL policies, under all
measures.
Although the average score of the COMMUNICATOR systems in Table 4 is well below
those of the hybrid and SL policies, under this measure the single best system (BBN)
beats our proposed system. Also, if we ignore confirmations (the ?no-conf? measure),
Table 3
The average scores from the different systems for single-leg and return dialogues, the score
excluding confirmations, and the three components of these scores.
System Total score No-conf Filled Confirmed Length
score slots slots penalty
hybrid RL/SL 140.3 70.3 88.0 70.0 ?17.7
pure SL 138.3 69.2 89.2 69.1 ?20.0
pure RL 34.9 25.6 56.9 8.3 ?31.3
all COMMUNICATOR 103.6 40.6 85.0 63.0 ?44.4
SRI 115.3 50.5 83.4 64.9 ?32.9
MIT 114.3 43.2 87.1 71.1 ?43.9
LUC 110.3 36.1 91.1 74.1 ?55.0
COL 105.9 47.0 90.6 59.0 ?43.6
BBN 102.4 27.1 82.5 75.2 ?55.4
ATT 94.0 38.7 78.3 55.3 ?39.6
CMU 92.1 24.0 81.7 68.1 ?57.7
IBM 77.0 61.8 85.4 15.3 ?23.6
504
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
Table 4
The average scores after the first flight offer for single-leg and return dialogues, the score
excluding confirmations, and the three components of these scores.
System Total score No-conf Filled Confirmed Length
score slots slots penalty
hybrid RL/SL 140.3 70.3 88.0 70.0 ?17.7
pure SL 138.3 69.2 89.2 69.1 ?20.0
pure RL 34.9 25.6 56.9 8.3 ?31.3
all COMMUNICATOR 127.1 63.2 84.5 63.9 ?21.3
BBN 148.9 73.2 88.6 75.6 ?15.4
LUC 138.5 59.1 91.1 79.4 ?32.1
MIT 136.4 66.9 82.8 69.4 ?15.9
COL 132.9 71.4 89.9 61.5 ?18.6
SRI 128.2 61.7 84.2 66.5 ?22.5
CMU 123.8 58.7 77.2 65.1 ?18.5
ATT 109.1 53.6 78.3 55.4 ?24.7
IBM 86.4 71.2 85.1 15.3 ?13.9
then three of the individual systems beat our proposed system by small amounts. How-
ever, as discussed in Section 3.2, our evaluation methodology is not really appropriate
for comparing against individual COMMUNICATOR systems, due to likely differences in
speech recognition performance across systems. To test this explanation, we looked at
the word error rates for the speech recognition outputs for the different systems. BBN
has the highest percentage of user utterances with no speech recognition errors (79%,
versus an average of 66%), and the second lowest average word error rate (12.1 versus
an average of 22.1). Because our simulated users simulate speech recognition errors at
the average rate, the difference in performance between BBN and our systems could
easily be explained simply by differences in the speech recognizers, and not differences
in the dialogue management policies.
3.5 Discussion
The most obvious conclusion to draw from these results is not a surprising one: Pure
reinforcement learning with such a huge state space and such limited data does not
performwell. Given the pure RL policy?s score of 34.9, all the policies in Figure 3 and all
the COMMUNICATOR systems in Tables 3 and 4 perform better by quite a large margin.
Inspection of the dialogues indicates that the pure RL policy does not result in a coherent
sequence of actions. This policy tends to choose actions that are associated with the end
of the dialogue, even at the beginning of the dialogue. Perhaps this is because these
actions are only chosen by the COMMUNICATOR systems during relatively successful
dialogues. This policy also tends to repeat the same actions many times, for example
repeatedly requesting information even after the user has supplied this information.
These phenomena are examples of the problemwe used tomotivate our hybrid learning
method, in that they both involve state?action pairs that the learner would never have
seen in the COMMUNICATOR training data.
Given the disappointing performance of the pure RL policy, it is surprising that our
hybrid policies outperform the pure SL policy, as shown in Figures 2 and 3. Though the
increase in performance is small, it is statistically significant, and consistent across the
505
Computational Linguistics Volume 34, Number 4
two hybrid methods and across a range of degrees of influence from RL.9 This indicates
that our hybrid policies are succeeding in getting useful information from the results of
reinforcement learning, even under these extremely difficult circumstances. Perhaps,
under less severe circumstances for RL, a greater gain can be achieved with hybrid
policies. For the second hybrid policy (unobserved state reward offset), the fact that the
best result was achievedwith aUO value (UO = ?10) that is very close to the theoretical
limit of this method (UO = 0) suggests that future improvements to this method could
result in even more useful information being extracted from the RL policy.
The different components of the scoring function give some indication of how the
hybrid policies differ from the SL policy. As indicated in the top two rows of Table 4,
the hybrid policies mostly improve over the SL policy in dialogue length, with a slight
increase in confirmed slots and a slight decrease in filled slots.
One striking conclusion from the results comparing the learned policies to the poli-
cies of the COMMUNICATOR systems, shown in Tables 3 and 4, is that the learned policies
score better than the policies they were trained on. This is particularly surprising for
the pure SL policy, given that this policy is simply trying to mimic the behavior of
these same systems. This can be explained by the fact that the SL policy is the result
of merging all policies of the COMMUNICATOR systems. Thus it can be thought of as
a form of multi-version system, where decisions are made based on what the majority
of systems would do.10 Multi-version systems are well known to perform better than
their component systems, because the mistakes tend to be different across the different
component systems. They remove errors made by any one system that are not shared
by most of the other systems.
The good performance of the SL policy compared to the COMMUNICATOR systems
makes the better performance of the hybrid policies even more impressive. As shown
on the x axis of Figure 3, the best hybrid systems choose a different action from the
SL policy about one action out of four. Despite the good performance of the action
chosen by the SL policy, RL is able to (on average) find a better action by looking at the
rewards achieved by the systems in the data when they chose those actions in similar
states. By following different systems? choices at different points in the dialogue, the
learned policy can potentially perform better than any individual system. Although our
current evaluation methodology is not fine-grained enough to determine if this is being
achieved, the most promising aspect of applying RL to fixed data sets is in learning to
combine the best aspects of each system in the data set.
Although we believe that these results provide an accurate picture of the relative
strengths of the different types of systems we compare, it should be noted that the
reliance on evaluation with simulated dialogues inevitably leads to some lack of pre-
cision in the evaluation. All these results are computed with users who have the same
goal (booking a return flight) and with an evaluation metric that only looks at dialogue
length and whether the four main slots were filled and (optionally) confirmed. On the
9 We previously reported results that showed that adding influence from reinforcement learning always
degraded performance slightly compared to the pure SL policy (Henderson, Lemon, and Georgila 2005).
However, these results were obtained with a preliminary version of the data annotation, which gave
a less accurate indication of when slots were filled and confirmed. The scores we are achieving with
the new data annotation (Georgila et al submitted) are all higher than those reported in Henderson,
Lemon, and Geogila (2005), including the scores calculated from the data for the COMMUNICATOR
systems themselves.
10 To be more technically accurate, we can think of the SL policy as in effect asking each COMMUNICATOR
system for a probability distribution over state?action pairs for the current state, summing these
probabilities across systems, and choosing the action with the highest probability.
506
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
other hand, all the systems were trained to handle a more complicated task than this,
including multi-leg flights, hotel bookings, and rental-car bookings. They were also
designed or trained to complete the task, rather than to fill the slots. Therefore the
evaluation does not reflect all the capabilities or behaviors of the systems. However,
there is no apparent reason to believe that this fact biases our results towards one type
of system or another. This claim is easiest to support for the comparisons between
the hybrid method and the two trained baselines, pure RL and pure SL. For all these
systems, the same data was used to train the systems, the same user models were
used to generate simulated dialogues, and the same evaluation metric was applied to
these simulated dialogues. For the comparisons between the hybrid method and the
COMMUNICATOR systems, only the evaluation metric is exactly the same, but the user
models used for testing the hybrid method were trained to mimic exactly the users
in the dialogues used to evaluate the COMMUNICATOR systems. Because we know of
no evaluation bias introduced when moving from real users to their simulation, we
conclude that this comparison is also indicative of the relative performance of these two
types of systems (particularly given the size of the improvement).
A more general methodological objection could be raised against any evaluation
that uses simulated users. Despite the substantial amount of dialogue system work that
has relied on simulated users (e.g., Scheffler and Young 2002; Pietquin 2004; Georgila,
Henderson, and Lemon 2006; Schatzmann et al 2006), to date there has not been a
systematic experiment that validates this methodology against results from human
users. However, in related work (Lemon, Georgila, and Henderson 2006), we have
demonstrated that a hybrid policy learned as proposed in this article performs better
than a state-of-the-art hand-coded system in experiments with human users. The exper-
iments were done using the ?Town Information?multimodal dialogue system of Lemon
et at. (2006) and Lemon, Georgila, and Stuttle (2005). The hybrid policy reported here
(trained on the COMMUNICATOR data) was ported to this domain, and then evaluated
with human subjects. The learned policy achieved an average gain in perceived task
completion of 14.2% (from 67.6% to 81.8% at p < 0.03) compared to a state-of-the-art
hand-coded system (Lemon, Georgila, and Henderson 2006). This demonstrates that a
policy that performs well in simulation also performs well in real dialogues.11
These experiments demonstrate improvements given an initial fixed data set which
has been generated from existing systems. For applications where there are no existing
systems, an alternative would be to generate the initial data with a Wizard-of-Oz
experiment, where a human plays the part of the system, as explored by Williams and
Young (2003) and Rieser and Lemon (2006b). The methods proposed in this article can
be used to train a policy from such data without having to first build an initial system.
4. Conclusions
In this article, we have investigated how reinforcement learning can be applied to learn
dialogue management policies with large action sets and very large state spaces given
only a fixed data set of dialogues. Under a variety of metrics, our proposed hybrid re-
inforcement learning method outperforms both a policy trained with standard RL and a
11 Future work is to port the hand-coded policy back to the COMMUNICATOR domain for use in simulation.
This will investigate whether a relative improvement in simulated dialogues translates into a relative
improvement in real dialogues.
507
Computational Linguistics Volume 34, Number 4
policy trained with supervised learning, as well as the COMMUNICATOR systems which
generated the data it was trained on. This performance is achieved despite the extremely
challenging task, with 74 actions to choose between, over 10386 possible states, and
very few hand-coded policy decisions. The two main features of our model that make
this possible are the incorporation of supervised learning into a reinforcement learning
model, and the use of linear function approximation with state features provided by the
Information State Update approach to dialogue management. The supervised learning
is used to avoid states not covered by the data set, and the linear function approximation
is used to handle the very large state spaces.
With such a large space of possible state?action pairs, and therefore a huge policy
space, pure reinforcement learning would require an enormous amount of data to find
good policies. We have succeeded in using RL with fairly small data sets of only around
1,000 dialogues (in the portion used for training). This is achieved by using supervised
learning to model when an action would lead to a state for which we do not have
enough data. We proposed two methods for estimating a default value for these unseen
states, and derived a principled way to combine this value with the value estimated by
RL, using the probability provided by SL to weight this combination. This gave us two
hybrid RL/SL methods, both of which outperform both the RL and SL policies alone.
The best hybrid policy performs 302% better than the standard RL policy, and 1.4%
better than the SL policy, according to our automatic evaluation method. In addition,
according to our automatic evaluation method, the hybrid RL/SL policy outperforms
the systems used to generate the data. The best hybrid policy improves over the average
COMMUNICATOR system policy by 10% on our metric. This good performance has
been corroborated in separate experiments with human subjects (Lemon, Georgila, and
Henderson 2006), where the learned policy outperforms a state-of-the-art hand-coded
system.
The success of the hybrid method (and of pure supervised learning) on this chal-
lenging task indicates that linear function approximation is a viable approach to the very
large state spaces produced by the ISU framework. It also demonstrates the utility of a
feature-based representation of states, such as that used in the ISU approach. Further
improvement should be possible by tailoring the representation of states and actions
based on our experience so far (e.g., by including information about specific sequences
of moves), and by using automatic feature selection techniques. We should also be able
to get some improvement from more sophisticated function approximation methods,
such as kernel-based methods.
The next step is to better exploit the advantages of reinforcement learning. One
promising approach is to apply RL while running the learned policy against simulated
users, thereby allowing RL to explore parts of the policy and state spaces that are
not included in the COMMUNICATOR data. The hybrid policy we have learned on the
COMMUNICATOR data is a good starting point for this exploration. Also, the supervised
component within the hybrid system can be used to constrain the range of policies
that need to be explored when training the RL component. All of these advances will
improve techniques for bootstrapping and automatic optimization of dialogue manage-
ment policies from limited initial data sets.
Acknowledgments
This work was partially supported by the
European Commission under the FP6 project
?TALK: Talk and Look, Tools for Ambient
Linguistic Knowledge? (507802) and the FP7
project ?CLASSIC: Computational
Learning in Adaptive Systems for Spoken
Conversation? (216594), by the EPSRC
under grant EP/E019501/1, and by SHEFC
HR04016?Wellcome Trust VIP Award.
508
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
We thank Johanna Moore for proposing
the use of the COMMUNICATOR data set for
this work.
References
Bohlin, Peter, Robin Cooper, Elisabet
Engdahl, and Staffan Larsson. 1999.
Information states and dialog move
engines. Electronic Transactions in AI,
3(9). Available at www.ep.liu.se/ej/
etai/1999/D/.
Bos, Johan, Ewan Klein, Oliver Lemon, and
Tetsushi Oka. 2003. DIPPER: Description
and formalisation of an information-state
update dialogue system architecture. In
Proceedings of the 4th SIGdial Workshop on
Discourse and Dialogue, pages 115?124,
Sapporo.
Cheyer, Adam and David Martin. 2001.
The open agent architecture. Journal of
Autonomous Agents and Multi-Agent
Systems, 4(1/2):143?148.
Denecke, Matthias, Kohji Dohsaka, and
Mikio Nakano, 2005. Fast reinforcement
learning of dialogue policies using stable
function approximation. In K. Y. Su,
J. Tsujii, J.-H. Lee, and O. Y. Kwong,
Natural Language Processing, IJCNLP 2004.
Springer, Berlin, pages 1?11.
Frampton, Matthew and Oliver Lemon.
2005. Reinforcement learning of dialogue
strategies using the user?s last dialogue
act. In Proceedings of the 4th Workshop on
Knowledge and Reasoning in Practical Dialog
Systems, International Joint Conference on
Artificial Intelligence (IJCAI), pages 83?90,
Edinburgh.
Frampton, Matthew and Oliver Lemon.
2006. Learning more effective dialogue
strategies using limited dialogue move
features. In Proceedings of the 44th Meeting
of the Association for Computational
Linguistics, pages 185?192, Sydney.
Gabsdil, Malte and Oliver Lemon. 2004.
Combining acoustic and pragmatic
features to predict recognition
performance in spoken dialogue systems.
In Proceedings of the 42nd Meeting of the
Association for Computational Linguistics,
pages 344?351, Barcelona.
Georgila, Kallirroi, James Henderson, and
Oliver Lemon. 2005. Learning user
simulations for Information State
Update dialogue systems. In Proceedings
of the 9th European Conference on
Speech Communication and Technology
(Interspeech ? Eurospeech), pages 893?896,
Lisbon.
Georgila, Kallirroi, James Henderson, and
Oliver Lemon. 2006. User simulation
for spoken dialogue systems: Learning
and evaluation. In Proceedings of the
9th International Conference on Spoken
Language Processing (Interspeech ? ICSLP),
pages 1065?1068, Pittsburgh, PA.
Georgila, Kallirroi, Oliver Lemon, and
James Henderson. 2005. Automatic
annotation of COMMUNICATOR
dialogue data for learning dialogue
strategies and user simulations.
In Proceedings of the Ninth Workshop
on the Semantics and Pragmatics of
Dialogue (SEMDIAL), pages 61?68,
Nancy.
Georgila, Kallirroi, Oliver Lemon, James
Henderson, and Johanna Moore.
(submitted). Automatic annotation of
context and speech acts for dialogue
corpora.
Goddeau, D. and J. Pineau. 2000. Fast
reinforcement learning of dialog strategies.
In Proceedings of the IEEE International
Conference on Acoustics Speech and Signal
Processing (ICASSP), pages II?1233?1236,
Istanbul.
Henderson, James, Oliver Lemon, and
Kallirroi Georgila. 2005. Hybrid
reinforcement/supervised learning for
dialogue policies from COMMUNICATOR
data. In Proceedings of the 4th Workshop on
Knowledge and Reasoning in Practical Dialog
Systems, International Joint Conference on
Artificial Intelligence (IJCAI), pages 68?75,
Edinburgh.
Larsson, Staffan and David Traum. 2000.
Information state and dialogue
management in the TRINDI Dialogue
Move Engine Toolkit. Natural Language
Engineering, 6(3?4):323?340.
Lemon, Oliver, Kallirroi Georgila, and James
Henderson. 2006. Evaluating effectiveness
and portability of reinforcement learned
dialogue strategies with real users: the
TALK TownInfo evaluation. In Proceedings
of the IEEE/ACL 2006 Workshop on Spoken
Language Technology, pages 178?181,
Aruba.
Lemon, Oliver, Kallirroi Georgila, James
Henderson, Malte Gabsdil, Ivan
Meza-Ruiz, and Steve Young. 2005.
Integration of learning and adaptivity with
the ISU approach. Technical Report D4.1,
TALK Project.
Lemon, Oliver, Kallirroi Georgila, James
Henderson, and Matthew Stuttle. 2006.
An ISU dialogue system exhibiting
reinforcement learning of dialogue
policies: generic slot-filling in the TALK
509
Computational Linguistics Volume 34, Number 4
in-car system. In Proceedings of the
Demonstrations of EACL, pages 119?122,
Trento.
Lemon, Oliver, Kallirroi Georgila, and
Matthew Stuttle. 2005. Showcase
exhibiting reinforcement learning for
dialogue strategies in the in-car domain.
Technical Report D4.2, TALK Project.
Levin, Esther and Roberto Pieraccini. 1997.
A stochastic model of computer-human
interaction for learning dialogue strategies.
In Proceedings of the 5th European Conference
on Speech Communication and Technology
(Interspeech ? Eurospeech), pages 1883?1886,
Rhodes.
Levin, Esther, Roberto Pieraccini, and
Wieland Eckert. 2000. A stochastic model
of human-machine interaction for learning
dialog strategies. IEEE Transactions on
Speech and Audio Processing, 8(1):11?23.
Pietquin, Olivier. 2004. A Framework for
Unsupervised Learning of Dialogue Strategies.
Presses Universitaires de Louvain,
SIMILAR Collection.
Pietquin, Olivier and Thierry Dutoit. 2006a.
Dynamic Bayesian networks for NLU
simulation with application to dialog
optimal strategy learning. In Proceedings
of the IEEE International Conference on
Acoustics Speech and Signal Processing
(ICASSP), pages 49?52, Toulouse.
Pietquin, Olivier and Thierry Dutoit. 2006b.
A probabilistic framework for dialog
simulation and optimal strategy learning.
IEEE Transactions on Speech and Audio
Processing, 14(2):589?599.
Rieser, Verena and Oliver Lemon. 2006a.
Cluster-based user simulations for
learning dialogue strategies and the
SUPER evaluation metric. In Proceedings
of the 9th International Conference on Spoken
Language Processing (Interspeech ? ICSLP),
pages 1766?1769, Pittsburgh, PA.
Rieser, Verena and Oliver Lemon. 2006b.
Using logistic regression to initialise
reinforcement-learning-based dialogue
systems. In Proceedings of the IEEE/ACL
2006 Workshop on Spoken Language
Technology, pages 190?193, Aruba.
Rieser, Verena and Oliver Lemon. 2006c.
Using machine learning to explore
human multimodal clarification
strategies. In Proceedings of the Poster
Session of the 44th Meeting of the
Association for Computational Linguistics,
pages 659?666, Sydney.
Roy, Nicholas, Joelle Pineau, and Sebastian
Thrun. 2000. Spoken dialog management
for robots. In Proceedings of the 38th Meeting
of the Association for Computational
Linguistics, pages 93?100, Hong Kong.
Schatzmann, Jost, Kallirroi Georgila,
and Steve Young. 2005. Quantitative
evaluation of user simulation techniques
for spoken dialogue systems. In
Proceedings of the 6th SIGdial Workshop
on Discourse and Dialogue, pages 45?54,
Lisbon.
Schatzmann, Jost, Matthew N. Stuttle,
Karl Weilhammer, and Steve Young.
2005. Effects of the user model on
simulation-based learning of dialogue
strategies. In Proceedings of the IEEE
Automatic Speech Recognition and
Understanding Workshop, pages 220?225,
San Juan, Puerto Rico.
Schatzmann, Jost, Karl Weilhammer,
Matthew N. Stuttle, and Steve Young.
2006. A survey of statistical user
simulation techniques for
reinforcement-learning of dialogue
management strategies. The Knowledge
Engineering Review, 21:97?126.
Scheffler, Konrad and Steve Young. 2001.
Corpus-based dialogue simulation
for automatic strategy learning and
evaluation. In Proceedings of the NAACL
Workshop on Adaptation in Dialogue
Systems, pages 64?70, Pittsburgh, PA.
Scheffler, Konrad and Steve Young.
2002. Automatic learning of dialogue
strategy using dialogue simulation
and reinforcement learning. In Proceedings
of the Human Language Technology
Conference, pages 12?19, San Diego, CA.
Shawe-Taylor, John and Nello Cristianini.
2004. Kernel Methods for Pattern Analysis.
Cambridge University Press.
Singh, Satinder, Michael Kearns, Diane
Litman, and Marilyn Walker. 2000a.
Empirical evaluation of a reinforcement
learning dialogue system. In Proceedings
of the AAAI, pages 645?651, Whistler.
Singh, Satinder, Michael Kearns, Diane
Litman, and Marilyn Walker. 2000b.
Reinforcement learning for spoken
dialogue systems. In Advances in Neural
Information Processing Systems, 12:956?962.
Singh, Satinder, Diane Litman, Michael
Kearns, and Marilyn Walker. 2002.
Optimizing dialogue management with
reinforcement learning: Experiments
with the NJFun system. Journal of
Artificial Intelligence Research (JAIR),
16:105?133.
Sutton, Richard and Andrew Barto. 1998.
Reinforcement Learning. MIT Press,
Cambridge, MA.
510
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
Walker, M., J. Aberdeen, J. Boland, E. Bratt,
J. Garofolo, L. Hirschman, A. Le, S. Lee,
S. Narayanan, K. Papineni, B. Pellom,
B. Polifroni, A. Potamianos, P. Prabhu,
A. Rudnicky, G. Sanders, S. Seneff,
D. Stallard, and S. Whittaker. 2001.
DARPA communicator dialog travel
planning systems: The June 2000 data
collection. In Proceedings of the 7th European
Conference on Speech Communication and
Technology (Interspeech ? Eurospeech),
pages 1371?1374, Aalborg.
Walker, M. and R. Passonneau. 2001. DATE:
A dialogue act tagging scheme for
evaluation of spoken dialogue systems.
In Proceedings of the Human Language
Technology Conference, pages 1?8, San
Diego, CA.
Walker, M., A. Rudnicky, J. Aberdeen,
E. Bratt, J. Garofolo, H. Hastie, A. Le,
B. Pellom, A. Potamianos, R. Passonneau,
R. Prasad, S. Roukos, G. Sanders, S. Seneff,
D. Stallard, and S. Whittaker. 2002a.
DARPA Communicator Evaluation:
Progress from 2000 to 2001. In Proceedings
of the 7th International Conference on Spoken
Language Processing (Interspeech ? ICSLP),
pages 273?276, Denver, CO.
Walker, M., A. Rudnicky, R. Prasad,
J. Aberdeen, E. Bratt, J. Garofolo,
H. Hastie, A. Le, B. Pellom, A. Potamianos,
R. Passonneau, S. Roukos, G. Sanders,
S. Seneff, and D. Stallard. 2002b. DARPA
Communicator: Cross-system results
for the 2001 evaluation. In Proceedings of
the 7th International Conference on Spoken
Language Processing (Interspeech ? ICSLP),
pages 269?272, Denver, CO.
Walker, Marilyn A., Jeanne C. Fromer, and
Shrikanth Narayanan. 1998. Learning
optimal dialogue strategies: A case study
of a spoken dialogue agent for email.
In Proceedings of the 17th International
Conference on Computational Linguistics,
pages 1345?1351, Montreal.
Walker, Marilyn A., Candace A. Kamm,
and Diane J. Litman. 2000. Towards
developing general models of usability
with PARADISE. Natural Language
Engineering, 6(3):363?377.
Walker, Marilyn A., Rebecca J. Passonneau,
and Julie E. Boland. 2001. Quantitative
and qualitative evaluation of DARPA
Communicator spoken dialogue systems.
In Proceedings of the 39th Meeting of the
Association for Computational Linguistics,
pages 515?522, Toulouse.
Williams, Jason. 2007. A method for
evaluating and comparing user
simulations: The Cramer-von Mises
divergence. In Proceedings of the IEEE
Automatic Speech Recognition and
Understanding Workshop, pages 508?513,
Kyoto.
Williams, Jason, Pascal Poupart, and
Steve Young. 2005a. Factored partially
observable Markov decision processes for
dialogue management. In Proceedings of the
4th Workshop on Knowledge and Reasoning in
Practical Dialog Systems, International Joint
Conference on Artificial Intelligence (IJCAI),
pages 76?82, Edinburgh.
Williams, Jason, Pascal Poupart, and Steve
Young. 2005b. Partially observable Markov
decision processes with continuous
observations for dialogue management.
In Proceedings of the 6th SIGdial Workshop
on Discourse and Dialogue, pages 25?34,
Lisbon.
Williams, Jason and Steve Young. 2003. Using
Wizard-of-Oz simulations to bootstrap
reinforcement-learning-based dialog
management systems. In Proceedings of the
4th SIGdial Workshop on Discourse and
Dialogue, pages 135?139, Sapporo.
Williams, Jason and Steve Young. 2005.
Scaling up POMDPs for dialog
management: The ?Summary POMDP?
method. In Proceedings of the IEEE
Automatic Speech Recognition and
Understanding Workshop, pages 177?182,
San Juan, Puerto Rico.
Young, Steve. 2000. Probabilistic methods in
spoken dialogue systems. Philosophical
Transactions of the Royal Society (Series A),
358(1769):1389?1402.
511

Inducing History Representations for
Broad Coverage Statistical Parsing
James Henderson
Department of Computer Science, University of Geneva, Gene`ve, Switzerland
James.Henderson@cui.unige.ch
Abstract
We present a neural network method for induc-
ing representations of parse histories and us-
ing these history representations to estimate the
probabilities needed by a statistical left-corner
parser. The resulting statistical parser achieves
performance (89.1% F-measure) on the Penn
Treebank which is only 0.6% below the best
current parser for this task, despite using a
smaller vocabulary size and less prior linguistic
knowledge. Crucial to this success is the use of
structurally determined soft biases in inducing
the representation of the parse history, and no
use of hard independence assumptions.
1 Introduction
Unlike most problems addressed with machine learning,
parsing natural language sentences requires choosing be-
tween an unbounded (or even infinite) number of possi-
ble phrase structure trees. The standard approach to this
problem is to decompose this choice into an unbounded
sequence of choices between a finite number of possible
parser actions. This sequence is the parse for the phrase
structure tree. We can then define a probabilistic model of
phrase structure trees by defining a probabilistic model of
each parser action in its parse context, and apply machine
learning techniques to learn this model of parser actions.
Many statistical parsers (Ratnaparkhi, 1999; Collins,
1999; Charniak, 2000) are based on a history-based
model of parser actions. In these models, the probabil-
ity of each parser action is conditioned on the history of
previous actions in the parse. But here again we are faced
with an unusual situation for machine learning problems,
conditioning on an unbounded amount of information.
A major challenge in designing a history-based statisti-
cal parser is choosing a finite representation of the un-
bounded parse history from which the probability of the
next parser action can be accurately estimated. Previ-
ous approaches have used a hand-crafted finite set of fea-
tures to represent the parse history (Ratnaparkhi, 1999;
Collins, 1999; Charniak, 2000). In the work presented
here, we automatically induce a finite set of real valued
features to represent the parse history.
We perform the induction of a history representation
using an artificial neural network architecture, called
Simple Synchrony Networks (SSNs) (Lane and Hen-
derson, 2001; Henderson, 2000). This machine learn-
ing method is specifically designed for processing un-
bounded structures. It allows us to avoid making a priori
independence assumptions, unlike with hand-crafted his-
tory features. But it also allows us to make use of our a
priori knowledge by imposing structurally specified and
linguistically appropriate biases on the search for a good
history representation.
The combination of automatic feature induction and
linguistically appropriate biases results in a history-based
parser with state-of-the-art performance. When trained
on just part-of-speech tags, the resulting parser achieves
the best current performance of a non-lexicalized parser
on the Penn Treebank. When a relatively small vocab-
ulary of words is used, performance is only marginally
below the best current parser accuracy. If either the bi-
ases are reduced or the induced history representations
are replaced with hand-crafted features, performance de-
grades.
2 Estimating the Parameters of the
Probability Model
The parsing system we propose consists of two compo-
nents, one which estimates the parameters of a proba-
bility model for phrase structure trees, and one which
searches for the most probable phrase structure tree given
these parameters. The probability model we use is gen-
erative and history-based. At each step, the model?s
stochastic process generates a characteristic of the tree
                                                               Edmonton, May-June 2003
                                                               Main Papers , pp. 24-31
                                                         Proceedings of HLT-NAACL 2003
or a word of the sentence. This sequence of decisions is
the derivation of the tree, which we will denote
  
	
.
Because there is a one-to-one mapping from phrase struc-
ture trees to our derivations, we can use the chain rule
for conditional probabilities to derive the probability of
a tree as the multiplication of the probabilities of each
derivation decision
 
conditional on that decision?s prior
derivation history
     
.

tree
      	Proceedings of NAACL HLT 2009: Short Papers, pages 125?128,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Domain Adaptation with Artificial Data for Semantic Parsing of Speech
Lonneke van der Plas
Department of Linguistics
University of Geneva
Geneva, Switzerland
James Henderson
Department of Computer Science
University of Geneva
Geneva, Switzerland
{Lonneke.vanderPlas,James.Henderson,Paola.Merlo}@unige.ch
Paola Merlo
Department of Linguistics
University of Geneva
Geneva, Switzerland
Abstract
We adapt a semantic role parser to the do-
main of goal-directed speech by creating an
artificial treebank from an existing text tree-
bank. We use a three-component model that
includes distributional models from both tar-
get and source domains. We show that we im-
prove the parser?s performance on utterances
collected from human-machine dialogues by
training on the artificially created data without
loss of performance on the text treebank.
1 Introduction
As the quality of natural language parsing improves
and the sophistication of natural language under-
standing applications increases, there are several do-
mains where parsing, and especially semantic pars-
ing, could be useful. This is particularly true in
adaptive systems for spoken language understand-
ing, where complex utterances need to be translated
into shallow semantic representation, such as dia-
logue acts.
The domain on which we are working is goal-
directed system-driven dialogues, where a system
helps the user to fulfil a certain goal, e.g. booking a
hotel room. Typically, users respond with short an-
swers to questions posed by the system. For exam-
ple In the South is an answer to the question Where
would you like the hotel to be? Parsing helps iden-
tifying the components (In the South is a PP) and
semantic roles identify the PP as a locative, yield-
ing the following slot-value pair for the dialogue act:
area=South. A PP such as in time is not identified as
a locative, whereas keyword-spotting techniques as
those currently used in dialogue systems may pro-
duce area=South and area=time indifferently.
Statistical syntactic and semantic parsers need
treebanks. Current available data is lacking in one or
more respects: Syntactic/semantic treebanks are de-
veloped on text, while treebanks of speech corpora
are not semantically annotated (e.g. Switchboard).
Moreover, the available human-human speech tree-
banks do not exhibit the same properties as the
system-driven speech on which we are focusing, in
particular in their proportion of non-sentential utter-
ances (NSUs), utterances that are not full sentences.
In a corpus study of a subset of the human-human
dialogues in the BNC, Ferna?ndez (2006) found that
only 9% of the total utterances are NSUs, whereas
we find 44% in our system-driven data.
We illustrate a technique to adapt an exist-
ing semantic parser trained on merged Penn Tree-
bank/PropBank data to goal-directed system-driven
dialogue by artificial data generation. Our main con-
tribution lies in the framework used to generate ar-
tificial data for domain adaptation. We mimic the
distributions over parse structures in the target do-
main by combining the text treebank data and the
artificially created NSUs, using a three-component
model. The first component is a hand-crafted model
of NSUs. The second component describes the dis-
tribution over full sentences and types of NSUs as
found in a minimally annotated subset of the target
domain. The third component describes the distribu-
tion over the internal parse structure of the generated
data and is taken from the source domain.
Our approach differs from most approaches to do-
main adaptation, which require some training on
fully annotated target data (Nivre et al, 2007),
whereas we use minimally annotated target data
only to help determine the distributions in the ar-
tificially created data. It also differs from previ-
125
ous work in domain adaptation by Foster (2007),
where similar proportions of ungrammatical and
grammatical data are combined to train a parser
on ungrammatical written text, and by Weilhammer
et al (2006), who use interpolation between two
separately trained models, one on an artificial cor-
pus of user utterances generated by a hand-coded
domain-specific grammar and one on available cor-
pora. Whereas much previous work on parsing
speech has focused on speech repairs, e.g. Charniak
and Johnson (2001), we focus on parsing NSUs.
2 The first component: a model of NSUs
To construct a model of NSUs we studied a subset of
the data under consideration: TownInfo. This small
corpus of transcribed spoken human-machine dia-
logues in the domain of hotel/restaurant/bar search
is gathered using the TownInfo tourist information
system (Lemon et al, 2006).
The NSUs we find in our data are mainly of the
type answers, according to the classification given
in Ferna?ndez (2006). More specifically, we find
short answers, plain and repeated affirmative an-
swers, plain and helpful rejections, but also greet-
ings.
Current linguistic theory provides several ap-
proaches to dealing with NSUs (Merchant, 2004;
Progovac et al, 2006; Ferna?ndez, 2006). Follow-
ing the linguistic analysis of NSUs as non-sentential
small clauses (Progovac et al, 2006) that do not have
tense or agreement functional nodes, we make the
assumption that they are phrasal projections. There-
fore, we reason, we can create an artificial data set
of NSUs by extracting phrasal projections from an
annotated treebank.
In the example given in the introduction, we saw
a PP fragment, but fragments can be NPs, APs, etc.
We define different types of NSUs based on the root
label of the phrasal projection and define rules that
allow us to extract NSUs (partial parse trees) from
the source corpus.1 Because the target corpus also
contains full sentences, we allow full sentences to
be taken without modification from the source tree-
bank.
1Not all of these rules are simple extractions of phrasal pro-
jections, as described in section 4.
3 The two distributional components
The distributional model consists of two compo-
nents. By applying the extraction rules to the source
corpus we build a large collection of both full sen-
tences and NSUs. The distributions in this collec-
tion follow the distributions of trees in the source do-
main (first distributional component). We then sam-
ple from this collection to generate our artificial cor-
pus following distributions from the target domain
(second distributional component).
The probability of an artificial tree P (fi(cj)) gen-
erated with an extraction rule fi applied to a con-
stituent from the source corpus cj is defined as
P (fi(cj)) = P (fi)P (cj |fi) ? Pt(fi)Ps(cj |fi)
The first distributional component originates from
the source domain. It is responsible for the internal
structure of the NSUs and full sentences extracted.
Ps(cj |fi) is the probability of the constituent taken
from the source treebank (cj), given that the rule fi
is applicable to that constituent.
Sampling is done according to distributions of
NSUs and full sentences found in the target corpus
(Pt(fi)). As explained in section 2, there are several
types of NSUs found in the target domain. This sec-
ond component describes the distributions of types
of NSUs (or full sentences) found in the target do-
main. It determines, for example, the proportion of
NP NSUs that will be added to the artificial corpus.
To determine the target distribution we classified
171 (approximately 5%) randomly selected utter-
ances from the TownInfo data, that were used as a
development set.2 In Table 1 we can see that 15.2 %
of the trees in the artificial corpus will be NP NSUs.3
4 Data generation
We constructed our artificial corpus from sections
2 to 21 of the Wall Street Journal (WSJ) section
of the Penn Treebank corpus (Marcus et al, 1993)
2We discarded very short utterances (yes, no, and greetings)
since they don?t need parsing. We also do not consider incom-
plete NSUs resulting from interruptions or recording problems.
3Because NSUs can be interpreted only in context, the same
NSU can correspond to several syntactic categories: South for
example, can be an noun, an adverb, or an adjective. In case of
ambiguity, we divided the score up for the several possible tags.
This accounts for the fractional counts.
126
Category # Occ. Perc. Category # Occ. Perc.
NP 19.0 15.2 RB 1.7 1.3
JJ 12.7 10.1 DT 1.0 0.8
PP 12.0 9.6 CD 1.0 0.8
NN 11.7 9.3 Total frag. 70.0 56.0
VP 11.0 8.8 Full sents 55.0 44.0
Table 1: Distribution of types of NSUs and full sentences
in the TownInfo development set.
merged with PropBank labels (Palmer et al, 2005).
We included all the sentences from this dataset in
our artificial corpus, giving us 39,832 full sentences.
In accordance with the target distribution we added
50,699 NSUs extracted from the same dataset. We
sampled NSUs according to the distribution given in
Table 1. After the extraction we added a root FRAG
node to the extracted NSUs4 and we capitalised the
first letter of each NSU to form an utterance.
There are two additional pre-processing steps.
First, for some types of NSUs maximal projections
are added. For example, in the subset from the tar-
get source we saw many occurrences of nouns with-
out determiners, such as Hotel or Bar. These types
of NSUs would be missed if we just extracted NPs
from the source data, since we assume that NSUs are
maximal projections. Therefore, we extracted single
nouns as well and we added the NP phrasal projec-
tions to these nouns in the constructed trees. Sec-
ond, not all extracted NSUs can keep their semantic
roles. Extracting part of the sentence often severs
the semantic role from the predicate of which it was
originally an argument. An exception to this are VP
NSUs and prepositional phrases that are modifiers,
such as locative PPs, which are not dependent on the
verb. Hence, we removed the semantic roles from
the generated NSUs except for VPs and modifiers.
5 Experiments
We trained three parsing models on both the original
non-augmented merged Penn Treebank/Propbank
corpus and the artificially generated augmented tree-
bank including NSUs. We ran a contrastive ex-
periment to examine the usefulness of the three-
component model by training two versions of the
4The node FRAG exists in the Penn Treebank. Our annota-
tion does not introduce new labels, but only changes their dis-
tribution.
augmented model: One with and one without the
target component.5
These models were tested on two test sets: a small
corpus of 150 transcribed utterances taken from the
TownInfo corpus, annotated with gold syntactic and
semantic annotation by two of the authors6: the
TownInfo test set. The second test set is used to
compare the performance of the parser on WSJ-style
sentences and consists of section 23 of the merged
Penn Treebank/Propbank corpus. We will refer to
this test set as the non-augmented test set.
5.1 The statistical parser
The parsing model is the one proposed in Merlo
and Musillo (2008), which extends the syntactic
parser of Henderson (2003) and Titov and Hender-
son (2007) with annotations which identify seman-
tic role labels, and has competitive performance.
The parser uses a generative history-based proba-
bility model for a binarised left-corner derivation.
The probabilities of derivation decisions are mod-
elled using the neural network approximation (Hen-
derson, 2003) to a type of dynamic Bayesian Net-
work called an Incremental Sigmoid Belief Network
(ISBN) (Titov and Henderson, 2007).
The ISBN models the derivation history with a
vector of binary latent variables. These latent vari-
ables learn to represent features of the parse history
which are useful for making the current and subse-
quent derivation decisions. Induction of these fea-
tures is biased towards features which are local in
the parse tree, but can find features which are passed
arbitrarily far through the tree. This flexible mecha-
nism for feature induction allows the model to adapt
to the parsing of NSUs without requiring any design
changes or feature engineering.
5.2 Results
In Table 2, we report labelled constituent recall, pre-
cision, and F-measure for the three trained parsers
(rows) on the two test sets (columns).7 These mea-
5The model without the target distribution has a uniform dis-
tribution over full sentences and NSUs and within NSUs a uni-
form distribution over the 8 types.
6This test set was constructed separately and is completely
different from the development set used to determine the distri-
butions in the target data.
7Statistical significance is determined using a stratified shuf-
fling method, using software available at http://www.cis.
127
Training Testing
TownInfo PTB nonaug
Rec Prec F Rec Prec F
PTB nonaug 69.4 76.7 72.9 81.4 82.1 81.7
PTB aug(+t) 81.4 77.8 79.5 81.3 82.0 81.7
PTB aug(?t) 62.6 64.3 63.4 81.2 81.9 81.6
Table 2: Recall, precision, and F-measure for the two test
sets, trained on non-augmented data and data augmented
with and without the target distribution component.
sures include both syntactic labels and semantic role
labels.
The results in the first two lines of the columns
headed TownInfo indicate the performance on the
real data to which we are trying to adapt our parser:
spoken data from human-machine dialogues. The
parser does much better when trained on the aug-
mented data. The differences between training on
newspaper text and newspaper texts augmented with
artificially created data are statistically significant
(p < 0.001) and particularly large for recall: almost
12%.
The columns headed PTB nonaug show that the
performance on parsing WSJ texts is not hurt by
training on data augmented with artificially cre-
ated NSUs (first vs. second line). The difference
in performance compared to training on the non-
augmented data is not statistically significant.
The last two rows of the TownInfo data show the
results of our contrastive experiment. It is clear
that the three-component model and in particular our
careful characterisation of the target distribution is
indispensable. The F-measure drops from 79.5% to
63.4% when we disregard the target distribution.
6 Conclusions
We have shown how a three-component model that
consists of a model of the phenomenon being stud-
ied and two distributional components, one from the
source data and one from the target data, allows
one to create data artificially for training a seman-
tic parser. Specifically, analysis and minimal anno-
tation of only a small subset of utterances from the
target domain of spoken dialogue systems suffices
to determine a model of NSUs as well as the nec-
essary target distribution. Following this framework
upenn.edu/?dbikel/software.html.
we were able to improve the performance of a statis-
tical parser on goal-directed spoken data extracted
from human-machine dialogues without degrading
the performance on full sentences.
Acknowledgements
The research leading to these results has received
funding from the EU FP7 programme (FP7/2007-
2013) under grant agreement nr 216594 (CLASSIC
project: www.classic-project.org).
References
E. Charniak and M. Johnson. 2001. Edit detection and
parsing for transcribed speech. In Procs. NAACL.
R. Ferna?ndez. 2006. Non-sentential utterances in dia-
logue: classification resolution and use. Ph.D. thesis,
University of London.
J. Foster. 2007. Treebanks gone bad: Parser evaluation
and retraining using a treebank of ungrammatical sen-
tences. International Journal of Document Analysis
and Recognition, 10:1?16.
J. Henderson. 2003. Inducing history representations for
broad-coverage statistical parsing. In Procs. NAACL-
HLT.
O. Lemon, K. Georgila, J. Henderson, and M. Stuttle.
2006. An ISU dialogue system exhibiting reinforce-
ment learning of dialogue policies: generic slot-filling
in the TALK in-car system. In Procs. EACL.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
the Penn Treebank. Comp. Ling., 19:313?330.
J. Merchant. 2004. Fragments and ellipsis. Linguistics
and Philosophy, 27:661?738.
P. Merlo and G. Musillo. 2008. Semantic parsing
for high-precision semantic role labelling. In Procs.
CONLL.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Procs. EMNLP-
CoNLL.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Comp. Ling., 31:71?105.
L. Progovac, K. Paesani, E. Casielles, and E. Barton.
2006. The Syntax of Nonsententials:Multidisciplinary
Perspectives. John Benjamins.
I Titov and J Henderson. 2007. Constituent parsing with
Incremental Sigmoid Belief Networks. In Procs. ACL.
K. Weilhammer, M. Stuttle, and S. Young. 2006. Boot-
strapping language models for dialogue systems. In
Procs. Conf. on Spoken Language Processing.
128
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 73?76,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Mixture Model POMDPs for Efficient Handling of Uncertainty
in Dialogue Management
James Henderson
University of Geneva
Department of Computer Science
James.Henderson@cui.unige.ch
Oliver Lemon
University of Edinburgh
School of Informatics
olemon@inf.ed.ac.uk
Abstract
In spoken dialogue systems, Partially Observ-
able Markov Decision Processes (POMDPs)
provide a formal framework for making di-
alogue management decisions under uncer-
tainty, but efficiency and interpretability con-
siderations mean that most current statistical
dialogue managers are only MDPs. These
MDP systems encode uncertainty explicitly in
a single state representation. We formalise
such MDP states in terms of distributions
over POMDP states, and propose a new di-
alogue system architecture (Mixture Model
POMDPs) which uses mixtures of these dis-
tributions to efficiently represent uncertainty.
We also provide initial evaluation results (with
real users) for this architecture.
1 Introduction
Partially Observable Markov Decision Processes
(POMDPs) provide a formal framework for mak-
ing decisions under uncertainty. Recent research
in spoken dialogue systems has used POMDPs for
dialogue management (Williams and Young, 2007;
Young et al, 2007). These systems represent the
uncertainty about the dialogue history using a prob-
ability distribution over dialogue states, known as
the POMDP?s belief state, and they use approxi-
mate POMDP inference procedures to make dia-
logue management decisions. However, these infer-
ence procedures are too computationally intensive
for most domains, and the system?s behaviour can be
difficult to predict. Instead, most current statistical
dialogue managers use a single state to represent the
dialogue history, thereby making them only Markov
Decision Process models (MDPs). These state rep-
resentations have been fine-tuned over many devel-
opment cycles so that common types of uncertainty
can be encoded in a single state. Examples of such
representations include unspecified values, confi-
dence scores, and confirmed/unconfirmed features.
We formalise such MDP systems as compact encod-
ings of POMDPs, where each MDP state represents
a probability distribution over POMDP states. We
call these distributions ?MDP belief states?.
Given this understanding of MDP dialogue man-
agers, we propose a new POMDP spoken dialogue
system architecture which uses mixtures of MDP be-
lief states to encode uncertainty. A Mixture Model
POMDP represents its belief state as a probability
distribution over a finite set of MDP states. This
extends the compact representations of uncertainty
in MDP states to include arbitrary disjunction be-
tween MDP states. Efficiency is maintained because
such arbitrary disjunction is not needed to encode
the most common forms of uncertainty, and thus the
number of MDP states in the set can be kept small
without losing accuracy. On the other hand, allow-
ing multiple MDP states provides the representa-
tional mechanism necessary to incorporate multiple
speech recognition hypotheses into the belief state
representation. In spoken dialogue systems, speech
recognition is by far the most important source of
uncertainty. By providing a mechanism to incorpo-
rate multiple arbitrary speech recognition hypothe-
ses, the proposed architecture leverages the main ad-
vantage of POMDP systems while still maintaining
the efficiency of MDP-based dialogue managers.
2 Mixture Model POMDPs
A POMDP belief state bt is a probability distribution
P (st|Vt?1, ut) over POMDP states st given the dia-
73
logue history Vt?1 and the most recent observation
(i.e. user utterance) ut. We formalise the meaning
of an MDP state representation rt as a distribution
b(rt) = P (st|rt) over POMDP states. We represent
the belief state bt as a list of pairs ?rit, p
i
t? such that?
i p
i
t = 1. This list is interpreted as a mixture of
the b(rit).
bt =
?
i
pitb(r
i
t) (1)
State transitions in MDPs are specified with an
update function, rt = f(rt?1, at?1, ht), which maps
the preceding state rt?1, system action at?1, and
user input ht to a new state rt. This function is in-
tended to encode in rt all the new information pro-
vided by at?1 and ht. The user input ht is the result
of automatic speech recognition (ASR) plus spoken
language understanding (SLU) applied to ut. Be-
cause there is no method for handling ambiguity in
ht, ht is computed from the single best ASR-SLU
hypothesis, plus some measure of ASR confidence.
In POMDPs, belief state transitions are done by
changing the distribution over states to take into ac-
count the new information from the system action
at?1 and an n-best list of ASR-SLU hypotheses h
j
t .
This new belief state can be estimated as
bt = P (st|Vt?1, ut)
=
?
hjt
?
st?1
P (st?1|Vt?1)P (h
j
t |Vt?1, st?1)
P (ut|Vt?1, st?1, h
j
t )
P (st|Vt?1, st?1, h
j
t , ut)
P (ut|Vt?1)
?
?
hjt
?
st?1
P (st?1|Vt?2, ut?1)P (h
j
t |at?1, st?1)
P (hjt |ut)P (st|at?1, st?1, h
j
t )
P (hjt )Z(Vt)
where Z(Vt) is a normalising constant.
P (st?1|Vt?2, ut?1) is the previous belief state.
P (hjt |ut) reflects the confidence of ASR-SLU in
hypothesis hjt . P (st|at?1, st?1, h
j
t ) is normally 1
for st = st?1, but can be used to allow users to
change their mind mid-dialogue. P (hjt |at?1, st?1)
is a user model. P (hjt ) is a prior over ASR-SLU
outputs.
Putting these two approaches together, we get the
following update equation for our mixture of MDP
belief states:
bt = P (st|Vt?1, ut)
?
?
hjt
?
rit?1
pit?1P (h
j
t |at?1, r
i
t?1)
P (hjt |ut)b(f(r
i
t?1, at?1, h
j
t ))
P (hjt )Z(Vt)
(2)
=
?
i?
pi
?
t b(r
i?
t )
where, for each i? there is one pair i, j such that
ri
?
t = f(r
i
t?1, at?1, h
j
t )
pi
?
t =
pit?1P (h
j
t |at?1,r
i
t?1)P (h
j
t |ut)
P (hjt )Z(Vt)
.
(3)
For equation (2) to be true, we require that
b(f(rit?1, at?1, h
j
t )) ? P (st|at?1, r
i
t?1, h
j
t ) (4)
which simply ensures that the meaning assigned to
MDP state representations and the MDP state tran-
sition function are compatible.
From equation (3), we see that the number
of MDP states will grow exponentially with the
length of the dialogue, proportionately to the num-
ber of ASR-SLU hypotheses. Some of the state-
hypothesis pairs rit?1, h
j
t may lead to equivalent
states f(rit?1, at?1, h
j
t ), but in general pruning is
necessary. Pruning should be done so as to min-
imise the change to the belief state distribution, for
example by minimising the KL divergence between
the pre- and post- pruning belief states. We use two
heuristic approximations to this optimisation prob-
lem. First, if two states share the same core features
(e.g. filled slots, but not the history of user inputs),
then the state with the lower probability is pruned,
and its probability is added to the other state. Sec-
ond, a fixed beam of the k most probable states is
kept, and the other states are pruned. The probabil-
ity pit from a pruned state r
i
t is redistributed to un-
pruned states which are less informative than rit in
their core features.1
The interface between the ASR-SLU module and
the dialogue manager is a set of hypotheses hjt paired
with their confidence scores P (hjt |ut). These pairs
are analogous to the state-probability pairs rit, p
i
t
within the dialogue manager, and we can extend our
mixture model architecture to cover these pairs as
well. Interpreting the set of hjt , P (h
j
t |ut) pairs as a
1In the current implementation, these pruned state probabil-
ities are simply added to an uninformative ?null? state, but in
general we could check for logical subsumption between states.
74
mixture of distributions over more specific hypothe-
ses becomes important when we consider pruning
this set before passing it to the dialogue manager. As
with the pruning of states, pruning should not sim-
ply remove a hypothesis and renormalise, it should
redistribute the probability of a pruned hypothesis to
similar hypotheses. This is not always computation-
ally feasible, but all interfaces within the Mixture
Model POMDP architecture are sets of hypothesis-
probability pairs which can be interpreted as finite
mixtures in some underlying hypothesis space.
Given an MDP state representation, this formali-
sation allows us to convert it into a Mixture Model
POMDP. The only additional components of the
model are the user model P (hjt |at?1, r
i
t?1), the
ASR-SLU prior P (hjt ), and the ASR-SLU confi-
dence score P (hjt |ut). Note that there is no need
to actually define b(rit), provided equation (4) holds.
3 Decision Making with MM POMDPs
Given this representation of the uncertainty in the
current dialogue state, the spoken dialogue system
needs to decide what system action to perform.
There are several approaches to POMDP decision
making which could be adapted to this representa-
tion, but to date we have only considered a method
which allows us to directly derive a POMDP policy
from the policy of the original MDP.
Here again we exploit the fact that the most fre-
quent forms of uncertainty are already effectively
handled in the MDP system (e.g. by filled vs. con-
firmed slot values). We propose that an effective di-
alogue management policy can be created by sim-
ply computing a mixture of the MDP policy applied
to the MDP states in the belief state list. More
precisely, we assume that the original MDP system
specifies a Q function QMDP(at, rt) which estimates
the expected future reward of performing action at
in state rt. We then estimate the expected future re-
ward of performing action at in belief state bt as the
mixture of these MDP estimates.
Q(at, bt) ?
?
i
pitQMDP(at, r
i
t) (5)
The dialogue management policy is to choose the
action at with the largest value for Q(at, bt). This is
known as a Q-MDP model (Littman et al, 1995), so
we call this proposal a Mixture Model Q-MDP.
4 Related Work
Our representation of POMDP belief states using a
set of distributions over POMDP states is similar to
the approach in (Young et al, 2007), where POMDP
belief states are represented using a set of partitions
of POMDP states. For any set of partitions, the mix-
ture model approach could express the same model
by defining one MDP state per partition and giving
it a uniform distribution inside its partition and zero
probability outside. However, the mixture model ap-
proach is more flexible, because the distributions in
the mixture do not have to be uniform within their
non-zero region, and these regions do not have to
be disjoint. A list of states was also used in (Hi-
gashinaka et al, 2003) to represent uncertainty, but
no formal semantics was provided for this list, and
therefore only heuristic uses were suggested for it.
5 Initial Experiments
We have implemented a Mixture Model POMDP ar-
chitecture as a multi-state version of the DIPPER
?Information State Update? dialogue manager (Bos
et al, 2003). It uses equation (3) to compute belief
state updates, given separate models for MDP state
updates (for f(rit?1, at?1, h
j
t )), statistical ASR-SLU
(for P (hjt |ut)/P (h
j
t )), and a statistical user model
(for P (hjt |at?1, r
i
t?1)). The state list is pruned as
described in section 2, where the ?core features?
are the filled information slot values and whether
they have been confirmed. For example, the sys-
tem will merge two states which agree that the user
only wants a cheap hotel, even if they disagree on
the sequence of dialogue acts which lead to this in-
formation. It also never prunes the ?null? state, so
that there is always some probability that the system
knows nothing.
The system used in the experiments described
below uses the MDP state representation and up-
date function from (Lemon and Liu, 2007), which
is designed for standard slot-filling dialogues. For
the ASR model, it uses the HTK speech recogniser
(Young et al, 2002) and an n-best list of three ASR
hypotheses on each user turn. The prior over user in-
puts is assumed to be uniform. The ASR hypotheses
are passed to the SLU model from (Meza-Ruiz et al,
2008), which produces a single user input for each
ASR hypothesis. This SLU model was trained on
75
TC % Av. length (std. deviation)
Handcoded 56.0 7.2 (4.6)
MDP 66.6 7.2 (4.0)
MM Q-MDP 73.3 7.3 (3.7)
Table 1: Initial test results for human-machine dialogues,
showing task completion and average length.
the TownInfo corpus of dialogues, which was col-
lected using the TownInfo human-machine dialogue
systems of (Lemon et al, 2006), transcribed, and
hand annotated. ASR hypotheses which result in the
same user input are merged (summing their proba-
bilities), and the resulting list of at most three ASR-
SLU hypotheses are passed to the dialogue manager.
Thus the number of MDP states in the dialogue man-
ager grows by up to three times at each step, before
pruning. For the user model, the system uses an n-
gram user model, as described in (Georgila et al,
2005), trained on the annotated TownInfo corpus.2
The system?s dialogue management policy is a
Mixture Model Q-MDP (MM Q-MDP) policy. As
with the MDP states, the MDP Q function is from
(Lemon and Liu, 2007). It was trained in an MDP
system using reinforcement learning with simulated
users (Lemon and Liu, 2007), and was not modified
for use in our MM Q-MDP policy.
We tested this system with 10 different users, each
attempting 9 tasks in the TownInfo domain (search-
ing for hotels and restaurants in a fictitious town),
resulting in 90 test dialogues. The users each at-
tempted 3 tasks with the MDP system of (Lemon
and Liu, 2007), 3 tasks with a state-of-the-art hand-
coded system (see (Lemon et al, 2006)), and 3 tasks
with the MM Q-MDP system. Ordering of sys-
tems and tasks was controlled, and 3 of the users
were not native speakers of English. We collected
the Task Completion (TC), and dialogue length for
each system, as reported in table 1. Task Comple-
tion is counted from the system logs when the user
replies that they are happy with their chosen option.
Such a small sample size means that these results are
not statistically significant, but there is a clear trend
showing the superiority of the the MM Q-MDP sys-
tem, both in terms of more tasks being completed
and less variability in overall dialogue length.
2Thanks to K. Georgilla for training this model.
6 Conclusions
Mixture Model POMDPs combine the efficiency of
MDP spoken dialogue systems with the ability of
POMDP models to make use of multiple ASR hy-
potheses. They can also be constructed from MDP
models without additional training, using the Q-
MDP approximation for the dialogue management
policy. Initial results suggest that, despite its sim-
plicity, this approach does lead to better spoken dia-
logue systems than MDP and hand-coded models.
Acknowledgments
This research received funding from UK EPSRC
grant EP/E019501/1 and the European Community?s
FP7 under grant no 216594 (CLASSIC project:
www.classic-project.org).
References
J Bos, E Klein, O Lemon, and T Oka. 2003. DIPPER:
Description and Formalisation of an Information-State
Update Dialogue System Architecture. In Proc. SIG-
dial Workshop on Discourse and Dialogue, Sapporo.
K Georgila, J Henderson, and O Lemon. 2005. Learning
User Simulations for Information State Update Dia-
logue Systems. In Proc. Eurospeech.
H Higashinaka, M Nakano, and K Aikawa. 2003.
Corpus-based discourse understanding in spoken dia-
logue systems. In Proc. ACL, Sapporo.
O Lemon and X Liu. 2007. Dialogue policy learning
for combinations of noise and user simulation: transfer
results. In Proc. SIGdial.
O Lemon, K Georgila, and J Henderson. 2006. Evalu-
ating Effectiveness and Portability of Reinforcement
Learned Dialogue Strategies with real users: the
TALK TownInfo Evaluation. In Proc. ACL/IEEE SLT.
ML Littman, AR Cassandra, and LP Kaelbling. 1995.
Learning policies for partially observable environ-
ments: Scaling up. In Proc. ICML, pages 362?370.
I Meza-Ruiz, S Riedel, and O Lemon. 2008. Accurate
statistical spoken language understanding from limited
development resources. In Proc. ICASSP. (to appear).
JD Williams and SJ Young. 2007. Partially Observ-
able Markov Decision Processes for Spoken Dialog Systems.
Computer Speech and Language, 21(2):231?422.
S Young, G Evermann, D Kershaw, G Moore, J Odell,
D Ollason, D Povey, V Valtchev, and P Woodland.
2002. The HTK Book. Cambridge Univ. Eng. Dept.
SJ Young, J Schatzmann, K Weilhammer, and H Ye.
2007. The Hidden Information State Approach to Di-
alog Management. In Proc. ICASSP, Honolulu.
76
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 37?42,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Latent Variable Model of Synchronous Syntactic-Semantic Parsing for
Multiple Languages
Andrea Gesmundo
Univ Geneva
Dept Computer Sci
Andrea.Gesmundo@
unige.ch
James Henderson
Univ Geneva
Dept Computer Sci
James.Henderson@
unige.ch
Paola Merlo
Univ Geneva
Dept Linguistics
Paola.Merlo@
unige.ch
Ivan Titov?
Univ Illinois at U-C
Dept Computer Sci
titov@uiuc.edu
Abstract
Motivated by the large number of languages
(seven) and the short development time (two
months) of the 2009 CoNLL shared task, we
exploited latent variables to avoid the costly
process of hand-crafted feature engineering,
allowing the latent variables to induce features
from the data. We took a pre-existing gener-
ative latent variable model of joint syntactic-
semantic dependency parsing, developed for
English, and applied it to six new languages
with minimal adjustments. The parser?s ro-
bustness across languages indicates that this
parser has a very general feature set. The
parser?s high performance indicates that its la-
tent variables succeeded in inducing effective
features. This system was ranked third overall
with a macro averaged F1 score of 82.14%,
only 0.5% worse than the best system.
1 Introduction
Recent research in syntax-based statistical machine
translation and the recent availability of syntac-
tically annotated corpora for multiple languages
(Nivre et al, 2007) has provided a new opportunity
for evaluating the cross-linguistic validity of statis-
tical models of syntactic structure. This opportu-
nity has been significantly expanded with the 2009
CoNLL shared task on syntactic and semantic pars-
ing of seven languages (Hajic? et al, 2009) belonging
to several different language families.
We participate in this task with a generative,
history-based model proposed in the CoNLL 2008
0Authors in alphabetical order.
shared task for English (Henderson et al, 2008) and
further improved to tackle non-planar dependencies
(Titov et al, 2009). This model maximises the joint
probability of the syntactic and semantic dependen-
cies and thereby enforces that the output structure be
globally coherent, but the use of synchronous pars-
ing allows it to maintain separate structures for the
syntax and semantics. The probabilistic model is
based on Incremental Sigmoid Belief Networks (IS-
BNs), a recently proposed latent variable model for
syntactic structure prediction, which has shown very
good performance for both constituency (Titov and
Henderson, 2007a) and dependency parsing (Titov
and Henderson, 2007b). The use of latent variables
enables this architecture to be extended to learning
a synchronous parse of syntax and semantics with-
out overly restrictive assumptions about the linking
between syntactic and semantic structures.
In this work, we evaluate the ability of this
method to generalise across several languages. We
take the model as it was developed for English, and
apply it directly to all seven languages. The only
fine-tuning was to evaluate whether to include one
feature type which we had previously found did not
help for English, but helped overall. No other fea-
ture engineering was done. The use of latent vari-
ables to induce features automatically from the data
gives our method the adaptability necessary to per-
form well across all seven languages, and demon-
strates the lack of language specificity in the models
of Henderson et al (2008) and Titov et al (2009).
The main properties of this model, that differen-
tiate it from other approaches, is the use of syn-
chronous syntactic and semantic derivations and the
37
use of online planarisation of crossing semantic de-
pendencies. This system was ranked third overall
with a macro averaged F1 score of 82.14%, only
0.5% worse than the best system.
2 The Synchronous Model
The use of synchronous parsing allows separate
structures for syntax and semantics, while still mod-
eling their joint probability. We use the approach
to synchronous parsing proposed in Henderson et al
(2008), where we start with two separate derivations
specifying each of the two structures, then synchro-
nise these derivations at each word. The individual
derivations are based on Nivre?s shift-reduce-style
parsing algorithm (Nivre et al, 2006), as discussed
further below. First we illustrate the high-level struc-
ture of the model, discussed in more detail in Hen-
derson et al (2008).
Let Td be a syntactic dependency tree with
derivation D1d, ..., Dmdd , and Ts be a semantic
dependency graph with derivation D1s , ..., Dmss .
To define derivations for the joint structure
Td, Ts, we divide the two derivations into the
chunks between shifting each word onto the
stack, ctd = Db
t
d
d , ..., D
etd
d and cts = Db
t
ss , ..., De
t
ss ,
where Dbtd?1d = Db
t
s?1s = Shiftt?1 and
De
t
d+1
d = De
t
s+1s = Shiftt. Then the actions of
the synchronous derivations consist of quadruples
Ct = (ctd, Switch, cts, Shiftt), where Switch means
switching from syntactic to semantic mode. This
gives us the following joint probability model,
where n is the number of words in the input.
P (Td, Ts) = ?nt=1 P (Ct|C1, . . . , Ct?1) (1)
These synchronous derivations C1, . . . , Cn only re-
quire a single input queue, since the Shift actions are
synchronised, but they require two separate stacks,
one for the syntactic derivation and one for the se-
mantic derivation.
The probability of each synchronous derivation
chunk Ct is the product of four factors, related to
the syntactic level, the semantic level and the two
synchronising steps. The probability of ctd is de-
composed into one probability for each derivation
action Di, conditioned on its history using the chain
rule, and likewise for cts. These probabilities are es-
timated using the method described in section 3.
Syn cross Sem cross Sem tree No parse
Cat 0% 0% 61.4% 0%
Chi 0% 28.0% 28.6% 9.5%
Cze 22.4% 16.3% 6.1% 1.8%
Eng 7.6% 43.9% 21.4% 3.9%
Ger 28.1% 1.3% 97.4% 0.0%
Jap 0.9% 38.3% 11.2% 14.4%
Spa 0% 0% 57.1% 0%
Table 1: For each language, percentage of training sen-
tences with crossing arcs in syntax and semantics, with
semantic arcs forming a tree, and which were not parsable
using the Swap action.
One of the main characteristics of our syn-
chronous representation, unlike other synchronous
representations of syntax and semantics (Nesson et
al., 2008), is that the synchronisation is done on
words, rather than on structural components. We
take advantage of this freedom and adopt different
methods for handling crossing arcs for syntax and
for semantics.
While both syntax and semantics are represented
as dependency graphs, these graphs differ substan-
tially in their properties. Some statistics which in-
dicate these differences are shown in table 1. For
example, English syntactic dependencies form trees,
while semantic dependency structures are only trees
21.4% of the time, since in general each struc-
ture does not form a connected graph and some
nodes may have more than one parent. The syn-
tactic dependency structures for only 7.6% of En-
glish sentences contain crossing arcs, while 43.9%
of the semantic dependency structures contain cross-
ing arcs. Due to variations both in language char-
acteristics and annotation decisions across corpora,
these differences between syntax and semantics vary
across the seven languages, but they are consis-
tent enough to motivate the development of new
techniques specifically for handling semantic depen-
dency structures. In particular, we use a different
method for parsing crossing arcs.
For parsing crossing semantic arcs (i.e. non-
planar graphs), we use the approach proposed in
Titov et al (2009), which introduces an action Swap
that swaps the top two elements on the parser?s
stack. The Swap action allows the parser to reorder
words online during the parse. This allows words
to be processed in different orders during different
38
portions of the parse, so some arcs can be specified
using one ordering, then other arcs can be specified
using another ordering. Titov et al (2009) found that
only using the Swap action as a last resort is the best
strategy for English (compared to using it preemp-
tively to address future crossing arcs) and we use
the same strategy here for all languages.
Syntactic graphs do not use a Swap action.
We adopt the HEAD method of Nivre and Nils-
son (2005) to de-projectivise syntactic dependencies
outside of parsing.1
3 Features and New Developments
The synchronous derivations described above are
modelled with a type of Bayesian Network called an
Incremental Sigmoid Belief Network (ISBN) (Titov
and Henderson, 2007a). As in Henderson et al
(2008), the ISBN model distinguishes two types of
latent states: syntactic states, when syntactic deci-
sions are considered, and semantic states, when se-
mantic decision are considered. Latent states are
vectors of binary latent variables, which are condi-
tioned on variables from previous states via a pattern
of connecting edges determined by the previous de-
cisions. These latent-to-latent connections are used
to engineer soft biases which reflect the relevant do-
mains of locality in the structure being built. For
these we used the set of connections proposed in
Titov et al (2009), which includes latent-to-latent
connections both from syntax states to semantics
states and vice versa. The latent variable vectors are
also conditioned on a set of observable features of
the derivation history. For these features, we start
with the feature set from Titov et al (2009), which
extends the semantic features proposed in Hender-
son et al (2008) to allow better handling of the non-
planar structures in semantics. Most importantly, all
the features previously included for the top of the
stack were also included for the word just under the
top of the stack. To this set we added one more type
of feature, discussed below.
We made some modifications to reflect differ-
ences in the task definition between the 2008 and
2009 shared tasks, and experimented with one
type of features which had been previously imple-
1The statistics in Table 1 suggest that, for some languages,
swapping might be beneficial for syntax as well.
mented. For the former modifications, the system
was adapted to allow the use of the PFEAT and
FILLPRED fields in the data, which both resulted
in improved accuracy for all the languages. The
PFEAT data field (automatically predicted morpho-
logical features) was introduced in the system in
two ways, as an atomic feature bundle that is pre-
dicted when predicting the word, and split into its
elementary components when conditioning on a pre-
vious word, as was done in Titov and Henderson
(2007b). Because the testing data included a spec-
ification of which words were annotated as predi-
cates (the FILLPRED data field), we constrained the
parser?s output so as to be consistent with this speci-
fication. For rare predicates, if the predicate was not
in the parser?s lexicon (extracted from the training
set), then a sense was taken from the list of senses
reported in the Lexicon and Frame Set resources
available for the closed challenge. If this informa-
tion was not available, then a default sense was con-
structed based on the automatically predicted lemma
(PLEMMA) of the predicate.
We also made use of a previously implemented
type of feature that allows the prediction of a seman-
tic link between two words to be conditioned on the
syntactic dependency already predicted between the
same two words. While this feature had previously
not helped for English, it did result in an overall im-
provement across the languages.
Also, in comparison with previous experiments,
the search beam used in the decoding phase was in-
creased from 50 to 80, producing a small improve-
ment in the overall development score.
All development effort took about two person-
months, mostly by someone who had no previous
experience with the system. Most of this time was
spent on the above differences in the task definition
between the 2008 and 2009 shared tasks.
4 Results and Discussion
We participated in the joint task of the closed chal-
lenge, as described in Hajic? et al (2009). The
datasets used in this challenge are described in Taule?
et al (2008) (Catalan and Spanish), Palmer and Xue
(2009) (Chinese), Hajic? et al (2006) (Czech), Sur-
deanu et al (2008) (English), Burchardt et al (2006)
(German), and Kawahara et al (2002) (Japanese).
39
Rank Average Catalan Chinese Czech English German Japanese Spanish
macro F1 3 82.14 82.66 76.15 83.21 86.03 79.59 84.91 82.43
syntactic acc 1 @85.77 @87.86 76.11 @80.38 88.79 87.29 92.34 @87.64
semantic F1 3 78.42 77.44 76.05 86.02 83.24 71.78 77.23 77.19
Table 2: The three main scores for our system. Rank is within task.
Rank Ave Cze-ood Eng-ood Ger-ood
macro F1 3 75.93 @80.70 75.76 71.32
syn Acc 2 78.01 @76.41 80.84 76.77
sem F1 3 73.63 84.99 70.65 65.25
Table 3: Results on out-of-domain for our system. Rank
is within task.
The official results on the testing set are shown in
tables 2, 3, and 4. The symbol ?@? indicates the
best result across systems. In table 5, we show our
rankings across the different datasets, amongst sys-
tems submitted for the same task.
The overall score used to rank systems is the un-
weighted average of the syntactic labeled accuracy
and the semantic labeled F1 measure, across all lan-
guages (?macro F1? in table 2). We were ranked
third, out of 14 systems. There was only a 0.5% dif-
ference between our score and that of the best sys-
tem, while there was a 1.29% difference between our
score and the fourth ranked system. Only consid-
ering syntactic accuracy, we had the highest aver-
age score of all systems, with the highest individual
score for Catalan, Czech, and Spanish. Only con-
sidering semantic F1, we were again ranked third.
Our results for out-of-domain data (table 3) achieved
a similar level of success, although here we were
ranked second for average syntactic accuracy. Our
precision on semantic arcs was generally much bet-
ter than our recall (shown in table 4). However,
other systems had a similar imbalance, resulting in
no change in our third place ranking for semantic
precision and for semantic recall. Only when the se-
mantic precision is averaged with syntactic accuracy
do we squeeze into second place (?macro Prec?).
To get a more detailed picture of the strengths
and weaknesses of our system, we computed its rank
within each dataset, shown in table 5. Overall, our
system is robust across languages, with little fluc-
tuation in ranking for the overall score, including
for out-of-domain data. The one noticeable excep-
tion to this consistency is the syntactic score for En-
data time (min) macro F1
Czech 25% 5007 73.84
50% 3699 77.57
75% 4201 79.10
100% 6870 80.55
English 25% 1300 79.02
50% 1899 81.61
75% 3196 82.41
100% 3191 83.27
Table 6: Training times and development set accuracies
using different percentages of the training data, for Czech
and English.
glish out-of-domain data. The other ranks for En-
glish out-of-domain and English in-domain scores
are also on the poor side. These results support our
claim that our parser has not undergone much hand-
tuning, since it was originally developed for English.
It is not currently clear whether this relative differ-
ence reflects a English-specific weakness in our sys-
tem, or that many of the other systems have been
fine-tuned for English.
On the higher end of our dataset rankings, we
do relatively well on Catalan, Czech, and Span-
ish. Catalan and Spanish are unique amongst these
datasets in that they have no crossing arcs in their
semantic structure. Czech seems to have semantic
structures which are relatively well handled by our
derivations with Swap. As indicated above in ta-
ble 1, only 2% of sentences are unparsable, despite
16% requiring the Swap action. However, this argu-
ment does not explain why our parser did relatively
poorly on German semantic dependencies. Regard-
less, these observations would suggest that our sys-
tem is still having trouble with crossing dependen-
cies, despite the introduction of the Swap operation,
and that our learning method could achieve better
performance with an improved treatment of cross-
ing semantic dependencies.
Table 6 shows how accuracies and training times
vary with the size of the training dataset, for Czech
and English. Training times vary in part because
40
Rank Ave Cat Chi Cze Eng Ger Jap Spa Cze-ood Eng-ood Ger-ood
semantic Prec 3 81.60 79.08 80.93 87.45 84.92 75.60 83.75 79.44 85.90 72.89 75.19
semantic Rec 3 75.56 75.87 71.73 @84.64 81.63 68.33 71.65 75.05 @84.09 68.55 57.63
macro Prec 2 83.68 83.47 78.52 83.91 86.86 81.44 88.05 83.54 81.16 76.86 @75.98
macro Rec 3 80.66 @81.86 73.92 @82.51 85.21 77.81 81.99 81.35 @80.25 74.70 67.20
Table 4: Semantic precision and recall and macro precision and recall for our system. Rank is within task.
Rank by Ave Cat Chi Cze Eng Ger Jap Spa Ave-ood Cze-ood Eng-ood Ger-ood
macro F1 3 2 3 2 4 4 3 2 3 1 4 3
syntactic Acc 1 1 4 1 3 2 2 1 2 1 7 2
semantic F1 3 2 4 2 4 5 4 2 3 2 4 3
Table 5: Our system?s rank within task according to the three main measures, for each dataset.
?1.4
?1.2
?1
?0.8
?0.6
?0.4
?0.2
 0
 0  10  20  30  40  50
M
ac
ro
 F
1 
D
iff
er
en
ce
Words per Second
Jap
Spa
Cat
Ger
Eng
Cze
Chi
Figure 1: Difference in development set macro F1 as the
search beam is decreased from the submitted beam (80)
to 40, 20, 10, and 5, plotted against parser speed.
random variations can result in different numbers of
training cycles before convergence. Accuracies ap-
pear to be roughly log-linear with data size.
Figure 1 shows how the accuracy of the parser de-
grades as we speed it up by decreasing the search
beam used in decoding, for each language. For some
languages, a slightly smaller search beam is actually
more accurate,2 but for smaller beams the trade-off
of accuracy versus words-per-second is roughly lin-
ear. Parsing time per word is also linear in beam
width, with a zero intercept.
5 Conclusion
In the joint task of the closed challenge of the
CoNLL 2009 shared task (Hajic? et al, 2009), we in-
vestigated how well a model of syntactic-semantic
dependency parsing developed for English would
2This fact suggests that we could have gotten improved re-
sults by tailoring the search beam to individual languages.
generalise to the other six languages. This model
provides a single generative probability of the joint
syntactic and semantic dependency structures, but
allows separate representations for these two struc-
tures by parsing the two structures synchronously.
Finding the statistical correlations both between and
within these structures is facilitated through the use
of latent variables, which induce features automat-
ically from the data, thereby greatly reducing the
need for hand-coded feature engineering.
This latent variable model proved very robust
across languages, achieving a ranking of between
second and fourth on each language, including for
out-of-domain data. The extent to which the parser
does not rely on hand-crafting is underlined by the
fact that its worst ranking is for English, the lan-
guage for which it was developed (particularly for
out-of-domain data). The parser was ranked third
overall out of 14 systems, with a macro averaged F1
score of 82.14%, only 0.5% worse than the best sys-
tem.
Both joint learning and conditioning decisions
about semantic dependencies on latent representa-
tions of syntactic parsing states were crucial to the
success of our model, as was previously demon-
strated in Henderson et al (2008). There, remov-
ing this conditioning led to a 3.5% drop in the SRL
score. This result seems to contradict the gen-
eral trend in the CoNLL-2008 shared task, where
joint learning had only limited success. The lat-
ter fact may be explained by recent theoretical re-
sults demonstrating that pipelines can be preferable
to joint learning (Roth et al, 2009) when no shared
hidden representation is learnt. Our system (Hender-
son et al, 2008) was the only one which attempted to
41
learn a common hidden representation for this mul-
titask learning problem and also was the only one
which achieved significant gain from joint parameter
estimation. We believe that learning shared hidden
representations for related NLP problems is a very
promising direction for further research.
Acknowledgements
We thank Gabriele Musillo and Dan Roth for help and
advice. This work was partly funded by Swiss NSF
grants 100015-122643 and PBGE22-119276, European
Community FP7 grant 216594 (CLASSiC, www.classic-
project.org), US NSF grant SoD-HCER-0613885 and
DARPA (Bootstrap Learning Program).
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
James Henderson, Paola Merlo, Gabriele Musillo, and
Ivan Titov. 2008. A latent variable model of syn-
chronous parsing for syntactic and semantic dependen-
cies. In Proceedings of CONLL 2008, pages 178?182,
Manchester, UK.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Rebecca Nesson, Giorgio Satta, and Stuart M. Shieber.
2008. Optimal k-arization of synchronous tree-
adjoining grammar. In Proceedings of ACL-08: HLT,
pages 604?612, Columbus, Ohio, June.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proc. 43rd Meeting of Asso-
ciation for Computational Linguistics, pages 99?106,
Ann Arbor, MI.
Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit,
and Svetoslav Marinov. 2006. Pseudo-projective de-
pendency parsing with support vector machines. In
Proc. of the Tenth Conference on Computational Nat-
ural Language Learning, pages 221?225, New York,
USA.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, pages 915?932,
Prague, Czech Republic, June.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Dan Roth, Kevin Small, and Ivan Titov. 2009. Sequential
learning of classifiers for structured prediction prob-
lems. In AISTATS, Clearwater, Florida, USA.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008).
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Ivan Titov and James Henderson. 2007a. Constituent
parsing with Incremental Sigmoid Belief Networks. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 632?639,
Prague, Czech Republic.
Ivan Titov and James Henderson. 2007b. Fast and ro-
bust multilingual dependency parsing with a genera-
tive latent variable model. In Proc. Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL 2007), Prague, Czech Republic. (CoNLL
Shared Task).
Ivan Titov, James Henderson, Paola Merlo, and Gabriele
Musillo. 2009. Online graph planarisation for syn-
chronous parsing of semantic and syntactic dependen-
cies. In Proc. Twenty-First International Joint Confer-
ence on Artificial Intelligence (IJCAI-09), Pasadena,
California.
42
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 899?908,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Heuristic Search for Non-Bottom-Up Tree Structure Prediction
Andrea Gesmundo
Department of Computer Science
University of Geneva
andrea.gesmundo@unige.ch
James Henderson
Department of Computer Science
University of Geneva
james.henderson@unige.ch
Abstract
State of the art Tree Structures Prediction
techniques rely on bottom-up decoding. These
approaches allow the use of context-free fea-
tures and bottom-up features. We discuss
the limitations of mainstream techniques in
solving common Natural Language Process-
ing tasks. Then we devise a new framework
that goes beyond Bottom-up Decoding, and
that allows a better integration of contextual
features. Furthermore we design a system that
addresses these issues and we test it on Hierar-
chical Machine Translation, a well known tree
structure prediction problem. The structure
of the proposed system allows the incorpora-
tion of non-bottom-up features and relies on
a more sophisticated decoding approach. We
show that the proposed approach can find bet-
ter translations using a smaller portion of the
search space.
1 Introduction
Tree Structure Prediction (TSP) techniques have
become relevant in many Natural Language Pro-
cessing (NLP) applications, such as Syntactic Pars-
ing, Semantic Role Labeling and Hierarchical Ma-
chine Translation (HMT) (Chiang, 2007). HMT
approaches have a higher complexity than Phrase-
Based Machine Translation techniques, but exploit
a more sophisticated reordering model, and can
produce translations with higher Syntactic-Semantic
quality.
TSP requires as inputs: a weighted grammar, G,
and a sequence of symbols or a set of sequences en-
coded as a Lattice (Chappelier et al, 1999). The
input sequence is often a sentence for NLP applica-
tions. Tree structures generating the input sequence
can be composed using rules, r, from the weighted
grammar, G. TSP techniques return as output a tree
structure or a set of trees (forest) that generate the
input string or lattice. The output forest can be rep-
resented compactly as a weighted hypergraph (Klein
and Manning, 2001). TSP tasks require finding the
tree, t, with the highest score, or the best-k such
trees. Mainstream TSP relies on Bottom-up Decod-
ing (BD) techniques.
With this paper we propose a new framework
as a generalization of the CKY-like Bottom-up ap-
proach. We also design and test an instantiation of
this framework, empirically showing that wider con-
textual information leads to higher accuracy for TSP
tasks that rely on non-local features, like HMT.
2 Beyond Bottom-up Decoding
TSP decoding requires scoring candidate trees,
cost(t). Some TSP tasks require only local features.
For these cases cost(t) depends only on the local
score of the rules that compose t :
cost(t) =
?
ri?t
cost(ri ) (1)
This is the case for Context Free Grammars. More
complex tasks need non-local features. Those fea-
tures can be represented by a non-local factor,
nonLocal(t), into the overall t score:
cost(t) =
?
ri?t
cost(ri ) + nonLocal(t) (2)
899
For example, in HMT the Language Model (LM) is
a non-local fundamental feature that approximates
the adequacy of the translation with the sum of log-
probabilities of composing n-grams.
CKY-like BD approaches build candidate trees in
a bottom-up fashion, allowing the use of Dynamic
Programming techniques to simplify the search
space by mering sub-trees with the same state, and
also easing application of pruning techniques (such
as Cube Pruning, e.g. Chiang (2007), Gesmundo
(2010)). For clarity of presentation and follow-
ing HMT practice, we will henceforth restrict our
focus to binary grammars. Standard CKY works
by building objects known as items (Hopkins and
Langmead, 2009). Each item, ?, corresponds to a
candidate sub-tree. Items are built linking a rule
instantiation, r, to two sub-items that represents
left context, ?1, and right context, ?2; formally:
? ? ? ?1 ? r ? ?2 ?. An item is a triple that
contains a span, a postcondition and a carry. The
span contains the indexes of the starting and end-
ing input words delimiting the continuous sequence
covered by the sub-tree represented by the item. The
postcondition is a string that represents r?s head non-
terminal label, telling us which rules may be applied.
The carry, ?, stores extra information required to
correctly score the non-local interactions of the item
when it will be linked in a broader context (for HMT
with LM the carry consists of boundary words that
will form new n-grams).
Items, ? ? ??1 ? r ? ?2?, are scored according to
the following formula:
cost(?) = cost(r) + cost(?1) + cost(?2) (3)
+ interaction(r, ?1, ?2)
Where: cost(r) is the cost associated to the weighted
rule r; cost(?1) and cost(?2) are the costs of the two
sub-items computed recursively using formula (3);
interaction(r, ?1, ?2) is the interaction cost between
the rule instantiation and the two sub-items. In HMT
the interaction cost includes the LM score of new n-
grams generated by connecting the childrens? sub-
spans with terminals of r. Notice that formula (3) is
equal to formula (2) for items that cover the whole
input sequence.
In many TSP applications, the search space is
too large to allow an exhaustive search and there-
fore pruning techniques must be used. Pruning deci-
sions are based on the score of partial derivations.
It is not always possible to compute exactly non-
local features while computing the score of partial
derivations, since partial derivations miss part of the
context. Formula (3) accounts for the interaction be-
tween r and sub-items ?1 and ?2, but it does not in-
tegrate the cost relative to the interaction between
the item and the surrounding context. Therefore the
item score computed in a bottom-up fashion is an
approximation of the score the item has in a broader
context. For example, in HMT the LM score for n-
grams that partially overlap the item?s span cannot
be computed exactly since the surrounding words
are not known.
Basing pruning decisions on approximated scores
can introduce search errors. It is possible to reduce
search errors using heuristics based on future cost
estimation. In general the estimation of the interac-
tion between ? and the surrounding context is func-
tion of the carry, ?. In HMT it is possible to estimate
the cost of n-grams that partially overlap ??s span
considering the boundary words. We can obtain the
heuristic cost for an item, ?, adding to formula (3)
the factor, est(?), for the estimation of interaction
with missing context:
heuristicCost(?) = cost(?) + est(?) (4)
And use heuristicCost(?) to guide BD pruning de-
cisions. Anyway, even if a good interaction estima-
tion is available, in practice it is not possible to avoid
search errors while pruning.
More sophisticated parsing models allow the use
of non-bottom-up features within a BD framework.
Caraballo and Charniak (1998) present best-first
parsing with Figures of Merit that allows condition-
ing of the heuristic function on statistics of the input
string. Corazza et al (1994), and Klein and Man-
ning (2003) propose an A* parsing algorithm that
estimates the upper bound of the parse completion
scores using contextual summary features. These
models achieve time efficiency and state-of-the-art
accuracy for PCFG parsing, but still use a BD frame-
work that doesn?t allow the application of a broader
class of non-bottom-up contextual features.
In HMT, knowing the sentence-wide context in
which a sub-phrase is translated is extremely impor-
tant. It is obviously important for word choice: as
900
a simple example consider the translation of the fre-
quent English word ?get? into Chinese. The choice
of the correct set of ideograms to translate ?get? of-
ten requires being aware of the presence of particles
that can be at any distance within the sentence. In a
common English to Chinese dictionary we found 93
different sets of ideograms that could be translations
of ?get?. Sentence-wide context is also important
in the choice of word re-ordering: as an example
consider the following translations from English to
German:
1. EN : I go home.
DE : Ich gehe nach Hause.
2. EN : I say, that I go home.
DE : Ich sage, dass ich nach Hause gehe.
3. EN : On Sunday I go home.
DE : Am Sonntag gehe ich nach Hause.
The English phrase ?I go home? is translated in Ger-
man using the same set of words but with different
orderings. It is not possible to choose the correct
ordering of the phrase without being aware of the
context. Thus a bottom-up decoder without context
needs to build all translations for ?I go home?, intro-
ducing the possibility of pruning errors.
Having shown the importance of contextual fea-
tures, we define a framework that overcomes the
limitations of bottom-up feature approximation.
3 Undirected-CKY Framework
Our aim is to propose a new Framework that over-
comes BD limitations allowing a better integration
of contextual features. The presented framework can
be regarded as a generalization of CKY.
To introduce the new framework let us focus on a
detail of CKY BD. The items are created and scored
in topological order. The ordering constraint can be
formally stated as: an item covering the span [i, j]
must be processed after items covering sub spans
[h, k]|h ? i, k ? j. This ordering constraint im-
plies that full yield information is available when
an item is processed, but information about ances-
tors and siblings is missing. Therefore non-bottom-
up context cannot be used because of the ordering
constraint. Now let us investigate how the decoding
algorithm could change if we remove the ordering
constraint.
Removing the ordering constraint would lead to
the occurrence of cases in which an item is pro-
cessed before all child items have been processed.
For example, we could imagine to create and score
an item, ?, with postcondition X and span [i, j], link-
ing the rule instantiation r : X?AB with only
the left sub-item, ?A, while information for the right
sub-item, ?B is still missing. In this case, we can
rely on local and partial contextual features to score
?. Afterwards, it is possible to process ?B using the
parent item, ?, as a source of additional informa-
tion about the parent context and sibling ?A yield.
This approach can avoid search errors in cases where
pruning at the parent level can be correctly done us-
ing only local and partial yield context, while prun-
ing at the child level needs extra non-bottom-up con-
text to make a better pruning decision. For exam-
ple, consider the translation of the English sentence
?I run? into French using the following synchronous
grammar:
r1 : S ? X 1 X 2 | X 1 X 2
r2 : X ? I | Je
r3 : X ? run | course
r4 : X ? run | courir
r5 : X ? run | cours
r6 : X ? run | courons
.
.
.
Where: r1 is a Glue rule and boxed indexes de-
scribe the alignment; r2 translates ?I? in the cor-
responding French pronoun; r3 translates ?run? as
a noun; remaining rules translate ?run? as one of
the possible conjugations of the verb ?courir?. Us-
ing only bottom-up features it is not possible to re-
solve the ambiguity of the word ?run?. If the beam
is not big enough the correct translation could be
pruned. Anyway a CKY decoder would give the
highest score to the most frequent translation. In-
stead, if we follow a non bottom-up approach, as
described in Figure 1, we can: 1) first translate ?I?;
2) Then create an item using r1 with missing right
child; 3) finally choose the correct translation for
?run? using r1 to access a wider context. Notice
that with this undirected approach it is possible to
reach the correct translation using only beam size of
901
Figure 1: Example of undirected decoding for HMT. The arrows point to the direction in which information is propa-
gated. Notice that the parent link at step 3 is fundamental to correctly disambiguate the translation for ?run?.
1 and the LM feature.
To formalize Undirected-CKY, we define a gen-
eralized item called undirected-item. Undirected-
items, ??, are built linking rule instantiations with
elements in L ? {left child, right child, parent};
for example: ?? ? ???1 ? r ?? ??p?, is built linking
r with left child, ??1, and parent, ??p. We denote
with L+?? the set of links for which the undirected-
item, ??, has a connection, and with L??? the set
of missing links. An undirected-item is a triple
that contains a span, a carry and an undirected-
postcondition. The undirected-postcondition is a
set of strings, one string for each of ???s missing links,
l ? L??? . Each string represents the non-terminal re-
lated to one of the missing links available for expan-
sion. Bottom-up items can be considered specific
cases of undirected-items having L+ = { left child,
right child} and L? = {parent}. We can formally
describe the steps of the example depicted in Figure
1 with:
1) r2 : X ? I|Je , terminal : [0, 1]??1 : [0, 1, {X p }, ?1]
2)
r1 : S ? X 1X 2 |X 1X 2 , ??1 : [0, 1, {X p }, ?1]
??2 : [0, 1, {X 2 }, ?2)]
3)r5 :X ? run|cours,? ?2 : [? ? ? ] , terminal : [1, 2]??3 : [0, 2, {}, ?3 ]
The scoring function for undirected-items can be ob-
tained generalizing formula (3):
cost(??) = cost(r)
+
?
l?L+
cost(??l ) (5)
+ interaction(r ,L+)
In CKY, each span is processed separately in
topological order, and the best-k items for each span
are selected in sequence according to scoring func-
tion (4). In the proposed framework, the selec-
tion of undirected-items can be done in any order,
for example: in a first step selecting an undirected-
item for span s1, then selecting an undirected-item
for span s2, and in a third step selecting a second
undirected-item for s1, and so on. As in agenda
based parsing (Klein and Manning, 2001), all candi-
date undirected-items can be handled with an unique
queue. Allowing the system to decide decoding or-
der based on the candidates? scores, so that candi-
dates with higher confidence can be selected earlier
and used as context for candidates with lower confi-
dence.
Having all candidates in the same queue intro-
duces comparability issues. In CKY, candidates are
comparable since each span is processed separately
and each candidate is scored with the estimation of
the yield score. Instead, in the proposed framework,
902
the unique queue contains candidates relative to dif-
ferent nodes and with different context scope. To
ensure comparability, we can associate to candidate
undirected-items a heuristic score of the full deriva-
tion:
heuristicCost(??) = cost(??) + est(??) (6)
Where est(??) estimates the cost of the missing
branches of the derivation as a function of ???s par-
tial structure and carry.
In this framework, the queue can be initialized
with a candidate for each rule instantiation. These
initializing candidates have no context information
and can be scored using only local features. A
generic decoding algorithm can loop selecting the
candidate undirected-item with the highest score, ??,
and then propagating its information to neighboring
candidates, which can update using ?? as context. In
this general framework the link to the parent node is
not treated differently from links to children. While
in CKY the information is always passed from chil-
dren to parent, in Undirected-CKY the information
can be propagated in any direction, and any decod-
ing order is allowed.
We can summarize the steps done to generalize
CKY into the proposed framework: 1) remove the
node ordering constraint; 2) define the scoring of
candidates with missing children or parent; 3) use
a single candidate queue; 4) handle comparability of
candidates from different nodes and/or with differ-
ent context scope; 5) allow information propagation
in any direction.
4 Undirected Decoding
In this section we propose Undirected Decod-
ing (UD), an instantiation of the Undirected-CKY
framework presented above. The generic framework
introduces many new degrees of freedom that could
lead to a higher complexity of the decoder. In our
actual instantiation we apply constraints on the ini-
tialization step, on the propagation policy, and fix a
search beam of k. These constraints allow the sys-
tem to converge to a solution in practical time, al-
low the use of dynamic programming techniques to
merge items with equivalent states, and gives us the
possibility of using non-bottom-up features and test-
ing their relevance.
Algorithm 1 Undirected Decoding
1: function decoder (k) : out-forest
2: Q? LeafRules();
3: while |Q| > 0 do
4: ??? PopBest (Q);
5: if CanPop(??) then
6: out-forest.Add(??);
7: if ??.HasChildrenLinks() then
8: for all r ? HeadRules(??) do
9: C?? NewUndirectedItems(r ,? ?);
10: for all c? ? C? do
11: if CanPop(c?) then
12: Q.Insert(c?);
13: end if
14: end for
15: end for
16: end if
17: end if
18: end while
Algorithm 1 summarizes the UD approach. The
beam size, k, is given as input. At line 2 the queue
of undirected-item candidates, Q, is initialized with
only leafs rules. At line 3 the loop starts, it will
terminate when Q is empty. At line 4 the candi-
date with highest score, ??, is popped from Q. line 5
checks if ?? is within the beam width: if ?? has a span
for which k candidates were already popped, then ??
is dropped and a new iteration is begun. Otherwise
?? is added to the out-forest at line 6. From line 7
to line 10 the algorithm deals with the generation of
new candidate undirected-items. line 7 checks if ??
has both children links, if not a new decoding iter-
ation is begun. line 8 loops over the rule instantia-
tions, r, that can use ?? as child. At line 9, the set of
new candidates, C?, is built linking r with ?? and any
context already available in the out-forest. Finally,
between line 10 and line 12, each element c? in C?
is inserted in Q after checking that c? is within the
beam width: if c? has a span for which k candidates
were already popped it doesn?t make sense to insert
it in Q since it will be surely discarded at line 5.
In more detail, the function
NewUndirectedItems(r,? ?) at line 9 creates new
undirected-items linking r using: 1) ?? as child; 2)
(optionally) as other child any other undirected-item
that has already been inserted in the out-forest and
903
doesn?t have a missing child and matches missing
span coverage; 3) and using as parent context the
best undirected-item with missing child link that
has been incorporated in the out-forest and can
expand the missing child link using r. In our current
method, only the best possible parent context is
used because it only provides context for ranking
candidates, as discussed at the end of this section.
In contrast, a different candidate is generated for
each possible other child in 2), as well as for
the case where no other child is included in the
undirected-item.
We can make some general observations on the
Undirected Decoding Algorithm. Notice that, the
if statement at line 7 and the way new undirected-
items are created at line 9, enforce that each
undirected-item covers a contiguous span. An
undirected-item that is missing a child link cannot
be used as child context but can be used as parent
context since it is added to the out-forest at line 6
before the if statement at line 7. Furthermore, the
if statements at line 5 and line 11 check that no
more than k candidates are selected for each span,
but the algorithm does not require the the selection
of exactly k candidates per span as in CKY.
The queue of candidates, Q, is ordered according
to the heuristic cost of formula (6). The score of the
candidate partial structure is accounted for with fac-
tor cost(??) computed according to formula (5). The
factor est(??) accounts for the estimation of the miss-
ing part of the derivation. We compute this factor
with the following formula:
est(??) =
?
l?L???
(
localCost(??, l) + contextEst(??, l)
)
(7)
For each missing link, l ? L??? , we estimate the cost
of the corresponding derivation branch with two fac-
tors: localCost(??, l) that computes the context-free
score of the branch with highest score that could
be attached to l; and contextEst(??, l) that estimates
the contextual score of the branch and its interac-
tion with ??. Because our model is implemented in
the Forest Rescoring framework (e.g. Huang and
Chiang (2007), Dyer et al (2010), Li et al (2009)),
localCost(??, l) can be efficiently computed exactly.
In HMT it is possible to exhaustively represent and
search the context-free-forest (ignoring the LM),
which is done in the Forest Rescoring framework be-
fore our task of decoding with the LM. We exploit
this context-free-forest to compute localCost(??, l):
for missing child links the localCost(?) is the In-
side score computed using the (max, +) semiring
(also known as the Viterbi score), and for missing
parent links the localCost(?) is the corresponding
Outside score. The factor contextEst(?) estimates
the LM score of the words generated by the missing
branch and their interaction with the span covered
by ??. To compute the expected interaction cost we
use the boundary words information contained in ???s
carry as done in BD. To estimate the LM cost of the
missing branch we use an estimation function, con-
ditioned on the missing span length, whose parame-
ters are tuned on held-out data with gradient descent,
using the search score as objective function.
To show that UD leads to better results than BD,
the two algorithms are compared in the same search
space. Therefore we ensure that candidates em-
bedded in the UD out-forest would have the same
score if they were scored from BD. We don?t need
to worry about differences derived from the missing
context estimation factor, est(?), since this factor is
only considered while sorting the queue, Q, accord-
ing to the heuristicCost(?). Also, we don?t have to
worry about candidates that are scored with no miss-
ing child and no parent link, because in that case
scoring function (3) for BD is equivalent to scoring
function (5) for UD. Instead, for candidates that are
scored with parent link, we remove the parent link
factor from the cost(?) function when inserting the
candidate into the out forest. And for the candi-
dates that are scored with a missing child, we ad-
just the score once the link to the missing child is
created in the out-forest. In this way UD and BD
score the same derivation with the same score and
can be regarded as two ways to explore the same
search space.
5 Experiments
In this section we test the algorithm presented, and
empirically show that it produces better translations
searching a smaller portion of the search space.
We implemented UD on top of a widely-used
HMT open-source system, cdec (Dyer et al, 2010).
We compare with cdec Cube Pruning BD. The ex-
904
 0
 100
 200
 300
 400
 500
 600
 700
 800
 900
 2  4  6  8  10  12  14  16
Te
st
 S
et
Beam Size
UD best score
BD best score
Figure 2: Comparison of the quality of the translations.
periments are executed on the NIST MT03 Chinese-
English parallel corpus. The training corpus con-
tains 239k sentence pairs with 6.9M Chinese words
and 8.9M English words. We use a hierarchical
phrase-based translation grammar extracted using a
suffix array rule extractor (Lopez, 2007). The NIST-
03 test set is used for decoding, it has 919 sentence
pairs. The experiments can be reproduced on an
average desktop computer. Since we compare two
different decoding strategies that rely on the same
training technique, the evaluation is primarily based
on search errors rather than on BLEU. We compare
the two systems on a variety of beam sizes between
1 and 16.
Figure 2 reports a comparison of the translation
quality for the two systems in relation to the beam
size. The blue area represents the portion of sen-
tences for which UD found a better translation. The
white area represents the portion of sentences for
which the two systems found a translation with the
same search score. With beam 1 the two systems ob-
viously have a similar behavior, since both the sys-
tems stop investigating the candidates for a node af-
ter having selected the best candidate immediately
available. For beams 2-4, UD has a clear advan-
tage. In this range UD finds a better translation for
two thirds of the sentences. With beam 4, we ob-
serve that UD is able to find a better translation for
63.76% of the sentences, instead BD is able to find a
better translation for only 21.54% of the sentences.
For searches that employ a beam bigger than 8, we
notice that the UD advantage slightly decreases, and
-126.5
-126
-125.5
-125
-124.5
-124
-123.5
-123
 2  4  6  8  10  12  14  16
Se
ar
ch
 S
co
re
Beam Size
Bottom-up Decoding
Guided Decoding
Figure 3: Search score evolution for BD and UD.
the number of sentences with equivalent translation
slowly increases. We can understand this behavior
considering that as the beam increases the two sys-
tems get closer to exhaustive search. Anyway with
this experiment UD shows a consistent accuracy ad-
vantage over BD.
Figure 3 plots the search score variation for dif-
ferent beam sizes. We can see that UD search leads
to an average search score that is consistently bet-
ter than the one computed for BD. Undirected De-
coding improves the average search score by 0.411
for beam 16. The search score is the logarithm of
a probability. This variation corresponds to a rel-
ative gain of 50.83% in terms of probability. For
beams greater than 8 we see that the two curves keep
a monotonic ascendant behavior while converging to
exhaustive search.
Figure 4 shows the BLEU score variation. Again
we can see the consistent improvement of UD over
BD. In the graph we report also the performance ob-
tained using BD with beam 32. BD reaches BLEU
score of 32.07 with beam 32 while UD reaches
32.38 with beam 16: UD reaches a clearly higher
BLEU score using half the beam size. The differ-
ence is even more impressive if we consider that UD
reaches a BLEU of 32.19 with beam 4.
In Figure 5 we plot the percentage reduction of the
size of the hypergraphs generated by UD compared
to those generated by BD. The size reduction grows
quickly for both nodes and edges. This is due to the
fact that BD, using Cube Pruning, must select k can-
didates for each node. Instead, UD is not obliged to
905
 31.2
 31.4
 31.6
 31.8
 32
 32.2
 32.4
 2  4  6  8  10  12  14  16
BL
EU
 S
co
re
Beam Size
Bottom-up Decoding
Undirected Decoding
BD beam 30
Figure 4: BLEU score evolution for BD and UD.
 5
 10
 15
 20
 25
 30
 35
 40
 2  4  6  8  10  12  14  16
R
ed
uc
tio
n 
(%
)
Beam Size
Nodes Reduction
Edges Reduction
Figure 5: Percentage of reduction of the size of the hy-
pergraph produced by UD.
select k candidates per f -node. As we can see from
Algorithm 1, the decoding loop terminates when the
queue of candidates is empty, and the statements at
line 5 and line 11 ensure that no more than k can-
didates are selected per f -node, but nothing requires
the selection of k elements, and some bad candidates
may not be generated due to the sophisticated prop-
agation strategy. The number of derivations that a
hypergraph represents is exponential in the number
of nodes and edges composing the structure. With
beam 16, the hypergraphs produced by UD contain
on average 4.6k fewer translations. Therefore UD
is able to find better translations even if exploring a
smaller portion of the search space.
Figure 6 reports the time comparison between
BD and UD with respect to sentence length. The
 0
 200
 400
 600
 800
 1000
 1200
 10  15  20  25  30  35  40  45  50
Ti
m
e 
(m
s)
Input Sentence Size
Bottom-up Decoding, beam = 16
Undirected Decoding, beam = 8
Figure 6: Time comparison between BD and UD.
sentence length is measured with the number of
ideogram groups appearing in the source Chinese
sentences. We compare BD with beam of 16 and
UD with beam of 8, so that we compare two sys-
tems with comparable search score. We can notice
that for short sentences UD is faster, while for longer
sentences UD becomes slower. To understand this
result consider that for simple sentences UD can
rely on the advantage of exploring a smaller search
space. While, for longer sentences, the amount of
candidates considered during decoding grows ex-
ponentially with the size of the sentence, and UD
needs to maintain an unique queue whose size is not
bounded by the beam size k, as for the queues used
in BD?s Cube Pruning. It may be possible to address
this issue with more efficient handling of the queue.
In conclusion we can assert that, even if explor-
ing a smaller portion of the search space, UD finds
often a translation that is better than the one found
with standard BD. UD?s higher accuracy is due to
its sophisticated search strategy that allows a more
efficient integration of contextual features. This set
of experiments show the validity of the UD approach
and empirically confirm our intuition about the BD?s
inadequacy in solving tasks that rely on fundamental
contextual features.
6 Future Work
In the proposed framework the link to the parent
node is not treated differently from links to child
nodes, the information in the hypergraph can be
propagated in any direction. Then the Derivation
906
Hypergraph can be regarded as a non-directed graph.
In this setting we could imagine applying mes-
sage passing algorithms from graphical model the-
ory (Koller and Friedman, 2010).
Furthermore, considering that the proposed
framework lets the system decide the decoding or-
der, we could design a system that explicitly learns
to infer the decoding order at training time. Sim-
ilar ideas have been successfully tried: Shen et al
(2010) and Gesmundo (2011) investigate the Guided
Learning framework, that dynamically incorporates
the tasks of learning the order of inference and train-
ing the local classifier.
7 Conclusion
With this paper we investigate the limitations of
Bottom-up parsing techniques, widely used in Tree
Structures Prediction, focusing on Hierarchical Ma-
chine Translation. We devise a framework that al-
lows a better integration of non-bottom-up features.
Compared to a state of the art HMT decoder the pre-
sented system produces higher quality translations
searching a smaller portion of the search space, em-
pirically showing that the bottom-up approximation
of contextual features is a limitation for NLP tasks
like HMT.
Acknowledgments
This work was partly funded by Swiss NSF grant
CRSI22 127510 and European Community FP7
grant 216594 (CLASSiC, www.classic-project.org).
References
Sharon A. Caraballo and Eugene Charniak. 1998. New
figures of merit for best-first probabilistic chart pars-
ing, Computational Linguistics, 24:275-298.
J. C. Chappelier and M. Rajman and R. Arages and A.
Rozenknop. 1999. Lattice Parsing for Speech Recog-
nition. In Proceedings of TALN 1999, Cargse, France.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201-228,
2007.
Anna Corazza, Renato De Mori, Roberto Gretter and
Giorgio Satta. 1994. Optimal Probabilistic Evalu-
ation Functions for Search Controlled by Stochastic
Context-Free Grammars. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 16(10):1018-
1027.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec:
A Decoder, Alignment, and Learning framework for
finite-state and context-free translation models. In
Proceedings of the Conference of the Association of
Computational Linguistics 2010, Uppsala, Sweden.
Andrea Gesmundo and James Henderson 2010. Faster
Cube Pruning. Proceedings of the seventh Inter-
national Workshop on Spoken Language Translation
(IWSLT), Paris, France.
Andrea Gesmundo 2011. Bidirectional Sequence Classi-
fication for Tagging Tasks with Guided Learning. Pro-
ceedings of TALN 2011, Montpellier, France.
Mark Hopkins and Greg Langmead 2009. Cube prun-
ing as heuristic search. Proceedings of the Conference
on Empirical Methods in Natural Language Processing
2009, Singapore.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the Conference of the Association of
Computational Linguistics 2007, Prague, Czech Re-
public.
Dan Klein and Christopher D. Manning. 2001 Pars-
ing and Hypergraphs, In Proceedings of the Interna-
tional Workshop on Parsing Technologies 2001, Bei-
jing, China.
Dan Klein and Christopher D. Manning. 2003 A* Pars-
ing: Fast Exact Viterbi Parse Selection, In Proceed-
ings of the Conference of the North American Associ-
ation for Computational Linguistics 2003, Edmonton,
Canada.
Daphne Koller and Nir Friedman. 2010. Probabilistic
Graphical Models: Principles and Techniques. The
MIT Press, Cambridge, Massachusetts.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient Minimum Error Rate
Training and Minimum Bayes-Risk decoding for
translation hypergraphs and lattices, In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, Suntec,
Singapore.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren N. G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Joshua: An Open Source Toolkit for
Parsing-based Machine Translation. In Proceedings of
the Workshop on Statistical Machine Translation 2009,
Athens, Greece.
Adam Lopez. 2007. Hierarchical Phrase-Based Transla-
tion with Suffix Arrays. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing 2007, Prague, Czech Republic.
907
Haitao Mi, Liang Huang and Qun Liu. 2008. Forest-
Based Translation. In Proceedings of the Conference
of the Association of Computational Linguistics 2008,
Columbus, OH.
Libin Shen, Giorgio Satta and Aravind Joshi. 2007.
Guided Learning for Bidirectional Sequence Classifi-
cation. In Proceedings of the Conference of the As-
sociation of Computational Linguistics 2007, Prague,
Czech Republic.
Andreas Stolcke. 2002. SRILM - An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing
2002, Denver, CO.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing,
Proceedings of the Workshop on Statistical Machine
Translation, New York City, New York.
908
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 10?19,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Undirected Machine Translation with
Discriminative Reinforcement Learning
Andrea Gesmundo
Google Inc.
andrea.gesmundo@gmail.com
James Henderson
Xerox Research Centre Europe
james.henderson@xrce.xerox.com
Abstract
We present a novel Undirected Machine
Translation model of Hierarchical MT that
is not constrained to the standard bottom-
up inference order. Removing the order-
ing constraint makes it possible to condi-
tion on top-down structure and surround-
ing context. This allows the introduc-
tion of a new class of contextual features
that are not constrained to condition only
on the bottom-up context. The model
builds translation-derivations efficiently in
a greedy fashion. It is trained to learn
to choose jointly the best action and the
best inference order. Experiments show
that the decoding time is halved and forest-
rescoring is 6 times faster, while reaching
accuracy not significantly different from
state of the art.
1 Introduction
Machine Translation (MT) can be addressed as a
structured prediction task (Brown et al., 1993; Ya-
mada and Knight, 2001; Koehn et al., 2003). MT?s
goal is to learn a mapping function, f , from an in-
put sentence, x, into y = (t, h), where t is the
sentence translated into the target language, and
h is the hidden correspondence structure (Liang
et al., 2006). In Hierarchical MT (HMT) (Chi-
ang, 2005) the hidden correspondence structure is
the synchronous-tree composed by instantiations
of synchronous rules from the input grammar, G.
Statistical models usually define f as: f(x) =
argmax
y?Y
Score(x, y), where Score(x, y) is a
function whose parameters can be learned with a
specialized learning algorithm. In MT applica-
tions, it is not possible to enumerate all y ? Y .
HMT decoding applies pruning (e.g. Cube Prun-
ing (Huang and Chiang, 2005)), but even then
HMT has higher complexity than Phrase Based
MT (PbMT) (Koehn et al., 2003). On the other
hand, HMT improves over PbMT by introducing
the possibility of exploiting a more sophisticated
reordering model not bounded by a window size,
and producing translations with higher syntactic-
semantic quality. In this paper, we present the
Undirected Machine Translation (UMT) frame-
work, which retains the advantages of HMT and
allows the use of a greedy decoder whose com-
plexity is lower than standard quadratic beam-
search PbMT.
UMT?s fast decoding is made possible through
even stronger pruning: the decoder chooses a sin-
gle action at each step, never retracts that action,
and prunes all incompatible alternatives to that ac-
tion. If this extreme level of pruning was ap-
plied to the CKY-like beam-decoding used in stan-
dard HMT, translation quality would be severely
degraded. This is because the bottom-up infer-
ence order imposed by CKY-like beam-decoding
means that all pruning decisions must be based on
a bottom-up approximation of contextual features,
which leads to search errors that affect the qual-
ity of reordering and lexical-choice (Gesmundo
and Henderson, 2011). UMT solves this problem
by removing the bottom-up inference order con-
straint, allowing many different inference orders
for the same tree structure, and learning the in-
ference order where the decoder can be the most
confident in its pruning decisions.
Removing the bottom-up inference order con-
straint makes it possible to condition on top-down
structure and surrounding context. This undirected
approach allows us to integrate contextual features
such as the Language Model (LM) in a more flex-
10
ible way. It also allows us to introduce a new class
of undirected features. In particular, we introduce
the Context-Free Factor (CFF) features. CFF fea-
tures compute exactly and efficiently a bound on
the context-free cost of a partial derivation?s miss-
ing branches, thereby estimating the future cost of
partial derivations. The new class of undirected
features is fundamental for the success of a greedy
approach to HMT, because the additional non-
bottom-up context is sometimes crucial to have the
necessary information to make greedy decisions.
Because UMT prunes all but the single cho-
sen action at each step, both choosing a good in-
ference order and choosing a correct action re-
duce to a single choice of what action to take
next. To learn this decoding policy, we propose
a novel Discriminative Reinforcement Learning
(DRL) framework. DRL is used to train mod-
els that construct incrementally structured out-
put using a local discriminative function, with
the goal of optimizing a global loss function.
We apply DRL to learn the UMT scoring func-
tion?s parameters, using the BLEU score as the
global loss function. DRL learns a weight vector
for a linear classifier that discriminates between
decisions based on which one leads to a com-
plete translation-derivation with a better BLEU
score. Promotions/demotions of translations are
performed by applying a Perceptron-style update
on the sequence of decisions that produced the
translation, thereby training local decisions to op-
timize the global BLEU score of the final trans-
lation, while keeping the efficiency and simplic-
ity of the Perceptron Algorithm (Rosenblatt, 1958;
Collins, 2002).
Our experiments show that UMT with DRL re-
duces decoding time by over half, and the time to
rescore translations with the Language Model by
6 times, while reaching accuracy non-significantly
different from the state of the art.
2 Undirected Machine Translation
In this section, we present the UMT frame-
work. For ease of presentation, and following
synchronous-grammar based MT practice, we will
henceforth restrict our focus to binary grammars
(Zhang et al., 2006; Wang et al., 2007).
A UMT decoder can be formulated as a func-
tion, f , that maps a source sentence, x ? X , into
a structure defined by y = (t, h) ? Y , where t
is the translation in the target language, and h
is the synchronous tree structure generating the
input sentence on the source side and its trans-
lation on the target side. Synchronous-trees are
composed of instantiations of synchronous-rules,
r, from a grammar, G. A UMT decoder builds
synchronous-trees, h, by recursively expanding
partial synchronous-trees, ? . ? includes a partial
translation. Each ? is required to be a connected
sub-graph of some synchronous-tree h. Thus, ?
is composed of a subset of the rules from any h
that generates x on the source side, such that there
is a connected path between any two rules in ? .
Differently from the partial structures built by a
bottom-up decoder, ? does not have to cover a
contiguous span on x. Formally, ? is defined by:
1) The set of synchronous-rule instantiations in ? :
I ? {r
1
, r
2
, ? ? ? , r
k
|r
i
? G, 1 ? i ? k};
2) The set of connections among the synchronous-
rule instantiations, C .
Let c
i
= (r
i
, r
j
i
) be the notation to represent the
connection between the i-th rule and the rule r
j
i
.
The set of connections can be expressed as:
C ? {(r
1
, r
j
1
), (r
2
, r
j
2
), ? ? ? , (r
k?1
, r
j
k?1
)}
3) The postcondition set, P , which specifies
the non-terminals in ? that are available for
creating new connections. Each postcondition,
p
i
= (r
x
,X y )i, indicates that the rule rx has the
non-terminal X y available for connections. The
index y identifies the non-terminal in the rule. In
a binary grammar y can take only 3 values: 1 for
the first non-terminal (the left child of the source
side), 2 for the second non-terminal, and h for the
head. The postcondition set can be expressed as:
P?{(r
x
1
,X
y
1
)
1
, ? ? ? , (r
x
m
,X
y
m
)
m
}
4) The set of carries, K . We define a different
carry, ?
i
, for each non-terminal available for
connections. Each carry stores the extra infor-
mation required to correctly score the non-local
interactions between ? and the rule that will be
connected at that non-terminal. Thus |K| = |P |.
Let ?
i
be the carry associated with the postcon-
dition p
i
. The set of carries can be expressed as:
K ? {?
1
, ?
2
, ? ? ? , ?
m
}
Partial synchronous-trees, ? , are expanded by
performing connection-actions. Given a ? we can
connect to it a new rule, r?, using one available non-
terminal represented by postcondition, p
i
? P ,
and obtain a new partial synchronous-tree ?? . For-
mally: ?? ? ? ? ? a? ?, where, a? = [r?, p
i
],
represents the connection-action.
11
Algorithm 1 UMT Decoding
1: function Decoder (x; w, G) : (t,h)
2: ?.{I, C, P,K} ? {?, ?, ?, ?} ;
3: Q? LeafRules(G);
4: while |Q| > 0 do
5: [r?, p
i
]? PopBestAction (Q,w);
6: ? ? CreateConnection(?, r? , p
i
);
7: UpdateQueue(Q, r?, p
i
);
8: end while
9: Return(?);
10: procedure CreateConnection(? , r?, p
i
) : ??
11: ?? .I ? ?.I + r?;
12: ?? .C ? ?.C + (r?, r
p
i
);
13: ?? .P ? ?.P ? p
i
;
14: ?? .K ? ?.K ? ?
i
;
15: ?? .K .UpdateCarries(r?, p
i
);
16: ?? .P .AddAvailableConnectionsFrom(r? , p
i
);
17: ?? .K .AddCarriesForNewConnections(r? , p
i
);
18: Return(?? );
19: procedure UpdateQueue( Q, r?, p
i
) :
20: Q.RemoveActionsWith(p
i
);
21: Q.AddNewActions(r?, p
i
);
2.1 Decoding Algorithm
Algorithm 1 gives details of the UMT decoding
algorithm. The decoder takes as input the source
sentence, x, the parameters of the scoring func-
tion, w, and the synchronous-grammar, G. At
line 2 the partial synchronous-tree ? is initialized
by setting I , C , P and K to empty sets ?. At
line 3 the queue of candidate connection-actions
is initialized as Q ? { [r
leaf
, null] | r
leaf
is a
leaf rule}, where null means that there is no post-
condition specified, since the first rule does not
need to connect to anything. A leaf rule r
leaf
is
any synchronous rule with only terminals on the
right-hand sides. At line 4 the main loop starts.
Each iteration of the main loop will expand ? us-
ing one connection-action. The loop ends when
Q is empty, implying that ? covers the full sen-
tence and has no more missing branches or par-
ents. The best scoring action according to the
parameter vector w is popped from the queue at
line 5. The scoring of connection-actions is dis-
cussed in details in Section 3.2. At line 6 the se-
lected connection-action is used to expand ? . At
line 7 the queue of candidates is updated accord-
ingly (see lines 19-21). At line 8 the decoder it-
erates the main loop, until ? is complete and is
returned at line 9.
Lines 10-18 describe the CreateConnection(?)
procedure, that connects the partial synchronous-
tree ? to the selected rule r? via the postcondi-
tion p
i
specified by the candidate-action selected
in line 5. This procedure returns the resulting par-
tial synchronous-tree: ?? ? ? ? ? [r?, p
i
] ?. At
line 11, r? is added to the rule set I . At line 12 the
connection between r? and r
p
i
(the rule specified
in the postcondition) is added to the set of connec-
tions C . At line 13, p
i
is removed from P . At
line 14 the carry k
i
matching with p
i
is removed
from K . At line 15 the set of carries K is updated,
in order to update those carries that need to pro-
vide information about the new action. At line 16
new postconditions representing the non-terminals
in r? that are available for subsequent connections
are added in P . At line 17 the carries associated
with these new postconditions are computed and
added to K . Finally at line 18 the updated partial
synchronous-tree is returned.
In the very first iteration, the
CreateConnection(?) procedure has nothing
to compute for some lines. Line 11 is not exe-
cuted since the first leaf rule needs no connection
and has nothing to connect to. lines 12-13 are
not executed since P and K are ? and p
i
is not
specified for the first action. Line 15 is not
executed since there are no carries to be updated.
Lines 16-17 only add the postcondition and carry
relative to the leaf rule head link.
The procedure used to update Q is reported in
lines 19-21. At line 20 all the connection-actions
involving the expansion of p
i
are removed from
Q. These actions are the incompatible alternatives
to the selected action. In the very first iteration,
all actions in Q are removed because they are all
incompatible with the connected-graph constraint.
At line 21 new connection-actions are added to
Q. These are the candidate actions proposing a
connection to the available non-terminals of the
selected action?s new rule r?. The rules used for
these new candidate-actions must not be in con-
flict with the current structure of ? (e.g. the rule
cannot generate a source side terminal that is al-
ready covered by ? ).
12
3 Discriminative Reinforcement
Learning
Training a UMT model simply means training the
parameter vector w that is used to choose the best
scoring action during decoding. We propose a
novel method to apply a kind of minimum error
rate training (MERT) to w. Because each ac-
tion choice must be evaluated in the context of
the complete translation-derivation, we formalize
this method in terms of Reinforcement Learning.
We propose Discriminative Reinforcement Learn-
ing as an appropriate way to train a UMT model to
maximize the BLEU score of the complete deriva-
tion. First we define DRL as a novel generic train-
ing framework.
3.1 Generic Framework of DRL
RL can be applied to any task, T , that can be for-
malized in terms of:
1) The set of states S1;
2) A set of actions A
s
for each state s ? S;
3) The transition function T : S ? A
s
? S, that
specifies the next state given a source state and
performed action2;
4) The reward function, R : S ?A
s
? R;
5) The discount factor, ? ? [0, 1].
A policy is defined as any map ? : S ? A. Its
value function is given by:
V
pi
(s
0
) =
?
?
i=0
?
i
R(s
i
, ?(s
i
)) (1)
where path(s
0
|?)? ?s
0
, s
1
, ? ? ? , s
?
|?? is the se-
quence of states determined by following policy ?
starting at state s
0
. The Q-function is the total fu-
ture reward of performing action a
0
in state s
0
and
then following policy ?:
Q
pi
(s
0
, a
0
) = R(s
0
, a
0
) + ?V
pi
(s
1
) (2)
Standard RL algorithms search for a policy that
maximizes the given reward.
Because we are taking a discriminative ap-
proach to learn w, we formalize our optimization
task similarly to an inverse reinforcement learning
problem (Ng and Russell, 2000): we are given in-
formation about the optimal action sequence and
we want to learn a discriminative reward func-
tion. As in other discriminative approaches, this
1
S can be either finite or infinite.
2For simplicity we describe a deterministic process. To
generalize to the stochastic process, replace the transition
function with the transition probability: P
sa
(s
?
), s
?
? S.
Algorithm 2 Discriminative RL
1: function Trainer (?,T ,D ) : w
2: repeat
3: s?SampleState(S);
4: a?? ?
w
(s);
5: a? ?SampleAction(A
s
);
6: if Qpiw(s, a?) < Qpiw(s, a?) in D then
7: w? w + ?w(s, a?)? ?w(s, a?);
8: end if
9: until convergence
10: Return(w);
approach simplifies the task of learning the re-
ward function in two respects: the learned reward
function only needs to be monotonically related
to the true reward function, and this property only
needs to hold for the best competing alternatives.
This is all we need in order to use the discrimina-
tive reward function in an optimal classifier, and
this simplification makes learning easier in cases
where the true reward function is too complicated
to model directly.
In RL, an optimal policy ?? is one which, at
each state s, chooses the action which maximizes
the future reward Qpi?(s, a). We assume that the
future discriminative reward can be approximated
with a linear function ?Qpi(s, a) in some feature-
vector representation ? : S ?A
s
? R
d that maps
a state-action pair to a d-dimensional features vec-
tor:
?
Q
pi
(s, a) = w ?(s, a) (3)
where w ? Rd. This gives us the following policy:
?
w
(s) = argmax
a?A
s
w ?(s, a) (4)
The set of parameters of this policy is the vec-
tor w. With this formalization, all we need to
learn is a vector w such that the resulting deci-
sions are compatible with the given information
about the optimal action sequence. We propose a
Perceptron-like algorithm to learn these parame-
ters.
Algorithm 2 describes the DRL meta-algorithm.
The Trainer takes as input ?, the task T , and a
generic set of data D describing the behaviors we
want to learn. The output is the weight vector w
of the learned policy that fits the data D. The al-
gorithm consists in a single training loop that is
repeated until convergence (lines 2-9). At line 3
a state, s, is sampled from S. At line 4, a? is set to
13
be the action that would be preferred by the cur-
rent w-policy. At line 5 an action, a?, is sampled
from A
s
such that a? 6= a?. At line 6 the algo-
rithm checks if preferring path(T (s, a?), ?
w
) over
path(T (s, a
?
), ?
w
) is a correct choice according
to the behaviors data D that the algorithm aims to
learn. If the current w-policy contradicts D, line 7
is executed to update the weight vector to promote
?
w
(s, a
?
) and penalize ?w(s, a?), where ?w(s, a)
is the summation of the features vectors of the en-
tire derivation path starting at (s, a) and following
policy ?
w
. This way of updating w has the ef-
fect of increasing the ?Q(?) value associated with
all the actions in the sequence that generated the
promoted structure, and reducing the ?Q(?) value
of the actions in the sequence that generated the
penalized structure3 .
We have described the DRL meta-algorithm to
be as general as possible. When applied to a spe-
cific problem, more details can be specified: 1) it
is possible to choose specific sampling techniques
to implement lines 3 and 5; 2) the test at line 6
needs to be detailed according to the nature of T
and D; 3) the update statement at line 7 can be re-
placed with a more sophisticated update approach.
We address these issues and describe a range of
alternatives as we apply DRL to UMT in Section
3.2.
3.2 Application of DRL to UMT
To apply DRL we formalize the task of translating
x with UMT as T ? {S, {A
s
}, T,R, ?}:
1) The set of states S is the space of all possible
UMT partial synchronous-trees, ? ;
2) The set A
?,x
is the set of connection-actions
that can expand ? connecting new synchronous-
rule instantiations matching the input sentence x
on the source side;
3) The transition function T is the connection
function ?? ? ? ? ? a ? formalized in Section 2
and detailed by the procedure CreateConnection(?)
in Algorithm 1;
4) The true reward function R is the BLEU score.
BLEU is a loss function that quantifies the differ-
ence between the reference translation and the out-
put translation t. The BLEU score can be com-
puted only when a terminal state is reached and a
full translation is available. Thus, the rewards are
all zero except at terminal states, called a Pure De-
3Preliminary experiments with updating only the features
for a? and a? produced substantially worse results.
layed Reward function;
5) Considering the nature of the problem and re-
ward function, we choose an undiscounted setting:
? = 1.
Next we specify the details of the DRL algo-
rithm. The data D consists of a set of pairs of
sentences, D ? {(x, t?)}, where x is the source
sentence and t? is the reference translation. The
feature-vector representation function ? maps a
pair (?, a) to a real valued vector having any num-
ber of dimensions. Each dimension corresponds
to a distinct feature function that maps: {?} ?
A
?,x
? R. Details of the features functions im-
plemented for our model are given in Section 4.
Each loop of the DRL algorithm analyzes a single
sample (x, t?) ? D. The state s is sampled from a
uniform distribution over ?s
0
, s
1
, ? ? ? , s
?
|??. The
action a? is sampled from a Zipfian distribution
over {A
?,x
? a?} sorted with the ?Qpiw(s, a) func-
tion. In this way actions with higher score have
higher probability to be drawn, while actions at the
bottom of the rank still have a small probability to
be selected. The if at line 6 tests if the translation
produced by path(T (s, a?), ?
w
) has higher BLEU
score than the one produced by path(T (s, a?), ?
w
).
For the update statement at line 7 we use
the Averaged Perceptron technique (Freund and
Schapire, 1999). Algorithm 2 can be eas-
ily adapted to implement the efficient Averaged
Perceptron updates (e.g. see Section 2.1.1 of
(Daume? III, 2006)). In preliminary experiments,
we found that other more aggressive update tech-
nique, such as Passive-Aggressive (Crammer et
al., 2006), Aggressive (Shen et al., 2007), or
MIRA (Crammer and Singer, 2003), lead to worst
accuracy. To see why this might be, consider that
a MT decoder needs to learn to construct struc-
tures (t, h), while the training data specifies the
gold translation t? but gives no information on the
hidden-correspondence structure h. As discussed
in (Liang et al., 2006), there are output structures
that match the reference translation using a wrong
internal structure (e.g. assuming wrong internal
alignment). While in other cases the output trans-
lation can be a valid alternative translation but gets
a low BLEU score because it differs from t?. Ag-
gressively promoting/penalizing structures whose
correctness can be only partially verified can be
expected to harm generalization ability.
14
4 Undirected Features
In this section we show how the features designed
for bottom-up HMT can be adapted to the undi-
rected approach, and we introduce a new feature
from the class of undirected features that are made
possible by the undirected approach.
Local features depend only on the action rule r.
These features can be used in the undirected ap-
proach without adaptation, since they are indepen-
dent of the surrounding structure. For our experi-
ments we use a standard set of local features: the
probability of the source phrase given the target
phrase; the lexical translation probabilities of the
source words given the target words; the lexical
translation probabilities of the target words given
the source words; and the Word Penalty feature.
Contextual features are dependent on the inter-
action between the action rule r and the avail-
able context. In UMT all the needed information
about the available context is stored in the carry
?
i
. Therefore, the computation of contextual fea-
tures whose carry?s size is bounded (like the LM)
requires constant time.
The undirected adaptation of the LM feature
computes the scores of the new n-grams formed
by adding the terminals of the action rule r to the
current partial translation ? . In the case that the
action rule r is connected to ? via a child non-
terminal, the carry is expressed as ?
i
? ([W
L
?
W
R
]). Where W
L
and W
R
are respectively the left
and right boundary target words of the span cov-
ered by ? . This notation is analogous to the stan-
dard star notation used for the bottom-up decoder
(e.g. (Chiang, 2007) Section 5.3.2). In the case
that r is connected to ? via the head non-terminal,
the carry is expressed as ?
i
? (W
R
]-[W
L
). Where
W
L
and W
R
are respectively the left and right
boundary target words of the surrounding context
provided by ? . The boundary words stored in the
carry and the terminals of the action rule are all the
information needed to compute and score the new
n-grams generated by the connection-action.
In addition, we introduce the Context-Free Fac-
tor (CFF) features. An action rule r is connected
to ? via one of r?s non-terminals, X
r,?
. Thus, the
score of the interaction between r and the context
structure attached to X
r,?
can be computed ex-
actly, while the score of the structures attached to
other r nonterminals (i.e. those in postconditions)
cannot be computed since these branches are miss-
ing. Each of these postcondition nonterminals
has an associated CFF feature, which is an upper
bound on the score of its missing branch. More
precisely, it is an upper bound on the context-free
component of this score. This upper bound can be
exactly and efficiently computed using the Forest
Rescoring Framework (Huang and Chiang, 2007;
Huang, 2008). This framework separates the MT
decoding in two steps. In the first step only the
context-free factors are considered. The output of
the first step is a hypergraph called the context-
free-forest, which compactly represents an expo-
nential number of synchronous-trees. The second
step introduces contextual features by applying a
process of state-splitting to the context-free-forest,
rescoring with non-context-free factors, and effi-
ciently pruning the search space.
To efficiently compute CFF features we run
the Inside-Outside algorithm with the (max,+)
semiring (Goodman, 1999) over the context-free-
forest. The result is a map that gives the maxi-
mum Inside and Outside scores for each node in
the context-free forest. This map is used to get the
value of the CFF features in constant time while
running the forest rescoring step.
5 Experiments
We implement our model on top of Cdec (Dyer et
al., 2010). Cdec provides a standard implemen-
tation of the HMT decoder (Chiang, 2007) and
MERT training (Och, 2003) that we use as base-
line.
We experiment on the NIST Chinese-English
parallel corpus. The training corpus contains
239k sentence pairs with 6.9M Chinese words and
8.9M English words. The test set contains 919
sentence pairs. The hierarchical translation gram-
mar was extracted using the Joshua toolkit (Li et
al., 2009) implementation of the suffix array rule
extractor algorithm (Callison-Burch et al., 2005;
Lopez, 2007).
Table 1 reports the decoding time measures.
HMT with beam1 is the fastest possible configu-
ration for HMT, but it is 71.59% slower than UMT.
This is because HMT b1 constructs O(n2) sub-
trees, many of which end up not being used in
the final result, whereas UMT only constructs the
rule instantiations that are required. HMT with
beam30 is the fastest configuration that reaches
state of the art accuracy, but increases the aver-
age time per sentence by an additional 131.36%
when compared with UMT. The rescoring time is
15
Model sent. t. sent. t. var. resc. t. resc. t. var.
UMT 135.2ms - 38.9 ms -
HMT b1 232.0ms +71.59% 141.3 ms +263.23%
HMT b30 312.8ms +131.36% 226.9 ms +483.29%
Table 1: Decoding speed comparison.
Model sent. t. sent. t. var.
UMT with DRL 267.4 ms -
HMT b1 765.2 ms +186.16%
HMT b30 1153.5 ms +331.37%
Table 2: Training speed comparison.
Model BLEU relative loss p-value
UMT with DRL 30.14 6.33% 0.18
HMT b1 30.87 4.07% 0.21
HMT b30 32.18 - -
Table 3: Accuracy comparison.
the average time spent on the forest rescoring step,
which is the only step where the decoders actu-
ally differ. This is the step that involves the inte-
gration of the Language Model and other contex-
tual features. For HMT b30, rescoring takes two
thirds of the total decoding time. Thus rescoring
is the most time consuming step in the pipeline.
The rescoring time comparison shows even bigger
gains for UMT. HMT b30 is almost 6 times slower
than UMT.
Table 2 reports the training time measures.
These results show HMT b30 training is more
than 4 times slower than UMT training with DRL.
Comparing with Table 1, we notice that the rela-
tive gain on average training time is higher than
the gain measured at decoding time. This is be-
cause MERT has an higher complexity than DRL.
Both of the training algorithms requires 10 train-
ing epochs to reach convergence.
Table 3 reports the accuracy measures. As ex-
pected, accuracy degrades the more aggressively
the search space is pruned. UMT trained with
DRL loses 2.0 BLEU points compared to HMT
b30. This corresponds to a relative-loss of 6.33%.
Although not inconsequential, this variation is
not considered big (e.g. at the WMT-11 Ma-
chine Translation shared task (Callison-Burch et
al., 2011)). To measure the significance of the
variation, we compute the sign test and measure
the one-tail p-value for the presented models in
comparison to HMT b30. From the values re-
ported in the fourth column, we can observe that
the BLEU score variations would not normally be
considered significant. For example, at WMT-11
two systems were considered equivalent if p >
0.1, as in these cases. The accuracy cannot be
compared in terms of search score since the mod-
els we are comparing are trained with distinct al-
gorithms and thus the search scores are not com-
parable.
To test the impact of the CFF features, we
trained and tested UMT with DRL with and with-
out these features. This resulted in an accuracy de-
crease of 2.3 BLEU points. Thus these features are
important for the success of the greedy approach.
They provide an estimate of the score of the miss-
ing branches, thus helping to avoid some actions
that have a good local score but lead to final trans-
lations with low global score.
To validate the results, additional experiments
were executed on the French to Italian portion
of the Europarl corpus v6. This portion contains
190k pairs of sentences. The first 186k sentences
were used to extract the grammar and train the two
models. The final tests were performed on the re-
maining 4k sentence pairs. With this corpus we
measured a similar speed gain. HMT b30 is 2.3
times slower at decoding compared to UMT, and
6.1 times slower at rescoring, while UMT loses
1.1 BLEU points in accuracy. But again the ac-
curacy differences are not considered significant.
We measured a p-value of 0.25, which is not sig-
nificant at the 0.1 level.
6 Related Work
Models sharing similar intuitions have been pre-
viously applied to other structure prediction tasks.
For example, Nivre et al. (2006) presents a linear
time syntactic dependency parser, which is con-
strained in a left-to-right decoding order. This
model offers a different accuracy/complexity bal-
ance than the quadratic time graph-based parser of
Mcdonald et al. (2005).
Other approaches learning a model specifically
for greedy decoding have been applied with suc-
16
cess to other less complex tasks. Shen et al. (2007)
present the Guided Learning (GL) framework for
bidirectional sequence classification. GL success-
fully combines the tasks of learning the order of
inference and training the local classifier in a sin-
gle Perceptron-like algorithm, reaching state of the
art accuracy with complexity lower than the ex-
haustive counterpart (Collins, 2002).
Goldberg and Elhadad (2010) present a simi-
lar training approach for a Dependency Parser that
builds the tree-structure by recursively creating
the easiest arc in a non-directional manner. This
model also integrates the tasks of learning the or-
der of inference and training the parser in a single
Perceptron. By ?non-directional? they mean the
removal of the constraint of scanning the sentence
from left to right, which is typical of shift-reduce
models. However this algorithm still builds the
tree structures in a bottom-up fashion. This model
has a O(n log n) decoding complexity and accu-
racy performance close to the O(n2) graph-based
parsers (Mcdonald et al., 2005).
Similarities can be found between DRL and pre-
vious work that applies discriminative training to
structured prediction: Collins and Roark (2004)
present an Incremental Parser trained with the Per-
ceptron algorithm. Their approach is specific to
dependency parsing and requires a function to test
exact match of tree structures to trigger parameter
updates. On the other hand, DRL can be applied to
any structured prediction task and can handle any
kind of reward function. LASO (Daume? III and
Marcu, 2005; Daume? III et al., 2005) and SEARN
(Daume? III et al., 2009; Daume? III et al., 2006)
are generic frameworks for discriminative training
for structured prediction: LASO requires a func-
tion that tests correctness of partial structures to
trigger early updates, while SEARN requires an
optimal policy to initialize the learning algorithm.
Such a test function or optimal policy cannot be
computed for tasks such as MT where the hidden
correspondence structure h is not provided in the
training data.
7 Discussion and Future Work
In general, we believe that greedy-discriminative
solutions are promising for tasks like MT, where
there is not a single correct solution: normally
there are many correct ways to translate the same
sentence, and for each correct translation there
are many different derivation-trees generating that
translation, and each correct derivation tree can be
built greedily following different inference orders.
Therefore, the set of correct decoding paths is a
reasonable portion of UMT?s search space, giving
a well-designed greedy algorithm a chance to find
a good translation even without beam search.
In order to directly evaluate the impact of our
proposed decoding strategy, in this paper the only
novel features that we consider are the CFF fea-
tures. But to take full advantage of the power
of discriminative training and the lower decoding
complexity, it would be possible to vastly increase
the number of features. The UMT?s undirected na-
ture allows the integration of non-bottom-up con-
textual features, which cannot be used by stan-
dard HMT and PbMT. And the use of a history-
based model allows features from an arbitrarily
wide context, since the model does not need to be
factorized. Exploring the impact of this advantage
is left for future work.
8 Conclusion
The main contribution of this work is the pro-
posal of a new MT model that offers an accu-
racy/complexity balance that was previously un-
available among the choices of hierarchical mod-
els.
We have presented the first Undirected frame-
work for MT. This model combines advantages
given by the use of hierarchical synchronous-
grammars with a more efficient decoding algo-
rithm. UMT?s nature allows us to design novel
undirected features that better approximate con-
textual features (such as the LM), and to introduce
a new class of undirected features that cannot be
used by standard bottom-up decoders. Further-
more, we generalize the training algorithm into
a generic Discriminative Reinforcement Learning
meta-algorithm that can be applied to any struc-
tured prediction task.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19:263?311.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
17
phrases. In ACL ?05: Proceedings of the 43rd Con-
ference of the Association for Computational Lin-
guistics, Ann Arbor, MI, USA.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In WMT ?11:
Proceedings of the 6th Workshop on Statistical Ma-
chine Translation, Edinburgh, Scotland.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In ACL
?05: Proceedings of the 43rd Conference of the As-
sociation for Computational Linguistics, Ann Arbor,
MI, USA.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In ACL ?04:
Proceedings of the 42rd Conference of the Associa-
tion for Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP ?02:
Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing, Philadel-
phia, PA, USA.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
Koby Crammer, Ofer Dekel, Shai Shalev-Shwartz, and
Yoram Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:551?585.
Hal Daume? III and Daniel Marcu. 2005. Learning
as search optimization: approximate large margin
methods for structured prediction. In ICML ?05:
Proceedings of the 22nd International Conference
on Machine Learning, Bonn, Germany.
Hal Daume? III, John Langford, and Daniel Marcu.
2005. Search-based structured prediction as clas-
sification. In ASLTSP ?05: Proceedings of the
NIPS Workshop on Advances in Structured Learn-
ing for Text and Speech Processing, Whistler, British
Columbia, Canada.
Hal Daume? III, John Langford, and Daniel Marcu.
2006. Searn in practice. Technical report.
Hal Daume? III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Submit-
ted to Machine Learning Journal.
Hal Daume? III. 2006. Practical structured learning
techniques for natural language processing. Ph.D.
thesis, University of Southern California.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Hendra Setiawan, Ferhan Ture, Vladimir Ei-
delman, Phil Blunsom, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In ACL ?10: Proceedings of the ACL 2010 System
Demonstrations, Uppsala, Sweden.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine Learning, 37(3):277?296.
Andrea Gesmundo and James Henderson. 2011.
Heuristic Search for Non-Bottom-Up Tree Structure
Prediction. In EMNLP ?11: Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, Edinburgh, Scotland, UK.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In NAACL ?10: Proceedings of the
11th Conference of the North American Chapter of
the Association for Computational Linguistics, Los
Angeles, CA, USA.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25:573?605.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In IWPT ?05: Proceedings of the 9th Inter-
national Workshop on Parsing Technology, Vancou-
ver, British Columbia, Canada.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In ACL ?07: Proceedings of the 45th Confer-
ence of the Association for Computational Linguis-
tics, Prague, Czech Republic.
Liang Huang. 2008. Forest-based algorithms in natu-
ral language processing. Ph.D. thesis, University of
Pennsylvania.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 4th Conference of
the North American Chapter of the Association for
Computational Linguistics, Edmonton, Canada.
Zhifei Li, Chris Callison-Burch, Chris Dyer, San-
jeev Khudanpur, Lane Schwartz, Wren Thornton,
Jonathan Weese, and Omar Zaidan. 2009. Joshua:
An open source toolkit for parsing-based machine
translation. In WMT ?09: Proceedings of the
4th Workshop on Statistical Machine Translation,
Athens, Greece.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In COLING-
ACL ?06: Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
Conference of the Association for Computational
Linguistics, Sydney, Australia.
18
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP-CoNLL ?07:
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, Prague,
Czech Republic.
Ryan Mcdonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In ACL ?05: Proceedings of the
43rd Conference of the Association for Computa-
tional Linguistics, Ann Arbor, MI, USA.
Andrew Y. Ng and Stuart Russell. 2000. Algorithms
for inverse reinforcement learning. In ICML ?00:
Proceedings of the 17th International Conference on
Machine Learning, Stanford University, CA, USA.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In LREC ?06: Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation, Genoa, Italy.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ?03: Pro-
ceedings of the 41st Conference of the Association
for Computational Linguistics, Sapporo, Japan.
Frank Rosenblatt. 1958. The Perceptron: A proba-
bilistic model for information storage and organiza-
tion in the brain. Psychological Review, 65(6):386?
408.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifi-
cation. In ACL ?07: Proceedings of the 45th Confer-
ence of the Association for Computational Linguis-
tics, Prague, Czech Republic.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based ma-
chine translation accuracy. In EMNLP-CoNLL ?07:
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, Prague,
Czech Republic.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In ACL ?01: Pro-
ceedings of the 39th Conference of the Association
for Computational Linguistics, Toulouse, France.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In NAACL ?06: Proceedings of the
7th Conference of the North American Chapter of
the Association for Computational Linguistics, New
York, New York.
19
Proceedings of NAACL-HLT 2013, pages 772?776,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Graph-Based Seed Set Expansion for Relation Extraction Using
Random Walk Hitting Times
Joel Lang
University of Geneva
7 Route de Drize
1227 Carouge, Switzerland
joel.lang@unige.ch
James Henderson
Xerox Research Centre Europe
6 Chemin de Maupertuis
38240 Meylan, France
james.henderson@xrce.xerox.com
Abstract
Iterative bootstrapping methods are
widely employed for relation extraction,
especially because they require only a
small amount of human supervision.
Unfortunately, a phenomenon known
as semantic drift can affect the accuracy
of iterative bootstrapping and lead to
poor extractions. This paper proposes
an alternative bootstrapping method,
which ranks relation tuples by measuring
their distance to the seed tuples in a
bipartite tuple-pattern graph. In contrast
to previous bootstrapping methods, our
method is not susceptible to semantic
drift, and it empirically results in better
extractions than iterative methods.
1 Introduction
The goal of relation extraction is to extract tu-
ples of a particular relation from a corpus of
natural language text. A widely employed ap-
proach to relation extraction is based on iter-
ative bootstrapping (Brin, 1998; Agichtein and
Gravano, 2000; Pasca et al, 2006; Pantel and
Pennacchiotti, 2006), which can be applied with
only small amounts of supervision and which
scales well to very large datasets.
A well-known problem with iterative boot-
strapping is a phenomenon known as seman-
tic drift (Curran et al, 2007): as bootstrap-
ping proceeds it is likely that unreliable pat-
terns will lead to false extractions. These extrac-
tion errors are amplified in the following itera-
tions and the extracted relation will drift away
from the intended target. Semantic drift often
results in low precision extractions and there-
fore poses a major limitation of iterative boot-
strapping algorithms. Previous work on itera-
tive bootstrapping has addressed the issue of re-
ducing semantic drift for example by bagging
the results of various runs employing differing
seed tuples, constructing filters which identify
false tuples or patterns and adding further con-
straints to the bootstrapping process (T. McIn-
tosh, 2010; McIntosh and Curran, 2009; Curran
et al, 2007).
However, the analysis of Komachi et al
(2008) has shown that semantic drift is an in-
herent property of iterative bootstrapping algo-
rithms and therefore poses a fundamental prob-
lem. They have shown that iterative bootstrap-
ping without pruning corresponds to an eigen-
vector computation and thus as the number of
iterations increases the resulting ranking will al-
ways converge towards the same static ranking
of tuples, regardless of the particular choice of
seed instances.
In this paper, we describe an alternative
method, that is not susceptible to semantic drift.
We represent our data as a bipartite graph,
whose vertices correspond to patterns and tu-
ples respectively and whose edges capture cooc-
currences and then measure the distance of a
tuple to the seed set in terms of random walk
hitting times. Experimental results confirm that
semantic drift is avoided by our method and
show that substantial improvements over iter-
ative forms of bootstrapping are possible.
772
2 Scoring with Hitting Times
From a given corpus, we extract a dataset con-
sisting of tuples and patterns. Tuples are pairs
of co-occurring strings in the corpus, such as
(Bill Gates, Microsoft), which potentially belong
to a particular relation of interest. In our case,
patterns are simply the sequence of tokens oc-
curring between tuple elements, e.g. ?is the
founder of?. We represent all the tuple types1
X and all the extraction pattern types Y con-
tained in a given corpus through an undirected,
weighted, bipartite graph G = (V,E) with ver-
tices V = X ? Y and edges E ? X ? Y , where
an edge (x, y) ? E indicates that tuple x oc-
currs with pattern y somewhere in the corpus.
Edge weights are defined through a weight ma-
trix W which holds the weight Wi,j = w(vi, vj)
for edges (vi, vj) ? E. Specifically, we use the
count of how many times a tuple occurs with
a pattern in the corpus and weights for uncon-
nected vertices are zero.
Our goal is to compute a score vector ? hold-
ing a score ?i = ?(xi) for each tuple xi ? X,
which quantifies how well the tuple matches the
seed tuples. Higher scores indicate that the tu-
ple is more likely to belong to the relation de-
fined through the seeds and thus the score vec-
tor effectively provides a ranking of the tuples.
We define scores of tuples based on their dis-
tance2 to the seed tuples in the graph. The dis-
tance of some tuple x to the seed set S can
be naturally formalized in terms of the aver-
age time it takes until a random walk starting
in S reaches x, the hitting time. The random
walk is defined through the probability distri-
bution over start vertices and through a ma-
trix of transition probabilities. Edge weights
are constrained to be non-negative, which al-
lows us to define the transition matrix P with
Pi,j = p(vj |vi) = 1dvi
w(vi, vj), where dv =
?
vk?V
w(v, vk) is the degree of a vertex v ? V .
The distance of two vertices is measured in
terms of the average time of a random walk be-
1Note that we are using tuple and pattern types rather
than particular mentions in the corpus.
2The term is used informally. In particular, hitting times
are not a distance metric, since they can be asymmetric.
tween the two. Specifically, we adopt the notion
of T-truncated hitting time (Sarkar and Moore,
2007) defined as the expected number of steps
it takes until a random walk of at most T steps
starting at vi reaches vj for the first time:
hT (vj |vi) =
{
0 iff. vj = vi or T=0
1 +
?
vk?V
p(vk|vi)hT?1(vj |vk)
The truncated hitting time hT (vj |vi) can be
approximately computed by sampling M inde-
pendent random walks starting at vi of length T
and computing
h?T (vj |vi) =
1
M
m?
k=1
tk + (1?
m
M
)T (1)
where {t1 . . . tm} are the sampled first-hit times
of random walks which reach vj within T steps
(Sarkar et al, 2008).
The score ?HT (v) of a vertex v /? S to the seed
set S is then defined as the inverse of the aver-
age T -truncated hitting time of random walks
starting at a randomly chosen vertex s ? S:
1
?HT (v)
= hT (v|S) =
1
|S|
?
s?S
hT (v|s) (2)
3 Experiments
We extracted tuples and patterns from the fifth
edition of the Gigaword corpus (Parker et al,
2011), by running a named entity tagger and
extracting all pairs of named entities and ex-
tracting occurring within the same sentence
which do not have another named entity stand-
ing between them. Gold standard seed and test
tuples for a set of relations were obtained from
YAGO (Suchanek et al, 2007). Specifically, we
took all relations for which there are at least
300 tuples, each of which occurs at least once
in the corpus. This resulted in the set of rela-
tions shown in Table 1, plus the development
relation hasWonPrize.
For evaluation, we use the percentile rank of
the median test set element (PRM, see Francois
et al 2007), which reflects the quality of the
773
full produced ranking, not just the top N ele-
ments and is furthermore computable with only
a small set of labeled test tuples 3.
We compare our proposed method based on
hitting times (HT) with two variants of iterative
bootstrapping. The first one (IB1) does not em-
ploy pruning and corresponds to the algorithm
described in Komachi et al (2008). The sec-
ond one (IB2) corresponds to a standard boot-
strapping algorithm which employs pruning af-
ter each step in order to reduce semantic drift.
Specifically, scores are pruned after projecting
from X onto Y and from Y onto X, retaining
only the top N (t) = N0t scores at iteration t and
setting all other scores to zero.
3.1 Parametrizations
The experiments in this section were conducted
on the held out development relation hasWon-
Prize. The ranking produced by both forms of
iterative bootstrapping IB1 and IB2 depend on
the number of iterations, as shown in Figure 1.
IB1 achieves an optimal ranking after just one
iteration and thereafter scores get worse due to
semantic drift. In contrast, pruning helps avoid
semantic drift for IB2, which attains an optimal
score after 2 iterations and achieves relatively
constant scores for several iterations. However,
during iteration 9 an incorrect pattern is kept
and this at once leads to a drastic loss in ac-
curacy, showing that semantic drift is only de-
ferred and not completely eliminated.
Our method HT has parameter T , correspond-
ing to the truncation time, i.e., maximal number
of steps of a random walk. Figure 2 shows the
PRM of our method for different values of T .
Performance gets better as T increases and is
optimal for T = 12, whereas for larger values,
the performance gets slightly worse again. The
figure shows that, if T is large enough (> 5), the
PRM is relatively constant and there is no phe-
nomenon comparable to semantic drift, which
causes instability in the produced rankings.
3other common metrics do not satisfy these conditions.
Figure 1: PRM for iterative bootstrapping with-
out pruning (IB1) and with pruning (IB2). A
lower PRM is better.
Figure 2: PRM for our method based on hitting
times, for different values of the truncation time
parameter T.
3.2 Method Comparison
To evaluate the methods, firstly the parameters
for each method were set to the optimal values
as determined in the previous section. For the
experiments here, we again use 200 randomly
chosen tuples as the seeds for each relation. All
the remaining gold standard tuples are used for
testing.
Table 1 shows the PRM for the three methods.
For a majority of the relations (12/16) HT at-
tains the best, i.e. lowest, PRM, which confirms
that hitting times constitute an accurate way of
measuring the distance of tuples to the seed set.
IB1 and IB2 each perform best on 2/16 of the
relations. A sign test on these results yields that
774
Relation IB1 IB2 HT
created 1.82 1.71 0.803
dealsWith 0.0262 0.107 0.0481
diedIn 30.5 18.4 20.4
directed 0.171 0.238 0.166
hasChild 7.66 32.2 4.26
influences 5.93 5.48 6.60
isAffiliatedTo 1.54 2.01 1.30
isCitizenOf 1.74 1.87 1.68
isLeaderOf 1.37 1.91 0.401
isMarriedTo 4.69 4.14 1.27
isPoliticianOf 0.0117 0.110 0.0409
livesIn 3.17 2.48 1.70
owns 11.0 2.10 2.07
produced 1.55 0.967 0.240
wasBornIn 11.3 9.37 8.42
worksAt 1.52 2.21 0.193
Table 1: PRM in percent for all relations, for all
three models. A lower PRM corresponds to a
better model, with the best score indicated in
bold.
Figure 3: PRM for the three methods, as a func-
tion of the size of the seed set for the relation
created.
HT is better than both IB1 and IB2 at signifi-
cance level ? < 0.01.
Moreover, the ranking produced by HT is sta-
ble and not affected by semantic drift, given that
even where results are worse than for IB1 or
IB2, they are still close to the best performing
method. In contrast, when semantic drift oc-
curs, the performance of IB1 and IB2 can dete-
riorate drastically, e.g. for the worksAt relation,
where both IB1 and IB2 produce rankings that
are a lot worse than the one produced by HT.
3.3 Sensitivity to Seed Set Size
Figure 3 shows the PRM for each of the three
methods as a function of the size of the seed set
for the relation created. For small seed sets, the
performance of the iterative methods can be in-
creased by adding more seeds. However, from
a seed set size of 50 onwards, performance re-
mains relatively constant. In other words, iter-
ative bootstrapping is not benefitting from the
information provided by the additional labeled
data, and thus has a poor learning performance.
In contrast, for our method based on hitting
times, the performance continually improves as
the seed set size is increased. Thus, also in terms
of learning performance, our method is more
sound than iterative bootstrapping.
4 Conclusions
The paper has presented a graph-based method
for seed set expansion which is not susceptible
to semantic drift and on most relations outper-
forms iterative bootstrapping. The method mea-
sures distance between vertices through random
walk hitting times. One property which makes
hitting times an appropriate distance measure
is their ability to reflect the overall connectivity
structure of the graph, in contrast to measures
such as the shortest path between two vertices.
The hitting time will decrease when the num-
ber of paths from the start vertex to the tar-
get vertex increases, when the length of paths
decreases or when the likelihood (weights) of
paths increases. These properties are particu-
larly important when the observed graph edges
must be assumed to be merely a sample of all
plausible edges, possibly perturbated by noise.
This has also been asserted by previous work,
which has shown that hitting times successfully
capture the notion of similarity for other natural
language processing problems such as learning
paraphrases (Kok and Brockett, 2010) and re-
lated problems such as query suggestion (Mei
et al, 2008). Future work will be aimed to-
wards employing our hitting time based method
in combination with a richer feature set.
775
References
Agichtein, E. and Gravano, L. (2000). Snow-
ball: Extracting Relations from Large Plain-
text Collections. In Proceedings of the Fifth
ACM Conference on Digital Libraries.
Brin, S. (1998). Extracting Patterns and Rela-
tions from the World-Wide Web. In Proceed-
ings of the 1998 International Workshop on the
Web and Databases.
Curran, J., Murphy, T., and Scholz, B. (2007).
Minimising Semantic Drift with Mutual Exclu-
sion Bootstrapping. In Proceedings of the 10th
Conference of the Pacific Association for Com-
putational Linguistics.
Francois, F., Pirotte, A., Renders, J., and
Saerens, M. (2007). Random-Walk Computa-
tion of Similarities between Nodes of a Graph
with Application to Collaborative Recommen-
dation. IEEE Transactions on Knowledge and
Data Engineering, 19(3):355 ?369.
Kok, S. and Brockett, C. (2010). Hitting the
Right Paraphrases in Good Time. In Proceed-
ings of the Annual Conference of the North
American Chapter of the Association for Com-
putational Linguistics.
Komachi, M., Kudo, T., Shimbo, M., and Mat-
sumoto, Y. (2008). Graph-based Analysis
of Semantic Drift in Espresso-like Bootstrap-
ping Algorithms. In Proceedings of the Con-
ference on Empirical Methods in Natural Lan-
guage Processing.
McIntosh, T. and Curran, J. (2009). Reduc-
ing Semantic Drift with Bagging and Distri-
butional Similarity. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the
ACL.
Mei, Q., Zhou, D., and Church, K. (2008). Query
Suggestion Using Hitting Time. In Proceed-
ings of the 17th ACM Conference on Informa-
tion and Knowledge Management.
Pantel, P. and Pennacchiotti, M. (2006).
Espresso: Leveraging Generic Patterns for Au-
tomatically Harvesting Semantic Relations. In
Proceedings of the 21st International Confer-
ence on Computational Linguistics and the 44th
Annual Meeting of the Association for Compu-
tational Linguistics.
Parker, R., Graff, D., Kong, J., Chen, K., and
Maeda, K. (2011). English Gigaword Fifth
Edition. Technical report, Linguistic Data
Consortium.
Pasca, M., Lin, D., Bigham, J., Lifchits, A., and
Jain, A. (2006). Organizing and Searching
the World Wide Web of Facts ? Step One: the
One-million Fact Extraction Challenge. In Pro-
ceedings of the 21st National Conference on Ar-
tificial Intelligence (AAAI).
Sarkar, P. and Moore, A. (2007). A Tractable
Approach to Finding Closest Truncated-
commute-time Neighbors in Large Graphs. In
Proceedings of the 23rd Conference on Uncer-
tainty in Artificial Intelligence.
Sarkar, P., Moore, A., and Prakash, A. (2008).
Fast Incremental Proximity Search in Large
Graphs. In Proceedings of the 25th Interna-
tional Conference on Machine Learning.
Suchanek, F., Kasneci, G., and Weikum, G.
(2007). Yago: A Core of Semantic Knowl-
edge. In Proceedings of the International World
Wide Web Conference (WWW).
T. McIntosh (2010). Unsupervised Discovery
of Negative Categories in Lexicon Bootstrap-
ping. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Pro-
cessing.
776
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 11?17,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Temporal Restricted Boltzmann Machines for Dependency Parsing
Nikhil Garg
Department of Computer Science
University of Geneva
Switzerland
nikhil.garg@unige.ch
James Henderson
Department of Computer Science
University of Geneva
Switzerland
james.henderson@unige.ch
Abstract
We propose a generative model based on
Temporal Restricted Boltzmann Machines for
transition based dependency parsing. The
parse tree is built incrementally using a shift-
reduce parse and an RBM is used to model
each decision step. The RBM at the current
time step induces latent features with the help
of temporal connections to the relevant previ-
ous steps which provide context information.
Our parser achieves labeled and unlabeled at-
tachment scores of 88.72% and 91.65% re-
spectively, which compare well with similar
previous models and the state-of-the-art.
1 Introduction
There has been significant interest recently in ma-
chine learning methods that induce generative mod-
els with high-dimensional hidden representations,
including neural networks (Bengio et al, 2003; Col-
lobert and Weston, 2008), Bayesian networks (Titov
and Henderson, 2007a), and Deep Belief Networks
(Hinton et al, 2006). In this paper, we investi-
gate how these models can be applied to dependency
parsing. We focus on Shift-Reduce transition-based
parsing proposed by Nivre et al (2004). In this class
of algorithms, at any given step, the parser has to
choose among a set of possible actions, each repre-
senting an incremental modification to the partially
built tree. To assign probabilities to these actions,
previous work has proposed memory-based classi-
fiers (Nivre et al, 2004), SVMs (Nivre et al, 2006b),
and Incremental Sigmoid Belief Networks (ISBN)
(Titov and Henderson, 2007b). In a related earlier
work, Ratnaparkhi (1999) proposed a maximum en-
tropy model for transition-based constituency pars-
ing. Of these approaches, only ISBNs induce high-
dimensional latent representations to encode parse
history, but suffer from either very approximate or
slow inference procedures.
We propose to address the problem of inference
in a high-dimensional latent space by using an undi-
rected graphical model, Restricted Boltzmann Ma-
chines (RBMs), to model the individual parsing
decisions. Unlike the Sigmoid Belief Networks
(SBNs) used in ISBNs, RBMs have tractable infer-
ence procedures for both forward and backward rea-
soning, which allows us to efficiently infer both the
probability of the decision given the latent variables
and vice versa. The key structural difference be-
tween the two models is that the directed connec-
tions between latent and decision vectors in SBNs
become undirected in RBMs. A complete parsing
model consists of a sequence of RBMs interlinked
via directed edges, which gives us a form of Tempo-
ral Restricted Boltzmann Machines (TRBM) (Tay-
lor et al, 2007), but with the incrementally speci-
fied model structure required by parsing. In this pa-
per, we analyze and contrast ISBNs with TRBMs
and show that the latter provide an accurate and
theoretically sound model for parsing with high-
dimensional latent variables.
2 An ISBN Parsing Model
Our TRBM parser uses the same history-
based probability model as the ISBN
parser of Titov and Henderson (2007b):
P (tree) = ?tP (vt|v1, ..., vt?1), where each
11
Figure 1: An ISBN network. Shaded nodes represent
decision variables and ?H? represents a vector of latent
variables. W (c)HH denotes the weight matrix for directed
connection of type c between two latent vectors.
vt is a parser decision of the type Left-Arc,
Right-Arc, Reduce or Shift. These decisions are fur-
ther decomposed into sub-decisions, as for example
P (Left-Arc|v1, ..., vt?1)P (Label|Left-Arc, v1, ..., vt?1).
The TRBMs and ISBNs model these probabilities.
In the ISBN model shown in Figure 1, the de-
cisions are shown as boxes and the sub-decisions
as shaded circles. At each decision step, the ISBN
model also includes a vector of latent variables, de-
noted by ?H?, which act as latent features of the
parse history. As explained in (Titov and Hender-
son, 2007b), the temporal connections between la-
tent variables are constructed to take into account the
structural locality in the partial dependency struc-
ture. The model parameters are learned by back-
propagating likelihood gradients.
Because decision probabilities are conditioned on
the history, once a decision is made the correspond-
ing variable becomes observed, or visible. In an
ISBN, the directed edges to these visible variables
and the large numbers of heavily inter-connected la-
tent variables make exact inference of decision prob-
abilities intractable. Titov and Henderson (2007a)
proposed two approximation procedures for infer-
ence. The first was a feed forward approximation
where latent variables were allowed to depend only
on their parent variables, and hence did not take into
account the current or future observations. Due to
this limitation, the authors proposed to make latent
variables conditionally dependent also on a set of
explicit features derived from the parsing history,
specifically, the base features defined in (Nivre et al,
2006b). As shown in our experiments, this addition
results in a big improvement for the parsing task.
The second approximate inference procedure,
called the incremental mean field approximation, ex-
tended the feed-forward approximation by updating
the current time step?s latent variables after each
sub-decision. Although this approximation is more
accurate than the feed-forward one, there is no ana-
lytical way to maximize likelihood w.r.t. the means
of the latent variables, which requires an iterative
numerical method and thus makes inference very
slow, restricting the model to only shorter sentences.
3 Temporal Restricted Boltzmann
Machines
In the proposed TRBM model, RBMs provide an an-
alytical way to do exact inference within each time
step. Although information passing between time
steps is still approximated, TRBM inference is more
accurate than the ISBN approximations.
3.1 Restricted Boltzmann Machines (RBM)
An RBM is an undirected graphical model with a
set of binary visible variables v, a set of binary la-
tent variables h, and a weight matrix W for bipar-
tite connections between v and h. The probability
of an RBM configuration is given by: p(v,h) =
(1/Z)e?E(v,h) where Z is the partition function and
E is the energy function defined as:
E(v,h) = ??iaivi ? ?jbjhj ? ?i,jvihjwij
where ai and bj are biases for corresponding visi-
ble and latent variables respectively, and wij is the
symmetric weight between vi and hj . Given the vis-
ible variables, the latent variables are conditionally
independent of each other, and vice versa:
p(hj = 1|v) = ?(bj +?iviwij) (1)
p(vi = 1|h) = ?(ai +?jhjwij) (2)
where ?(x) = 1/(1 + e?x) (the logistic sigmoid).
RBM based models have been successfully used
in image and video processing, such as Deep Belief
Networks (DBNs) for recognition of hand-written
digits (Hinton et al, 2006) and TRBMs for mod-
eling motion capture data (Taylor et al, 2007). De-
spite their success, RBMs have seen limited use in
the NLP community. Previous work includes RBMs
for topic modeling in text documents (Salakhutdinov
and Hinton, 2009), and Temporal Factored RBM for
language modeling (Mnih and Hinton, 2007).
3.2 Proposed TRBM Model Structure
TRBMs (Taylor et al, 2007) can be used to model
sequences where the decision at each step requires
some context information from the past. Figure 2
12
Figure 2: Proposed TRBM Model. Edges with no arrows
represent undirected RBM connections. The directed
temporal connections between time steps contribute a
bias to the latent layer inference in the current step.
shows our proposed TRBM model with latent to
latent connections between time steps. Each step
has an RBM with weights WRBM composed of
smaller weight matrices corresponding to different
sub-decisions. For instance, for the action Left-Arc,
WRBM consists of RBM weights between the la-
tent vector and the sub-decisions: ?Left-Arc? and
?Label?. Similarly, for the action Shift, the sub-
decisions are ?Shift?, ?Part-of-Speech? and ?Word?.
The probability distribution of a TRBM is:
p(vT1 ,hT1 ) = ?Tt=1p(vt,ht|h(1), ...,h(C))
where vT1 denotes the set of visible vectors from time
steps 1 to T i.e. v1 to vT . The notation for latent
vectors h is similar. h(c) denotes the latent vector
in the past time step that is connected to the current
latent vector through a connection of type c. To sim-
plify notation, we will denote the past connections
{h(1), ...,h(C)} by historyt. The conditional distri-
bution of the RBM at each time step is given by:
p(vt,ht|historyt) = (1/Z)exp(?iaivti +?i,jvtihtjwij
+?j(bj +?c,lw(c)HHljh
(c)
l )htj)
where vti and htj denote the ith visible and jth latent
variable respectively at time step t. h(c)l denotes a
latent variable in the past time step, and w(c)HHlj de-
notes the weight of the corresponding connection.
3.3 TRBM Likelihood and Inference
Section 3.1 describes an RBM where visible vari-
ables can take binary values. In our model, similar to
(Salakhutdinov et al, 2007), we have multi-valued
visible variables which we represent as one-hot bi-
nary vectors and model via a softmax distribution:
p(vtk = 1|ht) =
exp(ak +
?
j htjwkj)
?
i exp(ai +
?
j htjwij)
(3)
Latent variable inference is similar to equation 1
with an additional bias due to the temporal connec-
tions.
?tj = p(htj = 1|vt, historyt)
= ??(bj +?c,lw(c)HHljh
(c)
l +?ivtiwij)?
? ?(b?j +?ivtiwij), (4)
b?j = bj +?c,lw
(c)
HHlj
?(c)l .
Here, ? denotes the mean of the corresponding la-
tent variable. To keep inference tractable, we do not
do any backward reasoning across directed connec-
tions to update ?(c). Thus, the inference procedure
for latent variables takes into account both the parse
history and the current observation, but no future ob-
servations.
The limited set of possible values for the visi-
ble layer makes it possible to marginalize out latent
variables in linear time to compute the exact likeli-
hood. Let vt(k) denote a vector with vtk = 1 and
vti(i 6=k) = 0. The conditional probability of a sub-
decision is:
p(vt(k)|historyt) = (1/Z)?hte?E(v
t(k),ht) (5)
= (1/Z)eak?j(1 + eb
?
j+wkj),
where Z = ?i?visibleeai?j?latent(1 + eb
?
j+wij ).
We actually perform this calculation once for
each sub-decision, ignoring the future sub-decisions
in that time step. This is a slight approximation,
but avoids having to compute the partition function
over all possible combinations of values for all sub-
decisions.1
The complete probability of a derivation is:
p(vT1 ) = p(v1).p(v2|history2)...p(vT |historyT )
3.4 TRBM Training
The gradient of an RBM is given by:
? log p(v)/?wij = ?vihj?data ? ?vihj?model (6)
where ??d denotes the expectation under distribu-
tion d. In general, computing the exact gradient
is intractable and previous work proposed a Con-
trastive Divergence (CD) based learning procedure
that approximates the above gradient using only one
step reconstruction (Hinton, 2002). Fortunately, our
model has only a limited set of possible visible val-
ues, which allows us to use a better approximation
by taking the derivative of equation 5:
1In cases where computing the partition function is still not
feasible (for instance, because of a large vocabulary), sampling
methods could be used. However, we did not find this to be
necessary.
13
? log p(vt(k)|historyt)
?wij
=
(?ki ? p(vt(i)|historyt)) ?(b
?
j + wij)
(7)
Further, the weights on the temporal connections
are learned by back-propagating the likelihood gra-
dients through the directed links between steps.
The back-proped gradient from future time steps is
also used to train the current RBM weights. This
back-propagation is similar to the Recurrent TRBM
model of Sutskever et al (2008). However, unlike
their model, we do not use CD at each step to com-
pute gradients.
3.5 Prediction
We use the same beam-search decoding strategy as
used in (Titov and Henderson, 2007b). Given a
derivation prefix, its partial parse tree and associ-
ated TRBM, the decoder adds a step to the TRBM
for calculating the probabilities of hypothesized next
decisions using equation 5. If the decoder selects a
decision for addition to the candidate list, then the
current step?s latent variable means are inferred us-
ing equation 4, given that the chosen decision is now
visible. These means are then stored with the new
candidate for use in subsequent TRBM calculations.
4 Experiments & Results
We used syntactic dependencies from the English
section of the CoNLL 2009 shared task dataset
(Hajic? et al, 2009). Standard splits of training, de-
velopment and test sets were used. To handle word
sparsity, we replaced all the (POS, word) pairs with
frequency less than 20 in the training set with (POS,
UNKNOWN), giving us only 4530 tag-word pairs.
Since our model can work only with projective trees,
we used MaltParser (Nivre et al, 2006a) to projec-
tivize/deprojectivize the training input/test output.
4.1 Results
Table 1 lists the labeled (LAS) and unlabeled (UAS)
attachment scores. Row a shows that a simple ISBN
model without features, using feed forward infer-
ence procedure, does not work well. As explained
in section 2, this is expected since in the absence of
explicit features, the latent variables in a given layer
do not take into account the observations in the pre-
vious layers. The huge improvement in performance
Model LAS UAS
a. ISBN w/o features 38.38 54.52
b. ISBN w/ features 88.65 91.44
c. TRBM w/o features 86.01 89.78
d. TRBM w/ features 88.72 91.65
e. MST (McDonald et al, 2005) 87.07 89.95
f . Malt??AE (Hall et al, 2007) 85.96 88.64
g. MSTMalt (Nivre and McDonald, 2008) 87.45 90.22
h. CoNLL 2008 #1 (Johansson and Nugues, 2008) 90.13 92.45
i. ensemble3100% (Surdeanu and Manning, 2010) 88.83 91.47
j. CoNLL 2009 #1 (Bohnet, 2009) 89.88 unknown
Table 1: LAS and UAS for different models.
on adding the features (row b) shows that the feed
forward inference procedure for ISBNs relies heav-
ily on these feature connections to compensate for
the lack of backward inference.
The TRBM model avoids this problem as the in-
ference procedure takes into account the current ob-
servation, which makes the latent variables much
more informed. However, as row c shows, the
TRBM model without features falls a bit short of
the ISBN performance, indicating that features are
indeed a powerful substitute for backward inference
in sequential latent variable models. TRBM mod-
els would still be preferred in cases where such fea-
ture engineering is difficult or expensive, or where
the objective is to compute the latent features them-
selves. For a fair comparison, we add the same set
of features to the TRBM model (row d) and the per-
formance improves by about 2% to reach the same
level (non-significantly better) as ISBN with fea-
tures. The improved inference in TRBM does how-
ever come at the cost of increased training and test-
ing time. Keeping the same likelihood convergence
criteria, we could train the ISBN in about 2 days and
TRBM in about 5 days on a 3.3 GHz Xeon proces-
sor. With the same beam search parameters, the test
time was about 1.5 hours for ISBN and about 4.5
hours for TRBM. Although more code optimization
is possible, this trend is likely to remain.
We also tried a Contrastive Divergence based
training procedure for TRBM instead of equation
7, but that resulted in about an absolute 10% lower
LAS. Further, we also tried a very simple model
without latent variables where temporal connections
are between decision variables themselves. This
14
model gave an LAS of only 60.46%, which indi-
cates that without latent variables, it is very difficult
to capture the parse history.
For comparison, we also include the performance
numbers for some state-of-the-art dependency pars-
ing systems. Surdeanu and Manning (2010) com-
pare different parsing models using CoNLL 2008
shared task dataset (Surdeanu et al, 2008), which
is the same as our dataset. Rows e? i show the per-
formance numbers of some systems as mentioned in
their paper. Row j shows the best syntactic model
in CoNLL 2009 shared task. The TRBM model has
only 1.4% lower LAS and 0.8% lower UAS com-
pared to the best performing model.
4.2 Latent Layer Analysis
We analyzed the latent layers in our models to see if
they captured semantic patterns. A latent layer is a
vector of 100 latent variables. Every Shift operation
gives a latent representation for the corresponding
word. We took all the verbs in the development set2
and partitioned their representations into 50 clus-
ters using the k-means algorithm. Table 2 shows
some partitions for the TRBM model. The partitions
look semantically meaningful but to get a quantita-
tive analysis, we computed pairwise semantic simi-
larity between all word pairs in a given cluster and
aggregated this number over all the clusters. The se-
mantic similarity was calculated using two different
similarity measures on the wordnet corpus (Miller
et al, 1990): path and lin. path similarity is a score
between 0 and 1, equal to the inverse of the shortest
path length between the two word senses. lin simi-
larity (Lin, 1998) is a score between 0 and 1 based
on the Information Content of the two word senses
and of the Least Common Subsumer. Table 3 shows
the similarity scores.3 We observe that TRBM la-
tent representations give a slightly better clustering
than ISBN models. Again, this is because of the fact
that the inference procedure in TRBMs takes into ac-
count the current observation. However, at the same
time, the similarity numbers for ISBN with features
2Verbs are words corresponding to POS tags: VB, VBD,
VBG, VBN, VBP, VBZ. We selected verbs as they have good
coverage in Wordnet.
3To account for randomness in k-means clustering, the clus-
tering was performed 10 times with random initializations, sim-
ilarity scores were computed for each run and a mean was taken.
Cluster 1 Cluster 2 Cluster 3 Cluster 4
says needed pressing renewing
contends expected bridging cause
adds encouraged curing repeat
insists allowed skirting broken
remarked thought tightening extended
Table 2: K-means clustering of words according to their
TRBM latent representations. Duplicate words in the
same cluster are not shown.
Model path lin
ISBN w/o features 0.228 0.381
ISBN w/features 0.366 0.466
TRBM w/o features 0.386 0.487
TRBM w/ features 0.390 0.489
Table 3: Wordnet similarity scores for clusters given by
different models.
are not very low, which shows that features are a
powerful way to compensate for the lack of back-
ward inference. This is in agreement with their good
performance on the parsing task.
5 Conclusions & Future Work
We have presented a Temporal Restricted Boltz-
mann Machines based model for dependency pars-
ing. The model shows how undirected graphical
models can be used to generate latent representa-
tions of local parsing actions, which can then be
used as features for later decisions.
The TRBM model for dependency parsing could
be extended to a Deep Belief Network by adding
one more latent layer on top of the existing one
(Hinton et al, 2006). Furthermore, as done for
unlabeled images (Hinton et al, 2006), one could
learn high-dimensional features from unlabeled text,
which could then be used to aid parsing. Parser la-
tent representations could also help other tasks such
as Semantic Role Labeling (Henderson et al, 2008).
A free distribution of our implementation is avail-
able at http://cui.unige.ch/
?
garg.
Acknowledgments
This work was partly funded by Swiss NSF grant
200021 125137 and European Community FP7
grant 216594 (CLASSiC, www.classic-project.org).
15
References
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. 2003.
A neural probabilistic language model. The Journal of
Machine Learning Research, 3:1137?1155.
B. Bohnet. 2009. Efficient parsing of syntactic and
semantic dependency structures. In Proceedings of
the Thirteenth Conference on Computational Natural
Language Learning: Shared Task, CoNLL ?09, pages
67?72. Association for Computational Linguistics.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks
with multitask learning. In Proceedings of the 25th
international conference on Machine learning, pages
160?167. ACM.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara, M.A.
Mart??, L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?,
J. ?Ste?pa?nek, et al 2009. The CoNLL-2009 shared
task: Syntactic and semantic dependencies in multiple
languages. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning:
Shared Task, pages 1?18. Association for Computa-
tional Linguistics.
J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi,
M. Nilsson, and M. Saers. 2007. Single malt or
blended? A study in multilingual parser optimiza-
tion. In Proceedings of the CoNLL Shared Task Ses-
sion of EMNLP-CoNLL 2007, pages 933?939. Associ-
ation for Computational Linguistics.
J. Henderson, P. Merlo, G. Musillo, and I. Titov. 2008.
A latent variable model of synchronous parsing for
syntactic and semantic dependencies. In Proceedings
of the Twelfth Conference on Computational Natural
Language Learning, pages 178?182. Association for
Computational Linguistics.
G.E. Hinton, S. Osindero, and Y.W. Teh. 2006. A fast
learning algorithm for deep belief nets. Neural com-
putation, 18(7):1527?1554.
G.E. Hinton. 2002. Training products of experts by min-
imizing contrastive divergence. Neural Computation,
14(8):1771?1800.
R. Johansson and P. Nugues. 2008. Dependency-
based syntactic-semantic analysis with PropBank and
NomBank. In Proceedings of the Twelfth Conference
on Computational Natural Language Learning, pages
183?187. Association for Computational Linguistics.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the 15th International
Conference on Machine Learning, volume 1, pages
296?304.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proceedings of the conference on
Human Language Technology and Empirical Methods
in Natural Language Processing, pages 523?530. As-
sociation for Computational Linguistics.
G.A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K.J. Miller. 1990. Introduction to wordnet: An on-
line lexical database. International Journal of lexicog-
raphy, 3(4):235.
A. Mnih and G. Hinton. 2007. Three new graphical mod-
els for statistical language modelling. In Proceedings
of the 24th international conference on Machine learn-
ing, pages 641?648. ACM.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. Pro-
ceedings of ACL-08: HLT, pages 950?958.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of CoNLL, pages
49?56.
J. Nivre, J. Hall, and J. Nilsson. 2006a. MaltParser: A
data-driven parser-generator for dependency parsing.
In Proceedings of LREC, volume 6.
J. Nivre, J. Hall, J. Nilsson, G. Eryiit, and S. Marinov.
2006b. Labeled pseudo-projective dependency pars-
ing with support vector machines. In Proceedings
of the Tenth Conference on Computational Natural
Language Learning, pages 221?225. Association for
Computational Linguistics.
A. Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1):151?175.
R. Salakhutdinov and G. Hinton. 2009. Replicated soft-
max: an undirected topic model. Advances in Neural
Information Processing Systems, 22.
R. Salakhutdinov, A. Mnih, and G. Hinton. 2007. Re-
stricted Boltzmann machines for collaborative filter-
ing. In Proceedings of the 24th international confer-
ence on Machine learning, page 798. ACM.
M. Surdeanu and C.D. Manning. 2010. Ensemble mod-
els for dependency parsing: cheap and good? In Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 649?652.
Association for Computational Linguistics.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez, and
J. Nivre. 2008. The CoNLL-2008 shared task on
joint parsing of syntactic and semantic dependencies.
In Proceedings of the Twelfth Conference on Compu-
tational Natural Language Learning, pages 159?177.
Association for Computational Linguistics.
I. Sutskever, G. Hinton, and G. Taylor. 2008. The recur-
rent temporal restricted boltzmann machine. In NIPS,
volume 21, page 2008.
G.W. Taylor, G.E. Hinton, and S.T. Roweis. 2007.
Modeling human motion using binary latent variables.
Advances in neural information processing systems,
19:1345.
16
I. Titov and J. Henderson. 2007a. Constituent parsing
with incremental sigmoid belief networks. In Pro-
ceedings of the 45th Annual Meeting on Association
for Computational Linguistics, volume 45, page 632.
I. Titov and J. Henderson. 2007b. Fast and robust mul-
tilingual dependency parsing with a generative latent
variable model. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL, pages 947?951.
17
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 299?304,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Scaling up Automatic Cross-Lingual Semantic Role Annotation
Lonneke van der Plas
Department of Linguistics
University of Geneva
Geneva, Switzerland
Paola Merlo
Department of Linguistics
University of Geneva
Geneva, Switzerland
{Lonneke.vanderPlas,Paola.Merlo,James.Henderson}@unige.ch
James Henderson
Department of Computer Science
University of Geneva
Geneva, Switzerland
Abstract
Broad-coverage semantic annotations for
training statistical learners are only available
for a handful of languages. Previous ap-
proaches to cross-lingual transfer of seman-
tic annotations have addressed this problem
with encouraging results on a small scale. In
this paper, we scale up previous efforts by us-
ing an automatic approach to semantic anno-
tation that does not rely on a semantic on-
tology for the target language. Moreover,
we improve the quality of the transferred se-
mantic annotations by using a joint syntactic-
semantic parser that learns the correlations be-
tween syntax and semantics of the target lan-
guage and smooths out the errors from auto-
matic transfer. We reach a labelled F-measure
for predicates and arguments of only 4% and
9% points, respectively, lower than the upper
bound from manual annotations.
1 Introduction
As data-driven techniques tackle more and more
complex natural language processing tasks, it be-
comes increasingly unfeasible to use complete, ac-
curate, hand-annotated data on a large scale for
training models in all languages. One approach to
addressing this problem is to develop methods that
automatically generate annotated data by transfer-
ring annotations in parallel corpora from languages
for which this information is available to languages
for which these data are not available (Yarowsky et
al., 2001; Fung et al, 2007; Pado? and Lapata, 2009).
Previous work on the cross-lingual transfer of se-
mantic annotations (Pado?, 2007; Basili et al, 2009)
has produced annotations of good quality for test
sets that were carefully selected based on seman-
tic ontologies on the source and target side. It has
been suggested that these annotations could be used
to train semantic role labellers (Basili et al, 2009).
In this paper, we generate high-quality broad-
coverage semantic annotations using an automatic
approach that does not rely on a semantic ontol-
ogy for the target language. Furthermore, to our
knowledge, we report the first results on using joint
syntactic-semantic learning to improve the quality
of the semantic annotations from automatic cross-
lingual transfer. Results on correlations between
syntax and semantics found in previous work (Merlo
and van der Plas, 2009; Lang and Lapata, 2010) have
led us to make use of the available syntactic anno-
tations on the target language. We use the seman-
tic annotations resulting from cross-lingual transfer
combined with syntactic annotations to train a joint
syntactic-semantic parser for the target language,
which, in turn, re-annotates the corpus (See Fig-
ure 1). We show that the semantic annotations pro-
duced by this parser are of higher quality than the
data on which it was trained.
Given our goal of producing broad-coverage an-
notations in a setting based on an aligned corpus,
our choices of formal representation and of labelling
scheme differ from previous work (Pado?, 2007;
Basili et al, 2009). We choose a dependency repre-
sentation both for the syntax and semantics because
relations are expressed as direct arcs between words.
This representation allows cross-lingual transfer to
use word-based alignments directly, eschewing the
need for complex constituent-alignment algorithms.
299
Train?a?French?syntacticparser
?Transfer?semantic?annotationsfrom?EN?to?FR?using?wordalignments
EN?syntactic?semanticannotations
EN?FR?word?aligneddata
FR?syntacticannotations
FR?semanticannotations evaluatio
n
Train?Frenchjoint?syntactic?semantic?parser
evaluatio
n
FR?syntacticannotations
FR?semanticannotations
Figure 1: System overview
We choose the semantic annotation scheme defined
by Propbank, because it has broad coverage and in-
cludes an annotated corpus, contrary to other avail-
able resources such as FrameNet (Fillmore et al,
2003) and is the preferred annotation scheme for a
joint syntactic-semantic setting (Merlo and van der
Plas, 2009). Furthermore, Monachesi et al (2007)
showed that the PropBank annotation scheme can be
used for languages other than English directly.
2 Cross-lingual semantic transfer
Data-driven induction of semantic annotation based
on parallel corpora is a well-defined and feasible
task, and it has been argued to be particularly suit-
able to semantic role label annotation because cross-
lingual parallelism improves as one moves to more
abstract linguistic levels of representation. While
Hwa et al (2002; 2005) find that direct syntactic de-
pendency parallelism between English and Spanish
concerns 37% of dependency links, Pado? (2007) re-
ports an upper-bound mapping correspondence cal-
culated on gold data of 88% F-measure for in-
dividual semantic roles, and 69% F-measure for
whole scenario-like semantic frames. Recently, Wu
and Fung (2009a; 2009b) also show that semantic
roles help in statistical machine translation, capi-
talising on a study of the correspondence between
English and Chinese which indicates that 84% of
roles transfer directly, for PropBank-style annota-
tions. These results indicate high correspondence
across languages at a shallow semantic level.
Based on these results, our transfer of semantic
annotations from English sentences to their French
translations is based on a very strong mapping hy-
pothesis, adapted from the Direct Correspondence
Assumption for syntactic dependency trees by Hwa
et al (2005).
Direct Semantic Transfer (DST) For any
pair of sentences E and F that are transla-
tions of each other, we transfer the seman-
tic relationship R(xE , yE) to R(xF , yF ) if
and only if there exists a word-alignment
between xE and xF and between yE and
yF , and we transfer the semantic property
P (xE) to P (xF ) if and only if there exists
a word-alignment between xE and xF .
The relationships which we transfer are semantic
role dependencies and the properties are predicate
senses. We introduce one constraint to the direct se-
mantic transfer. Because the semantic annotations in
the target language are limited to verbal predicates,
we only transfer predicates to words the syntactic
parser has tagged as a verb.
As reported by Hwa et al (2005), the direct cor-
respondence assumption is a strong hypothesis that
is useful to trigger a projection process, but will not
work correctly for several cases.
We used a filter to remove obviously incomplete
annotations. We know from the annotation guide-
lines used to annotate the French gold sentences that
all verbs, except modals and realisations of the verb
e?tre, should receive a predicate label. We define a
filter that removes sentences with missing predicate
labels based on PoS-information in the French sen-
tence.
2.1 Learning joint syntactic-semantic
structures
We know from previous work that there is a strong
correlation between syntax and semantics (Merlo
and van der Plas, 2009), and that this correla-
tion has been successfully applied for the unsuper-
vised induction of semantic roles (Lang and Lap-
ata, 2010). However, previous work in machine
translation leads us to believe that transferring the
correlations between syntax and semantics across
languages would be problematic due to argument-
structure divergences (Dorr, 1994). For example,
the English verb like and the French verb plaire do
not share correlations between syntax and seman-
tics. The verb like takes an A0 subject and an A1
300
direct object, whereas the verb plaire licences an A1
subject and an A0 indirect object.
We therefore transfer semantic roles cross-
lingually based only on lexical alignments and add
syntactic information after transfer. In Figure 1, we
see that cross-lingual transfer takes place at the se-
mantic level, a level that is more abstract and known
to port relatively well across languages, while the
correlations with syntax, that are known to diverge
cross-lingually, are learnt on the target language
only. We train a joint syntactic-semantic parser
on the combination of the two linguistic levels that
learns the correlations between these structures in
the target language and is able to smooth out errors
from automatic transfer.
3 Experiments
We used two statistical parsers in our transfer of
semantic annotations from English to French, one
for syntactic parsing and one for joint syntactic-
semantic parsing. In addition, we used several cor-
pora.
3.1 The statistical parsers
For our syntactic-semantic parsing model, we use
a freely-available parser (Henderson et al, 2008;
Titov et al, 2009). The probabilistic model is a joint
generative model of syntactic and semantic depen-
dencies that maximises the joint probability of the
syntactic and semantic dependencies, while building
two separate structures.
For the French syntactic parser, we used the de-
pendency parser described in Titov and Hender-
son (2007). We train the parser on the dependency
version of the French Paris treebank (Candito et al,
2009), achieving 87.2% labelled accuracy on this
data set.
3.2 Data
To transfer semantic annotation from English to
French, we used the Europarl corpus (Koehn,
2003)1. We word-align the English sentences to the
French sentences automatically using GIZA++ (Och
1As is usual practice in preprocessing for automatic align-
ment, the datasets were tokenised and lowercased and only sen-
tence pairs corresponding to a one-to-one sentence alignment
with lengths ranging from one to 40 tokens on both French and
English sides were considered.
and Ney, 2003) and include only intersective align-
ments. Furthermore, because translation shifts are
known to pose problems for the automatic projection
of semantic roles across languages (Pado?, 2007), we
select only those parallel sentences in Europarl that
are direct translations from English to French, or
vice versa. In the end, we have a word-aligned par-
allel corpus of 276-thousand sentence pairs.
Syntactic annotation is available for French. The
French Treebank (Abeille? et al, 2003) is a treebank
of 21,564 sentences annotated with constituency an-
notation. We use the automatic dependency conver-
sion of the French Treebank into dependency format
provided to us by Candito and Crabbe? and described
in Candito et al (2009).
The Penn Treebank corpus (Marcus et al, 1993)
merged with PropBank labels (Palmer et al, 2005)
and NomBank labels (Meyers, 2007) is used to train
the syntactic-semantic parser described in Subsec-
tion 3.1 to annotate the English part of the parallel
corpus.
3.3 Test sets
For testing, we used the hand-annotated data de-
scribed in (van der Plas et al, 2010). One-thousand
French sentences are extracted randomly from our
parallel corpus without any constraints on the se-
mantic parallelism of the sentences, unlike much
previous work. We randomly split those 1000 sen-
tences into test and development set containing 500
sentences each.
4 Results
We evaluate our methods for automatic annotation
generation twice: once after the transfer step, and
once after joint syntactic-semantic learning. The
comparison of these two steps will tell us whether
the joint syntactic-semantic parser is able to improve
semantic annotations by learning from the syntactic
annotations available. We evaluate the models on
unrestricted test sets2 to determine if our methods
scale up.
Table 1 shows the results of automatically an-
notating French sentences with semantic role an-
notation. The first set of columns of results re-
2Due to filtering, the test set for the transfer (filter) model is
smaller and not directly comparable to the other three models.
301
Predicates Arguments (given predicate)
Labelled Unlabelled Labelled Unlabelled
Prec Rec F Prec Rec F Prec Rec F Prec Rec F
1 Transfer (no filter) 50 31 38 91 55 69 61 48 54 72 57 64
2 Transfer (filter) 51 46 49 92 84 88 65 51 57 76 59 67
3 Transfer+parsing (no filter) 71 29 42 97 40 57 77 57 65 87 64 74
4 Transfer+parsing (filter) 61 50 55 95 78 85 71 52 60 83 61 70
5 Inter-annotator agreement 61 57 59 97 89 93 73 75 74 88 91 89
Table 1: Percent recall, precision, and F-measure for predicates and for arguments given the predicate, for the four
automatic annotation models and the manual annotation.
ports labelling and identification of predicates and
the second set of columns reports labelling and iden-
tification of arguments, respectively, for the predi-
cates that are identified. The first two rows show
the results when applying direct semantic transfer.
Rows three and four show results when using the
joint syntactic-semantic parser to re-annotate the
sentences. For both annotation models we show re-
sults when using the filter described in Section 2 and
without the filter.
The most striking result that we can read from
Table 1 is that the joint syntactic-semantic learning
step results in large improvements, especially for
argument labelling, where the F-measure increases
from 54% to 65% for the unfiltered data. The parser
is able to outperform the quality of the semantic
data on which it was trained by using the infor-
mation contained in the syntax. This result is in
accordance with results reported in Merlo and Van
der Plas (2009) and Lang and Lapata (2010), where
the authors find a high correlation between syntactic
functions and PropBank semantic roles.
Filtering improves the quality of the transferred
annotations. However, when training a parser on the
annotations we see that filtering only results in better
recall scores for predicate labelling. This is not sur-
prising given that the filters apply to completeness in
predicate labelling specifically. The improvements
from joint syntactic-semantic learning for argument
labelling are largest for the unfiltered setting, be-
cause the parser has access to larger amounts of data.
The filter removes 61% of the data.
As an upper bound we take the inter-annotator
agreement for manual annotation on a random set
of 100 sentences (van der Plas et al, 2010), given
in the last row of Table 1. The parser reaches an
F-measure on predicate labelling of 55% when us-
ing filtered data, which is very close to the up-
per bound (59%). The upper bound for argument
inter-annotator agreement is an F-measure of 74%.
The parser trained on unfiltered data reaches an
F-measure of 65%. These results on unrestricted
test sets and their comparison to manual annotation
show that we are able to scale up cross-lingual se-
mantic role annotation.
5 Discussion and error analysis
A more detailed analysis of the distribution of im-
provements over the types of roles further strength-
ens the conclusion that the parser learns the corre-
lations between syntax and semantics. It is a well-
known fact that there exists a strong correlation be-
tween syntactic function and semantic role for the
A0 and A1 arguments: A0s are commonly mapped
onto subjects and A1s are often realised as direct ob-
jects (Lang and Lapata, 2010). It is therefore not
surprising that the F-measure on these types of ar-
guments increases by 12% and 15%, respectively,
after joint-syntactic semantic learning. Since these
arguments make up 65% of the roles, this introduces
a large improvement. In addition, we find improve-
ments of more than 10% on the following adjuncts:
AM-CAU, AM-LOC, AM-MNR, and AM-MOD that to-
gether comprise 9% of the data.
With respect to predicate labelling, comparison
of the output after transfer with the output after
parsing (on the development set) shows how the
parser smooths out transfer errors and how inter-
lingual divergences can be solved by making use
of the variations we find intra-lingually. An exam-
ple is given in Figure 2. The first line shows the
predicate-argument structure given by the English
302
EN (source) Postal [A1 services] [AM-MOD must] [CONTINUE.01 continue] [C-A1 to] be public services.
FR (transfer) Les [A1services] postaux [AM-MOD doivent] [CONTINUE.01rester] des services publics.
FR (parsed) Les [A1 services] postaux [AM-MOD doivent] [REMAIN.01rester] des [A3 services] publics.
Figure 2: Differences in predicate-argument labelling after transfer and after parsing
syntactic-semantic parser to the English sentence.
The second line shows the French translation and
the predicate-argument structure as it is transferred
cross-lingually following the method described in
Section 2. Transfer maps the English predicate la-
bel CONTINUE.01 onto the French verb rester, be-
cause these two verbs are aligned. The first oc-
currence of services is aligned to the first occur-
rence of services in the English sentence and gets
the A1 label. The second occurrence of services
gets no argument label, because there is no align-
ment between the C-A1 argument to, the head of
the infinitival clause, and the French word services.
The third line shows the analysis resulting from the
syntactic-semantic parser that has been trained on a
corpus of French sentences labelled with automat-
ically transferred annotations and syntactic annota-
tions. The parser has access to several labelled ex-
amples of the predicate-argument structure of rester,
which in many other cases is translated with remain
and has the same predicate-argument structure as
rester. Consequently, the parser re-labels the verb
with REMAIN.01 and labels the argument with A3.
Because the languages and annotation framework
adopted in previous work are not directly compara-
ble to ours, and their methods have been evaluated
on restricted test sets, results are not strictly com-
parable. But for completeness, recall that our best
result for predicate identification is an F-measure
of 55% accompanied with an F-measure of 60%
for argument labelling. Pado? (2007) reports a 56%
F-measure on transferring FrameNet roles, know-
ing the predicate, from an automatically parsed and
semantically annotated English corpus. Pado? and
Pitel (2007), transferring semantic annotation to
French, report a best result of 57% F-measure for
argument labelling given the predicate. Basili et
al. (2009), in an approach based on phrase-based
machine translation to transfer FrameNet-like anno-
tation from English to Italian, report 42% recall in
identifying predicates and an aggregated 73% recall
of identifying predicates and roles given these pred-
icates. They do not report an unaggregated number
that can be compared to our 60% argument labelling.
In a recent paper, Annesi and Basili (2010) improve
the results from Basili et al (2009) by 11% using
Hidden Markov Models to support the automatic
semantic transfer. Johansson and Nugues (2006)
trained a FrameNet-based semantic role labeller for
Swedish on annotations transferred cross-lingually
from English parallel data. They report 55% F-
measure for argument labelling given the frame on
150 translated example sentences.
6 Conclusions
In this paper, we have scaled up previous efforts of
annotation by using an automatic approach to se-
mantic annotation transfer in combination with a
joint syntactic-semantic parsing architecture. We
propose a direct transfer method that requires nei-
ther manual intervention nor a semantic ontology for
the target language. This method leads to semanti-
cally annotated data of sufficient quality to train a
syntactic-semantic parser that further improves the
quality of the semantic annotation by joint learning
of syntactic-semantic structures on the target lan-
guage. The labelled F-measure of the resulting an-
notations for predicates is only 4% point lower than
the upper bound and the resulting annotations for ar-
guments only 9%.
Acknowledgements
The research leading to these results has received
funding from the EU FP7 programme (FP7/2007-
2013) under grant agreement nr 216594 (CLAS-
SIC project: www.classic-project.org), and from the
Swiss NSF under grant 122643.
References
A. Abeille?, L. Cle?ment, and F. Toussenel. 2003. Building
a treebank for French. In Treebanks: Building and
Using Parsed Corpora. Kluwer Academic Publishers.
303
P. Annesi and R. Basili. 2010. Cross-lingual alignment
of FrameNet annotations through Hidden Markov
Models. In Proceedings of CICLing.
R. Basili, D. De Cao, D. Croce, B. Coppola, and A. Mos-
chitti, 2009. Computational Linguistics and Intelli-
gent Text Processing, chapter Cross-Language Frame
Semantics Transfer in Bilingual Corpora, pages 332?
345. Springer Berlin / Heidelberg.
M.-H. Candito, B. Crabbe?, P. Denis, and F. Gue?rin. 2009.
Analyse syntaxique du franc?ais : des constituants
aux de?pendances. In Proceedings of la Confe?rence
sur le Traitement Automatique des Langues Naturelles
(TALN?09), Senlis, France.
B. Dorr. 1994. Machine translation divergences: A for-
mal description and proposed solution. Computational
Linguistics, 20(4):597?633.
C. J. Fillmore, R. Johnson, and M.R.L. Petruck. 2003.
Background to FrameNet. International journal of
lexicography, 16.3:235?250.
P. Fung, Z. Wu, Y. Yang, and D. Wu. 2007. Learn-
ing bilingual semantic frames: Shallow semantic pars-
ing vs. semantic role projection. In 11th Conference
on Theoretical and Methodological Issues in Machine
Translation (TMI 2007).
J. Henderson, P. Merlo, G. Musillo, and I. Titov. 2008. A
latent variable model of synchronous parsing for syn-
tactic and semantic dependencies. In Proceedings of
CONLL 2008, pages 178?182.
R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002.
Evaluating translational correspondence using anno-
tation projection. In Proceedings of the 40th Annual
Meeting of the ACL.
R. Hwa, P. Resnik, A.Weinberg, C. Cabezas, and O. Ko-
lak. 2005. Bootstrapping parsers via syntactic projec-
tion accross parallel texts. Natural language engineer-
ing, 11:311?325.
R. Johansson and P. Nugues. 2006. A FrameNet-based
semantic role labeler for Swedish. In Proceedings of
the annual Meeting of the Association for Computa-
tional Linguistics (ACL).
P. Koehn. 2003. Europarl: A multilingual corpus for
evaluation of machine translation.
J. Lang and M. Lapata. 2010. Unsupervised induction
of semantic roles. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 939?947, Los Angeles, California, June.
Association for Computational Linguistics.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
the Penn Treebank. Comp. Ling., 19:313?330.
P. Merlo and L. van der Plas. 2009. Abstraction and gen-
eralisation in semantic role labels: PropBank, VerbNet
or both? In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 288?296, Suntec, Singapore.
A. Meyers. 2007. Annotation guidelines for NomBank
- noun argument structure for PropBank. Technical
report, New York University.
P. Monachesi, G. Stevens, and J. Trapman. 2007. Adding
semantic role annotation to a corpus of written Dutch.
In Proceedings of the Linguistic Annotation Workshop
(LAW), pages 77?84, Prague, Czech republic.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29:19?51.
Sebastian Pado? and Mirella Lapata. 2009. Cross-lingual
annotation projection of semantic roles. Journal of Ar-
tificial Intelligence Research, 36:307?340.
S. Pado? and G. Pitel. 2007. Annotation pre?cise du
franc?ais en se?mantique de ro?les par projection cross-
linguistique. In Proceedings of TALN.
S. Pado?. 2007. Cross-lingual Annotation Projection
Models for Role-Semantic Information. Ph.D. thesis,
Saarland University.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31:71?105.
I. Titov and J. Henderson. 2007. A latent variable model
for generative dependency parsing. In Proceedings of
the International Conference on Parsing Technologies
(IWPT-07), pages 144?155, Prague, Czech Republic.
I. Titov, J. Henderson, P. Merlo, and G. Musillo. 2009.
Online graph planarisation for synchronous parsing of
semantic and syntactic dependencies. In Proceedings
of the twenty-first international joint conference on ar-
tificial intelligence (IJCAI-09), Pasadena, California,
July.
L. van der Plas, T. Samardz?ic?, and P. Merlo. 2010. Cross-
lingual validity of PropBank in the manual annotation
of French. In In Proceedings of the 4th Linguistic An-
notation Workshop (The LAW IV), Uppsala, Sweden.
D. Wu and P. Fung. 2009a. Can semantic role labeling
improve SMT? In Proceedings of the Annual Confer-
ence of European Association of Machine Translation.
D. Wu and P. Fung. 2009b. Semantic roles for SMT:
A hybrid two-pass model. In Proceedings of the
Joint Conference of the North American Chapter of
ACL/Human Language Technology.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001. In-
ducing multilingual text analysis tools via robust pro-
jection across aligned corpora. In Proceedings of the
International Conference on Human Language Tech-
nology (HLT).
304
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 296?300,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Heuristic Cube Pruning in Linear Time
Andrea Gesmundo
Department of
Computer Science
University of Geneva
andrea.gesmundo@unige.ch
Giorgio Satta
Department of
Information Engineering
University of Padua
satta@dei.unipd.it
James Henderson
Department of
Computer Science
University of Geneva
james.henderson@unige.ch
Abstract
We propose a novel heuristic algorithm for
Cube Pruning running in linear time in the
beam size. Empirically, we show a gain in
running time of a standard machine translation
system, at a small loss in accuracy.
1 Introduction
Since its first appearance in (Huang and Chiang,
2005), the Cube Pruning (CP) algorithm has quickly
gained popularity in statistical natural language pro-
cessing. Informally, this algorithm applies to sce-
narios in which we have the k-best solutions for two
input sub-problems, and we need to compute the k-
best solutions for the new problem representing the
combination of the two sub-problems.
CP has applications in tree and phrase based ma-
chine translation (Chiang, 2007; Huang and Chi-
ang, 2007; Pust and Knight, 2009), parsing (Huang
and Chiang, 2005), sentence alignment (Riesa and
Marcu, 2010), and in general in all systems combin-
ing inexact beam decoding with dynamic program-
ming under certain monotonic conditions on the def-
inition of the scores in the search space.
Standard implementations of CP run in time
O(k log(k)), with k being the size of the in-
put/output beams (Huang and Chiang, 2005). Ges-
mundo and Henderson (2010) propose Faster CP
(FCP) which optimizes the algorithm but keeps the
O(k log(k)) time complexity. Here, we propose a
novel heuristic algorithm for CP running in time
O(k) and evaluate its impact on the efficiency and
performance of a real-world machine translation
system.
2 Preliminaries
Let L = ?x0, . . . , xk?1? be a list over R, that is,
an ordered sequence of real numbers, possibly with
repetitions. We write |L| = k to denote the length of
L. We say that L is descending if xi ? xj for every
i, j with 0 ? i < j < k. Let L1 = ?x0, . . . , xk?1?
and L2 = ?y0, . . . , yk??1? be two descending lists
over R. We write L1 ? L2 to denote the descending
list with elements xi+yj for every i, j with 0 ? i <
k and 0 ? j < k?.
In cube pruning (CP) we are given as input two
descending lists L1, L2 over R with |L1| = |L2| =
k, and we are asked to compute the descending list
consisting of the first k elements of L1 ?L2.
A problem related to CP is the k-way merge
problem (Horowitz and Sahni, 1983). Given de-
scending lists Li for every i with 0 ? i < k, we
write mergek?1i=0 Li to denote the ?merge? of all the
lists Li, that is, the descending list with all elements
from the lists Li, including repetitions.
For ? ? R we define shift(L,?) = L ? ???. In
words, shift(L,?) is the descending list whose ele-
ments are obtained by ?shifting? the elements of L
by ?, preserving the order. Let L1,L2 be descend-
ing lists of length k, with L2 = ?y0, . . . , yk?1?.
Then we can express the output of CP on L1,L2 as
the list
mergek?1i=0 shift(L1, yi) (1)
truncated after the first k elements. This shows that
the CP problem is a particular instance of the k-way
merge problem, in which all input lists are related by
k independent shifts.
296
Computation of the solution of the k-way merge
problem takes time O(q log(k)), where q is the
size of the output list. In case each input list has
length k this becomes O(k2 log(k)), and by restrict-
ing the computation to the first k elements, as re-
quired by the CP problem, we can further reduce to
O(k log(k)). This is the already known upper bound
on the CP problem (Huang and Chiang, 2005; Ges-
mundo and Henderson, 2010). Unfortunately, there
seems to be no way to achieve an asymptotically
faster algorithm by exploiting the restriction that the
input lists are all related by some shifts. Nonethe-
less, in the next sections we use the above ideas to
develop a heuristic algorithm running in time linear
in k.
3 Cube Pruning With Constant Slope
Consider lists L1,L2 defined as in section 2. We say
that L2 has constant slope if yi?1? yi = ? > 0 for
every i with 0 < i < k. Throughout this section we
assume that L2 has constant slope, and we develop
an (exact) linear time algorithm for solving the CP
problem under this assumption.
For each i ? 0, let Ii be the left-open interval
(x0 ? (i + 1) ? ?, x0 ? i ? ?] of R. Let alo s =
?(x0 ? xk?1)/?? + 1. We split L1 into (possibly
empty) sublists ?i, 0 ? i < s, called segments, such
that each ?i is the descending sublist consisting of
all elements fromL1 that belong to Ii. Thus, moving
down one segment in L1 is the closest equivalent to
moving down one element in L2.
Let t = min{k, s}; we define descending lists
Mi, 0 ? i < t, as follows. We set M0 =
shift(?0, y0), and for 1 ? i < t we let
Mi = merge{shift(?i, y0), shift(Mi?1,??)} (2)
We claim that the ordered concatenation of M0,
M1, . . . , Mt?1 truncated after the first k elements
is exactly the output of CP on input L1,L2.
To prove our claim, it helps to visualize the de-
scending list L1 ? L2 (of size k2) as a k ? k matrix
L whose j-th column is shift(L1, yj), 0 ? j < k.
For an interval I = (x, x?], we define shift(I, y) =
(x+ y, x?+ y]. Similarly to what we have done with
L1, we can split each column of L into s segments.
For each i, j with 0 ? i < s and 0 ? j < k, we de-
fine the i-th segment of the j-th column, written ?i,j ,
as the descending sublist consisting of all elements
of that column that belong to shift(Ii, yj). Then we
have ?i,j = shift(?i, yj).
For any d with 0 ? d < t, consider now all
segments ?i,j with i + j = d, forming a sub-
antidiagonal in L. We observe that these segments
contain all and only those elements of L that belong
to the interval Id. It is not difficult to show by in-
duction that these elements are exactly the elements
that appear in descending order in the list Mi defined
in (2).
We can then directly use relation (2) to iteratively
compute CP on two lists of length k, under our as-
sumption that one of the two lists has constant slope.
Using the fact that the merge of two lists as in (2) can
be computed in time linear in the size of the output
list, it is not difficult to implement the above algo-
rithm to run in time O(k).
4 Linear Time Heuristic Solution
In this section we further elaborate on the exact al-
gorithm of section 3 for the constant slope case, and
develop a heuristic solution for the general CP prob-
lem. Let L1,L2, L and k be defined as in sections 2
and 3. Despite the fact that L2 does not have a con-
stant slope, we can still split each column of L into
segments, as follows.
Let I?i, 0 ? i < k ? 1, be the left-open interval
(x0 + yi+1, x0+ yi] of R. Note that, unlike the case
of section 3, intervals I?i?s are not all of the same size
now. Let alo I?k?1 = [xk?1 + yk?1, x0 + yk?1].
For each i, j with 0 ? j < k and 0 ? i < k ?
j, we define segment ??i,j as the descending sublist
consisting of all elements of the j-th column of L
that belong to I?i+j . In this way, the j-th column
of L is split into segments I?j , I?j+1, . . . , I?k?1, and
we have a variable number of segments per column.
Note that segments ??i,j with a constant value of i+j
contain all and only those elements of L that belong
to the left-open interval I?i+j .
Similarly to section 3, we define descending lists
M?i, 0 ? i < k, by setting M?0 = ??0,0 and, for
1 ? i < k, by letting
M?i = merge{??i,0 , path(M?i?1, L)} (3)
Note that the function path(M?i?1, L) should not re-
turn shift(M?i?1,??), for some value ?, as in the
297
1: Algorithm 1 (L1, L2) : L??
2: L??.insert(L[0, 0]);
3: referColumn? 0;
4: xfollow ? L[0, 1];
5: xdeviate ? L[1, 0];
6: C ? CircularList([0, 1]);
7: C-iterator? C.begin();
8: while |L??| < k do
9: if xfollow > xdeviate then
10: L??.insert(xfollow );
11: if C-iterator.current()=[0, 1] then
12: referColumn++;
13: [i, j]? C-iterator.next();
14: xfollow ? L[i,referColumn+j];
15: else
16: L??.insert(xdeviate );
17: i? xdeviate .row();
18: C-iterator.insert([i,?referColumn]);
19: xdeviate ? L[i + 1, 0];
case of (2). This is because input list L2 does not
have constant slope in general. In an exact algo-
rithm, path(M?i?1, L) should return the descending
list L?i?1 = mergeij=1 ??i?j,j: Unfortunately, we do
not know how to compute such a i-way merge with-
out introducing a logarithmic factor.
Our solution is to define path(M?i?1, L) in such a
way that it computes a list L?i?1 which is a permu-
tation of the correct solution L?i?1. To do this, we
consider the ?relative? path starting at x0+yi?1 that
we need to follow in L in order to collect all the el-
ements of M?i?1 in the given order. We then apply
such a path starting at x0 + yi and return the list of
collected elements. Finally, we compute the output
list L?? as the concatenation of all lists M?i up to the
first k elements.
It is not difficult to see that when L2 has constant
slope we have M?i = Mi for all i with 0 ? i < k,
and list L?? is the exact solution to the CP prob-
lem. When L2 does not have a constant slope, list
L?? might depart from the exact solution in two re-
spects: it might not be a descending list, because
of local variations in the ordering of the elements;
and it might not be a permutation of the exact so-
lution, because of local variations at the end of the
list. In the next section we evaluate the impact that
 
 
	

 


 


 
 


 

 


 

  

  


  
 

  
 

CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 178?182
Manchester, August 2008
A Latent Variable Model of Synchronous Parsing
for Syntactic and Semantic Dependencies
James Henderson
Dept Computer Science
Univ Geneva
james.henderson@
cui.unige.ch
Paola Merlo
Dept Linguistics
Univ Geneva
merlo@
lettres.unige.ch
Gabriele Musillo
Depts Linguistics
and Computer Science
Univ Geneva
musillo@
lettres.unige.ch
Ivan Titov
?
Dept Computer Science
Univ Illinois at U-C
titov@uiuc.edu
Abstract
We propose a solution to the challenge
of the CoNLL 2008 shared task that uses
a generative history-based latent variable
model to predict the most likely derivation
of a synchronous dependency parser for
both syntactic and semantic dependencies.
The submitted model yields 79.1% macro-
average F1 performance, for the joint task,
86.9% syntactic dependencies LAS and
71.0% semantic dependencies F1. A larger
model trained after the deadline achieves
80.5% macro-average F1, 87.6% syntac-
tic dependencies LAS, and 73.1% seman-
tic dependencies F1.
1 Introduction
Successes in syntactic tasks, such as statistical
parsing and tagging, have recently paved the way
to statistical learning techniques for levels of se-
mantic representation, such as recovering the log-
ical form of a sentence for information extraction
and question-answering applications (e.g. (Wong
and Mooney, 2007)) or jointly learning the syntac-
tic structure of the sentence and the propositional
argument-structure of its main predicates (Musillo
and Merlo, 2006; Merlo and Musillo, 2008). In
this vein, the CoNLL 2008 shared task sets the
challenge of learning jointly both syntactic depen-
dencies (extracted from the Penn Treebank (Mar-
cus et al, 1993) ) and semantic dependencies (ex-
tracted both from PropBank (Palmer et al, 2005)
?
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
0
Authors in alphabetical order.
and NomBank (Meyers et al, 2004) under a uni-
fied representation.
We propose a solution that uses a generative
history-based model to predict the most likely
derivation of a synchronous dependency parser for
both syntactic and semantic dependencies. Our
probabilistic model is based on Incremental Sig-
moid Belief Networks (ISBNs), a recently pro-
posed latent variable model for syntactic struc-
ture prediction, which has shown very good be-
haviour for both constituency (Titov and Hender-
son, 2007a) and dependency parsing (Titov and
Henderson, 2007b). The ability of ISBNs to in-
duce their features automatically enables us to ex-
tend this architecture to learning a synchronous
parse of syntax and semantics without modifica-
tion of the main architecture. By solving the
problem with synchronous parsing, a probabilistic
model is learnt which maximises the joint proba-
bility of the syntactic and semantic dependencies
and thereby guarantees that the output structure is
globally coherent, while at the same time building
the two structures separately. This extension of the
ISBN architecture is therefore applicable to other
problems where two independent, but related, lev-
els of representation are being learnt, such as sta-
tistical machine translation.
Currently the largest model we have trained
achieves 80.5% macro-average F1 performance for
the joint task, 87.6% syntactic dependencies LAS,
and 73.1% semantic dependencies F1.
2 The Probability Model
Our probability model is a joint generative model
of syntactic and semantic dependencies. The
two dependency structures are specified as the se-
quence of actions for a synchronous parser, which
requires each dependency structure to be projec-
178
tivised separately.
2.1 Synchronous derivations
The derivations for syntactic dependency trees are
the same as specified in (Titov and Henderson,
2007b), which are based on the shift-reduce style
parser of (Nivre et al, 2006). The derivations use a
stack and an input queue. There are actions for cre-
ating a leftward or rightward arc between the top of
the stack and the front of the queue, for popping a
word from the stack, and for shifting a word from
the queue to the stack. The derivations for seman-
tic dependency graphs use virtually the same set
of actions, but impose fewer constraints on when
they can be applied, due to the fact that a word in
a semantic dependency graph can have more than
one parent. An additional action predicate
s
was
introduced to label a predicate with sense s.
Let T
d
be a syntactic dependency tree with
derivation D
1
d
, ..., D
m
d
d
, and T
s
be a semantic de-
pendency graph with derivation D
1
s
, ..., D
m
s
s
. To
define derivations for the joint structure T
d
, T
s
,
we need to specify how the two derivations are
synchronised, and in particular make the impor-
tant choice of the granularity of the synchronisa-
tion step. Linguistic intuition would perhaps sug-
gest that syntax and semantics are connected at the
clause level ? a big step size ? while a fully in-
tegrated system would synchronise at each pars-
ing decision, thereby providing the most commu-
nication between these two levels. We choose to
synchronise the construction of the two structures
at every word ? an intermediate step size. This
choice is simpler, as it is based on the natural to-
tal order of the input, and it avoids the problems
of the more linguistically motivated choice, where
chunks corresponding to different semantic propo-
sitions would be overlapping.
We divide the two derivations into the chunks
between shifting each word onto the stack,
c
t
d
= D
b
t
d
d
, ..., D
e
t
d
d
and c
t
s
= D
b
t
s
s
, ..., D
e
t
s
s
,
where D
b
t
d
?1
d
= D
b
t
s
?1
s
= shift
t?1
and
D
e
t
d
+1
d
= D
e
t
s
+1
s
= shift
t
. Then the actions of
the synchronous derivations consist of quadruples
C
t
= (c
t
d
, switch, c
t
s
, shift
t
), where switch means
switching from syntactic to semantic mode. This
gives us the following joint probability model,
where n is the number of words in the input.
P (T
d
, T
s
) = P (C
1
, . . . , C
n
)
=
?
t
P (C
t
|C
1
, . . . , C
t?1
)
(1)
The probability of each synchronous derivation
chunk C
t
is the product of four factors, related to
the syntactic level, the semantic level and the two
synchronising steps.
P (C
t
|C
1
, . . . , C
t?1
) =
P (c
t
d
|C
1
, . . . , C
t?1
)?
P (switch|c
t
d
, C
1
, . . . , C
t?1
)?
P (c
t
s
|switch, c
t
d
, C
1
, . . . , C
t?1
)?
P (shift
t
|c
t
d
, c
t
s
, C
1
, . . . , C
t?1
)
(2)
These synchronous derivations C
1
, . . . , C
n
only
require a single input queue, since the shift opera-
tions are synchronised, but they require two sepa-
rate stacks, one for the syntactic derivation and one
for the semantic derivation.
The probability of c
t
d
is decomposed into deriva-
tion action D
i
probabilities, and likewise for c
t
s
.
P (c
t
d
|C
1
, . . . , C
t?1
)
=
?
i
P (D
i
d
|D
b
t
d
d
,. . ., D
i?1
d
, C
1
,. . ., C
t?1
)
(3)
The actions are also sometimes split into a se-
quence of elementary decisions D
i
= d
i
1
, . . . , d
i
n
,
as discussed in (Titov and Henderson, 2007a).
2.2 Projectivisation of dependencies
These derivations can only specify projective
syntactic or semantic dependency graphs. Ex-
ploratory data analysis indicates that many in-
stances of non-projectivity in the complete graph
are due to crossings of the syntactic and seman-
tic graphs. The amount of non-projectivity of the
joint syntactic-semantic graph is approximately
7.5% non-projective arcs, while summing the non-
projectivity within the two separate graphs results
in only roughly 3% non-projective arcs.
Because our synchronous derivations use two
different stacks for the syntactic and semantic de-
pendencies, respectively, we only require each in-
dividual graph to be projective. As with many de-
pendency parsers (Nivre et al, 2006; Titov and
Henderson, 2007b), we handle non-projective (i.e.
crossing) arcs by transforming them into non-
crossing arcs with augmented labels.
1
Because
our syntactic derivations are equivalent to those of
(Nivre et al, 2006), we use their HEAD methods
to projectivise the syntactic dependencies.
Although our semantic derivations use the same
set of actions as the syntactic derivations, they dif-
fer in that the graph of semantic dependencies need
1
During testing, these projectivised structures are then
transformed back to the original format for evaluation.
179
not form a tree. The only constraints we place on
the set of semantic dependencies are imposed by
the use of a stack, which excludes crossing arcs.
Given two crossing arcs, we try to uncross them
by changing an endpoint of one of the arcs. The
arc (p, a), where p is a predicate and a is an argu-
ment, is changed to (p, h), where h is the syntactic
head of argument a. Its label r is then changed to
r/d where d is the syntactic dependency of a on
h. This transformation may need to be repeated
before the arcs become uncrossed. The choice of
which arc to transform is done using a greedy al-
gorithm and a number of heuristics, without doing
any global optimisation across the data.
This projectivisation method is similar to the
HEAD method of (Nivre et al, 2006), but has two
interesting new characteristics. First, syntactic de-
pendencies are used to projectivise the semantic
dependencies. Because the graph of semantic roles
is disconnected, moving across semantic arcs is of-
ten not possible. This would cause a large number
of roles to be moved to ROOT. Second, our method
changes the semantic argument of a given pred-
icate, whereas syntactic dependency projectivisa-
tion changes the head of a given dependent. This
difference is motivated by a predicate-centred view
of semantic dependencies, as it avoids changing a
predicate to a node which is not a predicate.
3 The Learning Architecture
The synchronous derivations described above are
modelled with an Incremental Sigmoid Belief Net-
work (ISBN) (Titov and Henderson, 2007a). IS-
BNs are dynamic Bayesian Networks which incre-
mentally specify their model structure based on the
partial structure being built by a derivation. They
have previously been applied to constituency and
dependency parsing. In both cases the derivations
were based on a push-down automaton, but ISBNs
can be directly applied to any automaton. We suc-
cessfully apply ISBNs to a two-stack automaton,
without changing the machine learning methods.
3.1 The Incremental Sigmoid Belief Networks
ISBNs use vectors of latent variables to represent
properties of parsing history relevant to the next
decisions. Latent variables do not need to be anno-
tated in the training data, but instead get induced
during learning. As illustrated by the vectors S
i
in figure 1, the latent feature vectors are used to
estimate the probabilities of derivation actions D
i
.
s
SS
DD
S
i?c
i?c i?1
i?1
i
ij
Di dki
Figure 1: An ISBN for estimating
P (d
i
k
|history(i, k)) ? one of the elementary
decisions. Variables whose values are given in
history(i, k) are shaded, and latent and current
decision variables are unshaded.
Latent variable vectors are connected to variables
from previous positions via a pattern of edges de-
termined by the previous decisions. Our ISBN
model distinguishes two types of latent states: syn-
tactic states, when syntactic decisions are consid-
ered, and semantic states, when semantic decision
are made. Different patterns of interconnections
are used for different types of states. We use the
neural network approximation (Titov and Hender-
son, 2007a) to perform inference in our model.
As also illustrated in figure 1, the induced latent
variables S
i
at state i are statistically dependent on
both pre-defined features of the derivation history
D
1
, . . . , D
i?1
and the latent variables for a finite
set of relevant previous states S
i
?
, i
?
< i. Choos-
ing this set of relevant previous states is one of the
main design decisions in building an ISBN model.
By connecting to a previous state, we place that
state in the local context of the current decision.
This specification of the domain of locality deter-
mines the inductive bias of learning with ISBNs.
Thus, we need to choose the set of local (i.e. con-
nected) states in accordance with our prior knowl-
edge about which previous decisions are likely to
be particularly relevant to the current decision.
3.2 Layers and features
To choose previous relevant decisions, we make
use of the partial syntactic and semantic depen-
dency structures which have been decided so far
in the parse. Specifically, the current latent state
vector is connected to the most recent previous la-
tent state vectors (if they exist) whose configura-
tion shares a node with the current configuration,
as specified in Table 1. The nodes are chosen be-
cause their properties are thought to be relevant to
the current decision. Each row of the table indi-
cates which nodes need to be identical, while each
180
Closest Current Syn-Syn Srl-Srl Syn-Srl
Input Input + + +
Top Top + + +
RDT Top + +
LDT Top + +
HT Top + +
LDN Top + +
Input Top +
Table 1: Latent-to-latent variable connections. In-
put= input queue; Top= top of stack; RDT= right-
most right dependent of top; LDT= leftmost left
dependent of top; HT= Head of top; LDN= left-
most dependent of next (front of input).
column indicates whether the latent state vectors
are for the syntactic or semantic derivations. For
example, the first row indicates edges between the
current state and a state which had the same in-
put as the current state. The three columns indi-
cate that this edge holds within syntactic states,
within semantic states, and from syntactic to se-
mantic states. The fourth cell of the third row, for
example, indicates that there is an edge between
the current semantic state on top of the stack and
the most recent semantic state where the rightmost
dependent of the current top of the semantic stack
was at the top of the semantic stack.
Each of these relations has a distinct weight ma-
trix for the resulting edges in the ISBN, but the
same weight matrix is used at each position where
the relation applies. Training and testing times
scale linearly with the number of relations.
The pre-defined features of the parse history
which also influence the current decision are spec-
ified in table 2. The model distinguishes argument
roles of nominal predicates from argument roles of
verbal predicates.
3.3 Decoding
Given a trained ISBN as our probability esti-
mator, we search for the most probable joint
syntactic-semantic dependency structure using a
beam search. Most pruning is done just after each
shift operation (when the next word is predicted).
Global constraints (such as label uniqueness) are
not enforced by decoding, but can be learnt.
For the system whose results we submitted, we
then do a second step to improve on the choice
of syntactic dependency structure. Because of the
lack of edges in the graphical model from seman-
tic to syntactic states, it is easy to marginalise out
the semantic structure, giving us the most proba-
ble syntactic dependency structure. This syntactic
structure is then combined with the semantic struc-
State Stack Syntactic step features
LEX POS DEP
Input + +
Top syn + +
Top - 1 syn +
HT syn +
RDT syn +
LDT syn +
LDN syn +
State Stack Semantic step features
LEX POS DEP SENSE
Input + + +
Top sem + + +
Top - 1 sem + +
HT sem + +
RDT sem +
LDT sem +
LDN sem +
A0-A5 of Top sem +
A0-A5 of Input sem +
Table 2: Pre-defined features. syn=syntactic stack;
sem=semantic stack. Input= input queue; Top=
top of stack; RDT= rightmost dependent of top;
LDT= leftmost dependent of Top; HT= Head of
top; LDN= leftmost dependent of next (front of
input); A0-A5 of Top/Input= arguments of top of
stack / input.
ture from the first stage, to get our submitted re-
sults. This second stage does not maximise perfor-
mance on the joint syntactic-semantic dependency
structure, but it better fits the evaluation measure
used to rank systems.
4 Experiments and Discussion
The experimental set-up common for all the teams
is described in the introduction (Surdeanu et al,
2008). The submitted model has latent variable
vectors of 60 units, and a word frequency cut-off
of 100, resulting in a small vocabulary of 1083
words. We used a beam of size 15 to prune deriva-
tions after each shift operation to obtain the joint
structure, and a beam of size 40 when perform-
ing the marginalisation. Training took approxi-
mately 2.5 days on a standard PC with 3.0 GHz
Pentium4 CPU. It took approximately 2 hours to
parse the entire testing set (2,824 sentences) and
an additional 3 hours to perform syntactic parsing
when marginalising out the semantic structures.
2
Shortly after the submission deadline, we trained a
?large? model with a latent variable vector of size
80, a word frequency cut-off of 20, and additional
latent-to-latent connections from semantics to syn-
tax of the same configuration as the last column
2
A multifold speed-up with a small decrease in accuracy
can be achieved by using a small beam.
181
Syn Semantic Overall
LAS P R F1 P R F1
Submitted
D 86.1 78.8 64.7 71.1 82.5 75.4 78.8
W 87.8 79.6 66.2 72.3 83.7 77.0 80.2
B 80.0 66.6 55.3 60.4 73.3 67.6 70.3
WB 86.9 78.2 65.0 71.0 82.5 76.0 79.1
Joint inference
D 85.5 78.8 64.7 71.1 82.2 75.1 78.5
Large, joint inference
D 86.5 79.9 67.5 73.2 83.2 77.0 80.0
W 88.5 80.4 69.2 74.4 84.4 78.8 81.5
B 81.0 68.3 57.7 62.6 74.7 69.4 71.9
WB 87.6 79.1 67.9 73.1 83.4 77.8 80.5
Table 3: Scores on the development set and the
final testing sets (percentages). D= development
set; W=WSJ; B=Brown; WB=WSJ+Brown;
of table 1. This model took about 50% longer in
training and testing.
In table 3, we report results for the marginalised
inference (?submitted?) and joint inference for the
submitted model, and the results for joint inference
with the ?large? model. The larger model improves
on the submitted results by almost 1.5%, a signifi-
cant improvement. If completed earlier, this model
would have been fifth overall, second for syntactic
LAS, and fifth for semantic F1.
To explore the relationship between the two
components of the model, we removed the edges
between the syntax and the semantics in the sub-
mitted model. This model?s performance drops by
about 3.5% for semantic role labelling, thereby in-
dicating that the latent annotation of parsing states
helps semantic role labelling. However, it also
indicates that there is much room for improve-
ment in developing useful semantic-specific fea-
tures, which was not done for these experiments
simply due to constraints on development time.
To test whether joint learning degrades the ac-
curacy of the syntactic parsing model, we trained a
syntactic parsing model with the same features and
the same pattern of interconnections as used for the
syntactic states in our joint model. The resulting
labelled attachment score was non-significantly
lower (0.2%) than the score for the marginalised
inference with the joint model. This result sug-
gests that, though the latent variables associated
with syntactic states in the joint model were trained
to be useful in semantic role labelling, this did not
have a negative effect on syntactic parsing accu-
racy, and may even have helped.
Finally, an analysis of the errors on the develop-
ment set for the submitted model paints a coherent
picture. We find attachment of adjuncts particu-
larly hard. For dependency labels, we make the
most mistakes on modification labels, while for se-
mantic labels, we find TMP, ADV, LOC, and PRN
particularly hard. NomBank arcs are not learnt as
well as PropBank arcs: we identify PropBank SRL
arguments at F1 70.8% while Nombank arguments
reach 58.1%, and predicates at accuracy 87.9% for
PropBank and 74.9% for NomBank.
5 Conclusions
While still preliminary, these results indicate that
synchronous parsing is an effective way of build-
ing joint models on separate structures. The gen-
erality of the ISBN design used so far suggests
that ISBN?s latent feature induction extends well to
estimating very complex probability models, with
little need for feature engineering. Nonetheless,
performance could be improved by task-specific
features, which we plan for future work.
Acknowledgements
This work was partly funded by European Community FP7
grant 216594 (CLASSiC, www.classic-project.org), Swiss
NSF grant 114044, and Swiss NSF fellowships PBGE2-
117146 and PBGE22-119276. Part of this work was done
when G. Musillo was visiting MIT/CSAIL, hosted by Prof.
Michael Collins.
References
Marcus, M., B. Santorini, and M.A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19:313?330.
Merlo, P. and G. Musillo. 2008. Semantic parsing for high-
precision semantic role labelling. In Procs of CoNLL
2008, Manchester, UK.
Meyers, A., R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The nombank
project: An interim report. In Meyers, A., editor, HLT-
NAACL 2004 Workshop: Frontiers in Corpus Annotation,
24?31, Boston, MA.
Musillo, G. and P. Merlo. 2006. Accurate semantic parsing
of the Proposition Bank. In Procs of NAACL 2006, New
York, NY.
Nivre, J., J. Hall, J. Nilsson, G. Eryigit, and S. Marinov. 2006.
Pseudo-projective dependency parsing with support vector
machines. In Proc. of CoNNL, 221?225, New York, USA.
Palmer, M., D. Gildea, and P. Kingsbury. 2005. The Propo-
sition Bank: An annotated corpus of semantic roles. Com-
putational Linguistics, 31:71?105.
Surdeanu, M., R. Johansson, A. Meyers, L. M`arquez, and J.
Nivre. 2008. The CoNLL-2008 shared task on joint pars-
ing of syntactic and semantic dependencies. In Procs of
CoNLL-2008, Manchester,UK.
Titov, I. and J. Henderson. 2007a. Constituent parsing with
incremental sigmoid belief networks. In Procs of ACL?07,
pages 632?639, Prague, Czech Republic.
Titov, I. and J. Henderson. 2007b. A latent variable model
for generative dependency parsing. In Procs of IWPT?07,
Prague, Czech Republic.
Wong, Y.W. and R. Mooney. 2007. Learning synchronous
grammars for semantic parsing with lambda calculus. In
Procs of ACL?07, 960?967, Prague, Czech Republic.
182
Proceedings of the SIGDIAL 2013 Conference, pages 154?156,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Demonstration of the Parlance system: a data-driven,
incremental, spoken dialogue system for interactive search
Helen Hastie, Marie-Aude Aufaure?, Panos Alexopoulos, Heriberto Cuay?huitl, Nina Dethlefs,
Milica Gasic, James Henderson, Oliver Lemon, Xingkun Liu, Peter Mika, Nesrine Ben Mustapha,
Verena Rieser, Blaise Thomson, Pirros Tsiakoulis, Yves Vanrompay, Boris Villazon-Terrazas, Steve Young
email: h.hastie@hw.ac.uk. See http://parlance-project.eu for full list of affiliations
Abstract
The Parlance system for interactive
search processes dialogue at a micro-
turn level, displaying dialogue phe-
nomena that play a vital role in hu-
man spoken conversation. These di-
alogue phenomena include more nat-
ural turn-taking through rapid sys-
tem responses, generation of backchan-
nels, and user barge-ins. The Par-
lance demonstration system differen-
tiates from other incremental systems
in that it is data-driven with an infras-
tructure that scales well.
1 Introduction
The Parlance system provides interactive
search through a Spoken Dialogue System
(SDS). This SDS aims to be incremental to al-
low for more natural spoken interaction. Tra-
ditionally, the smallest unit of speech process-
ing for interactive systems has been a full ut-
terance with strict, rigid turn-taking. The
Parlance architecture, however, is an incre-
mental framework that allows for processing
of smaller ?chunks? of user input, which en-
ables one to model dialogue phenomena such
as barge-ins and backchannels. This work is
carried out under the FP7 EC project Par-
lance 1, the goal of which is to develop inter-
active search through speech in multiple lan-
guages. The domain for the demonstration
system is interactive search for restaurants in
San Francisco. An example dialogue is given
in Table 1.
?Authors are in alphabetical order
1http://www.parlance-project.eu
SYS Thank you for calling the Parlance Restaurant
system. You may ask for information by cuisine
type, price range or area. How may I help you?
USR I want to find an Afghan restaurant.........which is
in the cheap price range.
SYS .......................................................[uhuhh]........
The Helmand Palace is a cheerful setting for au-
thentic Afghan cuisine.
USR What is the address and phone number?
SYS The address 2424 Van Ness Ave ....
Table 1: Example dialogue excerpt for restaurant in-
formation in San Francisco
2 Background
Previous work includes systems that can deal
with ?micro-turns? (i.e. sub-utterance process-
ing units), resulting in dialogues that are more
fluid and responsive. This has been backed up
by a large body of psycholinguistic literature
that indicates that human-human interaction
is in fact incremental (Levelt, 1989).
It has been shown that incremental dia-
logue behaviour can improve the user experi-
ence (Skantze and Schlangen, 2009; Baumann
et al, 2011; Selfridge et al, 2011) and en-
able the system designer to model several di-
alogue phenomena that play a vital role in
human discourse (Levelt, 1989) but have so
far been absent from systems. These dialogue
phenomena that will be demonstrated by the
Parlance system include more natural turn-
taking through rapid system responses, gener-
ation of backchannels and user barge-ins. The
system differentiates from other incremental
systems in that it is entirely data-driven with
an infrastructure that potentially scales well.
3 System Architecture
Figure 1 gives an overview of the Par-
lance system architecture, which maintains
154
LOCAL SEARCH ENGINE
AUTOMATIC SPEECH RECOGNITION
NLG
AUDIO I/O
TTS
BACKCHANNEL GENERATOR
IM
MIM
HUB
KNOWLEDGE BASE
WavePackets
1-Best Words
Segmentlabel
N-Best Phrase List
WavePackets
Micro-Turn Dialogue Act
System Dialogue Act
String Packets
StringPackets
VoIP Interface (PJSIP)
N-best Dialogue Act Units
 API call ( + metadata)
Search Response
Partial Dialogue Act (in case of interruption)
PartialString(in case of interruption)SPOKEN LANGUAGE UNDERSTANDING Decode from t0 to t1
Figure 1: Overview of the Parlance system
architecture
the modularity of a traditional SDS while at
the same time allowing for complex interaction
at the micro-turn level between components.
Each component described below makes use
of the PINC (Parlance INCremental) dialogue
act schema. In this scheme, a complete dia-
logue act is made up of a set of primitive di-
alogue acts which are defined as acttype-item
pairs. The PINC dialogue act scheme supports
incrementality by allowing SLU to incremen-
tally output primitive dialogue acts whenever
a complete acttype-item pair is recognised with
sufficient confidence. The complete dialogue
act is then the set of these primitive acts out-
put during the utterance.
3.1 Recognition and Understanding
The Automatic Speech Recogniser (ASR) and
Spoken Language Understanding (SLU) com-
ponents operate in two passes. The audio in-
put is segmented by a Voice Activity Detec-
tor and then coded into feature vectors. For
the first pass of the ASR2, a fast bigram de-
coder performs continuous traceback generat-
ing word by word output. During this pass,
while the user is speaking, an SLU module
called the ?segment decoder? is called incre-
2http://mi.eng.cam.ac.uk/research/dialogue/
ATK_Manual.pdf
mentally as words or phrases are recognised.
This module incrementally outputs the set of
primitive dialogue acts that can be detected
based on each utterance prefix. Here, the ASR
only provides the single best hypothesis, and
SLU only outputs a single set of primitive dia-
logue acts, without an associated probability.
On request from the Micro-turn Interaction
Manager (MIM), a second pass can be per-
formed to restore the current utterance using a
trigram language model, and return a full dis-
tribution over the complete phrase as a con-
fusion network. This is then passed to the
SLU module which outputs the set of alter-
native complete interpretations, each with its
associated probability, thus reflecting the un-
certainty in the ASR-SLU understanding pro-
cess.
3.2 Interaction Management
Figure 1 illustrates the role of the Micro-turn
Interaction Manager (MIM) component in the
overall Parlance architecture. In order to
allow for natural interaction, the MIM is re-
sponsible for taking actions such as listening to
the user, taking the floor, and generating back-
channels at the micro-turn level. Given various
features from different components, the MIM
selects a micro-turn action and sends it to the
IM and back-channel generator component to
generate a system response.
Micro-turn Interaction Manager A
baseline hand-crafted MIM was developed
using predefined rules. It receives turn-taking
information from the TTS, the audio-output
component, the ASR and a timer, and updates
turn-taking features. Based on the current
features and predefined rules, it generates
control signals and sends them to the TTS,
ASR, timer and HUB. In terms of micro-turn
taking, for example, if the user interrupts
the system utterance, the system will stop
speaking and listen to the user. The system
also outputs a short back-channel and stays in
user turn state if the user utterance provides
limited information.
Interaction Manager Once the MIM has
decided when the system should take the floor,
it is the task of the IM to decide what to say.
The IM is based on the partially observable
155
Markov decision process (POMDP) frame-
work, where the system?s decisions can be op-
timised via reinforcement learning. The model
adopted for Parlance is the Bayesian Update
of Dialogue State (BUDS) manager (Thom-
son and Young, 2010). This POMDP-based
IM factors the dialogue state into condition-
ally dependent elements. Dependencies be-
tween these elements can be derived directly
from the dialogue ontology. These elements
are arranged into a dynamic Bayesian network
which allows for their marginal probabilities
to be updated during the dialogue, compris-
ing the belief state. The belief state is then
mapped into a smaller-scale summary space
and the decisions are optimised using the nat-
ural actor critic algorithm.
HUB The HUB manages the high level flow
of information. It receives turn change infor-
mation from the MIM and sends commands
to the SLU/IM/NLG to ?take the floor? in the
conversation and generate a response.
3.3 Generation and TTS
We aim to automatically generate language,
trained from data, that is (1) grammatically
well formed, (2) natural, (3) cohesive and (4)
rapidly produced at runtime. Whilst the first
two requirements are important in any dia-
logue system, the latter two are key require-
ments for systems with incremental processing,
in order to be more responsive. This includes
generating back-channels, dynamic content re-
ordering (Dethlefs et al, 2012), and surface
generation that models coherent discourse phe-
nomena, such as pronominalisation and co-
reference (Dethlefs et al, 2013). Incremen-
tal surfacce generation requires rich context
awareness in order to keep track of all that has
been generated so far. We therefore treat sur-
face realisation as a sequence labelling task and
use Conditional Random Fields (CRFs), which
take semantically annotated phrase structure
trees as input, in order to represent long dis-
tance linguistic dependencies. This approach
has been compared with a number of compet-
itive state-of-the art surface realisers (Deth-
lefs et al, 2013), and can be trained from
minimally labelled data to reduce development
time and facilitate its application to new do-
mains.
The TTS component uses a trainable HMM-
based speech synthesizer. As it is a paramet-
ric model, HMM-TTS has more flexibility than
traditional unit-selection approaches and is es-
pecially useful for producing expressive speech.
3.4 Local Search and Knowledge Base
The domain ontology is populated by the local
search component and contains restaurants in
5 regional areas of San Francisco. Restaurant
search results are returned based on their lon-
gitude and latitude for 3 price ranges and 52
cuisine types.
4 Future Work
We intend to perform a task-based evaluation
using crowd-sourced users. Future versions
will use a dynamic Knowledge Base and User
Model for adapting to evolving domains and
personalised interaction respectively.
Acknowledgements
The research leading to this work was funded by the EC
FP7 programme FP7/2011-14 under grant agreement
no. 287615 (PARLANCE).
References
T. Baumann, O. Buss, and D. Schlangen. 2011. Eval-
uation and Optimisation of Incremental Processors.
Dialogue and Discourse, 2(1).
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012. Optimising Incremental Generation
for Spoken Dialogue Systems: Reducing the Need
for Fillers. In Proceedings of INLG, Chicago, USA.
N. Dethlefs, H. Hastie, H. Cuay?huitl, and O. Lemon.
2013. Conditional Random Fields for Responsive
Surface Realisation Using Global Features. In Pro-
ceedings of ACL, Sofia, Bulgaria.
W. Levelt. 1989. Speaking: From Intenion to Articu-
lation. MIT Press.
E. Selfridge, I. Arizmendi, P. Heeman, and J. Williams.
2011. Stability and Accuracy in Incremental Speech
Recognition. In Proceedings of SIGDIAL, Portland,
Oregon.
G. Skantze and D. Schlangen. 2009. Incremental Dia-
logue Processing in a Micro-Domain. In Proceedings
of EACL, Athens, Greece.
B Thomson and S Young. 2010. Bayesian update of
dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech and Language,
24(4):562?588.
156
Proceedings of the SIGDIAL 2014 Conference, pages 260?262,
Philadelphia, U.S.A., 18-20 June 2014. c?2014 Association for Computational Linguistics
The Parlance Mobile Application for Interactive Search in
English and Mandarin
Helen Hastie, Marie-Aude Aufaure?, Panos Alexopoulos,
Hugues Bouchard, Catherine Breslin, Heriberto Cuay?huitl, Nina Dethlefs,
Milica Ga?i?, James Henderson, Oliver Lemon, Xingkun Liu, Peter Mika, Nesrine Ben Mustapha,
Tim Potter, Verena Rieser, Blaise Thomson, Pirros Tsiakoulis, Yves Vanrompay,
Boris Villazon-Terrazas, Majid Yazdani, Steve Young and Yanchao Yu
email: h.hastie@hw.ac.uk. See http://parlance-project.eu for full list of affiliations
Abstract
We demonstrate a mobile application in
English and Mandarin to test and eval-
uate components of the Parlance di-
alogue system for interactive search un-
der real-world conditions.
1 Introduction
With the advent of evaluations ?in the wild?,
emphasis is being put on converting re-
search prototypes into mobile applications that
can be used for evaluation and data col-
lection by real users downloading the ap-
plication from the market place. This is
the motivation behind the work demonstrated
here where we present a modular framework
whereby research components from the Par-
lance project (Hastie et al., 2013) can be
plugged in, tested and evaluated in a mobile
environment.
The goal of Parlance is to perform inter-
active search through speech in multiple lan-
guages. The domain for the demonstration
system is interactive search for restaurants in
Cambridge, UK for Mandarin and San Fran-
cisco, USA for English. The scenario is that
Mandarin speaking tourists would be able to
download the application and use it to learn
about restaurants in English speaking towns
and cities.
2 System Architecture
Here, we adopt a client-server approach as il-
lustrated in Figure 1 for Mandarin and Figure
2 for English. The front end of the demon-
stration system is an Android application that
calls the Google Automatic Speech Recogni-
tion (ASR) API and sends the recognized user
utterance to a server running the Interaction
?Authors are in alphabetical order
Manager (IM), Spoken Language Understand-
ing (SLU) and Natural Language Generation
(NLG) components.
Figure 1: Overview of the Parlance Man-
darin mobile application system architecture
Figure 2: Overview of the Parlance En-
glish mobile application system architecture
extended to use the Yahoo API to populate
the application with additional restaurant in-
formation
When the user clicks the Start button, a di-
alogue session starts. The phone application
first connects to the Parlance server (via
the Java Socket Server) to get the initial sys-
tem greeting which it speaks via the Google
260
Text-To-Speech (TTS) API. After the system
utterance finishes the recognizer starts to lis-
ten for user input to send to the SLU compo-
nent. The SLU converts text into a semantic
interpretation consisting of a set of triples of
communicative function, attribute, and (op-
tionally) value1. Probabilities can be associ-
ated with candidate interpretations to reflect
uncertainty in either the ASR or SLU. The
SLU then passes the semantic interpretation
to the IM within the same server.
Chinese sentences are composed of strings of
characters without any space to mark words as
other languages do, for example:
In order to correctly parse and understand
Chinese sentences, Chinese word segmenta-
tions must be performed. To do this segmen-
tation, we use the Stanford Chinese word seg-
mentor2, which relies on a linear-chain condi-
tional random field (CRF) model and treats
word segmentation as a binary decision task.
The Java Socket Server then sends the seg-
mented Chinese sentence to the SLU on the
server.
The IM then selects a dialogue act, accesses
the database and in the case of English passes
back the list of restaurant identification num-
bers (ids) associated with the relevant restau-
rants. For the English demonstration system,
these restaurants are displayed on the smart
phone as seen in Figures 4 and 5. Finally,
the NLG component decides how best to re-
alise the restaurant descriptions and sends the
string back to the phone application for the
TTS to realise. The example output is illus-
trated in Figure 3 for Mandarin and Figure 4
for English.
As discussed above, the Parlance mobile
application can be used as a test-bed for com-
paring alternative techniques for various com-
ponents. Here we discuss two such compo-
nents: IM and NLG.
1This has been implemented for English; Mandarin
uses the rule-based Phoenix parser.
2http://nlp.stanford.edu/projects/chinese-
nlp.shtml
Figure 3: Screenshot and translation of the
Mandarin system
Figure 4: Screenshot of dialogue and the list
of recommended restaurants shown on a map
and in a list for English
2.1 Interaction Management
The Parlance Interaction Manager is based
on the partially observable Markov decision
process (POMDP) framework, where the sys-
tem?s decisions can be optimised via reinforce-
ment learning. The model adopted for Par-
lance is the Bayesian Update of Dialogue
State (BUDS) manager (Thomson and Young,
2010). This POMDP-based IM factors the di-
alogue state into conditionally dependent ele-
ments. Dependencies between these elements
can be derived directly from the dialogue on-
tology. These elements are arranged into a dy-
namic Bayesian network which allows for their
marginal probabilities to be updated during
the dialogue, comprising the belief state. The
belief state is then mapped into a smaller-scale
summary space and the decisions are optimised
using the natural actor critic algorithm. In the
Parlance application, hand-crafted policies
261
Figure 5: Screenshot of the recommended
restaurant for the English application
can be compared to learned ones.
2.2 Natural Language Generation
As mentioned above, the server returns the
string to be synthesised by the Google TTS
API. This mobile framework allows for testing
of alternative approaches to NLG. In particu-
lar, we are interested in comparing a surface re-
aliser that uses CRFs against a template-based
baseline. The CRFs take semantically anno-
tated phrase structure trees as input, which it
uses to keep track of rich linguistic contexts.
Our approach has been compared with a num-
ber of competitive state-of-the art surface real-
izers (Dethlefs et al., 2013), and can be trained
from example sentences with annotations of se-
mantic slots.
2.3 Local Search and Knowledge Base
For the English system, the domain database is
populated by the search Yahoo API (Bouchard
and Mika, 2013) with restaurants in San Fran-
sisco. These restaurant search results are
returned based on their longitude and lati-
tude within San Francisco for 5 main areas, 3
price categories and 52 cuisine types contain-
ing around 1,600 individual restaurants.
The Chinese database has been partially
translated from an English database for restau-
rants in Cambridge, UK and search is based
on 3 price categories, 5 areas and 35 cuisine
types having a total of 157 restaurants. Due
to the language-agnostic nature of the Par-
lance system, only the name and address
fields needed to be translated.
3 Future Work
Investigating application side audio compres-
sion and audio streaming over a mobile in-
ternet connection would enable further assess-
ment of the ASR and TTS components used
in the original Parlance system (Hastie et
al., 2013). This would allow for entire research
systems to be plugged directly into the mobile
interface without the use of third party ASR
and TTS.
Future work also involves developing a feed-
back mechanism for evaluation purposes that
does not put undue effort on the user and put
them off using the application. In addition,
this framework can be extended to leverage
hyperlocal and social information of the user
when displaying items of interest.
Acknowledgements
The research leading to this work was funded
by the EC FP7 programme FP7/2011-14
under grant agreement no. 287615 (PAR-
LANCE).
References
H. Bouchard and P. Mika. 2013. Interactive hy-
perlocal search API. Technical report, Yahoo
Iberia, August.
N. Dethlefs, H. Hastie, H. Cuay?huitl, and
O. Lemon. 2013. Conditional Random Fields
for Responsive Surface Realisation Using Global
Features. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics (ACL), Sofia, Bulgaria.
H. Hastie, M.A. Aufaure, P. Alexopoulos,
H. Cuay?huitl, N. Dethlefs, M. Gasic,
J. Henderson, O. Lemon, X. Liu, P. Mika,
N. Ben Mustapha, V. Rieser, B. Thomson,
P. Tsiakoulis, Y. Vanrompay, B. Villazon-
Terrazas, and S. Young. 2013. Demonstration
of the PARLANCE system: a data-driven
incremental, spoken dialogue system for in-
teractive search. In Proceedings of the 14th
Annual Meeting of the Special Interest Group
on Discourse and Dialogue (SIGDIAL), Metz,
France, August.
B. Thomson and S. Young. 2010. Bayesian up-
date of dialogue state: A POMDP framework
for spoken dialogue systems. Computer Speech
and Language, 24(4):562?588.
262
