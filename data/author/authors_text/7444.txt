Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Proceedings of NAACL HLT 2007, Companion Volume, pages 77?80,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
ILR-Based MT Comprehension Test with Multi-Level Questions 
 
Douglas Jones, Martha Herzog, Hussny Ibrahim, Arvind Jairam, Wade Shen,  
Edward Gibson and Michael Emonts 
 MIT Lincoln Laboratory 
Lexington, MA 02420 
{DAJ,Arvind,SWade}@LL.MIT.EDU 
MHerzog2005@comcast.net 
DLI Foreign Language Center
Monterey, CA 93944 
{Hussny.Ibrahim,Michael.Emonts}
@monterey.army.mil  
MIT Brain and Cognitive 
Sciences Department 
Cambridge MA, 02139 
EGibson@MIT.EDU 
Abstract 
We present results from a new Interagency 
Language Roundtable (ILR) based compre-
hension test. This new test design presents 
questions at multiple ILR difficulty levels 
within each document. We incorporated 
Arabic machine translation (MT) output 
from three independent research sites, arbi-
trarily merging these materials into one MT 
condition.  We contrast the MT condition, 
for both text and audio data types, with high 
quality human reference Gold Standard 
(GS) translations.  Overall, subjects 
achieved 95% comprehension for GS and 
74% for MT, across 4 genres and 3 diffi-
culty levels. Surprisingly, comprehension 
rates do not correlate highly with translation 
error rates, suggesting that we are measur-
ing an additional dimension of MT quality.   
We observed that it takes 15% more time 
overall to read MT than GS.  
1 Introduction 
The official Defense Language Proficiency Test 
(DLPT) is constructed according to rigorous and 
well-established principles that have been devel-
oped to measure the foreign language proficiency 
of human language learners in U.S. Department of 
Defense settings.  In 2004, a variant of that test 
type was constructed, following the general DLPT 
design principles, but modified to measure the 
quality of machine translation.  This test, known as 
the DLPTstar (Jones et al 2005),  was based on 
authentic Arabic materials at ILR  text difficulty 
levels 1, 2, and 3, accompanied by constructed-
response questions at matching levels.  The ILR 
level descriptors, used throughout the U.S. gov-
ernment, can be found at the website cited in the 
list of references. The text documents were pre-
sented in two conditions in English translation: (1) 
professionally translated into English, and (2) ma-
chine translated with state-of-the art MT systems, 
often quite garbled.  Results showed that native 
readers of English could generally pass the Levels 
1 and 2 questions on the test, but not those at Level 
3.  Also, Level 1 comprehension was less than ex-
pected, given the low level of the original material.  
It was not known whether the weak Level 1 per-
formance was due to systematic deficits in MT 
performance at Level 1, or whether the materials 
were simply mismatched to the MT capabilities. 
In this paper, we present a new variant of the 
test, using materials specifically created to test the 
capabilities of the MT systems.  To guarantee that 
the MT systems were up to the task of processing 
the documents, we used the DARPA GALE 2006 
evaluation data sets, against which several research 
sites were testing MT algorithms.  We arbitrarily 
merged the MT output from three sites. The ILR 
difficulty of the documents ranged from Level 2 to 
Level 3, but the test did not contain any true Level 
1 documents.  To compensate for this lack, we 
constructed questions about Level 1 elements (e.g., 
personal and place names) in Level 2 and 3 docu-
ments.  A standard DLPT would have more varia-
tion at Level 1.  
2 Related and Previous Work 
Earlier work in MT evaluation incorporated an in-
formativeness measure, based on comprehension 
test answers, in addition to fluency, a measure of 
output readability without reference to a gold stan-
dard, and adequacy, a measure of accuracy with 
reference to a gold standard translation (White and 
O'Connell, 1994).  Later MT evaluation found flu-
ency and adequacy to correlate well enough with 
automatic measures (BLEU), and since compre-
hension tests are relatively more expensive to cre-
ate, the informativeness test was not used in later 
77
MT evaluations, such as the ones performed by 
NIST from 2001-2006.  In other work, task-based 
evaluation has been used for MT evaluation (Voss 
and Tate, 2006), which measures human perform-
ance on exhaustively extracting ?who?, ?when?, and 
?where? type elements in MT output. The DLPT-
star also uses this type of factual question, particu-
larly for Level 2 documents, but not exhaustively.  
Instead, the test focuses on text elements most 
characteristic of the levels as defined in the ILR 
scale.  At Level 3, for example, questions may 
concern abstract concepts or hypotheses found in 
the documents.  Applying the ILR construct pro-
vides Defense Department decision makers with 
test scores that are readily interpretable. 
3 Test Construction and Administration 
In this paper, we present a new test, based entirely 
on the DARPA GALE 2006 evaluation data, se-
lecting approximately half of the material for our 
test. We selected twenty-four test documents, with 
balanced coverage across four genres: newswire, 
newsgroups, broadcast news and talk radio.  Our 
target was to have at least 2500 words for each 
genre, which we exceeded slightly with approxi-
mately 12,200 words in total for the test.  We be-
gan with a random selection of documents and 
adjusted it for better topic coverage.  We con-
structed an exhaustive set of questions for each 
document, approximately 200 questions in total.  
The questions ranged in ILR difficulty, from "0+, 
1,1+, 2, 2+ and 3, with Levels 0+, 1 and 1+ com-
bined to a pseudo-level we called L1~, providing 
four levels of difficulty to be measured.  We di-
vided the questions into two sets, and each indi-
vidual subject answered questions for one of the 
sets. The test itself was constructed by a DLPT 
testing expert and a senior native-speaking Arabic 
language instructor, using only the original Arabic 
documents and the Gold Standard translations.  
They had no access to any machine translation 
output during the test construction or scoring. 
In August 2006, we administered the test at MIT 
to 49 test subjects who responded to announce-
ments for paid experimental subjects.  The subjects 
read the documents in a Latin square design, mean-
ing that each subject saw each document, but only 
in one of the two conditions, randomly assigned.  
Subjects were allowed 5 hours to complete the test.  
Since the questions were divided into two sets for 
each document, the actual set of 49 subjects 
yielded approximately 25 ?virtual subjects? read-
ing the full list of 228 questions.  The mean time 
spent on testing, not counting breaks or subject 
orientation, was 2.5 hours; fastest was 1.1 hours, 
slowest was 3.4 hours. 
The subject responses were hand-graded by the 
two testing experts, following the pre-established 
answers in the test protocol.  There was no pre-
assessment of whether information was preserved 
or garbled in the MT when designing questions or 
responses in the test protocol.  The testing experts 
were provided the reference translations and the 
original Arabic documents, but not the MT during 
scoring.  Moreover, test conditions were masked in 
order to provide a blind assessment.  The two test-
ing experts provided both preliminary and final 
scores; multiple passes provided an opportunity to 
clarify the correct answers and to normalize scor-
ing.  The scoring agreement rate was 96% for the 
final scores. 
4 Overall Results 
The overall result for comprehension accuracy was 
95% for subjects reading the Gold Standard trans-
lation and 74% for reading Machine Translation, 
across each of the genres and difficulty levels. The 
comprehension accuracy for each genre is shown 
in Figure 1. The two text genres score better than 
the audio genres, which is to be expected because 
the audio MT condition has more opportunities for 
error.  Within each modality, the more standard, 
more structured genre fares better: newswire re-
sults are better than newsgroup results, and the 
more structured genre of broadcast news scores 
better than the less constrained, less structured 
conversations present in the talk radio shows. 
 
 
Figure 1. Comprehension Accuracy per Genre  
97% 93% 94% 94%
80% 77% 
72% 66%
0%
20%
40%
60%
80%
100%
Newswire Broadcast News Talk Radio 
GS
MT
Newsgroups 
Overall Comprehension Accuracy 
78
The break-down by ILR level of difficulty for each 
question is shown in Figure 2.  The general trend is 
consistent with what has been observed previously 
(Jones et al 2005).  The best results are at Level 2; 
Level 1 does well but not as well as expected.  
Thus the test has provided a key finding, which is 
that MT systems perform more poorly on Level 1, 
even when the data is matched to their capabilities. 
Level 3 is very challenging for the MT condition, 
and also more difficult in the GS condition.  Using 
a standard 70 percent passing threshold, responses 
to questions on all MT documents, except for 
Level 3, received a passing grade. 
 
Figure 2. Comprehension Accuracy per Level. 
To provide a snapshot of the ILR levels: L1 in-
dicates sentence-level comprehensibility, and may 
include factual local announcements, etc.; L2 indi-
cates paragraph-level comprehensibility; factual/ 
concrete, covering a wide spectrum of topics (poli-
tics, economy, society, culture, security, science); 
L3 involves extended discourse comprehensibility; 
the ability to understand hypotheses, supported 
opinion, implications, and abstract linguistic for-
mulations, etc. 
It was not possible to balance Level 3 documents 
across genres within the GALE evaluation data; 
except for those taken from Talk Radio, most 
documents did not reach that level of complexity.  
Hence, genre and difficulty level were not com-
pletely independent in this test. 
5 Comprehension and Translation Error 
We expect to see a relationship between compre-
hension rates and translation error.  In an idealized 
case, we may expect a precise inverse correlation.  
We then compared comprehension rates with Hu-
man Translation Error Rate (HTER), an error 
measure for machine translation that counts the 
number of human edits required to change system 
MT output so that it contains all and only the in-
formation present in a Gold Standard reference 
(NIST, 2006).  The linear regression line in Figure 
3 shows the kind of inverse correlation we might 
expect.  Subjects lose about 12% in comprehension 
for every 10% of translation error. The R2 value is 
33%.  The low correlation suggests that the com-
prehension results are measuring a somewhat inde-
pendent aspect of MT quality, which we feel is 
important.  HTER does not directly address the 
facts that not all MT errors are equally important 
and that the texts contain inherent redundancy that 
the readers use to answer the questions.  For ex-
ploratory purposes, we divide the graph of Figure 3 
into four quadrants.  Quadrant I and IV contain 
expected behavior: 122 data points of good transla-
tions and good comprehension results versus 43 
points of bad translations and poor comprehension.  
Q-II has 24 robust points: the translations have 
high error, but somehow managed to contain 
enough well-translated words that people can an-
swer the questions.  Q-III has 28 fragile points: the 
few translation errors impaired comprehension. 
 
Figure 3. Comprehension vs. Translation Error. 
We point out that there is a 1-to-1 mapping be-
tween comprehension questions and individual 
sub-passages of the documents in the data.  Each 
point in Figure 3 plots the HTER of a single seg-
ment versus the average comprehension score on 
the corresponding question. The good and bad 
items are essentially a sanity-check on the experi-
mental design.  We expect to see good comprehen-
sion when translations are good, and we expect to 
see poor comprehension when translations are bad.  
Next we will examine the two other types: fragile 
and robust translations. 
Overall Comprehension Accuracy 
97% 96% 91% 88%
77% 82% 76% 
51%
0% 
20% 
40% 
60% 
80% 
100% 
L1~ L2 L2+ L3
GS
MT
  Q-I (Good)                        Q-II (Robust) 
122 points (57%)               24 points (10%)                          
(All Levels and Genres)
0%
20%
40%
60%
80%
100%
0% 20% 40% 60% 80% 100%
x = Translation Error (HTER) 
y = Comprehension (DLPT*)
Q-III (Fragile)                      Q-IV (Bad) 
28 points (13%)                 43 points (20%)                          
79
A fragile translation is one that has a good 
HTER score but a bad comprehension score.  A 
sample fragile translation is one from a broadcast 
news which asks for a particular name:  the HTER 
was a respectable 24%, but the MT comprehension 
accuracy was a flat 0%, since the name was miss-
ing.  Everyone reading GS answered correctly. 
A robust translation is one that has a bad HTER 
score but still manages to get a good comprehen-
sion score.  A sample robust translation is one 
drawn from a posting providing instructions for 
foot massage.  The text was quite garbled, with an 
HTER score of 48%, but the MT comprehension 
accuracy was a perfect 100%. Everyone reading 
the GS condition also answered the question cor-
rectly, which was that one should start a foot mas-
sage with oil. We note in passing that the highest 
error rate for a question with 100% comprehension 
is about 50%, shown with the up-arrow in Figure 
3.  We should be surprised to see any items with 
100% comprehension for HTER rates above 50%, 
considering Shannon?s estimate that written Eng-
lish is about 50% redundant. We expect that MT 
readers are making use of their general world 
knowledge to interpret the garbled MT output.  A 
challenge is to identify robust translations, which 
are useful despite their high translation error rate. 
6 Detailed Discussion 
In this section we will discuss several aspects of 
the test in more detail: the scoring methodology, 
including a discussion of partial credit and inter-
rater agreement; timing information; questions 
about personal names. 
Each correct answer was assigned a score of 1, 
and each incorrect answer was assigned a score of 
0.  Partial credit was assigned on an ad-hoc basis, 
but normalized for scoring by assigning all non-
integer scores to 0.5.  This method yielded scores 
that were generally at the midpoint between binary 
scoring, in which non-integer scored were uni-
formly mapped either harshly to 0 or leniently to 1, 
the average difference between harsh and lenient 
scoring being approximately 11%.  Inter-rater 
agreement was 96%. 
The testing infrastructure we used recorded the 
amount of time spent on each document.  The gen-
eral trend is that people spend longer on MT than 
on GS.  The mean percentage of time spent on MT 
compared with GS is 115% per item, meaning that 
it takes 15% more time to read MT than GS. The 
standard error was 4%.  The median is 111%; 
minimum is 89% and maximum is 159%.  In future 
analysis and experimentation we will conduct more 
fine-grained temporal estimates.    
As we have seen in previous experiments, the 
performance for personal names is lower than for 
non-names.  We observed that the name questions 
have 71% comprehension accuracy, compared with 
the 83% for questions about things other than per-
sonal names.  
7 Conclusions and Future Work 
We have long felt that Level 2 is the natural and 
successful level for machine translation.  The abil-
ity to present concrete factual information that can 
be retrieved by the reader, without requirements 
for understanding the style, tone, or organizational 
pattern used by the writer seemed to be present in 
the previous work. It is worth pointing out that 
though we have many Level 1 questions, we are 
still not really testing Level 1 because the test does 
not contain true Level 1 documents. In future tests 
we wish to include Level 1 documents and ques-
tions.  
Continuing along these lines, we are currently 
creating two new tests. We are constructing a new 
Arabic DLPT-star test, tailoring the document se-
lection more specifically for comprehension testing 
and ensuring texts and tasks are at the intended 
ILR levels. We are also constructing a Mandarin 
Chinese test with similar design specifications.  
We intend for both of these tests to be available for 
a public machine translation evaluation to be con-
ducted in 2007. 
References 
Doddington, G. 2002. Automatic Evaluation of Machine 
Translation Quality Using N-gram Co-Occurrence 
Statistics. Proceedings of HLT 2002. 
NIST 2006. GALE Go/No-Go Eval Plan; www.nist.gov/ 
speech/tests/gale/2006/doc/GALE06_evalplan.v2.pdf 
Jones, D. A., W. Shen, et al 2005a. Measuring Transla-
tion Quality by Testing English Speakers with a New 
DLPT for Arabic. Int?l Conf. on Intel. Analysis. 
Interagency Language Roundtable Website. 2005. ILR 
Skill Level Descriptions: http://www.govtilr.org 
Voss, Clare and Calandra Tate. 2006. Task-based 
Evaluation of MT Engines. European Association for 
Machine Translation conference. 
White, JS and TA O'Connell. 1994. Evaluation in the 
ARPA machine translation program: 1993 method-
ology. Proceedings of the HLT workshop. 
80
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177?180,
Prague, June 2007. c?2007 Association for Computational Linguistics 
Moses: Open Source Toolkit for Statistical Machine Translation 
Philipp Koehn 
Hieu Hoang  
Alexandra Birch 
Chris Callison-Burch 
University of Edin-
burgh1 
Marcello Federico 
Nicola Bertoldi 
ITC-irst2 
Brooke Cowan 
Wade Shen 
Christine Moran 
MIT3 
Richard Zens 
RWTH Aachen4 
Chris Dyer 
University of Maryland5 
 
Ond?ej Bojar 
Charles University6 
Alexandra Constantin 
Williams College7 
Evan Herbst 
Cornell8 
1 pkoehn@inf.ed.ac.uk, {h.hoang, A.C.Birch-Mayne}@sms.ed.ac.uk, callison-burch@ed.ac.uk. 
2{federico, bertoldi}@itc.it. 3 brooke@csail.mit.edu, swade@ll.mit.edu, weezer@mit.edu. 4 
zens@i6.informatik.rwth-aachen.de. 5 redpony@umd.edu. 6 bojar@ufal.ms.mff.cuni.cz. 7 
07aec_2@williams.edu. 8 evh4@cornell.edu 
 
Abstract 
We describe an open-source toolkit for sta-
tistical machine translation whose novel 
contributions are (a) support for linguisti-
cally motivated factors, (b) confusion net-
work decoding, and (c) efficient data for-
mats for translation models and language 
models. In addition to the SMT decoder, 
the toolkit also includes a wide variety of 
tools for training, tuning and applying the 
system to many translation tasks.  
1 Motivation 
Phrase-based statistical machine translation 
(Koehn et al 2003) has emerged as the dominant 
paradigm in machine translation research. How-
ever, until now, most work in this field has been 
carried out on proprietary and in-house research 
systems. This lack of openness has created a high 
barrier to entry for researchers as many of the 
components required have had to be duplicated. 
This has also hindered effective comparisons of the 
different elements of the systems. 
By providing a free and complete toolkit, we 
hope that this will stimulate the development of the 
field. For this system to be adopted by the commu-
nity, it must demonstrate performance that is com-
parable to the best available systems. Moses has 
shown that it achieves results comparable to the 
most competitive and widely used statistical ma-
chine translation systems in translation quality and 
run-time (Shen et al 2006). It features all the ca-
pabilities of the closed sourced Pharaoh decoder 
(Koehn 2004). 
Apart from providing an open-source toolkit 
for SMT, a further motivation for Moses is to ex-
tend phrase-based translation with factors and con-
fusion network decoding. 
The current phrase-based approach to statisti-
cal machine translation is limited to the mapping of 
small text chunks without any explicit use of lin-
guistic information, be it morphological, syntactic, 
or semantic. These additional sources of informa-
tion have been shown to be valuable when inte-
grated into pre-processing or post-processing steps. 
Moses also integrates confusion network de-
coding, which allows the translation of ambiguous 
input. This enables, for instance, the tighter inte-
gration of speech recognition and machine transla-
tion. Instead of passing along the one-best output 
of the recognizer, a network of different word 
choices may be examined by the machine transla-
tion system. 
Efficient data structures in Moses for the 
memory-intensive translation model and language 
model allow the exploitation of much larger data 
resources with limited hardware. 
177
 2 Toolkit 
The toolkit is a complete out-of-the-box trans-
lation system for academic research. It consists of 
all the components needed to preprocess data, train 
the language models and the translation models. It 
also contains tools for tuning these models using 
minimum error rate training (Och 2003) and evalu-
ating the resulting translations using the BLEU 
score (Papineni et al 2002).  
Moses uses standard external tools for some of 
the tasks to avoid duplication, such as GIZA++ 
(Och and Ney 2003) for word alignments and 
SRILM for language modeling.  Also, since these 
tasks are often CPU intensive, the toolkit has been 
designed to work with Sun Grid Engine parallel 
environment to increase throughput.  
In order to unify the experimental stages, a 
utility has been developed to run repeatable ex-
periments. This uses the tools contained in Moses 
and requires minimal changes to set up and cus-
tomize. 
The toolkit has been hosted and developed un-
der sourceforge.net since inception. Moses has an 
active research community and has reached over 
1000 downloads as of 1st March 2007.  
The main online presence is at  
http://www.statmt.org/moses/ 
where many sources of information about the 
project can be found. Moses was the subject of this 
year?s Johns Hopkins University Workshop on 
Machine Translation (Koehn et al 2006). 
The decoder is the core component of Moses. 
To minimize the learning curve for many research-
ers, the decoder was developed as a drop-in re-
placement for Pharaoh, the popular phrase-based 
decoder. 
In order for the toolkit to be adopted by the 
community, and to make it easy for others to con-
tribute to the project, we kept to the following 
principles when developing the decoder: 
? Accessibility 
? Easy to Maintain 
? Flexibility 
? Easy for distributed team development 
? Portability 
It was developed in C++ for efficiency and fol-
lowed modular, object-oriented design. 
3 Factored Translation Model 
Non-factored SMT typically deals only with 
the surface form of words and has one phrase table, 
as shown in Figure 1. 
i am buying you a green cat
using phrase dictionary:
i
 am buying
you
a
green
cat
je
ach?te
vous
un
vert
chat
a une
je vous ach?te un chat vert
Translate:
 
In factored translation models, the surface 
forms may be augmented with different factors, 
such as POS tags or lemma. This creates a factored 
representation of each word, Figure 2.  
1 1 1 / sing /
                                 
je vous achet un chat
PRO PRO VB ART NN
je vous acheter un chat
st st st present masc masc
? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?
1 1 / 1 sing sing
i buy you a cat
PRO VB PRO ART NN
i tobuy you a cat
st st present st
? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?  
 
Mapping of source phrases to target phrases 
may be decomposed into several steps. Decompo-
sition of the decoding process into various steps 
means that different factors can be modeled sepa-
rately. Modeling factors in isolation allows for 
flexibility in their application. It can also increase 
accuracy and reduce sparsity by minimizing the 
number dependencies for each step. 
For example, we can decompose translating 
from surface forms to surface forms and lemma, as 
shown in Figure 3. 
Figure 2. Factored translation 
Figure 1. Non-factored translation 
178
  
Figure 3. Example of graph of decoding steps 
By allowing the graph to be user definable, we 
can experiment to find the optimum configuration 
for a given language pair and available data.  
The factors on the source sentence are consid-
ered fixed, therefore, there is no decoding step 
which create source factors from other source fac-
tors. However, Moses can have ambiguous input in 
the form of confusion networks. This input type 
has been used successfully for speech to text 
translation (Shen et al 2006). 
Every factor on the target language can have its 
own language model. Since many factors, like 
lemmas and POS tags, are less sparse than surface 
forms, it is possible to create a higher order lan-
guage models for these factors. This may encour-
age more syntactically correct output. In Figure 3 
we apply two language models, indicated by the 
shaded arrows, one over the words and another 
over the lemmas. Moses is also able to integrate 
factored language models, such as those described 
in (Bilmes and Kirchhoff 2003) and (Axelrod 
2006). 
4 Confusion Network Decoding 
Machine translation input currently takes the 
form of simple sequences of words. However, 
there are increasing demands to integrate machine 
translation technology into larger information 
processing systems with upstream NLP/speech 
processing tools (such as named entity recognizers, 
speech recognizers, morphological analyzers, etc.). 
These upstream processes tend to generate multiple, 
erroneous hypotheses with varying confidence. 
Current MT systems are designed to process only 
one input hypothesis, making them vulnerable to 
errors in the input.  
In experiments with confusion networks, we 
have focused so far on the speech translation case, 
where the input is generated by a speech recog-
nizer. Namely, our goal is to improve performance 
of spoken language translation by better integrating 
speech recognition and machine translation models. 
Translation from speech input is considered more 
difficult than translation from text for several rea-
sons. Spoken language has many styles and genres, 
such as, formal read speech, unplanned speeches, 
interviews, spontaneous conversations; it produces 
less controlled language, presenting more relaxed 
syntax and spontaneous speech phenomena. Fi-
nally, translation of spoken language is prone to 
speech recognition errors, which can possibly cor-
rupt the syntax and the meaning of the input. 
There is also empirical evidence that better 
translations can be obtained from transcriptions of 
the speech recognizer which resulted in lower 
scores. This suggests that improvements can be 
achieved by applying machine translation on a 
large set of transcription hypotheses generated by 
the speech recognizers and by combining scores of 
acoustic models, language models, and translation 
models. 
Recently, approaches have been proposed for 
improving translation quality through the process-
ing of multiple input hypotheses. We have imple-
mented in Moses confusion network decoding as 
discussed in (Bertoldi and Federico 2005), and de-
veloped a simpler translation model and a more 
efficient implementation of the search algorithm. 
Remarkably, the confusion network decoder re-
sulted in an extension of the standard text decoder. 
5 Efficient Data Structures for Transla-
tion Model and Language Models 
With the availability of ever-increasing 
amounts of training data, it has become a challenge 
for machine translation systems to cope with the 
resulting strain on computational resources. Instead 
of simply buying larger machines with, say, 12 GB 
of main memory, the implementation of more effi-
cient data structures in Moses makes it possible to 
exploit larger data resources with limited hardware 
infrastructure. 
A phrase translation table easily takes up giga-
bytes of disk space, but for the translation of a sin-
gle sentence only a tiny fraction of this table is 
needed. Moses implements an efficient representa-
tion of the phrase translation table. Its key proper-
ties are a prefix tree structure for source words and 
on demand loading, i.e. only the fraction of the 
phrase table that is needed to translate a sentence is 
loaded into the working memory of the decoder. 
179
 For the Chinese-English NIST  task, the mem-
ory requirement of the phrase table is reduced from 
1.7 gigabytes to less than 20 mega bytes, with no 
loss in translation quality and speed (Zens and Ney 
2007). 
The other large data resource for statistical ma-
chine translation is the language model. Almost 
unlimited text resources can be collected from the 
Internet and used as training data for language 
modeling. This results in language models that are 
too large to easily fit into memory. 
The Moses system implements a data structure 
for language models that is more efficient than the 
canonical SRILM (Stolcke 2002) implementation 
used in most systems. The language model on disk 
is also converted into this binary format, resulting 
in a minimal loading time during start-up of the 
decoder.  
An even more compact representation of the 
language model is the result of the quantization of 
the word prediction and back-off probabilities of 
the language model. Instead of representing these 
probabilities with 4 byte or 8 byte floats, they are 
sorted into bins, resulting in (typically) 256 bins 
which can be referenced with a single 1 byte index. 
This quantized language model, albeit being less 
accurate, has only minimal impact on translation 
performance (Federico and Bertoldi 2006). 
6 Conclusion and Future Work 
This paper has presented a suite of open-source 
tools which we believe will be of value to the MT 
research community. 
We have also described a new SMT decoder 
which can incorporate some linguistic features in a 
consistent and flexible framework. This new direc-
tion in research opens up many possibilities and 
issues that require further research and experimen-
tation. Initial results show the potential benefit of 
factors for statistical machine translation, (Koehn 
et al 2006) and (Koehn and Hoang 2007). 
References 
Axelrod, Amittai. "Factored Language Model for Sta-
tistical Machine Translation." MRes Thesis. 
Edinburgh University, 2006. 
Bertoldi, Nicola, and Marcello Federico. "A New De-
coder for Spoken Language Translation Based 
on Confusion Networks." Automatic Speech 
Recognition and Understanding Workshop 
(ASRU), 2005. 
Bilmes, Jeff A, and Katrin Kirchhoff. "Factored Lan-
guage Models and Generalized Parallel Back-
off." HLT/NACCL, 2003. 
Koehn, Philipp. "Pharaoh: A Beam Search Decoder for 
Phrase-Based Statistical Machine Translation 
Models." AMTA, 2004. 
Koehn, Philipp, Marcello Federico, Wade Shen, Nicola 
Bertoldi, Ondrej Bojar, Chris Callison-Burch, 
Brooke Cowan, Chris Dyer, Hieu Hoang, 
Richard Zens, Alexandra Constantin, Christine 
Corbett Moran, and Evan Herbst. "Open 
Source Toolkit for Statistical Machine Transla-
tion". Report of the 2006 Summer Workshop at 
Johns Hopkins University, 2006. 
Koehn, Philipp, and Hieu Hoang. "Factored Translation 
Models." EMNLP, 2007. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
"Statistical Phrase-Based Translation." 
HLT/NAACL, 2003. 
Och, Franz Josef. "Minimum Error Rate Training for 
Statistical Machine Translation." ACL, 2003. 
Och, Franz Josef, and Hermann Ney. "A Systematic 
Comparison of Various Statistical Alignment 
Models." Computational Linguistics 29.1 
(2003): 19-51. 
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. "BLEU: A Method for Automatic 
Evaluation of Machine Translation." ACL, 
2002. 
Shen, Wade, Richard Zens, Nicola Bertoldi, and 
Marcello Federico. "The JHU Workshop 2006 
Iwslt System." International Workshop on Spo-
ken Language Translation, 2006. 
Stolcke, Andreas. "SRILM an Extensible Language 
Modeling Toolkit." Intl. Conf. on Spoken Lan-
guage Processing, 2002. 
Zens, Richard, and Hermann Ney. "Efficient Phrase-
Table Representation for Machine Translation 
with Applications to Online MT and Speech 
Recognition." HLT/NAACL, 2007. 
180
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 657?669,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Finding Good Enough: A Task-Based Evaluation of Query Biased
Summarization for Cross Language Information Retrieval
Jennifer Williams, Sharon Tam, Wade Shen
MIT Lincoln Laboratory Human Language Technology Group
244 Wood Street, Lexington, MA 02420 USA
jennifer.williams@ll.mit.edu, sharontam@alum.mit.edu
swade@ll.mit.edu
Abstract
In this paper we present our task-based
evaluation of query biased summarization
for cross-language information retrieval
(CLIR) using relevance prediction. We de-
scribe our 13 summarization methods each
from one of four summarization strate-
gies. We show how well our methods
perform using Farsi text from the CLEF
2008 shared-task, which we translated to
English automtatically. We report preci-
sion/recall/F1, accuracy and time-on-task.
We found that different summarization
methods perform optimally for different
evaluation metrics, but overall query bi-
ased word clouds are the best summariza-
tion strategy. In our analysis, we demon-
strate that using the ROUGE metric on our
sentence-based summaries cannot make
the same kinds of distinctions as our evalu-
ation framework does. Finally, we present
our recommendations for creating much-
needed evaluation standards and datasets.
1 Introduction
Despite many recent advances in query biased
summarization for cross-language information re-
trieval (CLIR), there are no existing evaluation
standards or datasets to make comparisons among
different methods, and across different languages
(Tombros and Sanderson, 1998; Pingali et al.,
2007; McCallum et al., 2012; Bhaskar and Bandy-
opadhyay, 2012). Consider that creating this
kind of summary requires familiarity with tech-
niques from machine translation (MT), summa-
rization, and information retrieval (IR). In this
This work was sponsored by the Federal Bureau of Inves-
tigation under Air Force Contract FA8721-05-C-0002. Opin-
ions, interpretations, conclusions, and recommendations are
those of the authors and are not necessarily endorsed by the
United States Government.
paper, we arrive at the intersection of each of
these research areas. Query biased summariza-
tion (also known as query-focused, query-relevant,
and query-dependent) involves automatically cap-
turing relevant ideas and content from a document
with respect to a given query, and presenting it as a
condensed version of the original document. This
kind of summarization is mostly used in search en-
gines because when search results are tailored to a
user?s information need, the user can find texts that
they are looking for more quickly and more ac-
curately (Tombros and Sanderson, 1998; Mori et
al., 2004). Query biased summarization is a valu-
able research area in natural language processing
(NLP), especially for CLIR. Users of CLIR sys-
tems meet their information needs by submitting
their queries in L
1
to search through documents
that have been composed in L
2
, even though they
may not be familiar with L
2
(Hovy et al., 1999;
Pingali et al., 2007).
There are no standards for objectively evaluat-
ing summaries for CLIR ? a research gap that we
begin to address in this paper. The problem we
explore is two-fold: what kinds of summaries are
well-suited for CLIR applications, and how should
the summaries be evaluated. Our evaluation is ex-
trinsic, that is to say we are interested in how sum-
marization affects performance on a different task
(Mani et al., 2002; McKeown et al., 2005; Dorr
et al., 2005; Murray et al., 2009; McCallum et
al., 2012). We use relevance prediction as our ex-
trinsic task: a human must decide if a summary
for a given document is relevant to a particular in-
formation need, or not. Relevance prediction is
known to be useful as it correlates with some au-
tomatic intrinsic methods as well (President and
Dorr, 2006; Hobson et al., 2007). To the best of
our knowledge, we are the first to apply this eval-
uation framework to cross language query biased
summarization.
Each one of the summarization methods that we
657
present in this paper belongs to one of the fol-
lowing strategies: (1) unbiased full machine trans-
lated text, (2) unbiased word clouds, (3) query bi-
ased word clouds, and (4) query biased sentence
summaries. The methods and strategies that we
present are fast, cheap, and language-independent.
All of these strategies are extractive, meaning that
we used existing parts of a document to create the
condensed version, or summary.
We approach our task as an engineering prob-
lem: the goal is to decide if summaries are good
enough to help CLIR system users find what they
are looking for. We have simplified the task by as-
suming that a set of documents has already been
retrieved from a search engine, as CLIR tech-
niques are outside the scope of this paper. We
predict that showing the full MT English text as
a summarization strategy would not be particu-
larly helpful in our relevance prediction task be-
cause the words in the text could be mixed-up,
or sentences could be nonsensical, resulting in
poor readability. For the same reasons, we expect
that showing the full MT English text would take
longer to arrive at a relevance decision. Finally,
we predict that query biased summaries will result
in faster, more accurate decisions from the partic-
ipants (Tombros and Sanderson, 1998).
We treat the actual CLIR search engine as if it
were a black box so that we can focus on evaluat-
ing if the summaries themselves are useful. As a
starting point, we begin with some principles that
we expect to hold true when we evaluate. These
principles provide us with the kind of framework
that we need for a productive and judicious dis-
cussion about how well a summarization method
works. We encourage the NLP community to
consider the following concepts when developing
evaluation standards for this problem:
? End-user intelligiblity
? Query-salience
? Retrieval-relevance
Summaries should be presented to the end-user in
a way that is both concise and intelligible, even
if the machine translated text is difficult to under-
stand. Our notions of query-salience and retrieval-
relevance capture the expectation that good sum-
maries will be efficient enough to help end-users
fulfill their information needs. For query-salience,
we want users to positively identify relevant doc-
uments. Similarly, for retrieval-relevance we want
users to be able to find as many relevant docu-
ments as possible.
This paper is structured as follows: Section 2
presents related work; Section 3 describes our data
and pre-processing; Section 4 details our sum-
marization methods and strategies; Section 5 de-
scribes our experiments; Section 6 shows our re-
sults and analysis; and in Section 7, we conclude
and discuss some future directions for the NLP
community.
2 Related Work
Automatic summarization is generally a well-
investigated research area. Summarization is a
way of describing the relationships of words in
documents to the information content of that doc-
ument (Luhn, 1958; Edmunson, 1969; Salton and
Yang, 1973; Robertson and Walker, 1994; Church
and Gale, 1999; Robertson, 2004). Recent work
has looked at creating summaries of single and
multiple documents (Radev et al., 2004; Erkan and
Radev, 2004; Wan et al., 2007; Yin et al., 2012;
Chatterjee et al., 2012), as well as summary eval-
uation (Jing et al., 1998; Tombros and Sanderson
1998; Mani et al., 1998; Mani et al., 1999; Mani,
2001; Lin and Hovy, 2003; Lin, 2004; Nenkova
et al., 2007; Hobson et al., 2007; Owczarzak
et al., 2012), query and topic biased summariza-
tion (Berger and Mittal, 2000; Otterbacher et al.,
2005; Daume and Marcu, 2006; Chali and Joty,
2008; Otterbacher et al., 2009; Bando et al., 2010;
Bhaskar and Bandyopadhyay, 2012; Harwath and
Hazen, 2012; Yin et al., 2012), and summarization
across languages (Pingali et al., 2007; Or?asan and
Chiorean, 2008; Wan et al., 2010; Azarbonyad et
al., 2013).
2.1 Query Biased Summarization
Previous work most closely related to our own
comes from Pingali et al., (2007). In their work,
they present their method for cross-language
query biased summarization for Telugu and En-
glish. Their work was motivated by the need for
people to have access to foreign-language docu-
ments from a search engine even though the users
were not familiar with the foreign language, in
their case English. They used language model-
ing and translation probability to translate a user?s
query into L
2
, and then summarized each docu-
ment in L
2
with respect to the query. In their final
step, they translated the summary from L
2
back
658
to L
1
for the user. They evaluated their method
on the DUC 2005 query-focused summarization
shared-task with ROUGE scores. We compare our
methods to this work also on the DUC 2005 task.
Our work demonstrates the first attempt to draw at
a comparison between user-based studies and in-
trinsic evaluation with ROUGE. However, one of
the limitations with evaluating this way is that the
shared-task documents and queries are monolin-
gual.
Bhaskar and Bandyopadhyay (2012) tried a
subjective evaluation of extractive cross-language
query biased summarization for 7 different lan-
guages. They extracted sentences, then scored and
ranked the sentences to generate query dependent
snippets of documents for their cross lingual in-
formation access (CLIA) system. However, the
snippet quality was determined subjectively based
on scores on a scale of 0 to 1 (with 1 being best).
Each score indicated annotator satisfaction for a
given snippet. Our evaluation methodology is ob-
jective: we ask users to decide if a given document
is relevant to an information need, or not.
2.2 Machine Translation Effects
Machine translation quality can affect summa-
rization quality. Wan et al. (2010) researched
the effects of MT quality prediction on cross-
language document summarization. They gener-
ated 5-sentence summaries in Chinese using En-
glish source documents. To select sentences, they
used predicted translation quality, sentence posi-
tion, and sentence informativeness. In their eval-
uation, they employed 4 Chinese-speakers to sub-
jectively rate summaries on a 5-point scale (5 be-
ing best) along the dimensions of content, read-
ability, and overall impression. They showed that
their approach of using MT quality scores did im-
prove summarization quality on average. While
their findings are important, their work did not ad-
dress query biasing or objective evaluation of the
summaries. We attempt to overcome limitations of
machine translation quality by using word clouds
as one of our summarization strategies.
Knowing when to translate is another challenge
for cross-language query biased summarization.
Several options exist for when and what to trans-
late during the summarization process: (1) the
source documents can be translated, (2) the user?s
query can be translated, (3) the final summary can
be translated, or (4) some combination of these.
An example of translating only the summaries
themselves can be found in Wan et al., (2010).
On the other hand, Pingali et al. (2007) translated
the queries and the summaries. In our work, we
used gold-translated queries from the CLEF 2008
dataset, and machine translated source documents.
We briefly address this in our work, but note that a
full discussion of when and what to translate, and
those effects on summarization quality, is outside
of the scope of this paper.
2.3 Summarization Evaluation
There has been a lot of work towards developing
metrics for understanding what makes a summary
good. Evaluation metrics are either intrinsic or ex-
trinsic. Intrinsic metrics, such as ROUGE, mea-
sure the quality of a summary with respect to gold
human-generated summaries (Lin, 2004; Lin and
Hovy, 2003). Generating gold standard summaries
is expensive and time-consuming, a problem that
persists with cross-language query biased summa-
rization because those summaries must be query
biased as well as in a different language from the
source documents.
On the other hand, extrinsic metrics measure the
quality of summaries at the system level, by look-
ing at overall system performance on downstream
tasks (Jing et al, 1998; Tombros and Sanderson,
1998). One of the most important findings for
query biased summarization comes from Tombros
and Sanderson (1998). In their monolingual task-
based evaluation, they measured user speed and
accuracy at identifying relevant documents. They
found that query biased summarization improved
the user speed and accuracy when the user was
asked to make relevance judgements for IR tasks.
We also expect that our evaluation will demon-
strate that user speed and accuracy is better when
summaries are query biased.
3 Data and Pre-Processing
We used data from the Farsi CLEF 2008 ad hoc
task (Agirre et al., 2009). Each of the queries in-
cluded in this dataset consisted of a title, narrative,
and description. Figure 1 shows an example of the
elements of a CLEF 2008 query. All of the au-
tomatic query-biasing in this work was based on
the query titles. For our human relevance predic-
tion task on Mechanical Turk, we used the nar-
rative version. The CLEF 2008 dataset included
a ground-truth answer key indicating which docu-
659
ments were relevant to each query. For each query,
we randomly selected 5 documents that were rele-
vant as well as 5 documents that were not relevant.
The subset of CLEF 2008 data that we used there-
fore consisted of 500 original Farsi documents and
50 parallel English-Farsi queries. Next we will de-
scribe our text pre-processing steps for both lan-
guages as well as how we created our parallel En-
glish documents.
Figure 1: Full MT English summary and CLEF
2008 English query (title, description, narrative).
3.1 English Documents
All of our English documents were created auto-
matically by translating the original Farsi docu-
ments into English (Drexler et al., 2012). The
translated documents were sentence-aligned with
one sentence per line. For all of our summariza-
tion experiments (except unbised full MT text),
we processed the text as follows: removed extra
spaces, removed punctuation, folded to lowercase,
and removed digits. We also removed common
English stopwords
2
from the texts.
3.2 Farsi Documents
We used the original CLEF 2008 Farsi docu-
ments for two of our summarization methods. We
stemmed words in each document using automatic
morphological analysis with Morfessor CatMAP.
We note that within-sentence punctuation was re-
moved during this process (Creutz and Lagus,
2007). We also removed Farsi stopwords and dig-
its.
4 Summarization Strategies
All of our summarization methods were extrac-
tive except for unbiased full machine translated
text. In this section, we describe each of our
13 summarization methods which we have orga-
nized into one of the following strategies: (1) un-
biased full machine translated text, (2) unbiased
2
English and Farsi stopword lists from:
http://members.unine.ch/jacques.savoy/clef/index.html
word cloud summaries, (3) query biased word
cloud summaries, and (4) query biased sentence
summaries. Regardless of which summarization
method used, we highlighted words in yellow that
also appeard in the query. Let t be a term in
document d where d ? D
L
and D
L
is a collec-
tion of documents in a particular language. Note
that for our summarization methods, term weight-
ings were calculated separately for each language.
While |D| = 1000, we calculated term weightings
based on |D
E
| = 500 and |D
F
| = 500. Finally,
let q be a query where q ? Q and Q is our set of
50 parallel English-Farsi CLEF queries. Assume
that log refers to log
10
.
Figure 2: Full MT English summary and CLEF
2008 English query.
4.1 Unbiased Full Machine Translated
English
Our first baseline approach was to use all of the
raw machine translation output (no subsets of
the sentences were used). Each summary there-
fore consisted of the full text of an entire doc-
ument automatically translated from Farsi to En-
glish (Drexler et al., 2012). Figure 2 shows an ex-
ample full text document translated from Farsi to
English and a gold-standard English CLEF query.
Note that we use this particular document-query
pair as an example throughout this paper (docu-
ment: H-770622-42472S8, query: 10.2452/552-
AH). According to the CLEF answer key, the sam-
ple document is relevant to the sample query.
4.2 Unbiased Word Clouds
For our second baseline approach, we ranked
terms in a document and displayed them as word
clouds. Word clouds are one a way to arrange
a collection of words where each word can vary
660
in size. We used word clouds as a summariza-
tion strategy to overcome any potential disfluen-
cies from the machine translation output and also
to see if they are feasible at all for summarization.
All of our methods for word clouds used words
from machine translated English text. Each term-
ranking method below generates different ranked
lists of terms, which we used to create different
word clouds. We created one word cloud per doc-
ument using the top 12 ranked words. We used
the raw term scores to scale text font size, so that
words with a highter score appeared larger and
more prominent in a word cloud. Words were
shuffled such that the exact ordering of words was
at random.
I: Term Frequency (TF) Term frequency is
very commonly used for finding important terms
in a document. Given a term t in a document d,
the number of times that term occurs is:
tf
t,d
= |t ? d|
II: Inverse Document Frequency (IDF) The
idf term weighting is typically used in IR and
other text categorization tasks to make distinc-
tions between documents. The version of idf that
we used throughout our work came from Erkan
and Radev (2004) and Otterbacher et al. (2009),
in keeping consistent with theirs. Let N be the
number of documents in the collection, such that
N = |D| and n
t
is the number of documents that
contain term t, such that n
t
= |{d ? D : t ? d}|,
then:
idf
t
= log
N + 1
0.5? n
t
While idf is usually thought of as a type of
heuristic, there have been some discussions about
its theoretical basis (Robertson, 2004; Robertson
and Walker, 1994; Church and Gale, 1999; Salton
and Yang, 1973). An example of this summary is
shown in Figure 3.
III: Term Frequency Inverse Document Fre-
quency (TFIDF) We use tfidf
t,d
term weight-
ing to find terms which are both rare and impor-
tant for a document, with respect to terms across
all other documents in the collection:
tfidf
t,d
= tf
t,d
? idf
t
4.3 Query Biased Word Clouds
We generated query biased word clouds following
the same principles as our unbiased word clouds,
Figure 3: Word cloud summary for inverse docu-
ment frequency (IDF), for query ?Tehran?s stock
market?.
namely the text font scaling and highlighting re-
mained the same.
IV. Query Biased Term Frequency (TFQ) In
Figure 4 we show a sample word cloud summary
based on query biased term frequency. We define
query biased term frequency tfQ at the document
level, as:
tfQ
t,d,q
=
{
2tf
t,d
, if t ? q
tf
t,d
, otherwise
Figure 4: Word cloud summary for query biased
term frequency (TFQ), for query ?Tehran?s stock
market?.
V. Query Biased Inverse Document Frequency
(IDFQ) Since idf helps with identifying terms
that discriminate documents in a collection, we
would expect that query biased idf would help to
identify documents that are relevant to a query:
idfQ
t,q
=
{
2idf
t
, if t ? q
idf
t
, otherwise
VI. Query Biased TFIDF (TFIDFQ) We de-
fine query biased tf ? idf similarly to our TFQ
and IDFQ, at the document level:
tfidfQ
t,d,q
=
{
2tf
t,d
? idf
t
, if t ? q
tf
t,d
? idf
t
, otherwise
661
Figure 5: Word cloud summary for scaled query
biased term frequency (SFQ) for query ?Tehran?s
stock market?.
VII. Query Biased Scaled Frequency (SFQ)
This term weighting scheme, which we call scaled
query biased term frequency or sfQ, is a variant of
the traditional tf?idf weighting. First, we project
the usual term frequency into log-space, for a term
t in document d with:
tfS
t,d
= log(tf
t,d
)
We let tfS
t,d
? 0 when tf
t,d
= 1. We believe that
singleton terms in a document provide no indica-
tion that a document is query-relevant, and trea-
ment of singleton terms in this way would have the
potential to reduce false-positives in our relevance
prediction task. Note that scaled term frequency
differs from Robertson?s (2004) inverse total term
frequency in the sense that our method involves no
consideration of term position within a document.
Scaled query biased term frequency, shown in Fig-
ure 5, is defined as:
sfQ
t,d,q
=
{
2tfS
t,d
? idf
t
, if t ? q
tfS
t,d
? idf
t
, otherwise
VIII. Word Relevance (W) We adapted an
existing relevance weighting from Allan et al.,
(2003), that was originally formulated for ranking
sentences with respect to a query. However, we
modified their originaly ranking method so that we
could rank individual terms in a document instead
of sentences. Our method for word relevance, W
is defined as:
W
t,d,q
= log(tf
t,d
+ 1)? log(tf
t,q
+ 1)? idf
t
In W , term frequency values are smoothed by
adding 1. The smoothing could especially af-
fect rare terms and singletons, when tf
t,d
is very
low. All terms in a query or a document will
be weighted and each term could potentially con-
tribute to summary.
4.4 Query Biased Sentence Summaries
Sentences are a canonical unit to use in extractive
summaries. In this section we describe four differ-
ent sentence scoring methods that we used. These
methods show how to calculate sentence scores for
a given document with respect to a given query.
Sentences for a document were always ranked us-
ing the raw score value output generated from a
scoring method. Each document summary con-
tained the top 3 ranked sentences where the sen-
tences were simply listed out. Each of these meth-
ods used sentence-aligned English machine trans-
lated documents, and two of them also used the
original Farsi text.
IX. Sentence Relevance (REL) Our sentence
relevance scoring method comes from Allan et al.
(2003). The sentence weight is a summation over
words that appear in the query. We provide their
sentence scoring formula here. This calculates the
relevance score for a sentence s from document d,
to a query q:
rel
(s|q)
=
?
t?s
log(tf
t,s
+1)? log(tf
t,q
+1)? idf
t
Terms will occur in either the sentence or the
query, or both. We applied this method to machine
tranlsated English text. The output of this method
is a relevance score for each sentence in a given
document. We used those scores to rank sentences
in each document from our English machine trans-
lated text.
X. Query Biased Lexrank (LQ) We imple-
mented query biased LexRank, a well-known
graph-based summarization method (Otterbacher
et al., 2009). It is a modified version of the orig-
inal LexRank algorithm (Erkan and Radev, 2004;
Page et al., 1998). The similarity metric, sim
x,y
,
also known as idf-modified cosine similarity, mea-
sures the distance between two sentences x and y
in a document d, defined as:
sim
x,y
=
?
t?x,y
tf
t,x
? tf
t,y
? (idf
t
)
2
?
?
t?x
tfidf
2
t,x
?
?
t?y
tfidf
2
t,y
We used sim
x,y
to score the similarity of
sentence-to-sentence, resulting in a similarity
662
Figure 6: LQP - projecting Farsi sentence scores
onto parallel English sentences.
graph where each vertex was a sentence and each
edge was the cosine similarity between sentences.
We normalized the cosine matrix with a similarity
threshold (t = 0.05), so that sentences above this
threshold were given similarity 1, and 0 otherwise.
We used rel
(s|q)
to score sentence-to-query. The
LexRank score for each sentence was then calcu-
lated as:
LQ
s|q
=
d? rel
s|q
?
z?C
rel
z|q
+ (1? d)?
?
v?adj[s]
sim
s,v
?
r?adj[v]
sim
v,r
LQ
v|q
where C is the set of all sentences in a given doc-
ument. Here the parameter d is just a damper to
designate a probability of randomly jumping to
one of the sentences in the graph (d = 0.7). We
found the stationary distribution by applying the
power method ( = 5), which is guaranteed to
converge to a stationary distribution (Otterbacher
et al., 2009). The output of LQ is a score for each
sentence from a given document with respect to
a query. We used that score to rank sentences in
each document from our English machine trans-
lated text.
XI. Projected Cross-Language Query Biased
Lexrank (LQP) We introduce LQP to describe
a way of scoring and ranking sentences such that
the L
1
(English) summaries are biased from the
L
2
(Farsi) query and source document. Our gold-
standard Farsi queries were included with our
CLEF 2008 data, making them more reliable than
what we could get from automatic translation.
First, sentences from each Farsi document were
scored with Farsi queries using LQ, described
above. Then each LQ score was projected onto
sentence-aligned English. We demonstrate LQP
Figure 7: LQC - Farsi sentence scores are com-
bined with parallel English sentence scores to ob-
tain sentence re-ranking.
in Figure 6. By doing this, we simulated trans-
lating the user?s English query into Farsi with the
best possible query translation, before proceed-
ing with summarization. This approach to cross-
language summarization could be of interest for
CLIR systems that do query translation on-the-fly.
It is also of interest for summarization systems that
need to utilize previously translated source docu-
ments the capability is lacking to translate sum-
maries from L
2
to L
1
.
XII. Combinatory Query Biased Lexrank
(LQC) Another variation of LexRank that we
introduce in this work is LQC, which combines
LexRank scores from both languages to re-rank
sentences. A visual summary of this method is
shown in Figure 7. We accomplished our re-
ranking by first running LQ on Farsi and English
separately, then adding the two scores together.
This combination of Farsi and English scores pro-
vided us with a different way to score and rank
sentences, compared with LQ and LQP . The
idea behind combinatory query biased LexRank
is to take advantage of sentences which are high-
ranking in Farsi but not in English. The LQC
method exploits all available resources in our
dataset: L
1
and L
2
queries as well as L
1
and L
2
documents.
5 Experiments
We tested each of our summarization methods and
overall strategies in a task-based evaluation frame-
work using relevance prediction. We used Me-
chanical Turk for our experiments since it has been
shown to be useful for evaluating NLP systems
(Callison-Burch 2009; Gillick and Liu, 2010). We
obtained human judgments for whether or not a
document was considered relevant to a query, or
information need. We measured the relevance
663
judgements by precision/recall/F1, accuracy, and
also time-on-task based on the average response
time per Human Intelligence Task (HIT).
5.1 Mechanical Turk
In our Mechanical Turk experiment, we used ter-
minology from CLEF 2008 to describe a query
as an ?information need?. All of the Mechanical
Turk workers were presented with the following
for their individual HIT: instructions, an informa-
tion need and one summary for a document. Work-
ers were asked to indicate if the given summary
for a document was relevant to the given informa-
tion need (Hobson et al., 2007). Workers were
not shown the original Farsi source documents.
We paid workers $0.01 per HIT. We obtained 5
HITs for each information need and summary pair.
We used a built-in approval rate qualification pro-
vided by Mechanical Turk to restrict which work-
ers could work on our tasks. Each worker had an
approval rate of at least 95
Instructions: Each image below consists
of a statement summarizing the informa-
tion you are trying to find from a set
of documents followed by a summary
of one of the documents returned when
you query the documents. Based on the
summary, choose whether you think the
document returned is relevant to the in-
formation need. NOTE: It may be diffi-
cult to distinguish whether the document
is relevant as the text may be difficult
to understand. Just use your best judg-
ment.
6 Results and Analysis
We present our experiment results and additional
analysis. First, we report the results of our rel-
evance prediction task, showing performance for
individual summarization methods as well as per-
formance for the overall strategies. Then we
show analysis of our results from the monolin-
gual question-biased shared-task for DUC 2005,
as well as a comparison to previous work.
6.1 Results for Individual Methods
Our results are shown in Table 1. We report perfor-
mance for 13 individual methods as well as over-
all peformance on the 4 different summarization
strategies. To calculate the performance for each
strategy, we used the arithmetic mean of the corre-
sponding individual methods. We measured preci-
sion, recall and F1 to give us a sense of our sum-
maries might influence document retrieval in an
actual CLIR system. We also measured accuracy
and time-on-task. For these latter two metrics, we
distinguish between summaries that were relevant
(R) and non-relevant (NR).
All of the summarization-based methods fa-
vored recall over precision: documents were
marked ?relevant? more often than ?non-relevant?.
For many of the methods shown in Table 1, work-
ers spent more time correctly deciding ?relevant?
than correctly deciding ?non-relevant?. This sug-
gests some workers participated in our Mechanical
Turk task purposefully. For many of the summa-
rization methods, workers were able to positively
identify relevant documents.
From Table 1 we see that Full MT performed
better on precision than all of the other methods
and strategies, but we note that performance on
precision was generally very low. This might be
due to Mechanical Turk workers overgeneraliz-
ing by marking summaries as relevant when they
were not. Some individual methods preserve our
principle of retrieval-relevance, as indicated by
the higher recall scores for SQF, LQEF, and TFQ.
That is to say, these particular query biased sum-
marization methods can be used to assist users
with identifying more relevant documents. The ac-
curacy on relevant documents addresses our prin-
ciple of query-salience, and it is especially high
for our query-biased methods: LQEF, SQF, LQ,
and TFQ. The results also seem to fit our intuition
that the summary in Figure 3 seems less relevant
to the summaries shown in Figures 4 & 5 even
though these are the same documents biased on
the same query ?Tehran stock market?.
Overall, query biased word clouds outperform
the other summarization strategies for 5 out of
7 metrics. This could be due to the fact that
word clouds provide a very concise and overview
of a document, which is one of the main goals
for automatic summarization. Along these lines,
word clouds are probably not subject to the effects
of MT quality and we believe it is possible that
MT quality could have had a negative impact on
our query biased extracted sentence summaries, as
well as our full MT English texts.
664
Table 1: Individual method results: precision/recall/F1, time-on-task, and accuracy. Note that results for
time-on-task and accuracy scores are distinguished for relevant (R) and non-relevant (NR) documents.
Precision, Recall, F1 Time-on-Task Accuracy
Summarization Strategy Prec. Rec. F1 R NR R NR
Unbiased Full MT English 0.653 0.636 0.644 219.5 77.6 0.696 0.712
TF 0.615 0.777 0.686 33.5 34.6 0.840 0.508
IDF 0.537 0.470 0.501 84.7 45.8 0.444 0.700
TFIDF 0.647 0.710 0.677 33.2 38.2 0.772 0.656
Unbiased Word Clouds 0.599 0.652 0.621 50.5 39.5 0.685 0.621
TFQ 0.605 0.809 0.692 55.3 82.4 0.864 0.436
IDFQ 0.582 0.793 0.671 23.6 31.6 0.844 0.436
TFIDFQ 0.599 0.738 0.661 37.9 26.9 0.804 0.500
SFQ 0.591 0.813 0.685 55.7 49.4 0.876 0.504
W 0.611 0.738 0.669 28.2 28.9 0.840 0.564
Query Biased Word Clouds 0.597 0.778 0.675 36.4 34.2 0.846 0.488
REL 0.582 0.746 0.654 30.6 44.3 0.832 0.548
LQ 0.549 0.783 0.646 64.4 54.8 0.868 0.292
LQP 0.578 0.734 0.647 28.2 28.0 0.768 0.472
LQC 0.557 0.810 0.660 33.9 38.8 0.896 0.292
Query Biased Sentences 0.566 0.768 0.651 39.2 41.5 0.841 0.401
Table 2: Comparison of peer systems on DUC
2005 shared-task for monolingual question-biased
summarization, f-scores from ROUGE-2 and
ROUGE-SU4.
Peer ID ROUGE-2 ROUGE-SU4
17 0.07170 0.12970
8 0.06960 0.12790
4 0.06850 0.12770
Tel-Eng-Sum 0.06048 0.12058
LQ 0.05124 0.09343
REL 0.04914 0.09081
6.2 Analysis with DUC 2005
We analysed our summarization methods by
comparing two of our sentence-based methods
(LQ and REL) with peers from the monolin-
gual question-biased summarization shared-task
for DUC 2005. Even though DUC 2005 is a mono-
lingual task, we decided to use it as part of our
analysis for two reasons: (1) to see how well we
could do with query/question biasing while ignor-
ing the variables introduced by MT and cross-
language text, and (2) to make a comparison to
previous work. Pingali et al., (2007) also used this
the same DUC task to assess their cross-language
query biased summarization system. Systems
from the DUC 2005 question-biased summariza-
tion task were evaluated automatically against hu-
man gold-standard summaries using ROUGE (Lin
and Hovy, 2003) . Our results from the DUC
2005 shared-task are shown in Table 2, reported
as ROUGE-2 and ROUGE-SU4 f-scores, as these
two variations of ROUGE are the most helpful
(Dang, 2005; Pingali et al., 2007).
Table 2 shows scores for several top peer sys-
tems, as well as results for the Tel-Eng-Sum
method from Pingali et al., (2007). While we have
reported f-scores in our analysis, we also note that
our implementations of LQ and REL outperform
all of the DUC 2005 peer systems for precision, as
shown in Table 3. We also know that ROUGE can-
not be used for comparing sentence summaries to
ranked lists of words and there are no existing in-
trinsic methods to make that kind of comparison.
Therefore we were able to successfully compare
just 2 of our sentence-based methods to previous
work using ROUGE.
7 Discussion and Future Work
Cross-language query biased summarization is an
important part of CLIR, because it helps the user
decide which foreign-language documents they
might want to read. But, how do we know if
665
Table 3: Top 3 system precision scores for
ROUGE-2 and ROUGE-SU4.
Peer ID ROUGE-2 ROUGE-SU4
LQ 0.08272 0.15197
REL 0.0809 0.15049
15 0.07249 0.13129
a query biased summary is ?good enough? to be
used in a real-world CLIR system? We want to
be able to say that we can do query biased sum-
marization just as well for both monolingual and
cross-language IR systems. From previous work,
there has been some variability with regard to
when and what to translate - variables which have
no impact on monolingual summarization. We at-
tempted to address this issue with two of our meth-
ods: LQP and LQC. To fully exploit the MT vari-
able, we would need many more relevance pre-
diction experiments using humans who know L
1
and others who know L
2
. Unfortunately in our
case, we were not able to find Farsi speakers on
Mechanical Turk. Access to these speakers would
have allowed us to try further experiments as well
as other kinds of analysis.
Our results on the relevance prediction task
tell us that query biased summarization strategies
help users identify relevant documents faster and
with better accuracy than unbiased summaries.
Our findings support the findings of Tombros and
Sanderson (1998). Another important finding is
that now we can weigh tradeoffs so that different
summarization methods could be used to optimize
over different metrics. For example, if we want
to optimize for retrieval-relevance we might select
a summarization method that tends to have higher
recall, such as scaled query biased term frequency
(SFQ). Similarly, we could optimize over accu-
racy on relevant documents, and use Combinatory
LexRank (LQC) with Farsi and English together.
We have shown that the relevance prediction
tasks can be crowdsourced on Mechanical Turk
with reasonable results. The data we used from
the Farsi CLEF 2008 ad-hoc task included an an-
swer key, but there were no parallel English docu-
ments. However, in order for the NLP community
to make strides in evaluating cross-language query
biased summarization for CLIR, we will need star-
dards and data. Optimal data would be parallel
datasets consisting of documents in L
1
and L
2
with queries in L
1
and L
2
along with an answer
key specifying which documents are relevant to
the queries. Further we would also need sets of
human gold-standard query biased summaries in
L
1
and L
2
. These standards and data would al-
low us to compare method-to-method across dif-
ferent languages, while simultaneously allowing
us to tease apart other variables such as: when and
what to translate, translation quality, methods for
biasing, and type of summarization strategy (sen-
tences, words, etc). And of course it would be bet-
ter if this standard dataset was multilingual instead
of billingual, for obvious reasons.
We have approached cross-language query bi-
ased summarization as a stand-alone problem,
treating the CLIR system and document retrieval
as a black box. However, summaries need to pre-
serve query-salience: summaries should not make
it more difficult to positively identify relavant doc-
uments. And they should also preserve retrieval-
relevance: summaries should help users identify
as many relevant documents as possible.
Acknowledgments
We would like to express thanks to David Har-
wath at MIT Computer Science and Artificial In-
telligence Laboratory (CSAIL), who helped us de-
velop and implement ideas in this paper. We also
want to thank Terry Gleason from MIT Lincoln
Laboratory for providing machine translations.
References
Eneko Agirre, Giorgio Maria Di Nunzio, Nicola Ferro,
Thomas Mandl, and Carol Peters. CLEF 2008: Ad
hoc track overview. In Evaluating Systems for Mul-
tilingual and Multimodal Information Access, pp
15?37. Springer Berlin Heidelberg, 2009.
James Allan, Courtney Wade, and Alvaro Bolivar. Re-
trieval and Novelty Detection at the Sentence Level.
In Proceedings of the 26th Annual International
ACM SIGIR Conference on Research and Develop-
ment in Informaion Retrieval, (SIGIR ?03). ACM,
New York, NY, USA, 314-321.
Hosein Azarbonyad, Azadeh Shakery, and Heshaam
Faili. Exploiting Multiple Translation Resources for
English-Persian Cross Language Information Re-
trieval. In P. Forner, H. M?uller, R. Paredes, P. Rosso,
and B. Stein, editors, Information Access Evalua-
tion. Multilinguality, Multimodality, and Visualiza-
tion, volume 8138 of Lecture Notes in Computer Sci-
ence, pp 93?99. Springer Berlin Heidelberg, 2013.
Lorena Leal Bando, Falk Scholer, Andrew Turpin.
Constructing Query-biased Summaries: A Compar-
ison of Human and System Generated Snippets. In
666
Proceedings of the Third Symposium on Information
Interaction in Context (IIiX ?10), ACM 2010, New
York, NY, USA, 195-204.
Adam Berger and Vibhu O Mittal. Query-Relevant
Summarization Using FAQs. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, ACL 2000.
Pinaki Bhaskar and Sivaji Bandyopadhyay. Cross-
Lingual Query Dependent Snippet Generation. In-
ternational Journal of Computer Science and Infor-
mation Technology (IJCSIT), 3(4), 2012.
Pinaki Bhaskar and Sivaji Bandyopadhyay. Language
Independent Query Focused Snippet Generation. In
T. Catarci, P. Forner, D. Hiemstra, A. Pe?nas, and
G. Santucci, editors, Information Access Evaluation.
Multilinguality, Multimodality, and Visual Analytics,
volume 7488 of Lecture Notes in Computer Science,
pp 138?140. Springer Berlin Heidelberg, 2012.
Stephen P. Borgatti, Kathleen M. Carley, David Krack-
hardt. On the Robustness of Centrality Measures
Under Conditions of Imperfect Data. Social Net-
works, (28):124?136, 2006.
Florian Boudin, St?ephane Huet, and Juan-Manuel
Torres-Moreno. A Graph-Based Approach to Cross-
Language Multi-Document Summarization. Poli-
bits, (43):113?118, 2011.
Chris Callison-Burch. Fast, Cheap, and Creative: Eval-
uating Translation Quality Using Amazon?s Me-
chanical Turk. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pp 286?295, Singapore, ACL
2009.
Yllias Chali and Shafiq R. Joty. Unsupervised Ap-
proach for Selecting Sentences in Query-Based
Summarization. In Proceedings of the Twenty-First
International FLAIRS Conference, 2008.
Niladri Chatterjee, Amol Mittal, and Shubham Goyal.
Single Document Extractive Text Summarization
Using Genetic Algorithms. In Emerging Applica-
tions of Information Technology (EAIT), 2012 Third
International Conference, pp 19?23, 2012.
Kenneth W. Church and William A. Gale. Inverse Doc-
ument Frequency (IDF): A Measure of Deviations
From Poisson. In Natural language processing us-
ing very large corpora, pages 283?295. Springer,
1999.
Mathias Creutz and Krista Lagus. Unsupervised Mod-
els for Morpheme Segmentation and Morphology
Learning. ACM Transactions on Speech and Lan-
guage Processing, 4(1):3:1?3:34, February 2007.
Hoa Trang Dang. Overview of DUC 2005. In Pro-
ceedings of the Document Understanding Confer-
ence, 2005.
Hal Daum?e III, Daniel Marcu. Bayesian Query-
Focused Summarization. In Proceedings of the 21st
International Conference on Computational Lin-
guistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, ACL 2006.
Jennifer Drexler, Wade Shen, Terry P. Gleason, Timo-
thy R. Anderson, Raymond E. Slyh, Brian M. Ore,
and Eric G. Hansen. The MIT-LL/AFRL IWSLT-
2012 MT System. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation
(IWSLT), Hong Kong, December 2012.
Bonnie J. Dorr, Christof Monz, Stacy President,
Richard Schwartz, and David Zajic. A Methodol-
ogy for Extrinsic Evaluation of Text Summarization:
Does ROUGE Correlate? In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization, pp 1-8. Ann Arbor, ACL 2005.
H. P. Edmundson. New Methods in Automatic Extract-
ing. In Journal of the ACM, 16(2):264?285, April
1969.
G?unes? Erkan and Dragomir R. Radev. Lexrank:
Graph-based lexical centrality as salience in text
summarization. Journal of Artificial Intelligence
Research, 22(1):457?479, December 2004.
Dan Gillick and Yang Liu. Non-Expert Evaluation
of Summarization Systems is Risky. In Proceed-
ings of NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, pp 148-151, Los Angeles, California,
USA, June, 2010.
David Harwath and Timothy J. Hazen. Topic Identi-
fication Based Extrinsic Evaluation of Summariza-
tion Techniques Applied to Conversational Speech.
In Proceedings of ICASSP, 2012: 5073-5076.
Stacy P. Hobson, Bonnie J. Dorr, Christof Monz, and
Richard Schwartz. Task-Eased Evaluation of Text
Summarization Using Relevance Prediction. In In-
formation Processing Management, 43(6): 1482-
1499, 2007.
Hongyan Jing, Regina Barzilay, Kathleen McKeown,
and Michael Elhadad. Summarization Evaluation
Methods: Experiments and Analysis. In Proceed-
ings of American Association for Artificial Ingelli-
gence (AAAI), 1998.
Reza Karimpour, Amineh Ghorbani, Azadeh Pishdad,
Mitra Mohtarami, Abolfazl AleAhmad, Hadi Amiri,
and Farhad Oroumchian. Improving Persian Infor-
mation Retrieval Systems Using Stemming and Part
of Speech Tagging. In Proceedings of the 9th Cross-
language Evaluation Forum Conference on Evaluat-
ing Systems for Multilingual and Multimodal Infor-
mation Access, CLEF 2008, pp 89?96, Berlin, Hei-
delberg, 2009. Springer-Verlag.
667
Chin-Yew Lin. Looking For A Few Good Metrics:
Automatic Summarization Evaluation - How Many
Samples Are Enough? In Proceedings of NTCIR
Workshop 4, Tokyo, Japan, June 2004.
Annie Louis and Ani Nenkova. Automatic Summary
Evaluation without Human Models. In Proceedings
of Empirical Methods in Natural Language Process-
ing, EMNLP 2009.
H. P. Luhn. The Automatic Creation of Literature Ab-
stracts. IBM Journal of Research and Development,
2(2):159?165, April 1958.
Inderjeet Mani, Eric Bloedorn, and Barbara Gates. Us-
ing Cohesion and Coherence Models for Text Sum-
marization. In AAAI Symposium Technical Report
SS-989-06, AAAI Press, 69?76, 1998.
Inderjeet Mani, David House, Gary Klein, Lynette
Hirschman, Therese Firmin, and Beth Sundheim.
The TIPSTER SUMMAC Text Summarization
Evaluation. In Proceedings of European Associa-
tion for Coputational Linguistics, EACL 1999.
Inderjeet Mani. Summarization Evaluation: An
Overview. In Proceedings of the NTCIR Workshop,
Vol. 2, 2001.
Inderjeet Mani, Gary Klein, David House, Lynette
Hirschman, Therese Firmin, and Beth Sundheim.
SUMMAC: A Text Summarization Evaluation. Nat-
ural Language Engineering, 8(1) 43-68. March
2002.
Kathleen McKeown, Rebecca J. Passonneau, David K.
Elson, Ani Nenkova, and Julia Hirschberg. Do Sum-
maries Help? A Task-Based Evaluation of Multi-
Document Summarization. In Proceedings of the
28th Annual International ACM SIGIR Conference
on Research and Development in Information Re-
trieval, pp 210-217. ACM 2005.
Anthony McCallum, Gerald Penn, Cosmin Munteanu,
and Xiaodan Zhu. Ecological Validity and the Eval-
uation of Speech Summarization Quality. In Pro-
ceedings of Workshop on Evaluation Metrics and
System Comparison for Automatic Summarization.
2012 Association for Computational Linguistics,
Stroudsburg, PA, USA, 28-35.
Tatsunori Mori, Masanori Nozawa, and Yoshiaki
Asada. Multi-Answer Focused Multi-Document
Summarization Using a Question-Answering En-
gine. In Proceedings of the 20th International
Conference on Computational Linguistics, COLING
?04, Stroudsburg, PA, USA, ACL 2004.
Gabriel Murray, Thomas Kleinbauer, Peter Poller,
Tilman Becker, Steve Renals, and Jonathan Kilgour.
Extrinsic Summarization Evaluation: A Decision
Audit Task. ACM Transactions on Speech and Lan-
guage Processing, 6(2) Article 2, October 2009.
Ani Nenkova and Kathleen McKeown. A Survey of
Text Summarization Techniques. In C. C. Aggarwal
and C. Zhai, editors, Mining Text Data, pp 43?76.
Springer US, 2012.
Constantin Or?asan and Oana Andreea Chiorean. Eval-
uation of a Cross-Lingual Romanian-English Multi-
Document Summariser. In Proceedings of Lan-
guage Resources and Evaluation Conference, LREC
2008.
Jahna Otterbacher, G?unes? Erkan, and Dragomir R
Ravev. Using Random Walks for Question-focused
Sentence Retrieval. In Proceedings of Human Lan-
guage Technology Conference on Empirical Meth-
ods in Natural Language Processing, Vancouver,
Canada, pp 915-922, EMNLP 2005.
Jahna Otterbacher, G?unes? Erkan, and Dragomir R.
Ravev. Biased LexRank: Passage Retrieval Using
Random Walks With Question-Based Priors. In In-
formation Processing Management, 45(1), January
2009, pp 42-54.
Karolina Owczarzak, John M. Conroy, Hoa Trang
Dang, and Ani Nenkova. An Assessment of the Ac-
curacy of Automatic Evaluation in Summarization.
In Proceedings of the Workshop on Evaluation Met-
rics and System Comparison for Automatic Summa-
rization, pp 1-9, Montr?eal, Canada, ACL 2012.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. The Pagerank Citation Ranking:
Bringing Order to the Web. Technical report, Stan-
ford Digital Library Technologies Project, 1998.
Prasad Pingali, Jagadeesh Jagarlamudi, and Vasudeva
Varma. Experiments in Cross Language Query Fo-
cused Multi-Document Summarization In Work-
shop on Cross Lingual Information Access Address-
ing the Information Need of Multilingual Societies,
IJCAI 2007.
Stacy F. President and Bonnie J. Dorr. Text Sum-
marization Evaluation: Correlating Human Perfor-
mance on an Extrinsic Task With Automatic In-
trinsic Metrics. No. LAMP-TR-133. University of
Maryland College Park Language and Media Pro-
cessing Laboratory Institute for Advanced Computer
Studies (UMIACS), 2006.
Dragomir R. Radev, Hongyan Jing, Malgorzata Sty?s,
and Daniel Tam. Centroid-Based Summarization
of Multiple Documents. InProceedings of In-
formaion Processing Management, 40(6):919?938,
Nov. 2004.
Stephen Robertson. Understanding Inverse Docu-
ment Frequency: on Theoretical Arguments for IDF.
Journal of Documentation, 60(5):503?520, 2004.
Stephen E. Robertson and Steve Walker. Some Simple
Effective Approximations to the 2-Poisson Model
for Probabilistic Weighted Retrieval. In Proceed-
ings of the 17th annual international ACM SIGIR
668
conference on Research and development in infor-
mation retrieval, pp 232?241. Springer-Verlag New
York, Inc., 1994.
Gerard Salton and Chung-Shu Yang. On the Specifica-
tion of Term Values in Automatic Indexing. Journal
of Documentation, 29(4):351?372, 1973.
Anastasios Tombros and Mark Sanderson. Advantages
of Query Biased Summaries in Information Re-
trieval. In Proceedings of the 21st annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pp 2?10. ACM,
1998.
Xiaojun Wan and Jianguo Xiao. Graph-Based
Multi-Modality Learning for Topic-Focused Multi-
Document Summarization. In Proceedings of the
21st international jont conference on Artifical intel-
ligence (IJCAI?09), San Francisco, CA, USA, 1586-
1591.
Xiaojun Wan, Huiying Li, and Jianguo Xiao. Cross-
Language Document Summarization Based on Ma-
chine Translation Quality Prediction. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL ?10). Associ-
ation for Computational Linguistics, Stroudsburg,
PA, USA, 917-926.
Xiaojun Wan, Houping Jia, Shanshan Huang, and Jian-
guo Xiao. Summarizing the Differences in Multilin-
gual News. In Proceedings of the 34th International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, SIGIR ?11, pp 735?
744, New York, NY, USA, 2011. ACM.
Wenpeng Yin, Yulong Pei, Fan Zhang, and Lian?en
Huang. SentTopic-MultiRank: A Novel Ranking
Model for Multi-Document Summarization. In Pro-
ceedings of COLING, pages 2977?2992, 2012.
Junlin Zhang, Le Sun, and Jinming Min. Using
the Web Corpus to Translate the Queries in Cross-
Lingual Information Retrieval. In Proceedings in
2005 IEEE International Conference on Natural
Language Processing and Knowledge Engineering,
2005, IEEE NLP-KE ?05, pp 493?498, 2005.
669
Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 30?38,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Language-Independent Approach to Automatic Text DifficultyAssessment for Second-Language LearnersWade Shen1, Jennifer Williams1, Tamas Marius2, and Elizabeth Salesky ?1
1MIT Lincoln Laboratory Human Language Technology Group,
244 Wood Street Lexingon, MA 02420, USA
{swade,jennifer.williams,elizabeth.salesky}@ll.mit.edu
2DLI Foreign Language Center, Bldg. 420, Room 119 Monterey, CA 93944, USAtamas.g.marius.civ@mail.milAbstract
In this paper, we introduce a new base-
line for language-independent text diffi-
culty assessment applied to the Intera-
gency Language Roundtable (ILR) profi-
ciency scale. We demonstrate that reading
level assessment is a discriminative prob-
lem that is best-suited for regression. Our
baseline uses z-normalized shallow length
features and TF-LOG weighted vectors on
bag-of-words for Arabic, Dari, English,
and Pashto. We compare Support Vector
Machines and the Margin-Infused Relaxed
Algorithm measured by mean squared er-
ror. We provide an analysis of which fea-
tures are most predictive of a given level.1 Introduction
The ability to obtain new materials of an appro-
priate language proficiency level is an obstacle
for second-language learners and educators alike.
With the growth of publicly available Internet and
news sources, learners and instructors of foreign
languages should have ever-increasing access to
large volumes of foreign language text. How-
ever, sifting through this pool of foreign language
data poses a significant challenge. In this paper
we demonstrate two machine learning regression
methods which can be used to help both learn-
ers and course developers by automatically rat-
ing documents based on the text difficulty. These
methods can be used to automatically identify
documents at specific levels in order to speed
course or test development, providing learners
? This work was sponsored by the Department of De-
fense under Air Force Contract FA8721-05-C-0002. Opin-
ions, interpretations, conclusions, and recommendations are
those of the authors and are not necessarily endorsed by the
United States Government.
with custom-tailored materials that match their
learning needs.
ILR (Interagency Language Roundtable) levels
reflect differences in text difficulty for second-
language learners at different stages of their edu-
cation. A description of each level is shown in Ta-
ble 1 (Interagency Language Roundtable, 2013).
Some levels differ in terms of sentence structure,
length of document, type of communication, etc.,
while others, especially the higher levels, differ in
terms of the domain and style of writing. Given
these differences, we expect that both semantic
content and grammar-related features will be nec-
essary to distinguish between documents at differ-
ent levels.Level Description
0 No proficiency
0+ Memorized proficiency
1 Elementary proficiency
1+ Elementary proficiency, plus
2 Limited working proficiency
2+ Limited working proficiency, plus
3 General professional proficiency
3+ General professional proficiency, plus
4 Advanced professional proficienty
4+ Advanced professional proficiency, plus
5 Functionally native proficiency
Table 1: Description of ILR levels.
Automatically determining ILR levels from
documents is a research problem without known
solutions. We have developed and adapted a se-
ries of rating algorithms and a set of experiments
gauging the feasibility of automatic ILR level as-
signment for text documents. Using data provided
by the Defense Language Institute Foreign Lan-
guage Center (DLIFLC), we show that while the
problem is tractable, the performance of automatic
30
methods is not perfect.
Our general approach treats the ILR rating prob-
lem as one of text classification; given the contents
and structure of a document, which of the ILR lev-
els should this document be assigned to? This
differs from traditional topic classification tasks
where word-usage often uniquely defines topics,
since we are also interested in features of text com-
plexity that describe structure. Leveling text is a
problem better fit to regression because reading
level is a continuous scale. We want to know how
close a document is to a given level (or between
levels), so we measured performance using mean
squared error (MSE). We show that language-
independent features can be used for regression
with Support Vector Machines (SVMs) and the
Margin-Infused Relaxed Algorithm (MIRA), and
we present our results for this new baseline for
Arabic, Dari, English, and Pashto. To the best of
our knowledge, this is the first study to systemati-
cally examine a language-independent approach to
readability using the ILR rating scale for second-
language learners.
This paper is structured as follows: Section 2
describes previous work on reading level assess-
ment as a text classification problem, Section 3
describes the two algorithms that we used in our
present work, Section 4 describes our data and ex-
periments, Section 5 reports our results, Section 6
provides an analysis of our results, and Section 7
proposes different kinds of future work that can be
done to improve this baseline.2 Related Work
In this section we describe some work on the read-
ability problem that is most closely related to our
own.
One of the earliest formulas for reading level
assessment, called the Flesch Reading Ease For-
mula, measured readability based on shallow
length features (Flesch, 1948). This metric in-
cluded two measurements: the average number of
words per sentence and the average number of syl-
lables per word. Although these features appear to
be shallow at the offset, the number of syllables
per word could be taken as an abstraction of word
complexity. Those formulas, as well as their var-
ious revisions, have become popular because they
are easy to compute for a variety of applications,
including structuring highly technical text that is
comprehensible at lower reading levels (Kincaid
et al, 1975). Some of the revisions to the Flesch
Reading Ease Formula have included weighting
these shallow features in order to linearly regress
across different difficulty levels.
Much effort has been placed into automating
the scoring process, and recent work on this is-
sue has examined machine learning methods to
treat reading level as a text classification prob-
lem. Schwarm and Ostendorf (2005) worked on
automatically classifying text by grade level for
first-language learners. Their machine learning
approach was a one vs. all method using a set
of SVM binary classifiers that were constructed
for each grade level category: 2, 3, 4, and 5.
The following features were used for classfication:
average sentence length, average number of syl-
lables per word, Flesch-Kincaid score, 6 out-of-
vocabulary (OOV) rate scores, syntactic parse fea-
tures, and 12 language model perplexity scores.
Their data was taken from the Weekly Reader
newspaper, already separated by grade level. They
found that the error rate for misclassification by
more than one grade level was significantly lower
for the SVM classifier than for both Lexile and
Flesch-Kincaid. Petersen and Ostendorf (2009)
later replicated and expanded Schwarm and Os-
tendorf (2005), reaffirming that both classifica-
tion and regression with SVMs provided a better
approximation of readabilty by grade level when
compared with more traditional methods such as
the Flesch-Kincaid score. In the current work, we
also use SVM for regression, but have decided to
report mean squared error as a more meaningful
metric.
In an effort to uncover which features are the
most salient for discriminating among reading lev-
els, Feng et al, (2010) studied classification per-
formance using combinations of different kinds of
readability features using data from the Weekly
Reader newspaper. Their work examined the
following types of features: discourse, language
modeling, parsed syntactic features, POS fea-
tures, shallow length features, as well as some
features replicated from Schwarm and Ostendorf
(2005). They reported classifier accuracy and
mean squared error from two classifiers, SVM and
Logistic Regression, which were used to predict
grade level for grades 2 through 5. While they
found that POS features were the most predictive
overall, they also found that the average number of
words per sentence was the most predictive length
31
feature. This length feature alone achieved 52%
accuracy with the Logistic Regression classifier.
In the present work, we use the average number of
words per sentence as a length feature and show
that this metric has some correspondence with the
different ILR levels.
Another way to examine readability is to treat
it as a sorting problem; that is, given some collec-
tion of texts, to sort them from easiest to most dif-
ficult. Tanaka-Ishii et al, (2010) presented a novel
method for determining readibility based on sort-
ing texts using text from two groups: low difficulty
and high difficulty. They reported their results
in terms of the Spearman correlation coefficient
to compare performance of Flesch-Kincaid, Dale-
Chall, SVM regression, and their sorting method.
They showed that their sorting method was supe-
rior to the other methods, followed by SVM re-
gression. However, they call for a more mod-
ern and efficient approach to the problem, such as
online learning, that would estimate weights for
regression. We answer their call with an online
learning approach in this work.3 Algorithms
In this section, we describe two maximum margin
approaches that we used in our experiments. Both
are based on the principle of structural risk mini-
mization. We selected the SVM algorithm because
of its proven usefulness for automatic readability
assessment. In addition, the Margin-Infused Re-
laxed Algorithm is advantageous because it is an
online algorithm and therefore allows for incre-
mental training while still taking advantange of
structural risk minimization.3.1 Structural Risk Minimization
For many classification and regression problems,
maximum margin approaches are shown to per-
form well with minimal amounts of training data.
In general, these approaches involve linear dis-
criminative classifiers that attempt to learn hy-
perplane decision boundaries which separate one
class from another. Since multiple hyperplanes
that separate classes can exist, these methods add
an additional constraint: they attempt to learn hy-
perplanes while maximizing a region around the
boundary called the margin. We show an exam-
ple of this kind of margin in Figure 1, where the
margin represents the maximum distance between
the decision boundary and support vectors. The
maximum margin approach helps prevent overfit-
ting issues that can occur during training, a princi-
ple called structural risk minimization. Therefore
we experiment with two such margin-maximizing
algorithms, described below.
Figure 1: Graphical depiction of the maximum
margin principle.3.2 Support Vector Machines
For text classification problems, the most popular
maximum margin approach is the SVM algorithm,
introduced by Vapnik (1995). This approach uses
a quadratic programming method to find the sup-
port vectors that define the margin. This is a batch
training algorithm requiring all training data to be
present in order to perform the optimization pro-
cedure (Joachims, 1998a). We used LIBSVM to
implement our own SVM for regression (Chang
and Lin, 2001).
Discriminative methods seek to best divide
training examples in each class from out-of-class
examples. SVM-based methods are examples
of this approach and have been successfully ap-
plied to other text classification problems, includ-
ing previous work on reading level assessment
(Schwarm and Ostendorf, 2005; Petersen and Os-
tendorf, 2009; Feng et al, 2010). This approach
attempts to explicitly model the decision boundary
between classes. Discriminative methods build a
model for each class c that is defined by the bound-
ary between examples of class c and examples
from all other classes in the training data.
32
3.3 Margin-Infused Relaxed Algorithm
Online approaches have the advantage of allowing
incremental adaptation when new labeled exam-
ples are added during training. We implemented
a version of MIRA from Crammer and Singer
(2003), which we used for regression. Cram-
mer and Singer (2003) proved MIRA as an on-
line multiclass classifier that employs the prin-
ciple of structural risk minimization, and is de-
scribed as ultraconservative because it only up-
dates weights for misclassified examples. For
classification, MIRA is formulated as shown in
equation (1):
c? = argmax
c2C
fc(d) (1)
where
fc(d) = w ? d (2)
and w is the weight vector which defines the
model for class c. During training, examples are
presented to the algorithm in an online fashion (i.e.
one at a time) and the weight vector is updated
accourding to the update shown in equation (2):
wt = wt 1 + l(wt 1,dt 1)vt 1 (3)
l(wt 1,dt 1) = ||dt 1  wt 1||  ? (4)
vt 1 = (sign(||dt 1  wt 1||)  ?)dt 1 (5)
where l(?) is the loss function, ? corresponds to
the margin slack, and vt 1 is the negative gradient
of the loss vector for the previously seen example
||dt 1   wt 1||. This update forces the weight
vector towards erroneous examples during train-
ing. The magnitude of the change is proportional
to the l(?). For correct training examples, no up-
date is performed as l(?) = 0. In a binary classi-
fication task, MIRA attempts to minimize the loss
function in (4), such that the magnitude of the dis-
tance between a document vector and the weight
vector is also minimized.
However, unlike topic classification or classi-
fication of words based on their semantic class
where the classes are generally discrete, the ILR
levels lie on a continuum (i.e. level 2 >> level
1 >> level 0). Therefore we are more interested
in using MIRA for regression because we want
to compare the predicted value with the true real-
valued label, rather than a class label. For regres-
sion, we can redefine the MIRA loss function as
follows:
l(wt,dt) = |lt   dt ? wt|  ? (6)
In this case, lt is the correct value (in our case,
ILR level) for training document dt and dt ? wt is
the predicted value given the current weight vector
wt. We expect that minimizing this loss function
cumulatively over the entire training set will yield
a regression model that can predict ILR levels for
unseen documents.
This revised loss function results in a modi-
fied update equation for each online update of
the MIRA weight vector (generating a new set of
weights wt from the previously seen example):
wt = wt 1 + l(wt 1,dt 1)vt 1 (7)
vt 1 = (sign(|lt 1 dt 1 ?wt 1|) ?)dt 1 (8)
vt 1 defines the direction of loss and the mag-
nitude of the update relative to the current train-
ing example dt 1. Since this approach is online,
MIRA does not guarantee minimal loss or maxi-
mummargin constraints for all of the training data.
However, in practice, these methods perform as
well as their SVM counterparts without the need
for batch training (Crammer et al, 2006).4 Experiments4.1 Data
All of our experiments used data from four lan-
guages: Arabic (AR), Dari (DAR), English (EN),
and Pashto (PS). In Table 2, we show the distri-
bution of number of documents per ILR level for
each language. All of our data was obtained from
the Directorate of Language Science and Technol-
ogy (LST) and the Language Technology Evalua-
tion and Application Division (LTEA) at the De-
fense Language Institute Foreign Language Cen-
ter (DLIFLC). The data was compiled using an
online resource (Domino). Language experts (na-
tive speakers) used various texts from the Inter-
net which they considered to be authentic mate-
rial and they created the Global Language Online
Support System (GLOSS) system. The texts were
used to debug the GLOSS system and to see how
well GLOSS worked for the respective languages.
Each of the texts were labeled by two independent
linguists expertly trained in ILR level scoring. The
ratings from these two linguists were then adjudi-
cated by a third linguist. We used the resulting
adjudicated labels for our training and evaluation.
We preprocessed the data by doing the follow-
ing tokenization: removed extra whitespace, nor-
malized URIs, normalized currency, normalized
33
Level AR DAR EN PS
1 204 197 198 197
1+ 200 197 197 199
2 199 201 204 200
2+ 199 194 196 198
3 198 195 202 198
3+ 194 194 198 200
4 198 195 190 195
Overall 1394 1375 1390 1394
Table 2: Total collection documents per language
per ILR level.
numbers, normalized abbreviations, normalized
punctuation, and folded to lowercase. We identi-
fied words by splitting text on whitespace and we
identified sentences by splitting text on punctua-
tion.4.2 Features
It is necessary to define a set of features to help
the regressors distinguish between the ILR levels.
We conducted our experiments using two different
types of features: word-usage features and shallow
length features. Shallow length features are shown
to be useful in reading level prediction tasks (Feng
et al, 2010). Word-usage features, such as the
ones used here, are meant to capture some low-
level topical differences between ILR levels.Word-usage features: Word frequencies (or
weighted word frequencies) are commonly used
as features for topic classification problems, as
these features are highly correlated with topics
(e.g. words like player and touchdown are very
common in documents about topics like football,
whereas they are much less common in documents
about opera). We used TF-LOG weighted word
frequencies on bag-of-words for each document.Length features: In addition to word-usage, we
added three z-normalized length features: (1) av-
erage sentence length (in words) per document,
(2) number of words per document, and (3) aver-
age word length (in characters) per document. We
used these as a basic measure of language level
complexity. These features are easily computed
by automatic means, and they capture some of the
structural differences between the ILR levels.
Figures 2, 3, 4, and 5 show the z-normalized
average word count per sentence for Arabic, Dari,
English, and Pashto respectively. The overall data
set for each language has a normalized mean of
Figure 2: Arabic, z-normalized average word
count per sentence for ILR levels 1, 2 and 3.
Figure 3: Dari, z-normalized average word count
per sentence for ILR levels 1, 2 and 3.
Figure 4: English, z-normalized average word
count per sentence for ILR levels 1, 2 and 3.
34
MIRA SVM (linear)LEN WORDS COMBINED LEN WORDS COMBINEDAR 4.527 0.283 0.222 0.411 0.263 0.198DAR 5.538 0.430 0.330 0.473 0.409 0.301EN 5.155 0.181 0.148 0.430 0.181 0.147PS 5.371 0.410 0.360 1.871 0.393 0.391
Table 3: Performance results (MSE) for SVM and MIRA on Arabic, Dari, English and Pashto for three
different kinds of features/combinations.
Figure 5: Pashto, z-normalized average word
count per sentence for ILR levels 1, 2 and 3.
zero and unit variance, which were calculated sep-
arately for a given length feature. The x-axis
shows the deviation of documents relative to the
data set mean, in units of overall standard devia-
tion. It is clear from the separability of the levels
in these figures that sentence length could be an
important indicator of ILR level, though no fea-
ture is a perfect discriminator. This is indicated by
the significant overlap between the distributions of
document lengths at different ILR levels.4.3 Training
We split the data between training and testing us-
ing an 80/20 split of the total data for each lan-
guage. To formulate the ILR scale as continuous-
valued, we assumed that ?+? levels are 0.5 higher
than their basis (e.g. 2+ = 2.5). Though this may
not be optimal if distances between levels are non-
constant, the best systems in our experiments show
good prediction performance using this assump-
tion.
Both of the classifiers were trained to predict the
ILR value as a continuous value using regression.
We measured the performance of each method in
terms of the mean squared error on the unseen test
documents. We tested the following three con-
ditions: length-based features only (LEN), word-
usage features only (WORDS), and word and
length features combined (COMBINED). Since
each algorithm (SVM and MIRA) has a number
of parameters that can be tuned to optimize per-
formance, we report results for the best settings for
each of the algorithms. These settings were deter-
mined by sweeping parameters to optimize perfor-
mance on the training data for a range of values,
for both MIRA and SVM. For both algorithms,
we varied the number of training iterations from
500 to 3100 for each language, with stepsize of
100. We also varied the minimum word frequency
count from 2 to 26, with stepsize 1. For MIRA
only, we varied the slack parameter from 0.0005
to 0.0500, with stepsize 0.00025. For SVM (linear
kernel only), we varied the C parameter and   at a
coarse setting of 2n with values of n ranging from
-15 to 6 with stepsize 1.5 Results
We compared the performance of the online
MIRA approach with the SVM-based approach.
Table 3 shows the overall performance of MIRA
regression and SVM regression, respectively, for
the combinations of features for each language.
Mean squared error was averaged over all of the
levels in a given language. MIRA is an approx-
imation to SVM, however one of the advantages
of MIRA is that it is an online algorithm so it is
adaptable after training and training can be en-
hanced later with more data with a small number
of additional data points.
Figures 6 and 7 show the per-level performance
for each classifier with the overall best features
(COMBINED) for each language. The highest
level (Level 4) and lowest levels (Level 1) tend to
35
exhibit the worst performance across all languages
for each regression method. Poorer performance
on the outlying levels could be due to overfitting
for both SVM and MIRA on those levels. The
ILR scale includes 4 major levels at half-step in-
tervals between each one. We are not sure if us-
ing a different scale, such as grade levels ranging
from 1 to 12, would also exhibit poorer perfor-
mance on the outlying levels because the highest
ILR level corresponds to native-like fluency. This
U-shaped performance is seen across both classi-
fiers for each of the languages.6 Analysis
Our results show that SVM slightly outperformed
MIRA for all of the languages. We believe that
the reason whyMIRA performed worse than SVM
is because it was overfit during training whereas
SVM was not. This could be due to the parame-
ters that we set during our sweep in training. We
selected C and   as parameters to SVM linear-
kernel for the best performance. The   values for
English and Arabic were set at more than 1000
times smaller than the values for Pashto and Dari
(AR: =6.1035156 ? 10 5, DAR: =0.0078125,
EN: =3.0517578 ? 10 5, PS: =0.03125). This
means that the margins for Pashto and Dari were
set to be larger respective to English and Arabic.
One reason why these margins were larger is be-
cause the features that we used had more discrimi-
native power for English and Arabic. In fact, both
MIRA and SVM performed worse on Pashto and
Dari.
Since the method described here makes use of
Figure 6: MIRA performance (MSE) per ILR level
for each language.
Figure 7: SVM performance (MSE) per ILR level
for each language.
linear classifiers that weigh word-usage and length
features, it is possible to examine the weights that
a classifier learns during training to see which fea-
tures the algorithm deems most useful in discrim-
inating between ILR levels. One way to do this
is to use a multiclass classifier on our data for the
categorical levels (e.g. 1, 1+, 2, etc.) and exam-
ine the weights that were generated for each class.
MIRA is formulated to be a multiclass classifier
so we examined its weights for the features. We
chose MIRA instead of SVM, even though LIB-
SVM supports multiclass classification, because
we wanted to capture differences between levels
which we could not do with one vs. all. We exam-
ined classifier weights of greatest magnitude to see
which features were the most indicative and most
contra-indicative for that level. We report these
two types of features for Level 3 and Level 4 in
Tables 4 and 5, respectively. Level 3 documents
can have some complex topics, such as politics
and art, however it can be noted that some of the
more abstract topics like love and hate are contra-
indicative of Level 3. On the other hand, we see
that abstract topics are highly indicative Level 4
documents where topics such as philosophy, reli-gion, virtue, hypothesis, and theory are discussed.
We also note that moral is highly contra-indicative
of Level 3 but is highly indicative of Level 4.7 Discussion and Future Work
We have presented an approach to score docu-
ments based on their ILR level automatically us-
ing language-independent features. Measures of
structural complexity like the length-based fea-
36
MostIndicative + Most Contra-Indicative -
obama 1.739 said -2.259
to 1.681 your -1.480
republicans 1.478 is -1.334
? 1.398 moral -0.893
than 1.381 this -0.835
more 1.365 were -0.751
cells 1.355 area -0.751
american 1.338 love -0.730
americans 1.335 says -0.716
art 1.315 hate -0.702
it?s 1.257 against -0.682
could 1.180 people -0.669
democrats 1.143 body -0.669
as 1.139 you -0.666
a 1.072 man -0.652
but 1.041 all -0.644
america 0.982 over -0.591
Table 4: Dominant features for English at ILR
Level 3.
tures used in this work are important to achiev-
ing good ILR prediction performance. We intend
to investigate further measures that could improve
this baseline, including features from automatic
parsers or unsupervised morphology to measure
syntactic complexity. Here we have shown that
higher reading levels in English correspond more
with abstract topics. In future work, we also want
to capture some of the stylistic features of text,
such as the complexity of dialogue exchanges.
For both SVM and MIRA, the combination of
length and word-usage features had the best im-
pact on performance across languages. We found
better performance on this task overall for SVM
and we believe that MIRA was overfitting during
training. For MIRA, this is likely due to an inter-
action between a small number of features and the
stopping criterion (mean squared error = 0) that
we used in training, which tends to overfit. We in-
tend to investigate the stopping criterion in future
work. Still, we have shown that MIRA can be use-
ful in this task because it is an online algorithm,
and it allows for incremental training and active
learning.
Our current approach can be quickly adapted
for a new subset of languages because the features
that we used here were language-independent. We
plan to build a flexible architecture that enables
language-specific feature extraction to be com-
MostIndicative + Most Contra-Indicative -
of 3.298 +number+ -2.524
this 2.215 . -2.514
moral 1.880 government -1.120
philosophy 1.541 have -1.109
is 1.242 people -1.007
theory 1.138 would -0.909
in 1.131 could -0.878
absolute 1.034 after -0.875
religion 1.011 you -0.874
hyperbole 0.938 ,? -0.870
mind 0.934 were -0.827
as 0.919 was -0.811
hypothesis 0.904 years -0.795
schelling 0.883 your -0.747
thought 0.854 americans -0.746
virtue 0.835 at -0.745
alchemy 0.828 they -0.720
Table 5: Dominant features for English at ILR
Level 4.
bined with our method so that these techniques
can be easily used for new languages. We will
continuously improve this baseline using the ap-
proaches described in this paper. We found that
these two algorithms along with these types of
features performed pretty well on 4 different lan-
guages. It is surprising that these features would
correlate across languages even though there are
individual differences between each language. In
future work, we are interested to look deeper into
the nature of language-independence for this task.
With respect to content, we are interested to find
out if more word features are needed for some
languages but not others. There could be diver-
sity of vocabulary at higher ILR levels, which we
could measure with entropy. Additionally, since
the MIRA classifier that we are using is an on-
line classifier with weight vector representation
for each feature, we could examine the weights
and measure the mutual information by ILR level
above a certain threshold to find which features are
the most predictive of an ILR level, for each lan-
guage. Lastly, we have assumed that the ILR rat-
ing metric is approximately linear, and although
we have used linear classifiers in this task, we are
interested to learn if other transformations would
give us a better sense of ILR level discrimination.
37
References
Chih-Chung Chang and Chih-Jen Lin. 2001.
LIBSVM: a library for support vec-
tor machines. Software available athttp://www.csie.ntu.edu.tw/?cjlin/libsvm.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative Online Algorithms for Multiclass Prob-
lems. Journal of Machine Learning Research,
3(2003):951-991.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
Passive-Agressive Algorithms. Journal of MachineLearning Research, 7(2006):551-585.
George R. Doddington, Mark A. Przybocki, Alvin F.
Martin, and Douglas A. Reynolds. 2000. The NIST
speaker recognition evaluation - overview, method-
ology, systems, results, perspective. Speech Com-munication, 31(2-3):225-254.
Lijun Feng, Martin Jansche, Matt Huenerfauth,
Noe?mie Elhadad. 2010. A Comparison of Fea-
tures for Automatic Readability Assessment. InProceedings of the 23rd International Conference onComputational Linguistics: Posters. Association for
Computational Linguistics, 2010.
Rudolph Flesch. 1948. A new readability yardstick.Journal of Applied Psychology, 32(3):221-233.
Interagency Language Roundtable. ILR Skill Scale.http://www.govtilr.org/Skills/ILRscale4.htm, 2013. Accessed February 27,
2013.
Thorsten Joachims. 1998a. Text categorization with
support vector machines: learning with many rel-
evant features. In Proceedings of the EuropeanConference on Machine Learning, pages 137-142,
1998a.
Peter J. Kincaid, Lieutenant Robert P. Fishburne, Jr.,
Richard L. Rogers, and Brad S. Chissom. 1975.
Derivation of new readability formulas for Navy en-
listed personnel. Research Branch Report 8-75, U.S.
Naval Air Station, Memphis, 1975.
Sarah E. Petersen and Mari Ostendorf. 2009. A ma-
chine learning approach to reading level assessment.Computer Speech and Language, 23(2009):89-106.
Sarah. E. Schwarm and Mari Ostendorf. 2005. Read-
ing Level Assessment Using Support Vector Ma-
chines and Statistical Language Models. In Pro-ceedings of the 43rd Annual Meeting of the Asso-ciation for Computational Linguistics.
Kumiko Tanaka-Ishii, Satoshi Tezuka, and Hiroshi Ter-
ada. 2010. Sorting texts by readability. Computa-tional Linguistics, 36(2):203-227.
Vladimir Vapnik. 1995. The Nature of StatisticalLearning Theory. Springer, New York, 1995.
38
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 155?162,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Exploiting Morphological, Grammatical, and Semantic Correlates for
Improved Text Difficulty Assessment
Elizabeth Salesky, Wade Shen
?
MIT Lincoln Laboratory Human Language Technology Group, 244 Wood Street, Lexington MA 02420, USA
{elizabeth.salesky, swade}@ll.mit.edu
Abstract
We present a low-resource, language-
independent system for text difficulty as-
sessment. We replicate and improve upon
a baseline by Shen et al. (2013) on the
Interagency Language Roundtable (ILR)
scale. Our work demonstrates that the ad-
dition of morphological, information the-
oretic, and language modeling features to
a traditional readability baseline greatly
benefits our performance. We use the
Margin-Infused Relaxed Algorithm and
Support Vector Machines for experiments
on Arabic, Dari, English, and Pashto, and
provide a detailed analysis of our results.
1 Introduction
While there is a growing breadth of reading mate-
rials available in various languages, finding perti-
nent documents at suitable reading levels remains
difficult. Information retrieval methods can find
resources with desired vocabulary, but educators
still need to filter these to find appropriate diffi-
culty levels. This task is often more challeng-
ing than manually adapting the documents them-
selves. Reading level assessment systems can be
used to automatically find documents at specific
Interagency Language Roundtable (ILR) levels,
aiding both instructors and learners by providing
proficiency-tailored materials.
While interest in readability assessment has
been gaining momentum in many languages, the
majority of previous work is language-specific.
Shen et al. (2013) introduced a baseline for
language-independent text difficulty assessment,
based on the ILR proficiency scale. In this work,
we replicate and extend their results.
?
This work is sponsored by the Defense Language In-
stitute under Air Force Contract FA8721-05-C-0002. Opin-
ions, interpretations, conclusions and recommendations are
those of the authors and are not necessarily endorsed by the
United States Government.
The ILR scale is the standard language profi-
ciency measure for the U.S. federal government.
It ranges from no proficiency to native proficiency
on a scale of 0-5, with half-level denotations
where proficiency meets some but not all of the
criteria for the next level (Interagency Language
Roundtable, 2013). For second language learners,
it is sufficient to use up to ILR level 4. Since profi-
ciency is a continuous spectrum, text difficulty as-
sessment is often treated as a regression problem,
as we do here. Though the ILR levels may ap-
pear to be discrete categories, documents can fall
between levels. The degree to which they do is
important for us to measure.
Level Description
1 Elementary: can fulfill basic needs,
limited to fundamental vocabulary
2 Limited working: routine social demands,
gist of non-technical works, elementary
grasp of grammar
3 General professional: general vocabulary,
good control of grammar, errors do not
interfere with understanding
4 Advanced professional: fluent language
use on all levels, only rare & minute errors
Table 1: Description of proficiency at ILR levels
The ILR scale addresses semantic and gram-
matical capabilities, and to model it appropri-
ately, a system needs to reflect both. The base-
line system developed by Shen et al. (2013)
uses both term frequency log-weighted (TFLOG)
word-usage features and z-normalized word, sen-
tence, and document length features. However,
their results are not equally significant across its
set of test languages, which this paper addresses
with additional features.
The utilization of types for TFLOG weighted
vectors is not as representative for morpholog-
ically rich languages, where multiple types can
represent different word-forms within a single
155
paradigm. By incorporating morphology, we can
improve our TFLOG vectors? representation of
semantic complexity for these languages. We
employ the Morfessor Categories-MAP algorithm
for segmentation (Creutz & Lagus, 2007). Rela-
tive entropy and statistical language models (LMs)
can also measure semantic complexity, and class-
based language models (cLMs) can give us a mea-
sure of the grammatical complexity of the text. All
of these methods are low-resource and unsuper-
vised; they can be easily applied to new languages.
We have compared their performance to language-
specific methods where possible.
The remainder of this paper is structured as fol-
lows; Section 2 summarizes previous research on
readability assessment. Section 3 introduces our
corpus and approach, while Section 4 details our
results and their analyses. Section 5 provides a
summary and description of future work.
2 Background & Related Work
Early work on readability assessment approxi-
mated grammatical and lexical complexity us-
ing shallow features like sentence length and the
number of syllables in a word, like the promi-
nent Flesch-Kincaid measure, in large part due
to their low computational cost (Kincaid et al.,
1975). Such features over-generalize what makes
a text difficult; it is not always the case that longer
words and sentences are more grammatically com-
plex than their shorter counterparts. Subsequent
work such as the Dale-Chall model (Dale & Chall,
1995) added representation on static word lists:
in this case, one of 3,000 words familiar to 4th
graders. Such lists, however, are not readily avail-
able for many difficulty scales and languages.
Ensuing approaches have employed more so-
phisticated methods, such as word frequency es-
timates to measure lexical complexity (Stenner,
1996) and statistical language models to measure
semantic and syntactic complexity, and have seen
significant performance gains over previous work
(Collins-Thompson & Callan, 2004; Schwarm &
Ostendorf, 2005; Petersen & Ostendorf, 2009). In
the case of Heilman et al. (2007), the combina-
tion of lexical and grammatical features specifi-
cally addressed the order in which vocabulary and
grammar are acquired by second language learn-
ers, where grasp of grammar often trails other
markers of proficiency.
The extension of readability research to lan-
guages beyond English necessitated the introduc-
tion of new features such as morphology, which
have long been proven useful in other areas.
Dell?Orletta et al. (2011) developed a two-class
readability model for Italian based on its verbal
morphology. Franc?ois and Fairon (2012) built a
six-class readability model, but for adult learners
of French, utilizing verb tense and mood-based
features. Most recently, Hancke et al. (2012) built
a two-class German reading level assessment sys-
tem heavily utilizing morphology. In addition to
traditional syntactic, lexical, and language model-
ing features used in English readability research,
Hancke et al. (2012) tested a broad range of fea-
tures based on German inflectional and deriva-
tional morphology. While all of these systems
were very effective, they required many language-
specific resources, including part-of-speech tags.
Recent experiments have several noteworthy
characteristics in common. While some systems
discriminate between multiple grade-level cate-
gories, most are two- or three-class classifica-
tion tasks between ?easy? and ?difficult? which do
not require such fine-grained feature discrimina-
tion. Outside of English, there are few multi-level
graded datasets; for those that do exist, they are
very small, averaging less than a hundred labeled
documents per level. Further, though recent work
has been increasingly motivated by second lan-
guage learners, most systems have only been im-
plemented for a single language (Schwarm & Os-
tendorf, 2005; Petersen & Ostendorf, 2009); Va-
jjala & Meurers, 2012). The language-specific
morphological and syntactic features used by
many systems outside of English would make it
difficult to apply them to other languages. Shen et
al. (2013) address this problem by using language-
independent features and testing their work on
four languages. In this work, we extend their sys-
tem in order to improve upon their results.
3 Approach
3.1 Corpus
We conducted our experiments on the corpus used
by Shen et al. (2013). The dataset was collected by
the Defense Language Institute Foreign Language
Center (DLIFLC) for instructional use. It com-
prises approximately 1390 documents for each of
Arabic, Dari, English, and Pashto. The documents
are evenly distributed across seven test ILR levels:
{1, 1+, 2, 2+, 3, 3+, 4}. This equates to close to
156
200 documents per level per language. We use an
80/20 train test split.
Lang. Tokens Types Stems
Morphs
/ Word
Arabic 593,113 84,160 14,591 2.60
Dari 761,412 43,942 13,312 2.61
English 796,406 44,738 35,594 1.80
Pashto 840,673 59,031 20,015 2.34
Table 2: Corpus statistics
The documents were chosen by language in-
structors as representative of a particular level and
range from news articles to excerpts from philos-
ophy to craigslist postings. Three graders hand-
leveled each document. The corpus is annotate
only with the aggregate scores; we use only this
score for comparison. The creation of the corpus
took 70 hours per language on average. We as-
sume the ILR scale is linear and measure perfor-
mance by mean squared error (MSE), typical for
regression. MSE reflects the variance and bias of
our predictions, and is therefore a good measure
of performance uniformity within levels.
3.2 Experimental Design
We compare our results to the best performing Su-
port Vector Machine (SVM) and Margin-Infused
Relaxed Algorithm (MIRA) baselines from Shen
et al. (2013). Both of these baselines have the
same features: TFLOG weighted word vectors,
average sentence length by document, average
word length by document, and document word
count. We used an implementation of the MIRA
algorithm for regression (Crammer & Singer,
2003). We embedded Morfessor for unsupervised
morphological segmentation and preprocessed our
data as required by this algorithm (Creutz & La-
gus, 2007). To verify our results across classifiers,
we compare with SVM (Chang & Lin, 2001).
We also compare Morfessor to ParaMor (Mon-
son 2009), an unsupervised system with a differ-
ent level of segmentation aggression, as well as to
language-specific analyzers.
Our experiments apply word-usage features,
shallow length features, and language models. For
the first, we compare TFLOG vectors based on
word types, all morphemes, and stems only. For
the second, we tested the three baseline shallow
length features (average word length in characters
per document, average sentence length per docu-
ment, and document word count) as well as mea-
sures of relative entropy, average stem fertility, av-
erage morphemes per word, and the ratio of types
to tokens. Of these, only relative entropy posi-
tively impacted performance, and only its results
are reported in this paper. All length features were
z-normalized. We compare both word- and class-
based language models. We trained LMs for each
ILR level and used the document perplexity mea-
sured against each as features.
Optimal settings were determined by sweeping
algorithm parameters, and Morfessor?s perplexity
threshold for each language. We conducted a fea-
ture analysis for all combinations of word, length,
and LM features across all four languages.
4 Results & Analysis
We first replicate the baseline results of Shen et
al. (2013) using both the MIRA and SVM algo-
rithms. We find there is very overall little perfor-
mance difference between the two algorithms, and
the difference is language-dependent. It is incon-
clusive which algorithm performs best.
Algorithm AR DA EN PA
MIRA 0.216 0.296 0.154 0.348
SVM 0.198 0.301 0.147 0.391
Table 3: Baseline results in MSE, SVM vs. MIRA
Table 3 shows the averageMSE across the seven
ILR levels for each language. Figure 1 depicts
MSE performance on each individual ILR level.
Figure 1: MSE by ILR level, baseline
4.1 Morphological Analysis
Reading level assessment in English does not ne-
cessitate the use of morphological features, and so
157
they have not been researched for this task until
recently. Morphology has long been shown to be
useful in other areas; it is unsurprising that seg-
mentation should help with this task for morpho-
logically rich languages. What we demonstrate
is that unsupervised methods perform similarly to
language-specific methods, at a lower cost.
Language TYPES MORPHS STEMS
Arabic 0.216 0.198 0.208
Dari 0.296 0.304 0.294
English 0.154 0.151 0.151
Pashto 0.348 0.303 0.293
Table 4: Average MSE results comparing the use
of types, all morphs, and stems for TFLOG vec-
tors. Morfessor algorithm used for segmentation.
Table 4 compares the performance of the base-
line, which utilizes types for its TFLOG weighted
word vectors, to our configurations that alterna-
tively use all morphemes or stems only. We see
that morphological information improves perfor-
mance for all cases but one, all morphs for Dari,
and that using stems only shows the greatest im-
provement.
Our greatest improvement was seen in Pashto,
which has the most unique stems in our dataset
both outright and compared to types (see Table
4). Without stemming, TFLOG word vectors were
heavily biased by the frequency of alternate word
forms within a paradigm. With stemming, which
reduced overall MSE compared to the baseline by
16%, the number of word vectors in the optimized
configuration increased by 18%, and were much
more diverse, reflecting the actual semantic com-
plexity of the documents. We posit that the rea-
son Dari, which has a similar ratio of morphemes
per word to Pashto, does not improve in this way
is due to its much smaller and more uniform vo-
cabulary in our data. Our Pashto documents have
1.5 times as many unique words as our Dari, and
in fact, with stemming, the number of word vec-
tors utilized in our optimized configuration was
reduced by 20%, as fewer units were necessary to
reflect the same content.
We compare our results using Morfessor to an-
other unsupervised segmentation system, ParaMor
(Monson 2009). ParaMor is built on a differ-
ent mathematical framework than Morfessor, and
so has a very different splitting pattern. Morfes-
sor has a tunable perplexity threshold that dic-
tates how aggressively the algorithm segments.
Even set at its highest, ParaMor still segments
much more aggressively, sometimes isolating sin-
gle characters, which can be useful for down-
stream applications (Kurimo et al. 2009). This is
not the case here, as shown in Table 5. All further
results use Morfessor for stemming.
Algorithm AR DA EN PA
Morfessor 0.208 0.294 0.151 0.293
ParaMor 0.227 0.321 0.158 0.301
Table 5: Comparison of unsupervised segmenters
To our knowledge, no Pashto-specific morpho-
logical analyzer yet exists for comparison. How-
ever, in lacking both a standardized writing system
and spelling conventions, one word in Pashto may
be written in many different ways (Kathol, 2005).
To account for this, we normalized the data us-
ing the Levenshtein distance between types. We
swept possible cutoff thresholds up to 0.25, eval-
uated by the overall MSE of the subsequent re-
sults. Using normalized data did not improve re-
sults; in many cases the edit distance between al-
ternate misspellings is just as high or higher as the
distance between word types.
We believe that the limited change in Dari per-
formance is primarily related to corpus character-
istics; relatively uniform data provides low per-
plexity, making it more difficult for Morfessor to
discover all morphological segmentations. Using
the Perstem stemmer in place of Morfessor, the
number of word vectors in the optimized system
rose 143% and our results improved 8%. This
increase affirms that Morfessor is under-splitting.
Perstem is tailored to Farsi, and while the two di-
alects are mutually intelligible, they have gram-
matical, phonological, and loan word differences
(Shah et al. 2007).
We highlight that the overall MSE of all config-
urations in Table 4 vary only 2% for English, with
identical results using all morphs and only stems.
This is expected, as English is not morphologi-
cally complex. Given the readily available rule-
based systems for English, we compared results
with Morfessor to the traditional Porter and Paice
stemmers, as well as the multi-lingual FreeLing
stemmer, as seen in Table 6.
Performance variance between all analyzers of
only 3% points us to the similar and limited gram-
matical rules found in the different algorithms, as
well as the relatively limited number of unique
158
Baseline Morf. Porter Paice FreeLing
0.154 0.151 0.149 0.148 0.153
Table 6: Comparison of English segmenters
stems and affixes to be found in English. Topical
similarities in our data are also possible.
Like Pashto, Arabic has a rich morphologi-
cal structure, but in addition to affixes it con-
tains templatic morphology. It is difficult for un-
supervised analyzers not specifically tailored to
templatic morphology to capture non-contiguous
morphemes. Here, Morfessor consistently seg-
ments vowelized types into sequences of two char-
acter stems. When compared with MADA, a
rule-based Arabic analyzer (Habash, 2010), we
found that Morfessor outperformed MADA by
10%. This is likely because the representations
present in the dataset are what is significant; if a
form is ?morphologically correct? but perpetuates
a sparsity problem, linguistically-accurate stem-
ming will not help. Neither stemmer contributes
much to Arabic results, however, as MIRA does
not weight word-usage features very heavily for
either Arabic analyzer.
4.2 Relative Entropy and Word LMs
As mentioned in Section 2, traditional features
like document word count and average sentence
length overstate the importance of length to diffi-
culty. To capture the significance of the length of
the document, rather than merely the length itself,
we utilized relative entropy. Relative entropy, also
known as the Kullback-Leibler divergence (KL),
is a measure of the information lost by using one
probability distribution as compared to another.
Expressed as an equation, we have:
D(p, q) =
X
x2"
p(x) log
p(x)
q(x)
. (1)
In this work, we are comparing a unigram prob-
ability distribution of a document q(x) to a uni-
form distribution over the same length p(x). This
provides both a measure of the semantic and struc-
tural complexity of a document, allowing us to
differentiate between documents of similar length.
Figure 2 shows the normalized distribution of the
relative entropy feature for Pashto.
The separability of ILR levels suggests we will
be able to discriminate between them. As demon-
strated by the improved performance in Figure 3,
where the inclusion of relative entropy is super-
Figure 2: Pashto, normalized KL distribution
imposed over the baseline, this feature greatly con-
tributes to the separability of outlier levels of our
corpus. Common z-scores between levels 2 and
3 explain the system?s poorer performance on the
ILR levels 2.0 and 2.5 (Figure 3). Adding the rel-
ative entropy feature to the baseline produced an
average MSE reduction of 15%.
Figure 3: MSE by ILR level, baseline +stems +KL
The combination of stemming for TFLOG vec-
tors and relative entropy together is more effec-
tive than either alone. Further removing docu-
ment word count improved performance by an
average 1%. As seen in Figure 3, the combi-
nation of all these changes produces significant
gains over the baseline, particularly in Dari and
Pashto. The combination configuration reduced
overall MSE by 52% for Pashto documents and
by 18% for Dari. From Figure 3 above, we see
that the +stems+KL configuration exhibits very
poor performance in Arabic level 4, and on outly-
ing levels for Dari. While these MSE values are
clear outliers in this figure, they values are less
than 0.1 greater than their MIRA baseline coun-
159
terparts. This may be due to data similarity be-
tween level 3+ and 4 documents, or MIRA may
have been overfit during training. In contrast, the
variance for English and Pashto is much smaller;
overall, the variance has been greatly reduced.
Statistical language models (LMs) are a proba-
bility distribution over text. An n-gram language
predicts a word w
n
given the preceding context
w
1
...w
n 1
. We used the SRI Language Model-
ing Toolkit to train LMs on our training data for
each ILR level (Stolcke, 2002). To account for
unseen n-grams, we used Kneser-Ney smoothing.
To score documents against these LMs, we calcu-
late their perplexity (PP), a measure of how well
a probability distribution represents data. Perplex-
ity represents the average number of bits neces-
sary to encode each word. For each document in
our dataset, we use the perplexities against each
ILR level LM as features in MIRA. We compared
n-gram orders 2-5, and while we found an aver-
age decrease of 3% MSE between orders 2 and
3 across languages, there was a difference of less
than 1% between 3-gram and 5-gram LMs.
Features AR DA EN PA
baseline 0.216 0.296 0.154 0.348
+stems +KL 0.208 0.269 0.147 0.173
+LM 0.208 0.176 0.117 0.171
+LM -WVs 0.567 0.314 0.338 0.355
+stems +KL
+LM
0.168 0.167 0.096 0.137
Table 7: Average MSE results comparing features
from Sections 4.1 and 4.2. LMs are order 5.
As we can see from Table 7, the addition of lan-
guage models alone can provide a huge measure
of improvement from the baseline. For Arabic and
Pashto, it is the same improvement seen by stem-
ming TFLOG vectors and adding relative entropy.
For Dari and English, however, the performance
improvement is unmatched by any other features
presented thus far. We compare these results to
the same configuration without TFLOG vectors,
in order to measure the overlap between these fea-
tures; see Table 7. Based on the relative results,
it seems that word vector and LM features are or-
thogonal. The addition of all three new features
(stemmed word vectors, relative entropy, and lan-
guage models) provides considerable further im-
provement upon any previous configuration. It ap-
pears that the interactions between these features
have a further positive influence on our discrimi-
native ability.
4.3 Class-Based LMs
It is possible to group words based on similar
meaning and syntactic function. It is reasonable
to think that the probability distributions of words
in such groups would be similar (though not the
same). By assigning classes to words, we can
calculate the probability of a word based not on
the sequence of preceding words, but rather, word
classes. Doing so decreases the size of resulting
models and also allows for better predictions of
unseen word sequences. Sparsity is a concern with
language models, where we rely on the frequency
of sequences, not just words. Using word classes
assuages some of this concern. These word classes
are generated in an unsupervised manner. We train
our class-based language models (cLMs) using c-
discounting to account for data sparsity.
Features AR DA EN PA
baseline 0.216 0.296 0.154 0.348
+LM 0.208 0.176 0.117 0.171
+cLM 0.130 0.286 0.144 0.211
+LM +cLM 0.094 0.155 0.051 0.084
+stems +KL
+LM +cLM
0.092 0.152 0.049 0.079
Table 8: Average MSE results comparing all fea-
tures. LMs and cLMs are order 5.
Class-based and word-based LMs each help
different languages in our test set. The two
types of LMs model different information, with
word-based LMs providing a measure of semantic
complexity and class-based modeling grammati-
cal complexity. As seen in Table 8, the combina-
tion of this complementary information is highly
beneficial and strongly correlated to ILR level. We
see average MSE reductions of 56%, 48%, 67%,
and 77% in Arabic, Dari, English, and Pashto, re-
spectively, using both types of language model.
Algorithm AR DA EN PA
MIRA 0.091 0.156 0.049 0.079
SVM 0.089 0.159 0.069 0.070
Table 9: Final system results, comparing avg.
MSE with the MIRA and SVM algorithms
160
Figure 4: Comparison of final configuration with
all features to baseline by MSE, MIRA algorithm
The further inclusion of TFLOG stemming and
relative entropy reduces average MSE an addi-
tional 1%. Figure 4 reflects this configuration?s
performance across the seven ILR levels.
Figure 4 superimposes our final error results
over those of the baseline. It is clear that error has
become much less language-specific; performance
on all seven ILR levels has become considerably
more consistent across the four languages, as has
the accuracy at each individual ILR level. It seems
likely that our error measures would be similar to
inner-annotator disagreement, a measure that we
would like to quantify in the future.
We find that our results are significant across
classifiers. Table 9 shows the performance of our
final feature set with both MIRA and SVM. The
MSE exhibits the same trends across ILR levels
and languages with both algorithms. The average
difference in error between the algorithms remains
the same as it was with the baseline features.
5 Conclusions and Future Work
Our experiments demonstrate that language-
independent methods can improve text difficulty
assessment performance on the ILR scale for
four languages. Morphological segmentation for
TFLOG word vectors improves our measure of
semantic complexity and allows us to do topic
analysis better. Unsupervised methods perform
similarly to language-specific and linguistically-
accurate analyzers on this task; we are not sac-
rificing performance for a language-independent
system. Relative entropy gives structural con-
text to more traditional shallow length features,
and with word-based LM features provide another
way to measure semantic complexity. Class-based
LM features measure grammatical complexity and
to some degree account for data sparsity issues.
All of these features are low-cost and require no
language-specific resources to be applied to new
languages. The combination of all these features
significantly improves our performance as mea-
sured by mean square error across a diverse set of
languages.
We would like to expand our work to more di-
verse languages and datasets in future work. There
is room to improve upon features described in
this paper, such as new frequency-based measures
for word vectors and unsupervised morphological
segmentation methods. In the future, we would
like to directly compare inner-annotator error and
well-known formulas with our results. It would
also be interesting to look at performance on sub-
sets of the corpus to test dependence on dataset
size. We would also like to investigate the ILR
scale; while we assume that it is linear, this is not
likely to be the case.
Acknowledgments
This paper benefited from valuable discussion
with Jennifer Williams.
References
J. Chall, E. Dale. 1995. Readability revisited: The new
Dale-Chall readability formula. Brookline Books,
Cambridge, MA.
C-C. Chang, C-J. Lin. 2001. LIBSVM: a library
for support vector machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
K. Collins-Thompson, J. Callan. 2005. Predicting
reading difficulty with statistical language models.
Journal of the American Society for Information Sci-
ence and Technology 56(13), 1448-1462.
K. Crammer, Y. Singer. 2003. Ultraconservative On-
line Algorithms for Multiclass Problems. Journal of
Machine Learning Research, 3(2003):951-991.
M. Creutz, K. Lagus. 2007. Unsupervised models for
morpheme segmentation and morphology learning.
Association for Computing Machinery Transactions
on Speech and Language Processing (ACM TSLP),
4(1):1-34.
F. Dell?Orletta, S. Montemagni, G. Venturi. 2011.
Read-it: Assessing readability of italian texts with
a view to text simplification. Proceedings of the 2nd
Workshop on Speech and Language Processing for
Assistive Technologies (SLPAT) 73-83.
161
T. Franc?ois, C. Fairon. 2012. An AI readability
formula for French as a foreign language. Pro-
ceedings of the 2012 Joint Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP) and Computational Natural Language
Learning (CoNLL). 466-477.
N. Habash, O. Rambow, R. Roth. 2010. Mada+Tokan:
A toolkit for arabic tokenization, diacritization, mor-
phological disambiguation, pos tagging, stemming
and lemmatization. Proceedings of the 2nd Inter-
national Conference on Arabic Language Resources
and Tools (MEDAR).
J. Hancke, S. Vajjala, D. Meurers 2012. Readability
Classification for German using lexical, syntactic,
and morphological features. Proceedings of CoL-
ING 2012: Technical Papers, 1063-1080.
K.S. Hasan, M.A. ur Rahman, V. Ng. 2009.
Learning-Based Named Entity Recognition for
Morphologically-Rich, Resource-Scarce Lan-
guages. Proceedings of the 12th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL), 354-362.
M. Heilman, K. Collins-Thompson, J. Callan, M. Es-
kenazi. 2007. Combining Lexical and Grammat-
ical Features to Improve Readability Measures for
First and Second Language Texts. Proceedings of
NAACL HLT, 460-467.
Interagency Language Roundtable. ILR Skill Scale.
http://www.govtilr.org/Skills/
ILRscale4.htm. 2013.
A. Jadidinejad, F. Mahmoudi, J. Dehdari. 2010. Eval-
uation of perstem: a simple and efficient stemming
algorithm for Persian. Multilingual Information Ac-
cess Evaluation Text Retrieval Experiments.
A. Kathol, K. Precoda, D. Vergyri, W. Wang, S. Riehe-
mann. 2005. Speech translation for low-resource
languages: The case of pashto. Proceedings of IN-
TERSPEECH, 2273-2276.
J.P. Kincaid, R.P Fishburne Jr., R.L. Rodgers, and B.S.
Chisson 1975. Derivation of new readability formu-
las for Navy enlisted personnel. Research Branch
Report, U.S. Naval Air Station, Memphis, 8-75.
M. Kurimo, V. Turunen, M. Varjokallio. 2009.
Overview of Morpho Challenge 2008. Evaluating
Systems for Multilingual and Multimodal Informa-
tion Access, Springer Berlin Heidelberg, 951-966.
C. Monson. 2009. ParaMor: From Paradigm Structure
to Natural Language Morphology Induction. PhD
thesis. Carnegie Mellon University.
R. Munro, C.D. Manning. 2010. Subword Variation in
Text Message Classification. The 2010 Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, 510-518.
M. Padr. 2004. FreeLing: An Open-Source Suite of
Language Analyzers. Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation (LREC?04).
C.D. Paice. 1990. Another Stemmer. SIGIR Forum,
24:56-61.
S. E. Petersen and M. Ostendorf. 2009. A ma-
chine learning approach to reading level assessment.
Computer Speech and Language, 23(2009):89-106.
M.F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3): 130-137.
A. Ratnaparkhi. 1997. A simple introduction to max-
imum entropy models for natural language process-
ing. IRCS Technical Reports Series, 81.
S. E. Schwarm and M. Ostendorf. 2005. Reading
Level Assessment Using Support Vector Machines
and Statistical Language Models. In Proceedings
of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL).
M.I. Shah, J. Sadri, C.Y. Suen, N. Nobile. 2007.
A New Multipurpose Comprehensive Database for
Handwritten Dari Recognition. 11th International
Conference on Frontiers in Handwriting Recogni-
tion, Montreal, 635-40.
W. Shen, J. Williams, T. Marius, E. Salesky. 2013.
A language-independent approach to automatic text
difficulty assessment for second-language learners.
Proceedings of the 2nd Workshop on Predicting and
Improving Text Readability for Target Reader Popu-
lations (PITR) 2013.
A. Stolcke. 2002. SRILM - an extensible language
modeling toolkit. Proceedings of the ICSLP, vol. 2,
901-4.
S. Vajjala, D. Meurers. 2012. On improving the accu-
racy of readability classification using insights from
second language acquisition. Proceedings of the
Seventh Workshop on Building Educational Appli-
cations Using NLP. Association for Computational
Linguistics, 2012. 163-173.
162
