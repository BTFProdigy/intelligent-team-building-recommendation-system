Proceedings of NAACL-HLT 2013, pages 752?757,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
TruthTeller: Annotating Predicate Truth
Amnon Lotan
Department of Linguistics
Tel Aviv University
amnonlot@post.tau.ac.il
Asher Stern and Ido Dagan
Department of Computer Science
Bar Ilan University
astern7@gmail.com dagan@cs.biu.ac.il
Abstract
We propose a novel semantic anno-
tation type of assigning truth values
to predicate occurrences, and present
TruthTeller, a standalone publicly-
available tool that produces such annota-
tions. TruthTeller integrates a range
of semantic phenomena, such as nega-
tion, modality, presupposition, implicativ-
ity, and more, which were dealt only partly
in previous works. Empirical evaluations
against human annotations show satisfac-
tory results and suggest the usefulness of
this new type of tool for NLP.
1 Introduction
In a text, the action or relation denoted by ev-
ery predicate can be seen as being either pos-
itively or negatively inferred from its sentence,
or otherwise having an unknown truth status.
Only in (3) below can we infer that Gal sold
her shop, hence the positive truth value of the
predicate sell, while according to (2) and (4) Gal
did not sell it, hence the negative truth values,
and in (1) we do not know if she sold it or not
(the notations pt+, pt- and pt? denote truth
states, defined in Subsection 2.3). Identifying
these predicate truth values is an important sub-
task within many semantic processing scenarios,
including various applications such as Question
Answering (QA), Information Extraction (IE),
paraphrasing and summarization. The follow-
ing examples illustrate the phenomenon:
(1) Gal made an attempt pt+ to sell pt? her
shop.
(2) Gal did not try pt? to sell pt? her shop af-
ter hearing pt+ the offers.
(3) Maybe Gal wasn?t smart pt? to sell pt+ her
shop.
(4) Gal wasn?t smart pt? enough to sell pt? the
shop that she had bought pt+.
Previous works addressed specific aspects of
the truth detection problem: Nairn et al
(2006), and later MacCartney & Manning (2007;
2009), were the first to build paraphrasing and
inference systems that combine negation (see try
in (2)), modality (smart in (3)) and ?natural
logic?, a recursive truth value calculus (sell in
(1-3)); recently, Mausam et al (2012) built an
open IE system that identifies granulated vari-
ants of modality and conditions on predicates
(smart in (3)); and Kiparsky & Kiparsky (1970)
and Karttunen (1971; 2012) laid the ground
work for factive and implicative entailment cal-
culus (sell in (1-4)), as well as many generic
constructions of presupposition (hearing in (2)
is presupposed because it heads an adverbial
clause and bought in (4) heads a finite relative
clause), which, to our knowledge, have not yet
been implemented computationally. Notice in
the examples that presuppositions persist under
negation, in questions and if-clauses, while en-
tailments do not. In addition, there is a growing
research line of negation and modality detection.
See, for example, Morante & Daelemans (2012).
752
We present TruthTeller1, a novel algo-
rithm and system that identifies the truth value
of each predicate in a given sentence. It anno-
tates nodes in the text?s dependency parse-tree
via a combination of pattern-based annotation
rules and a recursive algorithm based on natu-
ral logic. In the course of computing truth value,
it also computes the implicativity/factivity sig-
nature of predicates, and their negation and
modality to a basic degree, both of which are
made available in the system output. It ad-
dresses and combines the aforementioned phe-
nomena (see Section 2), many of which weren?t
dealt in previous systems.
TruthTeller is an open source and pub-
licly available annotation tool, offers a relatively
simple algebra for truth value computation, and
is accompanied by a publicly available lexicon
of over 1,700 implicative and factive predicates.
Also, we provide an intuitive GUI for viewing
and modifying the algorithm?s annotation rules.
2 Annotation Types and Algorithm
This section summarizes the annotation algo-
rithm (a detailed report is available with the sys-
tem release). We perform the annotations over
dependency parse trees, generated according to
the Stanford Dependencies standard (de Marn-
effe and Manning, 2008). For all verbs, nouns
and adjectives in a sentence?s parse tree, we pro-
duce the following 4 annotation types, given in
the order they are calculated, as described in the
following subsections:
1. Predicate Implication Signature (sig) - de-
scribes the pattern by which the predi-
cate entails or presupposes its complements,
e.g., the verb refuse entails the negative of
its complements: Ed refused to pay entails
that Ed didn?t pay.
2. Negation and Uncertainty (NU) - indicates
whether the predicate is modified by an un-
certainty modifier like might, probably, etc.,
or whether it?s negated by no, never etc.
3. Clause-Truth (CT) - indicates whether the
1http://cs.biu.ac.il/~nlp/downloads/TruthTeller
entire clause headed by the predicate is en-
tailed by the complete sentence
4. Predicate Truth (PT) - indicates whether
the predicate itself is entailed by the sen-
tence, as defined below
Before presenting the detailed definitions and
descriptions below, we give a high-level descrip-
tion of TruthTeller?s algorithm, where each
step relies on the results of its predecessor: a)
every predicate in the parse tree is annotated
with a predicate implication signature, identi-
fied by lexicon lookup; b) NU annotations are
added, according to the presence of uncertainty
modifiers (maybe, might, etc.) and negation
modifies (not, never, etc.); c) predicates in cer-
tain presupposition constructions (e.g., adver-
bial clauses, WH arguments) are annotated with
positive CT values; d) the parse tree is depth-
first scanned, in order to compute both CT and
PT annotations by the recursive effects of fac-
tives and implicatives; e) in conjunction with
the previous step, relative clause constructions
are identified and annotated with CT and PT.
Except for steps a) and d), all of the pro-
cedure is implemented as an ordered sequence
of annotation rule applications. An annotation
rule is a dependency parse tree template, pos-
sibly including variables, which assigns certain
annotations to any parse tree node that matches
against it. Step a) is implemented with signa-
ture lexicon lookups, and step d) is an algorithm
implemented in code.
To illustrate this entire process, Figure 1
presents the annotation process of a sim-
ple sentence, step by step, resulting in
TruthTeller?s complete output, fully speci-
fied below. Most other examples in this paper
show only partial annotations for brevity.
2.1 Predicate Implication Signature
Our system marks the signature of each predi-
cate, as defined in Table 1. There, each signa-
ture has a left sign and a right sign. The left sign
determines the clause truth value of the pred-
icate?s complements, when the predicate is in
positive contexts (e.g., not negated), while the
right sign applies in negative contexts (clause
753
# Sig Positive context example Negative context example
1 +/- Ed managed to escape ? Ed escaped Ed didn?t manage to escape ? Ed didn?t escape
2 +/? Ed was forced to sell ? Ed sold Ed wasn?t forced to sell ? no entailments
3 ?/- Ed was allowed to go ? no entailments Ed wasn?t allowed to go ? Ed didn?t go
4 -/+ Ed forgot to pay ? Ed didn?t pay Ed didn?t forget to pay ? Ed paid
5 -/? Ed refused to fight ? Ed didn?t fight Ed didn?t refuse to fight ? no entailments
6 ?/+ Ed hesitated to ask ? no entailments Ed didn?t hesitate to ask ? Ed asked
7 +/+ Ed was glad to come ? Ed came Ed wasn?t glad to come ? Ed came
8 -/- Ed pretended to pay ? Ed didn?t pay Ed didn?t pretend to pay ? Ed didn?t pay
9 ?/? Ed wanted to fly ? no entailments Ed didn?t want to fly ? no entailments
Table 1: Implication signatures, based on MacCartney & Manning (2009) and Karttunen (2012). The first
six signatures are named implicatives, and the last three factive, counter factive and regular, respectively.
a) Annotate signatures via lexicons lookup
Gal wasn?t allowed?/? to come?/?
b) Annotate NU
Gal wasn?t allowed?/?,nu? to come?/?,nu+
c) Annotate CT to presupposition constructions
Gal wasn?t allowed?/?,nu?,ct+ to come?/?,nu+,ct+
d) Recursive CT and PT annotation
Gal wasn?t allowed?/?,nu?,ct+,pt? to
come?/?,nu+,ct?,pt?
e) Annotate CT and PT of relative clauses
(has no effect on this example)
Gal wasn?t allowed?/?,nu?,ct+,pt? to
come?/?,nu+,ct?,pt?
Figure 1: An illustration of the annotation process
truth is defined in Subsection 2.3). See exam-
ples for both context types in the table. Each
sign can be either + (positive), - (negative) or
? (unknown). The unknown sign signifies that
the predicate does not entail its complements in
any way.
Signatures are identified via lookup, using two
lexicons, one for single-word predicates and the
other for verb+noun phrasal verbs, e.g., take the
time to X. Our single-word lexicon is similar to
those used in (Nairn et al, 2006) and (Bar-Haim
et al, 2007), but is far greater, holding over
1,700 entries, while each of the previous two has,
to the best of our knowledge, less than 300 en-
tries. It was built semi automatically, out of
a kernel of 320 manually inspected predicates,
which was then expanded with WordNet syn-
onyms (Fellbaum, 1998). The second lexicon
is the implicative phrasal verb lexicon of Kart-
tunen (2012), adapted into our framework. The
+/? implicative serves as the default signature
for all unlisted predicates.
Signature is also sensitive to the type of the
complement. Consider:
(6) Ed forgot?/+ to call pt? Joe
(7) Ed forgot+/+ that he called pt+ Joe
Therefore, signatures are specified separately for
finite and non finite complements of each pred-
icate.
After the initial signature lookup, two anno-
tation rules correct the signatures of +/+ fac-
tives modified by enough and too, into +/- and
-/+, correspondingly, see Kiparsky & Kiparsky
(1970). Compare:
(8) Ed was mad+/+ to go ? Ed went
(9) Ed was too mad?/+ to go ? Ed didn?t go
In addition, we observed, like Karttunen (2012),
that most verbs that have passive voice and the
into preposition become +/? implicatives, e.g.,
(10) Workers were pushed / maddened /
managed+/? into signing ? They signed
(11) Workers weren?t pushed / maddened /
managed+/? into signing? It is unknown
whether they signed
so we captured this construction in another rule.
754
2.2 Negation and Uncertainty (NU)
NU takes the values {nu+, nu-, nu?}, stand-
ing for non-negated certain actions, negated cer-
tain actions, and uncertain actions. The first
NU rules match against a closed set of negation
modifiers around the predicate, like not, never,
neither etc. (see (2)), while later rules detect
uncertainty modifiers, like maybe, probably, etc.
Therefore, nu? takes precedence over nu-.
Many constructions of subject-negation,
object-negation and ?double negation? are
accounted for in our rules, as in:
(12) Nobody was seennu? at the site
(13) Almost nobody was seennu+ at the site
2.3 Clause Truth and Predicate Truth
Clause Truth (CT, denoted as ct(p)) corre-
sponds to polarity of Nairn et al (2006). It
represents whether the clause headed by a pred-
icate p is entailed by the sentence, contradicted
or unknown, and thus takes three values {ct+,
ct-, ct?}.
Predicate Truth (PT) (denoted as pt(p)) rep-
resents whether we can infer from the sentence
that the action described by the predicate hap-
pened (or that its relation holds). It is defined
as the binary product of NU and CT:
Definition 1. PT = NU ? CT
and takes analogous values: {pt+, pt-, pt?}.
Intuitively, the product of two identical posi-
tive/negative values yields pt+, a positive and a
negative yield pt-, and nu? or ct? always yield
pt?. To illustrate these definitions, consider:
(14) Meg may have sleptct+,pt? after
eatingct+,pt+ the meal Ed cookedct+,pt+,
while no one was therect+,pt?
After signatures and NU are annotated, CT
and PT are calculated. At first, we apply
a set of rules that annotate generic presup-
position constructions with ct+. These in-
clude adverbial clauses opening with {while, be-
fore, after, where, how come, because, since,
owing to, though, despite, yet, therefore...},
WH arguments (who, which, whom, what), and
ct(p) =
?
?????????
?????????
ct+ :
p was already annotated
by a presupposition rule
ct(gov(p)) :
p heads a relative
clause
compCT (p) :
otherwise, and p is
a complement
ct? : otherwise (default)
Figure 2: Formula of ct(p), for any predicate p.
ct(gov(p)) is the CT of p?s governing predicate.
parataxis2. See for example the effects of after
and while in (14).
Then, we apply the following recursive se-
quential procedure. The tree root always gets
ct+ (see slept in (14)). The tree is then scanned
downwards, predicate by predicate. At each one,
we compute CT by the formula in Figure 2, as
follows. First, we check if one of the aforemen-
tioned presupposition rules already matched the
node. Second, if none matched, we apply to the
node?s entire subtree another set of rules that
annotate each relative clause with the CT of its
governing noun3, ct(gov(p)) (see failed in (15)).
Third, if no previous rule matched, and p is a
complement of another predicate gov(p), then
compCT(p) is calculated, by the following logic:
when pt(gov(p)) is pt+ or pt-, the correspond-
ing left or right sign of sig(gov(p)) is copied.
Otherwise, if pt(gov(p)) = pt?, ct? is returned,
except when the signature of gov(p) is +/+ (or
-/-) factive, which always yields ct+ (or ct-).
Third, if nothing applied to p, ct? is returned
by default. Finally, PT is set, according to Def-
inition 1.
To illustrate, consider these annotations:
(15) Gal managed+/?,ct+,pt+ a
building+/?,ct+,pt+, which Ginger
failed?/+,ct+,pt+ to sell+/?,ct?,pt?
First, managed gets ct+ as the tree root. Then,
we get compCT (building) = ct+, as the com-
plement of managed+/?,pt+. Next, a relative
clause rule copies ct+ from building to failed.
2The placing of clauses or phrases one after another,
without words to indicate coordination, as in ?veni, vidi,
vici? in contrast to ?veni, vidi and vici?.
3We also annotate nouns and adjectives as predicates
in copular constructions, and in instances where nouns
have complements.
755
Finally, compCT (sell) = ct- is calculated, as
the complement of failed?/+,pt+.
3 Evaluation
To evaluate TruthTeller?s accuracy, we sam-
pled 25 sentences from each of the RTE5 and
RTE6 Test datasets (Bentivogli et al, 2009;
Bentivogli et al, 2010), widely used for textual
inference benchmarks. In these 50 sentences, we
manually annotated each predicate, 153 in to-
tal, forming a gold standard. As baseline, we
report the most frequent value for each annota-
tion. The results, in Table 2, show high accuracy
for all types, reducing the baseline CT and PT
errors by half. Furthermore, most of the remain-
ing errors were due to parser errors, according
to a manual error analysis we conducted.
The baseline for NU annotations shows that
negations are scarce in these RTE datasets,
which was also the case for ct- and pt- an-
notations. Thus, Table 2 mostly indicates
TruthTeller?s performance in distinguishing pos-
itive CT and PT annotations from unknown
ones, the latter constituting v20% of the gold
standard. To further assess ct- and pt- annota-
tions we performed two targeted measurements.
Precision for ct- and pt- was measured by man-
ually judging the correctness of such annotations
by TruthTeller, on a sample from RTE6 Test
including 50 ct- and 124 pt- annotations. This
test yielded 78% and 83% precision, respectively.
pt- is more frequent as it is typically triggered
by ct-, as well as by other constructions involv-
ing negation. Recall was estimated by employ-
ing a human annotator to go through the dataset
and look for ct- and pt- gold standard anno-
tations. The annotator identified 40 ?ct-?s and
50 ?pt-?s, out of which TruthTeller found
47.5% of the ?ct-?s and 74% of the ?pt-?s. In
summary, TruthTeller?s performance on our
target PT annotations is quite satisfactory with
89% accuracy overall, having 83% precision and
74% recall estimates specifically for pt-.
4 Conclusions and Future Work
We have presentedTruthTeller, a novel algo-
rithm and system that identifies truth values of
Annotation TruthTeller Baseline
Signature 89.5% 81% (+/?)
NU 98% 97.3% (nu+)
CT 90.8% 78.4% (ct+)
PT 89% 77% (pt+)
Table 2: The accuracy measures for
TruthTeller?s 4 annotations. The right col-
umn gives the accuracy for the corresponding
most-frequent baseline: {+/?, nu+, ct+, pt+}.
predicates, the first such system to a) address or
combine a wide variety of relevant grammatical
constructions; b) be an open source annotation
tool; c) address the truth value annotation task
as an independent tool, which makes it possible
for client systems to use its output, while pre-
vious works only embedded annotations in their
task-specific systems; and d) annotate unknown
truth values extensively and explicitly.
TruthTeller may be used for several pur-
poses, such as inferring parts of a sentence
from the whole and improving textual entail-
ment (and contradiction) detection. It includes
a novel, large and accurate, lexicon of predicate
implication signatures.
While in this paper we evaluated the correct-
ness of TruthTeller as an individual com-
ponent, in the future we propose integrating
it in a state-of-the-art RTE system and report
its impact. One challenge in this scenario is
having other system components interact with
TruthTeller?s decisions, possibly masking its
effects. In addition, we plan to incorporate
monotonicity calculations in the annotation pro-
cess, like in MacCartney and Manning (2009).
5 Acknowledgements
This work was partially supported by the Israel
Science Foundation grant 1112/08 and the Eu-
ropean Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
We thank Roni Katzir and Fred Landman for
useful discussions.
756
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI, pages
871?876.
Luisa Bentivogli, Bernardo Magnini, Ido Dagan,
Hoa Trang Dang, and Danilo Giampiccolo. 2009.
The fifth pascal recognizing textual entailment
challenge. In Preproceedings of the Text Analysis
Conference (TAC).
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa T.
Dang, and Danilo Giampiccolo. 2010. The sixth
PASCAL recognizing textual entailment chal-
lenge. In The Text Analysis Conference (TAC
2010).
Marie-Catherine de Marneffe and Christopher D.
Manning. 2008. The stanford typed dependencies
representation. In COLING Workshop on Cross-
framework and Cross-domain Parser Evaluation.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database (Language, Speech,
and Communication). MIT Press.
Lauri Karttunen. 1971. Implicative verbs. Lan-
guage, 47:340?358.
Lauri Karttunen. 2012. Simple and phrasal implica-
tives. In *SEM 2012, pages 124?131.
P. Kiparsky and C. Kiparsky. 1970. Fact.
In Progress in Linguistics, pages 143?173. The
Hague: Mouton de Gruyter.
Bill MacCartney and Christopher D. Manning. 2007.
Natural logic for textual inference. In Proceedings
of ACL workshop on textual entailment and para-
phrasing.
Bill MacCartney and Christopher D. Manning. 2009.
An extended model of natural logic. In Proceed-
ings of the Eighth International Conference on
Computational Semantics (IWCS-8).
Mausam, Michael Schmitz, Stephen Soderland,
Robert Bart, and Oren Etzioni. 2012. Open lan-
guage learning for information extraction. In Pro-
ceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
523?534.
Roser Morante and Walter Daelemans. 2012. An-
notating modality and negation for a machine
reading evaluation. In CLEF (Online Working
Notes/Labs/Workshop).
Rowan Nairn, Cleo Condoravdi, and Lauri Kart-
tunen. 2006. Computing relative polarity for tex-
tual inference. In In Proceedings of ICoS-5 (Infer-
ence in Computational Semantics).
757
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 283?291,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Efficient Search for Transformation-based Inference
Asher Stern?, Roni Stern?, Ido Dagan?, Ariel Felner?
? Computer Science Department, Bar-Ilan University
? Information Systems Engineering, Ben Gurion University
astern7@gmail.com
roni.stern@gmail.com
dagan@cs.biu.ac.il
felner@bgu.ac.il
Abstract
This paper addresses the search problem in
textual inference, where systems need to infer
one piece of text from another. A prominent
approach to this task is attempts to transform
one text into the other through a sequence
of inference-preserving transformations, a.k.a.
a proof, while estimating the proof?s valid-
ity. This raises a search challenge of find-
ing the best possible proof. We explore this
challenge through a comprehensive investi-
gation of prominent search algorithms and
propose two novel algorithmic components
specifically designed for textual inference: a
gradient-style evaluation function, and a local-
lookahead node expansion method. Evalua-
tions, using the open-source system, BIUTEE,
show the contribution of these ideas to search
efficiency and proof quality.
1 Introduction
In many NLP settings it is necessary to identify
that a certain semantic inference relation holds be-
tween two pieces of text. For example, in para-
phrase recognition it is necessary to identify that the
meanings of two text fragments are roughly equiva-
lent. In passage retrieval for question answering, it
is needed to detect text passages from which a sat-
isfying answer can be inferred. A generic formula-
tion for the inference relation between two texts is
given by the Recognizing Textual Entailment (RTE)
paradigm (Dagan et al, 2005), which is adapted here
for our investigation. In this setting, a system is
given two text fragments, termed ?text? (T) and ?hy-
pothesis? (H), and has to recognize whether the hy-
pothesis is entailed by (inferred from) the text.
An appealing approach to such textual inferences
is to explicitly transform T into H, using a sequence
of transformations (Bar-Haim et al, 2007; Harmel-
ing, 2009; Mehdad, 2009; Wang and Manning,
2010; Heilman and Smith, 2010; Stern and Dagan,
2011). Examples of such possible transformations
are lexical substitutions (e.g. ?letter?? ?message?)
and predicate-template substitutions (e.g. ?X [verb-
active] Y? ? ?Y [verb-passive] by X?), which are
based on available knowledge resources. Another
example is coreference substitutions, such as replac-
ing ?he? with ?the employee? if a coreference re-
solver has detected that these two expressions core-
fer. Table 1 exemplifies this approach for a particu-
lar T-H pair. The rationale behind this approach is
that each transformation step should preserve infer-
ence validity, such that each text generated along this
process is indeed inferred from the preceding one.
An inherent aspect in transformation-based infer-
ence is modeling the certainty that each inference
step is valid. This is usually achieved by a cost-
based or probabilistic model, which quantifies con-
fidence in the validity of each individual transfor-
mation and consequently of the complete chain of
inference.
Given a set of possible transformations, there may
be many transformation sequences that would trans-
form T to H. This creates a very large search space,
where systems have to find the ?best? transformation
sequence ? the one of lowest cost, or of highest prob-
ability. To the best of our knowledge, this search
challenge has not been investigated yet in a substan-
283
# Operation Generated text
0 - He received the letter from the secretary.
1 Coreference substitution The employee received the letter from the secretary.
2 X received Y from Z? Y was sent to X by Z The letter was sent to the employee by the secretary.
3 Y [verb-passive] by X? X [verb-active] Y The secretary sent the letter to the employee.
4 X send Y? X deliver Y The secretary delivered the letter to the employee.
5 letter? message The secretary delivered the message to the employee.
Table 1: A sequence of transformations that transform the text ?He received the letter from the secretary.? into the
hypothesis ?The secretary delivered the message to the employee.?. The knowledge required for such transformations
is often obtained from available knowledge resources and NLP tools.
tial manner: each of the above-cited works described
the search method they used, but none of them tried
alternative methods while evaluating search perfor-
mance. Furthermore, while experimenting with our
own open-source inference system, BIUTEE1, we
observed that search efficiency is a major issue, of-
ten yielding practically unsatisfactory run-times.
This paper investigates the search problem in
transformation-based textual inference, naturally
falling within the framework of heuristic AI (Ar-
tificial Intelligence) search. To facilitate such in-
vestigation, we formulate a generic search scheme
which incorporates many search variants as special
cases and enable a meaningful comparison between
the algorithms. Under this framework, we identify
special characteristics of the textual inference search
space, that lead to the development of two novel al-
gorithmic components: a special lookahead method
for node expansion, named local lookahead, and a
gradient-based evaluation function. Together, they
yield a new search algorithm, which achieved sub-
stantially superior search performance in our evalu-
ations.
The remainder of this paper is organized as
follows. Section 2 provides an overview of
transformation-based inference systems, AI search
algorithms, and search methods realized in prior in-
ference systems. Section 3 formulates the generic
search scheme that we have investigated, which cov-
ers a broad range of known algorithms, and presents
our own algorithmic contributions. These new algo-
rithmic contributions were implemented in our sys-
tem, BIUTEE. In Section 4 we evaluate them empir-
ically, and show that they improve search efficiency
as well as solution?s quality. Search performance is
evaluated on two recent RTE benchmarks, in terms
1www.cs.biu.ac.il/?nlp/downloads/biutee
of runtime, ability to find lower-cost transformation
chains and impact on overall inference.
2 Background
Applying sequences of transformations to recognize
textual inference was suggested by several works.
Such a sequence may be referred to as a proof, in
the sense that it is used to ?prove? the hypothesis
from the text. Although various works along this
line differ from each other in several respects, many
of them share the common challenge of finding an
optimal proof. The following paragraphs review the
major research approaches in this direction. We fo-
cus on methods that perform transformations over
parse trees, and highlight the search challenge with
which they are faced.
2.1 Transformation-based textual inference
Several researchers suggested using various types
of transformations in order to derive H from T .
Some suggested a set of predefined transforma-
tions, for example, insertion, deletion and substitu-
tion of parse-tree nodes, by which any tree can be
transformed to any other tree. These transforma-
tions were used by the open-source system EDITS
(Mehdad, 2009), and by (Wang and Manning, 2010).
Since the above mentioned transformations are lim-
ited in capturing certain interesting and prevalent
semantic phenomena, an extended set of tree edit
operations (e.g., relabel-edge, move-sibling, etc.)
was proposed by Heilman and Smith (2010). Simi-
larly, Harmeling (2009) suggested a heuristic set of
28 transformations, which include various types of
node-substitutions as well as restructuring of the en-
tire parse-tree.
In contrast to such predefined sets of transfor-
mations, knowledge oriented approaches were sug-
284
gested by Bar-Haim et al (2007) and de Salvo Braz
et al (2005). Their transformations are defined by
knowledge resources that contain a large amount of
entailment rules, or rewrite rules, which are pairs of
parse-tree fragments that entail one another. Typical
examples for knowledge resources of such rules are
DIRT (Lin and Pantel, 2001), and TEASE (Szpek-
tor et al, 2004), as well as syntactic transforma-
tions constructed manually. In addition, they used
knowledge-based lexical substitutions.
However, when only knowledge-based transfor-
mations are allowed, transforming the text into the
hypothesis is impossible in many cases. This limi-
tation is dealt by our open-source integrated frame-
work, BIUTEE (Stern and Dagan, 2011), which
incorporates knowledge-based transformations (en-
tailment rules) with a set of predefined tree-edits.
Motivated by the richer structure and search space
provided by BIUTEE, we adopted it for our empiri-
cal investigations.
The semantic validity of transformation-based in-
ference is usually modeled by defining a cost or
a probability estimation for each transformation.
Costs may be defined manually (Kouylekov and
Magnini, 2005), but are usually learned automati-
cally (Harmeling, 2009; Mehdad, 2009; Wang and
Manning, 2010; Heilman and Smith, 2010; Stern
and Dagan, 2011). A global cost (or probability esti-
mation) for a complete sequence of transformations
is typically defined as the sum of the costs of the
involved transformations.
Finding the lowest cost proof, as needed for de-
termining inference validity, is the focus of our re-
search. Textual inference systems limited to the
standard tree-edit operations (insertion, deletion,
substitution) can use an exact algorithm that finds
the optimal solution in polynomial time under cer-
tain constraints (Bille, 2005). Nevertheless, for the
extended set of transformations it is unlikely that ef-
ficient exact algorithms for finding lowest-cost se-
quences are available (Heilman and Smith, 2010).
In this harder case, the problem can be viewed
as an AI search problem. Each state in the search
space is a parse-tree, where the initial state is the text
parse-tree, the goal state is the hypothesis parse-tree,
and we search for the shortest (in terms of costs)
path of transformations from the initial state to the
goal state. Next we briefly review major concepts
from the field of AI search and summarize some rel-
evant proposed solutions.
2.2 Search Algorithms
Search algorithms find a path from an initial state to
a goal state by expanding and generating states in
a search space. The term generating a state refers
to creating a data structure that represents it, while
expanding a state means generating all its immedi-
ate derivations. In our domain, each state is a parse
tree, which is expanded by performing all applicable
transformations.
Best-first search is a common search framework.
It maintains an open list (denoted hereafter as
OPEN) containing all the generated states that have
not been expanded yet. States in OPEN are prior-
itized by an evaluation function, f(s). A best-first
search algorithm iteratively removes the best state
(according to f(s)) from OPEN, and inserts new
states being generated by expanding this best state.
The evaluation function is usually a linear combina-
tion of the shortest path found from the start state to
state s, denoted by g(s), and a heuristic function, de-
noted by h(s), which estimates the cost of reaching
a goal state from s.
Many search algorithms can be viewed as spe-
cial cases or variations of best-first search. The
well-known A* (Hart et al, 1968). algorithm is
a best-first search that uses an evaluation function
f(s) = g(s) + h(s). Weighted A* (Pohl, 1970)
uses an evaluation function f(s) = w ? g(s) + h(s),
where w is a parameter, while pure heuristic search
uses f(s) = h(s). K-BFS (Felner et al, 2003) ex-
pands k states in each iteration. Beam search (Furcy
and Koenig, 2005; Zhou and Hansen, 2005) limits
the number of states stored in OPEN, while Greedy
search limits OPEN to contain only the single best
state generated in the current iteration.
The search algorithm has crucial impact on the
quality of proof found by a textual inference system,
as well as on its efficiency. Next, we describe search
strategies used in prior works for textual inference.
2.3 Search in prior inference models
In spite of being a fundamental problem, prior so-
lutions to the search challenge in textual inference
were mostly ad-hoc. Furthermore, there was no in-
vestigation of alternative search methods, and no
285
evaluation of search efficiency and quality was re-
ported. For example, in (Harmeling, 2009) the order
by which the transformations are performed is pre-
determined, and in addition many possible deriva-
tions are discarded, to prevent exponential explo-
sion. Handling the search problem in (Heilman and
Smith, 2010) was by a variant of greedy search,
driven by a similarity measure between the current
parse-tree and the hypothesis, while ignoring the
cost already paid. In addition, several constraints on
the search space were implemented. In the earlier
version of BIUTEE (Stern and Dagan, 2011)2, a ver-
sion of beam search was incorporated, named here-
after BIUTEE-orig. This algorithm uses the evalua-
tion function f(s) = g(s) +wi ?h(s), where in each
iteration (i) the value of w is increased, to ensure
successful termination of the search. Nevertheless,
its efficiency and quality were not investigated.
In this paper we consider several prominent
search algorithms and evaluate their quality. The
evaluation concentrates on two measures: the run-
time required to find a proof, and proof quality (mea-
sured by its cost). In addition to evaluating standard
search algorithms we propose two novel compo-
nents specifically designed for proof-based textual-
inference and evaluate their contribution.
3 Search for Textual Inference
In this section we formalize our search problem and
specify a unifying search scheme by which we test
several search algorithms in a systematic manner.
Then we propose two novel algorithmic components
specifically designed for our problem. We conclude
by presenting our new search algorithm which com-
bines these two ideas.
3.1 Inference and search space formalization
Let t be a parse tree, and let o be a transforma-
tion. Applying o on t, yielding t?, is denoted by
t `o t?. If the underlying meaning of t? can in-
deed be inferred from the underlying meaning of t,
then we refer to the application of o as valid. Let
O = (o1, o2, . . . on) be a sequence of transforma-
tions, such that t0 `o1 t1 `o2 t2 . . . `on tn. We
write t0 `O tn, and say that tn can be proven from
2More details in www.cs.biu.ac.il/?nlp/
downloads/biutee/search_ranlp_2011.pdf
t0 by applying the sequence O. The proof might be
valid, if all the transformations involved are valid, or
invalid otherwise.
An inference system specifies a cost, C(o), for
each transformation o. In most systems the costs
are automatically learned. The interpretation of a
high cost is that it is unlikely that applying o will be
valid. The cost of a sequence O = (o1, o2, . . . on)
is defined as
?n
i=1C(o) (or ,in some systems,?n
i=1C(o)). Denoting by tT and tH the text parse
tree and the hypothesis parse tree, a proof system
has to find a sequenceO with minimal cost such that
tT `O tH. This forms a search problem of finding
the lowest-cost proof among all possible proofs.
The search space is defined as follows. A state
s is a parse-tree. The start state is tT and the goal
state is tH. In some systems any state s in which tH
is embedded is considered as goal as well.
Given a state s, let {o(1), o(2) . . . o(m)} be m
transformations that can be applied on it. Expand-
ing s means generating m new states, s(j), j =
1 . . .m, such that s `o(j) s
(j). The number m is
called branching factor. Our empirical observations
on BIUTEE showed that its branching factor ranges
from 2-3 for some states to about 30 for other states.
3.2 Search Scheme
Our empirical investigation compares a range
prominent search algorithms, described in Section 2.
To facilitate such investigation, we formulate them
in the following unifying scheme (Algorithm 1).
Algorithm 1 Unified Search Scheme
Parameters: f(?): state evaluation function
expand(?): state generation function
Input: kexpand: # states expanded in each iteration
kmaintain: # states in OPEN in each iteration
sinit: initial state
1: OPEN? {sinit}
2: repeat
3: BEST? kexpand best (according to f ) states in OPEN
4: GENERATED?
?
s?BEST expand(s)
5: OPEN? (OPEN \ Best) ? GENERATED
6: OPEN? kmaintain best (according to f ) states in OPEN
7: until BEST contains the goal state
Initially, the open list, OPEN contains the initial
state. Then, the best kexpand states from OPEN are
chosen, according to the evaluation function f(s)
286
Algorithm f() expand() kmaintain kexpand
A* g + h regular ? 1
Weighted A* g+w ?h regular ? 1
K-Weighted A* g+w ?h regular ? k > 1
Pure Heuristic h regular ? 1
Greedy g+w ?h regular 1 1
Beam g + h regular k > 1 k > 1
BIUTEE-orig g+wi?h regular k > 1 k > 1
LLGS ?g?h
local-
lookahead
1 1
Table 2: Search algorithm mapped to the unified search
scheme. ?Regular? means generating all the states which
can be generated by applying a single transformation. Al-
ternative greedy implementations use f = h.
(line 3), and expanded using the expansion func-
tion expand(s). In classical search algorithms,
expand(s) means generating a set of states by ap-
plying all the possible state transition operators to s.
Next, we remove from OPEN the states which were
expanded, and add the newly generated states. Fi-
nally, we keep in OPEN only the best kmaintain states,
according to the evaluation function f(s) (line 6).
This process repeats until the goal state is found in
BEST (line 7). Table 2 specifies how known search
algorithms, described in Section 2, fit into the uni-
fied search scheme.
Since runtime efficiency is crucial in our domain,
we focused on improving one of the simple but fast
algorithms, namely, greedy search. To improve the
quality of the proof found by greedy search, we in-
troduce new algorithmic components for the expan-
sion and evaluation functions, as described in the
next two subsections, while maintaining efficiency
by keeping kmaintain=kexpand= 1
3.3 Evaluation function
In most domains, the heuristic function h(s) esti-
mates the cost of the minimal-cost path from a cur-
rent state, s, to a goal state. Having such a function,
the value g(s) + h(s) estimates the expected total
cost of a search path containing s. In our domain, it
is yet unclear how to calculate such a heuristic func-
tion. Given a state s, systems typically estimate the
difference (the gap) between s and the hypothesis
tH (the goal state). In BIUTEE this is quantified by
the number of parse-tree nodes and edges of tH that
do not exist in s. However, this does not give an
estimation for the expected cost of the path (the se-
quence of transformations) from s to the goal state.
This is because the number of nodes and edges that
can be changed by a single transformation can vary
from a single node to several nodes (e.g., by a lexi-
cal syntactic entailment rule). Moreover, even if two
transformations change the same number of nodes
and edges, their costs might be significantly differ-
ent. Consequently, the measurement of the cost ac-
cumulated so far (g(s)) and the remaining gap to tH
(h(s)) are unrelated. We note that a more sophisti-
cated heuristic function was suggested by Heilman
and Smith (2010), based on tree-kernels. Neverthe-
less, this heuristic function, serving as h(s), is still
unrelated to the transformation costs (g(s)).
We therefore propose a novel gradient-style func-
tion to overcome this difficulty. Our function is
designed for a greedy search in which OPEN al-
ways contains a single state, s. Let sj be a state
generated from s, the cost of deriving sj from s
is ?g(sj) ? g(sj) ? g(s). Similarly, the reduc-
tion in the value of the heuristic function is de-
fined ?h(sj) ? h(s) ? h(sj). Now, we define
f?(sj) ?
?g(sj)
?h(sj)
. Informally, this function mea-
sures how costly it is to derive sj relative to the
obtained decrease in the remaining gap to the goal
state. For the edge case in which h(s)? h(sj) ? 0,
we define f?(sj) =?. Empirically, we show in our
experiments that the function f?(s) performs better
than the traditional functions f(s) = g(s) + h(s)
and fw(s) = g(s) + w ? h(s) in our domain.
3.4 Node expansion method
When examining the proofs produced by the above
mentioned algorithms, we observed that in many
cases a human could construct proofs that exhibit
some internal structure, but were not revealed by the
algorithms. Observe, for example, the proof in Ta-
ble 1. It can be seen that transformations 2,3 and
4 strongly depend on each other. Applying trans-
formation 3 requires first applying transformation 2,
and similarly 4 could not be applied unless 2 and 3
are first applied. Moreover, there is no gain in apply-
ing transformations 2 and 3, unless transformation 4
is applied as well. On the other hand, transformation
1 does not depend on any other transformation. It
may be performed at any point along the proof, and
287
moreover, changing all other transformations would
not affect it.
Carefully examining many examples, we general-
ized this phenomenon as follows. Often, a sequence
of transformations can be decomposed into a set of
coherent subsequences of transformations, where in
each subsequence the transformations strongly de-
pend on each other, while different subsequences are
independent. This phenomenon can be utilized in
the following way: instead of searching for a com-
plete sequence of transformations that transform tT
into tH, we can iteratively search for independent co-
herent subsequences of transformations, such that a
combination of these subsequences will transform
tT into tH. This is somewhat similar to the tech-
nique of applying macro operators, which is used in
automated planning (Botea et al, 2005) and puzzle
solving (Korf, 1985).
One technique for finding such subsequences is
to perform, for each state being expanded, a brute-
force depth-limited search, also known as looka-
head (Russell and Norvig, 2010; Bulitko and Lus-
trek, 2006; Korf, 1990; Stern et al, 2010). How-
ever, performing such lookahead might be slow if
the branching factor is large. Fortunately, in our
domain, coherent subsequences have the following
characteristic which can be leveraged: typically, a
transformation depends on a previous one only if
it is performed over some nodes which were af-
fected by the previous transformation. Accordingly,
our proposed algorithm searches for coherent subse-
quences, in which each subsequent transformation
must be applied to nodes that were affected by the
previous transformation.
Formally, let o be a transformation that has been
applied on a tree t, yielding t?. ?affected(o, t?) denotes
the subset of nodes in t? which were affected (modi-
fied or created) by the application of o.
Next, for a transformation o, applied on a parse
tree t, we define ?required(t, o) as the subset of t?s
nodes required for applying o (i.e., in the absence of
these nodes, o could not be applied).
Finally, let t be a parse-tree and ? be a subset of
its nodes. enabled ops(t, ?) is a function that re-
turns the set of the transformations that can be ap-
plied on t, which require at least one of the nodes
in ?. Formally, enabled ops(t, ?) ? {o ? O :
? ? ?required(t, o) 6= ?}, where O is the set of trans-
formations that can be applied on t. In our algo-
rithm, ? is the set of nodes that were affected by the
preceding transformation of the constructed subse-
quence.
The recursive procedure described in Algorithm 2
generates all coherent subsequences of lengths up to
d. It should be initially invoked with t - the current
state (parse tree) being expanded, ? - the set of all its
nodes, d - the maximal required length, and ? as an
empty initial sequence. We useO?o as concatenation
of an operation o to a subsequence O.
Algorithm 2 local-lookahead (t,?,d,O)
1: if d = 0 then
2: return ? (empty-set)
3: end if
4: SUBSEQUENCES? ?
5: for all o ? enabled ops(t, ?) do
6: Let t `o t?
7: Add {O?o}?local-lookahead(t?, ?affected(o, t?), d?1, O?
o) to SUBSEQUENCES
8: end for
9: return SUBSEQUENCES
The loop in lines 5 - 8 iterates over transforma-
tions that can be applied on the input tree, t, requir-
ing the same nodes that were affected by the pre-
vious transformation of the subsequence being con-
structed. Note that in the first call enabled ops(t, ?)
contain all operations that can be applied on t, with
no restriction. Applying an operation o results in a
new subsequence O ? o. This subsequence will be
part of the set of subsequences found by the proce-
dure. In addition, it will be used in the next recur-
sive call as the prefix of additional (longer) subse-
quences.
3.5 Local-lookahead gradient search
We are now ready to define our new algorithm
LOCAL-LOOKAHEAD GRADIENT SEARCH
(LLGS). In LLGS, like in greedy search,
kmaintain=kexpand= 1. expand(s) is defined to
return all states generated by subsequences found
by the local-lookahead procedure, while the evalua-
tion function is defined as f = f? (see last row of
Table 2).
4 Evaluation
In this section we first evaluate the search perfor-
mance in terms of efficiency (run time), the quality
288
of the found proofs (as measured by proof cost), and
overall inference performance achieved through var-
ious search algorithms. Finally we analyze the con-
tribution of our two novel components.
4.1 Evaluation settings
We performed our experiments on the last two
published RTE datasets: RTE-5 (2009) and RTE-
6 (2010). The RTE-5 dataset is composed of a
training and test corpora, each containing 600 text-
hypothesis pairs, where in half of them the text en-
tails the hypothesis and in the other half it does
not. In RTE-6, each of the training and test cor-
pora consists of 10 topics, where each topic con-
tains 10 documents. Each corpus contains a set of
hypotheses (211 in the training dataset, and 243 in
the test dataset), along with a set of candidate en-
tailing sentences for each hypothesis. The system
has to find for each hypothesis which candidate sen-
tences entail it. To improve speed and results, we
used the filtering mechanism suggested by (Mirkin
et al, 2009), which filters the candidate sentences
by the Lucene IR engine3. Thus, only top 20 candi-
dates per hypothesis were tested
Evaluation of each of the algorithms was
performed by running BIUTEE while replacing
BIUTEE-orig with this algorithm. We employed a
comprehensive set of knowledge resources (avail-
able in BIUTEE?s web site): WordNet (Fellbaum,
1998), Directional similarity (Kotlerman et al,
2010), DIRT (Lin and Pantel, 2001) and generic syn-
tactic rules. In addition, we used coreference substi-
tutions, detected by ArkRef4.
We evaluated several known algorithms, de-
scribed in Table 2 above, as well as BIUTEE-orig.
The latter is a strong baseline, which outperforms
known search algorithms in generating low cost
proofs. We compared all the above mentioned al-
gorithms to our novel one, LLGS.
We used the training dataset for parameter tun-
ing, which controls the trade-off between speed and
quality. For weighted A*, as well as for greedy
search, we used w = 6.0, since, for a few instances,
lower values of w resulted in prohibitive runtime.
For beam search we used k = 150, since higher val-
3http://lucene.apache.org
4www.ark.cs.cmu.edu/ARKref/ See (Haghighi and
Klein, 2009)
ues of k did not improve the proof cost on the train-
ing dataset. The value of d in LLGS was set to 3.
d = 4 yielded the same proof costs, but was about 3
times slower.
Since lower values of w could be used by
weighted A* for most instances, we also ran ex-
periments where we varied the value of w accord-
ing to the dovetailing method suggested in (Valen-
zano et al, 2010) (denoted dovetailing WA*) as fol-
lows. When weighted A* has found a solution, we
reran it with a new value of w, set to half of the
previous value. The idea is to guide the search for
lower cost solutions. This process was halted when
the total number of states generated by all weighted
A* instances exceeded a predefined constant (set to
10, 000).
4.2 Search performance
This experiment evaluates the search algorithms in
both efficiency (run-time) and proof quality. Effi-
ciency is measured by the average CPU (Intel Xeon
2.5 GHz) run-time (in seconds) for finding a com-
plete proof for a text-hypothesis instance, and by the
average number of generated states along the search.
Proof quality is measured by its cost.
The comparison of costs requires that all experi-
ments are performed on the same model which was
learned during training. Thus, in the training phase
we used the original search of BIUTEE, and then ran
the test phase with each algorithm separately. The
results, presented in Table 3, show that our novel
algorithm, LLGS, outperforms all other algorithms
in finding lower cost proofs. The second best is
BIUTEE-orig which is much slower by a factor of
3 (on RTE-5) to 8 (on RTE-6)5. While inherently
fast algorithms, particularly greedy and pure heuris-
tic, achieve faster running times, they achieve lower
proof quality, as well as lower overall inference per-
formance (see next subsection).
4.3 Overall inference performance
In this experiment we test whether, and how much,
finding better proofs, by a better search algorithm,
improves overall success rate of the RTE system.
Table 4 summarizes the results (accuracy in RTE-5
5Calculating T-test, we found that runtime improvement is
statistically significant with p < 0.01, and p < 0.052 for cost
improvement over BIUTEE-orig.
289
Algorithm Avg. time
Avg.
generated
Avg. cost
Weighted A* 0.22 / 0.09 301 / 143 1.11 / 10.52
Dovetailing
WA*
7.85 / 8.53 9797 / 9979 1.05 / 10.28
Greedy 0.20 / 0.10 468 / 158 1.10 / 10.55
Pure heuristic 0.09 / 0.10 123 / 167 1.35 / 12.51
Beam search 20.53 / 9.48 43925 / 18992 1.08 / 10.52
BIUTEE-orig 7.86 / 14.61 14749 / 22795 1.03 / 10.28
LLGS 2.76 / 1.72 1722 / 842 0.95 / 10.14
Table 3: Comparison of algorithms on RTE-5 / RTE-6
and F1 in RTE-6). We see that in RTE-5 LLGS out-
performs all other algorithms, and BIUTEE-orig is
the second best. This result is statistically significant
with p < 0.02 according to McNemar test. In RTE-
6 we see that although LLGS tends to finds lower
cost proofs, as shown in Table 3, BIUTEE obtains
slightly lower results when utilizing this algorithm.
Algorithm RTE-5 accuracy % RTE-6 F1 %
Weighted A* 59.50 48.20
Dovetailing WA* 60.83 49.01
Greedy 60.50 48.56
Pure heuristic 60.83 45.70
Beam search 61.33 48.58
BIUTEE-orig 60.67 49.25
LLGS 64.00 49.09
Table 4: Impact of algorithms on system success rate
4.4 Component evaluation
In this experiment we examine separately our two
novel components. We examined f? by running
LLGS with alternative evaluation functions. The re-
sults, displayed in Table 5, show that using f? yields
better proofs and also improves run time.
f Avg. time Avg. cost Accuracy %
f = g + h 3.28 1.06 61.50
f = g + w ? h 3.30 1.07 61.33
f = f? 2.76 0.95 64.0
Table 5: Impact of f? on RTE-5. w = 6.0. Accuracy
obtained by retraining with corresponding f .
Our local-lookahead (Subsection 3.4) was exam-
ined by running LLGS with alternative node expan-
sion methods. One alternative to local-lookahead
is standard expansion by generating all immediate
derivations. Another alternative is to use the stan-
dard lookahead, in which a brute-force depth-limited
search is performed in each iteration, termed here
?exhaustive lookahead?. The results, presented in
Table 6, show that by avoiding any type of looka-
head one can achieve fast runtime, while compro-
mising proof quality. On the other hand, both ex-
haustive and local lookahead yield better proofs and
accuracy, while local lookahead is more than 4 times
faster than exhaustive lookahead.
lookahead Avg. time Avg. cost Accuracy (%)
exhaustive 13.22 0.95 64.0
local 2.76 0.95 64.0
none 0.24 0.97 62.0
Table 6: Impact of local and global lookahead on RTE-5.
Accuracy obtained by retraining with the corresponding
lookahead method.
5 Conclusion
In this paper we investigated the efficiency and proof
quality obtained by various search algorithms. Con-
sequently, we observed special phenomena of the
search space in textual inference and proposed two
novel components yielding a new search algorithm,
targeted for our domain. We have shown empirically
that (1) this algorithm improves run time by factors
of 3-8 relative to BIUTEE-orig, and by similar fac-
tors relative to standard AI-search algorithms that
achieve similar proof quality; and (2) outperforms
all other algorithms in finding low cost proofs.
In future work we plan to investigate other search
paradigms, e.g., Monte-Carlo style approaches
(Kocsis and Szepesva?ri, 2006), which do not fall
under the AI search scheme covered in this paper.
In addition, while our novel components were moti-
vated by the search space of textual inference, we
foresee their potential utility in other application
areas for search, such as automated planning and
scheduling.
Acknowledgments
This work was partially supported by the Israel
Science Foundation grant 1112/08, the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886, and the Euro-
pean Community?s Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 287923
(EXCITEMENT).
290
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI.
Philip Bille. 2005. A survey on tree edit distance and
related problems. Theoretical Computer Science.
Adi Botea, Markus Enzenberger, Martin Mu?ller, and
Jonathan Schaeffer. 2005. Macro-FF: Improving ai
planning with automatically learned macro-operators.
J. Artif. Intell. Res. (JAIR), 24:581?621.
Vadim Bulitko and Mitja Lustrek. 2006. Lookahead
pathology in real-time path-finding. In proceedings of
AAAI.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In Proceedings of MLCW.
Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-
yakanok, Dan Roth, and Mark Sammons. 2005. An
inference model for semantic entailment in natural lan-
guage. In Proceedings of AAAI.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press, May.
Ariel Felner, Sarit Kraus, and Richard E. Korf. 2003.
KBFS: K-best-first search. Ann. Math. Artif. Intell.,
39(1-2):19?39.
David Furcy and Sven Koenig. 2005. Limited discrep-
ancy beam search. In proceedings of IJCAI.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Proceedings of EMNLP.
Stefan Harmeling. 2009. Inferring textual entailment
with a probabilistically sound calculus. Natural Lan-
guage Engineering.
Peter E. Hart, Nils J. Nilsson, and Bertram Raphael.
1968. A formal basis for the heuristic determination
of minimum cost paths. IEEE Transactions on Sys-
tems Science and Cybernetics, SSC-4(2):100?107.
Michael Heilman and Noah A. Smith. 2010. Tree
edit models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings of
NAACL.
Levente Kocsis and Csaba Szepesva?ri. 2006. Bandit
based monte-carlo planning. In proceedings of ECML.
Richard E. Korf. 1985. Macro-operators: A weak
method for learning. Artif. Intell., 26(1):35?77.
Richard E. Korf. 1990. Real-time heuristic search. Artif.
Intell., 42(2-3):189?211.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language En-
gineering.
Milen Kouylekov and Bernardo Magnini. 2005. Rec-
ognizing textual entailment with tree edit distance al-
gorithms. In Proceedings of Pascal Challenges Work-
shop on Recognising Textual Entailment.
Dekang Lin and Patrick Pantel. 2001. DIRT - discov-
ery of inference rules from text. In Proceedings of
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining.
Yashar Mehdad. 2009. Automatic cost estimation for
tree edit distance using particle swarm optimization.
In Proceedings of the ACL-IJCNLP.
Shachar Mirkin, Roy Bar-Haim, Jonathan Berant, Ido
Dagan, Eyal Shnarch, Asher Stern, and Idan Szpektor.
2009. Addressing discourse and document structure in
the rte search task. In Proceedings of TAC.
Ira Pohl. 1970. Heuristic search viewed as path finding
in a graph. Artificial Intelligence, 1(3-4):193 ? 204.
Stuart Russell and Peter Norvig. 2010. Artificial Intel-
ligence: A Modern Approach. Prentice-Hall, Engle-
wood Cliffs, NJ, 3rd edition edition.
Asher Stern and Ido Dagan. 2011. A confidence model
for syntactically-motivated entailment proofs. In Pro-
ceedings of RANLP.
Roni Stern, Tamar Kulberis, Ariel Felner, and Robert
Holte. 2010. Using lookaheads with optimal best-first
search. In proceedings of AAAI.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP.
Richard Anthony Valenzano, Nathan R. Sturtevant,
Jonathan Schaeffer, Karen Buro, and Akihiro Kishi-
moto. 2010. Simultaneously searching with multiple
settings: An alternative to parameter tuning for subop-
timal single-agent search algorithms. In proceedings
of ICAPS.
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question answer-
ing. In Proceedings of COLING.
Rong Zhou and Eric A. Hansen. 2005. Beam-stack
search: Integrating backtracking with beam search. In
proceedings of ICAPS.
291
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 73?78,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
BIUTEE: A Modular Open-Source System for Recognizing Textual
Entailment
Asher Stern
Computer Science Department
Bar-Ilan University
Ramat-Gan 52900, Israel
astern7@gmail.com
Ido Dagan
Computer Science Department
Bar-Ilan University
Ramat-Gan 52900, Israel
dagan@cs.biu.ac.il
Abstract
This paper introduces BIUTEE1, an open-
source system for recognizing textual entail-
ment. Its main advantages are its ability to uti-
lize various types of knowledge resources, and
its extensibility by which new knowledge re-
sources and inference components can be eas-
ily integrated. These abilities make BIUTEE
an appealing RTE system for two research
communities: (1) researchers of end applica-
tions, that can benefit from generic textual in-
ference, and (2) RTE researchers, who can in-
tegrate their novel algorithms and knowledge
resources into our system, saving the time and
effort of developing a complete RTE system
from scratch. Notable assistance for these re-
searchers is provided by a visual tracing tool,
by which researchers can refine and ?debug?
their knowledge resources and inference com-
ponents.
1 Introduction
Recognizing Textual Entailment (RTE) is the task of
identifying, given two text fragments, whether one
of them can be inferred from the other (Dagan et al,
2006). This task generalizes a common problem that
arises in many tasks at the semantic level of NLP.
For example, in Information Extraction (IE), a sys-
tem may be given a template with variables (e.g., ?X
is employed by Y?) and has to find text fragments
from which this template, with variables replaced
by proper entities, can be inferred. In Summariza-
tion, a good summary should be inferred from the
1www.cs.biu.ac.il/?nlp/downloads/biutee
given text, and, in addition, should not contain du-
plicated information, i.e., sentences which can be in-
ferred from other sentences in the summary. Detect-
ing these inferences can be performed by an RTE
system.
Since first introduced, several approaches have
been proposed for this task, ranging from shallow
lexical similarity methods (e.g., (Clark and Har-
rison, 2010; MacKinlay and Baldwin, 2009)), to
complex linguistically-motivated methods, which
incorporate extensive linguistic analysis (syntactic
parsing, coreference resolution, semantic role la-
belling, etc.) and a rich inventory of linguistic and
world-knowledge resources (e.g., (Iftene, 2008; de
Salvo Braz et al, 2005; Bar-Haim et al, 2007)).
Building such complex systems requires substantial
development efforts, which might become a barrier
for new-comers to RTE research. Thus, flexible and
extensible publicly available RTE systems are ex-
pected to significantly facilitate research in this field.
More concretely, two major research communities
would benefit from a publicly available RTE system:
1. Higher-level application developers, who
would use an RTE system to solve inference
tasks in their application. RTE systems for
this type of researchers should be adaptable
for the application specific data: they should
be configurable, trainable, and extensible
with inference knowledge that captures
application-specific phenomena.
2. Researchers in the RTE community, that would
not need to build a complete RTE system for
their research. Rather, they may integrate
73
their novel research components into an ex-
isting open-source system. Such research ef-
forts might include developing knowledge re-
sources, developing inference components for
specific phenomena such as temporal infer-
ence, or extending RTE to different languages.
A flexible and extensible RTE system is ex-
pected to encourage researchers to create and
share their textual-inference components. A
good example from another research area is the
Moses system for Statistical Machine Transla-
tion (SMT) (Koehn et al, 2007), which pro-
vides the core SMT components while being
extended with new research components by a
large scientific community.
Yet, until now rather few and quite limited RTE
systems were made publicly available. Moreover,
these systems are restricted in the types of knowl-
edge resources which they can utilize, and in the
scope of their inference algorithms. For example,
EDITS2 (Kouylekov and Negri, 2010) is a distance-
based RTE system, which can exploit only lexical
knowledge resources. NutCracker3 (Bos and Mark-
ert, 2005) is a system based on logical represen-
tation and automatic theorem proving, but utilizes
only WordNet (Fellbaum, 1998) as a lexical knowl-
edge resource.
Therefore, we provide our open-source textual-
entailment system, BIUTEE. Our system provides
state-of-the-art linguistic analysis tools and exploits
various types of manually built and automatically
acquired knowledge resources, including lexical,
lexical-syntactic and syntactic rewrite rules. Fur-
thermore, the system components, including pre-
processing utilities, knowledge resources, and even
the steps of the inference algorithm, are modu-
lar, and can be replaced or extended easily with
new components. Extensibility and flexibility are
also supported by a plug-in mechanism, by which
new inference components can be integrated with-
out changing existing code.
Notable support for researchers is provided by a
visual tracing tool, Tracer, which visualizes every
step of the inference process as shown in Figures 2
2http://edits.fbk.eu/
3http://svn.ask.it.usyd.edu.au/trac/
candc/wiki/nutcracker
and 3. We will use this tool to illustrate various in-
ference components in the demonstration session.
2 System Description
2.1 Inference algorithm
In this section we provide a high level description of
the inference components. Further details of the al-
gorithmic components appear in references provided
throughout this section.
BIUTEE follows the transformation based
paradigm, which recognizes textual entailment
by converting the text into the hypothesis via a
sequence of transformations. Such a sequence is
often referred to as a proof, and is performed, in our
system, over the syntactic representation of the text
- the text?s parse tree(s). A transformation modifies
a given parse tree, resulting in a generation of a
new parse tree, which can be further modified by
subsequent transformations.
Consider, for example, the following text-
hypothesis pair:
Text: ... Obasanjo invited him to step down as president
... and accept political asylum in Nigeria.
Hypothesis: Charles G. Taylor was offered asylum in
Nigeria.
This text-hypothesis pair requires two major
transformations: (1) substituting ?him? by ?Charles
G. Taylor? via a coreference substitution to an ear-
lier mention in the text, and (2) inferring that if ?X
accept Y? then ?X was offered Y?.
BIUTEE allows many types of transformations,
by which any hypothesis can be proven from any
text. Given a T-H pair, the system finds a proof
which generates H from T, and estimates the proof
validity. The system returns a score which indicates
how likely it is that the obtained proof is valid, i.e.,
the transformations along the proof preserve entail-
ment from the meaning of T.
The main type of transformations is application of
entailment-rules (Bar-Haim et al, 2007). An entail-
ment rule is composed of two sub-trees, termed left-
hand-side and right-hand-side, and is applied on a
parse-tree fragment that matches its left-hand-side,
by substituting the left-hand-side with the right-
hand-side. This formalism is simple yet power-
ful, and captures many types of knowledge. The
simplest type of rules is lexical rules, like car ?
74
vehicle. More complicated rules capture the en-
tailment relation between predicate-argument struc-
tures, like X accept Y ? X was offered
Y. Entailment rules can also encode syntactic
phenomena like the semantic equivalence of ac-
tive and passive structures (X Verb[active]
Y ? Y is Verb[passive] by X). Various
knowledge resources, represented as entailment
rules, are freely available in BIUTEE?s web-site. The
complete formalism of entailment rules, adopted by
our system, is described in (Bar-Haim et al, 2007).
Coreference relations are utilized via coreference-
substitution transformations: one mention of an en-
tity is replaced by another mention of the same en-
tity, based on coreference relations. In the above ex-
ample the system could apply such a transformation
to substitute ?him? with ?Charles G. Taylor?.
Since applications of entailment rules and coref-
erence substitutions are yet, in most cases, insuffi-
cient in transforming T into H, our system allows
on-the-fly transformations. These transformations
include insertions of missing nodes, flipping parts-
of-speech, moving sub-trees, etc. (see (Stern and
Dagan, 2011) for a complete list of these transforma-
tions). Since these transformations are not justified
by given knowledge resources, we use linguistically-
motivated features to estimate their validity. For ex-
ample, for on-the-fly lexical insertions we consider
as features the named-entity annotation of the in-
serted word, and its probability estimation according
to a unigram language model, which yields lower
costs for more frequent words.
Given a (T,H) pair, the system applies a search
algorithm (Stern et al, 2012) to find a proof O =
(o1, o2, . . . on) that transforms T into H. For each
proof step oi the system calculates a cost c(oi). This
cost is defined as follows: the system uses a weight-
vector w, which is learned in the training phase. In
addition, each transformation oi is represented by a
feature vector f(oi) which characterizes the trans-
formation. The cost c(oi) is defined as w ? f(oi).
The proof cost is defined as the sum of the costs of
the transformations from which it is composed, i.e.:
c(O) ,
n?
i=1
c(oi) =
n?
i=1
w ? f(oi) = w ?
n?
i=1
f(oi)
(1)
If the proof cost is below a threshold b, then the sys-
tem concludes that T entails H. The complete de-
scription of the cost model, as well as the method
for learning the parameters w and b is described in
(Stern and Dagan, 2011).
2.2 System flow
The BIUTEE system flow (Figure 1) starts with pre-
processing of the text and the hypothesis. BIUTEE
provides state-of-the-art pre-processing utilities:
Easy-First parser (Goldberg and Elhadad, 2010),
Stanford named-entity-recognizer (Finkel et al,
2005) and ArkRef coreference resolver (Haghighi
and Klein, 2009), as well as utilities for sentence-
splitting and numerical-normalizations. In addition,
BIUTEE supports integration of users? own utilities
by simply implementing the appropriate interfaces.
Entailment recognition begins with a global pro-
cessing phase in which inference related computa-
tions that are not part of the proof are performed.
Annotating the negation indicators and their scope
in the text and hypothesis is an example of such cal-
culation. Next, the system constructs a proof which
is a sequence of transformations that transform the
text into the hypothesis. Finding such a proof is a
sequential process, conducted by the search algo-
rithm. In each step of the proof construction the sys-
tem examines all possible transformations that can
be applied, generates new trees by applying selected
transformations, and calculates their costs by con-
structing appropriate feature-vectors for them.
New types of transformations can be added to
BIUTEE by a plug-in mechanism, without the need
to change the code. For example, imagine that a
researcher applies BIUTEE on the medical domain.
There might be some well-known domain knowl-
edge and rules that every medical person knows.
Integrating them is directly supported by the plug-in
mechanism. A plug-in is a piece of code which im-
plements a few interfaces that detect which transfor-
mations can be applied, apply them, and construct
appropriate feature-vectors for each applied trans-
formation. In addition, a plug-in can perform com-
putations for the global processing phase.
Eventually, the search algorithm finds a (approx-
imately) lowest cost proof. This cost is normalized
as a score between 0 and 1, and returned as output.
Training the cost model parameters w and b
(see subsection 2.1) is performed by a linear learn-
75
Figure 1: System architecture
RTE
challenge
Median Best BIUTEE
RTE-6 33.72 48.01 49.09
RTE-7 39.89 48.00 42.93
Table 1: Performance (F1) of BIUTEE on RTE chal-
lenges, compared to other systems participated in these
challenges. Median and Best indicate the median score
and the highest score of all submissions, respectively.
ing algorithm, as described in (Stern and Dagan,
2011). We use a Logistic-Regression learning algo-
rithm, but, similar to other components, alternative
learning-algorithms can be integrated easily by im-
plementing an appropriate interface.
2.3 Experimental results
BIUTEE?s performance on the last two RTE chal-
lenges (Bentivogli et al, 2011; Bentivogli et al,
2010) is presented in Table 1: BIUTEE is better than
the median of all submitted results, and in RTE-6 it
outperforms all other systems.
3 Visual Tracing Tool
As a complex system, the final score provided as
output, as well as the system?s detailed logging in-
formation, do not expose all the decisions and cal-
culations performed by the system. In particular,
they do not show all the potential transformations
that could have been applied, but were rejected by
the search algorithm. However, such information is
crucial for researchers, who need to observe the us-
age and the potential impact of each component of
the system.
We address this need by providing an interactive
visual tracing tool, Tracer, which presents detailed
information on each proof step, including potential
steps that were not included in the final proof. In the
demo session, we will use the visual tracing tool to
illustrate all of BIUTEE?s components4.
3.1 Modes
Tracer provides two modes for tracing proof con-
struction: automatic mode and manual mode. In au-
tomatic mode, shown in Figure 2, the tool presents
the complete process of inference, as conducted by
the system?s search: the parse trees, the proof steps,
the cost of each step and the final score. For each
transformation the tool presents the parse tree before
and after applying the transformation, highlighting
the impact of this transformation. In manual mode,
the user can invoke specific transformations pro-
actively, including transformations rejected by the
search algorithm for the eventual proof. As shown in
Figure 3, the tool provides a list of transformations
that match the given parse-tree, from which the user
chooses and applies a single transformation at each
step. Similar to automatic mode, their impact on the
parse tree is shown visually.
3.2 Use cases
Developers of knowledge resources, as well as other
types of transformations, can be aided by Tracer as
follows. Applying an entailment rule is a process
of first matching the rule?s left-hand-side to the text
parse-tree (or to any tree along the proof), and then
substituting it by the rule?s right-hand-side. To test a
4Our demonstration requirements are a large screen and In-
ternet connection.
76
Figure 2: Entailment Rule application visualized in tracing tool. The upper pane displays the parse-tree generated by
applying the rule. The rule description is the first transformation (printed in bold) of the proof, shown in the lower
pane. It is followed by transformations 2 and 3, which are syntactic rewrite rules.
rule, the user can provide a text for which it is sup-
posed to match, examine the list of potential trans-
formations that can be performed on the text?s parse
tree, as in Figure 3, and verify that the examined
rule has been matched as expected. Next, the user
can apply the rule, visually examine its impact on
the parse-tree, as in Figure 2, and validate that it op-
erates as intended with no side-effects.
The complete inference process depends on the
parameters learned in the training phase, as well as
on the search algorithm which looks for lowest-cost
proof from T to H. Researchers investigating these
algorithmic components can be assisted by the trac-
ing tool as well. For a given (T,H) pair, the auto-
matic mode provides the complete proof found by
the system. Then, in the manual mode the researcher
can try to construct alternative proofs. If a proof
with lower cost can be constructed manually it im-
plies a limitation of the search algorithm. On the
other hand, if the user can manually construct a bet-
ter linguistically motivated proof, but it turns out that
this proof has higher cost than the one found by the
system, it implies a limitation of the learning phase
which may be caused either by a limitation of the
learning method, or due to insufficient training data.
4 Conclusions
In this paper we described BIUTEE, an open-source
textual-inference system, and suggested it as a re-
search platform in this field. We highlighted key
advantages of BIUTEE, which directly support re-
searchers? work: (a) modularity and extensibility,
(b) a plug-in mechanism, (c) utilization of entail-
ment rules, which can capture diverse types of
knowledge, and (d) a visual tracing tool, which vi-
sualizes all the details of the inference process.
Acknowledgments
This work was partially supported by the Israel
Science Foundation grant 1112/08, the PASCAL-
77
Figure 3: List of available transformations, provided by Tracer in the manual mode. The user can manually choose
and apply each of these transformations, and observe their impact on the parse-tree.
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886, and the Euro-
pean Community?s Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 287923
(EXCITEMENT).
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Dang, and
Danilo Giampiccolo. 2010. The sixth pascal recog-
nizing textual entailment challenge. In Proceedings of
TAC.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Dang, and
Danilo Giampiccolo. 2011. The seventh pascal recog-
nizing textual entailment challenge. In Proceedings of
TAC.
Johan Bos and Katja Markert. 2005. Recognising textual
entailment with logical inference. In Proceedings of
EMNLP.
Peter Clark and Phil Harrison. 2010. Blue-lite: a
knowledge-based lexical entailment system for rte6.
In Proceedings of TAC.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment chal-
lenge. In Quionero-Candela, J.; Dagan, I.; Magnini,
B.; d?Alch-Buc, F. (Eds.) Machine Learning Chal-
lenges. Lecture Notes in Computer Science.
Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-
yakanok, Dan Roth, and Mark Sammons. 2005. An
inference model for semantic entailment in natural lan-
guage. In Proceedings of AAAI.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press, May.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of ACL.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of NAACL.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Proceedings of EMNLP.
Adrian Iftene. 2008. Uaic participation at rte4. In Pro-
ceedings of TAC.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL.
Milen Kouylekov and Matteo Negri. 2010. An open-
source package for recognizing textual entailment. In
Proceedings of ACL Demo.
Andrew MacKinlay and Timothy Baldwin. 2009. A
baseline approach to the rte5 search pilot. In Proceed-
ings of TAC.
Asher Stern and Ido Dagan. 2011. A confidence model
for syntactically-motivated entailment proofs. In Pro-
ceedings of RANLP.
Asher Stern, Roni Stern, Ido Dagan, and Ariel Felner.
2012. Efficient search for transformation-based infer-
ence. In Proceedings of ACL.
78
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 739?744,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Recognizing Implied Predicate-Argument Relationships
in Textual Inference
Asher Stern
Computer Science Department
Bar-Ilan University
astern7@cs.biu.ac.il
Ido Dagan
Computer Science Department
Bar-Ilan University
dagan@cs.biu.ac.il
Abstract
We investigate recognizing implied
predicate-argument relationships which
are not explicitly expressed in syntactic
structure. While prior works addressed
such relationships as an extension to se-
mantic role labeling, our work investigates
them in the context of textual inference
scenarios. Such scenarios provide prior
information, which substantially eases
the task. We provide a large and freely
available evaluation dataset for our task
setting, and propose methods to cope with
it, while obtaining promising results in
empirical evaluations.
1 Motivation and Task
This paper addresses a typical sub-task in tex-
tual inference scenarios, of recognizing implied
predicate-argument relationships which are not
expressed explicitly through syntactic structure.
Consider the following example:
(i)
The crucial role Vioxx plays in Merck?s port-
folio was apparent last week when Merck?s
shares plunged 27 percent to 33 dollars after the
withdrawal announcement.
While a human reader understands that the
withdrawal refers to Vioxx, and hence an im-
plied predicate-argument relationship holds be-
tween them, this relationship is not expressed in
the syntactic structure, and will be missed by syn-
tactic parsers or standard semantic role labelers.
This paper targets such types of implied rela-
tionships in textual inference scenarios. Partic-
ularly, we investigate the setting of Recognizing
Textual Entailment (RTE) as a typical scenario of
textual inference. We suggest, however, that the
same challenge, as well as the solutions proposed
in our work, are applicable, with proper adap-
tations, to other textual-inference scenarios, like
Question Answering, and Information Extraction
(see Section 6).
An RTE problem instance is composed of two
text fragments, termed Text and Hypothesis, as in-
put. The task is to recognize whether a human
reading the Text would infer that the Hypothesis
is most likely true (Dagan et al, 2006). For our
problem, consider a positive Text Hypothesis pair,
where the Text is example (i) above and the Hy-
pothesis is:
(ii)
Merck withdrew Vioxx.
A common approach for recognizing textual en-
tailment is to verify that all the textual elements
of the Hypothesis are covered, or aligned, by el-
ements of the Text. These elements typically in-
clude lexical terms as well as relationships be-
tween them. In our example, the Hypothesis lexi-
cal terms (?Merck?, ?withdrew? and ?Vioxx?) are
indeed covered by the Text. Yet, the predicate-
argument relationships (e.g., ?withdrawal-Vioxx?)
are not expressed in the text explicitly. In such
a case, an RTE system has to verify that the
predicate-argument relationships which are ex-
plicitly expressed in the Hypothesis, are implied
from the Text discourse. Such cases are quite fre-
quent (?17%) in the settings of our dataset, de-
scribed in Section 3.
Consequently, we define the task of recognizing
implied predicate-argument relationships, with il-
lustrating examples in Table 1, as follows. The
input includes a Text and a Hypothesis. Two terms
in the Hypothesis, predicate and argument, are
marked, where a predicate-argument relationship
between them is explicit in the Hypothesis syntac-
tic structure. Two terms in the Text, candidate-
predicate and candidate-argument, aligned to the
Hypothesis predicate and argument, are marked
as well. However, no predicate-argument rela-
tionship between them is expressed syntactically.
The task is to recognize whether the predicate-
739
# Hypothesis Text Y/N
1 Merck [withdrew]
pred
[Vioxx]
arg
from the market.
The crucial role [Vioxx]
cand-arg
plays in Merck?s
portfolio was apparent last week when Merck?s
shares plunged 27 percent to 33 dollars after the
[withdrawal]
cand-pred
announcement.
Y
2 Barbara Cummings heard the tale
of a woman who was coming
to Crawford to [join]
pred
Cindy
Sheehans [protest]
arg
.
Sheehan?s [protest]
cand-arg
is misguided and is hurting
troop morale. . . .
Sheehan never wanted Casey to [join]
cand-pred
the mil-
itary.
N
3 Casey Sheehan was [killed]
pred
in
[Iraq]
arg
.
5 days after he arrived in [Iraq]
cand-arg
last year, Casey
Sheehan was [killed]
cand-pred
.
Y
4 Hurricane Rita [threatened]
pred
[New Orleans]
arg
.
Hurricane Rita was upgraded from a tropical storm as
it [threatened]
cand-pred
the southeastern United States,
forcing an alert in southern Florida and scuttling plans
to repopulate [New Orleans]
cand-arg
after Hurricane
Katrina turned it into a ghost city 3 weeks earlier.
Y
5 Alberto Gonzales defends
[renewal]
pred
of the [Patriot
Act]
arg
to Congress.
A senior official defended the [Patriot Act]
cand-arg
. . .
. . . President Bush has urged Congress to
[renew]
cand-pred
the law . . .
Y
6 The [train]
arg
[crash]
pred
injured
nearly 200 people.
At least 10 people were killed . . . in the [crash]
cand-pred
. . .
Alvarez is accused of . . . causing the derailment of one
[train]
cand-arg
. . .
Y
Table 1: Example task instances from our dataset. The last column specifies the Yes/No annotation,
indicating whether the sought predicate-argument relationship is implied in the Text. For illustration, a
dashed line indicates an explicit argument that is related to the candidate argument through some kind of
discourse reference. Pred, arg and cand abbreviate predicate, argument and candidate respectively.
argument relationship, as expressed in the Hypoth-
esis, holds implicitly also in the Text.
To address this task, we provide a large and
freely available annotated dataset, and propose
methods for coping with it. A related task, de-
scribed in the next section, deals with such implied
predicate-argument relationships as an extension
to Semantic Role Labeling. While the results re-
ported so far on that annotation task were rela-
tively low, we suggest that the task itself may be
more complicated than what is actually required
in textual inference scenarios. On the other hand,
the results obtained for our task, which does fit
textual inference scenarios, are promising, and en-
courage utilizing algorithms for this task in actual
inference systems.
2 Prior Work
The most notable work targeting implied
predicate-argument relationships is the 2010
SemEval task of Linking Events and Their Par-
ticipants in Discourse (Ruppenhofer et al, 2009).
This task extends Semantic Role Labeling to cases
in which a core argument of a predicate is missing
in the syntactic structure but a filler for the
corresponding semantic role appears elsewhere
and can be inferred from discourse. For example,
in the following sentence the semantic role goal is
unfilled:
(iii)
He arrived (0
Goal
) at 8pm.
Yet, we can expect to find an implied filler for
goal elsewhere in the document.
The SemEval task, termed henceforth as Im-
plied SRL, involves three major sub-tasks. First,
for each predicate, the unfilled roles, termed Null
Instantiations (NI), should be detected. Second,
each NI should be classified as Definite NI (DNI),
meaning that the role filler must exist in the dis-
course, or Indefinite NI otherwise. Third, the DNI
fillers should be found (DNI linking).
Later works that followed the SemEval chal-
lenge include (Silberer and Frank, 2012) and
(Roth and Frank, 2013), which proposed auto-
740
matic dataset generation methods and features
which capture discourse phenomena. Their high-
est result was 12% F1-score. Another work is the
probabilistic model of Laparra and Rigau (2012),
which is trained by properties captured not only
from implicit arguments but also from explicit
ones, resulting in 19% F1-score. Another notable
work is (Gerber and Chai, 2012), which was lim-
ited to ten carefully selected nominal predicates.
2.1 Annotations vs. Recognition
Comparing to the implied SRL task, our task may
better fit the needs of textual inference. First, some
relatively complex steps of the implied SRL task
are avoided in our setting, while on the other hand
it covers more relevant cases.
More concretely, in textual inference the can-
didate predicate and argument are typically iden-
tified, as they are aligned by the RTE system to
a predicate and an argument of the Hypothesis.
Thus, the only remaining challenge is to verify
that the sought relationship is implied in the text.
Therefore, the sub-tasks of identifying and classi-
fying DNIs can be avoided.
On the other hand, in some cases the candi-
date argument is not a DNI, but is still required
in textual inference. One type of such cases are
non-core arguments, which cannot be Definite NIs.
However, textual inference deals with non-core ar-
guments as well (see example 3 in Table 1).
Another case is when an implied predicate-
argument relationship holds even though the cor-
responding role is already filled by another argu-
ment, hence not an NI. Consider example 4 of Ta-
ble 1. While the object of ?threatened? is filled (in
the Text) by ?southeastern United States?, a hu-
man reader also infers the ?threatened-New Or-
leans? relationship. Such cases might follow a
meronymy relation between the filler (?southeast-
ern United States?) and the candidate argument
(?New Orleans?), or certain types of discourse (co-
)references (e.g., example 5 in Table 1), or some
other linguistic phenomena. Either way, they are
crucial for textual inference, while not being NIs.
3 Dataset
This section describes a semi-automatic method
for extracting candidate instances of implied
predicate-argument relationship from an RTE
dataset. This extraction process directly follows
our task formalization. Given a Text Hypothe-
sis pair, we locate a predicate-argument relation-
ship in the Hypothesis, where both the predicate
and the argument appear also in the Text, while
the relationship between them is not expressed in
its syntactic structure. This process is performed
automatically, based on syntactic parsing (see be-
low). Then, a human reader annotates each in-
stance as ?Yes? ? meaning that the implied rela-
tionship indeed holds in the Text, or ?No? other-
wise. Example instances, constructed by this pro-
cess, are shown in Table 1.
In this work we used lemma-level lexical
matching, as well as nominalization matching, to
align the Text predicates and arguments to the Hy-
pothesis. We note that more advanced match-
ing, e.g., by utilizing knowledge resources (like
WordNet), can be performed as well. To identify
explicit predicate-argument relationships we uti-
lized dependency parsing by the Easy-First parser
(Goldberg and Elhadad, 2010). Nominalization
matching (e.g., example 1 of Table 1) was per-
formed with Nomlex (Macleod et al, 1998).
By applying this method on the RTE-6 dataset
(Bentivogli et al, 2010), we constructed a
dataset of 4022 instances, where 2271 (56%)
are annotated as positive instances, and 1751
as negative ones. This dataset is significantly
larger than prior datasets for the implied SRL
task. To calculate inter-annotator agreement, the
first author also annotated 185 randomly-selected
instances. We have reached high agreement score
of 0.80 Kappa. The dataset is freely available at
www.cs.biu.ac.il/
?
nlp/resources/
downloads/implied-relationships.
4 Recognition Algorithm
We defined 15 features, summarized in Table 2,
which capture local and discourse phenomena.
These features do not depend on manually built
resources, and hence are portable to resource-poor
languages. Some features were proposed in prior
works, and are marked by G&C (Gerber and Chai,
2012) or S&F (Silberer and Frank, 2012). Our best
results were obtained with the Random Forests
learning algorithm (Breiman, 2001). The first two
features are described in the next subsection, while
the others are explained in the table itself.
4.1 Statistical discourse features
Statistical features in prior works mostly cap-
ture general properties of the predicate and the
741
# Category Feature Prev. work
1 co-occurring predicate (explained in subsection 4.1) New
2
statistical
discourse co-occurring argument (explained in subsection 4.1) New
3 co-reference: whether an explicit argument of p co-refers with a. New
4 last known location: If the NE of a is ?location?, and it is the last
location mentioned before p in the document.
New
5 argument prominence: The frequency of the lemma of a in a two-
sentence windows of p, relative to all entities in that window.
S&F
6
local
discourse
predicate frequency in document: The frequency of p in the docu-
ment, relative to all predicates appear in the document.
G&C
7 statistical argument frequency: The Unigram-model likelihood of a
in English documents, calculated from a large corpus.
New
8 definite NP: Whether a is a definite NP G&C
9 indefinite NP: Whether a is an indefinite NP G&C
10 quantified predicate: Whether p is quantified (i.e., by expressions
like ?every . . . ?, ?a good deal of . . . ?, etc.)
G&C
11
local
candidate
properties
NE mismatch: Whether a is a named entity but the corresponding
argument in the hypothesis is not, or vice versa.
New
12 predicate-argument frequency: The likelihood of a to be an argu-
ment of p (formally: Pr(a|p)) in a large corpus.
similar feature
in G&C
13 sentence distance: The distance between p and a in sentences. G&C, S&F
14 mention distance: The distance between p and a in entity-mentions. S&F
15
predicate-
argument
relatedness
shared head-predicate: Whether p and a are themselves arguments
of another predicate.
G&C
Table 2: Algorithmic features. p and a denote the candidate predicate and argument respectively.
argument, like selectional preferences, lexical
similarities, etc. On the contrary, our statis-
tical features follow the intuition that explicit
predicate-argument relationships in the discourse
provide plausible indication that an implied
relationship holds as well. In our experiments
we collected the statistics from Reuters corpus
RCV1 (trec.nist.gov/data/reuters/
reuters.html), which contains more than
806,000 documents.
We defined two features: Co-occurring predi-
cate and Co-occurring argument. Let p and a be
the candidate predicate and the argument in the
text. While they are not connected syntactically,
each of them often has an explicit relationships
with other terms in the text, that might support the
sought (implied) relationship between a and p.
More concretely, a is often an explicit argument
of another predicate p
?
. For example, example 6 in
Table 1 includes the explicit relationship ?derail-
ment of train?, which might indicate the implied
relationship ?crash of train?. Hence p=?crash?,
a=?train? and p
?
=?derailment?. The Co-occurring
predicate feature estimates the probability that a
document would contain a as an argument of p,
given that a appears elsewhere in that document
as an argument of p
?
, based on explicit predicate-
argument relationships in a large corpus.
Similarly, the Co-occurring argument feature
captures cases where p has another explicit argu-
ment, a
?
. This is exemplified in example 5 of
Table 1, where p=?renew?, a=?Patriot Act? and
a
?
=?law?. Accordingly, the feature quantifies the
probability that a document including the relation-
ship p-a
?
would also include the relationship p-a.
More details about these features can be found
in the first author?s Ph.D. thesis at www.cs.biu.
ac.il/
?
nlp/publications/theses/
5 Results
We tested our method in a cross-validation setting,
and obtained high result as shown in the first row
of Table 3. Since our task and dataset are novel,
there is no direct baseline with which we can com-
pare this result. As a reference point we mention
the majority class proportion, and also report a
configuration in which only features adopted from
prior works (G&C and S&F) are utilized. This
742
Configuration Accuracy % ? %
Full algorithm 81.0 ?
Union of prior work 78.0 3.0
Major category (all true) 56.5 24.5
Ablation tests
no statistical discourse 79.9 1.1
no local discourse 79.3 1.7
no local candidate properties 79.2 1.8
no predicate-argument relatedness 79.7 1.3
Table 3: Accuracy of our method, followed by
baselines and ablation tests.
Configuration (input) Recall Precision F1 %
Explicit only 44.6 44.3 44.4
Human annotations 50.9 43.4 46.8
Algorithm recognition 48.5 42.3 45.2
Table 4: RTE-6 Experiment
comparison shows that the contribution of our new
features (3%) is meaningful, which is also statis-
tically significant with p < 0.01 using Bootstrap
Resampling test (Koehn, 2004). The high results
show that this task is feasible, and its solutions
can be adopted as a component in textual infer-
ence systems. The positive contribution of each
feature category is shown in ablation tests.
An additional experiment tests the contribution
of recognizing implied predicate-argument rela-
tionships for overall RTE, specifically on the RTE-
6 dataset. For the scope of this experiment we de-
veloped a simple RTE system, which uses the F1
optimized logistic regression classifier of Jansche
(2005) with two features: lexical coverage and
predicate-argument relationships coverage. We
ran three configurations for the second feature,
where in the first only syntactically expressed re-
lationships are used, in the second all the implied
relationships, as detected by a human annotator,
are added, and in the third only the implied rela-
tionships detected by our algorithm are added.
The results, presented in Table 4, first demon-
strate the full potential of the implied relation-
ship recognition task to improve textual entail-
ment recognition (Human annotation vs. Explicit
only). One third of this potential improvement is
achieved by our algorithm
1
. Note that all these re-
sults are higher than the median result in the RTE-
6 challenge (36.14%). While the delta in the F1
score is small in absolute terms, such magnitudes
1
Following the relatively modest size of the RTE dataset,
the Algorithm vs. Explicit result is not statistically significant
(p ' 0.1). However, the Human annotation vs. Explicit
result is statistically significant with p < 0.01.
are typical in RTE for most resources and tools
(see (Bentivogli et al, 2010)).
6 Discussion and Conclusions
We formulated the task of recognizing implied
predicate-argument relationships within textual in-
ference scenarios. We compared this task to the
labeling task of SemEval 2010, where no prior in-
formation about candidate arguments in the text is
available. We point out that in textual inference
scenarios the candidate predicate and argument
are given by the Hypothesis, while the challenge
is only to verify that a predicate-argument rela-
tionship between these candidates is implied from
the given Text. Accordingly, some complex steps
necessitated in the SemEval task can be avoided,
while additional relevant cases are covered.
Moreover, we have shown that this simpler task
is more feasibly solvable, where our 15 features
achieved more than 80% accuracy.
While our dataset and algorithm were presented
in the context of RTE, the same challenge and
methods are applicable to other textual inference
tasks as well. Consider, for example, the Ques-
tion Answering (QA) task. Typically QA sys-
tems detect a candidate predicate that matches the
question?s predicate. Similarly, candidate argu-
ments, which match either the expected answer
type or other arguments in the question are de-
tected too. Consequently, our methods which ex-
ploit the availability of the candidate predicate and
argument can be adapted to this scenario as well.
Similarly, a typical approach for Event Extrac-
tion (a sub task of Information Extraction) is to
start by applying an entity extractor, which identi-
fies argument candidates. Accordingly, candidate
predicate and arguments are detected in this sce-
nario too, while the remaining challenge is to as-
sess the likelihood that a predicate-argument rela-
tionship holds between them.
Following this observation, we propose future
work of applying our methods to other tasks. An
additional direction for future work is to further
develop new methods for our task, possibly by
incorporating SRL resources and/or linguistically
oriented rules, in order to improve the results we
achieved so far.
Acknowledgments
This work was partially supported by the EC-
funded project EXCITEMENT (FP7ICT-287923).
743
References
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The sixth
pascal recognizing textual entailment challenge. In
Proccidings of TAC.
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1).
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. Machine Learning Challenges. Evaluat-
ing Predictive Uncertainty, Visual Object Classifi-
cation, and Recognising Tectual Entailment, pages
177?190.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies rep-
resentation. In proceedings of COLING 2008 Work-
shop on Cross-framework and Cross-domain Parser
Evaluation.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of ACL.
Matthew Gerber and Joyce Y. Chai. 2012. Seman-
tic role labeling of implicit arguments for nominal
predicates. Computational Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of NAACL.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of EMNLP.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explorations, 11(1).
Martin Jansche. 2005. Maximum expected f-measure
training of logistic regression models. In Proceed-
ings of EMNLP.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP.
Egoitz Laparra and German Rigau. 2012. Exploiting
explicit annotations and semantic types for implicit
argument resolution. In Proceedings of IEEE-ICSC.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. Nomlex: A
lexicon of nominalizations. In Proceedings of EU-
RALEX.
Michael Roth and Anette Frank. 2013. Automatically
identifying implicit arguments to improve argument
linking and coherence modeling. In Proceedings of
*SEM.
Josef Ruppenhofer, Caroline Sporleder, Roser
Morante, Collin Baker, and Martha Palmer. 2009.
Semeval-2010 task 10: Linking events and their
participants in discourse. In The NAACL-HLT
2009 Workshop on Semantic Evaluations: Recent
Achievements and Future Directions (SEW-09).
Josef Ruppenhofer, Caroline Sporleder, Roser
Morante, Collin Baker, and Martha Palmer. 2010.
Semeval-2010 task 10: Linking events and their
participants in discourse. In Proceedings of the 5th
International Workshop on Semantic Evaluation.
Carina Silberer and Anette Frank. 2012. Casting im-
plicit role linking as an anaphora resolution task. In
Proceedings of *SEM.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of NAACL.
744
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 43?48,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
The Excitement Open Platform for Textual Inferences
Bernardo Magnini
?
, Roberto Zanoli
?
, Ido Dagan
?
, Kathrin Eichler
?
, G?unter Neumann
?
,
Tae-Gil Noh
?
, Sebastian Pado
?
, Asher Stern
?
, Omer Levy
?
?
FBK (magnini|zanoli@fbk.eu)
?
Heidelberg, Stuttgart Univ. (pado|noh@cl.uni-heidelberg.de)
?
DFKI (neumann|eichler@dfki.de)
?
Bar Ilan University (dagan|sterna3|omerlevy@cs.biu.ac.il)
Abstract
This paper presents the Excitement Open
Platform (EOP), a generic architecture and
a comprehensive implementation for tex-
tual inference in multiple languages. The
platform includes state-of-art algorithms,
a large number of knowledge resources,
and facilities for experimenting and test-
ing innovative approaches. The EOP is
distributed as an open source software.
1 Introduction
In the last decade textual entailment (Dagan et al.,
2009) has been a very active topic in Computa-
tional Linguistics, providing a unifying framework
for textual inference. Several evaluation exercises
have been organized around Recognizing Textual
Entailment (RTE) challenges and many method-
ologies, algorithms and knowledge resources have
been proposed to address the task. However, re-
search in textual entailment is still fragmented and
there is no unifying algorithmic framework nor
software architecture.
In this paper, we present the Excitement Open
Platform (EOP), a generic architecture and a com-
prehensive implementation for multilingual textual
inference which we make available to the scien-
tific and technological communities. To a large
extent, the idea is to follow the successful experi-
ence of the Moses open source platform (Koehn et
al., 2007) in Machine Translation, which has made
a substantial impact on research in that field. The
EOP is the result of a two-year coordinated work
under the international project EXCITEMENT.
1
A
consortium of four academic partners has defined
the EOP architectural specifications, implemented
the functional interfaces of the EOP components,
imported existing entailment engines into the EOP
1
http://www.excitement-project.eu
and finally designed and implemented a rich envi-
ronment to support open source distribution.
The goal of the platform is to provide function-
ality for the automatic identification of entailment
relations among texts. The EOP is based on a modu-
lar architecture with a particular focus on language-
independent algorithms. It allows developers and
users to combine linguistic pipelines, entailment al-
gorithms and linguistic resources within and across
languages with as little effort as possible. For ex-
ample, different entailment decision approaches
can share the same resources and the same sub-
components in the platform. A classification-based
algorithm can use the distance component of an
edit-distance based entailment decision approach,
and two different approaches can use the same set
of knowledge resources. Moreover, the platform
has various multilingual components for languages
like English, German and Italian. The result is an
ideal software environment for experimenting and
testing innovative approaches for textual inferences.
The EOP is distributed as an open source software
2
and its use is open both to users interested in using
inference in applications and to developers willing
to extend the current functionalities.
The paper is structured as follows. Section 2
presents the platform architecture, highlighting
how the EOP component-based approach favors
interoperability. Section 3 provides a picture of
the current population of the EOP in terms of both
entailment algorithms and knowledge resources.
Section 4 introduces expected use cases of the plat-
form. Finally, Section 5 presents the main features
of the open source package.
2 Architecture
The EOP platform takes as input two text portions,
the first called the Text (abbreviated with T), the
second called the Hypothesis (abbreviated with H).
2
http://hltfbk.github.io/
Excitement-Open-Platform/
43
Linguis'c)Analysis)Pipeline)(LAP))
Entailment)Core)(EC))
Entailment)Decision))Algorithm)(EDA))
Dynamic)and)Sta'c)Components)(Algorithms)and)Knowledge))
Linguis'c)Analysis)Components)
Decision)
1)
Raw)Data)
Figure 1: EOP architecture
The output is an entailment judgement, either ?En-
tailment? if T entails H, or ?NonEntailment? if the
relation does not hold. A confidence score for the
decision is also returned in both cases.
The EOP architecture (Pad?o et al., 2014) is based
on the concept of modularization with pluggable
and replaceable components to enable extension
and customization. The overall structure is shown
in Figure 1 and consists of two main parts. The
Linguistic Analysis Pipeline (LAP) is a series of
linguistic annotation components. The Entailment
Core (EC) performs the actual entailment recog-
nition. This separation ensures that (a) the com-
ponents in the EC only rely on linguistic analysis
in well-defined ways and (b) the LAP and EC can
be run independently of each other. Configuration
files are the principal means of configuring the EOP.
In the rest of this section we first provide an intro-
duction to the LAP, then we move to the EC and
finally describe the configuration files.
2.1 Linguistic Analysis Pipeline (LAP)
The Linguistic Analysis Pipeline is a collection of
annotation components for Natural Language Pro-
cessing (NLP) based on the Apache UIMA frame-
work.
3
Annotations range from tokenization to
part of speech tagging, chunking, Named Entity
Recognition and parsing. The adoption of UIMA
enables interoperability among components (e.g.,
substitution of one parser by another one) while
ensuring language independence. Input and output
of the components are represented in an extended
version of the DKPro type system based on UIMA
3
http://uima.apache.org/
Common Analysis Structure (CAS) (Gurevych et
al., 2007; Noh and Pad?o, 2013).
2.2 Entailment Core (EC)
The Entailment Core performs the actual entail-
ment recognition based on the preprocessed text
made by the Linguistic Analysis Pipeline. It con-
sists of one or more Entailment Decision Algo-
rithms (EDAs) and zero or more subordinate com-
ponents. An EDA takes an entailment decision
(i.e., ?entailment? or ?no entailment?) while com-
ponents provide static and dynamic information for
the EDA.
Entailment Decision Algorithms are at the top
level in the EC. They compute an entailment deci-
sion for a given Text/Hypothesis (T/H) pair, and
can use components that provide standardized al-
gorithms or knowledge resources. The EOP ships
with several EDAs (cf. Section 3).
Scoring Components accept a Text/Hypothesis
pair as an input, and return a vector of scores.
Their output can be used directly to build minimal
classifier-based EDAs forming complete RTE sys-
tems. An extended version of these components are
the Distance Components that can produce normal-
ized and unnormalized distance/similarity values
in addition to the score vector.
Annotation Components can be used to add dif-
ferent annotations to the Text/Hypothesis pairs. An
example of such a type of component is one that
produces word or phrase alignments between the
Text and the Hypothesis.
Lexical Knowledge Components describe se-
mantic relationships between words. In the
EOP, this knowledge is represented as directed
rules made up of two word?POS pairs, where
the LHS (left-hand side) entails the RHS (right-
hand side), e.g., (shooting star,Noun) =?
(meteorite,Noun). Lexical Knowledge Compo-
nents provide an interface that allows for (a) listing
all RHS for a given LHS; (b) listing all LHS for
a given RHS; and (c) checking for an entailment
relation for a given LHS?RHS pair. The interface
also wraps all major lexical knowledge sources cur-
rently used in RTE research, including manually
constructed ontologies like WordNet, and encyclo-
pedic resources like Wikipedia.
Syntactic Knowledge Components capture en-
tailment relationships between syntactic and
44
lexical-syntactic expressions. We represent such
relationships by entailment rules that link (option-
ally lexicalized) dependency tree fragments that
can contain variables as nodes. For example, the
rule fall of X =? X falls, or X sells Y to Z =?
Z buys Y from X express general paraphrasing pat-
terns at the predicate-argument level that cannot be
captured by purely lexical rules. Formally, each
syntactic rule consists of two dependency tree frag-
ments plus a mapping from the variables of the
LHS tree to the variables of the RHS tree.
4
2.3 Configuration Files
The EC components can be combined into actual
inference engines through configuration files which
contain information to build a complete inference
engine. A configuration file completely describes
an experiment. For example, it specifies the re-
sources that the selected EDA has to use and the
data set to be analysed. The LAP needed for data
set preprocessing is another parameter that can be
configured too. The platform ships with a set of
predefined configuration files accompanied by sup-
porting documentation.
3 Entailment Algorithms and Resources
This section provides a description of the Entail-
ment Algorithms and Knowledge Resources that
are distributed with the EOP.
3.1 Entailment Algorithms
The current version of the EOP platform ships with
three EDAs corresponding to three different ap-
proaches to RTE: an EDA based on transformations
between T and H, an EDA based on edit distance
algorithms, and a classification based EDA using
features extracted from T and H.
Transformation-based EDA applies a sequence
of transformations on T with the goal of making
it identical to H. If each transformation preserves
(fully or partially) the meaning of the original text,
then it can be concluded that the modified text
(which is actually the Hypothesis) can be inferred
from the original one. Consider the following sim-
ple example where the text is ?The boy was located
by the police? and the Hypothesis is ?The child
was found by the police?. Two transformations for
?boy? ? ?child? and ?located? ? ?found? do the
job.
4
Variables of the LHS may also map to null, when material
of the LHS must be present but is deleted in the inference step.
In the EOP we include a transformation based
inference system that adopts the knowledge based
transformations of Bar-Haim et al. (2007), while in-
corporating a probabilistic model to estimate trans-
formation confidences. In addition, it includes a
search algorithm which finds an optimal sequence
of transformations for any given T/H pair (Stern et
al., 2012).
Edit distance EDA involves using algorithms
casting textual entailment as the problem of map-
ping the whole content of T into the content of H.
Mappings are performed as sequences of editing
operations (i.e., insertion, deletion and substitu-
tion) on text portions needed to transform T into H,
where each edit operation has a cost associated with
it. The underlying intuition is that the probability
of an entailment relation between T and H is related
to the distance between them; see Kouylekov and
Magnini (2005) for a comprehensive experimental
study.
Classification based EDA uses a Maximum En-
tropy classifier to combine the outcomes of sev-
eral scoring functions and to learn a classification
model for recognizing entailment. The scoring
functions extract a number of features at various
linguistic levels (bag-of-words, syntactic dependen-
cies, semantic dependencies, named entities). The
approach was thoroughly described in Wang and
Neumann (2007).
3.2 Knowledge Resources
As described in Section 2.2, knowledge resources
are crucial to recognize cases where T and H use
different textual expressions (words, phrases) while
preserving entailment. The EOP platform includes
a wide range of knowledge resources, including lex-
ical and syntactic resources, where some of them
are grabbed from manual resources, like dictionar-
ies, while others are learned automatically. Many
EOP resources are inherited from pre-existing RTE
systems migrated into the EOP platform, but now
use the same interfaces, which makes them acces-
sible in a uniform fashion.
There are about two dozen lexical (e.g. word-
nets) and syntactic resources for three languages
(i.e. English, Italian and German). However,
since there is still a clear predominance of En-
glish resources, the platform includes lexical and
syntactic knowledge mining tools to bootstrap re-
sources from corpora, both for other languages and
45
EDA Accuracy / F1
Transformation-based English RTE-3 67.13%
Transformation-based English RTE-6 49.55%
Edit-Distance English RTE-3 64.38%
Edit-Distance German RTE-3 59.88%
Edit-Distance Italian RTE-3 63.50%
Classification-based English RTE-3 65.25%
Classification-based German RTE-3 63.75%
Median of RTE-3 (English) submissions 61.75%
Median of RTE-6 (English) submissions 33.72%
Table 1: EDAs results
for specific domains. Particularly, the EOP plat-
form includes a language independent tool to build
Wikipedia resources (Shnarch et al., 2009), as well
as a language-independent framework for building
distributional similarity resources like DIRT (Lin
and Pantel, 2002) and Lin similarity(Lin, 1998).
3.3 EOP Evaluation
Results for the three EDAs included in the EOP
platform are reported in Table 1. Each line rep-
resents an EDA, the language and the dataset
on which the EDA was evaluated. For brevity,
we omit here the knowledge resources used for
each EDA, even though knowledge configuration
clearly affects performance. The evaluations were
performed on RTE-3 dataset (Giampiccolo et al.,
2007), where the goal is to maximize accuracy. We
(manually) translated it to German and Italian for
evaluations: in both cases the results fix a refer-
ence for the two languages. The two new datasets
for German and English are available both as part
of the EOP distribution and independently
5
. The
transformation-based EDA was also evaluated on
RTE-6 dataset (Bentivogli et al., 2010), in which
the goal is to maximize the F1 measure.
The results of the included EDAs are higher than
median values of participated systems in RTE-3,
and they are competing with state-of-the-arts in
RTE-6 results. To the best of our knowledge, the
results of the EDAs as provided in the platform are
the highest among those available as open source
systems for the community.
4 Use Cases
We see four primary use cases for the EOP. Their
requirements were reflected in our design choices.
Use Case 1: Applied Textual Entailment. This
category covers users who are not interested in the
5
http://www.excitement-project.eu/
index.php/results
details of RTE but who are interested in an NLP
task in which textual entailment can take over part
of or all of the semantic processing, such as Ques-
tion Answering or Intelligent Tutoring. Such users
require a system that is as easy to deploy as possi-
ble, which motivates our offer of the EOP platform
as a library. They also require a system that pro-
vides good quality at a reasonable efficiency as
well as guidance as to the best choice of parame-
ters. The latter point is realized through our results
archive in the official EOP Wiki on the EOP site.
Use Case 2: Textual Entailment Development.
This category covers researchers who are interested
in Recognizing Textual Entailment itself, for exam-
ple with the goal of developing novel algorithms
for detecting entailment. In contrast to the first
category, this group need to look ?under the hood?
of the EOP platform and access the source code of
the EOP. For this reason, we have spent substantial
effort to provide the code in a well-structured and
well-documented form.
A subclass of this group is formed by researchers
who want to set up a RTE infrastructure for lan-
guages in which it does not yet exist (that is, al-
most all languages). The requirements of this class
of users comprises clearly specified procedures to
replace the Linguistic Analysis Pipeline, which are
covered in our documentation, and simple methods
to acquire knowledge resources for these languages
(assuming that the EDAs themselves are largely
language-independent). These are provided by the
language-independent knowledge acquisition tools
which we offer alongside the platform (cf. Section
3.2).
Use Case 3: Lexical Semantics Evaluation. A
third category consists of researchers whose pri-
mary interest is in (lexical) semantics.
As long as their scientific results can be phrased
in terms of semantic similarities or inference rules,
the EOP platform can be used as a simple and stan-
dardized workbench for these results that indicates
the impact that the semantic knowledge under con-
sideration has on deciding textual entailment. The
main requirement for this user group is the simple
integration of new knowledge resources into the
EOP platform. This is catered for through the defi-
nition of the generic knowledge component inter-
faces (cf. Section 2.2) and detailed documentation
on how to implement these interfaces.
46
Use Case 4: Educational Use. The fourth and
final use case is as an educational tool to support
academic courses and projects on Recognizing Tex-
tual Entailment and inference more generally. This
use case calls, in common with the others, for easy
usability and flexibility. Specifically for this use
case, we have also developed a series of tutorials
aimed at acquainting new users with the EOP plat-
form through a series of increasingly complexity
exercises that cover all areas of the EOP. We are
also posting proposals for projects to extend the
EOP on the EOP Wiki.
5 EOP Distribution
The EOP infrastructure follows state-of-the-art soft-
ware engineering standards to support both users
and developers with a flexible, scalable and easy to
use software environment. In addition to communi-
cation channels, like the mailing list and the issue
tracking system, the EOP infrastructure comprises
the following set of facilities.
Version Control System: We use GitHub,
6
a
web-based hosting service for code and documen-
tation storage, development, and issue tracking.
Web Site: The GitHub Automatic Page Genera-
tor was used to build the EOP web site and Wiki,
containing a general introduction to the software
platform, the terms of its license, mailing lists to
contact the EOP members and links to the code
releases.
Documentation: Both user and developer docu-
mentation is available from Wiki pages; the pages
are written with the GitHub Wiki Editor and hosted
on the GitHub repository. The documentation in-
cludes a Quick Start guide to start using the EOP
platform right away, and a detailed step by step
tutorial.
Results Archive: As a new feature for commu-
nity building, EOP users can, and are encouraged
to, share their results: the platform configuration
files used to produce results as well as contact infor-
mation can be saved and archived into a dedicated
page on the EOP GitHub repository. That allows
other EOP users to replicate experiments under
the same condition and/or avoid doing experiments
that have already been done.
6
https://github.com/
Build Automation Tool: The EOP has been de-
veloped as a Maven
7
multi-modules project, with
all modules sharing the same Maven standard struc-
ture, making it easier to find files in the project once
one is used to Maven.
Maven Artifacts Repository: Using a Maven
repository has a twofold goal: (i) to serve as an
internal private repository of all software libraries
used within the project (libraries are binary files
and should not be stored under version control sys-
tems, which are intended to be used with text files);
(ii) to make the produced EOP Maven artifacts
available (i.e., for users who want to use the EOP
as a library in their own code). We use Artifactory
8
repository manager to store produced artifacts.
Continuous Integration: The EOP uses Jenk-
ins
9
for Continuous Integration, a software develop-
ment practice where developers of a team integrate
their work frequently (e.g., daily).
Code Quality Tool: Ensuring the quality of the
produced software is one of the most important
aspects of software engineering. The EOP uses
tools like PMD
10
that can automatically be run
during development to help the developers check
the quality of their software.
5.1 Project Repository
The EOP Java source code is hosted on the EOP
Github repository and managed using Git. The
repository consists of three main branches: the
release branch contains the code that is supposed to
be in a production-ready state, whereas the master
branch contains the code to be incorporated into the
next release. When the source code in the master
branch reaches a stable point and is ready to be
released, all of the changes are merged back into
release. Finally, the gh-pages branch contains the
web site pages.
5.2 Licensing
The software of the platform is released under the
terms of General Public License (GPL) version
3.
11
The platform contains both components and
resources designed by the EOP developers, as well
as others that are well known and freely available
7
http://maven.apache.org/
8
http://www.jfrog.com/
9
http://jenkins-ci.org/
10
http://pmd.sourceforge.net
11
http://www.gnu.org/licenses/gpl.html
47
in the NLP research community. Additional com-
ponents and resources whose license is not compat-
ible with the EOP license have to be downloaded
and installed separately by the user.
6 Conclusion
This paper has presented the main characteristics
of Excitement Open Platform platform, a rich envi-
ronment for experimenting and evaluating textual
entailment systems. On the software side, the EOP
is a complex endeavor to integrate tools and re-
sources in Computational Linguistics, including
pipelines for three languages, three pre-existing
entailment engines, and about two dozens of lex-
ical and syntactic resources. The EOP assumes a
clear and modular separation between linguistic
annotations, entailment algorithms and knowledge
resources which are used by the algorithms. A
relevant benefit of the architectural design is that
a high level of interoperability is reached, provid-
ing a stimulating environment for new research in
textual inferences.
The EOP platform has been already tested in sev-
eral pilot research projects and educational courses,
and it is currently distributed as open source soft-
ware under the GPL-3 license. To the best of our
knowledge, the entailment systems and their con-
figurations provided in the platform are the best
systems available as open source for the commu-
nity. As for the future, we are planning several
initiatives for the promotion of the platform in the
research community, as well as its active experi-
mentation in real application scenarios.
Acknowledgments
This work was partially supported by the EC-
funded project EXCITEMENT (FP7ICT-287923).
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI, pages 871?
876, Vancouver, BC.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The Sixth
PASCAL Recognizing Textual Entailment Chal-
lenge. In Proceedings of TAC, Gaithersburg, MD.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Journal of Natural
Language Engineering, 15(4):i?xvii.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The Third PASCAL Recog-
nising Textual Entailment Challenge. In Proceed-
ings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing, Prague, Czech Repub-
lic.
Iryna Gurevych, Max M?uhlh?auser, Christof M?uller,
J?urgen Steimle, Markus Weimer, and Torsten Zesch.
2007. Darmstadt knowledge processing repository
based on UIMA. In Proceedings of the First Work-
shop on Unstructured Information Management Ar-
chitecture (UIMA@GSCL 2007), T?ubingen, Ger-
many.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the ACL demo session, pages 177?
180, Prague, Czech Republic.
Milen Kouylekov and Bernardo Magnini. 2005. Rec-
ognizing textual entailment with tree edit distance al-
gorithms. In Proceedings of the First PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
pages 17?20, Southampton, UK.
Dekang Lin and Patrick Pantel. 2002. Discovery of
Inference Rules for Question Answering. Journal of
Natural Language Engineering, 7(4):343?360.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of ACL/COLING,
pages 768?774, Montr?eal, Canada.
Tae-Gil Noh and Sebastian Pad?o. 2013. Using
UIMA to structure an open platform for textual en-
tailment. In Proceedings of the 3rd Workshop on
Unstructured Information Management Architecture
(UIMA@GSCL 2013).
Sebastian Pad?o, Tae-Gil Noh, Asher Stern, Rui Wang,
and Roberto Zanoli. 2014. Design and realiza-
tion of a modular architecture for textual entailment.
Journal of Natural Language Engineering. doi:
10.1017/S1351324913000351.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Ex-
tracting lexical reference rules from Wikipedia. In
Proceedings of ACL-IJCNLP, pages 450?458, Sin-
gapore.
Asher Stern, Roni Stern, Ido Dagan, and Ariel Felner.
2012. Efficient search for transformation-based in-
ference. In Proceedings of ACL, pages 283?291,
Jeju Island, South Korea.
Rui Wang and G?unter Neumann. 2007. Recogniz-
ing textual entailment using a subsequence kernel
method. In Proceedings of AAAI, pages 937?945,
Vancouver, BC.
48
