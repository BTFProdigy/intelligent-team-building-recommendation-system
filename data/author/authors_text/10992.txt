Multilingual Extension of a Temporal Expression Normalizer using
Annotated Corpora
E. Saquete P. Mart??nez-Barco R. Mun?oz
gPLSI
DLSI. UA
Alicante, Spain
fstela,patricio,rafaelg@dlsi.ua.es
M. Negri M. Speranza
ITC-irst
Povo (TN), Italy
fnegri,mansperag@itc.it
R. Sprugnoli
CELCT
Trento, Italy
sprugnoli@celct.it
Abstract
This paper presents the automatic exten-
sion to other languages of TERSEO, a
knowledge-based system for the recogni-
tion and normalization of temporal ex-
pressions originally developed for Span-
ish1. TERSEO was first extended to En-
glish through the automatic translation of
the temporal expressions. Then, an im-
proved porting process was applied to Ital-
ian, where the automatic translation of
the temporal expressions from English and
from Spanish was combined with the ex-
traction of new expressions from an Ital-
ian annotated corpus. Experimental re-
sults demonstrate how, while still adher-
ing to the rule-based paradigm, the devel-
opment of automatic rule translation pro-
cedures allowed us to minimize the ef-
fort required for porting to new languages.
Relying on such procedures, and without
any manual effort or previous knowledge
of the target language, TERSEO recog-
nizes and normalizes temporal expressions
in Italian with good results (72% precision
and 83% recall for recognition).
1 Introduction
Recently, the Natural Language Processing com-
munity has become more and more interested
in developing language independent systems,
in the effort of breaking the language barrier
hampering their application in real use scenar-
ios. Such a strong interest in multilingual-
ity is demonstrated by the growing number of
1This research was partially funded by the Spanish Gov-
ernment (contract TIC2003-07158-C04-01)
international conferences and initiatives plac-
ing systems? multilingual/cross-language capabil-
ities among the hottest research topics, such as
the European Cross-Language Evaluation Forum2
(CLEF), a successful evaluation campaign which
aims at fostering research in different areas of
multilingual information retrieval. At the same
time, in the temporal expressions recognition and
normalization field, systems featuring multilin-
gual capabilities have been proposed. Among
others, (Moia, 2001; Wilson et al, 2001; Negri
and Marseglia, 2004) emphasized the potentiali-
ties of such applications for different information
retrieval related tasks.
As many other NLP areas, research in auto-
mated temporal reasoning has recently seen the
emergence of machine learning approaches trying
to overcome the difficulties of extending a lan-
guage model to other languages (Carpenter, 2004;
Ittycheriah et al, 2003). In this direction, the out-
comes of the first Time Expression Recognition
and Normalization Workshop (TERN 20043) pro-
vide a clear indication of the state of the field. In
spite of the good results obtained in the recog-
nition task, normalization by means of machine
learning techniques still shows relatively poor re-
sults with respect to rule-based approaches, and
still remains an unresolved problem.
The difficulty of porting systems to new lan-
guages (or domains) affects both rule-based and
machine learning approaches. With rule-based ap-
proaches (Schilder and Habel, 2001; Filatova and
Hovy, 2001), the main problems are related to
the fact that the porting process requires rewriting
from scratch, or adapting to each new language,
large numbers of rules, which is costly and time-
2http://www.clef-campaign.org/
3http://timex2.mitre.org/tern.html
1
consuming work. Machine learning approaches
(Setzer and Gaizauskas, 2002; Katz and Arosio,
2001), on the other hand, can be extended with
little human intervention through the use of lan-
guage corpora. However, the large annotated cor-
pora that are necessary to obtain high performance
are not always available. In this paper we describe
a new procedure to build temporal models for new
languages, starting from previously defined ones.
While still adhering to the rule-based paradigm, its
main contribution is the proposal of a simple, but
effective, methodology to automate the porting of
a system from one language to another. In this pro-
cedure, we take advantage of the architecture of an
existing system developed for Spanish (TERSEO,
see (Saquete et al, 2005)), where the recognition
model is language-dependent but the normalizing
procedure is completely independent. In this way,
the approach is capable of automatically learning
the recognition model by adjusting the set of nor-
malization rules.
The paper is structured as follows: Section 2
provides a short overview of TERSEO; Section 3
describes the automatic extension of the system to
Italian; Section 4 presents the results of our evalu-
ation experiments, comparing the performance of
Ita-TERSEO (i.e. our extended system) with the
performance of a state of the art system for Italian.
2 The TERSEO system architecture
TERSEO has been developed in order to automat-
ically recognize temporal expressions (TEs) ap-
pearing in a Spanish written text, and normalize
them according to the temporal model proposed
in (Saquete, 2005), which is compatible with the
ACE annotation standards for temporal expres-
sions (Ferro et al, 2005). As shown in Figure 1,
the first step (recognition) includes pre-processing
of the input texts, which are tagged with lexical
and morphological information that will be used
as input to the temporal parser. The temporal
parser is implemented using an ascending tech-
nique (chart parser) and is based on a temporal
grammar. Once the parser has recognized the TEs
in an input text, these are passed to the normaliza-
tion unit, which updates the value of the reference
according to the date they refer to, and generates
the XML tags for each expression.
As TEs can be categorized as explicit and im-
plicit, the grammar used by the parser is tuned for
discriminating between the two groups. On the
TEXT
POS 
TAGGER
RECOGNITION: 
PARSER
Lexical and
morphological
information
Temporal 
expression
recognition
DATE
ESTIMATION
Dictionary
Temporal
Expression
Grammar
TEMPORAL
EXPRESSION
NORMALIZATION
EVENT 
ORDERING
ORDERED
TEXT
Documental 
DataBase
Figure 1: Graphic representation of the TERSEO
architecture.
one hand, explicit temporal expressions directly
provide and fully describe a date which does not
require any further reasoning process to be inter-
preted (e.g. ?1st May 2005?, ?05/01/2005?). On
the other hand, implicit (or anaphoric) time ex-
pressions (e.g. ?yesterday?, ?three years later?)
require some degree of reasoning (as in the case
of anaphora resolution). In order to translate such
expressions into explicit dates, such reasoning ca-
pabilities consider the information provided by the
lexical context in which they occur (see (Saquete,
2005) for a thorough description of the reasoning
techniques used by TERSEO).
2.1 Recognition using a temporal expression
parser
The parser uses a grammar based on two differ-
ent sets of rules. The first set of rules is in charge
of date and time recognition (i.e. explicit dates,
such as ?05/01/2005?). For this type of TEs, the
grammar adopted by TERSEO recognizes a large
number of date and time formats (see Table 1 for
some examples).
The second set of rules is in charge of the recog-
nition of the temporal reference for implicit TEs,
2
fecha! dd+?/?+mm+?/?+(yy)yy (12/06/1975)
(06/12/1975)
fecha! dd+?-?+mes+?-?+(yy)yy (12-junio-1975)
(12-Jun.-1975)
fecha! dd+?de?+mm+?de?+(yy)yy (12 de junio de 1975)
Table 1: Sample of rules for Explicit Dates Recognition.
reference! ?ayer? (yesterday)
Implicit dates reference! ?man?ana? (tomorrow)
referring to Document Date reference! ?anteayer? (the day before yesterdary)
Concrete reference! ?el pro?ximo d??a? (the next day)
Implicit Dates reference! ?un mes despue?s? (a month later)
Previous Date Period reference! num+?an?os despue?s?(num years later)
Imp. Dates Prev.Date Concrete reference! ?un d??a antes? (a day before)
Implicit Dates reference! ?d??as despue?s? (some days later)
Previous Date Fuzzy reference! ?d??as antes? (some days before)
Table 2: Sample of rules for Implicit Dates recognition.
i.e. TEs that need to be related to an explicit TE
to be interpreted. These can be divided into time
adverbs (e.g. ?yesterday?, ?tomorrow?), and nom-
inal phrases that are referring to temporal relation-
ships (e.g. ?three years later?, ?the day before?).
Table 2 shows some of the rules used for the de-
tection of these kinds of references.
2.2 Normalization
When the system finds an explicit temporal ex-
pression, the normalization process is direct as no
resolution of the expression is necessary. For im-
plicit expressions, an inference engine that inter-
prets every reference previously found in the input
text is used. In some cases references are solved
using the newspaper?s date (FechaP). Other TEs
have to be interpreted by referring to a date named
before in the text that is being analyzed (FechaA).
In these cases, a temporal model that allows the
system to determine the reference date over which
the dictionary operations are going to be done, has
been defined. This model is based on the follow-
ing two rules:
1. The newspaper?s date, when available, is
used as a base temporal referent by default;
otherwise, the current date is used as anchor.
2. In case a non-anaphoric TE is found, it is
stored as FechaA. This value is updated ev-
ery time a non-anaphoric TE appears in the
text.
Table 3 shows some of the entries of the dictio-
nary used in the inference engine.
3 Extending TERSEO: from Spanish
and English to Italian
As stated before, the main purpose of this paper is
to describe a new procedure to automatically build
temporal models for new languages, starting from
previously defined models. In our case, an English
model has been automatically obtained from the
Spanish one through the automatic translation of
the Spanish temporal expressions to English. The
resulting system for the recognition and normal-
ization of English TEs obtains good results both
in terms of precision (P) and recall (R) (Saquete et
al., 2004). The comparison of the results between
the Spanish and the English system is shown in
Table 4.
SPANISH ENGLISH
DOCS 50 100
POS 199 634
ACT 156 511
CORRECT 138 393
INCORRECT 18 118
MISSED 43 123
P 88% 77%
R 69% 62%
F 77% 69%
Table 4: Comparison between Spanish TERSEO
and English TERSEO.
This section presents the procedure we followed
to extend our system to Italian, starting from the
Spanish and English models already available, and
a manually annotated corpus. In this case, both
models have been considered as they can be com-
plemented reciprocally. The Spanish model was
3
REFERENCE DICTIONARY ENTRY
?ayer? Day(FechaP)-1/Month(FechaP)/Year(FechaP)
(yesterday)
?man?ana? Day(FechaP)+1/Month(FechaP)/Year(FechaP)
(tomorrow)
?anteayer? Day(FechaP)-2/Month(FechaP)/Year(FechaP)
(the day before yesterday)
?el pro?ximo d??a? Day(FechaP)+1/Month(FechaP)/Year(FechaP)
(the next day)
?un mes despue?s? [DayI/Month(FechaA)+1/Year(FechaA)--
(a month later) DayF/Month(FechaA)+1/Year(FechaA)]
num+?an?os despue?s? [01/01/Year(FechaA)+num --
(num years later) 31/12/Year(FechaA)+num]
?un d??a antes? Day(FechaA)-1/Month(FechaA)/Year(FechaA)
(a day before)
?d??as despue?s? >>>>FechaA
(some days later)
?d??as antes? <<<<FechaA
(some days before)
Table 3: Normalization rules
manually obtained and evaluated showing high
scores for precision (88%), so better results could
be expected when it is used. However, in spite of
the fact that the English model has shown lower
results on precision (77%), the on-line transla-
tors between Italian and English have better re-
sults than Spanish to Italian translators. As a re-
sult, both models are considered in the following
steps for the multilingual extension:
 Firstly, a set of Italian temporal expressions
is extracted from an Italian annotated corpus
and stored in a database. The selected cor-
pus is the training part of I-CAB, the Ital-
ian Content Annotation Bank (Lavelli et al,
2005). More detailed information about I-
CAB is provided in Section 4.
 Secondly, the resulting set of Italian TEs
must be related to the appropriate normaliza-
tion rule. In order to do that, a double transla-
tion procedure has been developed. We first
translate all the expressions into English and
Spanish simultaneously; then, the normaliza-
tion rules related to the translated expressions
are obtained. If both the Spanish and En-
glish expressions are found in their respec-
tive models in agreement with the same nor-
malization rule, then this rule is also assigned
to the Italian expression. Also, when only
one of the translated expressions is found in
the existing models, the normalization rule
is assigned. In case of discrepancies, i.e. if
both expressions are found, but not coincid-
ing in the same normalization rule, then one
of the languages must be prioritized. As the
Spanish model was manually obtained and
has shown a higher precision, Spanish rules
are preferred. In other cases, the expression
is reserved for a manual assignment.
 Finally, the set is automatically augmented
using the Spanish and English sets of tem-
poral expressions. These expressions were
also translated into Italian by on-line ma-
chine translation systems (Spanish-Italian4 ,
English-Italian5). In this case, a filtering
module is used to guarantee that all the ex-
pressions were correctly translated. This
module searches the web with Google6 for
the translated expression. If the expression
is not frequently found, then the translation
is abandoned. After that, the new Italian ex-
pression is included in the model, and related
to the same normalization rule assigned to the
Spanish or English temporal expression.
The entire translation process has been com-
pleted with an automatic generalization process,
oriented to obtain generalized rules from the con-
crete cases that have been collected from the cor-
4http://www.tranexp.com:2000/Translate/result.shtml
5http://world.altavista.com/
6http://www.google.com/
4
pus. This generalization process has a double ef-
fect. On the one hand, it reduces the number of
recognition rules. On the other hand, it allows the
system to identify new expressions that were not
previously learned. For instance, the expression
?Dieci mesi dopo? (i.e. ?Ten months later?) could
be recognized if the expression ?Nove mesi dopo?
(i.e. Nine months later) was learned.
The multilingual extension procedure (Figure 3)
is carried out in three phases:
Spanish
Temporal 
Recognition
Model
Spanish-Italian
TRANSLATOR
TEs FILTER
KEYWORDS Unit
NEW TEs
FINDER
RULE
ASSIGNMENTS
Google
WordNet
Italian TEs
Italian TEs
Temporal  keywords
New Italian TEs
New Normalizer rule
Italian
Temporal 
Normalizer
Model
Online 
DictionariesITALIAN TEs
GRAMATICS 
Generator
English
Temporal 
Recognition
Model
Italian
I-CAB   
Corpus
English-Italian
TRANSLATOR
Italian-Spanish
TRANSLATOR
Italian-English
TRANSLATOR
Spanish
Temporal 
Normalizer
Model
English
Temporal 
Normalizer
Model
Italian TEs
Italian generalized TEs
Phase
1
Phase 
2 
Phase 
3
Figure 2: Multilingual extension procedure.
 Phase 1: TE Collection. During this phase,
the Italian temporal expressions are col-
lected from I-CAB (Italian Content Annota-
tion Bank), and the automatically translated
Italian TEs are derived from the set of Span-
ish and English TEs. In this case, the TEs
are filtered removing those not being found
by Google.
 Phase 2: TE Generalization. In this phase,
the TEs Gramatics Generator uses the mor-
phological and syntactical information from
the collected TEs to generate the grammat-
ical rules that generalize the recognition of
the TEs. Moreover, the keyword unit is able
to extract the temporal keywords that will be
used to build new TEs. These keywords are
augmented with their synonyms in WordNet
(Vossen, 2000) to generate new TEs.
 Phase 3: TE Normalizing Rule Assignment.
In the last phase, the translators are used to
relate the recognizing rule to the appropriate
normalization rule. For this purpose, the sys-
tem takes advantage of the previously defined
Spanish and English temporal models.
4 Evaluation
The automatic extension of the system to Italian
(Ita-TERSEO) has been evaluated using I-CAB,
which has been divided in two parts: training and
test. The training part has been used, first of all,
in order to automatically extend the system. Af-
ter this extension, the system was evaluated both
against the training and the test corpora. The pur-
pose of this double evaluation experiment was to
compare the recall obtained over the training cor-
pus with the value obtained over the test corpus.
An additional evaluation experiment has also
been carried out in order to compare the perfor-
mance of the automatically developed system with
a state of the art system specifically developed for
Italian and English, i.e. the Chronos system de-
scribed in (Negri and Marseglia, 2004).
In the following sections, more details about I-
CAB and the evaluation process are presented, to-
gether with the evaluation results.
4.1 The I-CAB Corpus
The evaluation has been performed on the tem-
poral annotations of I-CAB (I-CAB-temp) cre-
ated as part of the three-year project ONTOTEXT7
funded by the Provincia Autonoma di Trento.
I-CAB consists of 525 news documents
taken from the local newspaper L?Adige
(http://www.adige.it). The selected news sto-
ries belong to four different days (September, 7th
and 8th 2004 and October, 7th and 8th 2004) and
are grouped into five categories: News Stories,
Cultural News, Economic News, Sports News
and Local News. The corpus consists of around
182,500 words (on average 347 words per file).
The total number of annotated temporal expres-
sions is 4,553; the average length of a temporal
expression is 1.9 words.
The annotation of I-CAB has been carried out
adopting the standards developed within the ACE
program (Automatic Content Extraction8) for the
Time Expressions Recognition and Normalization
7http://tcc.itc.it/projects/ontotext
8http://www.nist.gov/speech/tests/ace
5
tasks, which allows for a semantically rich and
normalized annotation of different types of tempo-
ral expressions (for further details on the TIMEX2
annotation standard for English see (Ferro et al,
2005)).
The ACE guidelines have been adapted to
the specific morpho-syntactic features of Italian,
which has a far richer morphology than English.
In particular, some changes concerning the exten-
sion of the temporal expressions have been in-
troduced. According to the English guidelines,
in fact, definite and indefinite articles are consid-
ered as part of the textual realization of an entity,
while prepositions are not. As the annotation is
word-based, this does not account for Italian artic-
ulated prepositions, where a definite article and a
preposition are merged. Within I-CAB, this type
of preposition has been included as possible con-
stituents of an entity, so as to consistently include
all the articles.
An assessment of the inter-annotator agreement
based on the Dice coefficient has shown that the
task is a well-defined one, as the agreement is
95.5% for the recognition of temporal expressions.
4.2 Evaluation process
The evaluation of the automatic extension of
TERSEO to Italian has been performed in three
steps. First of all, the system has been evaluated
both against the training and the test corpora with
two main purposes:
 Determining if the recall obtained in the eval-
uation of the training part of the corpus is a
bit higher than the one obtained in the eval-
uation of the test part of I-CAB, due to the
fact that in the TE collection phase of the ex-
tension, temporal expressions were extracted
from this part of the corpus.
 Determining the performance of the automat-
ically extended system without any manual
revision of both the Italian translations and
the resolution rules automatically related to
the expressions.
Secondly, we were also interested in verifying
if the performance of the system in terms of pre-
cision could be improved through a manual revi-
sion of the automatically translated temporal ex-
pressions.
Finally, a comparison with a state of the art sys-
tem for Italian has been carried out in order to es-
timate the real potentialities of the proposed ap-
proach. All the evaluation results are compared
and presented in the following sections using the
same metrics adopted at the TERN2004 confer-
ence.
4.2.1 Evaluation of Ita-TERSEO
In the automatic extension of the system, a to-
tal of 1,183 Italian temporal expressions have been
stored in the database. As shown in Table 5, these
expressions have been obtained from the different
resources available:
 ENG ITA: This group of expressions has
been obtained from the automatic translation
into Italian of the English Temporal Expres-
sions stored in the knowledge DB.
 ESP ITA: This group of expressions has been
obtained from the automatic translation into
Italian of the Spanish Temporal Expressions
stored in the knowledge DB.
 CORPUS: This group of expressions has
been extracted directly from the training part
of the I-CAB corpus.
Source N %
ENG ITA 593 50.1
ESP ITA 358 30.3
CORPUS 232 19.6
TOTAL TEs 1183 100.0
Table 5: Italian TEs in the Knowledge DB.
Both the training part and the test part of I-CAB
have been used for evaluation. The results of pre-
cision (P), recall (R) and F-Measure (F) are pre-
sented in Table 6, which provides details about the
system performance over the general recognition
task (timex2), and the different normalization at-
tributes used by the TIMEX2 annotation standard.
As expected, recall performance over the train-
ing corpus is slightly higher. However, although
the temporal expressions have been extracted from
such corpus, in the automatic process of obtain-
ing the normalization rules for these expressions,
some errors could have been introduced.
Comparing these results with those obtained by
the automatic extension of TERSEO to English
and taking into account the recognition task (see
Table 4), precision (P) is slightly better for En-
glish (77% Vs. 72%) whereas recall (R) is better
in the Italian extension (62% Vs. 83%). This is
6
Ita-TERSEO: TRAINING Ita-TERSEO: TEST Chronos: TEST
Tag P R F P R F P R F
timex2 0.694 0.848 0.763 0.726 0.834 0.776 0.925 0.908 0.917
anchor dir 0.495 0.562 0.526 0.578 0.475 0.521 0.733 0.636 0.681
anchor val 0.464 0.527 0.493 0.516 0.424 0.465 0.495 0.462 0.478
set 0.308 0.903 0.459 0.182 1.000 0.308 0.616 0.5 0.552
text 0.265 0.324 0.292 0.258 0.296 0.276 0.859 0.843 0.851
val 0.581 0.564 0.573 0.564 0.545 0.555 0.636 0.673 0.654
Table 6: Results obtained over I-CAB by Ita-TERSEO and Chronos.
due to the fact that in the Italian extension, more
temporal expressions have been covered with re-
spect to the English extension. In this case, in
fact, Ita-TERSEO is not only using the temporal
expressions translated from the English or Spanish
knowledge database, but also the temporal expres-
sions extracted from the training part of I-CAB.
4.2.2 Manual revision of the acquired TEs
A manual revision of the Italian TEs stored in
the Knowledge DB has been done in two steps.
First of all, the incorrectly translated expressions
(from Spanish and English to Italian) were re-
moved from the database. A total of 334 expres-
sions were detected as wrong translated expres-
sions. After this, another revision was performed.
In this case, some expressions were modified be-
cause the expressions have some minor errors in
the translation. 213 expressions were modified in
this second revision cycle. Moreover, since pattern
constituents in Italian might have different ortho-
graphical features (e.g. masculine/feminine, ini-
tial vowel/consonant, etc.), new patterns had to be
introduced to capture such variants. For exam-
ple, as months? names in Italian could start with
a vowel, the temporal expression pattern ?nell?-
MONTH? has been inserted in the Knowledge
DB. After these changes, the total amount of ex-
pressions stored in the DB are shown in Table 7.
Source N %
ING ITA 416 47.9
ESP ITA 201 23.1
CORPUS 232 26.7
REV MAN 20 2.3
TOTAL TEs 869 100.0
Table 7: Italian TEs in the Knowledge DB after
manual revision.
In order to evaluate the system after this manual
revision, the training and the test part of I-CAB
have been used. However, the results of preci-
sion (PREC), recall (REC) and F-Measure were
exactly the same as presented in Table 6. That
is not really surprising. The existence of wrong
expressions in the knowledge database does not
affect the final results of the system, as they will
never be used for recognition or resolution. This
is because these expressions will not appear in real
documents, and are redundant as the correct ex-
pression is also stored in the Knowledge DB.
4.2.3 Comparing Italian TERSEO with a
language-specific system
Finally, in order to compare Ita-TERSEO with
a state of the art system specifically designed for
Italian, we chose Chronos (Negri and Marseglia,
2004), a multilingual system for the recognition
and normalization of TEs in Italian and English.
Like all the other state of the art systems address-
ing the recognition/normalization task, Chronos
is a rule-based system. From a design point of
view, it shares with TERSEO a rather similar ar-
chitecture which relies on different sets of rules.
These are regular expressions that check for spe-
cific features of the input text, such as the pres-
ence of particular word senses, lemmas, parts
of speech, symbols, or strings satisfying specific
predicates. Each set of rules is in charge of deal-
ing with different aspects of the problem. In
particular, a set of around 350 rules is designed
for TE recognition and is capable of recogniz-
ing with high Precision/Recall rates both explicit
and implicit TEs. Other sets of regular expres-
sions, for a total of around 700 rules, are used
in the normalization phase, and are in charge of
handling a specific TIMEX2 attribute (i.e. VAL,
SET, ANCHOR VAL, and ANCHOR DIR). The
results obtained by the Italian version of Chronos
over the test part of I-CAB are shown in the last
three columns of Table 6.
As expected, the distance between the results
obtained by the two systems is considerable. How-
ever, the following considerations should be taken
into account. First, there is a great difference, both
7
in terms of the required time and effort, in the de-
velopment of the two systems. While the imple-
mentation of the manual one took several months,
the porting procedure of TERSEO to Italian is a
very fast process that can be accomplished in less
than an hour. Second, even if an annotated corpus
for a new language is not available, the automatic
porting procedure we present still remains feasi-
ble. In fact, most of the TEs for a new language
that are stored in the Knowledge DB are the result
of the translation of the Spanish/English TEs into
such a target language. In our case, as shown in
Table 5, more than 80% of the acquired Italian TEs
result from the automatic translation of the expres-
sions already stored in the DB. This makes the pro-
posed approach a viable solution which allows for
a rapid porting of the system to other languages,
while just requiring an on-line translator (note that
the Altavista Babel Fish translator9 provides trans-
lations from English to 12 target languages). In
light of these considerations, the results obtained
by Ita-TERSEO are encouraging.
5 Conclusions
In this paper we have presented an automatic ex-
tension of a rule-based approach to TEs recogni-
tion and normalization. The procedure is based
on building temporal models for new languages
starting from previously defined ones. This proce-
dure is able to fill the gap left by machine learning
systems that, up to date, are still far from provid-
ing acceptable performance on this task. As re-
sults illustrate, the proposed methodology (even
though with a lower performance with respect to
language-specific systems) is a viable and effec-
tive solution for a rapid and automatic porting of
an existing system to new languages.
References
B. Carpenter. 2004. Phrasal Queries with LingPipe
and Lucene. In 13th Text REtrieval Conference,
NIST Special Publication. National Institute of Stan-
dards and Technology.
L. Ferro, L. Gerber, I. Mani, B. Sundheim, and G. Wil-
son. 2005. TIDES 2005 Standard for the annotation
of temporal expressions. Technical report, MITRE.
E. Filatova and E. Hovy. 2001. Assigning time-stamps
to event-clauses. In ACL, editor, Proceedings of the
2001 ACL Workshop on Temporal and Spatial Infor-
mation Processing, pages 88?95, Toulouse, France.
9http://world.altavista.com/
A. Ittycheriah, L.V. Lita, N. Kambhatla, N. Nicolov,
S. Roukos, and M. Stys. 2003. Identifying and
Tracking Entity Mentions in a Maximum Entropy
Framework. In ACL, editor, Proceedings of the
NAACL Workshop WordNet and Other Lexical Re-
sources: Applications, Extensions and Customiza-
tions.
G. Katz and F. Arosio. 2001. The annotation of tem-
poral information in natural language sentences. In
ACL, editor, Proceedings of the 2001 ACL Work-
shop on Temporal and Spatial Information Process-
ing, pages 104?111, Toulouse, France.
A. Lavelli, B. Magnini, M. Negri, E. Pianta, M. Sper-
anza, and R. Sprugnoli. 2005. Italian Content An-
notation Bank (I-CAB): Temporal expressions (v.
1.0.): T-0505-12. Technical report, ITC-irst, Trento.
T. Moia. 2001. Telling apart temporal locating adver-
bials and time-denoting expressions. In ACL, editor,
Proceedings of the 2001 ACL Workshop on Tempo-
ral and Spatial Information Processing, Toulouse,
France.
M. Negri and L. Marseglia. 2004. Recognition and
normalization of time expressions: Itc-irst at TERN
2004. Technical report, ITC-irst, Trento.
E. Saquete, P. Mart??nez-Barco, and R. Mun?oz. 2004.
Evaluation of the automatic multilinguality for time
expression resolution. In DEXA Workshops, pages
25?30. IEEE Computer Society.
E. Saquete, R. Mun?oz, and P. Mart??nez-Barco. 2005.
Event ordering using terseo system. Data and
Knowledge Engineering Journal, page (To be pub-
lished).
E. Saquete. 2005. Temporal information Resolution
and its application to Temporal Question Answer-
ing. Phd, Departamento de Lenguages y Sistemas
Informa?ticos. Universidad de Alicante, June.
F. Schilder and C. Habel. 2001. From temporal expres-
sions to temporal information: Semantic tagging of
news messages. In ACL, editor, Proceedings of the
2001 ACL Workshop on Temporal and Spatial Infor-
mation Processing, pages 65?72, Toulouse, France.
A. Setzer and R. Gaizauskas. 2002. On the impor-
tance of annotating event-event temporal relations
in text. In LREC, editor, Proceedings of the LREC
Workshop on Temporal Annotation Standards, 2002,
pages 52?60, Las Palmas de Gran Canaria,Spain.
P. Vossen. 2000. EuroWordNet: Building a Multilin-
gual Database with WordNets in 8 European Lan-
guages. The ELRA Newsletter, 5(1):9?10.
G. Wilson, I. Mani, B. Sundheim, and L. Ferro. 2001.
A multilingual approach to annotating and extract-
ing temporal information. In ACL, editor, Pro-
ceedings of the 2001 ACL Workshop on Temporal
and Spatial Information Processing, pages 81?87,
Toulouse, France.
8
Proceedings of the Fifth Law Workshop (LAW V), pages 143?151,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Annotating Events, Temporal Expressions and Relations in Italian: 
the It-TimeML Experience for the Ita-TimeBank 
 
 
Tommaso Caselli Valentina Bartalesi Lenzi Rachele Sprugnoli 
ILC ?A.Zampolli? - CNR  
Via G. Moruzzi, 1  
56124 Pisa 
CELCT 
Via della Cascata 56/c  
38123 Povo (TN) 
CELCT 
Via della Cascata 56/c  
38123 Povo (TN) 
caselli@ilc.cnt.it bartalesi@celct.it sprugnoli@celct.it 
Emanuele Pianta Irina Prodanof 
CELCT 
Via della Cascata 56/c  
38123 Povo (TN) 
ILC ?A.Zampolli? - CNR  
Via G. Moruzzi, 1  
56124 Pisa 
pianta@fbk.eu prodanof@ilc.cnr.it 
 
Abstract 
This paper presents the annotation 
guidelines and specifications which have 
been developed for the creation of the 
Italian TimeBank, a language resource 
composed of two corpora manually 
annotated with temporal and event 
information. In particular, the adaptation 
of the TimeML scheme to Italian is 
described, and a special attention is 
given to the methodology used for the 
realization of the annotation 
specifications, which are strategic in 
order to create good quality annotated 
resources and to justify the annotated 
items. The reliability of the It-TimeML 
guidelines and specifications is 
evaluated on the basis of the results of 
the inter-coder agreement performed 
during the annotation of the two corpora. 
? Introduction 
In recent years a renewed interest in temporal 
processing has spread in the NLP community, 
thanks to the success of the TimeML annotation 
scheme (Pustejovsky et al, 2003a) and to the 
availability of annotated resources, such as the 
English and French TimeBanks (Pustejovsky et 
al., 2003b; Bittar, 2010) and the TempEval 
corpora (Verhagen et al, 2010). 
The ISO TC 37 / SC 4 initiative 
(?Terminology and other language and content 
resources?) and the TempEval-2 contest have 
contributed to the development of TimeML-
compliant annotation schemes in languages 
other than English, namely Spanish, Korean, 
Chinese, French and Italian. Once the 
corresponding corpora will be completed and 
made available, the NLP community will benefit 
from having access to different language 
resources with a common layer of annotation 
which could boost studies in multilingual 
temporal processing and improve the 
performance of complex multilingual NLP 
systems, such as Question-Answering and 
Textual Entailment. 
This paper focuses on the annotation 
guidelines and specifications which have been 
developed for the creation of the Italian 
TimeBank (hereafter, Ita-TimeBank). The 
distinction between annotation guidelines and 
annotation specifications is of utmost 
importance in order to distinguish between the 
abstract, formal definition of an annotation 
scheme and the actual realization of the 
annotated language resource. In addition to this, 
documenting the annotation specification 
facilitates the reduplication of annotations and 
justify the annotated items. 
The paper is organized as follows: Section 2 
will describe in detail specific issues related to 
the temporal annotation of Italian for the two 
main tags of the TimeML annotation scheme, 
143
namely <EVENT> and <TIMEX3>. Section 3 
will present the realization of the annotation 
specifications and will document them. Section 
4 focuses on the evaluation of the annotation 
scheme on the Ita-TimeBank, formed by two 
corpora independently realized by applying the 
annotation specifications. Finally, in Section 5 
conclusions and extensions to the current 
annotation effort will be reported. 
Notice that, for clarity's sake, in this paper the 
examples will focus only on the tag (or attribute 
or link) under discussion. 
? It-TimeML: Extensions and 
Language Specific Issues 
Applying an annotation scheme to a language 
other than the one for which it was initially 
developed, requires a careful study of the 
language specific issues related to the linguistic 
phenomena taken into account (Im et al, 2009; 
Bittar, 2008). 
TimeML focuses on Events (i.e. actions, 
states, and processes - <EVENT> tag), 
Temporal Expressions (i.e. durations, calendar 
dates, times of day and sets of time - 
<TIMEX3> tag), Signals (e.g. temporal 
prepositions and subordinators - <SIGNAL> 
tag) and various kind of dependencies between 
Events and/or Temporal Expressions (i.e. 
temporal, aspectual and subordination relations - 
<TLINK>, <ALINK> and <SLINK> tags 
respectively). 
An ISO language-independent specification 
of TimeML is under development but it is still 
in the enquiry stage1. For this reason, in the 
following subsections we will mostly compare 
the Italian annotation guidelines with the latest 
version of the English annotation guidelines 
(TimeML Working group, 2010), focusing on 
the two main tags, i.e <EVENT> and 
<TIMEX3>, in Italian. 
2.1 The <EVENT> tag 
The <EVENT> tag is used to mark-up instances 
of eventualities (Bach, 1986). This category 
comprises all types of actions (punctual or 
durative) and states as well. With respect to 
                                                          
1
http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalog
ue_detail.htm?csnumber=37331 
previous annotations schemes (Katz and Arosio, 
2001, Filatova and Hovy, 2001, Setzer and 
Gaizauskas, 2001 among other), TimeML 
allows for annotating as Events not only verbs 
but also nouns, adjectives and prepositional 
phrases. 
In the adaptation to Italian, two annotation 
principles adopted for English, that is an 
orientation towards surface linguistic 
phenomena and the notion of minimal chunk for 
the tag extent, have been preserved without 
major modifications. The main differences with 
respect to the English version rely i.) in the 
attribute list; and ii.) in the attributes values. 
In Italian 12 core attributes apply with respect 
to the 10 attributes in English. The newly 
introduced attributes are MOOD and VFORM 
which capture key distinctions of the Tense-
Mood-Aspect (TMA) system of the Italian 
language. These two attributes are common to 
other languages, such as Spanish, Catalan, 
French and Korean. 
The MOOD attribute captures the contrastive 
grammatical expression of different modalities 
of presentation of an Event when realized by a 
verb. Annotating this attribute is important since 
grammatical modality has an impact on the 
identification of temporal and subordinating 
relations, and on the assessment of 
veridicity/factivity values. Mood in Italian is 
expressed as part of the verb morphology and 
not by means of modal auxiliary verbs as in 
English (e.g. through the auxiliary ?would?),. 
Thus, the solution to deal with this phenomenon 
adopted for English TimeML (where the main 
verb is annotated with the attribute 
MODALITY=?would?, see below) is not 
applicable in Italian unless relevant information 
is lost. The values of the MOOD attribute, as 
listed below, have been adapted to Italian and 
extended with respect to those proposed in the 
ISO-TimeML specification: 
 
? NONE: it is used as the default value and 
corresponds to the Indicative mood: 
(1.) Le forze dell?ordine hanno <EVENT 
? mood="NONE"> schierato </EVENT> 
3.000 agenti. [The police has deployed 
3,000 agents.] 
 
144
? CONDITIONAL: it signals the conditional 
mood which is used to speak of an Event 
whose realization is dependent on a certain 
condition, or to signal the future-in-the-
past: 
(2.) <EVENT ... mood="COND"> 
Mangerei </EVENT> del pesce. [I would 
eat fish.] 
  
? SUBJUNCTIVE: it has several uses in 
independent clauses and is required for 
certain types of dependent clauses. 
(3.) Voglio che tu te ne <EVENT ? 
mood="SUBJUNCTIVE">vada</EVENT> 
[I want you to go.] 
  
? IMPERATIVE: it is used to express direct 
commands or requests, to signal a 
prohibition, permission or any other kind of 
exhortation. 
 
The attribute VFORM is responsible for 
distinguishing between non-finite and finite 
forms of verbal Events. Its values are: 
 
? NONE: it is the default value and signals 
finite verb forms: 
(4.) Le forze dell?ordine hanno <EVENT 
? vForm="NONE">schierato</EVENT> 
3.000 agenti. [The police has deployed 
3,000 agents.] 
 
? INFINITIVE: for infinitive verb forms: 
(5.) Non ? possibile <EVENT ? 
vForm=''INFINITIVE''>viaggiare</EVEN
T>. [It?s not possible to travel.] 
 
? GERUND: for gerundive verb forms: 
(6.) Ha evitato l'incidente <EVENT ? 
vForm=''GERUND''> andando </EVENT> 
piano. [Driving slowly, he avoided the 
incident.] 
 
? PARTICIPLE: for participle verb forms: 
(7.) <EVENT ? vForm=?PARTICIPLE?> 
Vista </EVENT> Maria, se ne and?. 
[Having seen Maria, he left.] 
 
As for attribute values, the most important 
changes introduced for Italian in comparison 
with the English TimeML, are related to the 
ASPECT and MODALITY attributes. 
The ASPECT attribute captures standard 
distinctions in the grammatical category of 
aspect or Event viewpoint (Smith, 1991). In 
English TimeML it has the following values: i.) 
PROGRESSIVE; ii.) PERFECTIVE; iii.) 
PERFECTIVE_PROGRESSIVE, or iv.) NONE. 
The main differences with respect to the English 
guidelines concern the following points:  
i.) the absence of the value 
PERFECTIVE_PROGRESSIVE and  
ii.) the presence of the value 
IMPERFECTIVE, which is part of the ISO 
TimeML current definition.  
These differences are due to language specific 
phenomena related to the expression of the 
grammatical aspect in Italian and English and to 
the application of the TimeML surface oriented 
annotation philosophy. In particular, the 
assignment of the aspectual values is strictly 
determined by the verb surface forms. For 
instance, in English the verb form ?is teaching? 
requires the PROGRESSIVE value. On the 
other hand, the Italian counterpart of ?is 
teaching? can be realized in two ways: either by 
means of the simple present (insegna [s/he 
teaches]) or by means of a specific verbal 
periphrasis (sta insegnando [s/he is teaching]). 
In order to distinguish between these two verb 
forms, and to account also for other typical 
Romance languages tense forms, such as the 
Italian Imperfetto, the use of the additional 
IMPERFECTIVE value is necessary. Thus, 
insegna [s/he teaches], as well as the Imperfetto 
insegnava [s/he was teaching] are annotated as 
IMPERFECTIVE, whereas sta insegnando [s/he 
is teaching] is annotated as PROGRESSIVE. On 
the other hand, the absence of the 
PERFECTIVE_PROGRESSIVE value, used for 
English tense forms of the kind ?he has been 
teaching?, is due to the lack of Italian verb 
surface forms which may require its use. 
In English, modal verbs are not annotated as 
Events and the MODALITY attribute is 
associated to the main verb (the value of the 
attribute is the token corresponding to the modal 
verb). Unlike English modals, Italian modal 
verbs, such as potere [can/could; may/might], 
volere [want; will/would] and dovere 
[must/have to; ought to; shall/should], are to be 
145
considered similar to other lexical verbs in that 
it is possible to assign them values for tense and 
aspect. Consequently, each instance of Italian 
modal verbs will be annotated with the tag 
<EVENT>. The value of the MODALITY 
attribute is the lemma of the verb (e.g. dovere). 
A further language specific aspect concerns 
the annotation of verbal periphrases, that is 
special constructions with at least two verbs 
(and sometimes other words) that behave as a 
group like a single verb would. In Italian, it is 
possible to identify different instances of verbal 
periphrases, namely: 
 
? aspectual periphrases (example 8 below), 
which encode progressive or habitual 
aspect; 
? modal periphrases (example 9), which 
encode modality not realized by proper 
modal verbs;  
? phasal periphrases (example 10), which 
encode information on a particular phase in 
the description of an Event. 
 
Following Bertinetto (1991), in the last two 
cases, i.e. modal periphrases and phasal 
periphrases, both verbal elements involved 
should be annotated, while in the case of the 
aspectual periphrasis only the main verb (verb 
head) has to be marked; e.g.: 
(8.) Maria stava <EVENT ? 
ASPECT=?PROGRESSIVE?> mangiando. 
[Maria was eating] 
(9.) Il compito di matematica <EVENT ... 
MODALITY=?ANDARE?> va </EVENT> 
<EVENT ... > svolto </EVENT> per domani. 
[Maths exercises must be done for tomorrow]  
(10.) I contestatori hanno <EVENT ... 
CLASS=?ASPECTUAL?> iniziato </EVENT> 
a <EVENT> lanciare </EVENT> pietre. 
[Demonstrators started to throw stones.] 
Similarly to what proposed for English, in 
presence of multi-tokens realization of Events, 
two main annotation strategies have been 
followed: 
 
? in case the multi-token Event expression 
corresponds to an instance of a collocation 
or of an idiomatic expression, then only the 
head (verbal, nominal or other) of the 
expression is marked up;  
? in case the multi-token Event is realized by 
light verb expressions, then two separate 
<EVENT> tags are to be created both for 
the verb and the nominal/prepositional 
complement.  
2.2 The <TIMEX3> tag  
The TIMEX3 tag relies on and is as much 
compliant as possible with the TIDES TIMEX2 
annotation. The Italian adaptation of this 
annotation scheme is presented in Magnini et al 
(2006). The only difference concerns the 
annotation of articulated prepositions which are 
annotated as signals, while in the TIMEX2 
specifications they are considered as part of the 
textual realization of Temporal Expressions: 
(11a.) <TIMEX2 ?> nel 2011 </TIMEX2> 
[in 2011] 
(11b.) <SIGNAL ?> nel </SIGNAL> 
<TIMEX3?>2011</TIMEX3> [in 2011] 
On the other hand, with respect to the 
TIMEX3 annotation of other languages such as 
English, we decided to follow the TIMEX2 
specification by annotating many adjectives as 
Temporal Expressions (e.g. recente [recent], ex 
[former]) and including modifiers like che 
rimane in l?anno che rimane [the remaining 
year] into the extent of the TIMEX3 tag since it 
is essential for the normalization of temporal 
expressions. 
3 From Annotation Guidelines to 
Specifications ?
As already stated, the annotation guidelines 
represent an abstract, formal level of description 
which, in this case, is mainly based on a detailed 
study of the relevant linguistic levels. Once the 
guidelines are applied to real language data, 
further issues arise and need to be tackled. This 
section focuses on a method for developing 
annotation specifications. Annotation 
specifications are to be seen as the actual 
realization of the annotation guidelines. The 
identification and distinction of annotation 
guidelines from annotation specification is of 
major importance as it is to be conceived as a 
new level of Best Practice for the creation of 
146
semantically annotated Language Resources 
(Calzolari and Caselli, 2009). 
The process of realization of the annotation 
specifications is strategic both to realize good 
quality annotated resources and to justify why 
certain textual items have to be annotated. As 
for the It-TimeML experience we will illustrate 
this process by making reference and reporting 
examples for two tags, namely for the 
<EVENT> and the <TLINK> tags. 
As a general procedure for the development 
of the annotation specifications, we have taken 
inspiration from the DAMSL Manual (Core and 
Allen, 1997). Different decision trees have been 
created for each task. For instance, for the 
annotation of the <EVENT> tag, four different 
decision trees have been designed for each POS 
(i.e. nouns, verbs, adjectives and prepositional 
phrases) which could be involved in the 
realization of an Event. In particular, the most 
complex decision tree is that developed for noun 
annotation. The identification of the eventive 
reading of nouns has been formalized into a 
discrimination process of different properties: 
firstly superficial properties are taken into 
consideration, i.e. whether a morphologically 
related verb exists or not, and whether the noun 
co-occurs with special verb predicates (for 
instance aspectual verbs such as iniziare [to 
start] or light verbs such as fare [to do]); then, 
deeper semantic properties are analyzed, which 
involve other levels such as word sense 
disambiguation and noun classification (e.g. 
whether the noun is a functional or an 
incremental one). 
Other decision trees have been improved to 
avoid inconsistencies in Event classification. 
For instance, the identification of Reporting 
Events showed to be problematic because of the 
vague definition adopted in the guidelines. A 
Reporting Event is a giving information speech 
act in which a communicator conveys a message 
to an addressee. To help annotators in deciding 
whether an event is a Reporting one, the 
annotation specifications suggest to rely on 
FrameNet as a starting point (Baker, et al 
1998). More specifically, an Italian lexical unit 
has been classified as Reporting if it is the 
translation equivalent of one of the lexical units 
assigned to the Communication frame, which 
has Message as a core element. Among the 
frames using and inherited from the 
Communication frame, only the ones having the 
Message as a core element and conveying a 
giving information speech act have been 
selected and the lexical units belonging to them 
have been classified as Reporting Events: e.g. 
urlare [to scream] from the 
Communication_noise frame, sottolineare [to 
stress] from the Convey_importance frame, 
dichiarare [to declare] from the Statement 
frame. 
Similarly, for the identification of TLINKs, a 
set of decision trees has been developed to 
identify the conditions under which a temporal 
relation is to be annotated and a method to 
decide the value of the reltype attribute. For 
instance, the annotation of temporal relations 
between nominal Events and Temporal 
Expressions in the same sentence is allowed 
only when the Temporal Expression is realized 
either by an adjective or a prepositional phrase 
of the form ''di (of) + TEMPORAL 
EXPRESSION'' e.g.: 
(12.) La <EVENT eid=''e1'' ... > riunione 
</EVENT> <SIGNAL sid=''s1'' ... > di 
</SIGNAL> <TIMEX3 tid=''t1'' ... > ieri 
</TIMEX3> [yesterday meeting] 
<TLINK lid=''l1'' eventInstanceID=''e01'' 
relatedToTime=''t01'' signalID="s1" 
relType=''IS_INCLUDED''/> 
In addition, decision trees based on the idea 
that signals provide useful information to 
TLINK classification have been used to assign 
the reltype value to TLINKs holding between a 
duration and an Event. For example, the pattern 
?EVENT + tra (in) + DURATION? identifies 
the value AFTER, while the pattern ?EVENT + 
per (for) + DURATION? is associated with the 
value MEASURE. 
(13.) Il pacco <EVENT eid=''e1'' ... >arriver? 
</EVENT> <SIGNAL sid=''s1'' ... > tra 
</SIGNAL> <TIMEX3 tid=''t1'' ... > due giorni 
</TIMEX3> [the package will arrive in two 
days] 
<TLINK lid=''l1'' eventInstanceID=''e1'' 
relatedToTime=''t1'' signalID="s1" 
relType=''AFTER?/> 
(14.) Sono stati <EVENT eid=''e1'' ... > 
sposati </EVENT> <SIGNAL sid=''s1'' ... > per 
</SIGNAL> <TIMEX3 tid=''t1'' ... > dieci anni 
147
</TIMEX3> [they have been married for ten 
years] 
<TLINK lid=''l1'' eventInstanceID=''e1'' 
relatedToTime=''t1'' signalID="s1" 
relType=''MEASURE?/> 
The advantages of this formalization are 
many. The impact of the annotators' subjectivity 
is limited, thus reducing the risk of 
disagreement. Moreover, trees can then be 
easily used either as features for the 
development of a automatic learner or as 
instructions in a rule-based automatic annotation 
system. 
? Evaluating Annotations 
Two corpora have been developed in parallel 
following the It-TimeML annotation scheme, 
namely the CELCT corpus and the ILC corpus. 
Once these two corpora will be completed and 
released, they will form the Italian TimeBank 
providing the NLP community with the largest 
resource annotated with temporal and event 
information (more than 150K tokens). 
In this section, the two corpora are briefly 
described and the results of the inter-coder 
agreement (Artstein and Poesio, 2008) achieved 
during their annotation are compared in order to 
evaluate the quality of the guidelines and of the 
resources. 
The CELCT corpus has been created within 
the LiveMemories project2 and it consists of 
news stories taken from the Italian Content 
Annotation Bank (I-CAB, Magnini et al, 
2006). More than 180,000 tokens have been 
annotated with Temporal Expressions and 
more than 90,000 tokens have been annotated 
also with Events, Signals and Links. The 
Brandeis Annotation Tool3 (BAT) has been 
used for the pilot annotation and for the 
automatic computation of the inter-coder 
agreement on the extent and the attributes of 
Temporal Expressions, Events and Signals. 
After the pilot annotation, the first prototype of 
the CELCT Annotation Tool (CAT) has been 
used to perform the annotation and to compute 
the inter-coder agreement on Links. For what 
concern the annotation effort, the work on 
                                                          
2
 http://www.livememories.org 
3
 http://www.timeml.org/site/bat/ 
Temporal Expressions, Events and Signals 
involved 2 annotators while 3 annotators have 
been engaged in the annotation of Links. The 
annotation started in January 2010 and required 
a total of 1.3 person/years. Table 1 shows the 
total number of annotated markables together 
with the results of the inter-coder agreement on 
tag extent performed by two annotators on a 
subset of the corpus of about four thousand 
tokens. For the annotation of Event and Signal 
extents, statistics include average precision and 
recall and Cohen? kappa, while the Dice 
Coefficient has been computed for the extent of 
Links and Temporal Expressions. 
 
Markable # Agreement 
TIMEX3 4,852 Dice=0.94 
EVENT 17,554 K=0.93 P&R=0.94 
SIGNAL 2,045 K=0.88 P&R=0.88  
TLINK 3,373 Dice=0.86 
SLINK 3,985 Dice=0.93 
ALINK 238 Dice=0.90 
Table 1: Annotated markables and results of 
the inter-coder agreement on tag extent4 
 
Table 2 provides the value of Fleiss? kappa 
computed for the annotation of Temporal 
Expression, Event and Link attributes. 
 
Tag and attribute Agreement-Kappa 
TIMEX3.type  1.00 
TIMEX3.value 0.92 
TIMEX3.mod 0.89 
EVENT.aspect  0.96  
EVENT.class  0.87  
EVENT.modality  1.00  
EVENT.mood  0.90  
EVENT.polarity  1.00  
EVENT.pos  1.00  
EVENT.tense  0.94  
EVENT.vform  0.98  
TLINK.relType 0.88 
SLINK.relType 0.93 
ALINK.relType 1.00 
Table 2: Inter-coder agreement on 
attributes 
                                                          
4 Please note that the number of annotated Temporal 
Expressions is calculated on a total of 180,000 tokens, 
while the number of Events, Signals and Links is 
calculated on more than 90,000 tokens. 
148
The ILC corpus is composed of 171 
newspaper stories collected from the Italian 
Syntactic-Semantic Treebank, the PAROLE 
corpus and the web for a total of 68,000 
tokens (40,398 tokens are freely available, the 
remaining are available with restrictions). The 
news reports were selected to be comparable 
in content and size to the English TimeBank 
and they are mainly about international and 
national affairs, political and financial subject. 
The annotation of Temporal Expressions, 
Event extents and Signals has been completed 
while the annotation of Event attributes and 
LINKs is a work in progress. A subset of the 
corpus has been used as data set in the 
TempEval-2 evaluation campaign organized 
within SemEval-2 in 2010. So far the 
annotation has been performed thanks to eight 
voluntary students under the supervision of 
two judges using BAT. The annotation started 
in March 2009 and is requiring a total of 3 
person/years. Table 3 reports the total number 
of Temporal Expressions, Events, Signals and 
TLINKs together with the results of the inter-
coder agreement on tag extent performed on 
about 30,000 tokens. To measure the 
agreement on tag extents, average precision 
and recall and Cohen? kappa have been 
calculated. The annotation of Temporal Links 
has been divided into three subtasks: the first 
subtask is the relation between two Temporal 
Expressions, the second is the relation 
between an Event and a Temporal Expression, 
the third regards the relation between two 
Events. 
 
Markable # Agreement 
TIMEX3 2,314 K=0.95 P&R= 0.95 
EVENT 10,633 K=0.87 P&R= 0.86 
SIGNAL 1,704 K=0.83 P&R= 0.84 
 
T
L
I
N
K 
TIMEX3?
TIMEX3 
353 K=0.95 
EVENT?
TIMEX3 
512 K=0.87 
EVENT?
EVENT 
1,014 in progress 
Table 3: Annotated markables and results of 
the inter-coder agreement on tag extent 
 
The values of Fleiss? kappa computed for 
the assignment of attribute values are 
illustrated in Table 4. 
 
Tag and attribute Agreement ? Kappa 
TIMEX3.type  0.96 
TIMEX3.value 0.96 
TIMEX3.mod 0.97 
EVENT.aspect  0.93  
EVENT.class  0.82  
EVENT.modality  0.92  
EVENT.mood  0.89  
EVENT.polarity  0.75  
EVENT.pos  0.95  
EVENT.tense  0.97  
EVENT.vform  0.94  
TLINK.relType in progress 
Table 4: Annotated TLINKs and results of the 
inter-coder agreement 
 
Given the data reported in the above tables, 
it is possible to claim that the results of the 
inter-coder agreement are good and 
comparable beyond the different annotation 
method used to develop the two corpora. So 
far, the ILC corpus has been annotated 
without time constraints by several annotators 
with varying backgrounds in linguistics using 
BAT. With this web-based tool, each file has 
been assigned to many annotators and an 
adjudication phase on discrepancies has been 
performed by an expert judge. As required by 
BAT, the annotation has been divided into 
many annotation layers so each annotator 
focused only on a specific set of It-TimeML 
tags. On the other hand, few expert annotators 
have been involved in the development of the 
CELCT corpus interacting and negotiating 
common solutions to controversial 
annotations. With respect to BAT, the CELCT 
Annotation Tool is stand-alone and it does not 
require neither the parallel annotation of the 
same text, nor the decomposition of 
annotation tasks allowing to have flexibility in 
the annotation process and a unitary view of 
all annotation layers. These features are 
helpful when working with strict project 
deadlines. 
A comparison with the inter-coder agreement 
achieved during the annotation of the English 
TimeBank 1.2 (Pustejovsky et al, 2006a), 
shows that the scores obtained for the CELCT 
149
and the ILC corpora are substantially higher in 
the following results: (i) average precision and 
recall on the identification of tag extent (e.g. 
0.83 vs. 0.95 of ILC Corpus and 0.94 of CELCT 
Corpus for TIMEX3; 0.78 vs. 0.87 of ILC 
Corpus and 0.93 of CECLT Corpus); (ii) kappa 
score on Event classification (0.67 vs. 0.82 of 
ILC Corpus and 0.87 of the CELCT Corpus); 
(iii) kappa score on TLINK classification (0.77 
vs. 0.86 of CELCT Corpus). 
The similarity of the agreement results among 
the three resources and the improvement of the 
scores obtained on the CELCT and the ILC 
corpora with respect to the English TimeBank 
1.2, can be taken as an indication of the quality 
and coverage of the It-TimeML annotation 
guidelines and specifications. Annotators 
showed to perform consistently demonstrating 
the reliability of the annotation scheme. 
? Conclusions and Future Works 
This paper reports on the creation of a new 
semantic resource for Italian which has been 
developed independently but with a joint effort 
between two different research institutions. The 
Ita-TimeBank will represent a large corpus 
annotated with information for temporal 
processing which can boost the multilingual 
research in this field and represent a case study 
for the creation of semantic annotated resources. 
One of the most interesting point of this work 
is represented by the methodology followed for 
the development of the corpora: in addition to 
the guidelines, annotation specifications have 
been created in order to report in detail the 
actual choices done during the annotation. This 
element should be pushed forward in the 
community as a new best practice for the 
creation of good quality semantically annotated 
resources. 
The results obtained show the reliability of 
the adaptation of the annotation guidelines to 
Italian and of the methodology used for the 
creation of the resources. 
Future works will concentrate in different 
directions, mainly due to the research interests 
of the two groups which have taken part to this 
effort but they will be coordinated. 
An interesting aspect which could be 
investigated is the annotation of the anaphoric 
relations between Events. This effort could be 
done in a more reliable way since the primary 
linguistic items have been already annotated. 
Moreover, this should boost research in the 
development of annotation schemes which could 
be easily integrated with each other without 
losing descriptive and representational 
information for other language phenomena. 
Another topic to deepen regards the definition 
of the appropriate argument structure in It-
TimeML in order to annotate relations between 
entities (e.g. persons and organizations) and 
Events in which they are involved (Pustejovsky 
et al, 2006b). 
As regards the distribution of the Ita-
TimeBank, the resource will soon be available 
in an in-line format. In order to integrate the 
temporal annotation with other linguistic 
annotations, a standoff version of the Ita-
TimeBank needs to be developed. When this is 
made available, we plan to merge the manual 
annotation of temporal and event information 
with other types of linguistic stand-off 
annotations (i.e. tokenization, lemma, PoS, 
multi-words, various kinds of named entities) 
which are already available for the I-CAB 
corpus.  
In order to encourage research on systems 
capable of temporal inference and event-based 
reasoning, the Ita-TimeBank could be used as 
gold standard within specific evaluation 
campaigns as the next TempEval initiative. 
Finally, the use of crowdsourcing will be 
explored to reduce annotation effort in terms of 
financial cost and time. The most difficult 
challenge to face will be the splitting of a 
complicated annotation scheme as It-TimeML 
into simple tasks which can be effectively 
performed by not expert contributors. 
Acknowledgments 
The development of the CELCT corpus has 
been supported by the LiveMemories project 
(Active Digital Memories of Collective Life), 
funded by the Autonomous Province of Trento 
under the Major Projects 2006 research 
program. We would like to thank Alessandro 
Marchetti, Giovanni Moretti and Marc 
Verhagen who collaborated with us in 
processing and annotating the CELCT corpus. 
150
References 
Andr? Bittar. 2008. Annotation des informations 
temporelles dans des textes en fran?ais,. In 
Proceedings of RECITAL 2008, Avignon, France. 
Andr? Bittar. 2010. Building a TimeBank for French: 
A Reference Corpus Annotated According to the 
ISO-TimeML Standard. PhD Thesis. 
Andrea Setzer and Robert Gaizauskas.2001. A Pilot 
Study On Annotating Temporal Relations In Text. 
In: Proceedings of the ACL 2001 Workshop on 
Temporal and Spatial Information Processing. 
Bernardo Magnini, Emanuele Pianta, Christian 
Girardi, Matteo Negri, Lorenza Romano, Manuela 
Speranza, Valentina Bartalesi Lenzi and Rachele 
Sprugnoli. 2006. I-CAB: the Italian Content 
Annotation Bank. In Proceedings of LREC 2006, 
Genova, Italy. 
Bernardo Magnini, Matteo Negri, Emanuele Pianta, 
Manuela Speranza, Valentina Bartalesi Lenzi, and 
Rachele Sprugnoli. 2006. Italian Content 
Annotation Bank (I-CAB): Temporal Expressions 
(V.2.0). Technical Report, FBK-irst. 
Carlota S. Smith. 1991. The Parameter of Aspect. 
Kluwer, Dordrecht. 
Collin F., Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet project. In: 
Proceedings of the COLING-ACL, pages 86-90. 
Montreal, Canada. 
Elena Filatova and Eduard Hovy. 2001. Assigning 
Time-Stamps To Event-Clauses. In: Proceedings 
of the ACL 2001 Workshop on Temporal and 
Spatial Information Processing. 
Emmon Bach. 1986. The algebra of events. 
Linguistics and Philosophy, 9, 5?16. 
Graham Katz and Fabrizio Arosio. 2001. The 
Annotation Of Temporal Information In Natural 
Language Sentences. In: Proceedings of the ACL 
2001 Workshop on Temporal and Spatial 
Information Processing. 
ISO: Language Resource Management ? Semantic 
Annotation Framework (SemAF) - Part 1: Time 
and Events. Secretariat KATS, August 2007. ISO 
Report ISO/TC37/SC4 N269 version 19 (ISO/WD 
24617-1). 
James Pustejovsky, Jessica Littman and Roser Saur?. 
2006b. Argument Structure in TimeML. In: 
Graham Katz, James Pustejovsky and Frank 
Schilder (eds.) Dagstuhl Seminar Proceedings. 
Internationales Begegnungs- und 
Forschungszentrum (IB-FI), Schloss Dagstuhl, 
Germany. 
James Pustejovsky, Jessica Littman, Roser Saur?, and 
Marc Verhagen. 2006a. TimeBank 1.2 
Documentation. 
http://timeml.org/site/timebank/documentation-
1.2.html 
James Pustejovsky, Jos? Casta?o, Robert Ingria, 
Roser Saur?, Robert Gaizauskas, Andrea Setzer 
and Graham Katz. 2003a. TimeML: Robust 
Specification of Event and Temporal Expressions 
in Text. In: Proceedings of IWCS-5, Fifth 
International Workshop on Computational 
Semantics. 
James Pustejovsky, Patrick Hanks, Roser, Saur?, 
Andrew See, Robert Gaizauskas, Andrea Setzer, 
Dragomir Radev, Beth Sundheim, David Day,Lisa 
Ferro, and Marcia Lazo. 2003b. The TIMEBANK 
corpus. In: Proceedings of Corpus Linguistics 
2003, pages 647-656. 
Marc Verhagen, Roser Saur?, Tommaso Caselli and 
James Pustejovsky. 2010. SemEval-2010 Task 13: 
TempEval-2. In: Proceedings of the 5th 
International Workshop on Semantic Evaluation. 
Mark G. Core and James F. Allen. 1997. Coding 
Dialogs with the DAMSL Annotation Scheme. In: 
Working Notes of AAAI Fall Symposium on 
Communicative Action in Humans and Machines. 
Nicoletta Calzolari, and Tommaso Caselli 2009. 
Short Report on the FLaReNet / SILT Workshop 
and Panel on Semantic Annotation, TR-ILC-CNR. 
Pier Marco Bertinetto. 1991. Il verbo. In: R. L. and 
G. Salvi (eds.) Grande Grammatica Italiana di 
Consultazione, volume II, pages 13-161. Il 
Mulino. 
Ron Artstein and Massimo Poesio. Inter-coder 
agreement for computational linguistics. 
Computational Linguistics, pages 555?596, 2008. 
Seohyun Im, Hyunjo You, Hayun Jang, Seungho 
Nam, and Hyopil Shin. 2009. KTimeML: 
Specification of Temporal and Event Expressions 
in Korean Text. In: Proceedings of the 7th 
workshop on Asian Language Resources in 
conjunction with ACL-IJCNLP 2009, Suntec City, 
Singapore. 
TimeML Working Group. 2010. TimeML 
Annotation Guidelines version 1.3.Manuscript, 
Brandeis University. 
151
Proceedings of the The 1st Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 11?20,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
GAF: A Grounded Annotation Framework for Events
Antske Fokkens, Marieke van Erp, Piek Vossen
The Network Institute
VU University Amsterdam
antske.fokkens@vu.nl
marieke.van.erp@vu.nl
piek.vossen@vu.nl
Sara Tonelli
FBK
Trento, Italy
satonelli@fbk.eu
Willem Robert van Hage
SynerScope B.V.
Eindhoven, The Netherlands
willem.van.hage
@synerscope.com
Luciano Serafini, Rachele Sprugnoli
FBK
Trento, Italy
serafini@fbk.eu
sprugnoli@fbk.eu
Jesper Hoeksema
The Network Institute
VU University Amsterdam
j.e.hoeksema@vu.nl
Abstract
This paper introduces GAF, a grounded an-
notation framework to represent events in a
formal context that can represent information
from both textual and extra-textual sources.
GAF makes a clear distinction between men-
tions of events in text and their formal rep-
resentation as instances in a semantic layer.
Instances are represented by RDF compliant
URIs that are shared across different research
disciplines. This allows us to complete textual
information with external sources and facili-
tates reasoning. The semantic layer can inte-
grate any linguistic information and is com-
patible with previous event representations in
NLP. Through a use case on earthquakes in
Southeast Asia, we demonstrate GAF flexibil-
ity and ability to reason over events with the
aid of extra-linguistic resources.
1 Introduction
Events are not only described in textual documents,
they are also represented in many other non-textual
sources. These sources include videos, pictures,
sensors or evidence from data registration such as
mobile phone data, financial transactions and hos-
pital registrations. Nevertheless, many approaches
to textual event annotation consider events as text-
internal-affairs, possibly across multiple documents
but seldom across different modalities. It follows
from the above that event representation is not ex-
clusively a concern for the NLP community. It also
plays a major role in several other branches of in-
formation science such as knowledge representation
and the Semantic Web, which have created their own
models for representing events.
We propose a grounded annotation framework
(GAF) that allows us to interconnect different ways
of describing and registering events, including non-
linguistic sources. GAF representations can be used
to reason over the cumulated and linked sources of
knowledge and information to interpret the often in-
complete and fragmented information that is pro-
vided by each source. We make a clear distinction
between mentions of events in text or any other form
of registration and their formal representation as in-
stances in a semantic layer.
Mentions in text are annotated using the Terence
Annotation Format (Moens et al, 2011, TAF) on top
of which the semantic layer is realized using Seman-
tic Web technologies and standards. In this semantic
layer, instances are denoted with Uniform Resource
Identifiers (URIs). Attributes and relations are ex-
pressed according to the Simple Event Model (Van
Hage et al, 2011, SEM) and other established on-
tologies. Statements are grouped in named graphs
based on provenance and (temporal) validity, en-
abling the representation of conflicting information.
External knowledge can be related to instances from
a wide variety of sources such as those found in the
Linked Open Data Cloud (Bizer et al, 2009a).
Instances in the semantic layer can optionally be
linked to one or more mentions in text or to other
sources. Because linking instances is optional, our
11
representation offers a straightforward way to in-
clude information that can be inferred from text,
such as implied participants or whether an event is
part of a series that is not explicitly mentioned. Due
to the fact that each URI is unique, it is clear that
mentions connected to the same URI have a coref-
erential relation. Other relations between instances
(participants, subevents, temporal relations, etc.) are
represented explicitly in the semantic layer.
The remainder of this paper is structured as fol-
lows. In Section 2, we present related work and ex-
plain the motivation behind our approach. Section 3
describes the in-text annotation approach. Our se-
mantic annotation layer is presented in Section 4.
Sections 5-7 present GAF through a use case on
earthquakes in Indonesia. This is followed by our
conclusions and future work in section 8.
2 Motivation and Background
Annotation of events and of relations between them
has a long tradition in NLP. The MUC confer-
ences (Grishman and Sundheim, 1996) in the 90s
did not explicitly annotate events and coreference
relations, but the templates used for evaluating the
information extraction tasks indirectly can be seen
as annotation of events represented in newswires.
Such events are not ordered in time or further related
to each other. In response, Setzer and Gaizauskas
(2000) describe an annotation framework to create
coherent temporal orderings of events represented
in documents using closure rules. They suggest that
reasoning with text independent models, such as a
calendar, helps annotating textual representations.
More recently, generic corpora, such as Prop-
bank (Palmer et al, 2005) and the Framenet cor-
pus (Baker et al, 2003) have been built according to
linguistic principles. The annotations aim at prop-
erly representing verb structures within a sentence
context, focusing on verb arguments, semantic roles
and other elements. In ACE 2004 (Linguistic Data
Consortium, 2004b), event detection and linking is
included as a pilot task for the first time, inspired by
annotation schemes developed for named entities.
They distinguish between event mentions and the
trigger event, which is the mention that most clearly
expresses its occurrence (Linguistic Data Consor-
tium, 2004a). Typically, agreement on the trigger
event is low across annotators (around 55% (Moens
et al, 2011)). Timebank (Pustejovsky et al, 2006b)
is a more recent corpus for representing events and
time-expressions that includes temporal relations in
addition to plain coreference relations.
All these approaches have in common that they
consider the textual representation as a closed world
within which events need to be represented. This
means that mentions are linked to a trigger event
or to each other but not to an independent semantic
representation. More recently, researchers started to
annotate events across multiple documents, such as
the EventCorefBank (Bejan and Harabagiu, 2010).
Cross-document coreference is more challenging for
establishing the trigger event, but it is in essence not
different from annotating textual event coreference
within a single document. Descriptions of events
across documents may complement each other pro-
viding a more complete picture, but still textual de-
scriptions tend to be incomplete and sparse with re-
spect to time, place and participants. At the same
time, the comparison of events becomes more com-
plex. We thus expect even lower agreement in as-
signing trigger events across documents. Nothman
et al (2012) define the trigger as the first new ar-
ticle that mentions an event, which is easier than
to find the clearest description and still report inter-
annotator agreement of .48 and .73, respectively.
Recent approaches to automatically resolve event
coreference (cf. Chambers and Jurafsky (2011a),
Bejan and Harabagiu (2010)) use some background
data to establish coreference and other relations be-
tween events in text. Background information, in-
cluding resources, and models learned from textual
data do not represent mentions of events directly but
are useful to fill gaps of knowledge in the textual
descriptions. They do not alter the model for anno-
tation as such.
We aim to take these recent efforts one step fur-
ther and propose a grounded annotation framework
(GAF). Our main goal is to integrate information
from text analysis in a formal context shared with
researchers across domains. Furthermore, GAF is
flexible enough to contain contradictory informa-
tion. This is both important to represent sources
that (partially) contradict each other and to com-
bine alternative annotations or output of different
NLP tools. Because conflicting information may be
12
present, provenance of information is provided in
our framework, so that we may decide which source
to trust more or use it as a feature to decide which in-
terpretation to follow. Different models of event rep-
resentation exist that can contribute valuable infor-
mation. Therefore our model is compliant with prior
approaches regardless of whether they are manual or
automatic. Finally, GAF makes a clear distinction
between instances and instance mentions avoiding
the problem of determining a trigger event. Addi-
tionally, it facilitates the integration of information
from extra-textual sources and information that can
be inferred from texts, but is not explicitly men-
tioned. Sections 5 to 7 will explain how we can
achieve this with GAF.
3 The TERENCE annotation format
The TERENCE Annotation Format (TAF) is de-
fined within the TERENCE Project1 with the goal
to include event mentions, temporal expressions and
participant mentions in a single annotation proto-
col (Moens et al, 2011). TAF is based on ISO-
TimeML (Pustejovsky et al, 2010), but introduces
several adaptations in order to fit the domain of chil-
dren?s stories for which it was originally developed.
The format has been used to annotate around 30 chil-
dren stories in Italian and 10 in English.
We selected TAF as the basis for our in-text anno-
tation for three reasons. First, it incorporates the (in
our opinion crucial) distinction between instances
and instance mentions. Second, it adapts some con-
solidated paradigms for linguistic annotation such as
TimeML for events and temporal expressions and
ACE for participants and participant mentions (Lin-
guistic Data Consortium, 2005). It is thus compat-
ible with other annotation schemes. Third, it inte-
grates the annotation of event mentions, participants
and temporal expressions into a unified framework.
We will elaborate briefly on these properties below.
As mentioned, TAF makes a clear distinction be-
tween instances and instance mentions. Originally,
this distinction only applied to nominal and named
entities, similar to ACE (Linguistic Data Consor-
tium, 2005), because children?s stories can gener-
ally be treated as a closed world, usually present-
1ICT FP7 Programme, ICT-2010-25410, http://www.
terenceproject.eu/
ing a simple sequence of events that do not corefer.
Event coreference and linking to other sources was
thus not relevant for this domain. In GAF, we ex-
tend the distinction between instances and instance
mentions to events to model event coreference, link
them to other sources and create a consistent model
for all instances.
Children?s stories usually include a small set of
characters, event sequences (mostly in chronologi-
cal order), and a few generic temporal expressions.
In the TERENCE project, modeling characters in
the stories is necessary. This requires an extension
of TimeML to deal with event participants. Puste-
jovsky et al (2006a) address the need to include ar-
guments in TimeML annotations, but that proposal
did not include specific examples and details on how
to perform annotation (e.g., on the participants? at-
tributes). Such guidelines were created for TAF.
The TAF annotation of event mentions largely
follows TimeML in annotating tense, aspect, class,
mood, modality and polarity and temporal expres-
sions. However, there are several differences be-
tween TAF and TimeML. First, temporal expres-
sions are not normalized into the ISO-8601 form,
because most children?s stories are not fixed to a spe-
cific date. In GAF, the normalization of expressions
takes place in the semantic layer as these go beyond
the scope of the text. As a result, temporal vague-
ness of linguistic expressions in text do not need to
be normalized in the textual representation to actual
time points and remain underspecified.2
In TAF, events and participant mentions are linked
through a has participant relation, which is defined
as a directional, one-to-one relation from the event
to the participant mentions. Only mentions corre-
sponding to mandatory arguments of the events in
the story are annotated. Annotators look up each
verb in a reference dictionary providing information
on the predicate-argument structure of each verb.
This makes annotation easier and generally not con-
troversial. However, this kind of information can be
provided only by annotators having a good knowl-
edge of linguistics.
All annotations are performed with the Celct An-
2Note that we can still use existing tools for normalization
at the linguistic level: early normalizations can be integrated
in the semantic layer alongside normalizations carried out at a
later point.
13
sem:sub
EventOf
sem:Event sem:Actor sem:Place sem:Time
sem:hasTime
sem:hasActor
sem:hasPlace
sem:PlaceType
sem:placeType
sem:EventType
sem:eventType
sem:ActorType
sem:actorType
sem:TimeType
sem:Type
sem:timeType
sem:Core
sem:subTypeOf
C
o
r
e
 
C
l
a
s
s
e
s
(
F
o
r
e
i
g
n
)
T
y
p
e
 
S
y
s
t
e
m
Literal sem:hasTimeStamp
Literal sem:hasTimeStamp
Figure 1: The SEM ontology
notation Tool (Bartalesi Lenzi et al, 2012), an online
tool supporting TimeML that can easily be extended
to include participant information. The annotated
file can be exported to various XML formats and im-
ported into the semantic layer. The next section de-
scribes SEM, the event model used in our semantic
layer, and how it complements the TAF annotations.
4 The Simple Event Model
The Simple Event Model (SEM) is an RDF
schema (Carroll and Klyne, 2004; Guha and Brick-
ley, 2004) to express who did what, where, and
when. There are many RDF schemas and OWL on-
tologies (Motik et al, 2009) that describe events,
e.g., Shaw et al (2009), Crofts et al (2008) and
Scherp et al (2009). SEM is among the most
flexible and easiest to adapt to different domains.
SEM describes events and related instances such as
the place, time and participants (called Actors in
SEM) by representing the interactions between the
instances with RDF triples. SEM models are se-
mantic networks that include events, places, times,
participants and all related concepts, such as their
types.
An overview of all the classes in the SEM ontol-
ogy and the relations connecting them is shown in
Figure 1. Nodes can be identified by URIs, which
universally identify them across all RDF models. If
for example one uses the URI used by DBpedia3
(Bizer et al, 2009b) for the 2004 catastrophe in In-
3http://dbpedia.org
donesia, then one really means the same event as ev-
erybody else who uses that URI. SEM does not put
any constraints on the RDF vocabulary, so vocabu-
laries can easily be reused. Places and place types
can for example be imported from GeoNames4 and
event types from the RDF version of WordNet.
SEM supports two types of abstraction: gener-
alization with hierarchical relations from other on-
tologies, such as the subclass relation from RDFS,
and aggregation of events into superevents with the
sem:subEventOf relation, as exemplified in Fig-
ure 2. Other types of abstractions can be represented
using additional schemas or ontologies in combina-
tion with SEM. For instance, temporal aggregation
can be done with constructs from the OWL Time
ontology (Hobbs and Pan, 2004).
Relations between events and other instances,
which could be other events, places, actors, times,
or external concepts, can be modeled using the
sem:eventProperty relation. This relation can
be refined to represent specific relations, such as
specific participation, causality or simultaneity rela-
tions. The provenance of information in the SEM
graph is captured through assigning contexts to
statements using the PROV Data Model (Moreau et
al., 2012). In this manner, all statements derived
from a specific newspaper article are stored in a
named graph that represents that origin. Conflicting
statements can be stored in different named graphs,
and can thus coexist. This gives us the possibility
4http://www.geonames.org/ontology/
14
sem:Event
sem:Place
sem:EventType
sem:Time
dbpedia:2004_Indian_Ocean_
earthquake_and_ tsunami
rdf:type
"December 2004 
Earthquake and 
Tsunami"@en
rdfs:label
rdf:type
rdf:type
"3.316"^^xsd:decimal
"2004-12-26"^^xsd:date
"95.854"^^xsd:decimal
wgs84:long
wgs84:lat
owltime:inXSD
DateTime
sem:hasPlace sem:hasTime
naacl:INSTANCE_186
rdf:type
sem:subEventOf
wn30:synset-
earthquake-noun-1
sem:eventType
rdf:type
naacl:INSTANCE_188
rdf:type
sem:subEventOf
naacl:INSTANCE_198
sem:hasTime
naacl:TIMEX3_81 "2004"str:anchorOfnwr:denotedBy
naacl:INSTANCE_MENTION_118
nwr:denotedBy "temblor"@en
str:anchorOf
nwr:denotedBy
"tsunami"@en
naacl:INSTANCE_MENTION_120
str:anchorOf
naacl:INSTANCE_189
sem:subEventOf
naacl:INSTANCE_MENTION_121
nwr:denotedBy
"swept"@en
str:anchorOf
sem:hasPlace
naacl:INSTANCE_67
naacl:INSTANCE_MENTION_19nwr:denotedBy
"Indian Ocean"@en
str:anchorOf
taf:LOCATION
taf:NSUBJ
geonames:1545739
skos:exactMatch
gaf:G1
gaf:G2
gaf:G3
gaf:G4
gaf:G5
dbpedia:Bloomberg
sem:accordingTo
taf:annotation_
2013_03_24
sem:accordingTo
gaf:annotation_
2013_04_29
sem:accordingTo
gaf:annotation_
2013_04_29
sem:accordingTo
taf:annotation_
2013_03_24
sem:accordingTo
sem:
derived
From
gaf:causes
Figure 2: Partial SEM representation of December 26th 2004 Earthquake
of delaying or ignoring the resolution of the conflict,
which enables use cases that require the analysis of
the conflict itself.
5 The GAF Annotation Framework
This section explains the basic idea behind GAF by
using texts on earthquakes in Indonesia. GAF pro-
vides a general model for event representation (in-
cluding textual and extra-textual mentions) as well
as exact representation of linguistic annotation or
output of NLP tools. Simply put, GAF is the combi-
nation of textual analyses and formal semantic rep-
resentations in RDF.
5.1 A SEM for earthquakes
We selected newspaper texts on the January 2009
West Papua earthquakes from Bejan and Harabagiu
(2010) to illustrate GAF. This choice was made be-
cause the topic ?earthquake? illustrates the advan-
tage of sharing URIs across domains. Gao and
Hunter (2011) propose a Linked Data model to cap-
ture major geological events such as earthquakes,
volcano activity and tsunamis. They combine infor-
mation from different seismological databases with
the intention to provide more complete information
to experts which may help to predict the occurrence
of such events. The information can also be used
in text interpretation. We can verify whether in-
terpretations by NLP tools correspond to the data
and relations defined by geologists or, through gen-
eralization, which interpretation is the most sensi-
ble given what we know about the events. General
information on events obtained from automatic text
processing, such as event templates (Chambers and
Jurafsky, 2011b) or typical event durations (Gusev
et al, 2010) can be integrated in SEM in a similar
manner. Provenance indications can be used to in-
dicate whether information is based on a model cre-
ated by an expert or an automatically derived model
obtained by a particular approach.
Figure 2 provides a fragment of a SEM represen-
tation for the earthquake and tsunami of December
26 2004.5 The model is partially inspired by Gao
and Hunter (2011)?s proposal. It combines infor-
mation extracted from texts with information from
DBpedia. The linking between the two can be es-
tablished either manually or automatically through
5The annotation and a larger representation including the
sentence it represents can be found on the GAF website http:
//wordpress.let.vu.nl/gaf.
15
an entity linking system.6 The combined event of
the earthquake and tsunami is represented by a DB-
pedia URI. The node labeled naacl:INSTANCE 186
represents the earthquake itself. The unambiguous
representation of the 2004 earthquake leads us to ad-
ditional information about it, for instance that the
earthquake is an event (sem:Event) and that the
sem:EventType is an earthquake, in this case
represented by a synset from WordNet, but also the
exact date it occurred and the exact location (cf
sem:hasTime, sem:hasPlace).
5.2 Integrating TAF representations into SEM
TAF annotations are converted to SEM relations.
For example, the TAF as participant relations
are translated to sem:hasActor relations, and
temporal relations are translated to sem:hasTime.
We use the relation nwr:denotedBy to link in-
stances to their mentions in the text which are repre-
sented by their unique identifiers in Figure 2.
Named graphs are used to model the source of
information as discussed in Section 4. The re-
lation sem:accordingTo indicates provenance
of information in the graph.7 For instance, the
mentions from the text in named graph gaf:G1
come from the source dbpedia:Bloomberg.
Relations between instances (e.g. between IN-
STANCE 189 and INSTANCE 188) are derived
from a specific grammatical relation in the text
(here, that tsunami is subject of swept) indicated
by the nwr:derivedFrom relation from gaf:G5
to gaf:G4. The grammatical relations included
in graph gaf:G5 come from a TAF annotation
(tag:annotation 2013 03 24).
6 GAF Earthquake Examples
This section takes a closer look at a few selected sen-
tences from the text that illustrate different aspects
of GAF. Figure 2 showed how a URI can provide a
formal context including important background in-
6Entity linking is the task of associating a mention to an
instance in a knowledge base. Several approaches and tools for
entity linking w.r.t. DBpedia and other data sets in the Linked
Open Data cloud are available and achieve good performances,
such as DBpedia Spotlight (Mendes et al, 2011); see (Rizzo
and Troncy, 2011) for a comparison of tools.
7The use of named graphs in this way to denote context is
compatible with the method used by Bozzato et al (2012).
formation on the event. Several texts in the corpus
refer to the tsunami of December 26, 2004, a 9.1
temblor in 2004 caused a tsunami and The catastro-
phe four years ago, among others. Compared to time
expressions such as 2004 and four years ago, time
indications extracted from external sources like DB-
pedia are not only more precise, but also permit us to
correctly establish the fact that these expressions re-
fer to the same event and thus indicate the same time.
The articles were published in January 2009: a direct
normalization of time indications would have placed
the catastrophe in 2005. The flexibility to combine
these seemingly conflicting time indications and de-
lay normalization can be used to correctly interpret
that four years ago early January 2009 refers to an
event taking place at the end of December 2004.
A fragment relating to one of the earthquakes of
January 2009: The quake struck off the coast [...] 75
kilometers (50 miles) west of [....] Manokwari pro-
vides a similar example. The expressions 75 kilo-
meters and 50 miles are clearly meant to express
the same distance, but not identical. The location
is most likely neither exactly 75 km nor 50 miles.
SEM can represent an underspecified location that
is included in the correct region. The exact location
of the earthquake can be found in external resources.
We can include both distances as expressions of the
location and decide whether they denote the general
location or include the normalized locations as alter-
natives to those from external resources.
Different sources may report different details.
Details may only be known later, or sources may
report from a different perspective. As provenance
information can be incorporated into the semantic
layer, we can represent different perspectives, and
choose which one to use when reasoning over the
information. For example, the following phrases
indicate the magnitude of the earthquakes that
struck Manokwari on January 4, 2009:
the 7.7 magnitude quake (source: Xinhuanet)
two quakes, measuring 7.6 and 7.4 (source: Bloomberg)
One 7.3-magnitude tremor (source: Jakartapost)
The first two magnitude indicators (7.7, 7.6)
are likely to pertain to the same earthquake, just as
the second two (7.4, 7.3) are. Trust indicators can
be found through the provenance trace of each men-
16
tion. Trust indicators can include the date on which
it was published, properties of the creation process,
the author, or publisher (Ceolin et al, 2010).
Furthermore, because the URIs are shared across
domains, we can link the information from the text
to information from seismological databases, which
may contain the exact measurement for the quake.
Similarly, external information obtained through
shared links can help us establish coreference. Con-
sider the sentences in Figure 3. There are several
ways to establish that the same event is meant in all
three sentences by using shared URIs and reasoning.
All sentences give us approximate time indications,
location of the affected area and casualties. Rea-
soning over these sentences combined with external
knowledge allows us to infer facts such as that un-
dersea [...] off [...] Aceh will be in the Indian Ocean,
or that the affected countries listed in the first sen-
tence are countries around the Indian Ocean, which
constitutes the Indian Ocean Community. The num-
ber of casualties in combination of the approximate
time indication or approximate location suffices to
identify the earthquake and tsunami in Indonesia on
December 26, 2004. The DBpedia representation
contains additional information such as the magni-
tude, exact location of the quake and a list of affected
countries, which can be used for additional verifica-
tion. This example illustrates how a formal context
using URIs that are shared across disciplines of in-
formation science can help to determine exact refer-
ents from limited or imprecise information.
7 Creating GAF
GAF entails integrating linguistic information
(e.g. TAF annotations) into RDF models (e.g. SEM).
The information in the model includes provenance
that points back to specific annotations. There are
two approaches to annotate text according to GAF.
The first approach is bottom-up. Mentions are
marked in the text as well as relations between them
(participants, time, causal relations, basically any-
thing except coreference). Consequently, these an-
notations are converted to SEM representations as
explained above. Coreference is established by link-
ing mentions to the same instance in SEM. The sec-
ond approach is top-down. Here, annotators mark
relations between instances (events, their partici-
pants, time relations, etc.) directly into SEM and
then link these to mentions in the text.
As mention in Section 2, inter-annotator agree-
ment on event annotation is generally low showing
that it is challenging. The task is somewhat simpli-
fied in GAF, since it removes the problem of identi-
fying an event trigger in the text. The GAF equiva-
lent of the event trigger in other linguistic annotation
approaches is an instance in SEM. However, other
challenges such as which mentions to select are in
principle not addressed by GAF, though differences
in inter-annotator agreement may be found depend-
ing on whether the bottom-up approach or the top-
down approach is selected. The formal context of
SEM may help frame annotations, especially for do-
mains such as earthquakes, where expert knowledge
was used to create basic event models. This may
help annotators while defining the correct relations
between events. On the other hand, the top-down
approach may lead to additional challenges, because
annotators are forced to link events to unambiguous
instances leading to hesitations as to when new in-
stances should be introduced.
Currently, we only use the bottom-up approach.
The main reason is the lack of an appropriate anno-
tation tool to directly annotate information in SEM.
We plan to perform comparative studies between the
two annotation approaches in future work.
8 Conclusion and Future Work
We presented GAF, an event annotation framework
in which textual mentions of events are grounded in
a semantic model that facilitates linking these events
to mentions in external (possibly non-textual) re-
sources and thereby reasoning. We illustrated how
GAF combines TAF and SEM through a use case
on earthquakes. We explained that we aim for a
representation that can combine textual and extra-
linguistic information, provides a clear distinction
between instances and instance mentions, is flexi-
ble enough to include conflicting information and
clearly marks the provenance of information.
GAF ticks all these boxes. All instances are rep-
resented by URIs in a semantic layer following stan-
dard RDF representations that are shared across re-
search disciplines. They are thus represented com-
pletely independent of the source and clearly distin-
17
There have been hundreds of earthquakes in Indonesia since a 9.1 temblor in 2004 caused a
tsunami that swept across the Indian Ocean, devastating coastal communities and leaving more
than 220,000 people dead in Indonesia, Sri Lanka, India, Thailand and other countries.
(Bloomberg, 2009-01-07 01:55 EST)
The catastrophe four years ago devastated Indian Ocean community and killed more than 230,000
people, over 170,000 of them in Aceh at northern tip of Sumatra Island of Indonesia.
(Xinhuanet, 2009-01-05 13:25:46 GMT)
In December 2004, a massive undersea quake off the western Indonesian province of Aceh
triggered a giant tsunami that left at least 230,000 people dead and missing in a dozen
countries facing the Indian Ocean. (Aljazeera, 2009-01-05 08:49 GMT)
Figure 3: Sample sentences mentioning the December 2004 Indonesian earthquake from sample texts
guished from mentions in text or mentions in other
sources. The Terence Annotation Format (TAF) pro-
vides a unified framework to annotate events, par-
ticipants and temporal expressions (and the corre-
sponding relations) by leaning on past, consolidated
annotation experiences such TimeML and ACE. We
will harmonize TAF, the Kyoto Annotation Format
(Bosma et al, 2009, KAF) and the NLP Interchange
Format (Hellmann et al, 2012, NIF) with respect
to the textual representation in the near future. The
NAF format includes the lessons learned from these
predecessors: layered standoff representations using
URI as identifiers and where possible standardized
data categories. The formal semantic model (SEM)
provides the flexibility to include conflicting infor-
mation as well as indications of the provenance of
this information. This allows us to use inferencing
and reasoning over the cumulated and aggregated
information, possibly exploiting the provenance of
the type of information source. This flexibility also
makes our representation compatible with all ap-
proaches dealing with event representation and de-
tections mentioned in Section 2. It can include au-
tomatically learned templates as well as specific re-
lations between events and time expressed in text.
Moreover, it may simultaneously contain output of
different NLP tools.
The proposed semantic layer may be simple, its
flexibility in importing external knowledge may in-
crease complexity in usage as it can model events in
every thinkable domain. To resolve this issue, it is
important to scope the domain by importing the ap-
propriate vocabularies, but no more. When keeping
this in mind, reasoning with SEM is shown to be rich
but still versatile (Van Hage et al, 2012).
While GAF provides us with the desired granu-
larity and flexibility for the event annotation tasks
we envision, a thorough evaluation still needs to be
carried out. This includes an evaluation of the anno-
tations created with GAF compared to other anno-
tation formats, as well as testing it within a greater
application. A comparative study of top-down and
bottom-up annotation will also be carried out. As al-
ready mentioned in Section 7, there is no appropriate
modeling tool for SEM yet. We are currently using
the CAT tool to create TAF annotations and convert
those to SEM, but will develop a tool to annotate the
semantic layer directly for this comparative study.
The most interesting effect of the GAF annota-
tions is that it provides us with relatively simple ac-
cess to a vast wealth of extra-linguistic information,
which we can utilize in a variety of NLP tasks; some
of the reasoning options that are made available by
the pairing up with Semantic Web technology may
for example aid us in identifying coreference rela-
tions between events. Investigating the implications
of this combination of NLP and Semantic Web tech-
nologies lies at the heart of our future work.
Acknowledgements
We thank Francesco Corcoglioniti for his helpful
comments and suggestions. The research lead-
ing to this paper was supported by the European
Union?s 7th Framework Programme via the News-
Reader Project (ICT-316404) and by the Biogra-
phyNed project, funded by the Netherlands eScience
Center (http://esciencecenter.nl/). Partners in Biog-
raphyNed are Huygens/ING Institute of the Dutch
Academy of Sciences and VU University Amster-
dam.
18
References
Collin F. Baker, Charles J. Fillmore, and Beau Cronin.
2003. The structure of the FrameNet database. Inter-
national Journal of Lexicography, 16(3):281?296.
Valentina Bartalesi Lenzi, Giovanni Moretti, and Rachele
Sprugnoli. 2012. CAT: the CELCT Annotation Tool.
In Proceedings of LREC 2012.
Cosmin Bejan and Sandra Harabagiu. 2010. Unsuper-
vised event coreference resolution with rich linguistic
features. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1412?1422.
Christian Bizer, Tom Heath, and Tim Berners-Lee.
2009a. Linked data - the story so far. International
Journal on Semantic Web and Information Systems,
5(3):1?22.
Christian Bizer, Jens Lehmann, Georgi Kobilarov, So?ren
Auer, Christian Becker, Richard Cyganiak, and Sebas-
tian Hellmann. 2009b. DBpedia - A crystallization
point for the Web of Data. Web Semantics: Science,
Services and Agents on the World Wide Web, 7(3):154
? 165.
Wauter Bosma, Piek Vossen, Aitor Soroa, German Rigau,
Maurizio Tesconi, Andrea Marchetti, Monica Mona-
chini, and Carlo Aliprandi. 2009. KAF: a generic se-
mantic annotation format. In Proceedings of the 5th
International Conference on Generative Approaches
to the Lexicon GL 2009, Pisa, Italy.
Loris Bozzato, Francesco Corcoglioniti, Martin Homola,
Mathew Joseph, and Luciano Serafini. 2012. Manag-
ing contextualized knowledge with the ckr (poster). In
Proceedings of the 9th Extended Semantic Web Con-
ference (ESWC 2012), May 27-31.
Jeremy J. Carroll and Graham Klyne. 2004. Re-
source description framework (RDF): Concepts and
abstract syntax. W3C recommendation, W3C, Febru-
ary. http://www.w3.org/TR/2004/REC-rdf-concepts-
20040210/.
Davide Ceolin, Paul Groth, and Willem Robert Van Hage.
2010. Calculating the trust of event descriptions using
provenance. Proceedings Of The SWPM.
Nathanael Chambers and Dan Jurafsky. 2011a.
Template-based information extraction without the
templates. In Proceedings of ACL-2011.
Nathanael Chambers and Dan Jurafsky. 2011b.
Template-based information extraction without the
templates. In Proceedings of ACL-2011, Portland, OR.
Nick Crofts, Martin Doerr, Tony Gill, Stephen Stead,
and Matthew Stiff. 2008. Definition of the CIDOC
Conceptual Reference Model. Technical report,
ICOM/CIDOC CRM Special Interest Group. version
4.2.5.
Lianli Gao and Jane Hunter. 2011. Publishing, link-
ing and annotating events via interactive timelines: an
earth sciences case study. In DeRiVE 2011 (Detec-
tion, Representation, and Exploitation of Events in the
Semantic Web) Workshop in conjunction with ISWC
2011, Bonn, Germany.
Ralph Grishman and Beth Sundheim. 1996. Message
understanding conference - 6: A brief history. In Pro-
ceedings of the 16th conference on Computational lin-
guistics (COLING?96), pages 466?471.
Ramanathan V. Guha and Dan Brickley. 2004.
RDF vocabulary description language 1.0: RDF
schema. W3C recommendation, W3C, Febru-
ary. http://www.w3.org/TR/2004/REC-rdf-schema-
20040210/.
Andrey Gusev, Nathanael Chambers, Pranav Khaitan,
Divye Khilnani, Steven Bethard, and Dan Jurafsky.
2010. Using query patterns to learn the duration of
events. In Proceedings of ISWC 2010.
Sebastian Hellmann, Jens Lehmann, and So?ren Auer.
2012. NIF: An ontology-based and linked-data-aware
NLP Interchange Format. Working Draft.
Jerry R Hobbs and Feng Pan. 2004. An ontology of time
for the semantic web. ACM Transactions on Asian
Language Information Processing (TALIP), 3(1):66?
85.
Linguistic Data Consortium. 2004a. Annotation
Guidelines for Event Detection and Characterization
(EDC). http://projects.ldc.upenn.edu/
ace/docs/EnglishEDCV2.0.pdf.
Linguistic Data Consortium. 2004b. The ACE 2004
Evaluation Plan. Technical report, LDC.
Linguistic Data Consortium. 2005. ACE (Automatic
Content Extraction) English annotation guidelines for
entities. Version 6.6, July.
Pablo N. Mendes, Max Jakob, Andre?s Garc??a-Silva, and
Christian Bizer. 2011. Dbpedia spotlight: shedding
light on the web of documents. In Proceedings of the
7th International Conference on Semantic Systems, I-
Semantics ?11, pages 1?8.
Marie-Francine Moens, Oleksandr Kolomiyets,
Emanuele Pianta, Sara Tonelli, and Steven Bethard.
2011. D3.1: State-of-the-art and design of novel
annotation languages and technologies: Updated
version. Technical report, TERENCE project ? ICT
FP7 Programme ? ICT-2010-25410.
Luc Moreau, Paolo Missier, Khalid Belhajjame, Reza
B?Far, James Cheney, Sam Coppens, Stephen Cress-
well, Yolanda Gil, Paul Groth, Graham Klyne, Timo-
thy Lebo, Jim McCusker, Simon Miles, James Myers,
Satya Sahoo, and Curt Tilmes. 2012. PROV-DM: The
PROV Data Model. Technical report.
Boris Motik, Bijan Parsia, and Peter F. Patel-
Schneider. 2009. OWL 2 Web Ontology
19
Language structural specification and functional-
style syntax. W3C recommendation, W3C,
October. http://www.w3.org/TR/2009/
REC-owl2-syntax-20091027/.
Joel Nothman, Matthew Honnibal, Ben Hachey, and
James R. Curran. 2012. Event linking: Ground-
ing event reference in a news archive. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers),
pages 228?232, Jeju Island, Korea, July. Association
for Computational Linguistics.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106, 2013/03/12.
James Pustejovsky, Jessica Littman, and Roser Saur?`.
2006a. Argument Structure in TimeML. In Dagstuhl
Seminar Proceedings. Internationales Begegnungs-
und Forschungszentrum.
James Pustejovsky, Jessica Littman, Roser Saur??, and
Marc Verhagen. 2006b. Timebank 1.2 documentation.
Technical report, Brandeis University, April.
James Pustejovsky, Kiyong Lee, Harry Bunt, and Lau-
rent Romary. 2010. ISO-TimeML: An international
standard for semantic annotation. In Proceedings o
the Fifth International Workshop on Interoperable Se-
mantic Annotation.
Giuseppe Rizzo and Raphae?l Troncy. 2011. NERD:
A framework for evaluating named entity recognition
tools in the Web of data. In Workshop on Web Scale
Knowledge Extraction, colocated with ISWC 2011.
Ansgar Scherp, Thomas Franz, Carsten Saathoff, and
Steffen Staab. 2009. F?a model of events based on
the foundational ontology dolce+ dns ultralight. In
Proceedings of the fifth international conference on
Knowledge capture, pages 137?144. ACM.
Andrea Setzer and Robert J. Gaizauskas. 2000. Annotat-
ing events and temporal information in newswire texts.
In LREC. European Language Resources Association.
Ryan Shaw, Raphae?l Troncy, and Lynda Hardman. 2009.
LODE: Linking Open Descriptions of Events. In 4th
Annual Asian Semantic Web Conference (ASWC?09),
Shanghai, China.
Willem Robert Van Hage, Ve?ronique Malaise?, Roxane
Segers, Laura Hollink, and Guus Schreiber. 2011. De-
sign and use of the simple event model (SEM). Jour-
nal of Web Semantics.
Willem Robert Van Hage, Marieke Van Erp, and
Ve?ronique Malaise?. 2012. Linked open piracy: A
story about e-science, linked data, and statistics. Jour-
nal on Data Semantics, 1(3):187?201.
20
Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 10?19,
Gothenburg, Sweden, April 26, 2014.
c?2014 Association for Computational Linguistics
Annotating causality in the TempEval-3 corpus
Paramita Mirza
FBK, Trento, Italy
University of Trento
paramita@fbk.eu
Rachele Sprugnoli
FBK, Trento, Italy
University of Trento
sprugnoli@fbk.eu
Sara Tonelli
FBK, Trento, Italy
satonelli@fbk.eu
Manuela Speranza
FBK, Trento, Italy
manspera@fbk.eu
Abstract
While there is a wide consensus in the NLP
community over the modeling of temporal
relations between events, mainly based on
Allen?s temporal logic, the question on how
to annotate other types of event relations, in
particular causal ones, is still open. In this
work, we present some annotation guide-
lines to capture causality between event
pairs, partly inspired by TimeML. We then
implement a rule-based algorithm to auto-
matically identify explicit causal relations
in the TempEval-3 corpus. Based on this
annotation, we report some statistics on the
behavior of causal cues in text and perform
a preliminary investigation on the interac-
tion between causal and temporal relations.
1 Introduction
The annotation of events and event relations in
natural language texts has gained in recent years in-
creasing attention, especially thanks to the develop-
ment of TimeML annotation scheme (Pustejovsky
et al., 2003), the release of TimeBank (Pustejovsky
et al., 2006) and the organization of several eval-
uation campaigns devoted to automatic temporal
processing (Verhagen et al., 2007; Verhagen et al.,
2010; UzZaman et al., 2013).
However, while there is a wide consensus in the
NLP community over the modeling of temporal
relations between events, mainly based on Allen?s
interval algebra (Allen, 1983), the question on how
to model other types of event relations is still open.
In particular, linguistic annotation of causal rela-
tions, which have been widely investigated from
a philosophical and logical point of view, are still
under debate. This leads, in turn, to the lack of
a standard benchmark to evaluate causal relation
extraction systems, making it difficult to compare
systems performances, and to identify the state-of-
the-art approach for this particular task.
Although several resources exist in which causal-
ity has been annotated, they cover only few aspects
of causality and do not model it in a global way,
comparable to what as been proposed for temporal
relations in TimeML. See for instance the annota-
tion of causal arguments in PropBank (Bonial et al.,
2010) and of causal discourse relations in the Penn
Discourse Treebank (The PDTB Research Group,
2008).
In this work, we propose annotation guidelines
for causality inspired by TimeML, trying to take ad-
vantage of the clear definition of events, signals and
relations proposed by Pustejovsky et al. (2003). Be-
sides, as a preliminary investigation of causality in
the TempEval-3 corpus, we perform an automatic
analysis of causal signals and relations observed in
the corpus. This work is a first step towards the an-
notation of the TempEval-3 corpus with causality,
with the final goal of investigating the strict connec-
tion between temporal and causal relations. In fact,
there is a temporal constraint in causality, i.e. the
cause must occur BEFORE the effect. We believe
that investigating this precondition on a corpus ba-
sis can contribute to improving the performance of
temporal and causal relation extraction systems.
2 Existing resources on Causality
Several attempts have been made to annotate causal
relations in texts. A common approach is to look
for specific cue phrases like because or since or to
look for verbs that contain a cause as part of their
meaning, such as break (cause to be broken) or
kill (cause to die) (Khoo et al., 2000; Sakaji et al.,
2008; Girju et al., 2007). In PropBank (Bonial et
al., 2010), causal relations are annotated in the form
of predicate-argument relations, where ARGM-CAU
is used to annotate ?the reason for an action?, for
example: ?They [
PREDICATE
moved] to London
[
ARGM-CAU
because of the baby].?
Another scheme annotates causal relations be-
tween discourse arguments, in the framework of
10
the Penn Discourse Treebank (PDTB). As opposed
to PropBank, this kind of relations holds only be-
tween clauses and do not involve predicates and
their arguments. In PDTB, the Cause relation type
is classified as a subtype of CONTINGENCY.
Causal relations have also been annotated as re-
lations between events in a restricted set of linguis-
tic constructions (Bethard et al., 2008), between
clauses in text from novels (Grivaz, 2010), or in
noun-noun compounds (Girju et al., 2007).
Several types of annotation guidelines for causal
relations have been presented, with varying de-
grees of reliability. One of the simpler approaches
asks annotators to check whether the sentence they
are reading can be paraphrased using a connective
phrase such as and as a result or and as a conse-
quence (Bethard et al., 2008).
Another approach to annotate causal relations
tries to combine linguistic tests with semantic rea-
soning tests. In Grivaz (2010), the linguistic para-
phrasing suggested by Bethard et al. (2008) is
augmented with rules that take into account other
semantic constraints, for instance if the potential
cause occurs before or after the potential effect.
3 Annotation of causal information
As part of a wider annotation effort aimed to an-
notate texts at the semantic level (Tonelli et al.,
2014), we propose guidelines for the annotation of
causal information. In particular, we define causal
relations between events based on the TimeML def-
inition of events (ISO TimeML Working Group,
2008), as including all types of actions (punctual
and durative) and states. Syntactically, events can
be realized by a wide range of linguistic expres-
sions such as verbs, nouns (which can realize even-
tualities in different ways, for example through a
nominalization process of a verb or by possessing
an eventive meaning), and prepositional construc-
tions.
Following TimeML, our annotation of events in-
volved in causal relations includes the polarity
attribute (see Section 3.3); in addition to this, we
have defined the factuality and certainty
event attributes, which are useful to infer informa-
tion about actual causality between events.
Parallel to the TimeML tag <SIGNAL> as an
indicator for temporal links, we have also intro-
duced the notion of causal signals through the use
of the <C-SIGNAL> tag.
3.1 C-SIGNAL
The <C-SIGNAL> tag is used to mark-up textual
elements that indicate the presence of a causal rela-
tion (i.e. a CLINK, see 3.2). Such elements include
all causal uses of:
? prepositions, e.g. because of, on account of,
as a result of, in response to, due to, from, by;
? conjunctions, e.g. because, since, so that,
hence, thereby;
? adverbial connectors, e.g. as a result, so,
therefore, thus;
? clause-integrated expressions, e.g. the result
is, the reason why, that?s why.
The extent of C-SIGNALs corresponds to the
whole expression, so multi-token extensions are
allowed.
3.2 CLINK (Causal Relations)
For the annotation of causal relations between
events, we use the <CLINK> tag, a directional
one-to-one relation where the causing event is the
source (the first argument, indicated as
S
in the
examples) and the caused event is the target (the
second argument, indicated as
T
). The annotation
of CLINKs includes the c-signalID attribute,
whose value is the ID of the C-SIGNAL indicating
the causal relation (if available).
A seminal research in cognitive psychology
based on the force dynamics theory (Talmy, 1988)
has shown that causation covers three main kinds of
causal concepts (Wolff, 2007), which are CAUSE,
ENABLE, and PREVENT, and that these causal
concepts are lexicalized as verbs (Wolff and Song,
2003): (i) CAUSE-type verbs: bribe, cause, com-
pel, convince, drive, have, impel, incite, induce,
influence, inspire, lead, move, persuade, prompt,
push, force, get, make, rouse, send, set, spur, start,
stimulate; (ii) ENABLE-type verbs: aid, allow, en-
able, help, leave, let, permit; (iii) PREVENT-type
verbs: bar, block, constrain, deter, discourage, dis-
suade, hamper, hinder, hold, impede, keep, prevent,
protect, restrain, restrict, save, stop. CAUSE, EN-
ABLE, and PREVENT categories of causation and
the corresponding verbs are taken into account in
our guidelines.
As causal relations are often not overtly ex-
pressed in text (Wolff et al., 2005), we restrict the
annotation of CLINKs to the presence of an explicit
11
causal construction linking two events in the same
sentence
1
, as detailed below:
? Basic constructions for CAUSE, ENABLE
and PREVENT categories of causation as
shown in the following examples:
The purchase
S
caused the creation
T
of the cur-
rent building
The purchase
S
enabled the diversification
T
of
their business
The purchase
S
prevented a future transfer
T
? Expressions containing affect verbs, such as
affect, influence, determine, and change. They
can be usually rephrased using cause, enable,
or prevent:
Ogun ACN crisis
S
affects the launch
T
of the
All Progressives Congress? Ogun ACN cri-
sis causes/enables/prevents the launch of the
All Progressives Congress
? Expressions containing link verbs, such as
link, lead, and depend on. They can usually
be replaced only with cause and enable:
An earthquake
T
in North America was linked
to a tsunami
S
in Japan ? An earthquake
in North America was caused/enabled by a
tsunami in Japan
*An earthquake in North America was pre-
vented by a tsunami in Japan
? Periphrastic causatives are generally com-
posed of a verb that takes an embedded clause
or predicate as a complement; for example,
in the sentence The blast
S
caused the boat
to heel
T
violently, the verb (i.e. caused) ex-
presses the notion of CAUSE while the em-
bedded verb (i.e. heel) expresses a particular
result. Note that the notion of CAUSE can
be expressed by verbs belonging to the three
categories previously mentioned (which are
CAUSE-type verbs, ENABLE-type verbs and
PREVENT-type verbs).
? Expressions containing causative conjunc-
tions and prepositions as listed in Section
3.1. Causative conjunctions and prepositions
are annotated as C-SIGNALs and their ID is
1
A typical example of implicit causal construction is rep-
resented by lexical causatives; for example, kill has the em-
bedded meaning of causing someone to die (Huang, 2012). In
the present guidelines, these cases are not included.
to be reported in the c-signalID attribute
of the CLINK.
2
In some contexts, the coordinating conjunction
and can imply causation; given the ambiguity of
this construction and the fact that it is not an ex-
plicit causal construction, however, we do not an-
notate CLINKs between two events connected by
and. Similarly, the temporal conjunctions after and
when can also implicitly assert a causal relation
but should not be annotated as C-SIGNALs and no
CLINKs are to be created (temporal relations have
to be created instead).
3.3 Polarity, factuality and certainty
The polarity attribute, present both in TimeML
and in our guidelines, captures the grammatical
category that distinguishes affirmative and negative
events. Its values are NEG for events which are
negated (for instance, the event cause in Serotonin
deficiency
S
may not cause depression
T
) and POS
otherwise.
The annotation of factuality that we added
to our guidelines is based on the situation to which
an event refers. FACTUAL is used for facts, i.e. sit-
uations that have happened, COUNTERFACTUAL
is used for counterfacts, i.e. situations that have no
real counterpart as they did not take place, NON-
FACTUAL is used for possibilities, i.e. speculative
situations, such as future events, events for which
it is not possible to determine whether they have
happened, and general statements.
The certainty attribute expresses the binary
distinction between certain (value CERTAIN) and
uncertain (value UNCERTAIN) events. Uncer-
tain events are typically marked in the text by the
presence of modals or modal adverbs (e.g. per-
haps, maybe) indicating possibility. In the sentence
Drinking
S
may cause memory loss
T
, the causal con-
nector cause is an example of a NON-FACTUAL
and UNCERTAIN event.
In the annotation algorithm presented in the fol-
lowing section, only the polarity attribute is
taken into account, given that information about
factuality and certainty of events is not annotated
in the TempEval-3 corpus. In particular, at the
time of the writing the algorithm considers only the
polarity of causal verbal connectors, because this
information is necessary to extract causal chains
2
The absence of a value for the c-signalID attribute
means that the causal relation is encoded by a verb.
12
between events in a text. However, adding informa-
tion on the polarity of the single events involved in
the relations would make possible also the identifi-
cation of positive and negative causes and effects.
4 Automatic annotation of explicit
causality between events
In order to verify the soundness of our annotation
framework for event causality, we implement some
simple rules based on the categories and linguistic
cues listed in Section 3. Our goal is two-fold: first,
we want to check how accurate rule-based identifi-
cation of (explicit) event causality can be. Second,
we want to have an estimate of how frequently
causality can be explicitly found in text.
The dataset we annotate has been released for
the TempEval-3 shared task
3
on temporal and event
processing. The TBAQ-cleaned corpus is the train-
ing set provided for the task, consisting of the Time-
Bank (Pustejovsky et al., 2006) and the AQUAINT
corpora. It contains around 100K words in total,
with 11K words annotated as events (UzZaman et
al., 2013). We choose this corpus because gold
events are already provided, and because it allows
us to perform further analyses on the interaction
between temporal and causal relations.
Our automatic annotation pipeline takes as in-
put the TBAQ-cleaned corpus with gold annotated
events and tries to automatically recognize whether
there is a causal relation holding between them.
The annotation algorithm performs the following
steps in sequence:
1. The TBAQ-cleaned corpus is PoS-tagged and
parsed using the Stanford dependency parser
(de Marneffe and Manning, 2008).
2. The corpus is further analyzed with the ad-
dDiscourse tagger (Pitler and Nenkova, 2009),
which automatically identifies explicit dis-
course connectives and their sense, i.e. EX-
PANSION, CONTINGENCY, COMPARISON
and TEMPORAL. This is used to disambiguate
causal connectives (e.g. we consider only the
occurrences of since when it is a causal con-
nective, meaning that it falls into CONTIN-
GENCY class instead of TEMPORAL).
3. Given the list of affect, link, causative verbs
(basic and periphrastic constructions) and
causal signals listed in Sections 3.1 and 3.2,
3
http://www.cs.york.ac.uk/semeval-2013/task1/
the algorithm looks for specific dependency
constructions where the causal verb or signal
is connected to two events, as annotated in the
TBAQ-cleaned corpus.
4. If such dependencies are found, a CLINK is
automatically set between the two events iden-
tifying the source (
S
) and the target (
T
) of the
relation.
5. When a causal connector corresponds to an
event, the algorithm uses the polarity of the
event to assign a polarity to the causal link.
Specific approaches to detect when ambiguous
connectors have a causal meaning are implemented,
as in the case of from and by, where the algorithm
looks for specific structures. For instance, in ?The
building was damaged
T
by the earthquake
S
?, by is
governed by a passive verb annotated as event.
Also the preposition due to is ambiguous as
shown in the following sentences where it acts as a
causal connector only in b):
a) It had been due to expire Friday evening.
b) It cut
T
the dividend due to its third-quarter loss
S
of $992,000.
The algorithm performs the disambiguation by
checking the dependency structures: in sentence a)
there is only one dependency relation xcomp(due,
expire), while in sentence b) the dependency rela-
tions are xcomp(cut, due) and prep to(due, loss).
Besides, both cut and loss are annotated as events.
We are aware that this type of automatic anno-
tation may be prone to errors because it takes into
account only a limited list of causal connectors.
Besides, it only partially accounts for possible am-
biguities of causal cues and may suffer from pars-
ing errors. However, this allows us to make some
preliminary remarks on the amount of causal in-
formation found in the TempEval-3 corpus. Some
statistics are reported in the following subsection.
4.1 Statistics of Automatic Annotation
Basic construction. In Table 1 we report some
statistics on the non-periphrastic structures
identified starting from verbs expressing the three
categories of causation. Note that for the verbs
have, start, hold and keep, even though they
connect two events, we cannot say that there
is always a causal relation between them, as
exemplified in the following sentence taken from
the corpus:
a) Gen. Schwarzkopf secretly picked
S
Saturday
13
night as the optimal time to start the offensive
T
.
b) On Tuesday, the National Abortion and
Reproductive Rights Action League plans
S
to hold
a news conference
T
to screen a TV advertisement.
Types Verbs CLINK
CAUSE
have 1
start 2
cause 1
compel 1
PREVENT
hold 1
keep 3
block 7
prevent 1
ENABLE - -
Total 17
Table 1: Statistics of CLINKs with basic construc-
tion
Affect verbs. The algorithm does not annotate
any causal relation containing affect verbs mostly
because the majority of the 36 affect verb occur-
rences found in the corpus connect two elements
that are not events, as in ?These big stocks greatly
influence the Nasdaq Composite Index.?
Link verbs. In total, we found 50 occurrences of
link verbs in the corpus, but the algorithm identifies
only 4 causal links. Similar to affect verbs, this is
mainly due to the fact that two events are not found
to be involved in the relation. For instance, the
system associated only one CLINK to link (out
of 12 occurrences of the verb) and no CLINKs
to depend (which occurs 3 times). Most of the
CLINKs identified are signaled by the verb lead;
for example, ?Pol Pot is considered responsible for
the radical policies
S
that led to the deaths
T
of as
many as 1.7 million Cambodians.?
Periphrastic causative verbs. Overall, there are
around 1K potential occurrences of periphrastic
causative verbs in the corpus. However, the algo-
rithm identifies only around 14% of them as part
of a periphrastic construction, as shown in Table 2.
This is because some verbs are often used in non-
periphrastic structures, e.g. make, have, get, keep
and hold. Among the 144 cases of periphrastic con-
structions, 41 causal links are found by our rules.
In Table 2, for each verb type, we report the list
of verbs that appear in periphrastic constructions
in the corpus, specifying the number of CLINKs
identified by the system for each of them.
Some other CAUSE-type (move, push, drive, in-
fluence, compel, spur), PREVENT-type (hold, save,
impede, deter, discourage, dissuade, restrict) and
ENABLE-type (aid) verbs occur in the corpus but
are not involved in periphrastic structures. Some
others do not appear in the corpus at all (bribe, im-
pel, incite, induce, inspire, rouse, stimulate, hinder,
restrain).
Types Verbs Periphr. CLINK All
CAUSE
have 34 0 239
make 6 2 125
get 1 0 50
lead 2 1 38
send 5 1 34
set 2 0 23
start 1 0 22
force 2 1 15
cause 3 2 12
prompt 3 2 6
persuade 2 1 3
convince 1 1 2
PREVENT
keep 1 1 58
stop 3 0 24
block 2 2 21
protect 2 1 15
prevent 6 2 12
hamper 1 0 2
bar 1 0 1
constrain 1 0 1
ENABLE
help 31 13 45
leave 2 2 45
allow 22 3 39
permit 2 1 6
enable 4 2 5
let 4 3 5
Total 144 41 848
Table 2: Statistics of periphrastic causative verbs
Causal signals. Similar to periphrastic causative
verbs, out of around 1.2K potential causal connec-
tors found in the corpus, only 194 are automatically
recognized as actual causal signals after disam-
biguation, as detailed in Table 3. Based on these
identified causal signals, the algorithm derives 111
CLINKs.
Even though the addDiscourse tool labels 11
occurrences of the adverbial connector so as having
a causal meaning, our algorithm does not annotate
any CLINKs for such connector. In most cases, it
is because it acts as an inter-sentential connector,
while we limit the annotation of CLINKs only to
events occurring within the same sentence.
CLINKs polarity. Table 4 shows the distribution
of the positive and negative polarity of the detected
CLINKs.
Only two cases of negated CLINKs are automat-
ically identified in the corpus. One example is the
following: ?Director of the U.S. Federal Bureau of
14
Types C-SIGNALs Causal CLINK All
prep.
because of 32 11 32
on account of 0 0 0
as a result of 13 9 13
in response to 7 1 7
due to 2 1 6
from 2 2 500
by 23 24 465
conj.
because 58 37 58
since 26 19 72
so that 5 4 5
adverbial
as a result 3 0 3
so 11 0 69
therefore 4 0 4
thus 6 2 6
hence 0 0 0
thereby 1 0 1
consequently 1 1 1
clausal
the result is 0 0 0
the reason why 0 0 0
that is why 0 0 0
Total 194 111 1242
Table 3: Statistics of causal signals in CLINKs
Investigation (FBI) Louis Freeh said here Friday
that U.S. air raid
T
on Afghanistan and Sudan is
not directly linked with the probe
S
into the August
7 bombings in east Africa.?
Connector types POS NEG
Basic
CAUSE 5 0
PREVENT 12 0
ENABLE - -
Affect verbs - -
Link verbs 3 1
Periphrastic
CAUSE 10 1
PREVENT 6 0
ENABLE 24 0
Total 60 2
Table 4: Statistics of CLINKs? polarity
CLINKs vs TLINKs. In total, the algorithm iden-
tifies 173 CLINKs in the TBAQ-cleaned corpus,
while the total number of TLINKs between pairs of
events is around 5.2K. For each detected CLINK
between an event pair, we identify the underlying
temporal relations (TLINKs) if any. We found that
from the total of CLINKs extracted, around 33%
of them have an underlying TLINK, as detailed in
Table 5. Most of them are CLINKs signaled by
causal signals.
For causative verbs, the BEFORE relation is the
only underlying temporal relation type, with the
exception of one SIMULTANEOUS relation.
As for C-SIGNALs, the distribution of temporal
relation types is less homogeneous, as shown in Ta-
ble 6. In most of the cases, the underlying temporal
relation is BEFORE. In few cases, CLINKs sig-
Connector types CLINK TLINK
Basic
CAUSE 5 2
PREVENT 12 0
ENABLE - -
Affect verbs - -
Link verbs 4 1
Periphrastic
CAUSE 11 1
PREVENT 6 0
ENABLE 24 0
C-SIGNALs 111 54
Total 173 58
Table 5: Statistics of CLINKs? overlapping with
TLINKs
naled by the connector because overlap with an AF-
TER relation, as in ?But some analysts questioned
T
how much of an impact the retirement package will
have, because few jobs will end
S
up being elimi-
nated.?
In some cases, CLINKs signaled by the con-
nector since match with a BEGINS relation. This
shows that since expresses merely a temporal and
not a causal link. As it has been discussed before,
the connector since is highly ambiguous and the
CLINK has been wrongly assigned because of a
disambiguation mistake of the addDiscourse tool.
5 Evaluation
We perform two types of evaluation. The first is
a qualitative one, and is carried out by manually
inspecting the 173 CLINKs that have been auto-
matically annotated. The second is a quantitative
evaluation, and is performed by comparing the au-
tomatic annotated data with a gold standard corpus
of 100 documents taken from TimeBank.
5.1 Qualitative Evaluation
The automatically annotated CLINKs have been
manually checked in order to measure the precision
of the adopted procedure. Out of 173 annotated
CLINKs, 105 were correctly identified obtaining a
precision of 0.61.
Details on precision calculated on the different
types of categories and linguistic cues defined in
Section 3.2 are provided in Table 7. Statistics show
that performances vary widely depending on the
category and linguistic cue taken into consideration.
In particular, relations expressing causation of PRE-
VENT type prove to be extremely difficult to be
correctly detected with a rule-based approach: the
algorithm precision is 0.25 for basic constructions
and 0.17 for periphrastic constructions.
During the manual evaluation, two main types
15
C-SIGNALs BEFORE AFTER IS INCLUDED BEGINS others
because of 5 - - - -
as a result of 2 - - - -
in response to 1 - - - -
due to 1 - - - -
by 11 - 1 2 3
because 14 2 1 - 1
since 4 1 - 3 -
so that 1 - - - -
thus 1 - - - -
Total 40 3 2 5 4
Table 6: Statistics of CLINKs triggered by C-SIGNALs overlapping with TLINKs
Connector types Extracted Correct P
Basic
CAUSE 5 3 0.60
PREVENT 12 3 0.25
ENABLE 0 n.a. n.a.
Affect Verbs 0 n.a. n.a.
Link Verbs 4 3 0.75
Periphrastic
CAUSE 11 8 0.73
PREVENT 6 1 0.17
ENABLE 24 17 0.71
C-SIGNALs 111 70 0.63
Total 173 105 0.61
Table 7: Precision of automatically annotated
CLINKs
of mistakes have been observed: the wrong iden-
tification of events involved in CLINKs and the
annotation of sentences that do not contain causal
relations.
The assignment of a wrong source or a wrong
target to a CLINK is primarily caused by the de-
pendency parser output that tends to establish a
connection between a causal verb or signal and the
closest previous verb. For example, in the sentence
?StatesWest Airlines said it withdrew
T
its offer to
acquire Mesa Airlines because the Farmington car-
rier did not respond
S
to its offer?, the CLINK is
annotated between respond and acquire instead of
between respond and withdrew. On the other hand,
dependency structure is very effective in identify-
ing cases where one event is the consequence or
the cause of multiple events, as in ?The president
offered to offset
T
Jordan?s costs because 40% of
its exports go
S
to Iraq and 90% of its oil comes
S
from there.? In this case, the algorithm annotates a
causal link between go and offset, and also between
comes and offset.
The annotation of CLINKs in sentences not con-
taining causal relations is strongly related to the
ambiguous nature of many verbs, prepositions and
conjunctions, which encode a causal meaning or
express a causal relation only in some specific
contexts. For instance, many mistakes are due to
the erroneous disambiguation of the conjunction
since. According to the addDiscourse tool, since is
a causal connector in around one third of the cases,
as in ?For now, though, that would be a theoretical
advantage since the authorities have admitted they
have no idea where Kopp is.? However, there are
many cases where the outcome of the tool is not
perfect, as in ?Since then, 427 fugitives have been
taken into custody or located, 133 of them as a
result of citizen assistance, the FBI said?, where
since acts as a temporal conjunction.
5.2 Quantitative Evaluation
In order to perform also a quantitative evaluation of
our automatic annotation, we manually annotated
100 documents taken from the TimeBank corpus
according to the annotation guidelines discussed
before. We then used this data set as a gold stan-
dard.
The agreement reached by two annotators on a
subset of 5 documents is 0.844 Dice?s coefficient
on C-SIGNALS (micro-average over markables)
and of 0.73 on CLINKS.
We found that there are several cases where the
algorithm failed to recognize causal links due to
events that were originally not annotated in Time-
Bank. Therefore, as we proceed with the manual
annotation, we also annotated missing events that
are involved in causal relations. Table 8 shows that,
in creating the gold standard, we annotated 61 new
events. As a result, we have around 52% increase
in the number of CLINKs. Nevertheless, explicit
causal relations between events are by far less fre-
quent than temporal ones, with an average of 1.4
relations per document.
If we compare the coverage of automatic anno-
tation with the gold standard data (without newly
added events, to be fair), we observe that automatic
annotation covers around 76% of C-SIGNALs and
only around 55% of CLINKs. This is due to the
limitation of the algorithm that only considers a
16
Annotation EVENT C-SIGNAL CLINK
manual 3933 78 144
manual-w/o new events 3872 78 95
automatic 3872 59 52
Table 8: Statistics of causality annotation in manual
versus automatic annotation
precision recall F1-score
C-SIGNAL 0.64 0.49 0.55
CLINK 0.42 0.23 0.30
Table 9: Automatic annotation performance
small list of causal connectors. Some examples of
manually annotated causal signals that are not in
the list used by the algorithm include due mostly
to, thanks in part to and in punishment for.
Finally, we evaluate the performance of the algo-
rithm for automatic annotation (shown in Table 9)
by computing precision, recall and F1 on gold stan-
dard data without newly added events. We observe
that our rule-based approach is too rigid to capture
the causal information present in the data. In partic-
ular, it suffers from low recall as regards CLINKs.
We believe that this issue may be alleviated by
adopting a supervised approach, where the list of
verbs and causal signals would be included in a
larger feature set, considering among others the
events? position, their PoS tags, the dependency
path between the two events, etc.
6 Conclusions
In this paper, we presented our guidelines for an-
notating causality between events. We further tried
to automatically identify in TempEval-3 corpus the
types of causal relations described in the guide-
lines by implementing some simple rules based on
causal cues and dependency structures.
In a manual revision of the annotated causal
links, we observe that the algorithm obtains a pre-
cision of 0.61, with some issues related to the class
of PREVENT verbs. Some mistakes are introduced
by the tools used for parsing and for disambiguat-
ing causal signals, which in turn impact on our
annotation algorithm. Another issue, more related
to recall, is that in the TBAQ-cleaned corpus not all
events are annotated, because it focuses originally
on events involved in temporal relations. There-
fore, the number of causal relations identified auto-
matically would be higher if we did not take into
account this constraint.
From the statistics presented in Section 4.1, we
can observe that widely used verbs such as have or
keep express causality relations only in few cases.
The same holds for affect verbs, which are never
found in the corpus with a causal meaning, and for
link verbs. This shows that the main sense of causal
verbs usually reported in the literature is usually
the non-causal one.
Recognizing CLINKs based on causal signals is
more straightforward, probably because very fre-
quent ones such as because of and as a result are
not ambiguous. Others, such as by, can be identi-
fied based on specific syntactic constructions.
As for the polarity of CLINKs, which is a very
important feature to discriminate between actual
and negated causal relations, this phenomenon is
not very frequent (only 2 cases) and can be easily
identified through dependency relations.
We chose to automatically annotate TBAQ-
cleaned corpus because one of our goals was to
investigate how TLINKs and CLINKs interact.
However, this preliminary study shows that there
are only few overlaps between the two relations,
again with C-SIGNALs being more informative
than causal verbs. This may be biased by the fact
that, according to our annotation guidelines, only
explicit causal relations are annotated. Introducing
also the implicit cases would probably increase the
overlap between TLINKs and CLINKs, because
annotator would be allowed to capture the tempo-
ral constrains existing in causal relations even if
the are not overtly expressed.
In the near future, we will complete the manual
annotation of TempEval-3 corpus with causal in-
formation in order to have enough data for training
a supervised system, in which we will incorpo-
rate the lessons learnt with this first analysis. We
will also investigate the integration of the proposed
guidelines into the Grounded Annotation Format
(Fokkens et al., 2013), a formal framework for cap-
turing semantic information related to events and
participants at a conceptual level.
Acknowledgments
The research leading to this paper was partially
supported by the European Union?s 7th Frame-
work Programme via the NewsReader Project (ICT-
316404).
References
James F. Allen. 1983. Maintaining knowledge about
temporal intervals. Commun. ACM, 26(11):832?
843, November.
17
Steven Bethard, William Corvey, Sara Klingenstein,
and James H. Martin. 2008. Building a corpus of
temporal-causal structure. In European Language
Resources Association (ELRA), editor, Proceedings
of the Sixth International Language Resources and
Evaluation (LREC?08), Marrakech, Morocco, may.
Claire Bonial, Olga Babko-Malaya, Jinho D.
Choi, Jena Hwang, and Martha Palmer.
2010. Propbank annotation guidelines, De-
cember. http://www.ldc.upenn.edu/
Catalog/docs/LDC2011T03/propbank/
english-propbank.pdf.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, pages 1?8. Association for Com-
putational Linguistics.
Antske Fokkens, Marieke van Erp, Piek Vossen, Sara
Tonelli, Willem Robert van Hage, Luciano Ser-
afini, Rachele Sprugnoli, and Jesper Hoeksema.
2013. GAF: A Grounded Annotation Framework
for Events. In Workshop on Events: Definition, De-
tection, Coreference, and Representation, pages 11?
20, Atlanta, Georgia, June. Association for Compu-
tational Linguistics.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic
relations between nominals. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 13?18, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
C?ecile Grivaz. 2010. Human Judgements on Causation
in French Texts. In Proceedings of the Seventh con-
ference on International Language Resources and
Evaluation (LREC?10), Valletta, Malta, may. Euro-
pean Language Resources Association (ELRA).
Li-szu Agnes Huang. 2012. The Effectiveness of a
Corpus-based Instruction in Deepening EFL Learn-
ers? Knowledge of Periphrastic Causatives. TESOL
Journal, 6:83?108.
ISO TimeML Working Group. 2008. ISO TC37 draft
international standard DIS 24617-1, August 14.
http://semantic-annotation.uvt.nl/
ISO-TimeML-08-13-2008-vankiyong.
pdf.
Christopher S. G. Khoo, Syin Chan, and Yun Niu.
2000. Extracting causal knowledge from a medi-
cal database using graphical patterns. In In Proceed-
ings of 38th Annual Meeting of the ACL, Hong Kong,
2000, pages 336?343.
Emily Pitler and Ani Nenkova. 2009. Using syn-
tax to disambiguate explicit discourse connectives
in text. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, ACLShort ?09, pages 13?
16, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
James Pustejovsky, J. Castano, R. Ingria, Roser Saur??,
R. Gaizauskas, A. Setzer, G. Katz, and D. Radev.
2003. TimeML: Robust specification of event and
temporal expressions in text. In Proceedings of the
Fifth International Workshop on Computational Se-
mantics.
James Pustejovsky, Jessica Littman, Roser Saur??, and
Marc Verhagen. 2006. Timebank 1.2 documenta-
tion. Technical report, Brandeis University, April.
Hiroki Sakaji, Satoshi Sekine, and Shigeru Masuyama.
2008. Extracting causal knowledge using clue
phrases and syntactic patterns. In Proceedings of the
7th International Conference on Practical Aspects
of Knowledge Management, PAKM ?08, pages 111?
122, Berlin, Heidelberg. Springer-Verlag.
Leonard Talmy. 1988. Force dynamics in language
and cognition. Cognitive science, 12(1):49?100.
The PDTB Research Group. 2008. The PDTB 2.0. An-
notation Manual. Technical Report IRCS-08-01, In-
stitute for Research in Cognitive Science, University
of Pennsylvania.
Sara Tonelli, Rachele Sprugnoli, and Manuela Sper-
anza. 2014. NewsReader Guidelines for Annotation
at Document Level, Extension of Deliverable
D3.1. Technical Report NWR-2014-2, Fondazione
Bruno Kessler. https://docs.google.
com/viewer?url=http%3A%2F%2Fwww.
newsreader-project.eu%2Ffiles%
2F2013%2F01%2FNWR-2014-2.pdf.
Naushad UzZaman, Hector Llorens, Leon Derczyn-
ski, Marc Verhagen, James Allen, and James Puste-
jovsky. 2013. Semeval-2013 task 1: Tempeval-3:
Evaluating events, time expressions, and temporal
relations. In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval 2013).
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval tempo-
ral relation identification. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 75?80, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Marc Verhagen, Roser Saur??, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 57?62, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Phillip Wolff and Grace Song. 2003. Models of cau-
sation and the semantics of causal verbs. Cognitive
Psychology, 47(3):276?332.
18
Phillip Wolff, Bianca Klettke, Tatyana Ventura, and
Grace Song. 2005. Expressing causation in english
and other languages. Categorization inside and out-
side the laboratory: Essays in honor of Douglas L.
Medin, pages 29?48.
Phillip Wolff. 2007. Representing causation. Journal
of experimental psychology: General, 136(1):82.
19
