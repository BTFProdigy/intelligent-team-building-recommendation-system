Learning a Robust Word Sense Disambiguation Model using
Hypernyms in Definition Sentences
Kiyoaki Shirai, Tsunekazu Yagi
School of Information Science, Japan Advanced Institute of Science and Technology
1-1, Asahidai, Tatsunokuchi, 923-1292, Ishikawa, Japan
{kshirai,t-yagi}@jaist.ac.jp
Abstract
This paper proposes a method to improve
the robustness of a word sense disambigua-
tion (WSD) system for Japanese. Two
WSD classifiers are trained from a word
sense-tagged corpus: one is a classifier ob-
tained by supervised learning, the other is
a classifier using hypernyms extracted from
definition sentences in a dictionary. The for-
mer will be suitable for the disambiguation
of high frequency words, while the latter is
appropriate for low frequency words. A ro-
bust WSD system will be constructed by
combining these two classifiers. In our ex-
periments, the F-measure and applicability
of our proposed method were 3.4% and 10%
greater, respectively, compared with a single
classifier obtained by supervised learning.
1 Introduction
Word sense disambiguation (WSD) is the pro-
cess of selecting the appropriate meaning or
sense for a given word in a document. Obvi-
ously, WSD is one of the fundamental and im-
portant processes needed for many natural lan-
guage processing (NLP) applications. Over the
past decade, many studies have been made on
WSD of Japanese. Most current research uses
machine learning techniques (Li and Takeuchi,
1997; Murata et al, 2001; Takamura et al,
2001), has achieved good performance. How-
ever, as supervised learning methods require
word sense-tagged corpora, they often suffer
from data sparseness, i.e., words which do not
occur frequently in a training corpus can not be
disambiguated. Therefore, we cannot use su-
pervised learning algorithms alone in practical
NLP applications, especially when it is neces-
sary to disambiguate both high frequency and
low frequency words.
To tackle this problem, this paper proposes a
method to combine two WSD classifiers. One
is a classifier obtained by supervised learning.
The learning algorithm used for this classifier is
the Support Vector Machine (SVM); this clas-
sifier will work well for the disambiguation of
high frequency words. The second classifier is
the Naive Bayes model, which will work well
for the disambiguation of low frequency words.
In this model, hypernyms extracted from defi-
nition sentences in a dictionary are considered
in order to overcome data sparseness.
The details of the SVM classifier are de-
scribed in Section 2, and the Naive Bayes model
in Section 3. The combination of these two clas-
sifiers is described in Section 4. The experi-
mental evaluation of the proposed method is re-
ported in Section 5. We mention some related
works in Section 6, and conclude the paper in
Section 7.
2 SVM Classifier
The first classifier is the SVM classifier. Since
SVM is a supervised learning algorithm, a word
sense-tagged corpus is required as training data,
and the classifier can not be used to disam-
biguate words which do not occur frequently
in the data. However, as the effectiveness of
SVM has been widely reported for a variety of
NLP tasks including WSD (Murata et al, 2001;
Takamura et al, 2001), we know that it will
work well for disambiguation of high frequency
words.
When training the SVM classifier, each train-
ing instance should be represented by a feature
vector. We used the following features, which
are typical for WSD.
? S(0), S(?1), S(?2), S(+1), S(+2)
Surface forms of a target word and words
just before or after a target word. A num-
ber in parentheses indicates the position of
a word from a target word.
? P (?1), P (?2), P (+1), P (+2)
Parts-of-speech (POSs) of words just before
or after a target word.
? S(?2)?S(?1), S(+1)?S(+2), S(?1)?S(+1)
Pairs of surface forms of words surrounding
a target word.
? P (?2)?P (?1), P (+1)?P (+2), P (?1)?P (+1)
Pairs of POSs of words surrounding a tar-
get word.
? Bsent
Base forms of content words in a sentence1.
? Csent
Semantic classes of content words in a
sentence. Semantic classes used here
are derived from the Japanese the-
saurus ?Nihongo-Goi-Taikei? (Ikehara et
al., 1997).
? Bhead, Bmod
Base forms of the head (Bhead) or modifiers
(Bmod) of a target word.
? (Bcase;Bnoun)
A pair of the base forms of a case marker
(Bcase) and a case filler noun (Bnoun) when
the target word is a verb.
? (Bcase;Cnoun)
A pair of the base form of a case marker
(Bcase) and the semantic class of a case
filler noun (Cnoun) when the target word
is a verb.
? (Bcase;Bverb)
A pair of the base forms of a case marker
(Bcase) and a head verb (Bverb) when the
target word is a case filler noun of a certain
verb.
We used the LIBSVM package 2 for train-
ing the SVM classifier. The SVM model is
??SVM (Scho?lkopf, 2000) with a linear kernel,
where the parameter ? = 0.0001. The pairwise
method is used to apply SVM to multi classifi-
cation.
3 Naive Bayes Classifier using
Hypernyms in Definition
Sentences
In this section, we will describe the details of
the WSD classifier using hypernyms of words
1We tried using the special symbol ?NUM? as a fea-
ture for any numbers in a sentence, but the performance
was slightly worse in our experiment. We thank the
anonymous reviewer who gave us the comment about
this.
2http://www.csie.ntu.edu.tw/%7Ecjlin/
libsvm/
CID Definition sentence
3c5631 ????????? ??(a comic
story-telling entertainment)
1f66e3 ????????(a rambling story)
Figure 1: Sense Set of ????
extracted from definition sentences in a dictio-
nary.
3.1 Overview
Let us explain the basic idea of the model by
considering the case in which the word ????
(mandan; comic chat) in the following example
sentence (A) should be disambiguated:
(A) ??????????????...
(Mr. Sakano was initiated into the
world of comic chat ...)
In this paper, word senses are defined accord-
ing to the EDR concept dictionary (EDR, 1995).
Figure 1 illustrates two meanings for ????
(comic chat) in the EDR concept dictionary.
?CID? indicates a concept ID, an identification
number of a sense.
One of the ways to disambiguate the senses
of ???? (comic chat) is to train the WSD clas-
sifier from the sense-tagged corpus, as with the
SVM classifier. However, when ???? (comic
chat) occurs infrequently or not at all in the
training corpus, we can not train any reliable
classifiers.
To train the WSD classifier for low frequency
words, we looked at hypernyms of senses in def-
inition sentences. For Japanese, in most cases
the last word in a definition sentence is a hy-
pernym. For example, the hypernym of sense
3c5631 in Figure 1 is the last underlined word
???? (engei ; entertainment), while the hyper-
nym of 1f66e3 is ??? (hanashi ; story).
In the EDR concept dictionary, there are
senses whose hypernyms are also ???? (en-
tertainment) or ??? (story). For example, as
shown in Figure 2, 10d9a4, 3c3fbb and 3c5ab3
are senses whose hypernyms are ???? (enter-
tainment), while the hypernym of 3cf737, 0f73c1
and 3c3071 is ??? (story). If these senses oc-
cur in the training corpus, we can train a clas-
sifier that determines whether the hypernym of
???? (comic chat) is ???? (entertainment)
or ??? (story). If we can determine the correct
hypernym, we can also determine which is the
correct sense, 3c5631 or 1f66e3. Notice that we
can train such a model even when ???? (comic
???? 10d9a4 ?????????, ??????????? ?? (a monologue-style,
comic story-telling entertainment always ending with a punch line)
???? 3c3fbb ???????????? (a type of medieval folk entertainment of Japan,
called ?Sarugaku?)
???? 3c5ab3 ?????????????????? (entertainment of cutting shapes
out of paper)
???? 3cf737 ?????????????? (a story passed down among people since
ancient times)
???? 0f73c1 ?????????? (a true story)
???? 101156 ?????????? (a story for children)
Figure 2: Examples of Senses whose Hypernyms are ???? (entertainment) or ??? (story)
0efb60 ??/?/??/?/??/?/??/?
(a word representing the number of
competitions or contests)
Figure 3: Definition Sentence of the sense
0efb60 of the word ?????
chat) itself does not occur in the training cor-
pus.
As described later, we train the probabilistic
model that predicts a hypernym of a given word,
instead of a word sense. Much more training
data will be available to train the model pre-
dicting hypernyms rather than the model pre-
dicting senses, because there are fewer types
of hypernyms than of senses. Figure 2 illus-
trates this fact clearly: all words labeled with
10d9a4, 3c3fbb and 3c5ab3 in the training data
can be used as the data labeled with the hy-
pernym ???? (entertainment). In this way,
we can train a reliable WSD classifier for low
frequency words. Furthermore, hypernyms will
be automatically extracted from definition sen-
tences, as described in Subsection 3.2, so that
the model can be automatically trained without
human intervention.
3.2 Extraction of Hypernyms
In this subsection, we will describe how to ex-
tract hypernyms from definition sentences in a
dictionary. In principle, we assume that the hy-
pernym of a sense is the last word of a defi-
nition sentence. For example, in the definition
sentence of sense 3c5631 of ???? (comic chat),
the last word ???? (entertainment) is the hy-
pernym, as shown in Figure 1. However, we can-
not always regard the last word as a hypernym.
Let us consider the definition of sense 0eb70d of
the word ????? (ge?mu; game). In the EDR
concept dictionary, the expression ?A/?/??
?/?? (a word representing A) often appears in
definition sentences. In this case, the hypernym
of the sense is not the last word but A. Thus
the hypernym of 0efb60 is not the last word
??? (go; word) but ???? (kaisuu; number)
in Figure 3.
When we extract a hypernym from a defini-
tion sentence, the definition sentence is first an-
alyzed morphologically (word segmentation and
POS tagging) by ChaSen 3. Then a hypernym
in a definition sentence is identified by pattern
matching. An example of patterns used here is
the rule extracting A when the expression ?A/
?/???/?? is found in a definition sentence.
We made 64 similar patterns manually in order
to extract hypernyms appropriately.
Out of the 194,303 senses of content words in
the EDR concept dictionary, hypernyms were
extracted for 191,742 senses (98.7%) by our
pattern matching algorithm. Furthermore, we
chose 100 hypernyms randomly and checked
their validity, and found that 96% of the hyper-
nyms were appropriate. Therefore, our method
for extracting hypernyms worked well. The ma-
jor reasons why acquisition of hypernyms failed
were lack of patterns and faults in the morpho-
logical analysis of definition sentences.
3.3 Naive Bayes Model
We will describe the details of our probabilis-
tic model that considers hypernyms in defini-
tion sentences. First of all, let us consider the
following probability:
P (s, c|F ) (1)
In (1), s is a sense of a target word, c is a hy-
pernym extracted from the definition sentence
of s, and F is the set of features representing an
input sentence including a target word.
3ChaSen is the Japanese morphological analyzer.
http://chasen.aist-nara.ac.jp/hiki/ChaSen/
Next, we approximate Equation (1) as (2):
P (s, c|F ) = P (s|c, F )P (c|F )  P (s|c)P (c|F )
(2)
The first term, P (s|c, F ), is the probabilistic
model that predicts a sense s given a feature
set F (and c). It is similar to the ordinary
Naive Bayes model for WSD (Pedersen, 2000).
However, we assume that this model can not be
trained for low frequency words due to a lack
of training data. Therefore, we approximate
P (s|c, F ) to P (s|c).
Using Bayes? rule, Equation (2) can be com-
puted as follows:
P (s|c)P (c|F ) =
P (s)P (c|s)
P (c)
P (c)P (F |c)
P (F )
(3)
=
P (s)P (F |c)
P (F )
(4)
Notice that P (c|s) in (3) is equal to 1, because
a hypernym c of a sense s is uniquely extracted
by pattern matching (Subsection 3.2).
As all we want to do is to choose an s? which
maximizes (4), P (F ) can be eliminated:
s? = arg max
s
P (s)P (F |c)
P (F )
(5)
= arg max
s
P (s)P (F |c) (6)
Finally, by the Naive Bayes assumption, that is
all features in F are conditionally independent,
Equation (6) can be approximated as follows:
s? = arg max
s
P (s)
?
f
i
?F
P (fi|c) (7)
In (7), P (s) is the prior probability of a sense
s which reflects statistics of the appearance of
senses, while P (fi|c) is the posterior probability
which reflects collocation statistics between an
individual feature fi and a hypernym c. The
parameters of these probabilistic models can be
estimated from the word sense-tagged corpus.
We estimated P (s) by Expected Likelihood Es-
timation and P (fi|c) by linear interpolation.
Feature Set
The features used in the Naive Bayes model are
almost same as ones used in the SVM classifier
except for the following features:
[Features not used in the Naive Bayes model]
? S(?2), S(+2), P (?2), P (+2)
? S(?2)?S(?1), S(+1)?S(+2), S(?1)?S(+1)
? P (?2)?P (?1), P (+1)?P (+2), P (?1)?P (+1)
? Csent, (Bcase;Cnoun)
According to the preliminary experiment, the
accuracy of the Naive Bayes model slightly de-
creased when all features in the SVM classifier
were used. This was the reason why we did not
use the above features.
3.4 Discussion
The following discussion examines our method
for extracting hypernyms from definition sen-
tences.
Multiple Hypernyms
In general, two or more hypernyms can be ex-
tracted from a definition sentence, when the def-
inition of a sense consists of several sentences
or a definition sentence contains a coordinate
structure. However, for this work we extracted
only one hypernym for a sense, because defini-
tions of all senses in the EDR concept dictionary
are described by a single sentence, and most of
them contain no coordinate structure.
In order to apply our model for multiple hy-
pernyms, we must consider the probabilistic
model P (s,C|F ) instead of Equation (1), where
C is a set of hypernyms. Unfortunately, the es-
timation of P (s,C|F ) is not obvious, so inves-
tigation of this will be done in future.
Ambiguity of hypernyms
The fact that hypernyms may have several
meanings does not appear to be a major prob-
lem, because most hypernyms in definition sen-
tences of a certain dictionary have a single
meaning according to our rough observation. So
for this work we ignored the possible ambiguity
of hypernyms.
Using other dictionaries
As described in Subsection 3.2, hypernyms are
extracted by pattern matching. We would have
to rebuild these patterns when we use other dic-
tionaries, but we do not expect to require too
much labor. Generally, in Japanese the last
word in a definition sentence can be regarded
as a hypernym. Furthermore, many extraction
patterns for the EDR concept dictionary may
also be applicable for other dictionaries. We
are already building patterns to extract hyper-
nyms from the other major Japanese dictionary,
the Iwanami Kokugo Jiten, and developing the
WSD system that will use them.
4 Combined Model
The details of two WSD classifiers are described
in the previous two sections: one is the SVM
classifier for high frequency words, and the
other is the Naive Bayes classifier for low fre-
quency words. These two classifiers are com-
bined to construct the robust WSD system. We
developed two kinds of combined models, de-
scribed below in subsections 4.1 and 4.2.
4.1 Simple Ensemble
In this model, the process combining the two
classifiers is quite simple. When only one of
classifiers, SVM or Naive Bayes, outputs senses
for a given word, the combined model outputs
senses provided by that classifier. When both
classifiers output senses, the ones provided by
the SVM classifier are always chosen for the final
output.
In the experiment in Section 5, SVM clas-
sifiers were trained for words which occur more
than 20 times in the training corpus. Therefore,
the simple ensemble described here is summa-
rized as follows: we use the SVM classifier for
high frequency words those which occur more
than 20 times and the Naive Bayes classifier for
the low frequency words.
4.2 Ensemble using Validation Data
First, we prepare validation data, which is a
sense-tagged corpus, as common test data for
the classifiers. The performance of the classi-
fiers for a word w is evaluated by correctness
Cw, defined by (8).
Cw =
# of words in which one of the senses
selected by a classifier is correct
# of words for which a classifier selects
one or more senses
(8)
The main reason for combining two classifiers
is to improve the recall and applicability of the
WSD system. Note that a classifier which often
outputs a correct sense would achieve high cor-
rectness Cw, even though it also outputs wrong
senses. Thus, the higher the Cw of a classifier,
the more it improves the recall of the combined
model.
Next, the correctness Cw of each classifier for
each word w is measured on the validation data.
When two classifiers output senses for a given
word, their Cw scores are compared. Then, the
word senses provided by the better classifier are
selected as the final outputs.
When the number of words in the validation
data is small, comparison of the classifiers? Cw
is unreliable. For that reason, when the number
of words in the validation data is less that a cer-
tain threshold Oh, a sense output by the SVM
classifier is chosen for the final output. This is
because the correctness for all words in the vali-
dation data is higher for the SVM classifier than
for the Naive Bayes classifier. In the experiment
in Section 5, we set Oh to 10.
5 Experiment
In this section, we will describe the experiment
to evaluate our proposed method. We used the
EDR corpus (EDR, 1995) in the experiment. It
is made up of about 200,000 Japanese sentences
extracted from newspaper articles and maga-
zines. In the EDR corpus, each word was an-
notated with a sense ID (CID). We used 20,000
sentences in the EDR corpus as the test data,
20,000 sentence as the validation data, and
the remaining 161,332 sentences as the training
data. The training data was used to train the
SVM classifier and the Naive Bayes classifier,
while the validation data was used for the com-
bined model described in Subsection 4.2. The
target instances used for evaluation were all am-
biguous content words in the test data; the num-
ber of target instances was 91,986.
We evaluated three single WSD classifiers and
two combined models:
? BL
The baseline model. This is the WSD clas-
sifier which always selects the most fre-
quently used sense. When there is more
than one sense with equally high frequency,
the classifier chooses all those senses.
? NB
The Naive Bayes classifier (Section 3).
? SVM
The SVM classifier (Section 2).
? SVM+NB(simple)
The combined model by simple ensemble
(Subsection 4.1).
? SVM+NB(valid)
The combined model using the validation
data (Subsection 4.2).
Table 1 reveals the precision(P), recall(R), F-
measure(F) 4, applicability(A) and number of
word types(T) of these five classifiers on the test
4
2PR
P+R
where P and R represent the precision and re-
call, respectively.
Table 1: Results of WSD Classifiers
R P F A T
1) .6047 .6036 .6042 .9962 10,310
2) .6274 .6543 .6406 .9568 10,501
3) .6366 .7080 .6704 .8992 4,575
4) .7016 .7010 .7013 .9993 10,592
5) .7050 .7043 .7046 .9993 10,592
1)=BL, 2)=NB, 3)=SVM,
4)=SVM+NB(simple), 5)=SVM+NB(valid)
data. A(applicability) indicates the ratio of the
number of instances disambiguated by a classi-
fier to the total number of target instances; T
indicates the number of word types which could
be disambiguated by a classifier.
The two combined models outperformed the
SVM classifier, for all criteria except precision.
The gains in recall and applicability were espe-
cially remarkable. Notice the figures in column
?T? in Table 1: the SVM classifiers could be ap-
plied only to 4,575 words, while the Naive Bayes
classifiers were applicable to 10,501 words, in-
cluding low frequency words. Thus, the ensem-
ble of these two classifiers would significantly
improve applicability and recall with little loss
of precision.
Comparing the performance of the two com-
bined models, ?SVM+NB(validation)? slightly
outperformed ?SVM+NB(simple)?, but there
was no significant difference between them. The
correctness, Cw, of the SVM classifier on the
validation data was usually greater than that of
the Naive Bayes classifier, so the SVM classifier
was preferred when both were applicable. This
was the almost same strategy for the simple en-
semble, and we think this was the reason why
the performance of two combined models were
almost the same. In the rest of this section, we
will show the results for the combined model
using the validation data only.
Our goal was to improve the robustness of the
WSD system. The naive way to construct a ro-
bust WSD system is to create an ensemble of a
supervised learned classifier and a baseline clas-
sifier. So, we compared our proposed method
(SVM+NB) with the combined model of the
SVM and baseline classifier (SVM+BL). The re-
sults are shown in Table 2 and Figure 4. Table 2
shows the same criteria as in Table 1, indicating
that ?SVM+NB? outperformed ?SVM+BL? for
all criteria. Figure 4 shows the relation between
the F-measure of the classifiers and word fre-
quency in the training data. The horizontal axis
Table 2: Results of the Combined Models (1)
R P F A T
5) .7050 .7043 .7046 .9993 10,592
6) .6977 .6976 .6977 .9962 10,310
5)=SVM+NB, 6)=SVM+BL
0.2
0.3
0.4
0.5
0.6
0.7
0.8
-0.5 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5
0
100
200
300
400
500
600
700
800
F (SVM+NB) F (SVM+BL) NF(o) N(o)
log   o
10
Figure 4: Results of the Combined Models (2)
indicates the occurrence of words in the train-
ing data (o) in log scale. Squares and triangles
with lines indicates the F (o) of the ?SVM+NB?
and ?SVM+BL?, respectively, where F (o) is the
macro average of F-measures for words which
occur o times in the training data. The broken
line indicates N(o), the number of word types
which occur o times 5. For convenience, when
o = 0, we plot F (0) and N(0) at x = ?0.5
instead of ??(= log
10
0). As shown in Fig-
ure 4, ?SVM+NB? significantly outperformed
?SVM+BL? for low frequency words, and the
number of word types (N(o)) became obviously
greater when o was small. In other words, the
Naive Bayes classifier proposed here could prob-
ably handle many more low frequency words
than the baseline classifier. Therefore, it was
more effective to combine the Naive Bayes clas-
sifier with the SVM classifier rather than the
baseline classifier in order to improve the ro-
bustness of the overall WSD system.
Finally, we constructed a combined model of
all three classifiers, the SVM, Naive Bayes and
baseline classifiers. As shown in Table 3, this
model slightly outperformed the two-classifier
combined models shown in Table 2.
5To be more accurate, F (o) and N(o) are the figures
for words which occurred more than or equal o times and
less than o+ t times, where o+ t is the next point at the
horizontal axis. t was chosen as the smallest integer so
that N(o) would be more than 100.
Table 3: Results of SVM+NB+BL
R P F A T
7) .7079 .7066 .7072 1 10,636
7)=SVM+NB+BL
6 Related Work
As described in Section 1, the goal of this
project was to improve robustness of the WSD
system. One of the promising ways to construct
a robust WSD system is unsupervised learn-
ing as with the EM algorithm (Manning and
Schu?tze, 1999), i.e. training a WSD classifier
from an unlabeled data set. On the other hand,
our approach is to use a machine readable dic-
tionary in addition to a corpus as knowledge
resources for WSD. Notice that we used hyper-
nyms of definition sentences in a dictionary to
train the Naive Bayes classifier, and this pro-
cess worked well for words which did not oc-
cur frequently in the corpus. However, we did
not compare our method and the unsupervised
learning method empirically. This will be one
of our future projects.
Using hypernyms of definition sentences is
similar to using semantic classes derived from a
thesaurus. One of the advantages of our method
is that a thesaurus is not obligatory when word
senses are defined according to a machine read-
able dictionary. Furthermore, our method is
to train the probabilistic model that predicts
a hypernym of a word, while most previous ap-
proaches use semantic classes as features (i.e.,
the condition of the posterior probability in
the case of the Naive Bayes model). In facts,
we also use features associated with semantic
classes derived from the thesaurus, Csent and
(Bcase;Cnoun), as described in Section 2.
Several previous studies have used both a
corpus and a machine readable dictionary for
WSD (Litkowski, 2002; Rigau et al, 1997;
Stevenson and Wilks, 2001). The difference
between those methods and ours is the way
we use information derived from the dictionary
for WSD. Training the probabilistic model that
predicts a hypernym in a dictionary is our own
approach. However, these various methods are
not in competition with our method. In fact,
the robustness of the WSD system would be
even more improved by combining these meth-
ods with that described in this paper.
7 Conclusion
This paper has proposed a method to develop
a robust WSD system. We combined a WSD
classifier obtained by supervised learning for
high frequency words and a classifier using hy-
pernyms in definition sentences in a dictionary
for low frequency words. Experimental results
showed that both recall and applicability were
remarkably improved with our method. In fu-
ture, we plan to investigate the optimum way to
combine these two classifiers or to train a single
probabilistic model using hypernyms in defini-
tion sentences, which is suitable for both high
and low frequency words.
References
EDR. 1995. EDR electronic dictionary technical
guide (second edition). Technical Report TR?045,
Japan Electronic Dictionary Research Institute.
Satoshi Ikehara et al 1997. Nihongo Goi Taikei (in
Japanese). Iwanami Shoten, Publishers.
Hand Li and Jun-ichi Takeuchi. 1997. Using evi-
dence that is both strong and reliable in Japanese
homograph disambiguation. In SIG-NL, Informa-
tion Processing Society of Japan, pages 53?59.
Kenneth C. Litkowski. 2002. Sense information
for disambiguation: Confluence of supervised
and unsupervised methods. In Proceedings of the
SIGLEX/SENSEVAL Workshop on Word Sense
Disambiguation, pages 47?53.
Christopher D. Manning and Hinrich Schu?tze, 1999.
Foundations of Statistical Natural Language Pro-
cessing, chapter 7. MIT Press.
Masaki Murata, Masao Utiyama, Kiyotaka Uchi-
moto, Qing Ma, and Hitoshi Isahara. 2001.
Japanese word sense disambiguation using the
simple bayes and support vector machine meth-
ods. In Proceedings of the SENSEVAL-2, pages
135?138.
Ted Pedersen. 2000. A simple approach to build-
ing ensembles of naive baysian classifiers for
word sense disambiguation. In Proceedings of the
NAACL, pages 63?69.
German Rigau, Jordi Atserias, and Eneko Agirre.
1997. Combining unsupervised lexical knowledge
methods for word sense disambiguation. In Pro-
ceedings of the ACL, pages 48?55.
Bernhard Scho?lkopf. 2000. New support vector al-
gorithms. Neural Computation, 12:1083?1121.
Mark Stevenson and Yorick Wilks. 2001. The inter-
action of knowledge sources in word sense disam-
biguation. Computational Linguistics, 27(3):321?
349.
Hiroya Takamura, Hiroyasu Yamada, Taku Kudoh,
Kaoru Yamamoto, and Yuji Matsumoto. 2001.
Ensembling based on feature space restructuring
with application to WSD. In Proceedings of the
NLPRS, pages 41?48.
Constructing Taxonomy of Numerative Classifiers for Asian Languages
Kiyoaki Shirai
JAIST
kshirai@jaist.ac.jp
Takenobu Tokunaga
Tokyo Inst. of Tech.
take@cl.cs.titech.ac.jp
Chu-Ren Huang
Academia Sinica
churenhuang@gmail.com
Shu-Kai Hsieh
National Taiwan Normal Univ.
shukai@gmail.com
Tzu-Yi Kuo
Academia Sinica
ivykuo@gate.sinica.edu.tw
Virach Sornlertlamvanich
TCL, NICT
virach@tcllab.org
Thatsanee Charoenporn
TCL, NICT
thatsanee@tcllab.org
Abstract
Numerative classifiers are ubiquitous in
many Asian languages. This paper pro-
poses a method to construct a taxonomy
of numerative classifiers based on a noun-
classifier agreement database. The taxon-
omy defines superordinate-subordinate rela-
tion among numerative classifiers and rep-
resents the relations in tree structures. The
experiments to construct taxonomies were
conducted for evaluation by using data from
three different languages: Chinese, Japanese
and Thai. We found that our method was
promising for Chinese and Japanese, but in-
appropriate for Thai. It confirms that there
really is no hierarchy among Thai classifiers.
1 Introduction
Many Asian languages do not mark grammatical
numbers (singular/plural) in noun form, but use nu-
merative classifiers together with numerals instead
when describing the number of nouns. Numerative
classifiers (hereafter ?classifiers?) are used with a
limited group of nouns, in particular material nouns.
In English, for example: ?three pieces of paper?. In
Asian languages these classifiers are ubiquitous and
used with common nouns. Therefore the number of
classifiers is much larger than in Western languages.
An agreement between nouns and classifiers is also
necessary, i.e., a certain noun specifies possible clas-
sifiers. The agreement is determined based on var-
ious aspects of a noun, such as its meaning, shape,
pragmatic aspect and so on.
This paper proposes a method to automati-
cally construct a taxonomy of numerative classi-
fiers for Asian languages. The taxonomy defines
superordinate-subordinate relations between classi-
fiers. For instance, the Japanese classifier ?? (to?)?
is used for counting big animals such as elephants
and tigers, while ?? (hiki)? is used for all animals.
Since ??? can be considered more general than ?
??, ??? is the superordinate classifier of ???, rep-
resented as ???  ??? in this paper. The taxon-
omy represents such superordinate-subordinate rela-
tions between classifiers in the form of a tree struc-
ture. A taxonomy of classifiers would be fundamen-
tal knowledge for natural language processing. In
addition, it will be useful for language learners, be-
cause learning usage of classifiers is rather difficult,
especially for Western language speakers.
We evaluate the proposed method by using the
data of three Asian languages: Chinese, Japanese
and Thai.
2 Noun-classifier agreement database
First, let us introduce usages of classifiers in Asian
languages. In the following examples, ?CL? stands
for classifier.
? Chinese: yi-ju
(CL)
dian-hua
(telephone)
? ? ? a telephone
? Japanese: inu
(dog)
2 hiki
(CL)
? ? ? 2 dogs
? Thai: nakrian
(student)
3 khon
(CL)
? ? ? 3 students
397
As mentioned earlier, the agreement between nouns
and classifiers is observed. For instance, the
Japanese classifier ?hiki? in the above example
agrees with only animals. The agreement is also
found in Chinese and Thai.
The proposed method to construct a classifier tax-
onomy is based on agreement between nouns and
classifiers. First we prepare a collection of pairs
(n, c) of a noun n and a classifier c which agrees
with n for a language. The statistics of our Chinese,
Japanese, and Thai database are summarized in Ta-
ble 1.
Table 1: Noun-classifier agreement database
Chinese Japanese Thai
No. of (n,c) pairs 28,202 9,582 9,618
No. of nouns (type) 10,250 4,624 8,224
No. of CLs (type) 205 331 608
The Japanese database was built by extracting
noun-classifier pairs from a dictionary (Iida, 2004)
which enumerates nouns and their corresponding
classifiers. The Chinese database was derived from
a dictionary (Huang et al, 1997). The Thai database
consists of a mixture of two kinds of noun-classifier
pairs: 8,024 nouns and their corresponding classi-
fiers from a dictionary of a machine translation sys-
tem (CICC, 1995) and 200 from a corpus. The pairs
from the corpus were manually checked for their va-
lidity.
3 Proposed Method
3.1 Extracting superordinate-subordinate
relations of classifiers
We extracted superordinate-subordinate classifier
pairs based on inclusive relations of sets of nouns
agreeing with those classifiers. Suppose that Nk is
a set of nouns that agrees with a classifier ck. If Ni
subsumes Nj (Ni ? Nj), we can estimate that ci
subsumes cj (ci  cj). For instance, in our Japanese
database, the classifier ?? (ten)? agrees with shops
such as ?drug store?, ?kiosk? and ?restaurant?, and
these nouns also agree with ?? (ken)?, since ??? is
a classifier which agrees with any kind of building.
Thus, we can estimate the relation ???  ???.
Given a certain classifier cj , ci satisfying the fol-
lowing two conditions (1) and (2) is considered as a
N
j
N
i
Figure 1: Relation of sets of nouns agreeing with
classifiers
superordinate classifier of cj .
|Ni| > |Nj | (1)
IR(ci, cj) ? Tir
where IR(ci, cj)
def
=
|N
i
?N
j
|
|N
j
|
(2)
Condition (1) requires that a superordinate classifier
agrees with more nouns than a subordinate classifier.
IR(ci, cj) is an inclusion ratio representing to what
extent nouns in Nj are also included in Ni (the ratio
of the light gray area to the area of the small circle
in Figure 1).
Condition (2) means that if IR(ci, cj) is greater
than a certain threshold T
ir
, we estimate a
superordinate-subordinate relation between ci and
cj . The basic idea is that superordinate-subordinate
relations are extracted when Nj is a proper subset
of Ni, i.e. IR(ci, cj) = 1, but this is too strict. In
order to extract more relations, we loosen this condi-
tion such that relations are extracted when IR(ci, cj)
is large enough. If we set Tir lower, more relations
can be acquired, but they may be less reliable.
Table 2: Extraction of superordinate-subordinate re-
lations
Chinese Japanese Thai
T
ir
0.7 0.6 0.6
No. of extracted relations 251 322 239
No. of CLs not in 36 76 395
the extracted relations (18%) (23%) (61%)
Table 2 shows the results of our experiments to
extract superordinate-subordinate relations of classi-
fiers. The threshold T
ir
was determined in an ad hoc
manner for each language. The numbers of extracted
superordinate-subordinate relations are shown in the
second row in the table. Manual inspection of the
sampled relations revealed that many reasonable re-
lations were extracted. The objective evaluation of
these extracted relations will be discussed in 4.2.
398
The third row in Table 2 indicates the numbers of
classifiers which were not included in the extracted
superordinate-subordinate relations with its ratio to
the total number of classifiers in the database in
parentheses. We found that no relation is extracted
for a large number of Thai classifiers.
3.2 Constructing structure
The structure of a taxonomy is constructed based
on a set of superordinate-subordinate relations be-
tween classifiers. Currently we adopt a very naive
approach to construct structures, i.e., starting from
the most superordinate classifiers as roots, we ex-
tend trees downward to less general classifiers by
using the extracted superordinate-subordinate rela-
tions. Note that since there is more than one classi-
fier that does not have any superordinate classifiers,
we will have a set of trees rather than a single tree.
When constructing structures, redundant relations
are ignored in order to make the structures as concise
as possible. A relation is considered redundant if the
relation can be inferred by using other relations and
transitivity of the relations. The formal definition of
redundant relations is given below:
ca  cb is redundant iff ?cm : ca  cm, cm  cb
Statistics of constructed structures for each lan-
guage are shown in Table 3. More than 50 iso-
lated structures (trees) were obtained for Chinese
and Japanese, while more than 100 for Thai. We ob-
tained several large structures, the largest containing
45, 85 and 23 classifiers for Chinese, Japanese and
Thai, respectively. As indicated in the fifth row in
Table 3, however, many structures consisting of only
2 classifiers were also constructed.
Table 3: Construction of structures
Chinese Japanese Thai
No. of structures 52 54 102
No. of CLs in a structure
Average 4.9 6.3 3.3
Maximum 45 85 23
Max. depth of structures 4 3 3
No. of structures with 2 CLs 18 24 54
4 Discussion
In this section, we will discuss the results of our
experiments. First 4.1 discusses appropriateness of
our method for the three languages. Then we eval-
uate our method in more detail. The evaluation of
extracted superordinate-subordinate relations is de-
scribed in 4.2, and the evaluation of structures in 4.3.
4.1 Comparison of different languages
According to the results of our experiments, the
proposed method seems promising for Chinese and
Japanese, but not for Thai. From the Thai data,
no relation was obtained for about 60% of classi-
fiers (Table 2), and many small fragmented struc-
tures were created (Table 3).
This is because of the characteristic that nouns
and classifiers are strongly coupled in Thai, i.e.,
many classifiers agree with only one noun. In our
Thai database, 252 (41.5%) classifiers agree with
only one noun. This means that the overlap between
two noun sets Ni and Nj can be quite small, making
the inclusion ratio IR(ci, cj) very small. Out basic
idea is that we can extract superordinate-subordinate
relations between two classifiers when the overlap of
their corresponding noun sets is large. However, this
assumption does not hold in Thai classifiers. The
above facts suggest that there seems to be no hierar-
chical taxonomy of classifiers in Thai.
4.2 Evaluation of extracted relations
4.2.1 Analysis of Nouns in Nj \ Ni
As explained in 3.1, our method extracts a relation
ci  cj even when Ni does not completely subsume
Nj . We analysed nouns in the relative complement
of Ni in Nj (Nj \Ni), i.e., the dark gray area in Fig-
ure 1. The relation ci  cj implies that all nouns
which are countable with a subordinate classifier cj
are also countable with its superordinate classifier ci,
but there is no guarantee of this for nouns in Nj \Ni,
since we loosened the condition as in (2) by intro-
ducing a threshold.
To see to what extent nouns in Nj \ Ni agree
with ci as well, we manually verified the agreement
of nouns in Nj \ Ni and ci for all extracted rela-
tions ci  cj . The verification was done by native
speakers of each language. Results of the valida-
tion are summarized in Table 4. For Japanese and
Chinese, multiple judges verified the results. When
judgments conflicted, we decided the final decision
by a discussion of two judges for Japanese, and by
majority voting for Chinese. The 4th and 5th rows
399
in Table 4 show the agreement of judgments. The
?Agreement ratio? is the ratio of cases that judg-
ments agree. Since three judges verified nouns for
Chinese, we show the average of the agreement ra-
tios for two judges out of the three. The agreement
ratio and Cohen?s ? is relatively high for Japanese,
but not for Chinese. We found many uncertain cases
for Chinese nouns. For example, ?? (wei)? is a clas-
sifier used when counting people with honorific per-
spective. However, judgement if ??? can modify
nouns such as ?political prisoner? or ?local villain?
is rather uncertain.
Table 4: Analysis of nouns in Nj \ Ni
Chinese Japanese Thai
No. of nouns in N
j
\N
i
1,650 579 43
No. of nouns countable 1,195 241 24
with c
i
as well 72% 42% 56%
No. of judges 3 2 1
Agreement ratio 0.677 0.936 ?
Cohen?s ? 0.484 0.868 ?
Table 4 reveals that a considerable number of
nouns in Nj \ Ni are actually countable with ci,
meaning that our databases do not include noun-
classifier agreement exhaustively.
4.2.2 Reliability of relations ??
Based on the analysis in 4.2.1, we evaluate ex-
tracted superordinate-subordinate relations. We de-
fine the reliability R of the relation ci  cj as
R(ci  cj) =
|Ni ? Nj |+ |NCj,i|
|Nj |
, (3)
where, NCj,i is a subset of Nj \ Ni consisting of
nouns which are manually judged to agree with ci.
We can consider that the more strictly this statement
holds, the more reliable the extracted relations will
be.
Figure 2 shows the relations between the thresh-
old T
ir
and both the number of extracted relations
and their reliability. The horizontal axis indicates
the threshold T
ir
in (2). The bar charts indicate the
number of extracted relations, while the line graphs
indicate the averages of reliability of all extracted re-
lations. Of course, if we set T
ir
lower, we can extract
more relations at the cost of their reliability. How-
ever, even when T
ir
is set to the lowest value, the
averages of reliability are relatively high, i.e. 0.98
(Chinese), 0.91 (Japanese) and 0.99 (Thai). Thus
we can conclude that the extracted superordinate-
subordinate relations are reliable enough.
4.3 Evaluation of structures
As in ordinary ontologies, we will assume that prop-
erties of superordinate classifiers can be inherited to
their subordinate classifiers. In other words, a clas-
sifier taxonomy suggests transitivity of agreement
with nouns over superordinate-subordinate relations
as
c
1
 c
2
? c
2
 c
3
? c
1
 c
3
.
In order to evaluate the structures of our taxonomy,
we verify the validity of transitivity.
First, we extracted all pairs of classifiers having
an ancestor-descendant relation from our classifier
taxonomy. Hereafter we denote ancestor-descendant
pairs of classifiers as (ca, cd), where ca is an ances-
tor and cd an descendant. The path from ca to cd on
the taxonomy can be represented as
c
0
(= ca)  c1  ...  cn(= cd). (4)
We denote a superordinate-subordinate relation de-
rived by transitivity as
?
, such as c
0
?
 cn. Among
all ancestor-descendant relations, we extracted ones
with a path length of more than one, or n > 1
in (4). Then we compare R(ca
?
 cd), the re-
liability of a relation derived by transitivity, with
R(ci  ci+1) (0 ? i < n), the reliability of di-
rect relations in the path from ca to cd. If these are
comparable, we can conclude that transitivity in the
taxonomy is valid.
Table 5 shows the results of the analysis of transi-
tivity. As indicated in the column ?all? in Table 5, 78
and 86 ancestor-descendant pairs (ca, cd) were ex-
tracted from the Chinese and Japanese classifier tax-
onomy, respectively. In contrast, only 6 pairs were
extracted from the Thai taxonomy, since each struc-
ture of the Thai taxonomy is rather small as we al-
ready discussed with Table 3. Thus we have omit-
ted further analysis of Thai. The extracted ancestor-
descendant pairs of classifiers are then classified into
three cases, (A), (B) and (C). Their numbers are
shown in the last three rows in Table 5, where mini
and maxi denote the minimum and maximum of re-
liability among all direct relations R(ci  ci+1) in
the path from ca to cd.
400
Chinese Japanese Thai
0
50
100
150
200
250
300
350
0.9
0.92
0.94
0.96
0.98
1
1.0 0.9 0.8 0.7 irT
# of Rel. Ave. of R
0
50
100
150
200
250
300
350
0.9
0.92
0.94
0.96
0.98
1
1.0 0.9 0.8 0.7 0.6 irT
# of Rel. Ave. of R
0
50
100
150
200
250
300
350
0.9
0.92
0.94
0.96
0.98
1
1.0 0.9 0.8 0.7 0.6 irT
# of Rel. Ave. of R
Figure 2: Reliability of extracted superordinate-subordinate relations
Table 5: Verification of transitivity
Chinese Japanese
all direct indirect all direct indirect
No. of (c
a
, c
d
) 78 58 20 86 55 31
Average of R(c
a
?
c
d
) 0.88 0.98 0.61 0.77 0.93 0.48
(A) min
i
> R(c
a
?
c
d
) 16 (21%) 4 (7%) 12 (60%) 24 (28%) 3 (5%) 21 (68%)
(B) min
i
? R(c
a
?
c
d
) < max
i
39 (50%) 34 (59%) 5 (25%) 27 (31%) 24 (44%) 3 (9%)
(C) max
i
? R(c
a
?
c
d
) 23 (29%) 20 (34%) 3 (15%) 35 (41%) 28 (51%) 7 (23%)
In case (A), reliability of a relation derived by
transitivity, R(ca
?
 cd), is less than that of any di-
rect relations, R(ci  ci+1). In case (B), reliability
of a transitive relation is comparable with that of di-
rect relations, i.e. R(ca
?
 cd) is greater or equal to
mini and less than maxi. In case (C), the transitive
relation is more reliable than direct relations.
The average of the reliability of ca
?
 cd is rela-
tively high, 0.88 for Chinese and 0.77 for Japanese.
We also found that more than 70% of derived rela-
tions (case (B) and case (C)) are comparable to or
greater than direct relations. The above facts indi-
cate transitivity on our structural taxonomy is valid
to some degree.
From a different point of view, we divided pairs
of (ca, cd) into two other cases, ?direct? and ?indi-
rect? as shown in the columns of Table 5. The ?di-
rect? case includes the relations which are also ex-
tracted by our method. Note that such relations are
discarded as redundant ones. On the other hand, the
?indirect? case includes the relations which can not
be extracted from the database but only inferred by
using transitivity on the taxonomy. That is, they are
truly new relations. In order to calculate reliability
of ?indirect? cases, we performed additional manual
validation of nouns in Nd\Na.
However, the average of R(ca
?
 cd) in ?in-
direct? cases is not so high for both Chinese and
Japanese, as a large amount of pairs are classi-
fied into case (A). Thus it is not effective to infer
new superordinate-subordinate relations by transi-
tivity. Since we currently only adopted a very naive
method to construct a classifier taxonomy, more so-
phisticated methods should be explored in order to
prevent inferring irrelevant relations.
5 Related Work
Bond (2000) proposed a method to choose an appro-
priate classifier for a noun by referring its seman-
tic class. This method is implemented in a sentence
generation module of a machine translation system.
Similar attempts to generate both Japanese and Ko-
rean classifiers were also reported (Paik and Bond,
2001). Bender and Siegel (2004) implemented a
HPSG that handles several intricate structures in-
cluding Japanese classifiers. Matsumoto (1993)
reported his close analysis of Japanese classi-
fiers based on prototype semantics. Sornlertlam-
vanich (1994) presented an algorithm for selecting
an adequate classifier for a noun by using a cor-
pus. Their research can be regarded as a method to
construct a noun-classifier agreement database au-
401
tomatically from corpora. We used databases de-
rived from dictionaries except for a small number
of noun-classifier pairs in Thai, because we believe
dictionaries provide more reliable and stable infor-
mation than corpora, and in addition they were avail-
able and on hand. Note that we are not concerned
with frequencies of noun-classifier coocurrence in
this study. Huang (1998) proposed a method to
construct a noun taxonomy based on noun-classifier
agreement that is very similar to ours, but aims at
developing a taxonomy for nouns rather than one for
classifiers. There has not been very much work on
building resources concerning noun-classifier agree-
ment. To our knowledge, this is the first attempt to
construct a classifier taxonomy.
6 Conclusion
This paper proposed a method to construct a tax-
onomy of numerative classifiers based on a noun-
classifier agreement database. First, superordinate-
subordinate relations of two classifiers are extracted
by measuring the overlap of two sets of nouns agree-
ing with each classifier. Then these relations are
used as building blocks to build a taxonomy of
tree structures. We conducted experiments to build
classifier taxonomies for three languages: Chinese,
Japanese and Thai. The effectiveness of our method
was evaluated by measuring reliability of extracted
relations, and verifying validity of transitivity in the
taxonomy. We found that extracted relations are re-
liable, and the transitivity in the taxonomy relatively
valid. Relations inferred by transitivity, however, are
less reliable than those directly derived from noun-
classifier agreement.
Future work includes investigating a way to en-
large classifier taxonomies. Currently, not all clas-
sifiers are included in our taxonomy, and it con-
sists of a set of fragmented structures. A more so-
phisticated method to build a large taxonomy in-
cluding more classifiers should be examined. Our
method should also be refined in order to make
superordinate-subordinate relations inferred by the
transitivity more reliable. We are now investigat-
ing a stepwise method to construct taxonomies that
prefers more reliable relations, i.e. an initial tax-
onomy is built with a small number of highly reli-
able relations, and is then expanded with less reli-
able ones.
Acknowledgment
This research was carried out through financial sup-
port provided under the NEDO International Joint
Research Grant Program (NEDO Grant).
References
Emily M. Bender and Melanie Siegel. 2004. Imple-
menting the syntax of Japanese numeral classifiers. In
Proceedings of the the First International Joint Con-
ference on Natural Language Processing, pages 398?
405.
Francis Bond and Kyonghee Paik. 2000. Reusing an on-
tology to generate numeral classifiers. In Proceedings
of the COLING, pages 90?96.
CICC. 1995. CICC Thai basic dictionary. (developed by
Center of the International Cooperation for Computer-
ization).
Chu-Ren Huang, Keh-Jian Chen, and Chin-Hsiung Lai,
editors. 1997. Mandarin Daily News Dictionary of
Measure Words. Mandarin Daily News Publisher.
Chu-Ren Huang, Keh-jiann Chen, and Zhao-ming Gao.
1998. Noun class extraction from a corpus-based col-
location dictionary: An integration of computational
and qualitative approaches. In Quantitative and Com-
putational Studies of Chinese Linguistics, pages 339?
352.
Asako Iida. 2004. Kazoekata no Ziten (Dictionary for
counting things). Sho?gakukan. (in Japanese).
Yo Matsumoto. 1993. The Japanese numeral classifiers:
A study of semantic categories and lexical organiza-
tion. Linguistics, 31:667?713.
Kyonghee Paik and Francis Bond. 2001. Multilin-
gual generation of numeral classifiers using a common
ontology. In Proceedings of the 19th International
Conference on Computer Processing of Oriental Lan-
guages (ICCPOL), pages 141?147.
Virach Sornlertlamvanich, Wantanee Pantachat, and
Surapant Meknavin. 1994. Classifier assignment by
corpus-based approach. In Proceedings of the COL-
ING, pages 556?561.
402
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 771?778,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Compiling a Lexicon of Cooking Actions for Animation Generation
Kiyoaki Shirai Hiroshi Ookawa
Japan Advanced Institute of Science and Technology
1-1, Asahidai, Nomi, 923-1292, Ishikawa, Japan
{kshirai,h-ookawa}@jaist.ac.jp
Abstract
This paper describes a system which gen-
erates animations for cooking actions in
recipes, to help people understand recipes
written in Japanese. The major goal of this
research is to increase the scalability of the
system, i.e., to develop a system which can
handle various kinds of cooking actions.
We designed and compiled the lexicon of
cooking actions required for the animation
generation system. The lexicon includes
the action plan used for animation genera-
tion, and the information about ingredients
upon which the cooking action is taken.
Preliminary evaluation shows that our lex-
icon contains most of the cooking actions
that appear in Japanese recipes. We also
discuss how to handle linguistic expres-
sions in recipes, which are not included
in the lexicon, in order to generate anima-
tions for them.
1 Introduction
The ability to visualize procedures or instruc-
tions is important for understanding documents
that guide or instruct us, such as computer manuals
or cooking recipes. We can understand such docu-
ments more easily by seeing corresponding figures
or animations. Several researchers have studied
the visualization of documents (Coyne and Sproat,
2001), including the generation of animation (An-
dre and Rist, 1996; Towns et al, 1998). Such ani-
mation systems help people to understand instruc-
tions in documents. Among the various types of
documents, this research focuses on the visualiza-
tion of cooking recipes.
Many studies related to the analysis or genera-
tion of cooking recipes have been done (Adachi,
1997; Webber and Eugenio, 1990; Hayashi et al,
2003; Shibata et al, 2003). Especially, several
researchers have proposed animation generation
systems in the cooking domain. Karlin, for exam-
ple, developed SEAFACT (Semantic Analysis For
the Animation of Cooking Tasks), which analyzed
verbal modifiers to determine several features of
an action, such as the aspectual category of an
event, the number of repetitions, duration, speed,
and so on (Karlin, 1988). Uematsu developed
?Captain Cook,? which generated animations from
cooking recipes written in Japanese (Uematsu et
al., 2001). However, these previous works did
not mention the scalability of the systems. There
are many linguistic expressions in the cooking do-
main, but it is uncertain to what extent these sys-
tems can convert them to animations.
This paper also aims at developing a system to
generate animations from cooking recipes written
in Japanese. We especially focused on increasing
the variety of recipes that could be accepted. After
presenting an overview of our proposed system in
Subsections 2.1 and 2.2, the more concrete goals
of this paper will be described in Subsection 2.3.
2 Proposed System
2.1 Overview
The overview of our animation generation sys-
tem is as follows. The system displays a cooking
recipe in a browser. As in a typical recipe, cooking
instructions are displayed step by step, and sen-
tences or phrases representing a cooking action in
the recipe are highlighted. When a user does not
understand a certain cooking action, he/she can
click the highlighted sentence/phrase. Then the
system will show the corresponding animation to
help the user understand the cooking instruction.
Note that the system does not show all proce-
dures in a recipe like a movie, but generates an
animation of a single action on demand. Further-
more, we do not aim at the reproduction of recipe
sentences in detail. Especially, we will not prepare
object data for many different kinds of ingredients.
For example, suppose that the system has object
data for a mackerel, but not for a sardine. When
a user clicks the sentence ?fillet a sardine? to see
the animation, the system will show how to fillet a
?mackerel? instead of ?sardine?, with a note indi-
cating that the ingredient is different. We believe
771
Animation 
Generator 
Action Plan 
Animation 
Lexicon of Cooking Actions 
(ex.  chop an onion finely) 
Input sentence 
Action Matcher Basic Action 1
    ``fry''
Basic Action 2 
    ``chop finely'' 
action plan 
action plan 
Figure 1: System Architecture
that the user will be more interested in ?how to fil-
let? than in the specific ingredient to be filleted.
In other words, the animation of the action will be
equally helpful as long as the ingredients are simi-
lar. Thus we will not make a great effort to prepare
animations for many kinds of ingredients. Instead,
we will focus on producing the various kinds of
cooking actions, to support users in understanding
cooking instructions in recipes.
2.2 System Architecture
Figure 1 illustrates the architecture of the proposed
system. First, we prepare the lexicon of cooking
actions. This is the collection of cooking actions
such as ?fry?, ?chop finely?, etc. The lexicon has
enough knowledge to generate an animation for
each cooking action. Figure 2 shows an exam-
ple of an entry in the lexicon. In the figure, ?ex-
pression? is a linguistic expression for the action;
?action plan? is a sequence of action primitives,
which are the minimum action units for animation
generation. Roughly speaking, the action plan in
Figure 2 represents a series of primitive actions,
such as cutting and rotating an ingredient, for the
basic action ?chop finely?. The system will gen-
erate an animation according to the action plan in
the lexicon. Other features, ?ingredient examples?
and ?ingredient requirement?, will be explained
later.
The process of generating an animation is as
follows. First, as shown in Figure 1, the system
compares an input sentence and expression of the
entries in the lexicon of cooking actions, and finds
the appropriate cooking action. This is done by the
module ?Action Matcher?. Then, the system ex-
tracts an action plan from the lexicon and passes it
to the ?Animation Generator? module. Finally An-
imation Generator interprets the action plan and
produces the animation.
2.3 Goal
The major goals of this paper are summarized as
follows:
G1. Construct a large-scale lexicon of cooking ac-
tions
In order to generate animations for various
kinds of cooking actions, we must prepare a
lexicon containing many basic actions.
G2. Handle a variety of linguistic expressions
Various linguistic expressions for cooking ac-
tions may occur in recipes. It is not realistic
to include all possible expressions in the lex-
icon. Therefore, when a linguistic expression
in an input sentence is not included in the lex-
icon, the system should calculate the similar-
ity between it and the basic action in the lex-
icon, and find an equivalent or almost similar
action.
G3. Include information about acceptable ingre-
dients in the lexicon
Even though linguistic expressions are the
same, cooking actions may be different ac-
cording to the ingredient upon which the ac-
tion is taken. For example, ?cut into fine
strips? may stand for several different cook-
ing actions. That is, the action of ?cut
cucumber into fine strips? may be differ-
ent than ?cut cabbage into fine strips?, be-
cause the shapes of cucumber and cabbage
are rather different. Therefore, each entry in
the lexicon should include information about
what kinds of ingredients are acceptable for a
certain cooking action.
As mentioned earlier, the main goal of this re-
search is to increase the scalability of the system,
i.e., to develop an animation generation system
that can handle various cooking actions. We hope
that this can be accomplished through goals G1
and G2.
In the rest of this paper, Section 3 describes
how to define the set of actions to be compiled
into the lexicon of cooking actions. This concerns
goal G1. Section 4 explains two major features
in the lexicon, ?action plan? and ?ingredient re-
quirement?. The feature ingredient requirement is
772
Basic Action 2
expression ???????? (chop finely)
action plan cut(ingredient,utensil,location, 2)
rotate(ingredient,location, x, 90)
cut(ingredient,utensil,location,20)
rotate(ingredient,location, z, 90)
cut2(ingredient,utensil,location, 10)
cut(ingredient,utensil,location, 20)
ingredient examples ??? (okra),???? (shiitake mushroom)
ingredient requirement kind=vegetable|mushroom
Figure 2: Example of an Entry in the Lexicon of Cooking Actions
related to goal G3. Section 5 reports a preliminary
survey to construct the module Action Matcher in
Figure 1, which is related to goal G2. Finally, Sec-
tion 6 concludes the paper.
3 Defining the Set of Basic Actions
In this and the following sections, we will explain
how to construct the lexicon of cooking actions.
The first step in constructing the lexicon is to de-
fine the set of basic actions. As mentioned earlier
(goal G1 in Subsection 2.3), a large-scale lexicon
is required for our system. Therefore, the set of ba-
sic actions should include various kinds of cook-
ing actions.
3.1 Procedure
We referred to three cooking textbooks or man-
uals (Atsuta, 2004; Fujino, 2003; Takashiro and
Kenmizaki, 2004) in Japanese to define the set of
basic actions. These books explain the fundamen-
tal cooking operations with pictures, e.g., how to
cut, roast, or remove skins/seeds for various kinds
of ingredients. We extracted the cooking opera-
tions explained in these three textbooks, and de-
fined them as the basic actions for the lexicon. In
other words, we defined the basic actions accord-
ing to the cooking textbooks. The reasons why we
used the cooking manuals as the standard for the
basic actions are summarized as follows:
1. The aim of cooking manuals used here is to
comprehensively explain basic cooking oper-
ations. Therefore, we expect that we can col-
lect an exhaustive set of basic actions in the
cooking domain.
2. Cooking manuals are for beginners. The
aim of animation generation system is to
help people, especially novices, to under-
stand cooking actions in recipes. The lexicon
of cooking actions based on the cooking text-
books includes many cooking operations that
novices may not know well.
3. The definition of basic actions does not de-
pend on the module Animation Generator.
One of the standards for the definition of ba-
sic actions is animations generated by the
system. That is, we can define basic cook-
ing actions so that each cooking action cor-
responds to an unique animation. This ap-
proach seems to be reasonable for an anima-
tion generation system; however, it depends
on the module Animation Generator in Fig-
ure 1. Many kinds of rendering engines are
now available to generate animations. There-
fore, Animation Generator can be imple-
mented in various ways. When changing the
rendering engine used in Animation Genera-
tor, the lexicon of cooking actions must also
be changed. So we decided that it would not
be desirable to define the set of basic actions
according to their corresponding animations.
In our framework, the definition of basic ac-
tions in the lexicon does not depend on Ani-
mation Generator. This enables us to use any
kind of rendering engine to produce an ani-
mation. For example, when we use a poor en-
gine and want to design the system so that it
generates the same animation for two or more
basic actions, we just describe the same ac-
tion plan for these actions.
We manually excerpted 267 basic actions from
three cooking textbooks. Although it is just a col-
lection of basic actions, we refer it as the initial
773
Table 1: Examples of Basic Actions
expression ingredient examples
?????? (fillet) ?? (mackerel)
???? (boil)
?? (boil)
????????
(cut into a comb shape)
??? (tomato),
????? (potato)
????????
(cut into a comb shape)
???? (pumpkin)
????????
(cut into a comb shape)
??
(turnip)
lexicon of cooking actions. Table 1 illustrates sev-
eral examples of basic actions in the initial lexi-
con. In the cooking manuals, every cooking op-
eration is illustrated with pictures. ?Ingredient ex-
amples? indicates ingredients in pictures used to
explain cooking actions.
3.2 Preliminary Evaluation
A preliminary experiment was conducted to eval-
uate the scalability of our initial lexicon of ba-
sic actions. The aim of this experiment was to
check how many cooking actions appearing in real
recipes are included in the initial lexicon.
First, we collected 200 recipes which are avail-
able on web pages 1. We refer to this recipe corpus
as R
a
hereafter. Next, we analyzed the sentences
in R
a
and automatically extracted verbal phrases
representing cooking actions. We used JUMAN 2
for word segmentation and part-of-speech tagging,
and KNP 3 for syntactic analysis. Finally, we
manually checked whether each extracted verbal
phrase could be matched to one of the basic ac-
tions in the initial lexicon.
Table 2 (A) shows the result of our survey. The
number of basic actions was 267 (a). Among these
actions, 145 (54.3%) actions occurred in R
a
(a1).
About half of the actions in the initial lexicon did
not occur in the recipe corpus. We guessed that
this was because the size of the recipe corpus was
not very large.
The number of verbal phrases in R
a
was 3977
(b). We classified them into the following five
cases: (b1) the verbal phrase corresponded with
one of the basic actions in the initial lexicon, and
1http://www.bob-an.com/
2http://www.kc.t.u-tokyo.ac.jp/
nl-resource/juman.html
3http://www.kc.t.u-tokyo.ac.jp/
nl-resource/knp.html
its linguistic expression was the same as one in the
lexicon; (b2) the verbal phrase corresponded with
a basic action, but its linguistic expression differed
from one in the lexicon; (b3) no corresponding ba-
sic action was found in the initial lexicon, (b4) the
extracted phrase was not a verbal phrase, caused
by error in analysis, (b5) the verbal phrase did not
stand for a cooking action. Note that the cases in
which verbal phrases should be converted to ani-
mations were (b1), (b2) and (b3). The numbers in
parentheses (...) indicate the ratio of each case to
the total number of verbal phrases, while numbers
in square brackets [...] indicate a ratio of each case
to the total number of (b1), (b2) and (b3).
We expected that the verbal phrases in (b1) and
(b2) could be handled by our animation generation
system because the initial lexicon contained the
corresponding basic actions. On the other hand,
our system cannot generate animations for verbal
phrases in (b3), which was 42.3% of the verbal
phrases our system should handle. Thus the appli-
cability of the initial lexicon was poor.
3.3 Adding Basic Actions from Recipe
Corpus
We have examined what kinds of verbal phrases
were in (b3). We found that there were many gen-
eral verbs, such as ???? (add)?, ???? (put
in)?, ???? (heat)?, ???? (attach)?, ???
? (put on)?, etc. Such general actions were not
included in the initial lexicon, because we con-
structed it by extracting basic actions from cook-
ing textbooks, and such general actions are not ex-
plained in these books.
In order to increase the scalability of the lexicon
of cooking actions, we selected verbs satisfying
the following conditions: (1) no corresponding ba-
sic action was found in the lexicon for a verb; (2)
a verb occurred more than 10 times in R
a
. In all,
31 verbs were found and added to the lexicon as
new basic actions. It is undesirable to define basic
actions in this way, because the lexicon may then
depend on a particular recipe corpus. However, we
believe that the new basic actions are very general,
and can be regarded as almost independent of with
the corpus from which they were extracted.
In order to evaluate the new lexicon, we pre-
pared another 50 cooking recipes (R
b
hereafter).
Then we classified the verbal phrases in R
b
in
the same way as in Subsection 3.2. The results
are shown in Table 2 (B). Notice that the ratio
774
Table 2: Result of Preliminary Evaluation
(A) Survey on R
a
(a) # of basic actions 267
(a1) basic actions occurred in R
a
145 (54.3%)
(b) # of verbal phrases 3977
(b1) basic action(same) 974 (24.5%) [28.0%]
(b2) basic action(dif.) 1031 (25.9%) [29.7%]
(b3) not basic action 1469 (36.9%) [42.3%]
(b4) analysis error 180 ( 4.5%)
(b5) not cooking action 323 ( 8.1%)
(B) Survey on R
b
(a) 298
(a1) 106 (35.6%)
(b) 959
(b1) 521 (54.3%) [62.2%]
(b2) 262 (27.3%) [31.3%]
(b3) 55 ( 5.7%) [6.6%]
(b4) 45 ( 4.7%)
(b5) 76 ( 7.9%)
of the number of verbal phrases contained in the
lexicon to the total number of target verb phrases
was 94.5% ((b1)62.2% + (b2)31.3%). This is
much greater than the ratio in Table 2 (A) (57.7%).
Therefore, although the size of test corpus is small,
we hope that the scalability of our lexicon is large
enough to generate animations for most of the ver-
bal phrases in cooking recipes.
4 Compilation of the Lexicon of Basic
Actions
After defining the set of basic actions for the lexi-
con, the information of each basic action must be
described. As shown in Figure 2, the main fea-
tures in our lexicon are expression, action plan,
ingredient examples and ingredient requirement.
The term expression stands for linguistic expres-
sions of basic actions, while ingredient examples
stands for examples of ingredients described in the
cooking manuals we referred to when defining the
set of basic actions. As shown in Table 1, these
two features have already been included in the ini-
tial lexicon created by the procedure in Section 3.
This section describes the compilation of the rest
of the features: action plan in Subsection 4.1 and
ingredient requirement in Subsection 4.2.
4.1 Action Plan
For each basic action in the lexicon, the action
plan to generate the corresponding animation is
described. Action plan is the sequence of action
primitives as shown in Figure 2. Of the 298 basic
actions in the lexicon, we have currently described
action plans for only 80 actions. Most of them are
actions to cut something.
We have also started to develop Animation Gen-
erator (see Figure 1), which is the module that in-
terprets action plans and generates animations. We
Figure 3: Snapshot of Generated Animation
used VRML for animation generation. Figure 3
is a snapshot of the animation for the basic ac-
tion ????????? (chop finely)? generated
by our system.
Our current focus has been on the design and
development of the lexicon of cooking actions,
rather than on animation generation. Implementa-
tion of the complete Animation Generator as well
as a description of the action plans for all basic
actions in the lexicon are important future works.
4.2 Ingredient Requirement
Several basic actions have the same expression in
our lexicon. For instance, in Figure 1, there are
three basic actions represented by the same lin-
guistic expression ????????? (cut into
a comb shape)?. These three actions stand for dif-
ferent cooking actions. The first one stands for the
action used to cut something like a ?tomato? or
?potato? into a comb shape. The second stands for
the following sequence of actions: first cut some-
thing in half, remove its core or seeds, and cut it
into a comb shape. This action is taken on pump-
kin, for instance. The third action represents the
cooking action for ?turnip?: remove the leaves of
the turnip and cut it into a comb shape. In other
words, there are different ways to cut different in-
775
gredients into a comb shape. Differences among
these actions depend on what kinds of ingredients
are to be cut.
As described in Section 2.2, the module Action
Matcher accepts a sentence or phrase for which a
user wants to see the animation, then finds a cor-
responding basic action from the lexicon. In or-
der to find an appropriate basic action for a recipe
sentence, the lexicon of cooking actions should in-
clude information about what kinds of ingredients
are acceptable for each basic action. Note that the
judgment as to whether an ingredient is suitable
or not highly depends on its features such as kind,
shape, and components (seed, peel etc.) of the in-
gredient. Therefore, the lexicon should include in-
formation about what features of the ingredients
must be operated upon by the basic actions.
For the above reason, ingredient requirement
was introduced in the lexicon of cooking actions.
In this field, we manually describe the required
features of ingredients for each basic action. Fig-
ure 4 illustrates the three basic actions of ??
?????? (chop into a comb shape) in the
lexicon 4. The basic action a1, ?kind=vegetable,
shape=sphere? in ingredient requirement, means
that only a vegetable whose shape is spherical is
acceptable as an ingredient for this cooking action.
On the other hand, for the basic action a2, only a
vegetable whose shape is spherical and contain-
ing seeds is acceptable. For a3, ?instance=??
(turnip)? means that only a turnip is suitable for
this action. In our lexicon, such specific cooking
actions are also included when the reference cook-
books illustrate special cooking actions for certain
ingredients. In this case, a cookbook illustrates
cutting a turnip into a comb shape in a different
way than for other ingredients.
4.2.1 Feature Set of Ingredient Requirement
Here are all the attributes and possible values
prepared for the ingredient requirement field:
? kind
This attribute specifies kinds of ingredients.
The possible values are:
vegetable, mushroom, fruit, meat,
fish, shellfish, seafood, condiment
?Seafood? means seafood other than fish or
shellfish, such as ?? (squid), ??? (cod
roe) and so on.
4action plan is omitted in Figure 4.
? veg
This attribute specifies subtypes of veg-
etables. Possible values for this attribute
are ?green?, ?root? and ?layer?. ?Green?
stands for green vegetables such as ???
?? (spinach) and ?? (Chinese cabbage).
?Root? stands for root vegetables such as
????? (potato) and ??? (burdock).
?Layer? stands for vegetables consisting of
layers of edible leaves such as ??? (let-
tuce) and???? (cabbage).
? shape
This attribute specifies shapes of ingredients.
The possible values are:
sphere, stick, cube, oval, plate, filiform
? peel, seed, core
These attributes specify various components
of ingredients. Values are always 1. For ex-
ample, ?peel=1? stands for ingredients with
peel.
? instance
This specifies a certain ingredient, as shown
in basic action a3 in Figure 4.
The information about ingredient requirements
was added for 186 basic actions out of the 298 ac-
tions in the lexicon. No requirement was needed
for the other actions, i.e., these actions accept any
kind of ingredients.
4.2.2 Lexicon of Ingredients
In addition to the lexicon of cooking actions, the
lexicon of ingredients is also required for our sys-
tem. It includes ingredients and their features such
as kind, shape and components. We believe that
this is domain-specific knowledge for the cooking
domain. Thesauri or other general-purpose lan-
guage resources would not provide such informa-
tion. Therefore, we newly compiled the lexicon
of ingredients, which consists of only those ingre-
dients appearing in the ingredients example in the
lexicon of cooking actions. The number of ingre-
dients included in the lexicon is 93. For each entry,
features of the ingredient are described. The fea-
ture set used for this lexicon is the same as that
for the ingredient requirement described in 4.2.1,
except for the ?instance? attribute.
776
Basic Action a1
expression ???????? (cut into a comb shape)
ingredient examples ??? (tomato),????? (potato)
ingredient requirement kind=vegetable, shape=sphere
Basic Action a2
expression ???????? (cut into a comb shape)
ingredient examples ???? (pumpkin)
ingredient requirement kind=vegetable, shape=sphere, seed=1
Basic Action a3
expression ???????? (cut into a comb shape)
ingredient examples ?? (turnip)
ingredient requirement instance=?? (turnip)
Figure 4: Three Basic Actions of ????????? (cut into a comb shape)?
The current lexicon of ingredients is too small.
Only 93 ingredients are included. A larger lexicon
is required to handle various recipe sentences. In
order to enlarge the lexicon of ingredients, we will
investigate a method for the automatically acqui-
sition of new ingredients with their features from
a collection of recipe documents.
5 Matching between Actions in a Recipe
and the Lexicon
Action Matcher in Figure 1 is the module which
accepts a recipe sentence and finds a basic action
corresponding to it from the lexicon. One of the
biggest difficulties in developing this module is
that linguistic expressions in a recipe may differ
from those in the lexicon. So we have to consider
a flexible matching algorithm between them.
To construct Action Matcher, we refer to the
verbal phrases classified in (b2) in Table 2. Note
that the linguistic expressions of these verbal
phrases are inconsistent with the expressions in the
lexicon. We examined the major causes of incon-
sistency for these verbal phrases. In this paper, we
will report the result of our analysis, and suggest
some possible ways to find the equivalent action
even when the linguistic expressions in a recipe
and the lexicon are different. The realization of
Action Matcher still remains as future work.
Figure 5 shows some examples of observed in-
consistency in linguistic expressions. In Figure 5,
the left hand side represents verbal phrases in
recipes, while the right hand side represents ex-
pressions in the lexicon of cooking actions. A
slash indicates word segmentation. Causes of in-
consistency in linguistic expressions are classified
as follows:
? Inconsistency in word segmentation
Word segmentation of verbal phrases in
recipes, as automatically given by a morpho-
logical analyzer, is different from one of the
basic actions in the lexicon, as shown in Fig-
ure 5 (a).
In order to succeed in matching, we need an
operation to concatenate two or more mor-
phemes in a phrase or to divide a morpheme
into to two or more, then try to check the
equivalence of both expressions.
? Inconsistency in case fillers
Verbs in a recipe and the lexicon agree, but
their case fillers are different. For instance,
in Figure 5 (b), the verb ??? (sprinkle)? is
the same, but the accusative case fillers ???
? (chili)? and ?? (salt)? are different. In this
case, we can regard both as representing the
same action: to sprinkle a kind of condiment.
In this case, the lexicon of ingredients (see
4.2.2) would be helpful for matching. That
is, if both ??? (chili) and ? (salt) have
the same feature ?kind=condiment? in the
lexicon of ingredients, we can judge that
the phrase ????/?/?? (sprinkle chili)?
corresponds to the basic action ??/?/??
(sprinkle salt)?.
? Inconsistency in verbs
Disagreement between verbs in a recipe and
the lexicon is one of the major causes of in-
consistency. See Figure 5 (c), for instance.
777
Expressions in Recipes Expressions in Lexicon
(a) ??
(divide)
/???
(loosen)
? ? ?break (egg) ?????
(break)
? ? ?break (egg)
(b) ???
(chili)
/ ?
(ACC)
/ ??
(sprinkle)
? ? ?sprinkle chili ?
(salt)
/ ?
(ACC)
/ ??
(sprinkle)
? ? ?sprinkle salt
(c) ???
(Spewing sand)
/ ?
(ACC)
/??
(do)
? ? ?make (shellfish)
spew out sand
??
(salt water)
/ ?
(LOC)
/???
(dip)
? ? ?dip it into
salt water
Figure 5: Inconsistency in Linguistic Expressions
These two phrases represent the same ac-
tion 5, but the linguistic expressions are to-
tally different.
In this case, the matching between them is
rather difficult. One solution would be to de-
scribe all equivalent expressions for each ac-
tion in the lexicon. Since it is not realistic to
list equivalent expressions exhaustively, how-
ever, we want to automatically collect pairs
of equivalent expressions from a large recipe
corpus.
6 Conclusion
In this paper, we have described the basic idea for
a system to generate animations for cooking ac-
tions in recipes. Although the system is not yet
complete and much work still remains to be done,
the main contribution of this paper is to show the
direction for improving the scalability of the sys-
tem. First, we designed a lexicon of cooking ac-
tions including information about action plans and
ingredient requirements, which are needed to gen-
erate the appropriate cooking animations. We also
showed that our lexicon covers most of the cook-
ing actions appearing in recipes. Furthermore, we
analyzed the recipe corpus and investigated how
to match actions in a recipe to the corresponding
basic action in the lexicon, even when they have
different linguistic expressions. Such a flexible
matching method would also increase the scala-
bility of the system.
References
Hisahiro Adachi. 1997. GCD: A generation method
of cooking definitions based on similarity between
a couple of recipes. In Proceedings of the Natural
Language Processing Pacific Rim Symposium, pages
135?140.
5Note that it is required to dip shellfish into salt water in
order to make it spew out sand.
Elisabeth Andre and Thomas Rist. 1996. Coping
with temporal constraints in multimedia presenta-
tion planning. In Proceedings of the National Con-
ference on Artificial Intelligence, pages 142?147.
Yoko Atsuta. 2004. How to cut vegetables (in
Japanese). Syu?eisha.
Bob Coyne and Richard Sproat. 2001. WordsEye: An
automatic text-to-scene conversion system. In Pro-
ceedings of the SIGGRAPH, pages 487?496.
Yoshiko Fujino. 2003. New Fundamental Cooking (in
Japanese). SS Communications.
Eri Hayashi, Suguru Yoshioka, and Satoshi Tojo. 2003.
Automatic generation of event structure for Japanese
cooking recipes (in Japanese). Journal of Natural
Language Processing, 10(2):3?17.
Robin F. Karlin. 1988. Defining the semantics of ver-
bal modifiers in the domain of cooking tasks. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, pages 61?67.
Tomohide Shibata, Daisuke Kawahara, Masashi
Okamoto, Sadao Kurohashi, and Toyoaki Nishida.
2003. Structural analysis of instruction utterances.
In Proceedings of the Seventh International Con-
ference on Knowledge-Based Intelligent Information
and Engineering Systems (KES2003), pages 1054?
1061.
Junko Takashiro and Satomi Kenmizaki. 2004.
Standard Cooking: Fundamentals of Cooking (in
Japanese). Sho?gakukan.
Stuart G. Towns, Charles B. Callaway, and James C.
Lester. 1998. Generating coordinated natural lan-
guage and 3D animations for complex spatial expla-
nations. In Proceedings of the National Conference
on Artificial Intelligence, pages 112?119.
Hideki Uematsu, Akira Shimazu, and Manabu Oku-
mura. 2001. Generation of 3D CG animations
from recipe sentences. In Proceedings of the Nat-
ural Language Processing Pacific Rim Symposium,
pages 461?466.
Bonnie Lynn Webber and Barbara Di Eugenio. 1990.
Free adjuncts in natural language instructions. In
Proceedings of the International Conference on
Computational Linguistics, pages 395?400.
778
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 827?834,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Infrastructure for standardization of Asian language resources
Tokunaga Takenobu
Tokyo Inst. of Tech.
Virach Sornlertlamvanich
TCL, NICT
Thatsanee Charoenporn
TCL, NICT
Nicoletta Calzolari
ILC/CNR
Monica Monachini
ILC/CNR
Claudia Soria
ILC/CNR
Chu-Ren Huang
Academia Sinica
Xia YingJu
Fujitsu R&D Center
Yu Hao
Fujitsu R&D Center
Laurent Prevot
Academia Sinica
Shirai Kiyoaki
JAIST
Abstract
As an area of great linguistic and cul-
tural diversity, Asian language resources
have received much less attention than
their western counterparts. Creating a
common standard for Asian language re-
sources that is compatible with an interna-
tional standard has at least three strong ad-
vantages: to increase the competitive edge
of Asian countries, to bring Asian coun-
tries to closer to their western counter-
parts, and to bring more cohesion among
Asian countries. To achieve this goal, we
have launched a two year project to create
a common standard for Asian language re-
sources. The project is comprised of four
research items, (1) building a description
framework of lexical entries, (2) building
sample lexicons, (3) building an upper-
layer ontology and (4) evaluating the pro-
posed framework through an application.
This paper outlines the project in terms of
its aim and approach.
1 Introduction
There is a long history of creating a standard
for western language resources. The human
language technology (HLT) society in Europe
has been particularly zealous for the standardiza-
tion, making a series of attempts such as EA-
GLES1, PAROLE/SIMPLE (Lenci et al, 2000),
ISLE/MILE (Calzolari et al, 2003) and LIRICS2.
These continuous efforts has been crystallized as
activities in ISO-TC37/SC4 which aims to make
an international standard for language resources.
1http://www.ilc.cnr.it/Eagles96/home.html
2lirics.loria.fr/documents.html
(1) Description 
framework of lexical 
entries
(2) Sample lexicons
(4) Evaluation 
through application
(3) Upper layer 
ontologyrefinement
description classification
refinement
evaluationevaluation
Figure 1: Relations among research items
On the other hand, since Asia has great lin-
guistic and cultural diversity, Asian language re-
sources have received much less attention than
their western counterparts. Creating a common
standard for Asian language resources that is com-
patible with an international standard has at least
three strong advantages: to increase the competi-
tive edge of Asian countries, to bring Asian coun-
tries to closer to their western counterparts, and to
bring more cohesion among Asian countries.
To achieve this goal, we have launched a two
year project to create a common standard for
Asian language resources. The project is com-
prised of the following four research items.
(1) building a description framework of lexical
entries
(2) building sample lexicons
(3) building an upper-layer ontology
(4) evaluating the proposed framework through
an application
Figure 1 illustrates the relations among these re-
search items.
Our main aim is the research item (1), building
a description framework of lexical entries which
827
fits with as many Asian languages as possible, and
contributing to the ISO-TC37/SC4 activities. As
a starting point, we employ an existing descrip-
tion framework, the MILE framework (Bertagna
et al, 2004a), to describe several lexical entries of
several Asian languages. Through building sam-
ple lexicons (research item (2)), we will find prob-
lems of the existing framework, and extend it so
as to fit with Asian languages. In this extension,
we need to be careful in keeping consistency with
the existing framework. We start with Chinese,
Japanese and Thai as target Asian languages and
plan to expand the coverage of languages. The re-
search items (2) and (3) also comprise the similar
feedback loop. Through building sample lexicons,
we refine an upper-layer ontology. An application
built in the research item (4) is dedicated to evalu-
ating the proposed framework. We plan to build an
information retrieval system using a lexicon built
by extending the sample lexicon.
In what follows, section 2 briefly reviews the
MILE framework which is a basis of our de-
scription framework. Since the MILE framework
is originally designed for European languages, it
does not always fit with Asian languages. We ex-
emplify some of the problems in section 3 and sug-
gest some directions to solve them. We expect
that further problems will come into clear view
through building sample lexicons. Section 4 de-
scribes a criteria to choose lexical entries in sam-
ple lexicons. Section 5 describes an approach
to build an upper-layer ontology which can be
sharable among languages. Section 6 describes
an application through which we evaluate the pro-
posed framework.
2 The MILE framework for
interoperability of lexicons
The ISLE (International Standards for Language
Engineering) Computational Lexicon Working
Group has consensually defined the MILE (Mul-
tilingual ISLE Lexical Entry) as a standardized
infrastructure to develop multilingual lexical re-
sources for HLT applications, with particular at-
tention toMachine Translation (MT) and Crosslin-
gual Information Retrieval (CLIR) application
systems.
The MILE is a general architecture devised
for the encoding of multilingual lexical informa-
tion, a meta-entry acting as a common representa-
tional layer for multilingual lexicons, by allowing
integration and interoperability between different
monolingual lexicons3.
This formal and standardized framework to en-
code MILE-conformant lexical entries is provided
to lexicon and application developers by the over-
all MILE Lexical Model (MLM). As concerns
the horizontal organization, the MLM consists of
two independent, but interlinked primary compo-
nents, the monolingual and the multilingual mod-
ules. The monolingual component, on the vertical
dimension, is organized over three different repre-
sentational layers which allow to describe differ-
ent dimensions of lexical entries, namely the mor-
phological, syntactic and semantic layers. More-
over, an intermediate module allows to define
mechanisms of linkage and mapping between the
syntactic and semantic layers. Within each layer, a
basic linguistic information unit is identified; basic
units are separated but still interlinked each other
across the different layers.
Within each of the MLM layers, different types
of lexical object are distinguished :
? the MILE Lexical Classes (MLC) represent
the main building blocks which formalize
the basic lexical notions. They can be seen
as a set of structural elements organized in
a layered fashion: they constitute an on-
tology of lexical objects as an abstraction
over different lexical models and architec-
tures. These elements are the backbone of
the structural model. In the MLM a defini-
tion of the classes is provided together with
their attributes and the way they relate to each
other. Classes represent notions like Inflec-
tionalParadigm, SyntacticFunction, Syntac-
ticPhrase, Predicate, Argument,
? the MILE Data Categories (MDC) which
constitute the attributes and values to adorn
the structural classes and allow concrete en-
tries to be instantiated. MDC can belong to
a shared repository or be user-defined. ?NP?
and ?VP? are data category instances of the
class SyntacticPhrase, whereas and ?subj?
and ?obj? are data category instances of the
class SyntacticFunction.
? lexical operations, which are special lexical
entities allowing the user to define multilin-
3MILE is based on the experience derived from exist-
ing computational lexicons (e.g. LE-PAROLE, SIMPLE, Eu-
roWordNet, etc.).
828
gual conditions and perform operations on
lexical entries.
Originally, in order to meet expectations placed
upon lexicons as critical resources for content pro-
cessing in the Semantic Web, the MILE syntactic
and semantic lexical objects have been formalized
in RDF(S), thus providing a web-based means to
implement the MILE architecture and allowing for
encoding individual lexical entries as instances of
the model (Ide et al, 2003; Bertagna et al, 2004b).
In the framework of our project, by situating our
work in the context of W3C standards and relying
on standardized technologies underlying this com-
munity, the original RDF schema for ISLE lexi-
cal entries has been made compliant to OWL. The
whole data model has been formalized in OWL by
using Prote?ge? 3.2 beta and has been extended to
cover the morphological component as well (see
Figure 2). Prote?ge? 3.2 beta has been also used as
a tool to instantiate the lexical entries of our sam-
ple monolingual lexicons, thus ensuring adherence
to the model, encoding coherence and inter- and
intra-lexicon consistency.
3 Existing problems with the MILE
framework for Asian languages
In this section, we will explain some problematic
phenomena of Asian languages and discuss pos-
sible extensions of the MILE framework to solve
them.
Inflection The MILE provides the powerful
framework to describe the information about in-
flection. InflectedForm class is devoted to de-
scribe inflected forms of a word, while Inflec-
tionalParadigm to define general inflection rules.
However, there is no inflection in several Asian
languages, such as Chinese and Thai. For these
languages, we do not use the Inflected Form and
Inflectional Paradigm.
Classifier Many Asian languages, such as
Japanese, Chinese, Thai and Korean, do not dis-
tinguish singularity and plurality of nouns, but use
classifiers to denote the number of objects. The
followings are examples of classifiers of Japanese.
? inu
(dog)
ni
(two)
hiki
(CL)
? ? ? two dogs
? hon
(book)
go
(five)
satsu
(CL)
? ? ? five books
?CL? stands for a classifier. They always follow
cardinal numbers in Japanese. Note that differ-
ent classifiers are used for different nouns. In the
above examples, classifier ?hiki? is used to count
noun ?inu (dog)?, while ?satsu? for ?hon (book)?.
The classifier is determined based on the semantic
type of the noun.
In the Thai language, classifiers are used in var-
ious situations (Sornlertlamvanich et al, 1994).
The classifier plays an important role in construc-
tion with noun to express ordinal, pronoun, for in-
stance. The classifier phrase is syntactically gener-
ated according to a specific pattern. Here are some
usages of classifiers and their syntactic patterns.
? Enumeration
(Noun/Verb)-(cardinal number)-(CL)
e.g. nakrian
(student)
3 khon
(CL)
? ? ? three students
? Ordinal
(Noun)-(CL)-/thi:/-(cardinal number)
e.g. kaew
(glass)
bai
(CL)
thi: 4
(4th)
? ? ? the 4th glass
? Determination
(Noun)-(CL)-(Determiner)
e.g. kruangkhidlek
(calculator)
kruang
(CL)
nii
(this)
? ? ? this calculator
Classifiers could be dealt as a class of the part-
of-speech. However, since classifiers depend on
the semantic type of nouns, we need to refer to
semantic features in the morphological layer, and
vice versa. Some mechanism to link between fea-
tures beyond layers needs to be introduced into the
current MILE framework.
Orthographic variants Many Chinese words
have orthographic variants. For instance, the con-
cept of rising can be represented by either char-
acter variants of sheng1: ? or ?. However,
the free variants become non-free in certain com-
pound forms. For instance, only? allowed for?
? ?liter?, and only? is allowed for?? ?to sub-
lime?. The interaction of lemmas and orthographic
variations is not yet represented in MILE.
Reduplication as a derivational process In
some Asian languages, reduplication of words de-
rives another word, and the derived word often has
a different part-of-speech. Here are some exam-
ples of reduplication in Chinese. Man4 ? ?to be
slow? is a state verb, while a reduplicated form
829
Inflectional
Paradigm
Lexical Entry SyntacticUnit
Form Lemmatized Form Stem
Inflected Form
Combiner
Calculator Mrophfeat
Operation Argument
Morph
DataCats
0..*
0..* 0..*
0..*
0..*
0..1
0..*
0..*
1..*
<LemmatizedForm rdf:ID="LFstar">
  <hasInflectedForm>
    <InflectedForm rdf:ID="stars">
      <hasMorphoFeat>
<MorphoFeat rdf:ID="pl">
  <number rdf:datatype="http://www.w3c.org/
2001/ XMLSchema#string">
    plural
  </number>
</MorphoFeat>
      </hasMorphoFeat>
    </InflectedForm>
  </hasInflectedForm>
  <hasInflectedForm>
    <InflectedForm rdf:ID="star">
      <hasMorphoFeat>
<MorphoFeat rdf:ID="sg">
  <number rdf:datatype="http://www.w3c.org/
2001/ XMLSchema#string">
    singular
  </number>
</MorphoFeat>
      </hasMorphoFeat>
    </InflectedForm>
  </hasInflectedForm>
</LemmatiedForm>
Figure 2: Formalization of the morphological layer and excerpt of a sample RDF instantiation
man4-man4 ?? is an adverb. Another example
of reduplication involves verbal aspect. Kan4 ?
?to look? is an activity verb, while the reduplica-
tive form kan4-kan4 ??, refers to the tentative
aspect, introducing either stage-like sub-division
or the event or tentativeness of the action of the
agent. This morphological process is not provided
for in the current MILE standard.
There are also various usages of reduplication in
Thai. Some words reduplicate themselves to add a
specific aspect to the original meaning. The redu-
plication can be grouped into 3 types according to
the tonal sound change of the original word.
? Word reduplication without sound change
e.g. /dek-dek/ ? ? ? (N) children, (ADV) child-
ishly, (ADJ) childish
/sa:w-sa:w/ ? ? ? (N) women
? Word reduplication with high tone on the first
word
e.g. /dam4-dam/ ? ? ? (ADJ) extremely black
/bo:i4-bo:i/ ? ? ? (ADV) really often
? Triple word reduplication with high tone on
the second word
e.g. /dern-dern4-dern/ ?? (V) intensively walk
/norn-norn4-norn/??(V) intensively sleep
In fact, only the reduplication of the same sound
is accepted in the written text, and a special sym-
bol, namely /mai-yamok/ is attached to the origi-
nal word to represent the reduplication. The redu-
plication occurs in many parts-of-speech, such as
noun, verb, adverb, classifier, adjective, preposi-
tion. Furthermore, various aspects can be added
to the original meaning of the word by reduplica-
tion, such as pluralization, emphasis, generaliza-
tion, and so on. These aspects should be instanti-
ated as features.
Change of parts-of-speech by affixes Af-
fixes change parts-of-speech of words in
Thai (Charoenporn et al, 1997). There are
three prefixes changing the part-of-speech of the
original word, namely /ka:n/, /khwa:m/, /ya:ng/.
They are used in the following cases.
? Nominalization
/ka:n/ is used to prefix an action verb and
/khwa:m/ is used to prefix a state verb
in nominalization such as /ka:n-tham-nga:n/
(working), /khwa:m-suk/ (happiness).
? Adverbialization
An adverb can be derived by using /ya:ng/ to
prefix a state verb such as /ya:ng-di:/ (well).
Note that these prefixes are also words, and form
multi-word expressions with the original word.
This phenomenon is similar to derivation which
is not handled in the current MILE framework.
Derivation is traditionally considered as a different
phenomenon from inflection, and current MILE
focuses on inflection. The MILE framework is al-
ready being extended to treat such linguistic phe-
nomenon, since it is important to European lan-
guages as well. It would be handled in either the
morphological layer or syntactic layer.
830
Function Type Function types of predicates
(verbs, adjectives etc.) might be handled in a
partially different way for Japanese. In the syn-
tactic layer of the MILE framework, Function-
Type class is prepared to denote subcategorization
frames of predicates, and they have function types
such as ?subj? and ?obj?. For example, the verb
?eat? has two FunctionType data categories of
?subj? and ?obj?. Function types basically stand
for positions of case filler nouns. In Japanese,
cases are usually marked by postpositions and case
filler positions themselves do not provide much in-
formation on case marking. For example, both of
the following sentences mean the same, ?She eats
a pizza.?
? kanojo
(she)
ga
(NOM)
piza
(pizza)
wo
(ACC)
taberu
(eat)
? piza
(pizza)
wo
(ACC)
kanojo
(she)
ga
(NOM)
taberu
(eat)
?Ga? and ?wo? are postpositions which mark
nominative and accusative cases respectively.
Note that two case filler nouns ?she? and ?pizza?
can be exchanged. That is, the number of slots is
important, but their order is not.
For Japanese, we might use the set of post-
positions as values of FunctionType instead of
conventional function types such as ?subj? and
?obj?. It might be an user defined data category or
language dependent data category. Furthermore,
it is preferable to prepare the mapping between
Japanese postpositions and conventional function
types. This is interesting because it seems more
a terminological difference, but the model can be
applied also to Japanese.
4 Building sample lexicons
4.1 Swadesh list and basic lexicon
The issue involved in defining a basic lexicon for a
given language is more complicated than one may
think (Zhang et al, 2004). The naive approach of
simply taking the most frequent words in a lan-
guage is flawed in many ways. First, all frequency
counts are corpus-based and hence inherit the bias
of corpus sampling. For instance, since it is eas-
ier to sample written formal texts, words used pre-
dominantly in informal contexts are usually under-
represented. Second, frequency of content words
is topic-dependent and may vary from corpus to
corpus. Last, and most crucially, frequency of a
word does not correlate to its conceptual necessity,
which should be an important, if not only, criteria
for core lexicon. The definition of a cross-lingual
basic lexicon is even more complicated. The first
issue involves determination of cross-lingual lexi-
cal equivalencies. That is, how to determine that
word a (and not a?) in language A really is word b
in language B. The second issue involves the deter-
mination of what is a basic word in a multilingual
context. In this case, not even the frequency of-
fers an easy answer since lexical frequency may
vary greatly among different languages. The third
issue involves lexical gaps. That is, if there is a
word that meets all criteria of being a basic word
in language A, yet it does not exist in language D
(though it may exist in languages B, and C). Is this
word still qualified to be included in the multilin-
gual basic lexicon?
It is clear not all the above issues can be un-
equivocally solved with the time frame of our
project. Fortunately, there is an empirical core lex-
icon that we can adopt as a starting point. The
Swadesh list was proposed by the historical lin-
guist Morris Swadesh (Swadesh, 1952), and has
been widely used by field and historical linguists
for languages over the world. The Swadesh list
was first proposed as lexico-statistical metrics.
That is, these are words that can be reliably ex-
pected to occur in all historical languages and can
be used as the metrics for quantifying language
variations and language distance. The Swadesh
list is also widely used by field linguists when
they encounter a new language, since almost all
of these terms can be expected to occur in any
language. Note that the Swadesh list consists of
terms that embody human direct experience, with
culture-specific terms avoided. Swadesh started
with a 215 items list, before cutting back to 200
items and then to 100 items. A standard list of
207 items is arrived at by unifying the 200 items
list and the 100 items list. We take the 207 terms
from the Swadesh list as the core of our basic lex-
icon. Inclusion of the Swadesh list also gives us
the possibility of covering many Asian languages
in which we do not have the resources to make a
full and fully annotated lexicon. For some of these
languages, a Swadesh lexicon for reference is pro-
vided by a collaborator.
4.2 Aligning multilingual lexical entries
Since our goal is to build a multilingual sample
lexicon, it is required to align words in several
831
Asian languages. In this subsection, we propose
a simple method to align words in different lan-
guages. The basic idea for multilingual alignment
is an intermediary by English. That is, first we
prepare word pairs between English and other lan-
guages, then combine them together to make cor-
respondence among words in several languages.
The multilingual alignment method currently we
consider is as follows:
1. Preparing the set of frequent words of each
language
Suppose that {Jw
i
}, {Cw
i
}, {Tw
i
} is the
set of frequent words of Japanese, Chinese
and Thai, respectively. Now we try to con-
struct a multilingual lexicon for these three
languages, however, our multilingual align-
ment method can be easily extended to han-
dle more languages.
2. Obtaining English translations
A word Xw
i
is translated into a set of En-
glish words EXw
ij
by referring to the bilin-
gual dictionary, where X denotes one of our
languages, J , C or T . We can obtain map-
pings as in (1).
Jw
1
: EJw
11
, EJw
12
, ? ? ?
Jw
2
: EJw
21
, EJw
22
, ? ? ?
...
Cw
1
: ECw
11
, ECw
12
, ? ? ?
Cw
2
: ECw
21
, ECw
22
, ? ? ?
...
Tw
1
: ETw
11
, ETw
12
, ? ? ?
Tw
2
: ETw
21
, ETw
22
, ? ? ?
...
(1)
Notice that this procedure is automatically
done and ambiguities would be left at this
stage.
3. Generating new mapping
From mappings in (1), a new mapping is gen-
erated by inverting the key. That is, in the
new mapping, a key is an English word Ew
i
and a correspondence for each key is sets
of translations XEw
ij
for 3 languages, as
shown in (2):
Ew
1
: (JEw
11
, JEw
12
, ? ? ?)
(CEw
11
, CEw
12
, ? ? ?)
(TEw
11
, TEw
12
, ? ? ?)
Ew
2
: (JEw
21
, JEw
22
, ? ? ?)
(CEw
21
, CEw
22
, ? ? ?)
(TEw
21
, TEw
22
, ? ? ?)
...
(2)
Notice that at this stage, correspondence be-
tween different languages is very loose, since
they are aligned on the basis of sharing only
a single English word.
4. Refinement of alignment
Groups of English words are constructed by
referring to the WordNet synset information.
For example, suppose that Ew
i
and Ew
j
be-
long to the same synset S
k
. We will make a
new alignment by making an intersection of
{XEw
i
} and {XEw
j
} as shown in (3).
Ew
i
: (JEw
i1
, ??) (CEw
i1
, ??) (TEw
i1
, ??)
Ew
j
: (JEw
j1
, ??)(CEw
j1
, ??)(TEw
j1
, ??)
? intersection
S
k
: (JEw?
k1
, ??)(CEw?
k1
, ??)(TEw?
k1
, ??)
(3)
In (3), the key is a synset S
k
, which is sup-
posed to be a conjunction of Ew
i
and Ew
j
,
and the counterpart is the intersection of set
of translations for each language. This oper-
ation would reduce the number of words of
each language. That means, we can expect
that the correspondence among words of dif-
ferent languages becomes more precise. This
new word alignment based on a synset is a
final result.
To evaluate the performance of this method,
we conducted a preliminary experiment using the
Swadesh list. Given the Swadesh list of Chi-
nese, Italian, Japanese and Thai as a gold stan-
dard, we tried to replicate these lists from the En-
glish Swadesh list and bilingual dictionaries be-
tween English and these languages. In this experi-
ment, we did not perform the refinement step with
WordNet. From 207 words in the Swadesh list,
we dropped 4 words (?at?, ?in?, ?with? and ?and?)
due to their too many ambiguities in translation.
As a result, we obtained 181 word groups
aligned across 5 languages (Chinese, English, Ital-
ian, Japanese and Thai) for 203 words. An
aligned word group was judged ?correct? when the
words of each language include only words in the
Swadesh list of that language. It was judged ?par-
tially correct? when the words of a language also
include the words which are not in the Swadesh
list. Based on the correct instances, we obtain
0.497 for precision and 0.443 for recall. These fig-
ures go up to 0.912 for precision and 0.813 for re-
call when based on the partially correct instances.
This is quite a promising result.
832
5 Upper-layer ontology
The empirical success of the Swadesh list poses
an interesting question that has not been explored
before. That is, does the Swadesh list instantiates a
shared, fundamental human conceptual structure?
And if there is such as a structure, can we discover
it?
In the project these fundamental issues are as-
sociated with our quest for cross-lingual interop-
erability. We must make sure that the items of
the basic lexicon are given the same interpreta-
tion. One measure taken to ensure this consists in
constructing an upper-ontology based on the ba-
sic lexicon. Our preliminary work of mapping the
Swadesh list items to SUMO (Suggested Upper
Merged Ontology) (Niles and Pease, 2001) has al-
ready been completed. We are in the process of
mapping the list to DOLCE (Descriptive Ontology
for Linguistic and Cognitive Engineering) (Ma-
solo et al, 2003). After the initial mapping, we
carry on the work to restructure the mapped nodes
to form a genuine conceptual ontology based on
the language universal basic lexical items. How-
ever one important observation that we have made
so far is that the success of the Swadesh list is
partly due to its underspecification and to the lib-
erty it gives to compilers of the list in a new lan-
guage. If this idea of underspecification is essen-
tial for basic lexicon for human languages, then we
must resolve this apparent dilemma of specifying
them in a formal ontology that requires fully spec-
ified categories. For the time being, genuine ambi-
guities resulted in the introduction of each disam-
biguated sense in the ontology. We are currently
investigating another solution that allows the in-
clusion of underspecified elements in the ontology
without threatening its coherence. More specifi-
cally we introduce a underspecified relation in the
structure for linking the underspecified meaning
to the different specified meaning. The specified
meanings are included in the taxonomic hierarchy
in a traditional manner, while a hierarchy of un-
derspecified meanings can be derived thanks to the
new relation. An underspecified node only inherits
from the most specific common mother of its fully
specified terms. Such distinction avoids the clas-
sical misuse of the subsumption relation for rep-
resenting multiple meanings. This method does
not reflect a dubious collapse of the linguistic and
conceptual levels but the treatment of such under-
specifications as truly conceptual. Moreover we
Internet
Query
Local 
DB
User interest
 model
Topic
Feedback
Search
engine
Crawler
Retrieval
results
Figure 3: The system architecture
hope this proposal will provide a knowledge rep-
resentation framework for the multilingual align-
ment method presented in the previous section.
Finally, our ontology will not only play the role
of a structured interlingual index. It will also serve
as a common conceptual base for lexical expan-
sion, as well as for comparative studies of the lex-
ical differences of different languages.
6 Evaluation through an application
To evaluate the proposed framework, we are build-
ing an information retrieval system. Figure 3
shows the system architecture.
A user can input a topic to retrieve the docu-
ments related to that topic. A topic can consist
of keywords, website URL?s and documents which
describe the topic. From the topic information, the
system builds a user interest model. The system
then uses a search engine and a crawler to search
for information related to this topic in WWW and
stores the results in the local database. Generally,
the search results include many noises. To filter
out these noises, we build a query from the user
interest model and then use this query to retrieve
documents in the local database. Those documents
similar to the query are considered as more related
to the topic and the user?s interest, and are returned
to the user. When the user obtains these retrieval
results, he can evaluate these documents and give
the feedback to the system, which is used for the
further refinement of the user interest model.
Language resources can contribute to improv-
ing the system performance in various ways.
Query expansion is a well-known technique which
expands user?s query terms into a set of similar and
related terms by referring to ontologies. Our sys-
tem is based on the vector space model (VSM) and
traditional query expansion can be applicable us-
ing the ontology.
There has been less research on using lexical in-
833
formation for information retrieval systems. One
possibility we are considering is query expansion
by using predicate-argument structures of terms.
Suppose a user inputs two keywords, ?hockey?
and ?ticket? as a query. The conventional query
expansion technique expands these keywords to
a set of similar words based on an ontology. By
referring to predicate-argument structures in the
lexicon, we can derive actions and events as well
which take these words as arguments. In the above
example, by referring to the predicate-argument
structure of ?buy? or ?sell?, and knowing that
these verbs can take ?ticket? in their object role,
we can add ?buy? and ?sell? to the user?s query.
This new type of expansion requires rich lexical
information such as predicate argument structures,
and the information retrieval system would be a
good touchstone of the lexical information.
7 Concluding remarks
This paper outlined a new project for creating a
common standard for Asian language resources
in cooperation with other initiatives. We start
with three Asian languages, Chinese, Japanese
and Thai, on top of the existing framework which
was designed mainly for European languages.
We plan to distribute our draft to HLT soci-
eties of other Asian languages, requesting for
their feedback through various networks, such
as the Asian language resource committee net-
work under Asian Federation of Natural Language
Processing (AFNLP)4, and Asian Language Re-
source Network project5. We believe our ef-
forts contribute to international activities like ISO-
TC37/SC46 (Francopoulo et al, 2006) and to the
revision of the ISO Data Category Registry (ISO
12620), making it possible to come close to the
ideal international standard of language resources.
Acknowledgment
This research was carried out through financial
support provided under the NEDO International
Joint Research Grant Program (NEDO Grant).
References
F. Bertagna, A. Lenci, M. Monachini, and N. Calzo-
lari. 2004a. Content interoperability of lexical re-
sources, open issues and ?MILE? perspectives. In
4http://www.afnlp.org/
5http://www.language-resource.net/
6http://www.tc37sc4.org/
Proceedings of the 4th International Conference on
Language Resources and Evaluation (LREC2004),
pages 131?134.
F. Bertagna, A. Lenci, M. Monachini, and N. Calzo-
lari. 2004b. The MILE lexical classes: Data cat-
egories for content interoperability among lexicons.
In A Registry of Linguistic Data Categories within
an Integrated Language Resources Repository Area
? LREC2004 Satellite Workshop, page 8.
N. Calzolari, F. Bertagna, A. Lenci, and M. Mona-
chini. 2003. Standards and best practice for mul-
tilingual computational lexicons. MILE (the mul-
tilingual ISLE lexical entry). ISLE Deliverable
D2.2&3.2.
T. Charoenporn, V. Sornlertlamvanich, and H. Isahara.
1997. Building a large Thai text corpus ? part-
of-speech tagged corpus: ORCHID?. In Proceed-
ings of the Natural LanguageProcessing Pacific Rim
Symposium.
G. Francopoulo, G. Monte, N. Calzolari, M. Mona-
chini, N. Bel, M. Pet, and C. Soria. 2006. Lex-
ical markup framework (LMF). In Proceedings of
LREC2006 (forthcoming).
N. Ide, A. Lenci, and N. Calzolari. 2003. RDF in-
stantiation of ISLE/MILE lexical entries. In Pro-
ceedings of the ACL 2003 Workshop on Linguistic
Annotation: Getting the Model Right, pages 25?34.
A. Lenci, N. Bel, F. Busa, N. Calzolari, E. Gola,
M. Monachini, A. Ogonowsky, I. Peters, W. Peters,
N. Ruimy, M. Villegas, and A. Zampolli. 2000.
SIMPLE: A general framework for the development
of multilingual lexicons. International Journal of
Lexicography, Special Issue, Dictionaries, Thesauri
and Lexical-Semantic Relations, XIII(4):249?263.
C. Masolo, A. Borgo, S.; Gangemi, N. Guarino, and
A. Oltramari. 2003. Wonderweb deliverable d18
?ontology library (final)?. Technical report, Labo-
ratory for Applied Ontology, ISTC-CNR.
I. Niles and A Pease. 2001. Towards a standard upper
ontology. In Proceedings of the 2nd International
Conference on Formal Ontology in Information Sys-
tems (FOIS-2001).
V. Sornlertlamvanich, W. Pantachat, and S. Mek-
navin. 1994. Classifier assignment by corpus-
based approach. In Proceedings of the 15th Inter-
national Conference on Computational Linguistics
(COLING-94), pages 556?561.
M. Swadesh. 1952. Lexico-statistical dating of pre-
historic ethnic contacts: With special reference to
north American Indians and Eskimos. In Proceed-
ings of the American Philo-sophical Society, vol-
ume 96, pages 452?463.
H. Zhang, C. Huang, and S. Yu. 2004. Distributional
consistency: A general method for defining a core
lexicon. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC2004), pages 1119?1222.
834
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 145?152,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Query Expansion using LMF-Compliant Lexical Resources
Tokunaga Takenobu
Tokyo Inst. of Tech.
Dain Kaplan
Tokyo Inst. of Tech.
Nicoletta Calzolari
ILC/CNR
Monica Monachini
ILC/CNR
Claudia Soria
ILC/CNR
Virach Sornlertlamvanich
TCL, NICT
Thatsanee Charoenporn
TCL, NICT
Xia Yingju
Fujitsu R&D Center
Chu-Ren Huang
The Hong Kong Polytec. Univ.
Shu-Kai Hsieh
National Taiwan Normal Univ.
Shirai Kiyoaki
JAIST
Abstract
This paper reports prototype multilin-
gual query expansion system relying on
LMF compliant lexical resources. The
system is one of the deliverables of a
three-year project aiming at establish-
ing an international standard for language
resources which is applicable to Asian
languages. Our important contributions
to ISO 24613, standard Lexical Markup
Framework (LMF) include its robustness
to deal with Asian languages, and its ap-
plicability to cross-lingual query tasks, as
illustrated by the prototype introduced in
this paper.
1 Introduction
During the last two decades corpus-based ap-
proaches have come to the forefront of NLP re-
search. Since without corpora there can be no
corpus-based research, the creation of such lan-
guage resources has also necessarily advanced
as well, in a mutually beneficial synergetic re-
lationship. One of the advantages of corpus-
based approaches is that the techniques used
are less language specific than classical rule-
based approaches where a human analyses the
behaviour of target languages and constructs
rules manually. This naturally led the way
for international resource standardisation, and in-
deed there is a long standing precedent in the
West for it. The Human Language Technol-
ogy (HLT) society in Europe has been particu-
larly zealous in this regard, propelling the cre-
ation of resource interoperability through a se-
ries of initiatives, namely EAGLES (Sanfilippo et
al., 1999), PAROLE/SIMPLE (Lenci et al, 2000),
ISLE/MILE (Ide et al, 2003), and LIRICS1. These
1http://lirics.loria.fr/
continuous efforts have matured into activities in
ISO-TC37/SC42, which aims at making an inter-
national standard for language resources.
However, due to the great diversity of languages
themselves and the differing degree of technolog-
ical development for each, Asian languages, have
received less attention for creating resources than
their Western counterparts. Thus, it has yet to be
determined if corpus-based techniques developed
for well-computerised languages are applicable on
a broader scale to all languages. In order to effi-
ciently develop Asian language resources, utilis-
ing an international standard in this creation has
substantial merits.
We launched a three-year project to create an
international standard for language resources that
includes Asian languages. We took the following
approach in seeking this goal.
? Based on existing description frameworks,
each research member tries to describe sev-
eral lexical entries and find problems with
them.
? Through periodical meetings, we exchange
information about problems found and gen-
eralise them to propose solutions.
? Through an implementation of an application
system, we verify the effectiveness of the pro-
posed framework.
Below we summarise our significant contribution
to an International Standard (ISO24613; Lexical
Markup Framework: LMF).
1st year After considering many characteristics
of Asian languages, we elucidated the shortcom-
ings of the LMF draft (ISO24613 Rev.9). The
draft lacks the following devices for Asian lan-
guages.
2http://www.tc37sc4.org/
145
(1) A mapping mechanism between syntactic
and semantic arguments
(2) Derivation (including reduplication)
(3) Classifiers
(4) Orthography
(5) Honorifics
Among these, we proposed solutions for (1) and
(2) to the ISO-TC37 SC4 working group.
2nd year We proposed solutions for above the
(2), (3) and (4) in the comments of the Committee
Draft (ISO24613 Rev. 13) to the ISO-TC37 SC4
working group. Our proposal was included in DIS
(Draft International Standard).
(2?) a package for derivational morphology
(3?) the syntax-semantic interface resolving the
problem of classifiers
(4?) representational issues with the richness of
writing systems in Asian languages
3rd year Since ISO 24613 was in the FDIS stage
and fairly stable, we built sample lexicons in Chi-
nese, English, Italian, Japanese, and Thai based
on ISO24613. At the same time, we implemented
a query expansion system utilising rich linguis-
tic resources including lexicons described in the
ISO 24613 framework. We confirmed that a sys-
tem was feasible which worked on the tested lan-
guages (including both Western and Asian lan-
guages) when given lexicons compliant with the
framework. ISO 24613 (LMF) was approved by
the October 2008 ballot and published as ISO-
24613:2008 on 17th November 2008.
Since we have already reported our first 2 year
activities elsewhere (Tokunaga and others, 2006;
Tokunaga and others, 2008), we focus on the
above query expansion system in this paper.
2 Query expansion using
LMF-compliant lexical resources
We evaluated the effectiveness of LMF on a mul-
tilingual information retrieval system, particularly
the effectiveness for linguistically motivated query
expansion.
The linguistically motivated query expansion
system aims to refine a user?s query by exploiting
the richer information contained within a lexicon
described using the adapted LMF framework. Our
lexicons are completely complaint with this inter-
national standard. For example, a user inputs a
keyword ?ticket? as a query. Conventional query
expansion techniques expand this keyword to a
set of related words by using thesauri or ontolo-
gies (Baeza-Yates and Ribeiro-Neto, 1999). Using
the framework proposed by this project, expand-
ing the user?s query becomes a matter of following
links within the lexicon, from the source lexical
entry or entries through predicate-argument struc-
tures to all relevant entries (Figure 1). We focus
on expanding the user inputted list of nouns to rel-
evant verbs, but the reverse would also be possible
using the same technique and the same lexicon.
This link between entries is established through
the semantic type of a given sense within a lexical
entry. These semantic types are defined by higher-
level ontologies, such as MILO or SIMPLE (Lenci
et al, 2000) and are used in semantic predicates
that take such semantic types as a restriction ar-
gument. Since senses for verbs contain a link to
a semantic predicate, using this semantic type, the
system can then find any/all entries within the lexi-
con that have this semantic type as the value of the
restriction feature of a semantic predicate for any
of their senses. As a concrete example, let us con-
tinue using the ?ticket? scenario from above. The
lexical entry for ?ticket? might contain a semantic
type definition something like in Figure 2.
<LexicalEntry ...>
<feat att="POS" val="N"/>
<Lemma>
<feat att="writtenForm"
val="ticket"/>
</Lemma>
<Sense ...>
<feat att="semanticType"
val="ARTIFACT"/>
...
</Sense>
...
</LexicalEntry>
Figure 2: Lexical entry for ?ticket?
By referring to the lexicon, we can then derive
any actions and events that take the semantic type
?ARTIFACT? as an argument.
First all semantic predicates are searched for ar-
guments that have an appropriate restriction, in
this case ?ARTIFACT? as shown in Figure 3, and
then any lexical entries that refer to these predi-
cates are returned. An equally similar definition
would exist for ?buy?, ?find? and so on. Thus,
by referring to the predicate-argument structure of
related verbs, we know that these verbs can take
146
<LexicalEntry ...>
  <feat att="POS" val="Noun"/>
  <Lemma>
    <feat att="writtenForm" val="ticket"/>
  </Lemma>
  <Sense ...>
    <feat att="semanticType" val="ARTIFACT"/>
    ...
  </Sense>
  ...
</LexicalEntry>
User Inputs
ticket
<Sense>
<SemanticFeature>
Semantic Features of type 
"restriction" that take 
Sense's semanticType
All senses for 
matched nouns
<SemanticPredicate 
  id="pred-sell-1">
  <SemanticArgument>
    <feat att="label" val="X"/>
    <feat att="semanticRole" val="Agent"/>
    <feat att="restriction" val="Human"/>
  </SemanticArgument>
  ...
  <SemanticArgument>
    <feat att="label" val="Z"/>
    <feat att="semanticRole" val="Patient"/>
    <feat att="restriction" 
          val="ARTIFACT,LOCATION"/>
  </SemanticArgument>
</SemanticPredicate>
All Semantic Predicates 
that contain matched 
Semantic Features
<Sense>
Senses that use matched 
Semantic Predicates
<LexicalEntry ...>
  <feat att="POS" val="Verb"/>
  <Lemma>
    <feat att="writtenForm" val="sell"/>
  </Lemma>
  <Sense id="sell-1" ...>
    ...
    <PredicativeRepresentation
      predicate="pred-sell-1" ...>
  </Sense>
</LexicalEntry>
<LexicalEntry>
<SemanticPredicate>
<LexicalEntry>
System outputs
"sell", ...
For each <Sense> find all 
<SemanticArgument> that 
take this semanticType as 
a feature of type 
"restriction"
Find all verbs <LexicalEntry> 
that use these 
<SemanticPredicate>
All verbs that have 
matched Senses
Figure 1: QE Process Flow
147
<LexicalEntry ...>
<feat att="POS" val="V"/>
<Lemma>
<feat att="writtenForm"
val="sell"/>
</Lemma>
<Sense id="sell-1" ...>
<feat att="semanticType"
val="Transaction"/>
<PredicativeRepresentation
predicate="pred-sell-1"
correspondences="map-sell1">
</Sense>
</LexicalEntry>
<SemanticPredicate id="pred-sell-1">
<SemanticArgument ...>
...
<feat att="restriction"
val="ARTIFACT"/>
</SemanticArgument>
</SemanticPredicate>
Figure 3: Lexical entry for ?sell? with its semantic
predicate
?ticket? in the role of object. The system then re-
turns all relevant entries, here ?buy?, ?sell? and
?find?, in response to the user?s query. Figure 1
schematically shows this flow.
3 A prototype system in detail
3.1 Overview
To test the efficacy of the LMF-compliant lexi-
cal resources, we created a system implementing
the query expansion mechanism explained above.
The system was developed in Java for its ?com-
pile once, run anywhere? portability and its high-
availability of reusable off-the-shelf components.
On top of Java 5, the system was developed us-
ing JBoss Application Server 4.2.3, the latest stan-
dard, stable version of the product at the time of
development. To provide fast access times, and
easy traversal of relational data, a RDB was used.
The most popular free open-source database was
selected, MySQL, to store all lexicons imported
into the system, and the system was accessed, as a
web-application, via any web browser.
3.2 Database
The finalised database schema is shown in Fig-
ure 4. It describes the relationships between en-
tities, and more or less mirrors the classes found
within the adapted LMF framework, with mostly
only minor exceptions where it was efficacious for
querying the data. Due to space constraints, meta-
data fields, such as creation time-stamps have been
left out of this diagram. Since the system also al-
lows for multiple lexicons to co-exist, a lexicon id
resides in every table. This foreign key has been
highlighted in a different color, but not connected
via arrows to make the diagram easier to read. In
addition, though in actuality this foreign key is not
required for all tables, it has been inserted as a con-
venience for querying data more efficiently, even
within join tables (indicated in blue). Having mul-
tiple lexical resources co-existing within the same
database allows for several advantageous features,
and will be described later. Some tables also con-
tain a text id, which stores the original id attribute
for that element found within the XML. This is
not used in the system itself, and is stored only for
reference.
3.3 System design
As mentioned above, the application is deployed
to JBoss AS as an ear-file. The system it-
self is composed of java classes encapsulating
the data contained within the database, a Pars-
ing/Importing class for handling the LMF XML
files after they have been validated, and JSPs,
which contain HTML, for displaying the inter-
face to the user. There are three main sections
to the application: Search, Browse, and Config-
ure. Explaining last to first, the Configure section,
shown in Figure 5, allows users to create a new
lexicon within the system or append to an exist-
ing lexicon by uploading a LMF XML file from
their web browser, or delete existing lexicons that
are no longer needed/used. After import, the data
may be immediately queried upon with no other
changes to system configuration, from within both
the Browse and Search sections. Regardless of
language, the rich syntactic/semantic information
contained within the lexicon is sufficient for car-
rying out query expansion on its own.
The Browse section (Figure 6) allows the user to
select any available lexicon to see the relationships
contained within it, which contains tabs for view-
ing all noun to verb connections, a list of nouns, a
list of verbs, and a list of semantic types. Each has
appropriate links allowing the user to easily jump
to a different tab of the system. Clicking on a noun
takes them to the Search section (Figure 7). In this
section, the user may select many lexicons to per-
form query extraction on, as is visible in Figure 7.
148
semantic_link 
VARCHAR (64)
sense
sense_id
PRIMARY KEY
synset_id
FOREIGN KEY
syn_sem_correspondence_id
FOREIGN KEY
semantic_predicate_id
FOREIGN KEY
semantic_type
VARCHAR (64)
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
lexicon_id
FOREIGN KEY
text_id
VARCHAR (100)
semantic_predicate_id
PRIMARY KEY
semantic_predicate
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
semantic_argument_id
PRIMARY KEY
semantic_argument
value
VARCHAR (100)
attribute
VARCHAR (100)
lexicon_id
FOREIGN KEY
semantic_feature_id
PRIMARY KEY
semantic_feature
lexicon_id
FOREIGN KEY
semantic_argument_id
FOREIGN KEY
semantic_predicate_id
FOREIGN KEY
semantic_predicate_to_argument
lexicon_id
FOREIGN KEY
semantic_feature_id
FOREIGN KEY
semantic_argument_id 
FOREIGN KEY
semantic_argument_to_feature
description
TEXT
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
synset_id
PRIMARY KEY
synset
written_form
VARCHAR (64) NOT NULL
part_of_speech
ENUM( 'Verb', 'Noun' , 'Unknown')
lexical_entry
text_id
VARCHAR (64)
entry_id 
PRIMARY KEY
lexicon_id 
FOREIGN KEY
semantic_feature
FOREIGN KEY
syntactic_feature
FOREIGN KEY
lexicon_id
FOREIGN KEY
argument_map_id
PRIMARY KEY
syn_sem_argument_map
lexicon_id
FOREIGN KEY
argument_map_id
FOREIGN KEY
syn_sem_correspondence_id 
FOREIGN KEY
syn_sem_correspondence_to_map
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
syn_sem_correspondence_id
PRIMARY KEY
syn_sem_correspondence
lexicon_id
FOREIGN KEY
sense_id
FOREIGN KEY
entry_id
FOREIGN KEY
lexical_entry_to_sense
lexicon_id
FOREIGN KEY
text_id
VARCHAR (100)
frame_id
PRIMARY KEY
subcat_frame
lexicon_id
FOREIGN KEY
frame_id
FOREIGN KEY
sense_id
FOREIGN KEY
entry_id
FOREIGN KEY
lexical_entry_to_subcat_frame
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
syntactic_argument_id
PRIMARY KEY
syntactic_argument
value
VARCHAR (100)
attribute
VARCHAR (100)
lexicon_id
FOREIGN KEY
syntactic_feature_id
PRIMARY KEY
syntactic_feature
lexicon_id
FOREIGN KEY
syntactic_argument_id
FOREIGN KEY
frame_id
FOREIGN KEY
subcat_frame_to_argument
lexicon_id
FOREIGN KEY
syntactic_feature_id
FOREIGN KEY
syntactic_argument_id 
FOREIGN KEY
syntactic_argument_to_feature
description
VARCHAR(128)
language
VARCHAR(64)
lexicon_id
PRIMARY KEY
lexicon
relation_type 
VARCHAR (64)
lexicon_id
FOREIGN KEY
related_sense_id
FOREIGN KEY
sense_id
FOREIGN KEY
sense_relation
Figure 4: Database schema
Figure 5: QE System - Configure Figure 6: QE System - Browse
149
Figure 7: QE System - Search
3.4 Semantic information
This new type of query expansion requires rich
lexical information. We augmented our data using
the SIMPLE ontology for semantic types, using
the same data for different languages. This had
the added benefit of allowing cross-language ex-
pansion as a result. In steps two and three of Fig-
ure 1 when senses are retrieved that take specific
semantic types as arguments, this process can be
done across all (or as many as are selected) lex-
icons in the database. Thus, results such as are
shown in Figure 7 are possible. In this figure the
Japanese word for ?nail? is entered, and results for
both selected languages, Japanese and Italian, are
returned. This feature requires the unification of
the semantic type ontology strata.
3.5 Possible extension
Next steps for the QE platform are to explore the
use of other information already defined within the
adapted framework, specifically sense relations.
Given to the small size of our sample lexicon, data
sparsity is naturally an issue, but hopefully by ex-
ploring and exploiting these sense relations prop-
erly, the system may be able to further expand a
user?s query to include a broader range of selec-
tions using any additional semantic types belong-
ing to these related senses. The framework also
contains information about the order in which syn-
tactic arguments should be placed. This informa-
tion should be used to format the results from the
user?s query appropriately.
4 An Additional Evaluation
We conducted some additional query expansion
experiments using a corpus that was acquired from
Chinese LDC (No. ?2004-863-009?) as a base (see
below). This corpus marked an initial achievement
in building a multi-lingual parallel corpus for sup-
porting development of cross-lingual NLP appli-
cations catering to the Beijing 2008 Olympics.
The corpus contains parallel texts in Chinese,
English and Japanese and covers 5 domains that
are closely related to the Olympics: traveling, din-
ing, sports, traffic and business. The corpus con-
sists of example sentences, typical dialogues and
articles from the Internet, as well as other language
teaching materials. To deal with the different lan-
guages in a uniform manner, we converted the cor-
pus into our proposed LMF-compliant lexical re-
sources framework, which allowed the system to
expand the query between all the languages within
the converted resources without additional modifi-
cations.
As an example of how this IR system func-
tioned, suppose that Mr. Smith will be visiting
Beijing to see the Olympic games and wants to
know how to buy a newspaper. Using this system,
he would first enter the query ?newspaper?. For
this query, with the given corpus, the system re-
turns 31 documents, fragments of the first 5 shown
below.
(1) I?ll bring an English newspaper immediately.
(2) Would you please hand me the newspaper.
(3) There?s no use to go over the newspaper ads.
(4) Let?s consult the newspaper for such a film.
(5) I have little confidence in what the newspa-
pers say.
Yet it can be seen that the displayed results are not
yet useful enough to know how to buy a newspa-
per, though useful information may in fact be in-
cluded within some of the 31 documents. Using
the lexical resources, the query expansion module
suggests ?buy?, ?send?, ?get?, ?read?, and ?sell?
as candidates to add for a revised query.
Mr. Smith wants to buy a newspaper, so he se-
lects ?buy? as the expansion term. With this query
the system returns 11 documents, fragments of the
first 5 listed below.
(6) I?d like some newspapers, please.
150
(7) Oh, we have a barber shop, a laundry, a store,
telegram services, a newspaper stand, table
tennis, video games and so on.
(8) We can put an ad in the newspaper.
(9) Have you read about the Olympic Games of
Table Tennis in today?s newspaper, Miss?
(10) newspaper says we must be cautious about
tidal waves.
This list shows improvement, as information about
newspapers and shopping is present, but still ap-
pears to lack any documents directly related to
how to buy a newspaper.
Using co-occurrence indexes, the IR system
returns document (11) below, because the noun
?newspaper? and the verb ?buy? appear in the
same sentence.
(11) You can make change at some stores, just buy
a newspaper or something.
From this example it is apparent that this sort
of query expansion is still too naive to apply to
real IR systems. It should be noted, however, that
our current aim of evaluation was in confirming
the advantage of LMF in dealing with multiple
languages, for which we conducted a similar run
with Chinese and Japanese. Results of these tests
showed that in following the LMF framework in
describing lexical resources, it was possibile to
deal with all three languages without changing the
mechanics of the system at all.
5 Discussion
LMF is, admittedly, a ?high-level? specification,
that is, an abstract model that needs to be fur-
ther developed, adapted and specified by the lex-
icon encoder. LMF does not provide any off-the-
shelf representation for a lexical resource; instead,
it gives the basic structural components of a lexi-
con, leaving full freedom for modeling the partic-
ular features of a lexical resource. One drawback
is that LMF provides only a specification manual
with a few examples. Specifications are by no
means instructions, exactly as XML specifications
are by no means instructions on how to represent
a particular type of data.
Going from LMF specifications to a true instan-
tiation of an LMF-compliant lexicon is a long way,
and comprehensive, illustrative and detailed ex-
amples for doing this are needed. Our prototype
system provides a good starting example for this
direction. LMF is often taken as a prescriptive
description, and its examples taken as pre-defined
normative examples to be used as coding guide-
lines. Controlled and careful examples of conver-
sion to LMF-compliant formats are also needed to
avoid too subjective an interpretation of the stan-
dard.
We believe that LMF will be a major base
for various SemanticWeb applications because it
provides interoperability across languages and di-
rectly contributes to the applications themselves,
such as multilingual translation, machine aided
translation and terminology access in different lan-
guages.
From the viewpoint of LMF, our prototype
demonstrates the adaptability of LMF to a rep-
resentation of real-scale lexicons, thus promoting
its adoption to a wider community. This project
is one of the first test-beds for LMF (as one of
its drawbacks being that it has not been tested on
a wide variety of lexicons), particularly relevant
since it is related to both Western and Asian lan-
guage lexicons. This project is a concrete attempt
to specify an LMF-compliant XML format, tested
for representative and parsing efficiency, and to
provide guidelines for the implementation of an
LMF-compliant format, thus contributing to the
reduction of subjectivity in interpretation of stan-
dards.
From our viewpoint, LMF has provided a for-
mat for exchange of information across differently
conceived lexicons. Thus LMF provides a stan-
dardised format for relating them to other lexical
models, in a linguistically controlled way. This
seems an important and promising achievement in
order to move the sector forward.
6 Conclusion
This paper described the results of a three-year
project for creating an international standard for
language resources in cooperation with other ini-
tiatives. In particular, we focused on query expan-
sion using the standard.
Our main contribution can be summarised as
follows.
? We have contributed to ISO TC37/SC4 ac-
tivities, by testing and ensuring the portabil-
ity and applicability of LMF to the devel-
opment of a description framework for NLP
lexicons for Asian languages. Our contribu-
tion includes (1) a package for derivational
151
morphology, (2) the syntax-semantic inter-
face with the problem of classifiers, and (3)
representational issues with the richness of
writing systems in Asian languages. As of
October 2008, LMF including our contribu-
tions has been approved as the international
standard ISO 26413.
? We discussed Data Categories necessary
for Asian languages, and exemplified sev-
eral Data Categories including reduplication,
classifier, honorifics and orthography. We
will continue to harmonise our activity with
that of ISO TC37/SC4 TDG2 with respect to
Data Categories.
? We designed and implemented an evaluation
platform of our description framework. We
focused on linguistically motivated query ex-
pansion module. The system works with lexi-
cons compliant with LMF and ontologies. Its
most significant feature is that the system can
deal with any language as far as the those lex-
icons are described according to LMF. To our
knowledge, this is the first working system
adopting LMF.
In this project, we mainly worked on three
Asian languages, Chinese, Japanese and Thai, on
top of the existing framework which was designed
mainly for European languages. We plan to dis-
tribute our results to HLT societies of other Asian
languages, requesting for their feedback through
various networks, such as the Asian language re-
source committee network under Asian Federation
of Natural Language Processing (AFNLP)3, and
the Asian Language Resource Network project4.
We believe our efforts contribute to international
activities like ISO-TC37/SC45 (Francopoulo et al,
2006).
Acknowledgments
This research was carried out through financial
support provided under the NEDO International
Joint Research Grant Program (NEDO Grant).
References
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Modern
Information Retrieval. Addison-Wesley.
3http://www.afnlp.org/
4http://www.language-resource.net/
5http://www.tc37sc4.org/
G. Francopoulo, G. Monte, N. Calzolari, M. Mona-
chini, N. Bel, M. Pet, and C. Soria. 2006. Lex-
ical markup framework (LMF). In Proceedings of
LREC2006.
N. Ide, A. Lenci, and N. Calzolari. 2003. RDF in-
stantiation of ISLE/MILE lexical entries. In Pro-
ceedings of the ACL 2003 Workshop on Linguistic
Annotation: Getting the Model Right, pages 25?34.
A. Lenci, N. Bel, F. Busa, N. Calzolari, E. Gola,
M. Monachini, A. Ogonowsky, I. Peters, W. Peters,
N. Ruimy, M. Villegas, and A. Zampolli. 2000.
SIMPLE: A general framework for the development
of multilingual lexicons. International Journal of
Lexicography, Special Issue, Dictionaries, Thesauri
and Lexical-Semantic Relations, XIII(4):249?263.
A. Sanfilippo, N. Calzolari, S. Ananiadou,
R. Gaizauskas, P. Saint-Dizier, and P. Vossen.
1999. EAGLES recommendations on semantic
encoding. EAGLES LE3-4244 Final Report.
T. Tokunaga et al 2006. Infrastructure for standard-
ization of Asian language resources. In Proceedings
of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 827?834.
T. Tokunaga et al 2008. Adapting international stan-
dard for asian language technologies. In Proceed-
ings of the Sixth International Language Resources
and Evaluation (LREC?08).
152
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 69?74,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
SemEval-2010 Task: Japanese WSD
Manabu Okumura
Tokyo Institute of Technology
oku@pi.titech.ac.jp
Kiyoaki Shirai
Japan Advanced Institute of Science and Technology
kshirai@jaist.ac.jp
Kanako Komiya
Tokyo University of Agriculture and Technology
kkomiya@cc.tuat.ac.jp
Hikaru Yokono
Tokyo Institute of Technology
yokono@lr.pi.titech.ac.jp
Abstract
An overview of the SemEval-2 Japanese
WSD task is presented. It is a lexical
sample task, and word senses are defined
according to a Japanese dictionary, the
Iwanami Kokugo Jiten. This dictionary
and a training corpus were distributed to
participants. The number of target words
was 50, with 22 nouns, 23 verbs, and 5
adjectives. Fifty instances of each target
word were provided, consisting of a to-
tal of 2,500 instances for the evaluation.
Nine systems from four organizations par-
ticipated in the task.
1 Introduction
This paper reports an overview of the SemEval-
2 Japanese Word Sense Disambiguation (WSD)
task. It can be considered an extension of the
SENSEVAL-2 Japanese monolingual dictionary-
based task (Shirai, 2001), so it is a lexical sam-
ple task. Word senses are defined according to
the Iwanami Kokugo Jiten (Nishio et al, 1994), a
Japanese dictionary published by Iwanami Shoten.
It was distributed to participants as a sense inven-
tory. Our task has the following two new charac-
teristics:
1. All previous Japanese sense-tagged corpora
were from newspaper articles, while sense-
tagged corpora were constructed in English
on balanced corpora, such as Brown corpus
and BNC corpus. The first balanced corpus
of contemporary written Japanese (BCCWJ
corpus) is now being constructed as part of
a national project in Japan (Maekawa, 2008),
and we are now constructing a sense-tagged
corpus based on it. Therefore, the task will
use the first balanced Japanese sense-tagged
corpus.
Because a balanced corpus consists of docu-
ments from multiple genres, the corpus can
be divided into multiple sub-corpora of a
genre. In supervised learning approaches
on word sense disambiguation, because word
sense distribution might vary across different
sub-corpora, we need to take into account the
genres of training and test corpora. There-
fore, word sense disambiguation on a bal-
anced corpus requires tackling a kind of do-
main (genre) adaptation problem (Chang and
Ng, 2006; Agirre and de Lacalle, 2008).
2. In previous WSD tasks, systems have been
required to select a sense from a given set of
senses in a dictionary for a word in one con-
text (an instance). However, the set of senses
in the dictionary is not always complete. New
word senses sometimes appear after the dic-
tionary has been compiled. Therefore, some
instances might have a sense that cannot be
found in the dictionary?s set. The task will
take into account not only the instances that
have a sense in the given set but also the in-
stances that have a sense that cannot be found
in the set. In the latter case, systems should
output that the instances have a sense that is
not in the set.
Training data, a corpus that consists of three
genres (books, newspaper articles, and white pa-
pers) and is manually annotated with sense IDs,
was also distributed to participants. For the evalu-
ation, we distributed a corpus that consists of four
genres (books, newspaper articles, white papers,
and documents from a Q&A site on the WWW)
with marked target words as test data. Participants
were requested to assign one or more sense IDs to
each target word, optionally with associated prob-
abilities. The number of target words was 50, with
22 nouns, 23 verbs, and 5 adjectives. Fifty in-
stances of each target word were provided, con-
69
sisting of a total of 2,500 instances for the evalua-
tion.
In what follows, section two describes the de-
tails of the data used in the Japanese WSD task.
Section three describes the process to construct
the sense tagged data, including the analysis of an
inter-annotator agreement. Section four briefly in-
troduces participating systems and section five de-
scribes their results. Finally, section six concludes
the paper.
2 Data
In the Japanese WSD task, three types of data were
distributed to all participants: a sense inventory,
training data, and test data1.
2.1 Sense Inventory
As described in section one, word senses are
defined according to a Japanese dictionary, the
Iwanami Kokugo Jiten. The number of headwords
and word senses in the Iwanami Kokugo Jiten is
60,321 and 85,870.
As described in the task description of
SENSEVAL-2 Japanese dictionary task (Shirai,
2001), the Iwanami Kokugo Jiten has hierarchi-
cal structures in word sense descriptions. The
Iwanami Kokugo Jiten has at most three hierarchi-
cal layers.
2.2 Training Data
An annotated corpus was distributed as the train-
ing data. It consists of 240 documents of three
genres (books, newspaper articles, and white pa-
pers) from the BCCWJ corpus. The annotated in-
formation in the training data is as follows:
? Morphological information
The document was annotated with morpho-
logical information (word boundaries, a part-
of-speech (POS) tag, a base form, and a read-
ing) for all words. All the morphological in-
formation was automatically annotated using
chasen2 with unidic and was manually post-
edited.
1Due to space limits, we unfortunately cannot present the
statistics of the training and test data, such as the number
of instances in different genres, the number of instances for
a new word sense, and the Jensen Shannon (JS) divergence
(Lin, 1991; Dagan et al, 1997) between the word sense dis-
tributions of two different genres. We hope we will present
them in another paper in the near future.
2http://chasen-legacy.sourceforge.jp/
? Genre code
Each document was assigned a code indicat-
ing its genre from the aforementioned list.
? Word sense IDs
3,437 word types in the data were annotated
for sense IDs, and the data contain 31,611
sense-tagged instances that include 2,500 in-
stances for the 50 target words. Words as-
signed with sense IDs satisfied the following
conditions:
1. The Iwanami Kokugo Jiten gave their
sense description.
2. Their POSs were either a noun, a verb,
or an adjective.
3. They were ambiguous, that is, there
were more than two word senses for
them in the dictionary.
Word sense IDs were manually annotated.
2.3 Test Data
The test data consists of 695 documents of four
genres (books, newspaper articles, white papers,
and documents from a Q&A site on the WWW)
from the BCCWJ corpus, with marked target
words. The documents used for the training and
test data are not mutually exclusive. The num-
ber of overlapping documents between the train-
ing and test data is 185. The instances used for the
evaluation were not provided as the training data3.
The annotated information in the test data is as fol-
lows:
? Morphological information
Similar to the training data, the document
was annotated with morphological informa-
tion (word boundaries, a POS tag, a base
form, and a reading) for all words. All mor-
phological information was automatically an-
notated using chasen with unidic and was
manually post-edited.
? Genre code
As in the training data, each document was
assigned a code indicating its genre from the
aforementioned list.
? Word sense IDs
Word sense IDs were manually annotated for
3The word sense IDs for them were hidden from the par-
ticipants.
70
the target words4.
The number of target words was 50, with 22
nouns, 23 verbs, and 5 adjectives. Fifty instances
of each target word were provided, consisting of a
total of 2,500 instances for the evaluation.
3 Word Sense Tagging
Except for the word sense IDs, the data described
in section two was developed by the National In-
stitute of Japanese Language. However, the word
sense IDs were newly annotated on the data. This
section presents the process of annotating the word
sense IDs, and the analysis of the inter-annotator
agreement.
3.1 Sampling Target Words
When we chose target words, we considered the
following conditions:
? The POSs of target words were either a noun,
a verb, or an adjective.
? We chose words that occurred more than 50
times in the training data.
? The relative ?difficulty? in disambiguating
the sense of words was taken into account.
The difficulty of the word w was defined by
the entropy of the word sense distribution
E(w) in the test data (Kilgarriff and Rosen-
zweig, 2000). Obviously, the higher E(w) is,
the more difficult the WSD for w is.
? The number of instances for a new sense was
also taken into account.
3.2 Manual Annotation
Nine annotators assigned the correct word sense
IDs for the training and test data. All of them had a
certain level of linguistic knowledge. The process
of manual annotation was as follows:
1. An annotator chose a sense ID for each word
separately in accordance with the following
guidelines:
? One sense ID was to be chosen for each
word.
? Sense IDs at any layers in the hierarchi-
cal structures were assignable.
4They were hidden from the participants during the for-
mal run.
? The ?new word sense? tag was to be
chosen only when all sense IDs were not
absolutely applicable.
2. For the instances that had a ?new word sense?
tag, another annotator reexamined carefully
whether those instances really had a new
sense.
Because a fragment of the corpus was tagged by
multiple annotators in a preliminary annotation,
the inter-annotator agreement between the two an-
notators in step 1 was calculated with Kappa statis-
tics. It was 0.678.
4 Evaluation Methodology
The evaluation was returned in the following two
ways:
1. The outputted sense IDs were evaluated, as-
suming the ?new sense? as another sense ID.
The outputted sense IDs were compared to
the given gold standard word senses, and the
usual precision measure for supervised word
sense disambiguation systems was computed
using the scorer. The Iwanami Kokugo Jiten
has three levels for sense IDs, and we used
the middle-level sense in the task. Therefore,
the scoring in the task was ?middle-grained
scoring.?
2. The ability of finding the instances of new
senses was evaluated, assuming the task
as classifying each instance into a ?known
sense? or ?new sense? class. The outputted
sense IDs (same as in 1.) were compared to
the given gold standard word senses, and the
usual accuracy for binary classification was
computed, assuming all sense IDs in the dic-
tionary were in the ?known sense? class.
5 Participating Systems
In the Japanese WSD task, 10 organizations reg-
istered for participation. However, only the nine
systems from four organizations submitted the re-
sults. In what follows, we outline them with the
following description:
1. learning algorithm used,
2. features used,
3. language resources used,
71
4. level of analysis performed in the system,
5. whether and how the difference in the text
genre was taken into account,
6. method to detect new senses of words, if any.
Note that most of the systems used supervised
learning techniques.
? HIT-1
1. Naive Bayes, 2. Word form/POS of the
target word, word form/POS before or after
the target word, content words in the con-
text, classes in a thesaurus for those words in
the context, the text genre, 3. ?Bunrui-Goi-
Hyou?, a Japanese thesaurus (National Insti-
tute of Japanese Language, 1964), 4. Mor-
phological analysis, 5. A genre is included in
the features. 6. Assuming that the posterior
probability has a normal distribution, the sys-
tem judges those instances deviating from the
distribution at the 0.05 significance level as a
new word sense
? JAIST-1
1. Agglomerative clustering, 2. Bag-of-
words in context, etc. 3. None, 4. Mor-
phological analysis, 5. The system does not
merge example sentences in different genre
sub-corpus into a cluster. 6. First, the system
makes clusters of example sentences, then
measures the similarity between a cluster and
a sense in the dictionary, finally regarding the
cluster as a collection of new senses when
the similarity is small. For WSD, the system
chooses the most similar sense for each clus-
ter, then it considers all the instances in the
cluster to have that sense.
? JAIST-2
1. SVM, 2. Word form/POS before or after
the target word, content words in the context,
etc. 3. None, 4. Morphological analysis, 5.
The system was trained with the feature set
where features are distinguished whether or
not they are derived from only one genre sub-
corpus. 6. ?New sense? is treated as one of the
sense classes.
? JAIST-3
The system is an ensemble of JAIST-1 and
JAIST-2. The judgment of a new sense is per-
formed by JAIST-1. The output of JAIST-1 is
chosen when the similarity between a cluster
and a sense in the dictionary is sufficiently
high. Otherwise, the output of JAIST-2 is
used.
? MSS-1,2,3
1. Maximum entropy, 2. Three word
forms/lemmas/POSs before or after the target
word, bigrams, and skip bigrams in the con-
text, bag-of-words in the document, a class
of the document categorized by a topic clas-
sifier, etc. 3. None, 4. None, 5. For each tar-
get word, the system selected the genre and
dictionary examples combinations for train-
ing data, which got the best results in cross-
validation. 6. The system calculated the en-
tropy for each target word given by the Maxi-
mum Entropy Model (MEM). It assumed that
high entropy (when probabilities of classes
are uniformly dispersed) was indicative of a
new sense. The threshold was tuned by using
the words with a new sense tag in the training
data. Three official submissions correspond
to different thresholds.
? RALI-1, RALI-2
1. Naive Bayes, 2. Only the ?writing? of
the words (inside of <mor> tag), 3. The
Mainichi 2005 corpus of NTCIR, parsed with
chasen+unidic, 4. None, 5. Not taken into ac-
count, 6. ?New sense? is only used when it is
evident in the training data
For more details, please refer to their description
papers.
6 Their Results
The evaluation results of all the systems are shown
in tables 1 and 2. ?Baseline? for WSD indicates
the results of the baseline system that used SVM
with the following features:
? Morphological features
Bag-of-words (BOW), Part-of-speech (POS),
and detailed POS classification. We extract
these features from the target word itself and
the two words to the right and left of it.
? Syntactic features
? If the POS of a target word is a noun,
extract the verb in a grammatical depen-
dency relation with the noun.
72
Table 1: Results: Word sense disambiguation
Precision
Baseline 0.7528
HIT-1 0.6612
JAIST-1 0.6864
JAIST-2 0.7476
JAIST-3 0.7208
MSS-1 0.6404
MSS-2 0.6384
MSS-3 0.6604
RALI-1 0.7592
RALI-2 0.7636
Table 2: Results: New sense detection
Accuracy Precision Recall
Baseline 0.9844 - 0
HIT-1 0.9132 0.0297 0.0769
JAIST-1 0.9512 0.0337 0.0769
JAIST-2 0.9872 1 0.1795
JAIST-3 0.9532 0.0851 0.2051
MSS-1 0.9416 0.1409 0.5385
MSS-2 0.9384 0.1338 0.5385
MSS-3 0.9652 0.2333 0.5385
RALI-1 0.9864 0.7778 0.1795
RALI-2 0.9872 0.8182 0.2308
? If the POS of a target word is a verb, ex-
tract the noun in a grammatical depen-
dency relation with the verb.
? Figures in Bunrui-Goi-Hyou
4 and 5 digits regarding the content word to
the right and left of the target word.
The baseline system did not take into account any
information on the text genre. ?Baseline? for new
sense detection (NSD) indicates the results of the
baseline system, which outputs a sense in the dic-
tionary and never outputs the new sense tag. Pre-
cision and recall for NSD are shown just for refer-
ence. Because relatively few instances for a new
word sense were found (39 out of 2500), the task
of the new sense detection was found to be rather
difficult.
Tables 3 and 4 show the results for nouns, verbs,
and adjectives. In our comparison of the base-
line system scores for WSD, the score for nouns
was the biggest, and the score for verbs was the
smallest (table 3). However, the average entropy
of nouns was the second biggest (0.7257), and that
Table 3: Results for each POS (Precision): Word
sense disambiguation
Noun Verb Adjective
Baseline 0.8255 0.6878 0.732
HIT-1 0.7436 0.5739 0.7
JAIST-1 0.7645 0.5957 0.76
JAIST-2 0.84 0.6626 0.732
JAIST-3 0.8236 0.6217 0.724
MSS-1 0.7 0.5504 0.792
MSS-2 0.6991 0.5470 0.792
MSS-3 0.7218 0.5713 0.8
RALI-1 0.8236 0.6965 0.764
RALI-2 0.8127 0.7191 0.752
Table 4: Results for each POS (Accuracy): New
sense detection
Noun Verb Adjective
Baseline 0.97 0.9948 1
HIT-1 0.8881 0.9304 0.944
JAIST-1 0.9518 0.9470 0.968
JAIST-2 0.9764 0.9948 1
JAIST-3 0.9564 0.9470 0.968
MSS-1 0.9355 0.9409 0.972
MSS-2 0.9336 0.9357 0.972
MSS-3 0.96 0.9670 0.98
RALI-1 0.9745 0.9948 1
RALI-2 0.9764 0.9948 1
of verbs was the biggest (1.194)5.
We set up three word classes, D
diff
(E(w) ?
1), D
mid
(0.5 ? E(w) < 1), and D
easy
(E(w) <
0.5). D
diff
, D
mid
, and D
easy
consist of 20, 19
and 11 words, respectively. Tables 5 and 6 show
the results for each word class. The results of
WSD are quite natural in that the higher E(w) is,
the more difficult WSD is, and the more the per-
formance degrades.
7 Conclusion
This paper reported an overview of the SemEval-2
Japanese WSD task. The data used in this task will
be available when you contact the task organizer
and sign a copyright agreement form. We hope
this valuable data helps many researchers improve
their WSD systems.
5The average entropy of adjectives was 0.6326.
73
Table 5: Results for entropy classes (Precision):
Word sense disambiguation
D
easy
D
mid
D
diff
Baseline 0.9418 0.7411 0.66
HIT-1 0.8436 0.6832 0.54
JAIST-1 0.8782 0.7158 0.553
JAIST-2 0.9509 0.7484 0.635
JAIST-3 0.92 0.7368 0.596
MSS-1 0.8291 0.6558 0.522
MSS-2 0.8273 0.6558 0.518
MSS-3 0.8345 0.6905 0.536
RALI-1 0.9455 0.7653 0.651
RALI-2 0.94 0.7558 0.674
Table 6: Results for Entropy classes (Accuracy):
New sense detection
D
easy
D
mid
D
diff
Baseline 1 0.9737 0.986
HIT-1 0.8909 0.9095 0.929
JAIST-1 0.9672 0.9505 0.943
JAIST-2 1 0.9811 0.986
JAIST-3 0.9673 0.9558 0.943
MSS-1 0.9818 0.9221 0.938
MSS-2 0.98 0.9221 0.931
MSS-3 0.9873 0.9611 0.957
RALI-1 1 0.9789 0.986
RALI-2 1 0.9811 0.986
Acknowledgments
We would like to thank all the participants and the
annotators for constructing this sense tagged cor-
pus.
References
Eneko Agirre and Oier Lopez de Lacalle. 2008. On ro-
bustness and domain adaptation using svd for word
sense disambiguation. In Proc. of COLING?08.
Yee Seng Chang and Hwee Tou Ng. 2006. Estimating
class priors in domain adaptation for wsd. In Proc.
of ACL?06.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1997.
Similarity-based methods for word sense disam-
biguation. In Proceedings of the Thirty-Fifth An-
nual Meeting of the Association for Computational
Linguistics and Eighth Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 56?63.
A. Kilgarriff and J. Rosenzweig. 2000. English sense-
val: Report and results. In Proc. LREC?00.
J. Lin. 1991. Divergence measures based on the shan-
non entropy. IEEE Transactions on Information
Theory, 37(1):145?151.
Kikuo Maekawa. 2008. Balanced corpus of con-
temporary written japanese. In Proceedings of the
6th Workshop on Asian Language Resources (ALR),
pages 101?102.
National Institute of Japanese Language. 1964. Bun-
ruigoihyou. Shuuei Shuppan. In Japanese.
Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizu-
tani. 1994. Iwanami Kokugo Jiten Dai Go Han.
Iwanami Publisher. In Japanese.
Kiyoaki Shirai. 2001. Senseval-2 japanese dictionary
task. In Proceedings of SENSEVAL-2: Second Inter-
national Workshop on Evaluating Word Sense Dis-
ambiguation Systems, pages 33?36.
74
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 379?382,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
JAIST: Clustering and Classification based Approaches
for Japanese WSD
Kiyoaki Shirai Makoto Nakamura
Japan Advanced Institute of Science and Technology
{kshirai,mnakamur}@jaist.ac.jp
Abstract
This paper reports about our three par-
ticipating systems in SemEval-2 Japanese
WSD task. The first one is a clustering
based method, which chooses a sense for,
not individual instances, but automatically
constructed clusters of instances. The sec-
ond one is a classification method, which
is an ordinary SVM classifier with simple
domain adaptation techniques. The last is
an ensemble of these two systems. Results
of the formal run shows the second system
is the best. Its precision is 0.7476.
1 Introduction
This paper reports about our systems inSemEval-
2 Japanese Word Sense Disambiguation (WSD)
task (Okumura et al, 2010). This task is a lexi-
cal sample task for Japanese WSD and has the fol-
lowing two characteristics. First, a balanced word-
sense tagged corpus is used for the task. Since it
consists of sub-corpora of several domains or gen-
res, domain adaptation might be required. Second,
the task takes into account not only the instances
having a sense in the given set but also the in-
stances having a sense not found in the set (called
?new sense?). Participants are required to identify
new senses of words in this task.
The second characteristics of the task is mainly
considered in our system. A clustering based
approach is investigated to identify new senses.
Our system first constructs a set of clusters of
given word instances using unsupervised cluster-
ing techniques. This is motivated by the fact that
the new sense is not defined in the dictionary, and
sense induction without referring to the dictionary
would be required. Clusters obtained would be
sets of instances having the same sense, and some
of them would be new sense instances. Then each
cluster is judged whether instances in it have a new
sense or not. An ordinary classification-based ap-
proach is also considered. That is, WSD classifiers
are trained by a supervised learning algorithm.
Furthermore, simple techniques considering gen-
res of sub-corpora are incorporated into both our
clustering and classification based systems.
The paper continues as follows, Section 2 de-
scribes our three participating systems, JAIST-1,
JAIST-2 and JAIST-3. The results of these systems
are reported and discussed in Section 3. Finally we
conclude the paper in Section 4.
2 Systems
2.1 JAIST-1: Clustering based WSD System
JAIST-1 was developed by a clustering based
method. The overview of the system is shown in
Figure 1. It consists of two procedures: (A) clus-
ters of word instances are constructed so that the
instances of the same sense are merged, (B) then
similarity between a cluster and a sense in a dic-
tionary is measured in order to determine senses
of instances in each cluster.
Corpus
?????? (service)
S  ?????????????
     help that people who work in a
     shop give you
S  ?????????????
     help that is provided by a 
     business to customers
S  ???
     volunteer work
Dictionary
instance
(sentence)
(A) (B)
2
1
3
Figure 1: Overview of JAIST-1
2.1.1 Clustering of Word Instances
As previous work applying clustering techniques
for sense induction (Schu?tze, 1998; Agirre and
Soroa, 2007), each instance is represented by a
feature vector. In JAIST-1, the following 4 vectors
are used for clustering.
Collocation Vector This vector reflects colloca-
tion including the target instance. Words or POSs
appearing just before and after the target instance
are used as features, i.e. they correspond to one di-
mension in the vector. The weight of each feature
is 1 if the feature exists for the instance, or 0 if not.
Context Vector The vector reflects words in the
context of the target instance. All content words
appearing in the context are used as features. The
window size of the context is set to 50. Further-
more, related words are also used as features to en-
379
rich the information in the vector. Related words
are defined as follows: first topics of texts are au-
tomatically derived by Latent Dirichlet Allocation
(LDA) (Blei et al, 2003), then words which are the
most closely associated with each topic are formed
into a ?related word set?. If one word in a related
word set appears in the context, other words in
that set alo have a positive weight in the vector.
More concretely, the weight of each feature is de-
termined to be 1 if the word appears in the context
or 0.5 if the word does not appear but is in the re-
lated word set.
Association Vector Similarly to context vector,
this reflects words in the context of the target in-
stance, but data sparseness is alleviated in a differ-
ent manner. In advance, the co-occurrence matrix
A is constructed from a corpus. Each row and col-
umn in A corresponds to one of the most frequent
10,000 content words. Each element a
i,j
in the
matrix is P (w
i
|w
j
), conditional probability repre-
senting how likely it is that two words w
i
and w
j
will occur in the same document. Now j-th col-
umn in A can be regarded as the co-occurrence
vector of w
j
, ~o(w
j
). Association vector is a nor-
malized vector of sum of ~o(w
j
) for all words in
the context.
Topic Vector Unlike other vectors, this vector re-
flects topics of texts. The topics z
j
automatically
derived by PLSI (Probabilistic Latent Semantic In-
dexing) are used as features. The weight for z
j
in
the vector is P (z
j
|d
i
) estimated by Folding-in al-
gorithm (Hofmann, 1999), where d
i
is the docu-
ment containing the instance. Topic vector is mo-
tivated by the well-known fact that word senses are
highly associated with the topics of documents.
Target instances are clustered by the agglomera-
tive clustering algorithm. Similarities between in-
stances are calculated by cosine measure of vec-
tors. Furthermore, pairs of instances in different
genre sub-corpora are treated as ?cannot-link?, so
that they will not be merged into the same cluster.
Clustering procedure is stopped when the num-
ber of instances in a cluster become more than a
threshold N
c
. N
c
is set to 5 in the participating
system.
The clustering is performed 4 times using 4 dif-
ferent feature vectors. Then the best one is chosen
from the 4 sets of clusters obtained. A set of clus-
ter C (={C
i
}) is evaluated by E(C)
E(C) =
?
i
coh(C
i
) (1)
where ?cohesiveness? coh(C
i
) for each cluster C
i
is defined by (2).
coh(C
i
) =
1
|C
i
|
|C
i
|
?
j=1
rel-sim(~v
ij
, ~g
i
)
=
1
|C
i
|
|C
i
|
?
j=1
sim(~v
ij
, ~g
i
)
max
j
sim(~v
ij
, ~g
i
)
(2)
~v
ij
is an instance vector in the cluster C
i
, while ~g
i
is an average vector of C
i
. rel-sim(~v
ij
, ~g
i
) means
the relative similarity between the instance vector
and average vector. Intuitively, coh(C
i
) evaluates
how likely instances in the cluster are similar each
other. C such that E(C) is maximum is chosen as
the final set of clusters.
2.1.2 Similarity between Clusters and Senses
After clustering, similarity between a cluster C
i
and a sense S
j
in the dictionary, sim(C
i
, S
j
), is
calculated for WSD. C
i
and S
j
are represented by
cluster vector ~c
i
and sense vector ~s
j
, respectively.
Then cosine measure between these two vectors is
calculated as sim(C
i
, S
j
).
The cluster vector ~c
i
is defined as (3):
~c
i
=
1
N
?
e
ik
?C
i
?
t
l
?e
ik
~o(t
l
) (3)
In (3), e
ik
stands for an instance in the cluster C
i
,
t
l
words appearing in the context of e
ik
, ~o(t
l
) co-
occurrence vector of t
l
(similar one used in asso-
ciation vector), and N the constant for normaliza-
tion. So ~c
i
is similar to association vector, but the
co-occurrence vectors of words in the contexts of
all instances in the cluster are summed.
The sense vector ~s
j
is defined as in (4).
~s
j
=
1
N
?
?
?
t
k
?D
j
~o(t
k
) +
?
t
l
?E
j
w
e
? ~o(t
l
)
?
? (4)
D
j
stands for definition sentences of the sense S
j
in the Japanese dictionary Iwanami Kokugo Jiten
(the sense inventory in this task), while E
j
a set of
example sentences of S
j
. Here E
j
includes both
example sentences from the dictionary and ones
excerpted from a sense-tagged corpus, the train-
ing data of this task. w
e
is the parameter putting
more weight on words in example sentences than
in definition sentences. We set w
e
= 2.0 through
the preliminary investigation.
Based on sim(C
i
, S
j
), the system judges
whether the cluster is a collection of new
380
sense instances. Suppose that MaxSim
i
is
max
j
sim(C
i
, S
j
), the maximum similarity be-
tween the cluster and the sense. If MaxSim
i
is
small, the cluster C
i
is not similar to any defined
senses, so instances in C
i
could have a new sense.
The system regards that the sense of instances in
C
i
is new when MaxSim
i
is less than a thresh-
old T
ns
. Otherwise, it regards the sense of in-
stances in C
i
as the most similar sense, S
j
such
that j = argmax
j
sim(C
i
, S
j
).
The threshold T
ns
for each target word is deter-
mined as follows. First the training data is equally
subdivided into two halves, the development data
D
dev
and the training data D
tr
. Next, JAIST-1 is
run for instances in D
dev
, while example sentences
in D
tr
are used as E
j
in (4) when sense vectors are
constructed. For words where new sense instances
exist in D
dev
, T
ns
is optimized for the accuracy
of new sense detection. For words where no new
sense instances are found in D
dev
, T
ns
is deter-
mined by the minimum of MaxSim
i
as follows:
T
ns
= (min
i
MaxSim
i
) ? ? (5)
Since even the cluster of which MaxSim
i
is min-
imum represents not a new but a defined sense, the
minimum of MaxSim
i
is decreased by ?. To de-
termine ?, the ratios
MaxSim
i
of clusters of new senses
MaxSim
i
of clusters of defined senses
(6)
are investigated for 5 words1. Since we found the
ratios are more than 0.95, we set ? to 0.95.
2.2 JAIST-2: SVM Classifier with Simple
Domain Adaptation
Our second system JAIST-2 is the classification
based method. It is a WSD classifier trained by
Support Vector Machine (SVM). SVM is widely
used for various NLP tasks including Japanese
WSD (Shirai and Tamagaki, 2004). In this system,
new sense is treated as one of the sense classes.
Thus it would never choose ?new sense? for any
instances when no new sense instance is found in
the training data. We used the LIBSVM package2
to train the SVM classifiers. Linear kernel is used
with default parameters.
The following conventional features of WSD
are used for training the SVM classifiers.
1Among 50 target words in this task, there exist new
sense instances of only ?kanou?(possibility) in D
dev
. So we
checked 4 more words, other than target words.
2http://www.csie.ntu.edu.tw/?cjlin/
libsvm/
? W (0),W (?1),W (?2),W (+1),W (+2)
P (?1), P (?2), P (+1), P (+2)
Words and their POSs appearing before or af-
ter a target instance. A number in parentheses
indicates the position of a word from a target
instance. W (0) means a target instance itself.
? W (?2)&W (?1),W (+1)&W (+2),W (?1)&W (+1)
P (?2)&P (?1), P (+1)&P (+2), P (?1)&P (+1)
Pairs of words (or their POSs) near a target
instance.
? Base form of content words appearing in the
context (bag-of-words).
The data used in this task is a set of documents
with 4 different genre codes: OC (Web page),
OW (white paper), PB (book) and PN (newspa-
per). The training data consists of documents of
3 genres OW, PB and PN, while the test data con-
tains all 4 genres. Considering domain adaptation,
each feature f
i
is represented as f
i
+g when SVM
classifiers are trained. g is one of the genre codes
{OW,PB,PN} if f
i
is derived from the docu-
ments of only one genre g in the training data, oth-
erwise g is ?multi?. For instances in the test data,
only features f
i
+g
t
and f
i
+multi are used, where
g
t
is the genre code of the document of the target
instance. If g
t
is OC (which is not included in the
training data), however, all features are used. The
above method aims at distinguishing genre intrin-
sic features and improving the WSD performance
by excluding features which might be associated
with different genres.
2.3 JAIST-3: Ensemble of Two Systems
The third system combines clustering based
method (JAIST-1) and classification based method
(JAIST-2). The basic idea is that JAIST-1 be used
only for reliable clusters, otherwise JAIST-2 is
used. Here ?reliable cluster? means a cluster such
that MaxSim
i
is high. The greater the similar-
ity between the cluster and the sense is, the more
likely the chosen sense is correct. Furthermore,
JAIST-1 is used for new sense detection. The de-
tailed procedure in JAIST-3 is:
1. If JAIST-1 judges a cluster to be a collection
of new sense instances, output ?new sense?
for instances in that cluster.
2. For instances in the top N
cl
clusters of
MaxSim
i
,output senses chosen by JAIST-1.
3. Otherwise output senses chosen by JAIST-2.
381
For the optimization of N
cl
, D
dev
and D
tr
, each
is a half of the training data described in Subsec-
tion 2.1, are used. D
tr
is used for training SVM
classifiers (JAIST-2). Then N
cl
is determined so
that the precision of WSD on D
dev
is optimized.
In the participating system, N
cl
is set to 1.
3 Evaluation
Table 1 shows the results of our participating sys-
tems and the baseline system MFS, which always
selects the most frequent sense in the training
data. The column WSD reveals the precision (P)
of word sense disambiguation, while the column
NSD shows accuracy (A), precision (P) and recall
(R) of new sense detection.
Table 1: Results
WSD NSD
P A P R
MFS 0.6896 0.9844 0 0
JAIST-1 0.6864 0.9512 0.0337 0.0769
JAIST-2 0.7476 0.9872 1 0.1795
JAIST-3 0.7208 0.9532 0.0851 0.2051
JAIST-1 is the clustering based method. Perfor-
mance of the clustering is also evaluated: Purity
was 0.9636, Inverse-Purity 0.1336 and F-measure
0.2333. Although this system was designed for
new sense detection, it seems not to work well.
It could correctly find only three new sense in-
stances. The main reason is that there were few
instances of the new sense in the test data. Among
2,500 instances (50 instances of each word, for 50
target word), only 39 instances had the new sense.
Our system supposes that considerable number of
new sense instances exist in the corpus, and tries to
gather them into clusters. However, JAIST-1 was
able to construct only one cluster containing mul-
tiple new sense instances. The proposed method is
inadequate for new sense detection when the num-
ber of new sense instances is quite small.
For domain adaptation, features which are in-
trinsic to different genres were excluded for test
instances in JAIST-2. When we trained the system
using all features, its precision was 0.7516, which
is higher than that of JAIST-2. Thus our method
does not work at all. This might be caused by re-
moving features that were derived from different
genre sub-corpora, but effective for WSD. More
sophisticated ways to remove ineffective features
would be required.
JAIST-3 is the ensemble of JAIST-1 and JAIST-
2. Although a little improvement is found by com-
bining two different systems in our preliminary ex-
periments, however, the performance of JAIST-3
was worse than JAIST-2 because of the low per-
formance of JAIST-1. We compared WSD pre-
cision of three systems for 50 individual target
words, and found that JAIST-2 is almost always
the best. The only exceptional case was the target
word ?ookii?(big). For this adjective, the precision
of JAIST-1, JAIST-2 and JAIST-3 were 0.74, 0.16
and 0.18, respectively. The precision of SVM clas-
sifiers (JAIST-2) is quite bad because of the differ-
ence of text genres. All 50 test instances of this
word were excerpted from Web sub-corpus, which
was not included in the training data. Furthermore,
word sense distributions of test and training data
were totally different. JAIST-1 works better in
such a case. Thus clustering based method might
be an alternative method for WSDwhen sense dis-
tribution in the test data is far from the training
data.
4 Conclusion
The paper reports the participating systems in
SemEval-2 Japanese WSD task. Clustering based
method was designed for new sense detection,
however, it was ineffective when there were few
new sense instances. In future, we would like to
examine the performance of our method when it is
applied to a corpus including more new senses.
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007
task 02: Evaluating word sense induction and dis-
crimination systems. In Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 7?12.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the SIGIR, pages 50?
57.
Manabu Okumura, Kiyoaki Shirai, Kanako Komiya,
and Hikaru Yokono. 2010. Semeval-2010 task:
Japanese WSD. In Proceedings of the SemEval-
2010: 5th International Workshop on Semantic
Evaluations.
Hinrich Schu?tze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123.
Kiyoaki Shirai and Takayuki Tamagaki. 2004. Word
sense disambiguation using heterogeneous language
resources. In Proceedings of the First IJCNLP,
pages 614?619.
382
