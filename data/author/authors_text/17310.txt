Proceedings of the ACL Student Research Workshop, pages 117?122,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Addressing Ambiguity in Unsupervised Part-of-Speech Induction with
Substitute Vectors
Volkan Cirik
Artificial Intelligence Laboratory
Koc University, Istanbul, Turkey
vcirik@ku.edu.tr
Abstract
We study substitute vectors to solve the
part-of-speech ambiguity problem in an
unsupervised setting. Part-of-speech tag-
ging is a crucial preliminary process in
many natural language processing applica-
tions. Because many words in natural lan-
guages have more than one part-of-speech
tag, resolving part-of-speech ambiguity is
an important task. We claim that part-
of-speech ambiguity can be solved using
substitute vectors. A substitute vector is
constructed with possible substitutes of a
target word. This study is built on pre-
vious work which has proven that word
substitutes are very fruitful for part-of-
speech induction. Experiments show that
our methodology works for words with
high ambiguity.
1 Introduction
Learning syntactic categories of words (i.e. part-
of-speech or POS tagging) is an important pre-
processing step for many natural language pro-
cessing applications because grammatical rules
are not functions of individual words, instead, they
are functions of word categories. Unlike super-
vised POS tagging systems, POS induction sys-
tems make use of unsupervised methods. They
categorize the words without any help of annotated
data.
POS induction is a popular topic and several
studies (Christodoulopoulos et al, 2010) have
been performed. Token based methods (Berg-
Kirkpatrick and Klein, 2010; Goldwater and Grif-
fiths, 2007) categorize word occurrences into syn-
tactic groups. Type based methods (Clark, 2003;
Blunsom and Cohn, 2011) on the other hand, cat-
egorize word types and yield the ambiguity prob-
lem unlike the token based methods.
Type based methods suffer from POS ambigu-
ity because one POS tag is assigned to each word
type. However, occurrences of many words may
have different POS tags. Two examples below are
drawn from the dataset we worked on. They il-
lustrate a situation where two occurrences of the
?offers? have different POS tags. In the first sen-
tence ?offers? is a noun, whereas, in the second
sentence it is a verb.
(1) ?Two rival bidders for Connaught
BioSciences extended their offers to ac-
quire the Toronto-based vaccine manu-
facturer Friday.?
(2) ?The company currently offers a
word-processing package for personal
computers called Legend.?
In this study, we try to extend the state-of-the-
art unsupervised POS tagger (Yatbaz et al, 2012)
by solving the ambiguity problem it suffers be-
cause it has a type based approach. The clustering
based studies (Schu?tze, 1995) (Mintz, 2003) rep-
resent the context of a word with a vector using
neighbour words. Similarly, (Yatbaz et al, 2012)
proposes to use word context. They claim that the
substitutes of a word have similar syntactic cate-
gories and they are determined by the context of
the word.
In addition, we suggest that the occurrences
with different part-of-speech categories of a word
should be seen in different contexts. In other
words, if we categorize the contexts of a word type
we can determine different POS tags of the word.
We represent the context of a word by construct-
ing substitute vectors using possible substitutes of
the word as (Yatbaz et al, 2012) suggests.
Table 1 illustrates the substitute vector of the oc-
currence of ?offers? in (1). There is a row for each
word in the vocabulary. For instance, probability
of occurring ?agreement? in the position of ?of-
fers? is 80% in this context. To resolve ambiguity
117
Probability Substitute Word
0.80 agreement
0.03 offer
0.01 proposal
0.01 bid
0.01 attempt
0.01 bids
. .
. .
. .
Table 1: Substitute Vector for ?offers? in above
sentence.
of a target word, we separate occurrences of the
word into different groups depending on the con-
text information represented by substitute vectors.
We conduct two experiments. In the first ex-
periment, for each word type we investigated, we
separate all occurences into two categories using
substitute vectors. In the second one we guess the
number of the categories we should separate for
each word type. Both experiments achieve bet-
ter than (Yatbaz et al, 2012) for highly ambigu-
ous words. The level of ambiguity can be mea-
sured with perplexity of word?s gold tag distribu-
tion. For instance,the gold tag perplexity of word
?offers? in the Penn Treebank Wall Street Journal
corpus we worked on equals to 1.966. Accord-
ingly, the number of different gold tags of ?of-
fers? is 2. Whereas, perplexity of ?board? equals
to 1.019. Although the number of different tags
for ?board? is equal to 2, only a small fraction
of the tags of board differs from each other. We
can conclude that ?offers? is more ambiguous than
?board?.
In this paper we present a method to solve POS
ambiguity for a type based POS induction ap-
proach. For the rest of the paper, we explain our
algorithm and the setup of our experiments. Lastly
we present the results and a conclusion.
2 Algorithm
We claim that if we categorize contexts a word
type occurs in, we can address ambiguity by sep-
arating its occurrences before POS induction. In
order to do that, we represent contexts of word
occurrences with substitute vectors. A substi-
tute vector is formed by the whole vocabulary of
words and their corresponding probabilities of oc-
curring in the position of the target word. To cal-
culate these probabilities, as described in (Yatbaz
et al, 2012), a 4-gram language model is built
with SRILM (Stolcke, 2002) on approximately
126 million tokens of Wall Street Journal data
(1987-1994) extracted from CSR-III Text (Graff
et al, 1995).
We generate substitute vectors for all tokens in
our dataset. We want to cluster occurrences of our
target words using them. In each substitute vector,
there is a row for every word in the vocabulary.
As a result, the dimension of substitute vectors is
equal to 49,206. Thus, in order not to suffer from
the curse of dimensionality, we reduce dimensions
of substitute vectors.
Before reducing the dimensions of these vec-
tors, distance matrices are created using Jensen
distance metric for each word type in step (a) of
Figure 1. We should note that these matrices are
created with substitute vectors of each word type,
not with all of the substitute vectors.
In step (b) of Figure 1, to reduce dimensionality,
the ISOMAP algorithm (Tenenbaum et al, 2000)
is used. The output vectors of the ISOMAP al-
gorithm are in 64 dimensions. We repeated our
experiments for different numbers of dimensions
and the best results are achieved when vectors are
in 64 dimensions.
In step (c) of Figure 1, after creating vectors
in lower dimension, using a modified k-means
algorithm (Arthur and Vassilvitskii, 2007) 64-
dimensional vectors are clustered for each word
type. The number of clusters given as an input to
k-means varies with experiments. We induce num-
ber of POS tags of a word type at this step.
Previous work (Yatbaz et al, 2012) demon-
strates that clustering substitute vectors of all word
types alone has limited success in predicting part-
of-speech tag of a word. To make use of both word
identity and context information of a given type,
we use S-CODE co-occurrence modeling (Maron
et al, 2010) as (Yatbaz et al, 2012) does.
Given a pair of categorical variables, the S-
CODE model represents each of their values on a
unit sphere such that frequently co-occurring val-
ues are located closely. We construct the pairs to
feed S-CODE as follows.
In step (d) of Figure 1, the first part of the pair is
the word identity concatenated with cluster ids we
got from the previous step. The cluster ids separate
word occurrences seen in different context groups.
By doing that, we make sure that the occurrences
118
Figure 1: General Flow of The Algorithm
of a same word can be separated on the unit sphere
if they are seen in different context groups.
The second part of the pair is a substitute word.
For an instance of a target word, we sample a sub-
stitute word according to the target word?s sub-
stitute vector probabilities. If occurrences of two
different or the same word types have the same
substitutes, they should be seen in the similar con-
texts. As a result, words occurring in the simi-
lar contexts will be close to each other on the unit
sphere. Furthermore, they will have the same POS
tags. We should note that the co-occurrence input
file contains all word types.
In step (e) of Figure 1, on the output of the S-
CODE sphere, the words occurring in the simi-
lar contexts and having the same word-identity are
closely located. Thus, we observe clusters on the
unit sphere. For instance, verb occurrences of ?of-
fers? are close to each other on the unit sphere.
They are also close to other verbs. Furthermore,
119
they are separated with occurrences of ?offers?
which are nouns.
Lastly, in step (f) of Figure 1, we run k-means
clustering method on the S-CODE sphere and split
word-substitute word pairs into 45 clusters be-
cause the treebank we worked on uses 45 part-
of-speech tags. The output of clustering induces
part-of-speech categories of words tokens.
3 Experiments
In this section, the setup of each experiment will
be presented. The experiments are conducted on
Penn Treebank Wall Street Journal corpus. There
are 1,173,766 tokens and, 49,206 types. Out of
49,206 word types, 1183 of them are chosen as
target words. They are fed to the algorithm de-
scribed above. Occurrences of these target words
correspond to 37.55% of the whole data. These
target words are seen in the dataset more than 100
times and less than 4000 times. This subset is cho-
sen as such because word types occurring more
than 4000 times are all with low gold tag perplex-
ity. They also increase computation time dramat-
ically. We exclude word types occurring less than
100 times, because the clustering algorithm run-
ning on 64-dimension vectors does not work accu-
rately. To avoid providing noisy results, the exper-
iments are repeated 10 times. We report many-to-
one scores of the experiments. The many-to-one
evaluation assigns each cluster to its most frequent
gold-tag. Overall result demonstrates the percent-
age of correctly assigned instances and standard
deviation in paranthesis.
3.1 Baseline
Because we are trying to improve (Yatbaz et al,
2012), we select the experiment on Penn Tree-
bank Wall Street Journal corpus in that work as
our baseline and replicate it. In that experiment,
POS induction is done by using word identities
and context information represented by substitute
words. Strictly one tag is assigned to each word
type. As a result, this method inaccurately induces
POS tags for the occurrences of word types with
high gold tag perplexity. The many-to-one accu-
racy of this experiment is 64%.
3.2 Upperbound
In this experiment, for each word occurence, we
concatenate the gold tag for the first part of the
pairs in the co-occurence input file. Thus, we
skipped steps (a), (b), (c). The purpose of this
experiment is to set an upperbound for all experi-
ments since we cannot cluster the word tokens any
better than the gold tags. The many-to-one accu-
racy of this experiment is 67.2%.
3.3 Experiment 1
In the algorithm section, we mention that after di-
mensionality reduction step, we cluster the vec-
tors to separate tokens of a target word seen in the
similar contexts. In this experiment, we set the
number of clusters for each type to 2. In other
words, we assume that the number of different
POS tags of each word type is equal to 2. Nev-
ertheless, separating all the words into 2 clusters
results in some inaccuracy in POS induction. That
is because not all words have POS ambiguity and
some have more than 2 different POS tags How-
ever, the main purpose of this experiment is to ob-
serve whether we can increase the POS induction
accuracy for ambiguous types with our approach.
The many-to-one accuracy of this experiment is
63.8%.
3.4 Experiment 2
In the previous experiment, we set the number of
clusters for each word type to 2. However, the
number of different POS tags differs for each word
type. More importantly, around 41% of our target
tokens belongs to unambiguous word types. Also,
around 36% of our target tokens comes from word
types whose gold perplexity is below 1.5. That
means, the Experiment 1 splits most of our word
types that should not be separated.
In this experiment, instead of splitting all types,
we guess which types should be splitted. Also, we
guess the number of clusters for each type. We
use gap statistic (Tibshirani et al, 2001) on 64-
dimensional vectors. The Gap statistic is a sta-
tistical method to guess the number of clusters
formed in given data points. We expect that substi-
tute vectors occurring in the similar context should
be closely located in 64-dimensional space. Thus,
gap statistic can provide us the number of groups
formed by vectors in 64-dimensional space. That
number is possibly equal to the number of the
number of different POS tags of the word types.
The many-to-one accuracy of this experiment is
63.4%.
120
3.5 Experiment 3
In this experiment, we set the number of clusters
for each type to gold number of tags of each type.
The purpose of this experiment is to observe how
the accuracy of number of tags given, which is
used at step (c), affects the system. The many-
to-one accuracy of this experiment is 63.9%.
3.6 Overall Results
In this section we present overall results of the
experiments. We present our results in 3 sepa-
rated tables because the accuracy of these methods
varies with the ambiguity level of word types.
In Table 2, many-to-one scores of three exper-
iments are presented. Since we exclude some of
the word types, our results correspond to 37.55%
of the data. In Table 3, results for the word types
whose gold tag perplexity is lower than 1.5 are
presented. They correspond to 29.11% of the data.
Lastly, in Table 4, we present the results for word
types whose gold tag perplexity is greater than 1.5.
Experiment Many-to-One Score
Baseline .64 (.01)
Experiment 1 .638 (.01)
Experiment 2 .634 (.01)
Experiment 3 .639 (.02)
Table 2: Results for the target words correspond-
ing to 37.55% of the data.
Experiment Many-to-One Score
Baseline .693 (.02)
Experiment 1 .682 (.01)
Experiment 2 .68 (.01)
Experiment 3 .684 (.02)
Table 3: Results for Target Words with gold tag
perplexity ?1.5 which corresponds to 29.11% of
the data.
Experiment Many-to-One Score
Baseline .458 (.01)
Experiment 1 .484 (.01)
Experiment 2 .474 (.02)
Experiment 3 .483 (.02)
Table 4: Results for Target Words with gold tag
perplexity ?1.5 which corresponds to 8.44% of
the data..
4 Conclusion
Table 2 shows that the baseline experiment is
slightly better than our experiments. That is be-
cause our experiments inaccurately induce more
than one tag to unambiguous types. Additionally,
most of our target words have low gold tag per-
plexity. Table 3 supports this claim. In Table 4,
we observe that our methods outscore the baseline
significantly. That is because, when ambiguity in-
creases, the baseline method inaccurately assigns
one POS tag to word types. On the other hand, the
gap statistic method is not fully efficient in guess-
ing the number of clusters. It sometimes separates
unambiguous types or it does not separate highly
ambiguous word types. As a result, there is a slight
difference between the results of our experiments.
Additionally, the results of our experiments
show that, accurately guessing number of clusters
plays a crucial role in this approach. Even using
the gold number of different tags in Experiment 3
does not result in a significantly accurate system.
That is because, the number of different tags does
not reflect the perplexity of a word type.
The results show that, POS ambiguity can be
addressed by using substitute vectors for word
types with high ambiguity. The accuracy of this
approach correlates with the level of ambiguity of
word types. Thus, the detection of the level of am-
biguity for word types should be the future direc-
tion of this research. We again propose that substi-
tute vector distributions could be useful to extract
perplexity information for a word type.
Acknowledgments
I would like to thank the members of the Koc Uni-
versity Artificial Intelligence Laboratory for their
help and support. Additionally, I would like to
thank two anonymous reviewers and Murat Sey-
han for their comments and suggestions.
References
D. Arthur and S. Vassilvitskii. 2007. k-means++: The
advantages of careful seeding. In Proceedings of the
eighteenth annual ACM-SIAM symposium on Dis-
crete algorithms, pages 1027?1035. Society for In-
dustrial and Applied Mathematics.
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phylo-
genetic grammar induction. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1288?1297, Uppsala,
121
Sweden, July. Association for Computational Lin-
guistics.
Phil Blunsom and Trevor Cohn. 2011. A hierarchi-
cal pitman-yor process hmm for unsupervised part
of speech induction. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
865?874, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsuper-
vised pos induction: how far have we come? In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 575?584, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the tenth conference on Eu-
ropean chapter of the Association for Computational
Linguistics - Volume 1, EACL ?03, pages 59?66,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 744?751, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
David Graff, Roni Rosenfeld, and Doug Paul. 1995.
Csr-iii text. Linguistic Data Consortium, Philadel-
phia.
Yariv Maron, Michael Lamar, and Elie Bienenstock.
2010. Sphere embedding: An application to part-of-
speech induction. In J. Lafferty, C. K. I. Williams,
J. Shawe-Taylor, R.S. Zemel, and A. Culotta, ed-
itors, Advances in Neural Information Processing
Systems 23, pages 1567?1575.
T.H. Mintz. 2003. Frequent frames as a cue for gram-
matical categories in child directed speech. Cogni-
tion, 90(1):91?117.
Hinrich Schu?tze. 1995. Distributional part-of-speech
tagging. In Proceedings of the seventh conference
on European chapter of the Association for Compu-
tational Linguistics, EACL ?95, pages 141?148, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Andreas Stolcke. 2002. Srilm-an extensible lan-
guage modeling toolkit. In Proceedings Interna-
tional Conference on Spoken Language Processing,
pages 257?286, November.
J.B. Tenenbaum, V. Silva, and J.C. Langford. 2000.
A global geometric framework for nonlinear dimen-
sionality reduction. Science, 290(5500):2319.
R. Tibshirani, G. Walther, and T. Hastie. 2001. Es-
timating the number of data clusters via the gap
statistic. Journal of the Royal Statistical Society B,
63:411?423.
Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret. 2012.
Learning syntactic categories using paradigmatic
representations of word context. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 940?951, Jeju
Island, Korea, July. Association for Computational
Linguistics.
122
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 300?306, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
AI-KU: Using Substitute Vectors and Co-Occurrence Modeling for Word
Sense Induction and Disambiguation
Osman Bas?kaya Enis Sert Volkan Cirik Deniz Yuret
Artificial Intelligence Laboratory
Koc? University, I?stanbul, Turkey
{obaskaya,esert,vcirik,dyuret}@ku.edu.tr
Abstract
Word sense induction aims to discover differ-
ent senses of a word from a corpus by us-
ing unsupervised learning approaches. Once a
sense inventory is obtained for an ambiguous
word, word sense discrimination approaches
choose the best-fitting single sense for a given
context from the induced sense inventory.
However, there may not be a clear distinction
between one sense and another, although for
a context, more than one induced sense can
be suitable. Graded word sense method al-
lows for labeling a word in more than one
sense. In contrast to the most common ap-
proach which is to apply clustering or graph
partitioning on a representation of first or sec-
ond order co-occurrences of a word, we pro-
pose a system that creates a substitute vec-
tor for each target word from the most likely
substitutes suggested by a statistical language
model. Word samples are then taken accord-
ing to probabilities of these substitutes and the
results of the co-occurrence model are clus-
tered. This approach outperforms the other
systems on graded word sense induction task
in SemEval-2013.
1 Introduction
There exists several drawbacks of representing the
word senses with a fixed-list of definitions of a man-
ually constructed lexical database. There is no guar-
antee that they reflect the exact meaning of a tar-
get word in a given context since they usually con-
tain definitions that are too general (Ve?ronis, 2004).
More so, lexical databases often include many rare
senses while missing corpus/domain-specific senses
(Pantel and Lin, 2004). The goal of Word Sense In-
duction (WSI) is to solve these problems by auto-
matically discovering the meanings of a target word
from a text, not pre-defined sense inventories. Word
Sense Discrimination (WSD) approaches determine
best-fitting sense among the meanings that are dis-
covered for an ambiguous word. However, (Erk
et al, 2009) suggested that annotators often gave
high ratings to more than one WordNet sense for the
same occurrence. They introduced a novel annota-
tion paradigm allowing that words have more than
one sense with a degree of applicability.
Unlike previous SemEval tasks in which systems
labeled a target word?s meaning with only one sense,
word sense induction task in SemEval-2013 relaxes
this by allowing a target word to have more than one
sense if applicable.
Word sense induction approaches can be catego-
rized into graph based models, bayesian, and vector-
space ones. In graph-based approaches, every con-
text word is represented as a vertex and if two con-
text words co-occur in one or more instances of a
target word, then two vertices are connected with
an edge. When the graph is obtained, one of the
graph clustering algorithm is employed. As a result,
different partitions indicate the different senses of a
target word (Ve?ronis, 2004). Agirre et al (2006) ex-
plored the use of two graph algorithms for unsuper-
vised induction and tagging of nominal word senses
based on corpora. Recently, Korkontzelos and Man-
andhar (2010) proposed a graph-based model which
achieved good results on word sense induction and
discrimination task in SemEval-2010.
300
Brody and Lapata (2009) proposed a Bayesian
approach modeling the contexts of the ambiguous
word as samples from a multinomial distribution
over senses which are in turn characterized as dis-
tributions over words.
Vector-space models, on the other hand, typically
create context vector by using first or second or-
der co-occurrences. Once context vector has been
constructed, different clustering algorithms may be
applied. However, representing the context with
first or second order co-occurrences can be difficult
since there are plenty of parameters to be consid-
ered such as the order of occurrence, context win-
dow size, statistical significance of words in the con-
text window and so on. Instead of dealing with
these, we suggest representing the context with the
most likely substitutes determined by a statistical
language model. Statistical language models based
on large corpora has been examined in (Yuret, 2007;
Hawker, 2007; Yuret and Yatbaz, 2010) for unsuper-
vised word sense disambiguation and lexical substi-
tution. Moreover, the best results in unsupervised
part-of-speech induction achieved by using substi-
tute vectors (Yatbaz et al, 2012).
In this paper, we propose a system that represents
the context of each target word by using high prob-
ability substitutes according to a statistical language
model. These substitute words and their probabili-
ties are used to create word pairs (instance id - sub-
stitute word) to feed our co-occurrence model. The
output of the co-occurrence model is clustered by k-
means algorithm. Our systems perform well among
other submitted systems in SemEval-2013.
Rest of the paper is organized as follows. Sec-
tion 2 describes the provided datasets and evalu-
ation measures of the task. Section 3 gives de-
tails of our algorithm and is divided into five con-
tiguous subsections that correspond to each step of
our system. In Section 4 we present the differ-
ences between our three systems and their perfor-
mances. Finally, Section 5 summarizes our work in
this task. The code to replicate this work is available
at http://goo.gl/jPTZQ.
2 Data and Evaluation Methodology
The test data for the graded word sense induction
task in SemEval-2013 includes 50 terms containing
20 verbs, 20 nouns and 10 adjectives. There are a
total of 4664 test instances provided. All evalua-
tion was performed on test instances only. In ad-
dition, the organizers provided sense labeled trial
data which can be used for tuning. This trial data
is a redistribution of the Graded Sense and Usage
data set provided by Katrin Erk, Diana McCarthy,
and Nicholas Gaylord (Erk et al, 2009). It consists
of 8 terms; 3 verbs, 3 nouns, and 2 adjectives all
with moderate polysemy (4-7 senses). Each term
in trial data has 50 contexts, in total 400 instances
provided. Lastly, participants can use ukWaC1, a 2-
billion word web-gathered corpus, for sense induc-
tion. Furthermore, unlike in previous WSI tasks, or-
ganizers allow participants to use additional contexts
not found in the ukWaC under the condition that they
submit systems for both using only the ukWaC and
with their augmented corpora.
The gold-standard of test data was prepared using
WordNet 3.1 by 10 annotators. Since WSI systems
report their annotations in a different sense inven-
tory than WordNet 3.1, a mapping procedure should
be used first. The organizers use the sense mapping
procedure explained in (Jurgens, 2012). This proce-
dure has adopted the supervised evaluation setting
of past SemEval WSI Tasks, but the main differ-
ence is that the former takes into account applica-
bility weights for each sense which is a necessary
for graded word sense.
Evaluation can be divided into two categories: (1)
a traditional WSD task for Unsupervised WSD and
WSI systems, (2) a clustering comparison setting
that evaluates the similarity of the sense inventories
for WSI systems. WSD evaluation is made accord-
ing to three objectives:
? Their ability to detect which senses are appli-
cable (Jaccard Index is used)
? Their ability to rank the applicable senses ac-
cording to the level of applicability (Weighted
Kendall?s ? is used)
? Their ability to quantify the level of applicabil-
ity for each sense (Weighted Normalized Dis-
counted Cumulative Gain is used)
Clustering comparison is made by using:
1Available here: http://wacky.sslmit.unibo.it
301
? Fuzzy Normalized Mutual Information: It cap-
tures the alignment of the two clusterings inde-
pendent of the cluster sizes and therefore serves
as an effective measure of the ability of an ap-
proach to accurately model rare senses.
? Fuzzy B-Cubed: It provides an item-based
evaluation that is sensitive to the cluster size
skew and effectively captures the expected per-
formance of the system on a dataset where the
cluster (i.e., sense) distribution would be equiv-
alent.
More details can be found on the task website.2
3 Algorithm
In this section, we explain our algorithm. First, we
describe data enrichment procedure then we will an-
swer how each instance?s substitute vector was con-
structed. In contrast to common practice which is
clustering the context directly, we first performed
word sampling on the substitute vectors and cre-
ated instance id - substitute word pairs as explained
in Subsection 3.3. These pairs were used in the
co-occurrence modeling step described in Subsec-
tion 3.4. Finally, we clustered these co-occurrence
modeling output with the k-means clustering algo-
rithm. It is worth noting that this pipeline is per-
formed on each target word separately.
SRILM (Stolcke, 2002) is employed on entire
ukWaC corpus for the 4-gram language model to
conduct all experiments.
3.1 Data Enrichment
Data enrichment aims to increase the number of in-
stances of target words. Our preliminary experi-
ments on the trial data showed that additional con-
texts increase the performance of our systems.
Assuming that our target word is book in noun
form. We randomly fetch 20,000 additional contexts
from ukWaC where our target word occurs with the
same part-of-speech tag. This implies that we skip
those sentences in which the word book functions as
a verb. These additional contexts are labeled with
unique numbers so that we can distinguish actual in-
stances in the test data. We follow this procedure for
2www.cs.york.ac.uk/semeval-2013/task13/
Substitute Probability
solve 0.305
complete 0.236
meet 0.096
overcome 0.026
counter 0.022
tackle 0.014
address 0.012
... ...
... ...
Table 1: The most likely substitutes for meet
every target word in the test data. In total, 1 mil-
lion additional instances were fetched from ukWac.
Hereafter we refer to this new dataset with as an ex-
panded dataset.
3.2 Substitute Vectors
Unlike other WSI methods which rely on the first or
the second order co-occurrences (Pedersen, 2010),
we represent the context of each target word instance
by finding the most likely substitutes suggested by
the 4-gram language model we built from ukWaC
corpus. The high probability substitutes reflect both
semantic and syntactic properties of the context as
seen in Table 1 for the following example:
And we need Your help to meet the chal-
lenge!
For every instance in our expanded dataset, we
use three tokens each on the left and the right side of
a target word as a context when estimating the prob-
abilities for potential lexical substitutes. This tight
window size might seem limited, however, tight con-
text windows give better scores for semantic simi-
larity, while larger context windows or second-order
context words are better for modeling general top-
ical relatedness (Sahlgren, 2006; Peirsman et al,
2008).
Fastsubs (Yuret, 2012) was used for this process
and the top 100 most likely substitutes were used for
representing each instance since the rest of the sub-
stitutes had negligible probabilities. These top 100
probabilities were normalized to add up to 1.0 giv-
ing us a final substitute vector for a particular target
word?s instance. Note that the substitute vector is a
302
Instance ID Substitute Word
meet1 complete
meet1 solve
meet1 solve
meet1 overcome
... ...
... ...
meet1 meet
meet1 complete
meet1 solve
meet1 solve
Table 2: Substitute word sampling for instance meet1
Figure 1: Co-Occcurrence Embedding Sphere for meet
function of the context only and is indifferent to the
target word.
At the end of this step, we had 1,004,466 sub-
stitute vectors. The next common step might be to
cluster these vectors either locally, which means ev-
ery target word will be clustered separately; or glob-
ally, which indicates all instances (approximately 1
million) will be clustered together. Both approaches
led us to lower scores than the presented method.
Therefore, instead of clustering substitute vectors di-
rectly, we relied on co-occurrence modeling.
3.3 Substitute Word Sampling
Before running S-CODE (Maron et al, 2010) to
model co-occurrence statistics, we needed to per-
form the substitute word sampling. For each target
word?s instance, we sample 100 substitutes from its
substitute vector. Assuming that our target word is
meet and its substitute vector is the one shown in
Instance ID Substitute Word
meet1 complete
meet1 solve
... ...
meet2 hold
meet2 visit
... ...
meet20100 assemble
... ...
meet20100 gather
Table 3: Substitute sampling for a target word meet.
Instance ID - Substitute word pairs
Table 1. We choose 100 substitutes from this in-
stance?s substitute vector by using individual proba-
bilities of substitutes. As seen in Table 2, those sub-
stitutes which have high probabilities dominate the
right column. Recall that Table 2 illustrates only one
instance (subscript denotes the instance number) for
the target word meet which has 20,000 and 100 in-
stances from the context enrichment procedure and
the test, respectively. We followed the same proce-
dure for every instance of each target word. Table 3
depicts instance id - substitute word pairs for the
target word meet rather than for only one instance
shown in Table 2.
3.4 Co-Occurrence Modeling
After sampling, we had approximately 20,000 in-
stance id - substitute word pairs. These pairs were
used to feed S-CODE. The premise is that words
with similar meanings will occur in similar contexts
(Harris, 1954), and at the end this procedure enables
us to put together words with similar meanings as
well as making the clustering procedure more accu-
rate. If two different instances have similar substi-
tute word pairs (i.e, similar contexts) then these two
word pairs attract each other and they will be located
closely on the unit sphere, otherwise they will repel
and eventually be far away from each other (see Fig-
ure 1).
3.5 Clustering
We used k-means clustering on S-CODE sphere.
Note that the procedures explained in the fore-
gone subsections were repeated for each target
303
System JI WKT WNDCG
A
ll
In
st
an
ce
s
ai-ku 0.759 0.804 0.432
ai-ku(a1000) 0.759 0.794 0.612
ai-ku(r5-a1000) 0.760 0.800 0.541
MFS 0.381 0.655 0.337
All-Senses 0.757 0.745 0.660
All-Senses-freq-ranked 0.757 0.789 0.671
All-Senses-avg-ranked 0.757 0.806 0.706
Random-3 0.776 0.784 0.306
Random-n 0.795 0.747 0.301
Table 4: Supervised results on the trial set using median
gold-standard (JI: Jaccard Index FScore, WKT: Weighted
Kendall?s Tau FScore, WNDCG: Weighted Normalized
Discounted Cumulative Gain FScore)
word. More precisely, the substitute sampling, co-
occurrence modeling and clustering were performed
one by one for each target word.
We picked 22 as k value since the test set con-
tained words with 3 to 22 senses. After all word
pairs were labeled, we counted all class labels for
each instance in the test set. For example, if meet1?s
50 word pairs are labeled with c1 and 30 word pairs
are labeled with c2 and finally 20 word pairs are la-
beled with c3, then this particular instance would
have 50% sense1, 30% sense2 and 20% sense3.
4 Evaluation Results
In this section, we will discuss evaluation scores and
the characteristics of the test and the trial data.
All three AI-KU systems followed the same pro-
cedures described in Section 3. After clustering,
some basic post-processing operations were per-
formed for ai-ku(a1000) and ai-ku(r5-a1000). For
ai-ku(a1000), we added 1000 to all sense labels
which were obtained from the clustering procedure;
for ai-ku(r5-a1000), those sense labels occurred less
than 5 times in clustering were removed since we
considered them to be unreliable labels, afterwards
we added 1000 for all remaining sense labels.
Supervised Metrics: Table 5 shows the perfor-
mance of our systems on the test data using all
instances (verbs, nouns, adjectives) for all super-
vised measures and in comparison with the sys-
tems that performed best and worst, most frequent
sense (MFS), all senses equally weighted, all senses
average weighted, random-3, and random-n base-
System JI WKT WNDCG
A
ll
In
st
an
ce
s
ai-ku 0.197 0.620 0.387
ai-ku(a1000) 0.197 0.606 0.215
ai-ku(r5-a1000) 0.244 0.642 0.332
Submitted-Best 0.244 0.642 0.387
All-Best 0.552 0.787 0.499
All-Worst 0.149 0.465 0.215
MFS 0.552 0.560 0.412
All-Senses-eq-weighted 0.149 0.787 0.436
All-Senses-avg-ranked 0.187 0.613 0.499
Random-3 0.244 0.633 0.287
Random-n 0.290 0.638 0.286
Table 5: Supervised results on the test set. (Submitted-
Best indicates the best scores among all submitted sys-
tem. All-Best indicates the best scores among all sub-
mitted systems and baselines. JI: Jaccard Index FS-
core, WKT: Weighted Kendall?s Tau FScore, WNDCG:
Weighted Normalized Discounted Cumulative Gain FS-
core)
Trial Data Test Data
Number of Sense 4.97 1.19
Sense Perplexity 5.79 3.78
Table 6: Average number of senses and average sense
perplexity for trial and test data
lines. Bold numbers indicate that ai-ku achieved
best scores among all submitted systems. Our sys-
tems performed generally well for all three super-
vised measures and slightly better for all submit-
ted systems. On the other hand, baselines achieved
better scores than all participants. More precisely,
on sense detection objective, MFS baseline obtained
0.552 which is the top score, while the best submit-
ted system could reach only 0.244. Why is it the case
that MFS had one of the worst sense detection score
on trial data (see Table 4), but best on test data? Un-
like the trial data, test data largely consists of only
one sense instances, MFS usually gives correct an-
swer. Table 6 illustrates the characteristics of the
test and trial data. Instances annotated with multiple
sense had a very small fraction in the test data. In
fact, 517 instances in the test set were annotated with
two senses (11%) and only 25 were annotated with
three senses (0.5%). However, trial data provided
by the organizers had almost 5 senses per instance
on the average. A similar results can be observed
in All-Senses baselines. On sense ranking objec-
304
System FScore FNMI FB-Cubed
A
ll
Si
ng
le
-s
en
se
In
st
an
ce
s
ai-ku 0.641 0.045 0.351
ai-ku(a1000) 0.601 0.023 0.288
ai-ku(r5-a1000) 0.628 0.026 0.421
Submitted-Best 0.641 0.045 0.441
All-Best 0.641 0.048 0.570
All-Worst 0.477 0.006 0.180
MFS 0.578 - -
SemCor-MFS 0.477 - -
One Sense 0.569 0.0 0.570
Random-3 0.555 0.010 0.359
Random-n 0.533 0.006 0.223
Table 7: Supervised and unsupervised results on the test
set using instances which have only one sense. Bold num-
bers indicate that ai-ku achieved the best submitted sys-
tem scores. (FScore: Supervised FScore, FNMI: Fuzzy
Normalized Mutual Information, FB-Cubed: Fuzzy B-
Cubed FScore)
tives, All-Sense-eq-weighted outperformed all other
systems. The reason is the same as the above. This
baseline ranks all senses equally and since most in-
stances had been annotated only one sense, the other
wrong senses were tied and placed at the second po-
sition in ranking. As a result, this baseline achieved
the highest score. Finally, for quantifying the level
of applicability for each sense, Weighted NDCG was
employed. ai-ku outperformed other submitted sys-
tems, but top score was achieved by all-sense-avg-
weighted baseline. Addition to these results, orga-
nizers provided scores for instances which have only
one sense. This setting contains 89% of the test data.
Table 7 shows supervised and unsupervised scores
for all single-sense instances. Our base system, ai-
ku, outperformed all other system and all baselines
for FScore. Moreover, it also achieved the second
best score (0.045) for Fuzzy NMI. Only one base-
line (one sense per instance) obtained slightly better
score (0.048) for this metric. For Fuzzy B-Cubed,
ai-ku(r5-a1000) obtained 0.421 which is the third
best score.
Clustering Comparison: This evaluation setting
aims to measure the similarity of the induced sense
inventories for WSI systems. Unlike supervised
metrics, it avoids potential loss of sense information
since this setting does not require any sense map-
ping procedure to convert induced senses to a Word-
System Fuzzy NMI Fuzzy B-Cubed
A
ll
In
st
an
ce
s
ai-ku 0.065 0.390
ai-ku(a1000) 0.035 0.320
ai-ku(r5-a1000) 0.039 0.451
Submitted-Best 0.065 0.483
All-Best 0.065 0.623
All-Worst 0.016 0.201
Random-2 0.028 0.474
Random-3 0.018 0.382
Random-n 0.016 0.245
Table 8: Scores on clustering measures (Fuzzy NMI:
Fuzzy Normalized Mutual Information, Fuzzy B-Cubed:
Fuzzy B-Cubed FScore)
All instances
ai-ku 7.72
ai-ku(a1000) 7.72
ai-ku(r5-a1000) 3.11
Table 9: Average number of senses for each ai-ku systems
on test data
Net sense. ai-ku performed best for Fuzzy NMI
among other systems included baselines. For Fuzzy
B-Cubed, ai-ku(r5a1000) outperformed random-3
and random-n baselines. Table 8 depicts the per-
formance of our systems, best and worst systems as
well as the random baselines.
The best scores for the graded word sense in-
duction task in SemEval-2013 are mostly achieved
by baselines in supervised setting. Major problem
is that there is huge sense differences between test
and trial data regarding to number of sense distribu-
tion. Participants that used trial data as for param-
eter tuning and picking the best algorithm achieved
lower scores than baselines since test data does not
show properties of trial data. Consequently, ai-ku
systems produce significantly more senses than the
gold-standard (see Table 9), and this mainly deterio-
rates our performance.
5 Conclusion
In this paper, we presented substitute vector repre-
sentation and co-occurrence modeling on WSI task.
Clustering substitute vectors directly gives lower
scores. Thus, taking samples from each target?s sub-
stitute vector, we obtained instance id - substitute
word pairs. These pairs were used by S-CODE. Fi-
305
nally we run k-means on the S-CODE. Although our
systems were highly ranked among the other submit-
ted systems, no system showed better performance
than the top baselines for all metrics. One explana-
tion is that trial data does not reflect the characteris-
tics of test data according to their number of sense
distributions. Systems used trial data biased to re-
turn more than one sense for each instance since av-
erage number of sense is almost five in trial data. In
addition, baselines (except random ones) know true
sense distribution in the test data beforehand which
make them harder to beat.
References
Eneko Agirre, David Mart??nez, Oier Lo?pez de Lacalle
and Aitor Soroa. 2006. Two graph-based algorithms
for state-of-the-art WSD. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 585-593.
Samuel Brody and Mirella Lapata. 2009. Bayesian Word
Sense Induction. In Proceedings of the 12th Con-
ference of the European Chapter of the ACL (EACL
2009), pages 103-111, Athens, Greece.
Katrin Erk, Diana McCarthy, Nicholas Gaylord. 2009.
Investigations on Word Senses and Word Usages, In
Proceedings of ACL-09 Singapore.
Zellig S. Harris. 2012. Distributional structure. Word,
Vol. 10, pages 146-162.
Tobias Hawker. 2007. USYD: WSD and lexical substi-
tution using the Web 1T corpus In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 207214, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Ioannis Korkontzelos and Suresh Manandhar. 2010.
UoY: Graphs of Unambiguous Vertices for Word
Sense Induction and Disambiguation. In Proceedings
of the 5th International Workshop on Semantic Evalu-
ation. Uppsala, Sweden.
David Jurgens. 2012. An Evaluation of Graded Sense
Disambiguation using Word Sense Induction. In Se-
mEval ?12 Proceedings of the First Joint Conference
on Lexical and Computational Semantics. pages 189-
198.
Yariv Maron, Michael Lamar, and Elie Bienenstock.
2012. Sphere embedding: An application to part-of-
speech induction. In J. Lafferty, C. K. I. Williams, J.
Shawe-Taylor, R.S. Zemel, and A. Culotta, editors, In
Advances in Neural Information Processing Systems
23, pages 1567-1575.
Patrick Pantel and Dekang Lin. 2002. Discovering Word
Senses from Text. In Proceedings of the 8th ACM
SIGKDD Conference, pages 613-619, New York, NY,
USA. ACM.
Ted Pedersen. 2010. Duluth-WSI: SenseClusters Ap-
plied to the Sense Induction Task of SemEval-2. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation. pages 363-366, Uppsala, Sweden.
Yves Peirsman, Kris Heylen and Dirk Geeraerts. 2008.
Size Matters. Tight and Loose Context Definitions in
English Word Space Models. In Proceedings of the
ESSLLI Workshop on Distributional Lexical Seman-
tics, Hamburg, Germany.
Magnus Sahlgren. 2002. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. dissertation, De-
partment of Linguistics, Stockholm University.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. Proceedings International
Conference on Spoken Language Processing, pages
257286.
Jean Ve?ronis. 2004. HyperLex: Lexical Cartography for
Information Retrieval. Computer Speech & Language,
18(3):223-252.
Mehmet Ali Yatbaz, Enis Sert and Deniz Yuret. 2012.
Learning Syntactic Categories Using Paradigmatic
Representations of Word Context. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL 2012,
July 12-14, 2012, Jeju Island, Korea.
Deniz Yuret. 2012. FASTSUBS: An Efficient Admis-
sible Algorithm for Finding the Most Likely Lexical
Substitutes Using a Statistical Language Model. Com-
puting Research Repository (CoRR).
Deniz Yuret. 2007. KU: Word sense disambiguation by
substitution. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007), pages 207214, Prague, Czech Republic, June.
Association for Computational Linguistics.
Deniz Yuret and Mehmet Ali Yatbaz. 2010. The noisy
channel model for unsupervised word sense disam-
biguation. Computational Linguistics, Volume 36 Is-
sue 1, March 2010, pages 111-127.
306
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 78?85,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
The AI-KU System at the SPMRL 2013 Shared Task : Unsupervised
Features for Dependency Parsing
Volkan Cirik Husnu Sensoy
Artificial Intelligence Laboratory
Koc? University, I?stanbul, Turkey
{vcirik,hsensoy}@ku.edu.tr
Abstract
We propose the use of the word categories and
embeddings induced from raw text as auxil-
iary features in dependency parsing. To in-
duce word features, we make use of contex-
tual, morphologic and orthographic properties
of the words. To exploit the contextual infor-
mation, we make use of substitute words, the
most likely substitutes for target words, gen-
erated by using a statistical language model.
We generate morphologic and orthographic
properties of word types in an unsupervised
manner. We use a co-occurrence model with
these properties to embed words onto a 25-
dimensional unit sphere. The AI-KU sys-
tem shows improvements for some of the lan-
guages it is trained on for the first Shared Task
of Statistical Parsing of Morphologically Rich
Languages.
1 Introduction
For the first shared task of Workshop on Statistical
Parsing of Morphologically Rich Languages (Sed-
dah et al, 2013), we propose to use unsupervised
features as auxillary features for dependency pars-
ing.
We induce the unsupervised features using con-
textual, morphological and orthographic properties
of the words. We use possible substitutes of the tar-
get word which are generated by a statistical lan-
guage model to exploit the contextual information.
We induce morphological features with a HMM-
based model (Creutz and Lagus, 2005). We combine
contextual, morphological and orthographic features
of co-occurring words within the co-occurrence
data embedding framework (Maron et al, 2010).
The framework embeds word types sharing simi-
lar context, morphological and orthographic prop-
erties closely on a 25-dimensional sphere. Thus, it
provides the word embeddings on a 25 dimensional
sphere. We conduct experiments using these word
embeddings with MaltParser (Nivre et al, 2007) and
MaltOptimizer (Ballesteros and Nivre, 2012). In
addition to CONLL features (Buchholz and Marsi,
2006a), they are added as additional features and the
parsers are configured such that they are able to ex-
ploit these additional features. As a first step we use
real valued word embeddings as they are. Secondly,
we discretize the real valued word embeddings. Fi-
nally, we cluster them and find fine-grained word
categories for word types.
Our experiments show that, the AI-KU system
leads to better results than the baseline experiments
for some languages. We claim that with the cor-
rect parameter settings, these unsupervised features
could be useful for dependency parsing.
In the following sections, we introduce the related
work, the algorithm, experiments, results and pro-
vide a conclusion.
2 Related Work
The features extracted from unlabeled corpora are
already used for all major NLP tasks. Early stud-
ies mainly use clustering based representations (es-
pecially Brown clustering (Brown et al, 1992)) to
obtain those features. Miller et al (2004; Freitag
(2004) utilized Brown Clusters to improve Named
Entity Recognition (NER) performance whereas
Biemann et al (2007) used them for NER, Word
Sense Disambiguation(WSD), and chunking. Ush-
ioda (1996) extended Brown Clustering to cluster
78
not only words but also phrases using hierarcical
clustering and uses them to improve supervised part-
of-speech (PoS) tagging. More recently, Brown
Clusters are used for Chinese word segmentation
and NER (Liang, 2005).
Just like other tasks, clustering based representa-
tions are used to improve parser performance. Koo
et al (2008; Suzuki et al (2009) improved depen-
dency parsing by using Brown clusters. While Can-
dito and Seddah (2010; Candito and Crabbe? (2009)
improved PCFG parsing by using them and Gold-
berg et al (2009) improved PCFG parser for He-
brew by using HMM generated features. More re-
cently Socher et al (2010) used word embeddings
computed using method explained in (Collobert and
Weston, 2008) for syntactic parsing.
3 Algorithm
In this section, the general flow of the algorithm will
be presented. First, we explain how we generate
the substitute vectors. Then, we explain the induc-
tion procedure of morphological features. In the fol-
lowing subsection, we explain how we use substi-
tute vectors and morphological features and gener-
ate word embeddings. The same flow is followed
for all languages we work on.
3.1 Substitute Vectors
A target word?s substitute vector is represented by
the vocabulary of words and their corresponding
probabilities of occurring in the position of the target
word.
(1) ? Nobody thought you could just in-
ject DNA into someone ?s body and they
would just suck it up.?
Probability Substitute Word
0.123 thought
0.091 knew
0.064 felt
0.062 said
0.052 believed
0.037 wish
Table 1: Substitute Vector for ?thought? in above sen-
tence.
Table 1 illustrates the substitute vector of
?thought? in (1). There is a row for each word in
the vocabulary. For instance, probability of ?knew?
occurring in the position of ?thought? is 9.1% in this
context.
To calculate these probabilities, as described in
(Yatbaz et al, 2012), a 4-gram language model is
built with SRILM (Stolcke, 2002) on the corpora of
the target languages. For French, Hungarian, Pol-
ish and Swedish we used Europarl Corpus1(Koehn,
2005). For German, CONLL-X German Corpus
is used (Buchholz and Marsi, 2006b). For He-
brew, we combined HaAretz and Arutz 7 corpora of
MILA2(Itai and Wintner, 2008). For the tokens seen
less than 5 times we replace them with an unknown
tag to handle unseen words in training and test data.
We should note that these corpora are not provided
to the other participants.
To estimate probabilities of lexical substitutes, for
every token in our datasets, we use three tokens each
on the left and the right side of the token as a con-
text. Using Fastsubs (Yuret, 2012) we generated top
100 most likely substitute words. Top 100 substi-
tute probabilities are then normalized to represent a
proper probability distribution.
We should emphasize that a substitute vector is a
function of the context and does not depend on the
target word.
3.2 Morphological Features
In order to generate unsupervised word features, the
second set of features that we use are morphological
and orthographic features.
The orthographic feature set used is similar to the
one defined in (Berg-Kirkpatrick et al,2010)
INITIAL-CAP Capitalized words with the
exception of sentence initial
words.
NUMBER The token starts with a
digit.
CONTAINS-HYPHEN Lowercase words with an
internal hyphen.
INITIAL-APOSTROPHE Tokens that start with an
apostrophe.
The morpological features are obtained using the
unlabeled corpora that are used for the generation
1http://www.statmt.org/europarl/
2http://www.mila.cs.technion.ac.il
79
Figure 1: The Flow of The Modification for Handling New Features
of substitute vectors, using Morfessor defined in
(Creutz and Lagus, 2005). We will only give a
brief sketch of the model used. Morfessor splits
each word into morphemes (word itself may also be
a morpheme) which can be categorized under four
groups, namely prefix, stem, suffix, non-morpheme.
The model is defined as a maximum a posteriori
(MAP) estimate which maximizes the lexicon (set
of morphemes) over the corpus.
The maximization problem is solved by using a
greedy algorithm that iteratively splits and merges
morphemes, then re-segments corpus using Viterbi
algorithm and reestimates probabilities until conver-
gence. Finally, a final merge step takes place to re-
move all non-morphemes.
3.3 Co-occurence Embedding
For a pair of categorical variables, the Spherical Co-
occurrence Data Embedding (S-CODE) framework
(Maron et al, 2010) represents each of their values
on a sphere such that frequently co-occurring values
are positioned closely on this sphere.
The input of S-CODE are tuples of values of cate-
gorical variables. In our case, these are word tokens,
their substitutes, morphological and orthograpic fea-
tures. We construct the tuples by sampling substitute
words using substitute vectors, their corresponding
morphological and orthographic features of the to-
kens. On each row of the co-occurrence input, there
are the target token, its substitute sampled from its
substitute vector, morphological and orthographic
features. Tokens having the similar substitutes, mor-
phological and orthographic features will be closely
located on the sphere at the end of this process. As
in (Yatbaz et al, 2012), the dimension of the sphere
is 25, in other words for each word type seen in the
corpora we have a 25 dimensional vector3.
4 Experiments
We conduct experiments using MaltParser (Nivre
et al, 2007) and MaltOptimizer (Ballesteros and
Nivre, 2012) with features provided in CONLL for-
mat and the additional unsupervised features that we
generated with default settings of the parsers. To
make use of additional features, we need to modify
MaltParser accordingly. Figure 1 shows that how
we use MaltOptimizer and MaltParser with new fea-
tures. In order to handle auxiliary features, the fea-
ture model file is modified in two different ways. We
handle new features with feature functions Input[0]
and Stack[0]4. We should note that other feature
functions should also be experimented as a future
work.
The following subsections explain the details of
the experiments.
4.1 Experiment I
Our first approach was trying to use word embed-
dings as they are with the MaltParser. For each token
in the training and the test set, we added the corre-
sponding 25-dimensional word vector from the word
embeddings file to the training and test sets. If the
word type is not present in the word embeddings,
then, we use the unknown word vector.
3The vectors can be downloaded here :
https://github.com/wolet/sprml13-word-embeddings
4Thanks for Joakim Nivre for his suggestions on this
80
Stack[0] Input[0]
LAS UAS LaA LAS UAS Labeled Accuracy
Real Valued Vectors 80.56 84.33 85.78 80.63 84.38 85.92
Binning, b=5 80.25 84.07 85.58 80.45 84.20 85.79
Binning, b=2 80.41 84.19 85.79 80.47 84.26 85.77
Clustering, k = 50 80.48 84.29 85.79 80.50 84.24 85.78
Clustering k = 300 80.49 84.23 85.83 80.58 84.31 85.82
LAS UAS LaS
Baseline 80.36 84.11 85.72
Table 2: Results on German with MaltParser of Development Set with Default Settings
Stack[0] Input[0]
LAS UAS LaS LAS UAS LaS
Real Valued Vectors 87.30 89.33 93.35 87.29 89.30 93.32
Binning, b =2 87.12 89.20 93.20 87.04 89.11 93.16
Clustering, k = 300 90.30 91.80 95.09 90.49 91.94 95.19
LAS UAS LaS
Baseline 90.38 91.88 95.14
Table 3: Results on German with MaltOptimizer of Development Set
Gold Predicted
LAS UAS LaS LAS UAS LaS Predicted (Unofficial)
Best System 90.29 91.92 95.95 85.86 89.19 92.20 LAS UAS LaS
AI-KU 1 86.39 88.21 94.07 72.57 78.54 82.39 AI-KU 1 79.92 83.94 88.51
AI-KU 2 86.31 88.14 94.05 72.55 78.55 82.36 AI-KU 2 79.84 83.85 88.45
Baseline 85.71 87.50 93.70 79.00 83.35 87.73
Table 4: Results on French
Gold Predicted
LAS UAS LaS LAS UAS LaS Predicted (Unofficial)
Best System 91.83 93.20 96.06 86.95 91.64 94.38 LAS UAS LaS
AI-KU 1 86.98 88.71 93.70 82.32 85.31 89.95 AI-KU 1 84.08 86.71 91.13
AI-KU 2 86.95 88.67 93.67 82.29 85.30 89.95 AI-KU 2 83.93 86.54 91.05
Baseline 86.96 87.67 93.67 82.75 85.38 90.15
Table 5: Results on German
Gold Predicted
LAS UAS LaS LAS UAS LaS
Best System 83.87 88.95 89.19 80.89 86.7 86.93
AI-KU 1 79.42 84.48 86.52 69.01 75.84 79.01
AI-KU 2 78.73 83.79 85.98 62.27 75.84 79.01
Baseline 80.03 84.9 86.97 73.01 79.89 81.28
Table 6: Results on Hebrew
81
Gold Predicted
LAS UAS LaS LAS UAS LaS Predicted (Unofficial)
Best System 88.06 91.14 92.58 86.13 89.81 90.92 LAS UAS LaS
AI-KU 1 83.67 87.08 89.64 78.92 83.77 85.98 AI-KU 1 79.98 84.42 87.12
AI-KU 2 83.63 87.06 89.58 78.76 83.60 85.95 AI-KU 2 79.74 84.12 86.93
Baseline 83.14 86.56 89.20 79.63 83.71 85.89
Table 7: Results on Hungarian
Gold Predicted
LAS UAS LaS LAS UAS LaS
Best System 89.58 93.24 93.42 87.07 91.75 91.24
AI-KU 1 85.16 88.86 90.87 81.86 86.96 88.06
AI-KU 2 85.12 88.79 90.84 78.31 84.18 85.64
Baseline 80.49 86.41 86.94 79.89 85.80 86.24
Table 8: Results on Polish
Gold Predicted
LAS UAS LaS LAS UAS LaS
Best System 83.97 89.11 87.63 82.13 88.06 85.93
AI-KU 1 78.87 85.19 83.44 76.35 83.30 81.37
AI-KU 2 78.57 85.12 83.25 76.35 83.24 81.35
Baseline 77.67 84.6 82.36 75.82 83.20 80.88
Table 9: Results on Swedish
Gold Predicted
Precision Recall F1 Precision Recal F1
Best System 99.41 99.38 99.39 81.68 79.97 80.81
AI-KU 1 99.41 99.38 99.39 74.47 71.51 72.96
AI-KU 2 99.38 99.36 99.37 74.34 71.51 72.89
MaltOptimizer Baseline 98.77 99.18 99.26 72.64 68.09 70.29
Table 10: Results of Multi Word Expressions on French
82
4.2 Experiment II
The second approach is discretizing the real valued
vectors. For each dimension of word embeddings,
we separate b equal sized bins. Then, for each vec-
tor?s dimensions, we assign their corresponding bin
numbers.
4.3 Experiment III
The third approach is clustering the word embed-
dings. We use a modified k-means algorithm (Arthur
and Vassilvitskii, 2007). We experiment with vary-
ing number of clusters k.
For each token in training and test file, we use
word type?s cluster id as an auxiliary feature. Again,
if the token is not in the word embeddings file, we
used the unknown word?s cluster id.
5 Results
In Table 2, the experiments on German with Malt-
Parser without the optimization step are demon-
strated. We use the default settings of the MaltParser
as our baseline. We use training data consisting of
5000 sentences with gold tags as training set and the
provided development data as test set.
When we use real valued word embeddings as
an auxiliary feature, we observe slight improvement
compared to MaltParser baseline. The large bin-
ning size results in worse results compared to base-
line due to sparsity. Clustering again leads to some
improvement compared to MaltParser baseline. We
also observe that increasing the number of clusters
result in better scores compared to smaller k.
In Table 3, the results on German with MaltOp-
timizer can be seen. As a baseline, again, we use
training data consisting of 5000 sentences with gold
tags as training set and the provided development
data as test set. We use the baseline experiment?s
parsing algorithm, feature model and learning algo-
rithm to experiment with word embedding, binning
and clustering on MaltParser.
Unlike in Table 2, in Table 3 we observe that only
the clustering experiment outperforms the baseline
but not significantly. Since clustering is leads to
best results, for all other languages, we apply the
same optimization and clustering pipeline. The only
difference is that when the MaltOptimizer suggests
Stack Projective as the best algorithm, instead of In-
put[0] ve use Stack[0], Stack[1], Stack[2] as feature
functions. The two systems of AI-KU only differ in
these feature functions.
In Table 3-7, the results of the best system, base-
line MaltOptimizer result and our two submitted
systems can be seen. For Polish, our system outper-
foms the MaltOptimizer baseline significantly. For
the rest of the languages, our systems are not signif-
icantly better or worse than the baseline. We make
an assumption that we need to find the optimum set-
tings, for instance the number of clusters, for each
language separately, instead of using the fixed set-
tings for all languages.
For French, German, Hungarian the model trained
on the data with gold features is mistakenly used for
testing on the data with predicted features. To cor-
rect these, for those languages, we report the unoffi-
cial results that are obtained by training on predicted
features.
For French, there is also another evaluation met-
ric. It is about capturing the Multi Word Expres-
sions(MWE). Table 10 reports the results of MWE
and it shows that our system is significantly better
than MaltOptimizer baseline.
6 Conclusion
We can speculate on these results in couple of ways.
First, for all languages we used the same number
of clusters. The optimum number of clusters may
vary with the syntactic properties of these languages.
Similarly, the optimum dimension of the word em-
beddings may vary with the languages. In addition,
for co-occurence embedding and morphological in-
duction we use the parameter settings of (Yatbaz et
al., 2012) which is optimized for Part-of-Speech in-
duction on Penn Treebank data. We suggest to find
the optimum parameter settings for co-occurrence
embedding and morphological induction as a future
work.
We only experimented with simple feature func-
tions, namely Input and Stack functions. Other con-
figuration of these functions may lead to better re-
sults. Lastly, as a future direction, we propose to
use real valued word embeddings and unsupervised
word categories as auxiliary features in the training
phase of the MaltOptimizer.
83
Acknowledgments
We would like to thank Joakim Nivre and Deniz
Yuret for valuable suggestions and their support.
References
D. Arthur and S. Vassilvitskii. 2007. k-means++: The
advantages of careful seeding. In Proceedings of the
eighteenth annual ACM-SIAM symposium on Discrete
algorithms, pages 1027?1035. Society for Industrial
and Applied Mathematics.
Miguel Ballesteros and Joakim Nivre. 2012. Maltop-
timizer: A system for maltparser optimization. In
LREC, pages 2757?2763.
Chris Biemann, Claudio Giuliano, and Alfio Gliozzo.
2007. Unsupervised part-of-speech tagging support-
ing supervised methods. In Proceedings of RANLP,
volume 7.
Peter F Brown, Peter V Desouza, Robert L Mercer, Vin-
cent J Della Pietra, and Jenifer C Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional linguistics, 18(4):467?479.
S. Buchholz and E. Marsi. 2006a. CoNLL-X shared task
on multilingual dependency parsing. SIGNLL.
Sabine Buchholz and Erwin Marsi. 2006b. Conll-x
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, CoNLL-X ?06,
pages 149?164, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Marie Candito and Beno??t Crabbe?. 2009. Improving gen-
erative statistical parsing with semi-supervised word
clustering. In Proceedings of the 11th International
Conference on Parsing Technologies, pages 138?141.
Association for Computational Linguistics.
Marie Candito and Djame? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL HLT 2010 First
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 76?84. Association for Com-
putational Linguistics.
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: Deep neu-
ral networks with multitask learning. In Proceedings
of the 25th international conference onMachine learn-
ing, pages 160?167. ACM.
Mathias Creutz and Krista Lagus. 2005. Inducing
the morphological lexicon of a natural language from
unannotated text. In Proceedings of AKRR?05, Inter-
national and Interdisciplinary Conference on Adap-
tive Knowledge Representation and Reasoning, pages
106?113, Espoo, Finland, June.
Dayne Freitag. 2004. Trained named entity recogni-
tion using distributional clusters. In Proceedings of
EMNLP, pages 262?269.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and em-hmm-based lexical probabilities. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 327?335. Association for Computational
Linguistics.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75?98, March.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In MT summit, volume 5.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. Colum-
bus, Ohio USA, June. ACL.
Percy Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, MIT, May.
Yariv Maron, Michael Lamar, and Elie Bienenstock.
2010. Sphere embedding: An application to part-of-
speech induction. In J. Lafferty, C. K. I. Williams,
J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors,
Advances in Neural Information Processing Systems
23, pages 1567?1575.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In In Proceedings of HLT-NAACL,
pages 337?342.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. Maltparser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Djame Seddah, Reut Tsarfaty, Sandra Kubler, Marie Can-
dito, Jinho Choi, Richard Farkas, Jennifer Foster, Iakes
Goenaga, Koldo Gojenola, Yoav Goldberg, Spence
Green, Nizar Habash, Marco Kuhlmann, Wolfgang
Maier, Joakim Nivre, Adam Przepiorkowski, Ryan
Roth, Wolfgang Seeker, Yannick Versley, Veronika
Vincze, Marcin Wolinski, Alina Wroblewska, and Eric
Villemonte de la Clergerie. 2013. Overview of the
spmrl 2013 shared task: A cross-framework evalua-
tion of parsing morphologically rich languages. In
Proceedings of the 4th Workshop on Statistical Pars-
ing of Morphologically Rich Languages: Shared Task,
Seattle, WA.
Richard Socher, Christopher D Manning, and Andrew Y
Ng. 2010. Learning continuous phrase representa-
tions and syntactic parsing with recursive neural net-
works. In Proceedings of the NIPS-2010 Deep Learn-
ing and Unsupervised Feature Learning Workshop.
84
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings International Con-
ference on Spoken Language Processing, pages 257?
286, November.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael
Collins. 2009. An empirical study of semi-supervised
structured conditional models for dependency parsing.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 2-
Volume 2, pages 551?560. Association for Computa-
tional Linguistics.
Akira Ushioda. 1996. Hierarchical clustering of words
and applications to nlp tasks. In Proceedings of the
Fourth Workshop on Very Large Corpora, pages 28?
41.
Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret. 2012.
Learning syntactic categories using paradigmatic rep-
resentations of word context. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 940?951, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Deniz Yuret. 2012. Fastsubs: An efficient and exact pro-
cedure for finding the most likely lexical substitutes
based on an n-gram language model. Signal Process-
ing Letters, IEEE, 19(11):725?728, Nov.
85
