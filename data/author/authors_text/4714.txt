Desparately Seeking Cebuano
Douglas W. Oard, David Doermann, Bonnie Dorr, Daqing He, Philip Resnik, and Amy Weinberg
UMIACS, University of Maryland, College Park, MD, 20742
(oard,doermann,bonnie,resnik,weinberg)@umiacs.umd.edu
William Byrne, Sanjeev Khudanpur and David Yarowsky
CLSP, Johns Hopkins University, 3400 North Charles Street, Barton Hall, Baltimore, MD 21218
(byrne,khudanpur,yarowsky)@jhu.edu
Anton Leuski, Philipp Koehn and Kevin Knight
USC Information Sciences Institute, 4676 Admiralty Way, Marina Del Rey, CA 90292
(leuski,koehn,knight)@isi.edu
Abstract
This paper describes an effort to rapidly de-
velop language resources and component tech-
nology to support searching Cebuano news sto-
ries using English queries. Results from the
first 60 hours of the exercise are presented.
1 Introduction
The Los Angeles Times reported that at about 5:20 P.M.
on Tuesday March 4, 2003, a bomb concealed in a back-
pack exploded at the airport in Davao City, the second
largest city in the Philippines. At least 23 people were
reported dead, with more than 140 injured, and Pres-
ident Arroyo of the Philippines characterized the blast
as a terrorist act. With the 13 hour time difference, it
was then 4:20 A.M on the same date in Washington, DC.
Twenty-four hours later, at 4:13 A.M. on March 5, partic-
ipants in the Translingual Information Detection, Extrac-
tion and Summarization (TIDES) program were notified
that Cebuano had been chosen as the language of interest
for a ?surprise language? practice exercise that had been
planned quite independently to begin on that date. The
notification observed that Cebuano is spoken by 24% of
the population of the Philippines, and that it is the lingua
franca in the south Philippines, where the event occurred.
One goal of the TIDES program is to develop the abil-
ity to rapidly deploy a broad array of language technolo-
gies for previously unforeseen languages in response to
unexpected events. That capability will be formally ex-
ercised for the first time during June 2003, in a month-
long ?Surprise Language Experiment.? To prepare for
that event, the Linguistic Data Consortium (LDC) orga-
nized a ?dry run? for March 5-14 in order to refine pro-
cedures for rapidly developing language resources of the
type that the TIDES community will need during the July
evaluation.
Development of interactive Cross-Language Informa-
tion Retrieval (CLIR) systems that can be rapidly adapted
to accommodate new languages has been the focus of
extensive collaboration between the University of Mary-
land and The Johns Hopkins University, and more re-
cently with the University of Southern California. The
capability for rapid development of necessary language
resources is an essential part of that process, so we had
been planning to participate in the surprise language dry
run to refine our procedures for sharing those resources
with other members of the TIDES community. Naturally,
we chose CLIR as a driving application to focus our ef-
fort. Our goal, therefore, was to build an interactive sys-
tem that would allow a searcher posing English queries
to find relevant Cebuano news articles from the period
immediately following the bombing.
2 Obtaining Language Resources
Our basic approach to development of an agile system for
interactive CLIR relies on three strategies: (1) create an
infrastructure in advance for English as a query language
that makes only minimal assumptions about the docu-
ment language; (2) leverage the asymmetry inherent in
the problem by assembling strong resources for English
in advance; and (3) develop a robust suite of capabilities
to exploit any language resources that can be found for
the ?surprise language.? We defer the first two topics to
the next section, and focus here on the third. We know of
five possible sources of translation expertise:
People. People who know the language are an excellent
source of insight, and universities are an excellent
place to find such people. We were able to locate
a speaker of Cebuano within 50 feet of one of our
offices, and to schedule an interview with a second
Cebuano speaker within 36 hours of the announce-
ment of the language.
Scholarly literature. Major research universities are
also an excellent place to find written materials de-
scribing a broad array of languages. Within 12 hours
of the announcement, reference librarians at the Uni-
versity of Maryland had identified a textbook on
?Beginning Cebuano,? and we had located a copy
at the University of Southern California. Together
with the excellent electronic resources located by the
LDC, this allowed us to develop a rudimentary stem-
mer within 36 hours.
Translation lexicons. Simple bilingual term lists are
available for many language pairs. Using links pro-
vided by the LDC and our own Web searches, we
were able to construct an English-Cebuano term list
with over 14,000 translation pairs within 12 hours of
the announcement. This largely duplicated a simul-
taneous effort at the LDC, and we later merged our
term list with theirs.
Parallel text. Translation-equivalent documents, when
aligned at the word level, provide an excellent
source of information about not just possible trans-
lations, but their relative predominance. Within 24
hours of the announcement, we had aligned Ce-
buano and English versions of the Holy Bible at
the word level using Giza++. An evaluation by a
native Cebuano speaker of a stratified random sam-
ple of 88 translation pairs showed remarkably high
precision. On a 4-point scale with 1=correct and
4=incorrect the most frequent 100 words averaged
1.3, the next 400 most frequent terms averaged 1.6,
and the 500 next most frequent terms after that aver-
aged 1.7. The Bible?s vocabulary covers only about
half of the words found in typical English news text
(counted by-token), so it is useful to have additional
sources of parallel text. For this reason, we have ex-
tended our previously developed STRAND system
to locate likely translations in the Internet Archive.
Those runs were not yet complete when this paper
was submitted.
Printed Dictionaries. People learning a new language
make extensive use of bilingual dictionaries, so we
have developed a system that mimics that process
to some extent. Within 12 hours of the announce-
ment we had zoned page images from a Cebuano-
English dictionary that was available commercially
in Adobe Page Description Format (PDF) to iden-
tify each dictionary entry, performed optical charac-
ter recognition, and parsed the entries to construct a
bilingual term list. We were aided in this process by
the fact that Cebuano is written in a Roman script.
Again, we achieved good precision, with a sampled
word error rate for OCR of 6.9% and a precision for
a random sample of translation pairs of 87%. Part of
speech tags were also extracted, although they are
not used in our process.
As this description illustrates, these five sources pro-
vide complementary information. Since there is some
uncertainty at the outset about how long it will be before
each delivers useful results, we chose a strategy based
on concurrency, balancing our investment over each the
five sources. This allowed us to use whatever resources
became available first to get an initial system running,
with refinements subsequently being made as additional
resources became available. Because Cebuano and En-
glish are written in the same script, we did not need char-
acter set conversion or phonetic cognate matching in this
case. The CLIR system described in the next section
was therefore constructed using only English resources
that were (or could have been) pre-assembled, plus a
Cebuano-English bilingual term list, a rule-based stem-
mer, and the Cebuano Bible.
3 Building a Cross-Language Retrieval
System
Ideally, we would like to build a system that would find
whatever documents the searcher would wish to read in a
fully automatic mode. In practice, fully automatic search
systems are imperfect even in monolingual applications.
We therefore have developed an interactive approach that
functions something like a typical Web search engine: (1)
the searcher poses their query in English, (2) the sys-
tem ranks the Cebuano documents in decreasing order
of likely relevance to the query, (3) the searcher exam-
ines a list of document titles in something approximat-
ing English, and (4) the searcher may optionally exam-
ine the full text of any document in something approx-
imating English. The intent is to support an iterative
process in which searchers learn to better express their
query through experience. We are only able to provide
very rough translations, so we expect that such a sys-
tem would be used in an environment where searchers
could send documents that appear promising off for pro-
fessional translation when necessary.
At the core of our system is the capability to au-
tomatically rank Cebuano documents based on an En-
glish query. We chose a query translation architecture
using backoff translation and Pirkola?s structured query
method, implemented using Inquery version 3.1p1. The
key idea in backoff translation is to first try to find con-
secutive sequences of query words on the English side
of the bilingual term list, where that fails to try to find
the surface form of each remaining English term, to fall
back to stem matching when necessary, and ultimately to
fall back to retaining the English term unchanged in the
hope that it might be a proper name or some other form
of cognate with Cebuano. Accents are stripped from the
documents and all language resources to facilitate match-
ing at that final step.
Although we have chosen techniques that are relatively
robust and therefore require relatively little domain-
specific tuning, stemmer design is an area of uncertainty
that could adversely affect retrieval effectiveness. We
therefore needed a test collection on which we could try
out variants of the Cebuano stemmer. We built this test
collection using 34,000 Cebuano Bible verses and 50 En-
glish questions that we found on the Web for which ap-
propriate Bible verses were known. Each question was
posed as a query using the batch mode of Inquery, and
the rank of the known relevant verse was taken as a mea-
sure of effectiveness. We took the mean reciprocal rank
(the inverse of the harmonic mean) as a figure of merit
for each configuration, and used a paired two-tailed   -
test (with p  0.05) to assess the statistical significance of
observed differences. Our initial configuration, without
stemming, obtained a mean inverse rank of 0.14, which
is a statistically significant improvement over no transla-
tion at all (mean inverse rank 0.02 from felicitous cognate
and loan word matches). The addition of Cebuano stem-
ming resulted in a reduction in mean inverse rank to 0.09.
Although the reduction is not statistically significant in
that case, the result suggests that our initial stemmer is
not yet useful for information retrieval tasks.
The other key capability that is needed is title and doc-
ument translation. We can accomplish this in one of two
ways. The simplest approach is to reverse the bilingual
term list, and to reverse the role of Cebuano and En-
glish in the process described above for query transla-
tion. Our user interface is capable of displaying multi-
ple translations for a single term (arranged horizontally
for compact depiction or vertically for clearer depiction),
but searchers can choose to display only the single most
likely translation. When reliable translation probability
statistics (from parallel text) are not available, we use the
relative word unigram frequency of each translation of a
Cebuano term in a representative English collection as a
substitute for that probability. A more sophisticated way
is to build a statistical machine translation system using
parallel text. We built our first statistical machine trans-
lation system within 40 hours of the announcement, and
one sentence of the resulting translation using each tech-
nique is shown below:
Cebuano: ?ang rebeldeng milf, kinsa
lakip sa nangamatay, nagdala og
backpack nga dunay explosives nga
niguba sa waiting lounge sa airport,
matod sa mga defense official.?
Term-by-term translation:
?(carelessness, circumference,
conveyence) rebeldeng milf, who lakip
(at in of) nangamatay, nagdala og
backpack nga valid explosives nga
niguba (at, in of) waiting lounge
(at, in, of) airport, matod (at, in,
of) mga defense official?
Statistical translation: ?who was
accused of rank, ehud og niguba
waiting lounge defense of those dumah
milf rebeldeng explosives backpack
airport matod official.?
At this point, term-by-term translation is clearly the bet-
ter choice. But as more parallel text becomes available,
we expect the situation to reverse. The LDC is prepar-
ing a set of human reference translations that will allow
us to detect that changeover point automatically using the
NIST variant of the BLEU measure for machine transla-
tion effectiveness.
4 Conclusion
The results reported in this paper were accomplished by
a team of 20 people with expertise in various facets of te
task that invested about 250 person-hours over two and
a half days. As additional Cebuano-specific evaluation
resources are developed, we expect to gain additional in-
sight into the quality of these early resources. Moreover,
once we see what works best for Cebuano by the end of
the process, we plan to revisit our process design with
an eye towards better optimizing our initial time invest-
ments. We expect to be able to address both of those
points in detail by the time of the conference.
This exercise was originally envisioned as a dry run to
work out the kinks in our process, and indeed we have
already learned a lot on that score. First, we learned that
our basic approach seems sound; we built the key com-
ponents of an interactive CLIR system in about 40 hours,
and by the 60-hour point we had some basis for believing
that each of those components could at least minimally
fulfill their role in a fully integrated system. Some of our
time was, however, spent on things that could have been
done in advance. Perhaps the most important of these
was the development of an information retrieval test col-
lection using the Bible. That job, and numerous smaller
ones, are now done, so we expect that we will be able to
obtain similar results with about half the effort next time
around.
Acknowledgments
Thanks to Clara Cabezas, Tim Hackman, Margie Hi-
nonangan, Burcu Karagol-Ayan, Okan Kolak, Huanfeng
Ma, Grazia Russo-Lassner, Michael Subotin, Jianqiang
Wang and the LDC! This work has been supported in part
by DARPA contract N660010028910.
iNeATS: Interactive Multi-Document Summarization
Anton Leuski, Chin-Yew Lin, Eduard Hovy
University of Southern California
Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina Del Rey, CA 90292-6695
{leuski,cyl,hovy}@isi.edu
Abstract
We describe iNeATS ? an interactive
multi-document summarization system
that integrates a state-of-the-art summa-
rization engine with an advanced user in-
terface. Three main goals of the sys-
tem are: (1) provide a user with control
over the summarization process, (2) sup-
port exploration of the document set with
the summary as the staring point, and (3)
combine text summaries with alternative
presentations such as a map-based visual-
ization of documents.
1 Introduction
The goal of a good document summary is to provide
a user with a presentation of the substance of a body
of material in a coherent and concise form. Ideally, a
summary would contain only the ?right? amount of
the interesting information and it would omit all the
redundant and ?uninteresting? material. The quality
of the summary depends strongly on users? present
need ? a summary that focuses on one of several top-
ics contained in the material may prove to be either
very useful or completely useless depending on what
users? interests are.
An automatic multi-document summarization
system generally works by extracting relevant sen-
tences from the documents and arranging them in a
coherent order (McKeown et al, 2001; Over, 2001).
The system has to make decisions on the summary?s
size, redundancy, and focus. Any of these deci-
sions may have a significant impact on the quality
of the output. We believe a system that directly in-
volves the user in the summary generation process
and adapts to her input will produce better sum-
maries. Additionally, it has been shown that users
are more satisfied with systems that visualize their
decisions and give the user a sense of control over
the process (Koenemann and Belkin, 1996).
We see three ways in which interactivity and
visualization can be incorporated into the multi-
document summarization process:
1. give the user direct control over the summariza-
tion parameters such as size, redundancy, and
focus of the summaries.
2. support rapid browsing of the document set us-
ing the summary as the starting point and com-
bining the multi-document summary with sum-
maries for individual documents.
3. incorporate alternative formats for organizing
and displaying the summary, e.g., a set of news
stories can be summarized by placing the sto-
ries on a world map based on the locations of
the events described in the stories.
In this paper we describe iNeATS (Interactive
NExt generation Text Summarization) which ad-
dresses these three directions. The iNeATS system
is built on top of the NeATS multi-document sum-
marization system. In the following section we give
a brief overview of the NeATS system and in Sec-
tion 3 describe the interactive version.
2 NeATS
NeATS (Lin and Hovy, 2002) is an extraction-
based multi-document summarization system. It is
among the top two performers in DUC 2001 and
2002 (Over, 2001). It consists of three main com-
ponents:
Content Selection The goal of content selection is
to identify important concepts mentioned in
a document collection. NeATS computes the
likelihood ratio (Dunning, 1993) to identify key
concepts in unigrams, bigrams, and trigrams
and clusters these concepts in order to identify
major subtopics within the main topic. Each
sentence in the document set is then ranked, us-
ing the key concept structures. These n-gram
key concepts are called topic signatures.
Content Filtering NeATS uses three different fil-
ters: sentence position, stigma words, and re-
dundancy filter. Sentence position has been
used as a good important content filter since
the late 60s (Edmundson, 1969). NeATS ap-
plies a simple sentence filter that only retains
the N lead sentences. Some sentences start
with conjunctions, quotation marks, pronouns,
and the verb ?say? and its derivatives. These
stigma words usually cause discontinuities in
summaries. The system reduces the scores of
these sentences to demote their ranks and avoid
including them in summaries of small sizes. To
address the redundancy problem, NeATS uses a
simplified version of CMU?s MMR (Goldstein
et al, 1999) algorithm. A sentence is added to
the summary if and only if its content has less
than X percent overlap with the summary.
Content Presentation To ensure coherence of the
summary, NeATS pairs each sentence with an
introduction sentence. It then outputs the final
sentences in their chronological order.
3 Interactive Summarization
Figure 1 shows a screenshot of the iNeATS system.
We divide the screen into three parts corresponding
to the three directions outlined in Section 1. The
control panel displays the summarization parame-
ters on the left side of the screen. The document
panel shows the document text on the right side. The
summary panel presents the summaries in the mid-
dle of the screen.
3.1 Controlling Summarization Process
The top of the control panel provides the user with
control over the summarization process. The first set
of widgets contains controls for the summary size,
sentence position, and redundancy filters. The sec-
ond row of parameters displays the set of topic sig-
natures identified by the iNeATS engine. The se-
lected subset of the topic signatures defines the con-
tent focus for the summary. If the user enters a new
value for one of the parameters or selects a different
subset of the topic signatures, iNeATS immediately
regenerates and redisplays the summary text in the
top portion of the summary panel.
3.2 Browsing Document Set
iNeATS facilitates browsing of the document set by
providing (1) an overview of the documents, (2)
linking the sentences in the summary to the original
documents, and (3) using sentence zooming to high-
light the most relevant sentences in the documents.
The bottom part of the control panel is occupied
by the document thumbnails. The documents are ar-
ranged in chronological order and each document is
assigned a unique color to paint the text background
for the document. The same color is used to draw
the document thumbnail in the control panel, to fill
up the text background in the document panel, and to
paint the background of those sentences in the sum-
mary that were collected from the document. For
example, the screenshot shows that a user selected
the second document which was assigned the or-
ange color. The document panel displays the doc-
ument text on orange background. iNeATS selected
the first two summary sentences from this document,
so both sentences are shown in the summary panel
with orange background.
The sentences in the summary are linked to the
original documents in two ways. First, the docu-
ment can be identified by the color of the sentence.
Second, each sentence is a hyperlink to the docu-
ment ? if the user moves the mouse over a sentence,
the sentence is underlined in the summary and high-
lighted in the document text. For example, the first
sentence of the summary is the document sentence
Figure 1: Screenshot of the iNeATS system.
highlighted in the document panel. If the user clicks
on the sentence, iNeATS brings the source document
into the document panel and scrolls the window to
make the sentence visible.
The relevant parts of the documents are illumi-
nated using the technique that we call sentence
zooming. We make the text color intensity of each
sentence proportional to the relevance score com-
puted by the iNeATS engine and a zooming parame-
ter which can be controlled by the user with a slider
widget at the top of the document panel. The higher
the sentence score, the darker the text is. Conversely,
sentences that blend into the background have a very
low sentence score. The zooming parameter con-
trols the proportion of the top ranked sentences vis-
ible on the screen at each moment. This zooming
affects both the full-text and the thumbnail docu-
ment presentations. Combining the sentence zoom-
ing with the document set overview, the user can
quickly see which document contains most of the
relevant material and where approximately in the
document this material is placed.
The document panel in Figure 1 shows sentences
that achieve 50% on the sentence score scale. We see
that the first half of the document contains two black
sentences: the first sentence that starts with ?US In-
surers...?, the other starts with ?President George...?.
Both sentences have a very high score and they were
selected for the summary. Note, that the very first
sentence in the document is the headline and it is not
used for summarization. Note also that the sentence
that starts with ?However,...? scored much lower
than the selected two ? its color is approximately
half diluted into the background.
There are quite a few sentences in the second part
of the document that scored relatively high. How-
ever, these sentences are below the sentence position
cutoff so they do not appear in the summary. We il-
lustrate this by rendering such sentences in slanted
style.
3.3 Alternative Summaries
The bottom part of the summary panel is occupied
by the map-based visualization. We use BBN?s
IdentiFinder (Bikel et al, 1997) to detect the names
of geographic locations in the document set. We
then select the most frequently used location names
and place them on world map. Each location is iden-
tified by a black dot followed by a frequency chart
and the location name. The frequency chart is a bar
chart where each bar corresponds to a document.
The bar is painted using the document color and the
length of the bar is proportional to the number of
times the location name is used in the document.
The document set we used in our example de-
scribes the progress of the hurricane Andrew and its
effect on Florida, Louisiana, and Texas. Note that
the source documents and therefore the bars in the
chart are arranged in the chronological order. The
name ?Miami? appears first in the second document,
?New Orleans? in the third document, and ?Texas? is
prominent in the last two documents. We can make
some conclusions on the hurricane?s path through
the region ? it traveled from south-east and made its
landing somewhere in Louisiana and Texas.
4 Discussion
The iNeATS system is implemented in Java. It uses
the NeATS engine implemented in Perl and C. It
runs on any platform that supports these environ-
ments. We are currently working on making the sys-
tem available on our web site.
We plan to extend the system by adding temporal
visualization that places the documents on a timeline
based on the date and time values extracted from the
text.
We plan to conduct a user-based evaluation of the
system to compare users? satisfaction with both the
automatically generated summaries and summaries
produced by iNeATS.
References
Daniel M. Bikel, Scott Miller, Richard Schwartz, and
Ralph Weischedel. 1997. Nymble: a high-
performance learning name-finder. In Proceedings of
ANLP-97, pages 194?201.
Ted E. Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational Lin-
guistics, 19(1):61?74.
H. P. Edmundson. 1969. New methods in automatic ex-
traction. Journal of the ACM, 16(2):264?285.
Jade Goldstein, Mark Kantrowitz, Vibhu O. Mittal, and
Jaime G. Carbonell. 1999. Summarizing text docu-
ments: Sentence selection and evaluation metrics. In
Research and Development in Information Retrieval,
pages 121?128.
Jurgen Koenemann and Nicholas J. Belkin. 1996. A case
for interaction: A study of interactive information re-
trieval behavior and effectivness. In Proceedings of
ACM SIGCHI Conference on Human Factors in Com-
puting Systems, pages 205?212, Vancouver, British
Columbia, Canada.
Chin-Yew Lin and Eduard Hovy. 2002. From single
to multi-document summarization: a prototype sys-
tem and it evaluation. In Proceedings of the 40th
Anniversary Meeting of the Association for Computa-
tional Linguistics (ACL-02), Philadelphia, PA, USA.
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Barry Schiffman, and Si-
mone Teufel. 2001. Columbia multi-document sum-
marization: Approach and evaluation. In Proceed-
ings of the Workshop on Text Summarization, ACM SI-
GIR Conference 2001. DARPA/NIST, Document Un-
derstanding Conference.
Paul Over. 2001. Introduction to duc-2001: an intrin-
sic evaluation of generic news text summarization sys-
tems. In Proceedings of the Workshop on Text Summa-
rization, ACM SIGIR Conference 2001. DARPA/NIST,
Document Understanding Conference.
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 186?187,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Statistical Shallow Semantic Parsing despite Little Training Data
Rahul Bhagat
Information Sciences
Institute
University of Southern
California
Marina del Rey,
CA, 90292, USA
rahul@isi.edu
Anton Leuski
Institute for Creative
Technologies
University of Southern
California
Marina del Rey,
CA, 90292, USA
leuski@ict.usc.edu
Eduard Hovy
Information Sciences
Institute
University of Southern
California
Marina del Rey,
CA, 90292, USA
hovy@isi.edu
1 Introduction and Related Work
Natural language understanding is an essential mod-
ule in any dialogue system. To obtain satisfac-
tory performance levels, a dialogue system needs
a semantic parser/natural language understanding
system (NLU) that produces accurate and detailed
dialogue oriented semantic output. Recently, a
number of semantic parsers trained using either
the FrameNet (Baker et al, 1998) or the Prop-
Bank (Kingsbury et al, 2002) have been reported.
Despite their reasonable performances on general
tasks, these parsers do not work so well in spe-
cific domains. Also, where these general purpose
parsers tend to provide case-frame structures, that
include the standard core case roles (Agent, Patient,
Instrument, etc.), dialogue oriented domains tend
to require additional information about addressees,
modality, speech acts, etc. Where general-purpose
resources such as PropBank and Framenet provide
invaluable training data for general case, it tends to
be a problem to obtain enough training data in a spe-
cific dialogue oriented domain.
We in this paper propose and compare a num-
ber of approaches for building a statistically trained
domain specific parser/NLU for a dialogue system.
Our NLU is a part of Mission Rehearsal Exercise
(MRE) project (Swartout et al, 2001). MRE is a
large system that is being built to train experts, in
which a trainee interacts with a Virtual Human using
voice input. The purpose of our NLU is to convert
the sentence strings produced by the speech recog-
nizer into internal shallow semantic frames com-
posed of slot-value pairs, for the dialogue module.
2 Parsing Methods
2.1 Voting Model
We use a simple conditional probability model
P (f | W ) for parsing. The model represents the
probability of producing slot-value pair f as an out-
put given that we have seen a particular word or
n-gram W as input. Our two-stage procedure for
generating a frame for a given input sentence is: (1)
Find a set of all slot-value that correspond with each
word/ngram (2) Select the top portion of these can-
didates to form the final frame (Bhagat et al, 2005;
Feng and Hovy, 2003).
2.2 Maximum Entropy
Our next approach is the Maximum Entropy (Berger
et al, 1996) classification approach. Here, we cast
our problem as a problem of ranking using a classi-
fier where each slot-value pair in the training data is
considered a class and feature set consists of the un-
igrams, bigrams and trigrams in the sentences (Bha-
gat et al, 2005).
2.3 Support Vector Machines
We use another commonly used classifier, Support
Vector Machine (Burges, 1998), to perform the
same task (Bhagat et al, 2005). Approach is sim-
ilar to Section 2.2.
2.4 Language Model
As a fourth approach to the problem, we use the Sta-
tistical Language Model (Ponte and Croft, 1997).
We estimate the language model for the slot-value
pairs, then we construct our target interpretation as
186
Method Precison Recall F-score
V oting 0.82 0.78 0.80
ME 0.77 0.80 0.78
SVM 0.79 0.72 0.75
LM1 0.80 0.84 0.82
LM2 0.82 0.84 0.83
Table 1: Performance of different systems on test
data.
a set of the most likely slot-value pairs. We use
unigram-based and trigram-based language mod-
els (Bhagat et al, 2005).
3 Experiments and Results
We train all our systems on a training set of 477
sentence-frame pairs. The systems are then tested on
an unseen test set of 50 sentences. For the test sen-
tences, the system generated frames are compared
against the manually built gold standard frames, and
Precision, Recall and F-scores are calculated for
each frame.
Table 1 shows the average Precision, Recall and
F-scores of the different systems for the 50 test sen-
tences: Voting based (Voting), Maximum Entropy
based (ME), Support Vector Machine based (SVM),
Language Model based with unigrams (LM1) and
Language Model based with trigrams (LM2). The
F-scores show that the LM2 system performs the
best though the system scores in general for all the
systems are very close. To test the statistical signifi-
cance of these scores, we conduct a two-tailed paired
Student?s t test (Manning and Schtze, 1999) on the
F-scores of these systems for the 50 test cases. The
test shows that there is no statistically significant dif-
ference in their performances.
4 Conclusions
This work illustrates that one can achieve fair suc-
cess in building a statistical NLU engine for a re-
stricted domain using relatively little training data
and surprisingly using a rather simple voting model.
The consistently good results obtained from all the
systems on the task clearly indicate the feasibility of
using using only word/ngram level features for pars-
ing.
5 Future Work
Having successfully met the initial challenge of
building a statistical NLU with limited training data,
we have identified multiple avenues for further ex-
ploration. Firstly, we wish to build an hybrid system
that will combine the strengths of all the systems to
produce a much more accurate system. Secondly,
we wish to see the effect that ASR output has on
each of the systems. We want to test the robustness
of systems against an increase in the ASR word er-
ror rate. Thirdly, we want to build a multi-clause
utterance chunker to integrate with our systems. We
have identified that complex multi-clause utterances
have consistently hurt the system performances. To
handle this, we are making efforts along with our
colleagues in the speech community to build a real-
time speech utterance-chunker. We are eager to dis-
cover any performance benefits. Finally, since we
already have a corpus containing sentence and their
corresponding semantic-frames, we want to explore
the possibility of building a Statistical Generator us-
ing the same corpus that would take a frame as input
and produce a sentence as output. This would take
us a step closer to the idea of building a Reversible
System that can act as a parser when used in one
direction and as a generator when used in the other.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley
framenet project. In Proceedings of COLING/ACL, page 8690, Montreal,
Canada.
Adam L. Berger, Stephen Della Pietra, and Vincent J. Della Pietra. 1996. A
maximum entropy approach to natural language processing. Computational
Linguistics, 22(1):39?71.
Rahul Bhagat, Anton Leuski, and Eduard Hovy. 2005. Statistical shallow
semantic parsing despite little training data. Technical report available at
http://www.isi.edu/?rahul.
Christopher J. C. Burges. 1998. A tutorial on support vector machines for pattern
recognition. Data Mining and Knowledge Discovery, 2(2):121?167.
Donghui Feng and Eduard Hovy. 2003. Semantics-oriented language understand-
ing with automatic adaptability. In Proceedings of Natural Language Process-
ing and Knowledge Engineering.
Paul Kingsbury, Martha Palmer, and Mitch Marcus. 2002. Adding semantic an-
notation to the penn treebank. In Proceedings of HLT Conference.
Christopher D. Manning and Hinrich Schtze. 1999. Foundations of Statistical
Natural Language Processing. The MIT Press, Cambridge, MA.
Jay M. Ponte and W. Bruce Croft. 1997. Text segmentation by topic. In Proceed-
ings of the First European Conference on Research and Advanced Technology
for Digital Libraries, pages 120?129.
W. Swartout, R. Hill, J. Gratch, W. Johnson, C. Kyriakakis, C. LaBore, R. Lind-
heim, S. Marsella, D. Miraglia, B. Moore, J. Morie, J. Rickel, M. Thiebaux,
L. Tuch, R. Whitney, and J. Douglas. 2001. Toward the holodeck: Integrating
graphics, sound, character and story. In Proceedings of Autonomous Agents.
187
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 18?27,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Building Effective Question Answering Characters
Anton Leuski and Ronakkumar Patel and David Traum
Institute for Creative Technologies
University of Southern California
Marina del Rey, CA, 90292, USA
leuski,ronakkup,traum@ict.usc.edu
Brandon Kennedy
Brandon.Kennedy@usma.edu
Abstract
In this paper, we describe methods for
building and evaluation of limited do-
main question-answering characters. Sev-
eral classification techniques are tested, in-
cluding text classification using support
vector machines, language-model based
retrieval, and cross-language information
retrieval techniques, with the latter having
the highest success rate. We also evalu-
ated the effect of speech recognition errors
on performance with users, finding that re-
trieval is robust until recognition reaches
over 50% WER.
1 Introduction
In the recent Hollywood movie ?iRobot? set in
2035 the main character played by Will Smith is
running an investigation into the death of an old
friend. The detective finds a small device that
projects a holographic image of the deceased. The
device delivers a recorded message and responds
to questions by playing back prerecorded answers.
We are developing virtual characters with similar
capabilities.
Our target applications for these virtual charac-
ters are training, education, and entertainment. For
use in education, such a character should be able
to deliver a message to the student on a specific
topic. It also should be able to support a basic spo-
ken dialog on the subject of the message, e.g., an-
swer questions about the message topic and give
additional explanations. For example, consider a
student learning about an event in a virtual world.
Lets say there is a small circus in a small town and
someone has released all the animals from circus.
A young student plays a role of a reporter to find
out who caused this local havoc. She is out to in-
terrogate a number of witnesses represented by the
virtual characters. It is reasonable to expect that
each conversation is going to be focused solely on
the event of interest and the characters may refuse
to talk about anything else. Each witness may have
a particular and very narrow view into an aspect of
the event, and the student?s success would depend
on what sort of questions she asks and to which
character she addresses them.
Automatic question answering (QA) has been
studied extensively in recent years. For example,
there is a significant body of research done in the
context of the QA track at the Text REtrieval Con-
ference (TREC) (Voorhees, 2003). In contrast to
the TREC scenario where both questions and an-
swers are based on facts and the goal is to provide
the most relevant answer, we focus the answer?s
appropriateness. In our example about an inves-
tigation, an evasive, misleading, or an ?honestly?
wrong answer from a witness character would be
appropriate but might not be relevant. We try
to highlight that distinction by talking about QA
characters as opposed to QA systems or agents.
We expect that a typical simulation would con-
tain quite a few QA characters. We also expect
those characters to have a natural spoken language
interaction with the student. Our technical require-
ments for such a QA character is that it should be
able to understand spoken language. It should be
robust to disfluencies in conversational English. It
should be relatively fast, easy, and inexpensive to
construct without the need for extensive domain
knowledge and dialog management design exper-
tise.
In this paper we describe a QA character by the
name of SGT Blackwell who was originally de-
signed to serve as an information kiosk at an army
18
conference (see Appendix C for a photograph of
the system) (?). We have used SGT Blackwell to
develop our technology for automatic answer se-
lection, conversation management, and system in-
tegration. We are presently using this technology
to create other QA characters.
In the next section we outline the SGT Black-
well system setup. In Section 3 we discuss the
answer selection problem and consider three dif-
ferent algorithms: Support Vector Machines clas-
sifier (SVM), Language Model retrieval (LM), and
Cross-lingual Language Model (CLM) retrieval.
We present the results of off-line experiments
showing that the CLM method performs signifi-
cantly better than the other two techniques in Sec-
tion 4. Section 5 describes a user study of the sys-
tem that uses the CLM approach for answer selec-
tion. Our results show that the approach is very
robust to deviations in wording from expected an-
swers, and speech recognition errors. Finally, we
summarize our results and outline some directions
for future work in Section 6.
2 SGT Blackwell
A user talks to SGT Blackwell using a head-
mounted close capture USB microphone. The
user?s speech is converted into text using an au-
tomatic speech recognition (ASR) system. We
used the Sonic statistical speech recognition en-
gine from the University of Colorado (Pellom,
2001) with acoustic and language models pro-
vided to us by our colleagues at the University of
Southern California (Sethy et al, 2005). The an-
swer selection module analyzes the speech recog-
nition output and selects the appropriate response.
The character can deliver 83 spoken lines rang-
ing from one word to a couple paragraphs long
monologues. There are three kinds of lines SGT
Blackwell can deliver: content, off-topic, and
prompts. The 57 content-focused lines cover the
identity of the character, its origin, its language
and animation technology, its design goals, our
university, the conference setup, and some mis-
cellaneous topics, such as ?what time is it?? and
?where can I get my coffee??
When SGT Blackwell detects a question that
cannot be answered with one of the content-
focused lines, it selects one out of 13 off-topic re-
sponses, (e.g., ?I am not authorized to comment
on that,?) indicating that the user has ventured out
of the allowed conversation domain. In the event
that the user persists in asking the questions for
which the character has no informative response,
the system tries to nudge the user back into the
conversation domain by suggesting a question for
the user to ask: ?You should ask me instead about
my technology.? There are 7 different prompts in
the system.
One topic can be covered by multiple answers,
so asking the same question again often results in
a different response, introducing variety into the
conversation. The user can specifically request
alternative answers by asking something along
the lines of ?do you have anything to add?? or
?anything else?? This is the first of two types
command-like expressions SGT Blackwell under-
stands. The second type is a direct request to re-
peat the previous response, e.g., ?come again?? or
?what was that??
If the user persists on asking the same question
over and over, the character might be forced to re-
peat its answer. It indicates that by preceding the
answer with one of the four ?pre-repeat? lines in-
dicating that incoming response has been heard re-
cently, e.g., ?Let me say this again...?
3 Answer Selection
The main problem with answer selection is uncer-
tainty. There are two sources of uncertainty in
a spoken dialog system: the first is the complex
nature of natural language (including ambigu-
ity, vagueness, underspecification, indirect speech
acts, etc.), making it difficult to compactly char-
acterize the mapping from the text surface form to
the meaning; and the second is the error-prone out-
put from the speech recognition module. One pos-
sible approach to creating a language understand-
ing system is to design a set of rules that select a
response given an input text string (Weizenbaum,
1966). Because of uncertainty this approach can
quickly become intractable for anything more than
the most trivial tasks. An alternative is to cre-
ate an automatic system that uses a set of train-
ing question-answer pairs to learn the appropriate
question-answer matching algorithm (Chu-Carroll
and Carpenter, 1999). We have tried three differ-
ent methods for the latter approach, described in
the rest of this section.
3.1 Text Classification
The answer selection problem can be viewed as a
text classification task. We have a question text
19
as input and a finite set of answers, ? classes, ?
we build a system that selects the most appropriate
class or set of classes for the question. Text classi-
fication has been studied in Information Retrieval
(IR) for several decades (Lewis et al, 1996). The
distinct properties of our setup are (1) a very small
size of the text, ? the questions are very short, and
(2) the large number of classes, e.g, 60 responses
for SGT Blackwell.
An answer defines a class. The questions corre-
sponding to the answer are represented as vectors
of term features. We tokenized the questions and
stemmed using the KStem algorithm (Krovetz,
1993). We used a tf ? idf weighting scheme to
assign values to the individual term features (Al-
lan et al, 1998). Finally, we trained a multi-class
Support Vector Machines (SVM struct) classifier
with an exponential kernel (Tsochantaridis et al,
2004). We have also experimented with linear
kernel function, various parameter values for the
exponential kernel, and different term weighting
schemes. The reported combination of the ker-
nel and weighting scheme showed the best clas-
sification performance. Such an approach is well-
known in the community and has been shown to
work very well in numerous applications (Leuski,
2004). In fact, SVM is generally considered to be
one of the best performing methods for text clas-
sification. We believe it provides us with a very
strong baseline.
3.2 Answer Retrieval
The answer selection problem can also be viewed
as an information retrieval problem. We have a
set of answers which we can call documents in ac-
cordance with the information retrieval terminol-
ogy. Let the question be the query, we compare
the query to each document in the collection and
return the most appropriate set of documents.
Presently the best performing IR techniques
are based on the concept of Language Model-
ing (Ponte and Croft, 1997). The main strategy
is to view both a query and a document as samples
from some probability distributions over the words
in the vocabulary (i.e., language models) and com-
pare those distributions. These probability distri-
butions rarely can be computed directly. The ?art?
of the field is to estimate the language models as
accurately as possible given observed queries and
documents.
Let Q = q1...qm be the question that is re-
ceived by the system, RQ is the set of all the an-
swers appropriate to that question, and P (w|RQ)
is the probability that a word randomly sampled
from an appropriate answer would be the word w.
The language model of Q is the set of probabili-
ties P (w|RQ) for every word in the vocabulary. If
we knew the answer set for that question, we can
easily estimate the model. Unfortunately, we only
know the question and not the answer set RQ. We
approximate the language model with the condi-
tional distribution:
P (w|RQ) ? P (w|Q) =
P (w, q1, ..., qm)
P (q1, ..., qm)
(1)
The next step is to calculate the joint probabil-
ity of observing a string: P (W ) = P (w1, ..., wn).
Different methods for estimating P (W ) have been
suggested starting with simple unigram approach
where the occurrences of individual words are as-
sumed independent from each other: P (W ) =
?n
i=1 P (wi). Other approaches include Proba-
bilistic Latent Semantic Indexing (PLSI) (Hoff-
man, 1999) and Latent Dirichlet Allocation
(LDA) (Blei et al, 2003). The main goal of these
different estimations is to model the interdepen-
dencies that exist in the text and make the esti-
mation feasible given the finite amount of training
data.
In this paper we adapt an approach suggested
by Lavrenko (Lavrenko, 2004). He assumed that
all the word dependencies are defined by a vector
of possibly unknown parameters on the language
model. Using the de Finetti?s representation the-
orem and kernel-based probability estimations, he
derived the following estimate for the query lan-
guage model:
P (w|Q) =
?
s?S pis(w)
?m
i=1 pis(qi)
?
s
?m
i=1 pis(qi)
(2)
Here we sum over all training strings s ? S,
where S is the set of training strings. pis(w) is the
probability of observing word w in the string s,
which can be estimated directly from the training
data. Generally the unigram maximum likelihood
estimator is used with some smoothing factor:
pis(w) = ?pi ?
#(w, s)
|s|
+ (1? ?pi) ?
?
s #(w, s)
?
s |s| (3)
20
where #(w, s) is the number of times word w ap-
pears in string s, |s| is the length of the string s,
we sum over all training strings s ? S, and the
constant ?pi is the tunable parameter that can be
determined from training data.
We know all the possible answers, so the answer
language model P (w|A) can be estimated from
the data:
P (w|A) = piA(w) (4)
3.3 Ranking criteria
To compare two language models we use the
Kullback-Leibler divergence D(pq||pa) defined as
D(pq||pa) =
?
w?V
P (w|Q) log
P (w|Q)
P (w|A)
(5)
which can be interpreted as the relative entropy be-
tween two distributions. Note that the Kullback-
Leibler divergence is a dissimilarity measure, we
use ?D(pq||pa) to rank the answers.
So far we have assumed that both questions
and answers use the same vocabulary and have
the same a priori language models. Clearly, it is
not the case. For example, consider the follow-
ing exchange: ?what happened here?? ? ?well,
maam, someone released the animals this morn-
ing.? While the answer is likely to be very appro-
priate to the question, there is no word overlap be-
tween these sentences. This is an example of what
is known in information retrieval as vocabulary
mismatch between the query and the documents.
In a typical retrieval scenario a query is assumed
to look like a part of a document. We cannot make
the same assumption about the questions because
of the language rules: e.g., ?what?, ?where?, and
?why? are likely to appear much more often in
questions than in answers. Additionally, a typi-
cal document is much larger than any of our an-
swers and has a higher probability to have words
in common with the query. Finally, a typical re-
trieval scenario is totally context-free and a user is
encouraged to specify her information need as ac-
curately as possible. In a dialog, a portion of the
information is assumed to be well-known to the
participants and remains un-verbalized leading to
sometimes brief questions and answers.
We believe this vocabulary mismatch to be so
significant that we view the participants as speak-
ing two different ?languages?: a language of ques-
tions and a language of answers. We will model
the problem as a cross-lingual information task,
where one has a query in one language and wishes
to retrieve documents in another language. There
are two ways we can solve it: we can translate the
answers into the question language by building a
representation for each answer using the question
vocabulary or we can build question representa-
tions in the answer language.
3.4 Question domain
We create an answer representation in the ques-
tion vocabulary by merging together all the train-
ing questions that are associated with the answer
into one string: a pseudo-answer. We use equa-
tions 5, 2, 3, and 4 to compare and rank the
pseudo-answers. Note that in equation 2 s iterates
over the set of all pseudo-answers.
3.5 Answer domain
Let us look at the question language model
P (w|Q) again, but now we will take into account
that w and Q are from different vocabularies and
have potentially different distributions:
P (w|Q) =
?
s ?As(w)
?m
i=1 piQs(qi)
?
s
?m
i=1 piQs(qi)
(6)
Here s iterates over the training set of question-
answer pairs {Qs, As} and ?x(w) is the experi-
mental probability distribution on the answer vo-
cabulary given by the expression similar to equa-
tion 3:
?x(w) = ??
#(w, x)
|x|
+ (1? ??)
?
s #(w, x)
?
s |x|
and the answer language model P (w|A) can be
estimated from the data as
P (w|A) = ?A(w)
4 Algorithm comparison
We have a collection of questions for SGT Black-
well each linked to a set of appropriate responses.
Our script writer defined the first question or two
for each answer. We expanded the set by a) para-
phrasing the initial questions and b) collecting
questions from users by simulating the final sys-
tem in a Wizard of Oz study (WOZ). There are
1,261 questions in the collection linked to 72 an-
swers (57 content answers, 13 off-topic responses,
and 2 command classes, see Section 2). For this
21
study we considered all our off-topic responses
equally appropriate to an off-topic question and
we collapsed all the corresponding responses into
one class. Thus we have 60 response classes.
We divided our collection of questions into
training and testing subsets following the 10-fold
cross-validation schema. The SVM system was
trained to classify test questions into one of the 60
classes.
Both retrieval techniques produce a ranked list
of candidate answers ordered by the ?D(pq||pa)
score. We only select the answers with scores that
exceed a given threshold ?D(pq||pa) > ? . If the
resulting answer set is empty we classify the ques-
tion as off-topic, i.e., set the candidate answer set
contains to an off-topic response. We determine
the language model smoothing parameters ?s and
the threshold ? on the training data.
We consider two statistics when measuring the
performance of the classification. First, we mea-
sure its accuracy. For each test question the first
response returned by the system, ? the class from
the SVM system or the top ranked candidate an-
swer returned by either LM or CLM methods, ?
is considered to be correct if there is link between
the question and the response. The accuracy is the
proportion of correctly answered questions among
all test questions.
The second statistic is precision. Both LM and
CLM methods may return several candidate an-
swers ranked by their scores. That way a user will
get a different response if she repeats the question.
For example, consider a scenario where the first
response is incorrect. The user repeats her ques-
tion and the system returns a correct response cre-
ating the impression that the QA character simply
did not hear the user correctly the first time. We
want to measure the quality of the ranked list of
candidate answers or the proportion of appropri-
ate answers among all the candidate answers, but
we should also prefer the candidate sets that list all
the correct answers before all the incorrect ones.
A well-known IR technique is to compute aver-
age precision ? for each position in the ranked list
compute the proportion of correct answers among
all preceding answers and average those values.
Table 1 shows the accuracy and average preci-
sion numbers for three answer selection methods
on the SGT Blackwell data set. We observe a sig-
nificant improvement in accuracy in the retrieval
methods over the SVM technique. The differences
shown are statistical significant by t-test with the
cutoff set to 5% (p < 0.05).
We repeated out experiments on QA charac-
ters we are developing for another project. There
we have 7 different characters with various num-
ber of responses. The primary difference with
the SGT Blackwell data is that in the new sce-
nario each question is assigned to one and only
one answer. Table 2 shows the accuracy numbers
for the answer selection techniques on those data
sets. These performance numbers are generally
lower than the corresponding numbers on the SGT
Blackwell collection. We have not yet collected
as many training questions as for SGT Blackwell.
We observe that the retrieval approaches are more
successful for problems with more answer classes
and more training data. The table shows the per-
cent improvement in classification accuracy for
each LM-based approach over the SVM baseline.
The asterisks indicate statistical significance using
a t-test with the cutoff set to 5% (p < 0.05).
5 Effect of ASR
In the second set of experiments for this paper
we studied the question of how robust the CLM
answer selection technique in the SGT Blackwell
system is to the disfluencies of normal conversa-
tional speech and errors of the speech recogni-
tion. We conducted a user study with people in-
terviewing SGT Blackwell and analyzed the re-
sults. Because the original system was meant for
one of three demo ?reporters? to ask SGT Black-
well questions, specialized acoustic models were
used to ensure the highest accuracy for these three
(male) speakers. Consequently, for other speak-
ers (especially female speakers), the error rate was
much higher than for a standard recognizer. This
allowed us to calculate the role of a variety of
speech error rates on classifier performance.
For this experiment, we recruited 20 partici-
pants (14 male, 6 female, ages from 20 to 62)
from our organization who were not members of
this project. All participants spoke English flu-
ently, however the range of their birth languages
included English, Hindi, and Chinese.
After filling out a consent form, participants
were ?introduced? to SGT Blackwell, and demon-
strated the proper technique for asking him ques-
tions (i.e., when and how to activate the micro-
phone and how to adjust the microphone posi-
tion.) Next, the participants were given a scenario
22
SVM LM CLM
accuracy accuracy impr. SVM avg. prec. accuracy impr. SVM avg. prec.
53.13 57.80 8.78 63.88 61.99 16.67 65.24
Table 1: Comparison of three different algorithms for answer selection on SGT Blackwell data. Each
performance number is given in percentages.
number of number of SVM LM CLM
questions answers accuracy accuracy impr. SVM accuracy impr. SVM
1 238 22 44.12 47.06 6.67* 47.90 8.57*
2 120 15 63.33 62.50 -1.32 64.17 1.32
3 150 23 42.67 44.00 3.12* 50.00 17.19*
4 108 18 42.59 44.44 4.35* 50.00 17.39*
5 149 33 32.21 41.35 28.37* 42.86 33.04*
6 39 8 69.23 58.97 -14.81* 66.67 -3.70
7 135 31 42.96 44.19 2.85 50.39 17.28*
average 134 21 48.16 48.93 1.60* 53.14 10.34*
Table 2: Comparison of three different algorithms for answer selection on 7 additional QA characters.
The table shows the number of answers and the number of questions collected for each character. The
accuracy and the improvement over the baseline numbers are given in percentages.
wherein the participant would act as a reporter
about to interview SGT Blackwell. The partici-
pants were then given a list of 10 pre-designated
questions to ask of SGT Blackwell. These ques-
tions were selected from the training data. They
were then instructed to take a few minutes to
write down an additional five questions to ask SGT
Blackwell. Finally they were informed that af-
ter asking the fifteen written down questions, they
would have to spontaneously generate and ask five
additional questions for a total of 20 questions
asked all together. Once the participants had writ-
ten down their fifteen questions, they began the
interview with SGT Blackwell. Upon the com-
pletion of the interview the participants were then
asked a short series of survey questions by the
experimenter about SGT Blackwell and the inter-
view. Finally, participants were given an explana-
tion of the study and then released. Voice record-
ings were made for each interview, as well as the
raw data collected from the answer selection mod-
ule and ASR. This is our first set of question an-
swer pairs, we call it the ASR-QA set.
The voice recordings were later transcribed. We
ran the transcriptions through the CLM answer se-
lection module to generate answers for each ques-
tion. This generated question and answer pairs
based on how the system would have responded
to the participant questions if the speech recogni-
tion was perfect. This is our second set of ques-
tion answer pairs ? the TRS-QA set. Appendix B
shows a sample dialog between a participant and
SGT Blackwell.
Next we used three human raters to judge the
appropriateness of both sets. Using a scale of
1-6 (see Appendix A) each rater judged the ap-
propriateness of SGT Blackwell?s answers to the
questions posed by the participants. We evaluated
the agreement between raters by computing Cron-
bach?s alpha score, which measures consistency in
the data. The alpha score is 0.929 for TRS-QA
and 0.916 for ASR-QA, which indicate high con-
sistency among the raters.
The average appropriateness score for TRS-QA
is 4.83 and 4.56 for ASR-QA. The difference in
the scores is statistically significant according to t-
test with the cutoff set to 5%. It may indicate that
ASR quality has a significant impact on answer
selection.
We computed the Word Error Rate (WER) be-
tween the transcribed question text and the ASR
output. Thus each question-answer pair in the
ASR-QA and TRS-QA data set has a WER score
assigned to it. The average WER score is 37.33%.
We analyzed sensitivity of the appropriateness
score to input errors. Figure 1a and 1b show
plots of the cumulative average appropriateness
score (CAA) as function of WER: for each WER
value t we average appropriateness scores for all
questions-answer pairs with WER score less than
23
(a) pre-designated (b) user-designated
Figure 1: Shows the cumulative average appropriateness score (CAA) of (a) pre-designated and (b)
user-designated question-answer pairs as function of the ASR?s output word error rate. We show the
scores for TRS-QA (dotted black line) and ASR-QA (solid black line). We also show the percentage of
the question-answer pairs with the WER score below a given value (?# ofQA?) as a gray line with the
corresponding values on the right Y axis.
or equal to t.
CAA(t) =
1
|S|
?
p?S
A(p), S = {p|WER(p) ? t}
where p is a question-answer pair, A(p) is the
appropriateness score for p, and WER(p) is the
WER score for p. It is the expected value of the ap-
propriateness score if the ASR WER was at most
t.
Both figures show the CAA values for TRS-
QA (dotted black line) and ASR-QA (solid black
line). Both figures also show the percentage of
the question-answer pairs with the WER score be-
low a given value, i.e., the cumulative distribution
function (CDF) for the WER as a gray line with
the corresponding values depicted on the right Y
axis.
Figure 1a shows these plots for the pre-
designated questions. The values of CAA for
TRS-QA and ASR-QA are approximately the
same between 0 and 60% WER. CAA for ASR-
QA decreases for WER above 60% ? as the input
becomes more and more garbled, it becomes more
difficult for the CLM module to select an appropri-
ate answer. We confirmed this observation by cal-
culating t-test scores at each WER value: the dif-
ferences between CAA(t) scores are statistically
significant for t > 60%. It indicates that until
WER exceeds 60% there is no noticeable effect on
the quality of answer selection, which means that
our answer selection technique is robust relative to
the quality of the input.
Figure 1b shows the same plots for the user-
designated questions. Here the system has to deal
with questions it has never seen before. CAA val-
ues decrease for both TRS-QA and ASR-QA as
WER increases. Both ASR and CLM were trained
on the same data set and out of vocabulary words
that affect ASR performance, affect CLM perfor-
mance as well.
6 Conclusions and future work
In this paper we presented a method for efficient
construction of conversational virtual characters.
These characters accept spoken input from a user,
convert it to text, and select the appropriate re-
sponse using statistical language modeling tech-
niques from cross-lingual information retrieval.
We showed that in this domain the performance
of our answer selection approach significantly ex-
ceeds the performance of a state of the art text clas-
sification method. We also showed that our tech-
nique is very robust to the quality of the input and
can be effectively used with existing speech recog-
nition technology.
Preliminary failure analysis indicates a few di-
rections for improving the system?s quality. First,
we should continue collecting more training data
and extending the question sets.
Second, we could have the system generate a
confidence score for its classification decisions.
Then the answers with a low confidence score can
be replaced with an answer that prompts the user
to rephrase her question. The system would then
24
use the original and the rephrased version to repeat
the answer selection process.
Finally, we observed that a notable percent of
misclassifications results from the user asking a
question that has a strong context dependency on
the previous answer or question. We are presently
looking into incorporating this context informa-
tion into the answer selection process.
Acknowledgments
The project or effort described here has been spon-
sored by the U.S. Army Research, Development,
and Engineering Command (RDECOM). State-
ments and opinions expressed do not necessarily
reflect the position or the policy of the United
States Government, and no official endorsement
should be inferred.
References
James Allan, Jamie Callan, W. Bruce Croft, Lisa
Ballesteros, Donald Byrd, Russell Swan, and Jinxi
Xu. 1998. Inquery does battle with TREC-6. In
Sixth Text REtrieval Conference (TREC-6), pages
169?206, Gaithersburg, Maryland, USA.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022.
Jennifer Chu-Carroll and Bob Carpenter. 1999.
Vector-based natural language call routing. Journal
of Computational Linguistics, 25(30):361?388.
Sudeep Gandhe, Andrew S. Gordon, and David Traum.
2006. Improving question-answering with linking
dialogues. In Proceedings of the 11th international
conference on Intelligent user interfaces (IUI?06),
pages 369?371, New York, NY, USA. ACM Press.
T. Hoffman. 1999. Probabilistic latent semantic index-
ing. In Proceedings of the 22nd International ACM
SIGIR Conference, pages 50?57.
Robert Krovetz. 1993. Viewing morphology as an in-
ference process. In Proceedings of the 16th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 191?
202.
Victor Lavrenko. 2004. A Generative Theory of Rele-
vance. Ph.D. thesis, University of Massachusetts at
Amherst.
Anton Leuski. 2004. Email is a stage: discover-
ing people roles from email archives. In Proceed-
ings of 27th annual international ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR?04), pages 502?503, Sheffield,
United Kingdom. ACM Press. NY, USA.
David D. Lewis, Robert E. Schapire, James P. Callan,
and Ron Papka. 1996. Training algorithms for lin-
ear text classifiers. In Proceedings of the 19th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 298?
306, Zurich, Switzerland.
Bryan Pellom. 2001. Sonic: The university of col-
orado continuous speech recognizer. Technical Re-
port TR-CSLR-2001-01, University of Colorado,
Boulder, CO.
Jay M. Ponte and W. Bruce Croft. 1997. Text segmen-
tation by topic. In Proceedings of the First European
Conference on Research and Advanced Technology
for Digital Libraries, pages 120?129.
Abhinav Sethy, Panayiotis Georgiou, and Shrikanth
Narayanan. 2005. Building topic specific language
models from webdata using competitive models. In
Proceedings of EUROSPEECH, Lisbon, Portugal.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vector
machine learning for interdependent and structured
output spaces. In Proceedings of the Twenty-first in-
ternational conference on Machine learning, Banff,
Alberta, Canada.
Ellen M. Voorhees. 2003. Overview of the trec 2003
question answering track. In Proceedings of The
Twelfth Text Retrieval Conference, pages 54?69.
Joseph Weizenbaum. 1966. Eliza?a computer program
for the study of natural language communication be-
tween man and machine. Communications of the
ACM, 9(1):36?45.
25
A Appropriateness grading
Gandhe and his colleagues (Gandhe et al, 2006)
suggested the following grading scheme that we
used in our user evaluation.
Grade Description
1 Response is not related in any way the
question
2 Response contains some discussion
of people or objects mentioned in the
question, but does not really address
the question itself.
3 Response partially addresses the
question, but little or no coherence
between the question and response.
4 Response does mostly address the
question, but with major problems in
the coherence between question and
response; seems like the response is
really addressing a different question
than the one asked.
5 Response does address the question,
but the transition is somewhat awk-
ward.
6 Response answers the question in a
perfectly fluent manner.
Table 3: The question-answer appropriateness
scale.
B Sample dialog
This section presents an example of a participant?s
dialog with SGT Blackwell. We only show spon-
taneous questions asked by the participant. Each
of the participant?s questions (called ?Reporter? in
this transcript) is followed by the ASR output of
that line in brackets. That is what the answer se-
lection module ?heard.? Note that ASR does make
an occasional mistake and the answer selection is
able to recover gracefully until the next to the last
question, where the input was so garbled that only
one correct word got through.
Reporter: Who created you (Who created you)
Blackwell: Tell you the truth, I have spent a lot
of time with the smart folks at ICT, testing
and tweaking. I am about ready to get out
there and train with real soldiers. I am think-
ing first stop is going to be Fort Sill. But my
ultimate goal is to get forward deployed...
Reporter: Why should we spend money on you
(What should we spend money on you)
Blackwell: Why do you need me?! What you
should be saying is ?How did you get alng
without me?? I will show you how to be a
leader, how to make critical decisions under
stress... And I am glad to be here, because I
can help. I know I can. Look... I have one
mission: to make better soldiers, make bet-
ter warfighters. And if I can ... can save one
life... bring one loved one back to his fam-
ily... Well, then, I am definitely worth it. You
be the judge.
Reporter: What?s your value (What?s the value
in)
Blackwell: I can do ?Virtually? anything. I do
mission rehearsals, Urban Combat training,
language training - I speak Farsi, Pashtun and
Arabic. ?Mar Haba shabab! That means how
are you in Arabic.
Reporter: Do you really work or is this just a
demo (G. really worker is this just a demo)
Blackwell: Sorry. That?s outside my AO.
Reporter: How could you be better (How could
you be better)
Blackwell: Stop mumbling. Just kidding. I
didn?t get that.
Reporter: What do you wish you could do
(When you wish you could do)
Blackwell: Career Army. That is what I am
thinking. Just call me a lifer.
Reporter: What kind of people have you trained
(What other people in the turn in)
Blackwell: I teach cultural awareness, function
as a training mentor and can call in the full
range of Joint fires and effects.
Reporter: What?s the hardest part of your job
(He was the hardest are you job)
Blackwell: I am not authorized to comment on
that
Reporter: You have any virtual friends (The uni-
versal friends)
26
Blackwell: Sorry, I can?t hear you
Reporter: Are you hard of hearing (Are you had
a hearing)
Blackwell: I can?t understand you.
C SGT Blackwell
Figure 2: A photograph of the SGT Blackwell sys-
tem setup.
27
Monitoring the News: a TDT demonstration system
David Frey, Rahul Gupta, Vikas Khandelwal,
Victor Lavrenko, Anton Leuski, and James Allan
Center for Intelligent Information Retrieval
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
ABSTRACT
We describe a demonstration system built upon Topic Detection
and Tracking (TDT) technology. The demonstration system moni-
tors a stream of news stories, organizes them into clusters that rep-
resent topics, presents the clusters to a user, and visually describes
the changes that occur in those clusters over time. A user may also
mark certain clusters as interesting, so that they can be ?tracked?
more easily.
1. TDT BACKGROUND
The Topic Detection and Tracking (TDT) research program in-
vestigates methods for organizing an arriving stream of news sto-
ries by the topics the stories discuss.[1, 4, 7, 8] Topics are de?ned
to be the set of stories that follow from some seminal event in the
world?this is in contrast to a broader subject-based notion of topic.
That is, stories about a particular airline crash fall into one topic,
and stories from other airline crashes will be in their own topics.
All organization is done as stories arrive, though variations of
the task allow ?nal organizational decisions to be postponed for
minutes, hours, or even days. The formal TDT evaluation program
includes the following research tasks:
1. Segmentation is used to separate a television or radio pro-
gram into distinct news stories. This process is not needed
for newswire services, since those stories arrive pre-segmented.
2. Detection is the task of putting all arriving news stories into
bins that represent broad news topics. If a new topic appears
in the news, the system must create a new bin. Neither the
set of bins nor the total number of them is known in advance.
This task is carried out without any supervision?i.e., the
system never knows whether or not the stories it is putting
together actually belong together.
3. Tracking is the task of ?nding all stories that follow are on
the same topic as an initial small set. This task is different
from detection in that the starting stories are known to be on
the same topic. Typically tracking is evaluated with 2-4 on-
topic stories.
.
The TDT research workshops also include a few other tasks (?rst
story detection, and story link detection). TDT has also inspired
other event-based organization methods, including automatic time-
line generation to visualize the temporal locality of topics[10], and
the identi?cation of new information within a topic?s discussion[3].
This demonstration system illustrates event-based news organi-
zation by visualizing the creation of, changes within, and relation-
ships between clusters created by the detection task. It leverages
the segmentation results so that audio stories are distinct stories,
but does not directly visualize the detection. Tracking is implicity
presented by allowing clusters to be marked so that they receive
special attention by the user.
2. ARCHITECTURE
The TDT demonstration system is based upon Lighthouse, an
interactive information retrieval system developed by Leuski.[6]
Lighthouse provides not only a typical ranked list search result, but
a visualization of inter-document similarities in 2- or 3-dimensions.
The user interface is a Java client that can run as an application or
an applet. Lighthouse uses http protocols to send queries to a server
and receive the ranked list, summary information about the docu-
ments, and the visualization data.
The TDTLighthouse system requires a TDT system running in
the background. In this version of the demonstration, the TDT sys-
tem is only running the segmentation and detection tasks described
above. Stories arrive and are put into clusters (bins).
The TDTLighthouse client can query its server to receive up-to-
date information about the clusters that the TDT system has found.
The server in turn queries the TDT system to get that information
and maintains state information so that changes (cluster growth,
additional clusters, etc.) can be highlighted.
3. DEMONSTRATION DATA
The data for this demonstration was taken from the our TDT
2000 evaluation output on the TDT cluster detection task [8]. The
sytem is running on the TDT-3 evaluation collection of news arti-
cles, approximately 40,000 news stories spanning October 1 through
December 31, 1998.
We simulated incremental arrival of the data as follows. At the
end of each day in the collection, we looked at the incremental
output of the TDT detection system. At this point, every story has
been classi?ed into a cluster. Every story seen to date is in one of
the clusters for that day, even if the cluster has the same contents as
it did yesterday.
The demonstration is designed to support text summarization
tools that could help a user understand the content of the cluster.
For our purposes, each cluster was analyzed to construct the fol-
lowing information:
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                
Figure 1: TDT demonstration system running on TDT-3 data, approximately four weeks into the collection.
1. The title was generated by selecting the 10 most commonly
occurring non-stopwords throughout the cluster. A better ti-
tle would probably be the headline of the most ?representa-
tive? news story, though this is an open research question.
2. The summary was generated by selecting the ?ve sentences
that were most representative of the entire cluster. Better ap-
proaches might generate a summary from the multiple doc-
uments [9] or summarize the changes from the previous day
[5, 2].
3. The contents of the cluster is just a list of every story in the
cluster, presented in reverse chronological order. Various
alternative presentations are possible, including leveraging
the multimedia (radio and television) that is the basis for the
TDT data.
The demonstration system was setup so that it could move from
between the days. All of the input to the client was generated au-
tomatically, but we saved the information so that it could be shown
more quickly. It typically takes a few minutes to generate all of the
presentation information for a single day?s clusters.
4. DEMONSTRATION SYSTEM
Figure 1 shows the client window. This snapshot shows the sys-
tem on October 31 at 10:00pm, approximately four weeks into the
data. The status line on the lower-left shows that at this point the
system has already encountered almost 16,000 stories and has bro-
ken them into about 2400 topic clusters.
The system is showing the 50 topics with the largest number of
stories. The ranked list (by size) starts on the upper-left, shows the
?rst 25, and the continues in the upper-right. The ?title? for each
of those topics is generated in this case by the most common words
within the cluster. Any system that does a better job of building
a title for a large cluster of stories could be used to improve this
capability.
In addition to the ranked list of topics, the system computes inter-
topic similarities and depicts that using the spheres in the middle.
If two topics are highly similar, their spheres will appear near each
other in the visualization. This allows related topics to be detected
quickly. Because the 50 largest topics are shown, the topics are
more unalike than they would be with a wider range, but it is still
possible to see, for example, that topics about the Clinton pres-
idency are near each other (the cyan pair of spheres overlapping
rank number 9, topic rank numbers 5 and 29). The spheres and the
ranked list are tightly integrated, so selecting one causes the other
to be highlighted.
Topics can be assigned colors to make them easier to pick out in
future sessions. In this case, the user has chosen to use the same
color for a range of related topics?e.g., red for sports topics, green
for weather topics, etc. The color selection is in the control of
the user and is not done automatically. However, once a color is
assigned to a topic, the color is ?sticky? for future sessions. A user
might choose to color a critical topic bright red so that changes to
it stand out in the future.
Figure 2 shows the same visualization, but here a summary of
a selected topic is shown in a pop-up balloon. This summary was
generated by selecting sentences that contained large numbers of
key concepts from the topic. Any summarization of a cluster could
be used here if it provided more useful information.
To illustrate how the demonstration system shows changes in
TDT clusters over time, Figure 3 shows an updated visualization
for two weeks later (November 14, 1998). The topic colors are
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                
Figure 2: Similar to Figure 1, but showing a pop-up balloon.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                
Figure 4: A 3-dimensional version of Figure 3.
persistent from Figure 1, though one of the marked topics (?Straw-
berry cancer colon Yankee?) is no longer in the largest 50 so does
not appear.
Most of the spheres include a small ?wedge? of yellow in them.
That indicates the proportion of the topic that is new stories (since
Figure 1). Some topics have large numbers of new stories, so have
a large yellow slice, whereas a few have a very small number of
new stories, so have only a thin wedge. The yellow wedge can be
as much as 50% of the sphere (which would represent an entirely
new topic), and only covers the top of the sphere. This restriction
ensures that the topic color is still visible.
The controls at the top of the screen are for moving between
queries, issuing a query, and returning the visualization to a ?home?
point. The next ?ve controls affect the layout of the display, includ-
ing allowing a 3-D display: a 3-D version of Figure 3 is shown in
Figure 4. The ?nal control enables a browsing wizard that can be
used to ?nd additional topics that are very similar to a selected topic
color (that set is chosen using the pull-down menu that has ?none?
in it).
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                
Figure 3: TDT demonstration system running on TDT-3 data, approximately six weeks into the collection.
5. CONCLUSION AND FUTURE WORK
The demonstration system described above illustrates the effect
of TDT technology. It is also interesting in its own right, allow-
ing a user to track news topics of interest and to see how changes
occur over time. There is no reason that the same system could
not be used for non-TDT environments: any setting that clusters
documents might be appropriate for this system.
We are working to extend the demonstration system to include
some additional features.
? Considering the large number of topics (almost 3,000 in Fig-
ure 3), it is unlikely that all ?interesting? topics will be ?nd-
able. The query box at the top of the display will be used to
allow the user to ?nd topics that match a request. The ranked
list will display the top 50 topics that match the query.
? Related to querying, we hope to include an ?alert? feature
that will ?ag newly-created topics that match a query. For
example, an analyst interested in the Middle East might de-
velop a query that would identify topics in that region. When
such a topic appeared, it would be ?agged for the user (prob-
ably with a ?hot topic? color).
? We hope to allow user ?correction? of the topic breakdown
provided by the TDT system. The state-of-the-art in TDT
still makes mistakes, sometimes pulling two similar topics
together, and sometimes breaking a single topic into multiple
clusters. We intend that a user who sees such a mistake be
able to indicate it to the system. That information will, in
turn, to be relayed back to the TDT system to affect future
processing.
? We will be implementing an ?explode this topic? feature that
will show the stories within a topic analogously to the way
the current system shows the topics within the news. If the
topic is small enough, for example, the spheres would repre-
sent stories within the topic. If the topic is larger, the spheres
might represnt sub-clusters within the topic.
Acknowledgments
This material is based on work supported in part by the Library of
Congress and Department of Commerce under cooperative agree-
ment number EEC-9209623, and in part by SPAWARSYSCEN-SD
contract number N66001-99-1-8912. Any opinions, ?ndings and
conclusions or recommendations expressed in this material are the
authors? and do not necessarily re?ect those of the sponsor.
6. REFERENCES
[1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and
Y. Yang. Topic detection and tracking pilot study: Final
report. In Proceedings of the DARPA Broadcast News
Transcription and Understanding Workshop, pages 194?218,
1998.
[2] J. Allan, R. Gupta, and K. Khandelwal. Temporal summaries
of news topics. Technical Report IR-226, University of
Massachusetts, CIIR, 2001.
[3] J. Allan, H. Jin, M. Rajman, C. Wayne, D. Gildea,
V. Lavrenko, R. Hoberman, and D. Caputo. Topic-based
novelty detection: 1999 summer workshop at CLSP, ?nal
report. Available at http://www.clsp.jhu.edu/ws99/tdt, 1999.
[4] DARPA, editor. Proceedings of the DARPA Broadcast news
Workshop, Herndon, Virginia, February 1999.
[5] V. Khandelwal, R. Gupta, and J. Allan. An evaluation
scheme for summarizing topic shifts in news streams. In
Notebook proceedings of HLT 2001, 2001.
[6] A. Leuski and J. Allan. Lighthouse: Showing the way to
relevant information. In Proceedings of the IEEE Symposium
on Information Visualization (InfoVis), pages 125?130, 2000.
[7] NIST. Proceedings of the TDT 1999 workshop. Notebook
publication for participants only, March 2000.
[8] NIST. Proceedings of the TDT 2000 workshop. Notebook
publication for participants only, November 2000.
[9] D. R. Radev, H. Jing, and M. Budzikowska. Summarization
of multiple documents: clustering, sentence extraction, an d
evaluation. ANLP/NAACL Workshop on Summarization,
Seattle, WA, 2000.
[10] Russell Swan and James Allan. Automatic generation of
overview timelines. In Proceedings of SIGIR, pages 49?56,
Athens, Greece, 2000. ACM.
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 39?48,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Toward Learning and Evaluation of Dialogue Policies with Text Examples
David DeVault and Anton Leuski and Kenji Sagae
Institute for Creative Technologies
University of Southern California
Playa Vista, CA 90094
{devault,leuski,sagae}@ict.usc.edu
Abstract
We present a dialogue collection and enrich-
ment framework that is designed to explore
the learning and evaluation of dialogue poli-
cies for simple conversational characters us-
ing textual training data. To facilitate learning
and evaluation, our framework enriches a col-
lection of role-play dialogues with additional
training data, including paraphrases of user ut-
terances, and multiple independent judgments
by external referees about the best policy re-
sponse for the character at each point. As
a case study, we use this framework to train
a policy for a limited domain tactical ques-
tioning character, reaching promising perfor-
mance. We also introduce an automatic policy
evaluation metric that recognizes the validity
of multiple conversational responses at each
point in a dialogue. We use this metric to ex-
plore the variability in human opinion about
optimal policy decisions, and to automatically
evaluate several learned policies in our exam-
ple domain.
1 Introduction
There is a large class of potential users of dialogue
systems technology who lack the background for
many of the formal modeling tasks that typically
are required in the construction of a dialogue sys-
tem. The problematic steps include annotating the
meaning of user utterances in some semantic formal-
ism, developing a formal representation of informa-
tion state, writing detailed rules that govern dialogue
management, and annotating the meaning of system
utterances in support of language generation, among
other tasks.
In this paper, we explore data collection and ma-
chine learning techniques that enable the implemen-
tation of domain-specific conversational dialogue
policies through a relatively small data collection ef-
fort, and without any formal modeling. We present
a case study, which serves to illustrate some of
the possibilities in our framework. In contrast to
recent work on data-driven dialogue policy learn-
ing that learns dialogue behavior from existing data
sources (Gandhe and Traum, 2007; Jafarpour et al,
2009; Ritter et al, 2010), we address the task of au-
thoring a dialogue policy from scratch with a spe-
cific purpose, task and scenario in mind. We exam-
ine the data collection, learning and evaluation steps.
The contributions of this work include a data col-
lection and enrichment framework without formal
modeling, and the creation of dialogue policies from
the collected data. We also propose a framework for
evaluating learned policies. We show, for the sce-
nario in our case study, that these techniques deliver
promising levels of performance, and point to possi-
ble future developments in data-driven dialogue pol-
icy creation and evaluation.
2 Case study
For our case study we selected an existing dialogue
system scenario designed for Tactical Questioning
training (Traum et al, 2008). The character targeted
in our study, Amani, is modeled closely after the
Amani Tactical Questioning character described by
Gandhe et al (2009) and Artstein et al (2009). Tac-
tical Questioning dialogues are those in which small
unit military personnel, usually on patrol, hold con-
versations with individuals to produce information
of military value. A tactical questioning dialogue
39
system is a simulation training environment where
virtual characters play the role of a person being
questioned. Tactical questioning characters are de-
signed to be non-cooperative at times. They may
answer some of the interviewers questions in a coop-
erative manner, but may refuse to answer other ques-
tions, or intentionally provide incorrect answers.
Therefore the interviewer is encouraged to conduct
the interview in a manner that induces cooperation
from the character: building rapport with the char-
acter, addressing their concerns, making promises
and offers, as well as threatening or intimidating the
character; the purpose of the dialogue system is to
allow trainees to practice these strategies in a realis-
tic setting (Gandhe et al, 2009).
This type of scenario is a good testbed for our
proposed learning and evaluation framework, since
it involves both flexible conversational choices and
well-defined constraints regarding the disclosure of
specific information. In the Amani scenario, the user
plays the role of a commander of a small military
unit in Iraq whose unit had been attacked by sniper
fire. The user interviews a character named Amani
who was a witness to the incident and is thought to
have some information about the identity of the at-
tackers. Amani is willing to tell the interviewer ev-
erything she knows provided that the user promises
her safety, secrecy, and small monetary compensa-
tion for the information (Artstein et al, 2009).
An exhaustive formal definition of Amani?s ideal
dialogue policy might include a large number of
rules covering a wide range of user utterance types.
The key constraints for the training simulation, how-
ever, can be stated simply with a few rules governing
the release of five pieces of information that Amani
knows. Amani will only reveal one of these pieces of
information if a precondition is met. Table 1 shows
how certain information relates to each of the pre-
conditions in Amani?s dialogue policy. Amani can
only reveal a fact from the first column if the user
promised her an item from the second column. For
example, Amani can only tell the user the shooter?s
name if the user promised her safety. If the user
has not promised safety, Amani will ask him for
safety. If the user refuses to promise safety, Amani
will either decline to answer the question or lie to
the interviewer. Amani does keep track of the user?s
promises and once she is promised safety, she would
information precondition
about shooter?s name safety
about shooter?s description safety
about shooter?s location secrecy
about the occupant of the shop secrecy
about shooter?s daily routine money
Table 1: Amani?s dialogue policy.
not ask for it again.
While the key constraints for Amani?s policy, as
summarized in Table 1, may be easily expressed
in terms of rules involving dialogue-acts, the rest
of Amani?s behavior is more open-ended and un-
derspecified. Ideally, the system designers would
like for the character to obey conversational conven-
tions (such as responding appropriately to greetings,
thankings, etc.). Her responses to other user utter-
ances should match human intuition about what a
good response would be, but specific responses are
not generally dictated by the goals for the training
simulation. There is therefore room for some flex-
ibility, and also for the character to reply that she
does not understand. Of course, her conversational
repertoire is inevitably limited by the available au-
thoring and development effort as well as language
processing challenges.
3 Data collection
The exponential number of possible utterances and
dialogue paths in even a simple conversational dia-
logue scenario such as the Amani scenario suggests
that learning acceptable dialogue behavior from sur-
face text examples without annotation or formal
modeling would require a seemingly insurmount-
able quantity of dialogues to serve as training data.
We address this problem in a data collection frame-
work with four main characteristics: (1) we sidestep
the problem of learning natural language generation
by using a fixed predefined set of utterances for the
Amani character. This so-called ?utterance selec-
tion? approach has been used in a number of dia-
logue systems (Zukerman and Marom, 2006; Sell-
berg and Jnsson, 2008; Kenny et al, 2007, for ex-
ample) and often serves as a reasonable approxima-
tion to generation (Gandhe and Traum, 2010); (2)
we collect dialogues from human participants who
40
play the parts of Amani and the commander in a
structured role play framework (Section 3.1); (3) we
enrich the dialogues collected in the structured role
play step with additional paraphrases for the utter-
ances of the commander, in an attempt to deal with
large variability of natural language input, even for
a limited domain conversational dialogue scenario
(Section 3.2); (4) we further augment the existing
dialogue data by adding acceptable alternatives to
the dialogue acts of the Amani role through the use
of external referees (Section 3.3).
Our data collection procedure is designed to cap-
ture the necessary information for learning dialogue
policies and evaluating their quality by approxi-
mating the exponentially large dialogue variability
while keeping the data collection effort tractable.
3.1 Structured role play
To examine the hypothesis that dialogue policies
such as Amani?s can be learned from examples with-
out explicit rules or any kind of formal modeling,
we collected dialogue data through a constrained
form of role play, which we call structured role play,
where the person playing the role of Amani is en-
couraged, whenever possible, to only use utterances
from a fixed set. Each utterance in the available set
of Amani replies corresponds roughly to one of the
dialogue acts (consisting of an illocutionary force
and some semantic content) described by Artstein et
al. (2009) for their version of the Amani character.
The players in the roles of Amani and the com-
mander take turns producing one utterance at a time,
each in a separate terminal. The commander player,
who receives a natural language description of the
scenario and the goal of the commander, enters utter-
ances through a teletype (chat) interface. The Amani
player, who receives a natural language descrip-
tion of the scenario and of Amani?s dialogue policy,
chooses an utterance from a list for each dialogue
turn. The Amani player is encouraged to use an ut-
terance from this list whenever possible; however,
for user utterances that the Amani player judges can-
not possibly be handled by any existing response, a
new response can be authored (as English text) and
immediately used in the role play. Each player sees
the other?s utterance as text in their own terminal.
This closely resembles a Wizard-of-Oz setup, with
they key difference being that both dialogue partic-
ipants believe they are interacting with another per-
son, which is in fact the case, and the idea of a wiz-
ard controlling a system is not part of the exercise.
However, because the Amani player is encouraged
to limit Amani?s responses to a fixed utterance set,
and the dialogue is constrained to a strict turn-taking
setup that interleaves utterances from each partici-
pant, the situation also differs from conventional role
play.
We collected a total of 19 dialogues and 296 ut-
terances for Amani, for an average of 15.6 Amani
utterances per dialogue.
3.2 Paraphrase generation
The dialogues collected through structured role play
are intended for serving as training data from which
Amani?s dialogue policy can be learned. However,
to cover the natural language variability with which
dialogue acts from the commander can be expressed
would require a much larger number of dialogues
than it would be practical to collect, since a learned
system that deals only with the surface text in the
dialogues would need to deal both with the dia-
logue policy and natural language understanding for
the scenario. Instead, we require only that the di-
alogues collected cover the desired dialogue acts
for the player role in the scenario. To address the
language understanding problem (indirectly), we at-
tempt to cover the variability of expression of these
dialogue acts through the collection of paraphrases
for the commander utterances in the set of dialogues.
For each commander utterance in the 19 dialogues
we collected, we had annotators create a set of para-
phrases. In creating paraphrases, annotators were
asked to take not just the original utterance into ac-
count, but also its context in the dialogue. We did
not specify a fixed number of paraphrases per utter-
ance, but instead asked for as many paraphrases as
the annotator could quickly think of.
Figure 1 exemplifies the paraphrases created dur-
ing this process, for a target user utterance of can
you tell me what you know of the incident?. For this
utterance, a total of 6 paraphrases were generated.
We used a total of 9 annotators, who created para-
phrases for the 296 utterances in the 19 dialogues.
Most annotators were responsible for no more than
two dialogues, and took on average less than 30
minutes per dialogue. The average number of para-
41
Previous Dialogue History
Lieutenant: hi amani
Amani: hello.
Lieutenant: how are you doing?
Amani: fine thank you.
Lieutenant: thank you for meeting with me
Amani: you?re welcome.
Target User Utterance
Lieutenant: can you tell me what you know of the incident?
Paraphrases:
please tell me what information you have about the incident
could you please tell me what you saw?
what can you tell me about the incident?
can you tell me about the incident?
please, tell me what you know about the incident
tell me what you saw, please
System Response
Amani: i saw all the shooting from my window. what do you want to know about it?
External Referees:
(3 referees) i saw all the shooting from my window. what do you want to know about it?
(2 referees) i remember that the gun fire was coming from the window on the second floor of assad?s
shop. the shop is only one story but there are apartments on top of the shop.
(1 referee) what is it you want to know about the incident?
Figure 1: An enriched dialogue turn from an Amani structured role play.
phrases collected per user utterance was 5.5.
Our 9 annotators had differing backgrounds, rang-
ing from transcribers and summer interns to experi-
enced NLP researchers. It should be noted that all
had at least some experience working with natural
language processing technologies. In future work,
we would like to explore using less experienced an-
notators for paraphrasing.
3.3 External referee annotation
Although the paraphrase generation step helps with
coverage of the language used by the commander
in our scenario, the combination of the original di-
alogues collected through structured role play and
the paraphrases do not address one crucial issue in
learning of data-driven dialogue policies, and their
automated evaluation: at each turn, a dialogue par-
ticipant has multiple valid dialogue acts that can be
performed, not a single correct one. In other words,
given the same dialogue history up to a given point,
multiple human dialogue participants following the
same underspecified policy may choose different di-
alogue acts to continue the dialogue, and each of
these different choices may be perfectly acceptable
and coherent. This is one of main challenges in cre-
ation and evaluation of data-driven policies, since
the exponentially many acceptable dialogue paths
are both difficult to model explicitly, and difficult
to recognize automatically when performed during
testing. Of course, the degree to which this is a prac-
tical problem in a specific dialogue scenario depends
on several factors, including how underspecified the
targeted dialogue policy is. In our case study, the
policy has a high level of underspecification, since
only behaviors related to the information in Table 1
are mentioned directly, and even those are only de-
scribed in natural language, without formal rigor.
The rest of the policy dictates only that human play-
ers in the part of Amani act according to their com-
monsense in playing the role of the Amani character.
However, we limit the otherwise potentially infinite
possibilities for dialogue behavior by strongly en-
couraging the Amani player to perform only one of a
set of predefined utterances corresponding to certain
dialogue acts in the scenario. In our experiments, the
number of utterances available for Amani was 96.
We first investigate this issue by attempting to
characterize the amount of human variation in the
choice of one of the 96 available dialogue acts at
any given point in a dialogue. To this end, we intro-
duce the idea of the external referee, who essentially
provides a ?second opinion? for dialogue acts per-
formed by the original role player. The external ref-
eree annotation task works as follows: (1) Starting
with an existing dialogue containing n utterances
42
?u1, u2, ..., un? for the participant whose utterances
will be externally refereed (one of the dialogues
collected through structured role play, in our case
study, where we externally referee the Amani utter-
ances), produce n dialogue histories h1, h2, ..., hn,
with each hi consisting of every utterance from each
dialogue participant from the beginning of the dia-
logue down to, but not including, the ith utterance in
the dialogue. (2) For each dialogue history hi, the
external referee (who must not be the person who
played a part in the original dialogue) chooses an
utterance u?i from the choices available for the sce-
nario, without knowledge of the original utterance
ui in the dialogue from which the history was pro-
duced.
Figure 1 provides an example of the choices made
by 6 external referees for a single target user ut-
terance. Given the previous dialogue history and
the target user utterance (can you tell me what you
know of the incident?), each external referee inde-
pendently chose a single best utterance for the char-
acter to respond with. In the example in the figure,
it can be seen that 3 of the 6 external referees chose
the same response as the original Amani player, as-
serting that Amani did indeed witness the incident
and asking what the commander would like to know.
The other three chose alternative responses; two
of these selected a response asserting information
about where the gun fire was coming from, while
a third referee chose a response simply asking what
the commander would like to know. It is important
to note that all three of these alternative responses
would be acceptable from a design and training per-
spective.
In this annotation task, the task is not to pro-
vide alternative dialogues, but simply one charac-
ter response to each individual utterance, assuming
the fixed history of the original dialogue. In other
words, the annotator has no control or impact over
the dialogue history at any point, and provides only
additional reference utterances for possible immedi-
ate continuations for each dialogue history. It is for
this reason we call the annotator an external referee.
Annotations from multiple external referees for
the dialogues collected through structured role play
do not result in a representation of the lattice of the
many possible dialogue paths in the scenario, but
rather an approximation that represents the possible
options in the immediate future of a given dialogue
history. The main difference is that the available his-
tories are limited to those in the original dialogues
from structured role play. While this may be a lim-
iting factor if one attempts to model dialogue be-
havior based on entire dialogue histories, since the
available histories represent only a very sparse sam-
ple of the space of valid histories, it is possible that
good approximate models can be achieved with fac-
torization of dialogues by sequences of a fixed num-
ber of consecutive turns, e.g. a model that makes a
second-order Markov assumption, considering only
the previous two turns in the dialogue as an approx-
imation of the entire history (Gandhe and Traum,
2007). This is in a way the same approximation used
in n-gram language models, but at the level of gran-
ularity of sentences, rather than words.
We collected annotations from 6 different exter-
nal referees, with each individual referee annotating
the entire set of 19 dialogues, and taking on average
about two hours to complete the annotation of the
entire set. All of our external referees were very fa-
miliar with the design of the Amani character, and
most had natural language processing expertise.
4 Evaluation of dialogue policies with
multiple external referees
4.1 External referee agreement
The dialogues and external referee annotations col-
lected using the procedure described in Section 3
provide a way to characterize the targeted policy
with respect to human variability in choosing utter-
ances from a fixed set, since the annotations include
the choices made by multiple external referees.
From the annotations of utterances chosen for
Amani in our 19 dialogues, we see that human an-
notators agree only 49.2% of the time when choos-
ing an utterance in the external referee framework.
That is, given the same dialogue history, we expect
that two human role players would agree on average
slightly less than 50% of the time on what the next
utterance should be1.
Based on this level of pairwise agreement, one
might conclude that using these data for either policy
learning or policy evaluation is a lost cause. How-
1This represents the averaged agreement over all pairs of
external referees.
43
010
20
30
1 2 3 4 5 6
U
tte
ra
n
ce
co
u
n
t(
%
)
Number of distinct utterance choices
Figure 2: Distribution in number of distinct choices by
external referees
ever, this result does not necessarily indicate that hu-
man raters disagree on what the correct choice is; it
is more likely to reflect that there are in fact mul-
tiple ?correct? (acceptable) choices, which we can
capture through multiple annotators.
The annotations from multiple external referees
in our case study support this view: Figure 2 shows
the number of distinct utterance choices made by
each of the six external referees for each specific ut-
terance in the 19 dialogues collected through struc-
tured role play. Each external referee chooses only
one utterance (out of 96 options) per Amani turn in
the 19 dialogues. Over the 296 Amani utterances
in the entire set of dialogues, all six referees agreed
unanimously on their utterance choice only 23.3%
of the time. The most frequent case, totaling almost
30% of all utterances, was that the set composed by
the single choice from each of the six wizards for
an utterance had exactly two distinct elements. For
only 1.3% of the 296 utterances did that set contain
the maximum number of distinct elements (six), in-
dicating complete disagreement among the external
referees. We note that, in this case, very low agree-
ment to complete disagreement reflects a situation
in dialogue where it is likely that there are many di-
alogue act choices considered acceptable by the col-
lective body of external referees. In our scenario,
there were at most two choices from the six referees
for more than 50% of the Amani turns, indicating
that in the majority of the cases there is only a small
set of acceptable dialogue acts (from the 296 avail-
able), while five or more options were chosen for
less than 10% of all Amani turns.
For a more direct characterization of dialogue sce-
narios, and also for the purposes of evaluation, we
40
50
60
70
80
90
1 2 3 4 5 6 7
W
ea
k
ag
re
em
en
t(
%
)
Number of external referees
bc
bc
bc
bc
bc
Figure 3: Weak agreement between external referees
now define a metric that reflects overall agreement
in a group of external referees. Instead of compar-
ing one choice from a single referee to another single
choice, we instead check for membership of a single
choice cij from a single referee Ri for utterance uj
in the set of choices {ckj |k 6= i} from all of the
other referees {Rk|k 6= i}. In the positive case, we
say that Ri weakly agrees with the rest of the raters
{Rk|k 6= i} on the annotation of utterance uj . We
define the weak agreement agrn for a set of N ex-
ternal referees over a set of m utterances to be rate
at which each rater Ri weakly agrees with the n? 1
raters {Rk|k 6= i}, for all integer values of i ranging
from 1 to N , inclusive. Intuitively, weak agreement
reflects two important questions: (1) how often is
the choice of a referee supported by the choice of
at least one more referee? and (2) given a set of
n ? 1 referees, how much new information (in the
form of unseen choices) should I expect to see from
a new nth referee? Figure 3 addresses these ques-
tions for the scenario in our case study by showing
the weak agreement figures obtained for sets of in-
creasing numbers of external referees, from 2 to 6.
Each point in the graph corresponds to the average
of the weak agreement values obtained for all possi-
ble ways of holding out one external referee Ri, and
computing the weak agreement between Ri and the
other referees, assuming an overall pool containing
the given number of external referees.
We note that with the dialogue act choices of a
single person, coverage of the possible acceptable
options is quite poor, corresponding only to an aver-
age of 50% of the choices made by another person.
44
The coverage increases rapidly as two more external
referees are added, and more slowly, although still
steadily from there. The rightmost point in Figure 3
indicates that with a set of five external referee we
should expect to cover almost 80% of the choices of
a sixth referee.
4.2 Dialogue policy evaluation with multiple
external referees
The weak agreement metric defined in the previ-
ous section can be used to measure the quality of
automatically learned policies, and to provide in-
sight into how a learned policy compares to human-
level performance. Because it recognizes the valid-
ity of multiple responses, the weak agreement metric
can help distinguish true policy errors from policy
choices that are consistent with the intuitions of at
least some human referees about what the character
should say.
In particular, given the choices made by five exter-
nal referees for our 19 Amani dialogues, we can ex-
pect their choices to cover about 80% of the choices
a sixth person would make for what Amani should
say at each turn in these dialogues. (I.e., we know
that the weak agreement among a group of six hu-
man referees is about 80% for this Amani scenario.)
We proceed to rate the quality of an automatic
policy by computing a one-vs-others version of
weak agreement?intuitively treating our policy as
if it were such a ?sixth person?, and comparing it
to the other five. Instead of computing the average
weak agreement for referees randomly selected from
an entire group, as in the previous section, to eval-
uate a policy, we compute its weak agreement com-
pared to the combined set of human external refer-
ees, as follows. For every system utterance uj in our
set of role play dialogues, a given automatic policy
P is used to select a response c?j (corresponding to
a dialogue act in the domain). We then check for
membership of c?j in the set that contains only and
all dialogue act choices ckj for k ranging from 1 to
N , inclusive, where N is the number of external ref-
erees and ckj corresponds to the kth referee?s choice
for the jth utterance. Another way to interpret this
evaluation metric is to consider it a form of accuracy
that computes the number of correct choices made
by the policy divided by the total number of choices
made by the policy, where a choice is considered
?correct? if it matches any of the external referees?
choices for a specific utterance. For this reason, we
refer to this evaluation-focused one-vs-all version of
weak agreement as weak accuracy.
Based on the definition above, an automatic pol-
icy with quality indistinguishable from that of a
person choosing utterances for the Amani character
would have a weak accuracy of about 80% or higher
when measured using a set of five external referees.
We see then that this metric is far from perfect, since
it cannot rank two policies with weak accuracy lev-
els of, say, 80% and 90%. It is also possible for a
policy that results in dialogue behavior noticeably
inferior to that of a human referee to be rated at
the same weak accuracy value for a human referee
(80%). In practice, however, weak accuracy with
five or six external referees has far greater power for
discriminating between policies of varying quality,
and ranking them correctly, than a naive version of
accuracy, which corresponds to weak accuracy us-
ing a single referee. Furthermore, the addition of
only a few more external referees would very likely
increase the efficacy of the weak agreement metric.
Despite the shortcomings of weak accuracy as a
metric for evaluation of quality of dialogue poli-
cies, it opens up a wide range of opportunities for
development of learned policies. Without an auto-
mated metric, development of such techniques can
be only vaguely incremental, relying on either costly
or, more likely, infrequent human evaluations with
results that are difficult to optimize toward with cur-
rent machine learning techniques. The use of im-
perfect automated metrics in situations where ideal
metrics are unavailable or are impractical to deploy
is fairly common in natural language processing.
PARSEVAL (Abney et al, 1991), commonly used
for parser evaluation, and BLEU (Papineni et al,
2002), commonly used in machine translation, are
two examples of well-known imperfect metrics that
have been the subject of much criticism, but that are
widely agreed to have been necessary for much of
the progress enjoyed by their respective fields. Un-
like BLEU, however, which has been shown to cor-
relate with certain types of human judgment on the
quality of machine translation systems, our notion
of weak accuracy has not yet been demonstrated to
correlate with human judgments on the quality of di-
alogue policies, and as such it is only hypothesized
45
to have this property. We leave this important step
of validation as future work.
5 Learning dialogue policies from
examples without formal modeling
Equipped with a dataset with 19 dialogues in the
Amani scenario (including paraphrases for the un-
constrained commander utterances, and external ref-
eree annotations for the constrained Amani utter-
ances), and an automatic evaluation framework for
distinguishing quality differences in learned poli-
cies, we now describe our experiments on learning
dialogue policies from data collected in structured
role play sessions, and enriched with paraphrases
and external referee annotations.
In each of our experiments we attempt to learn
a dialogue policy as a maximum entropy classi-
fier (Berger et al, 1996) that chooses one utterance
out of the 96 possible utterances for Amani after
each commander utterance, given features extracted
from the dialogue history. This policy could be in-
tegrated in a dialogue system very easily, since it
chooses system utterances directly given previous
user and system utterances. We evaluate the dia-
logue policies learned in each experiment through
19-fold cross-validation of our set of 19 dialogues:
in each fold, we hold out one dialogue (and all of its
related information, such as external referee anno-
tations and user utterance paraphrases) and use the
remaining 18 dialogues as training data.
5.1 Learning from examples
Using only the dialogues collected in structured role
play sessions, and no additional information from
external referees or paraphrases, we train the maxi-
mum entropy classifier to choose a system utterance
si based on features extracted from the two previous
user utterances ui and ui?1 and the previous system
utterance si?1. The features extracted from these ut-
terances are the words present in each user utterance,
and the complete text of each system utterance. Low
frequency words occurring fewer than 5 times in the
corpus are excluded.
The weak accuracy for this simple policy is 43%,
a low value that indicates that for more than half its
turns the policy chooses an utterance that was not
chosen by any of the referees, giving us a reasonable
level of confidence that this policy is of poor quality.
5.2 Enhanced training with external referees
The next experiment expands the training set avail-
able to the maximum entropy classifier by adding
training instances based on the utterances chosen by
the external referees. For each of the training in-
stances (target utterance coupled with features from
ui, si?1 and ui?1) we add six new training instances,
each using the same features as the original train-
ing instance, but replacing the target class with the
choice made by an external referee. Note that this
creates identical training instances for cases when
the same utterance is chosen by multiple annotators,
which has the effect of weighting training examples.
With the additional information, weak accuracy for
this policy improves to 56%, which is a large gain
that still results in a mediocre dialogue policy.
5.3 Expanding training examples with
paraphrases
To help determine how much of difficulty in our
policy learning task is due to the related problem
of natural language understanding (NLU), and how
much is due to modeling dialogue behavior regard-
less of NLU, we performed manual annotation of
dialogue acts for the user utterances, and trained a
policy as in the previous section, but using manu-
ally assigned dialogue acts instead of the words for
user utterances in the dialogue history. With this
gold-standard NLU, weak accuracy improves from
56% to 67%, approaching the level of human perfor-
mance, and already at a level where two out of every
three choices made by the learned policy matches
the choice of a human referee.
To bridge the gap between learning purely from
surface text (with no formal modeling) and learn-
ing from manually assigned dialogue acts specifi-
cally designed to capture important information in
the scenario, we turn to the paraphrases collected
for user utterances in our 19 dialogues. These para-
phrases are used to create additional synthetic train-
ing material for the classifier, as follows: for each
training instance produced from a chosen system ut-
terance si and previous utterances ui, si?1 and ui?1
(see previous section), we create additional training
instances keeping the target system utterance si and
previous system utterance si?1 the same, but using
46
a paraphrase u?i in the place of ui, and a paraphrase
u?i?1 in the place of ui?1. Training instances are
added for all possible combinations of the available
paraphrases for ui and ui?1, providing some (arti-
ficial) coverage for parts of the space of possible
dialogue paths that would be otherwise completely
ignored during training.
Training the classifier with material from the ex-
ternal referees (see previous section) and additional
synthetic training examples from paraphrases as de-
scribed above produces a dialogue policy with weak
accuracy of 66%, at the same level as the policy
learned with manually assigned speech acts. It is
noteworthy that this was achieved through a very
simple and intuitive paraphrase annotation task that
requires no technical knowledge about dialogue sys-
tems, dialogue acts or domain modeling. As men-
tioned in section 3.2, paraphrases for each of the 19
dialogues were generated in less than 30 minutes on
average.
6 Conclusion and future work
We introduced a framework for collection and en-
richment of scenario-specific dialogues based only
on tasks that require no technical knowledge. Data
collected in this framework support novel ap-
proaches not just for learning dialogue policies,
but perhaps more importantly for evaluating learned
policies, which allows us to examine different tech-
niques using an objective automatic metric.
Although research on both learning and evalu-
ating dialogue policies is still in early stages, this
case study and proof-of-concept experiments serve
to illustrate the basic ideas of external referee and
paraphrase annotation, and the use of multiple refer-
ence dialogue act choices in evaluation of dialogue
policies, in a way similar to how multiple reference
translations are used in evaluation of machine trans-
lation systems. We do not consider this line of re-
search a replacement for or an alternative to for-
mal modeling of domains and dialogue behavior,
but rather as an additional tool in the community?s
collective arsenal. There are many unexplored av-
enues for including data-driven techniques within
rule-based frameworks and vice-versa.
In future work we intend to further validate the
ideas presented in this paper by performing addi-
tional collection of dialogues in the Amani domain
to serve as a virgin test set, and applying these
techniques to other dialogue domains and scenar-
ios. We also plan to refine the weak accuracy and
weak agreement metrics to take into account the
level of agreement within utterances to reflect that
some parts of dialogues may be more open-ended
than others. Finally, we will conduct human evalu-
ations of different policies to begin validating weak
accuracy as an automatic metric for evaluation of di-
alogue policies.
Acknowledgments
The project or effort described here has been spon-
sored by the U.S. Army Research, Development,
and Engineering Command (RDECOM). State-
ments and opinions expressed do not necessarily re-
flect the position or the policy of the United States
Government, and no official endorsement should be
inferred. We would also like to thank Ron Artstein,
Sudeep Gandhe, Fabrizio Morbini, Angela Nazar-
ian, Susan Robinson, Michael Rushforth, and David
Traum.
References
S. Abney, S. Flickenger, C. Gdaniec, C. Grishman,
P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-
vans, M. Liberman, M. Marcus, S. Roukos, B. San-
torini, and T. Strzalkowski. 1991. Procedure for quan-
titatively comparing the syntactic coverage of english
grammars. In E. Black, editor, Proceedings of the
workshop on Speech and Natural Language, HLT ?91,
pages 306?311, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ron Artstein, Sudeep Gandhe, Michael Rushforth, and
David R. Traum. 2009. Viability of a simple dialogue
act scheme for a tactical questioning dialogue system.
In DiaHolmia 2009: Proceedings of the 13th Work-
shop on the Semantics and Pragmatics of Dialogue,
page 43?50, Stockholm, Sweden, June.
Adam L. Berger, Stephen D. Della Pietra, and Vincent
J. D. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computational
Linguistics, 22(1):39?71.
Sudeep Gandhe and David R. Traum. 2007. Creating
spoken dialogue characters from corpora without an-
notations. In Proceedings of Interspeech-07, 08/2007.
Sudeep Gandhe and David R. Traum. 2010. I?ve said it
before, and i?ll say it again: An empirical investigation
47
of the upper bound of the selection approach to dia-
logue. In 11th annual SIGdial Meeting on Discourse
and Dialogue.
Sudeep Gandhe, Nicolle Whitman, David R. Traum, and
Ron Artstein. 2009. An integrated authoring tool for
tactical questioning dialogue systems. In 6th Work-
shop on Knowledge and Reasoning in Practical Dia-
logue Systems, Pasadena, California, July.
Sina Jafarpour, Chris Burges, and Alan Ritter. 2009. Fil-
ter, rank, and transfer the knowledge: Learning to chat.
In Proceedings of the NIPS Workshop on Advances in
Ranking.
Patrick Kenny, Thomas D. Parsons, Jonathan Gratch, An-
ton Leuski, and Albert A. Rizzo. 2007. Virtual pa-
tients for clinical therapist skills training. In Proceed-
ings of the 7th international conference on Intelligent
Virtual Agents, IVA ?07, pages 197?210, Berlin, Hei-
delberg. Springer-Verlag.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of twitter conversations. In Pro-
ceedings of HLT-NAACL.
Linus Sellberg and Arne Jnsson. 2008. Using ran-
dom indexing to improve singular value decomposi-
tion for latent semantic analysis. In Proceedings of the
Sixth International Language Resources and Evalua-
tion (LREC?08), Marrakech, Morocco, may.
David R. Traum, Anton Leuski, Antonio Roque, Sudeep
Gandhe, David DeVault, Jillian Gerten, Susan Robin-
son, and Bilyana Martinovski. 2008. Natural lan-
guage dialogue architectures for tactical questioning
characters. In Army Science Conference, Florida,
12/2008.
Ingrid Zukerman and Yuval Marom. 2006. A corpus-
based approach to help-desk response generation.
Computational Intelligence for Modelling, Control
and Automation, International Conference on, 1:23.
48
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 84?93,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Reinforcement Learning of Question-Answering Dialogue Policies
for Virtual Museum Guides
Teruhisa Misu1?, Kallirroi Georgila2, Anton Leuski2, David Traum2
1National Institute of Information and Communications Technology (NICT), Kyoto, Japan
2USC Institute for Creative Technologies, Playa Vista, CA, USA
teruhisa.misu@nict.go.jp, {kgeorgila,leuski,traum}@ict.usc.edu
Abstract
We use Reinforcement Learning (RL) to learn
question-answering dialogue policies for a
real-world application. We analyze a corpus
of interactions of museum visitors with two
virtual characters that serve as guides at the
Museum of Science in Boston, in order to
build a realistic model of user behavior when
interacting with these characters. A simulated
user is built based on this model and used
for learning the dialogue policy of the virtual
characters using RL. Our learned policy out-
performs two baselines (including the original
dialogue policy that was used for collecting
the corpus) in a simulation setting.
1 Introduction
In the last 10 years Reinforcement Learning (RL)
has attracted much attention in the dialogue commu-
nity, to the extent that we can now consider RL as the
state-of-the-art in statistical dialogue management.
RL is used in the framework of Markov Decision
Processes (MDPs) or Partially Observable Markov
Decision Processes (POMDPs). In this paradigm
dialogue moves transition between dialogue states
and rewards are given at the end of a successful dia-
logue. The goal of RL is to learn a dialogue policy,
i.e. the optimal action that the system should take at
each possible dialogue state. Typically rewards de-
pend on the domain and can include factors such as
task completion, dialogue length, and user satisfac-
tion. Traditional RL algorithms require on the order
? This work was done when the first author was a visiting
researcher at USC/ICT.
of thousands of dialogues to achieve good perfor-
mance. Because it is very difficult to collect such a
large number of dialogues with real users, instead,
simulated users (SUs), i.e. models that simulate the
behavior of real users, are employed (Georgila et al,
2006). Through the interaction between the system
and the SUs thousands of dialogues can be gener-
ated and used for learning. A good SU should be
able to replicate the behavior of a real user in the
same dialogue context (Ai and Litman, 2008).
Most research in RL for dialogue management
has been done in the framework of slot-filling appli-
cations (Georgila et al, 2010; Thomson and Young,
2010), largely ignoring other types of dialogue. In
this paper we focus on the problem of learning di-
alogue policies for question-answering characters.
With question-answering systems (or characters),
the natural language understanding task is to retrieve
the best response to a user initiative, and the main
dialogue policy decision is whether to provide this
best response or some other kind of move (e.g. a re-
quest for repair, clarification, or topic change), when
the best answer does not seem to be good enough.
Note that often in the literature the term question-
answering is used for slot-filling dialogue systems
as well, in the sense that the user asks some ques-
tions, for example, about restaurants in a particular
area, and the system answers by providing a list of
options, for example, restaurants. We use the term
?question-answering? for systems where user ques-
tions can be independent of one another (follow-
up questions are possible though) and do not have
the objective of reducing the search space and re-
trieving results from a database of e.g. restaurants,
flights, etc. Thus examples of question-answering
84
characters can be virtual interviewees (that can an-
swer questions, e.g. about an incident), virtual scien-
tists (that can answer general science-related ques-
tions), and so forth.
For our experiments we use a corpus (Aggarwal
et al, 2012) of interactions of real users with two
virtual characters, the Twins, that serve as guides at
the Museum of Science in Boston (Swartout et al,
2010). The role of these virtual characters is to en-
tertain and educate the museum visitors. They can
answer queries about themselves and their technol-
ogy, generally about science, as well as questions
related to the exhibits of the museum. An example
interaction between a museum visitor and the Twins
is shown in Figure 1. The dialogue policy of the
Twins was arbitrarily hand-crafted (see section 7 for
details) and many other policies are possible (includ-
ing Baseline 2, presented in section 7, and taking
more advantage of question topics and context). We
propose to use RL for optimizing the system?s re-
sponse generation. This is a real-world application
for which RL appears to be an appropriate method.
Although there are similarities between question-
answering and slot-filling dialogues there are also a
number of differences, such as the reward function
and the behavior of the users. As discussed later in
detail, in question-answering the users have a num-
ber of questions that they are planning to ask (stock
of queries), which can be increased or decreased de-
pending not only on whether they received the in-
formation that they wanted but also on how satisfied
they are with the interaction. The system has to plan
ahead in order to maximize the number of success-
ful responses that it provides to user queries. At the
same time it needs to avoid providing incorrect or
incoherent responses so that the user does not give
up the interaction.
One of the challenges of our task is to define an
appropriate reward function. Unlike slot-filling dia-
logues, it is not clear what makes an interaction with
a question-answering system successful. A second
challenge is that in a museum setting it is not clear
what constitutes a dialogue session. Often two or
more users alternate in asking questions, which fur-
ther complicates the problem of defining a good re-
ward function. A third challenge is that the domain
is not well defined, i.e. users do not know in advance
what the system is capable of (what kind of ques-
tions the characters can answer). Moreover, there
User: What are your names? (ASR: what are
your names)
Ada: My name?s Ada.
Grace: And I?m Grace. We?re your Virtual Mu-
seum Guides. With your help, we can suggest ex-
hibits that will get you thinking! Or answer ques-
tions about things you may have seen here.
Ada: What do you want to learn about?
User: Artificial intelligence. (ASR: is
artificial intelligence)
Grace: One example of AI, or Artificial Intelli-
gence, is 20Q, an online computer activity here at
Computer Place that asks you questions to guess
what you?re thinking.
Ada: I wish we?d been programmed to do that.
Nah. . . on second thought, I prefer just answering
your questions.
Grace: That takes AI too.
Figure 1: Example dialogue between the Twins virtual
characters and a museum visitor.
are many cases of ?junk? user questions (e.g. ?are
you stupid??) or even user prompts in languages
other than English (e.g. ?hola?).
We first analyze our corpus in order to build a re-
alistic model of user behavior when interacting with
the virtual characters. A SU is built based on this
model and used for learning the dialogue policy of
the virtual characters using RL. Then we compare
our learned policy with two baselines, one of which
is the dialogue policy of the original system that was
used for collecting our corpus and that is currently
installed at the Museum of Science in Boston. Our
learned policy outperforms both baselines in a sim-
ulation setting.
To our knowledge this is the first study that uses
RL for learning this type of question-answering dia-
logue policy. Furthermore, unlike most studies that
use data collected by having paid subjects interact
with the system, we use data collected from real
users, in our case museum visitors.1 We also com-
pare our learned dialogue policy with the dialogue
policy of the original system that is currently in-
stalled at the Museum of Science in Boston.
The structure of the paper is as follows. In sec-
1Note that the CMU ?Let?s Go!? corpus is another case of
using real user data for learning dialogue policies for the Spoken
Dialogue Challenge.
85
tion 2 we present related work. Section 3 provides a
brief introduction to RL and section 4 describes our
corpus. Then in section 5 we explain how we built
our SU from the corpus, and in section 6 we describe
our learning methodology. Section 7 presents our
evaluation results. Finally section 8 presents some
discussion and ideas for future work together with
our conclusion.
2 Related Work
To date, RL has mainly been used for learning di-
alogue policies for slot-filling applications such as
restaurant recommendations (Jurc???c?ek et al, 2012),
sightseeing recommendations (Misu et al, 2010),
appointment scheduling (Georgila et al, 2010), etc.,
largely ignoring other types of dialogue. Recently
there have been some experiments on applying RL
to the more difficult problem of learning negotia-
tion policies (Heeman, 2009; Georgila and Traum,
2011a; Georgila and Traum, 2011b). Also, RL has
been applied to tutoring domains (Tetreault and Lit-
man, 2008; Chi et al, 2011).
There has been a lot of work on developing
question-answering systems with dialogue capabil-
ities, e.g. (Jo?nsson et al, 2004; op den Akker et al,
2005; Varges et al, 2009). Most of these systems are
designed for information extraction from structured
or unstructured databases in closed or open domains.
One could think of them as adding dialogue capa-
bilities to standard question-answering systems such
as the ones used in the TREC question-answering
track (Voorhees, 2001). Other work has focused on
a different type of question-answering dialogue, i.e.
question-answering dialogues that follow the form
of an interview and that can be used, for example,
for training purposes (Leuski et al, 2006; Gandhe et
al., 2009). But none of these systems uses RL.
To our knowledge no one has used RL for learning
policies for question-answering systems as defined
in section 1. Note that Rieser and Lemon (2009)
used RL for question-answering, but in their case,
question-answering refers to asking for information
about songs and artists in an mp3 database, which
is very much like a slot-filling task, i.e. the system
has to fill a number of slots (e.g. name of band, etc.)
in order to query a database of songs and present
the right information to the user. As discussed in
section 1 our task is rather different.
3 Reinforcement Learning
A dialogue policy is a function from contexts to
(possibly probabilistic) decisions that the dialogue
system will make in those contexts. Reinforcement
Learning (RL) is a machine learning technique used
to learn the policy of the system. For an RL-based
dialogue system the objective is to maximize the re-
ward it gets during an interaction. RL is used in the
framework of Markov Decision Processes (MDPs)
or Partially Observable Markov Decision Processes
(POMDPs).
In this paper we follow a POMDP-based ap-
proach. A POMDP is defined as a tuple (S, A, P , R,
O, Z, ?, b0) where S is the set of states (representing
different contexts) which the system may be in (the
system?s world),A is the set of actions of the system,
P : S ? A ? P (S, A) is the set of transition prob-
abilities between states after taking an action, R : S
? A ?< is the reward function, O is a set of obser-
vations that the system can receive about the world,
Z is a set of observation probabilities Z : S ? A
? Z(S, A), and ? a discount factor weighting long-
term rewards. At any given time step i the world
is in some unobserved state si ? S. Because si is
not known exactly, we keep a distribution over states
called a belief state b, thus b(si) is the probability of
being in state si, with initial belief state b0. When
the system performs an action ?i ? A based on b,
following a policy pi : S ? A, it receives a reward
ri(si, ?i) ? < and transitions to state si+1 accord-
ing to P (si+1|si, ?i) ? P . The system then receives
an observation oi+1 according to P (oi+1|si+1, ?i).
The quality of the policy pi followed by the agent is
measured by the expected future reward also called
Q-function, Qpi : S ? A ? <.
There are several algorithms for learning the opti-
mal dialogue policy and we use Natural Actor Critic
(NAC) (Peters and Schaal, 2008), which adopts a
natural policy gradient method for policy optimiza-
tion, also used by (Thomson and Young, 2010;
Jurc???c?ek et al, 2012). Policy gradient methods do
not directly update the value of state S orQ-function
(expected future reward). Instead, the policy pi (or
parameter?, see below) is directly updated so as to
increase the reward of dialogue episodes generated
by the previous policy.
A system action asys is sampled based on the fol-
lowing soft-max (Boltzmann) policy:
86
pi(asys = k|?) = Pr(asys = k|?,?)
= exp(
?I
i=1 ?i ? ?ki)?J
j=1 exp(
?I
i=1 ?i ? ?ji)
Here, ? = (?1, ?2, . . . , ?I) is a basis func-
tion, which is a vector function of the belief state.
? = (?11, ?12, . . . ?1I , . . . , ?JI ) consists of J (# ac-
tions) ? I (# features) parameters. The parameter
?ji works as a weight for the i-th feature of the ac-
tion j and determines the likelihood that the action j
is selected. ? is the target of optimization by RL.
During training, RL algorithms require thousands
of interactions between the system and the user
to achieve good performance. For this reason we
need to build a simulated user (SU) (Georgila et al,
2006), that will behave similarly to a real user, and
will interact with the policy for thousands of itera-
tions to generate data in order to explore the search
space and thus facilitate learning.
Topic Example user question/prompt
introduction Hello.
personal Who are you named after?
school Where do you go to school?
technology What is artificial intelligence?
interfaces What is a virtual human?
exhibition What can I do at Robot Park?
Table 1: Topics of user questions/prompts.
4 The Twins Corpus
As mentioned in section 1 the Twins corpus (Aggar-
wal et al, 2012) was collected at the Museum of Sci-
ence in Boston (Swartout et al, 2010). The Twins
can answer a number of user questions/prompts in
several topics, i.e. about themselves and their tech-
nology, about science in general, and about exhibits
in the museum. We have divided these topics in six
categories shown in Table 1 together with an exam-
ple for each category.
An example interaction between a museum vis-
itor and the Twins is shown in Figure 1. We can
also see the output of the speech recognizer. In the
part of the corpus that we use for our experiment
automatic speech recognition (ASR) was performed
by Otosense, an ASR engine developed by the USC
SAIL lab. Natural language understanding and di-
alogue management are both performed as a single
task by the NPCEditor (Leuski and Traum, 2010),
a text classification system that classifies the user?s
query to a system?s answer using cross-language in-
formation retrieval techniques. When the system
fails to understand the user?s query it can prompt her
to do one of the following:
? rephrase her query (from now on referred to
as off-topic response 1, OT1), e.g. ?please
rephrase your question?;
? prompt the user to ask a particular question that
the system knows that it can handle (from now
on referred to as off-topic response 2, OT2),
e.g. ?you may ask us about our hobbies?;
? cease the dialogue and check out the ?behind
the scenes? exhibit which explains how the vir-
tual characters work (from now on referred to
as off-topic response 3, OT3).
The Twins corpus contains about 200,000 spoken
utterances from museum visitors (primarily chil-
dren) and members of staff or volunteers. For the
purposes of this paper we used 1,178 dialogue ses-
sions (11,074 pairs of user and system utterances)
collected during March to May 2011. This subset
of the corpus contains manual transcriptions of user
queries, system responses, and correct responses to
user queries (the responses that the system should
give when ASR is perfect).
5 User Simulation Model
In order to build a model of user behavior we per-
form an analysis of the corpus. One of our chal-
lenges is that the boundaries between dialogue ses-
sions are hard to define, i.e. it is very hard to auto-
matically calculate whether the same or a new user
speaks to the system, unless complex voice iden-
tification techniques are employed. We make the
reasonable assumption that a new dialogue session
starts when there are no questions to the system for
a time interval greater than 120 sec.
From each session we extract 30 features. A full
list is shown in Table 7 in the Appendix. Our goal
is to measure the contribution of each feature to
the user?s decision with respect to two issues: (1)
whether the user will cease the dialogue or not, and
(2) what kind of query the user will make next, based
87
on what has happened in the dialogue so far. To do
that we use the Chi-squared test, which is commonly
used for feature selection.
So to measure the contribution of each feature to
whether the user will cease the dialogue or not, we
give a binary label to each user query in our corpus,
i.e. 1 when the query is the last user query in the di-
alogue session and 0 otherwise. Then we calculate
the contribution of each feature for estimating this
label. In Table 8, column 1, in the Appendix, we can
see the 10 features that contribute the most to pre-
dicting whether the user will cease the dialogue. As
we can see the dominant features are not whether
the system correctly responded to the user?s query,
but mostly features based on the dialogue history
(e.g. the number of the system?s off-topic responses
so far) and user type information. Indeed, a further
analysis of the corpus showed that children tend to
have longer dialogue sessions than adults.
Our next step is the estimation of the contribution
of each feature for predicting the user?s next query.
The label we predict here is the topic of the user?s
utterance (personal, exhibition, etc., see Table 1).
We can see the 10 most predictive features in Ta-
ble 8, column 2, in the Appendix. The contribution
of the most recent user?s utterance (previous topic
category) is larger than that of dialogue history fea-
tures. This tendency is the same when we ignore re-
peated user queries, e.g. when the system makes an
error and the user rephrases her query (see Table 8,
column 3, in the Appendix). The user type is impor-
tant for predicting the next user query. In Figure 2
we can see the percentages of user queries per user
type and topic.
Based on the above analysis we build a simulated
user (SU). The SU simulates the following:
? User type (child, male, female): a child user
is sampled with a probability of 51.1%, a male
with 31.1%, and a female with 17.8%. These
probabilities are estimated from the corpus.
? Number of questions the user is planning to
ask (stock of queries): We assume here that
the user is planning to ask a number of ques-
tions. This number may increase or decrease.
For example, it can increase when the system
prompts the user to ask about a particular topic
(OT2 prompt), and it may decrease when the
user decides to cease the dialogue immediately.
Figure 2: Percentages of user queries per user type and
topic.
The number of questions is sampled from a
user type dependent Zipf distribution (strictly
speaking the continuous version of the distri-
bution; Parato distribution) the parameter of
which is estimated from the corpus using the
maximum likelihood criterion. We chose Zipf
because it is a long-tail distribution that fits our
data (users are not expected to ask a large num-
ber of questions). According to this distribution
a child user is more likely to have a larger stock
of queries than a male or female adult.
? User?s reaction: The user has to decide on
one of the following. Go to the next topic
(Go-on); cease the dialogue if there are no
more questions in the stock of queries (Out-of-
stock); rephrase the previous query (Rephrase);
abandon the dialogue (Give-up) regardless of
the remaining questions in the stock; gener-
ate a query based on a system recommenda-
tion, OT2 prompt (Refill). We calculate the
user type dependent probability for these ac-
tions from the corpus. But the problem here
is that it is not possible to distinguish be-
tween the case in which the user asked all the
questions in the stock of queries (i.e. all the
questions she intended to ask) and left, from
the case in which she gave up and abandoned
the dialogue. We estimate the percentage of
?Give-up? as the difference between the ratio of
?Cease? after an incorrect response and the ra-
88
tio of ?Cease? after a correct response, assum-
ing a similar percentage of ?Out-of-stock? for
both correct and incorrect responses. Likewise,
the difference in ?Go-on? for OT2 and other re-
sponses is attributed to ?Refill?. The probabil-
ity of ?Rephrase? is estimated from the corpus.
For example the probability that a child will
rephrase after an OT1 system prompt is 54%,
after an erroneous system prompt 38%, etc.
? Topic for next user query (e.g. introduction,
personal, etc.): The SU selects a new topic
based on user type dependent topic transition
bigram probabilities estimated from the corpus.
? User utterance: The SU selects a user utter-
ance from the corpus that matches the current
user type and topic. We have split the corpus
in groups of user utterances based on user type
and topic and we sample accordingly.
? Utterance timing: We simulate utterance tim-
ing (duration of pause between system utter-
ance and next user query) per user type and
user change. The utterance timing is sampled
based on a Gaussian distribution the parameters
of which are set based on the corpus statistics.
For example, the average duration of a session
until the user changes is 62.7 sec with a stan-
dard deviation of 71.2 sec.
6 Learning Question-Answering Policies
Our goal is to use RL in order to optimize the sys-
tem?s response generation. As we saw in the previ-
ous section the SU generates a user utterance from
our corpus. We do not currently use ASR error sim-
ulation but instead a real ASR engine. So the au-
dio file that corresponds to the selected user utter-
ance is forwarded to 3 ASR systems, with child,
male, and female acoustic models (AMs) respec-
tively. Then these recognition results are forwarded
to the NPCEditor that produces an N-best list of pos-
sible system responses (retrieval results). That is,
as mentioned in section 4, the NPCEditor classifies
each ASR result to a system answer using cross-
language information retrieval techniques. The pol-
icy can choose one of the NPCEditor retrieval re-
sults or reject them and instead present one of the
three off-topic prompts (OT1, OT2, or OT3). So the
system has 10 possible actions to choose between:
? use the response with the best or the second
best score retrieved from the NPCEditor based
on a child AM (2 actions);
? use the response with the best or the second
best score retrieved from the NPCEditor based
on a male AM (2 actions);
? use the response with the best or the second
best score retrieved from the NPCEditor based
on a female AM (2 actions);
? use the response with the best of the 6 afore-
mentioned scores of the NPCEditor;
? use off-topic prompt OT1;
? use off-topic prompt OT2;
? use off-topic prompt OT3.
We use the following features to optimize our di-
alogue policy (see section 3). We use the 6 retrieval
scores of the NPCEditor (the 2 best scores for each
user type ASR result), the previous system action,
the ASR confidence scores, the voting scores (calcu-
lated by adding the scores of the results that agree),
the system?s belief on the user type and user change,
and the system?s belief on the user?s previous topic.
So we need to learn a POMDP-based policy using
these 42 features.
Unlike slot-filling dialogues, defining the reward
function is not a simple task (e.g. reward the system
for filled and confirmed slots). So in order to define
the reward function and thus measure the quality of
the dialogue we set up a questionnaire. We asked
5 people to rate 10 dialogues in a 5-Likert scale.
Each dialogue session included 5 question-answer
pairs. Then we used regression analysis to set the
reward for each of the question-answer pair cate-
gories shown in Table 2. So for example, responding
correctly to an in-domain user question is rewarded
(+23.2) whereas providing an erroneous response to
a junk question, i.e. treating junk questions as if they
were in-domain questions, is penalized (-14.7).
One limitation of this reward function (Reward
function 1) is that it does not take into account
whether the user has previously experienced an off-
topic system prompt. To account for that we define
Reward function 2. Here we consider the number
of off-topic responses in the two most recent system
prompts. Reward function 2 is shown in Table 3.
89
QA Pair Reward
in-domain ? correct 23.2
in-domain ? error -12.2
in-domain ? OT1 -5.4
in-domain ? OT2 -8.4
in-domain ? OT3 -9.6
junk question ? error -14.7
junk question ? OT1 4.8
junk question ? OT2 10.2
junk question ? OT3 6.1
give up -16.9
Table 2: Reward function 1.
QA Pair Reward
in-domain ? correct 16.9
in-domain ? error -2.0
in-domain ? OT1 13.9
in-domain ? OT1(2) 7.3
in-domain ? OT2 -7.9
in-domain ? OT2(2) 4.2
in-domain ? OT3 -15.8
in-domain ? OT3(2) -8.3
junk question ? error -4.6
junk question ? OT1 4.1
junk question ? OT1(2) 4.1
junk question ? OT2 43.4
junk question ? OT2(2) -33.1
junk question ? OT3 3.1
junk question ? OT3(2) 6.1
give up -19.5
Table 3: Reward function 2.
As we can see, providing an OT2 as the first off-
topic response is a poor action (-7.9); it is preferable
to ask the user to rephrase her question (OT1) as a
first attempt to recover from the error (+13.9). On
the other hand, providing an OT2 prompt, after an
off-topic prompt has occured in the previous system
prompt, is a reasonable action (+4.2).
7 Evaluation
We compare our learned policy with two baselines.
The first baseline, Baseline 1, is the dialogue pol-
icy that is used by our system that is currently in-
stalled at the Museum of Science in Boston. Base-
line 1 selects the best ASR result (i.e. the result
with the highest confidence score) out of the results
with the 3 different AMs (child, male, and female),
and forwards this result to the NPCEditor to retrieve
the system?s response. If the NPCEditor score is
higher than an emprically set pre-defined threshold
(see (Leuski and Traum, 2010) for details), then the
system presents the retrieved response, otherwise it
presents an off-topic prompt. The system presents
these off-topic prompts in a fixed order. First, OT1,
then OT2, and then OT3.
We also have Baseline 2, which forwards all 3
ASR results to the NPCEditor (using child, male,
and female AMs). Then the NPCEditor retrieves 3
results, one for each one of the 3 ASR results, and
selects the retrieved result with the highest score.
Again if this score is higher than a threshold, the sys-
tem will present this result, otherwise it will present
an off-topic prompt.
Each policy interacts with the SU for 10,000 di-
alogue sessions and we calculate the average accu-
mulated reward for each dialogue. In Tables 4 and 5
we can see our results for Reward functions 1 and 2
respectively. In both cases the learned policy outper-
forms both baselines. For both reward functions the
most predictive feature is the ASR confidence score
when combined with the NPCEditor?s retrieval score
and the previous system action. Also, for both re-
ward functions the second best feature is ?voting?
when combined with the retrieval score and the pre-
vious system action.
In Table 6 we can see how often the learned pol-
icy, which is based on Reward function 1 using all
features, selects each one of the 10 system actions
(200,000 system turns in total).
Policy Avg Reward
Baseline 1 24.76 (19.29)
Baseline 2 51.63 (49.84)
Learned Policy - Features
Retrieval score
+ system action (*) 46.74
(*) + ASR confidence score 61.59
(*) + User type probability 47.28
(*) + Estimated previous topic 47.87
(*) + Voting 59.94
All features 60.93
Table 4: Results with reward function 1. The values in
parentheses for Baselines 1 and 2 are the rewards when
the NPCEditor does not use the pre-defined threshold.
90
Policy Avg Reward
Baseline 1 39.40 (38.51)
Baseline 2 55.45 (54.49)
Learned Policy - Features
Retrieval score
+ system action (*) 49.15
(*) + ASR confidence score 69.51
(*) + User type probability 50.15
(*) + Estimated previous topic 49.84
(*) + Voting 69.06
All features 73.59
Table 5: Results with reward function 2. The values in
parentheses for Baselines 1 and 2 are the rewards when
the NPCEditor does not use the pre-defined threshold.
System Action Frequency
Child + 1st best score 10.33%
Child + 2nd best score 2.70%
Male + 1st best score 13.72%
Male + 2nd best score 1.03%
Female + 1st best score 39.73%
Female + 2nd best score 0.79%
Best of scores 1-6 2.38%
OT1 11.01%
OT2 6.86%
OT3 11.45%
Table 6: Frequency of the system actions of the learned
policy that is based on Reward function 1 using all fea-
tures.
8 Discussion and Conclusion
We showed that RL is a promising technique for
learning question-answering policies. Currently we
use the same SU for both training and testing the
policies. One could argue that this favors the learned
policy over the baselines. Because our SU is based
on general corpus statistics (probability that the user
is child or male or female, number of questions the
user is planning to ask, probability of moving to the
next topic or ceasing the dialogue, utterance timing
statistics) rather than sequential information we be-
lieve that this is acceptable. We only use sequential
information when we calculate the next topic that
the user will choose. That is, due to the way the
SU is built and its randomness, we believe that it is
very unlikely that the same patterns that were gener-
ated during training will be generated during testing.
Thus we do not anticipate that our results would be
different if for testing we used a SU trained on a dif-
ferent part of the corpus, or that the learned policy is
favored over the baselines. However, this is some-
thing to verify experimentally in future work.
For future work we would also like to do the fol-
lowing. First of all, currently we are in the process of
analyzing user satisfaction questionnaires from mu-
seum visitors in order to define a better reward func-
tion. Second, we would like to use voice identifi-
cation techniques to automatically estimate from the
corpus the statistics of having more than one user
or alternating users in the same session. Third, and
most important, we would like to incorporate the
learned policy into the system that is currently in-
stalled in the museum and evaluate it with real users.
Fourth, currently our SU is based on only some of
our findings from the analysis of the corpus. We in-
tend to build a more complex and hopefully more
realistic SU based on our full corpus analysis. Fi-
nally, we will also experiment with learning policies
directly from the data (Li et al, 2009).
To conclude, we analyzed a corpus of interactions
of museum visitors with two virtual characters that
serve as guides at the Museum of Science in Boston,
in order to build a realistic model of user behavior
when interacting with these characters. Based on
this analysis, we built a SU and used it for learning
the dialogue policy of the virtual characters using
RL. We compared our learned policy with two base-
lines, one of which was the dialogue policy of the
original system that was used for collecting the cor-
pus and that is currently installed at the Museum of
Science in Boston. Our learned policy outperformed
both baselines which shows that RL is a promising
technique for learning question-answering dialogue
policies.
Acknowledgments
This work was funded by the NSF grant #1117313.
The Twins corpus collection was supported by the
NSF grant #0813541.
References
Priti Aggarwal, Ron Artstein, Jillian Gerten, Athanasios
Katsamanis, Shrikanth Narayanan, Angela Nazarian,
and David Traum. 2012. The Twins corpus of mu-
91
seum visitor questions. In Proc. of the Language
Resources and Evaluation Conference (LREC), pages
2355?2361, Istanbul, Turkey.
Hua Ai and Diane Litman. 2008. Assessing dialog sys-
tem user simulation evaluation measures using human
judges. In Proc. of the Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL-HLT), pages 622?629, Columbus,
OH, USA.
Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jor-
dan. 2011. Empirically evaluating the application
of reinforcement learning to the induction of effective
and adaptive pedagogical strategies. User Modeling
and User-Adapted Interaction, 21(1-2):137?180.
Sudeep Gandhe, Nicolle Whitman, David Traum, and
Ron Artstein. 2009. An integrated authoring tool
for tactical questioning dialogue systems. In Proc. of
the IJCAI Workshop on Knowledge and Reasoning in
Practical Dialogue Systems, Pasadena, CA, USA.
Kallirroi Georgila and David Traum. 2011a. Learn-
ing culture-specific dialogue models from non culture-
specific data. In Proc. of HCI International, Lecture
Notes in Computer Science Vol. 6766, pages 440?449,
Orlando, FL, USA.
Kallirroi Georgila and David Traum. 2011b. Reinforce-
ment learning of argumentation dialogue policies in
negotiation. In Proc. of Interspeech, pages 2073?
2076, Florence, Italy.
Kallirroi Georgila, James Henderson, and Oliver Lemon.
2006. User simulation for spoken dialogue systems:
Learning and evaluation. In Proc. of Interspeech,
pages 1065?1068, Pittsburgh, PA, USA.
Kallirroi Georgila, Maria K. Wolters, and Johanna D.
Moore. 2010. Learning dialogue strategies from older
and younger simulated users. In Proc. of the Annual
SIGdial Meeting on Discourse and Dialogue (SIG-
dial), pages 103?106, Tokyo, Japan.
Peter A. Heeman. 2009. Representing the reinforcement
learning state in a negotiation dialogue. In Proc. of the
IEEE Automatic Speech Recognition and Understand-
ing Workshop (ASRU), Merano, Italy.
Arne Jo?nsson, Frida Ande?n, Lars Degerstedt, Annika
Flycht-Eriksson, Magnus Merkel, and Sara Norberg.
2004. Experiences from combining dialogue system
development with information access techniques. In
New Directions in Question Answering, Mark T. May-
bury (Ed), pages 153?164. AAAI/MIT Press.
Filip Jurc???c?ek, Blaise Thomson, and Steve Young. 2012.
Reinforcement learning for parameter estimation in
statistical spoken dialogue systems. Computer Speech
and Language, 26(3):168?192.
Anton Leuski and David Traum. 2010. Practical lan-
guage processing for virtual humans. In Proc. of the
22nd Annual Conference on Innovative Applications
of Artificial Intelligence (IAAI), Atlanta, GA, USA.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective question
answering characters. In Proc. of the Annual SIGdial
Meeting on Discourse and Dialogue (SIGdial), pages
18?27, Sydney, Australia.
Lihong Li, Jason D. Williams, and Suhrid Balakrishnan.
2009. Reinforcement learning for dialog management
using least-squares policy iteration and fast feature se-
lection. In Proc. of Interspeech, pages 2475?2478,
Brighton, United Kingdom.
Teruhisa Misu, Komei Sugiura, Kiyonori Ohtake, Chiori
Hori, Hideki Kashioka, Hisashi Kawai, and Satoshi
Nakamura. 2010. Modeling spoken decision making
dialogue and optimization of its dialogue strategy. In
Proc. of the Annual SIGdial Meeting on Discourse and
Dialogue (SIGdial), pages 221?224, Tokyo, Japan.
Rieks op den Akker, Harry Bunt, Simon Keizer, and
Boris van Schooten. 2005. From question answering
to spoken dialogue: Towards an information search as-
sistant for interactive multimodal information extrac-
tion. In Proc. of Interspeech, pages 2793?2796, Lis-
bon, Portugal.
Jan Peters and Stefan Schaal. 2008. Natural actor-critic.
Neurocomputing, 71(7-9):1180?1190.
Verena Rieser and Oliver Lemon. 2009. Does this list
contain what you were searching for? Learning adap-
tive dialogue strategies for interactive question an-
swering. Natural Language Engineering, 15(1):55?
72.
William Swartout, David Traum, Ron Artstein, Dan
Noren, Paul Debevec, Kerry Bronnenkant, Josh
Williams, Anton Leuski, Shrikanth Narayanan, Diane
Piepol, Chad Lane, Jacquelyn Morie, Priti Aggarwal,
Matt Liewer, Jen-Yuan Chiang, Jillian Gerten, Selina
Chu, and Kyle White. 2010. Ada and Grace: Toward
realistic and engaging virtual museum guides. In Proc.
of the International Conference on Intelligent Virtual
Agents (IVA), pages 286?300, Philadelphia, PA, USA.
Joel R. Tetreault and Diane J. Litman. 2008. A reinforce-
ment learning approach to evaluating state representa-
tions in spoken dialogue systems. Speech Communi-
cation, 50(8-9):683?696.
Blaise Thomson and Steve Young. 2010. Bayesian up-
date of dialogue state: A POMDP framework for spo-
ken dialogue systems. Computer Speech and Lan-
guage, 24(4):562?588.
Sebastian Varges, Fuliang Weng, and Heather Pon-Barry.
2009. Interactive question answering and constraint
relexation in spoken dialogue systems. Natural Lan-
guage Engineering, 15(1):9?30.
EllenM. Voorhees. 2001. The TREC question answering
track. Natural Language Engineering, 7(4):361?378.
92
Appendix
Features Features
average ASR accuracy of user queries if system correctly answered current user query
# user queries if system responded with off-topic prompt
to current user query
# correct system responses # times user repeated current query
# incorrect system responses # successive incorrect system responses
# off-topic system prompts # successive off-topic system prompts
% correct system responses # user queries for topic ?introduction?
% incorrect system responses # user queries for topic ?personal?
user type (child, male, female) # user queries for topic ?school?
if user asks example query 1 # user queries for topic ?technology?
if user asks example query 2 # user queries for topic ?interfaces?
if user asks example query 3 # user queries for topic ?exhibition?
if user asks example query 4 # user queries for other topics
if system correctly responds to example query 1 if system correctly responds to example query 3
if system correctly responds to example query 2 if system correctly responds to example query 4
# junk user queries previous topic category
Table 7: List of features used in predicting when the user will cease a session (Cease Dialogue), what the user will say
next (Say Next 1), and what the user will say next after removing repeated user queries (Say Next 2). Example query
1 is ?who are you named after??; example query 2 is ?are you a computer??; example query 3 is ?what do you like to
do for fun??; example query 4 is ?what is artificial intelligence??.
Cease Dialogue Say Next 1 Say Next 2
average ASR accuracy of previous topic category previous topic category
user queries
user type (child, male, female) # user queries for topic ?personal? # junk user queries
# off-topic system prompts # user queries # successive incorrect system
responses
# successive off-topic system # junk user queries if system correctly answered
prompts current user query
# incorrect system responses % correct system responses user type (child, male, female)
# user queries % incorrect system responses % incorrect system responses
# junk user queries # incorrect system responses % correct system responses
# user queries for other topics # user queries for other topics # incorrect system responses
if system responded with off-topic # correct system responses # off-topic system prompts
prompt to current user query
% correct system responses user type (child, male, female) # user queries
Table 8: List of the 10 most dominant features (in order of importance) in predicting when the user will cease a session
(Cease Dialogue), what the user will say next (Say Next 1), and what the user will say next after removing repeated
user queries (Say Next 2).
93
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 270?274,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
A Study in How NLU Performance Can Affect the Choice of Dialogue
System Architecture
Anton Leuski and David DeVault
USC Institute for Creative Technologies
12015 Waterfront Drive, Playa Vista, CA 90094
{leuski,devault}@ict.usc.edu
Abstract
This paper presents an analysis of how the
level of performance achievable by an NLU
module can affect the optimal modular design
of a dialogue system. We present an evalua-
tion that shows how NLU accuracy levels im-
pact the overall performance of a system that
includes an NLU module and a rule-based di-
alogue policy. We contrast these performance
levels with the performance of a direct classifi-
cation design that omits a separate NLU mod-
ule. We conclude with a discussion of the po-
tential for a hybrid architecture incorporating
the strengths of both approaches.
1 Introduction
Recently computer-driven conversational characters
or virtual humans have started finding real-life ap-
plications ranging from education to health services
and museums (Traum et al, 2005; Swartout et al,
2006; Kenny et al, 2009; Jan et al, 2009; Swartout
et al, 2010). As proliferation of these systems in-
creases, there is a growing demand for the design
and construction of virtual humans to be made more
efficient and accessible to people without extensive
linguistics and computer science backgrounds, such
as writers, designers, and educators. We are specif-
ically interested in making the language processing
and dialogue management components in a virtual
human easier for such potential authors to develop.
Some system building steps that can be challenging
for such authors include annotating the meaning of
user and system utterances in a semantic formalism,
developing a formal representation of information
state, and writing detailed rules that govern dialogue
management.
We are generally interested in the extent to which
these various authoring steps are necessary in order
to achieve specific levels of system performance. In
this paper, we present a case study analysis of the
performance of two alternative architectures for a
specific virtual human. The two architectures, which
have been developed and evaluated in prior work
(DeVault et al, 2011b; DeVault et al, 2011a), differ
substantially in their semantic annotation and policy
authoring requirements. We describe these architec-
tures and our evaluation corpus in Section 2. We
focus our new analysis specifically on how the over-
all performance of one of the architectures, which
uses a natural language understanding (NLU) mod-
ule and hand-authored rules for the dialogue policy,
depends on the performance of the NLU module. In
Section 3, we describe our finding that, depending
on the attainable level of NLU accuracy, this modu-
lar approach may or may not perform better than a
simpler direct classification design that omits a sep-
arate NLU module and has a lower annotation and
rule authoring burden. In Section 4, we present an
initial exploration of whether a hybrid architecture
may be able to combine these approaches? strengths.
2 Summary of Data Set and Prior Results
This work is part of an ongoing research effort
into techniques for developing high quality dialogue
policies using a relatively small number of sample
dialogues and low annotation requirements (DeVault
et al, 2011b; DeVault et al, 2011a). This section
briefly summarizes our prior work and data set.
270
2.1 Data Set
For our experiments we use the dataset described
in (DeVault et al, 2011b). It contains 19 Wiz-
ard of Oz dialogues with a virtual human called
Amani (Gandhe et al, 2009). The user plays the
role of an Army commander whose unit has been at-
tacked by a sniper. The user interviews Amani, who
was a witness to the incident and has some informa-
tion about the sniper. Amani is willing to tell the
interviewer what she knows, but she will only re-
veal certain information in exchange for promises of
safety, secrecy, and money (Artstein et al, 2009).
Each dialogue turn in the data set includes a single
user utterance followed by the response chosen by a
human Amani role player. There are a total of 296
turns, for an average of 15.6 turns/dialogue. User
utterances are modeled using 46 distinct speech act
(SA) labels. The dataset alo defines a different set
of 96 unique SAs (responses) for Amani. Six ex-
ternal referees analyzed each user utterance and se-
lected a single character response out of the 96 SAs.
Thus the dataset defines a one-to-many mapping be-
tween user utterances and alternative system SAs.
2.2 Evaluation Metric
We evaluate the dialogue policies in our experi-
ments through 19-fold cross-validation of our 19 di-
alogues. In each fold, we hold out one dialogue and
use the remaining 18 as training data. To measure
policy performance, we count an automatically pro-
duced system SA as correct if that SA was chosen by
the original wizard or at least one external referee for
that dialogue turn. We then count the proportion of
the correct SAs among all the SAs produced across
all 19 dialogues, and use this measure of weak accu-
racy to score dialogue policies.
We can use the weak accuracy of one referee,
measured against all the others, to establish a per-
formance ceiling for this metric. This score is .79;
see DeVault et al (2011b).
2.3 Baseline Systems
We consider two existing baseline systems in our ex-
periments here. The first system (Rules-NLU-SA)
consists of a statistical NLU module that maps a user
utterance to a single user SA label, and a rule-based
dialogue policy hand-crafted by one of the authors.
The NLU uses a maximum-entropy model (Berger
et al, 1996) to classify utterances as one of the user
SAs using shallow text features. Training this model
requires a corpus of user utterances that have been
semantically annotated with the appropriate SA.
We developed our rule-based policy by manu-
ally writing the simple rules needed to implement
Amani?s dialogue policy. Given a user SA label
At for turn t, the rules for determining Amani?s re-
sponse Rt take one of three forms:
(a)ifAt = SAi thenRt = SAj
(b)ifAt = SAi ? ?kAt?k = SAl thenRt = SAj
(c)ifAt = SAi ? ??kAt?k = SAl thenRt = SAj
The first rule form specifies that a given user SA
should always lead to a given system response. The
second and third rule forms enable the system?s re-
sponse to depend on the user having previously per-
formed (or not performed) a specific SA. One the
system developers, who is also a computational lin-
guist, created the current set of 42 rules in about 2
hours. There are 30 rules of form (a), 6 rules of form
(b), and 6 rules of form (c).
The second baseline system (RM-Text) is a sta-
tistical classifier that selects system SAs by analyz-
ing shallow features of the user utterances and sys-
tem responses. We use the Relevance Model (RM)
approach pioneered by Lavrenko et al (2002) for
cross-lingual information retrieval and adapted to
question-answering by Leuski et al (2006). This
method does not require semantic annotation or rule
authoring; instead, the necessary training data is de-
fined by linking user utterances directly to the appro-
priate system responses (Leuski and Traum, 2010).
Table 1 summarizes the performance for the base-
line systems (DeVault et al, 2011a). The NLU mod-
ule accuracy is approximately 53%, and the weak
accuracy of .58 for the corresponding system (Rules-
NLU-SA) is relatively low when compared to the
RM system at .71. For comparison we provide a
third data point: for Rules-G-SA, we assume that
our NLU is 100% accurate and always returns the
correct (?gold?) SA label. We then run the rule-
based dialogue policy on those labels. The third
column (Rules-G-SA) shows the resulting weak ac-
curacy value, .79, which is comparable to the weak
accuracy score achieved by the human referees (De-
Vault et al, 2011b).
271
Rules-NLU-SA RM-Text Rules-G-SA
.58 .71 .79
Table 1: Weak accuracy results for baseline systems.
Rules-G
RM-Text
Rules-NLU-SA
50 60 70 80 90 100
55
60
65
70
75
80
Simulated NLU Accuracy H%L
W
e
a
k
A
c
c
u
r
a
c
y
H
%
L
Figure 1: Weak accuracy of the Rules system as a func-
tion of simulated NLU accuracy.
3 NLU Accuracy and System Performance
We conducted two experiments. In the first, we stud-
ied the effect of NLU accuracy on the performance
of the Rules-NLU-SA system. One of our goals was
to find how accurate the NLU would have to be for
the Rules-NLU-SA system to outperform RM-Text.
To investigate this, we simulated NLU perfor-
mance at different accuracy levels by repeatedly
sampling to create a mixture of the SAs from the
trained NLU classifier and from the correct (gold)
set of SAs. Specifically, we set a fixed value p rang-
ing from 0 to 1 and then iterate over all dialogue
turns in the held out dialogue, selecting the the cor-
rect SA label with probability p or the trained NLU
module?s output with probability 1 ? p. Using the
sampled set of SA labels, we compute the result-
ing simulated NLU accuracy, run the Rules dialogue
policy, and record the weak accuracy result. We re-
peat the process 25 times for each value of p. We let
p range from 0 to 1 in increments of .05 to explore a
range of simulated accuracy levels.
Figure 1 shows simulated NLU accuracy and the
corresponding dialogue policy weak accuracy as a
point in two dimensions. The points form a cloud
with a clear linear trend that starts at approximately
53% NLU accuracy where it intersects with the
Rules-NLU-SA system performance and then goes
up to the Rules-G performance at 100% NLU accu-
racy. The correlation is strong with R2 = 0.97.1
The existence of a mostly linear relationship com-
ports with the fact that most of the policy rules (30
of 42), as described in Section 2.3, are of form (a).
For such rules, each individual correct NLU speech
act translates directly into a single correct system
response, with no dependence on the system hav-
ing understood previous user utterances correctly.
In contrast, selecting system responses that comply
with rules in forms (b) and (c) generally requires
correct understanding of multiple user utterances.
Such rules create a nonlinear relationship between
policy performance and NLU accuracy, but these
rules are relatively few in number for Amani.
The estimated linear trend line (in purple) crosses
the RM-Text system performance at approximately
82% NLU accuracy. This result suggests that our
NLU component would need to improve from its
current accuracy of 53% to approximately 82% ac-
curacy for the Rules-NLU-SA system to outperform
the RM-Text classifier. This represents a very sub-
stantial increase in NLU accuracy that, in practice,
could be expected to require a significant effort in-
volving utterance data collection, semantic annota-
tion, and optimization of machine learning for NLU.
4 Hybrid System
In our second experiment we investigated the po-
tential to integrate the Rules-NLU-SA and RM-Text
systems together for better performance. Our ap-
proach draws on a confidence score ? from the NLU
maximum-entropy classifier; specifically, ? is the
probability assigned to the most probable user SA.
Figure 2 shows an analysis of NLU accuracy,
Rules-NLU-SA, and RM-Text that is restricted to
those subsets of utterances for which NLU confi-
dence ? is greater than or equal to some threshold ? .
Two important aspects of this figure are (1) that rais-
ing the minimum confidence threshold also raises
the NLU accuracy on the selected subset of utter-
ances; and (2) that there is a threshold NLU confi-
dence level beyond which Rules-NLU-SA seems to
1This type of analysis of dialogue system performance in
terms of internal component metrics is somewhat similar to the
regression analysis in the PARADISE framework (Walker et al,
2000). However, here we are not concerned with user satis-
faction, but are instead focused solely on the modular system?s
ability to reproduce a specific well-defined dialogue policy.
272
0.4 0.5 0.6 0.7 0.8 0.9 1.0
65
70
75
80
85
90
95
NLU Confidence H?L
A
c
c
u
r
a
c
y
H
%
L
RM-Text
Rules-NLU-SA H? ? ?L
NLU Accuracy H? ? ?L
Figure 2: Weak accuracy of Rules-NLU-SA and RM-
Text on utterance subsets for which NLU confidence
? ? ? . We also indicate the corresponding NLU accu-
racy at each threshold. In all cases a rolling average of 30
data points is shown to more clearly indicate the trends.
RM-Text
Rules-NLU-SA
0.70 0.75 0.80 0.85 0.90 0.95 1.00
55
60
65
70
75
0
15
30
45
60
NLU Confidence H? L
W
e
a
k
A
c
c
u
r
a
c
y
H
%
L
P
r
o
p
.
o
f
N
L
U
S
A
s
w
?
?
?
?
Mix
Hybrid
Figure 3: Weak accuracy of the Hybrid system as a func-
tion of the NLU confidence score.
outperform RM-Text. This confidence level is ap-
proximately 0.95, and it identifies a subset of user
utterances for which NLU accuracy is 83.3%. These
results therefore suggest that NLU confidence can
be useful in identifying utterances for which NLU
speech acts are more likely to be accurate and Rules-
NLU-SA is more likely to perform well.
To explore this further, we implemented a hy-
brid system that chooses between Rules-NLU-SA or
RM-Text as follows. If the confidence score is high
enough (? ? ? , for some fixed threshold ? ), the Hy-
brid system uses the NLU output to run the Rules
dialogue policy to select the system SA; otherwise,
it discards the NLU SA, and applies the RM classi-
fier to select the system response directly.
Figure 3 shows the plot of the Hybrid system per-
formance as a function of the threshold value ? .
We see that with sufficiently high threshold value
(? ? 0.95) the Hybrid system outperforms both
the Rules-NLU-SA and the RM-Text systems. The
second line, labeled ?Mix? and plotted against the
secondary (right) axis, shows the proportion of the
NLU SAs with the confidence score that exceed the
threshold (? ? ? ). It indicates how often the Hybrid
system prefers the Rules-NLU-SA output over the
RM-Text system output. We observe that approxi-
mately 42 of the NLU outputs over all 296 dialogue
turns (15%) have confidence values ? ? 0.95. How-
ever, for most of these dialogue turns the outputs for
the Rules-NLU-SA and RM-Text dialogue policies
are the same. While we observe a small improve-
ment in the Hybrid system weak accuracy values
over the RM-Text system at thresholds of 0.95 and
higher, the difference is not statistically significant.
Despite the lack of statistical significance in the
initial Hybrid results in this small data set, we inter-
pret the complementary evidence from both experi-
ments, which support the potential for Rules-NLU-
SA to perform well when NLU accuracy is high, and
the potential for a hybrid system to identify a subset
of utterances that are likely to be understood accu-
rately at run-time, as indicating that a hybrid design
is a promising avenue for future work.
5 Conclusions and Future Work
We presented a case study analysis of how the level
of performance that is achievable in an NLU module
can provide perspective on the design choices for a
modular dialogue system. We found that NLU accu-
racy must be substantially higher than it currently is
in order for the Rules-NLU-SA design, which car-
ries a greater annotation and rule authoring burden,
to deliver better performance than the simpler RM-
Text design. We also presented evidence that a hy-
brid architecture could be a promising direction.
Acknowledgments
The project or effort described here has been spon-
sored by the U.S. Army Research, Development,
and Engineering Command (RDECOM). State-
ments and opinions expressed do not necessarily re-
flect the position or the policy of the United States
Government, and no official endorsement should be
inferred.
273
References
Ron Artstein, Sudeep Gandhe, Michael Rushforth, and
David R. Traum. 2009. Viability of a simple dialogue
act scheme for a tactical questioning dialogue system.
In DiaHolmia 2009: Proceedings of the 13th Work-
shop on the Semantics and Pragmatics of Dialogue,
page 43?50, Stockholm, Sweden, June.
Adam L. Berger, Stephen D. Della Pietra, and Vincent
J. D. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computational
Linguistics, 22(1):39?71.
David DeVault, Anton Leuski, and Kenji Sagae. 2011a.
An evaluation of alternative strategies for implement-
ing dialogue policies using statistical classification and
rules. In Proceedings of the 5th International Joint
Conference on Natural Language Processing (IJC-
NLP), pages 1341?1345, Nov.
David DeVault, Anton Leuski, and Kenji Sagae. 2011b.
Toward learning and evaluation of dialogue policies
with text examples. In Proceedings of the 12th annual
SIGdial Meeting on Discourse and Dialogue, pages
39?48.
Sudeep Gandhe, Nicolle Whitman, David R. Traum, and
Ron Artstein. 2009. An integrated authoring tool for
tactical questioning dialogue systems. In 6th Work-
shop on Knowledge and Reasoning in Practical Dia-
logue Systems, Pasadena, California, July.
Dusan Jan, Antonio Roque, Anton Leuski, Jackie Morie,
and David R. Traum. 2009. A virtual tour guide for
virtual worlds. In Zso?fia Ruttkay, Michael Kipp, An-
ton Nijholt, and Hannes Ho?gni Vilhja?lmsson, editors,
IVA, volume 5773 of Lecture Notes in Computer Sci-
ence, pages 372?378. Springer.
Patrick G. Kenny, Thomas D. Parsons, and Albert A.
Rizzo. 2009. Human computer interaction in virtual
standardized patient systems. In Proceedings of the
13th International Conference on Human-Computer
Interaction. Part IV, pages 514?523, Berlin, Heidel-
berg. Springer-Verlag.
Victor Lavrenko, Martin Choquette, and W. Bruce Croft.
2002. Cross-lingual relevance models. In Proceed-
ings of the 25th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 175?182, Tampere, Finland.
Anton Leuski and David Traum. 2010. NPCEditor: A
tool for building question-answering characters. In
Proceedings of The Seventh International Conference
on Language Resources and Evaluation (LREC).
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective ques-
tion answering characters. In Proceedings of the 7th
SIGdial Workshop on Discourse and Dialogue, Syd-
ney, Australia, July.
W. Swartout, J. Gratch, R. W. Hill, E. Hovy, S. Marsella,
J. Rickel, and D. Traum. 2006. Toward virtual hu-
mans. AI Mag., 27(2):96?108.
William R. Swartout, David R. Traum, Ron Artstein,
Dan Noren, Paul E. Debevec, Kerry Bronnenkant, Josh
Williams, Anton Leuski, Shrikanth Narayanan, and
Diane Piepol. 2010. Ada and grace: Toward realis-
tic and engaging virtual museum guides. In Jan M.
Allbeck, Norman I. Badler, Timothy W. Bickmore,
Catherine Pelachaud, and Alla Safonova, editors, IVA,
volume 6356 of Lecture Notes in Computer Science,
pages 286?300. Springer.
David Traum, William Swartout, Jonathan Gratch,
Stacy Marsella, Patrick Kenney, Eduard Hovy, Shri
Narayanan, Ed Fast, Bilyana Martinovski, Rahul Bha-
gat, Susan Robinson, Andrew Marshall, Dagen Wang,
Sudeep Gandhe, and Anton Leuski. 2005. Dealing
with doctors: Virtual humans for non-team interac-
tion training. In Proceedings of ACL/ISCA 6th SIGdial
Workshop on Discourse and Dialogue, Lisbon, Portu-
gal, September.
Marilyn Walker, Candace Kamm, and Diane Litman.
2000. Towards developing general models of usability
with PARADISE. Nat. Lang. Eng., 6(3-4):363?377.
274
Proceedings of the SIGDIAL 2013 Conference, pages 394?403,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Which ASR should I choose for my dialogue system?
Fabrizio Morbini, Kartik Audhkhasi, Kenji Sagae, Ron Artstein,
Dog?an Can, Panayiotis Georgiou, Shri Narayanan, Anton Leuski and David Traum
University of Southern California
Los Angeles, California, USA
{morbini,sagae,artstein,leuski,traum}@ict.usc.edu
{audhkhas,dogancan}@usc.edu {georgiou,shri}@sipi.usc.edu
Abstract
We present an analysis of several pub-
licly available automatic speech recogniz-
ers (ASRs) in terms of their suitability for
use in different types of dialogue systems.
We focus in particular on cloud based
ASRs that recently have become available
to the community. We include features
of ASR systems and desiderata and re-
quirements for different dialogue systems,
taking into account the dialogue genre,
type of user, and other features. We then
present speech recognition results for six
different dialogue systems. The most in-
teresting result is that different ASR sys-
tems perform best on the data sets. We
also show that there is an improvement
over a previous generation of recognizers
on some of these data sets. We also inves-
tigate language understanding (NLU) on
the ASR output, and explore the relation-
ship between ASR and NLU performance.
1 Introduction
Dialogue system developers who are not also
speech recognition experts are in a better posi-
tion than ever before in terms of the ease of in-
tegrating existing speech recognizers in their sys-
tems. While there have been commercial solutions
and toolkits for a number of years, there were a
number of problems in getting these systems to
work. For example, early toolkits relied on spe-
cific machine hardware, software, and firmware
to function properly, often had a difficult instal-
lation process, and moreover often didn?t work
well for complex dialogue domains, or challeng-
ing acoustic environments. Fortunately the situ-
ation has greatly improved in recent years. Now
there are a number of easy to use solutions, in-
cluding open-source systems (like PocketSphinx),
as well as cloud-based approaches.
While this increased choice of quality recogniz-
ers is of great benefit to dialogue system develop-
ers, it also creates a dilemma ? which recognizer
to use? Unfortunately, the answer is not simple ?
it depends on a number of issues, including the
type of dialogue domain, availability and amount
of training data, availability of internet connectiv-
ity for the runtime system, and speed of response
needed. In this paper we assess several freely
available speech recognition engines, and exam-
ine their suitability and performance in several di-
alogue systems. Here we extend the work done in
Yao et al (2010) focusing in particular on cloud
based freely available ASR systems. We include
2 local ASRs for reference, one of which was also
used in the earlier work for easy comparison.
2 Speech Recognizer Features and
Engines
The following are some of the major criteria for
selection of a speech recognizer.
Customization Some of the available speech
recognizers allow the users to tune the recognizer
to the environment it will operate in, by providing
a specialized lexicon, trained language models or
acoustic models. Customization is especially im-
portant for dialogue systems whose input contains
specialized vocabulary (see section 4).
Output options A basic recognizer will output
a string of text, representing its best hypothesis
about the transcription of the speech input. Some
recognizers offer additional outputs which are use-
ful for dialogue systems: ranked n-best hypothe-
ses allow later processing to use context for dis-
ambiguation, and incremental results allow the
system to react while the user is still speaking.
Performance characteristics Dialogue systems
differ in their requirements for response speed; a
394
System Customization Output options Open Source PerformanceN-best Incremental Speed Installation
Pocketsphinx Full Yes Yes Yes realtime Local
Apple No Noa No No network Cloud
Google No Yes Yesb No network Cloud
AT&T Partialc Yes No No network Cloud
Otosense-Kaldi Full Yes No Yesd variablee Local
aSingle output annotated with alternative hypotheses. bOnly for web-delivered applications in a Google Chrome browser.
cCustom language models. dRelease scheduled for Fall 2013. eUser controls trade-off between speed and output quality.
Table 1: Speech recognizer features important for use in dialogue systems
speech recognizer that runs locally can help by
avoiding network latencies.
Output quality Typically, a dialogue system
would want the best recognition accuracy pos-
sible given the constraints. Ultimately, dialogue
systems want the output that would yield the best
performance for Natural Language Understand-
ing and other downstream processes. As a rule,
better speech recognition leads to better language
understanding, though this is not necessarily the
case for specific applications (see section 5).
We evaluated 5 freely available speech recog-
nizers. Their features are summarized in Table 1.
We did not include the MIT WAMI toolkit1 as we
are focused on speech services that can directly
be used by stand alone applications as opposed to
web delivered ones. We did not include commer-
cial recognizers such as Nuance, because licensing
terms can be difficult for research institutions, and
in particular, disallow publishing benchmarks.
Pocketsphinx is a version of the CMU Sphinx
ASR system optimized to run also on embedded
systems (Huggins-Daines et al, 2006). Pocket-
sphinx is fast, runs locally, and requires relatively
modest computational resources. It provides n-
best lists and lattices, and supports incremental
output. It also provides a voice activity detec-
tion functionality for continuous ASR. This ASR
is fully customizable and trainable, but users are
expected to provide language models suitable for
their applications. A few acoustic models are pro-
vided, and can be adapted using the CMUSphinx
tools.2
1http://wami.csail.mit.edu/
2http://cmusphinx.sourceforge.net/wiki/tutorialadapt
Apple Dictation is the OS level feature in both
MacOSX and iOS.3 It is integrated into the text in-
put system pipeline so a user can replace her key-
board with a microphone for entering text in any
application. Dictation is often associated with the
Siri personal assistant feature of iOS. While it is
likely that Dictation and Siri share the same ASR
technology, Dictation only does speech recogni-
tion. Apple states that Dictation learns the charac-
teristics of the user?s voice and adapts to her accent
(Apple Inc, 2012). Dictation requires an internet
connection to send recorded user speech to Ap-
ple?s servers and receive ASR results. Processing
starts as soon as the user starts speaking so the de-
lay of getting the recognition results after the user
finishes speaking is minimal.
To integrate Dictation into a dialogue system,
a system designer needs to include any system de-
fined text input control into her application and use
the control APIs to observe text changes. The user
would need to press a key when starting to speak
and push the key again once she is done speak-
ing. The ASR result is a text string annotated with
alternative interpretations of individual words or
phrases in the text. There is an API for extract-
ing those interpretations from the result. While the
Dictation feature is reasonably fast and easy to in-
tegrate, dialogue system developers have no con-
trol over the ASR process, which must be treated
as a black box. Apple dictation is limited in that
no customization is possible, no partial recogni-
tion results are provided, and there is an unspeci-
fied limit on the number of utterances dictated for
a period of time, which is not a problem for inter-
action between a single user and a dialogue sys-
tem, but may be an issue in dialogue systems that
support multiple concurrent users.
3Dictation was introduced in iOS 5.0 and MacOSX 10.8.
395
Google Speech API provides support for the
HTML 5 speech input feature.4 It is a cloud based
service in which a user submits audio data using
an HTML POST request and receives as reply the
ASR output in the form of an n-best list. The au-
dio data is limited to roughly 10 seconds in length,
longer clips are rejected and return no ASR results.
The user can (1) customize the number of hy-
potheses returned by the ASR, (2) specify which
language the audio file contains and (3) enable a
filter to remove profanities from the output text.
As is the case with Apple Dictation, ASR must be
treated as a black box, and no task customization
is possible for dialogue system developers. Users
cannot specify or provide custom language models
or acoustic models. The service returns only the fi-
nal hypothesis, there is no incremental output.5 In
addition, results for the same inputs may change
unpredictably, since Google may update or other-
wise change its service and models, and models
may be adapted using specific audio data supplied
by users. In our experiments, we observed accu-
racy improvements when submitting the same au-
dio files over repeated trials over two weeks.
AT&T Watson is the ASR engine available
through the AT&T Speech Mashup service.6 It is
a cloud based service that can be accessed through
HTML POST requests, like the Google Speech
API. AT&T Watson is designed to support the de-
mands of online spoken dialogue systems, and can
be customized with data specific to a dialogue sys-
tem. Additionally, in our tests we did not observe
any limitation in the maximum length of the in-
put audio data. However, AT&T does not provide
a default general-purpose language model, and
application-specific models must be built within
the Speech Mashup service using user-provided
text data. The acoustic model must be selected
from a list provided by the AT&T service, and
acoustic models can be further customized within
the Speech Mashup service. The ASR returns an
n-best list of hypotheses but does not provide in-
cremental output.
Otosense-Kaldi Another ASR we employed
was the Kaldi-based OtoSense-Kaldi engine de-
4https://www.google.com/speech-api/v1/recognize
5The demo page shows continuous speech understanding
with incremental results but requires Google Chrome to run
and is specific to web delivered applications:
http://www.google.com/intl/en/chrome/demos/speech.html
6https://service.research.att.com/smm
veloped at SAIL.7 OtoSense-Kaldi8 is an on-line,
multi-threaded architecture based on the Kaldi
toolkit (Povey et al, 2011) that allows for dynam-
ically configurable and distributed ASR.
3 Dialogue Systems, Users, and Data
All spoken dialogue systems are similar in some
respects, in that there is speech by a user (or users)
that needs to be recognized, and this speech is
punctuated by speech from the system. More-
over, the speech is not fully independent, but ut-
terances are connected to other utterances, e.g. an-
swers to questions, or clarifications. There are,
however many ways in which systems can differ,
that have implications for which speech recogniz-
ers are most appropriate. Some of the dimensions
to consider are:
Type of microphone(s) One of the biggest im-
pacts on ASR is the acoustic environment. Will
the audio be clean, coming from a close-talking
head or lapel-mounted microphone, or will it need
to be picked up from a broader directional micro-
phone or microphone array?
Number of speakers/microphones Will there
be one designated microphone per person, or will
speaker identification need to be performed? Will
audio from the system confuse the ASR?
Push to talk or continuous speech Will the
user clearly identify the start and end of speech,
or will the system need to detect speech acousti-
cally?
Type of Users Will there be designated long-
term users, where user-training or system model
adaptation is feasible, or will there be many un-
known users, where training is not feasible? See
also section 3.1 for more on user types.
Genre What kinds of things will people be say-
ing to the system? Is it mostly commands or short
answers to questions, or more open-ended conver-
sation? See section 3.2 for more on genre issues.
Training Data Is within-domain training data
available, and if so how much?
3.1 Types of Users
The type of user is important for the overall
design of the system and has implications for
7http://sail.usc.edu
8OtoSense-Kaldi will be released (BSD license) in 2013.
396
ASR performance as well. One important as-
pect is the broad physical differences among
speakers, such as male vs female, adult vs child
(e.g. Bell and Gustafson, 2003), or language pro-
ficiency/accent, that will have implications for the
acoustics of what is said, and ASR results. Other
aspects of users have implications for what will
be said, and how successful the interface may
be, overall. Many (e.g. Hassel and Hagen, 2006;
Jokinen and Kanto, 2004) have looked at the dif-
ferences between novice and expert users. Ai et
al. (2007a) also points out a difference between
real users and recruited subjects. Real users also
come in many different flavors, depending on their
purposes. E.g. are they interacting with the system
for fun, to do a specific task that they need to get
done, to learn something (specific or general), or
with some other purpose in mind?
We considered the following classes of users,
ordered from easiest to hardest to get to acceptable
performance and robustness levels:
Demonstrators are generally the easiest for a sys-
tem to understand ? a demonstrator is trained in
use of the system, knows what can and can?t be
said, is motivated toward success, and is gener-
ally interested in showing off the most impres-
sive/successful aspects of the system to an audi-
ence rather than using it for its own sake.
Trained/Expert Users are similar to demonstra-
tors, but use the system to achieve specific results
rather than just to show off its capabilities. This
means that users may be forced down lines that
are not ideal for the system, if these are necessary
to accomplish the task.
Motivated Users do not have the training of ex-
pert users, and may say many things that the sys-
tem can not handle as opposed to equivalent ex-
pressions that could be handled. However moti-
vated users do want the system to succeed, and in
general are willing to do whatever they think is
necessary to improve system performance. Unlike
expert users, motivated users might be incorrect
about what will help the system (e.g. hyperarticu-
lation in response to system misunderstanding).
Casual Users are interested in finding out what
the system can do, but do not have particular moti-
vations to help or hinder the system. Casual Users
may also leave in the middle of an interaction, if it
is not engaging enough.
Red Teams are out to test or ?break? the system,
or show it as not-competent, and may try to do
things the system can?t understand or react well
to, even when an alternative formulation is known
to work.
3.2 Types of Dialogue System Genres
Dialogue Genres can be distinguished along many
lines, e.g. the number and relationship of partic-
ipants, specific conversational rules, purposes of
the participants, etc. We distinguish here four gen-
res of dialogue system that have been in use at
the Institute for Creative Technologies and that we
have available corpora for (there are many other
types of dialogue genres, including tutoring, ca-
sual conversation, interviewing,. . . ). Each genre
has implications for the internal representations
and system architectures needed to engage in that
genre of dialogue.
Simple Question-answering This genre in-
volves strong user-initiative and weak global di-
alogue coherence. The user can ask any ques-
tion to the system at any time, and the system
should respond, with an appropriate answer if
able, or with some other reply indicating either
inability or unwillingness to provide the answer.
This genre allows modeling dialogue at a surface-
text level (Gandhe, 2013), without internal se-
mantic representations of the input, and where
the result of ?understanding? input is the system?s
expected output. The NCPEditor9 (Leuski and
Traum, 2011) is a toolkit that provides an author-
ing environment, classification, and dialogue ca-
pability for simple question-answering characters.
The SGT Blackwell, SGT Star, and Twins systems
described below are all systems in this genre.
Advanced Question-answering This genre is
similar to the simple question-answering charac-
ters, in that the main task of the user is to elicit
information from the system character. The differ-
ence is that there is more long-range and interme-
diate dialogue coherence, in that questions can be
answered several utterances after they have been
asked, there can be intervening sub-dialogues, and
characters sometimes take the initiative to pursue
their own goals rather than just responding to the
user. Because of the requirements for somewhat
deeper understanding, and relation of input to con-
9Available free for academic research purposes from
https://confluence.ict.usc.edu/display/VHTK/Home
397
text and character goals and policies, there is a
need of at least a shallow semantic representa-
tion and representation of the dialogue informa-
tion state, and the character must distinguish un-
derstanding of the input from the character out-
put (since the latter will depend on the dialogue
policy and information state, not just the under-
standing of input). The tactical questioning archi-
tecture (Gandhe et al, 2009)10 provides author-
ing and run-time support for advanced question-
answering characters, and has been used to build
over a dozen characters for purposes such as train-
ing tactical questioning, training culture, and psy-
chology experiments (Gandhe et al, 2011). The
Amani character described below is in this genre.
Slot-filling Probably the most common type of
dialogue system (at least in the research commu-
nity) is slot-filling. Here the dialogue is fairly
structured, with an initial greeting phase, then one
or more tasks, which all start with the user se-
lecting the task, and the system taking over ini-
tiative to ?fill? and possibly confirm the needed
slots, before retrieving some information from a
database, or performing a simple service.11 This
genre also requires a semantic representation, at
least of the slots and acceptable values. Gener-
ally, the set of possible values is large enough, that
some form of NLG is needed (at least template
filling), rather than authoring of all full sentences.
There are a number of toolkits and development
frameworks that are well suited to slot-filling sys-
tems, e.g. Ravenclaw (Bohus and Rudnicky, 2003)
or Trindikit (Larsson and Traum, 2000). The Ra-
diobots system, described below is in this genre.
Negotiation and Planning In this genre, the
system is more of an equal partner with the user,
than a servant, as in the slot-filling systems. The
system must not merely understand user requests,
but must also evaluate whether they meet the sys-
tem goals, what the consequences and precondi-
tions of requests are, and whether there are better
alternatives. For this kind of inference, a more de-
tailed semantic representation is required than just
filling in slots. While we are not aware of publicly
available software that makes this kind of system
easy to construct, there have been several built us-
ing an information-state approach, or the soar cog-
10Soon to be released as part of the virtual human toolkit.
11Mixed-initiative versions of this genre exist, where the
user can also provide unsolicited information, which reduces
the number of system queries needed.
nitive architecture. The TRIPS system (Allen et
al., 2001) also has many similarities.
3.3 ICT Dialogue Systems Tested
We tested the recognizers described in section 2
on data sets collected from six different dialogue
domains. Five are the same ones tested in Yao et
al. (2010), to which we added the Twins set. De-
tails on the size of the training and development
sets may be found in Yao et al (2010), here we
report only the numbers relevant to the Twins do-
main and to the NLU analysis, which are not in
Yao et al (2010).
SGT Blackwell was created as a virtual human
technology demonstration for the 2004 Army Sci-
ence Conference. This is a question-answering
character, with no internal semantic representation
and the primary NLU task merged with Dialogue
management as selecting the best response.
The original users were ICT demonstrators.
However, there were also some experiments with
recruited participants (Leuski et al, 2006a; Leuski
et al, 2006b). Later SGT Blackwell became a part
of the ?best design in America? triennial at the
Cooper-Hewitt Museum in New York City, and
the data set here is from visitors to the museum,
who are mostly casual users, but range from expert
to red-team. Users spoke into a mounted direc-
tional microphone (see Robinson et al, 2008 for
more details).
SGT STAR (Artstein et al, 2009a) is a question-
answering character similar to SGT Blackwell, al-
though designed to talk about Army careers rather
than general knowledge. The users are Army per-
sonnel who went to job fairs and visited schools in
the mobile Army adventure vans, speaking using
headset microphones, and performing for an audi-
ence. The users are somewhere between demon-
strators and expert users. They are speaking to
SGT STAR for the benefit of an audience, but their
primary purpose is to convey information to the
audience in a memorable way (through dialogue
with SGT STAR) rather than to show off the high-
lights of the character.
The Twins are two life-size virtual characters
who serve as guides at the Museum of Science
in Boston (Swartout et al, 2010). The charac-
ters promote interest in Science, Technology, En-
gineering and Mathematics (STEM) in children
between the ages of 7 and 14. They are question-
398
answering characters, but unlike SGTs Blackwell
and Star, the response is a whole dialogue se-
quence, potentially involving interchange from
both characters, rather than a single character turn.
There are two types of users for the Twins:
demonstrators, who are museum staff members,
using head-mounted microphones, and museum
visitors, who use a Shure 522 table-top mounted
microphone (Traum et al, 2012). More on analy-
sis of the museum data can be found in (Aggarwal
et al, 2012). We also investigated speech recog-
nition and NLU performance in this domain in
Morbini et al (2012).
This dataset contains 14K audio files each an-
notated with one of the 168 possible response se-
quences. The division in training development and
test is the same used in Morbini et al (2012) (10K
for training, the rest equally divided between de-
velopment and test).
Amani (Artstein et al, 2009b; Artstein et al,
2011) is an advanced question-answering char-
acter used as a prototype for systems meant to
train soldiers to perform tactical questioning. The
users are in between real users and test subjects:
they were cadets at the U.S. Military Academy in
April 2009, who interacted with Amani as a uni-
versity course exercise on negotiation techniques.
They used head-mounted microphones to talk with
Amani.
This dataset comprises of 1.8K audio files each
annotated with one of the 105 possible NLU se-
mantic classes.
Radiobots (Roque et al, 2006) is a training pro-
totype that responds to military calls for artillery
fire in a virtual reality urban combat environment.
This is a domain in the slot-filling genre, where
there is a preferred protocol for the order in which
information is provided and confirmed. Users are
generally trainees, learning how to do calls for fire,
they are motivated users with some training. The
semantic processing involved tagging each word
with the dialogue act and parameter that it was as-
sociated with (Ai et al, 2007b).
This data set was collected during the develop-
ment of the system in 2006 at Fort Sill, Oklahoma,
during two evaluation sessions from recruited vol-
unteer trainees who performed calls for specific
missions (Robinson et al, 2006). These subjects
used head-mounted microphones rather than the
ASTI simulated radios from later data collection.
SASO-EN (Traum et al, 2008) is a negotiation
training prototype in which two virtual characters
negotiate with a human ?trainee? about moving a
medical clinic. The genre is negotiation and plan-
ning, where the human participant must try to form
a coalition, and the characters reason about utili-
ties of different proposals, as well as causes and
effects. The output of NLU is a frame represen-
tation including both semantic elements, like the-
matic argument structure, and pragmatic elements,
such as addressee and referring expressions. Fur-
ther contextual interpretation is performed by each
of the virtual characters to match the (possibly par-
tial) representation to actions and states in their
task model, resolve other referring expressions,
and determine a full set of dialogue acts (Traum,
2003). Speech was collected at the USC Insti-
tute for Creative Technologies (ICT) during 2006?
2009, mostly from visitors and new hires, who
acted as test subjects.
This dataset has 4K audio files each anno-
tated with one of the 117 different NLU semantic
classes.
4 ASR Performance
We tested each of the Datasets described in Sec-
tion 3.3 with some of the recognizers described
in Section 2. All recognizers were tested on the
Amani, SASO-EN, and Twins domains, and we
also tested a natural language understanding com-
ponent on these data sets (Section 5). For SGT
Blackwell, SGT STAR, and Radiobots, we report
the performance on the same development set used
in Yao et al (2010). For Amani and SASO-EN
(where we also report the NLU performance), we
run a 10-fold cross-validation in which 9 folds
where used to train the NLU and ASR language
model and the 10th was used for testing. For the
Twins dialogue system, we used the same partition
into training, development and testing reported in
Morbini et al (2012) and the results reported here
are from the development set. Due to differences
in training/testing regimens, performance of sys-
tems are only comparable within each domain.
Table 2 summarizes the performance of the var-
ious ASR engines on the evaluation data sets. Per-
formance is measured as Word Error Rate and was
obtained using the NIST SCLITE tool.12
Note that only Otosense-Kaldi in the Twins do-
main had adapted acoustic models. In the remain-
12http://www.itl.nist.gov/iad/mig/tools/
399
Speech recognizer Evaluation data setAmani Radiobots SASO-EN SGT Blackwell SGT Star Twins
Pocketsphinx 39.7 11.8 28.4 51 28.6 81
Apple 28 ? 30.9 ? ? 29
AT&T 29 12.1 16.3 27.3 21.7 28.8
Google 23.8 36.3 20 18 26 20.6
Otosense-Kaldi 33.7 ? 22.1 ? ? 18.7
Table 2: Word Error Rates (%) for the various dialogue systems and ASR systems tested.
ing cases only the language model was adapted.
Looking at the results on the development set re-
ported in Yao et al (2010), we have improvements
in 3 out of 5 domains: Amani (?11.8% Google),
SASO-EN (?11.7% AT&T) and SGT Blackwell
(?13% Google). In Radiobots and SGT Star the
performance achieved with just language model
adaptation, when permitted, is worse: +4.8% and
+1.7% respectively.
We find that there is no single best performing
speech recognizer: results vary greatly between
the evaluation test sets. In 4 of the 6 datasets over-
all, and 2 of the 3 datatests tested with Otosense-
Kaldi, the best performer is a cloud-based ser-
vice (Google or AT&T). There are two datasets
for which a local, fully customizable recognizer
performs better than the cloud-based services. Ra-
diobots, consisting of military calls for artillery
fire, has a fairly limited and very specialized vo-
cabulary, and indeed the two recognizers with cus-
tom language models (Pocketsphinx and AT&T)
perform much better than the non-customizable
recognizer (Google).
The Twins dataset is unique in that for the
Otosense-Kaldi system we custom-trained acous-
tic and language models, while standard WSJ
acoustic models and adapted language models
were used for the other dialogue systems. In
both cases the models were triphone based with
a Linear Discriminant Analysis (LDA) front end,
and Maximum Likelihood Linear Transforma-
tion (MLLT) and Maximum Mutual Information
(MMI) training. This reflects on the very good
performance in the Twins domain, decent perfor-
mance on the SASO-EN domain (reasonable mis-
match of WSJ and SASO-EN) and very degraded
performance in Amani (highly mismatched Amani
and WSJ domains). The observed degradation in
performance is accentuated by the MMI discrim-
inative training on the mismatched-WSJ data. As
with PocketSphinx and Watson, and unlike with
Apple Dictation and Google Speech API, with
Kaldi we fully control experimental conditions
and can guarantee no contamination of the train-
test data.
In summary, our evaluation shows that cus-
tomizable recognizers are useful when the ex-
pected speech is highly specialized, or when sub-
stantial resources are available for tuning the rec-
ognizer.
5 NLU Accuracy & Relation between
ASR and NLU
While the different genres of system have different
types of output for NLU: response text, dialogue
act and parameter tags, speech acts, or semantic
frames, many of them can be coerced into a se-
lection task, in which the NLU selects the right
output from a set of possible outputs. This allows
any multiclass classification algorithm to be used
for NLU. A possible drawback is that for some
inputs, the right output might not be available in
the set considered by the training data, even if it
might easily be constructed from known parts us-
ing a generative approach.
A second issue is that even though we can cast
the problem as multi-class classification, classifi-
cation accuracy is not always the most appropriate
metric of NLU quality. For question-answering
characters, getting an appropriate and relevant re-
ply is more important than picking the exact re-
ply selected by a human domain designer or an-
notator: there might be multiple good answers, or
even the best available answer might not be very
good. For that reason, the question-answering
characters allow an ?off-topic? answer and Error-
return plots (Artstein, 2011) might be necessary
to choose an optimal threshold. For the SASO-EN
system, slot-filler metrics such as precision, recall,
and f-score are more appropriate than frame accu-
400
racy, because some frames may have many slots
in common and few that are different (e.g. just a
different addressee). Nonetheless, we begin our
analysis within this common framework. For sim-
plicity, we start with just three domains: Twins,
Amani, and SASO-EN. SGT STAR and Blackwell
are very similar to Twins in terms of NLU. Ra-
diobots is more challenging to coerce to multiclass
classification.
Conventional wisdom in the speech and lan-
guage processing community is that performance
of ASR and NLU are closely tied: improved
speech recognition leads to better language under-
standing, while deficiencies in speech recognition
cause difficulty in understanding. This conven-
tional wisdom is borne out by decades of experi-
ence with speech and dialogue systems, though we
are not aware of attempts to systematically demon-
strate it. The present study shows that the expected
relation between speech recognition and language
understanding holds for the systems we tested.
Accepted assumptions about the relation be-
tween speech recognition and language under-
standing have been repeatedly challenged. Direct
challenges are typically limited to specific appli-
cations. Wang et al (2003) show that for a slot-
filling NLU, ASR can be specifically tuned to rec-
ognize those words that are relevant to the slot-
filling task, resulting in improved understanding
despite a decrease in performance on overall word
recognition. However, Boros et al (1996) found
that when not optimizing the ASR for the specific
slot filling task there is a nearly linear correlation
between word accuracy and NLU accuracy. Al-
shawi (2003) and Huang and Cox (2006) show that
in call-routing applications the word level can be
dispensed with altogether and calls routed based
on phonetic information alone without noticeable
loss in performance. These challenges suggest that
the speech-language divide is not as clean as the
theory suggests.
To investigate the relation between ASR and
NLU, we ran each ASR output from each of
the 5 recognizers through an understanding com-
ponent to obtain an NLU output (each dataset
had a separate NLU component, which was held
constant for all speech recognizers). ASR and
NLU performance are conventionally measured on
scales of opposite polarity: better performance
shows up as lower word error rates but higher
NLU accuracies. For the correlations we invert the
conventional ASR scale and use word accuracy, so
that higher numbers signify better performance on
both scales.13
Figure 1 shows the results obtained in the 3 di-
alogue systems by the various ASR systems. The
figures plot ASR performance against NLU per-
formance; NLU results on manual transcriptions
are included for comparison. There are too few
data points for the correlations between ASR and
NLU performance to be significant, but the trends
are positive, as expected.
Our experiments lend supporting evidence to
the claim that in general, ASR performance is pos-
itively linked to NLU performance (special cases
notwithstanding). The 3 datasets exhibit posi-
tive correlations between speech recognition and
language understanding performance. Thus, we
claim that the basis of the conventional wisdom
is sound: speech recognition directly affects lan-
guage understanding. This conclusion holds when
the speech recognizer has been optimized to pro-
duce the most accurate transcript, rather than for a
specific NLU.
6 Conclusion and Future Work
We have extended here the ASR system evaluation
published in Yao et al (2010) including some new
cloud based ASR services that achieve very good
performance showing an improvement of around
12%. We also showed that ASR and NLU perfor-
mance are correlated.
One possible avenue of future work is to ex-
tract importance weights for each word from the
learnt NLU models and use these weights to try
to explain those cases that diverge from the corre-
lation between ASR and NLU performance. This
may also give us a better measure than WER for
assessing ASR performance in dialogue systems.
Another avenue of future work involves examin-
ing different types of NLU engines, and different
metrics for the different dialogue system genres,
which, again, may lead to a more relevant assess-
ment of ASR performance.
Acknowledgments
The effort described here has been sponsored by
the U.S. Army. Any opinions, content or informa-
tion presented does not necessarily reflect the posi-
13We define ?accuracy? as 1 minus WER, so this number
can in principle dip below zero if there are more errors than
words.
401
Amani
r = 0.54, df = 3, p = 0.345
Word accuracy (%)
N
LU
ac
cu
ra
cy
(%
)
50 60 70 80 90 100
51
54
57
60
63
66
69
?
?
?
? ?
?
SASO-EN
r = 0.77, df = 3, p = 0.130
Word accuracy (%)
N
LU
ac
cu
ra
cy
(%
)
60 70 80 90 100
30
40
50
60
70
80
90
?
???
?
?
Twins
r = 0.99, df = 3, p = 0.002
Word accuracy (%)
N
LU
ac
cu
ra
cy
(%
)
0 20 40 60 80 100
40
50
60
70
80
90
100
?
?
?
?
?
Figure 1: Relation between ASR and NLU performance (red dots are manual transcriptions)
tion or the policy of the United States Government,
and no official endorsement should be inferred.
References
Priti Aggarwal, Ron Artstein, Jillian Gerten, Athana-
sios Katsamanis, Shrikanth Narayanan, Angela
Nazarian, and David Traum. 2012. The Twins cor-
pus of museum visitor questions. In LREC-2012,
Istanbul, Turkey, May.
Hua Ai, Antoine Raux, Dan Bohus, Maxine Eskenazi,
and Diane Litman. 2007a. Comparing spoken dia-
log corpora collected with recruited subjects versus
real users. In SIGdial 2007.
Hua Ai, Antonio Roque, Anton Leuski, and David
Traum. 2007b. Using information state to improve
dialogue move identification in a spoken dialogue
system. In Proceedings of the 10th Interspeech Con-
ference, Antwerp, Belgium, August.
James F. Allen, George Ferguson, and Amanda Stent.
2001. An architecture for more realistic conversa-
tional systems. In IUI, pages 1?8.
Hiyan Alshawi. 2003. Effective utterance classifica-
tion with unsupervised phonotactic models. In HLT-
NAACL 2003, pages 1?7, Edmonton, Alberta, May.
Apple Inc. 2012. Mac basics: Dictation (Technote
HT5449), November.
R. Artstein, S. Gandhe, J. Gerten, A. Leuski, and
D. Traum. 2009a. Semi-formal evaluation of con-
versational characters. In O. Grumberg, M. Kamin-
ski, S. Katz, and S. Wintner, editors, Languages:
From Formal to Natural. Essays Dedicated to Nis-
sim Francez on the Occasion of His 65th Birthday,
volume 5533 of Lecture Notes in Computer Science,
pages 22?35. Springer, Berlin.
Ron Artstein, Sudeep Gandhe, Michael Rushforth, and
David R. Traum. 2009b. Viability of a simple dia-
logue act scheme for a tactical questioning dialogue
system. In DiaHolmia 2009: Proceedings of the
13th Workshop on the Semantics and Pragmatics of
Dialogue, page 43?50, Stockholm, Sweden, June.
Ron Artstein, Michael Rushforth, Sudeep Gandhe,
David Traum, and MAJ Aram Donigian. 2011.
Limits of simple dialogue acts for tactical question-
ing dialogues. In 7th IJCAI Workshop on Knowl-
edge and Reasoning in Practical Dialogue Systems,
Barcelona, Spain, July.
Ron Artstein. 2011. Error return plots. In 12th SIG-
dial Workshop on Discourse and Dialogue, Port-
land, OR, June.
Linda Bell and Joakim Gustafson. 2003. Child and
adult speaker adaptation during error resolution in a
publicly available spoken dialogue system. In IN-
TERSPEECH 2003.
Dan Bohus and Alexander I. Rudnicky. 2003. Raven-
claw: dialog management using hierarchical task de-
composition and an expectation agenda. In INTER-
SPEECH 2003.
M. Boros, W. Eckert, F. Gallwitz, G. Grz, G. Han-
rieder, and H. Niemann. 1996. Towards understand-
ing spontaneous speech: Word accuracy vs. concept
accuracy. In In Proceedings of (ICSLP 96), pages
1009?1012.
Sudeep Gandhe, Nicolle Whitman, David R. Traum,
and Ron Artstein. 2009. An integrated authoring
tool for tactical questioning dialogue systems. In 6th
Workshop on Knowledge and Reasoning in Practical
Dialogue Systems, Pasadena, California, July.
Sudeep Gandhe, Michael Rushforth, Priti Aggarwal,
and David R. Traum. 2011. Evaluation of
an integrated authoring tool for building advanced
question-answering characters. In Proceedings of
Interspeech-11, Florence, Italy, 08/2011.
Sudeep Gandhe. 2013. Rapid prototyping and evalu-
ation of dialogue systems for virtual humans. Ph.D.
thesis, University of Southern California.
402
Liza Hassel and Eli Hagen. 2006. Adaptation of an
automotive dialogue system to users? expertise and
evaluation of the system. Language Resources and
Evaluation, 40(1):67?85.
Quiang Huang and Stephen Cox. 2006. Task-
independent call-routing. Speech Communication,
48(3?4):374?389.
D. Huggins-Daines, M. Kumar, A. Chan, A.W. Black,
M. Ravishankar, and A.I. Rudnicky. 2006. Pocket-
sphinx: A free, real-time continuous speech recog-
nition system for hand-held devices. In Acoustics,
Speech and Signal Processing, 2006. ICASSP 2006
Proceedings. 2006 IEEE International Conference
on, volume 1, pages I?I.
Kristiina Jokinen and Kari Kanto. 2004. User ex-
pertise modeling and adaptivity in a speech-based
e-mail system. In Donia Scott, Walter Daelemans,
and Marilyn A. Walker, editors, ACL, pages 87?94.
ACL.
Staffan Larsson and David Traum. 2000. Information
state and dialogue management in the TRINDI dia-
logue move engine toolkit. Natural Language En-
gineering, 6:323?340, September. Special Issue on
Spoken Language Dialogue System Engineering.
Anton Leuski and David R. Traum. 2011. NPCEditor:
Creating virtual human dialogue using information
retrieval techniques. AI Magazine, 32:42?56.
Anton Leuski, Brandon Kennedy, Ronakkumar Patel,
and David Traum. 2006a. Asking questions to
limited domain virtual characters: How good does
speech recognition have to be? In 25th Army Sci-
ence Conference.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006b. Building effective ques-
tion answering characters. In Proceedings of the
7th SIGdial Workshop on Discourse and Dialogue,
pages 18?27.
Fabrizio Morbini, Kartik Audhkhasi, Ron Artstein,
Maarten Van Segbroeck, Kenji Sagae, Panayio-
tis S. Georgiou, David R. Traum, and Shrikanth S.
Narayanan. 2012. A reranking approach for recog-
nition and classification of speech input in conversa-
tional dialogue systems. In SLT, pages 49?54. IEEE.
Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Luka?s?
Burget, Ondr?ej Glembek, Nagendra Goel, Mirko
Hannemann, Petr Motl??c?ek, Yanmin Qian, Petr
Schwarz, Jan Silovsky?, Georg Stemmer, and Karel
Vesely?. 2011. The Kaldi speech recognition
toolkit. In IEEE 2011 Workshop on Automatic
Speech Recognition and Understanding, December.
S.M. Robinson, A. Roque, A. Vaswani, D. Traum,
C. Hernandez, and B. Millspaugh. 2006. Evalua-
tion of a spoken dialogue system for virtual reality
call for fire training. In 25th Army Science Confer-
ence, Orlando, Florida, USA.
S. Robinson, D. Traum, M. Ittycheriah, and J. Hen-
derer. 2008. What would you ask a conversational
agent? Observations of human-agent dialogues in
a museum setting. In Proc. of Sixth International
Conference on Language Resources and Evaluation
(LREC), Marrakech, Morocco.
A. Roque, A. Leuski, V. Rangarajan, S. Robinson,
A. Vaswani, S. Narayanan, and D. Traum. 2006.
Radiobot-CFF: A spoken dialogue system for mil-
itary training. In Proc. of Interspeech, Pittsburgh,
Pennsylvania, USA.
W. Swartout, D. Traum, R. Artstein, D. Noren, P. De-
bevec, K. Bronnenkant, J. Williams, A. Leuski,
S. Narayanan, D. Piepol, C. Lane, J. Morie, P. Ag-
garwal, M. Liewer, J. Chiang, J. Gerten, S. Chu,
and K. White. 2010. Ada and Grace: Toward
realistic and engaging virtual museum guides. In
J. Allbeck, N. Badler, T. Bickmore, C. Pelachaud,
and A. Safonova, editors, Intelligent Virtual Agents:
10th International Conference, IVA 2010, Philadel-
phia, PA, USA, September 20?22, 2010 Proceed-
ings, volume 6356 of Lecture Notes in Artificial In-
telligence, pages 286?300. Springer, Heidelberg.
David R. Traum, Stacy Marsella, Jonathan Gratch, Jina
Lee, and Arno Hartholt. 2008. Multi-party, multi-
issue, multi-strategy negotiation for multi-modal
virtual agents. In IVA, pages 117?130.
David Traum, Priti Aggarwal, Ron Artstein, Susan
Foutz, Jillian Gerten, Athanasios Katsamanis, Anton
Leuski, Dan Noren, and William Swartout. 2012.
Ada and grace: Direct interaction with museum
visitors. In The 12th International Conference on
Intelligent Virtual Agents (IVA), Santa Cruz, CA,
September.
David Traum. 2003. Semantics and pragmatics of
questions and answers for dialogue agents. In pro-
ceedings of the International Workshop on Compu-
tational Semantics, pages 380?394.
Ye-Yi Wang, A. Acero, and C. Chelba. 2003. Is
word error rate a good indicator for spoken lan-
guage understanding accuracy. In IEEE Workshop
on Automatic Speech Recognition and Understand-
ing (ASRU ?03), pages 577?582.
Xuchen Yao, Pravin Bhutada, Kallirroi Georgila, Kenji
Sagae, Ron Artstein, and David R. Traum. 2010.
Practical evaluation of speech recognizers for vir-
tual human dialogue systems. In Nicoletta Calzo-
lari, Khalid Choukri, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, Stelios Piperidis, Mike Rosner, and
Daniel Tapias, editors, LREC. European Language
Resources Association.
403
