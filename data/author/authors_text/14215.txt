Proceedings of the ACL 2007 Demo and Poster Sessions, pages 57?60,
Prague, June 2007. c?2007 Association for Computational Linguistics
Support Vector Machines for Query-focused Summarization trained and
evaluated on Pyramid data
Maria Fuentes
TALP Research Center
Universitat Polite`cnica de Catalunya
mfuentes@lsi.upc.edu
Enrique Alfonseca
Computer Science Departament
Universidad Auto?noma de Madrid
Enrique.Alfonseca@gmail.com
Horacio Rodr??guez
TALP Research Center
Universitat Polite`cnica de Catalunya
horacio@lsi.upc.edu
Abstract
This paper presents the use of Support
Vector Machines (SVM) to detect rele-
vant information to be included in a query-
focused summary. Several SVMs are
trained using information from pyramids
of summary content units. Their per-
formance is compared with the best per-
forming systems in DUC-2005, using both
ROUGE and autoPan, an automatic scor-
ing method for pyramid evaluation.
1 Introduction
Multi-Document Summarization (MDS) is the task
of condensing the most relevant information from
several documents in a single one. In terms of the
DUC contests1, a query-focused summary has to
provide a ?brief, well-organized, fluent answer to a
need for information?, described by a short query
(two or three sentences). DUC participants have to
synthesize 250-word sized summaries for fifty sets
of 25-50 documents in answer to some queries.
In previous DUC contests, from 2001 to 2004, the
manual evaluation was based on a comparison with
a single human-written model. Much information
in the evaluated summaries (both human and auto-
matic) was marked as ?related to the topic, but not
directly expressed in the model summary?. Ideally,
this relevant information should be scored during the
evaluation. The pyramid method (Nenkova and Pas-
sonneau, 2004) addresses the problem by using mul-
tiple human summaries to create a gold-standard,
1http://www-nlpir.nist.gov/projects/duc/
and by exploiting the frequency of information in
the human summaries in order to assign importance
to different facts. However, the pyramid method re-
quires to manually matching fragments of automatic
summaries (peers) to the Semantic Content Units
(SCUs) in the pyramids. AutoPan (Fuentes et al,
2005), a proposal to automate this matching process,
and ROUGE are the evaluation metrics used.
As proposed by Copeck and Szpakowicz (2005),
the availability of human-annotated pyramids con-
stitutes a gold-standard that can be exploited in or-
der to train extraction models for the summary au-
tomatic construction. This paper describes several
models trained from the information in the DUC-
2006 manual pyramid annotations using Support
Vector Machines (SVM). The evaluation, performed
on the DUC-2005 data, has allowed us to discover
the best configuration for training the SVMs.
One of the first applications of supervised Ma-
chine Learning techniques in summarization was in
Single-Document Summarization (Ishikawa et al,
2002). Hirao et al (2003) used a similar approach
for MDS. Fisher and Roark (2006)?s MDS system is
based on perceptrons trained on previous DUC data.
2 Approach
Following the work of Hirao et al (2003) and
Kazawa et al (2002), we propose to train SVMs
for ranking the candidate sentences in order of rele-
vance. To create the training corpus, we have used
the DUC-2006 dataset, including topic descriptions,
document clusters, peer and manual summaries, and
pyramid evaluations as annotated during the DUC-
2006 manual evaluation. From all these data, a set
57
of relevant sentences is extracted in the following
way: first, the sentences in the original documents
are matched with the sentences in the summaries
(Copeck and Szpakowicz, 2005). Next, all docu-
ment sentences that matched a summary sentence
containing at least one SCU are extracted. Note that
the sentences from the original documents that are
not extracted in this way could either be positive (i.e.
contain relevant data) or negative (i.e. irrelevant for
the summary), so they are not yet labeled. Finally,
an SVM is trained, as follows, on the annotated data.
Linguistic preprocessing The documents from
each cluster are preprocessed using a pipe of general
purpose processors performing tokenization, POS
tagging, lemmatization, fine grained Named Enti-
ties (NE)s Recognition and Classification, anaphora
resolution, syntactic parsing, semantic labeling (us-
ing WordNet synsets), discourse marker annotation,
and semantic analysis. The same tools are used for
the linguistic processing of the query. Using these
data, a semantic representation of the sentence is
produced, that we call environment. It is a semantic-
network-like representation of the semantic units
(nodes) and the semantic relations (edges) holding
between them. This representation will be used to
compute the (Fuentes et al, 2006) lexico-semantic
measures between sentences.
Collection of positive instances As indicated be-
fore, every sentence from the original documents
matching a summary sentence that contains at least
one SCU is considered a positive example. We have
used a set of features that can be classified into three
groups: those extracted from the sentences, those
that capture a similarity metric between the sentence
and the topic description (query), and those that try
to relate the cohesion between a sentence and all the
other sentences in the same document or collection.
The attributes collected from the sentences are:
? The position of the sentence in its document.
? The number of sentences in the document.
? The number of sentences in the cluster.
? Three binary attributes indicating whether the
sentence contains positive, negative and neutral
discourse markers, respectively. For instance,
what?s more is positive, while for example and
incidentally indicate lack of relevance.
? Two binary attributes indicating whether
the sentence contains right-directed discourse
markers (that affect the relevance of fragment
after the marker, e.g. first of all), or discourse
markers affecting both sides, e.g. that?s why.
? Several boolean features to mark whether the
sentence starts with or contains a particular
word or part-of-speech tag.
? The total number of NEs included in the sen-
tence, and the number of NEs of each kind.
? SumBasic score (Nenkova and Vanderwende,
2005) is originally an iterative procedure that
updates word probabilities as sentences are se-
lected for the summary. In our case, word prob-
abilities are estimated either using only the set
of words in the current document, or using all
the words in the cluster.
The attributes that depend on the query are:
? Word-stem overlapping with the query.
? Three boolean features indicating whether the
sentence contains a subject, object or indirect
object dependency in common with the query.
? Overlapping between the environment predi-
cates in the sentence and those in the query.
? Two similarity metrics calculated by expanding
the query words using Google.
? SumFocus score (Vanderwende et al, 2006).
The cohesion-based attributes 2 are:
? Word-stem overlapping between this sentence
and the other sentences in the same document.
? Word-stem overlapping between this sentence
and the other sentences in the same cluster.
? Synset overlapping between this sentence and
the other sentences in the same document.
? Synset overlapping with other sentences in the
same collection.
Model training In order to train a traditional
SVM, both positive and negative examples are nec-
essary. From the pyramid data we are able to iden-
tify positive examples, but there is not enough ev-
idence to classify the remaining sentences as posi-
tive or negative. Although One-Class Support Vec-
tor Machine (OSVM) (Manevitz and Yousef, 2001)
can learn from just positive examples, according to
Yu et al (2002) they are prone to underfitting and
overfitting when data is scant (which happens in
2The mean, median, standard deviation and histogram of the
overlapping distribution are calculated and included as features.
58
this case), and a simple iterative procedure called
Mapping-Convergence (MC) algorithm can greatly
outperform OSVM (see the pseudocode in Figure 1).
Input: positive examples, POS, unlabeled examples U
Output: hypothesis at each iteration h?1, h?2, ..., h?k
1. Train h to identify ?strong negatives? in U :
N1 := examples from U classified as negative by h
P1 := examples from U classified as positive by h
2. Set NEG := ? and i := 1
3. Loop until Ni = ?,
3.1. NEG := NEG ? Ni
3.2. Train h?i from POS and NEG
3.3. Classify Pi by h?i:
Ni+1 = examples from Pi classified as negative
Pi+1 = examples from Pi classified as positive
5. Return {h?1, h?2, ..., h?k}
Figure 1: Mapping-Convergence algorithm.
The MC starts by identifying a small set of in-
stances that are very dissimilar to the positive exam-
ples, called strong negatives. Next, at each iteration,
a new SVM h?i is trained using the original positive
examples, and the negative examples found so far.
The set of negative instances is then extended with
the unlabeled instances classified as negative by h?i.
The following settings have been tried:
? The set of positive examples has been collected
either by matching document sentences to peer
summary sentences (Copeck and Szpakowicz,
2005) or by matching document sentences to
manual summary sentences.
? The initial set of strong negative examples for
the MC algorithm has been either built auto-
matically as described by Yu et al (2002), or
built by choosing manually, for each cluster, the
two or three automatic summaries with lowest
manual pyramid scores.
? Several SVM kernel functions have been tried.
For training, there were 6601 sentences from the
original documents, out of which around 120 were
negative examples and either around 100 or 500 pos-
itive examples, depending on whether the document
sentences had been matched to the manual or the
peer summaries. The rest were initially unlabeled.
Summary generation Given a query and a set of
documents, the trained SVMs are used to rank sen-
tences. The top ranked ones are checked to avoid re-
dundancy using a percentage overlapping measure.
3 Evaluation Framework
The SVMs, trained on DUC-2006 data, have been
tested on the DUC-2005 corpus, using the 20 clus-
ters manually evaluated with the pyramid method.
The sentence features were computed as described
before. Finally, the performance of each system
has been evaluated automatically using two differ-
ent measures: ROUGE and autoPan.
ROUGE, the automatic procedure used in DUC,
is based on n-gram co-occurrences. Both ROUGE-2
(henceforward R-2) and ROUGE-SU4 (R-SU4) has
been used to rank automatic summaries.
AutoPan is a procedure for automatically match-
ing fragments of text summaries to SCUs in pyra-
mids, in the following way: first, the text in the
SCU label and all its contributors is stemmed and
stop words are removed, obtaining a set of stem
vectors for each SCU. The system summary text is
also stemmed and freed from stop words. Next, a
search for non-overlapping windows of text which
can match SCUs is carried. Each match is scored
taking into account the score of the SCU as well as
the number of matching stems. The solution which
globally maximizes the sum of scores of all matches
is found using dynamic programming techniques.
According to Fuentes et al (2005), autoPan scores
are highly correlated to the manual pyramid scores.
Furthermore, autoPan also correlates well with man-
ual responsiveness and both ROUGE metrics.3
3.1 Results
Positive Strong neg. R-2 R-SU4 autoPan
peer pyramid scores 0.071 0.131 0.072
(Yu et al, 2002) 0.036 0.089 0.024
manual pyramid scores 0.025 0.075 0.024
(Yu et al, 2002) 0.018 0.063 0.009
Table 1: ROUGE and autoPan results using different SVMs.
Table 1 shows the results obtained, from which
some trends can be found: firstly, the SVMs
trained using the set of positive examples obtained
from peer summaries consistently outperform SVMs
trained using the examples obtained from the man-
ual summaries. This may be due to the fact that the
3In DUC-2005 pyramids were created using 7 manual sum-
maries, while in DUC-2006 only 4 were used. For that reason,
better correlations are obtained in DUC-2005 data.
59
number of positive examples is much higher in the
first case (on average 48,9 vs. 12,75 examples per
cluster). Secondly, generating automatically a set
with seed negative examples for the M-C algorithm,
as indicated by Yu et al (2002), usually performs
worse than choosing the strong negative examples
from the SCU annotation. This may be due to the
fact that its quality is better, even though the amount
of seed negative examples is one order of magnitude
smaller in this case (11.9 examples in average). Fi-
nally, the best results are obtained when using a RBF
kernel, while previous summarization work (Hirao
et al, 2003) uses polynomial kernels.
The proposed system attains an autoPan value of
0.072, while the best DUC-2005 one (Daume? III and
Marcu, 2005) obtains an autoPan of 0.081. The dif-
ference is not statistically significant. (Daume? III
and Marcu, 2005) system also scored highest in re-
sponsiveness (manually evaluated at NIST).
However, concerning ROUGE measures, the best
participant (Ye et al, 2005) has an R-2 score of
0.078 (confidence interval [0.073?0.080]) and an R-
SU4 score of 0.139 [0.135?0.142], when evaluated
on the 20 clusters used here. The proposed sys-
tem again is comparable to the best system in DUC-
2005 in terms of responsiveness, Daume? III and
Marcu (2005)?s R-2 score was 0.071 [0.067?0.074]
and R-SU4 was 0.126 [0.123?0.129] and it is better
than the DUC-2005 Fisher and Roark supervised ap-
proach with an R-2 of 0.066 and an R-SU4 of 0.122.
4 Conclusions and future work
The pyramid annotations are a valuable source of
information for training automatically text sum-
marization systems using Machine Learning tech-
niques. We explore different possibilities for apply-
ing them in training SVMs to rank sentences in order
of relevance to the query. Structural, cohesion-based
and query-dependent features are used for training.
The experiments have provided some insights on
which can be the best way to exploit the annota-
tions. Obtaining the positive examples from the an-
notations of the peer summaries is probably better
because most of the peer systems are extract-based,
while the manual ones are abstract-based. Also, us-
ing a very small set of strong negative example seeds
seems to perform better than choosing them auto-
matically with Yu et al (2002)?s procedure.
In the future we plan to include features from ad-
jacent sentences (Fisher and Roark, 2006) and use
rouge scores to initially select negative examples.
Acknowledgments
Work partially funded by the CHIL project, IST-2004506969.
References
T. Copeck and S. Szpakowicz. 2005. Leveraging pyramids. In
Proc. DUC-2005, Vancouver, Canada.
Hal Daume? III and Daniel Marcu. 2005. Bayesian summariza-
tion at DUC and a suggestion for extrinsic evaluation. In
Proc. DUC-2005, Vancouver, Canada.
S. Fisher and B. Roark. 2006. Query-focused summarization
by supervised sentence ranking and skewed word distribu-
tions. In Proc. DUC-2006, New York, USA.
M. Fuentes, E. Gonza`lez, D. Ferre?s, and H. Rodr??guez. 2005.
QASUM-TALP at DUC 2005 automatically evaluated with
the pyramid based metric autopan. In Proc. DUC-2005.
M. Fuentes, H. Rodr??guez, J. Turmo, and D. Ferre?s. 2006.
FEMsum at DUC 2006: Semantic-based approach integrated
in a flexible eclectic multitask summarizer architecture. In
Proc. DUC-2006, New York, USA.
T. Hirao, J. Suzuki, H. Isozaki, and E. Maeda. 2003. Ntt?s
multiple document summarization system for DUC2003. In
Proc. DUC-2003.
K. Ishikawa, S. Ando, S. Doi, and A. Okumura. 2002. Train-
able automatic text summarization using segmentation of
sentence. In Proc. 2002 NTCIR 3 TSC workshop.
H. Kazawa, T. Hirao, and E. Maeda. 2002. Ranking SVM and
its application to sentence selection. In Proc. 2002 Workshop
on Information-Based Induction Science (IBIS-2002).
L.M. Manevitz and M. Yousef. 2001. One-class SVM for docu-
ment classification. Journal of Machine Learning Research.
A. Nenkova and R. Passonneau. 2004. Evaluating content se-
lection in summarization: The pyramid method. In Proc.
HLT/NAACL 2004, Boston, USA.
A. Nenkova and L. Vanderwende. 2005. The impact of
frequency on summarization. Technical Report MSR-TR-
2005-101, Microsoft Research.
L. Vanderwende, H. Suzuki, and C. Brockett. 2006. Mi-
crosoft research at DUC 2006: Task-focused summarization
with sentence simplification and lexical expansion. In Proc.
DUC-2006, New York, USA.
S. Ye, L. Qiu, and T.S. Chua. 2005. NUS at DUC 2005: Under-
standing documents via concept links. In Proc. DUC-2005.
H. Yu, J. Han, and K. C-C. Chang. 2002. PEBL: Positive
example-based learning for web page classification using
SVM. In Proc. ACM SIGKDD International Conference on
Knowledge Discovery in Databases (KDD02), New York.
60
In: Proceedings of CoNLL-2000 and LLL-2000, pages 115-118, Lisbon, Portugal, 2000. 
Learning IE Rules for a Set of Related Concepts 
J. Turmo and H. Rodr iguez  
TALP Research Center. Universitat Polit~cnica de Catalunya 
Jordi Girona Salgado, 1-3 
E-08034 Barcelona - Spain 
1 In t roduct ion  
The growing availability of on-line text has led 
to an increase in the use of automatic knowledge 
acquisition approaches from textual data. In 
fact, a number of Information Extraction (IE) 
systems has emerged in the past few years in 
relation to the MUC conferences 1. The aim of 
an IE system consists in automatically extract- 
ing pieces of information from text, being this 
information relevant for a set of prescribed con- 
cepts (scenario). One of the main drawbacks of 
applying IE systems is the high cost involved in 
manually adapting them to new domains and 
text styles. 
In recent years, a variety of Machine Learn- 
ing (ML) techniques has been used to improve 
the portability of IE systems to new domains, 
as in SRV (Freitag, 1998), RAPIER (Califf 
and Mooney, 1997), LIEP (Huffman, 1996), 
CRYSTAL (Soderland et al, 1995) and WHISK 
(Soderland, 1999) . However, some drawbacks 
remain in the portability of these systems: a) 
existing systems generally depend on the sup- 
ported text style and learn IE-rules either for 
structured texts, semi-structured texts or free 
text , b) IE systems are mostly single-concept 
learning systems, c) consequently, an extrac- 
tor (e.g., a rule set) is learned for each con- 
cept within the scenario in an independent man- 
ner, d) the order of execution of the learners 
is set manually, and so are the scheduling and 
way of combination of the resulting extractors, 
and e) focusing on the training data, the size of 
available training corpora can be inadequate to 
accurately learn extractors for all the concepts 
within the scenario 2.
1 http://www.muc.saic.com/ 
~This is so when dealing with some combinations of
text style and domain. 
This paper describes EVIUS, a multi-concept 
learning system for free text that follows a 
multi-strategy constructive learning approach 
(MCL) (Michalshi, 1993) and supports insuffi- 
cient amounts of training corpora. EVIUS is 
a component of a multilingual IE system, M- 
TURBIO (Turmo et al, 1999). 
2 EV IUS .  Learn ing  ru le  sets  for a 
set  o f  re la ted  concepts  
The input of EVIUS is both a partially-parsed 
semantically-tagged 3 training corpus and a de- 
scription of the desired target structure. This 
description is provided as a set of concepts C 
related to a set of asymmetric binary relations, 
T~. 
In order to learn set S of IE rule sets for the 
whole C, EVIUS uses an MCL approach inte- 
grating constructive l arning, closed-loop learn- 
ing and deductive restructuring (Ko, 1998). 
In this multi-concept situation, the system 
determines which concepts to learn and, later, 
incrementally updates S. This can be relatively 
straightforward when using knowledge about 
the target structure in a closed-loop learning 
approach. Starting with C, EVIUS reduces et 
b/of  unlearned concepts iteratively by selecting 
subset P C/g formed by the primitive concepts 
in/.4 and learning a rule set for each c E P 4 
For instance, the single colour scenario 5 in fig- 
3With EuroWordNet (http://www.hum.uva.nl/-ewn/) 
synsets. No attempt has been made to disambiguate 
such tags. 
4No cyclic scenarios are allowed so that a topological 
sort of C is possible, which starts with a set of primitive 
concepts. 
5Our testing domain is mycology. Texts consists of 
Spanish descriptions ofspecimens. There is a rich variety 
of colour descriptions including basic colours, intervals, 
changes, etc. 
115 
ure 1 is provided to learn from instances of the 
following three related concepts: colour, such 
as in instance "azul ligeramente claro" (slightly 
pale blue), colour_interval, as in "entre rosa 
y rojo sangre" (between pink and blood red), 
and to_change, as in "rojo vira a marr6n" (red 
changes to brown). 
Initially, Lt = C = { colour, colour_interval, 
to_change}. Then, EVIUS calculates 
7 9 ={colour} and once a rule set has been 
learned for colour, the new L/={colour_interval, 
to_change} is studied identifying 79 = L/. 
to to 
from from 
Figure 1: A single scenario for the colour do- 
main 
In order to learn a rule set for a concept, 
EVIUS uses the relational learning method ex- 
plained in section 3, and defines the learn- 
ing space by means of a dynamic predicate 
model. As a pre-process of the system, the 
training corpus is translated into predicates 
using the following initial predicate model: 
a) attributive meta-predicates: pos_X(A), 
isa_X(A), has_hypernym_X(A), word_X(A) 
and lemma_X(A), where X is instantiated with 
closed categories, b) relational meta-predicates: 
distance_le._X(A,B), stating that there are X 
terminal nodes, at most, between A and B, and 
c) relational predicates: ancestor(A,B), where B 
is the syntactic ancestor of A, and brother(A,B), 
where B is the right brother node of A sharing 
the syntactic ancestor. 
Once a rule set for concept c is learned, 
new examples are added for further learning by 
means of a deductive restructuring approach: 
training examples are reduced to generate a
more compact and useful knowledge of the 
learned concept. This is achieved by using 
the induced rule set and a syntactico-semantic 
transformational grammar. Further to all this, 
a new predicate isa_c is added to the model. 
For instance, in figure 2 6 , the Spanish sen- 
tence "su color rojo vira a marrSn oscuro" 
(its red colour changes to dark brown) has 
6Which is presented here as a partially-parsed tree 
for simplicity. 
S (n12) 
spec n a v prep/ n a 
sucolorro~vira {lmarrdnloscurc ~ } 
(nl) (n2) (n3) (n4)(n5)~(n6) . (n7) /  
( n ~ e d u c t i o n  
spec n a v prep/( gnom . \  
~ ' r a  a marr6n oscur~ ) 
(nl) (n2) (n3) (n4) (n5)k _ ~  j 
Figure 2: Restructuring training examples 
two examples of colour, n3 and n6+n7, be- 
ing these "rojo" (red) and "marr6n'+"oscuro" 
(dark brown). No reduction is required by the 
former. However, the latter example is reduced 
to node n6'. As a consequence, two new at- 
tributes are added to the model: isa_colour(n3) 
and isa_colour(n6'). This new knowledge will 
be used to learn the concepts to_change and 
colour_interval. 
3 Ru le  set learn ing  
EVIUS uses FOIL (First-order Induction Learn- 
ing) (Quinlan, 1990) to build an initial rule set 
7~0 from a set of positive and negative xamples. 
Positive examples C+ can be selected using a 
friendly environment either as: 
? text relations: c(A:,A2) where both A: and 
A2 are terminal nodes that exactly delimit 
a text value for c. For instance, both text 
relations colour(n3,n3) or colour(n6,nT) in 
figure 2, or as: 
? ontology relations: c(A:,A2,...,An) where 
all Ai are terminal nodes which are in- 
stances of already learned concepts related 
to c in the scenario. For instance, the on- 
tology relation to_change(n3,n6') 7, in the 
same figure, means that the colour repre- 
sented by instance n3 changes to that rep- 
resented by n6'. 
Negative examples $ -  are automatically se- 
lected as explained in section 3.1. 
7Note that, after the deductive restructuring step, 
both n3 and n6' are instances of the concept colour. 
116 
If any uncovered examples et, g~-, remains 
after FOIL's performance, this is due to the lack 
of sufficient examples. Thus, the system tries 
to improve recall by growing set g+ with arti- 
ficial examples (pseudo-examples), as explained 
in 3.2. A new execution of FOIL is done by 
using the new g+. The resulting rule set 7~ 
is combined with T~0 in order to create 7?1 by 
appending the new rules from T?~ to 7?0. Conse- 
quently, the recall value of 7~1 is forced to be at 
least equal to that of 7~0, although the accuracy 
can decrease. A better method seems to be the 
merging of rules from 7~ and TO0 by studying 
empirical subsumptions. This last combination 
allows to create more compact and accurate rule 
sets. 
EVIUS uses an incremental learning approach 
to learn rule sets for each concept. This is done 
by iterating the process above while uncovered 
examples remain and the F1 score increment 
(AF1) is greater than pre-defined constant a: 
select g+ and generate g -  
7~0 = FOIL(g+,g -)  
$u + = uncover ed_ f r om ( 7~o ) 
= (7?o) 
while $u + ~ 0 and AF1 > a do 
g+ = g+ U pseudo-examples($u +) 
T?~ = FOIL(E+,g -) 
T~i+ l = combine_rules(7~i,T?~) 
gu + = uncovered_f rom( TQ+ l ) 
= E l (h i+ l )  - E l (h i )  
endwhile 
if AF1 > a then return "~i+1 
else return 7~i 
endi/ 
3.1 Generat ing  re levant  negat ive  
examples  
Negative examples can be defined as any com- 
bination of terminal nodes out of g+. However, 
this approach produces an extremely large num- 
ber of examples, out of which only a small sub- 
set is relevant o learn the concept. Related to 
this, (Freitag, 1998) uses words to learn only 
slot rules (learned from text-relation examples) 
, selecting as negative those non-positive word 
pairs that define a string as neither longer than 
the maximum length in positive examples, nor 
shorter than the minimum. 
A more general approach is adopted to define 
the distance between possible examples in the 
learning Space, applying a clustering method us- 
ing positive examples as medoids s. The N near- 
est non-positive examples to each medoid can be 
selected as negative ones. Distance, in our case, 
must be defined as multidimensional due to the 
typology of occurring features. It is relatively 
easy to define distances between examples for 
word_X and lemma_X predicates, being 1 when 
X values are equal, and 0 otherwise. For isa_X 
predicates, the minimum of all possible concep- 
tual distances (Agirre and Rigau, 1995) between 
X values in EWN has been used. Greater dif- 
ficulty is encountered when defining a distance 
from a morpho-syntactic point of view (e.g., a 
pronoun seems to be closer to a noun than a 
verb). In (Turmo et al, 1999), the concept of 
5-set has been presented as a syntactic relation 
generalization, and a distance measure has been 
based on this concept. 
3.2 Creat ing  pseudo-examples  
A method has been used inspired by the gen- 
eration of convex pseudo data (Breiman, 1998), 
in which a similar process to gene-combination 
in genetic algorithms is used. 
For each positive example c(A1,. . .  ,An) 9 of 
concept c to be dealt with, an attribute vector 
is defined as 
( word--X Bl ,. . . ,word._X B~ , lemma-X sl , . . . ,  
lemma_X B~ ,sem-X B1 ,... ,sem_X B~ ,context) 
where B1, . . . ,  Bn are the unrepeated terminal 
nodes from A1, . . . ,  An, context is the set of all 
predicates subsumed by the syntactico-semantic 
structure between the nearest positive exam- 
ple on the left and the nearest one on the 
right, and sem_XB~ is the list of isa_X and 
has_hypernym_X predicates for Bi. 
Then, for each example uncovered by the rule 
set learned by FOIL, a set of pseudo-examples is 
generated. A pseudo-example is built by com- 
bining both the uncovered example vector and 
a randomly selected covered one. This is done 
as follows: for each dimension, one of both pos- 
sible values is randomly selected as value for the 
pseudo-example. 
SA medoid is an actual data point representing a clus- 
ter. 
9As defined in section 3. 
117 
T. Set* $+ 
150 105 
25o 206 
35o 270 
45o 328 
55o 398 
Reca l l \ ]P rec .  F1 
56.86 100 0.725 
62.74 98.45 0.766 
73.53 97.40 0.838 
75.49 98.72 0.856 
75.49 98.7210.856 
Table 1: Results for the colour concept for dif- 
ferent training set sizes (* subscript 0 means 
only one FOIL iteration) 
4 Eva luat ion  
EVIUS has been tested on the mycological do- 
main. A set of 68 Spanish mycological docu- 
ments (covering 9800 words corresponding to 
1360 lemmas) has been used. 13 of them have 
been kept for testing and the others for train- 
ing. The target ontology consisted of 14 con- 
cepts and 24 relations. 
Several experiments have been carried out 
with different raining sets. Results of the initial 
rule set for the colour concept 1? are presented 
in table 1. 
Out of 34 in the 350 initial rule set, one of the 
most relevant learned rules is11: 
Col our ( A, B ) :-has_h ypern ym_OOO17586n ( B ) , 
has_hypernym_O3464624n (A), brother (A, B). 
Table 2 shows the results of adding pseudo- 
examples to the 35012 training set and using the 
algorithm in section 3. This was tested with 
a = 0.01 (two iterations are enough, 351 and 
352) and 5 pseudo-examples for each uncovered 
case. The algorithm returns the rule set pro- 
duced in the first iteration due to the fact that 
~F1T13> 0.01 between the first and the sec- 
ond iterations. Higher results can be generated 
when using lower values for a. 
Although no direct comparison with other 
systems is possible due to the domain and lan- 
guage used, our results can be considered state- 
1?This concept appears to be the most difficult to be 
learned. 
11A chromatic colour (03464624n) that is the left syn- 
tactic brother of an attribute (00017586n) such as lumi- 
nosity or another chromatic colour. 
12This size has been selected to allow a better com- 
parison with the results in table 1. 
laF1T means the F1 value for training sets 
T. Set E + F i r  Recall Prec. F1 
351 415 0.981 76.47 97.50 0.857 
352 465 0.987 79.41 97.50 0.875 
Table 2: Results from adding pseudo-examples 
to the initial training set with 35 documents. 
of-the-art regarding similar MUC competition 
tasks. 
Re ferences  
Eneko Agirre and German Rigau. 1995. A Proposal 
for Word Sense Disambiguation using Concep- 
tual Distance. In Proceedings of the International 
Conference RANLP, Tzigov Chark, Bulgaria. 
L. Breiman. 1998. Arcing Classifiers. The Annals 
of Statistics, 26(3):801-849. 
M.E. Califf and R. Mooney. 1997. Relational learn- 
ing of pattern-match rules for information extrac- 
tion. In Workshop on Natural Language Learning, 
pages 9-15. ACL. 
D. Freitag. 1998. Machine Learning for Informa- 
tion Extraction in Informal Domains. Ph.D. the- 
sis, Computer Science Department. Carnegie Mel- 
lon University. 
S. Huffman. 1996. Learning information extraction 
patterns from examples. In S. Wermter, E. Riloff, 
and G. Sheller, editors, Connectionist, statistical 
and symbolic approaches to learning for natural 
language processing. Springer-Verlag. 
H. Ko. 1998. Empirical assembly sequence planning: 
A multistrategy constructive l arning approach. 
In I. Bratko R. S. Michalsky and M. Kubat, ed- 
itors, Machine Learning and Data Mining. John 
Wiley & Sons LTD. 
R.S. Michalshi. 1993. Towards a unified theory of 
learning: Multistrategy task-adaptive l arning. 
In B.G. Buchanan and D. Wilkins, editors, Read- 
ings in Knowledge Acquisition and Learning. Mor- 
gan Kauffman. 
J.R. Quinlan. 1990. Learning logical definitions 
from relations. Machine Learning, 5:239-266. 
S. Soderland, D. Fisher, J. Aseltine, and W. Lehn- 
ert. 1995. Crystal: Inducing a conceptual dictio- 
nary. In XIV International Joint Conference on 
Artificial Intelligence, pages 1314-1321. 
S. Soderland. 1999. Learning information extraction 
rules for semi-structured and free text. Machine 
Learning, 34:233-272. 
J. Turmo, N. Catalk, and H. Rodrlguez. 1999. An 
adaptable i  system to new domains. Applied In- 
telligence, 10(2/3):225-246. 
118 
Semiautomatic creation of taxonomies
Javier Farreres and Horacio Rodr?guez
farreres@lsi.upc.es and horacio@lsi.upc.es
Department of Computer Languages and Systems
Universitat Polit?cnica de Catalunya
Karina Gibert
karina@eio.upc.es
Department of Statistics and Operations Research
Universitat Polit?cnica de Catalunya
Abstract
In this paper we face the automatic con-
struction of a lexical taxonomy for the
Spanish language using as input a taxon-
omy of English (WordNet)1 and a set of
bilingual (English/Spanish) resources.
Although applied to Spanish/English our
method claims to be general enough to be
applied in the cases a skeletal taxonomy
already exists and we dispose of methods
for mapping items to this taxonomy with
known confidence scores.
1 Introduction
The automatic construction of accurate tax-
onomies from sets of incomplete, partially
overlapping knowledge sources with different
coverage/confidence characteristics has been object
of interest for many researchers (Bisson et al, 2000;
Faatz et al, 2001; Mihalcea and Moldovan, 2001).
The case of lexical taxonomies is specially chal-
lenging because of their huge size that advises
against a manual construction.
Using bilingual dictionaries for mapping words or
senses of a language to English counterparts is not
new (Okumura and Hovy, 1994; Asanoma, 2001).
In fact the work presented here can be considered
an extension of (Atserias et al, 1997; Farreres et al,
1998) that was the base of the approach to building
the Spanish WordNet within EuroWordNet2 project.
From then within our group several efforts have
been devoted to:
1http://www.cogsci.princeton.edu/~wn/
2http://www.hum.uva.nl/~ewn/
1. Validating and debugging the content of the
most important (higher levels in the hierarchy)
and less confident (lesser scoring) synsets.
2. Extending wordnet in several thematic do-
mains.
Our approach is based on the use of WN structure as
a skeleton where words of the language to be studied
can be placed3. It must be pointed out that this work
is centered only in the nominal part of WN.
WN is a wide-coverage lexico-conceptual taxon-
omy of English. Its units (synsets) group together a
set of words (variants) related with a loose form of
synonymy. Synsets can be related by several seman-
tic relations, being hypernymy/hyponymy the most
important.
2 Extracting candidate tuples
The core of the original method was the extraction of
a huge amount of Spanish/English word pairs, Hbil,
from both parts of a bilingual dictionary4 .
Then 17 automatic methods were constructed
that, from Hbil, generated 17 sets of connections be-
tween Spanish words and WN synsets. These meth-
ods followed different criteria of pairing in such a
way that the resulting sets presented diverse degrees
of overlapping and different quality degrees mea-
sured in terms of precision and coverage. For a com-
plete explanation of the methods, see (Atserias et al,
1997). The 17 automatic methods can be grouped as
shown below:
3minor changes on semantic relationships are considered
too.
4VOX/Harrap?s Esencial Diccionario Espa?ol/Ingl?s, In-
gl?s/Espa?ol. Biblograf S.A. Barcelona, 1992.
  methods 1, 2, 3, 4, 5, 6, 7, 8: Connect Spanish
words to WN synsets taking into account the
multiplicity of the connection (1:1, 1:N, M:1,
M:N) and the polysemy of English words in
WN (mono, poly).
  methods 9, 10, 11, 12: Correspond to different
cases in which having Spanish word more than
one translation, the respective translations are
linked by taxonomic relations in WN.
  methods 13, 14, 15: They take profit of the
semantic relations in WN using it as a seman-
tic space for measuring conceptual distance be-
tween different elements (English translations
of Spanish words cooccurring in definitions of
a head-word entry, English translations of a
head-word and its genus). See (Agirre and
Rigau, 1995).
  method 16: It generates connections taking
profit of the coincidence of English words from
the same translation in the same synset.
  method 17: It generates connections taking
profit of thematic tags in the bilingual dictio-
nary.
Each of these methods generates a list of pairs of the
kind synset-word. Each connection can be generated
by multiple methods.
3 Selecting the most appropriate tuples:
first approach
In order to evaluate the quality of the different meth-
ods, a random stratified sample of around 2,500
links covering all the sets was extracted and veri-
fied manually. The results of this evaluation are pre-
sented in 3rd column of table 1.
All of the methods that gave a percentage of cor-
rectness of 85% or better were selected to build what
was called Subset1. This gave as a result an initial
set of 10,982 connections between Spanish words
and WN synsets from a potential volume of 66,609.
Based on the supposition that, if a connection is
a joint solution of two methods, its probability to be
correct would be higher, and then the joint evalua-
tion of both methods will be higher than that of each
Table 1: First and second method evaluations
method set volume accuracy reevaluation
1 3704 92% 91.25%
2 935 89% 86.40%
3 1888 89% 74.85%
4 2690 85% 69.27%
5 5123 80% 91.62%
6 1450 75% 85.28%
7 11687 58% 92.50%
8 40299 61% 85.06%
9 1256 79% 82.26%
10 1432 51% 76.05%
11 2202 57% 77.40%
12 1846 60% 76.43%
13 23829 56% 87.44%
14 24740 61% 87.98%
15 4567 75% 76.61%
16 3164 85% 86.28%
17 510 78% 89.76%
method separately, and having checked the high de-
gree of intersection between solutions of the differ-
ent methods, we decided to add to the previous set of
connections (Subset1) those connections occurring
as simultaneous solution of two of the methods not
considered in the previous phase, increasing cover-
age without loosing precision.
The links of those methods not selected for Sub-
set1 were crossed, and the volume of each intersec-
tion set was calculated. The percentage of correct
solutions in each set can be understood as an estima-
tion of the probability that one connection proposed
by some pair of methods is correct. The solutions of
certain pairs of methods presented accuracies equal
or above 85%. But some didn?t have a sample sig-
nificant enough to ensure a reliable estimation of the
probability of producing correct solutions.
In this second step of the sampling we proceeded
to complete the manual evaluation of those groups
that seem promising. The results are summarized in
table 2. The table identified 14 intersections with an
accuracy equal or above 85% (in bold). All connec-
tions belonging to those cells were selected to form
Subset2 which was formed, then, by:
  All links proposed by methods m  -m  ,m 
(10,982 connections) which had been accepted
Table 2: Intersections of methods 2 by 2. Size an accuracy of each intersection is presented
vol(%) m13 m14 m15 m12 m10 m5 m6 m7 m8
m11 855(70) 828(71) 435(79) 449(58) 405(6) 76(86) 107(89) 0 1872(67)
m13 15736(79) 1849(85) 576(68) 419(71) 2076(86) 556(86) 3146(72) 15105(64)
m14 2041(86) 571(71) 428(72% 2536(88) 592(86) 3777(75) 13246(67)
m15 391(79) 325(80) 205(95) 180(95) 215(100) 3114(77)
m12 1432(67) 69(78) 68(7) 0 1463(65)
m10 69(77) 61(70) 0 1101(67)
m5 0 77(100) 178(88)
m6 28(77) 78(96)
Table 3: Intersection comparison
#Links #Synsets #Words %
Subset1 10982 7131 8396 87.4
Inters 7244 5852 3939 85.6
Subset2 15535 10786 9986 86.4
in Subset1.
  All links proposed by m   xm  , m   xm  , ... ,
m  xm  (bold cells) adding up to 7,244 connec-
tions.
Table 3 shows the comparison of volumes and accu-
racies between the first sample stage (Subset1), the
connections extracted in the second stage of sam-
pling (Intersections) and the set resulting from the
fusion of both (Subset2). The cardinality of the set is
less than the addition of cardinalities because there
are connections belonging to both sets. This gives an
idea that the degree of intersection is far greater than
2, what is worth studying. Subset2 was the Span-
ish WordNet finally included within EuroWordNet
(1999). This is the origin of the present work.
4 Extending the coverage
Spanish WordNet has been further developed,
adding new synsets and variants and correcting man-
ually the links in Subset2, reaching a total of 54,753
links, almost all manually verified. As a result we
now dispose of a wider and more accurate database
that allows us to perform more robust estimations of
confidence score factors for the different methods.
We will call this manually verified database Subset3,
which were extracted on Dec. 2001. See table 6.
The result is that now, from the manually verified
links, 20,013 correspond to connections extracted
from automatic methods. The difference between
these two figures (15,535 and 20,013) is due to the
insertions performed for getting Subset3. Those
connections, as has already been pointed out, don?t
occur in Subset2 but can belong to the set of results
of some method not selected so far.
We decided to construct with those 20,013 con-
nections a third set of connections, all of them man-
ually evaluated as correct(OK) or incorrect(KO),
which would be used as test to evaluate again the
whole population, and try to obtain, by means of
a more detailed study of intersections, a Subset4
which would enhance the existing results in Subset3.
From the 20,013 connections there are 17,140
correct and 2,873 incorrect, giving an accuracy ratio
over 85%. This result some way validates the previ-
ous work, as it was our intention to obtain a large set
of connections with a value of above 85%.
In obtaining Subset2 it was evident the high de-
gree of intersection between the different methods, a
degree much larger than 2, but only the intersections
of two methods were studied. It is our goal to study
if the intersections could be exploited to extract from
them an individual evaluation for each connection,
and a formula that allows to calculate this value for
new connections depending on the set of methods
that propose them.
Concretely, the aim of the present work is to study
the statistical behavior of the links regarding the set
of methods supporting them, and not only intersec-
tions of two methods. To do this, all the data has
been condensed in a matrix of 66,609 vectors, one
vector for each link, of the kind
link m  m  ... m  m  eval
where mi are booleans indicating membership of
the link to the set of solutions of method i, eval is the
manual evaluation accepting one of two values (OK
being correct, KO being incorrect), and link is the
pair (WN1.5 synset,Spanish word). From this ma-
trix the 20,013 rows with manual verification have
been extracted to be studied separately, with the aim
of obtaining some statistical measure that permits us
to select the good connections of the set, in order to
apply later the statistic to the whole population.
4.1 Descriptive analysis
Using the set of 20,013 validated links, the accuracy
of each method can be reevaluated in a more pre-
cise way than in the first stage of the sampling (table
1). The results of the reevaluation are shown in 4th
column of table 1.
Comparing both tables the methods have different
accuracies. While some methods (1, 15, 16) keep a
similar accuracy, some suffer a light decrement (2,
3, 4), and most of them have a significant increment
(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 17). But these
variations must be studied with caution.
On one side, methods 1, 2, 3, 4 and 16 were ac-
cepted completely in the first stage of the sampling.
Thus for these methods it is fair to study the changes
observed. Methods 1 and 16 maintain the same lev-
els, while methods 2, 3 and 4 suffer a light decrease,
more important in the cases of methods 3 and 4.
The rest of the methods were not selected com-
pletely, but based on intersections with other meth-
ods, and must be analyzed from another point of
view. The general increment in these methods
comes from the manually added connections during
revision process.
Another assumption taken in the generation of the
set was that the accuracy increased with the number
of methods which produced them. In figure 1 the
percentage of correct links by the number of coin-
cident methods is shown, and it doesn?t ensure the
assumption, as it doesn?t seem to exist a correla-
tion between the number of methods that produce a
connection and the percentage of correct solutions.
There are indeed methods highly correlated and their
added evidences don?t cause an increase of global
evidence. Figure 1 depicts just sets of methods in-
tersecting, but they don?t distinguish which of the
methods are intersecting in each case. May be there
are some methods of higher predictive quality than
others, and this should be taken into account. So,
it seems better to analyze the relationship between
those methods according to the coincident links they
provide.
Figure 1: Accuracy of intersection levels
If a study is performed using the vectors previ-
ously shown, each connection is being evaluated
against the set of methods supporting it. In this anal-
ysis, the fact that in the second stage of the sampling
the less promising sets of solutions of certain pairs
of methods were under-represented doesn?t suppose
any risk, as now it is going to be considered the fact
that each connection is correct or not in relation to
the total set of methods that produce it.
4.2 Logistic regression model
We applied without success the Principal Compo-
nents Analysis method, trying to find a spatial dis-
persion that separated correct and incorrect connec-
tions. We chose instead to apply another statistical
method more appropriate to the problem: logistic re-
gression, which is used to obtain an adjustment for
the accuracy of a connection from the set of methods
that propose it.
4.2.1 Method formalization
The set of boolean variables m  ...m  defines 2 
possible combinations of methods that propose a
certain connection. Associating to each connection
a vector of this kind, the same description will be
used for all connections proposed by the same group
of methods and they will collapse in the same point.
Thus a new matrix can be constructed that for the
2  possibilities keeps the number of correct evalua-
tions and the total of evaluations, being the number
of incorrect ones the difference between both values:
m  m

... m  m  nok ntot
where nok is the number of correct evaluations for
the set of solutions of every group of methods, and
ntot accumulates the total number of evaluations.
It is clear that the probability that a link would be
correct can be estimated by the following expres-
sion
 
	 (considering that
the probability is the limit of the relative frequency).
The logistic regression is a technique which allows
finding a model (in the mathematical sense) for ap-
proximating
 
on the basis of a set of explica-
tive variables (in our case m  , m  , ... m  ).
In order to fit the formula  Experiments Adapting an Open-Domain Question Answering System to
the Geographical Domain Using Scope-Based Resources
Daniel Ferre?s and Horacio Rodr??guez
TALP Research Center
Software Department
Universitat Polite`cnica de Catalunya
{dferres, horacio}@lsi.upc.edu
Abstract
This paper describes an approach to adapt
an existing multilingual Open-Domain
Question Answering (ODQA) system for
factoid questions to a Restricted Domain,
the Geographical Domain. The adaptation
of this ODQA system involved the modifi-
cation of some components of our system
such as: Question Processing, Passage Re-
trieval and Answer Extraction. The new
system uses external resources like GNS
Gazetteer for Named Entity (NE) Classi-
fication and Wikipedia or Google in order
to obtain relevant documents for this do-
main. The system focuses on a Geograph-
ical Scope: given a region, or country, and
a language we can semi-automatically ob-
tain multilingual geographical resources
(e.g. gazetteers, trigger words, groups of
place names, etc.) of this scope. The
system has been trained and evaluated for
Spanish in the scope of the Spanish Geog-
raphy. The evaluation reveals that the use
of scope-based Geographical resources is
a good approach to deal with multilingual
Geographical Domain Question Answer-
ing.
1 Introduction
Question Answering (QA) is the task of, given
a query expressed in Natural Language (NL), re-
trieving its correct answer (a single item, a text
snippet,...). QA has become a popular task in the
NL Processing (NLP) research community in the
framework of different international ODQA eval-
uation contests such as: Text Retrieval Confer-
ence (TREC) for English, Cross-Lingual Evalua-
tion Forum (CLEF) for European languages, and
NTCIR for Asian languages.
In this paper we describe our experiments in the
adaptation and evaluation of an ODQA system to
a Restricted Domain, the Geographical Domain.
GeoTALP-QA is a multilingual Geographi-
cal Domain Question Answering (GDQA) sys-
tem. This Restricted Domain Question Answer-
ing (RDQA) system has been built over an existing
ODQA system, TALP-QA, a multilingual ODQA
system that processes both factoid and definition
questions (see (Ferre?s et al, 2005) and (Ferre?s et
al., 2004)). The system was evaluated for Spanish
and English in the context of our participation in
the conferences TREC and CLEF in 2005 and has
been adapted to a multilingual GDQA system for
factoid questions.
As pointed out in (Benamara, 2004), the Geo-
graphical Domain (GD) can be considered a mid-
dle way between real Restricted Domains and
open ones because many open domain texts con-
tain a high density of geographical terms.
Although the basic architecture of TALP-QA
has remained unchanged, a set of QA components
were redesigned and modified and we had to add
some specific components for the GD to our QA
system. The basic approach in TALP-QA consists
of applying language-dependent processes on both
question and passages for getting a language inde-
pendent semantic representation, and then extract-
ing a set of Semantic Constraints (SC) for each
question. Then, an answer extraction algorithm
extracts and ranks sentences that satisfy the SCs
of the question. Finally, an answer selection mod-
ule chooses the most appropriate answer.
We outline below the organization of the paper.
In the next section we present some characteris-
tics of RDQA systems. In Section 3, we present
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
69
the overall architecture of GeoTALP-QA and de-
scribe briefly its main components, focusing on
those components that have been adapted from an
ODQA to a GDQA. Then, the Scope-Based Re-
sources needed for the experimentation and the ex-
periments are presented in Sections 4 and 5. In
section 6 we present the results obtained over a
GD corpus. Finally, in Section 7 and 8 we describe
our conclusions and the future work.
2 Restricted Domain QA Systems
RDQAs present some characteristics that prevent
us from a direct use of ODQA systems. The most
important differences are:
? Usually, for RDQA, the answers are searched
in relatively small domain specific collec-
tions, so methods based on exploiting the re-
dundancy of answers in several documents
are not useful. Furthermore, a highly accu-
rate Passage Retrieval module is required be-
cause frequently the answer occurs in a very
small set of passages.
? RDQAs are frequently task-based. So, the
repertory of question patterns is limited al-
lowing a good accuracy in Question Process-
ing with limited effort.
? User requirements regarding the quality of
the answer tend to be higher in RDQA. As
(Chung et al, 2004) pointed out, no answer
is preferred to a wrong answer.
? In RDQA not only NEs but also domain spe-
cific terminology plays a central role. This
fact usually implies that domain specific lex-
icons and gazetteers have to be used.
? In some cases, as in GD, many documents in-
cluded in the collections are far to be stan-
dard NL texts but contain tables, lists, ill-
formed sentences, etc. sometimes following
a more or less defined structure. Thus, extrac-
tion systems based, as our, on the linguistic
structure of the sentences have to be relaxed
in some way to deal with this kind of texts.
More information about RDQA systems can be
found in the ACL 2004 Workshop on QA in Re-
stricted Domains1 and the AAAI 2005 Worshop
on Question Answering in Restricted Domains
(Molla and Vicedo, 2005) .
1http://acl.ldc.upenn.edu/acl2004/qarestricteddomain/
3 System Description
GeoTALP-QA has been developed within the
framework of ALIADO2 project. The system
architecture uses a common schema with three
phases that are performed sequentially without
feedback: Question Processing (QP), Passage Re-
trieval (PR) and Answer Extraction (AE). More
details about this architecture can be found in
(Ferre?s et al, 2005) and (Ferre?s et al, 2004).
Before describing these subsystems, we intro-
duce some additional knowledge sources that have
been added to our system for dealing with the
geographic domain and some language-dependent
NLP tools for English and Spanish. Our aim is to
develop a language independent system (at least
able to work with English and Spanish). Lan-
guage dependent components are only included
in the Question Pre-processing and Passage Pre-
processing components, and can be easily substi-
tuted by components for other languages.
3.1 Additional Knowledge Sources
One of the most important task to deal with the
problem of GDQA is to detect and classify NEs
with its correct Geographical Subclass (see classes
in Section 3.3). We use Geographical scope based
Knowledge Bases (KB) to solve this problem.
These KBs can be built using these resources:
? GEOnet Names Server (GNS3). A world-
wide gazetteer, excluding the USA and
Antarctica, with 5.3 million entries.
? Geographic Names Information System
(GNIS4). A gazetteer with 2.0 million entries
about geographic features of the USA.
? Grammars for creating NE aliases. Ge-
ographic NEs tend to occur in a great va-
riety of forms. It is important to take this
into account to avoid losing occurrences.
A set of patterns for expanding have been
created. (e.g. <toponym> Mountains,
<toponym> Range, <toponym> Chain).
? Trigger Words Lexicon. A lexicon con-
taining trigger words (including multi-word
terms) is used for allowing local disambigua-
tion of ambiguous NE, both in the questions
and in the retrieved passages.
2ALIADO. http://gps-tsc.upc.es/veu/aliado
3GNS. http://earth-info.nga.mil/gns/html
4GNIS. http://geonames.usgs.gov/geonames/stategaz
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
70
Working with geographical scopes avoids many
ambiguity problems, but even in a scope these
problems occur:
? Referent ambiguity problem. This problem
occurs when the same name is used for sev-
eral locations (of the same or different class).
In a question, sometimes it is impossible to
solve this ambiguity, and, in this case, we
have to accept as correct all of the possible in-
terpretations (or a superclass of them). Oth-
erwise, a trigger phrase pattern can be used
to resolve the ambiguity (e.g. ?Madrid? is
an ambiguous NE, but in the phrase, ?comu-
nidad de Madrid? (State of Madrid), ambigu-
ity is solved). Given a scope, we automati-
cally obtain the most common trigger phrase
patterns of the scope from the GNS gazetteer.
? Reference ambiguity problem. This prob-
lem occurs when the same location can have
more than one name (in Spanish texts this fre-
quently occurs as many place names occur
in languages other than Spanish, as Basque,
Catalan or Galician). Our approach to solve
this problem is to group together all the ge-
ographical names that refer to the same lo-
cation. All the occurrences of the geograph-
ical NEs in both questions and passages are
substituted by the identifier of the group they
belong to.
We used the geographical knowledge avail-
able in the GNS gazetteer to obtain this ge-
ographical NEs groups. First, for each place
name in the scope-based GNS gazetteer we
obtained all the NEs that have the same fea-
ture designation code, latitude and longitude.
For each group, we then selected an identifier
choosing one of the NE included in it using
the following heuristics: the information of
the GNS field ?native? tells if a place name is
native, conventional, a variant, or, is not ver-
ified. So we decided the group representative
assigning the following order of priorities to
the names: native, conventional name, vari-
ant name, unverified name. If there is more
than one place name in the group with the
same name type we decide that the additional
length gives more priority to be cluster repre-
sentative. It is necessary to establish a set of
priorities among the different place names of
the group because in some retrieval engines
(e.g. web search engines) is not possible to
do long queries.
3.2 Language-Dependent Processing Tools
A set of general purpose NLP tools are used for
Spanish and English. The same tools are used for
the linguistic processing of both the questions and
the passages (see (Ferre?s et al, 2005) and (Ferre?s
et al, 2004) for a more detailed description of
these tools). The tools used for Spanish are:
? FreeLing, which performs tokenization, mor-
phological analysis, POS tagging, lemmati-
zation, and partial parsing.
? ABIONET, a NE Recognizer and Classifier
(NERC) on basic categories.
? EuroWordNet, used to obtain a list of synsets,
a list of hypernyms of each synset, and the
Top Concept Ontology class.
The following tools are used to process English:
? TnT, a statistical POS tagger.
? WordNet lemmatizer 2.0.
? ABIONET.
? WordNet 1.5.
? A modified version of the Collins parser.
? Alembic, a NERC with MUC classes.
3.3 Question Processing
The main goal of this subsystem is to detect the
Question Type (QT), the Expected Answer Type
(EAT), the question logic predicates, and the ques-
tion analysis. This information is needed for the
other subsystems. We use a language-independent
formalism to represent this information.
We apply the processes described above to the
the question and passages to obtain the following
information:
? Lexical and semantic information for each
word: form, lemma, POS tag (Eagles or PTB
tag-set), semantic class and subclass of NE,
and a list of EWN synsets.
? Syntactic information: syntactic constituent
structure of the sentence and the information
of dependencies and other relations between
these components.
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
71
Once this information is obtained we can find
the information relevant to the following tasks:
? Environment Building. The semantic pro-
cess starts with the extraction of the semantic
relations that hold between the different com-
ponents identified in the question text. These
relations are organized into an ontology of
about 100 semantic classes and 25 relations
(mostly binary) between them. Both classes
and relations are related by taxonomic links.
The ontology tries to reflect what is needed
for an appropriate representation of the se-
mantic environment of the question (and the
expected answer). A set of about 150 rules
was built to perform this task. The ontology
has been extended for the GD (see below the
classes related with this domain).
ENTITY
ENTITY_PROPER_PLACE
GEOLOGICAL_REGION
ARCHIPELAGO
ISLAND
LAND_FORM
MOUNTAIN
SEA_FORM
CAPE
GULF
SEA
WATER_FORM
RIVER
POLITICAL_REGION
CITY
CONTINENT
COUNTY
COUNTRY
STATE
ENTITY_QUANTITY
NUMERIC
MAGNITUDE
AREA
LENGTH
FLOW
WEIGHT
? Question Classification. Our ODQA system
uses 25 QTs. For the GD we only used 10
Question Types (see Table 1). Only 5 QTs are
common with the ODQA QTs, 5 QTs have
been specially created for this domain.
Question Type Expected Answer Type
Count objects NUMBER
How many people NUMBER
What area MEASURE AREA
What flow MEASURE FLOW
What height MEASURE HEIGHT
What length MEASURE LENGTH
Where action LOCATION SUBCLASS
Where location LOCATION SUBCLASS
Where quality LOCATION SUBCLASS
Default class LOCATION
Table 1: QTs and Expected Answer Types.
In order to determine the QT our system uses
a Prolog DCG Parser. This parser uses the
following features: word form, word position
in the question, lemma and part-of-speech
(POS). A set of DCG rules was manually
configured in order to ensure a sufficient cov-
erage.
The parser uses external information: geo-
graphical NE subclasses, trigger words for
each Geographical subclass (e.g. ?poblado?
(ville)), semantically related words of each
subclass (e.g. ?water? related with sea and
river), and introductory phrases for each
Question Type (e.g. ?which extension? is a
phrase of the QT What area).
? Semantic Constraints Extraction. Depend-
ing on the QT, a subset of useful items of
the environment has to be selected in order
to extract the answer. Accordingly, we define
the set of relations (the semantic constraints)
that are supposed to be found in the answer.
These relations are classified as mandatory,
(MC), (i.e. they have to be satisfied in the
passage) or optional, (OC), (if satisfied the
score of the answer is higher). In order to
build the semantic constraints for each ques-
tion, a set of rules has been manually built.
A set of 88 rules is used. An example of the
constraints extracted from an environment is
shown in Table 2. This example shows the
question type predicted, the initial predicates
extracted from the question, the Environment
predicates, the MCs and the OCs. MCs are
entity(4) and i en city(6). The first predi-
cate refers to token number 4 (?autonomia?
(state)) and the last predicate refers to token
number 6 (?Barcelona?).
Question ? A que? autonom??a pertenece Barcelona?
(At which state pertains Barcelona?)
Q. Type where location
Predicates city(?Barcelona?),state(X),
pertains(?Barcelona?,X)
Environment action(5), participant in event(5,4),
theme of event(5,6),prep(4,2),entity(4),
i en proper place(6),det(4,3),qu(3)
Mandatory entity(4),i en city(6)
Constraints
Optional action(5),theme of event(5,6),
Constraints participant in event(5,4),prep(4,2),
type of location(5,5,i en state),
property(5,5,pertenecer,3,6)
Table 2: Question Analysis example.
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
72
3.4 Passage Retrieval
We use two different approaches for Passage Re-
trieval. The first one uses a pre-processed corpus
as a document collection. The second one uses the
web as document collection.
3.4.1 Off-line Corpus Retrieval
This approach uses a pre-processed and indexed
corpus with Scope-related Geographical Informa-
tion as a document collection for Passage Re-
trieval. The processed information was used for
indexing the documents. Storing this information
allows us to avoid the pre-processing step after
retrieval. The Passage Retrieval algorithm used
is the same of our ODQA system: a data-driven
query relaxation technique with dynamic passages
implemented using Lucene IR engine API (See
(Ferre?s et al, 2005) for more details).
3.4.2 Online Web Snippet Retrieval
The other approach uses a search-engine to get
snippets with relevant information. We expect to
get a high recall with few snippets. In our exper-
iments, we chose Google as the search-engine us-
ing a boolean retrieval schema that takes advan-
tage of its phrase search option and the Geograph-
ical KB to create queries that can retrieve highly
relevant snippets. We try to maximize the num-
ber of relevant sentences with only one query per
question.
The algorithm used to build the queries is sim-
ple. First, some expansion methods described be-
low can be applied over the keywords. Then, stop-
words (including normal stop-words and some
trigger words) are removed. Finally, only the
Nouns and Verbs are extracted from the keywords
list. The expansion methods used are:
? Trigger Words Joining (TWJ). Uses the
trigger words list and the trigger phrase pat-
tern list (automatically generated from GNS)
to join trigger phrases (e.g. ?isla Conejera? o
?Sierra de los Pirineos?).
? Trigger Words Expansion (TWE). This ex-
pansion is applied to the NEs that were not
detected as a trigger phrase. The expansion
uses its location subclass to create a key-
word with the pattern: TRIGGER + NE (e.g.
?Conejera? is expanded to: (?isla Conejera?
OR ?Conejera?)).
? GNS Grouping Expansion (CE). Noun
Phrase expansion based on the groups gen-
erated from GNS Gazetteer.
? Question-based Expansion (QBE). This
method appends keywords or expands the
query depending on the question type. As an
example, in the case of a question classified
as What length, trigger words and units as-
sociated to the question class like ?longitud?
(length) and ?kilo?metros? (kilometers) are ap-
pended to the query.
3.5 Answer Extraction
We used two systems for Answer Extraction: our
ODQA system (adapted for the GD) and a fre-
quency based system.
3.5.1 ODQA Extraction
The linguistic process of analyzing passages is
similar to the process carried out on questions and
leads to the construction of the environment of
each sentence. Then, a set of extraction rules are
applied following an iterative approach. In the first
iteration all the MC have to be satisfied by at least
one of the candidate sentences. Then, the itera-
tion proceeds until a threshold is reached by re-
laxing the MC. The relaxation process of the set
of semantic constraints is performed by means of
structural or semantic relaxation rules, using the
semantic ontology. The extraction process con-
sists on the application of a set of extraction rules
on the set of sentences that have satisfied the MC.
The Knowledge Source used for this process is a
set of extraction rules owning a credibility score.
Each QT has its own subset of extraction rules that
leads to the selection of the answer.
In order to select the answer from the set of can-
didates, the following scores are computed and ac-
cumulated for each candidate sentence: i) the rule
score (which uses factors such as the confidence
of the rule used, the relevance of the OC satisfied
in the matching, and the similarity between NEs
occurring in the candidate sentence and the ques-
tion), ii) the passage score, iii) a semantic score
(see (Ferre?s et al, 2005)) , iv) the extraction rule
relaxation level score. The answer to the question
is the candidate with the best global score.
3.5.2 Frequency-Based Extraction
This extraction algorithm is quite simple. First,
all snippets are pre-processed. Then, we make a
ranked list of all the tokens satisfying the expected
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
73
answer type of the question. The score of each to-
ken in the snippets is computed using the follow-
ing formula:
Score(tki) =
?
o?Occurrence(tki)
1
snippet rank(o)
Finally, the top-ranked token is extracted.
4 Resources for Scope-Based
Experiments
In this section we describe how we obtained the
resources needed to do experiments in the Span-
ish Geography domain using Spanish. These re-
sources were: the question corpus (validation and
test), the document collection required by the off-
line ODQA Passage Retrieval, and the geograph-
ical scope-based resources. Finally, we describe
the experiments performed.
4.1 Language and Scope based Geographical
Question Corpus
We obtained a corpus of Geographical questions
from Albayzin, a speech corpus (Diaz et al, 1998)
that contains a geographical subcorpus with utter-
ances of questions about the geography of Spain in
Spanish. We obtained from Albayzin a set of 6887
question patterns. We analyzed this corpus and we
extracted the following type of questions: Partial
Direct, Partial Indirect, and Imperative Interroga-
tive factoid questions with a simple level of dif-
ficulty (e.g. questions without nested questions).
We selected a set of 2287 question patterns. As a
question corpus we randomly selected a set of 177
question patterns from the previous selection (see
Table 3). These patterns have been randomly in-
stantiated with Geographical NEs of the Albayzin
corpus. Then, we searched the answers in the Web
and the Spanish Wikipedia (SW). The results of
this process were: 123 questions with answer in
the SW and the Web, 33 questions without answer
in the SW but with answer using the Web, and
finally, 21 questions without answer (due to the
fact that some questions when instantiated cannot
be answered (e.g. which sea bathes the coast of
Madrid?)). We divided the 123 questions with an-
swer in the SW in two sets: 61 questions for devel-
opment (setting thresholds and other parameters)
and 62 for test.
?A que? comunidad auto?noma pertenece el <PICO>?
At which state pertains <PEAK>?
?Cua?l es el capital de <COMUNIDAD>?
Which is the capital of <STATE>?
?Cua?l es la comunidad en la que desemboca el <R?IO>?
What is the state in which <RIVER> flows into?
?Cua?l es la extensio?n de <COMUNIDAD>?
Which is the extension of <STATE>?
Longitud del r??o <R?IO>.
Length of river <RIVER>.
?Cua?ntos habitantes tiene la <COMUNIDAD>?
How many people does <STATE> has?
Table 3: Some question patterns from Albayzin.
4.2 Document Collection for ODQA Passage
Retrieval
In order to test our ODQA Passage Retrieval sys-
tem we need a document collection with sufficient
geographical information to resolve the questions
of Albayzin corpus. We used the filtered Span-
ish Wikipedia5. First, we obtained the original
set of documents (26235 files). Then, we selected
two sets of 120 documents about the Spanish ge-
ography domain and the non-Spanish geography
domain. Using these sets we obtained a set of
Topic Signatures (TS) (Lin and Hovy, 2000) for
the Spanish geography domain and another set of
TS for the non-Spanish geography domain. Then,
we used these TS to filter the documents from
Wikipedia, and we obtained a set of 8851 doc-
uments pertaining to the Spanish geography do-
main. These documents were pre-processed and
indexed.
4.3 Geographical Scope-Based Resources
A Knowledge Base (KB) of Spanish Geography
has been built using four resources:
? GNS: We obtained a set of 32222 non-
ambiguous place names of Spain.
? Albayzin Gazetteer: a set of 758 places.
? A Grammar for creating NE aliases. We cre-
ated patterns for the summit and state classes
(the ones with more variety of forms), and we
expanded this patterns using the entries of Al-
bayzin.
? A lexicon of 462 trigger words.
We obtained a set of 7632 groups of place
names using the grouping process over GNS.
These groups contain a total of 17617 place
5Spanish Wikipedia. http://es.wikipedia.org
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
74
names, with an average of 2.51 place names per
group. See in Figure 1 an example of a group
where the canonical term appears underlined.
{Cordillera Pirenaica, Pireneus, Pirineos, Pyrenaei
Montes, Pyre?ne?es, Pyrene, Pyrenees}
Figure 1: Example of a group obtained from GNS.
In addition, a set of the most common trigger
phrases in the domain has been obtained from the
GNS gazetteer (see Table 4).
Geographical Scope
Spain UK
TRIGGER de NE NE TRIGGER
Top-ranked TRIGGER NE TRIGGER NE
Trigger TRIGGER del NE TRIGGER of NE
Phrases TRIGGER de la NE TRIGGER a? NE
TRIGGER de las NE TRIGGER na NE
Table 4: Sample of the top-ranked trigger phrases
automatically obtained from GNS gazetteer for the
geography of Spain and UK.
5 Experiments
We have designed some experiments in order to
evaluate the accuracy of the GDQA system and
its subsystems (QP, PR, and AE). For PR, we
evaluated the web-based snippet retrieval using
Google with some variants of expansions, versus
our ODQA Passage Retrieval with the corpus of
the SW. Then, the passages (or snippets) retrieved
by the best PR approach were used by the two dif-
ferent Answer Extraction algorithms. The ODQA
Answer Extractor has been evaluated taking into
account the answers that have a supported con-
text in the set of passages (or snippets). Finally,
we evaluated the global results of the complete
QA process with the different Answer Extractors:
ODQA and Frequency-Based.
6 Results
This section evaluates the behavior of our GDQA
system over a test corpus of 62 questions and re-
ports the errors detected on the best run. We evalu-
ated the three main components of our system and
the global results.
? Question Processing. The Question Clas-
sification task has been manually evaluated.
This subsystem has an accuracy of 96.77%.
? Passage Retrieval. The evaluation of this
subsystem was performed using a set of cor-
rect answers (see Table 5). We computed the
answer accuracy: it takes into account the
number of questions that have a correct an-
swer in its set of passages.
Retrieval Accuracy at N passages/snippets
Mode N=10 N=20 N=50 N=100
Google 0.6612 0.6935 0.7903 0.8225
+TWJ 0.6612 0.6774 0.7419 0.7580
+TWJ+TWE 0.6612 0.6774 0.7419 0.7580
+CE 0.6612 0.6774 0.7741 0.8064
+QBE 0.8064 0.8387 0.9032 0.9354
+TWJ+QB+CE 0.7903 0.8064 0.8548 0.8870
Google+All 0.7903 0.8064 0.8548 0.8870
ODQA+Wiki 0.4354 0.4516 0.4677 0.5000
Table 5: Passage Retrieval results (refer to sec-
tion 3.4.2 for detailed information of the different
query expansion acronyms).
? Answer Extraction. The evaluation of
the ODQA Answer Extractor subsystem is
shown in Table 6. We evaluated the ac-
curacy taking into account the number of
correct and supported answers by the pas-
sages divided by the total number of ques-
tions that have a supported answer in its set
of passages. This evaluation has been done
using the results of the top-ranked retrieval
configuration over the development set: the
Google+TWJ+QB+CE configuration of the
snippet retriever.
Accuracy at N Snippets
N=10 N=20 N=50
0.2439 (10/41) 0.3255 (14/43) 0.3333 (16/48)
Table 6: Results of the ODQA Answer Extraction
subsystem (accuracy).
In Table 7 are shown the global results of the
two QA Answer Extractors used (ODQA and
Frequency-Based). The passages retrieved by the
Google+TWJ+QB+CE configuration of the snip-
pet retriever were used.
Accuracy
Num. Snippets ODQA Freq-based
10 0.1774 (11/62) 0.5645 (35/62)
20 0.2580 (16/62) 0.5967 (37/62)
50 0.3387 (21/62) 0.6290 (39/62)
Table 7: QA results over the test set.
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
75
We analyzed the 23 questions that fail in our
best run. The analysis detected that 10 questions
had no answer in its set of passages. In 5 of these
questions it is due to have a non common ques-
tion or location. The other 5 questions have prob-
lems with ambiguous trigger words (e.g. capital)
that confuse the web-search engine. On the other
hand, 13 questions had the answer in its set of pas-
sages, but were incorrectly answered. The reasons
are mainly due to the lack of passages with the an-
swer (8), answer validation and spatial-reasoning
(3), multilabel Geographical NERC (1), and more
context in the snippets (1).
7 Evaluation and Conclusions
This paper summarizes our experiments adapting
an ODQA to the GD and its evaluation in Spanish
in the scope of the Spanish Geography. Out of 62
questions, our system provided the correct answer
to 39 questions in the experiment with the best re-
sults.
Our Passage Retrieval for ODQA offers less at-
tractive results when using the SW corpus. The
problem of using SW to extract the answers is that
it gives few documents with the correct answer,
and, it is difficult to extract the answer because
the documents contain tables, lists, ill-formed sen-
tences, etc. Our ODQA AE needs a grammati-
cally well-structured text to extract correctly the
answers. The QA system offers a low perfor-
mance (33% of accuracy) when using this AE over
the web-based retrieved passages. In some cases,
the snippets are cut and we could expect a better
performance retrieving the whole documents from
Google.
On the other hand, web-based snippet retrieval,
with only one query per question, gives good re-
sults in Passage Retrieval. The QA system with
the Frequency-Based AE obtained better results
than with the ODQA AE (62.9% of accuracy).
Finally, we conclude that our approach with
Geographical scope-based resources are notably
helpful to deal with multilingual Geographical
Domain Question Answering.
8 Future Work
As a future work we plan to improve the AE mod-
ule using a semantic analysis with extended con-
texts (i.e. more than one sentence) and adding
some spatial reasoning. We also want to improve
the retrieval by crawling relevant documents from
web search-engines instead of using snippets. This
could be a good method to find more sentences
with supported answers. Finally, we expect to do
tests with English in another scope.
Acknowledgements
This work has been partially supported by the
European Commission (CHIL, IST-2004-506909)
and the Spanish Research Dept. (ALIADO,
TIC2002-04447-C02). Daniel Ferre?s is sup-
ported by a UPC-Recerca grant from Universitat
Polite`cnica de Catalunya (UPC). TALP Research
Center is recognized as a Quality Research Group
(2001 SGR 00254) by DURSI, the Research De-
partment of the Catalan Government.
References
F. Benamara. 2004. Cooperative Question Answering
in Restricted Domains: the WEBCOOP Experiment.
In Proceedings of the Workshop Question Answering
in Restricted Domains, within ACL-2004.
H. Chung, Y. Song, K. Han, D. Yoon, J. Lee, H. Rim,
and S. Kim. 2004. A Practical QA System in Re-
stricted Domains. In Proceedings of the Workshop
Question Answering in Restricted Domains, within
ACL-2004.
J. Diaz, A. Rubio, A. Peinado, E. Segarra, N. Prieto,
and F. Casacuberta. 1998. Development of Task-
Oriented Spanish Speech Corpora. In Procceed-
ings of the First International Conference on Lan-
guage Resources and Evaluation, pages 497?501,
Granada, Spain, May. ELDA.
D. Ferre?s, S. Kanaan, A. Ageno, E. Gonza?lez,
H. Rodr??guez, M. Surdeanu, and J. Turmo. 2004.
The TALP-QA System for Spanish at CLEF 2004:
Structural and Hierarchical Relaxing of Semantic
Constraints. In C. Peters, P. Clough, J. Gonzalo,
G. J. F. Jones, M. Kluck, and B. Magnini, editors,
CLEF, volume 3491 of Lecture Notes in Computer
Science, pages 557?568. Springer.
D. Ferre?s, S. Kanaan, E. Gonza?lez, A. Ageno,
H. Rodr??guez, M. Surdeanu, and J. Turmo. 2005.
TALP-QA System at TREC 2004: Structural and
Hierarchical Relaxation Over Semantic Constraints.
In Proceedings of the Text Retrieval Conference
(TREC-2004).
C-Y. Lin and E. Hovy. 2000. The automated acquisi-
tion of topic signatures for text summarization. In
COLING, pages 495?501. Morgan Kaufmann.
D. Molla and J.L. Vicedo. 2005. AAAI-05 Work-
shop on Question Answering in Restricted Domains.
AAAI Press. to appear.
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
76
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 60?65,
Prague, June 2007. c?2007 Association for Computational Linguistics
Machine Learning with Semantic-Based Distances Between Sentences for
Textual Entailment
Daniel Ferre?s
TALP Research Center
Software Department
Universitat Polite`cnica de Catalunya
dferres@lsi.upc.edu
Horacio Rodr??guez
TALP Research Center
Software Department
Universitat Polite`cnica de Catalunya
horacio@lsi.upc.edu
Abstract
This paper describes our experiments on
Textual Entailment in the context of the
Third Pascal Recognising Textual Entail-
ment (RTE-3) Evaluation Challenge. Our
system uses a Machine Learning approach
with Support Vector Machines and Ad-
aBoost to deal with the RTE challenge. We
perform a lexical, syntactic, and semantic
analysis of the entailment pairs . From this
information we compute a set of semantic-
based distances between sentences. The re-
sults look promising specially for the QA en-
tailment task.
1 Introduction
This paper describes our participation in the RTE-
3 challenge. It is our first attempt to RTE and we
have taken profit of an analysis of the approaches
followed in previous challenges (see (Dagan et al,
2005), and (Bar-Haim et al, 2006) for overviews
of RTE-1 and RTE-2). Our approach, however, is
based on a set of semantic-based distance measures
between sentences used by our group in previous
contests in Question Answering (TREC 2004, see
(Ferre?s et al, 2005), and CLEF 2004, see (Ferre?s
et al, 2004)) , and Automatic Summarization (DUC
2006, see (Fuentes et al, 2006)). Although the use
of such measures (distance between question and
sentences in passages candidates to contain the an-
swer, distance between query and sentences candi-
dates to be included in the summary, ...) is different
for RTE task, our claim is that with some modifica-
tions the approach can be useful in this new scenario.
The organization of this paper is as follows. Af-
ter this introduction we present in section 2 a de-
scription of the measures upon which our approach
is built. Section 3 describes in detail our proposal.
Results are discussed in section 4. Conclusions and
further work is finally included in section 5.
2 System Description
Our approach for computing distance measures be-
tween sentences is based on the degree of overlap-
ping between the semantic content of the two sen-
tences. Obtaining the semantic content implies a
depth Linguistic Processing. Upon this semantic
representation of the sentences several distance mea-
sures are computed. We next describe such issues.
2.1 Linguistic Processing
Linguistic Processing (LP) consists of a pipe of
general purpose Natural Language (NL) processors
that performs tokenization, morphologic tagging,
lemmatization, Named Entities Recognition and
Classification (NERC) with 4 basic classes (PER-
SON, LOCATION, ORGANIZATION, and OTH-
ERS), syntactic parsing and semantic labelling, with
WordNet synsets, Magnini?s domain markers and
EuroWordNet Top Concept Ontology labels. The
Spear1 parser performs full parsing and robust de-
tection of verbal predicate arguments. The syntactic
constituent structure of each sentence (including the
specification of the head of each constituent) and the
relations among constituents (subject, direct and in-
direct object, modifiers) are obtained. As a result
1Spear. http://www.lsi.upc.edu/?surdeanu/
spear.html
60
of the performance of these processors each sen-
tence is enriched with a lexical and syntactic lan-
guage dependent representations. A semantic lan-
guage independent representation of the sentence
(called environment) is obtained from these analy-
ses (see (Ferre?s et al, 2005) for details). The en-
vironment is a semantic network like representation
built using a process to extract the semantic units
(nodes) and the semantic relations (edges) that hold
between the different tokens in the sentence. These
units and relations belong to an ontology of about
100 semantic classes (as person, city, action, mag-
nitude, etc.) and 25 relations (mostly binary) be-
tween them (e.g. time of event, actor of action, lo-
cation of event, etc.). Both classes and relations are
related by taxonomic links (see (Ferre?s et al, 2005)
for details) allowing inheritance. Consider, for in-
stance, the sentence ?Romano Prodi 1 is 2 the 3
prime 4 minister 5 of 6 Italy 7?. The following envi-
ronment is built:
i en proper person(1), entity has quality(2),
entity(5), i en country(7), quality(4),
which entity(2,1), which quality(2,5), mod(5,7),
mod(5,4).
2.2 Semantic-Based Distance Measures
We transform each environment into a labelled di-
rected graph representation with nodes assigned to
positions in the sentence, labelled with the corre-
sponding token, and edges to predicates (a dummy
node, 0, is used for representing unary predicates).
Only unary (e.g. entity(5) in Figure 1) and binary
(e.g. in Figure 2 which quality(2,5)) predicates are
used. Over this representation a rich variety of
lexico-semantic proximity measures between sen-
tences have been built. Each measure combines two
components:
? A lexical component that considers the set of
common tokens occurring in both sentences.
The size of this set and the strength of the com-
patibility links between its members are used
for defining the measure. A flexible way of
measuring token-level compatibility has been
set ranging from word-form identity, lemma
identity, overlapping of WordNet synsets, ap-
proximate string matching between Named En-
tities etc. For instance, ?Romano Prodi? is lex-
ically compatible with ?R. Prodi? with a score
of 0.5 and with ?Prodi? with a score of 0.41.
?Italy? and ?Italian? are also compatible with
score 0.7. This component defines a set of (par-
tial) weighted mapping between the tokens of
the two sentences that will be used as anchors
in the next component.
? A semantic component computed over the sub-
graphs corresponding to the set of lexically
compatible nodes (anchors). Four different
measures have been defined:
? Strict overlapping of unary predicates.
? Strict overlapping of binary predicates.
? Loose overlapping of unary predicates.
? Loose overlapping of binary predicates.
The loose versions allow a relaxed match-
ing of predicates by climbing up in the ontol-
ogy of predicates (e.g. provided that A and B
are lexically compatible, i en city(A) can match
i en proper place(B), i en proper named entity(B),
location(B) or entity(B)) 2. Obviously, loose over-
lapping implies a penalty on the score that depends
on the length of the path between the two predicates
and their informative content.
Romano Prodi1 is2 prime4 minister5 Italy7
0i_en_proper_person i_en_country
entity_has_quality
which_qualitywhich_entity mod
mod
quality entity
Figure 1: Example of an environment of a sentence.
2The ontology contains relations as i en city
isa i en proper place, i en proper place isa
i en proper named entity, proper place isa location,
i en proper named entity isa entity, location isa entity
61
3 System Architecture
We have adapted the set of measures described be-
fore for RTE in the following way:
1. We follow a Machine Learning (ML) approach
for building a classifier to perform the RTE
task. In previous applications the way of
weighting and combining the different mea-
sures was based on a crude optimization using
a development corpus.
2. We extract a more complex set of features for
describing the semantic content of the Text (T)
and the Hypothesis (H) as well as the set of se-
mantic measures between them. Table 1 con-
tains a brief summary of the features used.
3. We perform minor modifications on the token-
level compatibility measures for dealing with
the asymmetry of the entailment relation (basi-
cally using the hyponymy and the verbal entail-
ment relations of WordNet)
4. We add three new task-specific features (see
Table 1)
The overall architecture of the system is depicted
in Figure 2. As usual in ML, the system proceeds in
two phases, learning and classification. The left side
of the figure shows the learning process and the right
part the classification process. The set of examples
(tuples H, T) is first processed, in both phases, by LP
for obtaining a semantic representation of the tuple
(Hsem and Tsem). From this representation a Fea-
ture Extraction component extracts a set of features.
This set is used in the learning phase for getting a
classifier that is applied to the set of features of the
test, during the classification phase, in order to ob-
tain the answer.
4 Experiments
Before the submission we have performed a set of
experiments in order to choose the Machine Learn-
ing algorithms and the training sets to apply in the
final submission.
HTraining setT H
Test setT
Linguistic ProcessingLinguistic Processing
HsTraining set (sem)Ts
Feature Extraction Feature Extraction
HsTest set (sem)Ts
Features Features
Machine Learning Classifier
Answers
Figure 2: System Architecture.
4.1 Machine Learning Experiments
We used the WEKA3 ML platform (Witten and
Frank, 2005) to perform the experiments. We tested
9 different ML algorithms: AdaBoostM1, Bayes
Networks, Logistic Regression, MultiBoostAB,
Naive Bayes, RBF Network, LogitBoost (Simple Lo-
gistic in WEKA), Support Vector Machines (SMO in
WEKA), and Voted Perceptron. We used the previ-
ous corpora of the RTE Challenge (RTE-1 and RTE-
2) and the RTE-3 development test. A filtering pro-
cess has been applied removing pairs with more than
two sentences in the text or hypothesis, resulting a
total of 3335 Textual Entailment (TE) pairs. The re-
sults over 10-fold-Cross-Validation using a data set
composed by RTE-1, RTE-2, and RTE-3 develop-
ment set are shown in Table 2.
The results shown that AdaBoost, LogitBoost, and
SVM obtain the best results. Then we selected Ad-
aBoost and SVM to perform the classification of the
RTE-3 test set. The SVM algorithm tries to compute
the hyperplane that best separates the set of training
examples (the hyperplane with maximum margin)
(Vapnik, 1995). On the other hand, AdaBoost com-
3WEKA. http://www.cs.waikato.ac.nz/ml/
weka/
62
Features #features Description
semantic content of T 12 #locations, #persons, #dates, #actions, ...
semantic content of H 12 ...
intersection of T and H 12 ...
length of intersection
score of intersection
Strict overlapping of unary predicates 5 ratio of intersection related to shortest env
ratio of intersection related to longest env
ratio of intersection related to both (union of)
Strict overlapping of binary predicates 5 . . .
Loose overlapping of unary predicates 5 . . .
Loose overlapping of binary predicates 5 ...
Verbal entailment (WordNet) 1 V1 ? T, V2 ? H, such that V1 verbal entails V2
Antonymy 1 A1 ? T, A2 ? H, such that A1 and A2 are antonyms and
no token compatible with A2
#occurs in H Negation 1 Difference between # negation tokens in H and T
Table 1: Features used for classification with Machine Learning algorithms.
Algorithm #correct Accuracy
AdaBoostM1 1989 59.6402
BayesNet 1895 56.8216
Logistic 1951 58.5007
MultiBoostAB 1959 58.7406
NaiveBayes 1911 57.3013
RBFNetwork 1853 55.5622
LogitBoost 1972 59.1304
SVM 1972 59.1304
VotedPerceptron 1969 59.0405
Table 2: Results over 10-fold-Cross-Validation us-
ing a filtered data set composed by RTE-1, RTE-2,
and RTE-3 (a total of 3335 entailment pairs).
bines a set of weak classifiers into a strong one us-
ing lineal combination (Freund and Schapire, 1996).
The idea is combining many moderately accurate
rules into a highly accurate prediction rule. A weak
learning algorithm is used to find the weak rules.
4.2 Training Set Experiments
We designed two experiments in order to decide the
best training set to apply in the RTE-3 challenge. We
performed an experiment using RTE-1 and RTE-2
data sets as a training set and the RTE-3 develop-
ment set filtered (541 TE pairs) as a test set. In this
experiment AdaBoost and SVM obtained accuracies
of 0.6672 and 0.6396 respectively (see results in Ta-
ble 3. We performed the same experiment joining
the Answer Validation Exercise4 (AVE) 2006 En-
glish data set (Pen?as et al, 2006) and the Microsoft
Research Paraphrase Corpus5 (MSRPC) (Dolan et
al., 2004) to the previous corpora (RTE-1 and RTE-
2) resulting a total of 8585 entailment pairs filtering
pairs with a text or a hypothesis with more than 1
sentence. In our approach we considered that para-
phrases were bidirectional entailments. The para-
phrases of the MSRPC have been used as textual en-
tailments in only one direction: the first sentence in
the paraphrase has been considered the hypothesis
and the second one has been considered the text.
Using the second corpus for training and the RTE-
3 development set as test set resulted in a notable
degradation of accuracy (see Table 3).
Accuracy
Algorithm Corpus A Corpus B
AdaBoost 66.72% 53.78%
SVM 63.95% 59.88%
Table 3: Results over the RTE-3 development set
filtered (541 TE pairs) using as training set corpus A
(RTE-1 and RTE-2) and corpus B (RTE-1, RTE-2,
MSRPC, and AVE2006 English)
Finally, we performed a set of experiments to de-
tect the contribution of the different features used for
Machine Learning. These experiments revealed that
4AVE. http://nlp.uned.es/QA/AVE
5MSRPC. http://research.microsoft.com/
nlp/msr_paraphrase.htm
63
the three most relevant features were: Strict overlap-
ping of unary predicates, Semantic content of Hy-
pothesis, and Loose overlapping of unary predicates.
4.3 Official Results
Our official results at RTE-3 Challenge are shown
in Table 4. We submitted two experiments: the first
one with AdaBoost (run1) and the second one with
SVM (run2). Training data set for final experiments
were corpus: RTE-1 (development and test), RTE-
2 (development and test), and RTE-3 development.
The test set was the RTE-3 test set without filtering
the entailments (text or hypothesis) with more than
one sentence. In this case we joined multiple sen-
tences in a unique sentence that has been processed
by the LP component.
We obtained accuracies of 0.6062 and 0.6150. In
the QA task we obtained the best per-task results
with accuracies of 0.7450 and 0.7000 with AdaBoost
and SVM respectively.
Accuracy
Task run1 run2
AdaBoost SVM
IE 0.4350 0.4950
IR 0.6950 0.6800
QA 0.7450 0.7000
SUM 0.5500 0.5850
Overall 0.6062 0.6150
Table 4: RTE-3 official results.
5 Conclusions and Further Work
This paper describes our experiments on Textual En-
tailment in the context of the Third Pascal Recog-
nising Textual Entailment (RTE-3) Evaluation Chal-
lenge. Our approach uses Machine Learning al-
gorithms (SVM and AdaBoost) with semantic-based
distance measures between sentences. Although fur-
ther analysis of the results is in process, we observed
that our official per-task results at RTE-3 show a dif-
ferent distribution compared with the global results
of all system at RTE-2 challenge. The RTE-2 per-
task analysis showed that most of the systems scored
higher in accuracy in the multidocument summariza-
tion (SUM) task while in our system this measure is
low. Our system at RTE-3 challenge scored higher
in the QA and IR tasks with accuracies of 0.7450
and 0.6950 respectively in the first run.
References
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second pascal recognising textual entail-
ment challenge. In Proceedings of the Second PAS-
CAL Challenges Workshop on Recognising Textual
Entailment.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In Joaquin Quin?onero Candela, Ido Dagan,
Bernardo Magnini, and Florence d?Alche? Buc, editors,
MLCW, volume 3944 of Lecture Notes in Computer
Science, pages 177?190. Springer.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In COL-
ING ?04: Proceedings of the 20th international con-
ference on Computational Linguistics, page 350, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Daniel Ferre?s, Samir Kanaan, Alicia Ageno, Edgar
Gonza?lez, Horacio Rodr??guez, Mihai Surdeanu, and
Jordi Turmo. 2004. The TALP-QA System for Span-
ish at CLEF 2004: Structural and Hierarchical Relax-
ing of Semantic Constraints. In Carol Peters, Paul
Clough, Julio Gonzalo, Gareth J. F. Jones, Michael
Kluck, and Bernardo Magnini, editors, CLEF, volume
3491 of Lecture Notes in Computer Science, pages
557?568. Springer.
Daniel Ferre?s, Samir Kanaan, Edgar Gonza?lez, Alicia
Ageno, Horacio Rodr??guez, Mihai Surdeanu, and Jordi
Turmo. 2005. TALP-QA System at TREC 2004:
Structural and Hierarchical Relaxation Over Seman-
tic Constraints. In Proceedings of the Text Retrieval
Conference (TREC-2004).
Yoav Freund and Robert E. Schapire. 1996. Experiments
with a new boosting algorithm. In International Con-
ference on Machine Learning, pages 148?156.
Maria Fuentes, Horacio Rodr??guez, Jordi Turmo, and
Daniel Ferre?s. 2006. Femsum at duc 2006: Semantic-
based approach integrated in a flexible eclectic mul-
titask summarizer architecture. In Proceedings of
the Document Understanding Conference 2006 (DUC
2006). HLT-NAACL 2006 Workshop., New York City,
NY, USA, June.
Anselmo Pen?as, ?Alvaro Rodrigo, Valent??n Sama, and Fe-
lisa Verdejo. 2006. Overview of the answer val-
idation exercise 2006. In Working Notes for the
64
CLEF 2006 Workshop. ISBN: 2-912335-23-x, Ali-
cante, Spain, September.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc., New
York, NY, USA.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques, Second
Edition (Morgan Kaufmann Series in Data Manage-
ment Systems). Morgan Kaufmann, June.
Acknowledgments
This work has been supported by the Spanish Re-
search Dept. (TEXT-MESS, TIN2006-15265-C06-
05). Daniel Ferre?s is supported by a UPC-Recerca
grant from Universitat Polite`cnica de Catalunya
(UPC). TALP Research Center is recognized as
a Quality Research Group (2001 SGR 00254) by
DURSI, the Research Department of the Catalan
Government.
65
Empirical Methods for the Study of
Denotation in Nominalizations
in Spanish
Aina Peris?
University of Barcelona
Mariona Taule??
University of Barcelona
Horacio Rodr??guez??
Technical University of Catalonia
This article deals with deverbal nominalizations in Spanish; concretely, we focus on the deno-
tative distinction between event and result nominalizations. The goals of this work is twofold:
first, to detect the most relevant features for this denotative distinction; and, second, to build an
automatic classification system of deverbal nominalizations according to their denotation. We
have based our study on theoretical hypotheses dealing with this semantic distinction and
we have analyzed them empirically by means of Machine Learning techniques which are the
basis of the ADN-Classifier. This is the first tool that aims to automatically classify deverbal
nominalizations in event, result, or underspecified denotation types in Spanish. The ADN-
Classifier has helped us to quantitatively evaluate the validity of our claims regarding deverbal
nominalizations. We set up a series of experiments in order to test the ADN-Classifier with differ-
ent models and in different realistic scenarios depending on the knowledge resources and natural
language processors available. The ADN-Classifier achieved good results (87.20% accuracy).
1. Introduction
The last few years have seen an increasing amount of work in the semantic treatment
of unrestricted text, such as Minimal Recursive Semantics in Lingo/LKB (Copestake
2007), Frame Semantics in Shalmaneser (Erk and Pado? 2006), Discourse Representation
Structures in Boxer (Bos 2008), and the automatic learning of Semantic Grammars
(Mooney 2007), but we are still a long way from representing the full meaning of texts
when not restricted to narrow domains. Many Natural Language Processing (NLP)
applications such as Question Answering, Information Extraction, Machine Reading
and high-quality Machine Translation or Summarization systems, and many NLP
intermediate level tasks such as Textual Entailment, Paraphrase Detection, or Word
? CLiC, Centre de Llenguatge i Computacio?/University of Barcelona, Gran Via de les Corts Catalanes 585,
08007 Barcelona. E-mail: aina.peris@ub.edu; mtaule@ub.edu.
?? TALP Research Center - Technical University of Catalonia, Jordi Girona Salgado 1-3, 08034 Barcelona.
E-mail: horacio@lsi.upc.edu.
Submission received: 22 July 2011; revised submission received: 27 December 2011; accepted: 1 February 2012.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 4
Sense Disambiguation (WSD), have almost reached their practical upper bounds and
it is difficult to move forward without using a serious semantic representation of the
text under consideration. Given the limitations and the difficulties in obtaining an
in-depth semantic representation of texts as a whole, most efforts have been focused
on partial semantic representation using less expressive semantic formalisms, such as
those that come under the umbrella of Description Logic variants, or on discarding the
whole semantic interpretation task in order to focus on smaller (and easier) subtasks.
This is the case, for instance, in Semantic Role Labeling (SRL) systems, which indicate
the semantic relations that hold between a predicate and its associated participants
and properties, the relations of which are drawn from a pre-specified list of possible
semantic roles for that predicate or class of predicates. See Ma?rquez et al (2008) and
Palmer, Gildea, and Xue (2010) for recent surveys. Closely related to SRL is the task
of learning Selectional Restrictions for a predicate, for example, the kind of semantic
class each argument of the predicate must belong to (Mechura 2008). In this case
a predefined set of semantic classes must also be used to perform the classification
task. WordNet (Fellbaum 1998), VerbNet (Kipper et al 2006), PropBank (Palmer,
Kingsbury, and Gildea 2005), FrameNet (Ruppenhofer et al 2006), and OntoNotes
(Hovy et al 2006) are resources frequently used for this purpose. Most of these efforts
are verb-centered and reduce role labeling to the roles played by entities around a
predicate instantiated as a verb. At a finer level, there is the task of WSD, for example,
assigning the most appropriate sense to each lexical unit of the text from a predefined
lexical-semantic resource. Once again a catalogue of classes has to be used as a range
for the assignment.1 In this case as well, despite its excessive finer granularity, WordNet
is the most widely used reference. See Navigli (2009) for a recent survey.
In this line of research, there has recently been a growing interest in going beyond
verb-centered hypotheses to tackle the computational treatment of deverbal nominal-
izations (nouns derived from verbs), in order to move forward to the full compre-
hension of texts. Deverbal nominalizations are lexical units that contain rich semantic
information equivalent to a clausal structure. Many recent studies have focused on
the detection of semantic relations between pairs of nominals that belong to different
Nominal Phrases (NPs), such as Task 4 of SemEval 2007 (Girju et al 2009) and Task 8
of SemEval 2010 (Hendrickx et al 2010), or between nominals taking part in noun
compound constructions. In the latter case, they take into account a predefined set
of semantic relations (Girju et al 2005) or use verb paraphrases with prepositions
(Task 9 of SemEval 2010 [Butnariu et al 2010a, 2010b]). Although these works include
nominalizations, they are not strictly focused on them but cover all type of nouns.
Actually, most of the work studying only deverbal nominalizations is focused on their
argument structure: Some authors focus on the detection of arguments within the NP
headed by the nominalization (Hull and Gomez 2000; Lapata 2002; Gurevich et al 2006;
Pado?, Pennacchiotti, and Sporleder 2008; Surdeanu et al 2008; Gurevich and Waterman
2009), whereas others center their attention on detecting the implicit arguments of the
nominalizations which are outside the NP (Gerber and Chai 2010; Ruppenhofer et al
2010). Among the former group, there are different approaches to the problem: Lapata
(2002) and Gurevich and Waterman (2009) use probabilistic models, Hull and Gomez
(2000) and Gurevich et al (2006) develop heuristic rules, Pado?, Pennacchiotti, and
1 Some approaches simply discriminate between different senses for a case without assigning it to a
predefined specific class, however. Clustering techniques rather than classification are used in these
approaches.
828
Peris, Taule?, and Rodr??guez Empirical Methods for the Study of Denotation in Nominalizations
Sporleder (2008) work with an unsupervised SRL system, and in Surdeanu et al (2008)
the work presented uses supervised SRL systems. The kind of argument annotated is
also different in these works: Although only two, more syntactic labels (subj [subject]
and obj [object]), are used to annotate the arguments in Lapata (2002), Gurevich et al
(2006), and Gurevich and Waterman (2009), Pado?, Pennacchiotti, and Sporleder (2008)
use FrameNet labels and Surdeanu et al (2008) use NomBank (Meyers, Reeves, and
Macleod 2004)2 labels. The interpretation of nominalizations is crucial because they are
common in texts and an important amount of information is representedwithin them. In
the AnCora-ES corpus (Taule?, Mart??, and Recasens 2008), for instance, the semantic in-
formation is mostly coded in verbs (56,590 verbal occurrences) but a significant number
of deverbal nominalizations (23,431 occurrences) also encode rich semantic information.
Most of the work on this topic sets out from the denotative distinction between
nominalizations referring to an event, those that express an action or a process, and
nominalizations referring to a result, those expressing the outcome of an action or
process. From a theoretical point of view, it is stated that this denotative distinction
may have repercussions on the argument-taking capability of deverbal nominaliza-
tions. Despite being aware of this distinction, computational approaches focus on event
nominalizations, not taking into account the result ones or, more frequently, without
characterizing the difference. For instance, SRL systems are mostly applied to event
nominalizations (Pradhan et al 2004; Erk and Pado? 2006; Liu and Ng 2007). Result
nominalizations are more frequent than the event types, however, at least in Spanish
(1,845 event occurrences in contrast to 20,037 result occurrences in AnCora-ES). In the
present work, we hypothesize that result nominalizations, like event nominalizations,
can take arguments; therefore, discarding result nominalizations would imply a loss of
semantic information, equally relevant to text representation. In this article, we focus
our interest on this denotative distinction. Concretely, we aim to determine the relevant
linguistic information required to classify deverbal nominalizations as event or result
types in Spanish. In order to achieve this goal, we have built an automatic classifier of
deverbal nominalizations?the ADN-Classifier?for Spanish, aimed at identifying the
semantic denotation of these nominal predicates (Peris et al 2010). The ADN-Classifier
is a tool that takes into account different levels of linguistic information depending on
its availability, such as senses, lemmas, or syntactic and semantic information coded
in the verbal and nominal lexicons (AnCora-Verb [Aparicio, Taule?, and Mart?? 2008]
and AnCora-Nom [Peris and Taule? 2011]) or in the AnCora-ES corpus.
Therefore, this article contributes to the semantic analysis of texts focusing on
Spanish deverbal nominalizations, although the proposal presented could be extended
to other Romance languages. We base our study on theoretical hypotheses that we
analyze empirically, and as a result we have developed three new resources: 1) the
ADN-Classifier, the first tool that allows for the automatic classification of deverbal
nouns as event or result nominalizations; 2) the AnCora-ES corpus enriched with
the annotation of deverbal nominalizations according to their semantic denotation,
the only Spanish corpus that incorporates this information; and 3) AnCora-Nom, a
lexicon of deverbal nominalizations containing information about denotation types
and argument structure.
The ADN-Classifier can be used independently in NLP tasks, such as Corefer-
ence Resolution and Paraphrase Detection (Recasens and Vila 2010). For Coreference
2 In the work of Hull and Gomez (2000) it is not stated explicitly which set of arguments are used, although
from their examples we infer that they are semantic roles such as those of VerbNet.
829
Computational Linguistics Volume 38, Number 4
Resolution tasks it would be useful to have the nominalizations classified into denota-
tions in order to detect coreference types. For instance, if a nominalization has a verbal
antecedent (anchor) and its denotation is of the event type, an identity coreference rela-
tion could be established between them (Example (1)). If the nominalization is of the
result type, however, the relation established between verb and noun would be a bridg-
ing coreference relation (Example (2)) (Clark 1975; Recasens, Mart??, and Taule? 2007).
(1) En Francia los precios cayeron un 0,1% en septiembre. La ca??da<event> ha
provocado que la inflacio?n quedara en el 2,2%.
?In France prices fell by 0.1 % in September. The fall<event> caused
inflation to remain at 2.2 %.?
(2) La imprenta se invento? en 1440. El invento<result>permitio? difundir las
ideas y conocimientos con eficacia.
?The printing press was invented in 1440. This invention<result> allowed
for ideas and knowledge to be spread efficiently.?
As for Paraphrase Detection (Androutsopoulos and Malakasiotis 2010; Madnani and
Dorr 2010), event nouns (but not result nouns) are paraphrases for full sentences, so
this type of information can also be useful for this task. For instance, the sentence in
Example (3) and the NP headed by an event nominalization in Example (4) are typically
considered to be paraphrases.
(3) Se ha ampliado el capital de la empresa en un 20%.
?The company?s capital has been increased by 20%.?
(4) La ampliacio?n<event>del capital de la empresa en un 20%.
?The increase<event> of company?s capital by 20%.?
(5) Se han vendido muchas traducciones<result>de su u?ltimo libro.
?Many translations<result> of his latest book have been sold.?
(6) Se han vendido muchos libros traducidos de su u?ltimo t??tulo.
?Many translated editions of his latest book have been sold.?
If the nominalization, however, has a result interpretation as in Example (5)?
traducciones, ?translations? refers to the concrete object, that is, the book translated?
it is impossible to have a paraphrase with a clausal structure. This is due to the fact
that result nominalizations can denote an object whereas verbs cannot denote objects.
In fact, result nominalizations can only be paraphrases of other NPs denoting objects
(Example (6)).
The AnCora-ES corpus enriched with denotative information could be used as
training and test data for WSD systems. The work presented in this article also provides
an additional insight into the linguistic question underlying it: the characterization of
deverbal nominalizations according to their denotation and the identification of the
most useful criteria for distinguishing between these denotation types.
The remainder of this article is organized as follows. Section 2 summarizes the
theoretical approaches to the semantic denotation with which we deal here. Section 3
describes the methodology used in this work. Section 4 presents the empirical linguistic
study in which the initial model is established; in Section 5 the different knowledge
resources used are presented, paying special attention to the nominal lexicon, AnCora-
Nom. In Section 6, the ADN-Classifier is presented and in Section 7 the different
830
Peris, Taule?, and Rodr??guez Empirical Methods for the Study of Denotation in Nominalizations
experiments conducted are described and their results are reported. Section 8 reviews
related work and, finally, our conclusions are drawn in Section 9.
2. Theoretical Background
In the linguistics literature related to nominalizations, one of the most studied and
controversial topics is the denotative distinction between event and result nominaliza-
tions. By event nominalization we mean those nouns that denote an action or process
in the same way that a verb does. In other words, as their verbal counterparts, event
nominalizations have the aspectual property of dynamicity (Example (7)). In contrast, a
result nominalization refers to the state (Example (9)) or the concrete or abstract object
resulting from the event (Example (8)). Both types of result nominalizations (states and
objects) lack the aspectual property of dynamicity.
(7) El proyecto americano consiste en la adaptacio?n<event>de la novela
Paper Boy.
?The American project is the adaptation< event> of the Paper Boy novel.?
(8) Esta adaptacio?n<result> cinematogra?fica ha recibido buenas cr??ticas.
?This film adaptation<result> has received good reviews.?
(9) Reforzo? la tendencia<result> al alza del Euro de los u?ltimos d??as.
?The upward trend<result> of the euro has been reinforced in recent days.?
In Example (7), the noun adaptacio?n (?adaptation?) denotes an event because it expresses
an action in the same way that a verb does (it is equivalent to El proyecto americano con-
siste en adaptar la novela Paper Boy, ?The American project consists of adapting the Paper
Boy novel?). The event interpretation is characterized as dynamic because it implies a
change from ?not being adapted? to ?being adapted.? In contrast, in Example (8) the same
nominalization is understood as a result because it denotes a specific object that is the
outcome of the action of adapting a creative work into a film. In Example (9), the result
interpretation is due to the fact that the verb base of tendencia, ?trend,? denotes a state,
so the noun inherits the property of stativity (non-dynamicity) and does not imply any
change.
In this sense, our notions of event and result are equivalent to the complex-event
and result nominalizations, respectively, in the terminology of Grimshaw (1990)3 or the
terms process and result used in Pustejovsky (1995)4 and Alexiadou (2001). Although
the event vs. result distinction we make is widespread, it is true that event and result
nominalizations can also be represented in a more fine-grained way. For instance,
Eberle, Faasz, and Heid (2009) distinguish between events (messung, ?measurement?),
states (teilung, ?division?), and objects (lieferung, ?furnished material?) in German
nominalizations, and Balvet, Barque, and Mar??n (2010) propose a typology of French
nominalizations that contemplates four aspectual types: states (admiration, ?admiration?),
durative events (ope?ration, ?operation?), punctual events (explosion, ?explosion?), and objects
(ba?timent, ?building?). For English, Levi (1978) identifies four types of nominalizations:
actions (parental refusal), which are equivalent to the event notion; agents ( financial
3 She distinguishes a third denotative type, simple event nouns like trip in That trip took two weeks, but we
discard them because, although expressing an event, they are not derived from verbs; in this research
we focus on deverbal nominalizations.
4 This author characterizes nominalizations as dot-objects that include both process and resultmeanings.
831
Computational Linguistics Volume 38, Number 4
analyst), which denote the agent of an action and are characterized by using a different
set of suffixes; products (human error), which denote the result of an action; and patients
(student?s inventions), which denote the patient object of the action. Also for English,
Nunes (1993) defines five types of nominalizations: process nouns, that name the action
or process denoted by the base verb (The documents? destruction by the North); result
nouns, that denote a new creation resulting from the base verb (The invention was put on
display); accumulated-action nouns, that name the sum total of a verb activity (The attack
was unexpected); experiential-state nouns, nominalized stative verbs or nominalizations of
a state brought about by a particular verb (Sam?s interest in maths); and experiential-state
results, the result counterpart of the previous class (Sam has many interests).
The authors working on this topic mainly differ on two issues. On the one
hand, they do not agree on how to consider (and therefore, how to represent) these
two denotations: as two senses of the same lexical unit (Pustejovsky 1995; Badia
2002; Alonso 2004) or as two different lexical units (Grimshaw 1990; Picallo 1999;
Alexiadou 2001). Regarding this denotative distinction, several linguistic criteria have
been proposed in order to identify each of these denotations, mostly for English,
although there are some proposals for Spanish (Picallo 1999), French, Greek, Russian,
and Polish (Alexiadou 2001) (see Table 2 in Section 4.1). On the other hand, authors
differ on the argument-taking capacity of deverbal nominalizations: Some linguists
maintain that only event deverbal nominalizations can take arguments (Zubizarreta
1987; Grimshaw 1990), whereas others consider that both event and result nominaliza-
tions can take arguments (Pustejovsky 1995; Picallo 1999; Alexiadou 2001).
Authors who conceptualize event and result nominalizations as different lexical
units justify this in different ways. Grimshaw (1990) considers that only complex-event
nominals legitimate an argument structure, and that constitutes the main difference
with respect to result nominalizations which, according to her, lack argument structure.
In Alexiadou (2001) and Picallo (1999), the idea that event and result nominalizations
are different lexical units is justified by the different functional projection of these two
types of nominalization and by word-formation at different levels of the language,5
respectively. In contrast to Grimshaw, however, they state that both types of nominal-
izations can select arguments. In the words of Alexiadou (2001, page 69): ?Given that
there is no lexical difference between verbs and process nouns, and between result and
process nouns, apart from the functional domain, all can take arguments.? Picallo also
believes that complements of result nominalizations are arguments when argumental
relationships can be established between them and the nominal head.
In contrast, those who consider both denotations as senses of the same lexical unit
maintain that nominalizations are underspecified lexical units (Pustejovsky 1995), units
in which a disjunction of meaning is present (Alonso 2004), or simply lexical units with
different senses (Badia 2002). Specifically, Pustejovsky accounts for the event-result ambi-
guity in nominalizations bymeans of an underspecified lexical representation called dot-
object, arguing that event-result nominalizations are cases of complementary polysemy:
?both senses of a logically polysemous noun seem relevant for the interpretation of the
noun in the context, but one sense seems ?focused? for purposes of a particular context?
(Pustejovsky 1995, page 31). In relation to argument-taking capacity, both types of
nominalizations are argumental because the dot-object representation has an argument
structure in which the nominal arguments are specified.
5 In Picallo (1999) it is stated that event nominalizations are created in the syntax whereas result
nominalizations are created in the lexicon; therefore they have different derivation processes.
832
Peris, Taule?, and Rodr??guez Empirical Methods for the Study of Denotation in Nominalizations
Alonso (2004) argues that these nominalizations present a disjunction of meaning
because they can update an event and a result reading in the same sentence without the
understanding of the sentence being affected. For instance, declaracio?n (?declaration?) in
Example (10)6 can be interpreted as an event and as a result at the same time: Only event
nominalizations can be said to have an initial moment and only result nominalizations
can be said to have five pages. These two senses both originate in the same lexical unit,
which includes both, and the context provides both meanings.
(10) La declaracio?n<event/result> que el juez tomo? al testigo, que comenzo? a
las once, ocupa cinco folios.
?The statement<event/result> that the judge took from the witness, which
began at eleven, takes up five pages.?
Regarding argument-taking capacity, Alonso maintains that all nominalizations taking
part in a support-verb construction can select arguments. So, if a result nominalization
is part of a support-verb construction, it will also have argument structure. Following
these authors, we also consider that result nominalizations can take arguments.
Within a computational framework, there are different models that represent nom-
inalizations; not all of them take into account the event and result distinction, however.
For instance, in NomBank (Meyers, Reeves, and Macleod 2004) this distinction is com-
pletely ignored and the authors focus on argument structure. In contrast, Spencer and
Zaretskaya (1999) label each nominal sense with one of the Grimshaw semantic cate-
gories (i.e., complex event, simple event, and result). Their database contains information
about 7,000 Russian verbs and their 5,000 corresponding nominalizations, distinguish-
ing between the verbal entries that nominalize the whole event while preserving the
verbal argument structure, and those that denote a concrete or abstract result of the
verb. The Nomage project (Balvet, Barque, and Mar??n 2010) annotates French deverbal
nominalizations in the French TreeBank (Abeille?, Cle?ment, and Kinyon 2000) with one
of the four classes proposed in their work (i.e., states, durative events, punctual events, and
objects). In the middle ground between these two positions, we find the representation
models proposed in WordNet (Fellbaum 1998), FrameNet (Baker, Fillmore, and Lowe
1998; Ruppenhofer et al 2006), and Ontonotes (Hovy et al 2006). WordNet, possibly
due to its extremely fine granularity, usually includes among the senses corresponding
to deverbal nouns one paraphrased as ?the acting of verb x? (our event nominalization)
and another paraphrased to something similar to ?the thing X-verb+ed? (our result
nominalization). FrameNet distinguishes between deverbal nominalizations defined
as the action or process of X verb and nominalizations defined as entities. Concerning
nouns, Ontonotes is interested in the disambiguation of noun senses in order to create
an ontology. Within deverbal nouns the authors distinguish between nominalization
senses that truly inherit the verb meaning and deverbal noun senses whose denotation
is not directly related to the verb meaning.7 That is, they distinguish between two
different types of deverbal nouns but this distinction is not akin to the event-result one.
The distinctions established in WordNet and FrameNet are more similar to the one
proposed in OntoNotes.
To sum up, the event vs. result distinction in deverbal nominalizations has received
much attention in linguistics literature. It seems to be less relevant in the computational
6 This example is taken from Alonso (2004).
7 For instance, building in The building was made mostly of concrete and glass.
833
Computational Linguistics Volume 38, Number 4
framework, in contrast, although some computational models do represent a semantic
distinction that is similar to the one we are analyzing (see Section 8).
3. Methodology
The aim of the current work is twofold: first, to detect the most relevant features for the
denotative distinction between event and result nominalizations; and, second, to build
Figure 1
Scheme of the methodology followed.
834
Peris, Taule?, and Rodr??guez Empirical Methods for the Study of Denotation in Nominalizations
an automatic classification system for deverbal nominalizations according to their de-
notation. In Figure 1 the overall methodology for carrying out this work is schematized.
In order to determine the most relevant features, the first step was to carry out a
linguistic study of deverbal nominalizations (see Section 4). This study consisted of the
application of the linguistic criteria stated in the literature to a reduced set of nominal-
izations corresponding to the occurrences extracted from a 100,000 word (henceforth
100kw) subset of the AnCora-ES corpus. As an outcome, we manually obtained (see
step 1 in Figure 1) a lexicon for this deverbal nominalization set, AnCora-Nom-R1 (Peris,
Taule?, and Rodr??guez 2009), which allowed us to annotate the corresponding deverbal
occurrences in the corpus subset. The nominalization classifying model (see step 2 in
Figure 1) underlying these two initial resources was tested by empirical methods. This
first model of classification is based on sense distinctions?that is, the extraction of
features was performed at sense level and the examples for learning (both positive and
negative) corresponded to different senses in the lexicon; thus, we will refer to it as the
sense-based model. We set up a series of experiments based on Machine Learning (ML)
techniques in order to evaluate the consistency of the data annotated in AnCora-Nom-
R1, to analyze the relevance of the attributes used in the lexical-semantic representation
and to infer new useful features to distinguish between event and result nominalizations.
As well as experimenting with AnCora-Nom-R1 simple features, we experimented with
the binarization8 and grouping9 of several of them to address sparseness problems.
Furthermore, with these experiments the foundations of the automatic classification
system, the ADN-Classifier-R1, were laid (see step 3 in Figure 1). See Section 6 for a
description of the ADN-Classifier.
Once the consistency of the annotated data was corroborated and the most relevant
features were established, we focused on the building of the ADN-Classifier, the second
goal of the research presented in this article. In order to develop the final version of
the ADN-Classifier (R3) we increased the corpus data used for learning. Therefore, we
needed to annotate the denotation types in the whole AnCora-ES. Because this involves
an increase in the number of nominalization occurrences to annotate (23,431 occurrences
in contrast to 3,077), we carried out this annotation automatically. To do so, wemodified
the (sense-based) model of classification underlying the ADN-Classifier-R1 (see step 2
in Figure 1) creating a new model able to carry out the more realistic task of classifying
the occurrences in the whole AnCora-ES corpus.
This new model (see step 5 in Figure 1) uses the following as knowledge resources:
1) the AnCora-Verb lexicon to obtain the features from the verbs related to nominaliza-
tions; 2) the complete AnCora-ES corpus (500kw); and 3) AnCora-Nom-R2, an extended
lexicon of nominalizations without denotation types obtained automatically (see step 4
in Figure 1). This lexicon contains a total of 1,655 lexical entries, corresponding to the
1,655 nominalization types in the whole AnCora-ES. Because we annotate the occur-
rences in the AnCora-ES corpus, however, we reduce our dependence on the lexical
source AnCora-Nom-R2 (see Section 5.3), removing the sense-specific information from
it and taking into account only the information shared by all the senses of one lemma,
while maintaining the features extracted from the corpus. In this sense, we adapted the
sense-based model developed in ADN-Classifier-R1 (see step 2 in Figure 1) to obtain
a new classification model that is based on lemmas (and not senses) (see step 5 in
8 Binarizing a k-value categorial feature means transforming it into k binary features.
9 Combining several simple features into a complex feature using a combination (for instance,
a logical OR).
835
Computational Linguistics Volume 38, Number 4
Figure 1). This new model was used for the automatic annotation of the AnCora-ES
corpus with denotation information (see step 7 in Figure 1). Afterwards, in order to
evaluate the performance of the developed model, the corpus annotation was manually
validated (Peris, Taule?, and Rodr??guez 2010) (see step 8 in Figure 1). Thismanual process
also guarantees the quality of the corpus annotation, leading to the final version of the
AnCora-Nom lexicon (R3) containing denotation type information.
At that moment, we were able to build the final version of the ADN-Classifier (R3)
(see step 10 in Figure 1). In order to do so, we set up a series of experiments leading to
the development of new sense- and lemma-based models using the resources already
built (AnCora-ES with denotation information and AnCora-Nom-R3), that is, models
learned with more instances. We also replicated the experiments at sense and lemma
level with the subset of 100kw from the already enriched AnCora-ES and the subset
of 817 lexical entries from the AnCora-Nom-R3. Specifically, we carried out a set of
new ML experiments using the simple and binarized features from the nominal and
verbal lexicons as well as additional features obtained from the AnCora-ES corpus (the
so-called contextual features). For the evaluation of the different sense- and lemma-
based models derived from this set of experiments (see step 9 in Figure 1), tenfold
cross-validation was used with the pre-created resources. These models give rise to the
final version of the ADN-Classifier (R3) (see step 10 in Figure 1). See Section 6 for a
description of the ADN-Classifier.
4. Previous Linguistic Study
The aim of the corpus-based linguistic study conducted was twofold. First, we wanted
to determine which of the criteria stated in the literature were the most relevant linguis-
tic features to establish the distinction between event and result denotations in Spanish.
Secondly, we wanted to find other features that could be used to reinforce the semantic
distinction we are dealing with.
In order to do this, we selected a sample of 817 Spanish deverbal nominalizations
corresponding to 3,077 occurrences. These nominalizations were obtained semiauto-
matically from a 100kw subset of the AnCora-ES corpus. In this selection we took into
account a predefined list of ten suffixes (-a, -aje, -io?n/-cio?n/-sio?n/-o?n, -da/-do, -dura/-ura, -e,
-ido, -miento/-mento, -ncia/-nza, -o/-eo? [Santiago and Bustos 1999]) that contribute to an
event or resultmeaning and which take verbs as a basis for the derivation process.
This sample corresponds to the original 3LB corpus (Civit and Mart?? 2004), that can
be considered to be a preliminary version of AnCora-ES. The set of 817 nominalizations
consists of those occurrences in this sub-corpus. Despite coming from different sources,
the 100kw corpus and the full 500kw corpus are comparable as is shown in Table 1.
In Table 1 we present some metrics for describing the whole AnCora-ES corpus
and its 100kw subset. We present the metrics we have used in three rows: degree
Table 1
Descriptive content of AnCora-ES and its 100Kw subset. In each cell the values corresponding to
the subset and the whole corpus are present.
Min Max Mean Standard Deviation
sense/lemma 1/1 13/13 2.19/1.86 1.54/1.31
examples/lemma 1/1 239/255 19.99/14.15 30.76/26.44
length sentences 4/4 149/149 39.14/39.51 10.69/12.08
836
Peris, Taule?, and Rodr??guez Empirical Methods for the Study of Denotation in Nominalizations
of polysemy (number of senses per lemma), the number of examples (sentences) per
lemma in the corpus, and the average length of sentences per lemma. We depict the
minimum and maximum values, the mean and the standard deviation for each metric.
The values in the figure seem reasonable. The only anomalous figures correspond to
the extremely high values of the standard deviation of the number of examples metric.
It is due to the highly biased shape of the curve towards small values. In fact, most
of the lemmas have only one example (121 for the 100kw sample) and the number of
lemmas having values over the mean are very few. As can be seen in Table 1, there
are no notable differences in the values corresponding to the whole set and the subset.
Additionally we computed the ratio of lemmas containing only one example.
4.1 Analyzing Features from the Literature
In order to determine which criteria stated in the literature were the most relevant, we
selected seven criteria that satisfy two conditions: first, they are some of the most widely
used by other authors, and second, it is possible to search for them in the AnCora-ES
corpus without suffering data sparseness. These criteria and the authors who propose
them are shown schematically in Table 2.
The seven criteria were analyzed by contrasting them with the behavior of the 817
Spanish deverbal nominalizations in the AnCora-ES corpus. Concretely, two graduate
students in linguistics classified the 3,077 occurrences independently into event or result
class. After this first annotation, we discussed the results with them and reached an
agreement in those cases in which the denotation type assigned was not the same. It
should be noted that the aim of this analysis was to encourage reflection on the suit-
ability of these seven criteria and on finding new clues to interpret the nominalizations
semantically.
During the classification procedure, we observed that these two denotations did
not allow us to account for all the data in the corpus. On the one hand, it is not
always possible to distinguish between event and result, because the linguistic context
(the sentence) is not always informative enough. We label such cases underspecified
types, resulting finally in three possible denotation values. On the other hand, we
noticed that some nominalizations can take part in a lexicalized construction, thus,
we added the attribute <lexicalized>. In such cases, we distinguish between six types
of lexicalization according to their similarity to different word classes: nominal (e.g.,
s??ndrome de abstinencia, ?withdrawal symptoms?), verbal (e.g., estar de acuerdo, ?to be in
agreement?), adjectival (e.g., al alza, ?rising?), adverbial (e.g., con cuidado, ?with care?),
prepositional (e.g., en busca de, ?in search of?), and conjunctive (e.g., en la medida que,
Table 2
Linguistic criteria for distinguishing between result and event nominalizations from different
authors.
Criteria Grimshaw?90 Alexiadou?01 Picallo?99 Alonso?04 Badia?02
Verbal Class - + + - +
Pluralization + - - + -
Determiner Type + - + + -
Preposition+Agent - - + - +
Internal Argument + - + - -
External Arguments + - - - -
Verbal Predicates + - + - +
837
Computational Linguistics Volume 38, Number 4
?as far as?). One of the three denotation values?event, result, underspecified?is assigned
to the whole lexicalized construction only in the case of nominal lexicalizations. It is
important to recognize such lexicalized cases because if the nominalization takes part
in a lexicalized construction other than the nominal, it does not receive a denotation
(a semantic distinction that is only associated with nouns).
The 3,077 occurrences were classified into 1,121 senses considering that different
denotations associated with a lemma are different senses. Henceforth, we refer to them
as nominalization senses. Among these 1,121 senses, 807 were annotated as result (72%),
113 as event (10%), 131 as underspecified (12%), and 70 as non-nominal lexicalized noun
(6%). It is not surprising that result nominalizations are the most frequent because events
tend to be realized mostly by verbal clauses and nominalizations are more frequently
used for the result (non-dynamic) meaning, more typical of nouns.
The fact that AnCora-ES is annotated with different linguistic information levels
allowed for the evaluation of the seven morphosyntactic and semantic criteria se-
lected. Next, we briefly present each criterion, how they were applied, and the results
obtained.10
4.1.1 Verbal Class.One of the most commonly used criterion to determine the denotation
is the verbal class from which the nominalization is derived (Picallo 1999; Alexiadou
2001; Badia 2002). It is claimed that unergative and stative verbs give rise to result
nominalizations, and unaccusative verbs usually result in ambiguous, or what we call
underspecified, nominalizations. Regarding transitive verbs, they give rise to either event,
result, or underspecified nominalizations. To analyze this criterion, we set out from the
semantic verbal classification proposed in AnCora-Verb. In this verbal lexicon, each
predicate is associated with one or more semantic classes depending on the four basic
types of events (Vendler 1967) (accomplishments which correspond to transitive verbs;
achievements corresponding to unaccusative verbs; states corresponding to stative
verbs; and activities corresponding to unergative verbs) and on the diathesis alterna-
tions in which the verb can occur. We therefore looked up the verbal classes from which
the 817 nominalizations are derived in AnCora-Verb. This allowed us to determine
whether the claims about the relation between the nominalization denotation type and
the corresponding verbal classes are valid.
In the sample analyzed, most of the nominal senses were classified as results
(72%), thus, it should not surprise us that all the verbal classes have a wide per-
centage of result nominalizations. The most significant result, however, is that stative
and unergative verbs lead nearly exclusively to result nominalizations in Spanish,
97% and 100%, respectively. Regarding transitive and unaccusative verbs, they lead to
event, result, or underspecified nominalizations. It is also interesting to remark that event
nominalizations derive mostly from transitive verbs (15%, in contrast to the 1% derived
from achievement verbs) and underspecified nominalizations derive from unaccusative
verbs (28%, in contrast to the 11% and 3% derived from transitive and state verbs,
respectively), confirming the hypothesis stated by Picallo (1999), Alexiadou (2001), and
Badia (2002).
4.1.2 Pluralization. According to Grimshaw (1990) and Alonso (2004), one of the features
that clearly identifies result nominalizations is their pluralization capacity because it is
10 In the following criteria we do not consider lexicalized senses because the criteria do not apply to these
complex lexical units.
838
Peris, Taule?, and Rodr??guez Empirical Methods for the Study of Denotation in Nominalizations
more usual to quantify objects than events. It is important to point out, however, that
it is also possible to make an event reading of a plural nominalization. For instance, in
Example (11) bombardeos (?shelling?) refers to multiple actions of bombing, therefore, it
is open to an event reading.
(11) Los bombardeos<event>de Sarajevo por parte del eje?rcito Bosnio.
?The shelling<event> of Sarajevo by the Bosnian army.?
This criterion was measured taking into account whether the 817 nominalizations
appeared in the plural in some of their occurrences in the sample analyzed. The results
obtained (98% of the nominalizations in the plural were classified as result and the
remaining 2% as underspecified) confirmed plurality as one of the features able to detect
result nominalizations. In contrast, the singular feature is not informative enough to
discard any of the nominal denotations (69% of the nominalizations were classified as
results, 15% as events, and 16% as underspecified type). Therefore, in the sample analyzed
all event nominalizations and most of the underspecified nominalizations appeared only
in the singular.
4.1.3 Determiner Types. Authors such as Grimshaw (1990), Picallo (1999), and Alonso
(2004) claim that event nominalizations usually appear with definite articles whereas
result nominalizations may be specified by all types of determiners. For instance,
demonstrative determiners can only specify result nominalizations because this type
of determiner is used to refer to an entity in a frame of reference. In order to evaluate
this criterion we took into account the types of determiners combining with the nomi-
nalization and also whether the noun appeared without a determiner.
Because most of the nominal senses were classified as results (72%), it should not
surprise us that all types of determiners have a wide percentage of result nominaliza-
tions. A striking result observed in Table 3, however, is that indefinite articles (99%),
demonstratives (100%), and quantifiers (100%) nearly always appear with result senses.
In contrast, the definite article, the possessive, and the empty position can occur in all
nominalization classes. Seventy-two percent of definite articles appear with result, 13%
with event, and 15% with underspecified nominalizations; 82% of possessive determiners
appear with result, 10% with events, and 8% with underspecified nominalizations; and
88% of nominalizations without determiner are classified as result, 5% as events, and 7%
as underspecified nominalizations. The data therefore partially confirm the hypotheses
from the literature: Result nominalizations appear with a wider range of determiners.
Although event nominalizations are not always specified by a definite article, they can
also appear with possessive determiners or without any determiner.
4.1.4 Preposition Introducing the Agent. In Spanish nominalizations derived from tran-
sitive verbs are considered to be results if the agent complement is introduced by the
preposition de (?of?) and events if the preposition used is por (?by?) (Picallo 1999).11 We
took into consideration agent complements introduced by prepositions appearing in
the sample analyzed. As shown in Table 3, Prepositional Phrases (PPs) interpreted as
agents in the NPs analyzed are introduced by the following prepositions: de (?of?), entre
(?between?), por (?by?), and por parte de (?by?). We observed that the distribution of the
four PPs is complementary between event and result denotations: The agent nominal
11 A similar claim is stated by Badia (2002) for Catalan.
839
Computational Linguistics Volume 38, Number 4
Table 3
Distribution of the denotation types according to the criteria evaluated.
Criteria Values Result (%) Event (%) Underspecified (%)
Verbal Class
Accomplishments 74 15 11
Achievements 71 1 28
States 97 ? 3
Activities 100 ? ?
Pluralization
Plural 98 ? 2
Singular 69 15 16
Determiners
Definite 72 13 15
Indefinite 99 ? 1
Demonstrative 100 ? ?
Possessive 82 10 8
Quantifier 100 ? ?
No Determiner 88 5 7
Preposition-Agent
de ?of? 98 ? 2
entre ?between? 100 ? ?
por ?by? ? 100 ?
por parte de ?by? ? 100 ?
Internal argument
Possessive 41 38 21
PPs 53 25 22
Relative Pronoun 71 29 ?
Relational Adjectives 97 ? 3
External argument
por ?by? PPs ? 100 ?
Relational Adjectives 100 ? ?
Possessive 95 ? 5
Predicates
Attributive 75 6 18
Eventive 44 41 15
complement introduced by de or entre occurs with result nominalizations (98% and
100%, respectively) and the agent nominal complement introduced by por or por parte
de occurs with event nominalizations (100% both), corroborating the hypothesis put
forward by Picallo (1999).
4.1.5 Internal Argument. The internal argument criterion proposed by Grimshaw (1990)
and Picallo (1999) states that only event deverbal nominalizations require the presence
of an internal argument because they are more similar to verbs, whereas in result-
nominalized NPs the internal argument is not needed. Badia (2002) argues that the real-
ization of this argument is not always compulsory to obtain an event interpretation of the
nominalization. To analyze this criterion, we observed those nominalized NPs in which
the internal argument was explicit and the type of argument that realized it. As a result,
we observed that the majority of event nominalizations are complemented by an inter-
nal argument (98%). This is also the case for underspecified nominalizations in a fairly
high percentage (78%). The percentage decreases considerably in result nominalizations
(34%), however. Therefore, the data seem to confirm Picallo?s and Grimshaw?s hypoth-
esis. Table 3 shows that there are four constituents that realize an internal argument:
possessive determiners, PPs, genitive relative pronouns, and relational adjectives. The
840
Peris, Taule?, and Rodr??guez Empirical Methods for the Study of Denotation in Nominalizations
first two constituents appear in the three nominal denotation types: 41% of possessive
determiners appear with result nominalizations, 38% with events nominalizations, and
21% with underspecified nominalizations; and 53% of PPs complement result, 25% events,
and 22% underspecified nominalizations. Relative pronouns only occur with event (29%)
and result (71%) denotation types, and, finally relational adjectives occur nearly exclu-
sively in result nominalizations (97%). This last fact constitutes an identification feature
for result nominalizations, as Picallo states (see next criterion).
Table 3 shows the results obtained for each criterion.
4.1.6 External Arguments vs. Possessors. Grimshaw (1990) states that PPs introduced
by the preposition ?by? (by-PPs), relational adjectives, and possessive determiners in
English would be interpreted as external arguments (subjects) in the case of event
nominalizedNPs. These constituents, however, would be interpreted as possessors (that
is, as non-argumental) in result nominalized NPs. Other authors, like Picallo (1999),
nevertheless, claim that possessive determiners may be argumental in result and event
nominalizations in Spanish. Regarding relational adjectives, Picallo argues that these
constituents can only be arguments of result nominalizations. As seen, there is no
consensus among authors; therefore, it seemed to us to be an interesting criterion to
contrast. We observed whether these constituents (by-PPs, relational adjectives, and
possessive determiners) were interpreted as external arguments in the nominalized NP
sample. If this was so, we also analyzed whether the fact of being external arguments
conditioned the denotation of the nominalization.12
The results obtained are very clear: PPs introduced by por (?by?) with an agent
interpretation only occur in NPs headed by event nominalizations. Relational adjectives
are interpreted as external arguments only in NPs headed by result nominalizations.
Possessive determiners with an agent interpretation are mostly (95%) constituents of
NPs headed by result nominalizations though they can also be constituents (5%) of
NPs headed by underspecified nominalizations. Therefore, for Spanish, Grimshaw?s hy-
pothesis is only partially corroborated because only by-PPs guarantee the event read-
ing. Regarding relational adjectives, Picallo?s thesis is confirmed, because this type of
constituent mostly appears as an argument of result nominalizations. Moreover, we
observed a preference for possessive determiners to be external arguments of result
nominalizations, which is not stated in any of the theoretical proposals.
4.1.7 Verbal Predicates. The type of verbal predicate that can be combined with nominal-
izations may be an indicator for determining the denotation (Grimshaw 1990; Picallo
1999; Badia 2002). Result nominalizations tend to combine with attributive predicates,
whereas event nominalizations tend to be subjects of predicates such as tener lugar (?to
take place?) or ocurrir (?to happen?) because these predicates tend to select event type
subjects. In order to examine this criterion, we analyzed the types of predicates com-
bined with the 817 nominalization types. We observed whether the predicates belong
to the event-denoting class (tener lugar, ?to take place?; ocurrir, ?to happen?; comenzar,
?to begin?; acabar, ?to finish?; durar, ?to last?; llevar a cabo, ?to carry out?) or if they were
attributive predicates (ser, ?to be?; estar, ?to be?; parecer, ?to seem? ). Table 3 illustrates that
12 The way we decided whether these constituents were external arguments consisted of paraphrasing
the nominalized NPs into clause structures in order to see if they were semantically equivalent to
verbal subjects.
841
Computational Linguistics Volume 38, Number 4
attributive predicates tend to choose result nominalizations (75%) as subjects whereas
eventive predicates do not show a clear preference for any type of nominal: Forty-
four percent of them combine with result, 41% with events, and 15% with underspecified
nominalizations. These results partially confirmedwhat is stated by these three authors:
result nominals combine preferentially with attributive predicates.
From the corpus-based study, we conclude that the semantic distinction between
event and result nominalizations is not always as clear as is stated in the literature. The
criteria proposed in the literature are well suited to constructed examples but when
they are applied to naturally occurring data they do not work so well: Some of them
cannot be applied and sometimes we found contradictory criteria in the same example.
That said, it is important to point out that these criteria are not irrefutable proofs for
making an event or a result reading, but rather indicators that can help us to strengthen
our semantic intuition. In fact, we propose the third denotation type underspecified for
those cases in which semantic intuition is insufficient and the criteria for reinforcing one
of the two main denotation types are not clear.
Regarding the criteria established in the literature, the main conclusion drawn
is that not all the criteria analyzed seem to hold for Spanish. Among the evaluated
criteria, those that appear to be most helpful for distinguishing between event and result
nominalizations are: 1) the semantic class of the verb from which the noun is derived;
2) the pluralization capacity; 3) the determiner types; 4) the preposition introducing an
agentive complement; and 5) the obligatory presence of an internal argument. These
features are represented as attributes in the nominal lexical entries of the AnCora-Nom
lexicon (see Section 5.3).
It is interesting to note that the number of criteria found that reinforce result read-
ings is significantly higher than the number of criteria found that strengthen event
readings. In every criterion we find features that support the identification of result
nominalizations but not event nominalizations. To support result nominalizations the
following features were found: nominalizations deriving from unergative and stative
verbs; nominalizations appearing in the plural; nominalizations specified by an indeter-
minate article, a demonstrative, or quantifier determiner; nominalizations with an agent
complement introduced by de (?of?) or entre (?between?); the nonrealization of the inter-
nal argument; and nominalizations having relational adjectives as arguments and the
attributive predicate combined with them. In order to underpin event nominalizations,
however, the only unambiguous criterion found was when the preposition introducing
a PP agent complement is por (?by?) or por parte de (?by?).13 If we take into account that
the agent complement is mostly optional in an NP configuration, it is very difficult to
find a criterion within the NP context to support event nominalizations.
We believe that there are more features to support result nominalizations because
they are closer to non-derived nouns and, like them, admit a wide variety of config-
urations: plural inflection, different types of determiners, the possibility of appearing
without complements, and so forth. In contrast, event nominalizations (since they are
not typical nouns because they denote an action), like verbs, do not admit this variety
of configurations: They rarely appear without complements, admit fewer types of deter-
miners, and appear in the plural less frequently. Most of the configurations they admit
are also admitted by result nominalizations; this explains why there are more criteria
to support result than event nominalizations.
13 Literally, ?on the part of.?
842
Peris, Taule?, and Rodr??guez Empirical Methods for the Study of Denotation in Nominalizations
In fact, the remaining criteria?the fact of deriving from transitive or unaccusative
verbs; the nominalization being in the singular; the co-occurrence with a definite ar-
ticle, a possessive determiner, or without any determiner; the presence of the internal
argument; and the combination with typically eventive predicates?do not support any
specific denotation. As a result, there are several cases where it is very difficult to assign
a denotation, especially when the context is not clear enough, and therefore, we need
to apply the underspecified tag.
In the next section, we present other indicators found in the empirical study that
provide us with clues for the differentiation between event and result nominalizations.
These indicators are data-driven and we can only guarantee that they work for Spanish.
4.2 Finding New Clues to Support Event and Result Denotations
The analysis of 3,077 nominalization occurrences, focusing on the semantic distinction
between those denoting an event, result, or underspecified type, has allowed us to find
new clues that strengthen these readings, especially the event reading.
One of the clearest clues for detecting the event nominalizations is the possibility of
paraphrasing the NP with a clausal structure, as we saw in Section 1, Examples (3)?
(6). Another valuable clue is to check whether the nominalization admits an agent
complement introduced by por (?by?) or por parte de (?by?). We use this criterion because
it is the most informative one to support event nominalizations but it is also a very
optional complement and is scarcely represented in the corpus. The annotators could
use these two tests to decide the denotation type. Therefore, they had two extra criteria
that the data did not provide.
Furthermore, we found other indicators that can help to select one denotation
type, the so-called selectors. We identified two types of selectors:
1. External selectors: Prepositions like durante (?during?), nouns like proceso
(?process?), adjectives like resultante (?resulting?), verbs like empezar
(?begin?), and adverbs en v??a de (?on the way to?), which are elements that
point to a specific denotation from outside the nominalized NP. For
instance, in Example (12) the preposition durante (?during?) gives a clue to
interpret presentacio?n (?presentation?) as an event.
2. Internal selectors: Prefixes within the nominalization that indicate a
specific denotation type; for instance, a noun with the prefix re-with a
reiterative meaning such as reubicacio?n (?relocation?) in Example (13). This
is due to the fact that the reiterative meaning only applies to bases that
denote actions.
(12) Durante [la presentacio?n<event>del libro], e?l abogo? por la formacio?n de
los investigadores en innovacio?n tecnolo?gica.
?During [the presentation<event> of the book], he advocated the training
of researchers in technological innovation.?
(13) Hoy [la reubicacio?n<event>del ex ministro] no resulta fa?cil.
?Today, [the relocation<event> of the ex minister] does not seem easy.?
These new clues allow us to support our semantic classification independently from
the literature criteria under evaluation. The only inconvenience of these tests and the
selectors is that they cannot be implemented as features in the ADN-classifier.
843
Computational Linguistics Volume 38, Number 4
5. Knowledge Resources
This section presents the linguistic resources used in building the final version of the
ADN-Classifier (R3). We briefly describe the AnCora-ES corpus and the AnCora-Verb
lexicon, and we focus in more detail on the description of the AnCora-Nom lexicon
from which we obtain most of the features for the building of the ADN-Classifier.
5.1 AnCora-ES Corpus
AnCora-ES is a 500,000 word (henceforth, 500kw) Spanish corpus14 consisting of news-
paper texts annotated at different linguistic levels: morphology (part of speech and
lemmas), syntax (constituents and functions), semantics (verbal argument structure,
thematic roles, semantic verb classes, named entities, and WordNet nominal senses),
and pragmatics (coreference).15 The corpus contains 10,678 fully parsed sentences. As
we explained in Section 3, nominalization occurrences (23,431) were automatically an-
notated with denotation types using an intermediate model of classification (see step 5
in Figure 1). This automatic annotation was then manually validated by three graduate
students in linguistics. These annotators were selected from a group of five, because
they achieved an observed agreement of over 80%, corresponding to a kappa of 65% in
an inter-annotator agreement test, whereas the average observed agreement was 75%
corresponding to a 60% kappa. For the purpose of annotation, the three annotators took
into account the semantic definition we provided, the criteria presented in Section 4.1,
and the semantic tests described in Section 4.2. The inter-annotator agreement was
carried out to ensure the consistency and quality of the AnCora-ESmanual annotation.16
Therefore, the AnCora-ES corpus enriched with denotation type annotation is used
for learning the different models of the ADN-Classifier-R3. From this resource we
obtained two kinds of features:
(a) The corpus versions of the features from the lexicon (see Section 5.3): the
type of determiner used in Section 4; the number (plural or singular) in
which the nominalization occurrences appear; and the constituent type of
the complements.
(b) The contextual features such as the tense and the semantic class of the verb
that dominates the nominalization in the sentence; the syntactic function
of the NP headed by a nominalization; and whether the noun appears in a
named entity.
We use the Tgrep217 tool for the feature extraction from the corpus; this allows us
to efficiently inspect the syntactic trees in a Treebank format.18
14 A similar version exists for Catalan, AnCora-CA.
15 AnCora-ES is the largest multilayer annotated corpus of Spanish freely available at:
http://clic.ub.edu/corpus/ancora.
16 For more details on the manual validation and the inter-annotator agreement test, see Peris, Taule?, and
Rodr??guez (2010).
17 http://tedlab.mit.edu/ dr/TGrep2/. Tgrep2 is an improvement of Tgrep. Both tools are tree-based
counterparts of the widely used string searching Unix Grep tool.
18 In the following link the set of tgrep rules as well as some implemented examples are available:
http://clic.ub.edu/corpus/en/documentation.
844
Peris, Taule?, and Rodr??guez Empirical Methods for the Study of Denotation in Nominalizations
5.2 AnCora-Verb
AnCora-Verb-ES is a verbal lexicon that contains 2,830 Spanish verbs.19 In this lex-
icon, each predicate is related to one or more semantic classes, depending on its
senses, basically differentiated according to the four event classes?accomplishments,
achievements, states, and activities (Vendler 1967; Dowty 1979)?and on the diathe-
ses alternations in which a verb can occur (Va?zquez, Ferna?ndez, and Mart?? 2000).
The semantic class of the verb base of the nominalization is used as a feature in the
ADN-Classifier.
5.3 AnCora-Nom
This section presents AnCora-Nom,20 a Spanish lexicon of deverbal nominalizations
that has been iteratively used and improved as a result of the experiments reported
here. At present, it contains 1,655 lexical entries corresponding to 3,094 senses and
3,204 frames. These lexical entries represent the lemmas corresponding to the 23,431
deverbal nominalization occurrences appearing in the annotated AnCora-ES corpus.
For each of these lemmas we created a lexical entry using the information annotated in
the corpus.21 The features of each lexical entry are organized in three levels: lexical entry,
sense, and frame level. The lexical entry attributes are not extracted from the corpus but
added in order to document the lexical entry. Sense and frame attributes, in contrast,
were extracted from the AnCora-ES corpus. Each lexical entry is organized in different
senses, which were established taking into account the denotation type, the sense of the
base verb, and whether or not the nominalization is part of a lexicalized construction. In
turn, each sense can also contain one or more nominal frames, depending on the verbal
frame fromwhich the nominalization is derived. Next, we detail the attributes specified
in the three levels described above. Figure 2 shows the full information associated with
the lexical entry aceptacio?n (?acceptance?).
5.3.1 Lexical Entry Level Attributes. These are as follows:
(a) Lemma. In Figure 2, the value for this attribute is the noun aceptacio?n
(lemma=?aceptacio?n?).
(b) The attribute language (?lng?) codifies the language represented in the lexical
entry. AnCora resources work with Spanish and Catalan, so the values of this attribute
are ?es? for Spanish (lng=?es?) and ?ca? for Catalan (lng=?ca?). At present, AnCora-
Nom only deals with Spanish nominalizations.
(c) The attribute origin indicates the type of word from which the nouns are de-
rived. In Figure 2, the value for this attribute is ?deverbal?, meaning that this lexical
entry concerns a noun derived from a verb. At present, AnCora-Nom only contains
deverbal nouns but in the future it will include other types of nominalizations such as
deadjectivals.
(d) The attribute type refers to the word class, ?noun? in Figure 2.
19 A similar version exists for Catalan, AnCora-Verb-CA.
20 We describe here AnCora-Nom-R3, the final version of the lexicon.
21 For a detailed explanation of the automated extraction process see Peris and Taule? (2011).
845
Computational Linguistics Volume 38, Number 4
Figure 2
Aceptacio?n (?acceptance?) lexical entry in AnCora-Nom.
846
Peris, Taule?, and Rodr??guez Empirical Methods for the Study of Denotation in Nominalizations
5.3.2 Sense Level Attributes. These include:
(e) The attribute cousin marks whether the nominalization is morphologically de-
rived from a verb (cousin=?no?, in Figure 2) or is a cousin noun (cousin=?yes?). Cousin
nouns (Meyers, Reeves, andMacleod 2004) are nouns that give rise to verbs (e.g., relacio?n
[?relation?] >relacionar [?to relate?]), or nouns semantically related to verbs (e.g., escarnio
[?mockery?] is related to mofarse [?to make fun?]).
(f) The denotation attribute indicates the semantic interpretation of the deverbal
noun. The possible values are: ?event,? ?result,? and ?underspecified.? In Figure 2, there
are two senses, the first one being result (denotation=?result?) and the second one event
(denotation=?event?).
(g) Each sense contains an identifier (?id?) to indicate the sense number.
(h) The lexicalized attribute indicates whether or not the nominalization is part
of a lexicalized construction (idiomatic expression) (Figure 2: lexicalized=?no?). In the
first case, two additional attributes are added: (i) the alternative-lemma, specifying
the whole lexicalized construction of which the nominalization is part (for instance,
alternative-lemma=?golpe de estado,? [?coup d?etat?]), and (ii) lexicalization-type, to
distinguish between the six types of lexicalizations: ?nominal,? ?verbal,? ?adjectival,?
?adverbial,? ?prepositional,? or ?conjunctive? (see Section 4). We should bear in mind
that one of the three above-mentioned denotation values is assigned to the whole
lexicalized construction only in the case of nominal lexicalizations. For instance, the
lexicalized construction golpe de estado is a nominal lexicalization (lexicalization-type=
?nominal?), and therefore, it has a denotation value (denotation=?result?).
(i) The attribute originlemma specifies the verb lemma from which the noun is
derived. In Figure 2, the value for this attribute is ?aceptar? in both senses (origin-
lemma=?aceptar?).
(j) Because verbs can have different senses, the attribute originlink indicates the
concrete verbal sense of the base verb. In Figure 2, the originlink attribute takes the
same value in both senses: ?verb.aceptar.1? (oringinlink=?verb.aceptar.1?).
(k) Because nouns in the AnCora corpus are annotated with WordNet synsets,22 we
incorporate this information in the attribute wordnetsynset. In Figure 2, the first sense
of aceptacio?n corresponds to two synsets (wordnetsynset=?16:00117820+16:10039397?),
whereas the second only corresponds to one (wordnetsynset=?16:00117820?). It should
be noted that senses in AnCora-Nom are coarser grained than in WordNet: a sense can
group together more than one WordNet synset.
5.3.3 Frame Level Attributes. These are detailed as follows:
(l) The attribute type indicates the verbal frame from which the nominalization is
derived. In AnCora-Verb, each verbal sense can be realized in more than one frame:
default, passive, anticausative, locative, and so forth. In the nominal entries, we mark
the corresponding verbal frames, which are the possible values for this attribute. In
most cases, its value is ?default? as in Figure 2 (type=?default?). This feature is needed
to look for the corresponding verbal semantic class in AnCora-Verb.
22 We used WordNet 1.6 for Spanish and WordNet offsets for identifying synsets.
847
Computational Linguistics Volume 38, Number 4
(m) Argument (Structure). In this complex attribute, the different arguments
(argument) and the corresponding thematic roles (thematicrole) are specified. To
represent the arguments we follow the same annotation scheme used in AnCora-
Verb. For instance, in Figure 2, the event sense has one argument (?arg1?) with a
patient thematic role (?pat?). This argument is realized twice (frequency=?2?) by
a prepositional phrase (constituent type =?sp?) introduced by the preposition de
(?of?) (preposition=?de?) and once by a possessive determiner (type=?determiner,?
postype=?possessive?).
(n) The attribute referencemodifier represents the nominal complements that are
not arguments but which modify the reference of the nominalization. Frequency is
also taken into account. Strictly speaking, this attribute does not fit perfectly at the
frame level. We were interested in representing this information, however, and the
most suitable level was the frame level because it allows for a seamless comparison
of argumental and nonargumental nominal complements.
(o) The type of determiner has proved to be a useful criterion for distinguishing
between result and event readings, so we include this information in the attribute
specifier.23 The possible values are: ?article,? ?indefinite,? ?demonstrative,? ?exclama-
tive,? ?numeral,? ?interrogative,? ?possessive,? ?ordinal,? and ?void? when there is
no determiner. In this attribute, we also take into account the frequency with which
the determiners are realized. In Figure 2, the event sense is specified twice (constituent
frequency=?2?) by an article determiner (type=?determiner,? postype=?article?).
(p) The attribute appearsinplural indicates whether or not an occurrence of a
nominalization in a particular frame appears in the plural. It is a boolean attribute. In
Figure 2, neither of the senses appear in the plural, thus, the value is ?no.?
(q) Finally, each lexical entry also contains all the examples from which the infor-
mation has been extracted, specifying the corpus file, the node path, and the sentence
in which each is located.
6. ADN-Classifier
As stated previously, our goals for building the ADN-Classifier were twofold: On the
one hand, to have at our disposal a tool to help us to quantitatively evaluate the validity
of our claims regarding deverbal nominalizations as discussed in Section 4; and, on the
other hand, to provide a classification tool able to take advantage of all the available
information in a specific scenario in the automatic classification of a deverbal noun. The
aim of the task is to classify a deverbal nominalization candidate in an event, result, or
underspecified denotation type, as well as to identify whether the nominalization takes
part in a lexicalized construction (idiomatic expression). Therefore, we model the task
as a four-way classification problem. In order to achieve these goals, some functional
requirements on the software to be built were necessary. Regarding the first goal, we
required that a tool be able to:
1. Use all the properties discussed in Section 4 as features for classification.
23 The name of the attribute refers to the syntactic position that determiners occupy in the NP;
the determiners specify the nominalization.
848
Peris, Taule?, and Rodr??guez Empirical Methods for the Study of Denotation in Nominalizations
2. Tune the features: binarization, grouping the possible values,
generalization, combination of features, and computation of
derived features.
3. Perform feature selection.
4. Facilitate the human interpretation of the model used by the classifier.
5. Analyze the accuracy of the individual features.
6. Use either senses or lemmas corresponding to deverbal nominalization
candidates as units for classifying.
In order to achieve the first aim, the first version of the ADN-Classifier (R1) (Peris,
Taule?, and Rodr??guez 2009) was developed. This is the basis for the building of the
intermediate and final versions of the ADN-Classifier. The final version is presented in
detail next. Obviously, the second goal imposes heavier constraints on the design of the
tool (the ADN-Classifier-R3). As is usual in other lexical classification tasks, such as Part
Of Speech (POS) tagging or WSD, a word taken as an isolated unit is ambiguous but
can be disambiguated, or at least partially disambiguated, if enough context is taken
into account. An additional constraint is that the nominalization candidate has to be
tagged as a noun. For our classification task at least four processes are carried out: 1)
tokenization; 2) segmentation at sentence level; 3) POS tagging; and 4) localization of
a nominalization candidate by means of a set of regular expressions looking for verbal
nominalization endings.24
In this setting, a case for classifying consists of a nominalization candidate using
the POS-tagged sentence where it occurs as context, although this context is not always
sufficient for disambiguation. Additional processes could be carried out on the nomi-
nalization candidate and the sentence (WSD, chunking, full parsing, SRL, linking of the
nominalization candidate with the origin verb, etc.). Each of these processes increases
the number of possible features used for classifying but, because they are not error
free, they could involve a decrement in the global accuracy of the preprocess step.
Therefore, a careful examination of the processes, their accuracy, and the improvement
of classification accuracy is needed. For instance, performing WSD on the nominal-
ization candidate could allow for the use of sense-based features and, thus, a clear
improvement in classification accuracy. The inconvenience is that the state-of-the-art
accuracy rate of WSD is not very promising. In recent SemEval challenges, the accuracy
rate in All-Words tasks is between 60% and 70% for a baseline of 51.4% using the first
sense in WordNet, and 89% in Lexical-Sample tasks for a baseline of 78% (Chklovski
and Mihalcea 2002; Decadt et al 2004; Pradhan et al 2007).25 These figures depend
on the sense inventory used for disambiguation: The All-Words task uses fine-grained
senses (WordNet synsets) and the Lexical Sample task uses more coarse-grained sense
inventory (Ontonotes senses).
Therefore, we approach the problem of classification taking into account different
feature sets which come from different knowledge resources, and we examine and
evaluate the task performance when a decreasing number of knowledge resources
are used. Depending on the available knowledge resources and natural language
24 We used 10 suffixes such as -cio?n (see Section 4).
25 All these figures are for English. To have some idea of the relative difficulty of the task for Spanish
we have measured this baseline in Ancora-ES resulting in a value of 42%, that is, an 18% drop with
respect to English.
849
Computational Linguistics Volume 38, Number 4
Table 4
Description of the scenarios used for evaluation.
Scenario Knowledge Resources Features level NL Pre-Process
1 AnCora-Nom+AnCora-Verb lemma POS
2 AnCora-Nom+AnCora-Verb sense POS+WSD
3 AnCora-Nom+AnCora-Verb lemma POS+Parsing
4 AnCora-Nom+AnCora-Verb sense POS+WSD+Parsing
5 AnCora-Nom lemma POS
6 AnCora-Nom sense POS+WSD
7 AnCora-Nom lemma POS+Parsing
8 AnCora-Nom sense POS+WSD+Parsing
9 ? lemma POS+Parsing
10 ? lemma POS+Parsing+SRL
(NL) processors, we designed the classification task in different scenarios, which are
presented in Table 4. The columns include the knowledge resources used in each
scenario (column 2), whether the features used are extracted at sense or lemma level
(column 3), and the NL processors that are necessary in each case.
Scenario 1 in Table 4 presents the case in which the nominal lexicon (AnCora-
Nom in our case) is available and the nominalization candidate is an entry in this
lexicon. The sentence where the nominalization candidate occurs is POS-tagged and
no other NL processes are carried out.26 In this case, we apply the ADN-Classifier with
a model learned using only features coming from the lexicon at the lemma level with
Acclemma;lex accuracy. Scenario 2 is the same as Scenario 1 but adds a WSD process to
the nominalization candidate with AccWSD accuracy. In this case, we apply the ADN-
Classifier with a model learned using only lexicon features at a sense level achiev-
ing Accsense;lex accuracy. Obviously, applying this model is only useful if Acclemma;lex?
Accsense;lex outperforms the expected WSD error (1 ? AccWSD). Scenario 3 is the same as
Scenario 1 but adds constituent parsingwith Accparser accuracy. In this case, we apply the
ADN-Classifier with amodel learned using lexicon and corpus features27 at lemma level
with Acclemma;lex+corpus accuracy. Again, thismodel is only useful if the Acclemma;lex+corpus?
Acclemma;lex outperforms the expected parsing error (1 ? Accparser). Scenario 4 consists of
a combination of Scenarios 2 and 3. Scenarios 5, 6, 7, and 8 reproduce Scenarios 1, 2, 3,
and 4, respectively, without using the features extracted from the AnCora-Verb lexicon,
so obtaining the origin verb of the candidate is not necessary. In Scenario 9, the nominal
lexicon is not available or the nominalization candidate is a noun that does not occur
in the nominal lexicon, and only the features extracted from the parsed tree at lemma
level are used. Finally, Scenario 10 is the same as Scenario 9 but adds an SRL process
in order to obtain argument structure information. Taking into account these two sets
of requirements the final version of the ADN-Classifier (R3) has been built.
We used ML techniques to build the ADN-Classifier. Specifically, we used the
J48.Part rule-based classifier, the rule version of the decision-tree classifier C4.5.Rules
(Quinlan 1993) as implemented in the Weka toolbox (Witten and Frank 2005). We chose
a rule-based classifier because it provides a natural representation of classification rules,
thus allowing for the inspection of the model without diminishing accuracy and it
26 POS-tagging implies previous tokenization and sentence segmentation steps.
27 The parse tree obtained can be inspected in the same way as AnCora-ES.
850
Peris, Taule?, and Rodr??guez Empirical Methods for the Study of Denotation in Nominalizations
allows us to perform a ranking of the individualized rules and a definition of a thresh-
olding mechanism for performing a precision oriented classification. Because most of
the features are binary and the others are discrete with small range values, using more
complex rule-based classifiers such as Cohen?s Ripper supposes no real improvement
over our choice.28
The ADN-Classifier therefore consists of the J48.Part classifier within the Weka
toolbox, the appropriate learned model (from the set described in Section 7.2 and listed
in Table 5), and the list of features to be used. During the exploitation phase the input to
the system consists of a table. Each row in the table corresponds to a case for classifying
and each column to the values of the corresponding feature. The result of the process
is a column vector containing the result of classifying each instance.
7. Experiments and Evaluation
In this section we present and evaluate the experiments carried out with the ADN-
Classifier. First we present the settings of these experiments, then we focus on the
experiments themselves, and finally we evaluate the results.
7.1 Setting
In order to validate the performance of the ADN-Classifier, a sequence of experiments
was conducted. Concretely, two sets of experiments were carried out: we experimented
with different models of the classifier structured in five dimensions (see Section 7.2) and
we applied the appropriate models in the different scenarios set out in Table 4. We use
a tenfold cross-validation method29 for the evaluation of these two sets of experiments.
In order to evaluate the features selected and to carry out the classification task in each
scenario we used the models learned as described in Section 7.2. As noted earlier, using
the ADN-Classifier for classify involves using the J48.Part classifier within the Weka
toolbox and the appropriate learned model.
7.2 Experiments
The experiments carried out with the ADN-Classifier-R3 are presented here. Firstly,
we describe those experiments related to the different models of the classifier and
secondly, we focus on how some of these models are applied in different scenarios.
We apply the ADN-Classifier in different modes that correspond to the following five
dimensions.
28 J48.Part learns first a decision tree and then builds the rules traversing all the branches of the tree. Ripper,
instead, learns the rules one by one (increasing the learning cost). This can result in a more accurate and
smaller rule set just in the case of splitting numerical attributes; that is not our case.
29 In n-fold cross-validation, the original sample is randomly partitioned into n subsamples. Of the n
subsamples, a single subsample is retained as the validation data for testing the model, and the
remaining n ? 1 subsamples are used as training data. The cross-validation process is then repeated n
times (the folds), with each of the n subsamples used exactly once as the validation data. The n results
from the folds can then be averaged (or otherwise combined) to produce a single estimation. The
advantage of this method over repeated random sub-sampling is that all observations are used for
both training and validation, and each observation is used for validation exactly once. n is commonly
set to 10 (McLachlan, Do, and Ambroise 2004).
851
Computational Linguistics Volume 38, Number 4
Table 5
Experiments and evaluation of the models. Legend of the model name: 1st letter
(S = sense-based, L = lemma-based); 2nd letter (S = sense, L = lemma, E = corpus example);
3rd letter (L = lexicon features, C = corpus features, A = all features); 4th letter (R = reduced
vocabulary, F = full vocabulary) and 5th letter (R = reduced corpus, F = full corpus).
Model Inst. Att. Rules Baseline (%) Accu. (%) ?error (%) Red?error (%)
S
e
n
se
-b
a
se
d
m
o
d
e
ls
S
e
n
se
s SSLRR 609 937 51 71.75 76.84 5.09 18.02
E
x
a
m
p
le
s
SSLRF 964 937 78 60.68 70.02 9.33 23.74
SSLFR 1,428 1,671 84 70.86 81.72 10.85 37.25
SSLFF 3,094 1,671 224 60.95 74.36 13.41 34.35
SELRR 1,840 937 42 85.32 93.80 8.47 57.77
SELRF 9,278 937 137 87.03 97.82 10.78 83.20
SELFR 3,994 1,671 117 83.92 93.69 9.76 60.74
SELFF 23,431 1,671 366 85.45 96.65 11.19 76.99
SECRR 1,840 197 35 85.32 83.96 ?1.35 ?9.25
SECRF 9,278 197 116 87.03 86.34 ?0.68 ?5.31
SECFR 3,994 197 81 83.92 82.72 ?1.20 ?7.47
SECFF 23,431 197 211 85.45 84.93 ?0.52 ?3.60
SEARR 1,840 1,133 76 85.32 91.57 6.25 42.59
SEARF 9,278 1,133 196 87.03 96.38 9.35 72.15
SEAFR 3,994 1,867 146 83.92 91.72 7.80 48.52
SEAFF 23,431 1,867 498 85.45 95.46 10.01 68.83
L
e
m
m
a
-b
a
se
d
m
o
d
e
ls
L
e
m
m
a
s LLLRR 242 852 6 90.90 88.84 ?2.06 ?22.72
E
x
a
m
p
le
s
LLLRF 242 852 6 90.90 88.84 ?2.06 ?22.72
LLLFR 532 1,559 14 89.84 89.66 ?0.18 ?1.85
LLLFF 972 1,559 26 87.55 89.09 1.54 12.39
LELRR 1,840 852 50 85.32 83.96 ?1.35 ?9.25
LELRF 9,278 852 76 87.03 86.88 ?0.15 ?1.16
LELFR 3,994 1,559 162 83.92 83.50 ?0.42 ?2.64
LELFF 23,431 1,559 322 85.45 85.62 0.16 1.14
LECRR 1,840 197 35 85.32 84.02 ?1.30 ?8.88
LECRF 9,278 197 116 87.03 86.35 ?0.67 ?5.23
LECFR 3,994 197 81 83.92 82.57 ?1.35 ?8.41
LECFF 23,431 197 211 85.45 84.86 ?0.58 ?4.04
LEARR 1,840 1,048 109 85.32 85.05 ?0.27 ?1.85
LEARF 9,278 1,048 355 87.03 87.64 0.61 4.7
LEAFR 3,994 1,755 236 83.92 85.27 1.35 8.41
LEAFF 23,431 1,755 981 85.45 87.20 1.74 12.00
? Application level. We distinguish between sense-based and lemma-based mod-
els. Sense-based models use the information from the AnCora-Nom-R3 lexicon at sense
level, that is, the features for learning (and classification) are associated with the specific
senses. In contrast, in lemma-based models, when extracting features from the lexicon,
we use as features for learning and classification those attributes whose values are
shared by all senses of the same lemma. Therefore, at this second level of application
the features are not so informative but, at the same time, we reduce our dependence
on the lexicon, which was a step that had to be taken to move towards a more realistic
scenario.
? Unit of learning and classification (i.e., the instance to be classified). These
sense- or lemma-based models are in turn distinguished depending on whether the
unit of learning and classification comes from the lexicon (sense or lemma) or from the
852
Peris, Taule?, and Rodr??guez Empirical Methods for the Study of Denotation in Nominalizations
AnCora-ES corpus (examples), that is, if they correspond to types or tokens. In the first
case all the features come from the lexicon, whereas in the latter contextual features,
extracted from the corpus, can also be used. Consequently, in sense-based models the
units used are senses (from the lexicon) or examples (from the corpus), and in lemma-
based models they are lemmas (from the lexicon) or examples (from the corpus). It has
to be taken into account that depending on this dimension, the number of instances
for learning varies: There are more senses than lemmas in the lexicon and there are
more nominalization occurrences (examples) in the corpus than nominalization senses
or lemmas in the lexicon.
? Features involved. The features used for both learning and classifying are ob-
tained from the lexicon (lexical features) or from the corpus (contextual features). The
different models are distinguished by using only lexical features, only contextual fea-
tures, or the combination of both types of features.
? Vocabulary size. The data sets taken into account correspond to a reduced set of
817 nominalization lemmas obtained from the 100kw subset of the corpus used for the
first version of the ADN-Classifier (R1) or to the full set of 1,655 nominalization lemmas
occurring in the whole AnCora-ES (500kw). Depending on this dimension, the number
of instances for learning also vary.
? Corpus size. Two corpus sets are used in the different models: a reduced subset
of 100kw used for the first version of the ADN-Classifier (R1) or the full 500kw corre-
sponding to the whole corpus.30 Depending on this dimension, the number of instances
for learning also vary.
In order to identify the models as presented in Table 5, we use five letters as
notation, each of which identifies one of these five dimensions. The first letter corre-
sponds to the application level: If the model is sense-based an S will be used for the
identification and an L in the case of lemma-based models. The second letter refers
to the unit of classification and is L for lemmas, S for senses, and E for examples. In
the third position the reference to the origin of the features involved in the model is
found: L (from the lexicon), C (from the corpus), A (from both resources, all features).
In fourth place, we refer to the vocabulary size: R (reduced) stands for the reduced
set of 817 nominalization lemmas and F (full) for the full set of 1,655 nominalization
lemmas. In last place, we also designate the corpus size by an R (reduced set of
100kw) or an F (full set of 500kw). Therefore a lemma-based model that uses examples
as units of classification, uses all the features, the whole vocabulary, and the whole
corpus is identified as the LEAFF model. In total we experimented with 32 different
models.31
For the different scenarios described in Section 6, we applied the appropriate
models so as not to use the noninformed features for each one.
7.3 Evaluation
The classifier performance of the different models was evaluated by a tenfold cross-
validation method. Next, we focus on the results of the 32 models resulting from the
five dimensions described in Section 7.2. Table 5 presents the overall results: the models
30 For obtaining the learning curve of some of our models intermediate sizes have been used.
31 Not all the combinations of values of the dimensions are allowed.
853
Computational Linguistics Volume 38, Number 4
used (column 1), the number of instances used for learning (column 2), the number of
attributes used, and the number of rules built by the classifier (columns 3 and 4), and
finally, the baseline, the accuracy, the decrease error over the baseline (?-error), and the
relative error-reduction ratio (Red-?-error) obtained by each model (columns 5, 6, 7,
and 8). The rows correspond to the different models presented. Recall that the names
of the models are assigned according to the five dimensions presented in Section 7.2.
It should be borne in mind here that in column 2, the number of instances for learning
depends on the type of unit used for learning and classification (senses in sense-based
models, lemmas in lemma-based models, and examples) and on the vocabulary and
corpus size. The interaction between these three dimensions also explains why the
figures for the baseline change for each model. The baseline is a majority baseline
which assigns all the instances to the result class. In general, when the unit used is
from the lexicon, the lemma baseline increases relative to the sense baseline. This is
because in lemma-based models we group the senses that share all the features under
a lemma; because different senses do not normally share all the features, in the end,
only monosemic lemmas are in fact taken into account. This fact, therefore, shows that
there are more result type monosemic lemmas than event and underspecified monosemic
lemmas. Furthermore, it is worth noting that when the unit of learning and classifi-
cation used are the examples from the AnCora-ES corpus and not the senses from
the lexicon, the baseline also increases. Therefore, it seems that proportionally result
nominalizations are more highly represented in the corpus than event and underspecified
nominalizations. Regarding the number of features used for learning, the type of feature
involved and the vocabulary size (when features from the lexicon are used) are the two
relevant dimensions. Finally, it should be said that the accuracy and the other two cor-
related measures are obtained by evaluating the performance of the different models by
tenfold cross-validation.
As can be seen in Table 5, the sense-based models (the first 16 rows) outperform
the corresponding lemma-based models (the last 16 rows). This is explained by the fact
that there are features in the lexicon coded at the sense level that cannot be recovered
at the lemma level because in lemma-based models we only use as features for the
classification those attributes whose values are shared by all senses of the same lemma,
and this does not commonly happen. At the sense level, the best results are achieved
when the features used in the classification come exclusively from the lexicon, with
the unit of classification being senses from the lexicon (the first block of four rows)
or examples from the corpus (the second block of four rows). The contextual features
(those coming from the corpus) can only be applied to models using examples from the
corpus as the unit of classification. These features harm accuracy: When they are used
alone (the third block of rows) they yield accuracy values that are below the baseline
and when they are used in combination with features obtained from the lexicon (the
fourth block of rows) the accuracy decreases in relation to the models that use only
the lexicon as the source of the features (the second block of four rows). This shows
that there is crucial information in the lexicon that is not possible to recover from the
corpus. Furthermore, it should be mentioned that there is a generalized improvement
across sense-based models correlated to the vocabulary and especially corpus size: The
bigger the set of vocabulary and corpus, the better the result. This fact is also present
in lemma-based models.
The sense-based models represent the upper bounds for our task. In a realistic
scenario, however, given the state-of-the-art results in WSD, we would not have access
to sense labels, so we are much more interested in the performance of lemma-based
models. The best results are achieved when features from the lexicon and from the
854
Peris, Taule?, and Rodr??guez Empirical Methods for the Study of Denotation in Nominalizations
corpus are combined (the last block of rows), showing that the sum of both types of
features gives rise to positive results, which are not achieved by lexical features or
contextual features on their own. When the features used in the classification come
exclusively from the lexicon, with the unit of classification being lemmas from the
lexicon (the fifth block of four rows) or the examples from the corpus (the sixth block
of four rows), the results are negative (below the baseline) except when the vocabulary
and corpus size are both the full sets (1.54% and 0.16% improvement, respectively).
In these cases, the information from the lexicon is not as accurate as in sense-based
models. The contextual features alone do not achieve positive results, not even with the
full vocabulary and the full corpus. Therefore, the combination of features is needed in
a realistic scenario in order to achieve good performance of the classifier. In these cases,
only when the reduced vocabulary and the reduced corpus are used are the results
slightly negative. From now on, we will focus on the last model (LEAFF) because even
if it has a lower accuracy than the corresponding sense-model, we expect it to exhibit a
more robust behavior when tackling unseen data.
An important point for the classifier to learn a model is whether or not the sample
size is large enough for accurate learning. We performed a learning curve analysis of
the LEAFF model for different sample sizes (from 1,000 examples to the whole set
of 23,431 examples). The results are depicted in Figure 3. We have also plotted the
confidence intervals at 95%. The results seem to imply that for sizes over 5,000 examples
the accuracy tends to stabilize; we are, therefore, highly confident of our results. As
expected, the confidence intervals consistently diminish as the corpus grows.
7.4 Precision Oriented Classifier: Threshold
All the experiments reported so far are based on a full coverage setting. Coverage
is 100% in all cases and, therefore, accuracy and precision have the same score.
Figure 3
Learning curve for the LEAFF model.
855
Computational Linguistics Volume 38, Number 4
Figure 4
Precision/coverage plot for different rules thresholding of the SEAFR model.
Additionally, we performed a precision oriented experiment based on a classifier
aiming to achieve a high precision at the cost of a drop in coverage. In order to do this,
we scored each of the rules in the rule set from the LEAFF model of the ADN-Classifier
individually (not taking into account the order of such rules). We sorted the rules in the
rule set by their individual scores, provided by the Weka toolbox, and built a classifier
based on a thresholding mechanism: Only the rules over the threshold were applied.
This resulted in higher precision at the cost of lower coverage. The LEAFF model, as
presented in Table 5, consists of 981 rules obtaining an overall accuracy of 87.20%. The
results of the application of the n most accurate rules, for n from 981 to 1, are depicted
in Figure 4. Note that removing the 500 least accurate rules has a small effect on the
coverage and the overall precision has risen to 94%. An additional reduction of 200
rules results in an increase of the overall precision to 96.5% at a cost of a drop in the
coverage to 90%. Using only the 100 most accurate rules obtains a precision of 98.5%
for a still useful coverage of 80%.
7.5 Evaluation of Scenarios
The results of the experiments applying the scenarios described in Section 6 (see Table 4)
are presented in Table 6. The table shows the results of the ten scenarios set out in rows,
and in columns we provide the scenario identification (column 1); the model applied
out of the 32 generated, following the notation in Section 7.2 (column 2); the number of
features in the original model (column 3); the number of features in the model adapted
for that scenario after removing noninformed features, that is, the features used in the
original model that do not fit in the description of a concrete scenario (column 4);
and the accuracies of the original and final model (columns 5 and 6, respectively). In
each scenario we applied the best model of the 32 we generated taking into account
856
Peris, Taule?, and Rodr??guez Empirical Methods for the Study of Denotation in Nominalizations
Table 6
Experiment and evaluation of scenarios.
Scenario Model Initial Att. Final Att. Initial Acc. (%) Final Acc. (%)
1 LELFF 1,559 1,559 85.62 85.62
2 SELFF 1,671 1,671 96.65 96.65
3 LEAFF 1,755 1,755 87.20 87.20
4 SEAFF 1,867 1,867 95.46 95.46
5 LELFF 1,559 1,416 85.62 85.56
6 SELFF 1,671 1,613 96.65 96.17
7 LEAFF 1,755 1,611 87.20 87.12
8 SEAFF 1,867 1,808 95.46 95.41
9 LECFF 197 197 84.86 84.86
10 LEAFF 1,755 1,556 87.20 87.08
the features that each model uses and that fit the best in each scenario according to
the hypothesized available linguistic processors. When there is no concrete model to
project how the ADN-Classifier would perform in a concrete scenario, we selected the
model that fits approximately in that scenario and removed the noninformed features.
For instance, Scenario 10 describes the case where the nominal lexicon is not available
or the nominalization candidate is a noun that does not occur in the nominal lexicon,
and the features used are extracted from the parsed tree at lemma level and from the
SRL process in order to obtain argument structure information. Because we do not
have a model that perfectly fits in that scenario, we select the LEAFF (lemma based
model using examples from the corpus as the unit of classification and obtaining the
features from both lexicon and corpus, with full vocabulary and full corpus sets), and
we removed all the features from the lexicon except the ones related to the argument
structure, simulating an SRL process.32
These results show that the difference between lemma-based and sense-based mod-
els shown in Table 5 is also present here. There is a decrease in accuracy in all the cases
in which features are removed, although this decrease is not statistically significant.
This could be due to the large number of features available for rule learning and the
possibility of using alternate features when some of the original ones are removed.
7.6 Error Analysis
The analysis of errors focuses on the lemma-based model using lexicon and corpus
information with the full vocabulary and the full corpus (LEAFF). Table 7 presents the
confusion matrix of the model. Rows correspond to manually labeled data and columns
are predictions from the classifier. The correct predictions are in the diagonal (in bold-
face). The errors are marked in italics.
The rate of error is almost equally split between the three main classes: incorrectly
classified event nominalizations represent 31%, result nominalizations 34%, and under-
specified nominalizations 32%. Lexicalized instances,33 however, only display an error
rate of 3%. Among event nominalizations incorrectly classified by the ADN-Classifier,
32 In the same way, sense-based models simulate how the ADN-Classifier would perform with an ideal
WSD automatic process.
33 By lexicalized nominalizations we refer here to those lexicalized nominalizations where a denotation
type is not assigned, that is, non-nominal lexicalizations.
857
Computational Linguistics Volume 38, Number 4
Table 7
Confusion matrix for the LEAFF model.
ADN Result Event Underspecified Lexicalized Total
Result 18,997 575 397 54 20,023
Manually Event 676 933 242 2 1,853
Validated Underspecified 643 309 453 7 1,412
Lexicalized 90 2 2 49 143
Total 20,406 1,819 1,094 112 23,431
73% (676 instances) were classified as result; 26% (242 instances) as underspecified, and a
marginal 3% (2 instances) as lexicalized nominalizations. These errors are attributable to
four main causes. Firstly, 27% of the errors are in fact human errors,34 which means that
the ADN-Classifier performed well. Secondly, the annotation guidelines contain criteria
that the ADN-Classifier cannot recognize: the paraphrase criterion, the agent criterion,
and the so-called selectors (see Section 4.2). These errors represent 51% of the wrongly
classified events. Therefore, there were cases (a total of 61) where manual annotators
had an extra criterion that the ADN-Classifier could not use. We thought that imple-
menting the selectors as features in the ADN-Classifier would be an excessively ad hoc
approach. Thirdly, an error of 21% in event classification is explained because there are a
number of criteria, implemented as features in the ADN-Classifier, that suffer from data
sparseness, and, therefore, the ADN-Classifier cannot learn them as being as relevant
as they are. For instance, a very conclusive clue for detecting event nominalizations is
that they are specified by a possessive determiner that is interpreted as an arg1-patient.
And, finally, the cases in which the ADN-Classifier annotated event nominalizations as
lexicalized constructions are explained by the ADN-Classifier confusing them with real
lexicalized constructions in which the lemma is shared (an error rate of 1%).
Among result nominalizations incorrectly classified by the ADN-Classifier, 56%
(575 instances) were classified as event, 39% (397 instances) as underspecified, and 5%
(54 instances) as lexicalized nominalizations. These errors are attributable to the same
four causes set out above. The rate of human errors is now 51%, however, meaning that
there are event and underspecified nominalizations that were incorrectly validated. The
rate of errors explained by the selectors is now just 10% because there are more selectors
for identifying event than for detecting result nominalizations. And finally, an error rate
of 37% is explained by those criteria that are implemented as features of the ADN-
Classifier, but which suffer from data sparseness, and, therefore, despite their relevance,
cannot be learned by the ADN-Classifier. In the case of result nominalizations, there
are more criteria of this type: nominalizations deriving from unergative and stative
verb classes, relational adjectives as arguments, and temporal arguments realized by
a PP introduced by de (?of?). And finally, the cases in which the classifier annotated
result nominalizations as lexicalized constructions are explained by the ADN-Classifier
confusing them with real lexicalized constructions in which the lemma is shared (an
error rate of 2%).
34 When comparing automatic annotation with the manual validation, some cases were considered to be
doubtful. We discussed those cases with all the annotators and decided which annotation (automatic or
human) was the correct one. Therefore, by human errors we mean those incorrectly classified in the
manual validation process.
858
Peris, Taule?, and Rodr??guez Empirical Methods for the Study of Denotation in Nominalizations
Among incorrectly classified underspecified nominalizations, 32% (309 instances)
were classified as events, 67% (643 instances) as results, and a marginal 1% (7 instances)
as lexicalized nominalizations. The difficulty in identifying underspecified nominaliza-
tions is to be expected, given that these are either cases with no clear contextual hints
or truly ambiguous examples. In this case, the rate of human error is 45%. Although
there are no selectors that identify underspecified nominalizations, in some cases an NP
containing a nominalization presents contradictory criteria. For instance, an indefinite
determiner is a criterion that points to a result denotation and the selector durante (?dur-
ing?) typically selects an event denotation. In these cases, the annotators were instructed
to tag them as underspecified. The ADN-Classifier could not use the selectors in its
classification, however, and most of these cases therefore were annotated as results. This
type of error represents 19% of the incorrectly classified underspecified nominalizations.
The agent criterion explains an error rate of 20%. If both types of PPs (introduced by the
preposition por [?by?] or introduced by the preposition de [?of?]) are valid for the NP
the annotators were validating, they tagged the nominalization as underspecified type.
Again, human annotators had an extra criterion that the ADN-Classifier could not use.
The remaining 5% is explained by the failure of the ADN-Classifier to detect a pattern
that is typical of underspecified nominalizations: those derived from an achievement verb
with an arg1-patient. And finally, the cases in which the ADN-Classifier annotated
underspecified nominalizations as lexicalized constructions are explained by the ADN-
Classifier confusing them with real lexicalized constructions in which the lemma is
shared (an error rate of 1%).
Most incorrectly classified lexicalized constructions (96%, 90 instances) were clas-
sified as result nominalizations. This is probably due to the fact that most nominal
lexicalized nominalizations are of the result type. Therefore, the key failure of the
ADN-Classifier is basically in distinguishing between the different types of lexicalized
constructions.
8. Related Work
Although there are several works that contemplate the computational treatment of
nominalizations, most of them are basically interested in two issues: 1) automatically
classifying semantic relations between nominals (Task 4 of SemEval 2007 [Girju et al
2009] and Task 8 of SemEval 2010 [Hendrickx et al 2010]) or in noun compound
constructions (Girju et al [2005] and Task 9 of SemEval 2010 [Butnariu et al 2010a,
2010b]); and 2) taking advantage of verbal data to interpret, represent, and assign
semantic roles to complements of nominalizations (Hull and Gomez 2000; Lapata 2002;
Pado?, Pennacchiotti, and Sporleder 2008; Gurevich andWaterman 2009). Althoughmost
of these works show a certain awareness of the linguistic distinction between event
and result nominalizations, none of them applies this distinction in their systems. The
notion of event appears in the work of Creswell et al (2006), in which a classifier that
distinguishes between nominal mentions of events and non-events is presented. Their
distinction is not comparable to our event and result distinction for one main reason,
however: they do not focus on nominalizations but on nouns in general, and therefore
the difficulty in distinguishing events from non-events among all types of nouns is less
than distinguishing between event and result nominalizations, which, as has been seen,
are highly ambiguous. We state that it is easier because in a wide nominal domain there
are many nouns which under any circumstance can be understood as non-dynamic (in
fact, nouns tend to denote objects, non-events) and if Creswell et al include as seed for
859
Computational Linguistics Volume 38, Number 4
learning these types of nouns, such as airport or electronics, it will necessarily increase
the accuracy of the automatic distinction between their two denotation types.
As far as we know, the only work closely related to ours is that of Eberle, Faasz,
and Ulrich (2011) who are working on German ?ung nominalizations, in which the
assignment of a denotation type is also carried out. In that paper they state that this
kind of nominalization can denote an event, a state, or an object. Specifically, they analyze
those ?ung nominalizations derived from verbs of saying embedded in PPs introduced
by the preposition nach (?to?). According to the authors, these nominalizations can
denote either an event or a proposition, which is a type of object. They present a semantic
analysis tool (Eberle et al 2008) which disambiguates this type of nominalization on the
basis of nine criteria, which they call indicators. The tool extracts the indicators from
the semantic representation that it provides and computes the preferred denotation
according to a pre-established weighting schema. They apply this tool to a set of 100
sentences where the relevant material (the nine indicators) is completely familiar to the
system and the tool recognizes the preferred reading in 82% of cases.
Because the ADN-Classifier is based on ML techniques and does not restrict the
nominalizations to a specific suffix and to those derived from verbs of saying, their
work is not directly comparable to ours. We tried to replicate their experiment, however,
selecting only those nominalizations created with the suffix -cio?n (the most productive
Spanish suffix and the nearest equivalent to ?ung in German) and which derive from
verbs of saying. The subset obtained includes 66 types of nominalizations, compared
with the 1,655 in our work. We applied the LEAFF model to the 719 tokens of these
66 nominalization types and we obtained an accuracy of 85.6%. This implies a 3.6
percentage point increase in accuracy with respect to the results of their work, despite
the fact that our model is not trained on this specific nominalization class and does not
dispose of specially designed indicators. We have to take this result with due caution
because we are dealing with two not closely related languages and considering a close
but not identical set of nominalizations.
9. Conclusions
This article contributes to the semantic analysis of texts focusing on Spanish deverbal
nominalizations. We base our study on theoretical hypotheses that we analyze empir-
ically, and as a result we have developed three new resources: the ADN-Classifier, the
first tool that allows for the automatic classification of deverbal nouns as event or result
types; the AnCora-ES corpus enriched with the annotation of deverbal nominalizations
according to their semantic denotation, being in fact the only Spanish corpus which
incorporates this information; and the AnCora-Nom lexicon, a resource containing
1,655 deverbal nominalizations linked to their occurrences in the AnCora-ES corpus.
These resources could be very useful for NLP applications. The work presented in this
article also provides an additional insight into the linguistic question underlying it:
The characterization of deverbal nominalizations according to their denotation and the
identification of the most useful criteria to distinguish between these denotation types.
We can classify our contributions in three directions:
1) The study of the relevant features for the classification of a nominalization
as being of event or result type. The set of features considered were selected from
the linguistics literature, mostly devoted to the English language, and its relevance
was established empirically for Spanish. From the corpus-based study, we concluded
that not all the criteria posited for English seem to port to Spanish. Among the
860
Peris, Taule?, and Rodr??guez Empirical Methods for the Study of Denotation in Nominalizations
evaluated criteria, the most relevant for distinguishing between event and result nomi-
nalizations are: 1) the semantic class of the verb from which the noun is derived;
2) its pluralization capacity; 3) its determiner types; 4) the preposition introducing an
agentive complement; and 5) the obligatory presence of an internal argument. These
features are represented as attributes in the nominal lexical entries of the AnCora-Nom
lexicon. Models including features coming from the lexicon outperform those that only
take into account features from the corpus. As expected, models working at the sense
level outperform those working at the lemma level. When working at the lemma level
only the combination of features from both the lexicon and the corpus provides results
that outperform the baseline. It is interesting to note that the number of features used to
support result nominalizations is significantly superior to those used to strengthen event
nominalizations. In each criterion we find features for supporting result nominalizations
but not event nominalizations. As a result, the ADN-Classifier uses more features for
detecting result than event nominalizations, and therefore achieves a greater degree
of accuracy on the former than in the latter. Furthermore, the corpus base study has
allowed us to find new clues that support denotation types, especially the event reading.
The paraphrase and agent criteria, as well as the selectors, have proved very useful
to human annotators for distinguishing between an event and a result reading. These
criteria are difficult to implement automatically, however.
2) Lexical resources derived from this work. We have enriched the AnCora-ES
corpus with the annotation of 23,431 deverbal nominalization occurrences according to
their semantic denotation; and we have built AnCora-Nom from scratch, representing
the 1,655 nominalization types that correspond to these occurrences.
3) The ADN-Classifier. This classifier is the first tool that aims to automatically
classify deverbal nominalizations in event, result, or underspecified denotation types, and
to identify whether the nominalization takes part in a lexicalized construction in Spanish.
We set up a series of experiments in order to test the ADN-Classifier under different
models and in different realistic scenarios, achieving good results. The ADN-Classifier
has helped us to quantitatively evaluate the validity of our claims regarding deverbal
nominalizations. An error analysis was performed and its conclusions can be used to
pursue further lines of improvements.
Further work. Two of the main sources of error found in the performance of the
ADN-Classifier are data sparseness of some of the features (such as PP agent) and the
fact that there are criteria at the disposal of human annotators that the ADN-Classifier
is unable to detect. In order to reduce the problem of data sparseness it would be
interesting to look for some linguistic generalizations of the sparse features in order to
implement a backoff mechanism. Another line of future work is to analyze the criteria
used by human annotators and not currently implemented either in the lexicon or in the
corpus. Some additional features could be incorporated in the Classifier. Among them
are path-based syntactic patterns that have been applied with success to related tasks
(see Gildea and Palmer 2002).
We have also experimented with a meta-classifier working on the results of binary
classifiers (one for each class). The global accuracy of the meta-classifier was not greater
than that of the current ADN. We think, however, that a binary classifier for the
underspecified type (the most difficult one) could result in improvements. Most of the
considerations regarding the scenarios described in Table 4 are based on a crude global
evaluation of complementary NL processors such as a word sense disambiguator; for
861
Computational Linguistics Volume 38, Number 4
example, a specific scenario can be followed when the global accuracy of the NL pro-
cessor crosses a given threshold. A more precise approach can also be adopted. Con-
sider, for instance, the WSD task instead of a simple classifier providing a global
accuracy?a regressor can provide individual scores of accuracy for each case (de-
gree of confidence, margin, probability of correct classification, etc.). This more precise
approach can lead to new scenarios incorporating hybrid models.
The last point of future work consists in analyzing to what extent the ADN-
Classifier and its models are applicable to other languages. Concretely, because we have
a similar corpus for Catalan (lacking deverbal nominalization information) we plan to
apply the models learned for Spanish to this closely related Romance language.
Acknowledgments
We are grateful to Maria Anto`nia Mart?? and
Marta Recasens for their helpful advice and
to David Bridgewater for the proofreading of
English. We would also like to express our
gratitude to the three anonymous reviewers
for their comments and suggestions to improve
this article. This work was partly supported
by the projects Araknion (FFI2010-114774-E),
Know2 (TIN2009-14715-C04-04), and TEXT-
MESS 2.0 (TIN2009-13391-C04-04) from the
Spanish Ministry of Science and Innovation,
and by a FPU grant (AP2007-01028) from the
Spanish Ministry of Education.
References
Abeille?, Anne, Lionel Cle?ment, and
Alexandra Kinyon. 2000. Building a
treebank for French. In Proceedings of the
Second International Language Resources
and Evaluation (LREC?00), pages 87?94,
Athens.
Alexiadou, Artemis. 2001. The Functional
Structure in Nominals. Nominalizations
and Ergativity. John Benjamins,
Amsterdam/Philadelphia, PA.
Alonso, Margarita. 2004. Las construcciones
con verbos de apoyo. Visor Libros, Madrid.
Androutsopoulos, Ion and Prodromos
Malakasiotis. 2010. A survey of
paraphrasing and textual entailment
methods. Journal of Artificial Intelligence
Research, 38:135?187.
Aparicio, Juan, Mariona Taule?, and
M. Anto`nia Mart??. 2008. AnCora-Verb:
A lexical resource for the semantic
annotation of corpora. In Proceedings
of the Sixth International Language
Resources and Evaluation (LREC?08),
pages 797?802, Marrakech.
Badia, Toni. 2002. Els complements
nominals. In Joan Sola`, editor, Grama`tica
del Catala` Contemporani, volume 3.
Empu?ries, Barcelona, pages 1591?1640.
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley
FrameNet Project. In Proceedings of the
36th Annual Meeting of the Association
for Computational Linguistics and 17th
International Conference on Computational
Linguistics - Volume 1, ACL?98,
pages 86?90, Stroudsburg, PA.
Balvet, Antonio, Lucie Barque, and
Rafael Mar??n. 2010. Building a lexicon of
French deverbal nouns from a semantically
annotated corpus. In Proceedings of the
Seventh Conference on International Language
Resources and Evaluation (LREC?10),
pages 1408?1413, Valletta.
Bos, Johan. 2008. Wide-coverage semantic
analysis with Boxer. In Semantics in
Text Processing. STEP 2008 Conference
Proceedings, pages 277?286, Venice.
Butnariu, Cristina, Su Nam Kim,
Preslav Nakov, Diarmuid O? Se?aghdha,
Stan Szpakowicz, and Tony Veale.
2010a. SemEval-2010 Task 9: The
interpretation of noun compounds using
paraphrasing verbs and prepositions.
In Proceedings of the Workshop on Semantic
Evaluations: Recent Achievements
and Future Directions (SEW-2010),
pages 100?105, Boulder, CO.
Butnariu, Cristina, Su Nam Kim, Preslav
Nakov, Diarmuid O? Se?aghdha, Stan
Szpakowicz, and Tony Veale. 2010b.
SemEval-2010 Task 9: The interpretation
of noun compounds using paraphrasing
verbs and prepositions. In Proceedings of
the 5th International Workshop on Semantic
Evaluation, pages 39?44, Uppsala.
Chklovski, Timothy and Rada Mihalcea.
2002. Building a sense tagged corpus with
open mind word expert. In Proceedings
of the ACL-02 Workshop on Word Sense
Disambiguation: Recent Successes and
Future Directions - Volume 8, WSD ?02,
pages 116?122, Stroudsburg, PA.
Civit, Montserrat and Maria Anto`nia Mart??.
2004. Building Cast3LB: A Spanish
862
Peris, Taule?, and Rodr??guez Empirical Methods for the Study of Denotation in Nominalizations
Treebank. Research on Language and
Computation, 2(4):549?574.
Clark, Herbert H. 1975. Bridging. In
Proceedings of the 1975 Workshop on
Theoretical Issues in Natural Language
Processing, TINLAP ?75, pages 169?174,
Stroudsburg, PA.
Copestake, Ann. 2007. Semantic composition
with (Robust) Minimal Recursion
Semantics. In Proceedings of the Workshop
on Deep Linguistic Processing, DeepLP ?07,
pages 73?80, Prague.
Creswell, Cassandre, Matthew J. Beal,
John Chen, Thomas L. Cornell, Lars
Nilsson, and Rohini K. Srihari. 2006.
Automatically extracting nominal
mentions of events with a bootstrapped
probabilistic classifier. In Proceedings of
the Computational Linguistics/Association
for Computational Linguistics on Main
Conference Poster Sessions, COLING-ACL
?06, pages 168?175, Stroudsburg, PA.
Decadt, Bart, Ve?ronique Hoste, Walter
Daelemans, and Antal Van Den Bosch.
2004. GAMBL, genetic algorithm
optimization of memory-based WSD.
In Proceedings of the Association of
Computational Linguistics/SIGLEX
Senseval-3, pages 108?112,
Stroudsburg, PA.
Dowty, David. 1979.Word Meaning and
Montague Grammar. Reidel, Dordrecht.
Eberle, Kurt, Gertrud Faasz, and Ulrich
Heid. 2009. Corpus-based identification
and disambiguation of reading indicators
in German nominalizations. In Online
Proceedings of the 5th Corpus Linguistics
Conference. Available at ucrel.lancs.
ac.uk/publications/cl2009/.
Eberle, Kurt, Gertrud Faasz, and Heid Ulrich.
2011. Approximating the disambiguation
of some German nominalizations by use
of weak structural, lexical, and corpus
information. Procesamiento del Lenguaje
Natural, 46:67?75.
Eberle, Kurt, Ulrich Heid, Manuel Kountz,
and Kerstin Eckart. 2008. A tool for corpus
analysis using partial disambiguation and
bootstrapping of the lexicon. In Text
Resources and Lexical Knowledge: Selected
Papers from the 9th Conference on Natural
Language Processing KONVENS 2008,
pages 145?158, Konvens.
Erk, Katrin and Sebastian Pado?. 2006.
Shalmaneser: A flexible toolbox for
semantic role assignment. In Proceedings
of the Fifth International Language
Resources and Evaluation (LREC?06),
pages 527?532, Genoa.
Fellbaum, Christiane. 1998. An Electronic
Lexical Database. The MIT Press,
Cambridge, MA.
Gerber, Matthew and Joyce Y. Chai. 2010.
Beyond NomBank: A study of implicit
argumentation for nominal predicates.
In Proceedings of the ACL Conference 2010,
pages 1583?1592, Uppsala.
Gildea, Daniel and Martha Palmer. 2002.
The necessity of parsing for predicate
argument recognition. In Proceedings
of the 40th Annual Meeting on Association
for Computational Linguistics, ACL ?02,
pages 239?246, Stroudsburg, PA.
Girju, Roxana, Dan Moldovan, Marta Tatu,
and Daniel Antohe. 2005. On the semantics
of noun compounds. Computer, Speech and
Language, 19(4):479?496.
Girju, Roxana, Preslav Nakov, Vivi Nastase,
Stan Szpakowicz, Peter D. Turney, and
Deniz Yuret. 2009. Classification of
semantic relations between nominals.
Language Resources and Evaluation,
43(2):105?121.
Grimshaw, Jane. 1990. Argument Structure.
The MIT Press, Cambridge, MA.
Gurevich, Olga, Richard Crouch, Tracy
Holloway King, and Valeria De Paiva.
2006. Deverbal nouns in knowledge
representation. In Proceedings of Florida
Artificial Intelligence Research Society
Conference, pages 670?675, Florida.
Gurevich, Olga and Scott Waterman. 2009.
Mapping verbal argument preferences
to deverbals. In Proceedings of the 2009
IEEE International Conference on Semantic
Computing, pages 17?24, Washington, DC.
Hendrickx, Iris, Su Nam Kim, Zornitsa
Kozareva, Preslav Nakov, Diarmuid
O? Se?aghdha, Sebastian Pado?, Marco
Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2010. SemEval-2010
Task 8: Multi-way classification of
semantic relations between pairs of
nominals. In Proceedings of the 5th
International Workshop on Semantic
Evaluation, pages 33?38, Uppsala.
Hovy, Eduard, Mitchell Marcus, Martha
Palmer, Lance Ramshaw, and Ralph
Weischedel. 2006. OntoNotes: The 90%
solution. In Proceedings of Human Language
Technologies?North American Chapter of the
Association of Computational Linguistics
(HLT-NAACL? 06), pages 57?60,
New York, NY.
Hull, Richard D. and Fernando Gomez.
2000. Semantic interpretation of deverbal
nominalizations. Natural Language
Engineering, 6(2):139?161.
863
Computational Linguistics Volume 38, Number 4
Kipper, K., A. Korhonen, N. Ryant, and
M. Palmer. 2006. Extending VerbNet
with novel verb classes. In Proceedings of
the 5th International Conference on Language
Resources and Evaluation, pages 1027?1032,
Genova.
Lapata, Maria. 2002. The disambiguation of
nominalizations. Computational Linguistics,
28(3):357?388.
Levi, Judith N. 1978. The Syntax and Semantics
of Complex Nominals. Academic Press Inc.,
New York.
Liu, Chang and Hwee Tou Ng. 2007.
Learning predictive structures for
semantic role labeling of NomBank.
In Proceedings of the 45th Annual Meeting
of the Association of Computational
Linguistics, pages 208?215, Prague.
Madnani, Nitin and Bonnie J. Dorr. 2010.
Generating phrasal and sentential
paraphrases: A survey of data-driven
methods. Computational Linguistics,
36(3):341?387.
Ma?rquez, Llu??s, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008.
Semantic role labeling: An introduction to
the special issue. Computational Linguistics,
34(2):145?159.
McLachlan, G. J., K. A. Do, and C. Ambroise.
2004. Analyzing Microarray Gene Expression
Data. Wiley, Hoboken, NJ.
Mechura, Michal. 2008. Selectional Preferences,
Corpora and Ontologies. Ph.D. thesis, Trinity
College, University of Dublin.
Meyers, Adam, Ruth Reeves, and Catherine
Macleod. 2004. NP-external arguments:
A study of argument sharing in English.
In Proceedings of the Workshop on Multiword
Expressions: Integrating Processing
(MWE?04), pages 96?103, Stroudsburg, PA.
Mooney, Raymond J. 2007. Learning for
semantic parsing. In Computational
Linguistics and Intelligent Text Processing:
Proceedings of the 8th International
Conference (CICLing 2007) (invited
paper), pages 311?324, Berlin.
Navigli, Roberto. 2009. Word sense
disambiguation: A survey. ACM
Computing Surveys, 41(2):1?69.
Nunes, Mary L. 1993. Argument linking
in English derived nominals. In Robert
D. Van Valin, editor, Advances in Role
Reference Grammar. John Benjamins,
Amsterdam/Philadelphia, pages 375?432.
Pado?, Sebastian, Marco Pennacchiotti,
and Caroline Sporleder. 2008. Semantic
role assignment for event nominalisations
by leveraging verbal data. In Proceedings
of the 22nd International Conference on
Computational Linguistics?Volume 1,
COLING ?08, pages 665?672,
Stroudsburg, PA.
Palmer, Martha, Daniel Gildea, and
Nianwen Xue. 2010. Semantic Role
Labeling. Synthesis on Human Languages
Technologies.Morgan & Claypool
Publishers, Toronto.
Palmer, Martha, Paul Kingsbury, and
Daniel Gildea. 2005. The proposition bank:
An annotated corpus of semantic roles.
Computational Linguistics, 31(1):76?105.
Peris, Aina and Mariona Taule?. 2011.
AnCora-Nom: A Spanish lexicon of
deverbal nominalizations. Procesamiento
del Lenguaje Natural, 46:11?19.
Peris, Aina, Mariona Taule?, Gemma
Boleda, and Horacio Rodr??guez. 2010.
ADN-Classifier: Automatically assigning
denotation types to nominalizations. In
Proceedings of the Language Resources and
Evaluation Conference, pages 1422?1428,
Valleta.
Peris, Aina, Mariona Taule?, and Horacio
Rodr??guez. 2009. Hacia un sistema de
clasificacio?n automa?tica de sustantivos
deverbales. Procesamiento del Lenguaje
Natural, 43:23?31.
Peris, Aina, Mariona Taule?, and Horacio
Rodr??guez. 2010. Semantic annotation of
deverbal nominalizations in the Spanish
AnCora corpus. In Proceedings of the Ninth
International Workshop on Treebanks and
Linguistic Theories, pages 187?198, Tartu.
Picallo, Carme. 1999. La estructura del
Sintagma Nominal: las nominalizaciones
y otros sustantivos con complementos
argumentales. In Ignacio Bosque and
Violeta Demonte, editors, Grama?tica
Descriptiva de la Lengua Espan?ola,
volume 1. Espasa Calpe, Madrid,
pages 363?393.
Pradhan, Sameer, Honglin Sun, Wayne Ward,
James H. Martin, and Dan Jurafsky. 2004.
Parsing arguments of nominalizations
in English and Chinese. In Proceedings
of Human Language Technologies?
North American Chapter of the Association of
Computational Linguistics (HLT-NAACL? 04,
pages 208?215, Boston, MA.
Pradhan, Sameer S., Edward Loper,
Dmitriy Dligach, and Martha Palmer.
2007. Semeval-2007 task 17: English lexical
sample, SRL and all words. In Proceedings
of the 4th International Workshop on Semantic
Evaluations, SemEval ?07, pages 87?92,
Stroudsburg, PA.
Pustejovsky, James. 1995. The Generative
Lexicon. The MIT Press, Cambridge, MA.
864
Peris, Taule?, and Rodr??guez Empirical Methods for the Study of Denotation in Nominalizations
Quinlan, J. Ross. 1993. C4.5: Programs for
Machine Learning. Morgan Kaufmann,
San Mateo, CA.
Recasens, Marta, M. Anto`nia Mart??, and
Mariona Taule?. 2007. Text as scene:
Discourse deixis and bridging relations.
Revista de la Asociacio?n Espan?ola para el
Procesamiento del Lenguaje Natural, 39,
pages 205?212.
Recasens, Marta and Marta Vila. 2010. On
paraphrase and coreference. Computational
Linguistics, 36(4):639?647.
Ruppenhofer, Josef, Michael Ellsworth,
Miriam R. L. Petruck, Christopher R.
Johnson, and Jan Scheffczyk. 2006.
FrameNet II: Extended theory
and practice. Technical report,
International Computer Science
Institute, Berkeley, CA. Available at
framenet.icsi.berkeley.edu/
book/book.pdf.
Ruppenhofer, Josef, Caroline Sporleder,
Roser Morante, Collin Baker, and
Martha Palmer. 2010. SemEval-2010
Task 10: Linking events and their
participants in discourse. In Proceedings
of the 5th International Workshop on
Semantic Evaluation, pages 296?299,
Uppsala.
Santiago, Ramo?n and Enrique Bustos. 1999.
La derivacio?n nominal. In Ignacio Bosque
and Violeta Demonte, editors, Grama?tica
Descriptiva de la Lengua Espan?ola, volume 3.
Espasa Calpe, Madrid, pages 4505?4594.
Spencer, Andrew and Marina Zaretskaya.
1999. The Essex database of Russian
verbs and their nominalizations.
Technical report, University of Essex,
Colchester.
Surdeanu, Mihai, Richard Johansson,
Adam Meyers, Llu??s Ma`rquez, and
Joakim Nivre. 2008. The CoNLL-2008
shared task on joint parsing of syntactic
and semantic dependencies. In Proceedings
of the Twelfth Conference on Computational
Natural Language Learning, CoNLL ?08,
pages 159?177, Stroudsburg, PA.
Taule?, Mariona, M. Anto?nia Mart??, and
Marta Recasens. 2008. AnCora: Multilevel
annotated corpora for Catalan and
Spanish. In Proceedings of the Sixth
International Language Resources and
Evaluation (LREC?08), pages 96 ?101,
Marrakech.
Va?zquez, Glo`ria, Ana Ferna?ndez, and
Maria Anto`nia Mart??. 2000. Clasificacio?n
Verbal. Alternancias de Dia?tesis. Quaderns
de Sintagma, 3, Edicions de la Universitat
de Lleida, Lerida, Spain.
Vendler, Zeno. 1967. Linguistics in Philosophy.
Cornell University Press, Ithaca, NY.
Witten, Ian H. and Eibe Frank. 2005. Data
Mining: Practical Machine Learning
Tools and Techniques. Morgan Kaufmann,
San Francisco, CA.
Zubizarreta, Maria Luisa. 1987. Levels of
Representation in the Lexicon and in the
Syntax. Foris, Dordrect.
865

