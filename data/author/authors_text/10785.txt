Simple Syntactic and Morphological Processing Can Help English-Hindi
Statistical Machine Translation
Ananthakrishnan Ramanathan,
Pushpak Bhattacharyya
Department of Computer Science
and Engineering
Indian Institute of Technology
Powai, Mumbai-400076
India
{anand,pb}@cse.iitb.ac.in
Jayprasad Hegde, Ritesh M. Shah,
Sasikumar M
CDAC Mumbai (formerly NCST)
Gulmohar Cross Road No. 9
Juhu, Mumbai-400049
India
{jjhegde,ritesh,sasi}
@cdacmumbai.in
Abstract
In this paper, we report our work on incor-
porating syntactic and morphological infor-
mation for English to Hindi statistical ma-
chine translation. Two simple and compu-
tationally inexpensive ideas have proven to
be surprisingly effective: (i) reordering the
English source sentence as per Hindi syntax,
and (ii) using the suffixes of Hindi words.
The former is done by applying simple trans-
formation rules on the English parse tree.
The latter, by using a simple suffix separa-
tion program. With only a small amount of
bilingual training data and limited tools for
Hindi, we achieve reasonable performance
and substantial improvements over the base-
line phrase-based system. Our approach es-
chews the use of parsing or other sophisti-
cated linguistic tools for the target language
(Hindi) making it a useful framework for
statistical machine translation from English
to Indian languages in general, since such
tools are not widely available for Indian lan-
guages currently.
1 Introduction
Techniques for leveraging syntactic and morpholog-
ical information for statistical machine translation
(SMT) are receiving a fair amount of attention nowa-
days. For SMT from English to Indian languages,
these techniques are especially important for the fol-
lowing three reasons: (i) Indian languages differ
widely from English in terms of word-order; (ii) In-
dian languages are morphologically quite rich; and
(iii) large amounts of parallel corpora are not avail-
able for these languages, though smaller amounts of
text in specific domains (such as health, tourism, and
agriculture) are now becoming accessible. It might
therefore be expected that using syntactic and mor-
phological information for English to Indian lan-
guage SMT will prove highly beneficial in terms
of achieving reasonable performance out of limited
parallel corpora. However, the difficulty in this is
that crucial tools, such as parsers and morphological
analyzers, are not widely available for Indian lan-
guages yet.
In this paper, we present our work on incorporat-
ing syntactic and morphological information for En-
glish to Hindi SMT. Our approach, which eschews
the use of parsing and other tools for Hindi, is two-
pronged:
1. Incorporating syntactic information by com-
bining phrase-based models with a set of struc-
tural preprocessing rules on English
2. Incorporating morphological information by
using a simple suffix separation program for
Hindi, the likes of which can be created with
limited effort for other Indian languages as well
Significant improvements over the baseline
phrase-based SMT system are obtained using our
approach. Table 1 illustrates this with an example 1.
Since only limited linguistic effort and tools are
required for the target language, we believe that the
framework we propose is suitable for SMT from En-
glish to other Indian languages as well.
1This example is discussed further in section 4
513
input For a celestial trip of the scientific kind, visit the planetarium.
reference
	 ta	 	 e	   	 e , ta ae?
vaigyaanika tariike ke eka divya saira ke lie, taaraamandala aaem
scientific kind of a celestial trip for, planetarium visit (come)
baseline
	   	 	 pr	 , 	 ta

 ?
ke svargiiya yaatraa ke vaigyaanika prakaara, kaa taaraagruha hai
of celestial trip of scientific kind, of planetarium is
baseline+syn
	 pr	 	   	 e , ta

 ?
vaigyaanika prakaara ke svargiiya yaatraa ke lie, taaraagruha hai
scientific kind of celestial trip for, planetarium is
baseline+syn+morph
	 pr	 	   	 e , ta

 ?
vaigyaanika prakaara ke svargiiya yaatraa ke lie, taaraagruha dekhem
scientific kind of celestial trip for, planetarium visit (see)
Table 1: Effects of Syntactic and Morphological Processing (reference: human reference translation;
baseline: phrase-based system; syn: with syntactic information; morph: with morphological information)
The rest of this paper is organized as follows: Sec-
tion 2 outlines related work. Section 3 describes our
approach ? first, the phrase-based baseline system is
sketched briefly, leading up to the techniques used
for incorporating syntactic and morphological infor-
mation within this system. Experimental results are
discussed in section 4. Section 5 concludes the pa-
per with some directions for future work.
2 Related Work
Statistical translation models have evolved from the
word-based models originally proposed by Brown
et al (1990) to syntax-based and phrase-based tech-
niques.
The beginnings of phrase-based translation can
be seen in the alignment template model introduced
by Och et al (1999). A joint probability model
for phrase translation was proposed by Marcu and
Wong (2002). Koehn et al (2003) propose certain
heuristics to extract phrases that are consistent with
bidirectional word-alignments generated by the IBM
models (Brown et al, 1990). Phrases extracted us-
ing these heuristics are also shown to perform bet-
ter than syntactically motivated phrases, the joint
model, and IBM model 4 (Koehn et al, 2003).
Syntax-based models use parse-tree representa-
tions of the sentences in the training data to learn,
among other things, tree transformation probabili-
ties. These methods require a parser for the target
language and, in some cases, the source language
too. Yamada and Knight (2001) propose a model
that transforms target language parse trees to source
language strings by applying reordering, insertion,
and translation operations at each node of the tree.
Graehl and Knight (2004) and Melamed (2004), pro-
pose methods based on tree-to-tree mappings. Ima-
mura et al (2005) present a similar method that
achieves significant improvements over a phrase-
based baseline model for Japanese-English transla-
tion.
Recently, various preprocessing approaches have
been proposed for handling syntax within SMT.
These algorithms attempt to reconcile the word-
order differences between the source and target lan-
guage sentences by reordering the source language
data prior to the SMT training and decoding cy-
cles. Nie?en and Ney (2004) propose some restruc-
turing steps for German-English SMT. Popovic and
Ney (2006) report the use of simple local trans-
formation rules for Spanish-English and Serbian-
English translation. Collins et al (2006) propose
German clause restructuring to improve German-
English SMT.
The use of morphological information for SMT
has been reported in (Nie?en and Ney, 2004) and
(Popovic and Ney, 2006). The detailed experi-
ments by Nie?en and Ney (2004) show that the use
of morpho-syntactic information drastically reduces
the need for bilingual training data.
Recent work by Koehn and Hoang (2007) pro-
514
poses factored translation models that combine fea-
ture functions to handle syntactic, morphological,
and other linguistic information in a log-linear
model.
Our work uses a preprocessing approach for in-
corporating syntactic information within a phrase-
based SMT system. For incorporating morphology,
we use a simple suffix removal program for Hindi
and a morphological analyzer for English. These as-
pects are described in detail in the next section.
3 Syntactic & Morphological Information
for English-Hindi SMT
3.1 Phrase-Based SMT: the Baseline
Given a source sentence f , SMT chooses as its trans-
lation e?, which is the sentence with the highest prob-
ability:
e? = arg max
e
p(e|f)
According to Bayes? decision rule, this is written
as:
e? = arg max
e
p(e)p(f |e)
The phrase-based model that we use as our base-
line system (defined by Koehn et al (2003)) com-
putes the translation model p(f |e) by using a phrase
translation probability distribution. The decoding
process works by segmenting the input sentence f
into a sequence of I phrases f
I
1
. A uniform proba-
bility distribution over all possible segmentations is
assumed. Each phrase f
i
is translated into a target
language phrase e
i
with probability ?(f
i
|e
i
). Re-
ordering is penalized according to a simple exponen-
tial distortion model.
The phrase translation table is learnt in the fol-
lowing manner: The parallel corpus is word-aligned
bidirectionally, and using various heuristics (see
(Koehn et al, 2003) for details) phrase correspon-
dences are established. Given the set of collected
phrase pairs, the phrase translation probability is cal-
culated by relative frequency:
?(f |e) = count(f, e)?
f
count(f, e)
Lexical weighting, which measures how well
words within phrase pairs translate to each other,
validates the phrase translation, and addresses the
problem of data sparsity.
The language model p(e) used in our baseline sys-
tem is a trigram model with modified Kneser-Ney
smoothing (Chen and Goodman, 1998).
The weights for the various components of the
model (phrase translation model, language model,
distortion model etc.) are set by minimum error rate
training (Och, 2003).
3.2 Syntactic Information
As mentioned in section 2, phrase-based models
have emerged as the most successful method for
SMT. These models, however, do not handle syntax
in a natural way. Reordering of phrases during trans-
lation is typically managed by distortion models,
which have proved not entirely satisfactory (Collins
et al, 2006), especially for language pairs that differ
a lot in terms of word-order. We use a preprocess-
ing approach to get over this problem, by reordering
the English sentences in the training and test corpora
before the SMT system kicks in. This reduces, and
often eliminates, the ?distortion load? on the phrase-
based system.
The reordering rules that we use for prepro-
cessing can be broadly described by the following
transformation rule going from English to Hindi
word order (Rao et al 2000):
SS
m
V V
m
OO
m
Cm ? C
?
m
S
?
m
S
?
O
?
m
O
?
V
?
m
V
?
where,
S: Subject
O: Object
V : Verb
C
m
: Clause modifier
X
?: Corresponding constituent in Hindi,
where X is S, O, or V
X
m
: modifier of X
Essentially, the SVO order of English is changed
to SOV order, and post-modifiers are converted to
pre-modifiers. Our preprocessing module effects
this by parsing the input English sentence 2 and ap-
2Dan Bikel?s parser was used for parsing
(http://www.cis.upenn.edu/d?bikel/license.html).
515
structural transformation
morph analysis (English) Giza++
alignment correction
phrase extraction
suffix separation 
(Hindi)
decoder
	



	



Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 800?808,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Case markers and Morphology: Addressing the crux of the fluency
problem in English-Hindi SMT
Ananthakrishnan Ramanathan, Hansraj Choudhary
Avishek Ghosh, Pushpak Bhattacharyya
Department of Computer Science and Engineering
Indian Institute of Technology Bombay
Powai, Mumbai-400076
India
{anand, hansraj, avis, pb}@cse.iitb.ac.in
Abstract
We report in this paper our work on
accurately generating case markers and
suffixes in English-to-Hindi SMT. Hindi
is a relatively free word-order language,
and makes use of a comparatively richer
set of case markers and morphological
suffixes for correct meaning representa-
tion. From our experience of large-scale
English-Hindi MT, we are convinced that
fluency and fidelity in the Hindi output get
an order of magnitude facelift if accurate
case markers and suffixes are produced.
Now, the moot question is: what entity on
the English side encodes the information
contained in case markers and suffixes on
the Hindi side? Our studies of correspon-
dences in the two languages show that case
markers and suffixes in Hindi are predom-
inantly determined by the combination of
suffixes and semantic relations on the En-
glish side. We, therefore, augment the
aligned corpus of the two languages, with
the correspondence of English suffixes and
semantic relations with Hindi suffixes and
case markers. Our results on 400 test
sentences, translated using an SMT sys-
tem trained on around 13000 parallel sen-
tences, show that suffix + semantic rela-
tion? case marker/suffix is a very useful
translation factor, in the sense of making a
significant difference to output quality as
indicated by subjective evaluation as well
as BLEU scores.
1 Introduction
Two fundamental problems in applying statistical
machine translation (SMT) techniques to English-
Hindi (and generally to Indian language) MT are:
i) the wide syntactic divergence between the lan-
guage pairs, and ii) the richer morphology and
case marking of Hindi compared to English. The
first problem manifests itself in poor word-order in
the output translations, while the second one leads
to incorrect inflections (word-endings) and case
marking. Being a free word-order language, Hindi
suffers badly when morphology and case markers
are incorrect.
To solve the former, word-order related, prob-
lem, we use a preprocessing technique, which we
have discussed in (Ananthakrishnan et al, 2008).
This procedure is similar to what is suggested in
(Collins et al, 2005) and (Wang, 2007), and re-
sults in the input sentence being reordered to fol-
low Hindi structure.
The focus of this paper, however, is on the
thorny problem of generating case markers and
morphology. It is recognized that translating from
poor to rich morphology is a challenge (Avramidis
and Koehn, 2008) that calls for deeper linguistic
analysis to be part of the translation process. Such
analysis is facilitated by factored models (Koehn
et al, 2007), which provide a framework for incor-
porating lemmas, suffixes, POS tags, and any other
linguistic factors in a log-linear model for phrase-
based SMT. In this paper, we motivate a factoriza-
tion well-suited to English-Hindi translation. The
factorization uses semantic relations and suffixes
to generate inflections and case markers. Our ex-
periments include two different kinds of semantic
relations, namely, dependency relations provided
by the Stanford parser, and the deeper semantic
roles (agent, patient, etc.) provided by the univer-
sal networking language (UNL). Our experiments
show that the use of semantic relations and syntac-
tic reordering leads to substantially better quality
translation. The use of even moderately accurate
semantic relations has an especially salubrious ef-
fect on fluency.
800
2 Related Work
There have been quite a few attempts at includ-
ing morphological information within statistical
MT. Nie?en and Ney (2004) show that the use of
morpho-syntactic information drastically reduces
the need for bilingual training data. Popovic and
Ney (2006) report the use of morphological and
syntactic restructuring information for Spanish-
English and Serbian-English translation.
Koehn and Hoang (2007) propose factored
translation models that combine feature functions
to handle syntactic, morphological, and other lin-
guistic information in a log-linear model. This
work also describes experiments in translating
from English to German, Spanish, and Czech, in-
cluding the use of morphological factors.
Avramidis and Koehn (2008) report work on
translating from poor to rich morphology, namely,
English to Greek and Czech translation. They use
factored models with case and verb conjugation
related factors determined by heuristics on parse
trees. The factors are used only on the source side,
and not on the target side.
To handle syntactic differences,
Melamed (2004) proposes methods based on
tree-to-tree mappings. Imamura et al (2005)
present a similar method that achieves significant
improvements over a phrase-based baseline model
for Japanese-English translation.
Another method for handling syntactic differ-
ences is preprocessing, which is especially perti-
nent when the target language does not have pars-
ing tools. These algorithms attempt to recon-
cile the word-order differences between the source
and target language sentences by reordering the
source language data prior to the SMT training
and decoding cycles. Nie?en and Ney (2004) pro-
pose some restructuring steps for German-English
SMT. Popovic and Ney (2006) report the use
of simple local transformation rules for Spanish-
English and Serbian-English translation. Collins
et al (2005) propose German clause restructur-
ing to improve German-English SMT, while Wang
et al (2007) present similar work for Chinese-
English SMT. Our earlier work (Ananthakrishnan
et al, 2008) describes syntactic reordering and
morphological suffix separation for English-Hindi
SMT.
3 Motivation
The fundamental differences between English and
Hindi are:
? English follows SVO order, whereas Hindi
follows SOV order
? English uses post-modifiers, whereas Hindi
uses pre-modifiers
? Hindi allows greater freedom in word-order,
identifying constituents through case mark-
ing
? Hindi has a relatively richer system of mor-
phology
We resolve the first two syntactic differences
by reordering the English sentence to conform to
Hindi word-order in a preprocessing step as de-
scribed in (Ananthakrishnan et al, 2008).
The focus of this paper, however, is on the last
two of these differences, and here we dwell a bit
on why this focus on case markers and morphol-
ogy is crucial to the quality of translation.
3.1 Case markers
While in English, the major constituents of a sen-
tence (subject, object, etc.) can usually be iden-
tified by their position in the sentence, Hindi is a
relatively free word-order language. Constituents
can be moved around in the sentence without im-
pacting the core meaning. For example, the fol-
lowing sentence pair conveys the same meaning
(John saw Mary), albeit with different emphases.
jAn n mrF ko dKA
John ne Mary ko dekhaa
John-nom Mary-acc saw
mrF ko jAn n dKA
Mary ko John ne dekhaa
Mary-acc John-nom saw
The identity of John as the subject and Mary
as the object in both sentences comes from the
case markers n (ne ? nominative) and ko (ko ?
accusative). Therefore, even though Hindi is pre-
dominantly SOV in its word-order, correct case
marking is a crucial part of making translations
convey the right meaning.
801
3.2 Morphology
The following examples illustrate the richer mor-
phology of Hindi compared to English:
Oblique case: The plural-marker in the word
?boys? in English is translated as e (e ? plural di-
rect) or ao\ (on ? plural oblique):
The boys went to school.
lXk pAWfAlA gy
ladake paathashaalaa gaye
The boys ate apples.
lXko\ n sb KAy
ladokon ne seba khaaye
Future tense: Future tense in Hindi is marked
on the verb. In the following example, ?will go? is
translated as jAy\g (jaaenge), with e\g (enge) as
the future tense marker:
The boys will go to school.
lXk pAWfAlA jAy\g
ladake paathashaalaa jayenge
Causative constructions: The aAyA (aayaa)
suffix indicates causativity:
The boys made them cry.
lXko\ n uh zlAyA
ladakon ne unhe rulaayaa
3.3 Sparsity
Using a standard SMT system for English-Hindi
translation will cause severe data sparsity with re-
spect to case marking and morphology.
For example, the fact that the word boys in
oblique case (say, when followed by n (ne))
should take the form lXko\ (ladakon) will be
learnt only if the correspondence between boys
and lXko\ n (ladakon ne) exists in the training
corpus. The more general rule that n (ne) should
be preceded by the oblique case ending ao\ (on)
cannot be learnt. Similarly, the plural form of boys
will be produced only if that form exists in the
training corpus.
Essentially, all morphological forms of a word
and its translations have to exist in the training cor-
pus, and every word has to appear with every pos-
sible case marker, which will require an impossi-
ble amount of training data. Therefore, it is im-
perative to make it possible for the system to learn
general rules for morphology and case marking.
The next section describes our approach to facili-
tating the learning of such rules.
4 Approach
While translating from a language of moderate
case marking and morphology (English) to one
with relatively richer case marking and morphol-
ogy (Hindi), we are faced with the problem of ex-
tracting information from the source language sen-
tence, transferring the information onto the target
side, and translating this information into the ap-
propriate case markers and morphological affixes.
The key bits of information for us are suffixes
and semantic relations, and the vehicle that trans-
fers and translates the information is the factored
model for phrase based SMT (Koehn 2007).
4.1 Factored Model
Factored models allow the translation to be broken
down into various components, which are com-
bined using a log-linear model:
p(e|f) =
1
Z
exp
n?
i=1
?ihi(e, f) (1)
Each hi is a feature function for a component of
the translation (such as the language model), and
the ? values are weights for the feature functions.
4.2 Our Factorization
Our factorization, which is illustrated in figure 1,
consists of:
1. a lemma to lemma translation factor (boy?
lXk^ (ladak))
2. a suffix + semantic relation to suffix/case
marker factor (-s + subj? e (e))
3. a lemma + suffix to surface form genera-
tion factor (lXk^ + e (ladak + e) ? lXk
(ladake))
The above factorization is motivated by the fol-
lowing:
? Case markers are decided by semantic re-
lations and tense-aspect information in suf-
fixes.
For example, if a clause has an object, and
has a perfective form, the subject usually re-
quires the case marker n (ne).
John ate an apple.
John|empty|subj eat|ed|empty an|empty|det
apple|empty|obj
802
Figure 1: Semantic and Suffix Factors: the combination of English suffixes and semantic relations is
aligned with Hindi suffixes and case markers
jAn n sb KAyA
john ne seba khaayaa
Thus, the combination of the suffix and
semantic relation generates the right case
marker (ed|empty + empty|obj? n (ne)).
? Target language suffixes are largely deter-
mined by source language suffixes and case
markers (which in turn are determined by the
semantic relations)
The boys ate apples.
The|empty|det boy|s|subj eat|ed|empty
apple|s|obj
lXko\ n sb KAy
ladakon ne seba khaaye
Here, the plural suffix on boys leads to two
possibilities ? lXk (ladake ? plural direct)
and lXko\ (ladakon ? plural oblique). The
case marker n (ne) requires the oblique case.
? Our factorization provides the system with
two sources to determine the case markers
and suffixes. While the translation steps dis-
cussed above are one source, the language
model over the suffix/case marker factor re-
inforces the decisions made.
For example, the combination lXkA n
(ladakaa ne) is impossible, while lXko\ n
(ladakon ne) is very likely. The separation of
the lemma and suffix helps in tiding over the
data sparsity problem by allowing the system
to reason about the suffix-case marker com-
bination rather than the combination of the
specific word and the case marker.
5 Semantic Relations
The experiments have been conducted with two
kinds of semantic relations. One of them is the re-
lations from the Universal Networking Language
(UNL), and the other is the grammatical relations
produced by the Stanford parser.
The relations in both UNL and the Stanford de-
pendency parser are strictly binary and form a di-
rected graph. These relations express the semantic
dependencies among the various words in the sen-
tence.
Stanford: The Stanford dependency
parser (Marie-Catherine and Manning, 2008)
uses 55 relations to express the dependencies
among the various words in a sentence. These
relations form a hierarchical structure with the
most general relation at the root. There are
various argument relations like subject, object,
objects of prepositions, and clausal complements,
modifier relations like adjectival, adverbial,
participial, and infinitival modifiers, and other
relations like coordination, conjunct, expletive,
and punctuation.
UNL: The 44 UNL relations1 include relations
such as agent, object, co-agent, and partner, tem-
poral relations, locative relations, conjunctive and
disjunctive relations, comparative relations and
also hierarchical relationships like part-of and an-
instance-of.
Comparison: Unlike the Stanford parser which
expresses the semantic relationships through
grammatical relations, UNL uses attributes and
universal words, in addition to the semantic roles,
to express the same. Universal words are used to
disambiguate words, while attributes are used to
express the speaker?s point of view in the sentence.
UNL relations, compared to the relations in the
Stanford parser, are more semantic than grammat-
ical. For instance, in the Stanford parser, the agent
relation is the complement of a passive verb intro-
duced by the preposition by, whereas in UNL it
1http://www.undl.org/unlsys/unl/unl2005/
803
Figure 2: UNL and Stanford semantic relation graphs for the sentence ?John said that he was hit
by Jack?
#sentences #words
Training 12868 316508
Tuning 600 15279
Test 400 8557
Table 1: Corpus Statistics
signifies the doer of an action. Consider the fol-
lowing sentence:
John said that he was hit by Jack.
In this sentence, the Stanford parser produces
the relation agent(hit, Jack) and nsubj(said, John)
as shown in figure 2. In UNL, however, both the
cases use the agent relation. The other distinguish-
ing aspect of UNL is the hyper-node that repre-
sents scope. In the example sentence, the whole
clause ?that he was hit by Jack? forms the ob-
ject of the verb said, and hence is represented in
a scope. The Stanford dependency parser on the
other hand represents these dependencies with the
help of the clausal complement relation, which
links said with hit, and uses the complementizer
relation to introduce the subordinating conjunc-
tion.
The pre-dependency accuracy of the Stan-
ford dependency parser is around 80% (Marie-
Catherine et al, 2006), while the accuracy
achieved by the UNL generating system is
64.89%.
6 Experiments
6.1 Setup
The corpus described in table 1 was used for the
experiments.
The SRILM toolkit 2 was used to create Hindi
language models using the target side of the train-
ing corpus.
Training, tuning, and decoding were performed
using the Moses toolkit 3. Tuning (learning the
? values discussed in section 4.1) was done using
minimum error rate training (Och, 2003).
The Stanford parser 4 was used for parsing the
English text for syntactic reordering and to gener-
ate ?stanford? semantic relations.
The program for syntactic reordering used the
parse trees generated by the Stanford parser,
and was written in perl using the module
Parse::RecDescent.
English morphological analysis was performed
using morpha (Minnen et al, 2001), while Hindi
suffix separation was done using the stemmer de-
scribed in (Ananthakrishnan and Rao, 2003).
Syntactic and morphological transformations,
in the models where they were employed, were ap-
plied at every phase: training, tuning, and testing.
Evaluation Criteria: Automatic evaluation
was performed using BLEU and NIST on the en-
tire test set of 400 sentences. Subjective evaluation
was performed on 125 sentences from the test set.
? BLEU (Papineni et al, 2001): measures the
precision of n-grams with respect to the ref-
erence translations, with a brevity penalty. A
higher BLEU score indicates better transla-
tion.
? NIST 5: measures the precision of n-grams.
This metric is a variant of BLEU, which was
2http://www.speech.sri.com/projects/srilm/
3http://www.statmt.org/moses/
4http://nlp.stanford.edu/software/lex-parser.shtml
5www.nist.gov/speech/tests/mt/doc/ngram-study.pdf
804
shown to correlate better with human judg-
ments. Again, a higher score indicates better
translation.
? Subjective: Human evaluators judged the
fluency and adequacy, and counted the num-
ber of errors in case markers and morphology.
6.2 Results
Table 2 shows the impact of suffix and semantic
factors. The models experimented with are de-
scribed below:
baseline: The default settings of Moses were
used for this model.
lemma + suffix: This uses the lemma and suf-
fix factors on the source side, and the lemma and
suffix/case marker on the target side. The trans-
lation steps are i) lemma to lemma and ii) suffix
to suffix/case marker, and the generation step is
lemma+suffix/case marker to surface form.
lemma + suffix + unl: This model uses, in ad-
dition to the factors in the lemma+suffix model,
a semantic relation factor (UNL relations). The
translation steps are i) lemma to lemma and ii)
suffix+semantic relation to suffix/case marker, and
the generation step again is lemma+suffix/case
marker to surface form.
lemma + suffix + stanford: This is identical
to the previous model, except that stanford depen-
dency relations are used instead of UNL relations.
We can see a substantial improvement in scores
when semantic relations are used.
Table 5 shows the impact of syntactic reorder-
ing. The surface form with distortion-based, lex-
icalized, and syntactic reordering were experi-
mented with. The model with the suffix and se-
mantic factors was used with syntactic reordering.
For subjective evaluation, sentences were
judged on fluency, adequacy and the number of er-
rors in case marking/morphology.
To judge fluency, the judges were asked to look
at how well-formed the output sentence is accord-
ing to Hindi grammar, without considering what
the translation is supposed to convey. The five-
point scale in table 3 was used for evaluation.
To judge adequacy, the judges were asked to
compare each output sentence to the reference
translation and judge how well the meaning con-
veyed by the reference was also conveyed by the
output sentence. The five-point scale in table 4
was used.
Table 6 shows the average fluency and adequacy
scores, and the average number of errors per sen-
tence.
All differences are significant at the 99%
level, except the difference in adequacy be-
tween the surface-syntactic model and the
lemma+suffix+stanford syntactic model, which is
significant at the 95% level.
7 Discussion
We can see from the results that better fluency and
adequacy are achieved with the use of semantic re-
lations. The improvement in fluency is especially
noteworthy. Figure 3 shows the distribution of flu-
ency and adequacy scores. What is worth noting
is that the number of sentences at levels 4 and 5
in terms of fluency and adequacy are much higher
in case of the model that uses semantic relations.
That is, the use of semantic relations, in combi-
nation with syntactic reordering, produces many
more sentences that are reasonably or even per-
fectly fluent and convey most or all of the mean-
ing.
Table 7 shows the impact of sentence length on
translation quality. We can see that with smaller
sentences the improvements using syntactic re-
ordering and semantic relations are much more
pronounced. All models find long sentences dif-
ficult to handle, which contributes to bringing the
mean performances closer. However, it is clear
that many more useful translations are being pro-
duced due to syntactic reordering and semantic re-
lations.
The following is an example of the kind of im-
provements achieved:
Input: Inland waterway is one of the most pop-
ular picnic spots in Alappuzha.
Baseline: m\ ek ata,-TlFy jlmAg k
sbs ?Es? EpkEnk -Tl m\ jlo\ m\ dOXtaF
h{
men eka antahsthaliiya jalamaarga ke sabase
prasiddha pikanika sthala men jalon men daudatii
hai
gloss: in a waterway of most popular picnic spot
in waters runs.
Reorder: ata,-TlFy jlmAg aAlp  yA k
sbs ?Es? EpkEnk -Tl m\ s ek h{
antahsthaliiya jalamaarga aalapuzaa ke sabase
prasiddha pikanika sthala men se eka hai
805
Model BLEU NIST
Baseline (surface) 24.32 5.85
lemma + suffix 25.16 5.87
lemma + suffix + unl 27.79 6.05
lemma + suffix + stanford 28.21 5.99
Table 2: Results: The impact of suffix and semantic factors
Level Interpretation
5 Flawless Hindi, with no grammatical errors whatsoever
4 Good Hindi, with a few minor errors in morphology
3 Non-native Hindi, with possibly a few minor grammatical errors
2 Disfluent Hindi, with most phrases correct, but ungrammatical overall
1 Incomprehensible
Table 3: Subjective Evaluation: Fluency Scale
Level Interpretation
5 All meaning is conveyed
4 Most of the meaning is conveyed
3 Much of the meaning is conveyed
2 Little meaning is conveyed
1 None of the meaning is conveyed
Table 4: Subjective Evaluation: Adequacy Scale
Model Reordering BLEU NIST
surface distortion 24.42 5.85
surface lexicalized 28.75 6.19
surface syntactic 31.57 6.40
lemma + suffix + stanford syntactic 31.49 6.34
Table 5: Results: The impact of reordering and semantic relations
Model Reordering Fluency Adequacy #errors
surface lexicalized 2.14 2.26 2.16
surface syntactic 2.6 2.71 1.79
lemma + suffix + stanford syntactic 2.88 2.82 1.44
Table 6: Subjective Evaluation: The impact of reordering and semantic relations
Baseline Reorder Stanford
F A E F A E F A E
Small (<19 words) 2.63 2.84 1.30 3.30 3.52 0.74 3.66 3.75 0.62
Medium (20-34 words) 1.92 2.00 2.23 2.32 2.43 2.05 2.62 2.46 1.74
Large (>34 words) 1.62 1.69 4.00 1.86 1.73 3.36 1.86 1.86 2.82
Table 7: Impact of sentence length (F: Fluency; A:Adequacy; E:# Errors)
806
Figure 3: Subjective evaluation: analysis
gloss: waterway Alappuzha of most popular
picnic spot of one is
Semantic: ata,-TlFy jlmAg aAlp  yA k
sbs ?Es? EpkEnk -Tlo\ m\ s ek h{
antahsthaliiya jalamaarga aalapuzaa ke sabase
prasiddha pikanika sthalon men se eka hai
gloss: waterway Alappuzha of most popular
picnic spots of one is
We can see that poor word-order makes the
baseline output almost incomprehensible, while
syntactic reordering solves the problem correctly.
The morphology improvement using semantic
relations can be seen in the correct inflection
achieved in the word -Tlo\ (sthalon ? plural
oblique ? spots), whereas the output without using
semantic relations generates -Tl (sthala ? singu-
lar ? spot).
The next couple of examples illustrate how case
marking improves through the use of semantic re-
lations.
Input: Gandhi Darshan and Gandhi National
Museum is across Rajghat.
Reorder: gA\DF dfn v gA\DF rA?~ Fy s\g}hAly
rAjGAV m\ h{
gaandhii darshana va gaandhii raashtriiya san-
grahaalaya raajaghaata men hai
Semantic: gA\DF dfn v gA\DF rA?~ Fy
s\g}hAly rAjGAV k pAr h{
gaandhii darshana va gaandhii raashtriiya san-
grahaalaya raajaghaata ke paara hai
Here, the use of semantic relations produces the
correct meaning that the locations mentioned are
across (k pAr (ke paara)) Rajghat, and not in (m\
(men)) Rajghat as suggested by the translation pro-
duced without using semantic relations.
Another common error in case marking is that
two case markers are produced in successive po-
sitions in the translation, which is not possible in
Hindi. The following example (a fragment) shows
this error (kF (kii) repeated) being correctly han-
dled by using semantic relations:
Input: For varieties of migratory birds
Reorder: ?vAsF pE"yo\ kF kF ?kAr k Ely
pravaasii pakshiyon kii kii prakaara ke liye
Semantic: ?vAsF pE"yo\ kF ?kAr k Ely
pravaasii pakshiyon kii prakaara ke liye
It is important to note that the gains made us-
ing syntactic reordering and semantic relations are
limited by the accuracy of the parsers (see section
5). We observe that even the use of moderate qual-
ity semantic relations goes a long way in increas-
ing the quality of translation.
8 Conclusion
We have reported in this paper the marked im-
provement in the output quality of Hindi transla-
tions ? especially fluency ? when the correspon-
dence of English semantic relations and suffixes
with Hindi case markers and inflections is used as
a translation factor in English-Hindi SMT. The im-
provement is statistically significant. Subjective
evaluation too lends ample credence to this claim.
Future work consists of investigations into (i) how
the internal structure of constituents can be strictly
preserved and (ii) how to glue together correctly
the syntactically well-formed bits and pieces of
the sentences. This course of future action is sug-
gested by the fact that smaller sentences are much
more fluent in translation compared to medium
length and long sentences.
807
References
Ananthakrishnan, R., and Rao, D., A Lightweight
Stemmer for Hindi, Workshop on Com-
putational Linguistics for South-Asian Lan-
guages, EACL, 2003.
Ananthakrishnan, R., Bhattacharyya, P., Hegde, J.
J., Shah, R. M., and Sasikumar, M., Sim-
ple Syntactic and Morphological Processing
Can Help English-Hindi Statistical Machine
Translation, Proceedings of IJCNLP, 2008.
Avramidis, E., and Koehn, P., Enriching Morpho-
logically Poor Languages for Statistical Ma-
chine Translation, Proceedings of ACL-08:
HLT, 2008.
Collins, M., Koehn, P., and I. Kucerova, Clause
Restructuring for Statistical Machine Trans-
lation, Proceedings of ACL, 2005.
Imamura, K., Okuma, H., Sumita, E., Prac-
tical Approach to Syntax-based Statistical
Machine Translation, Proceedings of MT-
SUMMIT X, 2005.
Koehn, P., and Hoang, H., Factored Translation
Models, Proceedings of EMNLP, 2007.
Marie-Catherine de Marneffe, MacCartney, B.,
and Manning, C., Generating Typed Depen-
dency Parses from Phrase Structure Parses,
Proceedings of LREC, 2006.
Marie-Catherine de Marneffe and Manning, C.,
Stanford Typed Dependency Manual, 2008.
Melamed, D., Statistical Machine Translation by
Parsing, Proceedings of ACL, 2004.
Minnen, G., Carroll, J., and Pearce, D., Applied
Morphological Processing of English, Natu-
ral Language Engineering, 7(3), pages 207?
223, 2001.
Nie?en, S., and Ney, H., Statistical Machine
Translation with Scarce Resources Using
Morpho-syntactic Information, Computa-
tional Linguistics, 30(2), pages 181?204,
2004.
Och, F., Minimum Error Rate Training in Sta-
tistical Machine Translation, Proceedings of
ACL, 2003.
Papineni, K., Roukos, S., Ward, T., and Zhu,
W., BLEU: a Method for Automatic Evalu-
ation of Machine Translation, IBM Research
Report, Thomas J. Watson Research Center,
2001.
Popovic, M., and Ney, H., Statistical Machine
Translation with a Small Amount of Bilin-
gual Training Data, 5th LREC SALTMIL
Workshop on Minority Languages, 2006.
Wang, C., Collins, M., and Koehn, P., Chinese
Syntactic Reordering for Statistical Machine
Translation, Proceedings of the EMNLP-
CoNLL, 2007.
808
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 486?496,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Word Reordering Model for Improved Machine Translation
Karthik Visweswariah
IBM Research India
Bangalore, India
v-karthik@in.ibm.com
Rajakrishnan Rajkumar
Dept. of Linguistics
Ohio State University
raja@ling.osu.edu
Ankur Gandhe
IBM Research India
Bangalore, India
ankugand@in.ibm.com
Ananthakrishnan Ramanathan
IBM Research India
Bangalore, India
aramana2@in.ibm.com
Jiri Navratil
IBM T.J. Watson Research Center
Yorktown Heights, New York
jiri@us.ibm.com
Abstract
Preordering of source side sentences has
proved to be useful in improving statistical
machine translation. Most work has used a
parser in the source language along with rules
to map the source language word order into
the target language word order. The require-
ment to have a source language parser is a ma-
jor drawback, which we seek to overcome in
this paper. Instead of using a parser and then
using rules to order the source side sentence
we learn a model that can directly reorder
source side sentences to match target word or-
der using a small parallel corpus with high-
quality word alignments. Our model learns
pairwise costs of a word immediately preced-
ing another word. We use the Lin-Kernighan
heuristic to find the best source reordering ef-
ficiently during training and testing and show
that it suffices to provide good quality reorder-
ing.
We show gains in translation performance
based on our reordering model for translating
from Hindi to English, Urdu to English (with
a public dataset), and English to Hindi. For
English to Hindi we show that our technique
achieves better performance than a method
that uses rules applied to the source side En-
glish parse.
1 Introduction
Languages differ in the way they order words to pro-
duce sentences representing the same meaning. Ma-
chine translation systems need to reorder words in
the source sentence to produce fluent output in the
target language that preserves the meaning of the
source sentence.
Current phrase based machine translation systems
can capture short range reorderings via the phrase
table. Even the capturing of these local reordering
phenomena is constrained by the amount of training
data available. For example, if adjectives precede
nouns in the source language and follow nouns in the
target language we still need to see a particular ad-
jective noun pair in the parallel corpus to handle the
reordering via the phrase table. Phrase based sys-
tems also rely on the target side language model to
produce the right target side order. This is known
to be inadequate (Al-Onaizan and Papineni, 2006),
and this inadequacy has spurred various attempts to
overcome the problem of handling differing word
order in languages.
One approach is through distortion models, that
try to model which reorderings are more likely
than others. The simplest models just penalize
long jumps in the source sentence when producing
the target sentence. These models have also been
generalized (Al-Onaizan and Papineni, 2006; Till-
man, 2004) to allow for lexical dependencies on the
source. While these models are simple, and can
be integrated with the decoder they are insufficient
to capture long-range reordering phenomena espe-
cially for language pairs that differ significantly.
The weakness of these simple distortion models
has been overcome using syntax of either the source
or target sentence (Yamada and Knight, 2002; Gal-
ley et al, 2006; Liu et al, 2006; Zollmann and Venu-
gopal, 2006). While these methods have shown to
be useful in improving machine translation perfor-
486
mance they generally involve joint parsing of the
source and target language which is significantly
more computationally expensive when compared to
phrase based translation systems. Another approach
that overcomes this weakness, is to to reorder the
source sentence based on rules applied on the source
parse (either hand written or learned from data) both
when training and testing (Collins et al, 2005; Gen-
zel, 2010; Visweswariah et al, 2010).
In this paper we propose a novel method for deal-
ing with the word order problem that is efficient and
does not rely on a source or target side parse being
available. We cast the word ordering problem as a
Traveling Salesman Problem (TSP) based on previ-
ous work on word-based and phrased-based statis-
tical machine translation (Tillmann and Ney, 2003;
Zaslavskiy et al, 2009). Words are the cities in the
TSP and the objective is to learn the distance be-
tween words so that the shortest tour corresponds to
the ordering of the words in the source sentence in
the target language. We show that the TSP distances
for reordering can be learned from a small amount
of high-quality word alignment data by means of
pairwise word comparisons and an informative fea-
ture set involving words and part-of-speech (POS)
tags adapted and extended from prior work on de-
pendency parsing (McDonald et al, 2005b). Ob-
taining high-quality word alignments that we require
for training is fairly easy compared with obtaining a
treebank required to obtain parses for use in syntax
based methods.
We show experimentally that our reordering
model, even when used to reorder sentences for
training and testing (rather than being used as an
additional score in the decoder) improves machine
translation performance for: Hindi ? English, En-
glish?Hindi, and Urdu? English. Although Urdu
is similar to Hindi from the point of reordering phe-
nomena we include it in our experiments since there
are publicly available datasets for Urdu-English. For
English ? Hindi we obtained better machine trans-
lation performance with our reordering model as
compared to a method that uses reordering rules ap-
plied to the source side parse.
The rest of the paper is organized as follows. Sec-
tion 2 reviews related work and places our work in
context. Section 3 outlines reordering issues due
to syntactic differences between Hindi and English.
Section 4 presents our reordering model, Section 5
presents experimental results and Section 6 presents
our conclusions and possible future work.
2 Related work
There have been several studies demonstrating im-
proved machine translation performance by reorder-
ing source side sentences based on rules applied to
the source side parse during training and decoding.
Much of this work has used hand written rules and
several language pairs have been studied e.g German
to English (Collins et al, 2005), Chinese to English
(Wang et al, 2007), English to Hindi (Ramanathan
et al, 2009), English to Arabic (Badr et al, 2009)
and Japanese to English (Lee et al, 2010). There
have also been some studies where the rules are
learned from the data (Genzel, 2010; Visweswariah
et al, 2010; Xia and McCord, 2004). In addition
there has been work (Yamada and Knight, 2002;
Zollmann and Venugopal, 2006; Galley et al, 2006;
Liu et al, 2006) which uses source and/or target
side syntax in a Context Free Grammar framework
which results in machine translation decoding being
considered as a parsing problem. In this paper we
propose a model that does not require either source
or target side syntax while also preserving the effi-
ciency of reordering techniques based on rules ap-
plied to the source side parse.
In work that is closely related to ours, (Tromble
and Eisner, 2009) formulated word reordering as a
Linear Ordering Problem (LOP), an NP-hard permu-
tation problem. They learned LOP model weights
capable of assigning a score to every possible per-
mutation of the source language sentence from an
aligned corpus by using a averaged perceptron learn-
ing model. The key difference between our model
and the model in (Tromble and Eisner, 2009) is that
while they learn costs of a word wi appearing any-
where before wj , we learn costs of wi immediately
preceding wj . This results in more compact models
and (as we show in Section 5) better models.
Our model results in us having to solve a TSP
instance. The relation between the TSP and ma-
chine translation decoding has been explored before.
(Knight, 1999) showed that TSP is a sub-class ofMT
decoding and thus established that the latter is NP-
hard. (Zaslavskiy et al, 2009) casts phrase-based
487
decoding as a TSP and they show favorable speed
performance trade-offs compared with Moses, an
existing state-of-the-art decoder. In (Tillmann and
Ney, 2003), a beam-search algorithm used for TSP
is adapted to work with an IBM-4 word-based model
and phrase-based model respectively. As opposed
to calculating TSP distances from existing machine
translation components ( viz. the translation, dis-
tortion and language model probabilities) we learn
model weights to reorder source sentences to match
target word order using an informative feature set
adapted from graph-based dependency parsing (Mc-
Donald et al, 2005a).
3 Hindi-English reordering issues
This section provides a brief survey of constructions
that the two languages in question differ as well as
have in common. (Ramanathan et al, 2009) notes
the following divergences:
? English follows SVO order while Hindi follows
SOV order
? English uses prepositions while Hindi uses
post-positions
? Hindi allows greater word order freedom
? Hindi has a relatively richer case-marking sys-
tem
In addition to these differences, (Visweswariah et
al., 2010) mention the similarity in word order in
the case of adjective noun sequences (some books
vs. kuch kitab).
4 Reordering model
Consider a source sentence w consisting of a se-
quence of n words w1, w2, ... wn that we would
like to reorder into the target language order. Given
a permutation pi of the indices 1..n, let the candi-
date reordering be wpi1 , wpi2 , ..., wpin . Thus, pii de-
notes the index of the word in the source sentence
that maps to position i in the candidate reordering.
Clearly there are n! such permutations. Our reorder-
ing model assigns costs to candidate permutations
as:
C(pi|w) =
?
i
c(pii?1, pii).
The cost c(m,n) can be thought of as the cost of the
word at index m immediately preceding the word
with index n in the candidate reordering. In this pa-
per, we parametrize the costs as:
c(m,n) = ?T?(w,m, n),
where ? is a learned vector of weights and ? is a
vector of feature functions.
Given a source sentence w we reorder it accord-
ing to the permutation pi that minimizes the cost
C(pi|w). Thus, we would like our cost function
C(pi|w) to be such that the correct reordering pi? has
the lowest cost of all possible reorderings pi. In Sec-
tion 4.1 we describe the features ? that we use, and
in Section 4.2 we describe how we train the weights
? to obtain a good reordering model.
Given our model structure, the minimization
problem that we need to solve is identical to solving
a Asymmetric Traveling Salesman Problem (ATSP)
with each word corresponding to a city, and the costs
c(m,n) representing the pairwise distances between
the cities. Consider the following example:
English input: John eats apples
Hindi: John seba(apples) khaataa hai(eats)
Desired reordered English: John apples eats
The ATSP that we need to solve is represented
pictorially in Figure 1 with sample costs. Note that
we have one extra node numbered 0. We start and
end the tour at node 0, and this determines the first
word in the reordered sentence. In this example the
minimum cost tour is:
Start ? John ? apple ? eats
recovering the right reordering for translation into
Hindi.
Solving the ATSP (which is a well known NP hard
problem) efficiently is crucial for the efficiency of
our reordering model. To solve the ATSP, we first
convert the ATSP to a symmetric TSP and then use
the Lin-Kernighan heuristic as implemented in Con-
corde, a state-of-the-art TSP solver (Applegate et al,
2005). We also experimented with using the exact
TSP solver in Concorde but since it was slower and
did not improve performance we preferred using the
Lin-Kernighan heuristic. To convert the ATSP to
a symmetric TSP we double the size of the orig-
inal problem creating a node N ? for every node
N in the original graph. Following (Hornik and
488
3apples
2eats1John
0Start
c(2,3)=3c(3,2)=1
c(0,3)=5c(3,0)=2
c(0,1)=-1c(1,0)= 5 c(1,3)=0
c(0,2)=5
c(1,2)=3c(2,1)=5
c(3,1)=5
c(2,0)=-2
Figure 1: Example of an ATSP for reordering the sen-
tence: John eats apples.
Hahsler, 2009), we then set new costs as follows:
c?(A,B) = ?, c?(A,B?) = c?(B? , A) = c(A,B)
and C(A,A?) = ??. Even with this doubling of
the number of nodes, we observed that solving the
TSPs with the Lin-Kernighan heuristic is very fast,
taking roughly 10 milliseconds per sentence on av-
erage. Overall, this means that our reordering model
is as fast as parsing and hence our model is compara-
ble in performance to techniques based on applying
rules to the parse tree.
4.1 Features
Since we would like to model reordering phenomena
which are largely related to analyzing the syntax of
the source sentence, we chose to use features based
on those that have in the past been used for parsing
(McDonald et al, 2005a). A subset of the features
we use was also used for reordering in (Tromble and
Eisner, 2009).
To be able to generalize from relatively small
amounts of data, we use features that in addition to
depending on the words in the input sentence w de-
pend on the part-of-speech (POS) tags of the words
in the input sentence. All features ?(w, i, j) we use
are binary features, that fire based on the identities
of the words and POS tags at or surrounding posi-
tions i and j in the source sentence. The first set of
feature templates we use are given in Table 1. These
features depend only on the identities of the word
and POS tag of the two positions i and j and we call
wi pi wj pj
? ? ? ?
?
?
?
?
? ?
? ?
? ?
? ?
? ?
? ?
? ? ?
Table 1: Bigram feature templates used to calculate the
cost that word at position i immediately precedes word at
position j in the target word order. wi (pi) denotes the
word (POS tag) at position i in the source sentence. Each
of the templates is also conjoined with i-j the signed dis-
tance between the two words in the source sentence.
these Bigram features.
The second set of feature templates we use are
given in Table 2. These features, in addition to ex-
amining positions i and j examine the surround-
ing positions. We instantiate these feature templates
separately for the POS tag sequence and for the
word sequence. We call these two feature sets Con-
textPOS and ContextWord respectively. When in-
stantiated with POS tags, the first row of Table 2
looks at all POS tags between positions i and j.
(Tromble and Eisner, 2009) use Bigram and Con-
textPOS features, while we extend their feature set
with the use of ContextWord features. Since Hindi
is verb final, in Hindi sentences with multiple verb
groups it is rare for words with a verb in between
to be placed together in the reordering to match En-
glish. Looking at the POS tags of words between
positions i and j allows us to penalize such reorder-
ings.
Each of the templates described in Table 1 and
Table 2 is also conjoined with i-j the signed dis-
tance between the two words in the source sentence.
The values of i-j between 5 and 10, and greater than
10 are quantized (negative values are similarly quan-
tized).
In Section 5.2 we report on experiments showing
the relative performance of these different feature
489
oi?1 oi oi+1 ob oj?1 oj oj+1
? ? ?
? ? ? ?
? ? ?
? ? ?
? ? ?
? ? ?
? ? ? ?
? ? ?
? ? ?
? ? ?
? ? ?
? ? ? ?
? ? ?
? ? ?
? ? ? ?
? ? ?
? ? ?
Table 2: Context feature templates used to calculate the
cost that word at position i immediately precedes word
at position j in the target word order. oi denotes the ob-
servation at position i in the source sentence and ob de-
notes an observation at a position between i and j (i.e
i + 1 ? b ? j ? 1). Each of the templates is instan-
tiated with the observation sequence o taken to be the
word sequence w and the POS tag sequence p. Each of
the templates is also conjoined with i-j the signed dis-
tance between the two positions in the source sentence.
types for the task of reordering Hindi sentences to
be in English word order.
4.2 Training
To train the weights ? in our model, we need a
collection of sentences, where we have the desired
reference reordering pi?(x) for each input sentence
x. To obtain these reference reorderings we use
word aligned source-target sentence pairs. The qual-
ity and consistency of these reference reorderings
will depend on the quality of the word alignments
that we use. Given word aligned source and tar-
get sentences, we drop the source words that are not
aligned. Let mi be the mean of the target word po-
sitions that the source word at index i is aligned to.
We then sort the source indices in increasing order
of mi. If mi = mj (for example, because wi and wj
are aligned to the same set of words) we keep them
in the same order that they occurred in the source
sentence. Obtaining the target ordering in this man-
ner, is certainly not the only possible way and we
would like to explore better treatment of this in fu-
ture work.
We used the single best Margin Infused Re-
laxed Algorithm (MIRA) ((McDonald et al, 2005b),
(Crammer and Singer, 2003)) with the online up-
dates to our parameters being given by
?i+1 = argmin
?
||? ? ?i||
s.t. C(pi?|w) < C(p?i|w)? L(pi?, p?i).
In the equation above,
p?i = argmin
pi
C(pi|x)
is the best reordering based on the current parameter
value and L is a loss function. We take the loss of
a reordering to be the number of words for which
the preceding word is wrong relative to the reference
target order.
We also experimented with the averaged percep-
tron algorithm (Collins, 2002), but found single best
MIRA to work slightly better and hence used MIRA
for all our experiments.
5 Experiments
In this section we report on experiments to evalu-
ate our reordering model. The first method we use
for evaluation (monolingual BLEU) is by generat-
ing the desired reordering of the source sentence (as
described in Section 4.2) and compare the reordered
output to this desired reordered sentence using the
BLEU metric. In addition, to these monolingual
BLEU results, we also evaluate (in Section 5.5) the
reordering by its effect on eventual machine transla-
tion performance.
We note that our reordering techniques uses POS
information for the input sentence. The POS taggers
used in this paper are Maximum Entropy Markov
models trained using manually annotated POS cor-
pora. For Hindi, we used roughly fifty thousand
words with twenty six tags from the corpus de-
scribed in (Dalal et al, 2007). For Urdu we used
roughly fifty thousand words and forty six tags from
the CRULP corpus (Hussain, 2008) and for English
we used the Wall Street Journal section of the Penn
Treebank.
490
5.1 Reordering model training data and
alignment quality
To train our reordering models we need training data
where we have the input source language sentence
and the desired reordering in the target language.
As described in Section 4.2 we derive the refer-
ence reordered sentence using word alignments. Ta-
ble 3 presents our monolingual BLEU results for
Hindi to English reordering as the source of the
word alignments is varied. All results in Table 3
are with Bigram and ContextPOS features. We have
word alignments from three sources: A small set
of hand aligned sentences, HMM alignments (Vo-
gel et al, 1996) and alignments obtained using a su-
pervised Maximum Entropy aligner (Ittycheriah and
Roukos, 2005) trained on the hand alignments. The
F-measure for the HMM alignments were 65% and
78% for the Maximum Entropy model alignments.
We see that the quality of the alignments is an im-
portant determiner of reordering performance. Row
1 shows the BLEU for unreordered (baseline) Hindi
compared with the Hindi sentences reordered in En-
glish Order. Using just HMM alignments to train
our model we do worse than unreordered Hindi. Al-
though using the Maximum Entropy alignments is
better than using HMM alignments, we do not im-
prove upon a small number of hand alignments by
using all the Maximum Entropy alignments.
To improve upon the model trained with only
hand alignments we selected a small number of snip-
pets of sentences from our Maximum Entropy align-
ments. The goal was to pick parts of sentences
where the alignment is reliable enough to use for
training. The heuristic we used in the selection of
snippets was to pick maximal snippets of at least
7 consecutive Hindi words with all Hindi words
aligned to a consecutive span of English words,
with no unaligned English words in the span and no
English words aligned to Hindi words outside the
span. Adding snippets selected with this heuristic
improves the reordering performance of our model
as seen in the last row of Table 3.
5.2 Feature set comparison
In this section we report on experiments to deter-
mine the performance of the different classes of fea-
tures (Bigram, ContextPos and ContextWord) dis-
HMM MaxEnt Hand BLEU
- - - 35.9
220K - - 35.4
- 220K - 47.0
- 220K 6K 48.4
- - 6K 49.0
- Good 17K 6K 51.3
Table 3: Monolingual BLEU scores for Hindi to English
reordering using models trained on different alignment
types and tested on a development set of 280 Hindi sen-
tences (5590 tokens).
Feature template
Bigram ContextPOS ContextWord BLEU
- - - 35.9
? - - 43.8
? ? - 49.0
? ? ? 51.3
Table 4: Monolingual BLEU scores for Hindi to En-
glish reordering using models trained with different fea-
ture sets and tested on a development set of 280 Hindi
sentences (5590 tokens).
cussed in Section 4.1. Table 4 shows monolingual
BLEU results for training with different features sets
for Hindi to English reordering. In all cases, we
use a set of 6000 sentence pairs which were hand
aligned to generate the training data. It is clear that
all three sets of features contribute to performance of
the reordering model, however the number of Con-
textWord features is larger than the number of Bi-
gram and ContextPOS features put together, and it
may be desirable to select from this set of features
especially when training on large amounts of data.
5.3 Monolingual reordering comparisons
Table 5 compares our reordering model with a reim-
plementation of the reordering model proposed in
(Tromble and Eisner, 2009). Both the models use
exactly the same features (bigram features and Con-
textPOS features) and are trained on the same data.
To generate our training data, for Hindi to English
and English to Hindi we use a set of 6000 hand
aligned sentences, for Urdu to English we use a set
of 8500 hand aligned sentences and for English to
French we use a set of 10000 hand aligned sentences
(a subset of Europarl and Hansards corpus). Our
491
Language pair Monolingual BLEU
Source Target Unreordered LOP TSP
Hindi English 35.9 36.6 49.0
English Hindi 34.4 48.4 56.7
Urdu English 35.6 39.5 49.9
English French 64.4 78.2 81.2
Table 5: Monolingual BLEU scores comparing the orig-
inal source order with desired target reorder without re-
ordering, and reordering using our model (TSP) and the
model proposed in (Tromble and Eisner, 2009) (LOP).
test data consisted of 280 sentences for Hindi to En-
glish and 400 sentences for all other language pairs
generated from hand aligned sentences. We include
English-French here to compare on a fairly similar
language pair with local reordering phenomena (the
main difference being that in French adjectives gen-
erally follow nouns). We note that our model outper-
forms the model proposed in (Tromble and Eisner,
2009) in all cases.
5.4 Analysis of reordering performance
To get a feel for the qualitative performance of our
reordering algorithm and the kind of phenomena it
is able to capture, we analyze the reordering per-
formance in terms of (i) whether the clause restruc-
turing is done correctly ? these can be thought of
as medium-to-long range reorderings, (ii) whether
clause boundaries are respected, and (iii) whether lo-
cal (short range) reordering is performed correctly.
The following analysis is for Hindi to English re-
ordering with the best model (this is also the model
used for Machine Translation experiments reported
on in Section 5.5).
? Clause structure: As discussed in Section 3,
the canonical clause order in Hindi is SOV,
while in English it is SVO. However, variations
on this structure are possible and quite frequent
(e.g., clauses with two objects). To evaluate
clause restructuring, we compared sequences
of subjects, objects and verbs in the output and
reference reorderings.
We had a set of 70 sentences annotated with
subject, direct object, indirect object and verb
information ? these annotations were made on
the head word of each phrase, and the compar-
isons were on sequences of these words alone
and not the entire constituent phrase. 52 sen-
tences were reordered by the model to match
the order of the corresponding reference. Eight
sentences were ordered correctly but differently
from the reference, because the reference was
expressed in non-canonical fashion (e.g., in the
passive) ? note that these cases negatively im-
pact the monolingual BLEU score. The follow-
ing example shows a sentence being reordered
correctly, where, however, the reference is ex-
pressed differently (note the position of the
subject ?policy? (niiti) in the reference and the
reordered output) 1:
Input: aba1 (now) taka2 (till) aisii3 (this) niiti4
(policy) kabhii5 (ever) nahii6 (not) rahii7 (has)
hai8 (been)
Reordered: taka2 (till) aba1 (now) aisii3 (this)
niiti4 (policy) hai8 (been) kabhii5 (ever) nahii6
(not) rahii7 (has)
Reference: taka2 (till) aba1 (now) aisii3 (this)
kabhii5 (ever) nahii6 (not) rahii7 (has) hai8
(been) niiti4 (policy)
English: Till now this never has been the policy
The remaining ten sentences were reordered in-
correctly. These errors are largely in clauses
which deviate from the SVO order in some
way ? clauses with multiple subjects or objects,
clauses with no object, etc.. For example, the
following sentence with two subjects and ob-
jects corresponding to the verb wearing has not
been reordered correctly.
Input: sabhii1 (all) purusha2 (men) safeda3
(white) evama4 (and) mahilaaen5 (women)
kesariyaa6 (saffron) vastra7 (clothes) dhaarana8
(wear) kiye9 hue10 (-ing) thiin11 (were)
Reordered: sabhii1 (all) purusha2 (men)
safeda3 (white) evama4 (and) mahilaaen5
(women) kesariyaa6 (saffron) vastra7 (clothes)
dhaarana8 (wear) thiin11 (were) kiye9 hue10 (-
ing)
Reference: sabhii1 (all) purusha2 (men)
thiin11 (were) dhaarana8 (wear) kiye9 hue10 (-
1The numeric subscripts in the examples indicate word po-
sitions in the input.
492
ing) safeda3 (white) evama4 (and) mahilaaen5
(women) kesariyaa6 (saffron)
English: All men were wearing white and the
women saffron
The model possibly needs more data with pat-
terns that deviate from the standard SOV order
to learn to reorder them correctly. We could
also add to the model, features pertaining to
subject, object, etc.
? Clause boundaries: Measured on a set of
844 sentences which were marked with clause
boundaries, 37 sentences (4.4 %) had reorder-
ings that violated these boundaries. An exam-
ple of such a clause-boundary violation is be-
low:
Input: main1 (I) sarakaara2 (government) kaa3
(of) dhyaana4 (attention) maananiiya5 (hon-
ourable) pradhaana6 (prime) mantri7 (min-
ister) dvaaraa8 (by) isa9 (this) sabhaa10
(house) me11 (in) kiye12 gaye13 (made) isa14
(this) vaade15 (promise) ki16 ora17 (towards)
dilaanaa18 (to bring) chaahuungaa19 (would
like)
Reordered: main1 (I) chahuungaa19 (would
like) dilaanaa18 (to bring) kii16 ora17 (to-
wards) isa9 (this) vaade15 (promise) kiye12
gaye13 (made) dvaaraa8 (by) maananiiya5
(honourable) mantri7 (minister) pradhaana6
(prime) dhyaana4 (attention) kaa3 (of)
sarakaara2 (government) men11 (in) isa14 (this)
sabhaa10 (house)
Reference: main1 (I) chahuungaa19 (would
like) dilaanaa18 (to bring) dhyaana4 (attention)
kaa3 (of) sarakaara2 (government) kii16 ora17
(towards) isa9 (this) vaade15 (promise) kiye12
gaye13 (made) dvaaraa8 (by) maananiiya5
(honourable) mantri7 (minister) pradhaana6
(prime) men11 (in) isa9 (this) sabhaa10 (house)
English I would like to bring the attention of
the government towards this promise made by
the honourable prime minister in this house.
Note how the italicized clause, which is kept
together in the reference, is split up incorrectly
in the reordered output. The proportion of such
boundary violations is, however, quite low, be-
cause Hindi being a verb-final language, most
clauses end with a verb and it is probably quite
straightforward for the model to keep clauses
separate. A clause boundary detection program
should make it possible to eliminate the re-
maining errors.
? Local reordering: To estimate the short range
reordering performance, we consider how of-
ten different POS bigrams in the input are re-
ordered correctly. Here, we expect the model
to reorder prepositions correctly, and to avoid
any reordering that moves apart nouns and their
adjectival pre-modifiers or components of com-
pound nouns (see Section 3). Table 6 sum-
marizes the reordering performance for these
categories for a set of 280 sentences (same as
the test set used in Section 5.1). Each row
in Table 6 indicates the total number of cor-
rect instances for the pair, i.e., the number of
instances of the pair in the reference (column
titled Total), the number of instances that al-
ready appear in the correct order in the input
(column Input), and the number that are or-
dered correctly by the reordering model (col-
umn Reordered). The first two rows show that
adjective-noun and noun-noun (compounds)
are in most cases correctly retained in the orig-
inal order by the model. The final row shows
that while many prepositions have been moved
into their correct positions, there are still quite a
few mismatches with the reference. An impor-
tant reason why this happens is that nouns mod-
ified by prepositional phrases can often also be
expressed as noun compounds. For example,
vidyuta (electricity) kii (of) aavashyakataaen
(requirements) in Hindi can be expressed either
as ?requirements of electricity? or ?electricity
requirements?. The latter expression results in
a match with the input (explaining many of the
104 correct orders in the input) and a mismatch
with the model?s reordering. The same problem
in the training data would also adversely impact
the learning of the preposition reordering rule.
493
POS pair Total Input Reordered
adj-noun 234 192 196
noun-noun 46 44 42
prep-noun 436 104 250
Table 6: An analyis of reordering for a few POS bigrams
5.5 Machine translation results
We now present experiments in incorporating the re-
ordering model in machine translation systems. For
all results presented here, we reorder the training and
test data using the single best reordering based on
our reordering model for each sentence. For each of
the language pairs we evaluated, we trained Direct
Translation Model 2 (DTM) systems (Ittycheriah
and Roukos, 2007) with and without reordering and
compared performance on test data. We note that the
DTM system includes features that allow it to model
lexicalized reordering phenomena. The reordering
window size was set to +/-8 words for both the base-
line and our reordered input. In our experiments, we
left the word alignments fixed, i.e we reordered the
existing word alignments rather than realigning the
sentences after reordering. Redoing the word align-
ments with the reordered data could potentially give
further small improvements. We note that we ob-
tained better baseline performance using DTM sys-
tems than the standard Moses/Giza++ pipeline (e.g
we obtained a BLEU of 14.9 for English to Hindi
with a standard Moses/Giza++ pipeline). For all of
our systems we used a combination of HMM (Vo-
gel et al, 1996) and MaxEnt alignments (Ittycheriah
and Roukos, 2005).
For our Hindi-English experiments we use a train-
ing set of roughly 250k sentences (5.5Mwords) con-
sisting of the Darpa-TIDES dataset (Bojar et al,
2010) and an internal dataset from several domains
but dominated by news. Our test set was roughly
1.2K sentences from the news domain with a sin-
gle reference. To train our reordering model, we
used roughly 6K alignments plus 17K snippets se-
lected from MaxEnt alignments as described in Sec-
tion 5.1 with bigram, ContextPOS and ContextWord
features. The monolingual reordering BLEU (on the
same data reported on in Section 5.3) was 54.0 for
Hindi to English and 60.8 for English to Hindi.
For our Urdu-English experiments we used 70k
Language pair BLEU
Source Target Unreordered Reordered
Hindi English 14.7 16.7
Urdu English 23.3 24.8
English Hindi 20.7 22.5
Table 7: Translation performance without reordering
(baseline) compared with performance after preordering
with our reordering model.
sentences from the NIST MT-08 training corpus
and used the MT-08 eval set for testing. We note
that the MT-08 eval set has four references as com-
pared to one reference for our Hindi-English test
set. This largely explains the improved baseline per-
formance for Urdu-English as compared to Hindi-
English. We present averaged results for the Web
and News part of the test sets. To train the reorder-
ing model we used 9K hand alignments and 11K
snippets extracted from MaxEnt alignments as de-
scribed in Section 5.1 with bigram, ContextPOS and
ContextWord context feature. The monolingual re-
ordering BLEU for the reordering model thus ob-
tained (on the same data reported on in Section 5.3)
was 52.7.
Table 7 shows that for Hindi to English, English
to Hindi and for Urdu to English we see a gain
of 1.5 - 2 BLEU points. For English ? Hindi
we also experimented with a system that uses rules
(learned from the data using the methods described
in (Visweswariah et al, 2010)) applied to a parse to
reorder source side English sentences. This system
had a BLEU score of 21.2, which is an improvement
over the baseline, but our reordering model is better
by 1.3 BLEU points.
An added benefit of our reordering model is that
the decoder can be run with a smaller search space
exploring only a small amount of reordering with-
out losing accuracy but running substantially faster.
Table 8 shows the variation in machine Hindi to En-
glish translation performance with varying skip size
(this parameter sets the maximum number of words
skipped during decoding, lower values are associ-
ated with a restricted decoder search space and in-
creased speed).
494
skip Unreordered Reordered
2 12.2 16.7
4 13.4 16.7
8 14.7 16.4
Table 8: Translation performance with/without reorder-
ing with varying decoder search space.
6 Conclusion and future work
In this paper we presented a reordering model to
reorder source language data to make it resemble
the target language word order without using either
a source or target parser. We showed consistent
gains of up to 2 BLEU points in machine transla-
tion performance using this model to preorder train-
ing and test data. We show better performance com-
pared to syntax based reordering rules for English
to Hindi translation. Our model used only a part of
speech tagger (sometimes trained with fairly small
amounts of data) and a small corpus of word align-
ments. Considering the fact that treebanks required
to build high quality parsers are costly to obtain, we
think that our reordering model is a viable alterna-
tive to using syntax for reordering. We also note,
that with the preordering based on our reordering
model we can achieve the best BLEU scores with
a much tighter search space in the decoder. Even ac-
counting for the cost of finding the best reordering
according to our model, this usually results in faster
processing than if we did not have the reordering in
place.
In future work we plan to explore using more data
from automatic alignments, perhaps by considering
a joint model for aligning and reordering. We would
also like to explore doing away with the requirement
of having a POS tagger, using completely unsuper-
vised methods to class words. We currently only
look at word pairs in calculating the loss function
used in MIRA updates. We would like to investigate
the use of other loss functions and their effect on re-
ordering performance. We also would like to explore
whether the use of scores from our reordering model
directly in machine translation systems can improve
performance relative to using just the single best re-
ordering.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of ACL, ACL-44, pages 529?536, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
David L. Applegate, Robert E. Bixby, Vasek Chvatal, and
William J. Cook. 2005. Concorde tsp solver. In
http://www.tsp.gatech.edu/.
Ibrahim Badr, Rabih Zbib, and James Glass. 2009. Syn-
tactic phrase reordering for English-to-Arabic statisti-
cal machine translation. In Proceedings of EACL.
Ondrej Bojar, Pavel Stranak, and Daniel Zeman. 2010.
Data issues in English-to-Hindi machine translation.
In LREC.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540,
Morristown, NJ, USA. Association for Computational
Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. Journal of
Machine Learning Research.
Aniket Dalal, Kumar Nagaraj, Uma Sawant, Sandeep
Shelke, and Pushpak Bhattacharyya. 2007. Building
feature rich pos tagger for morphologically rich lan-
guages: Experiences in Hindi. In Proceedings of In-
ternational Conference on Natural Language Process-
ing.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable inference and
training of context-rich syntactic translation models.
In Proceedings of ACL.
D. Genzel. 2010. Automatically learning source-side re-
ordering rules for large scale machine translation. In
Proceedings of the 23rd International Conference on
Computational Linguistics.
Kurt Hornik and Michael Hahsler. 2009. TSP?
infrastructure for the traveling salesperson problem.
Journal of Statistical Software, 23(i02).
Sarmad Hussain. 2008. Resources for Urdu language
processing. In Proceedings of the 6th Workshop on
Asian Language Resources, IJCNLP?08.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for Arabic-English ma-
chine translation. In Proceedings of HLT/EMNLP,
HLT ?05, pages 89?96, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
495
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In Proceedings of HLT-NAACL,
pages 57?64.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Comput. Linguist.,
25:607?615, December.
Young-Suk Lee, Bing Zhao, and Xiaoqian Luo. 2010.
Constituent reordering and syntax models for English-
to-Japanese statistical machine translation. In COL-
ING.
Y. Liu, Q. Liu, and S. Lin. 2006. Tree-to-String align-
ment template for statistical machine translation. In
Proceedings of ACL.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
HLT.
Ananthakrishnan Ramanathan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and morphology: addressing the crux
of the fluency problem in English-Hindi smt. In
Proceedings of ACL-IJCNLP.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a dynamic programming beam search al-
gorithm for statistical machine translation. Computa-
tional Linguistics, 29(1):97?133.
Roy Tromble and Jason Eisner. 2009. Learning linear or-
dering problems for better translation. In Proceedings
of EMNLP.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax based reordering with automat-
ically derived rules for improved statistical machine
translation. In Proceedings of the 23rd International
Conference on Computational Linguistics.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational Linguistics.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of EMNLP-CoNLL.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical MT system with automatically learned rewrite
patterns. In COLING.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical mt. In Proceedings of ACL.
Mikhail Zaslavskiy, Marc Dymetman, and Nicola Can-
cedda. 2009. Phrase-based statistical machine transla-
tion as a traveling salesman problem. In Proceedings
of ACL-IJCNLP.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation.
496
Proceedings of NAACL-HLT 2013, pages 315?324,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Improving reordering performance using higher order and structural
features
Mitesh M. Khapra
IBM Research India
mikhapra@in.ibm.com
Ananthakrishnan Ramanathan
IBM Research India
anandr42@gmail.com
Karthik Visweswariah
IBM Research India
v-karthik@in.ibm.com
Abstract
Recent work has shown that word aligned data
can be used to learn a model for reordering
source sentences to match the target order.
This model learns the cost of putting a word
immediately before another word and finds the
best reordering by solving an instance of the
Traveling Salesman Problem (TSP). However,
for efficiently solving the TSP, the model is
restricted to pairwise features which examine
only a pair of words and their neighborhood.
In this work, we go beyond these pairwise fea-
tures and learn a model to rerank the n-best
reorderings produced by the TSP model us-
ing higher order and structural features which
help in capturing longer range dependencies.
In addition to using a more informative set
of source side features, we also capture target
side features indirectly by using the transla-
tion score assigned to a reordering. Our exper-
iments, involving Urdu-English, show that the
proposed approach outperforms a state-of-the-
art PBSMT system which uses the TSP model
for reordering by 1.3 BLEU points, and a pub-
licly available state-of-the-art MT system, Hi-
ero, by 3 BLEU points.
1 Introduction
Handling the differences in word orders between
pairs of languages is crucial in producing good ma-
chine translation. This is especially true for lan-
guage pairs such as Urdu-English which have sig-
nificantly different sentence structures. For exam-
ple, the typical word order in Urdu is Subject Object
Verb whereas the typical word order in English is
Subject Verb Object. Phrase based systems (Koehn
et al, 2003) rely on a lexicalized distortion model
(Al-Onaizan and Papineni, 2006; Tillman, 2004)
and the target language model to produce output
words in the correct order. This is known to be in-
adequate when the languages are very different in
terms of word order (refer to Table 3 in Section 3).
Pre-ordering source sentences while training and
testing has become a popular approach in overcom-
ing the word ordering challenge. Most techniques
for pre-ordering (Collins et al, 2005; Wang et al,
2007; Ramanathan et al, 2009) depend on a high
quality source language parser, which means these
methods work only if the source language has a
parser (this rules out many languages). Recent work
(Visweswariah et al, 2011) has shown that it is pos-
sible to learn a reordering model from a relatively
small number of hand aligned sentences . This elim-
inates the need of a source or target parser.
In this work, we build upon the work of
Visweswariah et al (2011) which solves the reorder-
ing problem by treating it as an instance of the
Traveling Salesman Problem (TSP). They learn a
model which assigns costs to all pairs of words in
a sentence, where the cost represents the penalty of
putting a word immediately preceding another word.
The best permutation is found via the chained Lin-
Kernighan heuristic for solving a TSP. Since this
model relies on solving a TSP efficiently, it cannot
capture features other than pairwise features that ex-
amine the words and neighborhood for each pair of
words in the source sentence. In the remainder of
this paper we refer to this model as the TSP model.
Our aim is to go beyond this limitation of the TSP
model and use a richer set of features instead of us-
ing pairwise features only. In particular, we are in-
terested in features that allow us to examine triples
of words/POS tags in the candidate reordering per-
315
mutation (this is akin to going from bigram to tri-
gram language models), and also structural features
that allow us to examine the properties of the seg-
mentation induced by the candidate permutation. To
go beyond the set of features incorporated by the
TSP model, we do not solve the search problem
which would be NP-hard. Instead, we restrict our-
selves to an n-best list produced by the base TSP
model and then search in that list. Using a richer
set of features, we learn a model to rerank these n-
best reorderings. The parameters of the model are
learned using the averaged perceptron algorithm. In
addition to using a richer set of source side features
we also indirectly capture target side features by in-
terpolating the score assigned by our model with the
score assigned by the decoder of a MT system.
To justify the use of these informative features,
we point to the example in Table 1. Here, the head
(driver) of the underlined English Noun Phrase (The
driver of the car) appears to the left of the Noun
Phrase whereas the head (chaalak {driver}) of the
corresponding Urdu Noun Phrase (gaadi {car} ka
{of} chaalak {driver}) appears to the right of the
Noun Phrase. To produce the correct reordering of
the source Urdu sentence the model has to make an
unusual choice of putting gaadi {car} before bola
{said}. We say this is an unusual choice because the
model examines only pairwise features and it is un-
likely that it would have seen sentences having the
bigram ?car said?. If the exact segmentation of the
source sentence was known, then the model could
have used the information that the word gaadi {car}
appears in a segment whose head is the noun chaalak
{driver} and hence its not unusual to put gaadi {car}
before bola {said} (because the construct ?NP said?
is not unusual). However, since the segmentation
of the source sentence is not known in advance, we
use a heuristic (explained later) to find the segmen-
tation induced by a reordering. We then extract
features (such as first word current segment,
end word current segment) to approximate these
long range dependencies.
Using this richer set of features with Urdu-
English as the source language pair, our approach
outperforms the following state of the art systems:
(i) a PBSMT system which uses TSP model for re-
ordering (by 1.3 BLEU points), (ii) a hierarchical
PBSMT system (by 3 BLEU points). The overall
Input Urdu: fir gaadi ka chaalak kuch bola
Gloss: then car of driver said something
English: Then the driver of the car said something.
Ref. reordering: fir chaalak ka gaadi bola kuch
Table 1: Example motivating the use of structural features
gain is 6.3 BLEU points when compared to a stan-
dard PBSMT system which uses a lexicalized distor-
tion model (Al-Onaizan and Papineni, 2006).
The rest of this paper is organized as follows. In
Section 2 we discuss our approach of re-ranking the
n-best reorderings produced by the TSP model. This
includes a discussion of the model used, the features
used and the algorithm used for learning the parame-
ters of the model. It also includes a discussion on the
modification to the Chained Lin-Kernighan heuris-
tic to produce n-best reorderings. Next, in Section
3 we describe our experimental setup and report the
results of our experiments. In Section 4 we present
some discussions based on our study. In section 5 we
briefly describe some prior related work. Finally, in
Section 6, we present some concluding remarks and
highlight possible directions for future work.
2 Re-ranking using higher order and
structural features
As mentioned earlier, the TSP model (Visweswariah
et al, 2011) looks only at local features for a word
pair (wi, wj). We believe that for better reorder-
ing it is essential to look at higher order and struc-
tural features (i.e., features which look at the overall
structure of a sentence). The primary reason why
Visweswariah et al (2011) consider only pairwise
bigram features is that with higher order features the
reordering problem can no longer be cast as a TSP
and hence cannot be solved using existing efficient
heuristic solvers. However, we do not have to deal
with an NP-Hard search problem because instead of
considering all possible reorderings we restrict our
search space to only the n-best reorderings produced
by the base TSP model. Formally, given a set of
reorderings, ? = [pi1, pi2, pi3, ...., pin], for a source
sentence s, we are interesting in assigning a score,
score(pi), to each of these reorderings and pick the
reordering which has the highest score. In this paper,
we parametrize this score as:
score(pi) = ?T?(pi) (1)
316
where, ? is the weight vector and ?(pi) is a vector
of features extracted from the reordering pi. The aim
then is to find,
pi? = arg max
pi??
score(pi) (2)
In the following sub-sections, we first briefly
describe our overall approach towards finding pi?.
Next, we describe our modification to the Lin-
Kernighan heuristic for producing n-best outputs
for TSP instead of the 1-best output used by
(Visweswariah et al, 2011). We then discuss the fea-
tures used for re-ranking these n-best outputs, fol-
lowed by a discussion on the learning algorithm used
for estimating the parameters of the model. Finally,
we describe how we interpolate the score assigned
by our model with the score assigned by the decoder
of a SMT engine to indirectly capture target side fea-
tures.
2.1 Overall approach
The training stage of our approach involves two
phases : (i) Training a TSP model which will be
used to generate n-best reorderings and (ii) Training
a re-ranking model using these n-best reorderings.
For training both the models we need a collection
of sentences where the desired reordering pi?(x) for
each input sentence x is known. These reference or-
derings are derived from word aligned source-target
sentence pairs (see first 4 rows of Figure 1). We first
divide this word aligned data into N parts and use
A?i to denote the alignments leaving out the i-th
part. We then train a TSP model M?i using refer-
ence reorderings derived from A?i as described in
(Visweswariah et al, 2011). Next, we produce n-
best reorderings for the source sentences using the
algorithm getNBestReorderings(sentence) de-
scribed later. Dividing the data into N parts is nec-
essary to ensure that the re-ranking model is trained
using a realistic n-best list rather than a very opti-
mistic n-best list (which would be the case if part i
is reordered using a model which has already seen
part i during training).
Each of the n-best reorderings is then repre-
sented as a feature vector comprising of higher
order and structural features. The weights
of these features are then estimated using the
averaged perceptron method. At test time,
getNBestReorderings(sentence) is used to gen-
erate the n-best reorderings for the test sentence us-
ing the trained TSP model. These reorderings are
then represented using higher order and structural
features and re-ranked using the weights learned ear-
lier. We now describe the different stages of our al-
gorithm.
2.2 Generating n-best reorderings for the TSP
model
The first stage of our approach is to train a TSP
model and generate n-best reorderings using it. The
decoder used by Visweswariah et al (2011) relies
on the Chained Lin-Kernighan heuristic (Lin and
Kernighan, 1973) to produce the 1-best permutation
for the TSP problem. Since our algorithm aims at
re-ranking an n-best list of permutations (reorder-
ings), we made a modification to the Chained Lin-
Kernighan heuristic to produce this n-best list as
shown in Algorithm 1 .
Algorithm 1 getNBestReorderings(sentence)
NbestSet = ?
pi? = Identity permutation
pi? = linkernighan(pi?)
insert(NbestSet, pi?)
for i = 1? nIter do
pi
?
= perturb(pi?)
pi
?
= linkernighan(pi
?
)
if C(pi
?
) < maxpi?NbestSetC(pi) then
InsertOrReplace(NbestSet, pi
?
)
end if
if C(pi
?
) < C(pi?) then
pi? = pi
?
end if
end for
In Algorithm 1 perturb() is a four-edge pertur-
bation described in (Applegate et al, 2003), and
linkernighan() is the Lin-Kernighan heuristic that
applies a sequence of flips that potentially returns
a lower cost permutation as described in (Lin and
Kernighan, 1973). The cost C(pi) is calculated us-
ing a trained TSP model.
2.3 Features
We represent each of the n-best reorderings obtained
above as a vector of features which can be divided
into two sets : (i) higher order features and (ii) struc-
317
Segmentation Based Features
(extracted for every segment in
the induced segmentation)
Features fired for the seg-
ment [mere(PRP) ghar(NN)]
in Figure1
end lex current segment ghar
end lex prev segment Shyam
end pos current segment NN
end pos prev segment NN
length of current segment 2
first lex current segment mere
first lex next segment aaye
first pos current segment PRP
first pos next segment V RB
Higher order features Features fired for the triplet
Shyam(NN) the(Vaux)
aaye(VRB) in Figure1
lex triplet jumps lex triplet = ?Shyam the
aaye? && jumps = [4,?1]
pos triplet jumps pos triplet = ?NN Vaux
VRB? && jumps = [4,?1]
Table 2: Features used in our model.
tural features. The higher order features are es-
sentially trigram lexical and pos features whereas
the structural features are derived from the sentence
structure induced by a reordering (explained later).
2.3.1 Higher Order Features
Since deriving a good reordering would essen-
tially require analyzing the syntactic structure of the
source sentence, the tasks of reordering and parsing
are often considered to be related. The main motiva-
tion for using higher order features thus comes from
a related work on parsing (Koo and Collins, 2010)
where the performance of a state of the art parser
was improved by considering higher order depen-
dencies. In our model we use trigram features (see
Table 2) of the following form:
?(rui, rui+1, rui+2, J(rui, rui+1), J(rui+1, rui+2))
where rui =word at position i in the reordered
source sentence and J(x, y) = difference between
the positions of x and y in the original source
sentence.
Figure 1 shows an example of jumps between dif-
ferent word pairs in an Urdu sentence. Since such
higher order features will typically be sparse, we
also use some back-off features. For example, in-
stead of using the absolute values of jumps we di-
vide the jumps into 3 buckets, viz., high, low and
medium and use these buckets in conjunction with
the triplets as back-off features.
Figure 1: Segmentation induced on the Urdu sentence
when it is reordered according to its English translation.
Note that the words Shyam and mere are adjacent to each
other in the original Urdu sentence but not in the re-
ordered Urdu sentence. Hence, the word mere marks the
beginning of a new segment.
2.3.2 Structural Features
The second set of features is based on the hy-
pothesis that any reordering of the source sentence
induces a segmentation on the sentence. This seg-
mentation is based on the following heuristic: if wi
and wi+1 appear next to each other in the original
sentence but do not appear next to each other in the
reordered sentence then wi marks the end of a seg-
ment and wi+1 marks the beginning of the next seg-
ment. To understand this better please refer to Fig-
ure 1 which shows the correct reordering of an Urdu
sentence based on its English translation and the cor-
responding segmentation induced on the Urdu sen-
tence. If the correct segmentation of a sentence is
known in advance then one could use a hierarchical
model where the goal would be to reorder segments
instead of reordering words individually (basically,
instead of words, treat segments as units of reorder-
ing. In principle, this is similar to what is done by
parser based reordering methods). Since the TSP
model does not explicitly use segmentation based
features it often produces wrong reorderings (refer
to the motivating example in Section 1).
Reordering such sentences correctly requires
some knowledge about the hierarchical structure of
the sentence. To capture such hierarchical informa-
tion, we use features which look at the elements
318
(words, pos tags) of a segment and its neighboring
segments. These features along with examples are
listed in Table 2. These features should help us in
selecting a reordering which induces a segmentation
which is closest to the correct segmentation induced
by the reference reordering. Note that every feature
listed in Table 2 is a binary feature which takes on
the value 1 if it fires for the given reordering and
value 0 if it does not fire for the given reordering. In
addition to the features listed in Table 2 we also use
the score assigned by the TSP model as a feature.
2.4 Estimating model parameters
We use perceptron as the learning algorithm for es-
timating the parameters of our model described in
Equation 1. To begin with, all parameters are ini-
tialized to 0 and the learning algorithm is run for N
iterations. During each iteration the parameters are
updated after every training instance is seen. For ex-
ample, during the i-th iteration, after seeing the j-th
training sentence, we update the k-th parameter ?k
using the following update rule:
?(i,j)k = ?
(i,j?1)
k + ?k(pi
gold
j )? ?k(pi
?
j ) (3)
where, ?(i,j)k = value of the k-th parameter after
seeing sentence j in iteration i
?k = k-th feature
pigoldj = gold reordering for the j-th sentence
pi?j = arg max
pi??j
?(i,j?1)
T
?(pi)
where ?j is the set of n-best reorderings for the j-
th sentence. pi?j is thus the highest-scoring reorder-
ing for the j-th sentence under the current parame-
ter vector. Since the averaged perceptron method is
known to perform better than the perceptron method,
we used the averaged values of the parameters at the
end of N iterations, calculated as:
?avgk =
1
N ? t
N?
i=1
t?
j=1
?(i,j)k (4)
where, N = Number of iterations
t = Number of training instances
We observed that in most cases the reference re-
ordering in not a part of the n-best list produced
by the TSP model. In such cases instead of using
?k(pi
gold
j ) for updating the weights in Equation 3 we
use ?k(pi
closest to gold
j ) as this is known to be a better
strategy for learning a re-ranking model (Arun and
Koehn, 2007). piclosest to goldj is given by:
arg max
piij??j
# of common bigram pairs in piij and pi
gold
j
len(pigoldj )
where, ?j = set of n-best reorderings for j
th sentence
piclosest to goldj is thus the reordering which has the
maximum overlap with pigoldj in terms of the number
of word pairs (wm, wn) where wn is put next to wm.
2.5 Interpolating with MT score
The approach described above aims at producing a
better reordering by extracting richer features from
the source sentence. Since the final aim is to im-
prove the performance of an MT system, it would
potentially be beneficial to interpolate the scores as-
signed by Equation 1 to a given reordering with the
score assigned by the decoder of an MT system to
the translation of the source sentence under this re-
ordering. Intuitively, the MT score would allow us
to capture features from the target sentence which
are obviously not available to our model. With this
motivation, we use the following interpolated score
(scoreI ) to select the best translation.
scoreI(ti) = ??score?(pii) + (1? ?) ? scoreMT (ti)
where, ti =translation produced under the i-th
reordering of the source sentence
score?(pii) =score assigned by our model to the
i-th reordering
scoreMT (ti) =score assigned by the MT system to ti
The weight ? is used to ensure that score?(pii) and
scoreMT (pii) are in the same range (it just serves as
a normalization constant). We acknowledge that the
above process is expensive because it requires the
MT system to decode n reorderings for every source
sentence. However, the aim of this work is to show
that interpolating with the MT score which implic-
itly captures features from the target sentence helps
in improving the performance. Ideally, this interpo-
lation should (and can) be done at decode time with-
out having to decode n reorderings for every source
319
sentence (for example by expressing the n reorder-
ings as a lattice), but, we leave this as future work.
3 Empirical evaluation
We evaluated our reordering approach on Urdu-
English. We use two types of evaluation, one in-
trinsic and one extrinsic. For intrinsic evaluation,
we compare the reordered source sentence in Urdu
with a reference reordering obtained from the hand
alignments using BLEU (referred to as monolingual
BLEU or mBLEU by (Visweswariah et al, 2011) ).
Additionally, we evaluate the effect of reordering on
MT performance using BLEU (extrinsic evaluation).
As mentioned earlier, our training process in-
volves two phases : (i) Generating n-best reorder-
ings for the training data and (ii) using these n-best
reorderings to train a perceptron model. We use the
same data for training the reordering model as well
as our perceptron model. This data contains 180K
words of manual alignments (part of the NIST MT-
08 training data) and 3.9M words of automatically
generated machine alignments (1.7M words from
the NIST MT-08 training data1 and 2.2M words ex-
tracted from sources on the web2). The machine
alignments were generated using a supervised maxi-
mum entropy model (Ittycheriah and Roukos, 2005)
and then corrected using an improved correction
model (McCarley et al, 2011). We first divide the
training data into 10 folds. The n-best reorder-
ings for each fold are then generated using a model
trained on the remaining 9 folds. This division into
10 folds is done for reasons explained earlier in Sec-
tion 2.1. These n-best reorderings are then used to
train the perceptron model as described in Section
2.4. Note that Visweswariah et al (2011) used only
manually aligned data for training the TSP model.
However, we use machine aligned data in addition
to manually aligned data for training the TSP model
as it leads to better performance. We used this im-
provised TSP model as the state of the art baseline
(rows 2 and 3 in Tables 3 and 4 respectively) for
comparing with our approach.
We observed that the perceptron algorithm con-
verges after 5 iterations beyond which there is very
little (<1%) improvement in the bigram precision on
1http://www.ldc.upenn.edu
2http://centralasiaonline.com
the training data itself (bigram precision is the frac-
tion of word pairs which are correctly put next to
each other). Hence, for all the numbers reported in
this paper, we used 5 iterations of perceptron train-
ing. Similarly, while generating the n-best reorder-
ings, we experimented with following values of n :
10, 25, 50, 100 and 200. We observed that, by re-
stricting the search space to the top-50 reorderings
we get the best reordering performance (mBLEU)
on a development set. Hence, we used n=50 for our
MT experiments.
For intrinsic evaluation we use a development set
of 8017 Urdu tokens reordered manually. Table 3
compares the performance of the top-1 reordering
output by our algorithm with the top-1 reordering
generated by the improved TSP model in terms of
mBLEU. We see a gain of 1.8 mBLEU points with
our approach.
Next, we see the impact of the better reorderings
produced by our system on the performance of
a state-of-the-art MT system. For this, we used
a standard phrase based system (Al-Onaizan and
Papineni, 2006) with a lexicalized distortion model
with a window size of +/-4 words (Tillmann and
Ney, 2003). As mentioned earlier, our training data
consisted of 3.9M words including the NIST MT-08
training data. We use HMM alignments along with
higher quality alignments from a supervised aligner
(McCarley et al, 2011). The Gigaword English
corpus was used for building the English language
model. We report results on the NIST MT-08
evaluation set, averaging BLEU scores from the
News and Web conditions to provide a single BLEU
score. Table 4 compares the MT performance
obtained by reordering the training and test data
using the following approaches:
1. No pre-ordering: A baseline system which
does not use any source side reordering as a pre-
processing step
2. HIERO : A state of the art hierarchical phrase
based translation system (Chiang, 2007)
3. TSP: A system which uses the 1-best reordering
produced by the TSP model
4. Higher order & structural features: A system
320
Approach mBLEU
Unreordered 31.2
TSP 56.6
Higher order & structural features 58.4
Table 3: mBLEU scores for Urdu to English reordering
using different models.
Approach BLEU
No pre-ordering 21.9
HIERO 25.2
TSP 26.9
Higher order & structural features 27.5
Interpolating with MT score 28.2
Table 4: MT performance for Urdu to English without re-
ordering and with reordering using different approaches.
which reranks n-best reorderings produced by TSP
using higher order and structural features
5. Interpolating with MT score : A system which
interpolates the score assigned to a reordering by
our model with the score assigned by a MT system
We used Joshua 4.0 (Ganitkevitch et al, 2012)
which provides an open source implementation of
HIERO. For training, tuning and testing HIERO
we used the same experimental setup as described
above. As seen in Table 4, we get an overall gain of
6.2 BLEU points with our approach as compared to
a baseline system which does not use any reordering.
More importantly, we outperform (i) a PBSMT sys-
tem which uses the TSP model by 1.3 BLEU points
and (ii) a state of the art hierarchical phrase based
translation system by 3 points.
4 Discussions
We now discuss some error corrections and ablation
tests.
4.1 Example of error correction
We first give an example where the proposed ap-
proach performed better than the TSP model. In the
example below, I = input sentence, E= gold English
translation, T = incorrect reordering produced by
TSP and O = correct reordering produced by our
approach. Note that the words roman catholic aur
protestant in the input sentence get translated as
Sentence length mBLEU
Unreordered TSP Our
approach
1-14 words (small) 29.7 58.7 57.8
15-22 words (med.) 28.2 56.8 59.2
23+ words (long) 33.4 55.8 58.2
All 31.2 56.6 58.4
Table 5: mBLEU improvements on sentences of different
lengths
a continuous phrase in English (Roman Catholic
and Protestant) and hence should be treated as a
single unit by the reordering model. The TSP model
fails to keep this segment intact whereas our model
(which uses segmentation based features) does so
and matches the reference reordering.
I: ab roman catholic aur protestant ke darmiyaan
ikhtilafat khatam ho chuke hai
E: The differences between Roman Catholics and
Protestants have now ended
T: ab roman ikhtilafat ke darmiyaan catholic aur
protestant hai khatam ho chuke
O: ab ikhtilafat ke darmiyaan roman catholic aur
protestant hai khatam ho chuke
4.2 Performance based on sentence length
We split the test data into roughly three equal parts
based on length, and calculated the mBLEU im-
provements on each of these parts as reported in
Table 5. These results show that the model works
much better for medium-to-long sentences. In fact,
we see a drop in performance for small sentences. A
possible reason for this could be that the structural
features that we use are derived through a heuristic
that is error-prone, and in shorter sentences, where
there would be fewer reordering problems, these er-
rors hurt more than they help. While this needs to be
analyzed further, we could meanwhile combine the
two models fruitfully by using the base TSP model
for small sentences and the new model for longer
sentences.
321
Disabled feature mBLEU
end lex current segment 57.6
end lex prev segment 57.6
end pos current segment 57.8
end pos prev segment 57.4
length 57.6
lex triplet jumps 58.0
pos triplet jumps 56.1
first lex current segment 58.2
first lex next segment 58.2
first pos current segment 57.6
first pos next segment 57.6
NONE 58.4
Table 6: Ablation test indicating the contribution of each
feature to the reordering performance.
4.3 Ablation test
To study the contribution of each feature to the
reordering performance, we did an ablation test
wherein we disabled one feature at a time and mea-
sured the change in the mBLEU scores. Table 6
summarizes the results of our ablation test. The
maximum drop in performance is obtained when the
pos triplet jumps feature is disabled. This obser-
vation supports our claim that higher order features
(more than bigrams) are essential for better reorder-
ing. The lex triplet jumps feature has the least
impact on the performance mainly because it is a
lexicalized feature and hence very sparse. Also note
that there is a high correlation between the perfor-
mances obtained by dropping one feature from each
of the following pairs :
i) first lex current segment, first lex next segment
ii) first pos current segment, first pos next segment
iii) end lex current segment, end lex next segment.
This is because these pairs of features are
highly dependent features. Note that similar to
the pos triplet jumps feature we also tried a
pos quadruplet jumps feature but it did not help
(mainly due to overfitting and sparsity).
5 Related Work
There are several studies which have shown that re-
ordering the source side sentence to match the target
side order leads to improvements in Machine Trans-
lation. These approaches can be broadly classified
into three types. First, approaches which reorder
source sentences by applying rules to the source side
parse; the rules are either hand-written (Collins et
al., 2005; Wang et al, 2007; Ramanathan et al,
2009) or learned from data (Xia and McCord, 2004;
Genzel, 2010; Visweswariah et al, 2010). These
approaches require a source side parser which is
not available for many languages. The second type
of approaches treat machine translation decoding
as a parsing problem by using source and/or tar-
get side syntax in a Context Free Grammar frame-
work. These include Hierarchical models (Chi-
ang, 2007) and syntax based models (Yamada and
Knight, 2002; Galley et al, 2006; Liu et al, 2006;
Zollmann and Venugopal, 2006). The third type of
approaches, avoid the use of a parser (as required
by syntax based models) and instead train a reorder-
ing model using reference reorderings derived from
aligned data. These approaches (Tromble and Eis-
ner, 2009; Visweswariah et al, 2011; DeNero and
Uszkoreit, 2011; Neubig et al, 2012) have a low de-
code time complexity as reordering is done as a pre-
processing step and not integrated with the decoder.
Our work falls under the third category, as it im-
proves upon the work of (Visweswariah et al, 2011)
which is closely related to the work of (Tromble
and Eisner, 2009) but performs better. The focus
of our work is to use higher order and structural
features (based on segmentation of the source sen-
tence) which are not captured by their model. Some
other works have used collocation based segmenta-
tion (Henr??quez Q. et al, 2010) and Multiword Ex-
pressions as segments (Bouamor et al, 2012) to im-
prove the performance of SMT but without much
success. The idea of improving performance by re-
ranking a n-best list of outputs has been used re-
cently for the related task of parsing (Katz-Brown et
al., 2011) using targeted self-training for improving
the performance of reordering. However, in contrast,
in our work we directly aim at improving the perfor-
mance of a reordering model.
6 Conclusion
In this work, we proposed a model for re-ranking
the n-best reorderings produced by a state of the
art reordering model (TSP model) which is limited
to pair wise features. Our model uses a more in-
formative set of features consisting of higher order
features, structural features and target side features
322
(captured indirectly using translation scores). The
problem of intractability is solved by restricting the
search space to the n-best reorderings produced by
the TSP model. A detailed ablation test shows that
of all the features used, the pos triplet features are
most informative for reordering. A gain of 1.3 and 3
BLEU points over a state of the art phrase based and
hierarchical machine translation system respectively
provides good extrinsic validation of our claim that
such long range features are useful.
As future work, we would like to evaluate our al-
gorithm on other language pairs. We also plan to
integrate the score assigned by our model into the
decoder to avoid having to do n decodings for ev-
ery source sentence. Also, it would be interesting
to model the segmentation explicitly, where the aim
would be to first segment the sentence and then use
a two level hierarchical reordering model which first
reorders these segments and then reorders the words
within the segment.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of ACL, ACL-44, pages 529?536, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
David Applegate, William Cook, and Andre Rohe. 2003.
Chained lin-kernighan for large traveling salesman
problems. In INFORMS Journal On Computing.
Abhishek Arun and Philipp Koehn. 2007. Online
learning methods for discriminative training of phrase
based statistical machine translation. In In Proceed-
ings of MT Summit.
Dhouha Bouamor, Nasredine Semmar, and Pierre
Zweigenbaum. 2012. Identifying bilingual multi-
word expressions for statistical machine translation.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Thierry Declerck, Mehmet Uur Doan, Bente
Maegaard, Joseph Mariani, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Eight Interna-
tional Conference on Language Resources and Eval-
uation (LREC?12), Istanbul, Turkey, may. European
Language Resources Association (ELRA).
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Comput. Linguist., 33(2):201?228, June.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540,
Morristown, NJ, USA. Association for Computational
Linguistics.
John DeNero and Jakob Uszkoreit. 2011. Inducing sen-
tence structure from parallel corpora for reordering.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11,
pages 193?203, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, ACL-44,
pages 961?968, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post,
and Chris Callison-Burch. 2012. Joshua 4.0: Pack-
ing, pro, and paraphrases. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
pages 283?291, Montre?al, Canada, June. Association
for Computational Linguistics.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, COLING ?10,
pages 376?384, Stroudsburg, PA, USA. Association
for Computational Linguistics.
A. Carlos Henr??quez Q., R. Marta Costa-jussa`, Vidas
Daudaravicius, E. Rafael Banchs, and B. Jose? Marin?o.
2010. Using collocation segmentation to augment the
phrase table. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, WMT ?10, pages 98?102, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for Arabic-English ma-
chine translation. In Proceedings of HLT/EMNLP,
HLT ?05, pages 89?96, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Jason Katz-Brown, Slav Petrov, Ryan McDonald, Franz
Och, David Talbot, Hiroshi Ichikawa, Masakazu Seno,
and Hideto Kazawa. 2011. Training a parser for
machine translation reordering. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?11, pages 183?192,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
323
Annual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1?11, Stroudsburg, PA,
USA. Association for Computational Linguistics.
S. Lin and B. W. Kernighan. 1973. An effective heuristic
algorithm for the travelling-salesman problem. Oper-
ations Research, pages 498?516.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, ACL-44, pages 609?616, Stroudsburg,
PA, USA. Association for Computational Linguistics.
J. Scott McCarley, Abraham Ittycheriah, Salim Roukos,
Bing Xiang, and Jian-ming Xu. 2011. A correc-
tion model for word alignments. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?11, pages 889?898,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a discriminative parser to optimize
machine translation reordering. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 843?853, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Ananthakrishnan Ramanathan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and morphology: addressing the crux
of the fluency problem in English-Hindi smt. In
Proceedings of ACL-IJCNLP.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a dynamic programming beam search al-
gorithm for statistical machine translation. Computa-
tional Linguistics, 29(1):97?133.
Roy Tromble and Jason Eisner. 2009. Learning linear or-
dering problems for better translation. In Proceedings
of EMNLP.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax based reordering with automat-
ically derived rules for improved statistical machine
translation. In Proceedings of the 23rd International
Conference on Computational Linguistics.
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A word reordering model for
improved machine translation. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?11, pages 486?496,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of EMNLP-CoNLL.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical MT system with automatically learned rewrite
patterns. In COLING.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical mt. In Proceedings of ACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation.
324
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1275?1284,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Cut the noise: Mutually reinforcing reordering and alignments for
improved machine translation
Karthik Visweswariah
IBM Research India
v-karthik@in.ibm.com
Mitesh M. Khapra
IBM Research India
mikhapra@in.ibm.com
Ananthakrishnan Ramanathan
IBM Research India
anandr42@gmail.com
Abstract
Preordering of a source language sentence
to match target word order has proved to
be useful for improving machine transla-
tion systems. Previous work has shown
that a reordering model can be learned
from high quality manual word alignments
to improve machine translation perfor-
mance. In this paper, we focus on further
improving the performance of the reorder-
ing model (and thereby machine transla-
tion) by using a larger corpus of sentence
aligned data for which manual word align-
ments are not available but automatic ma-
chine generated alignments are available.
The main challenge we tackle is to gen-
erate quality data for training the reorder-
ing model in spite of the machine align-
ments being noisy. To mitigate the effect
of noisy machine alignments, we propose
a novel approach that improves reorder-
ings produced given noisy alignments and
also improves word alignments using in-
formation from the reordering model. This
approach generates alignments that are 2.6
f-Measure points better than a baseline su-
pervised aligner. The data generated al-
lows us to train a reordering model that
gives an improvement of 1.8 BLEU points
on the NIST MT-08 Urdu-English eval-
uation set over a reordering model that
only uses manual word alignments, and a
gain of 5.2 BLEU points over a standard
phrase-based baseline.
1 Introduction
Dealing with word order differences between
source and target languages presents a significant
challenge for machine translation systems. Failing
to produce target words in the correct order results
in machine translation output that is not fluent and
is often very hard to understand. These problems
are particularly severe when translating between
languages which have very different structure.
Phrase based systems (Koehn et al, 2003) use
lexicalized distortion models (Al-Onaizan and Pa-
pineni, 2006; Tillman, 2004) and scores from the
target language model to produce words in the cor-
rect order in the target language. These systems
typically are only able to capture short range re-
orderings and the amount of data required to po-
tentially capture longer range reordering phenom-
ena is prohibitively large.
There has been a large body of work showing
the efficacy of preordering source sentences using
a source parser and applying hand written or auto-
matically learned rules (Collins et al, 2005; Wang
et al, 2007; Ramanathan et al, 2009; Xia and Mc-
Cord, 2004; Genzel, 2010; Visweswariah et al,
2010). Recently, approaches that address the prob-
lem of word order differences between the source
and target language without requiring a high qual-
ity source or target parser have been proposed
(DeNero and Uszkoreit, 2011; Visweswariah et
al., 2011; Neubig et al, 2012). These methods
use a small corpus of manual word alignments
(where the words in the source sentence are man-
ually aligned to the words in the target sentence)
to learn a model to preorder the source sentence to
match target order.
In this paper, we build upon the approach in
(Visweswariah et al, 2011) which uses manual
word alignments for learning a reordering model.
Specifically, we show that we can significantly
improve reordering performance by using a large
number of sentence pairs for which manual word
alignments are not available. The motivation for
going beyond manual word alignments is clear:
the reordering model can have millions of features
and estimating weights for the features on thou-
sands of sentences of manual word alignments is
1275
likely to be inadequate. One approach to deal with
this problem would be to use only part-of-speech
tags as features for all but the most frequent words.
This will cut down on the number of features and
perhaps the model would be learnable with a small
set of manual word alignments. Unfortunately, as
we will see in the experimental section, leaving
out lexical information from the models hurts per-
formance even with a relatively small set of man-
ual word alignments. Another option would be to
collect more manual word alignments but this is
undesirable because it is time consuming and ex-
pensive.
The challenge in going beyond manual word
alignments and using machine alignments is the
noise in the machine alignments which affects the
performance of the reordering model (see Section
5). We illustrate this with the help of a motivating
example. Consider the example English sentence
and its translation shown in Figure 1.
He went to the stadium to play
vaha khelne keliye stadium ko gaya
Figure 1: An example English sentence with
its Urdu translation with alignment links. Red
(dotted) links are incorrect links while the blue
(dashed) links are the corresponding correct links.
A standard word alignment algorithm that we
used (McCarley et al, 2011) made the mistake of
mis-aligning the Urdu ko and keliye (it switched
the two). Deriving reference reorderings from
these wrong alignments would give us an incor-
rect reordering. A reordering model trained on
such incorrect reorderings would obviously per-
form poorly. Our task is thus two-fold (i) im-
prove the quality of machine alignments (ii) use
these less noisy alignments to derive cleaner train-
ing data for a reordering model.
Before proceeding, we first point out that the
two tasks, viz., reordering and word alignment
are related: Having perfect reordering makes the
alignment task easier while having perfect align-
ments in turn makes the task of finding reorder-
ings trivial. Motivated by this fact, we introduce
models that allow us to connect the source/target
reordering and the word alignments and show
that these models help in mutually improving the
performance of word alignments and reordering.
Specifically, we build two models: the first scores
reorderings given the source sentence and noisy
alignments, the second scores alignments given
the noisy source and target reorderings and the
source and target sentences themselves. The sec-
ond model helps produce better alignments, while
we use the first model to help generate better ref-
erence reordering given noisy alignments. These
improved reference reorderings will then be used
to train a reordering model.
Our experiments show that reordering models
trained using these improved machine alignments
perform significantly better than models trained
only on manual word alignments. This results in
a 1.8 BLEU point gain in machine translation per-
formance on an Urdu-English machine translation
task over a preordering model trained using only
manual word alignments. In all, this increases
the gain in performance by using the preordering
model to 5.2 BLEU points over a standard phrase-
based system with no preordering.
The rest of this paper is structured as follows.
Section 2 describes the main reordering issues in
Urdu-English translation. Section 3 introduces the
reordering modeling framework that forms the ba-
sis for our work. Section 4 describes the two mod-
els we use to tie together reordering and align-
ments and how we use these models to generate
training data for training our reordering model.
Section 5 presents the experimental setup used for
evaluating the models proposed in this paper on
an Urdu-English machine translation task. Sec-
tion 6 presents the results of our experiments.
We describe related work in Section 7 and finally
present some concluding remarks and potential fu-
ture work in Section 8.
2 Reordering issues in Urdu-English
translation
In this section we describe the main sources of
word order differences between Urdu and English
since this is the language pair we experiment with
in this paper.
The typical word order in Urdu is Subject-
Object-Verb unlike English in which the order is
Subject-Verb-Object. Urdu has case markers that
sometimes (but not always) mark the subject and
the object of a sentence. This difference in the
placement of verbs can often lead to movements of
verbs over long distances (depending on the num-
ber of words in the object). Phrase based systems
do not capture such long distance movements well.
1276
Another difference is that Urdu uses post-
positions unlike English which uses prepositions.
This can also lead to long range movements de-
pending on the length of the noun phrase that the
post-position follows. The order of noun phrases
and prepositional phrases is also swapped in Urdu
as compared with English.
3 Reordering model
In this section we briefly describe the reordering
model (Visweswariah et al, 2011) that forms the
basis of our work. We also describe an approx-
imation we make in the training process that sig-
nificantly speeds up the training without much loss
of accuracy which enables training on much larger
data sets. Consider a source sentence w that we
would like to reorder to match the target order. Let
pi represent a candidate permutation of the source
sentence w. pii denotes the index of the word in the
source sentence that maps to position i in the can-
didate reordering, thus reordering with this candi-
date permutation pi we will reorder the sentence
w to wpi1 , wpi2 , ..., wpin . The reordering model we
use assigns costs to candidate permutations as:
C(pi|w) =
?
i
c(pii?1, pii).
The costs c(m,n) are pairwise costs of putting
wm immediately before wn in the reordering. We
reorder the sentence w according to the permu-
tation pi that minimizes the cost C(pi|w). We
find the minimal cost permutation by converting
the problem into a symmetric Travelling Salesman
Problem (TSP) and then using an implementation
of the chained Lin-Kernighan heuristic (Applegate
et al, 2003). The costs in the reordering model
c(m,n) are parameterized by a linear model:
c(m,n) = ?T?(w,m, n)
where ? is a learned vector of weights and ? is a
vector of binary feature functions that inspect the
words and POS tags of the source sentence at and
around positions m and n. We use the features
(?) described in Visweswariah et al (2011) that
were based on features used in dependency pars-
ing (McDonald et al, 2005a).
To learn the weight vector ? we require a cor-
pus of sentences w with their desired reorderings
pi?. Past work Visweswariah et al (2011) used
high quality manual word alignments to derive the
desired reorderings pi? as follows. Given word
aligned source and target sentences, we drop the
source words that are not aligned1. Let mi be the
mean of the target word positions that the source
word at index i is aligned to. We then sort the
source indices in increasing order ofmi (this order
defines pi?). If mi = mj (for example, because wi
and wj are aligned to the same set of words) we
keep them in the same order that they occurred in
the source sentence.
We used the single best Margin Infused Relaxed
Algorithm (MIRA) (McDonald et al (2005b),
Crammer and Singer (2003)) with online updates
to our parameters given by:
?i+1 = argmin
?
||? ? ?i||
s.t. C(pi?|w) < C(p?i|w) ? L(pi?, p?i).
In the equation above, p?i = argminpi C(pi|w) is
the best reordering based on the current parameter
value ?i and L is a loss function. We take L to be
the number of words for which the hypothesized
permutation p?i has a different preceding word as
compared with the reference permutation pi?.
In this paper we focus on the case where in ad-
dition to using a relatively small number of man-
ual word aligned sentences to derive the refer-
ence permutations pi? used to train our model,
we would like to use more abundant but nois-
ier machine aligned sentence pairs. To handle
the larger amount of training data we obtain from
machine alignments, we make an approximation
in training that we found empirically to not af-
fect performance but that makes training faster
by more than a factor of five. This allows us
to train the reordering model with roughly 150K
sentences in about two hours. The approximation
we make is that instead of using the chained Lin-
Kernighan heuristic to solve the TSP problem to
find p?i = argminpi C(pi|w), we select greedily
for each word the preceding word that has the low-
est cost2. Using ?i to denote argminj c(j, i) and
letting
C(?|w) =
?
i
c(?i, i),
1Note that the unaligned source words are dropped only at
the time of training. At the time of testing all source words are
retained as the alignment information is obviously not avail-
able at test time.
2It should be noted that this approximation was done only
at the time of training. At the time of testing we still use the
chained Lin-Kernighan heuristic to solve the TSP problem.
1277
we do the update according to:
?i+1 = argmin
?
||? ? ?i||
s.t. C(pi?|w) < C(?|w) ? L(pi?,?).
Again the loss L(pi?,?) is the number of positions
i for which pi?i?1 is different from ?i?1.
4 Generating reference reordering from
parallel sentences
The main aim of our work is to improve the re-
ordering model by using parallel sentences for
which manual word alignments are not avail-
able. In other words, we want to generate rel-
atively clean reference reorderings from parallel
sentences and use them for training a reordering
model. A straightforward approach for this is to
use a supervised aligner to align the words in the
sentences and then derive the reference reordering
as we do for manual word alignments. However,
as we will see in the experimental results, the qual-
ity of a reordering model trained from automatic
alignments is very sensitive to the quality of align-
ments. This motivated us to explore if we can fur-
ther improve our aligner and the method for gen-
erating reference reorderings given alignments.
We improve upon the above mentioned ba-
sic approach by coupling the tasks of reorder-
ing and word alignment. We do this by build-
ing a reordering model (C(pis|ws,wt,a)) that
scores reorderings pis given the source sentence
ws, target sentence wt and machine alignments
a. Complementing this model, we build an align-
ment model (P (a|ws,wt,pis,pit)) that scores
alignments a given the source and target sen-
tences and their predicted reorderings according to
source and target reordering models. The model
(C(pis|ws,wt,a)) helps to produce better refer-
ence reorderings for training our final reordering
model given fixed machine alignments and the
alignment model (P (a|ws,wt,pis,pit)) helps im-
prove the machine alignments taking into account
information from reordering models. In the fol-
lowing sections, we describe our overall approach
followed by a description of the two models.
4.1 Overall approach to generating training
data
We first describe our overall approach to gen-
erating training data for the reordering model
given a small corpus of sentences with manual
C(pis|ws) C(pit|wt)
Step 1: Train reordering models
using manual word alignments
P (a|ws,wt, pis, pit)
C(pis|ws,a) C(pit|wt,a)
Step 2: Feed predictions
of the reordering models
to the alignment model
Step 3: Feed predictions
of the alignment model
to the reordering models
Figure 2: Overall approach: Building a sequence
of reordering and alignment models.
word alignments (H) and a much larger corpus of
parallel sentences (U ) that are not word aligned.
The basic idea is to chain together the two models,
viz., reordering model and alignment model, as
illustrated in Figure 2. The steps involved are as
described below:
Step 1: First, we use manual word alignments
(H) to train source and target reordering models
as described in (Visweswariah et al, 2011).
Step 2: Next, we use the hand alignments to train
an alignment model P (a|ws,wt,pis,pit). In
addition to the original source and target sentence,
we also feed the predictions of the reordering
model trained in Step 1 to this alignment model
(see section 4.2 for details of the model itself).
Step 3: Finally, we use the predictions of the
alignment model trained in Step 2 to train reorder-
ing models C(pis|ws,wt,a) (see section 4.3 for
details on the reordering model itself).
After building the sequence of models shown in
Figure 2, we apply them in sequence on the un-
aligned parallel data U , starting with the reorder-
ing models C(pis|ws) and C(pit|wt). The re-
orderings obtained for the source side in U (after
applying the final model C(pis|ws,a)) are used
along with reference reorderings obtained from
the manual word alignments to train our reorder-
ing model. Note that, in theory, we could iterate
over steps 2 and 3 several times but, in practice
we did not see a benefit of going beyond one iter-
1278
ation in our experiments. Also, since we are inter-
ested only in the source side reorderings produced
by the model C(pis|ws,a), the target reordering
model C(pit|wt,a) is needed only if we iterate
over steps 2 and 3.
We now point to some practical considerations
of our approach. Consider the case when we are
training an alignment model conditioned on re-
orderings (P (a|ws,wt,pis,pit)). If the reorder-
ing model that generated these reorderings pis,pit
were trained on the same data that we are using
to train the alignment model, then the reorder-
ings would be much better than we would ex-
pect on unseen test data, and hence the align-
ment model (P (a|ws,wt,pis,pit)) may learn to
make the alignment overly consistent with the re-
orderings pis and pit. To counter this problem,
we divide the training data H into K parts and
at each stage we apply a model (reordering or
alignment) on part i that had not seen part i in
training. This ensures that the alignment model
does not see very optimistic reorderings and vice
versa. We now describe the individual models,
viz., P (a|ws,wt,pis,pit) and C(pis|ws,a).
4.2 Modeling alignments given reordering
In this section we describe how we fuse informa-
tion from source and target reordering models to
improve word alignments.
As a base model we use the correction model
for word alignments proposed by McCarley et
al. (2011). This model was significantly better
than the MaxEnt aligner (Ittycheriah and Roukos,
2005) and is also flexible in the sense that it allows
for arbitrary features to be introduced while still
keeping training and decoding tractable by using a
greedy decoding algorithm that explores potential
alignments in a small neighborhood of the current
alignment. The model thus needs a reasonably
good initial alignment to start with for which we
use the MaxEnt aligner (Ittycheriah and Roukos,
2005) as in McCarley et al (2011).
The correction model is a log-linear model:
P (a|ws,wt) = exp(?
T?(a,ws,wt))
Z(ws,wt) .
The ?s are trained using the LBFGS algorithm
(Liu et al, 1989) to maximize the log-likelihood
smoothed with L2 regularization. The feature
functions ? we start with are those used in Mc-
Carley et al (2011) and include features encoding
the Model 1 probabilities between pairs of words
linked in the alignment a, features that inspect
source and target POS tags and parses (if avail-
able) and features that inspect the alignments of
adjacent words in the source and target sentence.
To incorporate information from the reorder-
ing model, we add features that use the predicted
source pis and target permutations pit. We intro-
duce some notation to describe these features. Let
Sm and Sn be the set of indices of target words
thatwsm andwsn are aligned to respectively. We de-
fine the minimum signed distance (msd) between
these two sets as:
msd(Sm, Sn) = i? ? j?
where, (i?, j?) = arg min
(i,j)?Sm?Sn
|i? j|
We quantize and encode with binary features
the minimum signed distance between the sets of
the indices of the target words that source words
adjacent in the reordering pis (wspisi and wspisi+1) are
aligned to. We instantiate similar features with the
roles of source and target sentences reversed. With
this addition of features we use the same training
and testing procedure as in McCarley et al (2011).
If the reorderings pis were perfect we would learn
to only allow alignments where wspisi and w
s
pisi+1
were aligned to adjacent words in the target sen-
tence. Although the reordering model is not per-
fect, preferring alignments consistent with the re-
ordering models improves the aligner.
4.3 Modeling reordering given alignments
To model source permutations given source (ws)
and target (wt) sentences, and alignments (a) we
reuse the reordering model framework described
in Section 3 adding additional features capturing
the relation between a hypothesized permutation
pi and alignments a. To allow for searching via
the same TSP formulation we once again assign
costs to candidate permutations as:
C(pis|ws,wt,a) =
?
i
c(pii?1, pii|ws,a).
Note that we introduce a dependence on the target
sentence wt only through the alignment a. Once
again we parameterize the costs by a linear model:
c(m,n) = ?T?(ws,a,m, n).
For the feature functions ?, in addition to the
features that only depend on ws,m, n (that we
1279
use in our standard reordering model) we add
binary indicator features based on msd(Sm, Sn)
and msd(Sm, Sn) conjoined with POS(wsm) and
POS(wsn).
Here, Sm and Sn are the set of indices of tar-
get words that wsm and wsn are aligned to respec-
tively. We conjoin the msd (minimum signed dis-
tance) with the POS tags to allow the model to cap-
ture the fact that the alignment error rate maybe
higher for some POS tags than others (e.g., we
have observed verbs have a higher error rate in
Urdu-English alignments).
Given these features we train the parameters ?
using the MIRA algorithm as described in Sec-
tion 3. Using this model, we can find the low-
est cost permutation C(pis|ws,a) using the Lin-
Kernighan heuristic as described in Section 3.
This model allows us to combine features from
the original reordering model along with informa-
tion coming from the alignments to find source re-
orderings given a parallel corpus and alignments.
We will see in the experimental section that this
improves upon the simple heuristic for deriving re-
orderings described in Section 3.
5 Experimental setup
In this section we describe the experimental setup
that we used to evaluate the models proposed in
this paper. All experiments were done on Urdu-
English and we evaluate reordering in two ways:
Firstly, we evaluate reordering performance di-
rectly by comparing the reordered source sentence
in Urdu with a reference reordering obtained from
the manual word alignments using BLEU (Pap-
ineni et al, 2002) (we call this measure monolin-
gual BLEU or mBLEU). All mBLEU results are
reported on a small test set of about 400 sentences
set aside from our set of sentences with manual
word alignments. Additionally, we evaluate the ef-
fect of reordering on our final systems for machine
translation measured using BLEU.
We use about 10K sentences (180K words) of
manual word alignments which were created in
house using part of the NIST MT-08 training data3
to train our baseline reordering model and to train
our supervised machine aligners. We use a parallel
corpus of 3.9M words consisting of 1.7M words
from the NIST MT-08 training data set and 2.2M
words extracted from parallel news stories on the
3http://www.ldc.upenn.edu
web4. The parallel corpus is used for building our
phrased based machine translation system and to
add training data for our reordering model. For
our English language model, we use the Gigaword
English corpus in addition to the English side of
our parallel corpus. Our Part-of-Speech tagger is
a Maximum Entropy Markov model tagger trained
on roughly fifty thousand words from the CRULP
corpus (Hussain, 2008).
For our machine translation experiments, we
used a standard phrase based system (Al-Onaizan
and Papineni, 2006) with a lexicalized distortion
model with a window size of +/-4 words5. To
extract phrases we use HMM alignments along
with higher quality alignments from a supervised
aligner (McCarley et al, 2011). We report results
on the (four reference) NIST MT-08 evaluation set
in Table 4 for the News and Web conditions. The
News and Web conditions each contain roughly
20K words in the test set, with the Web condition
containing more informal text from the web.
6 Results and Discussions
We now discuss the results of our experiments.
Need for additional data: We first show the need
for additional data in Urdu-English reordering.
Column 2 of Table 1 shows mBLEU as a function
of the number of sentences with manual word
alignments that are used to train the reordering
model. We see a roughly 3 mBLEU points drop
in performance per halving of data indicating a
potential for improvement by adding more data.
Using fewer features: We compare the perfor-
mance of a model trained using lexical features
for all words (Column 2 of Table 1) with a model
trained using lexical features only for the 1000
most frequent words (Column 3 of Table 1). The
motivation for this is to explore if a good model
can be learned even from a small amount of data if
we restrict the number of features in a reasonable
manner. However, we see that even with only
2.4K sentences with manual word alignments our
model benefits from lexical identities of more
than the 1000 most frequent words.
Effect of quality of machine alignments: We
next look at the use of automatically generated
4http://centralasiaonline.com
5Note that the same window size of +/-4 words was used
for all the systems, i.e., the baseline system as well as the
systems using different preordering techniques.
1280
Data size All features Frequent lex only
10K 52.5 50.8
5K 49.6 49.0
2.5K 46.6 46.2
Table 1: mBLEU scores for Urdu to English re-
ordering using different number of sentences of
manually word aligned training data with all fea-
tures and with lexical features instantiated only for
the 1000 most frequent words.
machine alignments to train the reordering model
and see the effect of aligner quality on the re-
ordering model generated using this data. These
experiments also form the baseline for the mod-
els we propose in this paper to clean up align-
ments. We experimented with two different super-
vised aligners : a maximum entropy aligner (Itty-
cheriah and Roukos, 2005) and an improved cor-
rection model that corrects the maximum entropy
alignments (McCarley et al, 2011).
Aligner Train size mBLEU
Type f-Measure (words)
None - 35.5
Manual 180K 52.5
MaxEnt 70.0 3.9M 49.5
Correction model 78.1 3.9M 55.1
Table 2: mBLEU scores for Urdu to English re-
ordering using models trained on different data
sources and tested on a development set of 8017
Urdu tokens.
Table 2 shows mBLEU scores when the re-
ordering model is trained on reordering references
created from aligners with different quality. We
see that the quality of the alignments matter a
great deal to the reordering model; using MaxEnt
alignments cause a degradation in performance
over just using a small set of manual word align-
ments. The alignments obtained using the aligner
of McCarley et al (2011) are of much better
quality and hence give higher reordering perfor-
mance. Note that this reordering performance
is much better than that obtained using manual
word alignments because the size of machine
alignments is much larger (3.9M v/s 180K words).
Improvements in reordering performance us-
ing the proposed models: Table 3 shows im-
provements in the reordering model when using
the models proposed in this paper. We useH to re-
fer to the manually word aligned data and U to re-
fer to the additional sentence pairs for which man-
ual word alignments are not available. We report
the following numbers :
1. Base correction model: This is the baseline
where we use the correction model of McCar-
ley et al (2011) for generating word alignments.
The f-Measure of this aligner is 78.1% (see row
1, column 2). Corresponding to this, we also re-
port the baseline for our reordering experiments
in the third column. Here, we first generate word
alignments for U using the aligner of McCarley et
al. (2011) and then extract reference reorderings
from these alignments. We then combine these
reference reorderings with the reference reorder-
ings derived fromH and use this combined data to
train a reordering model which serves as the base-
line (mBLEU = 55.1).
2. Correction model, C(pi|a): Here, once again
we generate alignments for U using the correc-
tion model of McCarley et al (2011). However,
instead of using the basic approach of extracting
reference reorderings, we use our improved model
C(pi|a) to generate reference reorderings from U .
These reference reorderings are again combined
with the reference reorderings derived fromH and
used to train a reordering model (mBLEU = 56.4).
3. P (a|pi), C(pi|a): Here, we build the entire se-
quence of models shown in Figure 2. The align-
ment model P (a|pi) is first improved by using pre-
dictions from the reordering model. These im-
proved alignments are then used to extract better
reference reorderings from U using C(pi|a).
We see substantial improvements over simply
adding in the data from the machine alignments.
Improvements come roughly in equal parts from
the two techniques we proposed in this paper : (i)
using a model to generate reference reorderings
from noisy alignments and (ii) using reordering in-
formation to improve the aligner.
Method f-Measure mBLEU
Base Correction model 78.1 55.1
Correction model, C(pi|a) 78.1 56.4
P (a|pi), C(pi|a) 80.7 57.6
Table 3: mBLEU with different methods to gener-
ate reordering model training data from a machine
aligned parallel corpus in addition to manual word
alignments.
Improvements in MT performance using the
proposed models: We report results for a phrase
based system with different preordering tech-
niques. For results including a reordering model,
we simply reorder the source side Urdu data both
while training and at test time. In addition to
1281
phrase based systems with different preordering
methods, we also report on a hierarchical phrase
based system for which we used Joshua 4.0 (Gan-
itkevitch et al, 2012). We see a significant gain of
1.8 BLEU points in machine translation by going
beyond manual word alignments using the best re-
ordering model reported in Table 3. We also note a
gain of 2.0 BLEU points over a hierarchical phrase
based system.
System type MT-08 evalWeb News All
Baseline (no preordering) 18.4 25.6 22.2
Hierarchical phrase based 19.6 30.7 25.4
Reordering: Manual alignments 20.7 30.0 25.6
+ Machine alignments simple 21.3 30.9 26.4
+ machine alignments, model based 22.1 32.2 27.4
Table 4: MT performance without preordering
(phrase based and hierarchical phrase based),
and with reordering models using different data
sources (phrase based).
7 Related work
Dealing with the problem of handling word order
differences in machine translation has recently re-
ceived much attention. The approaches proposed
for solving this problem can be broadly divided
into 3 sets as discussed below.
The first set of approaches handle the reorder-
ing problem as part of the decoding process. Hier-
archical models (Chiang, 2007) and syntax based
models (Yamada and Knight, 2002; Galley et
al., 2006; Liu et al, 2006; Zollmann and Venu-
gopal, 2006) improve upon the simpler phrase
based models but with significant additional com-
putational cost (compared with phrase based sys-
tems) due to the inclusion of chart based parsing in
the decoding process. Syntax based models also
require a high quality source or target language
parser.
The second set of approaches rely on a source
language parser and treat reordering as a separate
process that is applied on the source language sen-
tence at training and test time before using a stan-
dard approach to machine translation. Preordering
the source data with hand written or automatically
learned rules is effective and efficient (Collins
et al, 2005; Wang et al, 2007; Ramanathan et
al., 2009; Xia and McCord, 2004; Genzel, 2010;
Visweswariah et al, 2010) but requires a source
language parser.
Recent approaches that avoid the need for a
source or target language parser and retain the ef-
ficiency of preordering models were proposed in
(Tromble and Eisner, 2009; DeNero and Uszko-
reit, 2011; Visweswariah et al, 2011; Neubig
et al, 2012). (DeNero and Uszkoreit, 2011;
Visweswariah et al, 2011; Neubig et al, 2012) fo-
cus on the use of manual word alignments to learn
preordering models and in both cases no benefit
was obtained by using the parallel corpus in ad-
dition to manual word alignments. Our work is
an extension of Visweswariah et al (2011) and
we focus on being able to incorporate relatively
noisy machine alignments to improve the reorder-
ing model.
In addition to being related to work in reorder-
ing, our work is also more broadly related to sev-
eral other efforts which we now outline. Seti-
awan et al (2010) proposed the use of function
word reordering to improve alignments. While
this work is similar to one of our models (model
of alignments given reordering) we differ in us-
ing a reordering model of all words (not just func-
tion words) and both source and target sentences
(not just the source sentence). The task of directly
learning a reordering model for language pairs that
are very different is closely related to the task of
parsing and hence work on semi-supervised pars-
ing (Koo et al, 2008; McClosky et al, 2006;
Suzuki et al, 2009) is broadly related to our work.
Our work coupling reordering and alignments is
also similar in spirit to approaches where parsing
and alignment are coupled (Wu, 1997).
8 Conclusion
In the paper we showed that a reordering model
can benefit from data beyond a relatively small
corpus of manual word alignments. We proposed
a model that scores reorderings given alignments
and the source sentence that we use to gener-
ate cleaner training data from noisy alignments.
We also proposed a model that scores alignments
given source and target sentence reorderings that
improves a supervised alignment model by 2.6
points in f-Measure. While the improvement in
alignment performance is modest, the improve-
ment does result in improved reordering models.
Cumulatively, we see a gain of 1.8 BLEU points
over a baseline reordering model that only uses
manual word alignments, a gain of 2.0 BLEU
points over a hierarchical phrase based system,
and a gain of 5.2 BLEU points over a phrase based
1282
system that uses no source preordering on a pub-
licly available Urdu-English test set.
As future work we would like to evaluate our
models on other language pairs. Another avenue
of future work we would like to explore is the use
of monolingual source and target data to further
assist the reordering model. We hope to be able to
learn lexical information such as how many argu-
ments a verb takes, what nouns are potential sub-
jects for a given verb by gathering statistics from
an English parser and projecting to the source lan-
guage via our word/phrase translation table.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of ACL, ACL-44, pages 529?536, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
David Applegate, William Cook, and Andre Rohe.
2003. Chained lin-kernighan for large traveling
salesman problems. In INFORMS Journal On Com-
puting.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201?228, June.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991, March.
John DeNero and Jakob Uszkoreit. 2011. Inducing
sentence structure from parallel corpora for reorder-
ing. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 193?203, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, ACL-44, pages 961?968, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt
Post, and Chris Callison-Burch. 2012. Joshua 4.0:
Packing, pro, and paraphrases. In Proceedings of
the Seventh Workshop on Statistical Machine Trans-
lation, pages 283?291, Montre?al, Canada, June. As-
sociation for Computational Linguistics.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics.
Sarmad Hussain. 2008. Resources for Urdu language
processing. In Proceedings of the 6th Workshop on
Asian Language Resources, IJCNLP?08.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for Arabic-English ma-
chine translation. In Proceedings of HLT/EMNLP,
HLT ?05, pages 89?96, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In ACL, pages 595?603.
Dong C. Liu, Jorge Nocedal, and Dong C. 1989. On
the limited memory bfgs method for large scale op-
timization. Mathematical Programming, 45:503?
528.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Com-
putational Linguistics, ACL-44, pages 609?616,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
J. Scott McCarley, Abraham Ittycheriah, Salim
Roukos, Bing Xiang, and Jian-ming Xu. 2011. A
correction model for word alignments. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?11, pages 889?
898, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
HLT-NAACL.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?05, pages 91?98, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT.
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a discriminative parser to optimize
machine translation reordering. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
1283
Natural Language Learning, pages 843?853, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Ananthakrishnan Ramanathan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and morphology: addressing the crux
of the fluency problem in English-Hindi smt. In Pro-
ceedings of ACL-IJCNLP.
Hendra Setiawan, Chris Dyer, and Philip Resnik. 2010.
Discriminative word alignment with a function word
reordering model. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 534?544, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An empirical study of semi-
supervised structured conditional models for depen-
dency parsing. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 2 - Volume 2, EMNLP ?09,
pages 551?560, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Christoph Tillman. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL.
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proceed-
ings of EMNLP.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax based reordering with automat-
ically derived rules for improved statistical machine
translation. In Proceedings of the 23rd International
Conference on Computational Linguistics.
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A word reordering model for im-
proved machine translation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?11, pages 486?496,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proceedings of EMNLP-
CoNLL.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Comput. Linguist., 23(3):377?403, September.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In COLING.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical MT. In Proceedings of ACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings on the Workshop on Statistical Ma-
chine Translation.
1284
