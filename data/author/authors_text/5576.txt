c? 2002 Association for Computational Linguistics
Summarizing Scientific Articles:
Experiments with Relevance and
Rhetorical Status
Simone Teufel? Marc Moens?
Cambridge University Rhetorical Systems and University of
Edinburgh
In this article we propose a strategy for the summarization of scientific articles that concentrates
on the rhetorical status of statements in an article: Material for summaries is selected in such a
way that summaries can highlight the new contribution of the source article and situate it with
respect to earlier work.
We provide a gold standard for summaries of this kind consisting of a substantial corpus of
conference articles in computational linguistics annotated with human judgments of the rhetorical
status and relevance of each sentence in the articles. We present several experiments measuring
our judges? agreement on these annotations.
We also present an algorithm that, on the basis of the annotated training material, selects
content from unseen articles and classifies it into a fixed set of seven rhetorical categories. The
output of this extraction and classification system can be viewed as a single-document summary
in its own right; alternatively, it provides starting material for the generation of task-oriented
and user-tailored summaries designed to give users an overview of a scientific field.
1. Introduction
Summarization systems are often two-phased, consisting of a content selection step
followed by a regeneration step. In the first step, text fragments (sentences or clauses)
are assigned a score that reflects how important or contentful they are. The highest-
ranking material can then be extracted and displayed verbatim as ?extracts? (Luhn
1958; Edmundson 1969; Paice 1990; Kupiec, Pedersen, and Chen 1995). Extracts are
often useful in an information retrieval environment since they give users an idea as
to what the source document is about (Tombros and Sanderson 1998; Mani et al 1999),
but they are texts of relatively low quality. Because of this, it is generally accepted that
some kind of postprocessing should be performed to improve the final result, by
shortening, fusing, or otherwise revising the material (Grefenstette 1998; Mani, Gates,
and Bloedorn 1999; Jing and McKeown 2000; Barzilay et al 2000; Knight and Marcu
2000).
The extent to which it is possible to do postprocessing is limited, however, by
the fact that contentful material is extracted without information about the general
discourse context in which the material occurred in the source text. For instance, a
sentence describing the solution to a scientific problem might give the main contri-
? Simone Teufel, Computer Laboratory, Cambridge University, JJ Thomson Avenue, Cambridge,
CB3 OFD, England. E-mail: Simone.Teufel@cl.cam.ac.uk
? Marc Moens, Rhetorical Systems and University of Edinburgh, 2 Buccleuch Place, Edinburgh, EH8 9LS,
Scotland. E-mail: marc@cogsci.ed.ac.uk
410
Computational Linguistics Volume 28, Number 4
bution of the paper, but it might also refer to a previous approach that the authors
criticize. Depending on its rhetorical context, the same sentence should be treated very
differently in a summary. We propose in this article a method for sentence and con-
tent selection from source texts that adds context in the form of information about the
rhetorical role the extracted material plays in the source text. This added contextual
information can then be used to make the end product more informative and more
valuable than sentence extracts.
Our application domain is the summarization of scientific articles. Summariza-
tion of such texts requires a different approach from, for example, that used in the
summarization of news articles. For example, Barzilay, McKeown, and Elhadad (1999)
introduce the concept of information fusion, which is based on the identification of re-
current descriptions of the same events in news articles. This approach works well
because in the news domain, newsworthy events are frequently repeated over a short
period of time. In scientific writing, however, similar ?events? are rare: The main focus
is on new scientific ideas, whose main characteristic is their uniqueness and difference
from previous ideas.
Other approaches to the summarization of news articles make use of the typical
journalistic writing style, for example, the fact that the most newsworthy information
comes first; as a result, the first few sentences of a news article are good candidates
for a summary (Brandow, Mitze, and Rau 1995; Lin and Hovy 1997). The structure
of scientific articles does not reflect relevance this explicitly. Instead, the introduction
often starts with general statements about the importance of the topic and its history
in the field; the actual contribution of the paper itself is often given much later.
The length of scientific articles presents another problem. Let us assume that our
overall summarization strategy is first to select relevant sentences or concepts, and
then to synthesize summaries using this material. For a typical 10- to 20-sentence
news wire story, a compression to 20% or 30% of the source provides a reasonable
input set for the second step. The extracted sentences are still thematically connected,
and concepts in the sentences are not taken completely out of context. In scientific ar-
ticles, however, the compression rates have to be much higher: Shortening a 20-page
journal article to a half-page summary requires a compression to 2.5% of the original.
Here, the problematic fact that sentence selection is context insensitive does make a
qualitative difference. If only one sentence per two pages is selected, all information
about how the extracted sentences and their concepts relate to each other is lost; with-
out additional information, it is difficult to use the selected sentences as input to the
second stage.
We present an approach to summarizing scientific articles that is based on the idea
of restoring the discourse context of extracted material by adding the rhetorical status
to each sentence in a document. The innovation of our approach is that it defines
principles for content selection specifically for scientific articles and that it combines
sentence extraction with robust discourse analysis. The output of our system is a list
of extracted sentences along with their rhetorical status (e.g. sentence 11 describes the
scientific goal of the paper, and sentence 9 criticizes previous work), as illustrated in
Figure 1. (The example paper we use throughout the article is F. Pereira, N. Tishby, and
L. Lee?s ?Distributional Clustering of English Words? [ACL-1993, cmp lg/9408011]; it
was chosen because it is the paper most often cited within our collection.) Such lists
serve two purposes: in themselves, they already provide a better characterization of
scientific articles than sentence extracts do, and in the longer run, they will serve as
better input material for further processing.
An extrinsic evaluation (Teufel 2001) shows that the output of our system is al-
ready a useful document surrogate in its own right. But postprocessing could turn
411
Teufel and Moens Summarizing Scientific Articles
AIM 10 Our research addresses some of the same questions and uses similar raw data,
but we investigate how to factor word association tendencies into associations
of words to certain hidden senses classes and associations between the classes
themselves.
11 While it may be worthwhile to base such a model on preexisting sense classes
(Resnik, 1992), in the work described here we look at how to derive the classes
directly from distributional data.
162 We have demonstrated that a general divisive clustering procedure for probability
distributions can be used to group words according to their participation in
particular grammatical relations with other words.
BASIS 19 The corpus used in our first experiment was derived from newswire text auto-
matically parsed by Hindle?s parser Fidditch (Hindle, 1993).
113 The analogy with statistical mechanics suggests a deterministic annealing pro-
cedure for clustering (Rose et al, 1990), in which the number of clusters is
determined through a sequence of phase transitions by continuously increasing
the parameter EQN following an annealing schedule.
CONTRAST 9 His notion of similarity seems to agree with our intuitions in many cases, but it is
not clear how it can be used directly to construct word classes and corresponding
models of association.
14 Class construction is then combinatorially very demanding and depends on
frequency counts for joint events involving particular words, a potentially un-
reliable source of information as we noted above.
Figure 1
Extract of system output for example paper.
0 This paper?s topic is to automatically classify words according to their contexts of use.
4 The problem is that for large enough corpora the number of possible joint events is much
larger than the number of event occurrences in the corpus, so many events are seen rarely
or never, making their frequency counts unreliable estimates of their probabilities. 162
This paper?s specific goal is to group words according to their participation in particular
grammatical relations with other words, 22 more specifically to classify nouns according
to their distribution as direct objects of verbs.
Figure 2
Nonexpert summary, general purpose.
the rhetorical extracts into something even more valuable: The added rhetorical con-
text allows for the creation of a new kind of summary. Consider, for instance, the
user-oriented and task-tailored summaries shown in Figures 2 and 3. Their composi-
tion was guided by fixed building plans for different tasks and different user models,
whereby the building blocks are defined as sentences of a specific rhetorical status.
In our example, most textual material is extracted verbatim (additional material is
underlined in Figures 2 and 3; the original sentences are given in Figure 5). The first
example is a short abstract generated for a nonexpert user and for general information;
its first two sentences give background information about the problem tackled. The
second abstract is aimed at an expert; therefore, no background is given, and instead
differences between this approach and similar ones are described.
The actual construction of these summaries is a complex process involving tasks
such as sentence planning, lexical choice and syntactic realization, tasks that are outside
the scope of this article. The important point is that it is the knowledge about the
rhetorical status of the sentences that enables the tailoring of the summaries according
to users? expertise and task. The rhetorical status allows for other kinds of applications
too: Several articles can be summarized together, contrasts or complementarity among
412
Computational Linguistics Volume 28, Number 4
44 This paper?s goal is to organise a set of linguistic objects such as words according to
the contexts in which they occur, for instance grammatical constructions or n-grams.
22 More specifically: the goal is to classify nouns according to their distribution as direct
objects of verbs. 5 Unlike Hindle (1990), 9 this approach constructs word classes and
corresponding models of association directly. 14 In comparison to Brown et al (1992),
the method is combinatorially less demanding and does not depend on frequency counts
for joint events involving particular words, a potentially unreliable source of information.
Figure 3
Expert summary, contrastive links.
articles can be expressed, and summaries can be displayed together with citation links
to help users navigate several related papers.
The rest of this article is structured as follows: section 2 describes the theoretical
and empirical aspects of document structure we model in this article. These aspects
include rhetorical status and relatedness:
? Rhetorical status in terms of problem solving: What is the goal and
contribution of the paper? This type of information is often marked by
metadiscourse and by conventional patterns of presentation (cf.
section 2.1).
? Rhetorical status in terms of intellectual attribution: What information is
claimed to be new, and which statements describe other work? This type
of information can be recognized by following the ?agent structure? of
text, that is, by looking at all grammatical subjects occurring in sequence
(cf. section 2.2).
? Relatedness among articles: What articles is this work similar to, and in
what respect? This type of information can be found by examining fixed
indicator phrases like in contrast to . . . , section headers, and citations (cf.
section 2.3).
These aspects of rhetorical status are encoded in an annotation scheme that we present
in section 2.4. Annotation of relevance is covered in section 2.5.
In section 3, we report on the construction of a gold standard for rhetorical status
and relevance and on the measurement of agreement among human annotators. We
then describe in section 4 our system that simulates the human annotation. Section 5
presents an overview of the intrinsic evaluation we performed, and section 6 closes
with a summary of the contribution of this work, its limitations, and suggestions for
future work.
2. Rhetorical Status, Citations, and Relevance
It is important for our task to find the right definition of rhetorical status to describe the
content in scientific articles. The definition should both capture generalizations about
the nature of scientific texts and also provide the right kind of information to enable the
construction of better summaries for a practical application. Another requirement is
that the analysis should be applicable to research articles from different presentational
traditions and subject matters.
413
Teufel and Moens Summarizing Scientific Articles
For the development of our scheme, we used the chronologically first 80 articles
in our corpus of conference articles in computational linguistics (articles presented
at COLING, ANLP, and (E)ACL conferences or workshops). Because of the inter-
disciplinarity of the field, the papers in this collection cover a challenging range of
subject matters, such as logic programming, statistical language modeling, theoreti-
cal semantics, computational dialectology, and computational psycholinguistics. Also,
the research methodology and tradition of presentation is very different among these
fields; (computer scientists write very different papers than theoretical linguists). We
thus expect our analysis to be equally applicable in a wider range of disciplines and
subdisciplines other than those named.
2.1 Rhetorical Status
Our model relies on the following dimensions of document structure in scientific
articles.
Problem structure. Research is often described as a problem-solving activity (Jordan
1984; Trawinski 1989; Zappen 1983). Three information types can be expected to occur
in any research article: problems (research goals), solutions (methods), and results. In
many disciplines, particularly the experimental sciences, this problem-solution struc-
ture has been crystallized in a fixed presentation of the scientific material as introduc-
tion, method, result and discussion (van Dijk 1980). But many texts in computational
linguistics do not adhere to this presentation, and our analysis therefore has to be
based on the underlying logical (rhetorical) organization, using textual representation
only as an indication.
Intellectual attribution. Scientific texts should make clear what the new contribution
is, as opposed to previous work (specific other researchers? approaches) and back-
ground material (generally accepted statements). We noticed that intellectual attribu-
tion has a segmental character. Statements in a segment without any explicit attribution
are often interpreted as belonging to the most recent explicit attribution statement
(e.g., Other researchers claim that). Our rhetorical scheme assumes that readers have
no difficulty in understanding intellectual attribution, an assumption that we verified
experimentally.
Scientific argumentation. In contrast to the view of science as a disinterested ?fact
factory,? researchers like Swales (1990) have long claimed that there is a strong social
aspect to science, because the success of a researcher is correlated with her ability to
convince the field of the quality of her work and the validity of her arguments. Au-
thors construct an argument that Myers (1992) calls the ?rhetorical act of the paper?:
The statement that their work is a valid contribution to science. Swales breaks down
this ?rhetorical act? into single, nonhierarchical argumentative moves (i.e., rhetorically
coherent pieces of text, which perform the same communicative function). His Con-
structing a Research Space (CARS) model shows how patterns of these moves can be
used to describe the rhetorical structure of introduction sections of physics articles.
Importantly, Swales?s moves describe the rhetorical status of a text segment with re-
spect to the overall message of the document, and not with respect to adjacent text
segments.
Attitude toward other people?s work. We are interested in how authors include refer-
ence to other work into their argument. In the flow of the argument, each piece of
other work is mentioned for a specific reason: it is portrayed as a rival approach, as
a prior approach with a fault, or as an approach contributing parts of the authors?
own solution. In well-written papers, this relation is often expressed in an explicit
way. The next section looks at the stylistic means available to the author to express
the connection between previous approaches and their own work.
414
Computational Linguistics Volume 28, Number 4
2.2 Metadiscourse and Agentivity
Explicit metadiscourse is an integral aspect of scientific argumentation and a way of
expressing attitude toward previous work. Examples for metadiscourse are phrases
like we argue that and in contrast to common belief, we. Metadiscourse is ubiquitous in
scientific writing: Hyland (1998) found a metadiscourse phrase on average after every
15 words in running text.
A large proportion of scientific metadiscourse is conventionalized, particularly in
the experimental sciences, and particularly in the methodology or result section (e.g.,
we present original work . . . , or An ANOVA analysis revealed a marginal interaction/a main ef-
fect of . . . ). Swales (1990) lists many such fixed phrases as co-occurring with the moves
of his CARS model (pages 144, 154?158, 160?161). They are useful indicators of overall
importance (Pollock and Zamora 1975); they can also be relatively easily recognized
with information extraction techniques (e.g., regular expressions). Paice (1990) intro-
duces grammars for pattern matching of indicator phrases, e.g., the aim/purpose of this
paper/article/study and we conclude/propose.
Apart from this conventionalized metadiscourse, we noticed that our corpus con-
tains a large number of metadiscourse statements that are less formalized: statements
about aspects of the problem-solving process or the relation to other work. Figure 4,
for instance, shows that there are many ways to say that one?s research is based on
somebody else?s (?research continuation?). The sentences do not look similar on the
surface: The syntactic subject can be the authors, the originators of the method, or
even the method itself. Also, the verbs are very different (base, be related, use, follow).
Some sentences use metaphors of change and creation. The wide range of linguistic
expression we observed presents a challenge for recognition and correct classification
using standard information extraction patterns.
With respect to agents occurring in scientific metadiscourse, we make two sug-
gestions: (1) that scientific argumentation follows prototypical patterns and employs
recurrent types of agents and actions and (2) that it is possible to recognize many of
these automatically. Agents play fixed roles in the argumentation, and there are so
? We employ Suzuki?s algorithm to learn case frame patterns as dendroid distributions.
(9605013)
? Our method combines similarity-based estimates with Katz?s back-off scheme, which is widely used for language
modeling in speech recognition. (9405001)
? Thus, we base our model on the work of Clark and Wilkes-Gibbs (1986), and Heeman and Hirst (1992) . . .
(9405013)
? The starting point for this work was Scha and Polanyi?s discourse grammar (Scha and Polanyi, 1988; Pruest et
al., 1994). (9502018)
? We use the framework for the allocation and transfer of control of Whittaker and Stenton (1988).
(9504007)
? Following Laur (1993), we consider simple prepositions (like ?in?) as well as prepositional phrases (like ?in front
of?). (9503007)
? Our lexicon is based on a finite-state transducer lexicon (Karttunen et al, 1992).
(9503004)
? Instead of . . . we will adopt a simpler, monostratal representation that is more closely related to those found in
dependency grammars (e.g., Hudson (1984)). (9408014)
Figure 4
Statements expressing research continuation, with source article number.
415
Teufel and Moens Summarizing Scientific Articles
few of these roles that they can be enumerated: agents appear as rivals, as contrib-
utors of part of the solution (they), as the entire research community in the field, or
as the authors of the paper themselves (we). Note the similarity of agent roles to the
three kinds of intellectual attribution mentioned above. We also propose prototypical
actions frequently occurring in scientific discourse: the field might agree, a particular
researcher can suggest something, and a certain solution could either fail or be success-
ful. In section 4 we will describe the three features used in our implementation that
recognize metadiscourse.
Another important construct that expresses relations to other researchers? work is
formal citations, to which we will now turn.
2.3 Citations and Relatedness
Citation indexes are constructs that contain pointers between cited texts and citing
texts (Garfield 1979), traditionally in printed form. When done on-line (as in CiteSeer
[Lawrence, Giles, and Bollacker 1999], or as in Nanba and Okumura?s [1999] work),
citations are presented in context for users to browse. Browsing each citation is time-
consuming, but useful: just knowing that an article cites another is often not enough.
One needs to read the context of the citation to understand the relation between the
articles. Citations may vary in many dimensions; for example, they can be central or
perfunctory, positive or negative (i.e., critical); apart from scientific reasons, there is
also a host of social reasons for citing (?politeness, tradition, piety? [Ziman 1969]).
We concentrate on two citation contexts that are particularly important for the
information needs of researchers:
? Contexts in which an article is cited negatively or contrastively.
? Contexts in which an article is cited positively or in which the authors
state that their own work originates from the cited work.
A distinction among these contexts would enable us to build more informative citation
indexes. We suggest that such a rhetorical distinction can be made manually and
automatically for each citation; we use a large corpus of scientific papers along with
humans? judgments of this distinction to train a system to make such distinctions.
2.4 The Rhetorical Annotation Scheme
Our rhetorical annotation scheme (cf. Table 1) encodes the aspects of scientific argu-
mentation, metadiscourse, and relatedness to other work described before. The cat-
egories are assigned to full sentences, but a similar scheme could be developed for
clauses or phrases.
The annotation scheme is nonoverlapping and nonhierarchical, and each sentence
must be assigned to exactly one category. As adjacent sentences of the same status can
be considered to form zones of the same rhetorical status, we call the units rhetorical
zones. The shortest of these zones are one sentence long.
The rhetorical status of a sentence is determined on the basis of the global context
of the paper. For instance, whereas the OTHER category describes all neutral descrip-
tions of other researchers? work, the categories BASIS and CONTRAST are applicable to
sentences expressing a research continuation relationship or a contrast to other work.
Generally accepted knowledge is classified as BACKGROUND, whereas the author?s own
work is separated into the specific research goal (AIM) and all other statements about
the author?s own work (OWN).
416
Computational Linguistics Volume 28, Number 4
Table 1
Annotation scheme for rhetorical status.
AIM Specific research goal of the current paper
TEXTUAL Statements about section structure
OWN (Neutral) description of own work presented in current paper: Method-
ology, results, discussion
BACKGROUND Generally accepted scientific background
CONTRAST Statements of comparison with or contrast to other work; weaknesses of
other work
BASIS Statements of agreement with other work or continuation of other work
OTHER (Neutral) description of other researchers? work
The annotation scheme expresses important discourse and argumentation aspects
of scientific articles, but with its seven categories it is not designed to model the full
complexity of scientific texts. The category OWN, for instance, could be further sub-
divided into method (solution), results, and further work, which is not done in the
work reported here. There is a conflict between explanatory power and the simplicity
necessary for reliable human and automatic classification, and we decided to restrict
ourselves to the rhetorical distinctions that are most salient and potentially most useful
for several information access applications. The user-tailored summaries and more in-
formative citation indexes we mentioned before are just two such applications; another
one is the indexing and previewing of the internal structure of the article. To make
such indexing and previewing possible, our scheme contains the additional category
TEXTUAL, which captures previews of section structure (section 2 describes our data . . . ).
Such previews would make it possible to label sections with the author?s indication
of their contents.
Our rhetorical analysis, as noted above, is nonhierarchical, in contrast to Rhetorical
Structure Theory (RST) (Mann and Thompson 1987; Marcu 1999), and it concerns
text pieces at a lower level of granularity. Although we do agree with RST that the
structure of text is hierarchical in many cases, it is our belief that the relevance and
function of certain text pieces can be determined without analyzing the full hierarchical
structure of the text. Another difference between our analysis and that of RST is that
our analysis aims at capturing the rhetorical status of a piece of text in respect to the
overall message, and not in relation to adjacent pieces of text.
2.5 Relevance
As our immediate goal is to select important content from a text, we also need a second
set of gold standards that are defined by relevance (as opposed to rhetorical status).
Relevance is a difficult issue because it is situational to a unique occasion (Saracevic
1975; Sparck Jones 1990; Mizzaro 1997): Humans perceive relevance differently from
each other and differently in different situations. Paice and Jones (1993) report that they
abandoned an informal sentence selection experiment in which they used agriculture
articles and experts in the field as participants, as the participants were too strongly
influenced by their personal research interest.
As a result of subjectivity, a number of human sentence extraction experiments
over the years have resulted in low agreement figures. Rath, Resnick, and Savage
(1961) report that six participants agreed on only 8% of 20 sentences they were asked
to select out of short Scientific American texts and that five agreed on 32% of the
sentences. They found that after six weeks, subjects selected on average only 55%
of the sentences they themselves selected previously. Edmundson and Wyllys (1961)
417
Teufel and Moens Summarizing Scientific Articles
find similarly low human agreement for research articles. More recent experiments
reporting more positive results all used news text (Jing et al 1998; Zechner 1995).
As discussed above, the compression rates on news texts are far lower: there are
fewer sentences from which to choose, making it easier to agree on which ones to
select. Sentence selection from scientific texts also requires more background knowl-
edge, thus importing an even higher level of subjectivity into sentence selection
experiments.
Recently, researchers have been looking for more objective definitions of relevance.
Kupiec, Pedersen, and Chen (1995) define relevance by abstract similarity: A sentence
in a document is considered relevant if it shows a high level of similarity to a sentence
in the abstract. This definition of relevance has the advantage that it is fixed (i.e., the
researchers have no influence over it). It relies, however, on two assumptions: that the
writing style is such that there is a high degree of overlap between sentences in the
abstract and in the main text and that the abstract is indeed the target output that is
most adequate for the final task.
In our case, neither assumption holds. First, the experiments in Teufel and Moens
(1997) showed that in our corpus only 45% of the abstract sentences appear elsewhere
in the body of the document (either as a close variant or in identical form), whereas
Kupiec, Pedersen, and Chen report a figure of 79%. We believe that the reason for the
difference is that in our case the abstracts were produced by the document authors and
by professional abstractors in Kupiec, Pedersen, and Chen?s case. Author summaries
tend to be less systematic (Rowley 1982) and more ?deep generated,? whereas sum-
maries by professional abstractors follow an internalized building plan (Liddy 1991)
and are often created through sentence extraction (Lancaster 1998).
Second, and more importantly, the abstracts and improved citation indexes we
intend to generate are not modeled on traditional summaries, which do not pro-
vide the type of information needed for the applications we have in mind. Infor-
mation about related work plays an important role in our strategy for summarization
and citation indexing, but such information is rarely found in abstracts. We empir-
ically found that the rhetorical status of information occurring in author abstracts
is very limited and consists mostly of information about the goal of the paper and
specifics of the solution. Details of the analysis we conducted on this topic are given in
section 3.2.2.
We thus decided to augment our corpus with an independent set of human judg-
ments of relevance. We wanted to replace the vague definition of relevance often
used in sentence extraction experiments with a more operational definition based on
rhetorical status. For instance, a sentence is considered relevant only if it describes
the research goal or states a difference with a rival approach. More details of the
instructions we used to make the relevance decisions are given in section 3.
Thus, we have two parallel human annotations in our corpus: rhetorical annotation
and relevance selection. In both tasks, each sentence in the articles is classified: Each
sentence receives one rhetorical category and also the label irrelevant or relevant. This
strategy can create redundant material (e.g., when the same fact is expressed in a
sentence in the introduction, a sentence in the conclusion, and one in the middle of
the document). But this redundancy also helps mitigate one of the main problems
with sentence-based gold standards, namely, the fact that there is no one single best
extract for a document. In our annotation, all qualifying sentences in the document
are identified and classified into the same group, which makes later comparisons
with system performance fairer. Also, later steps cannot only find redundancy in the
intermediate result and remove it, but also use the redundancy as an indication of
importance.
418
Computational Linguistics Volume 28, Number 4
Aim:
10 Our research addresses some of the same questions and uses similar raw data, but we investigate
how to factor word association tendencies into associations of words to certain hidden senses
classes and associations between the classes themselves.
22 We will consider here only the problem of classifying nouns according to their distribution as
direct objects of verbs; the converse problem is formally similar.
25 The problem we study is how to use the EQN to classify the EQN.
44 In general, we are interested on how to organise a set of linguistic objects such as words according
to the contexts in which they occur, for instance grammatical constructions or n-grams.
46 Our problem can be seen as that of learning a joint distribution of pairs from a large sample of
pairs.
162 We have demonstrated that a general divisive clustering procedure for probability distributions
can be used to group words according to their participation in particular grammatical relations
with other words.
Background:
0 Methods for automatically classifying words according to their contexts of use have both scientific
and practical interest.
4 The problem is that for large enough corpora the number of possible joint events is much larger
than the number of event occurrences in the corpus, so many events are seen rarely or never,
making their frequency counts unreliable estimates of their probabilities.
Own (Details of Solution):
66 The first stage of an iteration is a maximum likelihood, or minimum distortion, estimation of
the cluster centroids given fixed membership probabilities.
140 The evaluation described below was performed on the largest data set we have worked with so far,
extracted from 44 million words of 1988 Associated Press newswire with the pattern matching
techniques mentioned earlier.
163 The resulting clusters are intuitively informative, and can be used to construct class-based word
coocurrence [sic] models with substantial predictive power.
Contrast with Other Approaches/Weaknesses of Other Approaches:
9 His notion of similarity seems to agree with our intuitions in many cases, but it is not clear how
it can be used directly to construct word classes and corresponding models of association.
14 Class construction is then combinatorially very demanding and depends on frequency counts
for joint events involving particular words, a potentially unreliable source of information as we
noted above.
41 However, this is not very satisfactory because one of the goals of our work is precisely to avoid
the problems of data sparseness by grouping words into classes.
Basis (Imported Solutions):
65 The combined entropy maximization entropy [sic] and distortion minimization is carried out
by a two-stage iterative process similar to the EM method (Dempster et al, 1977).
113 The analogy with statistical mechanics suggests a deterministic annealing procedure for clus-
tering (Rose et al, 1990), in which the number of clusters is determined through a sequence
of phase transitions by continuously increasing the parameter EQN following an annealing
schedule.
153 The data for this test was built from the training data for the previous one in the following way,
based on a suggestion by Dagan et al (1993).
Figure 5
Example of manual annotation: Relevant sentences with rhetorical status.
Figure 5 gives an example of the manual annotation. Relevant sentences of all
rhetorical categories are shown. Our system creates a list like the one in Figure 5
automatically (Figure 12 shows the actual output of the system when run on the
example paper). In the next section, we turn to the manual annotation step and the
development of the gold standard used during system training and system evaluation.
419
Teufel and Moens Summarizing Scientific Articles
3. Human Judgments: The Gold Standard
For any linguistic analysis that requires subjective interpretation and that is therefore
not objectively true or false, it is important to show that humans share some intuitions
about the analysis. This is typically done by showing that they can apply it indepen-
dently of each other and that the variation they display is bounded (i.e., not arbitrarily
high). The argument is strengthened if the judges are people other than the developers
of the analysis, preferably ?na??ve? subjects (i.e., not computational linguists). Apart
from the cognitive validation of our analysis, high agreement is essential if the anno-
tated corpus is to be used as training material for a machine learning process, like the
one we describe in section 4. Noisy and unreliably annotated training material will
very likely deteriorate the classification performance.
In inherently subjective tasks, it is also common practice to consider human perfor-
mance as an upper bound. The theoretically best performance of a system is reached
if agreement among a pool of human annotators does not decrease when the system
is added to the pool. This is so because an automatic process cannot do any better in
this situation than to be indistinguishable from human performance.
3.1 Corpus
The annotated development corpus consists of 80 conference articles in computational
linguistics (12,188 sentences; 285,934 words). It is part of a larger corpus of 260 articles
(1.1 million words) that we collected from the CMP LG archive (CMP LG 1994). The
appendix lists the 80 articles (archive numbers, titles and authors) of our development
corpus; it consists of the 80 chronologically oldest articles in the larger corpus, con-
taining articles deposited between May 1994 and May 1996 (whereas the entire corpus
stretches until 2001).
Papers were included if they were presented at one of the following conferences
(or associated workshops): the annual meeting of the Association for Computational
Linguistics (ACL), the meeting of the European Chapter of the Association for Compu-
tational Linguistics (EACL), the conference on Applied Natural Language Processing
(ANLP), the International Joint Conference on Artificial Intelligence (IJCAI), or the In-
ternational Conference on Computational Linguistics (COLING). As mentioned above,
a wide range of different subdomains of the field of computational linguistics are
covered.
We added Extensible Markup Language (XML) markup to the corpus: Titles, au-
thors, conference, date, abstract, sections, headlines, paragraphs, and sentences were
marked up. Equations, tables, images were removed and replaced by placeholders.
Bibliography lists were marked up and parsed. Citations and occurrences of au-
thor names in running text were recognized, and self-citations were recognized and
specifically marked up. (Linguistic) example sentences and example pseudocode were
manually marked up, such that clean textual material (i.e., the running text of the
article without interruptions) was isolated for automatic processing. The implemen-
tation uses the Text Tokenization Toolkit (TTT) software (Grover, Mikheev, and
Matheson 1999).
3.2 Annotation of Rhetorical Status
The annotation experiment described here (and in Teufel, Carletta, and Moens [1999]
in more detail) tests the rhetorical annotation scheme presented in section 2.4.
420
Computational Linguistics Volume 28, Number 4
3.2.1 Rationale and Experimental Design.
Annotators. Three task-trained annotators were used: Annotators A and B have
degrees in cognitive science and speech therapy. They were paid for the experiment.
Both are well-used to reading scientific articles for their studies and roughly under-
stand the contents of the articles they annotated because of the closeness of their fields
to computational linguistics. Annotator C is the first author. We did not want to de-
clare annotator C the expert annotator; we believe that in subjective tasks like the one
described here, there are no real experts.
Guidelines. Written guidelines (17 pages) describe the semantics of the categories,
ambiguous cases, and decision strategies. The guidelines also include the decision tree
reproduced in Figure 6.
Training. Annotators received a total of 20 hours of training. Training consisted
of the presentation of annotation of six example papers and the annotation of eight
training articles under real conditions (i.e., independently). In subsequent training ses-
sions, decision criteria for difficult cases encountered in the training articles were dis-
cussed. Obviously, the training articles were excluded from measurements of human
agreement.
Materials and procedure. Twenty-five articles were used for annotation. As no annota-
tion tool was available at the time, annotation was performed on paper; the categories
were later transferred to the electronic versions of the articles by hand. Skim-reading
and annotation typically took between 20 and 30 minutes per article, but there were
no time restrictions. No communication between the annotators was allowed during
annotation. Six weeks after the initial annotation, annotators were asked to reannotate
6 random articles out of the 25.
Evaluation measures. We measured two formal properties of the annotation: stability
and reproducibility (Krippendorff 1980). Stability, the extent to which one annotator
will produce the same classifications at different times, is important because an unsta-
ble annotation scheme can never be reproducible. Reproducibility, the extent to which
different annotators will produce the same classifications, is important because it mea-
sures the consistency of shared understandings (or meaning) held between annotators.
work of the authors)?
Does this sentence refer to new, current
BACKGROUND
CONTRAST
YES NO
YES NO
NOYES
YES NO
YES
BASIS
NO
NOYES
AIM
OWN
background, including phenomena
Does the sentence describe general
to be explained or linguistic example sentences?
that describes the specific aim
Does this sentence contain material
of the paper?
explicit reference to the
structure of the paper?
Does this sentence make
TEXTUAL
OTHER
or comparison of the own work to it?
of other work, or a contrast
Does it describe a negative aspect
or support for the current paper?
Does this sentence mention
other work as basis of 
work by the authors (excluding previous 
Figure 6
Decision tree for rhetorical annotation.
421
Teufel and Moens Summarizing Scientific Articles
We use the kappa coefficient K (Siegel and Castellan 1988) to measure stability and
reproducibility, following Carletta (1996). The kappa coefficient is defined as follows:
K =
P(A) ? P(E)
1 ? P(E)
where P(A) is pairwise agreement and P(E) random agreement. K varies between 1
when agreement is perfect and ?1 when there is a perfect negative correlation. K = 0
is defined as the level of agreement that would be reached by random annotation
using the same distribution of categories as the actual annotators did.
The main advantage of kappa as an annotation measure is that it factors out
random agreement by numbers of categories and by their distribution. As kappa also
abstracts over the number of annotators considered, it allows us to compare the agree-
ment numerically among a group of human annotators with the agreement between
the system and one or more annotators (section 5), which we use as one of the per-
formance measures of the system.
3.2.2 Results. The annotation experiments show that humans distinguish the seven
rhetorical categories with a stability of K = .82, .81, .76 (N = 1,220; k = 2, where K
stands for the kappa coefficient, N for the number of items (sentences) annotated, and
k for the number of annotators). This is equivalent to 93%, 92%, and 90% agreement.
Reproducibility was measured at K = .71 (N = 4,261, k = 3), which is equivalent
to 87% agreement. On Krippendorff?s (1980) scale, agreement of K = .8 or above is
considered as reliable, agreement of .67?.8 as marginally reliable, and less than .67 as
unreliable. On Landis and Koch?s (1977) more forgiving scale, agreement of .0?.2 is
considered as showing ?slight? correlation, .21?.4 as ?fair,? .41?.6 as ?moderate,? .61?
0.8 as ?substantial,? and .81 ?1.0 as ?almost perfect.? According to these guidelines,
our results can be considered reliable, substantial annotation.
Figure 7 shows that the distribution of the seven categories is very skewed, with
67% of all sentences being classified as OWN. (The distribution was calculated using
all three judgments per sentence [cf. the calculation of kappa]. The total number of
items is then k ? N, i.e., 12,783 in this case.)
Table 2 shows a confusion matrix between two annotators. The numbers repre-
sent absolute sentence numbers, and the diagonal (boldface numbers) are the counts of
sentences that were identically classified by both annotators. We used Krippendorff?s
diagnostics to determine which particular categories humans had most problems with:
For each category, agreement is measured with a new data set in which all categories
Figure 7
Distribution of rhetorical categories (entire document).
422
Computational Linguistics Volume 28, Number 4
Table 2
Confusion matrix between annotators B and C.
ANNOTATOR B
AIM CTR TXT OWN BKG BAS OTH Total
ANNOTATOR C AIM 35 2 1 19 3 2 62
CTR 86 31 16 23 156
TXT 31 7 1 39
OWN 10 62 5 2,298 25 3 84 2,487
BKG 5 13 115 20 153
BAS 2 18 1 18 14 53
OTH 1 18 2 55 10 1 412 499
Total 48 173 39 2,441 170 22 556 3,449
except for the category of interest are collapsed into one metacategory. Original agree-
ment is compared to that measured on the new (artificial) data set; high values show
that annotators can distinguish the given category well from all others. When their re-
sults are compared to the overall reproducibility of K = .71, the annotators were good
at distinguishing AIM (Krippendorff?s diagnostics; K = .79) and TEXTUAL (K = .79).
The high agreement in AIM sentences is a positive result that seems to be at odds with
previous sentence extraction experiments. We take this as an indication that some types
of rhetorical classification are easier for human minds to do than unqualified relevance
decision. We also think that the positive results are partly due to the existence of the
guidelines.
The annotators were less consistent at determining BASIS (K = .49) and CONTRAST
(K = .59). The same picture emerges if we look at precision and recall of single cate-
gories between two annotators (cf. Table 3). Precision and recall for AIM and TEXTUAL
are high at 72%/56% and 79%/79%, whereas they are lower for CONTRAST (50%/55%)
and BASIS (82%/34%).
This contrast in agreement might have to do with the location of the rhetori-
cal zones in the paper: AIM and TEXTUAL zones are usually found in fixed locations
(beginning or end of the introduction section) and are explicitly marked with metadis-
course, whereas CONTRAST sentences, and even more so BASIS sentences, are usually
interspersed within longer OWN zones. As a result, these categories are more exposed
to lapses of attention during annotation.
With respect to the longer, more neutral zones (intellectual attribution), annotators
often had problems in distinguishing OTHER work from OWN work, particularly in
cases where the authors did not express a clear distinction between new work and
previous own work (which, according to our instructions, should be annotated as OTHER).
Another persistently problematic distinction for our annotators was that between OWN
Table 3
Annotator C?s precision and recall per category if annotator B is gold standard.
AIM CTR TXT OWN BKG BAS OTH
Precision 72% 50% 79% 94% 68% 82% 74%
Recall 56% 55% 79% 92% 75% 34% 83%
423
Teufel and Moens Summarizing Scientific Articles
and BACKGROUND. This could be a sign that some authors aimed their papers at an
expert audience and thus thought it unnecessary to signal clearly which statements
are commonly agreed upon in the field, as opposed to their own new claims. If a
paper is written in such a way, it can indeed be understood only with a considerable
amount of domain knowledge, which our annotators did not have.
Because intellectual attribution (the distinction between OWN, OTHER, and BACK-
GROUND material) is an important part of our annotation scheme, we conducted a
second experiment measuring how well our annotators could distinguish just these
three roles, using the same annotators and 22 different articles. We wrote seven pages
of new guidelines describing the semantics of the three categories. Results show higher
stability compared to the full annotation scheme (K = .83, .79, .81; N = 1,248; k = 2)
and higher reproducibility (K = .78, N = 4,031, k = 3), corresponding to 94%, 93%,
and 93% agreement (stability) and 93% (reproducibility). It is most remarkable that
agreement of annotation of intellectual attribution in the abstracts is almost perfect:
K = .98 (N = 89, k = 3), corresponding to 99% agreement. This points to the fact that
authors, when writing abstracts for their papers, take care to make it clear to whom
a certain statement is attributed. This effect also holds for the annotation with the
full scheme with all seven categories: again, reproducibility in the abstract is higher
(K = .79) than in the entire document (K = .71), but the effect is much weaker.
Abstracts might be easier to annotate than the rest of a paper, but this does not
necessarily make it possible to define a gold standard solely by looking at the ab-
stracts. As foreshadowed in section 2.5, abstracts do not contain all types of rhetorical
information. AIM and OWN sentences make up 74% of the sentences in abstracts, and
only 5% of all CONTRAST sentences and 3% of all BASIS sentences occur in the abstract.
Abstracts in our corpus are also not structurally homogeneous. When we inspected
the rhetorical structure of abstracts in terms of sequences of rhetorical zones, we found
a high level of variation. Even though the sequence AIM?OWN is very common (con-
tained in 73% of all abstracts), the 80 abstracts still contain 40 different rhetorical
sequences, 28 of which are unique. This heterogeneity is in stark contrast to the sys-
tematic structures Liddy (1991) found to be produced by professional abstractors. Both
observations, the lack of certain rhetorical types in the abstracts and their rhetorical
heterogeneity, reassure us in our decision not to use human-written abstracts as a gold
standard.
3.3 Annotation of Relevance
We collected two different kinds of relevance gold standards for the documents in our
development corpus: abstract-similar document sentences and additional manually
selected sentences.
In order to establish alignment between summary and document sentences, we
used a semiautomatic method that relies on a simple surface similarity measure (long-
est common subsequence of content words, i.e., excluding words on a stop list). As in
Kupiec, Pedersen, and Chen?s experiment, final alignment was decided by a human
judge, and the criterion was semantic similarity of the two sentences. The following
sentence pair illustrates a direct match:
Summary: In understanding a reference, an agent determines his
confidence in its adequacy as a means of identifying the referent.
Document: An agent understands a reference once he is confident
in the adequacy of its (inferred) plan as a means of identifying the
referent.
424
Computational Linguistics Volume 28, Number 4
Of the 346 abstract sentences contained in the 80 documents, 156 (45%) could be
aligned this way. Because of this low agreement and because certain rhetorical types
are not present in the abstracts, we decided not to rely on abstract alignment as our
only gold standard. Instead, we used manually selected sentences as an alternative
gold standard, which is more informative, but also more subjective.
We wrote eight pages of guidelines that describe relevance criteria (e.g., our defi-
nition prescribes that neutral descriptions of other work be selected only if the other
work is an essential part of the solution presented, whereas all statements of criti-
cism are to be included). The first author annotated all documents in the development
corpus for relevance using the rhetorical zones and abstract similarity as aides in the
relevance decision, and also skim-reading the whole paper before making the decision.
This resulted in 5 to 28 sentences per paper and a total of 1,183 sentences.
Implicitly, rhetorical classification of the extracted sentences was already given as
each of these sentences already had a rhetorical status assigned to it. However, the
rhetorical scheme we used for this task is slightly different. We excluded TEXTUAL, as
this category was designed for document uses other than summarization. If a selected
sentence had the rhetorical class TEXTUAL, it was reclassified into one of the other
six categories. Figure 8 shows the resulting category distribution among these 1,183
sentences, which is far more evenly distributed than the one covering all sentences (cf.
Figure 7). CONTRAST and OWN are the two most frequent categories.
We did not verify the relevance annotation with human experiments. We accept
that the set of sentences chosen by the human annotator is only one possible gold
standard. What is more important is that humans can agree on the rhetorical status of
the relevant sentences. Liddy observed that agreement on rhetorical status was easier
for professional abstractors than sentence selection: Although they did not necessarily
agree on which individual sentences should go into an abstract, they did agree on the
rhetorical information types that make up a good abstract.
We asked our trained annotators to classify a set of 200 sentences, randomly
sampled from the 1,183 sentences selected by the first author, into the six rhetori-
cal categories. The sentences were presented in order of occurrence in the document,
but without any context in terms of surrounding sentences. We measured stability at
K = .9, .86, .83 (N = 100, k = 2) and reproducibility at K = .84 (N = 200, k = 3). These
results are reassuring: They show that the rhetorical status for important sentences can
be particularly well determined, better than rhetorical status for all sentences in the
document (for which reproducibility was K = .71; cf. section 3.2.2).
Figure 8
Distribution of rhetorical categories (relevant sentences).
425
Teufel and Moens Summarizing Scientific Articles
4. The System
We now describe an automatic system that can perform extraction and classification
of rhetorical status on unseen text (cf. also a prior version of the system reported
in Teufel and Moens [2000] and Teufel [1999]). We decided to use machine learning
to perform this extraction and classification, based on a variety of sentential features
similar to the ones reported in the sentence extraction literature. Human annotation is
used as training material such that the associations between these sentential features
and the target sentences can be learned. It is also used as gold standard for intrinsic
system evaluation.
A simpler machine learning approach using only word frequency information
and no other features, as typically used in tasks like text classification, could have
been employed (and indeed Nanba and Okumura [1999] do so for classifying citation
contexts). To test if such a simple approach would be enough, we performed a text
categorization experiment, using the Rainbow implementation of a na??ve Bayes term
frequency times inverse document frequency (TF*IDF) method (McCallum 1997) and
considering each sentence as a ?document.? The result was a classification performance
of K = .30; the classifier nearly always chooses OWN and OTHER segments. The rare but
important categories AIM, BACKGROUND, CONTRAST, and BASIS could be retrieved only
with low precision and recall. Therefore, text classification methods do not provide a
solution to our problem. This is not surprising, given that the definition of our task has
little to do with the distribution of ?content-bearing? words and phrases, much less so
than the related task of topic segmentation (Morris and Hirst 1991; Hearst 1997; Choi
2000), or Saggion and Lapalme?s (2000) approach to the summarization of scientific
articles, which relies on scientific concepts and their relations. Instead, we predict that
other indicators apart from the simple words contained in the sentence could provide
strong evidence for the modeling of rhetorical status. Also, the relatively small amount
of training material we have at our disposal requires a machine learning method that
makes optimal use of as many different kinds of features as possible. We predicted that
this would increase precision and recall on the categories in which we are interested.
The text classification experiment is still useful as it provides a nontrivial baseline for
comparison with our intrinsic system evaluation presented in section 5.
4.1 Classifiers
We use a na??ve Bayesian model as in Kupiec, Pedersen, and Chen?s (1995) experiment
(cf. Figure 9). Sentential features are collected for each sentence (Table 4 gives an
overview of the features we used). Learning is supervised: In the training phase,
associations between these features and human-provided target categories are learned.
The target categories are the seven categories in the rhetorical annotation experiment
and relevant/nonrelevant in the relevance selection experiment. In the testing phase,
the trained model provides the probability of each target category for each sentence
of unseen text, on the basis of the sentential features identified for the sentence.
4.2 Features
Some of the features in our feature pool are unique to our approach, for instance, the
metadiscourse features. Others are borrowed from the text extraction literature (Paice
1990) or related tasks and adapted to the problem of determining rhetorical status.
Absolute location of a sentence. In the news domain, sentence location is the single
most important feature for sentence selection (Brandow, Mitze, and Rau 1995); in our
domain, location information, although less dominant, can still give a useful indication.
Rhetorical zones appear in typical positions in the article, as scientific argumentation
426
Computational Linguistics Volume 28, Number 4
P(C | F0, . . . , Fn?1) ? P(C)
?n?1
j=0 P(Fj | C)
?n?1
j=0 P(Fj)
P(C | F0, . . . , Fn?1): Probability that a sentence has target category C, given its feature val-
ues F0, . . . , Fn?1;
P(C): (Overall) probability of category C;
P(Fj | C): Probability of feature-value pair Fj, given that the sentence is of target
category C;
P(Fj): Probability of feature value Fj;
Figure 9
Na??ve Bayesian classifier.
Table 4
Overview of feature pool.
Type Name Feature Description Feature Values
Absolute
location
1. Loc Position of sentence in relation to 10
segments
A-J
Explicit
structure
2. Section
Struct
Relative and absolute position of
sentence within section (e.g., first
sentence in section or somewhere in
second third)
7 values
3. Para
Struct
Relative position of sentence within a
paragraph
Initial, Medial, Final
4. Headline Type of headline of current section 15 prototypical headlines
or Non-prototypical
Sentence
length
5. Length Is the sentence longer than a certain
threshold, measured in words?
Yes or No
Content
features
6. Title Does the sentence contain words also
occurring in the title or headlines?
Yes or No
7. TF*IDF Does the sentence contain ?significant
terms? as determined by the TF*IDF
measure?
Yes or No
Verb syntax 8. Voice Voice (of first finite verb in sentence) Active or Passive or
NoVerb
9. Tense Tense (of first finite verb in sentence) 9 simple and complex
tenses or NoVerb
10. Modal Is the first finite verb modified by
modal auxiliary?
Modal or NoModal or
NoVerb
Citations 11. Cit Does the sentence contain a citation or
the name of an author contained in the
reference list? If it contains a citation,
is it a self-citation? Whereabouts in the
sentence does the citation occur?
{Citation (self), Citation
(other), Author Name,
or None} ? {Beginning,
Middle, End}
History 12. History Most probable previous category 7 Target Categories +
?BEGIN?
Meta-
discourse
13. Formulaic Type of formulaic expression occur-
ring in sentence
18 Types of Formulaic
Expressions + 9 Agent
Types or None
14. Agent Type of agent 9 Agent Types or None
15. SegAgent Type of agent 9 Agent Types or None
16. Action Type of action, with or without
negation
27 Action Types or None
427
Teufel and Moens Summarizing Scientific Articles
follows certain patterns (Swales 1990). For example, limitations of the author?s own
method can be expected to be found toward the end of the article, whereas limitations
of other researchers? work are often discussed in the introduction. We observed that the
size of rhetorical zones depends on location, with smaller rhetorical zones occurring
toward the beginning and the end of the article. We model this by assigning location
values in the following fashion: The article is divided into 20 equal parts, counting
sentences. Sentences occurring in parts 1, 2, 3, 4, 19, and 20 receive the values A, B, C,
D, I, and J, respectively. Parts 5 and 6 are pooled, and sentences occurring in them are
given the value E; the same procedure is applied to parts 15 and 16 (value G) and 17
and 18 (value H). The remaining sentences in the middle (parts 7?14) all receive the
value F (cf. Figure 10).
Section structure. Sections can have an internal structuring; for instance, sentences
toward the beginning of a section often have a summarizing function. The section
location feature divides each section into three parts and assigns seven values: first
sentence, last sentence, second or third sentence, second-last or third-last sentence, or
else either somewhere in the first, second, or last third of the section.
Paragraph structure. In many genres, paragraphs also have internal structure (Wiebe
1994), with high-level or summarizing sentences occurring more often at the periphery
of paragraphs. In this feature, sentences are distinguished into those leading or ending
a paragraph and all others.
Headlines. Prototypical headlines can be an important predictor of the rhetorical
status of sentences occurring in the given section; however, not all texts in our collec-
tion use such headlines. Whenever a prototypical headline is recognized (using a set
of regular expressions), it is classified into one of the following 15 classes: Introduction,
Implementation, Example, Conclusion, Result, Evaluation, Solution, Experiment, Discussion,
Method, Problems, Related Work, Data, Further Work, Problem Statement. If none of the
patterns match, the value Non-Prototypical is assigned.
Sentence length. Kupiec, Pedersen, and Chen (1995) report sentence length as a
useful feature for text extraction. In our implementation, sentences are divided into
long or short sentences, by comparison to a fixed threshold (12 words).
Title word contents. Sentences containing many ?content-bearing? words have been
hypothesized to be good candidates for text extraction. Baxendale (1958) extracted all
words except those on the stop list from the title and the headlines and determined
for each sentence whether or not it contained these words. We received better results
by excluding headline words and using only title words.
TF*IDF word contents. How content-bearing a word is can also be measured with
frequency counts (Salton and McGill 1983). The TF*IDF formula assigns high values
to words that occur frequently in one document, but rarely in the overall collection of
documents. We use the 18 highest-scoring TF*IDF words and classify sentences into
those that contain one or more of these words and those that do not.
Verb syntax. Linguistic features like tense and voice often correlate with rhetorical
zones; Biber (1995) and Riley (1991) show correlation of tense and voice with prototyp-
ical section structure (?method,? ?introduction?). In addition, the presence or absence
BA C D E F G H I J
126 7 81 2 3 4 9 10 11 13 14 15 16 17 18 19 205
Figure 10
Values of location feature.
428
Computational Linguistics Volume 28, Number 4
of a modal auxiliary might be relevant for detecting the phenomenon of ?hedging?
(i.e., statements in which an author distances herself from her claims or signals low
certainty: these results might indicate that . . . possibly . . . [Hyland 1998]). For each sen-
tence, we use part-of-speech-based heuristics to determine tense, voice, and presence
of modal auxiliaries. This algorithm is shared with the metadiscourse features, and
the details are described below.
Citation. There are many connections between citation behavior and relevance or
rhetorical status. First, if a sentence contains a formal citation or the name of another
author mentioned in the bibliography, it is far more likely to talk about other work than
about own work. Second, if it contains a self-citation, it is far more likely to contain
a direct statement of continuation (25%) than a criticism (3%). Third, the importance
of a citation has been related to the distinction between authorial and parenthetical
citations. Citations are called authorial if they form a syntactically integral part of the
sentence or parenthetical if they do not (Swales 1990). In most cases, authorial citations
are used as the subject of a sentence, and parenthetical ones appear toward the middle
or the end of the sentence.
We built a recognizer for formal citations. It parses the reference list at the end of
the article and determines whether a citation is a self-citation (i.e., if there is an overlap
between the names of the cited researchers and the authors of the current paper), and
it also finds occurrences of authors? names in running text, but outside of formal
citation contexts (e.g., Chomsky also claims that . . . ). The citation feature reports whether
a sentence contains an author name, a citation, or nothing. If it contains a citation,
the value records whether it is a self-citation and also records the location of the
citation in the sentence (in the beginning, the middle, or the end). This last distinction
is a heuristic for the authorial/parenthetical distinction. We also experimented with
including the number of different citations in a sentence, but this did not improve
results.
History. As there are typical patterns in the rhetorical zones (e.g., AIM sentences
tend to follow CONTRAST sentences), we wanted to include the category assigned to
the previous sentence as one of the features. In unseen text, however, the previous
target is unknown at training time (it is determined during testing). It can, however,
be calculated as a second pass process during training. In order to avoid a full Viterbi
search of all possibilities, we perform a beam search with width of three among the
candidates of the previous sentence, following Barzilay et al (2000).
Formulaic expressions. We now turn to the last three features in our feature pool, the
metadiscourse features, which are more sophisticated than the other features. The first
metadiscourse feature models formulaic expressions like the ones described by Swales,
as they are semantic indicators that we expect to be helpful for rhetorical classification.
We use a list of phrases described by regular expressions, similar to Paice?s (1990)
grammar. Our list is divided into 18 semantic classes (cf. Table 5), comprising a total
of 644 patterns. The fact that phrases are clustered is a simple way of dealing with
data sparseness. In fact, our experiments in section 5.1.2 will show the usefulness
of our (manual) semantic clusters: The clustered list performs much better than the
unclustered list (i.e., when the string itself is used as a value instead of its semantic
class).
Agent. Agents and actions are more challenging to recognize. We use a mechanism
that, dependent on the voice of a sentence, recognizes agents (subjects or prepositional
phrases headed by by) and their predicates (?actions?). Classification of agents and
actions relies on a manually created lexicon of manual classes. As in the Formulaic
feature, similar agents and actions are generalized and clustered together to avoid data
sparseness.
429
Teufel and Moens Summarizing Scientific Articles
Table 5
Formulaic expression lexicon.
Indicator Type Example Number
GAP INTRODUCTION to our knowledge 3
GENERAL FORMULAIC in traditional approaches 10
DEIXIS in this paper 11
SIMILARITY similar to 56
COMPARISON when compared to our 204
CONTRAST however 6
DETAIL this paper has also 4
METHOD a novel method for VERB?ing 33
PREVIOUS CONTEXT elsewhere, we have 25
FUTURE avenue for improvement 16
AFFECT hopefully 4
CONTINUATION following the argument in 19
IN ORDER TO in order to 1
POSITIVE ADJECTIVE appealing 68
NEGATIVE ADJECTIVE unsatisfactory 119
THEM FORMULAIC along the lines of 6
TEXTSTRUCTURE in section 3 16
NO TEXTSTRUCTURE described in the last section 43
Total of 18 classes 644
Table 6
Agent lexicon.
Agent Type Example Number Removed
US AGENT we 22
THEM AGENT his approach 21
GENERAL AGENT traditional methods 20 X
US PREVIOUS AGENT the approach in SELFCITE 7
OUR AIM AGENT the point of this study 23
REF US AGENT this method (this WORK NOUN) 6
REF AGENT the paper 11 X
THEM PRONOUN AGENT they 1 X
AIM REF AGENT its goal 8
GAP AGENT none of these papers 8
PROBLEM AGENT these drawbacks 3 X
SOLUTION AGENT a way out of this dilemma 4 X
TEXTSTRUCTURE AGENT the concluding chapter 33
Total of 13 classes 167
The lexicon for agent patterns (cf. Table 6) contains 13 types of agents and a total
of 167 patterns. These 167 patterns expand to many more strings as we use a replace
mechanism (e.g., the placeholder WORK NOUN in the sixth row of Table 6 can be
replaced by a set of 37 nouns including theory, method, prototype, algorithm).
The main three agent types we distinguish are US AGENT, THEM AGENT, and GEN-
ERAL AGENT, following the types of intellectual attribution discussed above. A fourth
type is US PREVIOUS AGENT (the authors, but in a previous paper).
Additional agent types include nonpersonal agents like aims, problems, solu-
tions, absence of solution, or textual segments. There are four equivalence classes of
430
Computational Linguistics Volume 28, Number 4
agents with ambiguous reference (?this system?): REF AGENT, REF US AGENT, THEM PRO-
NOUN AGENT, and AIM REF AGENT.
Agent classes were created based on intuition, but subsequently each class was
tested with corpus statistics to determine whether it should be removed or not. We
wanted to find and exclude classes that had a distribution very similar to the overall
distribution of the target categories, as such features are not distinctive. We measured
associations using the log-likelihood measure (Dunning 1993) for each combination of
target category and semantic class by converting each cell of the contingency into a 2?2
contingency table. We kept only classes of verbs in which at least one category showed
a high association (gscore > 5.0), as that means that in these cases the distribution was
significantly different from the overall distribution. The last column in Table 6 shows
that the classes THEM PRONOUN, GENERAL, SOLUTION, PROBLEM, and REF were removed;
removal improved the performance of the Agent feature.
SegAgent. SegAgent is a variant of the Agent feature that keeps track of previously
recognized agents; unmarked sentences receive these previous agents as a value (in
the Agent feature, they would have received the value None).
Action. We use a manually created action lexicon containing 365 verbs (cf. Table 7).
The verbs are clustered into 20 classes based on semantic concepts such as similarity,
contrast, competition, presentation, argumentation, and textual structure. For exam-
ple, PRESENTATION ACTIONs include communication verbs like present, report, and state
(Myers 1992; Thompson and Yiyun 1991), RESEARCH ACTIONS include analyze, conduct,
define and observe, and ARGUMENTATION ACTIONS include argue, disagree, and object to.
Domain-specific actions are contained in the classes indicating a problem (fail, degrade,
waste, overestimate) and solution-contributing actions (circumvent, solve, mitigate). The
Table 7
Action lexicon.
Action Type Example Number Removed
AFFECT we hope to improve our results 9 X
ARGUMENTATION we argue against a model of 19 X
AWARENESS we are not aware of attempts 5 +
BETTER SOLUTION our system outperforms . . . 9 ?
CHANGE we extend CITE?s algorithm 23
COMPARISON we tested our system against . . . 4
CONTINUATION we follow CITE . . . 13
CONTRAST our approach differs from . . . 12 ?
FUTURE INTEREST we intend to improve . . . 4 X
INTEREST we are concerned with . . . 28
NEED this approach, however, lacks . . . 8 X
PRESENTATION we present here a method for . . . 19 ?
PROBLEM this approach fails . . . 61 ?
RESEARCH we collected our data from . . . 54
SIMILAR our approach resembles that of 13
SOLUTION we solve this problem by . . . 64
TEXTSTRUCTURE the paper is organized . . . 13
USE we employ CITE?s method . . . 5
COPULA our goal is to . . . 1
POSSESSION we have three goals . . . 1
Total of 20 classes 365
431
Teufel and Moens Summarizing Scientific Articles
recognition of negation is essential; the semantics of not solving is closer to being prob-
lematic than it is to solving.
The following classes were removed by the gscore test described above, because
their distribution was too similar to the overall distribution: FUTURE INTEREST, NEED,
ARGUMENTATION, AFFECT in both negative and positive contexts (X in last column of
Table 7), and AWARENESS only in positive context (+ in last column). The following
classes had too few occurrences in negative context (< 10 occurrences in the whole verb
class) and thus the negative context of the class was also removed: BETTER SOLUTION,
CONTRAST, PRESENTATION, PROBLEM (? in last column). Again, the removal improved
the performance of the Action feature.
The algorithm for determining agents and actions relies on finite-state patterns
over part-of-speech (POS) tags. Starting from each finite verb, the algorithm collects
chains of auxiliaries belonging to the associated finite clause and thus determines the
clause?s tense and voice. Other finite verbs and commas are assumed to be clause
boundaries. Once the semantic verb is found, its stem is looked up in the action
lexicon. Negation is determined if one of 32 fixed negation words is present in a
six-word window to the right of the finite verb.
As our classifier requires one unique value for each classified item for each feature,
we had to choose one value for sentences containing more than one finite clause. We
return the following values for the action and agents feature: the first agent/action
pair, if both are nonzero, otherwise the first agent without an action, otherwise the
first action without an agent, if available.
In order to determine the level of correctness of agent and action recognition, we
had first to evaluate manually the error level of the POS tagging of finite verbs, as our
algorithm crucially relies on finite verbs. In a random sample of 100 sentences from
our development corpus that contain any finite verbs at all (they happened to contain
a total of 184 finite verbs), the tagger (which is part of the TTT software) showed a
recall of 95% and a precision of 93%.
We found that for the 174 correctly determined finite verbs, the heuristics for
negation and presence of modal auxiliaries worked without any errors (100% accu-
racy, eight negated sentences). The correct semantic verb was determined with 96%
accuracy; most errors were due to misrecognition of clause boundaries. Action Type
lookup was fully correct (100% accuracy), even in the case of phrasal verbs and longer
idiomatic expressions (have to is a NEED ACTION; be inspired by is a CONTINUE ACTION).
There were seven voice errors, two of which were due to POS-tagging errors (past par-
ticiple misrecognized). The remaining five voice errors correspond to 98% accuracy.
Correctness of Agent Type determination was tested on a random sample of 100
sentences containing at least one agent, resulting in 111 agents. No agent pattern that
should have been identified was missed (100% recall). Of the 111 agents, 105 cases were
correct (precision of 95%). Therefore, we consider the two features to be adequately
robust to serve as sentential features in our system.
Having detailed the features and classifiers of the machine learning system we
use, we will now turn to an intrinsic evaluation of its performance.
5. Intrinsic System Evaluation
Our task is to perform content selection from scientific articles, which we do by clas-
sifying sentences into seven rhetorical categories. The summaries based on this classi-
fication use some of these sentences directly, namely, sentences that express the con-
tribution of a particular article (AIM), sentences expressing contrasts with other work
(CONTRAST), and sentences stating imported solutions from other work (BASIS). Other,
432
Computational Linguistics Volume 28, Number 4
more frequent rhetorical categories, namely OTHER, OWN, and BACKGROUND, might
also be extracted into the summary.
Because the task is a mixture of extraction and classification, we report system
performance as follows:
? We first report precision and recall values for all categories, in
comparison to human performance and the text categorization baseline,
as we are primarily interested in good performance on the categories
AIM, CONTRAST, BASIS, and BACKGROUND.
? We are also interested in good overall classification performance, which
we report using kappa and macro-F as our metric. We also discuss how
well each single features does in the classification.
? We then compare the extracted sentences to our human gold standard
for relevance and report the agreement in precision and agreement per
category.
5.1 Determination of Rhetorical Status
The results of stochastic classification were compiled with a 10-fold cross-validation on
our 80-paper corpus. As we do not have much annotated material, cross-validation is
a practical way to test as it can make use of the full development corpus for training,
without ever using the same data for training and testing.
5.1.1 Overall Results. Table 8 and Figure 11 show that the stochastic model obtains
substantial improvement over the baseline in terms of precision and recall of the im-
portant categories AIM, BACKGROUND, CONTRAST, and BASIS. We use the F-measure,
defined by van Rijsbergen (1979) as 2PRP+R , as a convenient way of reporting precision (P)
and recall (R) in one value. F-measures for our categories range from .61 (TEXTUAL)
and .52 (AIM) to .45 (BACKGROUND), .38 (BASIS), and .26 (CONTRAST). The recall for
some categories is relatively low. As our gold standard is designed to contain a lot
of redundant information for the same category, this is not too worrying. Low pre-
cision in some categories (e.g., 34% for CONTRAST, in contrast to human precision of
50%), however, could potentially present a problem for later steps in the document
summarization process.
Overall, we find these results encouraging, particularly in view of the subjective
nature of the task and the high compression achieved (2% for AIM, BASIS, and TEXTUAL
sentences, 5% for CONTRAST sentences, and 6% for BACKGROUND sentences). No direct
comparison with Kupiec, Pedersen, and Chen?s results is possible as different data
sets are used and as Kupiec et al?s relevant sentences do not directly map into one
of our categories. Assuming, however, that their relevant sentences are probably most
Table 8
Performance per category: F-measure (F), precision (P) and recall (R).
AIM CONTR. TEXTUAL OWN BACKG. BASIS OTHER
F P R F P R F P R F P R F P R F P R F P R
System 52 44 65 26 34 20 61 57 66 86 84 88 45 40 50 38 37 40 44 52 39
Baseline 11 30 7 17 31 12 23 56 15 83 78 90 22 32 17 7 15 5 44 47 42
Humans 63 72 56 52 50 55 79 79 79 93 94 92 71 68 75 48 82 34 78 74 83
433
Teufel and Moens Summarizing Scientific Articles
Figure 11
Performance per category: F-measure.
Table 9
Confusion matrix: Human versus automatic annotation.
MACHINE
AIM CTR TXT OWN BKG BAS OTH Total
HUMAN AIM 127 6 13 23 19 5 10 203
CTR 21 112 4 204 87 18 126 572
TXT 14 1 145 46 6 2 6 220
OWN 100 108 84 7,231 222 71 424 8,240
BKG 14 31 1 222 370 5 101 744
BAS 17 7 7 60 8 97 39 235
OTH 6 70 10 828 215 72 773 1,974
Total 299 335 264 8,614 927 270 1,479 12,188
comparable to our AIM sentences, our precision and recall of 44% and 65% compare
favorably to theirs (42% and 42%).
Table 9 shows a confusion matrix between one annotator and the system. The
system is likely to confuse AIM and OWN sentences (e.g., 100 out of 172 sentences
incorrectly classified as AIM by the system turned out to be OWN sentences). It also
shows a tendency to confuse OTHER and OWN sentences. The system also fails to dis-
tinguish categories involving other people?s work (e.g. OTHER, BASIS, and CONTRAST).
Overall, these tendencies mirror human errors, as can be seen from a comparison with
Table 2.
Table 10 shows the results in terms of three overall measures: kappa, percentage
accuracy, and macro-F (following Lewis [1991]). Macro-F is the mean of the F-measures
of all seven categories. One reason for using macro-F and kappa is that we want to
measure success particularly on the rare categories that are needed for our final task
(i.e., AIM, BASIS, and CONTRAST). Microaveraging techniques like traditional accuracy
tend to overestimate the contribution of frequent categories in skewed distributions
like ours; this is undesirable, as OWN is the least interesting category for our purposes.
This situation has parallels in information retrieval, where precision and recall are used
because accuracy overestimates the performance on irrelevant items.
434
Computational Linguistics Volume 28, Number 4
Table 10
Overall classification results.
System/Baseline Compared with One Human Annotator 3 Humans
System Text Class. Random Random (Distr.) Most Freq.
Kappa .45 .30 ?.10 0 ?.13 .71
Accuracy .73 .72 .14 .48 .67 .87
Macro-F .50 .30 .09 .14 .11 .69
In the case of macro-F, each category is treated as one unit, independent of the
number of items contained in it. Therefore, the classification success of the individual
items in rare categories is given more importance than the classification success of
frequent-category items. When looking at the numerical values, however, one should
keep in mind that macroaveraging results are in general numerically lower (Yang and
Liu 1999). This is because there are fewer training cases for the rare categories, which
therefore perform worse with most classifiers.
In the case of kappa, classifications that incorrectly favor frequent categories are
punished because of a high random agreement. This effect can be shown most easily
when the baselines are considered. The most ambitious baseline we use is the output of
a text categorization system, as described in section 4. Other possible baselines, which
are all easier to beat, include classification by the most frequent category. This baseline
turns out to be trivial, as it does not extract any of the rare rhetorical categories in which
we are particularly interested, and therefore receives a low kappa value at K = ?.12.
Possible chance baselines include random annotation with uniform distribution (K =
?.10; accuracy of 14%) and random annotation with observed distribution. The latter
baseline is built into the definition of kappa (K = 0; accuracy of 48%).
Although our system outperforms an ambitious baseline (macro-F shows that our
system performs roughly 20% better than text classification) and also performs much
above chance, there is still a big gap in performance between humans and machine.
Macro-F shows a 20% difference between our system and human performance. If
the system is put into a pool of annotators for the 25 articles for which three-way
human judgment exists, agreement drops from K = .71 to K = .59. This is a clear
indication that the system?s annotation is still distinguishably different from human
annotation.
5.1.2 Feature Impact. The previous results were compiled using all features, which is
the optimal feature combination (as determined by an exhaustive search in the space
of feature combinations). The most distinctive single feature is Location (achieving an
agreement of K = .22 against one annotator, if this feature is used as the sole feature),
followed by SegAgent (K = .19), Citations (K = .18), Headlines (K = .17), Agent
(K = .08), and Formulaic (K = .07). In each case, the unclustered versions of Agent,
SegAgent, and Formulaic performed much worse than the clustered versions; they
did not improve final results when added into the feature pool.
Action performs slightly better at K = ?.11 than the baseline by most frequent
category, but far worse than random by observed distribution. The following features
on their own classify each sentence as OWN (and therefore achieve K = ?.12): Relative
Location, Paragraphs, TF*IDF, Title, Sentence Length, Modality, Tense, and
Voice. History performs very badly on its own at K = ?.51; it classifies almost all
sentences as BACKGROUND. It does this because the probability of the first sentence?s
435
Teufel and Moens Summarizing Scientific Articles
being a BACKGROUND sentence is almost one, and, if no other information is available,
it is very likely that another BACKGROUND sentence will follow after a BACKGROUND
sentence.
Each of these features, however, still contributes to the final result: If any of them
is taken out of the feature pool, classification performance decreases. How can this be,
given that the individual features perform worse than chance? As the classifier de-
rives the posterior probability by multiplying evidence from each feature, even slight
evidence coming from one feature can direct the decision in the right direction. A
feature that contributes little evidence on its own (too little to break the prior prob-
ability, which is strongly biased toward OWN) can thus, in combination with others,
still help in disambiguating. For the na??ve Bayesian classification method, indeed, it
is most important that the features be as independent of each other as possible. This
property cannot be assessed by looking at the feature?s isolated performance, but only
in combination with others.
It is also interesting to see that certain categories are disambiguated particularly
well by certain features (cf. Table 11). The Formulaic feature, which is by no means
the strongest feature, is nevertheless the most diverse, as it contributes to the disam-
biguation of six categories directly. This is because many different rhetorical categories
have typical cue phrases associated with them (whereas not all categories have a pre-
ferred location in the document). Not surprisingly, Location and History are the
features particularly useful for detecting BACKGROUND sentences, and SegAgent addi-
tionally contributes toward the determination of BACKGROUND zones (along with the
Formulaic and the Absolute Location features). The Agent and Action features also
prove their worth as they manage to disambiguate categories that many of the other
features alone cannot disambiguate (e.g., CONTRAST).
5.1.3 System Output: The Example Paper. In order to give the reader an impression
of how the figures reported in the previous section translate into real output, we
present in figure 12 the output of the system when run on the example paper (all
AIM, CONTRAST, and BASIS sentences). The second column shows whether the human
judge agrees with the system?s decision (a tick for correct decisions, and the human?s
preferred category for incorrect decisions). Ten out of the 15 extracted sentences have
been classified correctly.
The example also shows that the determination of rhetorical status is not always
straightforward. For example, whereas the first AIM sentence that the system proposes
(sentence 8) is clearly wrong, all other ?incorrect? AIM sentences carry important in-
Table 11
Precision and recall of rhetorical classification, individual features.
Features Precision/Recall per Category (in %)
AIM CONTR. TXT. OWN BACKG. BASIS OTHER
SegAgent alone ? 17/0 ? 74/94 53/16 ? 46/33
Agent alone ? ? ? 71/93 ? ? 36/23
Location alone ? ? ? 74/97 40/36 ? 28/9
Headlines alone ? ? ? 75/95 ? ? 29/25
Citation alone ? ? ? 73/96 ? ? 43/30
Formulaic alone 40/2 45/2 75/39 71/98 ? 40/1 47/13
Action alone ? 43/1 ? 68/99 ? ? ?
History alone ? ? ?- 70/8 16/99 ? ?
436
Computational Linguistics Volume 28, Number 4
System Human
AIM (OTH) 8 In Hindle?s proposal, words are similar if we have strong statistical
evidence that they tend to participate in the same events.?
* 10 Our research addresses some of the same questions and uses similar raw
data, but we investigate how to factor word association tendencies into
associations of words to certain hidden senses classes and associations
between the classes themselves.?
11 While it may be worthwhile to base such a model on preexisting sense
classes (Resnik, 1992), in the work described here we look at how to
derive the classes directly from distributional data.
(OWN) 12 More specifically, we model senses as probabilistic concepts or clusters
c with corresponding cluster membership probabilities EQN for each
word w.?
* 22 We will consider here only the problem of classifying nouns according
to their distribution as direct objects of verbs; the converse problem is
formally similar.
(CTR) 41 However, this is not very satisfactory because one of the goals of our
work is precisely to avoid the problems of data sparseness by grouping
words into classes.
(OWN) 150 We also evaluated asymmetric cluster models on a verb decision task
closer to possible applications to disambiguation in language analysis.?
* 162 We have demonstrated that a general divisive clustering procedure for
probability distributions can be used to group words according to their
participation in particular grammatical relations with other words.
BAS
?
19 The corpus used in our first experiment was derived from newswire text
automatically parsed by Hindle?s parser Fidditch (Hindle, 1993).?
20 More recently, we have constructed similar tables with the help of a
statistical part-of-speech tagger (Church, 1988) and of tools for regular
expression pattern matching on tagged corpora (Yarowsky, 1992).?
* 113 The analogy with statistical mechanics suggests a deterministic anneal-
ing procedure for clustering (Rose et al, 1990), in which the number
of clusters is determined through a sequence of phase transitions by
continuously increasing the parameter EQN following an annealing
schedule.
CTR
?
* 9 His notion of similarity seems to agree with our intuitions in many
cases, but it is not clear how it can be used directly to construct word
classes and corresponding models of association.?
* 14 Class construction is then combinatorially very demanding and de-
pends on frequency counts for joint events involving particular words,
a potentially unreliable source of information as we noted above.
(OWN) 21 We have not yet compared the accuracy and coverage of the two methods,
or what systematic biases they might introduce, although we took care
to filter out certain systematic errors, for instance the misparsing of the
subject of a complement clause as the direct object of a main verb for
report verbs like ?say?.?
43 This is a useful advantage of our method compared with agglomerative
clustering techniques that need to compare individual objects being
considered for grouping.
Figure 12
System output for example paper.
formation about research goals of the paper: Sentence 41 states the goal in explicit
terms, but it also contains a contrastive statement, which the annotator decided to rate
higher than the goal statement. Both sentences 12 and 150 give high-level descrip-
tions of the work that might pass as a goal statement. Similarly, in sentence 21 the
agent and action features detected that the first part of the sentence has something to
do with comparing methods, and the system then (plausibly but incorrectly) decided
437
Teufel and Moens Summarizing Scientific Articles
to classify the sentence as CONTRAST. All in all, we feel that the extracted material
conveys the rhetorical status adequately. An extrinsic evaluation additionally showed
that the end result provides considerable added value when compared to sentence
extracts (Teufel 2001).
5.2 Relevance Determination
The classifier for rhetorical status that we evaluated in the previous section is an im-
portant first step in our implementation; the next step is the determination of relevant
sentences in the text. One simple solution for relevance decision would be to use all
AIM, BASIS, and CONTRAST sentences, as these categories are rare overall. The classifier
we use has the nice property of roughly keeping the distribution of target categories,
so that we end up with a sensible number of these sentences.
The strategy of using all AIM, CONTRAST, and BASIS sentences can be evaluated
in a similar vein to the previous experiment. In terms of relevance, the asterisk in
figure 12 marks sentences that the human judge found particularly relevant in the
overall context (cf. the full set in figure 5). Six out of all 15 sentences, and 6 out of
the 10 sentences that received the correct rhetorical status, were judged relevant in the
example.
Table 12 reports the figure for the entire corpus by comparing the system?s output
of correctly classified rhetorical categories to human judgment. In all cases, the results
are far above the nontrivial baseline. On AIM, CONTRAST, and BASIS sentences, our
system achieves very high precision values of 96%, 70%, and 71%. Recall is lower at
70%, 24%, and 39%, but low recall is less of a problem in our final task. Therefore,
the main bottleneck is correct rhetorical classification. Once that is accomplished, the
selected categories show high agreement with human judgment and should therefore
represent good material for further processing steps.
If, however, one is also interested in selecting BACKGROUND sentences, as we are,
simply choosing all BACKGROUND sentences would result in low precision of 16%
(albeit with a high recall of 83%), which does not seem to be the optimal solution.
We therefore use a second classifier for finding the most relevant sentences indepen-
dently that was trained on the relevance gold standard. Our best classifier operates
at a precision of 46.5% and recall of 45.2% (using the features Location, Section
Struct, Paragraph Struct, Title, TF*IDF, Formulaic, and Citation for classifi-
cation). The second classifier (cf. rightmost columns in figure 12) raises the preci-
sion for BACKGROUND sentences from 16% to 38%, while keeping recall high at 88%.
This example shows that the right procedure for relevance determination changes
from category to category and also depends on the final task one is trying
to accomplish.
Table 12
Relevance by human selection: Precision (P) and recall (R).
AIM CONTR. BASIS BACKGROUND
Without With
Classifier Classifier
P R P R P R P R P R
System 96.2 69.8 70.1 23.8 70.5 39.4 16.0 83.3 38.4 88.2
Baseline 26.1 6.4 23.5 14.4 6.94 2.7 0.0 0.0 0.0 0.0
438
Computational Linguistics Volume 28, Number 4
6. Discussion
6.1 Contribution
We have presented a new method for content selection from scientific articles. The anal-
ysis is genre-specific; it is based on rhetorical phenomena specific to academic writing,
such as problem-solution structure, explicit intellectual attribution, and statements of
relatedness to other work. The goal of the analysis is to identify the contribution of
an article in relation to background material and to other specific current work.
Our methodology is situated between text extraction methods and fact extraction
(template-filling) methods: Although our analysis has the advantage of being more
context-sensitive than text extraction methods, it retains the robustness of this approach
toward different subdomains, presentational traditions, and writing styles.
Like fact extraction methods (e.g., Radev and McKeown 1998), our method also
uses a ?template? whose slots are being filled during analysis. The slots of our template
are defined as rhetorical categories (like ?Contrast?) rather than by domain-specific
categories (like ?Perpetrator?). This makes it possible for our approach to deal with
texts of different domains and unexpected topics.
Sparck Jones (1999) argues that it is crucial for a summarization strategy to relate
the large-scale document structure of texts to readers? tasks in the real world (i.e., to
the proposed use of the summaries). We feel that incorporating a robust analysis of
discourse structure into a document summarizer is one step along this way.
Our practical contributions are twofold. First, we present a scheme for the anno-
tation of sentences with rhetorical status, and we have shown that the annotation is
stable (K = .82, .81, .76) and reproducible (K = .71). Since these results indicate that
the annotation is reliable, we use it as our gold standard for evaluation and training.
Second, we present a machine learning system for the classification of sentences by
relevance and by rhetorical status. The contribution here is not the statistical classifier,
which is well-known and has been used in a similar task by Kupiec, Pedersen, and
Oren (1995), but instead the features we use. We have adapted 13 sentential features
in such a way that they work robustly for our task (i.e., for unrestricted, real-world
text). We also present three new features that detect scientific metadiscourse in a novel
way. The results of an intrinsic system evaluation show that the system can identify
sentences expressing the specific goal of a paper with 57% precision and 79% recall,
sentences expressing criticism or contrast with 57% precision and 42% recall, and
sentences expressing a continuation relationship to other work with 62% precision
and 43% recall. This substantially improves a baseline of text classification which uses
only a TF*IDF model over words. The agreement of correctly identified rhetorical roles
with human relevance judgments is even higher (96% precision and 70% recall for goal
statements, 70% precision and 24% recall for contrast, 71% precision and 39% recall for
continuation). We see these results as an indication that shallow discourse processing
with a well-designed set of surface-based indicators is possible.
6.2 Limitations and Future Work
The metadiscourse features, one focus of our work, currently depend on manual re-
sources. The experiments reported here explore whether metadiscourse information is
useful for the automatic determination of rhetorical status (as opposed to more shallow
features), and this is clearly the case. The next step, however, should be the automatic
creation of such resources. For the task of dialogue act disambiguation, Samuel, Car-
berry, and Vijay-Shanker (1999) suggest a method of automatically finding cue phrases
for disambiguation. It may be possible to apply this or a similar method to our data
and to compare the performance of automatically gained resources with manual ones.
439
Teufel and Moens Summarizing Scientific Articles
Further work can be done on the semantic verb clusters described in section 4.2.
Klavans and Kan (1998), who use verb clusters for document classification according to
genre, observe that verb information is rarely used in current practical natural language
applications. Most tasks such as information extraction and document classification
identify and use nominal constructs instead (e.g., noun phrases, TF*IDF words and
phrases).
The verb clusters we employ were created using our intuition of which type of
verb similarity would be useful in the genre and for the task. There are good reasons
for using such a hand-crafted, genre-specific verb lexicon instead of a general resource
such as WordNet or Levin?s (1993) classes: Many verbs used in the domain of scien-
tific argumentation have assumed a specialized meaning, which our lexicon readily
encodes. Klavans and Kan?s classes, which are based on Levin?s classes, are also man-
ually created. Resnik and Diab (2000) present yet other measures of verb similarity,
which could be used to arrive at a more data-driven definition of verb classes. We
are currently comparing our verb clusterings to Klavans and Kan?s, and to bottom-up
clusters of verb similarities generated from our annotated data.
The recognition of agents, which is already the second-best feature in the pool,
could be further improved by including named entity recognition and anaphora res-
olution. Named entity recognition would help in cases like the following,
LHIP provides a processing method which allows selected portions of the input
to be ignored or handled differently. (S-5, 9408006)
where LHIP is the name of the authors? approach and should thus be tagged as
US AGENT; to do so, however, one would need to recognize it as a named approach,
which is associated with the authors. It is very likely that such a treatment, which
would have to include information from elsewhere in the text, would improve re-
sults, particularly as named approaches are frequent in the computational linguistics
domain. Information about named approaches in themselves would also be an impor-
tant aspect to include in summaries or citation indexes.
Anaphora resolution helps in cases in which the agent is syntactically ambiguous
between own and other approaches (e.g., this system). To test whether and how much
performance would improve, we manually simulated anaphora resolution on the 632
occurrences of REF AGENT in the development corpus. (In the experiments in section 5
these occurrences had been excluded from the Agent feature by giving them the value
None; we include them now in their disambiguated state). Of the 632 REF AGENTs,
436 (69%) were classified as US AGENT, 175 (28%) as THEM AGENT, and 20 (3%) as
GENERAL AGENT. As a result of this manual disambiguation, the performance of the
Agent feature increased dramatically from K = .08 to K = .14 and that of SegAgent
from K = .19 to K = .22. This is a clear indication of the potential added value of
anaphora resolution for our task.
As far as the statistical classification is concerned, our results are still far from
perfect. Obvious ways of improving performance are the use of a more sophisticated
statistical classifier and more training material. We have experimented with a maxi-
mum entropy model, Repeated Incremental Pruning to Produce Error Reduction (RIP-
PER), and decision trees; preliminary results do not show significant improvement
over the na??ve Bayesian model. One problem is that 4% of the sentences in our cur-
rent annotated material are ambiguous: They receive the same feature representation
but are classified differently by the annotators. A possible solution is to find better
and more distinctive features; we believe that robust, higher-level features like actions
and agents are a step in the right direction. We also suspect that a big improvement
440
Computational Linguistics Volume 28, Number 4
could be achieved with smaller annotation units. Many errors come from instances in
which one half of a sentence serves one rhetorical purpose, the other another, as in
the following example:
The current paper shows how to implement this general notion, without fol-
lowing Krifka?s analysis in detail. (S-10, 9411019)
Here, the first part describes the paper?s research goal, whereas the second expresses a
contrast. Currently, one target category needs to be associated with the whole sentence
(according to a rule in the guidelines, AIM is given preference over CONTRAST). As
an undesired side effect, the CONTRAST-like textual parts (and the features associated
with this text piece, e.g., the presence of an author?s name) are wrongly associated
with the AIM target category. If we allowed for a smaller annotation unit (e.g., at the
clause level), this systematic noise in the training data could be removed.
Another improvement in classification accuracy might be achieved by performing
the classification in a cascading way. The system could first perform a classification
into OWN-like classes (OWN, AIM, and TEXTUAL pooled), OTHER-like categories (OTHER,
CONTRAST, and BASIS pooled), and BACKGROUND, similar to the way human annotation
proceeds. Subclassification among these classes would then lead to the final seven-way
classification.
Appendix: List of articles in CL development corpus
No. Title, Conference, Authors
0 9405001 Similarity-Based Estimation of Word Cooccurrence Probabilities (ACL94), I. Dagan et al
1 9405002 Temporal Relations: Reference or Discourse Coherence? (ACL94 Student), A. Kehler
2 9405004 Syntactic-Head-Driven Generation (COLING94), E. Koenig
3 9405010 Common Topics and Coherent Situations: Interpreting Ellipsis in the Context of Dis-
course Inference (ACL94), A. Kehler
4 9405013 Collaboration on Reference to Objects That Are Not Mutually Known (COLING94),
P. Edmonds
5 9405022 Grammar Specialization through Entropy Thresholds (ACL94), C. Samuelsson
6 9405023 An Integrated Heuristic Scheme for Partial Parse Evaluation (ACL94 Student), A. Lavie
7 9405028 Semantics of Complex Sentences in Japanese (COLING94), H. Nakagawa, S. Nishizawa
8 9405033 Relating Complexity to Practical Performance in Parsing with Wide-Coverage Unifica-
tion Grammars (ACL94), J. Carroll
9 9405035 Dual-Coding Theory and Connectionist Lexical Selection (ACL94 Student), Y. Wang
10 9407011 Discourse Obligations in Dialogue Processing (ACL94), D. Traum, J. Allen
11 9408003 Typed Feature Structures as Descriptions (COLING94 Reserve), P. King
12 9408004 Parsing with Principles and Probabilities (ACL94 Workshop), A. Fordham, M. Crocker
13 9408006 LHIP: Extended DCGs for Configurable Robust Parsing (COLING94), A. Bal-
lim, G. Russell
14 9408011 Distributional Clustering of English Words (ACL93), F. Pereira et al
15 9408014 Qualitative and Quantitative Models of Speech Translation (ACL94 Workshop),
H. Alshawi
16 9409004 An Experiment on Learning Appropriate Selectional Restrictions from a Parsed Corpus
(COLING94), F. Ribas
17 9410001 Improving Language Models by Clustering Training Sentences (ANLP94), D. Carter
18 9410005 A Centering Approach to Pronouns (ACL87), S. Brennan et al
19 9410006 Evaluating Discourse Processing Algorithms (ACL89), M. Walker
20 9410008 Recognizing Text Genres with Simple Metrics Using Discriminant Analysis (COLING94),
J. Karlgren, D. Cutting
21 9410009 Reserve Lexical Functions and Machine Translation (COLING94), D. Heylen et al
22 9410012 Does Baum-Welch Re-estimation Help Taggers? (ANLP94), D. Elworthy
23 9410022 Automated Tone Transcription (ACL94 SIG), S. Bird
24 9410032 Planning Argumentative Texts (COLING94), X. Huang
25 9410033 Default Handling in Incremental Generation (COLING94), K. Harbusch et al
26 9411019 Focus on ?Only? and ?Not? (COLING94), A. Ramsay
27 9411021 Free-Ordered CUG on Chemical Abstract Machine (COLING94), S. Tojo
441
Teufel and Moens Summarizing Scientific Articles
28 9411023 Abstract Generation Based on Rhetorical Structure Extraction (COLING94), K. Ono et
al.
29 9412005 Segmenting Speech without a Lexicon: The Roles of Phonotactics and Speech Source
(ACL94 SIG), T. Cartwright, M. Brent
30 9412008 Analysis of Japanese Compound Nouns Using Collocational Information (COLING94),
Y. Kobayasi et al
31 9502004 Bottom-Up Earley Deduction (COLING94), G. Erbach
32 9502005 Off-Line Optimization for Earley-Style HPSG Processing (EACL95), G. Minnen et al
33 9502006 Rapid Development of Morphological Descriptions for Full Language Processing Sys-
tems (EACL95), D. Carter
34 9502009 On Learning More Appropriate Selectional Restrictions (EACL95), F. Ribas
35 9502014 Ellipsis and Quantification: A Substitutional Approach (EACL95), R. Crouch
36 9502015 The Semantics of Resource Sharing in Lexical-Functional Grammar (EACL95), A. Kehler
et al
37 9502018 Algorithms for Analysing the Temporal Structure of Discourse (EACL95), J. Hitzeman
et al
38 9502021 A Tractable Extension of Linear Indexed Grammars (EACL95), B. Keller, D. Weir
39 9502022 Stochastic HPSG (EACL95), C. Brew
40 9502023 Splitting the Reference Time: Temporal Anaphora and Quantification in DRT (EACL95),
R. Nelken, N. Francez
41 9502024 A Robust Parser Based on Syntactic Information (EACL95), K. Lee et al
42 9502031 Cooperative Error Handling and Shallow Processing (EACL95 Student), T. Bowden
43 9502033 An Algorithm to Co-ordinate Anaphora Resolution and PPS Disambiguation Process
(EACL95 Student), S. Azzam
44 9502035 Incorporating ?Unconscious Reanalysis? into an Incremental, Monotonic Parser
(EACL95 Student), P. Sturt
45 9502037 A State-Transition Grammar for Data-Oriented Parsing (EACL95 Student), D. Tugwell
46 9502038 Implementation and Evaluation of a German HMM for POS Disambiguation (EACL95
Workshop), H. Feldweg
47 9502039 Multilingual Sentence Categorization According to Language (EACL95 Workshop),
E. Giguet
48 9503002 Computational Dialectology in Irish Gaelic (EACL95), B. Kessler
49 9503004 Creating a Tagset, Lexicon and Guesser for a French Tagger (EACL95 Workshop),
J. Chanod, P. Tapanainen
50 9503005 A Specification Language for Lexical Functional Grammars (EACL95), P. Blackburn,
C. Gardent
51 9503007 The Semantics of Motion (EACL95), P. Sablayrolles
52 9503009 Distributional Part-of-Speech Tagging (EACL95), H. Schuetze
53 9503013 Incremental Interpretation: Applications, Theory, and Relationship to Dynamic Seman-
tics (COLING95), D. Milward, R. Cooper
54 9503014 Non-constituent Coordination: Theory and Practice (COLING94), D. Milward
55 9503015 Incremental Interpretation of Categorial Grammar (EACL95), D. Milward
56 9503017 Redundancy in Collaborative Dialogue (COLING92), M. Walker
57 9503018 Discourse and Deliberation: Testing a Collaborative Strategy (COLING94), M. Walker
58 9503023 A Fast Partial Parse of Natural Language Sentences Using a Connectionist Method
(EACL95), C. Lyon, B. Dickerson
59 9503025 Occurrence Vectors from Corpora vs. Distance Vectors from Dictionaries (COLING94),
Y. Niwa, Y. Nitta
60 9504002 Tagset Design and Inflected Languages (EACL95 Workshop), D. Elworthy
61 9504006 Cues and Control in Expert-Client Dialogues (ACL88), S. Whittaker, P. Stenton
62 9504007 Mixed Initiative in Dialogue: An Investigation into Discourse Segmentation (ACL90),
M. Walker, S. Whittaker
63 9504017 A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and
Sequences of Utterances (ACL95), D. Marcu, G. Hirst
64 9504024 A Morphographemic Model for Error Correction in Nonconcatenative Strings (ACL95),
T. Bowden, G. Kiraz
65 9504026 The Intersection of Finite State Automata and Definite Clause Grammars (ACL95),
G. van Noord
66 9504027 An Efficient Generation Algorithm for Lexicalist MT (ACL95), V. Poznanski et al
67 9504030 Statistical Decision-Tree Models for Parsing (ACL95), D. Magerman
68 9504033 Corpus Statistics Meet the Noun Compound: Some Empirical Results (ACL95), M. Lauer
79 9504034 Bayesian Grammar Induction for Language Modeling (ACL95), S. Chen
70 9505001 Response Generation in Collaborative Negotiation (ACL95), J. Chu-Carroll, S. Carberry
71 9506004 Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Con-
structs (ACL95), S. Kulick
72 9511001 Countability and Number in Japanese-to-English Machine Translation (COLING94),
F. Bond et al
442
Computational Linguistics Volume 28, Number 4
73 9511006 Disambiguating Noun Groupings with Respect to WordNet Senses (ACL95 Workshop),
P. Resnik
74 9601004 Similarity between Words Computed by Spreading Activation on an English Dictionary
(EACL93), H. Kozima, T. Furugori
75 9604019 Magic for Filter Optimization in Dynamic Bottom-up Processing (ACL96), G. Minnen
76 9604022 Unsupervised Learning of Word-Category Guessing Rules (ACL96), A. Mikheev
77 9605013 Learning Dependencies between Case Frame Slots (COLING96), H. Li, N. Abe
78 9605014 Clustering Words with the MDL Principle (COLING96), H. Li, N. Abe
79 9605016 Parsing for Semidirectional Lambek Grammar is NP-Complete (ACL96), J. Doerre
Acknowledgments
The work reported in this article was
conducted while both authors were in the
HCRC Language Technology Group at the
University of Edinburgh.
The authors would like to thank Jean
Carletta for her help with the experimental
design, Chris Brew for many helpful
discussions, Claire Grover and Andrei
Mikheev for advice on the XML
implementation, and the annotators, Vasilis
Karaiskos and Anne Wilson, for their
meticulous work and criticism, which led to
several improvements in the annotation
scheme. Thanks also to Byron
Georgantopolous, who helped to collect the
first version of the corpus, and to the four
anonymous reviewers.
References
Barzilay, Regina, Michael Collins, Julia
Hirschberg, and Steve Whittaker. 2000.
The rules behind roles. In Proceedings of
AAAI-00.
Barzilay, Regina, Kathleen R. McKeown,
and Michael Elhadad. 1999. Information
fusion in the context of multi-document
summarization. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics (ACL-99),
pages 550?557.
Baxendale, Phyllis B. 1958. Man-made index
for technical literature?An experiment.
IBM Journal of Research and Development,
2(4):354?361.
Biber, Douglas. 1995. Dimensions of Register
Variation: A Cross-Linguistic Comparison.
Cambridge University Press, Cambridge,
England.
Brandow, Ronald, Karl Mitze, and Lisa
F. Rau. 1995. Automatic condensation of
electronic publications by sentence
selection. Information Processing and
Management, 31(5):675?685.
Carletta, Jean. 1996. Assessing agreement on
classification tasks. The kappa statistic.
Computational Linguistics, 22(2):249?254.
Choi, Freddy Y. Y. 2000. Advances in
domain independent linear text
segmentation. In Proceedings of the Sixth
Applied Natural Language Conference
(ANLP-00) and the First Meeting of the North
American Chapter of the Association for
Computational Linguistics (NAACL-00),
pages 26?33.
CMP LG. 1994. The computation and
language e-print archive.
http://xxx.lanl.gov/cmp-lg.
Dunning, Ted. 1993. Accurate methods for
the statistics of surprise and coincidence.
Computational Linguistics, 19(1):61?74.
Edmundson, H. P. 1969. New methods in
automatic extracting. Journal of the
Association for Computing Machinery,
16(2):264?285.
Edmundson, H. P., and Wyllys, R. E. 1961.
Automatic abstracting and
indexing?Survey and recommendations.
Communications of the ACM, 4(5):226?234.
Garfield, Eugene. 1979. Citation Indexing: Its
Theory and Application in Science, Technology
and Humanities. J. Wiley, New York.
Grefenstette, Gregory. 1998. Producing
intelligent telegraphic text reduction to
provide an audio scanning service for the
blind. In D. R. Radev and E. H. Hovy,
editors, Working Notes of the AAAI Spring
Symposium on Intelligent Text
Summarization, pages 111?117.
Grover, Claire, Andrei Mikheev, and Colin
Matheson. 1999. LT TTT version 1.0: Text
tokenisation software. Technical Report,
Human Communication Research Centre,
University of Edinburgh.
http://www.ltg.ed.ac.uk/software/ttt/.
Hearst, Marti A. 1997. Texttiling:
Segmenting text into multi-paragraph
subtopic passages. Computational
Linguistics, 23(1):33?64.
Hyland, Ken. 1998. Persuasion and context:
The pragmatics of academic
metadiscourse. Journal of Pragmatics,
30(4):437?455.
Jing, Hongyan, Regina Barzilay, Kathleen
R. McKeown, and Michael Elhadad. 1998.
Summarization evaluation methods:
Experiments and analysis. In D. R. Radev
and E. H. Hovy, editors, Working Notes of
the AAAI Spring Symposium on Intelligent
443
Teufel and Moens Summarizing Scientific Articles
Text Summarization, pages 60?68.
Jing, Hongyan and Kathleen R. McKeown.
2000. Cut and paste based summarization.
In Proceedings of the Sixth Applied Natural
Language Conference (ANLP-00) and the First
Meeting of the North American Chapter of the
Association for Computational Linguistics
(NAACL-00), pages 178?185.
Jordan, Michael P. 1984. Rhetoric of Everyday
English Texts. Allen and Unwin, London.
Klavans, Judith L. and Min-Yen Kan. 1998.
Role of verbs in document analysis. In
Proceedings of the 36th Annual Meeting of the
Association for Computational Linguistics and
the 17th International Conference on
Computational Linguistics
(ACL/COLING-98), pages 680?686.
Knight, Kevin and Daniel Marcu. 2000.
Statistics-based summarization?Step one:
Sentence compression. In Proceedings of the
17th National Conference of the American
Association for Artificial Intelligence
(AAAI-2000), pages 703?710.
Krippendorff, Klaus. 1980. Content Analysis:
An Introduction to Its Methodology. Sage
Publications, Beverly Hills, CA.
Kupiec, Julian, Jan O. Pedersen, and
Francine Chen. 1995. A trainable
document summarizer. In Proceedings of
the 18th Annual International Conference on
Research and Development in Information
Retrieval (SIGIR-95), pages 68?73.
Lancaster, Frederick Wilfrid. 1998. Indexing
and Abstracting in Theory and Practice.
Library Association, London.
Landis, J. Richard and Gary G. Koch. 1977.
The measurement of observer agreement
for categorical data. Biometrics, 33:159?174.
Lawrence, Steve, C. Lee Giles, and Kurt
Bollacker. 1999. Digital libraries and
autonomous citation indexing. IEEE
Computer, 32(6):67?71.
Levin, Beth. 1993. English Verb Classes and
Alternations. University of Chicago Press,
Chicago.
Lewis, David D. 1991. Evaluating text
categorisation. In Speech and Natural
Language: Proceedings of the ARPA Workshop
of Human Language Technology.
Liddy, Elizabeth DuRoss. 1991. The
discourse-level structure of empirical
abstracts: An exploratory study.
Information Processing and Management,
27(1):55?81.
Lin, Chin-Yew and Eduard H. Hovy. 1997.
Identifying topics by position. In
Proceedings of the Fifth Applied Natural
Language Conference (ANLP-97),
pages 283?290.
Luhn, Hans Peter. 1958. The automatic
creation of literature abstracts. IBM Journal
of Research and Development, 2(2):159?165.
Mani, Inderjeet, Therese Firmin, David
House, Gary Klein, Beth Sundheim, and
Lynette Hirschman. 1999. The TIPSTER
SUMMAC text summarization evaluation.
In Proceedings of the Ninth Meeting of the
European Chapter of the Association for
Computational Linguistics (EACL-99),
pages 77?85.
Mani, Inderjeet, Barbara Gates, and Eric
Bloedorn. 1999. Improving summaries by
revising them. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics (ACL-99),
pages 558?565.
Mann, William C. and Sandra A.
Thompson. 1987. Rhetorical structure
theory: Description and construction of
text structures. In Gerard Kempen, editor,
Natural Language Generation: New Results in
Artificial Intelligence, Psychology, and
Linguistics. Martinus Nijhoff Publishers,
Dordrecht, the Netherlands, pages 85?95.
Marcu, Daniel. 1999. Discourse trees are
good indicators of importance in text. In
I. Mani and M. T. Maybury, editors,
Advances in Automatic Text Summarization.
MIT Press, Cambridge, pages 123?136.
McCallum, Andrew. 1997. Training
algorithms for linear text classifiers. In
Proceedings of the 19th Annual International
Conference on Research and Development in
Information Retrieval (SIGIR-97).
Mizzaro, Stefano. 1997. Relevance: The
whole history. Journal of the American
Society for Information Science,
48(9):810?832.
Morris, Jane and Graeme Hirst. 1991.
Lexical cohesion computed by thesaural
relations as an indicator of the structure of
text. Computational Linguistics, 17(1):21?48.
Myers, Greg. 1992. In this paper we report
. . . ?Speech acts and scientific facts.
Journal of Pragmatics, 17(4):295?313.
Nanba, Hidetsugu and Manabu Okumura.
1999. Towards multi-paper summarization
using reference information. In
Proceedings of IJCAI-99, pages 926?931.
Paice, Chris D. 1990. Constructing literature
abstracts by computer: Techniques and
prospects. Information Processing and
Management, 26(1):171?186.
Paice, Chris D. and A. Paul Jones. 1993. The
identification of important concepts in
highly structured technical papers. In
Proceedings of the 16th Annual International
Conference on Research and Development in
Information Retrieval (SIGIR-93),
pages 69?78.
Pollock, Joseph J. and Antonio Zamora.
1975. Automatic abstracting research at
444
Computational Linguistics Volume 28, Number 4
the chemical abstracts service. Journal of
Chemical Information and Computer Sciences,
15(4):226?232.
Radev, Dragomir R. and Kathleen
R. McKeown. 1998. Generating natural
language summaries from multiple
on-line sources. Computational Linguistics,
24(3):469?500.
Rath, G. J., A. Resnick, and T. R. Savage.
1961. The formation of abstracts by the
selection of sentences. American
Documentation, 12(2):139?143.
Resnik, Philip and Mona Diab. 2000.
Measuring verb similarity. In
Twenty-Second Annual Meeting of the
Cognitive Science Society (COGSCI2000).
Riley, Kathryn. 1991. Passive voice and
rhetorical role in scientific writing. Journal
of Technical Writing and Communication,
21(3):239?257.
Rowley, Jennifer. 1982. Abstracting and
Indexing. Bingley, London.
Saggion, Horacio and Guy Lapalme. 2000.
Selective analysis for automatic
abstracting: Evaluating indicativeness and
acceptability. In Proceedings of
Content-Based Multimedia Information Access
(RIAO), pages 747?764.
Salton, Gerard and Michael J. McGill. 1983.
Introduction to Modern Information Retrieval.
McGraw-Hill, Tokyo.
Samuel, Ken, Sandra Carberry, and
K. Vijay-Shanker. 1999. Automatically
selecting useful phrases for dialogue act
tagging. In Proceedings of the Pacific
Association for Computational Linguistics
(PACLING-99).
Saracevic, Tefko. 1975. Relevance: A review
of and a framework for the thinking on
the notion in information science. Journal
of the American Society for Information
Science, 26(6):321?343.
Siegel, Sidney and N. John Castellan Jr.
1988. Nonparametric Statistics for the
Behavioral Sciences. McGraw-Hill, Berkeley,
CA, second edition.
Sparck Jones, Karen. 1990. What sort of
thing is an AI experiment? In
D. Partridge and Yorick Wilks, editors,
The Foundations of Artificial Intelligence: A
SourceBook. Cambridge University Press,
Cambridge, pages 274?281.
Sparck Jones, Karen. 1999. Automatic
summarising: Factors and directions. In
I. Mani and M. T. Maybury, editors,
Advances in Automatic Text Summarization.
MIT Press, Cambridge, pages 1?12.
Swales, John, 1990. Research articles in
English. In Genre Analysis: English in
Academic and Research Settings. Cambridge
University Press, Cambridge, chapter 7,
pages 110?176.
Teufel, Simone, 1999. Argumentative Zoning:
Information Extraction from Scientific Text.
Ph.D. thesis, School of Cognitive Science,
University of Edinburgh, Edinburgh.
Teufel, Simone. 2001. Task-based evaluation
of summary quality: Describing
relationships between scientific papers. In
Proceedings of NAACL-01 Workshop
?Automatic Text Summarization.?
Teufel, Simone, Jean Carletta, and Marc
Moens. 1999. An annotation scheme for
discourse-level argumentation in research
articles. In Proceedings of the Eighth Meeting
of the European Chapter of the Association for
Computational Linguistics (EACL-99),
pages 110?117.
Teufel, Simone and Marc Moens. 1997.
Sentence extraction as a classification
task. In Proceedings of the ACL/EACL-97
Workshop on Intelligent Scalable Text
Summarization, pages 58?65.
Teufel, Simone and Marc Moens. 2000.
What?s yours and what?s mine:
Determining intellectual attribution in
scientific text. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large
Corpora.
Thompson, Geoff and Ye Yiyun. 1991.
Evaluation in the reporting verbs used in
academic papers. Applied Linguistics,
12(4):365?382.
Tombros, Anastasios, and Mark Sanderson.
1998. Advantages of query biased
summaries. In Proceedings of the 21st
Annual International Conference on Research
and Development in Information Retrieval
(SIGIR-98). Association of Computing
Machinery.
Trawinski, Bogdan. 1989. A methodology
for writing problem-structured abstracts.
Information Processing and Management,
25(6):693?702.
van Dijk, Teun A. 1980. Macrostructures: An
Interdisciplinary Study of Global Structures in
Discourse, Interaction and Cognition.
Lawrence Erlbaum, Hillsdale, NJ.
van Rijsbergen, Cornelis Joost. 1979.
Information Retrieval. Butterworth,
London, second edition.
Wiebe Janyce. 1994. Tracking point of view
in narrative. Computational Linguistics,
20(2):223?287.
445
Teufel and Moens Summarizing Scientific Articles
Yang, Yiming and Xin Liu. 1999. A
re-examination of text categorization
methods. In Proceedings of the 22nd Annual
International Conference on Research and
Development in Information Retrieval
(SIGIR-99), pages 42?49.
Zappen, James P. 1983. A rhetoric for
research in sciences and technologies. In
Paul V. Anderson, R. John Brockman, and
Carolyn R. Miller, editors, New Essays in
Technical and Scientific Communication
Research Theory Practice. Baywood,
Farmingdale, NY, pages 123?138.
Zechner, Klaus. 1995. Automatic text
abstracting by selecting relevant passages.
Master?s thesis, Centre for Cognitive
Science, University of Edinburgh,
Edinburgh.
Ziman, John M. 1969. Information,
communication, knowledge. Nature,
224:318?324.
Scaling Context Space
James R. Curran and Marc Moens
Institute for Communicating and Collaborative Systems
University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW
United Kingdom
  jamesc,marc  @cogsci.ed.ac.uk
Abstract
Context is used in many NLP systems as
an indicator of a term?s syntactic and se-
mantic function. The accuracy of the sys-
tem is dependent on the quality and quan-
tity of contextual information available to
describe each term. However, the quan-
tity variable is no longer fixed by lim-
ited corpus resources. Given fixed train-
ing time and computational resources, it
makes sense for systems to invest time
in extracting high quality contextual in-
formation from a fixed corpus. However,
with an effectively limitless quantity of
text available, extraction rate and repre-
sentation size need to be considered. We
use thesaurus extraction with a range of
context extracting tools to demonstrate the
interaction between context quantity, time
and size on a corpus of 300 million words.
1 Introduction
Context plays an important role in many natural lan-
guage tasks. For example, the accuracy of part of
speech taggers or word sense disambiguation sys-
tems depends on the quality and quantity of con-
textual information these systems can extract from
the training data. When predicting the sense of a
word, for instance, the immediately preceding word
is likely to be more important than the tenth previ-
ous word; similar observations can be made about
POS taggers or chunkers. A crucial part of train-
ing these systems lies in extracting from the data
high-quality contextual information, in the sense of
defining contexts that are both accurate and corre-
lated with the information (the POS tags, the word
senses, the chunks) the system is trying to extract.
The quality of contextual information is often de-
termined by the size of the training corpus: with
less data available, extracting context information
for any given phenomenon becomes less reliable.
However, corpus size is no longer a limiting fac-
tor: whereas up to now people have typically worked
with corpora of around one million words, it has be-
come feasible to build much larger document collec-
tions; for example, Banko and Brill (2001) report on
experiments with a one billion word corpus.
When using a much larger corpus and scaling the
context space, there are, however, other trade-offs to
take into consideration: the size of the corpus may
make it unfeasible to train some systems because of
efficiency issues or hardware costs; it may also result
in an unmanageable expansion of the extracted con-
text information, reducing the performance of the
systems that have to make use of this information.
This paper reports on experiments that try to es-
tablish some of the trade-offs between corpus size,
processing time, hardware costs and the perfor-
mance of the resulting systems. We report on ex-
periments with a large corpus (around 300 mil-
lion words). We trained a thesaurus extraction sys-
tem with a range of context-extracting front-ends to
demonstrate the interaction between context quality,
extraction time and representation size.
2 Automatic Thesaurus Extraction
Thesauri have traditionally been used in informa-
tion retrieval tasks to expand words in queries with
synonymous terms (e.g. Ruge, (1997)). More re-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 231-238.
                         Proceedings of the 40th Annual Meeting of the Association for
cently, semantic resources have also been used in
collocation discovery (Pearce, 2001), smoothing and
model estimation (Brown et al, 1992; Clark and
Weir, 2001) and text classification (Baker and Mc-
Callum, 1998). Unfortunately, thesauri are very ex-
pensive and time-consuming to produce manually,
and tend to suffer from problems of bias, inconsis-
tency, and lack of coverage. In addition, thesaurus
compilers cannot keep up with constantly evolving
language use and cannot afford to build new thesauri
for the many subdomains that information extraction
and retrieval systems are being developed for. There
is a clear need for methods to extract thesauri auto-
matically or tools that assist in the manual creation
and updating of these semantic resources.
Most existing work on thesaurus extraction and
word clustering is based on the general observation
that related terms will appear in similar contexts.
The differences tend to lie in the way ?context? is
defined and in the way similarity is calculated. Most
systems extract co-occurrence and syntactic infor-
mation from the words surrounding the target term,
which is then converted into a vector-space repre-
sentation of the contexts that each target term ap-
pears in (Brown et al, 1992; Pereira et al, 1993;
Ruge, 1997; Lin, 1998b). Other systems take the
whole document as the context and consider term
co-occurrence at the document level (Crouch, 1988;
Sanderson and Croft, 1999). Once these contexts
have been defined, these systems then use clustering
or nearest neighbour methods to find similar terms.
Finally, some systems extract synonyms directly
without extracting and comparing contextual rep-
resentations for each term. Instead, these systems
recognise terms within certain linguistic patterns
(e.g. X, Y and other Zs) which associate synonyms
and hyponyms (Hearst, 1992; Caraballo, 1999).
Thesaurus extraction is a good task to use to ex-
periment with scaling context spaces. The vector-
space model with nearest neighbour searching is
simple, so we needn?t worry about interactions be-
tween the contexts we select and a learning algo-
rithm (such as independence of the features). But
also, thesaurus extraction is a task where success
has been limited when using small corpora (Grefen-
stette, 1994); corpora of the order of 300 million
words have already been shown to be more success-
ful at this task (Lin, 1998b).
3 Experiments
Vector-space thesaurus extraction can be separated
into two independent processes. The first step ex-
tracts the contexts from raw text and compiles them
into a vector-space statistical description of the con-
texts each potential thesaurus term appears in.
We define a context relation as a tuple (w, r,w?)
where w is a thesaurus term, occurring in relation
type r, with another word w? in the sentence. The
type can be grammatical or the position of w? in a
context window: the relation (dog, direct-obj,
walk) indicates that the term dog, was the direct ob-
ject of the verb walk. Often we treat the tuple (r,w?)
as a single unit and refer to it as an attribute of w.
The context extraction systems used for these exper-
iments are described in the following section.
The second step in thesaurus extraction performs
clustering or nearest-neighbour analysis to deter-
mine which terms are similar based on their context
vectors. Our second component is similar to Grefen-
stette?s SEXTANT system, which performs nearest-
neighbour calculations for each pair of potential the-
saurus terms. For nearest-neighbour measurements
we must define a function to judge the similarity be-
tween two context vectors (e.g. the cosine measure)
and a function to combine the raw instance frequen-
cies for each context relation into weighted vector
components.
SEXTANT uses a generalisation of the Jaccard
measure to measure similarity. The Jaccard measure
is the cardinality ratio of the intersection and union
of attribute sets (atts(wn) is the attribute set for wn):
| atts(wm) ? atts(wn)|
| atts(wm) ? atts(wn)| (1)
The generalised Jaccard measure allows each rela-
tion to have a significance weight (based on word,
attribute and relation frequencies) associated with it:
?
a?atts(wm)?atts(wn) min(wgt(wm, a),wgt(wn, a))
?
a?atts(wm)?atts(wn) max(wgt(wm, a),wgt(wn, a))
(2)
Grefenstette originally used the weighting function:
wgt(wi, a j) = log2( f (wi,
a j) + 1)
log2(n(a j) + 1)
(3)
where f (wi, a j) is the frequency of the relation and
n(a j) is the number of different words a j appears in
relations with.
Name Context Description
W(L1R1) one word to left or right
W(L1) one word to the left
W(L1,2) one or two words to the left
W(L1?3) one to three words to the left
Table 1: Window extractors
However, we have found that using the t-test be-
tween the joint and independent distributions of a
word and its attribute:
wgt(wi, a j) = p(wi,
a j) ? p(wi)p(a j)
?p(wi)p(a j)
(4)
gives superior performance (Curran and Moens,
2002) and is therefore used for our experiments.
4 Context Extractors
We have experimented with a number of different
systems for extracting the contexts for each word.
These systems show a wide range in complexity
of method and implementation, and hence develop-
ment effort and execution time.
The simplest method we implemented extracts the
occurrence counts of words within a particular win-
dow surrounding the thesaurus term. These window
extractors are very easy to implement and run very
quickly. The window geometries used in this experi-
ment are listed in Table 1. Extractors marked with an
asterisk, for example W(L1R1?), do not distinguish
(within the relation type) between different positions
of the word w? in the window.
At a greater level of complexity we have two shal-
low NLP systems which provide extra syntactic in-
formation in the extracted contexts. The first sys-
tem is based on the syntactic relation extractor from
SEXTANT with a different POS tagger and chunker.
The SEXTANT-based extractor we developed uses a
very simple Na??ve Bayes POS tagger and chunker.
This is very simple to implement and is extremely
fast since it optimises the tag selection locally at the
current word rather than performing beam or Viterbi
search over the entire sentence. After the raw text
has been POS tagged and chunked, the SEXTANT re-
lation extraction algorithm is run over the text. This
consists of five passes over each sentence that asso-
ciate each noun with the modifiers and verbs from
the syntactic contexts that it appears in.
Corpus Sentences Words
British National Corpus 6.2M 114M
Reuters Corpus Vol 1 8.7M 193M
Table 2: Training Corpora Statistics
The second shallow parsing extractor we used was
the CASS parser (Abney, 1996), which uses cas-
caded finite state transducers to produce a limited
depth parse of POS tagged text. We used the out-
put of the Na??ve Bayes POS tagger output as input
to the CASS. The context relations used were ex-
tracted directly by the tuples program (using e8
demo grammar) included in the CASS distribution.
The FST parsing algorithm is very efficient and so
CASS also ran very quickly. The times reported be-
low include the Na??ve Bayes POS tagging time.
The final, most sophisticated extractor used was
the MINIPAR parser (Lin, 1998a), which is a broad-
coverage principle-based parser. The context rela-
tions used were extracted directly from the full parse
tree. Although fast for a full parser, MINIPAR was
no match for the simpler extractors.
For this experiment we needed a large quantity of
text which we could group into a range of corpus
sizes. We combined the BNC and Reuters corpus to
produce a 300 million word corpus. The respective
sizes of each are shown in Table 2. The sentences
were randomly shuffled together to produce a sin-
gle homogeneous corpus. This corpus was split into
two 150M word corpora over which the main experi-
mental results are averaged. We then created smaller
corpora of size 12 down to 164 th of each 150M corpus.The next section describes the method of evaluating
each thesaurus created by the combination of a given
context extraction system and corpus size.
5 Evaluation
For the purposes of evaluation, we selected 70 single
word noun terms for thesaurus extraction. To avoid
sample bias, the words were randomly selected from
Wordnet such that they covered a range of values for
the following word properties:
occurrence frequency based on frequency counts
from the Penn Treebank, BNC and Reuters;
number of senses based on the number of Wordnet
synsets and Macquarie Thesaurus entries;
generality/specificity based on depth of the term in
the Wordnet hierarchy;
abstractness/concreteness based on even distribu-
tion across all Wordnet subtrees.
Table 3 shows some of the selected terms with fre-
quency and synonym set data. For each term we
extracted a thesaurus entry with 200 potential syn-
onyms and their weighted Jaccard scores.
The most difficult aspect of thesaurus extraction
is evaluating the quality of the result. The sim-
plest method of evaluation is direct comparison of
the extracted thesaurus with a manually created gold
standard (Grefenstette, 1994). However on smaller
corpora direct matching alone is often too coarse-
grained and thesaurus coverage is a problem.
Our experiments use a combination of three the-
sauri available in electronic form: The Macquarie
Thesaurus (Bernard, 1990), Roget?s Thesaurus (Ro-
get, 1911), and the Moby Thesaurus (Ward, 1996).
Each thesaurus is structured differently: Roget?s and
Macquarie are topic ordered and the Moby thesaurus
is head term ordered. Roget?s is quite dated and has
low coverage, and contains a deep hierarchy (depth
up to seven) with terms grouped in 8696 small syn-
onym sets at the leaves of the hierarchy. The Mac-
quarie consists of 812 large topics (often in antonym
related pairs), each of which is separated into 21174
small synonym sets. Roget?s and the Macquarie
provide sense distinctions by placing terms in mul-
tiple synonym sets. The Moby thesaurus consists
of 30259 head terms and large synonym lists which
conflate all the head term senses. The extracted the-
saurus does not distinguish between different head
senses. Therefore, we convert the Roget?s and Mac-
quarie thesaurus into head term ordered format by
combining each small sense set that the head term
appears in.
We create a gold standard thesaurus containing
the union of the synonym lists from each thesaurus,
giving a total of 23207 synonyms for the 70 terms.
With these gold standard resources in place, it is
possible to use precision and recall measures to cal-
culate the performance of the thesaurus extraction
systems. To help overcome the problems of coarse-
grained direct comparisons we use three different
types of measure to evaluate thesaurus quality:
1. Direct Match (DIRECT)
2. Precision of the n top ranked synonyms (P(n))
3. Inverse Rank (INVR)
A match is an extracted synonym that appears in
the corresponding gold standard synonym list. The
direct match score is the number of such matches for
each term. Precision of the top n is the percentage
of matches in the top n extracted synonyms. In these
experiments, we calculate this for n = 1, 5, and 10.
The inverse rank score is the sum of the inverse rank
of each match. For example, if matching synonyms
appear in the extracted synonym list at ranks 3, 5
and 28, then the inverse rank score is 13 + 15 + 128 =0.569. The maximum inverse rank score is 5.878 for
a synonym list of 200 terms. Inverse rank is a good
measure of subtle differences in ranked results. Each
measure is averaged over the extracted synonym lists
for all 70 thesaurus terms.
6 Results
Since MINIPAR performs morphological analysis on
the context relations we have added an existing mor-
phological analyser (Minnen et al, 2000) to the
other extractors. Table 4 shows the improvement
gained by morphological analysis of the attributes
and relations for the SEXTANT 150M corpus.
The improvement in results is quite significant, as
is the reduction in the representation space and num-
ber of unique context relations. The reduction in the
number of terms is a result of coalescing the plu-
ral nouns with their corresponding singular nouns,
which also reduces data sparseness problems. The
remainder of the results use morphological analysis
of both the words and attributes.
Table 5 summarises the average results of ap-
plying all of the extraction systems to the two
150M word corpora. The first thing to note is
the time spent extracting contextual information:
MINIPAR takes significantly longer to run than the
other extractors. Secondly, SEXTANT and MINI-
PAR have quite similar results overall, but MINIPAR
is slightly better across most measures. However,
SEXTANT runs about 28 times faster than MINI-
PAR. Also, MINIPAR extracts many more terms
and relations with a much larger representation than
SEXTANT. This is partly because MINIPAR ex-
tracts more types of relations from the parse tree
Word PTB Rank PTB # BNC # Reuters # Macquarie # WordNet # Min / Max WordNet subtree roots
company 38 4076 52779 456580 8 9 3 / 6 entity, group, state
interest 138 919 37454 146043 12 12 3 / 8 abs., act, group, poss., state
problem 418 622 56361 63333 4 3 3 / 7 abs., psych., state
change 681 406 35641 55081 8 10 2 / 12 abs., act, entity, event, phenom.
house 896 223 47801 45651 10 12 3 / 6 act, entity, group
idea 1227 134 32754 13527 10 5 3 / 7 entity, psych.
opinion 1947 78 9122 16320 4 6 4 / 8 abs., act, psych.
radio 2278 59 9046 20913 2 3 6 / 8 entity
star 5130 29 8301 6586 11 7 4 / 8 abs., entity
knowledge 5197 19 14580 2813 3 1 1 / 1 psych.
pants 13264 5 429 282 3 2 6 / 9 entity
tightness 30817 1 119 2020 5 3 4 / 5 abs., state
Table 3: Examples of the 70 thesaurus evaluation terms with distribution information
Morph. Analysis Space Unique Terms DIRECT P(1) P(5) P(10) INVR
None 345Mb 14.70M 298k 20.33 32.5 % 36.9 % 33.6 % 1.37
Attributes 302Mb 13.17M 298k 20.65 32.0 % 37.6 % 32.5 % 1.36
Both 274Mb 12.08M 269k 23.74 64.5 % 47.0 % 39.0 % 1.86
Table 4: Effect of morphological analysis on SEXTANT thesaurus quality
than SEXTANT, and partly because it extracts ex-
tra multi-word terms. Amongst the simpler meth-
ods, W(L1R1) and W(L1,2) give reasonable results.
The larger windows with low correlation between
the thesaurus term and context, extract a massive
context representation but the results are about 10%
worse than the syntactic extractors.
Overall the precision and recall are relatively
poor. Poor recall is partly due to the gold stan-
dard containing some plurals and multi-word terms
which account for about 25% of the synonyms.
These have been retained because the MINIPAR and
CASS systems are capable of identifying (at least
some) multi-word terms.
Given a fixed time period (of more than the four
days MINIPAR takes) and a fixed 150M corpus we
would probably still choose to use MINIPAR unless
the representation was too big for our learning algo-
rithm, since the thesaurus quality is slightly better.
Table 6 shows what happens to thesaurus quality
as we decrease the size of the corpus to 164 th of itsoriginal size (2.3M words) for SEXTANT. Halving
the corpus results in a significant reduction for most
of the measures. All five evaluation measures show
the same log-linear dependence on the size of the
corpus. Figure 1 shows the same trend for Inverse
Rank evaluation of the MINIPAR thesaurus with a
log-linear fitting the data points.
We can use the same curve fitting to estimate the-
0 50 100 150 200
Number of words (millions)
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
In
ve
rs
e 
R
an
k 
m
ea
su
re
Raw data
Fitted Curve
Figure 1: MINIPAR INVR scores versus corpus size
saurus quality on larger corpora for three of the best
extractors: SEXTANT, MINIPAR and W(L1R1). Fig-
ure 2 does this with the direct match evaluation. The
estimate indicates that MINIPAR will continue to be
the best performer on direct matching. We then plot
the direct match scores for the 300M word corpus
to see how accurate our predictions are. The SEX-
TANT system performs almost exactly as predicted
and the other two slightly under-perform their pre-
dicted scores, thus the fitting is accurate enough to
make reasonable predictions.
Figure 2 is a graph for making engineering deci-
sions in conjunction with the data in Table 5. For
instance, if we fix the total time and computational
System Space Relations Unique Terms DIRECT P(1) P(5) P(10) INVR Time
MINIPAR 399Mb 142.27M 16.62M 914k 24.55 61.5 % 46.5 % 40.5 % 1.85 4438.9m
SEXTANT 274Mb 53.07M 12.08M 269k 23.75 64.5 % 47.0 % 39.0 % 1.85 159.0m
CASS 186Mb 50.63M 9.09M 204k 20.20 48.5 % 38.5 % 32.5 % 1.51 173.7m
W(L1) 117Mb 105.62M 7.04M 406k 20.60 51.5 % 40.0 % 32.5 % 1.56 6.8m
W(L1,2) 336Mb 206.02M 18.04M 440k 21.30 58.5 % 44.5 % 36.5 % 1.71 7.2m
W(L1,2?) 258Mb 206.02M 15.34M 440k 20.75 55.0 % 41.5 % 35.5 % 1.64 6.8m
W(L1?3) 570Mb 301.10M 30.62M 444k 20.50 60.0 % 43.5 % 37.0 % 1.69 8.2m
W(L1?3?) 388Mb 301.10M 22.86M 444k 19.85 48.5 % 39.5 % 33.5 % 1.53 8.2m
W(L1R1) 262Mb 211.24M 14.07M 435k 22.40 62.0 % 44.5 % 37.0 % 1.76 7.2m
W(L1R1?) 211Mb 211.24M 12.56M 435k 20.90 54.5 % 42.5 % 34.5 % 1.64 7.2m
Table 5: Average thesaurus quality results for different extraction systems
Corpus Space Relations Unique Terms DIRECT P(1) P(5) P(10) INVR
150.0M 274Mb 53.07M 12.08M 268.94k 23.75 64.5 % 47.0 % 39.0 % 1.85
75.0M 166Mb 26.54M 7.38M 181.73k 22.60 58.0 % 43.5 % 36.0 % 1.73
37.5M 98Mb 13.27M 4.36M 120.48k 21.75 54.0 % 41.0 % 34.5 % 1.62
18.8M 56Mb 6.63M 2.54M 82.33k 20.45 47.0 % 36.5 % 31.0 % 1.46
9.4M 32Mb 3.32M 1.44M 55.55k 18.50 40.0 % 32.5 % 27.5 % 1.29
4.7M 18Mb 1.66M 0.82M 37.95k 16.65 34.0 % 29.5 % 23.5 % 1.13
2.3M 10Mb 0.83M 0.46M 25.97k 14.60 27.5 % 25.0 % 19.5 % 0.93
Table 6: Average SEXTANT thesaurus quality results for different corpus sizes
0 50 100 150 200 250 300 350
Number of words (millions)
0
4
8
12
16
20
24
28
32
N
um
be
r 
of
 D
ir
ec
t M
at
ch
es
Sextant
Minipar
W(L1R1)
Figure 2: Direct matches versus corpus size
resources at an arbitrary point, e.g. the point where
MINIPAR can process 75M words, we get a best
direct match score of 23.5. However, we can get
the same resultant accuracy by using SEXTANT on
a corpus of 116M words or W(L1R1) on a corpus
of 240M words. From Figure 5, extracting contexts
from corpora of these sizes would take MINIPAR 37
hours, SEXTANT 2 hours and W(L1R1) 12 minutes.
Interpolation on Figure 3 predicts that the extraction
would result in 10M unique relations from MINI-
PAR and SEXTANT and 19M from W(L1R1). Fig-
ure 4 indicates that extraction would result in 550k
0 50 100 150 200 250 300 350
Number of words (millions)
0
4
8
12
16
20
24
28
32
N
um
be
r 
of
 u
ni
qu
e 
re
la
tio
ns
 (
m
ill
io
ns
)
Sextant
Minipar
Cass
W(L1R1)
W(L1R1*)
W(L1,2)
Figure 3: Representation size versus corpus size
MINIPAR terms, 200k SEXTANT terms and 600k
W(L1R1) terms.
Given these values and the fact that the time com-
plexity of most thesaurus extraction algorithms is at
least linear in the number of unique relations and
squared in the number of thesaurus terms, it seems
SEXTANT may represent the best solution.
With these size issues in mind, we finally consider
some methods to limit the size of the context rep-
resentation. Table 7 shows the results of perform-
ing various kinds of filtering on the representation
size. The FIXED and LEXICON filters run over the
System Space Relations Unique Terms DIRECT P(1) P(5) P(10) INVR
SEXTANT 300M 431Mb 80.33M 20.41M 445k 25.30 61.0 % 47.0 % 39.0 % 1.87
SEXTANT 150M 274Mb 53.07M 12.08M 269k 23.75 64.5 % 47.0 % 39.0 % 1.85
SEXTANT FIXED 244Mb 61.17M 10.74M 265k 24.35 65.0 % 46.5 % 38.5 % 1.86
SEXTANT LEXICON 410Mb 78.69M 18.09M 264k 25.25 62.0 % 47.0 % 40.0 % 1.87
SEXTANT >1 149Mb 67.97M 6.63M 171k 24.20 66.0 % 45.0 % 38.0 % 1.85
SEXTANT >2 88Mb 62.57M 3.93M 109k 23.20 66.0 % 46.0 % 36.0 % 1.82
Table 7: Thesaurus quality with relation filtering
0 50 100 150 200 250 300 350
Number of words (millions)
0
200
400
600
800
1000
1200
1400
1600
1800
N
um
be
r 
of
 th
es
au
ru
s 
te
rm
s 
(m
ill
io
ns
)
Sextant
Minipar
Cass
W(L1R1)
Figure 4: Thesaurus terms versus corpus size
full 300M word corpus, but have size limits based
on the 150M word corpus. The FIXED filter does not
allow any object/attribute pairs to be added that were
not extracted from the 150M word corpus. The LEX-
ICON filter does not allow any objects to be added
that were not extracted from the 150M word cor-
pus. The > 1 and > 2 filters prune relations with a
frequency of less than or equal to one or two. The
FIXED and LEXICON filters show that counting over
larger corpora does produce marginally better re-
sults. The > 1 and > 2 filters show that the many
relations that occur infrequently do not contribute
significantly to the vector comparisons and hence
don?t impact on the final results, even though they
dramatically increase the representation size.
7 Conclusion
It is a phenomenon common to many NLP tasks that
the quality or accuracy of a system increases log-
linearly with the size of the corpus. Banko and Brill,
(2001) also found this trend for the task of confu-
sion set disambiguation on corpora of up to one bil-
lion words. They demonstrated behaviour of differ-
ent learning algorithms with very simple contexts on
extremely large corpora. We have demonstrated the
behaviour of a simple learning algorithm on much
more complicated contextual information on very
large corpora.
Our experiments suggest that the existing method-
ology of evaluating systems on small corpora with-
out reference to the execution time and representa-
tion size ignores important aspects of the evaluation
of NLP tools.
These experiments show that efficiently imple-
menting and optimising the NLP tools used for con-
text extraction is of crucial importance since the in-
creased corpus sizes make execution speed an im-
portant evaluation factor when deciding between
different learning algorithms for different tasks and
corpora. These results also motivate further re-
search into improving the asymptotic complexity of
the learning algorithms used in NLP systems. In
the new paradigm, it could well be that far simpler
but scalable learning algorithms significantly out-
perform existing systems.
Finally, the mass availability of online text re-
sources should be taken on board. It is important
that language engineers and computational linguists
continue to try and find new unsupervised or (as
Banko and Brill suggest) semi-supervised methods
for tasks which currently rely on annotated data. It
is also important to consider how information ex-
tracted by systems such as thesaurus extraction sys-
tems can be incorporated into tasks which use pre-
dominantly supervised techniques, e.g. in the form
of class information for smoothing.
We would like to extend this analysis to at least
one billion words for at least the most successful
methods and try other tools and parsers for extract-
ing the contextual information. However, to do this
we must look at methods of compressing the vector-
space model and approximating the full pair-wise
comparison of thesaurus terms. We would also like
to investigate how this thesaurus information can be
used to improve the accuracy or generality of other
NLP tasks.
Acknowledgements
We would like to thank Miles Osborne, Stephen
Clark, Tara Murphy, and the anonymous reviewers
for their comments on drafts of this paper. This re-
search is supported by a Commonwealth scholarship
and a Sydney University Travelling scholarship.
References
Steve Abney. 1996. Partial parsing via finite-state cas-
cades. Journal of Natural Language Engineering,
2(4):337?344, December.
L. Douglas Baker and Andrew McCallum. 1998. Distri-
butional clustering of words for text classification. In
Proceedings of the 21st annual international ACM SI-
GIR conference on Research and Development in In-
formation Retrieval, pages 96?103, Melbourne, Aus-
tralia, 24?28 August.
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In Proceedings of the 39th annual meeting of the
Association for Computational Linguistics, pages 26?
33, Toulouse, France, 9?11 July.
John R. L. Bernard, editor. 1990. The Macquarie Ency-
clopedic Thesaurus. The Macquarie Library, Sydney,
Australia.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jennifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479, December.
Sharon A. Caraballo. 1999. Automatic construction of
a hypernym-labeled noun hierarchy from text. In Pro-
ceedings of the 37th annual meeting of the Association
for Computational Linguistics, pages 120?126, Col-
lege Park, MD USA, 20?26 June.
Stephen Clark and David Weir. 2001. Class-based prob-
ability estimation using a semantic hierarchy. In Pro-
ceedings of the Second Meeting of the North American
Chapter of the Association for Computational Linguis-
tics, pages 95?102, Pittsburgh, PA USA, 2?7 June.
Carolyn J. Crouch. 1988. Construction of a dynamic the-
saurus and its use for associated information retrieval.
In Proceedings of the eleventh international confer-
ence on Research and Development in Information Re-
trieval, pages 309?320, Grenoble, France, 13?15 June.
James R. Curran and Marc Moens. 2002. Improve-
ments in automatic thesaurus extraction. In ACL-
SIGLEX Workshop on Unsupervised Lexical Acquisi-
tion, Philadelphia, PA USA. (to appear).
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers,
Boston, USA.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th international conference on Computational Lin-
guistics, pages 539?545, Nantes, France, 23?28 July.
Dekang Lin. 1998a. Dependency-based evaluation of
MINIPAR. In Workshop on the Evaluation of Parsing
Systems, Proceedings of the First International Con-
ference on Language Resources and Evaluation, pages
234?241, Granada, Spain, 28?30 May.
Dekang Lin. 1998b. An information-theoretic definition
of similarity. In Proceedings of the Fifteen Interna-
tional Conference on Machine Learning, pages 296?
304, Madison, WI USA, 24?27 July.
Guido Minnen, John Carroll, and Darren Pearce. 2000.
Robust applied morphological generation. In In Pro-
ceedings of the First International Natural Language
Generation Conference, pages 201?208, Mitzpe Ra-
mon, Israel, 12?16 June.
Darren Pearce. 2001. Synonymy in collocation extrac-
tion. In Workshop on WordNet and Other Lexical
Resources: Applications, Extensions and Customiza-
tions, (NAACL 2001), pages 41?46, Pittsburgh, PA
USA, 2?7 June.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Pro-
ceedings of the 31st annual meeting of the Associ-
ation for Computational Linguistics, pages 183?190,
Columbus, Ohio USA, 22?26 June.
Peter Roget. 1911. Thesaurus of English words and
phrases. Longmans, Green and Co., London, UK.
Gerda Ruge. 1997. Automatic detection of thesaurus re-
lations for information retrieval applications. In Foun-
dations of Computer Science: Potential - Theory -
Cognition, Lecture Notes in Computer Science, vol-
ume LNCS 1337, pages 499?506. Springer Verlag,
Berlin, Germany.
Mark Sanderson and Bruce Croft. 1999. Deriving con-
cept hierarchies from text. In Proceedings of the
22nd annual international ACM SIGIR conference on
Research and Development in Information Retrieval,
pages 206?213, Berkeley, CA USA, 15?19 August.
Grady Ward. 1996. Moby Thesaurus. Moby Project.
What's yours and what's mine: Determining Intellectual 
Attribution in Scientific Text 
S imone Teufe l  t
Computer  Science Department  
Columbia University 
t eu fe l?cs ,  co lumbia ,  edu  
Marc  Moens  
HCRC Language Technology Group 
University of Ed inburgh 
Marc .  Moens?ed.  ac. uk  
Abst rac t  
We believe that identifying the structure of scien- 
tific argumentation in articles can help in tasks 
such as automatic summarization or the auto- 
mated construction of citation indexes. One par- 
ticularly important aspect of this structure is the 
question of who a given scientific statement is at- 
tributed to: other researchers, the field in general, 
or the authors themselves. 
We present he algorithm and a systematic eval- 
uation of a system which can recognize the most 
salient textual properties that contribute to the 
global argumentative structure of a text. In this 
paper we concentrate on two particular features, 
namely the occurrences ofprototypical gents and 
their actions in scientific text. 
1 In t roduct ion  
When writing an article, one does not normally 
go straight to presenting the innovative scien- 
tific claim. Insteacl, one establishes other, well- 
known scientific facts first, which are contributed 
by other researchers. Attribution of ownership of- 
ten happens explicitly, by phrases uch as "Chom- 
sky (1965) claims that". The question of intel- 
lectual attribution is important for researchers: 
not understanding the argumentative status of 
part of the text is a common problem for non- 
experts reading highly specific texts aimed at ex- 
perts (Rowley, 1982). In particular, after reading 
an article, researchers need to know who holds the 
"knowledge claim" for a certain fact that interests 
them. 
We propose that segmentation according to in- 
tellectual ownership can be done automatically, 
and that such a segmentation has advantages for 
various hallow text understanding tasks. At the 
heart of our classification scheme is the following 
trisection: 
* BACKGROUND (generally known work) 
* OWN,  new work and 
. specific OTHER work. 
The advantages of a segmentation at a rhetori- 
cal level is that rhetorics is conveniently constant 
tThis work was done while the first author was at the 
HCRC Language T chnology Group, Edinburgh. 
BACKGROUND:  
Researchers in knowledge representa- 
tion agree that one of the hard problems of 
understanding narrative is the representation 
of temporal information. Certain facts of nat- 
ural language make it hard to capture tempo- 
ral information \[...\] 
OTHER WORK:  
Recently, Researcher-4 has suggested the 
following solution to this problem \[...\]. 
WEAKNESS/CONTRAST:  
But this solution cannot be used to inter- 
pret the following Japanese examples: \[...\] 
OWN CONTRIBUT ION:  
We propose a solution which circumvents 
this p row while retaining the explanatory 
power of Researcher-4's approach. 
Figure h Fictional introduction section 
across different articles. Subject matter, on the 
contrary, is not constant, nor are writing style and 
other factors. 
We work with a corpus of scientific pa- 
pers (80 computational linguistics conference ar- 
ticles (ACL, EACL, COLING or ANLP), de- 
posited on the CMP_LG archive between 1994 
and 1996). This is a difficult test bed due to 
the large variation with respect o different fac- 
tors: subdomain (theoretical linguistics, statisti- 
cal NLP, logic programming, computational psy- 
cholinguistics), types of research (implementa- 
tion, review, evaluation, empirical vs. theoreti- 
cal research), writing style (formal vs. informal) 
and presentational styles (fixed section structure 
of type Introduction-Method-Results-Conclusion 
vs. more idiosyncratic, problem-structured presen- 
tation). 
One thing, however, is constant across all arti- 
cles: the argumentative aim of every single article 
is to show that the given work is a contribution to 
science (Swales, 1990; Myers, 1992; Hyland, 1998). 
Theories of scientific argumentation in research ar- 
ticles stress that authors follow well-predictable 
stages of argumentation, as in the fictional intro- 
duction in figure 1. 
9 
Are the scientific statements expressed 
in this sentence attributed to the 
authors, the general field, or specific other 
n work / Other Work 
Does this sentence contain material 
that describes the specific aim 
of the paper? 
Does this sentence make 
reference to the external 
structure of the paper? 
I SACKCRO D I 
D.~s it describe.a negative aspect 
of me omer worK, or a contzast 
or comparison of the own work to it? 
I CONTRAST I Does this sentence mention 
the other work as basis of 
or support for own work? 
Figure 2: Annotation Scheme for Argumentative Zones 
Our hypothesis i that a segmentation based on 
regularities of scientific argumentation and on at- 
tribution of intellectual ownership is one of the 
most stable and generalizable dimensions which 
contribute to the structure of scientific texts. In 
the next section we will describe an annotation 
scheme which we designed for capturing these ef- 
fects. Its categories are based on Swales' (1990) 
CARS model. 
1.1 The scheme 
As our corpus contains many statements talking 
about relations between own and other work, we 
decided to add two classes ("zones") for express- 
ing relations to the core set of OWN, OTHER 
and BACKGROUND, namely contrastive statements 
(CONTRAST;  comparable to Swales' (1990) move 
2A/B) and statements of intellectual ancestry 
(BAsis; Swales' move 2D). The label OTHER is 
thus reserved for neutral descriptions of other 
work. OWN segments are further subdivided to 
mark explicit aim statements (AIM; Swales' move 
3.1A/B), and explicit section previews (TEXTUAL; 
Swales' move 3.3). All other statements about the 
own work are classified as OwN. Each of the seven 
category covers one sentence. 
Our classification, which is a further develop- 
ment of the scheme in Teufel and Moens (1999), 
can be described procedurally as a decision tree 
(Figure 2), where five questions are asked about 
each sentence, concerning intellectual attribution, 
author stance and continuation vs. contrast. Fig- 
ure 3 gives typical example sentences for each zone. 
The intellectual-attribution distinction we make 
is comparable with Wiebe's (1994) distinction into 
subjective and objective statements. Subjectivity 
is a property which is related to the attribution of 
authorship as well as to author stance, but it is 
just one of the dimensions we consider. 
1.2 Use o f  Argumentat ive  Zones 
Which practical use would segmenting a paper into 
argumentative zones have? 
Firstly, rhetorical information as encoded in 
these zones should prove useful for summariza- 
tion. Sentence extracts, still the main type of 
summarization around, are notoriously context- 
insensitive. Context in the form of argumentative 
relations of segments to the overall paper could 
provide a skeleton by which to tailor sentence x- 
tracts to user expertise (as certain users or certain 
tasks do not require certain types of information). 
A system which uses such rhetorical zones to pro- 
duce task-tailored extracts for medical articles, al- 
beit on the basis of manually-segmented xts, is 
given by Wellons and Purcell (1999). 
Another hard task is sentence xtraction from 
long texts, e.g. scientific journal articles of 20 
pages of length, with a high compression. This 
task is hard because one has to make decisions 
about how the extracted sentences relate to each 
other and how they relate to the overall message 
of the text, before one can further compress them. 
Rhetorical context of the kind described above is 
very likely to make these decisions easier. 
Secondly, it should also help improve citation 
indexes, e.g. automatically derived ones like 
Lawrence et al's (1999) and Nanba and Oku- 
mura's (1999). Citation indexes help organize sci- 
entific online literature by linking cited (outgoing) 
and citing (incoming) articles with a given text. 
But these indexes are mainly "quantitative", list- 
ing other works without further qualifying whether 
a reference to another work is there to extend the 
10 
AIM "We have proposed a method of clustering words based on large corpus data." 
TEXTUAL "Section $ describes three unification-based parsers which are... " 
OWN "We also compare with the English language and draw some conclusions on the benefits 
of our approach." 
BACKGROUND "Part-of-speech tagging is the process of assigning rammatical categories to individual 
words in a corpus." 
CONTRAST "However, no method for extracting the relationships from superficial inguistic ex- 
pressions was described in their paper." 
BASIS "Our disambiauation method is based on the similaritu of context vectors, which was 
OTHER 
C :g y
originated by Wilks et al 1990." 
"Strzalkowski's Essential Arguments Approach (EAA) is a top-down approach to gen- 
eration... " 
Figure 3: Examples for Argumentative Zones 
earlier work, correct it, point out a weakness in 
it, or just provide it as general background. This 
"qualitative" information could be directly con- 
tributed by our argumentative zones. 
In this paper, we will describe the algorithm of 
an argumentative zoner. The main focus of the 
paper is the description of two features which are 
particularly useful for attribution determination: 
prototypical gents and actions. 
2 Human Annotat ion  o f  
Argumentat ive  Zones  
We have previously evaluated the scheme mpiri- 
cally by extensive experiments with three subjects, 
over a range of 48 articles (Teufel et al, 1999). 
We measured stability (the degree to which the 
same annotator will produce an annotation after 
6 weeks) and reproducibility (the degree to which 
two unrelated annotators will produce the same 
annotation), using the Kappa coefficient K (Siegel 
and Castellan, 1988; Carletta, 1996), which con- 
trols agreement P(A) for chance agreement P(E): 
K = P{A)-P(E) 
1-P(Z) 
Kappa is 0 for if agreement is only as would be 
expected by chance annotation following the same 
distribution as the observed istribution, and 1 for 
perfect agreement. Values of Kappa surpassing 
.8 are typically accepted as showing a very high 
level of agreement (Krippendorff, 1980; Landis and 
Koch, 1977). 
Our experiments show that humans can distin- 
guish own, other specific and other general work 
with high stability (K=.83, .79, .81; N=1248; k=2, 
where K stands for the Kappa coefficient, N for 
the number of items (sentences) annotated and k 
for the number of annotators) and reproducibil- 
ity (K=.78, N=4031, k=3), corresponding to 94%, 
93%, 93% (stability) and 93% (reproducibility) 
agreement. 
The full distinction into all seven categories of 
the annotation scheme is slightly less stable and 
reproducible (stability: K=.82, .81, .76; N=1220; 
k=2 (equiv. to 93%, 92%, 90% agreement); repro- 
ducibility: K=.71, N=4261, k=3 (equiv. to 87% 
agreement)), but still in the range of what is gener- 
ally accepted as reliable annotation. We conclude 
from this that humans can distinguish attribution 
and full argumentative zones, if trained. Human 
annotation is used as trMning material in our sta- 
tistical classifier. 
3 Automat ic  Argumentat ive  
Zon ing  
As our task is not defined by topic coherence 
like the related tasks of Morris and Hirst (1991), 
Hearst (1997), Kan et al (1998) and Reynar 
(1999), we predict hat keyword-based techniques 
for automatic argumentative zoning will not work 
well (cf. the results using text categorization as
described later). We decided to perform machine 
learning, based on sentential features like the ones 
used by sentence xtraction. Argumentative zones 
have properties which help us determine them on 
the surface: 
? Zones appear in typical positions in the article 
(Myers, 1992); we model this with a set of 
location features. 
? Linguistic features like tense and voice cor- 
relate with zones (Biber (1995) and Riley 
(1991) show correlation for similar zones like 
"method" and "introduction"). We model 
this with syntactic features. 
? Zones tend to follow particular other zones 
(Swales, 1990); we model this with an ngram 
model operating over sentences. 
? Beginnings of attribution zones are linguisti- 
cally marked by meta-discourse like "Other 
researchers claim that" (Swales, 1990; Hy- 
land, 1998); we model this with a specialized 
agents and actions recognizer, and by recog- 
nizing formal citations. 
? Statements without explicit attribution are 
interpreted as being of the same attribution 
as previous entences in the same segment of 
attribution; we model this with a modified 
agent feature which keeps track of previously 
recognized agents. 
11 
3.1 Recognizing Agents and Actions 
Paice (1981) introduces grammars for pattern 
matching of indicator phrases, e.g. "the 
aim/purpose of this paper/article/study" and "we 
conclude/propose". Such phrases can be useful 
indicators of overall importance. However, for 
our task, more flexible meta-diiscourse expressions 
need to be determined. The ,description of a re- 
search tradition, or the stateraent that the work 
described in the paper is the continuation ofsome 
other work, cover a wide range of syntactic and 
lexical expressions and are too hard to find for a 
mechanism like simple pattern matching. 
Agent Type Example 
US-AGENT 
THEM_AGENT 
GENERAL_AGENT 
US_PREVIOUS. AGENT 
OUR_AIM_AGENT 
REF_US_AGENT 
REF._AGENT 
THEM_PRONOUN_AGENT 
AIM_I:LEF_AGENT 
GAP_AGENT 
PROBLEM_AGENT 
SOLUTION_AGENT 
TEXTSTRUCTURE_AGENT 
we 
his approach 
traditional methods 
the approach given in 
X (99) 
the point o\] this study 
thia paper 
the paper 
they 
its goal 
none of these papers 
these drawbacks 
a way out o\] this 
dilemma 
the concluding chap- 
ter 
Figure 4: Agent Lexicon: 168 Patterns, 13 Classes 
We suggest hat the robust recognition of pro- 
totypical agents and actions is one way out of this 
dilemma. The agents we propose to recognize de- 
scribe fixed role-players in the argumentation. I  
Figure 1, prototypical agents are given in bold- 
face ("Researchers in knowledge representation, 
"Researcher-4" and "we"). We also propose pro- 
totypical actions frequently occurring in scientific 
discourse (shown underlined in Figure 1): the re- 
searchers "agree", Researcher-4 "suggested" some- 
thing, the solution "cannot be used". 
We will now describe an algorithm which rec- 
ognizes and classifies agents and actions. We 
use a manually created lexicon for patterns for 
agents, and a manually clustered verb lexicon for 
the verbs. Figure 4 lists the agent types we dis- 
tinguish. The main three types are US_aGENT, 
THEM-AGENT and GENERAL.AGENT. A fourth 
type is US.PREVIOUS_AGENT (the authors, but in 
a previous paper). 
Additional agent types include non-personal 
agents like aims, problems, solutions, absence of 
solution, or textual segments. There are four 
equivalence classes of agents with ambiguous 
reference ("this system"), namely REF_US_AGENT, 
THEM-PRONOUN_AGENT, AIM.-REF-AGENT, 
REF_AGENT. The total of 168 patterns in the 
lexicon expands to many more as we use a replace 
mechanism (@WORK_NOUN is expanded to 
"paper, article, study, chapter" etc). 
For verbs, we use a manually created the ac- 
tion lexicon summarized in Figure 6. The verb 
classes are based on semantic oncepts uch as 
similarity, contrast, competition, presentation, ar- 
gumentation and textual structure. For ex- 
ample, PRESENTATION..ACTIONS include commu- 
nication verbs like "present", "report", "state" 
(Myers, 1992; Thompson and Yiyun, 1991), RE- 
SEARCH_ACTIONS include "analyze", "conduct" 
and "observe", and ARGUMENTATION_ACTIONS 
"argue", "disagree", "object to". Domain-specific 
actions are contained in the classes indicating 
a problem ( ".fail", "degrade", "overestimate"), 
and solution-contributing actions (" "circumvent', 
solve", "mitigate"). 
The main reason for using a hand-crafted, genre-- 
specific lexicon instead of a general resource such 
as WordNet or Levin's (1993) classes (as used in 
Klavans and Kan (1998)), was to avoid polysemy 
problems without having to perform word sense 
disambiguation. Verbs in our texts often have a 
specialized meaning in the domain of scientific ar- 
gumentation, which our lexicon readily encodes. 
We did notice some ambiguity problems (e.g. "fol- 
low" can mean following another approach, or it 
can mean follow in a sense having nothing to do 
with presentation of research, e.g. following an 
arc in an algorithm). In a wider domain, however, 
ambiguity would be a much bigger problem. 
Processing of the articles includes transforma- 
tion from I~TEX into XML format, recognition 
of formal citations and author names in running 
text, tokenization, sentence separation and POS- 
tagging. The pipeline uses the TTT software pro- 
vided by the HCRC Language Technology Group 
(Grover et al, 1999). The algorithm for deter- 
mining agents in subject positions (or By-PPs in 
passive sentences) is based on a finite automaton 
which uses POS-input; cf. Figure 5. 
In the case that more than one finite verb is 
found in a sentence, the first finite verb which has 
agents and/or actions in the sentences i used as 
a value for that sentence. 
4 Eva luat ion  
We carried out two evaluations. Evaluation A 
tests whether all patterns were recognized as in- 
tended by the algorithm, and whether patterns 
were found that should not have been recognized. 
Evaluation B tests how well agent and action 
recognition helps us perform argumentative zon- 
ing automatically. 
4.1 Evaluation A: Cor rectness  
We first manually evaluated the error level of the 
POS-Tagging of finite verbs, as our algorithm cru- 
cially relies on finite verbs. In a random sample of 
100 sentences from our corpus (containing a total 
of 184 finite verbs), the tagger showed a recall of 
12 
1. Start from the first finite verb in the sentence. 
2. Check right context of the finite verb for verbal forms of interest which might make up more 
complex tenses. Remain within the assumed clause boundaries; do not cross commas or other 
finite verbs. Once the main verb of that construction (the "semantic" verb) has been found, 
a simple morphological nalysis determines its lemma; the tense and voice of the construction 
follow from the succession of auxiliary verbs encountered. 
3. Look up the lemma of semantic verb in Action Lexicon; return the associated Action Class if 
successful. Else return Action 0. 
4. Determine if one of the 32 fixed negation words contained in the lexicon (e.g. "not, don't, 
neither") is present within a fixed window of 6 to the right of the finite verb. 
5. Search for the agent either as a by-PP to the right, or as a subject-NP to the left, depending on 
the voice of the construction as determined in step 2. Remain within assumed clause boundaries. 
6. If one of the Agent Patterns matches within that area in the sentence, return the Agent Type. 
Else return Agent 0. 
7. Repeat Steps 1-6 until there are no more finite verbs left. 
Figure 5: Algorithm for Agent and Action Detection 
Action Type Example Action Type Example 
AFFECT 
ARGUMENTATION 
AWARENESS 
BETTER_SOLUTION 
CHANGE 
COMPARISON 
CONTINUATION 
CONTRAST 
FUTURE_INTEREST 
INTEREST 
we hope to improve our results 
we argue against a model of 
we are not aware of attempts 
our system outperforms . . .  
we extend <CITE /> 's  algo- 
rithm 
we tested our system against.. .  
we follow <REF/> . . .  
our approach differs from . . .  
we intend to improve . . .  
we are concerned with . . .  
NEED 
PRESENTATION 
PROBLEM 
RESEARCH 
SIMILAR 
SOLUTION 
TEXTSTRUCTURE 
USE 
COPULA 
POSSESSION 
this approach, however, lacks... 
we present here a method for. .  . 
this approach fai ls . . .  
we collected our data f rom. . .  
our approach resembles that of 
we solve this problem by. . .  
the paper is organize&.. 
we employ <REF/> 's method...  
our goal ~ to . . .  
we have three goals... 
Figure 6: Action Lexicon: 366 Verbs, 20 Classes 
95% and a precision of 93%. 
We found that for the 174 correctly determined 
finite verbs (out of the total 184), the heuristics for 
negation worked without any errors (100% accu- 
racy). The correct semantic verb was determined 
in 96% percent of all cases; errors are mostly due 
to misrecognition of clause boundaries. Action 
Type lookup was fully correct, even in the case 
of phrasal verbs and longer idiomatic expressions 
("have to" is a NEED..ACTION; "be inspired by" is 
a, CONTINUE_ACTION). There were 7 voice errors, 
2 of which were due to POS-tagging errors (past 
participle misrecognized). The remaining 5 voice 
errors correspond to a 98% accuracy. Figure 7 
gives an example for a voice error (underlined) in 
the output of the action/agent determination. 
Correctness of Agent Type determination was 
tested on a random sample of 100 sentences con- 
taining at least one agent, resulting in 111 agents. 
No agent pattern that should have been identi- 
fied was missed (100% recall). Of the 111 agents, 
105 cases were completely correct: the agent pat- 
tern covered the complete grammatical subject or 
by-PP intended (precision of 95%). There was one 
complete rror, caused by a POS-tagging error. In 
5 of the 111 agents, the pattern covered only part  
At the point where John <ACTION 
TENSE=Pi~SENT VOICE=ACTIVE 
MODAL=NOMODAL NEGATION=0 
ACT IONTYPE=0> knows </ACTION> the truth 
has been  <FINITE TENSE=PRESENT_PERFECT 
VOICE=PASSIVE  MODAL=NOMODAL NEGA-  
T ION=0 ACTIONTYPE=0> processed 
</ACTION> , a complete clause will have 
been <ACTION TENSE=FUTURE.PERFECT 
VOICE=ACTIVE MODAL=NOMODAL NEGA- 
TION=0 ACTIONTYPE=0> bu i l t  </ACTION> 
Figure 7: Sample Output of Action Detection 
of a subject NP (typically the NP in a postmodify- 
ing PP), as in the phrase "the problem with these 
approaches" which was classified as REF_AGENT. 
These cases (counted as errors) indeed constitute 
no grave errors, as they still give an indication 
which type of agents the nominal phrase is associ- 
ated with. 
13 
4.2 Evaluation B: Usefulness for 
Argumentat ive Zoning 
We evaluated the usefulness of the Agent and Ac- 
tion features by measuring if they improve the 
classification results of our stochastic classifier for 
argumentative zones. 
We use 14 features given in figure 8, some of 
which are adapted from sentence xtraction tech- 
niques (Paice, 1990; Kupiec et eL1., 1995; Teufel and 
Moens, 1999). 
. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 
11. 
12. 
13. 
14. 
Absolute location of sentence in document 
Relative location of sentence in section 
Location of a sentence in paragraph 
Presence of citations 
Location of citations 
Type of citations (self citation or not) 
Type of headline 
Presence of tf/idf key words 
Presence of title words 
Sentence length 
Presence of modal auxiliaries 
Tense of the finite verb 
Voice of the finite verb 
Presence of Formulaic Expressions 
Figure 8: Other features used 
All features except Citation Location and 
Citation Type proved helpful for classification. 
Two different statistical models were used: a Naive 
Bayesian model as in Kupiec et al's (1995) exper- 
iment, cf. Figure 9, and an ngram model over sen- 
tences, cf. Figure 10. Learning is supervised and 
training examples are provided by our previous hu- 
man annotation. Classification preceeds sentence 
by sentence. The ngram model combines evidence 
from the context (Cm-1, Cm-2) and from I senten- 
tiai features (F,~,o...Fmj-t), assuming that those 
two factors are independent ofeach other. It uses 
the same likelihood estimation as the Naive Bayes, 
but maximises a context-sensitive prior using the 
Viterbi algorithm. We received best results for 
n=2, i.e. a bigram model. 
The results of stochastic lassification (pre- 
sented in figure 11) were compiled with a 10-fold 
cross-validation our 80-paper corpus, contain- 
ing a total of 12422 sentences (classified items). 
As the first baseline, we use a standard text cat- 
egorization method for classification (where each 
sentence is considered as a document*) Baseline 1 
has an accuracy of 69%, which is low considering 
that the most frequent category (OWN) also coy- 
errs 69% of all sentences. Worse still, the classifier 
classifies almost all sentences as OWN and OTHER 
segments (the most frequent categories). Recall on 
the rare categories but important categories AIM, 
TEXTUAL, CONTRAST and BASIS is zero or very 
low. Text classification is therefore not a solution. 
*We used the Rainbow implementation of a Naive Bayes 
tf/idf method, 10-fold cross-validation. 
Baseline 2, the most frequent category (OWN), 
is a particularly bad baseline: its recall on all cate- 
gories except OWN is zero. We cannot see this bad 
performance in the percentage accuracy values, 
but only in the Kappa values (measured against 
one human annotator, i.e. k=2). As Kappa takes 
performance on rare categories into account more, 
it is a more intuitive measure for our task. 
In figure 11, NB refers to the Naive Bayes model, 
and NB+ to the Naive Bayes model augmented 
with the ngram model. We can see that the 
stochastic models obtain substantial improvement 
over the baselines, particularly with respect to pre- 
cision and recall of the rare categories, raising re- 
call considerably in all cases, while keeping preci- 
sion at the same level as Baseline 1 or improving 
it (exception: precision for BASIS drops; precision 
for AIM is insignificantly lower). 
If we look at the contribution of single features 
(reported for the Naive Bayes system in figure 12), 
we see that Agent and Action features improve 
the overall performance of the system by .02 and 
.04 Kappa points respectively (.36 to .38/.40). 
This is a good performance for single features. 
Agent is a strong feature beating both baselines. 
Taken by itself, its performance at K=.08 is still 
weaker than some other features in the pool, e.g. 
the Headline feature (K=.19), the C i tat ion fea- 
ture (K=.I8) and the Absolute Location Fea- 
ture (K=.17). (Figure 12 reports classification re- 
sults only for the stronger features, i.e. those who 
are better than Baseline 2). The Action feature, 
if considered on its own, is rather weak: it shows 
a slightly better Kappa value than Baseline 2, but 
does not even reach the level of random agreement 
(K=0). Nevertheless, if taken together with the 
other features, it still improves results. 
Building on the idea that intellectual attribu- 
tion is a segment-based phenomena, we improved 
the Agent feature by including history (feature 
SAgent). The assumption is that in unmarked sen- 
tences the agent of the previous attribution isstill 
active. Wiebe (1994) also reports segment-based 
agenthood as one of the most successful features. 
SAgent alone achieved a classification success of 
K=.21, which makes SAgent the best single fea- 
tures available in the entire feature pool. Inclusion 
of SAgent to the final model improved results to 
K=.43 (bigram model). 
Figure 12 also shows that different features are 
better at disambiguating certain categories. The 
Formulaic feature, which is not very strong on 
its own, is the most diverse, as it contributes to 
the disambiguation f six categories directly. Both 
Agent and Action features disambiguate cate-, 
gories which many of the other 12 features cannot 
disambiguate ( .g. CONTRAST), and SAgent addi- 
tionally contributes towards the determination f 
BACKGROUND zones (along with the Fo~ula ic  
and the Absolute Location feature). 
14 
P(CIFo, ..., F,~_,) ~ P(C) Nj~---?l P(FyIC) 
n- -1  
I ' I j=o P (F j )  
P(CIFo .... , F.-i ): 
P(C): 
P(FjIC): 
P(FA: 
Probability that a sentence has target category C, given its feature values F0, . . . ,  
F . - i ;  
(OveraU) probability of category C); 
Probability of feature-value pair Fj, given that the sentence is of target category C; 
Probability of feature value Fj; 
Figure 9: Naive Bayesian Classifier 
I--I F C 
P(CmlFm,o,. .,F~,~-i,C0,. . ,6~-1) ~ P(V,~lCm-l,C~-2) l-I~=?P( ~,~1 ,~) 
? " l - -1  FI~=o P(Fm,~) 
m: 
l: 
P( C,~IF~,o, . . . , F,~,~-t, Co , . . . ,  C,~-l ): 
P (C ,~IC~- , ,C~-2) :  
P(F,~j\[C,~): 
P(F~,j): 
index of sentence (ruth sentence in text) 
number of features considered 
target category associated with sentence at index m 
Probability that sentence rn has target category Cm, given its 
feature values Fro,o, . . . ,  Fmj-1 and given its context Co, ...C,~-1; 
Probability that sentence rn has target category C, given the cat- 
egories of the two previous entences; 
Probability of feature-value pair Fj occu~ing within target cate- 
gory C at position m; 
Probability of feature value Fmj; 
Figure 10: Bigram Model 
5 Discuss ion  
The result for automatic lassification is in agree- 
ment with our previous experimental results for 
human classification: humans, too, recognize the 
categories AIM and TEXTUAL most robustly (cf. 
Figure 11). AIM and TEXTUAL sentences, tating 
knowledge claims and organizing the text respec- 
tively, are conventionalized to a high degree. The 
system's results for AIM sentences, for instance, 
compares favourably to similar sentence xtraction 
experiments (cf. Kupiec et al's (1995) results of 
42%/42% recall and precision for extracting "rel- 
evant" sentences from scientific articles). BASIS 
and CONTRAST sentences have a less prototypical 
syntactic realization, and they also occur at less 
predictable places in the document. Therefore, it 
is far more difficult for both machine and human 
to recognize such sentences. 
While the system does well for AIM and TEX- 
TUAL sentences, and provides ubstantial improve- 
ment over both baselines, the difference to human 
performance is still quite large (cf. figure 11). We 
attribute most of this difference to the modest size 
of our training corpus: 80 papers are not much for 
machine learning of such high-level features. It is 
possible that a more sophisticated model, in com- 
bination with more training material, would im- 
prove results significantly. However, when we ran 
them on our data as it is now, different other sta- 
tistical models, e.g. Ripper (Cohen, 1996) and a 
Maximum Entropy model, all showed similar nu- 
merical results. 
Another factor which decreases results are in- 
consistencies in the training data: we discovered 
that 4% of the sentences with the same features 
were classified differently by the human annota- 
tion. This points to the fact that our set of fea- 
tures could be made more distinctive. In most 
of these cases, there were linguistic expressions 
present, such as subtle signs of criticism, which 
humans correctly identified, but for which the fea- 
tures are too coarse. Therefore, the addition of 
"deeper" features to the pool, which model the se- 
mantics of the meta-discourse hallowly, seemed 
a promising avenue. We consider the automatic 
and robust recognition of agents and actions, as 
presented here, to be the first incarnations of such 
features. 
6 Conc lus ions  
Argumentative zoning is the task of breaking a 
text containing a scientific argument into linear 
zones of the same argumentative status, or zones 
of the same intellectual attribution. We plan to 
use argumentative zoning as a first step for IR and 
shallow document understanding tasks like sum- 
marization. In contrast o hierarchical segmenta- 
tion (e.g. Marcu's (1997) work, which is based on 
RST (Mann and Thompson, 1987)), this type of 
segmentation aims at capturing the argumentative 
status of a piece of text in respect o the overall 
argumentative act of the paper. It does not deter- 
15 
I Method Acc. K Precision/recall per category (in %) I 
(~) AIM CONTR. TXT. OWN BACKG. BASIS OTHER 
I Human Performance 87 .71 72/56 50/55 79/79 94/92 68/75 82/34 74/83 \] 
I NB+ (best results) 71 .43 40/53 33/20 62/57 85/85 30/58 28/31 50/38 I 
I NB (best results) 7'2 .41 42/60 34/22 61/60 82/90 40/43 27/41 53/29 I 
. 
I BasoL 1: Text catog 69 13 44/9 32/42 58/14 77/90 20/5 47/12 31/16 I 
I Basel. 2: Most freq. cat. 69 -.12 0/0 0/0 0/0 69/100 0/0 0/0 0/0 I 
Figure 11: Accuracy, Kappa, Precision and Recall of Human and Automatic Processing, in comparison 
to baselines 
Features used Acc. K Precision/recallper category(in%) 
(Naive Bayes System) (%) AIM CONTR. TXT. OWN BACKG. BASIS OTHER 
Action alone 68 -.II 0/0 43/1 0/0 68/99 0/0 0/0 0/0 
Agent alone 67 .08 0/0 0/0 0/0 71/93 0/0 0/0 36/23 
Shgent alone 70 .21 0/0 17/0 0/0 74/94 53/16 0/0 46/33 
Abs. Locationalone 70 .17 0/0 0/0 0/0 74/97  40/36 0/0 28/9 
Headlinesalone 69 .19 0/0 0/0 0/0 75/95 0/0 0/0 29/25 
CitaCionalone 70 .18 0/0 0/0 0/0 73/96 0/0 0/0 43/30 
Citat2on Type alone 70 .13 0/0 0/0 0/0 72/98 0/0 0/0 43/24 
Citation Locat. alone 70 .13 0/0 0/0 0/0 72/97 0/0 0/0 43/24 
Foz~mlaicalone 70 .07 40/2 45/2 75/39 71/98 0/0 40/1 47/13 
12 other features 71 .36 37/53 32/17 54/47 81/91 39/41 22/32 45/22 
12 fea.+hction 71 .38 38/57 34/22 58/59 81/91 39/40 25/38 48/22 
12fea.+hgent 72 .40 40/57 35/18 59/51 82/91 39/43 25/34 52/29 
12fea.+SAgent 73 .40 39/57 33/19 61/51 81/91 42/43 25/33 52/29 
12 ~a.+Action+hgent 71 .43 40/53 33/20 62/57 85/85 30/58 28/31 50/38 
12 fea.+Action+Shgen~ 73 .41 41/59 34/22 62/61 82/91 41/42 27/39 51/29 
Figure 12: Accuracy, Kappa, 
individual features 
Precision and Recall of Automatic Processing (Naive Bayes system), per 
mine the rhetorical structure within zones. Sub- 
zone structure is most likely related to domain- 
specific rhetorical relations which are not directly 
relevant to the discourse-level relations we wish to 
recognize. 
We have presented a fully implemented proto- 
type for argumentative zoning. Its main inno- 
vation are two new features: prototypical agents 
and actions - -  semi-shallow representations of the 
overall scientific argumentation f the article. For 
agent and action recognition, we use syntactic 
heuristics and two extensive libraries of patterns. 
Processing is robust and very low in error. We 
evaluated the system without and with the agent 
and action features and found that the features im- 
prove results for automatic argumentative zoning 
considerably. History-aware agents are the best 
single feature in a large, extensively tested feature 
pool. 
References 
Biber, Douglas. 1995. Dimensions of Register Varia- 
tion: A Cross-linguistic Comparison. Cambridge, 
England: Cambridge University Press. 
Carletta, Jean. 1996. Assessing agreement on classi- 
fication tasks: The kappa statistic. Computational 
Linguistics 22(2): 249-.-254. 
Cohen, William W. 1996. Learning trees and rules 
with set-valued features. In Proceedings ofAAAL 
96. 
Grocer, Claire, Andrei Mikheev, and Colin Mathe- 
son. 1999. LT TTT Version 1.0: Text Tokenisa- 
tion Software. Technical report, Human Commu- 
nication Research Centre, University of Edinburgh. 
ht tp  : / /~w.  ltg. ed. ac. uk/software/ttt/.  
Hearst, Marti A. 1997. TextTiling: Segmenting text 
into multi-paragraph subtopic passages. Computa- 
tional Linguistics 23(1): 33---64. 
Hyland, Ken. 1998. Persuasion and context: The prag- 
matics of academic metadiscourse. Journal o\] Prag- 
matics 30(4): 437-455. 
Kan, Min-Yen, Judith L. Klavans, and Kathleen R. 
McKeown. 1998. Linear Segmentation and Segment 
Significance. In Proceedings o~ the Sixth Workshop 
on Very Large Corpora (COLIN G/ACL-98), 197- 
205. 
Klavans, Judith L., and Min-Yen Kan. 1998. Role 
of verbs in document analysis. In Proceedings 
of 36th Annual Meeting o\] the Association /or 
Computational Linguistics and the 17th Interna- 
tional Conference on Computational Linguistics 
(,4 CL/COLING-gS), 68O--686. 
Krippendorff, Klaus. 1980. Content Analysis: An In- 
troduction to its Methodology. Beverly Hills, CA: 
Sage Publications. 
Kupiee, Julian, Jan O. Pedersen, and Franeine Chela. 
16 
1995. A trainable document summarizer. In Pro- 
ceedings of the 18th Annual International Confer- 
ence on Research and Development in Information 
Retrieval (SIGIR-95), 68--73. 
Landis, J.R., and G.G. Koch. 1977. The Measurement 
of Observer Agreement for Categorical Data. Bio- 
metrics 33: 159-174. 
Lawrence, Steve, C. Lee Giles, and Ku_t Bollaeker. 
1999. Digital libraries and autonomous citation in- 
dexing. IEEE Computer 32(6): 67-71. 
Levin, Beth. 1993. English Verb Classes and Alterna- 
tions. Chicago, IL: University of Chicago Press. 
Mann, William C., and Sandra A. Thompson. 1987. 
Rhetorical Structure Theory: Description and Con- 
struction of text structures. In Gerard Kempen, 
ed., Natural Language Generation: New Results in 
Artificial Intelligence, Psychology, and Linguistics, 
85-95. Dordrecht, NL: Marinus Nijhoff Publishers. 
Marcu, Daniel. 1997. From Discourse Structures to 
Text Summaries. In Inderjeet Mani and Mark T. 
Maybury, eds., Proceedings of the ACL/EACL-97 
Workshop on Intelligent Scalable Text Summariza- 
tion, 82-88. 
Morris, Jane, and Graeme Hirst. 1991. Lexical cohe- 
sion computed by thesau.ral relations as an indicator 
of the structure of text. Computational Linguistics 
17: 21-48. 
Myers, Greg. 1992. In this paper we report...---speech 
acts and scientific facts. Journal of Pragmatics 
17(4): 295-313. 
:Nanba, I:Iidetsugu, and Manabu Okumura. 1999. To- 
wards multi-paper summarization using reference 
in.formation. In Proceedings of IJCAI-99, 926- 
931. http://galaga, jaist, ac. jp: 8000/'nanba/ 
study/papers .html. 
Paice, Chris D. 1981. The automatic generation of 
literary abstracts: an approach based on the iden- 
tification of self-indicating phrases. In Robert Nor- 
man Oddy, Stephen E. Robertson, Cornelis Joost 
van Pdjsbergen, and P. W. Williams, eds., Infor- 
mation Retrieval Research, 172-191. London, UK: 
Butterworth. 
Paice, Chris D. 1990. Constructing literature abstracts 
by computer: techniques and prospects. Informa- 
tion Processing and Management 26: 171-186. 
Reynar, Jeffrey C. 1999. Statistical models for topic 
segmentation. In Proceedings of the 37th Annual 
Meeting of the Association for Computational Lin- 
guistics (A CL-99), 357-364. 
Riley, Kathryn. 1991. Passive voice and rhetorical role 
in scientific writing. Journal of Technical Writing 
and Communication 21(3): 239--257. 
Rowley, Jennifer. 1982. Abstracting and Indexing. 
London, UK: Bingley. 
Siegel, Sidney, and N. John Jr. CasteUan. 1988. Non- 
parametric Statistics for the Behavioral Sciences. 
Berkeley, CA: McGraw-Hill, 2nd edn. 
Swales, John. 1990. Genre Analysis: English in Aca- 
demic and Research Settings. Chapter 7: Research 
articles in English, 110-.-176. Cambridge, UK: Cam- 
bridge University Press. 
Teufel, Simone, Jean Carletta, and Marc Moens. 1999. 
An annotation scheme for discourse-level argumen- 
tation in research articles. In Proceedings of the 8th 
Meeting of the European Chapter of the Association 
for Computational Linguistics (EA CL-99), 110-117. 
Teufel, Simone, and Marc Moens. 1999. Argumenta- 
tive classification of extracted sentences as a first 
step towards flexible abstracting. In Inderjeet Mani 
and Mark T. Maybury, eds., Advances in Auto- 
matic Text Summarization, 155-171. Cambridge, 
MA: MIT Press. 
Thompson, Geoff, and Ye Yiyun. 1991. Evaluation in 
the reporting verbs used in academic papers. Ap- 
plied Linguistics 12(4): 365-382. 
Wellons, M. E., and G. P. Purcell. 1999. Task-specific 
extracts for using the medical iterature. In Pro- 
ceedings of the American Medical Informatics Sym- 
posium, 1004-1008. 
Wiebe, Janyce. 1994. Tracking point of view in narra- 
tive. Computational Linguistics 20(2): 223-287. 
17 
Improvements in Automatic Thesaurus Extraction
James R. Curran and Marc Moens
Institute for Communicating and Collaborative Systems
University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW
United Kingdom
  jamesc,marc  @cogsci.ed.ac.uk
Abstract
The use of semantic resources is com-
mon in modern NLP systems, but methods
to extract lexical semantics have only re-
cently begun to perform well enough for
practical use. We evaluate existing and
new similarity metrics for thesaurus ex-
traction, and experiment with the trade-
off between extraction performance and
efficiency. We propose an approximation
algorithm, based on canonical attributes
and coarse- and fine-grained matching,
that reduces the time complexity and ex-
ecution time of thesaurus extraction with
only a marginal performance penalty.
1 Introduction
Thesauri have traditionally been used in information
retrieval tasks to expand words in queries with syn-
onymous terms (e.g. Ruge, (1997)). Since the de-
velopment of WordNet (Fellbaum, 1998) and large
electronic thesauri, information from semantic re-
sources is regularly leveraged to solve NLP prob-
lems. These tasks include collocation discovery
(Pearce, 2001), smoothing and model estimation
(Brown et al, 1992; Clark and Weir, 2001) and text
classification (Baker and McCallum, 1998).
Unfortunately, thesauri are expensive and time-
consuming to create manually, and tend to suffer
from problems of bias, inconsistency, and limited
coverage. In addition, thesaurus compilers cannot
keep up with constantly evolving language use and
cannot afford to build new thesauri for the many sub-
domains that NLP techniques are being applied to.
There is a clear need for methods to extract thesauri
automatically or tools that assist in the manual cre-
ation and updating of these semantic resources.
Much of the existing work on thesaurus extraction
and word clustering is based on the observation that
related terms will appear in similar contexts. These
systems differ primarily in their definition of ?con-
text? and the way they calculate similarity from the
contexts each term appears in.
Most systems extract co-occurrence and syntactic
information from the words surrounding the target
term, which is then converted into a vector-space
representation of the contexts that each target term
appears in (Pereira et al, 1993; Ruge, 1997; Lin,
1998b). Other systems take the whole document
as the context and consider term co-occurrence at
the document level (Crouch, 1988; Sanderson and
Croft, 1999). Once these contexts have been de-
fined, these systems then use clustering or nearest
neighbour methods to find similar terms.
Alternatively, some systems are based on the ob-
servation that related terms appear together in par-
ticular contexts. These systems extract related terms
directly by recognising linguistic patterns (e.g. X, Y
and other Zs) which link synonyms and hyponyms
(Hearst, 1992; Caraballo, 1999).
Our previous work (Curran and Moens, 2002) has
evaluated thesaurus extraction performance and effi-
ciency using several different context models. In this
paper, we evaluate some existing similarity metrics
and propose and motivate a new metric which out-
performs the existing metrics. We also present an
approximation algorithm that bounds the time com-
plexity of pairwise thesaurus extraction. This re-
sults in a significant reduction in runtime with only
a marginal performance penalty in our experiments.
                     July 2002, pp. 59-66.  Association for Computational Linguistics.
                     ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia,
                  Unsupervised Lexical Acquisition: Proceedings of the Workshop of the
2 Automatic Thesaurus Extraction
Vector-space thesaurus extraction systems can be
separated into two components. The first compo-
nent extracts the contexts from raw text and com-
piles them into a statistical description of the con-
texts each potential thesaurus term appears in. Some
systems define the context as a window of words
surrounding each thesaurus term (McDonald, 2000).
Many systems extract grammatical relations using
either a broad coverage parser (Lin, 1998a) or shal-
low statistical tools (Grefenstette, 1994; Curran and
Moens, 2002). Our experiments use a shallow rela-
tion extractor based on (Grefenstette, 1994).
We define a context relation instance as a tuple
(w, r,w?) where w is the thesaurus term, which oc-
curs in some grammatical relation r with another
word w? in the sentence. We refer to the tuple (r,w?)
as an attribute of w. For example, the tuple (dog,
direct-obj, walk) indicates that the term dog was
the direct object of the verb walk.
Our relation extractor begins with a Na??ve Bayes
POS tagger and chunker. After the raw text has
been tagged and chunked, noun phrases separated
by prepositions and conjunctions are concatenated,
and the relation extracting algorithm is run over each
sentence. This consists of four passes over the sen-
tence, associating each noun with the modifiers and
verbs from the syntactic contexts they appear in:
1. nouns with pre-modifiers (left to right)
2. nouns with post-modifiers (right to left)
3. verbs with subjects/objects (right to left)
4. verbs with subjects/objects (left to right)
This results in tuples representing the contexts:
1. term is the subject of a verb
2. term is the (direct/indirect) object of a verb
3. term is modified by a noun or adjective
4. term is modified by a prepositional phrase
The relation tuple is then converted to root form
using the Sussex morphological analyser (Minnen
et al, 2000) and the POS tags are removed. The
relations for each term are collected together and
counted, producing a context vector of attributes and
(adjective, good) 2005
(adjective, faintest) 89
(direct-obj, have) 1836
(indirect-obj, toy) 74
(adjective, preconceived) 42
(adjective, foggiest) 15
Figure 1: Example attributes of the noun idea
their frequencies in the corpus. Figure 1 shows some
example attributes for idea.
The second system component performs nearest-
neighbour or cluster analysis to determine which
terms are similar based on their context vectors.
Both methods require a function that calculates the
similarity between context vectors. For experimen-
tal analysis we have decomposed this function into
measure and weight functions. The measure func-
tion calculates the similarity between two weighted
context vectors and the weight function calculates a
weight from the raw frequency information for each
context relation. The primary experiments in this
paper evaluate the performance of various existing
and new measure and weight functions, which are
described in the next section.
The simplest algorithm for thesaurus extraction is
nearest-neighbour comparison, which involves pair-
wise vector comparison of the target with every ex-
tracted term. Given n terms and up to m attributes
for each term, the asymptotic time complexity of
nearest-neighbour thesaurus extraction is O(n2m).
This is very expensive with even a moderate vocab-
ulary and small attribute vectors. The number of
terms can be reduced by introducing a minimum cut-
off that ignores potential synonyms with a frequency
less than the cutoff, which for our experiments was
5. Section 5 reports on the trade-off between the
minimum cutoff and execution time.
3 Experiments
Early experiments in thesaurus extraction (Grefen-
stette, 1994) suffered from the limited size of avail-
able corpora, but more recent experiments have
used much larger corpora with greater success (Lin,
1998a). For these experiments we ran our relation
extractor over the British National Corpus (BNC)
consisting of 114 million words in 6.2 million sen-
tences. The POS tagging and chunking took 159
minutes, and the relation extraction took an addi-
SETCOSINE |(wm,?,?)?(wn,?,?)|?|(wm,?,?)|?|(wn,?,?)|
COSINE
?
(r,w? ) wgt(wm,?r,?w? )?wgt(wn,?r ,?w? )?
?wgt(wm,?,?)2??wgt(wn,?,?)2
SETDICE 2|(wm,?,?)?(wn,?,?)||(wm,?,?)|+|(wn,?,?)|
DICE
?
(r,w? ) wgt(wm,?r,?w? )?wgt(wn,?r ,?w? )
?
(r,w? ) wgt(wm,?r,?w? )+wgt(wn,?r ,?w? )
DICE? 2
?
(r,w? ) min(wgt(wm,?r ,?w? ),wgt(wn,?r,?w? ))
?
(r,w? ) wgt(wm,?r,?w? )+wgt(wn,?r ,?w? )
SETJACCARD |(wm,?,?)?(wn,?,?)||(wm,?,?)?(wn,?,?)|
JACCARD
?
(r,w? ) min(wgt(wm,?r,?w? ),wgt(wn,?r,?w? ))
?
(r,w?) max(wgt(wm,?r,?w? ),wgt(wn,?r ,?w? ))
JACCARD?
?
(r,w? ) wgt(wm,?r,?w? )?wgt(wn,?r ,?w? )
?
(r,w? ) wgt(wm,?r,?w? )+wgt(wn,?r ,?w? )
LIN
?
(r,w? ) wgt(wm,?r,?w? )+wgt(wn,?r ,?w? )
?wgt(wm,?,?)+?wgt(wn,?,?)
Table 1: Measure functions evaluated
tional 7.5 minutes. The resultant representation con-
tained a total of 28 million relation occurrences over
10 million different relations.
We describe the functions evaluated in these ex-
periments using an extension of the asterisk notation
used by Lin (1998a), where an asterisk indicates a
set ranging over all existing values of that variable.
For example, the set of attributes of the term w is:
(w, ?, ?) ? {(r,w?) | ?(w, r,w?)}
For convenience, we further extend the notation for
weighted attribute vectors. A subscripted asterisk
indicates that the variables are bound together:
?
(r,w?)
wgt(wm, ?r, ?w? ) ? wgt(wn, ?r, ?w? )
which is a notational abbreviation of:
?
(r,w?)?(wm,?,?)?(wn,?,?)
wgt(wm, r,w?) ? wgt(wn, r,w?)
For weight functions we use similar notation:
f (w, ?, ?) ?
?
(r,w?)?(w,?,?)
f (w, r,w?)
n(w, ?, ?) ? |(w, ?, ?)|
Nw ? |{w | ?(w, ?, ?) , ?}|
Table 1 defines the measure functions evaluated in
these experiments. The simplest measure func-
tions (prefix SET) use the attribute set model from
IDENTITY 1.0
CHI2 cf. Manning and Sch?utze (1999)
LR cf. Manning and Sch?utze (1999)
LIN98A log( f (w,r,w?) f (?,r,?)f (?,r,w?) f (w,r,?) )
LIN98B ? log( n(?,r,w?)Nw )
DICE 2p(w,r,w?)p(w,?,?)+p(?,r,w?)
GREF94 log2( f (w,r,w?)+1)log2(n(?,r,w?)+1)
MI log( p(w,r,w?)p(w,?,?)p(?,r,w?) )
TTEST p(w,r,w?)?p(?,r,w?)p(w,?,?)?p(?,r,w?)p(w,?,?)
Table 2: Weight functions evaluated
IR and are taken from Manning and Schu?tze (1999),
pp. 299. When these are used with weighted at-
tributes, if the weight is greater than zero, then it is
considered in the set. Other measures, such as LIN
and JACCARD have previously been used for the-
saurus extraction (Lin, 1998a; Grefenstette, 1994).
Finally, we have generalised some set measures us-
ing similar reasoning to Grefenstette (1994). Alter-
native generalisations are marked with a dagger.
These experiments also cover a range of weight
functions as defined in Table 2. The weight func-
tions LIN98A, LIN98B, and GREF94 are taken
from existing systems (Lin, 1998a; Lin, 1998b;
Grefenstette, 1994). Our proposed weight func-
tions are motivated by our intuition that highly pre-
dictive attributes are strong collocations with their
terms. Thus, we have implemented many of the
statistics described in the Collocations chapter of
Manning and Schu?tze (1999), including the T-Test,
?2-Test, Likelihood Ratio, and Mutual Informa-
tion. Some functions (suffix LOG) have an extra
log2( f (w, r,w?) + 1) factor to promote the influence
of higher frequency attributes.
4 Evaluation
For the purposes of evaluation, we selected 70
single-word noun terms for thesaurus extraction. To
avoid sample bias, the words were randomly se-
lected from WordNet such that they covered a range
of values for the following word properties:
Word PTB Rank PTB # BNC # Reuters # Macquarie # WordNet # Min / Max WordNet subtree roots
company 38 4076 52779 456580 8 9 3 / 6 entity, group, state
interest 138 919 37454 146043 12 12 3 / 8 abs., act, group, poss., state
problem 418 622 56361 63333 4 3 3 / 7 abs., psych., state
change 681 406 35641 55081 8 10 2 / 12 abs., act, entity, event, phenom.
idea 1227 134 32754 13527 10 5 3 / 7 entity, psych.
radio 2278 59 9046 20913 2 3 6 / 8 entity
star 5130 29 8301 6586 11 7 4 / 8 abs., entity
knowledge 5197 19 14580 2813 3 1 1 / 1 psych.
pants 13264 5 429 282 3 2 6 / 9 entity
tightness 30817 1 119 2020 5 3 4 / 5 abs., state
Table 3: Examples of the 70 thesaurus evaluation terms
frequency Penn Treebank and BNC frequencies;
number of senses WordNet and Macquarie senses;
specificity depth in the WordNet hierarchy;
concreteness distribution across WordNet subtrees.
Table 3 lists some example terms with frequency
and frequency rank data from the PTB, BNC and
REUTERS, as well as the number of senses in Word-
Net and Macquarie, and their maximum and mini-
mum depth in the WordNet hierarchy. For each term
we extracted a thesaurus entry with 200 potential
synonyms and their similarity scores.
The simplest method of evaluation is direct com-
parison of the extracted thesaurus with a manually-
created gold standard (Grefenstette, 1994). How-
ever, on small corpora, rare direct matches provide
limited information for evaluation, and thesaurus
coverage is a problem. Our evaluation uses a com-
bination of three electronic thesauri: the Macquarie
(Bernard, 1990), Roget?s (Roget, 1911) and Moby
(Ward, 1996) thesauri. Roget?s and Macquarie are
topic ordered and the Moby thesaurus is head or-
dered. As the extracted thesauri do not distinguish
between senses, we transform Roget?s and Mac-
quarie into head ordered format by conflating the
sense sets containing each term. For the 70 terms
we create a gold standard from the union of the syn-
onyms from the three thesauri.
With this gold standard in place, it is possible
to use precision and recall measures to evaluate the
quality of the extracted thesaurus. To help overcome
the problems of direct comparisons we use several
measures of system performance: direct matches
(DIRECT), inverse rank (INVR), and precision of the
top n synonyms (P(n)), for n = 1, 5 and 10.
Measure DIRECT P(1) P(5) P(10) INVR
SETCOSINE 1276 14% 15% 15% 0.76
SETDICE 1496 63% 44% 34% 1.69
SETJACCARD 1458 59% 43% 34% 1.63
COSINE 1276 14% 15% 15% 0.76
DICE 1536 19% 20% 20% 0.97
DICE? 1916 76% 52% 45% 2.10
JACCARD 1916 76% 52% 45% 2.10
JACCARD? 1745 40% 30% 28% 1.36
LIN 1826 60% 46% 40% 1.85
Table 4: Evaluation of measure functions
INVR is the sum of the inverse rank of each
matching synonym, e.g. matching synonyms at
ranks 3, 5 and 28 give an inverse rank score of
1
3 +
1
5 +
1
28 , and with at most 200 synonyms, the max-imum INVR score is 5.878. Precision of the top n is
the percentage of matching synonyms in the top n
extracted synonyms. There are a total of 23207 syn-
onyms for the 70 terms in the gold standard. Each
measure is averaged over the extracted synonym lists
for all 70 thesaurus terms.
5 Results
For computational practicality, we assume that the
performance behaviour of measure and weight func-
tions are independent of each other. Therefore, we
have evaluated the weight functions using the JAC-
CARD measure, and evaluated the measure functions
using the TTEST weight because they produced the
best results in our previous experiments.
Table 4 presents the results of evaluating the mea-
sure functions. The best performance across all mea-
sures was shared by JACCARD and DICE?, which
produced identical results for the 70 words. DICE?
is easier to compute and is thus the preferred mea-
sure function.
Table 5 presents the results of evaluating the
Weight DIRECT P(1) P(5) P(10) INVR
CHI2 1623 33% 27% 26% 1.24
DICE 1480 61% 45% 34% 1.70
DICELOG 1498 67% 45% 35% 1.73
GREF94 1258 54% 38% 29% 1.46
IDENTITY 1228 46% 34% 29% 1.33
LR 1510 53% 39% 32% 1.58
LIN98A 1735 73% 50% 42% 1.96
LIN98B 1271 47% 34% 30% 1.37
MI 1736 66% 49% 42% 1.92
MILOG 1841 71% 52% 43% 2.05
TTEST 1916 76% 52% 45% 2.10
TTESTLOG 1865 70% 49% 41% 1.99
Table 5: Evaluation of bounded weight functions
Weight DIRECT P(1) P(5) P(10) INVR
MI? 1511 59% 44% 39% 1.74
MILOG? 1566 61% 46% 41% 1.84
TTEST? 1670 67% 50% 43% 1.96
TTESTLOG? 1532 63% 50% 42% 1.89
Table 6: Evaluation of unbounded weight functions
weight functions. Here TTEST significantly outper-
formed the other weight functions, which supports
our intuition that good context descriptors are also
strong collocates of the term. Surprisingly, the other
collocation discovery functions did not perform as
well, even though TTEST is not the most favoured
for collocation discovery because of its behaviour at
low frequency counts.
One difficulty with weight functions involving
logarithms or differences is that they can be nega-
tive. The results in Table 6 show that weight func-
tions that are not bounded below by zero do not per-
form as well on thesaurus extraction. However, un-
bounded weights do produce interesting and unex-
pected results: they tend to return misspellings of
the term and synonyms, abbreviations and lower fre-
quency synonyms. For instance, TTEST? returned
Co, Co. and PLC for company, but they do not ap-
pear in the synonyms extracted with TTEST. The
unbounded weights also extracted more hyponyms,
such as corporation names for company, includ-
ing Kodak and Exxon. Finally unbounded weights
tended to promote the rankings of synonyms from
minority senses because the frequent senses are de-
moted by negative weights. For example, TTEST?
returned writings, painting, fieldwork, essay
and masterpiece as the best synonyms for work,
whereas TTEST returned study, research, job,
activity and life.
0 25 50 75 100 125 150 175 200
Minimum Frequency Cutoff
1600
1700
1800
1900
2000
D
ir
ec
t M
at
ch
es
TTest matches
Lin98b matches
500
1000
1500
2000
2500
3000
R
un
 T
im
e 
(s
ec
on
ds
)
TTest time
Lin98b time
Figure 2: Performance against minimum cutoff
Introducing a minimum cutoff that ignores low
frequency potential synonyms can eliminate many
unnecessary comparisons. Figure 2 presents both
the performance of the system using direct match
evaluation (left axis) and execution times (right axis)
for increasing cutoffs. This test was performed using
JACCARD and the TTEST and LIN98A weight func-
tions. The first feature of note is that as we increase
the minimum cutoff to 30, the direct match results
improve for TTEST, which is probably a result of
the TTEST weakness on low frequency counts. Ini-
tially, the execution time is rapidly reduced by small
increments of the minimum cutoff. This is because
Zipf?s law applies to relations, and so by small incre-
ments of the cutoff we eliminate many terms from
the tail of the distribution. There are only 29,737
terms when the cutoff is 30; 88,926 terms when the
cutoff is 5; and 246,067 without a cutoff, and be-
cause the extraction algorithm is O(n2m), this re-
sults in significant efficiency gains. Since extracting
only 70 thesaurus terms takes about 43 minutes with
a minimum cutoff of 5, the efficiency/performance
trade-off is particularly important from the perspec-
tive of implementing a practical extraction system.
6 Efficiency
Even with a minimum cutoff of 30 as a reason-
able compromise between speed and accuracy, ex-
tracting a thesaurus for 70 terms takes approxi-
mately 20 minutes. If we want to extract a com-
plete thesaurus for 29,737 terms left after the cut-
off has been applied, it would take approximately
one full week of processing. Given that the size
of the training corpus could be much larger (cf.
Curran and Moens (2002)), which would increase
both number of attributes for each term and the total
number of terms above the minimum cutoff, this is
not nearly fast enough. The problem is that the time
complexity of thesaurus extraction is not practically
scalable to significantly larger corpora.
Although the minimum cutoff helps by reduc-
ing n to a reasonably small value, it does not con-
strain m in any way. In fact, using a cutoff in-
creases the average value of m across the terms be-
cause it removes low frequency terms with few at-
tributes. For instance, the frequent company ap-
pears in 11360 grammatical relations, with a total
frequency of 69240 occurrences, whereas the infre-
quent pants appears in only 401 relations with a to-
tal frequency of 655 occurrences.
The problem is that for every comparison, the al-
gorithm must examine the length of both attribute
vectors. Grefenstette (1994) uses bit signatures to
test for shared attributes, but because of the high fre-
quency of the most common attributes, this does not
skip many comparisons. Our system keeps track of
the sum of the remaining vector which is a signifi-
cant optimisation, but comes at the cost of increased
representation size. However, what is needed is
some algorithmic reduction that bounds the number
of full O(m) vector comparisons performed.
7 Approximation Algorithm
One way of bounding the complexity is to perform
an approximate comparison first. If the approxima-
tion returns a positive result, then the algorithm per-
forms the full comparison. We can do this by in-
troducing another, much shorter vector of canoni-
cal attributes, with a bounded length k. If our ap-
proximate comparison returns at most p positive re-
sults for each term, then the time complexity be-
comes O(n2k + npm), which, since k is constant, is
O(n2 + npm). So as long as we find an approxima-
tion function and vector such that p  n, the system
will run much faster and be much more scalable in
m, the number of attributes. However, p  n im-
plies that we are discarding a very large number of
potential matches and so there will be a performance
penalty. This trade-off is governed by the number of
the canonical attributes and how representative they
are of the full attribute vector, and thus the term it-
(adjective, smarty) 3 0.0524
(direct-obj, pee) 3 0.0443
(noun-mod, loon) 5 0.0437
(direct-obj, wet) 14 0.0370
(direct-obj, scare) 10 0.0263
(adjective, jogging) 5 0.0246
(indirect-obj, piss) 4 0.0215
(noun-mod, ski) 14 0.0201
Figure 3: The top weighted attributes of pants
(direct-obj, wet) 14 0.0370
(direct-obj, scare) 10 0.0263
(direct-obj, wear) 17 0.0071
(direct-obj, keep) 7 0.0016
(direct-obj, get) 5 0.0004
Figure 4: Canonical attributes for pants
self. It is also dependent on the functions used to
compare the canonical attribute vectors.
The canonical vector must contain attributes that
best describe the thesaurus term in a bounded num-
ber of entries. The obvious first choice is the
most strongly weighted attributes from the full vec-
tor. Figure 3 shows some of the most strongly
weighted attributes for pants with their frequencies
and weights. However, these attributes, although
strongly correlated with pants, are in fact too spe-
cific and idiomatic to be a good summary, because
there are very few other words with similar canoni-
cal attributes. For example, (adjective, smarty)
only appears with two other terms (bun and number)
in the entire corpus. The heuristic is so aggressive
that too few positive approximate matches result.
To alleviate this problem we filter the attributes so
that only strongly weighted subject, direct-obj
and indirect-obj relations are included in the
canonical vectors. This is because in general they
constrain the terms more and partake in fewer id-
iomatic collocations with the terms. So the gen-
eral principle is the most descriptive verb relations
constrain the search for possible synonyms, and the
other modifiers provide finer grain distinctions used
to rank possible synonyms. Figure 4 shows the
5 canonical attributes for pants. This canonical
vector is a better general description of the term
pants, since similar terms are likely to appear as
the direct object of wear, even though it still con-
tains the idiomatic attributes (direct-obj, wet)
and (direct-obj, scare).
One final difficulty this example shows is that at-
Word DIRECT BIG / MAX P(1) P(5) P(10) INVR BIG / MAX
company 27 110 / 355 100 % 80 % 60 % 2.60 2.71 / 6.45
interest 64 232 / 730 100 % 80 % 70 % 3.19 3.45 / 7.17
problem 25 82 / 250 100 % 60 % 50 % 2.46 2.52 / 6.10
change 31 104 / 544 100 % 60 % 40 % 2.35 2.44 / 6.88
idea 59 170 / 434 100 % 100 % 80 % 3.67 3.87 / 6.65
radio 19 45 / 177 100 % 60 % 60 % 2.31 2.35 / 5.76
star 31 141 / 569 100 % 60 % 60 % 2.36 2.49 / 6.92
knowledge 26 56 / 151 100 % 80 % 70 % 2.50 2.55 / 5.60
pants 12 13 / 222 100 % 80 % 50 % 2.40 2.40 / 5.98
tightness 3 3 / 152 0 % 0 % 0 % 0.03 0.03 / 5.60
Average (over 70) 26 86 / 332 76 % 52 % 44 % 2.08 2.17 / 6.13
Table 7: Example performance using techniques described in this paper
tributes like (direct-obj, get) are not informa-
tive. We know this because (direct-obj, get) ap-
pears with 8769 different terms, which means the
algorithm may perform a large number of unnec-
essary full comparisons since (direct-obj, get)
could be a canonical attribute for many terms. To
avoid this problem, we apply a maximum cutoff on
the number of terms the attribute appears with.
With limited experimentation, we have found that
TTESTLOG is the best weight function for selecting
canonical attributes. This may be because the extra
log2( f (w, r,w?) + 1) factor encodes the desired bias
towards relatively frequent canonical attributes. If a
canonical attribute is shared by the two terms, then
our algorithm performs the full comparison.
Figure 5 shows system performance and speed,
as canonical vector size is increased, with the maxi-
mum cutoff at 4000, 8000, and 10,000. As an exam-
ple, with a maximum cutoff of 10,000 and a canoni-
cal vector size of 70, the total DIRECT score of 1841
represents a 3.9% performance penalty over full ex-
traction, for an 89% reduction in execution time. Ta-
ble 7 presents the example term results using the
techniques we have described: JACCARD measure
and TTEST weight functions; minimum cutoff of 30;
and approximation algorithm with canonical vector
size of 100 with TTESTLOG weighting. The BIG
columns show the previous measure results if we re-
turned 10,000 synonyms, and MAX gives the results
for a comparison of the gold standard against itself.
8 Conclusion
In these experiments we have proposed new mea-
sure and weight functions that, as our evaluation has
shown, significantly outperform existing similarity
0 20 40 60 80 100 120 140 160
Canonical Set Size
1000
1100
1200
1300
1400
1500
1600
1700
1800
1900
2000
D
ir
ec
t M
at
ch
es
MaxCutoff 4000 matches
MaxCutoff 8000 matches
MaxCutoff 10000 matches
0
50
100
150
200
250
300
350
400
450
500
R
un
 T
im
e 
(s
ec
on
ds
)
MaxCutoff 4000 times
MaxCutoff 8000 times
MaxCutoff 10000 times
Figure 5: Performance against canonical set size
functions. The list of measure and weight functions
we compared against is not complete, and we hope
to add other functions to provide a general frame-
work for thesaurus extraction experimentation. We
would also like to expand our evaluation to include
direct methods used by others (Lin, 1998a) and us-
ing the extracted thesaurus in NLP tasks.
We have also investigated the speed/performance
trade-off using frequency cutoffs. This has lead to
the proposal of a new approximate comparison algo-
rithm based on canonical attributes and a process of
coarse- and fine-grained comparisons. This approx-
imation algorithm is dramatically faster than simple
pairwise comparison, with only a small performance
penalty, which means that complete thesaurus ex-
traction on large corpora is now feasible. Further,
the canonical vector parameters allow for control of
the speed/performance trade-off. These experiments
show that large-scale thesaurus extraction is practi-
cal, and although results are not yet comparable with
manually-constructed thesauri, may now be accurate
enough to be useful for some NLP tasks.
Acknowledgements
We would like to thank Stephen Clark, Caroline
Sporleder, Tara Murphy and the anonymous review-
ers for their comments on drafts of this paper. This
research is supported by Commonwealth and Syd-
ney University Travelling scholarships.
References
L. Douglas Baker and Andrew McCallum. 1998. Distri-
butional clustering of words for text classification. In
Proceedings of the 21st annual international ACM SI-
GIR conference on Research and Development in In-
formation Retrieval, pages 96?103, Melbourne, Aus-
tralia, 24?28 August.
John R. L. Bernard, editor. 1990. The Macquarie Ency-
clopedic Thesaurus. The Macquarie Library, Sydney,
Australia.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jennifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479, December.
Sharon A. Caraballo. 1999. Automatic construction of
a hypernym-labeled noun hierarchy from text. In Pro-
ceedings of the 37th annual meeting of the Association
for Computational Linguistics, pages 120?126, Col-
lege Park, MD USA, 20?26 June.
Stephen Clark and David Weir. 2001. Class-based prob-
ability estimation using a semantic hierarchy. In Pro-
ceedings of the Second Meeting of the North American
Chapter of the Association for Computational Linguis-
tics, pages 95?102, Pittsburgh, PA USA, 2?7 June.
Carolyn J. Crouch. 1988. Construction of a dynamic the-
saurus and its use for associated information retrieval.
In Proceedings of the eleventh international confer-
ence on Research and Development in Information Re-
trieval, pages 309?320, Grenoble, France, 13?15 June.
James R. Curran and Marc Moens. 2002. Scaling con-
text space. In Proceedings of the 40th annual meet-
ing of the Association for Computational Linguistics,
Philadelphia, PA USA, 7?12 July. (to appear).
Cristiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. The MIT Press, Cambridge,
MA USA.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers,
Boston, USA.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th international conference on Computational Lin-
guistics, pages 539?545, Nantes, France, 23?28 July.
Dekang Lin. 1998a. Automatic retrieval and cluster-
ing of similar words. In Proceedings of the 17th In-
ternational Conference on Computational Linguistics
and of the 36th Annual Meeting of the Association for
Computational Linguistics, pages 768?774, Montre?al,
Que?bec, Canada, 10?14 August.
Dekang Lin. 1998b. An information-theoretic definition
of similarity. In Proceedings of the Fifteen Interna-
tional Conference on Machine Learning, pages 296?
304, Madison, WI USA, 24?27 July.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press, Cambridge, MA USA.
Scott McDonald. 2000. Environmental determinants of
lexical processing effort. Ph.D. thesis, University of
Edinburgh.
Guido Minnen, John Carroll, and Darren Pearce. 2000.
Robust applied morphological generation. In In Pro-
ceedings of the First International Natural Language
Generation Conference, pages 201?208, 12?16 June.
Darren Pearce. 2001. Synonymy in collocation extrac-
tion. In Workshop on WordNet and Other Lexical
Resources: Applications, Extensions and Customiza-
tions, (NAACL 2001), pages 41?46, Pittsburgh, PA
USA, 2?7 June.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Pro-
ceedings of the 31st annual meeting of the Associ-
ation for Computational Linguistics, pages 183?190,
Columbus, Ohio USA, 22?26 June.
Peter Roget. 1911. Thesaurus of English words and
phrases. Longmans, Green and Co., London, UK.
Gerda Ruge. 1997. Automatic detection of thesaurus re-
lations for information retrieval applications. In Foun-
dations of Computer Science: Potential - Theory -
Cognition, Lecture Notes in Computer Science, vol-
ume LNCS 1337, pages 499?506. Springer Verlag,
Berlin, Germany.
Mark Sanderson and Bruce Croft. 1999. Deriving con-
cept hierarchies from text. In Proceedings of the
22nd annual international ACM SIGIR conference on
Research and Development in Information Retrieval,
pages 206?213, Berkeley, CA USA, 15?19 August.
Grady Ward. 1996. Moby Thesaurus. Moby Project.
