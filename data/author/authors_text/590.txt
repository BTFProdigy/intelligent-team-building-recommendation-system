Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1321?1330,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Language Acquisition and Probabilistic Models: keeping it simple
Aline Villavicencio?, Marco Idiart?Robert Berwick?, Igor Malioutov?
?Institute of Informatics, Federal University of Rio Grande do Sul (Brazil)
?Institute of Physics, Federal University of Rio Grande do Sul (Brazil)
?LIDS, Dept. of EECS, Massachusetts Institute of Technology (USA)
? CSAIL, Dept. of EECS, Massachusetts Institute of Technology (USA)
avillavicencio@inf.ufrgs.br, marco.idiart@if.ufrgs.br
berwick@csail.mit.edu, igorm@mit.edu
Abstract
Hierarchical Bayesian Models (HBMs)
have been used with some success
to capture empirically observed pat-
terns of under- and overgeneralization
in child language acquisition. How-
ever, as is well known, HBMs are
?ideal? learning systems, assuming ac-
cess to unlimited computational re-
sources that may not be available
to child language learners. Conse-
quently, it remains crucial to carefully
assess the use of HBMs along with al-
ternative, possibly simpler, candidate
models. This paper presents such
an evaluation for a language acquisi-
tion domain where explicit HBMs have
been proposed: the acquisition of En-
glish dative constructions. In particu-
lar, we present a detailed, empirically-
grounded model-selection compari-
son of HBMs vs. a simpler alternative
based on clustering along with max-
imum likelihood estimation that we
call linear competition learning (LCL).
Our results demonstrate that LCL can
match HBM model performance with-
out incurring on the high computa-
tional costs associated with HBMs.
1 Introduction
In recent years, with advances in probability
and estimation theory, there has been much
interest in Bayesian models (BMs) (Chater,
Tenenbaum, and Yuille, 2006; Jones and
Love, 2011) and their application to child lan-
guage acquisition with its challenging com-
bination of structured information and in-
complete knowledge, (Perfors, Tenenbaum,
and Wonnacott, 2010; Hsu and Chater, 2010;
Parisien, Fazly, and Stevenson, 2008; Parisien
and Stevenson, 2010) as they offer several ad-
vantages in this domain. They can readily
handle the evident noise and ambiguity of ac-
quisition input, while at the same time pro-
viding efficiency via priors that mirror known
pre-existing language biases. Further, hierar-
chical Bayesian Models (HBMs) can combine
distinct abstraction levels of linguistic knowl-
edge, from variation at the level of individ-
ual lexical items, to cross-item variation, using
hyper-parameters to capture observed pat-
terns of both under- and over-generalization
as in the acquisition of e.g. dative alterna-
tions in English (Hsu and Chater, 2010; Per-
fors, Tenenbaum, and Wonnacott, 2010), and
verb frames in a controlled artificial language
(Wonnacott, Newport, and Tanenhaus, 2008).
HBMs can thus be viewed as providing a
?rational? upper bound on language learn-
ability, yielding optimal models that account
for observed data while minimizing any re-
quired prior information. In addition, the
clustering implicit in HBM modeling intro-
duces additional parameters that can be tuned
to specific data patterns. However, this comes
at a well-known price: HBMs generally are
also ideal learning systems, known to be
computationally infeasible (Kwisthout, Ware-
ham, and van Rooij, 2011). Approximations
proposed to ensure computational tractabil-
ity, like reducing the number of classes that
need to be learned may also be linguisti-
cally and cognitively implausible. For in-
stance, in terms of verb learning, this could
1321
take the form of reducing the number of sub-
categorization frames to the relevant subset,
as in (Perfors, Tenenbaum, and Wonnacott,
2010), where only 2 frames are considered for
?take?, when in fact it is listed in 6 frames
by Levin (1993). Finally, comparison of vari-
ous Bayesian models of the same task is rare
(Jones and Love, 2011) and Bayesian infer-
ence generally can be demonstrated as sim-
ply one class of regularization or smooth-
ing techniques among many others; given the
problem at hand, there may well be other,
equally compelling regularization methods
for dealing with the bias-variance dilemma
(e.g., SVMs (Shalizi, 2009)). Consequently, the
relevance of HBMs for cognitively accurate ac-
counts of human learning remains uncertain
and needs to be carefully assessed.
Here we argue that the strengths of HBMs
for a given task must be evaluated in light of
their computational and cognitive costs, and
compared to other viable alternatives. The fo-
cus should be on finding the simplest statis-
tical models consistent with a given behav-
ior, particularly one that aligns with known
cognitive limitations. In the case of many
language acquisition tasks this behavior often
takes the form of overgeneralization, but with
eventual convergence to some target language
given exposure to more data.
In particular, in this paper we consider how
children acquire English dative verb construc-
tions, comparing HBMs to a simpler alterna-
tive, a linear competition learning (LCL) al-
gorithm that models the behavior of a given
verb as the linear competition between the ev-
idence for that verb, and the average behav-
ior of verbs belonging to its same class. The
results show that combining simple cluster-
ing methods along with ordinary maximum
likelihood estimation yields a result compara-
ble to HBM performance, providing an alter-
native account of the same facts, without the
computational costs incurred by HBM models
that must rely, for example, on Markov Chain
Monte Carlo (MCMC) methods for numeri-
cally integrating complex likelihood integrals,
or on Chinese Restaurant Process (CRP) for
producing partitions.
In terms of Marr?s hierarchy (Marr, 1982)
learning verb alternations is an abstract com-
putational problem (Marr?s type I), solvable
by many type II methods combining repre-
sentations (models, viz. HBMs or LCLs) with
particular algorithms. The HBM convention
of adopting ideal learning amounts to invok-
ing unbounded algorithmic resources, solv-
ability in principle, even though in practice
such methods, even approximate ones, are
provably NP-hard (cf. (Kwisthout, Wareham,
and van Rooij, 2011)). Assuming cognitive
plausibility as a desideratum, we therefore ex-
amine whether HBMs can also be approxi-
mated by another type II method (LCLs) that
does not demand such intensive computa-
tion. Any algorithm that approximates an
HBM can be viewed as implementing a some-
what different underlying model; if it repli-
cates HBM prediction performance but is sim-
pler and less computationally complex then
we assume it is preferable.
This paper is organized as follows: we start
with a discussion of formalizations of lan-
guage acquisition tasks, ?2. We present our
experimental framework for the dative acqui-
sition task, formalizing a range of learning
models from simple MLE methods to HBM
techniques, ?3, and a computational evalua-
tion of each model, ?4. We finish with conclu-
sions and possibilities for future work, ?5.
2 Evidence in Language Acquisition
A familiar problem for language acquisition is
how children learn which verbs participate in
so-called dative alternations, exemplified by
the child-produced sentences 1 to 3, from the
Brown (1973) corpus in CHILDES (MacWhin-
ney, 1995).
1. you took me three scrambled eggs (a direct object da-
tive (DOD) from Adam at age 3;6)
2. Mommy can you fix dis for me ? (a prepositional da-
tive (PD) from Adam at age 4;7)
3. *Mommy, fix me my tiger (from Adam at age 5;2)
Examples like these show that children gen-
eralize their use of verbs. For example, in sen-
tence (1), the child Adam uses take as a DOD
before any recorded occurrence of a similar
use of take in adult speech to Adam. Such
verbs alternate because they can also occur
with a prepositional form, as in sentence (2).
However, sometimes a child?s use of verbs like
1322
these amounts to an overgeneralization ? that
is, their productive use of a verb in a pattern
that does not occur in the adult grammar, as in
sentence (3), above. Faced with these two verb
frames the task for the learner is to decide for a
particular verb if it is a non-alternating DOD
only verb, a PD only verb, or an alternating
verb that allows both forms.
This ambiguity raises an important learn-
ability question, conventionally known as
Baker?s paradox (Baker, 1979). On the as-
sumption that children only receive positive
examples of verb forms, then it is not clear
how they might recover from the overgener-
alization exhibited in sentence (3) above, be-
cause they will never receive positive sen-
tences from adults like (3), using fix in a DOD
form. As has long been noted, if negative ex-
amples were systematically available to learn-
ers, then this problem would be solved, since
the child would be given evidence that the
DOD form is not possible in the adult gram-
mar. However, although parental correction
could be considered to be a source of negative
evidence, it is neither systematic nor generally
available to all children (Marcus, 1993). Even
when it does occur, all careful studies have in-
dicated that it seems mostly concerned with
semantic appropriateness rather than syntax.
In the cases where it is related to syntax, it
is often difficult to determine what the cor-
rection refers to in the utterance and besides
children seem to be oblivious to the correction
(Brown and Hanlon, 1970; Ingram, 1989).
One alternative solution to Baker?s paradox
that has been widely discussed at least since
Chomsky (1981) is the use of indirect negative
evidence. On the indirect negative evidence
model, if a verb is not found where it would
be expected to occur, the learner may con-
clude it is not part of the adult grammar. Cru-
cially, the indirect evidence model is inher-
ently statistical. Different formalizations of in-
direct negative evidence have been incorpo-
rated in several computational learning mod-
els for learning e.g. grammars (Briscoe, 1997;
Villavicencio, 2002; Kwiatkowski et al, 2010);
dative verbs (Perfors, Tenenbaum, and Won-
nacott, 2010; Hsu and Chater, 2010); and mul-
tiword verbs (Nematzadeh, Fazly, and Steven-
son, 2013). Since a number of closely related
models can all implement the indirect nega-
tive evidence approach, the decision of which
one to choose for a given task may not be en-
tirely clear. In this paper we compare a range
of statistical models consistent with a certain
behavior: early overgeneralization, with even-
tual convergence to the correct target on the
basis of exposure to more data.
3 Materials and Methods
3.1 Dative Corpora
To emulate a child language acquisition en-
vironment we use naturalistic longitudinal
child-directed data, from the Brown corpus in
CHILDES, for one child (Adam) for a subset
of 19 verbs in the DOD and PD verb frames,
figure 1. This dataset was originally reported
in Perfors, Tenenbaum, and Wonnacott (2010),
and longitudinal and incremental aspects to
acquisition are approximated by dividing the
data available into 5 incremental epochs (E1 to
E5 in the figures), where at the final epoch the
learner has seen the full corpus.
Model comparison requires a gold standard
database for acquisition, reporting which
frames have been learned for which verbs at
each stage, and how likely a child is of mak-
ing creative uses of a particular verb in a new
frame. An independent gold standard with
developmental information (e.g. Gropen et
al. (1989)) would clearly be ideal. Absent
this, a first step is demonstrating that sim-
pler alternative models can replicate HBM
performance on their own terms. Therefore,
the gold standard we use for evaluation is
the classification predicted by Perfors, Tenen-
baum, and Wonnacott (2010). The evaluations
reported in our analysis take into account in-
trinsic characteristics of each model in rela-
tion to the likelihoods of the verbs, to deter-
mine the extent to which the models go be-
yond the data they were exposed to, discussed
in section 2. Further, since it has been ar-
gued that very low frequency verbs may not
yet be firmly placed in a child?s lexicon (Yang,
2010; Gropen et al, 1989), at each epoch we
also impose a low-frequency threshold of 5
occurrences, considering only verbs that the
learner has seen at least 5 times. This use of a
low-frequency threshold for learning has ex-
tensive support in the literature for learning
1323
of all kinds in both human and non-human
animals, e.g. (Gallistel, 2002). A cut-off fre-
quency in this range has also commonly been
used in NLP tasks like POS tagging (Ratna-
parkhi, 1999).
3.2 The learners
We selected a set of representative statistical
models that are capable in principle of solv-
ing this classification task, ranging from what
is perhaps the simplest possible, a simple bi-
nomial, all the way to multi-level hierarchical
Bayesian approaches.
A Binomial distribution serves as the sim-
plest model for capturing the behavior of a
verb occurring in either DOD or PD frame.
Representing the probability of DOD as ?, af-
ter n occurrences of the verb the probability
that y of them are DOD is:
p( y| ?,n) =
(n
y
)
?y (1 ? ?)n?y (1)
Considering that p(y| ?,n) is the likelihood
in a Bayesian framework, the simplest and the
most intuitive estimator of ?, given y in n verb
occurrences, is the Maximum Likelihood Esti-
mator (MLE):
?MLE =
y
n (2)
?MLE is viable as a learning model in the sensethat its accuracy increases as the amount of ev-
idence for a verb grows (n ? ?), reflecting
the incremental, on-line character of language
learning. However, one well known limita-
tion of MLE is that it assigns zero probability
mass to unseen events. Ruling out events on
the grounds that they did not occur in a finite
data set early in learning may be too strong ?
though it should be noted that this is simply
one (overly strong) version of the indirect neg-
ative evidence position.
Again as is familiar, to overcome zero
count problem, models adopt one or another
method of smoothing to assign a small prob-
ability mass to unseen events. In a Bayesian
formulation, this amounts to assigning non-
zero probability mass to some set of priors;
smoothing also captures the notion of gener-
alization, making predictions about data that
has never been seen by the learner. In the
context of verb learning smoothing could be
based on several principles:
? an (innate) expectation as to how verbs in
general should behave;
? an acquired class-based expectation of
the behavior of a verb, based on its associ-
ation to similar but more frequent verbs.
The former can be readily implemented
in terms of prior probability estimates. As
we discuss below, class-based estimates arise
from one or another clustering method, and
can produce more accurate estimates for less
frequent verbs based on patterns already
learned for more frequent verbs in the same
class; see (Perfors, Tenenbaum, and Wonna-
cott, 2010). In this case, smoothing is a side-
effect of the behavior of a class as a whole.
When learning begins, the prior probability
is the only source of information for a learner
and, as such, dominates the value of the poste-
rior probability. However, in the large sample
limit, it is the likelihood that dominates the
posterior distribution regardless of the prior.
In Hierarchical Bayesian Models both effects
are naturally incorporated. The prior distri-
bution is structured as a chain of distributions
of parameters and hyper-parameters, and the
data may be divided into classes that share
some of the hyper-parameters, as defined be-
low for the case of a three levels model:
? ? Exponential(1)
? ? Exponential(1)
?k ? Exponential(?)
?k ? Beta(?, ?)
?ik ? Beta(?k?k, ?k(1 ? ?k))
yi|ni ? Binomial(?ik)
The indices refer to the possible hierarchies
among the hyper-parameters. ? and ? are in
the top, and they are shared by all verbs. Then
there are classes of different ?k, ?k, and theprobabilities for the DOD frame for the dif-
ferent verbs (?ik) are drawn according to theclasses k assigned to them. An estimate for
(?ik) for a given configuration of clusters isgiven by
1324
Figure 1: Verb tokens per epoch (E1 to E5)
Figure 2: Verb tokens ? 5 per epoch (E1 to E5)
where P(Y) is the evidence of the data,
the unnormalized posterior for the hyper-
parameters is
and the likelihood for ? and ? is
The Hierarchical Bayesian Model prediction
for?i is the average of the estimate?ikHBM overall possible partitions of the verbs in the task.
To simplify the notation we can write
?HBM = E
[ y + ??
n + ?
]
(3)
where in the expression E[. . . ] are included
the integrals described above and the average
of all possible class partitions. Due to this
complexity, in practice even small data sets re-
quire the use of MCMC methods, and statisti-
cal models for partitions, like CRP (Gelman et
al., 2003; Perfors, Tenenbaum, and Wonnacott,
2010). This complexity also calls into question
the cognitive fidelity of such approaches.
Eq.3 is particularly interesting because by
fixing? and ? (instead of averaging over them)
it is possible to deduce simpler (and classical)
models: MLE corresponds to ? = 0; the so
called ?add-one? smoothing (referred in this
paper as L1) corresponds to ? = 2 and ? = 1/2.
From Eq.3 it is also clear that if ? and ? (or
their distributions) are unchanged, as the evi-
dence of a verb grows (n??), the HBM esti-
mate approaches MLE?s, (?HBM ? ?MLE). Onthe other hand, when ? >> n, ?HBM ? ?, sothat ? can be interpreted as a prior value for ?
in the low frequency limit.
Following this reasoning, we propose an
alternative approach, a linear competition
learner (LCL), that explicitly models the be-
havior of a given verb as the linear competi-
tion between the evidence for the verb, and
the average behavior of verbs of the same
class. As clustering is defined independently
from parameter estimation, the advantages of
the proposed approach are twofold. First, it
is computationally much simpler, not requir-
ing approximations by Monte Carlo meth-
ods. Second, differently from HBMs where
the same attributes are used for clustering and
parameter estimation (in this case the DOD
and PD counts for each verb), in LCL cluster-
1325
ing may be done using more general contexts
that employ a variety of linguistic and envi-
ronmental attributes.
For LCL the prior and class-based informa-
tion are incorporated as:
?LCL =
yi + ?C?C
ni + ?C (4)
where ?C and ?C are defined via justifiableheuristic expressions dependent solely on the
statistics of the class attributed to each verb i.
The strength of the prior (?C) is a mono-tonic function of the number of elements (mC)in the class C, excluding the target verb vi.To approximate the gold standard behavior of
the HBM for this task (Perfors, Tenenbaum,
and Wonnacott, 2010) we chose the following
function for ?C:
?C = mC3/2(1 ?mC?1/5) + 0.1 (5)
with the strength of the prior for the LCL
model depending on the number of verbs in
the class, not on their frequency. Eq.5 was
chosen as a good fit to HBMs, without incur-
ring their complexity. The powers are simple
fractions, not arbitrary numbers. A best fit
was not attempted due to the lack of assess-
ment of how accurate HBMs are on real data.
The prior value (?C) is a smoothed estima-tion of the probability of DOD in a given class,
combining the evidence for all verbs in that
class:
?C =
YC + 1/2
NC + 1 (6)
in this case YC is the number of DOD occur-rences in the class, and NC the total numberof verb occurrences in the class, in both cases
excluding the target verb vi.The interpretation of these parameters is
as follows: ?C is the estimate of ? in the ab-sence of any data for a verb; and ?C controlsthe crossover between this estimate and MLE,
with a large ?C requiring a larger sample (ni)to overcome the bias given by ?C.For comparative purposes, in this paper we
examine alternative models for (a) probability
estimation and (b) clustering. The models are
the following:
? two models without clusters: MLE and
L1;
? two models where clusters are performed
independently: LCL and MLE??; and
? the full HBM described before.
MLE?? corresponds to replacing ?, ? in eq.3by their maximal likelihood values calculated
from P({yi,ni}i?k|?, ?) described before.For models without clustering, estimation
is based solely on the observed behavior of
verbs. With clustering, same-cluster verbs
share some parameters, influencing one an-
other. HBMs place distributions over pos-
sible clusters, with estimation derived from
averages over distributions. In HBMs, clus-
tering and probability estimation are calcu-
lated jointly. In the other models these two
estimates are calculated separately, permit-
ting ?plug-and-play? use of external cluster-
ing methods, like X-means (Pelleg and Moore,
2000)1. However, to further assess the impact
of cluster assignment on alternative model
performance, we also used the clusters that
maximize the evidence of the HBM for the
DOD and PD counts of the target verbs, and
we refer to these as Maximum Evidence (ME)
clusters. In MWE clusters, verbs are separated
into 3 classes: one if they have counts for both
frames; another for only the DOD frame; and
a final for only the PD frame.
4 Evaluation
The learning task consists of estimating the
probability that a given verb occurs in a partic-
ular frame, using previous occurrences as the
basis for this estimation. In this context, over-
generalization can be viewed as the model?s
predictions that a given verb seen only in one
frame (say, a PD) can also occur in the other
(say, a DOD) as well, and it decreases as the
learner receives more data. In one extreme
we have MLE, which does not overgeneralize,
and in the other the L1 model, which assigns
uniform probability for all unseen cases. The
other 3 models fall somewhere in between,
overgeneralizing beyond the observed data,
using the prior and class-based smoothing to
assign some (low) probability mass to an un-
seen verb-frame pair. The relevant models?
1Other clustering algorithms were also used; here
we report X-means results as representative of these
models. X-means is available from http://www.cs.
waikato.ac.nz/ml/weka/
1326
predictions for each of the target verbs in the
DOD frame, given the full corpus, are in fig-
ure 3. In either end of the figure are the verbs
that were attested in only one of the frames
(PD only at the left-hand end, and DOD only
at the right-hand end). For these verbs, LCL
and HBM exhibit similar behavior. When the
low-frequency threshold is applied, MLE??,HBM and LCL work equally well, figure 4.
Figure 4: Probability of verbs in DOD frame,
Low Frequency Threshold.
To examine how overgeneralization pro-
gresses during the course of learning as the
models were exposed to increasing amounts
of data, we used the corpus divided by cumu-
lative epochs, as described in ?3.1. For each
epoch, verbs seen in only one of the frames
were divided in 5 frequency bins, and the
models were assessed as to how much over-
generalization they displayed for each of these
verbs. Following Perfors, Tenenbaum, and
Wonnacott (2010) overgeneralization is calcu-
lated as the absolute difference between the
models predicted ? and ?MLE, for each of theepochs, figure 5, and for comparative pur-
poses their alternating/non-alternating clas-
sification is also adopted. For non-alternating
verbs, overgeneralization reflects the degree
of smoothing of each model. As expected, the
more frequent a verb is, the more confident
the model is in the indirect negative evidence
it has for that verb, and the less it overgeneral-
izes, shown in the lighter bars in all epochs. In
addition, the overall effect of larger amounts
of data are indicated by a reduction in over-
generalization epoch by epoch. The effects of
class-based smoothing can be assessed com-
paring L1, a model without clustering which
displays a constant degree of overgeneraliza-
tion regardless of the epoch, while HBM uses
a distribution over clusters and the other mod-
els X-means. If a low-frequency threshold is
applied, the differences between the models
decrease significantly and so does the degree
of overgeneralization in the models? predic-
tions, as shown in the 3 lighter bars in the fig-
ure.
Figure 5: Overgeneralization, per epoch, per
frequency bin, where 0.5 corresponds to the
maximum overgeneralization.
While the models differ somewhat in their
predictions, the quantitative differences need
to be assessed more carefully. To compare
the models and provide an overall difference
measure, we use the predictions of the more
complex model, HBM, as a baseline and then
calculate the difference between its predic-
tions and those of the other models. We
used three different measures for comparing
models, one for their standard difference; one
that prioritizes agreement for high frequency
verbs; and one that focuses more on low fre-
quency verbs.
The first measure, denoted Difference, cap-
tures a direct comparison between two mod-
els, M1 and M2 as the average prediction dif-ference among the verbs, and is defined as:
This measure treats all differences uniformly,
regardless of whether they relate to high or
low frequency verbs in the learning sample
(e.g. for bring with 150 counts and serve with
only 1 have the same weight). To focus on high
frequency verbs, we also define the Weighted
Difference between two models as:
Here we expect Dn < D since models tend to
1327
Figure 3: Probability of verbs in DOD frame.
agree as the amount of evidence for each verb
increases. Conversely, our third measure, de-
noted Inverted, prioritizes the agreement be-
tween two models on low frequency verbs, de-
fined as follows:
D1/n captures the degree of similarity in over-generalization between two models. The re-
sults of applying these three difference mea-
sures are shown in figure 6 for the relevant
models, where grey is for D(M1,M2), blackfor Dn(M1,M2) and white for D1/n(M1,M2).Given the probabilistic nature of Monte Carlo
methods, there is also a variation between dif-
ferent runs of the HBM model (HBM to HBM-
2), and this indicates that models that per-
form within these bounds can be considered
to be equivalent (e.g. HBMs and ME-MLE??for Weighted Difference, and the HBMs and
X-MLE?? for the Inverted Difference).
Comparing the prediction agreement, the
strong influence of clustering is clear: the
models that have compatible clusters have
similar performances. For instance, all the
models that adopt the ME clusters for the
data perform closest to HBMs. Moreover, the
weighted differences tend to be smaller than
0.01 and around 0.02 for the inverted differ-
ences. The results for these measures become
even closer in most cases when the low fre-
quency threshold is adopted, figure 7, as the
Figure 6: Model Comparisons.
Figure 7: Model Comparison - Low Frequency
Threshold.
0 5 10 15 20 25 30 35 40 45 500.5
0.6
0.7
0.8
0.9
1
number of examples
DO
D p
rob
abi
lity
 
 
MLE
L1
HBM
LCLMLE
L1
HBM
LCL
Figure 8: DOD probability evolution for mod-
els with increase in evidence
evidence reduces the influence of the prior.
To examine the decay of overgeneralization
with the increase in evidence for these mod-
els, two simulated scenarios are defined for a
single generic verb: one where the evidence
for DOD amounts to 75% of the data (dashed
lines) and in the other to 100% (solid lines),
figures 9 and 8. Unsurprisingly, the perfor-
mance of the models is dependent on the
amount of evidence available. This is a con-
sequence of the decrease in the influence of
the priors as the sample size increases in a rate
of 1/N, as shown in figure 9 for the decrease
in overgeneralization. Ultimately it is the ev-
1328
100 101 102
10?4
10?3
10?2
10?1
100
number of examples
ove
rge
ner
aliz
atio
n
 
 L1HBMLCLL1HBMLCL
Figure 9: Overgeneralization reduction with
increase in evidence
idence that dominates the posterior probabil-
ity. Although the Bayesian model exhibits fast
convergence, after 10 examples, the simpler
model L1 is only approximately 3% below the
Bayesian model in performance for scenario 1
and is still 90% accurate in scenario 2, figure 8.
These results suggest that while these mod-
els all differ slightly in the degree of overgen-
eralization for low frequency data and noise,
these differences are small, and as evidence
reaches approximately 10 examples per verb,
the overall performance for all models ap-
proaches that of MLE.
5 Conclusions and Future Work
HBMs have been successfully used for a
number of language acquisition tasks captur-
ing both patterns of under- and overgeneral-
ization found in child language acquisition.
Their (hyper)parameters provide robustness
for dealing with low frequency events, noise,
and uncertainty and a good fit to the data,
but this fidelity comes at the cost of complex
computation. Here we have examined HBMs
against computationally simpler approaches
to dative alternation acquisition, which imple-
ment the indirect negative approach. We also
advanced several measures for model com-
parison in order to quantify their agreement
to assist in the task of model selection. The re-
sults show that the proposed LCL model, in
particular, that combines class-based smooth-
ing with maximum likelihood estimation, ob-
tains results comparable to those of HBMs,
in a much simpler framework. Moreover,
when a cognitively-viable frequency thresh-
old is adopted, differences in the performance
of all models decrease, and quite rapidly ap-
proach the performance of MLE.
In this paper we used standard clustering
techniques grounded solely on verb counts to
enable comparison with previous work. How-
ever, a variety of additional linguistic and dis-
tributional features could be used for cluster-
ing verbs into more semantically motivated
classes, using a larger number of frames and
verbs. This will be examined in future work.
We also plan to investigate the use of cluster-
ing methods more targeted to language tasks
(Sun and Korhonen, 2009).
Acknowledgements
We would like to thank the support of
projects CAPES/COFECUB 707/11, CNPq
482520/2012-4, 478222/2011-4, 312184/2012-
3, 551964/2011-1 and 312077/2012-2. We also
want to thank Amy Perfors for kindly sharing
the input data.
References
Baker, Carl L. 1979. Syntactic Theory and the Pro-
jection Problem. Linguistic Inquiry, 10(4):533?
581.
Briscoe, Ted. 1997. Co-evolution of language and
the language acquisition device. In Proceedings
of the 35th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 418?427.
Morgan Kaufmann.
Brown, Roger. 1973. A first language: Ehe early
stages. Harvard University Press, Cambridge,
Massachusetts.
Brown, Roger and Camille Hanlon. 1970. Deriva-
tional complexity and the order of acquisition of
child?s speech. In J. Hays, editor, Cognition and
the Development of Language. NY: John Wiley.
Chater, Nick, Joshua B. Tenenbaum, and Alan
Yuille. 2006. Probabilistic models of cogni-
tion: where next? Trends in Cognitive Sciences,
10(7):292 ? 293.
Chomsky, Noam. 1981. Lectures on government and
binding. Mouton de Gruyter.
1329
Gallistel, Charles R. 2002. Frequency, contin-
gency, and the information processing theory of
conditioning. In P.Sedlmeier and T. Betsch, ed-
itors, Frequency processing and cognition. Oxford
University Press, pages 153?171.
Gelman, Andrew, John B. Carlin, Hal S. Stern, and
Donald B. Rubin. 2003. Bayesian Data Analy-
sis, Second Edition (Chapman & Hall/CRC Texts in
Statistical Science). Chapman and Hall/CRC, 2
edition.
Gropen, Jess, Steve Pinker, Michael Hollander,
Richard Goldberg, and Ronald Wilson. 1989.
The learnability and acquisition of the dative al-
ternation in English. Language, 65(2):203?257.
Hsu, Anne S. and Nick Chater. 2010. The logi-
cal problem of language acquisition: A proba-
bilistic perspective. Cognitive Science, 34(6):972?
1016.
Ingram, David. 1989. First Language Acquisition:
Method, Description and Explanation. Cambridge
University Press.
Jones, Matt and Bradley C. Love. 2011. Bayesian
Fundamentalism or Enlightenment? On the ex-
planatory status and theoretical contributions
of Bayesian models of cognition. Behavioral and
Brain Sciences, 34(04):169?188.
Kwiatkowski, Tom, Luke Zettlemoyer, Sharon
Goldwater, and Mark Steedman. 2010. Induc-
ing probabilistic CCG grammars from logical
form with higher-order unification. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1223?1233.
Kwisthout, Johan, Todd Wareham, and Iris van
Rooij. 2011. Bayesian intractability is not an
ailment that approximation can cure. Cognitive
Science, 35(5):779?1007.
Levin, B. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. University of
Chicago Press, Chicago, IL.
MacWhinney, Brian. 1995. The CHILDES project:
tools for analyzing talk. Hillsdale, NJ: Lawrence
Erlbaum Associates, second edition.
Marcus, Gary F. 1993. Negative evidence in lan-
guage acquisition. Cognition, 46:53?85.
Marr, D. 1982. Vision. San Francisco, CA: W. H.
Freeman.
Nematzadeh, Aida, Afsaneh Fazly, and Suzanne
Stevenson. 2013. Child acquisition of multi-
word verbs: A computational investigation. In
A. Villavicencio, T. Poibeau, A. Korhonen, and
A. Alishahi, editors, Cognitive Aspects of Com-
putational Language Acquisition. Springer, pages
235?256.
Parisien, Christopher, Afsaneh Fazly, and Suzanne
Stevenson. 2008. An incremental bayesian
model for learning syntactic categories. In Pro-
ceedings of the Twelfth Conference on Computational
Natural Language Learning, CoNLL ?08, pages
89?96, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Parisien, Christopher and Suzanne Stevenson.
2010. Learning verb alternations in a usage-
based bayesian model. In Proceedings of the 32nd
Annual Conference of the Cognitive Science Society.
Pelleg, Dan and Andrew Moore. 2000. X-means:
Extending k-means with efficient estimation of
the number of clusters. In Proceedings of the
Seventeenth International Conference on Machine
Learning, pages 727?734, San Francisco. Morgan
Kaufmann.
Perfors, Amy, Joshua B. Tenenbaum, and Eliz-
abeth Wonnacott. 2010. Variability, nega-
tive evidence, and the acquisition of verb argu-
ment constructions. Journal of Child Language,
(37):607?642.
Ratnaparkhi, Adwait. 1999. Learning to parse nat-
ural language with maximum entropy models.
Machine Learning, pages 151?175.
Shalizi, Cosma R. 2009. Dynamics of bayesian
updating with dependent data and misspeci-
fied models. ElectroCosmanic Journal of Statistics,
3:1039?1074.
Sun, Lin and Anna Korhonen. 2009. Improving
verb clustering with automatically acquired se-
lectional preferences. In EMNLP, pages 638?
647.
Villavicencio, Aline. 2002. The Acquisition of a
Unification-Based Generalised Categorial Grammar.
Ph.D. thesis, Computer Laboratory, University
of Cambridge.
Wonnacott, Elizabeth, Elissa L. Newport, and
Michael K. Tanenhaus. 2008. Acquiring and
processing verb argument structure: Distribu-
tional learning in a miniature language. Cogni-
tive Psychology, 56:165?209.
Yang, Charles. 2010. Three factors in language
variation. Lingua, 120:1160?1177.
1330
Proceedings of the EACL 2012 Workshop on Computational Models of Language Acquisition and Loss, pages 23?25,
Avignon, France, April 24 2012. c?2012 Association for Computational Linguistics
An annotated English child language database
Aline Villavicencio??, Beracah Yankama?, Rodrigo Wilkens?,
Marco A. P. Idiart?, Robert Berwick?
?Federal University of Rio Grande do Sul (Brazil)
?MIT (USA)
alinev@gmail.com, beracah@mit.edu, rswilkens@gmail.com, marco.idiart@gmail.com, berwick@csail.mit.edu
1 Introduction
The use of large-scale naturalistic data has been
opening up new investigative possibilities for lan-
guage acquisition studies, providing a basis for
empirical predictions and for evaluations of alter-
native acquisition hypotheses. One widely used
resource is CHILDES (MacWhinney, 1995) with
transcriptions for over 25 languages of interac-
tions involving children, with the English corpora
available in raw, part-of-speech tagged, lemma-
tized and parsed formats (Sagae et al, 2010; But-
tery and Korhonen, 2005). With a recent increase
in the availability of lexical and psycholinguistic
resources and robust natural language processing
tools, it is now possible to further enrich child-
language corpora with additional sources of infor-
mation.
In this paper we describe the English CHILDES
Verb Database (ECVD), which extends the orig-
inal lexical and syntactic annotation of verbs
in CHILDES with information about frequency,
grammatical relations, semantic classes, and other
psycholinguistic and statistical information. In
addition, these corpora are organized in a search-
able database that allows the retrieval of data ac-
cording to complex queries that combine different
sources of information. This database is also mod-
ular and can be straightforwardly extended with
additional annotation levels. In what follows, we
discuss the tools and resources used for the anno-
tation (?2), and conclude with a discussion of the
implications of this initial work along with direc-
tions for future research (?3).
2 Linguistic and Statistical
Properties
The English CHILDES Verb Database con-
tains information about the English corpora in
CHILDES parsed using three different pipelines:
(1) MEGRASP; (2) RASP; and (3) the CHILDES
Treebank. In the first, made available as part of
the CHILDES distribution1, the corpora are POS
1http://childes.psy.cmu.edu/
tagged (in %mor), and parsed using MEGRASP
(Sagae et al, 2010) which provides information
about dependency parses and grammatical rela-
tions (in %gra):2
*MOT: I said (.) Adam you could have a banana
and offer Robin and Ursula one (.)would you
?
%mor: pro|I v|say&PAST n:prop|Adam pro|you
aux|could v|have det|a n|banana ...
%gra: 1|2|SUBJ 2|6|CJCT 3|2|OBJ 4|6|SUBJ
5|6|AUX 6|9|COORD 7|8|DET 8|6|OBJ ...
In the second pipeline, the RASP system
(Briscoe et al, 2006) is used for tokenisation,
tagging, lemmatization and parsing of the input
sentences, outputting syntactic trees (in %ST)
and grammatical relations (%GR).3 In both
examples each GR denotes a relation, along with
its head and dependent:
*MOT: oh no # he didn?t say anything about win-
dow .
%ST: (T Oh:1 no:2 ,:3 (S he:4 (VP do+ed:5
not+:6 say:7 anything:8 (PP about:9 (N1
window:10)))) .:11)
%GR: (|ncsubj| |say:7 VV0| |he:4 PPHS1| )
(|aux| |say:7 VV0| |do+ed:5 VDD|)
(|ncmod| |say:7 VV0| |not+:6 XX|)
(|iobj| |say:7 VV0| |about:9 II|) (|dobj|
|say:7 VV0| |anything:8 PN1|) (|dobj|
|about:9 II| |window:10 NN1|)
The third focuses on the Adam corpus from
the Brown data set (Brown, 1973) and uses
the Charniak parser with Penn Treebank style
part of speech tags and output, followed by
hand-curation, as described by Pearl and Sprouse
(2012):
(S1 (SBARQ (WHNP (WP who)) (SQ (VP (COP is)
(NP (NN that)))) (. ?)))
2In an evaluation MEGRASP produced correct depen-
dency relations for 96% of the relations in the gold stan-
dard, with the dependency relations being labelled with the
correct GR 94% of the time.
3The data was kindly provided by P. Buttery and A.
Korhonen and generated as described in (Buttery and Ko-
rhonen, 2005).
23
The use of annotations from multiple parsers
enables the combination of the complementary
strengths of each in terms of coverage and ac-
curacy, similar to inter-annotator agreement ap-
proaches. These differences are also useful for op-
timizing search patterns in terms of the source
which produces the best accuracy for a particu-
lar case. Information about corpora sizes and the
annotated portions for each of the parsers is dis-
played in table 1.
Information Sentences
Total Raw 4.84 million
MEGRASP & RASP Raw 2.5 million
MEGRASP Parsed 109,629
RASP Parsed 2.21 million
CHILDES Treebank 26,280
MEGRASP & RASP Parsed 98,456
Table 1: Parsed Sentences
The verbs in each sentence are also annotated
with information about shared patterns of mean-
ing and syntactic behavior from 190 fine-grained
subclasses that cover 3,100 verb types (Levin,
1993). This annotation allows searches defined
in terms of verb classes, and include all sentences
that contain verbs that belong to a given class.
For instance, searching for verbs of running would
return sentences containing not only run but also
related verbs like slide, roll and stroll.
Additional annotation of properties linked to
language use and recognition include extrinsic fac-
tors such as word frequency and intrinsic factors
such as the length of a word in terms of sylla-
bles; age of acquisition; imageability; and familiar-
ity. Some of this annotation is obtained from the
MRC Psycholinguistic Database (Coltheart, 1981)
which contains 150,837 entries with information
about 26 properties, although not all properties
are available for every word (e.g. IMAG is only
available for 9,240 words).
For enabling complex search functionalities
that potentially combine information from several
sources, the annotated sentences were organized
in a database, and Tables 2 and 3 list some of the
available annotations. Given the focus on verbs,
for search efficiency each sentence is indexed ac-
cording to the verbs it contains. In addition, verbs
and nouns are further annotated with information
shown in table 3 whenever it is available in the
existing resources.
These levels of annotation allow for complex
searches involving for example, a combination of
information about a verb?s lemma, target gram-
matical relations, and occurrence of Levin?s classes
in the corpora.
Not all sentences have been successfully ana-
lyzed, and the comments field contains informa-
Fields
Sentence ID
Corpus
Speaker
File
Raw sentence
MOR and POST tags
MEGRASP dep. and GRs
RASP syntactic tree
RASP dep. and GRs
Comments
Table 2: Information about Sentences
Fields
Word ID
Sentence ID
Levin?s classes
Age of acquisition
Familiarity
Concreteness
Frequency
Imageability
Number of syllables
Table 3: Information about Words
tion about the missing annotations and cases of
near perfect matches that arise from the parsers
using different heuristics for e.g. non-words, meta-
characters and punctuation. These required more
complex matching procedures for identifying the
corresponding cases in the annotations of the
parsers.
3 Conclusions and future work
This paper describes the construction of the En-
glish CHILDES Verb Database. It combines in-
formation from different parsing systems to capi-
talize on their complementary recall and precision
strengths and ensure the accuracy of the searches.
It also includes information about Levin?s classes
for verbs, and some psycholinguistic information
for some of the words, like age of acquisition,
familiarity and imageability. The result is a
large-scale integrated resource that allows com-
plex searches involving different annotation lev-
els. This database can be used to inform analysis,
for instance, about the complexity of the language
employed with and by a child as her age increases,
that can shed some light on discussions about the
poverty of the stimulus. This is an ongoing project
to make the annotated data available to the re-
search community in a user-friendly interface that
allows complex patterns to be specified in a simple
way.
Acknowledgements
This research was partly supported by CNPq
Projects 551964/2011-1, 202007/2010-3,
24
305256/2008-4 and 309569/2009-5.
References
E. Briscoe, J. Carroll, and R. Watson. 2006. The
second release of the rasp system. In Proceedings
of the COLING/ACL 2006 Interactive Presentation
Sessions, Sydney, Australia.
R. Brown. 1973. A first language: The early
stages. Harvard University Press, Cambridge, Mas-
sachusetts.
P. Buttery and A. Korhonen. 2005. Large-scale anal-
ysis of verb subcategorization differences between
child directed speech and adult speech. In Proceed-
ings of the Interdisciplinary Workshop on the Iden-
tification and Representation of Verb Features and
Verb Classes.
M. Coltheart. 1981. The MRC psycholinguistic
database. Quarterly Journal of Experimental Psy-
chology, 33A:497?505.
B. Levin. 1993. English verb classes and alterna-
tions - a preliminary investigation. The University
of Chicago Press.
B. MacWhinney. 1995. The CHILDES project: tools
for analyzing talk. Hillsdale, NJ: Lawrence Erlbaum
Associates, second edition.
L. Pearl and J. Sprouse, 2012. Experimental Syntax
and Islands Effects, chapter Computational Models
of Acquisition for Islands. Cambridge University
Press.
K. Sagae, E. Davis, A. Lavie, B. MacWhinney, and
S. Wintner. 2010. Morphosyntactic annotation of
CHILDES transcripts. Journal of Child Language,
37(03):705?729.
25
Proceedings of the EACL 2012 Workshop on Computational Models of Language Acquisition and Loss, pages 43?50,
Avignon, France, April 24 2012. c?2012 Association for Computational Linguistics
Get out but don?t fall down: verb-particle constructions in child language
Aline Villavicencio??, Marco A. P. Idiart?, Carlos Ramisch?,
V??tor Arau?jo?, , Beracah Yankama?, Robert Berwick?
?Federal University of Rio Grande do Sul (Brazil)
?MIT (USA)
alinev@gmail.com, marco.idiart@gmail.com, ceramisch@inf.ufrgs.br,
vbuaraujo@inf.ufrgs.br, beracah@mit.edu, berwick@csail.mit.edu
Abstract
Much has been discussed about the chal-
lenges posed by Multiword Expressions
(MWEs) given their idiosyncratic, flexi-
ble and heterogeneous nature. Nonethe-
less, children successfully learn to use them
and eventually acquire a number of Mul-
tiword Expressions comparable to that of
simplex words. In this paper we report
a wide-coverage investigation of a partic-
ular type of MWE: verb-particle construc-
tions (VPCs) in English and their usage
in child-produced and child-directed sen-
tences. Given their potentially higher com-
plexity in relation to simplex verbs, we
examine whether they appear less promi-
nently in child-produced than in child-
directed speech, and whether the VPCs
that children produce are more conserva-
tive than adults, displaying proportionally
reduced lexical repertoire of VPCs or of
verbs in these combinations. The results
obtained indicate that regardless of any ad-
ditional complexity VPCs feature widely in
children data following closely adult usage.
Studies like these can inform the develop-
ment of computational models for language
acquisition.
1 Introduction
There has been considerable discussion about
the challenges imposed by Multiword Expres-
sions (MWEs) which in addition to crossing word
boundaries act as a single lexical unit at some lev-
els of linguistic analysis (Calzolari et al, 2002;
Sag et al, 2002; Fillmore, 2003). They include a
wide range of grammatical constructions such as
verb-particle constructions (VPCs), idioms, com-
pound nouns and listable word configurations,
such as terminology and formulaic linguistic units
(Wray, 2009). Depending on the definition, they
may also include less traditional sequences like
copy of in They gave me a copy of the book (Fill-
more et al, 1988), greeting formulae like how
do you do?, and lexical bundles such as I dont
know whether or memorized poems and famil-
iar phrases from TV commercials (Jackendoff,
1997). These expressions may have reduced syn-
tactic flexibility, and be semantically more opaque
so that their semantics may not be easily inferred
from their component words. For instance, to play
down X means to (try to) make X seem less im-
portant than it really is and not literally a playing
event.
These expressions may also breach general
syntactic rules, sometimes spanning phrasal
boundaries and often having a high degree of lex-
icalisation and conventionality. They form a com-
plex of features that interact in various, often un-
tidy, ways and represent a broad continuum be-
tween non-compositional (or idiomatic) and com-
positional groups of words (Moon, 1998). In ad-
dition, they are usually sequences or groups of
words that co-occur more often than would be ex-
pected by chance, and have been argued to appear
in the same order of magnitude in a speaker?s lex-
icon as the simplex words (Jackendoff, 1997).
In terms of language acquisition difficulties
may arise as the interpretation of these expres-
sions often demands more knowledge than just
about (1) unitary words and (2) word-to-word re-
lations. This introduces a distinction between
what a learner is able to computationally disam-
biguate or figure out automatically from language
and what must be explicitly stored/memorized
and retrieved whole from memory at the time of
43
use, rather than being subject to generation or
analysis by the language grammar (Wray, 2009,
p. 9). Yet, according to Fillmore et al (1988),
in an ideal learning environment, most of the
knowledge about how to use a language should
be computable while explicitly memorized se-
quences should be kept to a minimum.
Due to these idiosyncrasies they have been
noted as easily phonetically mislearned: e.g. by
and large mistaken for by in large, to all in-
tents and purposes for to all intensive purposes,
and an arm and a leg for a nominal egg (Fill-
more, 2003). For second language (L2) learn-
ers in particular (Wray, 2002) MWEs are in-
deed a well-known cause of problems and less
likely to be used by them than by native speak-
ers in informal spoken contexts (Siyanova and
Schmitt, 2007). Even if L2 learners may be capa-
ble of producing a large number of MWEs, their
underlying intuitions and fluency do not match
those of native speakers (Siyanova and Schmitt,
2008) and they may produce marked combina-
tions that are not conventionally used together
(e.g. plastic surgery/?operation, strong/?powerful
tea) (Pearce, 2002; Siyanova and Schmitt, 2007).
Given the potential additional sources of com-
plexity of MWEs for learning, in this paper we
investigate whether children shy away from us-
ing them when they communicate. We focus on
a particular type of MWEs, VPCs, which present
a wide range of syntactic and semantic idyosin-
crasies examining whether children produce pro-
portionally less VPCs than adults. In addition, we
analyze whether any potential added processing
costs for VPCs are reflected in a reduced choice
of VPCs or verbs to form these combinations in
child-produced sentences compared to adult us-
age. Finally, given the possibility of flexible word
orders in VPCs with the verb and particle not only
occurring adjacently but also with an NP object
between them, we compare these two groups in
terms of distances between the verb and the par-
ticle in these combinations, to determine whether
there is a preference for a joint or a split config-
uration and if children and adults adopt distinct
strategies for their usage. By profiling the VPC
usage by children our aim is to provide the basis
for a computational modeling of the acquisition of
these constructions.
This paper is structured as follows: in sec-
tion 2 describes VPCs and related works; sec-
tion 3 presents the resources and methods used in
this paper. The analyses of VPCs in children and
adults sentences are in section 4. We finish with
conclusions and possibilities of future works.
2 Related Work
VPCs are combinations of verbs and prepositional
(up, down, ...), adverbial (away, back,...), adjecti-
val (short,...) or verbal (go, be,...) particles, and in
this work we focus on VPCs with prepositional or
adverbial particles like put off and move on. From
a language acquisition perspective, the complex-
ity of VPCs arises from their wide syntactic as
semantic variability.
Syntactically, like simplex verbs, VPCs can oc-
cur in different subcategorisation frames (e.g. in-
transitive in break down and transitive in print NP
up). However, the type of verb and the num-
ber of arguments of a VPC seem to have an
impact in learning as both children with typical
development and with specific language impair-
ments (SLI) seem to use obligatory arguments and
inflectional morphology more consistently with
general all purpose verbs, like make, go, do, put,
than with more specific verbs. Moreover, as the
number of obligatory arguments increases chil-
dren with SLI seem to produce more general and
fewer specific verbs (Boynton-Hauerwas, 1998).
Goldberg (1999b) refers to these verbs as light
verbs, suggesting that due to their frequency of
use, they are acquired earlier by children, and sub-
sequently act as centers of gravity from which
more specific instances can be learnt. These verbs
are very common and frequent in the everyday
communication, that could be used in place of
more specialized instances (e.g. make instead of
build).
In transitive VPCs there is the additional diffi-
culty of the particle appearing in different word
orders in relation to the verb: in a joint configu-
ration, adjacent to the verb (e.g. make up NP) or
in a split configuration after the NP complement
(make NP up) (Lohse et al, 2004). While some
VPCs can appear in both configurations, others
are inseparable (run across NP), and a learner has
to successfully account for these. Gries (2002)
using a multifactorial analysis to investigate 25
variables that could be linked to particle place-
ment like size of the direct object (in syllables
and words), type of NP (pronoun or lexical), type
of determiner (indefinite or definite). For a set
44
of 403 VPCs from the British National Corpus
he obtains 84% success in predicting (adult) na-
tive speakers? choice. Lohse et al (2004) propose
that these factors can be explained by consider-
ations of processing efficiency based on the size
of the object NP and on semantic dependencies
among the verb, the particle, and the object. In a
similar study for children Diessel and Tomasello
(2005) found that the type of the NP (pronoun vs
lexical NP) and semantics of the particle (spatial
vs non-spatial) were good predictors of placement
on child language data.
Semantically, one source of difficulties for
learners comes from the wide spectrum of compo-
sitionality that VPCs present. On one end of the
spectrum some combinations like take away com-
positionally combine the meaning of a verb with
the core meaning of a particle giving a sense of
motion-through-location (Bolinger, 1971). Other
VPCs like boil up are semi-idiomatic (or aspec-
tual) and the particle modifies the meaning of the
verb adding a sense of completion or result. At the
other end of the spectrum, idiomatic VPCs like
take off, meaning to imitate have an opaque mean-
ing that cannot be straightforwardly inferred from
the meanings of each of the components literally.
Moreover, even if some verbs form combinations
with almost every particle (e.g., get, fall, go,...),
others are selectively combined with only a few
particles (e.g., book and sober with up), or do not
combine well with them at all (e.g., know, want,
resemble,...) (Fraser, 1976). Although there are
some semi-productive patterns in these combina-
tions, like verbs of cooking and the aspectual up
(cook up, boil up, bake up), and stative verbs not
forming VPCs, for a learner it may not be clear
whether an unseen combination of verb and parti-
cle is indeed a valid VPC that can be produced or
not. Sawyer (1999) longitudinal analysis of VPCs
in child language found that children seem to treat
aspectual and compositional combinations differ-
ently, with the former being more frequent and
employing a larger variety of types than the lat-
ter. The sources of errors also differ and while
for compositional cases the errors tend to be lexi-
cal, for aspectuals there is a predominance of syn-
tactic errors such as object dropping, which ac-
counts for 92% of the errors in split configura-
tion for children under 5 (Sawyer, 1999). Chil-
dren with SLI tended to produce even more object
dropping errors for VPCs than children with typ-
ical development, despite both groups producing
equivalent numbers of VPCs (Juhasz and Grela,
2008). Given that compositionality seems to have
an impact on learning, to help reduce avoidance
of phrasal verbs Sawyer (2000) proposes a seman-
tic driven approach for second language learning
where transparent compositional cases would be
presented first to help familiarization with word
order variation, semi-idiomatic cases would be
taught next in groups according to the contribu-
tion of the particle (e.g telicity or completive-
ness), and lastly the idiomatic cases that need to
be memorized.
In this paper we present a wide coverage ex-
amination of VPC distributions in child produced
and child-directed sentences, comparing whether
children reproduce the linguistic environment to
which they are exposed or whether they present
distinct preferences in VPC usage.
3 Materials and Methods
For this work we use the English corpora from
the CHILDES database (MacWhinney, 1995)
containing transcriptions of child-produced and
child-directed speech from interactions involving
children of different age groups and in a variety
of settings, from naturalistic longitudinal studies
to task oriented latitudinal cases. These corpora
are available in raw, part-of-speech-tagged, lem-
matized and parsed formats (Sagae et al, 2010).
Moreover the English CHILDES Verb Construc-
tion Database (ECVCD) (Villavicencio et al,
2012) also adds for each sentence the RASP pars-
ing and grammatical relations (Briscoe and Car-
roll, 2006), verb semantic classes (Levin, 1993),
age of acquisition, familiarity, frequency (Colt-
heart, 1981) and other psycholinguistic and dis-
tributional characteristics. These annotated sen-
tences are divided into two groups according to
the speaker annotation available in CHILDES, the
Adults Set and the Children Set contain respec-
tively all the sentences spoken by adults and by
children1, as shown in table 1 as Parsed.
VPCs in these corpora are detected by look-
ing in the RASP annotation for all occurrences
of verbs followed by particles, prepositions and
adverbs up to 5 words to the right, following
Baldwin (2005), shown as Sentences with VPCs
1For the latter sentences which did not contain informa-
tion about age were removed.
45
Sentences Children Set Adults Set
Parsed 482,137 988,101
with VPCs 44,305 83,098
with VPCs Cleaned 38,326 82,796
% with VPCs 7.95 8.38
Table 1: VPCs in English Corpora in the Children
and Adults Sets
in table 1. The resulting sentences are subse-
quently automatically processed to remove noise
and words mistagged as verbs. For these candi-
dates with non-alphabetic characters, like @ in
a@l up, were removed as were those that did not
involve verbs (e.g. di, dat,), using the Comlex
Lexicon as reference for verb validity (Macleod
and Grishman, 1998). The resulting sets are listed
as Sentences with VPCs Cleaned in table 1. The
analyses reported in this paper use these sen-
tences, and the distribution of VPCs per children
age group is shown in table 2. Given the non-
uniform amounts of VPC for each age group, and
the larger proportion of VPC sentences in younger
ages in these corpora, we consider children as a
unique group. For these, the individual frequen-
cies of the verb, the particle and the VPC are col-
lected separately in the children set and in the
adult set, using the mwetoolkit (Ramisch et al,
2010).
Age in months VPC Sentences
0-24 2,799
24-48 26,152
48-72 8,038
72-96 1,337
>96 514
No age 4,841
Table 2: VPCs in Children Set per Age
To evaluate the VPCs in these sets, we use:
? English VPC dataset (Baldwin, 2008); which
lists 3,078 VPCs with valency (intransitive
and transitive) information;
? Comlex lexicon (Macleod and Grishman,
1998) containing 10,478 phrasal verbs;
? the Alvey Natural Language Tools (ANLT)
lexicon (Carroll and Grover, 1989) with
6,351 phrasal verbs.
4 VPCs in Child Language
To investigate whether any extra complexity in the
acquisition of VPCs is reflected in their reduced
presence in child-produced than in child-directed
sentences, we compare the proportion of VPCs in
the Children and Adults Sets, table 3. In absolute
terms adults produced more than double the num-
ber of VPCs that children did. However, given
the differences in size of the two sets, in relative
terms there was a similar proportion of VPC us-
age in these corpora for each of the groups: 7.95%
of the sentences produced by children contained
VPCs vs 8.38% of those by adults. Moreover, the
frequencies with which these VPCs are used by
both children and adults reflects the Zipfian distri-
bution found for the use of words in natural lan-
guages, with a large part of the VPCs occurring
just once in the data, table 4. In addition, in terms
of frequency, children?s production of VPCs re-
sembles that of the adults.
Total VPC Children Set Adults Set
Tokens 38,326 82,796
Types 1,579 2,468
Table 3: VPC usage in CHILDES
Frequency Children Set Adults Set
1 42.62% 43.03%
2 13.05% 15%
3 8.36% 6.48%
4 4.05% 4.5%
?5 31.92% 31%
Table 4: VPC types per frequency
Another possible source of divergence between
children and adults is in the lexical variety found
in VPCs. The potential difficulties with VPCs
may be manifested in children producing a re-
duced repertoire of VPCs or using a smaller set
of verbs to form these combinations. As shown in
table 3, adults, as expected, employ a larger VPC
vocabulary with 1.56 more types than children.
However, an examination of the distributions of
types reveals that they only differ by a scale. As
a result when children frequencies are multiplied
by a factor of 2.16, which corresponds to the ra-
tio between VPC tokens used by adults and chil-
dren (table 3), the resulting distribution has a very
46
good match with the adult distribution, see fig-
ure 1. Therefore, the lower number of VPC types
used by children can be explained totally by the
lower number of sentences they produced, and the
hypothesis that difficulties in VPCs would lead to
their avoidance is not confirmed by the data.
Nonetheless, there is a discrepancy between
the distributions found for the higher frequency
VPCs. Children have a more uniform distribution
and adults tend to repeat more often the higher
frequency combinations (top left corner of fig-
ure 1). An evidence that this discrepancy is partic-
ular for high frequency VPCs, and not their con-
stituent verbs, is shown in figure 2. This figure
displays the rank plot for the verbs present in the
VPCs, for both adults and children. The same
scale factor used in figure 1 is applied to compen-
sate for the lower number of VPC sentences in the
children set. This time the match is extraordinary,
spanning the whole vocabulary.
100 102 10410
0
101
102
103
104
105
rank
freq
uen
cy
VPC Usage
 
 adults
children*
Figure 1: VPC Usage Frequency vs Ranking. The
children frequency is scaled to match adult total
VPC usage.
Ranks however, might not tell the whole story.
It is important to verify if the same VPCs and
verbs are present in the both vocabularies, and fur-
ther if their orders in the ranks are similar. The
two groups have very similar preferences for VPC
usage, with a Kendall ? score of 0.63 which indi-
cates that they are highly correlated, as Kendall
? ranges from -1 to 1. Furthermore they use a
very similar set of verbs in VPCs, with a Kendall
100 102 10410
0
101
102
103
104
105
rank
freq
uen
cy
Verbs in VPCs Usage
 
 adultschildren*
Figure 2: Verbs in VPCs Usage Frequency vs
Ranking. The children frequency is scaled to
match adult total VPC usage.
? score of 0.84 pointing to a very strong corre-
lation. We find less agreement between the or-
ders of VPCs and verbs for both children and
adults, indicating that the order of the verbs in
the data is not predictive of the relative frequen-
cies of VPCs. We examined (a) if children?s VPC
ranks followed their verb ranks, (b) if adults VPC
ranks followed their verb ranks and (c) if chil-
dren?s VPC ranks followed adults? verb ranks.
The resulting Kendall scores were around 0.2 for
all three cases. Moreover, if the lower frequency
VPCs are removed to avoid potential cases of
noise, the Kendall ? score for VPCs by adults and
children increases with the threshold, second line
from the top in Figure 3, while it remains constant
for all the other cases. As an example, the top 10
VPC types used by children and adults are listed
in table 5. From these, 9 out of the 10 are the
same differing only in the order in which they ap-
pear. Most of these combinations are listed in one
of the dictionaries used for evaluation: 72% for
adults and 75.87% for children. When a thresh-
old of at least 5 counts is applied these values go
up to 87.72% for adults and 79.82% for children,
as would be expected. This indicates that besides
any possible lack of coverage for child-directed
VPCs in the lexicons or noise, it is in the lower
frequency combinations that novel and domains
specific non-standard usages can be found. Some
47
Rank Chidren Children Adult Adult Child
VPC Freq VPC Freq Rank
1 put on 2005 come on 6244 7
2 go in 1608 put on 4217 1
3 get out 1542 go on 2660 9
4 take off 1525 get out 2251 3
5 fall down 1329 take off 2249 4
6 put in 1284 put in 2177 6
7 come on 1001 sit down 2133 8
8 sit down 981 go in 1661 2
9 go on 933 come out 1654 10
10 come out 872 pick up 1650 18
Table 5: Top VPCs for Children and Adults
of the combinations not found in these dictionar-
ies include crawl in and creep up by adults and
erase off and crash down by children.
0
0.2
0.4
0.6
0.8
1
0 5 10 20
Lexical Choices for VPCs
K
e
n
d
a
l
l
 
t
a
u
threshold
Children / Adults VPCs Children VPCs / Verbs
Adults VPC / Verbs Children VPCs / Adult Verbs
Children /Adult Verbs
Figure 3: Kendall ? score per VPC frequency
threshold
Finally, despite adults having a larger verb vo-
cabulary used in VPCs than children, the two
groups have similar ratios of verb per VPCs: 2.81
VPCs for children and 2.79 for adults, table 6.
The top verbs used in VPCs types are also respon-
sible for very frequent VPC tokens (e.g. go, get,
come, take, put, make and move) accounting for
5.83% VPC types and 43.76% tokens for adults
and 7.02% of the types and 47.81% of the to-
kens for children, confirming the discrepancy dis-
cussed earlier. These are very general verbs and
some of the most frequent in the data, reported
among the first to be learned (Goldberg, 1999a)
which may facilitate their acquisition and use in
VPCs.
Comparing VPC types used by children and by
adults, this trend is confirmed: a large proportion
(72.32%) of the VPC types that children use is
also used by adults, Children ? Adult in table 6.
When low frequency VPCs types are removed,
this proportion increases (89.48%). Moreover,
when the VPCs used only by the adults are con-
sidered, most of these (93.44%) occur with fre-
quency lower than 5. This suggests that children
tend to follow quite closely the combinations em-
ployed by adults, and the lower frequency cases
may not yet be incorporated in their active vocab-
ulary.
In terms of the distance between verb and par-
ticle, there is a strong preference in the data for
joint combinations for both children and adults,
table 7. For the split cases, the majority contains
only one word between the verb and the particle.
Children in particular display a slight disprefer-
ence for longer distances between verbs and parti-
cles, and over 97% of VPCs have at most 2 words
between them.
Distance Children Set Adults Set
0 65.13% 64.14%
1 23.48% 22.15%
2 9.33% 10.90%
3 1.65% 2.15%
4 0.29% 0.47%
5 0.09% 0.16%
Table 7: Distance between verb and particle
5 Conclusions and future work
In this paper we presented an investigation of
VPCs in child-produced and child-directed sen-
tences in English to determine whether potential
complexities in the nature of these combinations
48
Children Adult Children ?Adult Children Adult
VPCs VPCs VPCs only VPCs only VPCs
VPCs 1579 2468 1142 437 1243
Verb in VPCs 561 884 401 160 483
Particle in VPCs 28 35 24 4 9
VPCs ? 5 504 766 451 53 278
Verb in VPCs ? 5 207 282 183 24 99
Particle in VPCs ? 5 18 20 17 1 3
Table 6: Number of VPC, Verb and Particle types by group, common usages
are reflected in their reduced usage by children.
The combination of these results shows that, de-
spite any additional difficulties, VPCs are as much
a feature in children?s data as in adults?. Children
follow very closely adult usage in terms of the
types and are sensitive to their frequencies, dis-
playing similar distributions to adults. They also
seem to use them in a similar manner in terms of
particle placement. Therefore no correction for
VPC complexity was found in this data.
Despite these striking similarities in many of
the distributions, there are still some discrepan-
cies between these two groups. In particular in the
VPC ranks, children present a more uniform dis-
tribution for higher frequency VPCs when com-
pared to adults. Moreover, there is a modest but
significant dispreference for longer distances be-
tween verb and particle for children. Whether
these reflect different strategies or efficiency con-
siderations deserves to be further investigated.
Acknowledgements
This research was partly supported by CNPq
Projects 551964/2011-1, 202007/2010-3,
305256/2008-4 and 309569/2009-5.
References
Timothy Baldwin. 2005. Deep lexical acquisition
of verb-particle constructions. Computer Speech &
Language Special issue on MWEs, 19(4):398?414.
Timothy Baldwin. 2008. A resource for evaluating
the deep lexical acquisition of english verb-particle
constructions. In Proceedings of the LREC Work-
shop Towards a Shared Task for Multiword Expres-
sions (MWE 2008), pages 1?2, Marrakech, Mo-
rocco, June.
Dwight Bolinger. 1971. The phrasal verb in English.
Harvard University Press, Harvard, USA.
L. S. Boynton-Hauerwas. 1998. The role of general
all purpose verbs in language acquisition: A com-
parison of children with specific language impair-
ments and their language-matched peers. 59.
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on
the PARC depbank. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics (COLING/ACL 2006),
pages 41?48, Sidney, Australia, July. Association
for Computational Linguistics.
Nicoleta Calzolari, Charles Fillmore, Ralph Grishman,
Nancy Ide, Alessandro Lenci, Catherine Macleod,
and Antonio Zampolli. 2002. Towards best prac-
tice for multiword expressions in computational
lexicons. In Third International Conference on
Language Resources and Evaluation (LREC 2002),
pages 1934?1940, Las Palmas, Canary Islands,
Spain. European Language Resources Association.
John Carroll and Claire Grover. 1989. The derivation
of a large computational lexicon of English from
LDOCE. In B. Boguraev and E. Briscoe, editors,
Computational Lexicography for Natural Language
Processing. Longman.
M. Coltheart. 1981. The MRC psycholinguistic
database. Quarterly Journal of Experimental Psy-
chology, 33A:497?505.
Holger Diessel and Michael Tomasello. 2005. Particle
placement in early child language : A multifactorial
analysis. Corpus Linguistics and Linguistic Theory,
1(1):89?112.
Charles J. Fillmore, Paul Kay, and Mary C. O?Connor.
1988. Regularity and idiomaticity in grammatical
constructions: The case of Let Alone. Language,
64(3):510?538.
Charles Fillmore. 2003. Multiword expressions: An
extremist approach. Presented at Collocations and
idioms 2003: linguistic, computational, and psy-
cholinguistic perspectives.
Bruce Fraser. 1976. The Verb-Particle Combination
in English. Academic Press, New York, USA.
49
Adele E. Goldberg, 1999a. The Emergence of Lan-
guage, chapter Emergence of the semantics of
argument structure constructions, pages 197?212.
Carnegie Mellon Symposia on Cognition Series.
Adele E. Goldberg. 1999b. The emergence of the
semantics of argument structure constructions. In
B. MacWhinney, editor, Emergence of language.
Lawrence Erlbaum Associates, Hillsdale, NJ.
Stefan Gries. 2002. The influence of processing on
syntactic variation: Particle placement in english.
In Nicole Dehe?, Ray Jackendoff, Andrew McIn-
tyre, and Silke Urban, editors, Verb-Particle Ex-
plorations, pages 269?288. New York: Mouton de
Gruyter.
Ray Jackendoff. 1997. Twistin? the night away. Lan-
guage, 73:534?559.
C. R. Juhasz and B. Grela. 2008. Verb particle errors
in preschool children with specific language impair-
ment. Contemporary Issues in Communication Sci-
ence & Disorders, 35:76?83.
Beth Levin. 1993. English Verb Classes and Alter-
nations: a preliminary investigation. University of
Chicago Press, Chicago, USA.
Barbara Lohse, John A Hawkins, and Thomas Wa-
sow. 2004. Domain minimization in english verb-
particle constructions. Language, 80(2):238?261.
Catherine Macleod and Ralph Grishman. 1998.
COMLEX syntax reference manual, Proteus
Project.
B. MacWhinney. 1995. The CHILDES project: tools
for analyzing talk. Hillsdale, NJ: Lawrence Erl-
baum Associates, second edition.
Rosamund E. Moon. 1998. Fixed Expressions and
Idioms in English: A Corpus-based Approach. Ox-
ford University Press.
Darren Pearce. 2002. A comparative evaluation of
collocation extraction techniques. In Third Inter-
national Conference on Language Resources and
Evaluation (LREC 2002), Las Palmas, Canary Is-
lands, Spain. European Language Resources Asso-
ciation.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010. mwetoolkit: a framework for mul-
tiword expression identification. In Proceedings of
the Seventh International Conference on Language
Resources and Evaluation (LREC 2010), Malta,
May. European Language Resources Association.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multi-
word expressions: A pain in the neck for NLP.
In Proceedings of the 3rd International Conference
on Intelligent Text Processing and Computational
Linguistics (CICLing-2002), volume 2276/2010 of
Lecture Notes in Computer Science, pages 1?15,
Mexico City, Mexico, February. Springer.
K. Sagae, E. Davis, A. Lavie, B. MacWhinney, and
S. Wintner. 2010. Morphosyntactic annotation of
CHILDES transcripts. Journal of Child Language,
37(03):705?729.
J.H. Sawyer. 1999. Verb adverb and verb particle
constructions: their syntax and acquisition. s.n.
Joan H. Sawyer. 2000. Comments on clayton m. dar-
win and loretta s. gray?s ?going after the phrasal
verb: An alternative approach to classification?. a
reader reacts. TESOL Quarterly, 34(1):151?159.
Anna Siyanova and Norbert Schmitt. 2007. Na-
tive and nonnative use of multi-word vs. one-word
verbs. International Review of Applied Linguistics,
45:109139.
Anna Siyanova and Norbert Schmitt. 2008. L2 learner
production and processing of collocation: A multi-
study perspective. Canadian Modern Language Re-
view, 64(3):429458.
Aline Villavicencio, Beracah Yankama, Robert
Berwick, and Marco Idiart. 2012. A large scale
annotated child language construction database. In
Proceedings of the 8th LREC, Istanbul, Turkey.
Alison Wray. 2002. Formulaic Language and the Lex-
icon. Cambridge University Press, Cambridge, UK.
Alison Wray. 2009. Formulaic language in learn-
ers and native speakers. Language Teaching,
32(04):213?231.
50
