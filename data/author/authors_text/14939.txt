Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 317?325,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improving nonparameteric Bayesian inference: experiments on
unsupervised word segmentation with adaptor grammars
Mark Johnson
Brown University
Providence, RI
Mark Johnson@Brown.edu
Sharon Goldwater
University of Edinburgh
Edinburgh EH8 9AB
sgwater@inf.ed.ac.uk
Abstract
One of the reasons nonparametric Bayesian
inference is attracting attention in computa-
tional linguistics is because it provides a prin-
cipled way of learning the units of generaliza-
tion together with their probabilities. Adaptor
grammars are a framework for defining a va-
riety of hierarchical nonparametric Bayesian
models. This paper investigates some of
the choices that arise in formulating adap-
tor grammars and associated inference proce-
dures, and shows that they can have a dra-
matic impact on performance in an unsuper-
vised word segmentation task. With appro-
priate adaptor grammars and inference proce-
dures we achieve an 87% word token f-score
on the standard Brent version of the Bernstein-
Ratner corpus, which is an error reduction of
over 35% over the best previously reported re-
sults for this corpus.
1 Introduction
Most machine learning algorithms used in computa-
tional linguistics are parametric, i.e., they learn a nu-
merical weight (e.g., a probability) associated with
each feature, where the set of features is fixed be-
fore learning begins. Such procedures can be used
to learn features or structural units by embedding
them in a ?propose-and-prune? algorithm: a feature
proposal component proposes potentially useful fea-
tures (e.g., combinations of the currently most useful
features), which are then fed to a parametric learner
that estimates their weights. After estimating fea-
ture weights and pruning ?useless? low-weight fea-
tures, the cycle repeats. While such algorithms can
achieve impressive results (Stolcke and Omohundro,
1994), their effectiveness depends on how well the
feature proposal step relates to the overall learning
objective, and it can take considerable insight and
experimentation to devise good feature proposals.
One of the main reasons for the recent interest in
nonparametric Bayesian inference is that it offers a
systematic framework for structural inference, i.e.,
inferring the features relevant to a particular prob-
lem as well as their weights. (Here ?nonparamet-
ric? means that the models do not have a fixed set of
parameters; our nonparametric models do have pa-
rameters, but the particular parameters in a model
are learned along with their values). Dirichlet Pro-
cesses and their associated predictive distributions,
Chinese Restaurant Processes, are one kind of non-
parametric Bayesian model that has received consid-
erable attention recently, in part because they can be
composed in hierarchical fashion to form Hierarchi-
cal Dirichlet Processes (HDP) (Teh et al, 2006).
Lexical acquisition is an ideal test-bed for explor-
ing methods for inferring structure, where the fea-
tures learned are the words of the language. (Even
the most hard-core nativists agree that the words of a
language must be learned). We use the unsupervised
word segmentation problem as a test case for eval-
uating structural inference in this paper. Nonpara-
metric Bayesian methods produce state-of-the-art
performance on this task (Goldwater et al, 2006a;
Goldwater et al, 2007; Johnson, 2008).
In a computational linguistics setting it is natu-
ral to try to align the HDP hierarchy with the hi-
erarchy defined by a grammar. Adaptor grammars,
which are one way of doing this, make it easy to ex-
plore a wide variety of HDP grammar-based mod-
els. Given an appropriate adaptor grammar, the fea-
317
tures learned by adaptor grammars can correspond
to linguistic units such as words, syllables and col-
locations. Different adaptor grammars encode dif-
ferent assumptions about the structure of these units
and how they relate to each other. A generic adaptor
grammar inference program infers these units from
training data, making it easy to investigate how these
assumptions affect learning (Johnson, 2008).1
However, there are a number of choices in the de-
sign of adaptor grammars and the associated infer-
ence procedure. While this paper studies the im-
pact of these on the word segmentation task, these
choices arise in other nonparametric Bayesian infer-
ence problems as well, so our results should be use-
ful more generally. The rest of this paper is orga-
nized as follows. The next section reviews adaptor
grammars and presents three different adaptor gram-
mars for word segmentation that serve as running
examples in this paper. Adaptor grammars contain
a large number of adjustable parameters, and Sec-
tion 3 discusses how these can be estimated using
Bayesian techniques. Section 4 examines several
implementation options within the adaptor grammar
inference algorithm and shows that they can make
a significant impact on performance. Cumulatively
these changes make a significant difference in word
segmentation accuracy: our final adaptor grammar
performs unsupervised word segmentation with an
87% token f-score on the standard Brent version
of the Bernstein-Ratner corpus (Bernstein-Ratner,
1987; Brent and Cartwright, 1996), which is an er-
ror reduction of over 35% compared to the best pre-
viously reported results on this corpus.
2 Adaptor grammars
This section informally introduces adaptor gram-
mars using unsupervised word segmentation as a
motivating application; see Johnson et al (2007b)
for a formal definition of adaptor grammars.
Consider the problem of learning language from
continuous speech: segmenting each utterance into
words is a nontrivial problem that language learn-
ers must solve. Elman (1990) introduced an ideal-
ized version of this task, and Brent and Cartwright
(1996) presented a version of it where the data
consists of unsegmented phonemic representations
of the sentences in the Bernstein-Ratner corpus of
1The adaptor grammar inference program is available for
download at http://www.cog.brown.edu/?mj/Software.htm.
child-directed speech (Bernstein-Ratner, 1987). Be-
cause these phonemic representations are obtained
by looking up orthographic forms in a pronounc-
ing dictionary and appending the results, identifying
the word tokens is equivalent to finding the locations
of the word boundaries. For example, the phoneme
string corresponding to ?you want to see the book?
(with its correct segmentation indicated) is as fol-
lows:
y ?u Nw ?a ?n ?t Nt ?u Ns ?i ND ?6 Nb ?U ?k
We can represent any possible segmentation of any
possible sentence as a tree generated by the follow-
ing unigram grammar.
Sentence ? Word+
Word ? Phoneme+
The nonterminal Phoneme expands to each pos-
sible phoneme; the underlining, which identifies
?adapted nonterminals?, will be explained below. In
this paper ?+? abbreviates right-recursion through a
dummy nonterminal, i.e., the unigram grammar ac-
tually is:
Sentence ? Word
Sentence ? Word Sentence
Word ? Phonemes
Phonemes ? Phoneme
Phonemes ? Phoneme Phonemes
A PCFG with these productions can represent all
possible segmentations of any Sentence into a se-
quence of Words. But because it assumes that the
probability of a word is determined purely by mul-
tiplying together the probability of its individual
phonemes, it has no way to encode the fact that cer-
tain strings of phonemes (the words of the language)
have much higher probabilities than other strings
containing the same phonemes. In order to do this,
a PCFG would need productions like the following
one, which encodes the fact that ?want? is a Word.
Word ? w a n t
Adaptor grammars can be viewed as a way of for-
malizing this idea. Adaptor grammars learn the
probabilities of entire subtrees, much as in tree sub-
stitution grammar (Joshi, 2003) and DOP (Bod,
318
1998). (For computational efficiency reasons adap-
tor grammars require these subtrees to expand to ter-
minals). The set of possible adapted tree fragments
is the set of all subtrees generated by the CFG whose
root label is a member of the set of adapted non-
terminals A (adapted nonterminals are indicated by
underlining in this paper). For example, in the uni-
gram adaptor grammar A = {Word}, which means
that the adaptor grammar inference procedure learns
the probability of each possible Word subtree. Thus
adaptor grammars are simple models of structure
learning in which adapted subtrees are the units of
generalization.
One might try to reduce adaptor grammar infer-
ence to PCFG parameter estimation by introducing
a context-free rule for each possible adapted subtree,
but such an attempt would fail because the number
of such adapted subtrees, and hence the number of
corresponding rules, is unbounded. However non-
parametric Bayesian inference techniques permit us
to sample from this infinite set of adapted subtrees,
and only require us to instantiate the finite number
of them needed to analyse the finite training data.
An adaptor grammar is a 7-tuple
(N,W,R, S,?, A,C) where (N,W,R, S,?) is
a PCFG with nonterminals N , terminals W , rules
R, start symbol S ? N and rule probabilities ?,
where ?r is the probability of rule r ? R, A ? N is
the set of adapted nonterminals and C is a vector
of adaptors indexed by elements of A, so CX is the
adaptor for adapted nonterminal X ? A.
Informally, an adaptor CX nondeterministically
maps a stream of trees from a base distribution HX
whose support is TX (the set of subtrees whose root
node is X ? N generated by the grammar?s rules)
into another stream of trees whose support is also
TX . In adaptor grammars the base distributions HX
are determined by the PCFG rules expanding X and
the other adapted distributions, as explained in John-
son et al (2007b). When called upon to generate an-
other sample tree, the adaptor either generates and
returns a fresh tree from HX or regenerates a tree
it has previously emitted, so in general the adapted
distribution differs from the base distribution.
This paper uses adaptors based on Chinese
Restaurant Processes (CRPs) or Pitman-Yor Pro-
cesses (PYPs) (Pitman, 1995; Pitman and Yor, 1997;
Ishwaran and James, 2003). CRPs and PYPs non-
deterministically generate infinite sequences of nat-
ural numbers z1, z2, . . ., where z1 = 1 and each
zn+1 ? m+ 1 where m = max(z1, . . . , zn). In the
?Chinese Restaurant? metaphor samples produced
by the adaptor are viewed as ?customers? and zn
is the index of the ?table? that the nth customer is
seated at. In adaptor grammars each table in the
adaptor CX is labeled with a tree sampled from the
base distribution HX that is shared by all customers
at that table; thus the nth sample tree from the adap-
tor CX is the znth sample from HX .
CRPs and PYPs differ in exactly how the
sequence {zk} is generated. Suppose z =
(z1, . . . , zn) have already been generated and m =
max(z). Then a CRP generates the next table index
zn+1 according to the following distribution:
P(Zn+1 = k | z) ?
{
nk(z) if k ? m
? if k = m+ 1
where nk(z) is the number of times table k appears
in z and ? > 0 is an adjustable parameter that deter-
mines how often a new table is chosen. This means
that if CX is a CRP adaptor then the next tree tn+1
it generates is the same as a previously generated
tree t? with probability proportional to the number
of times CX has generated t? before, and is a ?fresh?
tree t sampled from HX with probability propor-
tional to ?XHX(t). This leads to a powerful ?rich-
get-richer? effect in which popular trees are gener-
ated with increasingly high probabilities.
Pitman-Yor Processes can control the strength of
this effect somewhat by moving mass from existing
tables to the base distribution. The PYP predictive
distribution is:
P(Zn+1 = k | z) ?
{
nk(z)? a if k ? m
ma+ b if k = m+ 1
where a ? [0, 1] and b > 0 are adjustable parame-
ters. It?s easy to see that the CRP is a special case of
the PRP where a = 0 and b = ?.
Each adaptor in an adaptor grammar can be
viewed as estimating the probability of each adapted
subtree t; this probability can differ substantially
from t?s probability HX(t) under the base distribu-
tion. Because Words are adapted in the unigram
adaptor grammar it effectively estimates the proba-
bility of each Word tree separately; the sampling es-
timators described in section 4 only instantiate those
Words actually used in the analysis of Sentences in
the corpus. While the Word adaptor will generally
319
prefer to reuse Words that have been used elsewhere
in the corpus, it is always possible to generate a fresh
Word using the CFG rules expanding Word into a
string of Phonemes.
We assume for now that all CFG rules RX ex-
panding the nonterminal X ? N have the same
probability (although we will explore estimating ?
below), so the base distribution HWord is a ?mon-
keys banging on typewriters? model. That means the
unigram adaptor grammar implements the Goldwa-
ter et al (2006a) unigram word segmentation model,
and in fact it produces segmentations of similar ac-
curacies, and exhibits the same characteristic under-
segmentation errors. As Goldwater et al point out,
because Words are the only units of generalization
available to a unigram model it tends to misanal-
yse collocations as words, resulting in a marked ten-
dancy to undersegment.
Goldwater et al demonstrate that modelling bi-
gram dependencies mitigates this undersegmenta-
tion. While adaptor grammars cannot express the
Goldwater et al bigram model, they can get much
the same effect by directly modelling collocations
(Johnson, 2008). A collocation adaptor grammar
generates a Sentence as a sequence of Collocations,
each of which expands to a sequence of Words.
Sentence ? Colloc+
Colloc ? Word+
Word ? Phoneme+
Because Colloc is adapted, the collocation adap-
tor grammar learns Collocations as well as Words.
(Presumably these approximate syntactic, semantic
and pragmatic interword dependencies). Johnson
reported that the collocation adaptor grammar seg-
ments as well as the Goldwater et al bigram model,
which we confirm here.
Recently other researchers have emphasised the
utility of phonotactic constraints (i.e., modeling
the allowable phoneme sequences at word onsets
and endings) for word segmentation (Blanchard and
Heinz, 2008; Fleck, 2008). Johnson (2008) points
out that adaptor grammars that model words as se-
quences of syllables can learn and exploit these con-
straints, significantly improving segmentation accu-
racy. Here we present an adaptor grammar that mod-
els collocations together with these phonotactic con-
straints. This grammar is quite complex, permitting
us to study the effects of the various model and im-
plementation choices described below on a complex
hierarchical nonparametric Bayesian model.
The collocation-syllable adaptor grammar gen-
erates a Sentence in terms of three levels of
Collocations (enabling it to capture a wider range
of interword dependencies), and generates Words as
sequences of 1 to 4 Syllables. Syllables are subcat-
egorized as to whether they are initial (I), final (F) or
both (IF).
Sentence ? Colloc3+
Colloc3 ? Colloc2+
Colloc2 ? Colloc1+
Colloc1 ? Word+
Word ? SyllableIF
Word ? SyllableI (Syllable) (Syllable) SyllableF
Syllable ? Onset Rhyme
Onset ? Consonant+
Rhyme ? Nucleus Coda
Nucleus ? Vowel+
Coda ? Consonant+
SyllableIF ? OnsetI RhymeF
OnsetI ? Consonant+
RhymeF ? Nucleus CodaF
CodaF ? Consonant+
SyllableI ? OnsetI Rhyme
SyllableF ? Onset RhymeF
Here Consonant and Vowel expand to all possible
consonants and vowels respectively, and the paren-
theses in the expansion of Word indicate optional-
ity. Because Onsets and Codas are adapted, the
collocation-syllable adaptor grammar learns the pos-
sible consonant sequences that begin and end syl-
lables. Moreover, because Onsets and Codas are
subcategorized based on whether they are word-
peripheral, the adaptor grammar learns which con-
sonant clusters typically appear at word boundaries,
even though the input contains no explicit word
boundary information (apart from what it can glean
from the sentence boundaries).
3 Bayesian estimation of adaptor
grammar parameters
Adaptor grammars as defined in section 2 have a
large number of free parameters that have to be
chosen by the grammar designer; a rule probabil-
ity ?r for each PCFG rule r ? R and either one or
two hyperparameters for each adapted nonterminal
X ? A, depending on whether Chinese Restaurant
320
or Pitman-Yor Processes are used as adaptors. It?s
difficult to have intuitions about the appropriate set-
tings for the latter parameters, and finding the opti-
mal values for these parameters by some kind of ex-
haustive search is usually computationally impracti-
cal. Previous work has adopted an expedient such as
parameter tying. For example, Johnson (2008) set
? by requiring all productions expanding the same
nonterminal to have the same probability, and used
Chinese Restaurant Process adaptors with tied pa-
rameters ?X , which was set using a grid search.
We now describe two methods of dealing with the
large number of parameters in these models that are
both more principled and more practical than the ap-
proaches described above. First, we can integrate
out ?, and second, we can infer values for the adap-
tor hyperparameters using sampling. These meth-
ods (the latter in particular) make it practical to use
Pitman-Yor Process adaptors in complex grammars
such as the collocation-syllable adaptor grammar,
where it is impractical to try to find optimal parame-
ter values by grid search. As we will show, they also
improve segmentation accuracy, sometimes dramat-
ically.
3.1 Integrating out ?
Johnson et al (2007a) describe Gibbs samplers for
Bayesian inference of PCFG rule probabilities ?,
and these techniques can be used directly with adap-
tor grammars as well. Just as in that paper, we
place Dirichlet priors on ?: here ?X is the subvector
of ? corresponding to rules expanding nonterminal
X ? N , and ?X is a corresponding vector of posi-
tive real numbers specifying the hyperparameters of
the corresponding Dirichlet distributions:
P(? | ?) = ?
X?N
Dir(?X | ?X)
Because the Dirichlet distribution is conjugate to the
multinomial distribution, it is possible to integrate
out the rule probabilities ?, producing the ?collapsed
sampler? described in Johnson et al (2007a).
In our experiments we chose an uniform prior
?r = 1 for all rules r ? R. As Table 1 shows,
integrating out ? only has a major effect on re-
sults when the adaptor hyperparameters themselves
are not sampled, and even then it did not have
a large effect on the collocation-syllable adaptor
grammar. This is not too surprising: because the
Onset, Nucleus and Coda adaptors in this gram-
mar learn the probabilities of these building blocks
of words, the phoneme probabilities (which is most
of what ? encodes) play less important a role.
3.2 Slice sampling adaptor hyperparameters
As far as we know, there are no conjugate priors for
the adaptor hyperparameters aX or bX (which cor-
responds to ?X in a Chinese Restaurant Process),
so it is not possible to integrate them out as we did
with the rule probabilities ?. However, it is possible
to perform Bayesian inference by putting a prior on
them and sampling their values.
Because we have no strong intuitions about the
values of these parameters we chose uninformative
priors. We chose a uniform Beta(1, 1) prior on aX ,
and a ?vague? Gamma(10, 0.1) prior on bX = ?X
(MacKay, 2003). (We experimented with other pa-
rameters in the Gamma prior, but found no signifi-
cant difference in performance).
After each Gibbs sweep through the parse trees t
we resampled each of the adaptor parameters from
the posterior distribution of the parameter using a
slice sampler 10 times. For example, we resample
each bX from:
P(bX | t) ? P(t | bX) Gamma(bX | 10, 0.1)
Here P(t | bX) is the likelihood of the current se-
quence of sample parse trees (we only need the fac-
tors that depend on bX ) and Gamma(bX | 10, 0.1)
is the prior. The same formula is used for sampling
aX , except that the prior is now a flat Beta(1, 1) dis-
tribution.
In general we cannot even compute the normaliz-
ing constants for these posterior distributions, so we
chose a sampler that does not require this. We use a
slice sampler here because it does not require a pro-
posal distribution (Neal, 2003). (We initially tried
a Metropolis-Hastings sampler but were unable to
find a proposal distribution that had reasonable ac-
ceptance ratios for all of our adaptor grammars).
As Table 1 makes clear, sampling the adaptor pa-
rameters makes a significant difference, especially
on the collocation-syllable adaptor grammar. This
is not surprising, as the adaptors in that grammar
play many different roles and there is no reason to
to expect the optimal values of their parameters to
be similar.
321
Condition Word token f-scores
Sample average Max. Marginal
B
at
ch
in
iti
al
iz
at
io
n
Ta
bl
e
la
be
lr
es
am
pl
in
g
In
te
gr
at
e
o
u
t?
Sa
m
pl
e
? X
=
b X
Sa
m
pl
e
a X
u
n
ig
ra
m
co
llo
c
co
llo
c-
sy
ll
u
n
ig
ra
m
co
llo
c
co
llo
c-
sy
ll
? ? ? ? ? 0.55 0.74 0.85 0.56 0.76 0.87
? ? ? ? 0.55 0.72 0.84 0.56 0.74 0.84
? ? ? 0.55 0.72 0.78 0.57 0.75 0.78
? ? 0.54 0.66 0.75 0.56 0.69 0.76
? ? ? ? 0.54 0.70 0.87 0.56 0.74 0.88
? ? ? ? 0.55 0.42 0.54 0.57 0.51 0.55
? ? ? ? 0.74 0.83 0.88 0.81 0.86 0.89
? ? ? 0.75 0.43 0.74 0.80 0.56 0.82
? ? 0.71 0.41 0.76 0.77 0.49 0.82
? ? ? 0.71 0.73 0.87 0.77 0.75 0.88
Table 1: Word segmentation accuracy measured by word token f-scores on Brent?s version of the Bernstein-Ratner
corpus as a function of adaptor grammar, adaptor and estimation procedure. Pitman-Yor Process adaptors were used
when aX was sampled, otherwise Chinese Restaurant Process adaptors were used. In runs where ? was not integrated
out it was set uniformly, and all ?X = bX were set to 100 they were not sampled.
4 Inference for adaptor grammars
Johnson et al (2007b) describe the basic adaptor
grammar inference procedure that we use here. That
paper leaves unspecified a number of implemen-
tation details, which we show can make a crucial
difference to segmentation accuracy. The adaptor
grammar algorithm is basically a Gibbs sampler of
the kind widely used for nonparametric Bayesian in-
ference (Blei et al, 2004; Goldwater et al, 2006b;
Goldwater et al, 2006a), so it seems reasonable to
expect that at least some of the details discussed be-
low will be relevant to other applications as well.
The inference algorithm maintains a vector t =
(t1, . . . , tn) of sample parses, where ti ? TS is a
parse for the ith sentence wi. It repeatedly chooses a
sentence wi at random and resamples the parse tree
ti for wi from P(ti | t?i, wi), i.e., conditioned on wi
and the parses t?i of all sentences except wi.
4.1 Maximum marginal decoding
Sampling algorithms like ours produce a stream of
samples from the posterior distribution over parses
of the training data. It is standard to take the out-
put of the algorithm to be the last sample produced,
and evaluate those parses. In some other applica-
tions of nonparametric Bayesian inference involv-
ing latent structure (e.g., clustering) it is difficult to
usefully exploit multiple samples, but that is not the
case here.
In maximum marginal decoding we map each
sample parse tree t onto its corresponding word seg-
mentation s, marginalizing out irrelevant detail in
t. (For example, the collocation-syllable adaptor
grammar contains a syllabification and collocational
structure that is irrelevant for word segmentation).
Given a set of sample parse trees for a sentence we
compute the set of corresponding word segmenta-
tions, and return the one that occurs most frequently
(this is a sampling approximation to the maximum
probability marginal structure).
For each setting in the experiments described in
Table 1 we ran 8 samplers for 2,000 iterations (i.e.,
passes through the training data), and kept the sam-
ple parse trees from every 10th iteration after itera-
tion 1000, resulting in 800 sample parses for every
sentence. (An examination of the posterior proba-
bilities suggests that all of the samplers using batch
initialization and table label resampling had ?burnt
322
batch initialization, table label resampling
incremental initialization, table label resampling
batch initialization, no table label resampling
2000150010005000
220000
215000
210000
205000
200000
195000
190000
185000
Figure 1: Negative log posterior probability (lower is bet-
ter) as a function of iteration for 24 runs of the collo-
cation adaptor grammar samplers with Pitman-Yor adap-
tors. The upper 8 runs use batch initialization but no ta-
ble label resampling, the middle 8 runs use incremental
initialization and table label resampling, while the lower
8 runs use batch initialization and table label resampling.
in? by iteration 1000). We evaluated the word to-
ken f-score of the most frequent marginal word seg-
mentation, and compared that to average of the word
token f-score for the 800 samples, which is also re-
ported in Table 1. For each grammar and setting we
tried, the maximum marginal segmentation was bet-
ter than the sample average, sometimes by a large
margin. Given its simplicity, this suggests that max-
imum marginal decoding is probably worth trying
when applicable.
4.2 Batch initialization
The Gibbs sampling algorithm is initialized with a
set of sample parses t for each sentence in the train-
ing data. While the fundamental theorem of Markov
Chain Monte Carlo guarantees that eventually sam-
ples will converge to the posterior distribution, it
says nothing about how long the ?burn in? phase
might last (Robert and Casella, 2004). In practice
initialization can make a huge difference to the per-
formance of Gibbs samplers (just as it can with other
unsupervised estimation procedures such as Expec-
tation Maximization).
There are many different ways in which we could
generate the initial trees t; we only study two of the
obvious methods here. Batch initialization assigns
every sentence a random parse tree in parallel. In
more detail, the initial parse tree ti for sentence wi
is sampled from P(t | wi, G?), where G? is the PCFG
obtained from the adaptor grammar by ignoring its
last two components A and C (i.e., the adapted non-
terminals and their adaptors), and seated at a new
table. This means that in batch initialization each
initial parse tree is randomly generated without any
adaptation at all.
Incremental initialization assigns the initial parse
trees ti to sentences wi in order, updating the adaptor
grammar as it goes. That is, ti is sampled from P(t |
wi, t1, . . . , ti?1). This is easy to do in the context
of Gibbs sampling, since this distribution is a minor
variant of the distribution P(ti | t?i, wi) used during
Gibbs sampling itself.
Incremental initialization is greedier than batch
initialization, and produces initial sample trees with
much higher probability. As Table 1 shows, across
all grammars and conditions after 2,000 iterations
incremental initialization produces samples with
much better word segmentation token f-score than
does batch initialization, with the largest improve-
ment on the unigram adaptor grammar.
However, incremental initialization results in
sample parses with lower posterior probability for
the unigram and collocation adaptor grammars (but
not for the collocation-syllable adaptor grammar).
Figure 1 plots the posterior probabilities of the sam-
ple trees t at each iteration for the collocation adap-
tor grammar, showing that even after 2,000 itera-
tions incremental initialization results in trees that
are much less likely than those produced by batch
initialization. It seems that with incremental initial-
ization the Gibbs sampler gets stuck in a local op-
timum which it is extremely unlikely to move away
from.
It is interesting that incremental initialization re-
sults in more accurate word segmentation, even
though the trees it produces have lower posterior
probability. This seems to be because the most prob-
able analyses produced by the unigram and, to a
lesser extent, the collocation adaptor grammars tend
to undersegment. Incremental initialization greed-
ily searches for common substrings, and because
such substrings are more likely to be short rather
than long, it tends to produce analyses with shorter
words than batch initialization does. Goldwater et
al. (2006a) show that Brent?s incremental segmenta-
tion algorithm (Brent, 1999) has a similar property.
We favor batch initialization because we are in-
323
terested in understanding the properties of our mod-
els (expressed here as adaptor grammars), and batch
initialization does a better job of finding the most
probable analyses under these models. However, it
might be possible to justify incremental initializa-
tion as (say) cognitively more plausible.
4.3 Table label resampling
Unlike the previous two implementation choices
which apply to a broad range of algorithms, table
label resampling is a specialized kind of Gibbs step
for adaptor grammars and similar hierarchical mod-
els that is designed to improve mobility. The adap-
tor grammar algorithm described in Johnson et al
(2007b) repeatedly resamples parses for the sen-
tences of the training data. However, the adaptor
grammar sampler itself maintains of a hierarchy of
Chinese Restaurant Processes or Pitman-Yor Pro-
cesses, one per adapted nonterminal X ? A, that
cache subtrees from TX . In general each of these
subtrees will occur many times in the parses for the
training data sentences. Table label resampling re-
samples the trees in these adaptors (i.e., the table
labels, to use the restaurant metaphor), potentially
changing the analysis of many sentences at once.
For example, each Collocation in the collocation
adaptor grammar can occur in many Sentences, and
each Word can occur in many Collocations. Resam-
pling a single Collocation can change the way it is
analysed into Words, thus changing the analysis of
all of the Sentences containing that Collocation.
Table label resampling is an additional resam-
pling step performed after each Gibbs sweep
through the training data in which we resample the
parse trees labeling the tables in the adaptor for each
X ? A. Specifically, if the adaptor CX for X ? A
currently contains m tables labeled with the trees
t = (t1, . . . , tm) then table label resampling re-
places each tj , j ? 1, . . . ,m in turn with a tree sam-
pled from P(t | t?j , wj), where wj is the terminal
yield of tj . (Within each adaptor we actually resam-
ple all of the trees t in a randomly chosen order).
Table label resampling is a kind of Gibbs sweep,
but at a higher level in the Bayesian hierarchy than
the standard Gibbs sweep. It?s easy to show that ta-
ble label resampling preserves detailed balance for
the adaptor grammars presented in this paper, so in-
terposing table label resampling steps with the stan-
dard Gibbs steps also preserves detailed balance.
We expect table label resampling to have the
greatest impact on models with a rich hierarchi-
cal structure, and the experimental results in Ta-
ble 1 confirm this. The unigram adaptor grammar
does not involve nested adapted nonterminals, so
we would not expect table label resampling to have
any effect on its analyses. On the other hand, the
collocation-syllable adaptor grammar involves a rich
hierarchical structure, and in fact without table la-
bel resampling our sampler did not burn in or mix
within 2,000 iterations. As Figure 1 shows, table
label resampling produces parses with higher pos-
terior probability, and Table 1 shows that table la-
bel resampling makes a significant difference in the
word segmentation f-score of the collocation and
collocation-syllable adaptor grammars.
5 Conclusion
This paper has examined adaptor grammar infer-
ence procedures and their effect on the word seg-
mentation problem. Some of the techniques inves-
tigated here, such as batch versus incremental ini-
tialization, are quite general and may be applica-
ble to a wide range of other algorithms, but some
of the other techniques, such as table label resam-
pling, are specialized to nonparametric hierarchi-
cal Bayesian inference. We?ve shown that sampling
adaptor hyperparameters is feasible, and demon-
strated that this improves word segmentation accu-
racy of the collocation-syllable adaptor grammar by
almost 10%, corresponding to an error reduction of
over 35% compared to the best results presented in
Johnson (2008). We also described and investigated
table label resampling, which dramatically improves
the effectiveness of Gibbs sampling estimators for
complex adaptor grammars, and makes it possible
to work with adaptor grammars with complex hier-
archical structure.
Acknowledgments
We thank Erik Sudderth for suggesting sampling the
Pitman-Yor hyperparameters and the ACL review-
ers for their insightful comments. This research was
funded by NSF awards 0544127 and 0631667 to
Mark Johnson.
324
References
N. Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck, editors,
Children?s Language, volume 6. Erlbaum, Hillsdale,
NJ.
Daniel Blanchard and Jeffrey Heinz. 2008. Improv-
ing word segmentation by simultaneously learning
phonotactics. In CoNLL 2008: Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning, pages 65?72, Manchester, England,
August.
David Blei, Thomas L. Griffiths, Michael I. Jordan, and
Joshua B. Tenenbaum. 2004. Hierarchical topic
models and the nested chinese restaurant process.
In Sebastian Thrun, Lawrence Saul, and Bernhard
Scho?lkopf, editors, Advances in Neural Information
Processing Systems 16. MIT Press, Cambridge, MA.
Rens Bod. 1998. Beyond grammar: an experience-based
theory of language. CSLI Publications, Stanford, Cal-
ifornia.
M. Brent and T. Cartwright. 1996. Distributional reg-
ularity and phonotactic constraints are useful for seg-
mentation. Cognition, 61:93?125.
M. Brent. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery. Ma-
chine Learning, 34:71?105.
Jeffrey Elman. 1990. Finding structure in time. Cogni-
tive Science, 14:197?211.
Margaret M. Fleck. 2008. Lexicalized phonotactic
word segmentation. In Proceedings of ACL-08: HLT,
pages 130?138, Columbus, Ohio, June. Association
for Computational Linguistics.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006a. Contextual dependencies in unsupervised
word segmentation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 673?680, Sydney, Aus-
tralia. Association for Computational Linguistics.
Sharon Goldwater, Tom Griffiths, and Mark Johnson.
2006b. Interpolating between types and tokens
by estimating power-law generators. In Y. Weiss,
B. Scho?lkopf, and J. Platt, editors, Advances in Neural
Information Processing Systems 18, pages 459?466,
Cambridge, MA. MIT Press.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2007. Distributional cues to word boundaries:
Context is important. In David Bamman, Tatiana
Magnitskaia, and Colleen Zaller, editors, Proceedings
of the 31st Annual Boston University Conference on
Language Development, pages 239?250, Somerville,
MA. Cascadilla Press.
H. Ishwaran and L. F. James. 2003. Generalized
weighted Chinese restaurant processes for species
sampling mixture models. Statistica Sinica, 13:1211?
1235.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007a. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 139?146,
Rochester, New York, April. Association for Compu-
tational Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007b. Adaptor Grammars: A framework
for specifying compositional nonparametric Bayesian
models. In B. Scho?lkopf, J. Platt, and T. Hoffman, ed-
itors, Advances in Neural Information Processing Sys-
tems 19, pages 641?648. MIT Press, Cambridge, MA.
Mark Johnson. 2008. Using adaptor grammars to identi-
fying synergies in the unsupervised acquisition of lin-
guistic structure. In Proceedings of the 46th Annual
Meeting of the Association of Computational Linguis-
tics, Columbus, Ohio. Association for Computational
Linguistics.
Aravind Joshi. 2003. Tree adjoining grammars. In Rus-
lan Mikkov, editor, The Oxford Handbook of Compu-
tational Linguistics, pages 483?501. Oxford Univer-
sity Press, Oxford, England.
David J.C. MacKay. 2003. Information Theory, Infer-
ence, and Learning Algorithms. Cambridge Univer-
sity Press.
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31:705?767.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. Annals of Probability, 25:855?900.
J. Pitman. 1995. Exchangeable and partially exchange-
able random partitions. Probability Theory and Re-
lated Fields, 102:145?158.
Christian P. Robert and George Casella. 2004. Monte
Carlo Statistical Methods. Springer.
Andreas Stolcke and Stephen Omohundro. 1994. Induc-
ing probabilistic grammars by Bayesian model merg-
ing. In Rafael C. Carrasco and Jose Oncina, editors,
Grammatical Inference and Applications, pages 106?
118. Springer, New York.
Y. W. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hier-
archical Dirichlet processes. Journal of the American
Statistical Association, 101:1566?1581.
325
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 548?556,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Inducing Compact but Accurate Tree-Substitution Grammars
Trevor Cohn and Sharon Goldwater and Phil Blunsom
School of Informatics
University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
Scotland, United Kingdom
{tcohn,sgwater,pblunsom}@inf.ed.ac.uk
Abstract
Tree substitution grammars (TSGs) are a com-
pelling alternative to context-free grammars
for modelling syntax. However, many popu-
lar techniques for estimating weighted TSGs
(under the moniker of Data Oriented Parsing)
suffer from the problems of inconsistency and
over-fitting. We present a theoretically princi-
pled model which solves these problems us-
ing a Bayesian non-parametric formulation.
Our model learns compact and simple gram-
mars, uncovering latent linguistic structures
(e.g., verb subcategorisation), and in doing so
far out-performs a standard PCFG.
1 Introduction
Many successful models of syntax are based on
Probabilistic Context Free Grammars (PCFGs)
(e.g., Collins (1999)). However, directly learning a
PCFG from a treebank results in poor parsing perfor-
mance, due largely to the unrealistic independence
assumptions imposed by the context-free assump-
tion. Considerable effort is required to coax good
results from a PCFG, in the form of grammar en-
gineering, feature selection and clever smoothing
(Collins, 1999; Charniak, 2000; Charniak and John-
son, 2005; Johnson, 1998). This effort must be re-
peated when moving to different languages, gram-
mar formalisms or treebanks. We propose that much
of this hand-coded knowledge can be obtained auto-
matically as an emergent property of the treebanked
data, thereby reducing the need for human input in
crafting the grammar.
We present a model for automatically learning a
Probabilistic Tree Substitution Grammar (PTSG),
an extension to the PCFG in which non-terminals
can rewrite as entire tree fragments (elementary
trees), not just immediate children. These large frag-
ments can be used to encode non-local context, such
as head-lexicalisation and verb sub-categorisation.
Since no annotated data is available providing TSG
derivations we must induce the PTSG productions
and their probabilities in an unsupervised way from
an ordinary treebank. This is the same problem ad-
dressed by Data Oriented Parsing (DOP, Bod et al
(2003)), a method which uses as productions all sub-
trees of the training corpus. However, many of the
DOP estimation methods have serious shortcomings
(Johnson, 2002), namely inconsistency for DOP1
(Bod, 2003) and overfitting of the maximum like-
lihood estimate (Prescher et al, 2004).
In this paper we develop an alternative means of
learning a PTSG from a treebanked corpus, with the
twin objectives of a) finding a grammar which ac-
curately models the data and b) keeping the gram-
mar as simple as possible, with few, compact, ele-
mentary trees. This is achieved using a prior to en-
courage sparsity and simplicity in a Bayesian non-
parametric formulation. The framework allows us to
perform inference over an infinite space of gram-
mar productions in an elegant and efficient manner.
The net result is a grammar which only uses the in-
creased context afforded by the TSG when necessary
to model the data, and otherwise uses context-free
rules.1 That is, our model learns to use larger rules
when the CFG?s independence assumptions do not
hold. This contrasts with DOP, which seeks to use
all elementary trees from the training set. While our
model is able, in theory, to use all such trees, in prac-
tice the data does not justify such a large grammar.
Grammars that are only about twice the size of a
1While TSGs and CFGs describe the same string lan-
guages, TSGs can describe context-sensitive tree-languages,
which CFGs cannot.
548
treebank PCFG provide large gains in accuracy. We
obtain additional improvements with grammars that
are somewhat larger, but still much smaller than the
DOP all-subtrees grammar. The rules in these gram-
mars are intuitive, potentially offering insights into
grammatical structure which could be used in, e.g.,
the development of syntactic ontologies and guide-
lines for future treebanking projects.
2 Background and related work
A Tree Substitution Grammar2 (TSG) is a 4-tuple,
G = (T,N, S,R), where T is a set of terminal sym-
bols, N is a set of non-terminal symbols, S ? N is
the distinguished root non-terminal and R is a set
of productions (a.k.a. rules). The productions take
the form of elementary trees ? tree fragments of
depth ? 2, where each internal node is labelled with
a non-terminal and each leaf is labelled with either a
terminal or a non-terminal. Non-terminal leaves are
called frontier non-terminals and form the substitu-
tion sites in the generative process of creating trees
with the grammar.
A derivation creates a tree by starting with the
root symbol and rewriting (substituting) it with an
elementary tree, then continuing to rewrite frontier
non-terminals with elementary trees until there are
no remaining frontier non-terminals. Unlike Con-
text Free Grammars (CFGs) a syntax tree may not
uniquely specify the derivation, as illustrated in Fig-
ure 1 which shows two derivations using different
elementary trees to produce the same tree.
A Probabilistic Tree Substitution Grammar
(PTSG), like a PCFG, assigns a probability to each
rule in the grammar. The probability of a derivation
is the product of the probabilities of its component
rules, and the probability of a tree is the sum of the
probabilities of its derivations.
As we mentioned in the introduction, work within
the DOP framework seeks to induce PTSGs from
treebanks by using all possible subtrees as rules, and
one of a variety of methods for estimating rule prob-
abilities.3 Our aim of inducing compact grammars
contrasts with that of DOP; moreover, we develop a
probabilistic estimator which avoids the shortcom-
ings of DOP1 and the maximum likelihood esti-
2A TSG is a Tree Adjoining Grammar (TAG; Joshi (2003))
without the adjunction operator.
3TAG induction (Chiang and Bikel, 2002; Xia, 2002) also
tackles a similar learning problem.
mate (Bod, 2000; Bod, 2003; Johnson, 2002). Re-
cent work on DOP estimation also seeks to address
these problems, drawing from estimation theory to
solve the consistency problem (Prescher et al, 2004;
Zollmann and Sima?an, 2005), or incorporating a
grammar brevity term into the learning objective
(Zuidema, 2007). Our work differs from these pre-
vious approaches in that we explicitly model a prior
over grammars within a Bayesian framework.4
Models of grammar refinement (Petrov et al,
2006; Liang et al, 2007; Finkel et al, 2007) also
aim to automatically learn latent structure underly-
ing treebanked data. These models allow each non-
terminal to be split into a number of subcategories.
Theoretically the grammar space of our model is a
sub-space of theirs (projecting the TSG?s elementary
trees into CFG rules). However, the number of non-
terminals required to recreate our TSG grammars
in a PCFG would be exorbitant. Consequently, our
model should be better able to learn specific lexical
patterns, such as full noun-phrases and verbs with
their sub-categorisation frames, while theirs are bet-
ter suited to learning subcategories with larger mem-
bership, such as the terminals for days of the week
and noun-adjective agreement. The approaches are
orthogonal, and we expect that combining a category
refinement model with our TSG model would pro-
vide better performance than either approach alone.
Our model is similar to the Adaptor Grammar
model of Johnson et al (2007b), which is also
a kind of Bayesian nonparametric tree-substitution
grammar. However, Adaptor Grammars require that
each sub-tree expands completely, with only termi-
nal symbols as leaves, while our own model permits
non-terminal frontier nodes. In addition, they disal-
low recursive containment of adapted non-terminals;
we impose no such constraint.
3 Model
Recall the nature of our task: we are given a corpus
of parse trees t and wish to infer a tree-substitution
grammar G that we can use to parse new data.
Rather than inferring a grammar directly, we go
through an intermediate step of inferring a distri-
bution over the derivations used to produce t, i.e.,
4A similar Bayesian model of TSG induction has been de-
veloped independently to this work (O?Donnell et al, 2009b;
O?Donnell et al, 2009a).
549
(a)
S
NP
NP
George
VP
V
hates
NP
NP
broccoli
(b)
S
NP
George
VP
V
V
hates
NP
broccoli
S? NP (VP (V hates) NP)
NP? George
NP? broccoli
S? (NP George) (VP V (NP broccoli))
V? hates
Figure 1: Example derivations for the same tree,
where arrows indicate substitution sites. The ele-
mentary trees used in (a) and (b) are shown below
as grammar productions in bracketed tree notation.
a distribution over sequences of elementary trees e
that compose to form t. We will then essentially read
the grammar off the elementary trees, as described
in Section 5. Our problem therefore becomes one of
identifying the posterior distribution of e given t,
which we can do using Bayes? Rule:
P (e|t) ? P (t|e)P (e) (1)
Since the sequence of elementary trees can be split
into derivations, each of which completely specifies
a tree, P (t|e) is either equal to 1 (when t and e
are consistent) or 0 (otherwise). Therefore, the work
in our model is done by the prior distribution over
elementary trees. Note that this is analogous to the
Bayesian model of word segmentation presented by
Goldwater et al (2006); indeed, the problem of in-
ferring e from t can be viewed as a segmentation
problem, where each full tree must be segmented
into one or more elementary trees. As in Goldwater
et al (2006), we wish to favour solutions employing
a relatively small number of elementary units (here,
elementary trees). This can be done using a Dirichlet
process (DP) prior. Specifically, we define the distri-
bution of elementary tree e with root non-terminal
symbol c as
Gc|?c, P0 ? DP(?c, P0(?|c))
e|c ? Gc
whereP0(?|c) (the base distribution) is a distribution
over the infinite space of trees rooted with c, and ?c
(the concentration parameter) controls the model?s
tendency towards either reusing elementary trees or
creating novel ones as each training instance is en-
countered (and consequently, the tendency to infer
larger or smaller sets of elementary trees from the
observed data). We discuss the base distribution in
more detail below.
Rather than representing the distribution Gc ex-
plicitly, we integrate over all possible values of Gc.
The resulting distribution over ei, conditioned on
e<i = e1 . . . ei?1 and the root category c is:
p(ei|e<i, c, ?c, P0) = n
<i
ei,c + ?cP0(ei|c)
n<i?,c + ?c (2)
where n<iei,c is the number number of times ei has
been used to rewrite c in e<i, and n<i?,c =
?
e n<ie,c isthe total count of rewriting c.
As with other DP models, ours can be viewed as a
cache model, where ei can be generated in one of
two ways: by drawing from the base distribution,
where the probability of any particular tree is pro-
portional to ?cP0(ei|c), or by drawing from a cache
of previous expansions of c, where the probability of
any particular expansion is proportional to the num-
ber of times that expansion has been used before.
This view makes it clear that the model embodies
a ?rich-get-richer? dynamic in which a few expan-
sions will occur with high probability, but many will
occur only once or twice, as is typical of natural lan-
guage. Our model is similar in this way to the Adap-
tor Grammar model of Johnson et al (2007a).
We still need to define P0, the base distribution
over tree fragments. We use two such distributions.
The first, PM0 generates each elementary tree by
a series of random decisions: whether to expand a
non-terminal, how many children to produce and
their identities. The probability of expanding a non-
terminal node labelled c is parameterised via a bino-
mial distribution, Bin(?c), while all other decisions
are chosen uniformly at random. The second base
distribution, PC0 , has a similar generative process
but draws non-terminal expansions from a treebank-
trained PCFG instead of a uniform distribution.
Both choices of P0 have the effect of biasing the
model towards simple rules with a small number of
internal nodes. The geometric increase in cost dis-
courages the model from using larger rules; for this
to occur these rules must yield a large increase in the
data likelihood. As PC0 incorporates PCFG probabil-
550
SNP,1
George
VP,0
V,0
hates
NP,1
broccoli
Figure 2: Gibbs state e specifying the derivation in
Figure 1a. Each node is labelled with its substitution
indicator variable.
ities, it assigns higher relative probability to larger
rules, compared to the more draconian PM0 .
4 Training
To train our model we use Gibbs sampling (Geman
and Geman, 1984), a Markov chain Monte Carlo
method in which variables are repeatedly sampled
conditioned on the values of all other variables in
the model. After a period of burn-in, each sam-
pler state (set of variable assignments) is a sample
from the posterior distribution of the model. In our
case, we wish to sample from P (e|t, ?, ?), where
(?, ?) = {?c, ?c} for all categories c. To do so,
we associate a binary variable with each non-root
internal node of each tree in the training set, indi-
cating whether that node is a substitution point or
not. Each substitution point forms the root of some
elementary tree, as well as a frontier non-terminal
of an ancestor node?s elementary tree. Collectively,
the training trees and substitution variables specify
the sequence of elementary trees e that is the current
state of the sampler. Figure 2 shows an example tree
with its substitution variables, corresponding to the
TSG derivation in Figure 1a.
Our Gibbs sampler works by sampling the value
of each substitution variable, one at a time, in ran-
dom order. If d is the node associated with the sub-
stitution variable s under consideration, then the two
possible values of s define two options for e: one
in which d is internal to some elementary tree eM ,
and one in which d is the substitution site con-
necting two smaller trees, eA and eB . In the ex-
ample in Figure 2, when sampling the VP node,
eM = (S NP (VP (V hates) NP)), eA = (S NP VP),
and eB = (VP (V hates) NP). To sample a value for
s, we compute the probabilities of eM and (eA, eB),
conditioned on e?: all other elementary trees in the
training set that share at most a root or frontier non-
terminal with eM , eA, or eB . This is easy to do
because the DP is exchangeable, meaning that the
probability of a set of outcomes does not depend on
their ordering. Therefore, we can treat the elemen-
tary trees under consideration as the last ones to be
sampled, and apply Equation 2, giving us
P (eM |cM )=n
?
eM ,cM + ?cMP0(eM |cM )
n??,cM + ?cM
(3)
P (eA, eB|cA)=n
?
eA,cA + ?cAP0(eA|cA)
n??,cA + ?cA
(4)
?
n?eB ,cB + ?(eA, eB) + ?cBP0(eB|cB)
n??,cB + ?(cA, cB) + ?cB
where cx is the root label of ex, x ? {A,B,M},
the counts n? are with respect to e?, and ?(?, ?) is
the Kronecker delta function, which returns 1 when
its arguments are identical and 0 otherwise. We have
omitted e?, t, ? and ? from the conditioning con-
text. The ? terms in the second factor of (4) account
the changes to n? that would occur after observing
eA, which forms part of the conditioning context for
eB . If the trees eA and eB are identical, then the
count n?eB ,cB would increase by one, and if the treesshare the same root non-terminal, then n??,cB wouldincrease by one.
In the previous discussion, we have assumed
that the model hyperparameters, (?, ?), are known.
However, selecting their values by hand is extremely
difficult and fitting their values on heldout data is of-
ten very time consuming. For this reason we treat
the hyper-parameters as variables in our model and
infer their values during training. We choose vague
priors for each hyper-parameter, encoding our lack
of information about their values. We treat the con-
centration parameters, ?, as being generated by a
vague gamma prior, ?c ? Gamma(0.001, 1000).
We sample a new value ??c using a log-normal dis-
tribution with mean ?c and variance 0.3, which is
then accepted into the distribution p(?c|e, t, ??, ?)
using the Metropolis-Hastings algorithm. We use a
Beta prior for the binomial specification parameters,
?c ? Beta(1, 1). As the Beta distribution is conju-
gate to the binomial, we can directly resample the
? parameters from the posterior, p(?c|e, t, ?, ??).
Both the concentration and substitution parameters
are resampled after every full Gibbs sampling itera-
tion over the training trees.
551
5 Parsing
We now turn to the problem of using the model
to parse novel sentences. This requires finding the
maximiser of
p(t|w, t) =
?
p(t|w, e, ?, ?) p(e, ?, ?|t) de d? d?
(5)
wherew is the sequence of words being parsed and t
the resulting tree, t are the training trees and e their
segmentation into elementary trees.
Unfortunately solving for the maximising parse
tree in (5) is intractable. However, it can approxi-
mated using Monte Carlo techniques. Given a sam-
ple of (e, ?, ?)5 we can reason over the space of
possible trees using a Metropolis-Hastings sampler
(Johnson et al, 2007a) coupled with a Monte Carlo
integral (Bod, 2003). The first step is to sample from
the posterior over derivations, p(d|w, e, ?, ?). This
is achieved by drawing samples from an approxima-
tion grammar, p?(d|w), which are then accepted to
the true distribution using the Metropolis-Hastings
algorithm. The second step records for each sampled
derivation the CFG tree. The counts of trees consti-
tute an approximation to p(t|w, e, ?, ?), from which
we can recover the maximum probability tree.
A natural proposal distribution, p?(d|w), is the
maximum a posterior (MAP) grammar given the el-
ementary tree analysis of our training set (analogous
to the PCFG approximation used in Johnson et al
(2007a)). This is not practical because the approx-
imation grammar is infinite: elementary trees with
zero count in e still have some residual probabil-
ity under P0. In the absence of a better alternative,
we discard (most of) the zero-count rules from MAP
grammar. This results in a tractable grammar repre-
senting the majority of the probability mass, from
which we can sample derivations. We specifically
retain all zero-count PCFG productions observed in
the training set in order to provide greater robustness
on unseen data.
In addition to finding the maximum probability
parse (MPP), we also report results using the maxi-
mum probability derivation (MPD). While this could
be calculated in the manner as described above, we
5Using many samples of (e, ?, ?) in a Monte Carlo inte-
gral is a straight-forward extension to our parsing algorithm. We
did not observe a significant improvement in parsing accuracy
when using a multiple samples compared to a single sample,
and therefore just present results for a single sample.
S ? A | B
A? A A | B B | (A a) (A a) | (B a) (B a)
B ? A A | B B | (A b) (A b) | (B b) (B b)
Figure 3: TSG used to generate synthetic data. All
production probabilities are uniform.
found that using the CYK algorithm (Cocke, 1969)
to find the Viterbi derivation for p? yielded consis-
tently better results. This algorithm maximises an
approximated model, as opposed to approximately
optimising the true model. We also present results
using the tree with the maximum expected count of
CFG rules (MER). This uses counts of the CFG rules
applied at each span (compiled from the derivation
samples) followed by a maximisation step to find the
best tree. This is similar to the MAX-RULE-SUM
algorithm of Petrov and Klein (2007) and maximum
expected recall parsing (Goodman, 2003).
6 Experiments
Synthetic data Before applying the model to
natural language, we first create a synthetic problem
to confirm that the model is capable of recovering
a known tree-substitution grammar. We created 50
random trees from the TSG shown in Figure 3. This
produces binary trees with A and B internal nodes
and ?a? and ?b? as terminals, such that the termi-
nals correspond to their grand-parent non-terminal
(A and a or B and b). These trees cannot be mod-
elled accurately with a CFG because expanding A
and B nodes into terminal strings requires knowing
their parent?s non-terminal.
We train the model for 100 iterations of Gibbs
sampling using annealing to speed convergence.
Annealing amounts to smoothing the distributions
in (3) and (4) by raising them to the power of 1T .Our annealing schedule begins at T = 3 and lin-
early decreases to reach T = 1 in the final iteration.
The sampler converges to the correct grammar, with
the 10 rules from Figure 3.
Penn-treebank parsing We ran our natural lan-
guage experiments on the Penn treebank, using the
standard data splits (sections 2?21 for training, 22
for development and 23 for testing). As our model is
parameter free (the ? and ? parameters are learnt in
training), we do not use the development set for pa-
552
rameter tuning. We expect that fitting these param-
eters to maximise performance on the development
set would lead to a small increase in generalisation
performance, but at a significant cost in runtime. We
replace tokens with count? 1 in the training sample
with one of roughly 50 generic unknown word mark-
ers which convey the token?s lexical features and po-
sition in the sentence, following Petrov et al (2006).
We also right-binarise the trees to reduce the branch-
ing factor in the same manner as Petrov et al (2006).
The predicted trees are evaluated using EVALB6 and
we report the F1 score over labelled constituents and
exact match accuracy over all sentences in the test-
ing sets.
In our experiments, we initialised the sampler by
setting all substitution variables to 0, thus treating
every full tree in the training set as an elementary
tree. Starting with all the variables set to 1 (corre-
sponding to CFG expansions) or a random mix of
0s and 1s considerably increases time until conver-
gence. We hypothesise that this is due to the sampler
getting stuck in modes, from which a series of lo-
cally bad decisions are required to escape. The CFG
solution seems to be a mode and therefore starting
the sampler with maximal trees helps the model to
avoid this mode.
Small data sample For our first treebank exper-
iments, we train on a small data sample by using
only section 2 of the treebank. Bayesian methods
tend to do well with small data samples, while for
larger samples the benefits diminish relative to point
estimates. The models were trained using Gibbs
sampling for 4000 iterations with annealing linearly
decreasing from T = 5 to T = 1, after which
the model performed another 1000 iterations with
T = 1. The final training sample was used in the
parsing algorithm, which used 1000 derivation sam-
ples for each test sentence. All results are the aver-
age of five independent runs.
Table 1 presents the prediction results on the de-
velopment set. The baseline is a maximum likeli-
hood PCFG. The TSG model significantly outper-
forms the baseline with either base distribution PM0
or PC0 . This confirms our hypothesis that CFGs are
not sufficiently powerful to model syntax, but that
the increased context afforded to the TSG can make
a large difference. This result is even more impres-
sive when considering the difference in the sizes of
6See http://nlp.cs.nyu.edu/evalb/.
F1 EX # rules
PCFG 60.20 4.29 3500
TSG PM0 : MPD 72.17 11.92 6609MPP 71.27 12.33 6609
MER 74.25 12.30 6609
TSG PC0 : MPD 75.24 15.18 14923MPP 75.30 15.74 14923
MER 76.89 15.76 14923
SM?=2: MPD 71.93 11.30 16168
MER 74.32 11.77 16168
SM?=5: MPD 75.33 15.64 39758
MER 77.93 16.94 39758
Table 1: Development results for models trained on
section 2 of the Penn tree-bank, showing labelled
constituent F1 and exact match accuracy. Grammar
sizes are the number of rules with count ? 1.
grammar in the PCFG versus TSG models. The TSG
using PM0 achieves its improvements with only dou-
ble as many rules, as a consequence of the prior
which encourages sparse solutions. The TSG results
with the CFG base distribution, PC0 , are more ac-
curate but with larger grammars.7 This base distri-
bution assigns proportionally higher probability to
larger rules than PM0 , and consequently the model
uses these additional rules in a larger grammar.
Surprisingly, the MPP technique is not systemati-
cally better than the MPD approach, with mixed re-
sults under the F1 metric. We conjecture that this is
due to sampling variance for long sentences, where
repeated samples of the same tree are exceedingly
rare. The MER technique results in considerably
better F1 scores than either MPD or MPP, with a
margin of 1.5 to 3 points. This method is less af-
fected by sampling variance due to its use of smaller
tree fragments (PCFG productions at each span).
For comparison, we trained the Berkeley split-
merge (SM) parser (Petrov et al, 2006) on the same
data and decoded using the Viterbi algorithm (MPD)
and expected rule count (MER a.k.a. MAX-RULE-
SUM). We ran two iterations of split-merge training,
after which the development F1 dropped substan-
tially (in contrast, our model is not fit to the devel-
opment data). The result is an accuracy slightly be-
low that of our model (SM?=2). To be fairer to their
model, we adjusted the unknown word threshold to
their default setting, i.e., to apply to word types oc-
7The grammar is nevertheless far smaller than the full DOP
grammar on this data set, which has 700K rules.
553
0 1 2 3 4 5 6 7 8
count
cou
nt of
 cou
nts
0
100
200
300
400
500 depthnodeslexemesvars
Figure 4: Grammar statistics for a TSG PM0 model
trained on section 2 of the Penn treebank, show-
ing a histogram over elementary tree depth, num-
ber of nodes, terminals (lexemes) and frontier non-
terminals (vars).
curring fewer than five times (SM?=5). We expect
that tuning the treatment of unknown words in our
model would also yield further gains. The grammar
sizes are not strictly comparable, as the Berkeley bi-
narised grammars prohibit non-binary rules, and are
therefore forced to decompose each of these rules
into many child rules. But the trend is clear ? our
model produces similar results to a state-of-the-art
parser, and can do so using a small grammar. With
additional rounds of split-merge training, the Berke-
ley grammar grows exponentially larger (200K rules
after six iterations).
Full treebank We now train the model using
PM0 on the full training partition of the Penn tree-
bank, using sections 2?21. We run the Gibbs sampler
for 15,000 iterations while annealing from T = 5 to
T = 1, after which we finish with 5,000 iterations
at T = 1. We repeat this three times, giving an av-
erage F1 of 84.0% on the testing partition using the
maximum expected rule algorithm and 83.0% using
the Viterbi algorithm. This far surpasses the ML-
PCFG (F1 of 70.7%), and is similar to Zuidema?s
(2007) DOP result of 83.8%. However, it still well
below state-of-the art parsers (e.g., the Berkeley
parser trained using the same data representation
scores 87.7%). But we must bear in mind that these
parsers have benefited from years of tuning to the
Penn-treebank, where our model is much simpler
and is largely untuned. We anticipate that careful
data preparation and model tuning could greatly im-
prove our model?s performance.
NP?
(NNP Mr.) NNP
CD (NN %)
(NP CD (NN %)) (PP (IN of) NP)
(NP ($ $) CD) (NP (DT a) (NN share))
(NP (DT the) (N?P (NN company) POS)) N?P
(NP QP (NN %)) (PP (IN of) NP)
(NP CD (NNS cents)) (NP (DT a) (NN share))
(NP (NNP Mr.) (N?P NNP (POS ?s))) NN
QP (NN %)
(NP (NN president)) (PP (IN of) NP)
(NP (NNP Mr.) (N?P NNP (POS ?s))) N?P
NNP (N?P NNP (NNP Corp.))
NNP (N?P NNP (NNP Inc.))
(NP (NN chairman)) (PP (IN of) NP)
VP?
(VBD said) (SBAR (S (NP (PRP it)) VP))
(VBD said) (SBAR (S NP VP))
(VBD rose) (V?P (NP CD (NN %)) V?P)
(VBP want) S
(VBD said) (SBAR (S (NP (PRP he)) VP))
(VBZ plans) S
(VBD said) (SBAR S)
(VBZ says) (SBAR (S NP VP))
(VBP think) (SBAR S)
(VBD agreed) (S (VP (TO to) (VP VB V?P)))
(VBZ includes) NP
(VBZ says) (SBAR (S (NP (PRP he)) VP))
(VBZ wants) S
(VBD closed) (V?P (PP (IN at) NP) (V?P , ADVP))
Table 3: Most frequent lexicalised expansions for
noun and verb phrases, excluding auxiliary verbs.
7 Discussion
So what kinds of non-CFG rules is the model learn-
ing? Figure 4 shows the grammar statistics for a
TSG model trained on the small data sample. This
model has 5611 CFG rules and 1008 TSG rules.
The TSG rules vary in depth from two to nine levels
with the majority between two and four. Most rules
combine a small degree of lexicalisation and a vari-
able or two. This confirms that the model is learn-
ing local structures to encode, e.g., multi-word units,
subcategorisation frames and lexical agreement. The
few very large rules specify full parses for sentences
which were repeated in the training corpus. These
complete trees are also evident in the long tail of
node counts (up to 27; not shown in the figure) and
counts for highly lexicalised rules (up to 8).
To get a better feel for the types of rules being
learnt, it is instructive to examine the rules in the re-
554
NP? PP? ADJP?
DT N?P IN NP JJ
NNS (IN in) NP RB JJ
DT NN (TO to) NP JJ ( ?ADJP CC JJ)
(DT the) N?P TO NP JJ PP
JJ NNS (IN with) NP (RB very) JJ
NP (PP (IN of) NP) (IN of) NP RB ?ADJP
NP PP (IN by) NP (RBR more) JJ
NP (N?P (CC and) NP) (IN at) NP JJ ?ADJP
JJ N?P IN (NP (DT the) N?P) ADJP ( ?ADJP CC ADJP)
NN NNS (IN on) NP RB VBN
(DT the) NNS (IN from) NP RB ( ?ADJP JJ PP)
DT (N?P JJ NN) IN (S (VP VBG NP)) JJ (PP (TO to) NP)
NN IN (NP NP PP) ADJP (PP (IN than) NP)
JJ NN (IN into) NP (RB too) JJ
(NP DT NN) (PP (IN of) NP) (IN for) NP (RB much) JJR
Table 2: Top fifteen expansions sorted by frequency (most frequent at top), taken from the final sample of a
model trained on the full Penn treebank. Non-terminals shown with an over-bar denote a binarised sub span
of the given phrase type.
sultant grammar. Table 2 shows the top fifteen rules
for three phrasal categories for the model trained on
the full Penn treebank. We can see that many of these
rules are larger than CFG rules, showing that the
CFG rules alone are inadequate to model the tree-
bank. Two of the NP rules encode the prevalence
of preposition phrases headed by ?of? within a noun
phrase, as opposed to other prepositions. Also note-
worthy is the lexicalisation of the determiner, which
can affect the type of NP expansion. For instance,
the indefinite article is more likely to have an ad-
jectival modifier, while the definite article appears
more frequently unmodified. Highly specific tokens
are also incorporated into lexicalised rules.
Many of the verb phrase expansions have been
lexicalised, encoding the verb?s subcategorisation,
as shown in Table 3. Notice that each verb here ac-
cepts only one or a small set of argument frames,
indicating that by lexicalising the verb in the VP ex-
pansion the model can find a less ambiguous and
more parsimonious grammar.
The model also learns to use large rules to de-
scribe the majority of root node expansions (we add
a distinguished TOP node to all trees). These rules
mostly describe cases when the S category is used
for a full sentence, which most often include punc-
tuation such as the full stop and quotation marks. In
contrast, the majority of expansions for the S cat-
egory do not include any punctuation. The model
has learnt to differentiate between the two different
classes of S ? full sentence versus internal clause ?
due to their different expansions.
8 Conclusion
In this work we have presented a non-parametric
Bayesian model for inducing tree substitution gram-
mars. By incorporating a structured prior over ele-
mentary rules our model is able to reason over the
infinite space of all such rules, producing compact
and simple grammars. In doing so our model learns
local structures for latent linguistic phenomena, such
as verb subcategorisation and lexical agreement. Our
experimental results show that the induced gram-
mars strongly out-perform standard PCFGs, and are
comparable to a state-of-the-art parser on small data
samples. While our results on the full treebank are
well shy of the best available parsers, we have pro-
posed a number of improvements to the model and
the parsing algorithm that could lead to state-of-the-
art performance in the future.
References
Rens Bod, Remko Scha, and Khalil Sima?an, editors.
2003. Data-oriented parsing. Center for the Study of
Language and Information - Studies in Computational
Linguistics. University of Chicago Press.
Rens Bod. 2000. Combining semantic and syntactic
structure for language modeling. In Proceedings of
the 6th International Conference on Spoken Language
Processing, Beijing, China.
Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proceedings of the 10th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, Budapest, Hungary, April.
555
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
173?180, Ann Arbor, Michigan, June.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of 1st Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 132?139.
David Chiang and Daniel M. Bikel. 2002. Recovering
latent information in treebanks. In Proceedings of the
19th International Conference on Computational Lin-
guistics, pages 183?189, Taipei, Taiwan.
John Cocke. 1969. Programming languages and their
compilers: Preliminary notes. Courant Institute of
Mathematical Sciences, New York University.
Michael John Collins. 1999. Head-driven statistical
models for natural language parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2007. The infinite tree. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 272?279, Prague, Czech
Republic, June.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distributions and the Bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6:721?741.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Contextual dependencies in unsupervised
word segmentation. In Proceedings of COLING/ACL,
Sydney.
Joshua Goodman. 2003. Efficient parsing of DOP with
PCFG-reductions. In Bod et al (Bod et al, 2003),
chapter 8.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007a. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Proceedings of Hu-
man Language Technologies 2007: The Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 139?146, Rochester,
New York, April.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007b. Adaptor grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
In Advances in Neural Information Processing Sys-
tems 19.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24(4),
December.
Mark Johnson. 2002. The DOP estimation method is
biased and inconsistent. Computational Lingusitics,
28(1):71?76, March.
Aravind Joshi. 2003. Tree adjoining grammars. In Rus-
lan Mikkov, editor, The Oxford Handbook of Computa-
tional Linguistics, pages 483?501. Oxford University
Press, Oxford, England.
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical Dirichlet
processes. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 688?697, Prague, Czech
Republic, June.
Timothy J. O?Donnell, Noah D. Goodman, Jesse
Snedeker, and Joshua B. Tenenbaum. 2009a. Com-
putation and reuse in language. In 31st Annual Con-
ference of the Cognitive Science Society, Amsterdam,
The Netherlands, July. To appear.
Timothy J. O?Donnell, Noah D. Goodman, and Joshua B.
Tenenbaum. 2009b. Fragment grammar: Exploring
reuse in hierarchical generative processes. Technical
Report MIT-CSAIL-TR-2009-013, MIT.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of Hu-
man Language Technologies 2007: The Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 404?411, Rochester,
New York, April. Association for Computational Lin-
guistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433?440, Sydney, Aus-
tralia, July.
Detlef Prescher, Remko Scha, Khalil Sima?an, and An-
dreas Zollmann. 2004. On the statistical consistency
of dop estimators. In Proceedings of the 14th Meet-
ing of Computational Linguistics in the Netherlands,
Antwerp, Belgium.
Fei Xia. 2002. Automatic grammar generation from
two different perspectives. Ph.D. thesis, University of
Pennsylvania.
Andreas Zollmann and Khalil Sima?an. 2005. A consis-
tent and efficient estimator for data-oriented parsing.
Journal of Automata, Languages and Combinatorics,
10(2):367?388.
Willem Zuidema. 2007. Parsimonious data-oriented
parsing. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 551?560, Prague, Czech Republic, June.
556
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 337?340,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
A Note on the Implementation of
Hierarchical Dirichlet Processes
Phil Blunsom
?
pblunsom@inf.ed.ac.uk
Sharon Goldwater
?
sgwater@inf.ed.ac.uk
Trevor Cohn
?
tcohn@inf.ed.ac.uk
Mark Johnson
?
mark johnson@brown.edu
?
Department of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
?
Department of Cognitive and Linguistic Sciences
Brown University
Providence, RI, USA
Abstract
The implementation of collapsed Gibbs
samplers for non-parametric Bayesian
models is non-trivial, requiring con-
siderable book-keeping. Goldwater et
al. (2006a) presented an approximation
which significantly reduces the storage
and computation overhead, but we show
here that their formulation was incorrect
and, even after correction, is grossly inac-
curate. We present an alternative formula-
tion which is exact and can be computed
easily. However this approach does not
work for hierarchical models, for which
case we present an efficient data structure
which has a better space complexity than
the naive approach.
1 Introduction
Unsupervised learning of natural language is one
of the most challenging areas in NLP. Recently,
methods from nonparametric Bayesian statistics
have been gaining popularity as a way to approach
unsupervised learning for a variety of tasks,
including language modeling, word and mor-
pheme segmentation, parsing, and machine trans-
lation (Teh et al, 2006; Goldwater et al, 2006a;
Goldwater et al, 2006b; Liang et al, 2007; Finkel
et al, 2007; DeNero et al, 2008). These mod-
els are often based on the Dirichlet process (DP)
(Ferguson, 1973) or hierarchical Dirichlet process
(HDP) (Teh et al, 2006), with Gibbs sampling
as a method of inference. Exact implementation
of such sampling methods requires considerable
bookkeeping of various counts, which motivated
Goldwater et al (2006a) (henceforth, GGJ06) to
develop an approximation using expected counts.
However, we show here that their approximation
is flawed in two respects: 1) It omits an impor-
tant factor in the expectation, and 2) Even after
correction, the approximation is poor for hierar-
chical models, which are commonly used for NLP
applications. We derive an improvedO(1) formula
that gives exact values for the expected counts in
non-hierarchical models. For hierarchical models,
where our formula is not exact, we present an
efficient method for sampling from the HDP (and
related models, such as the hierarchical Pitman-
Yor process) that considerably decreases the mem-
ory footprint of such models as compared to the
naive implementation.
As we have noted, the issues described in this
paper apply to models for various kinds of NLP
tasks; for concreteness, we will focus on n-gram
language modeling for the remainder of the paper,
closely following the presentation in GGJ06.
2 The Chinese Restaurant Process
GGJ06 present two nonparametric Bayesian lan-
guage models: a DP unigram model and an HDP
bigram model. Under the DP model, words in a
corpus w = w
1
. . . w
n
are generated as follows:
G|?
0
, P
0
? DP(?
0
, P
0
)
w
i
|G ? G
where G is a distribution over an infinite set of
possible words, P
0
(the base distribution of the
DP) determines the probability that an item will
be in the support of G, and ?
0
(the concentration
parameter) determines the variance of G.
One way of understanding the predictions that
the DP model makes is through the Chinese restau-
rant process (CRP) (Aldous, 1985). In the CRP,
customers (word tokensw
i
) enter a restaurant with
an infinite number of tables and choose a seat. The
table chosen by the ith customer, z
i
, follows the
distribution:
P (z
i
= k|z
?i
) =
{
n
z
?i
k
i?1+?
0
, 0 ? k < K(z
?i
)
?
0
i?1+?
0
, k = K(z
?i
)
337
The
1
meow
4
cats
2
cats 
3
cats
5
a
b
c
d
e f
g
h
Figure 1. A seating assignment describing the state of
a unigram CRP. Letters and numbers uniquely identify
customers and tables. Note that multiple tables may
share a label.
where z
?i
= z
1
. . . z
i?1
are the table assignments
of the previous customers, n
z
?i
k
is the number of
customers at table k in z
?i
, andK(z
?i
) is the total
number of occupied tables. If we further assume
that table k is labeled with a word type `
k
drawn
from P
0
, then the assignment of tokens to tables
defines a distribution over words, with w
i
= `
z
i
.
See Figure 1 for an example seating arrangement.
Using this model, the predictive probability of
w
i
, conditioned on the previous words, can be
found by summing over possible seating assign-
ments for w
i
, and is given by
P (w
i
= w|w
?i
) =
n
w
?i
w
+ ?
0
P
0
i? 1 + ?
0
(1)
This prediction turns out to be exactly that of the
DP model after integrating out the distribution G.
Note that as long as the base distribution P
0
is
fixed, predictions do not depend on the seating
arrangement z
?i
, only on the count of word w
in the previously observed words (n
w
?i
w
). How-
ever, in many situations, we may wish to estimate
the base distribution itself, creating a hierarchical
model. Since the base distribution generates table
labels, estimates of this distribution are based on
the counts of those labels, i.e., the number of tables
associated with each word type.
An example of such a hierarchical model is the
HDP bigram model of GGJ06, in which each word
typew is associated with its own restaurant, where
customers in that restaurant correspond to words
that follow w in the corpus. All the bigram restau-
rants share a common base distribution P
1
over
unigrams, which must be inferred. Predictions in
this model are as follows:
P
2
(w
i
|h
?i
) =
n
h
?i
(w
i?1
,w
i
)
+ ?
1
P
1
(w
i
|h
?i
)
n
h
?i
(w
i?1
,?)
+ ?
1
P
1
(w
i
|h
?i
) =
t
h
?i
w
i
+ ?
0
P
0
(w
i
)
t
h
?i
?
+ ?
0
(2)
where h
?i
= (w
?i
, z
?i
), t
h
?i
w
i
is the number of
tables labelled with w
i
, and t
h
?i
?
is the total num-
ber of occupied tables. Of particular note for our
discussion is that in order to calculate these condi-
tional distributions we must know the table assign-
ments z
?i
for each of the words in w
?i
. Moreover,
in the Gibbs samplers often used for inference in
1         10         100         1000   
0.1
 
  
  
   
1
 
  
  
   
10
 
  
  
   
100
 
 
Me
an 
num
ber
 of 
lex
ica
l en
trie
s
Word frequency (nw)
 
 
Expectation
Antoniak approx.
Empirical, fixed base
Empirical, inferred base
Figure 2. Comparison of several methods of approx-
imating the number of tables occupied by words of
different frequencies. For each method, results using
? = {100, 1000, 10000, 100000} are shown (from bottom
to top). Solid lines show the expected number of tables,
computed using (3) and assuming P
1
is a fixed uni-
form distribution over a finite vocabulary (values com-
puted using the Digamma formulation (7) are the same).
Dashed lines show the values given by the Antoniak
approximation (4) (the line for ? = 100 falls below the
bottom of the graph). Stars show the mean of empirical
table counts as computed over 1000 samples from an
MCMC sampler in which P
1
is a fixed uniform distri-
bution, as in the unigram LM. Circles show the mean
of empirical table counts when P
1
is inferred, as in the
bigram LM. Standard errors in both cases are no larger
than the marker size. All plots are based on the 30114-
word vocabulary and frequencies found in sections 0-20
of the WSJ corpus.
these kinds of models, the counts are constantly
changing over multiple samples, with tables going
in and out of existence frequently. This can create
significant bookkeeping issues in implementation,
and motivated GGJ06 to present a method of com-
puting approximate table counts based on word
frequencies only.
3 Approximating Table Counts
Rather than explicitly tracking the number of
tables t
w
associated with each word w in their
bigram model, GGJ06 approximate the table
counts using the expectation E[t
w
]. Expected
counts are used in place of t
h
?i
w
i
and t
h
?i
?
in (2).
The exact expectation, due to Antoniak (1974), is
E[t
w
] = ?
1
P
1
(w)
n
w
?
i=1
1
?
1
P
1
(w) + i? 1
(3)
338
Antoniak also gives an approximation to this
expectation:
E[t
w
] ? ?
1
P
1
(w) log
n
w
+ ?
1
P
1
(w)
?
1
P
1
(w)
(4)
but provides no derivation. Due to a misinterpre-
tation of Antoniak (1974), GGJ06 use an approx-
imation that leaves out all the P
1
(w) terms from
(4).
1
Figure 2 compares the approximation to
the exact expectation when the base distribution
is fixed. The approximation is fairly good when
?P
1
(w) > 1 (the scenario assumed by Antoniak);
however, in most NLP applications, ?P
1
(w) <
1 in order to effect a sparse prior. (We return
to the case of non-fixed based distributions in a
moment.) As an extreme case of the paucity of
this approximation consider ?
1
P
1
(w) = 1 and
n
w
= 1 (i.e. only one customer has entered the
restaurant): clearly E[t
w
] should equal 1, but the
approximation gives log(2).
We now provide a derivation for (4), which will
allow us to obtain an O(1) formula for the expec-
tation in (3). First, we rewrite the summation in (3)
as a difference of fractional harmonic numbers:
2
H
(?
1
P
1
(w)+n
w
?1)
?H
(?
1
P
1
(w)?1)
(5)
Using the recurrence for harmonic numbers:
E[t
w
] ? ?
1
P
1
(w)
[
H
(?
1
P
1
(w)+n
w
)
?
1
?
1
P
1
(w) + n
w
?H
(?
1
P
1
(w)+n
w
)
+
1
?
1
P
1
(w)
]
(6)
We then use the asymptotic expansion,
H
F
? logF + ? +
1
2F
, omiting trailing terms
which are O(F
?2
) and smaller powers of F :
3
E[t
w
] ? ?
1
P
1
(w) log
n
w
+?
1
P
1
(w)
?
1
P
1
(w)
+
n
w
2(?
1
P
1
(w)+n
w
)
Omitting the trailing term leads to the
approximation in Antoniak (1974). However, we
can obtain an exact formula for the expecta-
tion by utilising the relationship between the
Digamma function and the harmonic numbers:
?(n) = H
n?1
? ?.
4
Thus we can rewrite (5) as:
5
E[t
w
] = ?
1
P
1
(w)?
[
?(?
1
P
1
(w) + n
w
)? ?(?
1
P
1
(w))
]
(7)
1
The authors of GGJ06 realized this error, and current
implementations of their models no longer use these approx-
imations, instead tracking table counts explicitly.
2
Fractional harmonic numbers between 0 and 1 are given
by H
F
=
R
1
0
1?x
F
1?x
dx. All harmonic numbers follow the
recurrence H
F
= H
F?1
+
1
F
.
3
Here, ? is the Euler-Mascheroni constant.
4
AccurateO(1) approximations of the Digamma function
are readily available.
5
(7) can be derived from (3) using: ?(x+1)??(x) =
1
x
.
Explicit table tracking:
customer(w
i
)? table(z
i
)
n
a : 1, b : 1, c : 2, d : 2, e : 3, f : 4, g : 5, h : 5
o
table(z
i
)? label(`)
n
1 : The, 2 : cats, 3 : cats, 4 : meow, 5 : cats
o
Histogram:
word type?
{
table occupancy? frequency
}
n
The : {2 : 1}, cats : {1 : 1, 2 : 2}, meow : {1 : 1}
o
Figure 3. The explicit table tracking and histogram rep-
resentations for Figure 1.
A significant caveat here is that the expected
table counts given by (3) and (7) are only valid
when the base distribution is a constant. However,
in hierarchical models such as GGJ06?s bigram
model and HDP models, the base distribution is
not constant and instead must be inferred. As can
be seen in Figure 2, table counts can diverge con-
siderably from the expectations based on fixed
P
1
when P
1
is in fact not fixed. Thus, (7) can
be viewed as an approximation in this case, but
not necessarily an accurate one. Since knowing
the table counts is only necessary for inference
in hierarchical models, but the table counts can-
not be approximated well by any of the formu-
las presented here, we must conclude that the best
inference method is still to keep track of the actual
table counts. The naive method of doing so is to
store which table each customer in the restaurant
is seated at, incrementing and decrementing these
counts as needed during the sampling process. In
the following section, we describe an alternative
method that reduces the amount of memory neces-
sary for implementing HDPs. This method is also
appropriate for hierarchical Pitman-Yor processes,
for which no closed-form approximations to the
table counts have been proposed.
4 Efficient Implementation of HDPs
As we do not have an efficient expected table
count approximation for hierarchical models we
could fall back to explicitly tracking which table
each customer that enters the restaurant sits at.
However, here we describe a more compact repre-
sentation for the state of the restaurant that doesn?t
require explicit table tracking.
6
Instead we main-
tain a histogram for each dish w
i
of the frequency
of a table having a particular number of customers.
Figure 3 depicts the histogram and explicit repre-
sentations for the CRP state in Figure 1.
Our alternative method of inference for hierar-
chical Bayesian models takes advantage of their
6
Teh et al (2006) also note that the exact table assign-
ments for customers are not required for prediction.
339
Algorithm 1 A new customer enters the restaurant
1: w: word type
2: P
w
0
: Base probability for w
3: HD
w
: Seating Histogram for w
4: procedure INCREMENT(w,P
w
0
,HD
w
)
5: p
share
?
n
w
?1
w
n
w
?1
w
+?
0
. share an existing table
6: p
new
?
?
0
?P
w
0
n
w
?1
w
+?
0
. open a new table
7: r ? random(0, p
share
+ p
new
)
8: if r < p
new
or n
w
?1
w
= 0 then
9: HD
w
[1] = HD
w
[1] + 1
10: else
. Sample from the histogram of customers at tables
11: r ? random(0, n
w
?1
w
)
12: for c ? HD
w
do . c: customer count
13: r = r ? (c? HD
w
[c])
14: if r ? 0 then
15: HD
w
[c] = HD
w
[c] + 1
16: Break
17: n
w
w
= n
w
?1
w
+ 1 . Update token count
Algorithm 2 A customer leaves the restaurant
1: w: word type
2: HD
w
: Seating histogram for w
3: procedure DECREMENT(w,P
w
0
,HD
w
)
4: r ? random(0, n
w
w
)
5: for c ? HD
w
do . c: customer count
6: r = r ? (c? HD
w
[c])
7: if r ? 0 then
8: HD
w
[c] = HD
w
[c]? 1
9: if c > 1 then
10: HD
w
[c? 1] = HD
w
[c? 1] + 1
11: Break
12: n
w
w
= n
w
w
? 1 . Update token count
exchangeability, which makes it unnecessary to
know exactly which table each customer is seated
at. The only important information is how many
tables exist with different numbers of customers,
and what their labels are. We simply maintain a
histogram for each word type w, which stores, for
each number of customersm, the number of tables
labeled with w that have m customers. Figure 3
depicts the explicit representation and histogram
for the CRP state in Figure 1.
Algorithms 1 and 2 describe the two operations
required to maintain the state of a CRP.
7
When
a customer enters the restaurant (Alogrithm 1)),
we sample whether or not to open a new table.
If not, we sample an old table proportional to the
counts of how many customers are seated there
and update the histogram. When a customer leaves
the restaurant (Algorithm 2), we decrement one
of the tables at random according to the number
of customers seated there. By exchangeability, it
doesn?t actually matter which table the customer
was ?really? sitting at.
7
A C++ template class that implements
the algorithm presented is made available at:
http://homepages.inf.ed.ac.uk/tcohn/
5 Conclusion
We?ve shown that the HDP approximation pre-
sented in GGJ06 contained errors and inappropri-
ate assumptions such that it significantly diverges
from the true expectations for the most common
scenarios encountered in NLP. As such we empha-
sise that that formulation should not be used.
Although (7) allowsE[t
w
] to be calculated exactly
for constant base distributions, for hierarchical
models this is not valid and no accurate calculation
of the expectations has been proposed. As a rem-
edy we?ve presented an algorithm that efficiently
implements the true HDP without the need for
explicitly tracking customer to table assignments,
while remaining simple to implement.
Acknowledgements
The authors would like to thank Tom Grif-
fiths for providing the code used to produce
Figure 2 and acknowledge the support of the
EPSRC (Blunsom, grant EP/D074959/1; Cohn,
grant GR/T04557/01).
References
D. Aldous. 1985. Exchangeability and related topics. In
?
Ecole d?
?
Et?e de Probabiliti?es de Saint-Flour XIII 1983, 1?
198. Springer.
C. E. Antoniak. 1974. Mixtures of dirichlet processes with
applications to bayesian nonparametric problems. The
Annals of Statistics, 2(6):1152?1174.
J. DeNero, A. Bouchard-C?ot?e, D. Klein. 2008. Sampling
alignment structure under a Bayesian translation model.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, 314?323, Hon-
olulu, Hawaii. Association for Computational Linguistics.
S. Ferguson. 1973. A Bayesian analysis of some nonpara-
metric problems. Annals of Statistics, 1:209?230.
J. R. Finkel, T. Grenager, C. D. Manning. 2007. The infinite
tree. In Proc. of the 45th Annual Meeting of the ACL
(ACL-2007), Prague, Czech Republic.
S. Goldwater, T. Griffiths, M. Johnson. 2006a. Contex-
tual dependencies in unsupervised word segmentation. In
Proc. of the 44th Annual Meeting of the ACL and 21st
International Conference on Computational Linguistics
(COLING/ACL-2006), Sydney.
S. Goldwater, T. Griffiths, M. Johnson. 2006b. Interpolating
between types and tokens by estimating power-law gener-
ators. In Y. Weiss, B. Sch?olkopf, J. Platt, eds., Advances
in Neural Information Processing Systems 18, 459?466.
MIT Press, Cambridge, MA.
P. Liang, S. Petrov, M. Jordan, D. Klein. 2007. The infinite
PCFG using hierarchical Dirichlet processes. In Proc. of
the 2007 Conference on Empirical Methods in Natural
Language Processing (EMNLP-2007), 688?697, Prague,
Czech Republic.
Y. W. Teh, M. I. Jordan, M. J. Beal, D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the American
Statistical Association, 101(476):1566?1581.
340
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 676?683, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Improving Statistical MT through Morphological Analysis
Sharon Goldwater
Dept. of Cognitive and Linguistic Sciences
Brown University
sharon goldwater@brown.edu
David McClosky
Dept. of Computer Science
Brown University
dmcc@cs.brown.edu
Abstract
In statistical machine translation, estimat-
ing word-to-word alignment probabilities
for the translation model can be difficult
due to the problem of sparse data: most
words in a given corpus occur at most a
handful of times. With a highly inflected
language such as Czech, this problem can
be particularly severe. In addition, much
of the morphological variation seen in Czech
words is not reflected in either the morphol-
ogy or syntax of a language like English. In
this work, we show that using morphologi-
cal analysis to modify the Czech input can
improve a Czech-English machine transla-
tion system. We investigate several differ-
ent methods of incorporating morphological
information, and show that a system that
combines these methods yields the best re-
sults. Our final system achieves a BLEU
score of .333, as compared to .270 for the
baseline word-to-word system.
1 Introduction
In a statistical machine translation task, the goal is
to find the most probable translation of some foreign
language text f into the desired language e. That is,
the system seeks to maximize P (e|f). Rather than
maximizing P (e|f) directly, the standard noisy chan-
nel approach to translation uses Bayes inversion to
split the problem into two separate parts:
argmax
e
P(e|f) = argmax
e
P(e)P(f |e) (1)
where P (e) is known as the language model and
P (f |e) is known as the translation model. The limit-
ing factor in machine translation is usually the qual-
ity of the translation model, since the monolingual
resources needed for training the language model are
generally more available than the parallel corpora
needed for training the translation model.
Due to the difficulty in obtaining large parallel cor-
pora, sparse data is a serious issue when estimating
the parameters of the translation model. This prob-
lem is compounded when one or both of the lan-
guages involved is a highly inflected language. In this
paper, we present a series of experiments suggesting
that morphological analysis can be used to reduce
data sparseness and increase similarity between lan-
guages, thus improving the quality of machine trans-
lation for highly inflected languages. Our work is on
a language pair in which the input language (Czech)
is highly inflected, and the output language (English)
is not. We discuss in Section 5 how our methods
might be generalized to pairs where both languages
are highly inflected.
The plan of this paper is as follows: In Section
2, we review previous work on using morphologi-
cal analysis for statistical machine translation. In
Section 3, we describe several methods for utilizing
morphological information in a statistical translation
model. Section 4 presents the results of our experi-
ments using these methods. Sections 5 and 6 discuss
the results of our experiments and conclude the pa-
per.
2 Previous Work
Until recently, most machine translation projects in-
volved translating between languages with relatively
little morphological structure. Nevertheless, a few
research projects have investigated the use of mor-
phology to improve translation quality. Niessen and
Ney (2000; 2004) report work on German-English
translation, where they investigate various types
of morphosyntactic restructuring, including merging
German verbs with their detached prefixes, annotat-
ing a handful of frequent ambiguous German words
with POS tags, combining idiomatic multi-word ex-
pressions into single words, and undoing question in-
676
version and do-insertion in both German and En-
glish. In addition, Niessen and Ney (2004) decom-
pose German words into a hierarchical representa-
tion using lemmas and morphological tags, and use
a MaxEnt model to combine the different levels of
representation in the translation model. The results
from these papers indicate that on corpus sizes up
to 60,000 parallel sentences, the restructuring op-
erations yielded a large improvement in translation
quality, but the morphological decomposition pro-
vided only a slight additional benefit. However, since
German is not as morphologically complex as Czech,
we might expect a larger benefit from morphological
analysis in Czech.
Another project utilizing morphological analysis
for statistical machine translation is described by Lee
(2004). Lee?s system for Arabic-English translation
takes as input POS-tagged English and Arabic text,
where the Arabic words have been pre-segmented
into stems and affixes. The system performs an ini-
tial alignment of the Arabic morphemes to the En-
glish words. Based on the consistency of the English
POS tag that each Arabic morpheme aligns to, the
system determines whether to keep that morpheme
as a separate item, merge it back onto the stem,
or delete it altogether. In addition, multiple occur-
rences of the determiner Al within a single Arabic
noun phrase are deleted (i.e. only one occurrence
is allowed). Using a phrase-based translation model,
Lee found that Al-deletion was more helpful than the
rest of the morphological analysis. Also, Al-deletion
helped for training corpora up to 3.3 million sen-
tences, but the other morphological analysis helped
only on the smaller corpus sizes (up to 350,000 paral-
lel sentences). This result is consistent with anecdo-
tal evidence suggesting that morphological analysis
becomes less helpful as corpus sizes increase. How-
ever, since parallel corpora of hundreds of thousands
of sentences or more are often difficult to obtain, it
would still be worthwhile to develop a method for
improving systems trained on smaller corpora.
Previous results on Czech-English machine trans-
lation suggest that morphological analysis may be
quite productive for this highly inflected language
where there is only a small amount of closely trans-
lated material. C?mejrek et al (2003), while not fo-
cusing on the use of morphology, give results indicat-
ing that lemmatization of the Czech input improves
BLEU score relative to baseline. These results sup-
port the earlier findings of Al-Onaizan et al (1999),
who used subjective scoring measures. Al-Onaizan
et al measured translation accuracy not only for
lemmatized input, but for an input form they re-
fer to as Czech?. Czech? is intended to capture many
of the morphological distinctions of English, while
discarding those distinctions that are Czech-specific.
The Czech? input was created by distinguishing the
Czech lemmas for singular and plural nouns, differ-
ent verb tenses, and various inflections on pronouns.
Artificial words were also added automatically in
cases where syntactic information in the Czech parse
trees indicated that articles, pronouns, or preposi-
tions might be expected in English. The transforma-
tion to Czech? provided a small additional increase
in translation quality over basic lemmatization.
The experiments described here are similar to
those performed by Al-Onaizan et al (1999), but
there are several important differences. First, we use
no syntactic analysis of the Czech input. Our intent
is to determine how much can be gained by a purely
morphological approach to translation. Second, we
present some experiments in which we modify the
translation model itself to take advantage of morpho-
logical information, rather than simply transforming
the input. Finally, our use of BLEU scores rather
than subjective measurements allows us to perform
more detailed evaluation. We examine the effects of
each type of morphological information separately.
3 Morphology for MT
Morphological variations in Czech are reflected in
several different ways in English. In some cases, such
as verb past tenses or noun plurals, morphological
distinctions found in Czech are also found in English.
In other instances, English may use function words
to express a meaning that occurs as a morphological
variant in Czech. For example, genitive case marking
can often be translated as of and instrumental case
as by or with. In still other instances, morphologi-
cal distinctions made in Czech are either completely
absent in English (e.g. gender on common nouns)
or are reflected in English syntax (e.g. many case
markings). Handling these correspondences between
morphology and syntax requires analysis above the
lexical level and is therefore beyond the scope of this
paper. However, morphological analysis of the Czech
input can potentially be used to improve the trans-
lation model by exploiting the other types of corre-
spondences we have mentioned.
Before we describe how this can be done, it is im-
portant to clarify the kind of morphological anal-
ysis we assume in our input. Our data comes
from the Prague Czech-English Dependency Tree-
bank (PCEDT) (Hajic?, 1998; C?mejrek et al, 2004),
the Czech portion of which has been fully annotated
with morphological information. Each Czech word in
the corpus is associated with an analysis containing
the word?s lemma and a sequence of morphological
677
Pro/pro/RR--4----------
ne?koho/ne?kdo/PZM-4----------
by/by?t/Vc-X---3-------
jej??/jeho/PSZS1FS3-------
proveden??/proveden??/NNNS4-----A----
me?lo/m??t/VpNS---XR-AA---
smysl/smysl/NNIS4-----A----
././Z:-------------
Figure 1: A sentence from the PCEDT corpus. Each
token is followed by its lemma and a string giving
the values of up to 15 morphological tags. Dashes
indicates tags that are not applicable for a particu-
lar token. This sentence corresponds to the English
sentence It would make sense for somebody to do it.
tags. These tags provide values along several mor-
phological dimensions, such as part of speech, gen-
der, number, tense, and negation. There are a total
of 15 dimensions along which words may be charac-
terized, although most words have a number of di-
mensions unspecified. An example sentence from the
Czech corpus is shown in Figure 1.
In what follows, we describe four different ways
that the Czech lemma and tag information can be
used to modify the parameters of the translation
model. The first three of these are similar to the work
of Al-Onaizan et al (1999) and involve transforma-
tions to the input data only. The assumptions un-
derlying the word alignment model P (fj|ei) (where
fj and ei are individual words in an aligned sen-
tence pair) are maintained. The fourth method of
incorporating morphological information is novel and
changes the alignment model itself.
3.1 Lemmas
A very simple way to modify the input data us-
ing morphological information is by replacing each
wordform with its associated lemma (see Figure 2).
Based on previous results (Al-Onaizan et al, 1999;
C?mejrek et al, 2003), we expected that this trans-
formation would lead to an improvement in trans-
lation quality due to reduction of data sparseness.
However, since lemmatization does remove some use-
ful information from the Czech wordforms, we also
tried two alternative lemmatization schemes. First,
we tried lemmatizing only certain parts of speech,
leaving other parts of speech alone. We reasoned
that nouns, verbs, and pronouns all carry inflectional
morphology in English, so by lemmatizing only the
other parts of speech, we might retain some of the
benefits of full lemmatization without losing as much
information. We also tried lemmatizing all parts of
speech except pronouns, which are very common and
therefore should be less affected by sparse data prob-
lems.
As a second alternative to full lemmatization, we
experimented with lemmatizing only the less fre-
quent wordforms in the corpus. This allows the
translation system to use the full wordform infor-
mation from more frequent forms, where sparse data
is less of a problem.
To determine whether knowledge of lemmas was
actually necessary, we compared lemmatization with
word truncation. We truncated each wordform in the
data after a fixed number of characters, as suggested
by Och (1995).
3.2 Pseudowords
As discussed earlier, much of the information en-
coded in Czech morphology is encoded as function
words in English. One way to reintroduce some of
the information lost during Czech lemmatization is
by using some of the morphological tags to add ex-
tra ?words? to the Czech input. In many cases,
these pseudowords will also increase the correspon-
dence of English function words to items in the Czech
input. In our system, each pseudoword encodes a
single morphological tag (feature/value pair), such
as PER 1 (?first person?) or TEN F (?future tense?).
Figure 2 shows a Czech input sentence after gener-
ating pseudowords for the person feature on verbs.
We expected that the class of tags most likely to
be useful as pseudowords would be the person tags,
because Czech is a pro-drop language. Using the
person tags as pseudowords should simulate the ex-
istence of pronouns for the English pronouns to align
to. We also expected that negation (which is ex-
pressed on verbs in Czech) would be a useful pseu-
doword, and that case markings might also be helpful
since they sometimes correspond to prepositions in
English, such as of, with, or to.
3.3 Modified Lemmas
In some cases, such as the past tense, Czech mor-
phology is likely to correspond not to a function
word in English, but rather to English inflectional
morphology. In order to capture this kind of phe-
nomenon, we experimented with concatenating the
Czech morphological tags onto their lemmas instead
of inserting them as separate input tokens. See Fig-
ure 2 for an example. This concatenation creates
distinctions between some lemmas, which will ide-
ally correspond to morphological distinctions made
in English. Although this transformation splits the
Czech data (relative to pure lemmatization), it still
suppresses many of the distinctions made in the full
Czech wordforms. We expected that number mark-
678
Words: Pro ne?koho by jej?? proveden?? me?lo smysl .
Lemmas: pro ne?kdo by?t jeho proveden?? m??t smysl .
Lemmas+Pseudowords: pro ne?kdo by?t PER 3 jeho proveden?? m??t PER X smysl .
Modified Lemmas: pro ne?kdo by?t+PER 3 jeho proveden?? m??t+PER X smysl .
Figure 2: Various transformations of the Czech sentence from Figure 1. The pseudowords and modified
lemmas encode the verb person feature, with the values 3 (third person) and X (?any? person).
ing on nouns and tense marking on verbs would be
the tags best treated in this way.
3.4 Morphemes
Our final set of experiments used the same input for-
mat as the Modified Lemma experiments. However,
in this set of experiments, we changed the model used
to calculate the word-to-word alignment probabili-
ties. In the standard system, the alignment model
parameters P (fj |ei) are found using maximum like-
lihood estimation based on the expected number of
times fj aligns to ei in the parallel corpus. Our new
model assumes a compositional structure for fj , so
that fj = fj0 . . . fjK , where fj0 is the lemma of
fj , and fj1 . . . fjK are morphemes generated from
the tags associated with fj . We assume that every
word contains exactly K morphemes, and that the
kth morpheme in each word is used to encode the
value for the kth class of morphological tag, where
the classes (e.g. person or tense) are assigned an or-
dering beforehand. fjk is assigned a null value if the
value of the kth tag class is unspecified for fj .
Given this decomposition of words into mor-
phemes, and a generative model in which each mor-
pheme in fj is generated independently conditioned
on ei, we have
P(fj|ei) =
K
?
k=0
P(fjk|ei) (2)
We can now estimate P(fj |ei) using a slightly
modified version of the standard EM algorithm for
learning alignment probabilities. During the E step,
we calculate the expected alignment counts between
Czech morphemes and English words based on the
current word alignments, and revise our estimate of
P(fj|ei) using Equation 2. The M step of the algo-
rithm remains the same.
The morpheme-based model in Equation 2 is sim-
ilar to the modified lemma model in that it removes
much of the differentiation between Czech word-
forms, but leaves the differences that are most likely
to appear as inflection on English words. However,
it also performs an additional smoothing function.
The model assumes that, in the absence of other in-
formation, an English word that has aligned mostly
to Czech words with a particular morphological tag
is more likely to align to another word with this tag
than to a Czech word with a different tag. For ex-
ample, an English word aligned to mostly past tense
forms is more likely to align to another past tense
form than to a present or future tense form.
4 Experiments
In order to evaluate the effectiveness of the tech-
niques described in the previous section, we ran a
number of experiments using data from the PCEDT
corpus. The English portion of this corpus (used to
train the language model) contains the same material
as the Penn WSJ corpus, but with a different divi-
sion into training, development, and test sets. About
250 sentences each for development and test were
translated once into Czech and then back into En-
glish by five different translators. These translations
are used to calculate BLEU scores. The remainder
of the corpus (about 50,000 sentences) is used for
training. About 21,000 of the training sentences have
been translated into Czech and morphologically an-
notated for use as a parallel corpus.
Some statistics on the parallel corpus are shown
in the graph in Figure 3. This graph illustrates the
sparse data problem in Czech that our morpholog-
ical analysis is intended to address. Although the
number of infrequently occurring lemmas is about
the same in both English and Czech, the number of
infrequently occurring inflected wordforms is approx-
imately twice as high in Czech.1
For all of our experiments, we used the same lan-
guage model, trained with the CMU Statistical Lan-
guage Modelling Toolkit (Clarkson and Rosenfeld,
1997). Our translation models were trained using
GIZA++ (Och and Ney, 2003), which we modi-
1Although we did not use it for the experiments in
this paper, the PCEDT corpus does contain lemma in-
formation for the English data. There is a slight discrep-
ancy between the English and Czech data in the lemma
information for pronouns, in that English pronouns (in-
cluding accusitive, possessive, and other forms) are as-
signed themselves as lemmas, whereas Czech pronouns
are reduced to uninflected forms. Given that pronouns
generally have many tokens, this discrepancy should not
affect the data in Figure 3.
679
1 2 3 4 5 6 7 8 9 10
0
0.5
1
1.5
2
2.5
3
3.5
x 104
Token count
Ite
m
 c
ou
nt
English Wordforms
Czech Wordforms
English Lemmas
Czech Lemmas
Figure 3: The number of items (full wordforms or
lemmas) y appearing in the parallel corpus with a
token count of x.
fied as necessary for the morpheme-based experi-
ments. We used the ISI ReWrite Decoder (Marcu
and Germann, 2005) for producing translations. Be-
fore beginning our experiments, we obtained a base-
line BLEU score by training a standard word-to-word
translation model. Our baseline results indicate that
the test set for this corpus is considerably more diffi-
cult than the development set: word-to-word scores
were .311 (development) and .270 (test).
4.1 Lemmas
As Figure 3 shows, lemmatization of the Czech cor-
pus cuts the number of unique items by more than
half, and the number of items with no more than
ten occurrences by nearly half. The lemmatization
BLEU scores in Table 1 indicate that this has a large
impact on the quality of translation. As expected,
full lemmatization performed better than word-to-
word translation, with an an improvement of about
.04 in the development set BLEU score and .03 in
the test set. (In this and the following experiments,
BLEU score differences of .009 or more are signifi-
cant at the .05 level.) Experiments on the develop-
ment set showed that leaving certain parts of speech
unlemmatized did not improve results, but lemma-
tizing only low-frequency words did. A frequency
cutoff of 50 worked best on the development set (i.e.
only words with frequency less than 50 were lemma-
tized). Despite the improvement on the development
set, using this cutoff with the test set yielded only a
non-significant improvement over full lemmatization.
The results of these lemmatization experiments
support the argument that lemmatization improves
translation quality by reducing data sparseness, but
also removes potentially useful information. Our re-
Dev Test
word-to-word .311 .270
lemmatize all .355 .299
except Pro .350
except Pro, V, N .346
lemmatize n < 50 .370 .306
truncate all .353 .283
Table 1: BLEU scores for the word-to-word baseline,
lemmatization, and word truncation experiments.
sults suggest that lemmatizing only infrequent words
may, in some cases, work better than lemmatizing all
words.
As Table 1 indicates, it is possible to get some
of the benefits of lemmatization without using any
morphological knowledge at all. For both dev and
test sets, truncating words to 6 characters (the best
length on the dev set) provided a significant im-
provement over word-to-word translation, but was
also significantly worse than the best lemmatization
scores. Changing the frequency cutoff for trunca-
tion did not produce any significant differences in
the BLEU score.
4.2 Pseudowords
Results for the pseudoword experiments on the devel-
opment set are shown in the first column of Table 2.
Note that in these (and the following) experiments,
we treated all words the same way regardless of their
frequency, so the effects of adding morphological in-
formation are in comparison to the full lemmatiza-
tion scheme. In most of our experiments, we added
morphological information for only a single class of
tags at a time in order to determine the effects of
each class individually. The classes we used were
verb person (PER), verb tense (TEN), noun number
(NUM), noun case (CASE), and negation (NEG).
Most of the results of the pseudoword experiments
confirm our expectations. Adding the verb person
tags was helpful, and examination of the alignments
revealed that they did indeed align to English pro-
nouns with high probability. The noun number tags
did not help, since plurality is expressed as an affix
in English. Negation tags helped slightly, though the
improvement was not significant. This is probably
because negation tags are relatively infrequent, as
can be seen in Table 3. The addition of pseudowords
for case did not yield an improvement, probably be-
cause these pseudowords were so frequent. The ad-
ditional ambiguity caused by so many extra words
likely overwhelmed any positive effect.
A somewhat puzzling result is the behavior of the
680
Tag type Pseudo Mod-Lem Morph
PER .365 .356 .356
TEN .365 .361 .364
PER,TEN .355 .362 .355
NUM .354 .367 .361
CASE .353 .340 .337
NEG .357 .356 .353
Table 2: BLEU scores indicating the results of in-
corporating the information from different classes
of morphological tags in the the experiments us-
ing pseudowords (Pseudo), modified lemmas (Mod-
Lem), and morphemes (Morph). Scores are from the
development set. Differences of .009 are significant
(p < .05).
Tag class Count Avg/sentence
PER 49700 2.35
TEN 47744 2.26
past 22544 1.07
pres 20291 0.96
fut 1707 0.08
?any? 3202 0.15
NUM 151646 7.17
CASE 151646 7.17
NEG 3326 0.16
Table 3: Number of occurrences of each class of tags
in the Czech training data.
verb tense tags. With the exception of future tense,
English generally does not mark tense with an aux-
iliary. Yet Table 3 shows that only a very small per-
centage of sentences have a future tense marker, so
it seems unlikely that this explains the positive ef-
fects of the tense pseudowords. In fact, we tried
adding only future tense pseudowords to the lem-
matized Czech data, and found that the results were
no better than basic lemmatization.
The other unusual behavior we see with pseu-
dowords is that when verb person and tense tags are
combined, they seem to cancel each other out, result-
ing in a score that is no better than lemmatization
alone. Examination of the alignments did not reveal
any obvious reason for this effect.
4.3 Modified Lemmas
As shown in the second column of Table 2, the num-
ber and tense tags yield an improvement under the
modified lemma transformation, while the person
tags do not. Again, this confirms our predictions
based on the morphology of English.
Our results using the case tags under this model
actually decreased performance, but this is not
surprising given that differentiating Czech lemmas
based on case marking creates as much as a 7-way
split of the data (there are seven cases in Czech),
without adding much information that would be use-
ful in English.
4.4 Morphemes
BLEU scores for the morpheme-based model are
given in the third column of Table 2. None of the
differences in scores between this model and the mod-
ified lemma model are significant, although the trend
for most of the tag classes is for this model to per-
form slightly worse. This suggests that the type of
smoothing induced by the morpheme-based model
may not be as helpful as simply attempting to cre-
ate Czech words that reflect the same morphological
distinctions as the English words. In Section 5, we
propose a generalized version of the morpheme model
that might be an improvement.
4.5 Combined Model
In the experiments described so far, we used only
a single method at a time of incorporating mor-
phological information into the translation process.
However, it is straightforward to combine the pseu-
doword method with either the modified-lemma or
morpheme-based methods by using pseudowords for
certain tags and attaching others to the Czech lem-
mas. The experiments described above allowed us to
confirm our intuitions about how each class of tags
should be treated under such a combined model. We
then created a model using the pseudoword treat-
ment of the person and negation tags, and the mod-
ified lemma treatment of number and tense. We did
not use the case tags in this model, since they did
not seem to yield an improvement in any of the three
basic morphological models.
Our combined model achieved a BLEU score of
.390 (development) and .333 (test), outperforming
the models in all of our previous experiments.
5 Discussion
The results of our experiments provide additional
support for the findings of previous researchers that
using morphological analysis can improve the quality
of statistical machine translation for highly-inflected
languages. While human judgment is probably the
best metric for evaluating translation quality, our use
of the automatically-derived BLEU score allowed us
to easily compare many different translation models
and evaluate the effects of each one individually. We
found that simple lemmatization, by significantly re-
ducing the sparse data problem, was quite effective
681
despite the loss of information involved. Lemmatiz-
ing the less frequent words in the corpus seemed to
increase performance slightly, but these results were
inconclusive. Word truncation, which requires no
morphological information at all, was effective at in-
creasing scores over the word-to-word baseline, but
did not perform quite as well as lemmatization. This
result conflicts with Och?s (Och, 1995), and is likely
due to the much smaller size of our corpus. In any
case, our results suggest that lemmatization or word
truncation could yield a significant improvement in
the quality of translation from a highly-inflected to
a less-inflected language, even when limited morpho-
logical information is available.
Our primary results concern the use of full mor-
phological information. We found that certain tags
were more useful when we treated them as discrete
input words, while others provided a greater benefit
when attached directly to their lemmas. The best
choice of which method to use for each class of tags
seems to correspond closely with how that class of in-
formation is expressed in English (either using func-
tion words or inflection). In a sense, the goal of the
morphological analysis is to make the Czech input
data more English-like by suppressing unnecessary
morphological distinctions and expressing necessary
distinctions in ways that are similar to English. This
sort of procedure could be taken further by incorpo-
rating syntactic information as well, but as we stated
earlier, our goal was to determine exactly how much
benefit we could derive from a strictly morphological
approach.
In the work we have presented, the output lan-
guage (English) is low in inflection. We therefore
considered it less important to perform morphologi-
cal analysis on the English data. However, we expect
that the work described here could be generalized to
highly inflected output languages by doing morpho-
logical analysis on both the input and output lan-
guages. The most promising way to do this seems
to be by extending the morpheme-based translation
model in Equation 2 to incorporate morphemes in
both languages, so that
P(fj|ei) =
K
?
k=0
P(fjk|eik) (3)
where fjk are the morphemes in the input language,
and eik are the corresponding morphemes in the out-
put language. This extended model may also prove
a benefit to Czech-English translation; we are cur-
rently investigating this possibility.
In this work, we used a word-based translation sys-
tem due to the availability of source code that could
be modified for our morph experiments. An obvious
extension to the current work would be to move to a
phrase-based translation system. One advantage of
phrase-based models is their ability to align phrases
in one language to morphologically complex words in
the other language. However, this feature still suffers
from the same sparse data problems as a word-based
system: if a morphologically complex word only ap-
pears a handful of times in the training corpus, the
system will have difficulty determining its (phrasal
or word) alignment. We expect that morphological
analysis would still be helpful in this situation, at the
very least because it can be used to remove distinc-
tions that appear in only one language.
6 Conclusion
In this paper we used morphological analysis of
Czech to improve a Czech-English statistical machine
translation system. We have argued that this im-
provement was primarily due to a reduction of the
sparse data problem caused by the highly inflected
nature of Czech. An alternative method for reducing
sparse data is to use a larger parallel corpus; however,
it is often easier to obtain additional monolingual re-
sources, such as a morphological analyzer or tagged
corpus, than additional parallel data for a specific
language pair. For that reason, we believe that the
approach taken here is a promising one.
We have described several different ways of using
morphological information for machine translation,
and have shown how these can be combined to yield
an improved translation model. In general, we would
not expect the exact combination of techniques that
yielded our best results for Czech-English to be op-
timal for other language pairs. Rather, we have sug-
gested that these techniques should be combined in
a way that makes the input language more similar
to the output language. Although this combination
will need to be determined for each language pair, the
general approach outlined here should provide ben-
efits for any MT system involving a highly inflected
language.
Acknowledgements
We would like to thank Eugene Charniak and the
members of BLLIP for their encouragement and
helpful suggestions. This research was partially sup-
ported by NSF awards IGERT 9870676 and ITR
0085940.
682
References
Y. Al-Onaizan, J. Cur?in, M. Jahr, K. Knight, J. Laf-
ferty, D. Melamed, F. Och, D. Purdy, N. Smith,
and D. Yarowsky. 1999. Statistical machine trans-
lation. Final Report, JHU Summer Workshop
1999.
P. Clarkson and R. Rosenfeld. 1997. Sta-
tistical language modeling using the CMU-
Cambridge Toolkit. In Proceedings of ESCA
Eurospeech. Current version available at
http://mi.eng.cam.ac.uk/?prc14/toolkit.html.
J. Hajic?. 1998. Building a Syntactically Anno-
tated Corpus: The Prague Dependency Treebank.
In Eva Hajic?ova?, editor, Issues of Valency and
Meaning. Studies in Honor of Jarmila Panevova?,
pages 12?19. Prague Karolinum, Charles Univer-
sity Press.
Y. Lee. 2004. Morphological analysis for statistical
machine translation. In Proceedings NAACL.
D. Marcu and U. Germann. 2005. The
ISI ReWrite Decoder 1.0.0a. Available at
http://www.isi.edu/licensed-sw/rewrite-decoder/.
S. Niessen and H. Ney. 2000. Improving SMT quality
with morpho-syntactic analysis. In Proceedings of
COLING.
S. Niessen and H. Ney. 2004. Statistical machine
translation with scarce resources using morpho-
syntactic analysis. Computational Linguistics,
30(2):181?204.
F. J. Och and H. Ney. 2003. A systematic compari-
son of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51.
F. Och. 1995. Statistical machine translation: The
fabulous present and future. Invited talk at the
Workshop on Building and Using Parallel Texts
at ACL?05.
M. C?mejrek, J. Cur???n, and J. Havelka. 2003. Czech-
english dependency-based machine translation. In
Proceedings of EACL.
M. C?mejrek, J. Cur???n, J. Havelka, J. Hajic?, and
V. Kubon?. 2004. Prague czech-english dependecy
treebank: Syntactically annotated resources for
machine translation. In 4th International Confer-
ence on Language Resources and Evaluation, Lis-
bon, Portugal.
683
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 673?680,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Contextual Dependencies in Unsupervised Word Segmentation?
Sharon Goldwater and Thomas L. Griffiths and Mark Johnson
Department of Cognitive and Linguistic Sciences
Brown University
Providence, RI 02912
{Sharon Goldwater,Tom Griffiths,Mark Johnson}@brown.edu
Abstract
Developing better methods for segment-
ing continuous text into words is impor-
tant for improving the processing of Asian
languages, and may shed light on how hu-
mans learn to segment speech. We pro-
pose two new Bayesian word segmenta-
tion methods that assume unigram and bi-
gram models of word dependencies re-
spectively. The bigram model greatly out-
performs the unigram model (and previous
probabilistic models), demonstrating the
importance of such dependencies for word
segmentation. We also show that previous
probabilistic models rely crucially on sub-
optimal search procedures.
1 Introduction
Word segmentation, i.e., discovering word bound-
aries in continuous text or speech, is of interest for
both practical and theoretical reasons. It is the first
step of processing orthographies without explicit
word boundaries, such as Chinese. It is also one
of the key problems that human language learners
must solve as they are learning language.
Many previous methods for unsupervised word
segmentation are based on the observation that
transitions between units (characters, phonemes,
or syllables) within words are generally more pre-
dictable than transitions across word boundaries.
Statistics that have been proposed for measuring
these differences include ?successor frequency?
(Harris, 1954), ?transitional probabilities? (Saf-
fran et al, 1996), mutual information (Sun et al,
?This work was partially supported by the following
grants: NIH 1R01-MH60922, NIH RO1-DC000314, NSF
IGERT-DGE-9870676, and the DARPA CALO project.
1998), ?accessor variety? (Feng et al, 2004), and
boundary entropy (Cohen and Adams, 2001).
While methods based on local statistics are
quite successful, here we focus on approaches
based on explicit probabilistic models. Formulat-
ing an explicit probabilistic model permits us to
cleanly separate assumptions about the input and
properties of likely segmentations from details of
algorithms used to find such solutions. Specifi-
cally, this paper demonstrates the importance of
contextual dependencies for word segmentation
by comparing two probabilistic models that dif-
fer only in that the first assumes that the proba-
bility of a word is independent of its local context,
while the second incorporates bigram dependen-
cies between adjacent words. The algorithms we
use to search for likely segmentations do differ,
but so long as the segmentations they produce are
close to optimal we can be confident that any dif-
ferences in the segmentations reflect differences in
the probabilistic models, i.e., in the kinds of de-
pendencies between words.
We are not the first to propose explicit prob-
abilistic models of word segmentation. Two
successful word segmentation systems based on
explicit probabilistic models are those of Brent
(1999) and Venkataraman (2001). Brent?s Model-
Based Dynamic Programming (MBDP) system as-
sumes a unigram word distribution. Venkatara-
man uses standard unigram, bigram, and trigram
language models in three versions of his system,
which we refer to as n-gram Segmentation (NGS).
Despite their rather different generative structure,
the MBDP and NGS segmentation accuracies are
very similar. Moreover, the segmentation accuracy
of the NGS unigram, bigram, and trigram mod-
els hardly differ, suggesting that contextual depen-
dencies are irrelevant to word segmentation. How-
673
ever, the segmentations produced by both these
methods depend crucially on properties of the
search procedures they employ. We show this by
exhibiting for each model a segmentation that is
less accurate but more probable under that model.
In this paper, we present an alternative frame-
work for word segmentation based on the Dirich-
let process, a distribution used in nonparametric
Bayesian statistics. This framework allows us to
develop extensible models that are amenable to
standard inference procedures. We present two
such models incorporating unigram and bigram
word dependencies, respectively. We use Gibbs
sampling to sample from the posterior distribution
of possible segmentations under these models.
The plan of the paper is as follows. In the next
section, we describe MBDP and NGS in detail. In
Section 3 we present the unigram version of our
own model, the Gibbs sampling procedure we use
for inference, and experimental results. Section 4
extends that model to incorporate bigram depen-
dencies, and Section 5 concludes the paper.
2 NGS and MBDP
The NGS and MBDP systems are similar in some
ways: both are designed to find utterance bound-
aries in a corpus of phonemically transcribed ut-
terances, with known utterance boundaries. Both
also use approximate online search procedures,
choosing and fixing a segmentation for each utter-
ance before moving onto the next. In this section,
we focus on the very different probabilistic mod-
els underlying the two systems. We show that the
optimal solution under the NGS model is the un-
segmented corpus, and suggest that this problem
stems from the fact that the model assumes a uni-
form prior over hypotheses. We then present the
MBDP model, which uses a non-uniform prior but
is difficult to extend beyond the unigram case.
2.1 NGS
NGS assumes that each utterance is generated in-
dependently via a standard n-gram model. For
simplicity, we will discuss the unigram version of
the model here, although our argument is equally
applicable to the bigram and trigram versions. The
unigram model generates an utterance u according
to the grammar in Figure 1, so
P (u) = p$(1? p$)n?1
n
?
j=1
P (wj) (1)
1? p$ U?W U
p$ U?W
P (w) W?w ?w ? ??
Figure 1: The unigram NGS grammar.
where u consists of the words w1 . . . wn and p$ is
the probability of the utterance boundary marker
$. This model can be used to find the highest prob-
ability segmentation hypothesis h given the data d
by using Bayes? rule:
P (h|d) ? P (d|h)P (h)
NGS assumes a uniform prior P (h) over hypothe-
ses, so its goal is to find the solution that maxi-
mizes the likelihood P (d|h).
Using this model, NGS?s approximate search
technique delivers competitive results. However,
the true maximum likelihood solution is not com-
petitive, since it contains no utterance-internal
word boundaries. To see why not, consider the
solution in which p$ = 1 and each utterance is a
single ?word?, with probability equal to the empir-
ical probability of that utterance. Any other so-
lution will match the empirical distribution of the
data less well. In particular, a solution with ad-
ditional word boundaries must have 1 ? p$ > 0,
which means it wastes probability mass modeling
unseen data (which can now be generated by con-
catenating observed utterances together).
Intuitively, the NGS model considers the unseg-
mented solution to be optimal because it ranks all
hypotheses equally probable a priori. We know,
however, that hypotheses that memorize the input
data are unlikely to generalize to unseen data, and
are therefore poor solutions. To prevent memo-
rization, we could restrict our hypothesis space to
models with fewer parameters than the number of
utterances in the data. A more general and mathe-
matically satisfactory solution is to assume a non-
uniform prior, assigning higher probability to hy-
potheses with fewer parameters. This is in fact the
route taken by Brent in his MBDP model, as we
shall see in the following section.
2.2 MBDP
MBDP assumes a corpus of utterances is gener-
ated as a single probabilistic event with four steps:
1. Generate L, the number of lexical types.
2. Generate a phonemic representation for each
type (except the utterance boundary type, $).
674
3. Generate a token frequency for each type.
4. Generate an ordering for the set of tokens.
In a final deterministic step, the ordered tokens
are concatenated to create an unsegmented cor-
pus. This means that certain segmented corpora
will produce the observed data with probability 1,
and all others will produce it with probability 0.
The posterior probability of a segmentation given
the data is thus proportional to its prior probability
under the generative model, and the best segmen-
tation is that with the highest prior probability.
There are two important points to note about
the MBDP model. First, the distribution over L
assigns higher probability to models with fewer
lexical items. We have argued that this is neces-
sary to avoid memorization, and indeed the unseg-
mented corpus is not the optimal solution under
this model, as we will show in Section 3. Second,
the factorization into four separate steps makes
it theoretically possible to modify each step in-
dependently in order to investigate the effects of
the various modeling assumptions. However, the
mathematical statement of the model and the ap-
proximations necessary for the search procedure
make it unclear how to modify the model in any
interesting way. In particular, the fourth step uses
a uniform distribution, which creates a unigram
constraint that cannot easily be changed. Since our
research aims to investigate the effects of different
modeling assumptions on lexical acquisition, we
develop in the following sections a far more flex-
ible model that also incorporates a preference for
sparse solutions.
3 Unigram Model
3.1 The Dirichlet Process Model
Our goal is a model of language that prefers
sparse solutions, allows independent modification
of components, and is amenable to standard search
procedures. We achieve this goal by basing our
model on the Dirichlet process (DP), a distribution
used in nonparametric Bayesian statistics. Our un-
igram model of word frequencies is defined as
wi|G ? G
G|?0, P0 ? DP(?0, P0)
where the concentration parameter ?0 and the
base distribution P0 are parameters of the model.
Each word wi in the corpus is drawn from a
distribution G, which consists of a set of pos-
sible words (the lexicon) and probabilities asso-
ciated with those words. G is generated from
a DP(?0, P0) distribution, with the items in the
lexicon being sampled from P0 and their proba-
bilities being determined by ?0, which acts like
the parameter of an infinite-dimensional symmet-
ric Dirichlet distribution. We provide some intu-
ition for the roles of ?0 and P0 below.
Although the DP model makes the distribution
G explicit, we never deal with G directly. We
take a Bayesian approach and integrate over all
possible values of G. The conditional probabil-
ity of choosing to generate a word from a particu-
lar lexical entry is then given by a simple stochas-
tic process known as the Chinese restaurant pro-
cess (CRP) (Aldous, 1985). Imagine a restaurant
with an infinite number of tables, each with infinite
seating capacity. Customers enter the restaurant
and seat themselves. Let zi be the table chosen by
the ith customer. Then
P (zi|z?i) =
?
?
?
n(z?i)k
i?1+?0 0 ? k < K(z?i)
?0
i?1+?0 k = K(z?i)
(2)
where z?i = z1 . . . zi?1, n(z?i)k is the number of
customers already sitting at table k, and K(z?i) is
the total number of occupied tables. In our model,
the tables correspond to (possibly repeated) lexical
entries, having labels generated from the distribu-
tion P0. The seating arrangement thus specifies
a distribution over word tokens, with each cus-
tomer representing one token. This model is an
instance of the two-stage modeling framework de-
scribed by Goldwater et al (2006), with P0 as the
generator and the CRP as the adaptor.
Our model can be viewed intuitively as a cache
model: each word in the corpus is either retrieved
from a cache or generated anew. Summing over
all the tables labeled with the same word yields
the probability distribution for the ith word given
previously observed words w?i:
P (wi|w?i) =
n(w?i)wi
i? 1 + ?0
+ ?0P0(wi)i? 1 + ?0
(3)
where n(w?i)w is the number of instances of w ob-
served in w?i. The first term is the probability
of generating w from the cache (i.e., sitting at an
occupied table), and the second term is the proba-
bility of generating it anew (sitting at an unoccu-
pied table). The actual table assignments z?i only
become important later, in the bigram model.
675
There are several important points to note about
this model. First, the probability of generating a
particular word from the cache increases as more
instances of that word are observed. This rich-
get-richer process creates a power-law distribution
on word frequencies (Goldwater et al, 2006), the
same sort of distribution found empirically in nat-
ural language. Second, the parameter ?0 can be
used to control how sparse the solutions found by
the model are. This parameter determines the total
probability of generating any novel word, a proba-
bility that decreases as more data is observed, but
never disappears. Finally, the parameter P0 can
be used to encode expectations about the nature
of the lexicon, since it defines a probability distri-
bution across different novel words. The fact that
this distribution is defined separately from the dis-
tribution on word frequencies gives the model ad-
ditional flexibility, since either distribution can be
modified independently of the other.
Since the goal of this paper is to investigate the
role of context in word segmentation, we chose
the simplest possible model for P0, i.e. a unigram
phoneme distribution:
P0(w) = p#(1? p#)n?1
n
?
i=1
P (mi) (4)
where word w consists of the phonemes
m1 . . . mn, and p# is the probability of the
word boundary #. For simplicity we used
a uniform distribution over phonemes, and
experimented with different fixed values of p#.1
A final detail of our model is the distribution
on utterance lengths, which is geometric. That is,
we assume a grammar similar to the one shown in
Figure 1, with the addition of a symmetric Beta( ?2 )
prior over the probability of the U productions,2
and the substitution of the DP for the standard
multinomial distribution over the W productions.
3.2 Gibbs Sampling
Having defined our generative model, we are left
with the problem of inference: we must determine
the posterior distribution of hypotheses given our
input corpus. To do so, we use Gibbs sampling,
a standard Markov chain Monte Carlo method
(Gilks et al, 1996). Gibbs sampling is an itera-
tive procedure in which variables are repeatedly
1Note, however, that our model could be extended to learn
both p# and the distribution over phonemes.
2The Beta distribution is a Dirichlet distribution over two
outcomes.
W
U
w1 = w2.w3
UW
U
W
w3
w2
h1: h2:
Figure 2: The two hypotheses considered by the
unigram sampler. Dashed lines indicate possible
additional structure. All rules except those in bold
are part of h?.
sampled from their conditional posterior distribu-
tion given the current values of all other variables
in the model. The sampler defines a Markov chain
whose stationary distribution is P (h|d), so after
convergence samples are from this distribution.
Our Gibbs sampler considers a single possible
boundary point at a time, so each sample is from
a set of two hypotheses, h1 and h2. These hy-
potheses contain all the same boundaries except
at the one position under consideration, where h2
has a boundary and h1 does not. The structures are
shown in Figure 2. In order to sample a hypothe-
sis, we need only calculate the relative probabili-
ties of h1 and h2. Since h1 and h2 are the same ex-
cept for a few rules, this is straightforward. Let h?
be all of the structure shared by the two hypothe-
ses, including n? words, and let d be the observed
data. Then
P (h1|h?, d) = P (w1|h?, d)
= n
(h?)
w1 + ?0P0(w1)
n? + ?0
(5)
where the second line follows from Equation 3
and the properties of the CRP (in particular, that it
is exchangeable, with the probability of a seating
configuration not depending on the order in which
customers arrive (Aldous, 1985)). Also,
P (h2|h?, d)
= P (r, w2, w3|h?, d)
= P (r|h?, d)P (w2|h?, d)P (w3|w2, h?, d)
= nr +
?
2
n? + 1 + ? ?
n(h
?)
w2 + ?0P0(w2)
n? + ?0
?n
(h?)
w3 + I(w2 = w3) + ?0P0(w3)
n? + 1 + ?0
(6)
where nr is the number of branching rules r =
U ? W U in h?, and I(.) is an indicator func-
tion taking on the value 1 when its argument is
676
true, and 0 otherwise. The nr term is derived by
integrating over all possible values of p$, and not-
ing that the total number of U productions in h?
is n? + 1.
Using these equations we can simply proceed
through the data, sampling each potential bound-
ary point in turn. Once the Gibbs sampler con-
verges, these samples will be drawn from the pos-
terior distribution P (h|d).
3.3 Experiments
In our experiments, we used the same corpus
that NGS and MBDP were tested on. The cor-
pus, supplied to us by Brent, consists of 9790
transcribed utterances (33399 words) of child-
directed speech from the Bernstein-Ratner cor-
pus (Bernstein-Ratner, 1987) in the CHILDES
database (MacWhinney and Snow, 1985). The ut-
terances have been converted to a phonemic rep-
resentation using a phonemic dictionary, so that
each occurrence of a word has the same phonemic
transcription. Utterance boundaries are given in
the input to the system; other word boundaries are
not.
Because our Gibbs sampler is slow to converge,
we used annealing to speed inference. We began
with a temperature of ? = 10 and decreased ? in
10 increments to a final value of 1. A temperature
of ? corresponds to raising the probabilities of h1
and h2 to the power of 1? prior to sampling.
We ran our Gibbs sampler for 20,000 iterations
through the corpus (with ? = 1 for the final 2000)
and evaluated our results on a single sample at
that point. We calculated precision (P), recall (R),
and F-score (F) on the word tokens in the corpus,
where both boundaries of a word must be correct
to count the word as correct. The induced lexicon
was also scored for accuracy using these metrics
(LP, LR, LF).
Recall that our DP model has three parameters:
?, p#, and ?0. Given the large number of known
utterance boundaries, we expect the value of ? to
have little effect on our results, so we simply fixed
? = 2 for all experiments. Figure 3 shows the ef-
fects of varying of p# and ?0.3 Lower values of
p# cause longer words, which tends to improve re-
call (and thus F-score) in the lexicon, but decrease
token accuracy. Higher values of ?0 allow more
novel words, which also improves lexicon recall,
3It is worth noting that all these parameters could be in-
ferred. We leave this for future work.
0.1 0.3 0.5 0.7 0.9
50
55
60
(a) Varying P(#)
 
 
1 2 5 10 20 50 100 200 500
50
55
60
(b) Varying ?0
 
 
LF
F
LF
F
Figure 3: Word (F) and lexicon (LF) F-score (a)
as a function of p#, with ?0 = 20 and (b) as a
function of ?0, with p# = .5.
but begins to degrade precision after a point. Due
to the negative correlation between token accuracy
and lexicon accuracy, there is no single best value
for either p# or ?0; further discussion refers to the
solution for p# = .5, ?0 = 20 (though others are
qualitatively similar).
In Table 1(a), we compare the results of our sys-
tem to those of MBDP and NGS.4 Although our
system has higher lexicon accuracy than the oth-
ers, its token accuracy is much worse. This result
occurs because our system often mis-analyzes fre-
quently occurring words. In particular, many of
these words occur in common collocations such
as what?s that and do you, which the system inter-
prets as a single words. It turns out that a full 31%
of the proposed lexicon and nearly 30% of tokens
consist of these kinds of errors.
Upon reflection, it is not surprising that a uni-
gram language model would segment words in this
way. Collocations violate the unigram assumption
in the model, since they exhibit strong word-to-
word dependencies. The only way the model can
capture these dependencies is by assuming that
these collocations are in fact words themselves.
Why don?t the MBDP and NGS unigram mod-
els exhibit these problems? We have already
shown that NGS?s results are due to its search pro-
cedure rather than its model. The same turns out
to be true for MBDP. Table 2 shows the probabili-
4We used the implementations of MBDP and NGS avail-
able at http://www.speech.sri.com/people/anand/ to obtain re-
sults for those systems.
677
(a) P R F LP LR LF
NGS 67.7 70.2 68.9 52.9 51.3 52.0
MBDP 67.0 69.4 68.2 53.6 51.3 52.4
DP 61.9 47.6 53.8 57.0 57.5 57.2
(b) P R F LP LR LF
NGS 76.6 85.8 81.0 60.0 52.4 55.9
MBDP 77.0 86.1 81.3 60.8 53.0 56.6
DP 94.2 97.1 95.6 86.5 62.2 72.4
Table 1: Accuracy of the various systems, with
best scores in bold. The unigram version of NGS
is shown. DP results are with p# = .5 and ?0 =
20. (a) Results on the true corpus. (b) Results on
the permuted corpus.
Seg: True None MBDP NGS DP
NGS 204.5 90.9 210.7 210.8 183.0
MBDP 208.2 321.7 217.0 218.0 189.8
DP 222.4 393.6 231.2 231.6 200.6
Table 2: Negative log probabilities (x 1000) un-
der each model of the true solution, the solution
with no utterance-internal boundaries, and the so-
lutions found by each algorithm. Best solutions
under each model are bold.
ties under each model of various segmentations of
the corpus. From these figures, we can see that
the MBDP model assigns higher probability to the
solution found by our Gibbs sampler than to the
solution found by Brent?s own incremental search
algorithm. In other words, Brent?s model does pre-
fer the lower-accuracy collocation solution, but his
search algorithm instead finds a higher-accuracy
but lower-probability solution.
We performed two experiments suggesting that
our own inference procedure does not suffer from
similar problems. First, we initialized our Gibbs
sampler in three different ways: with no utterance-
internal boundaries, with a boundary after every
character, and with random boundaries. Our re-
sults were virtually the same regardless of initial-
ization. Second, we created an artificial corpus by
randomly permuting the words in the true corpus,
leaving the utterance lengths the same. The ar-
tificial corpus adheres to the unigram assumption
of our model, so if our inference procedure works
correctly, we should be able to correctly identify
the words in the permuted corpus. This is exactly
what we found, as shown in Table 1(b). While all
three models perform better on the artificial cor-
pus, the improvements of the DP model are by far
the most striking.
4 Bigram Model
4.1 The Hierarchical Dirichlet Process Model
The results of our unigram experiments suggested
that word segmentation could be improved by
taking into account dependencies between words.
To test this hypothesis, we extended our model
to incorporate bigram dependencies using a hi-
erarchical Dirichlet process (HDP) (Teh et al,
2005). Our approach is similar to previous n-gram
models using hierarchical Pitman-Yor processes
(Goldwater et al, 2006; Teh, 2006). The HDP is
appropriate for situations in which there are multi-
ple distributions over similar sets of outcomes, and
the distributions are believed to be similar. In our
case, we define a bigram model by assuming each
word has a different distribution over the words
that follow it, but all these distributions are linked.
The definition of our bigram language model as an
HDP is
wi|wi?1 = w,Hw ? Hw ?w
Hw|?1, G ? DP(?1, G) ?w
G|?0, P0 ? DP(?0, P0)
That is, P (wi|wi?1 = w) is distributed accord-
ing to Hw, a DP specific to word w. Hw is linked
to the DPs for all other words by the fact that they
share a common base distribution G, which is gen-
erated from another DP.5
As in the unigram model, we never deal with
Hw or G directly. By integrating over them, we get
a distribution over bigram frequencies that can be
understood in terms of the CRP. Now, each word
type w is associated with its own restaurant, which
represents the distribution over words that follow
w. Different restaurants are not completely inde-
pendent, however: the labels on the tables in the
restaurants are all chosen from a common base
distribution, which is another CRP.
To understand the HDP model in terms of a
grammar, we consider $ as a special word type,
so that wi ranges over ?? ? {$}. After observing
w?i, the HDP grammar is as shown in Figure 4,
5This HDP formulation is an oversimplification, since it
does not account for utterance boundaries properly. The
grammar formulation (see below) does.
678
P2(wi|w?i, z?i) Uwi?1?Wwi Uwi ?wi ? ??,
wi?1 ? ???{$}
P2($|w?i, z?i) Uwi?1?$ ?wi?1 ? ??
1 Wwi ?wi ?wi ? ??
Figure 4: The HDP grammar after observing w?i.
with
P2(wi|h?i) =
n(wi?1,wi) + ?1P1(wi|h?i)
nwi?1 + ?1
(7)
P1(wi|h?i) =
?
?
?
t??+ ?2
t+? ?
twi+?0P0(wi)
t??+?0 wi ? ?
?
t$+ ?2
t+? wi = $
where h?i = (w?i, z?i); t$, t?? , and twi are the
total number of tables (across all words) labeled
with $, non-$, and wi, respectively; t = t$ + t??
is the total number of tables; and n(wi?1,wi) is the
number of occurrences of the bigram (wi?1, wi).
We have suppressed the superscript (w?i) nota-
tion in all cases. The base distribution shared by
all bigrams is given by P1, which can be viewed as
a unigram backoff where the unigram probabilities
are learned from the bigram table labels.
We can perform inference on this HDP bigram
model using a Gibbs sampler similar to our uni-
gram sampler. Details appear in the Appendix.
4.2 Experiments
We used the same basic setup for our experiments
with the HDP model as we used for the DP model.
We experimented with different values of ?0 and
?1, keeping p# = .5 throughout. Some results
of these experiments are plotted in Figure 5. With
appropriate parameter settings, both lexicon and
token accuracy are higher than in the unigram
model (dramatically so, for tokens), and there is
no longer a negative correlation between the two.
Only a few collocations remain in the lexicon, and
most lexicon errors are on low-frequency words.
The best values of ?0 are much larger than in the
unigram model, presumably because all unique
word types must be generated via P0, but in the
bigram model there is an additional level of dis-
counting (the unigram process) before reaching
P0. Smaller values of ?0 lead to fewer word types
with fewer characters on average.
Table 3 compares the optimal results of the
HDP model to the only previous model incorpo-
rating bigram dependencies, NGS. Due to search,
the performance of the bigram NGS model is not
much different from that of the unigram model. In
100 200 500 1000 2000
40
60
80
(a) Varying ?0
 
 
F
LF
5 10 20 50 100 200 500
40
60
80
(b) Varying ?1
 
 
F
LF
Figure 5: Word (F) and lexicon (LF) F-score (a)
as a function of ?0, with ?1 = 10 and (b) as a
function of ?1, with ?0 = 1000.
P R F LP LR LF
NGS 68.1 68.6 68.3 54.5 57.0 55.7
HDP 79.4 74.0 76.6 67.9 58.9 63.1
Table 3: Bigram system accuracy, with best scores
in bold. HDP results are with p# = .5, ?0 =
1000, and ?1 = 10.
contrast, our HDP model performs far better than
our DP model, leading to the highest published ac-
curacy for this corpus on both tokens and lexical
items. Overall, these results strongly support our
hypothesis that modeling bigram dependencies is
important for accurate word segmentation.
5 Conclusion
In this paper, we have introduced a new model-
based approach to word segmentation that draws
on techniques from Bayesian statistics, and we
have developed models incorporating unigram and
bigram dependencies. The use of the Dirichlet
process as the basis of our approach yields sparse
solutions and allows us the flexibility to modify
individual components of the models. We have
presented a method of inference using Gibbs sam-
pling, which is guaranteed to converge to the pos-
terior distribution over possible segmentations of
a corpus.
Our approach to word segmentation allows us to
investigate questions that could not be addressed
satisfactorily in earlier work. We have shown that
the search algorithms used with previous models
of word segmentation do not achieve their ob-
679
P (h1|h?, d) =
n(wl,w1) + ?1P1(w1|h?, d)
nwl + ?1
?
n(w1,wr) + I(wl =w1 =wr) + ?1P1(wr|h?, d)
nw1 + 1 + ?1
P (h2|h?, d) =
n(wl,w2) + ?1P1(w2|h?, d)
nwl + ?1
?
n(w2,w3) + I(wl =w2 =w3) + ?1P1(w3|h?, d)
nw2 + 1 + ?1
?
n(w3,wr) + I(wl =w3, w2 =wr) + I(w2 =w3 =wr) + ?1P1(wr|h?, d)
nw3 + 1 + I(w2 =w4) + ?1
Figure 6: Gibbs sampling equations for the bigram model. All counts are with respect to h?.
jectives, which has led to misleading results. In
particular, previous work suggested that the use
of word-to-word dependencies has little effect on
word segmentation. Our experiments indicate in-
stead that bigram dependencies can be crucial for
avoiding under-segmentation of frequent colloca-
tions. Incorporating these dependencies into our
model greatly improved segmentation accuracy,
and led to better performance than previous ap-
proaches on all measures.
References
D. Aldous. 1985. Exchangeability and related topics. In
?Ecole d?e?te? de probabilite?s de Saint-Flour, XIII?1983,
pages 1?198. Springer, Berlin.
C. Antoniak. 1974. Mixtures of Dirichlet processes with ap-
plications to Bayesian nonparametric problems. The An-
nals of Statistics, 2:1152?1174.
N. Bernstein-Ratner. 1987. The phonology of parent-child
speech. In K. Nelson and A. van Kleeck, editors, Chil-
dren?s Language, volume 6. Erlbaum, Hillsdale, NJ.
M. Brent. 1999. An efficient, probabilistically sound al-
gorithm for segmentation and word discovery. Machine
Learning, 34:71?105.
P. Cohen and N. Adams. 2001. An algorithm for segment-
ing categorical timeseries into meaningful episodes. In
Proceedings of the Fourth Symposium on Intelligent Data
Analysis.
H. Feng, K. Chen, X. Deng, and W. Zheng. 2004. Acces-
sor variety criteria for chinese word extraction. Computa-
tional Lingustics, 30(1).
W.R. Gilks, S. Richardson, and D. J. Spiegelhalter, editors.
1996. Markov Chain Monte Carlo in Practice. Chapman
and Hall, Suffolk.
S. Goldwater, T. Griffiths, and M. Johnson. 2006. Interpo-
lating between types and tokens by estimating power-law
generators. In Advances in Neural Information Process-
ing Systems 18, Cambridge, MA. MIT Press.
Z. Harris. 1954. Distributional structure. Word, 10:146?162.
B. MacWhinney and C. Snow. 1985. The child language data
exchange system. Journal of Child Language, 12:271?
296.
J. Saffran, E. Newport, and R. Aslin. 1996. Word segmenta-
tion: The role of distributional cues. Journal of Memory
and Language, 35:606?621.
M. Sun, D. Shen, and B. Tsou. 1998. Chinese word seg-
mentation without using lexicon and hand-crafted training
data. In Proceedings of COLING-ACL.
Y. Teh, M. Jordan, M. Beal, and D. Blei. 2005. Hierarchical
Dirichlet processes. In Advances in Neural Information
Processing Systems 17. MIT Press, Cambridge, MA.
Y. Teh. 2006. A Bayesian interpretation of interpolated
kneser-ney. Technical Report TRA2/06, National Univer-
sity of Singapore, School of Computing.
A. Venkataraman. 2001. A statistical model for word dis-
covery in transcribed speech. Computational Linguistics,
27(3):351?372.
Appendix
To sample from the posterior distribution over seg-
mentations in the bigram model, we define h1 and
h2 as we did in the unigram sampler so that for the
corpus substring s, h1 has a single word (s = w1)
where h2 has two (s = w2.w3). Let wl and wr be
the words (or $) preceding and following s. Then
the posterior probabilities of h1 and h2 are given
in Figure 6. P1(.) can be calculated exactly using
the equation in Section 4.1, but this requires ex-
plicitly tracking and sampling the assignment of
words to tables. For easier and more efficient im-
plementation, we use an approximation, replacing
each table count twi by its expected value E[twi ].
In a DP(?,P ), the expected number of CRP tables
for an item occurring n times is ? log n+?? (Anto-
niak, 1974), so
E[twi ] = ?1
?
j
log
n(wj ,wi) + ?1
?1
This approximation requires only the bigram
counts, which we must track anyway.
680
Priors in Bayesian Learning of Phonological Rules
Sharon Goldwater and Mark Johnson
Department of Cognitive and Linguistic Sciences
Box 1978
Brown University
Providence, RI 02912
USA
{sharon goldwater, mark johnson}@brown.edu
Abstract
This paper describes a Bayesian procedure for un-
supervised learning of phonological rules from an
unlabeled corpus of training data. Like Goldsmith?s
Linguistica program (Goldsmith, 2004b), whose
output is taken as the starting point of this proce-
dure, our learner returns a grammar that consists of
a set of signatures, each of which consists of a set
of stems and a set of suffixes. Our grammars dif-
fer from Linguistica?s in that they also contain a set
of phonological rules, specifically insertion, dele-
tion and substitution rules, which permit our gram-
mars to collapse far more words into a signature
than Linguistica can. Interestingly, the choice of
Bayesian prior turns out to be crucial for obtaining a
learner that makes linguistically appropriate gener-
alizations through a range of different sized training
corpora.
1 Introduction
Unsupervised learning presents unusual challenges
to the field of computational linguistics. In super-
vised systems, the task of learning can often be re-
stricted to finding the optimal values for the param-
eters of a pre-specified model. In contrast, an un-
supervised learning system must often propose the
structure of the model itself, as well as the values
for any parameters in that model. In general, there
is a trade-off between the structural complexity of a
model and its ability to explain a set of data. One
way to deal with this trade-off is by using Bayesian
learning techniques, where the objective function
used to evaluate the overall goodness of a system
takes the form
Pr(H)Pr(D|H)
where Pr(H) is the prior probability of the hypoth-
esized model H , and Pr(D|H) is the likelihood of
the data D given that model. In a Bayesian sys-
tem, we want to find the hypothesis H for which
Pr(H)Pr(D|H) is highest (or, equivalently, where
? log Pr(H)? log Pr(D|H) is lowest). While cal-
culating the likelihood of the data given a particu-
lar hypothesis is generally straightforward, the more
difficult question in Bayesian learning is how to de-
termine the prior probabilities of various hypothe-
ses.
In this paper, we compare the results of using
two different prior distributions for an unsupervised
learning task in the domain of morpho-phonology.
Our goal is to learn transformation rules of the form
x ? y / C , where x and y are individual charac-
ters (or the empty character ) and C is some rep-
resentation of the context licensing the transforma-
tion. Our input is an existing segmentation of words
from the Penn Treebank (Marcus et al, 93) into
stems and suffixes. This segmentation is provided
by the Linguistica morphological analyzer (Gold-
smith, 2001; Goldsmith, 2004b), itself an unsuper-
vised algorithm. Using the transformation rules we
learn, we are able to output a new segmentation that
more closely matches our linguistic intuitions.1
We are not the first to apply Bayesian learning
techniques for unsupervised learning of morphol-
ogy and phonology. Several other researchers have
also pursued these methods, usually within a Mini-
mum Description Length (MDL) framework (Ris-
sanen, 1989). In MDL approaches, ? log Pr(H)
is taken to be proportional to the length of H in
some standard encoding, and ? log Pr(D|H) is the
length of D using the encoding specified by H .
MDL-based systems have been relatively successful
for tasks including word segmentation (de Marcken,
1996; Brent and Cartwright, 1996), morphological
1Since we use ordinary text, rather than phonological tran-
scriptions, as input, the rules we learn are really spelling rules,
not phonological rules. We believe that the work discussed
here would be equally applicable, and possibly more success-
ful, with phonological transcriptions. However, since we wish
to have an entirely unsupervised system and we require a mor-
phological segmentation as input, we are currently limited by
the capabilities of Linguistica, which requires standard textual
input. For the remainder of this paper, we use ?phonology? and
?phonological rules? in a broad sense to include orthography as
well.
                                                                  Barcelona, July 2004
                                              Association for Computations Linguistics
                       ACL Special Interest Group on Computational Phonology (SIGPHON)
                                                    Proceedings of the Workshop of the
??
?
?
?
?
?
?
?
lift
jump
roll
walk
. . .
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?

?s
?ed
?ing
. . .
?
?
?
?
?
?
?
?
?
Figure 1: An example signature
segmentation (Goldsmith, 2001; Creutz and Lagus,
2002), discovery of syllabicity and sonority (Elli-
son, 1993), and learning constraints on vowel har-
mony and consonant clusters (Ellison, 1994). How-
ever, our work shows that a straightforward MDL
approach, where the prior ? log Pr(H) depends on
the length of the phonological rules and the rest of
the grammar in the obvious way, does not result in a
successful system for learning phonological rules.
We discuss why this is so, and then present sev-
eral changes that can be made to the prior in order
to learn phonological rules successfully. Our con-
clusion is that, although Bayesian techniques can
be successful for unsupervised learning of linguis-
tic information, careful choice of the prior, with at-
tention to both linguistic and statistical factors, is
important.
In the remainder of this paper, we first review
the basics behind Goldsmith?s Linguistica program,
which serves as the starting point for our own work.
We then explain the additional framework neces-
sary for learning phonological rules, and describe
our search algorithm. In Section 5, we describe the
results of two experiments using our search algo-
rithm, first with an MDL prior, then with a modified
prior. We discuss why the modified prior works bet-
ter for our task, and implications for other Bayesian
learners.
2 Linguistica
Since our algorithm is designed to take as input
a morphological analysis produced by Linguistica,
we first briefly review what that analysis consists of
and how it is arrived at. Linguistica is based on the
MDL principle, which states that the optimal hy-
pothesis to explain a set of data is the one that min-
imizes the total number of bits required to describe
both the hypothesis and the data under that hypoth-
esis. Information theory tells us that the description
length of the data under a given hypothesis is simply
the negative log likelihood of the data, so the MDL
criterion is equivalent to a Bayesian prior favoring
hypotheses that can be described succintly.
Linguistic hypotheses (grammars) all contain
some primitive types. Linguistica uses three primi-
tive types in its grammar: stems, suffixes, and sig-
natures. 2 Each signature is associated with a set
of stems, and each stem is associated with exactly
one signature representing those suffixes with which
it combines freely. For example, walk and jump
might be associated with the signature ?.ed.ing.s?
(see Figure 1), while bad might be associated with
?.ly?. Unanalyzed words can be thought of as be-
longing to the ?? signature. A possible grammar
under this scenario consists of a set of signatures,
where each signature contains a set of stems and a
set of suffixes. Rather than modeling the probabil-
ity of each word in the corpus directly, the gram-
mar assumes that each word consists of a stem and
a (possibly empty) suffix, and assigns a probability
to each word w according to
Pr(w = t + f) = Pr(?)Pr(t|?)Pr(f |?),
where ? is the signature containing stem t. (We
have adopted Goldsmith?s notation here, using f to
denote suffixes, t for stems, and ? for signatures.)
Clearly, grouping words into signatures will cause
their probabilities to be modeled less well than mod-
eling each word individually. The negative log like-
lihood of the corpus will therefore increase, and this
portion of the description length will grow. How-
ever, listing each word individually in the grammar
requires as many stems as there are words. As-
signing words to signatures significantly reduces the
number of stems, and thus the length of the gram-
mar. If the stems are chosen well, then the length of
the grammar will decrease more than the length of
the encoded corpus will increase, leading to an over-
all win. Goldsmith (2001) provides a detailed de-
scription of the exact grammar encoding and search
heuristics used to find the optimal set of stems, suf-
fixes, and signatures under this type of model.
Goldsmith?s algorithm is not without its prob-
lems, however. We concern ourselves here with its
tendency to postulate spurious signatures in cases
where phonological constraints operate. For exam-
ple, many English verb stems ending in e are placed
in the signature ?e.ed.es.ing?, while stems not end-
ing in e have the signature ?.ed.ing.s?. This is due
to the fact that the stem-final e deletes before suf-
fixes beginning in e or i. Similarly, words like match
and index are likely to be given the signature ?.es?,
whereas most nouns would be ?.s?. The toy gram-
mar G1 in Figure 2 illustrates the sort of analysis
produced by Linguistica.
Goldsmith himself has noted the problem of spu-
rious signatures (Goldsmith, 2004a), and recent ver-
2Linguistica actually can perform prefix analysis as well as
suffix analysis, but in our work we used only the suffixing func-
tions.
?1 = ({work, roll}?{, ed, ing, er})
?2 = ({din, bik}?{e, ed, ing, er})
?3 = ({wait}?{, ed, er})
?4 = ({carr}?{y, ied, ier})
?5 = ({carry}?{, ing})
?6 = ({bike, booth, worker}?{, s})
?7 = ({beach, match}?{, es})
Figure 2: G1: A Sample Linguistica Grammar
sions of Linguistica include some functionality de-
voted to detecting allomorphs. Superficially, our
work may seem similar to Goldsmith?s, but in fact it
is quite different. First of all, the allomorphic vari-
ation detected by Linguistica is suffix-based. That
is, suffixes are proposed that operate to delete cer-
tain stem-final material. For example, a suffix (e)ing
could be proposed in order to include both hope
and walk in the signature ?.(e)ing.s?. This suf-
fix is actually separate in the grammar from the
ordinary ing suffix, so there is no recognition of
the fact that any occurrence of ing in any signa-
ture should delete a preceding stem-final e. More-
over, this approach is not really phonological, in
the sense that other suffixes beginning with i might
or might not be analyzed as deleting stem-final
e. While many languages do contain some affix-
specific morpho-phonological processes, our goal
here is to find phonological rules that apply at all
stem-suffix boundaries, given certain context crite-
ria.
A second major difference between the allomor-
phy detection in Linguistica and the work presented
here is that a Linguistica suffix such as (e)ing is as-
sumed to delete any stem-final e, without exception.
While this assumption may be valid in this case,
there are other suffixes and phonological processes
that are not categorical. For example, the English
plural s requires insertion of an e after certain stems,
including those ending in x or s. However, there
is no simple way to describe the context for this
rule based solely on orthography, because of stems
such as beach (+es) and stomach (+s). For this rea-
son, and to add robustness against errors in the input
morphological segmentation, we allow stems to be
listed in the grammar as exceptions to phonological
rules.
In addition to these theoretical differences, the
work presented here covers a wider range of phono-
logical processes than does Linguistica. Linguis-
tica is capable of detecting only stem-final deletion,
whereas our algorithm can also detect insertion (as
in match + s ? matches) and stem-final substitu-
tion (as in carry + ed? carried). In the following
section we discuss the structure of the grammar we
use to describe the words in our corpus.
3 A Morpho-Phonological Grammar
Since the morphology we use as input to our pro-
gram is obtained directly from Linguistica, our
grammar is necessarily similar to the one in that
program. As discussed above, Linguistica contains
three primitives types in its grammar: signatures,
stems, and suffixes. We add one more primitive type
to our grammar, the notion of a rule.
Each rule consists of a transformation, for ex-
ample  ? e or y ? i, and a conditioning con-
text. A context consists of a string of four charac-
ters XtytyfXf , where Xi ? {C, V,#} (consonant,
vowel, end-of-word) and yi is in the set of characters
in our text.3 The first half of the context is from the
end of the stem, and the second half is from the be-
ginning of the suffix. For example, the stem-suffix
pair jump + ed has the context CpeC . All trans-
formations are assumed to occur stem-finally, i.e.
at the second context position (or after the second
position, for insertions). Of course, these contexts
are more detailed than necessary for certain phono-
logical rules, and don?t capture all the information
required for others. In future work, we plan to al-
low for different types of contexts and generaliza-
tion over contexts, but for the present, all contexts
have the same form.
Using these four primitives, we can construct a
grammar in the following way: As in Goldsmith?s
work, we list a set of signatures, each of which
contains a set of stems and suffixes. In addition,
we list a set of phonological rules. In many cases,
only one rule will apply in a particular context, in
which case it applies to all stem-suffix pairs that
meet its context. If more than one rule applies, we
list the rule with the most common transformation
first and assume that it applies unless a particular
stem specifies otherwise. Stems can thus be listed
as exceptions to rules by using a non-default *no-
change* rule with the appropriate context. Note
that the more exceptions a rule has, the more expen-
sive it is to add to the grammar: each new type of
transformation in a particular context must be listed,
and each stem requiring a non-default transforma-
tion must specify the transformation required. Any
prior preferring short grammars will therefore tend
3The knowledge of which characters are consonants and
which are vowels is the only information we provide to our pro-
gram, other than the text corpus and the Linguistica-produced
morphology. Aside from the C/V distinction, our program is
entirely knowledge-free.
?1 = ({work, roll, dine, carry}?{, ed, er, ing})
?2 = ({bike}?{, ed, er, ing, s})
?3 = ({wait}?{, ed, er})
?4 = ({booth (r5), worker, beach, match}?{, s})
r1 = e? / CeeC
r2 = e? / CeiC
r3 = y? i / CyeC
r4 = ?e / Chs#
r5 = *no-change* / Chs#
Figure 3: G2: A Sample Grammar with Transfor-
mation Rules
to reject rules requiring many exceptions (i.e. those
without a consistent application context).
Grammar G2, in Figure 3, shows a sample of the
kind of grammar we use. This grammar generates
exactly the same wordforms as G1, but using fewer
signatures due to the effects of the phonological
rules. All the stem-suffix parings in this grammar
undergo the default rules for their contexts except
for the stem booth, which is listed as an exception
to the e-insertion rule. For booth + s, the grammar
therefore generates booths, not boothes.
Our model generates data in much the same way
as Goldsmith?s: a word is generated by selecting a
signature and then independently generating a stem
and suffix from that signature. This means that
the likelihood of the data takes the same form in
our model as in Goldsmith?s, namely Pr(w) =
Pr(?)Pr(t|?)Pr(f |?), where the word w consists
of a stem t and a suffix f both drawn from the
same signature ?. Our model differs from Gold-
smith?s in the way that stems and suffixes are pro-
duced; because we use phonological rules a great
many more stems and suffixes can belong to a sin-
gle signature. We defer discussion of how we define
the prior probability over grammars to Section 5,
and assume for the moment that we are given prior
and likelihood functions that can evaluate the utility
of a grammar and training data.
4 Search Algorithm
Since it is clearly infeasible to evaluate the utility of
every possible grammar, we need a search algorithm
to guide us toward a good solution. Our algorithm
uses certain heuristics to make small changes to the
initial grammar (the one provided by Linguistica),
evaluating each change using our objective func-
tion, and accepting or rejecting it based on the result
of evaluation. Our algorithm contains three major
components: a procedure to find signatures that are
similar in ways suggesting phonological change, a
procedure to identify possible contexts for phono-
logical change, and a procedure to collapse related
signatures and add phonological rules to the gram-
mar. We discuss each of these components in turn.
4.1 Identifying Similar Signatures
An important first step in simplifying the mor-
phological analysis of our data using phonologi-
cal rules is to identify signatures that might be re-
lated via such rules. Since our algorithm considers
three different types of possible phonological pro-
cesses (deletion, substitution, and insertion), there
are three different ways in which signatures may be
related. We need to look for pairs of signatures that
are similar in any of these three ways.
Insertion We look for potential insertion rules by
finding pairs of signatures in which all suffixes but
one are common to both signatures. The distinct
pair of suffixes must be such that one can be formed
from the other by inserting a single character at the
beginning. Example pairs found by our algorithm
include ?.s?/?.es? and ?.y?/?.ly?. In searching
for these pairs (as well as deletion and substitution
pairs), we consider only pairs where each signature
contains at least two stems. This is partly in the
interests of efficiency and partly due to the fact that
signatures with only one stem are often less reliable.
Deletion Signature pairs exhibiting possible dele-
tion behavior are similar to those exhibiting inser-
tion behavior, except that one of the suffixes not
common to both signatures must be the empty suf-
fix. Examples of possible deletion pairs include
?.ed.ing?/?e.ed.ing? and ?.ed.ing?/?ed.ing.s?.
Substitution In a possible substitution pair, one
signature (the one potentially exhibiting stem-final
substitution) contains suffixes that all begin with
one of two characters: the basic stem-final char-
acter, and the substituted character. The signature
?ied.ier.y? from G1 is such a signature. The other
signature in a possible substitution pair must con-
tain the empty suffix, and the two signatures must
be identical when the first character of each suffix in
the first signature is removed. Possible substitution
pairs include ?ied.ier.y?/?.ed.er? and ?ous.y?/?.us?.
Using the set of similar signatures we have de-
tected, we can propose a set of possible phonolog-
ical processes in our data. Some transformations,
such as e ? , will be suggested by more than one
pair of signatures, while others, such as y ? o,
will occur with only one pair. We create a list of
all the possible transformations, ranked according
to the number of signature pairs attesting to them.
4.2 Identifying possible contexts
Once we have found a set of possible transforma-
tions, we need to identify the contexts in which
those transformations might apply. To see how this
works, suppose we are looking at the proposed e-
deletion rule and our input grammar is G1. Using
one of the signature pairs attesting to this rule, such
as ?.ed.er.ing?/?e.ed.er.ing?, we can find possible
conditioning contexts by examining the set of stems
and suffixes in the second signature. If we want to
reanalyze the stems din and bik as dine and bike,
we hypothesize that each wordform generated us-
ing the suffixes present in both signatures (ed, er,
and ing) must have deleted an e. We can find the
context for this deletion by looking at these suffixes
together with the reanalyzed stems. The contexts
for deletion that we would get from {bike, dine} ?
{ed, ing} are {CeeC , CeiC}.4
Our methods for finding possible contexts for
substitution and insertion rules are similar: reana-
lyze the stems and suffixes in the signature hypoth-
esized to require a phonological rule, combine them,
and note the context generated. In this way, we can
get contexts such as CyeC for the y ? i rule (from
carry + ed) and V xs# for the ? ? e rule (from
index + s).
Just as we ranked the set of possible phonologi-
cal rules according to the number of signature pairs
attesting to them, we can rank the set of contexts
proposed for each rule. We do this by calculating
r = Pr(Xtyt|yfXf )/Pr(Xtyt), the ratio between
the probability of seeing a particular stem context
given a particular suffix context to the prior proba-
bility of the stem context. If a stem context (such
as Ce) is quite common overall but hardly ever ap-
pears before a particular suffix context (iC), this is
good evidence that some phonological process has
modified the stem in the context of that suffix. Low
values of r are therefore better evidence of condi-
tioning for a rule than are high values of r.
4.3 Collapsing signatures
Given a set of similar signature pairs, the rules
relating them, and the possible contexts for those
rules, we need to determine which rules are actu-
ally phonologically legitimate and which are sim-
ply accidents of the data. We do this by simply
considering each rule and context in turn, proceed-
ing from the most attested to least attested rules and
from most likely to least likely contexts. For each
rule-context pair, we add the rule to the grammar
4The reasoning we use to finding conditioning contexts for
deletion rules was also described by Goldsmith (2004a), and is
similar to the much earlier work of Johnson (1984).
FINDPHONORULES()
1 G? grammar produced by Linguistica
2 R? ordered set of possible rules
3 for each r ? R
4 do
5 Cr ? ordered set of possible contexts for r
6 C ? ?
7 while Cr 6= ?
8 do c? next c ? Cr
9 Cr ? Cr \ {c}
10 C ? C ? {c}
11 G? ? collapseInContext(G, r, C)
12 G? ? pruneRules(G?)
13 if score(G?) < score(G)
14 then G? G?
15 return G
COLLAPSEINCONTEXT(G, r, C)
1 for each ?i ? G
2 do for each ?j ? G
3 do if (?i?r ?j) ? (?(t, f) ? ?i, ctx(t, f) ? C)
4 then collapseSigs(?i, ?j)
Figure 4: Pseudocode for our search algorithm
with that context and collapse any pairs of signa-
tures related by the rule, as long as all stem-suffix
pairs contain a context at least as likely as the one
under consideration. Collapsing a pair of signa-
tures means reanalyzing all the stems and suffixes
in one of the signatures, and possibly adding excep-
tions for any stems that don?t fit the rule. We have
found that exceptions are often required to handle
stems that were originally misanalyzed by Linguis-
tica. For that reason, we prune the rules added to the
grammar, and for each rule, if fewer than 2% of the
stems require exceptions, we assume that these are
errors and de-analyze the stems, returning the word-
forms they generated to the ?? signature. We then
evaluate the new analysis using our objective func-
tion, and accept it if it scores better than our previ-
ous analysis. Otherwise, we revert to the previous
analysis and continue trying new rule-context pairs.
Pseudocode for our algorithm is presented in Fig-
ure 4. We use the notation ?i?r ?j to indicate that
?i and ?j are similar with respect to rule r, with ?j
being the more ?basic? signature (i.e. adding r to
the grammar would allow us to move the stems in
?i into ?j).
Note that collapsing a pair of signatures does not
always result in an overall reduction in the number
of signatures in the grammar. To see why this is
Morph Only Morph+Phon
Small Large Small Large
Tokens 100k 888k 100k 888k
Types 11313 35631 11313 35631
?s 435 1634 404 1594
Singleton ?s 280 1231 259 1215
Stems 8255 24529 8186 24379
Non- Stems 2363 7673 2286 7494
Table 1: Grammatical Analysis of our Corpora
so, consider the effect of collapsing ?1 and ?2 and
adding r1 and r2 (the e-deletion rules) to G1. When
the stem bik gets reanalyzed as bike, the algorithm
recognizes that bike is already a stem in the gram-
mar, so rather than placing the reanalyzed stem in
?1, it combines the reanalyzed suffixes {, ed, er,
ing} with the suffixes {, s} from ?6 and creates a
new signature for the stem bike ? ?.ed.er.ing.s?.
The two stems carr and carry are also combined
in this way, but in that case, the combined suffixes
form a signature already present in the grammar, so
no new signature is required.
5 Experiments
For our experiments with learning phonological
rules, we used two different corpora obtained from
the Penn Treebank. The larger corpus contains the
words from sections 2-21 of the treebank, filtered to
remove most numbers, acronyms, and words con-
taining puctuation. This corpus consists of approx-
imately 900,000 tokens. The smaller corpus is sim-
ply the first 100,000 words from the larger corpus.
We ran each corpus through the Linguistica pro-
gram to obtain an initial morphological segmenta-
tion. Statistics on the results of this segmentation
are shown in the left half of Table 1. ?Singleton
signatures? are those containing a single stem, and
?Non- stems? refers to stems in a signature other
than the ?? signature, i.e. those stems that combine
with at least one non- suffix.
The original function we used to evaluate the util-
ity of our grammars was an MDL prior very simi-
lar to the one described by Goldsmith (2001). This
prior is simply the number of bits required to de-
scribe the grammar using a fairly straightforward
encoding. The encoding essentially lists all the suf-
fixes in the grammar along with pointers to each
one; then lists the phonological rules with their
pointers; then lists all the signatures. Each signa-
ture is a list of stems and their pointers, and a list of
pointers to suffixes. Each exceptional stem also has
Init. Grammar Change
# ?s 1617 -10
# Stems 24374 -17
Grammar Size: 1335425 +520
?s, Suffixes 53933 -253
Stems 1280617 +493
Phonology 875 +279
Likelihood: 6478490 +166
Total: 7813915 +686
Table 2: Effects of adding y ? i rules under MDL
prior (large corpus).
a pointer to a phonological rule.5
Our algorithm considered a total of 11 possible
transformations in the small corpus and 40 in the
large corpus, but using this prior, only a single type
of transformation appeared in any rule in the final
grammar: e ? , with seven contexts in the small
corpus and eight contexts in the large corpus. In
analyzing why our algorithm failed to accept any
other types of rules, we realized that there were sev-
eral problems with the MDL prior. Consider what
happens to the overall evaluation when two signa-
tures are collapsed. In general, the likelihood of the
corpus will go down, because the stem and suffix
probabilities in the combined signature will not fit
the true probabilities of the words as well as two
separate signatures could. For large corpora like the
ones we are using, this likelihood drop can be quite
large. In order to counterbalance it, there must be a
large gain in the prior.
But now look at Table 2, which shows the effects
of adding all the y ? i rules to the grammar for
the large corpus under the MDL prior. The first
two lines give the number of signatures and stems
in each grammar. The next line shows the total
length (in bits) of each grammar, and this value is
then broken down into three different components:
the overhead caused by listing the signatures and
their suffixes, the length of the stem list (not in-
cluding the length required to specify exceptions to
rules), and the length of the phonological compo-
nent (including both rules and exception specifica-
tions). Finally, we have the negative log likelihood
under each grammar and the total MDL cost (gram-
mar plus likelihood).
As expected, the likelihood term for the grammar
5There are some additional complexities in the grammar en-
coding that we have not mentioned, due to the fact that stems
can be recursively analyzed using shorter stems. These com-
plexities are irrelevant to the points we wish to make here, but
are described in detail in Goldsmith (2001).
Init. Grammar Change
# ?s 1601 -7
# Stems 24386 -7
Grammar Size: 1249629 -318
?s, Suffixes 241465 -493
Stems 1005887 -111
Phonology 2277 +286
Likelihood: 6478764 +39
Total: 7728393 -279
Table 3: Effects of adding y ? i rules under modi-
fied prior (large corpus).
with y ? i rules has increased, indicating a drop
in the probability of the corpus under this gram-
mar. But notice that the total grammar size has
also increased, leading to an overall evaluation that
is worse than for the original grammar. There are
two main reasons for this increase in grammar size.
Initially, the more puzzling of the two is the fact
that the number of bits required to list all the stems
has increased, despite the fact that the number of
stems has decreased due to reanalyzing some pairs
of stems into single stems. It turns out that this ef-
fect is due to the encoding used for stems, which is
simply a bitwise encoding of each character in the
stem. This encoding means that longer stems re-
quire longer descriptions. When reanalysis requires
shifting a character from a suffix onto the entire set
of stems in a signature (as in {certif, empt, hurr} ?
{ied, y} ? {certify, empty, hurry} ? {, ed}), there
can be a large gain in description length simply due
to the extra characters in the stems. If the number of
stems eliminated through reanalysis is high enough
(as it is for the e ?  rules), this stem length effect
will be outweighed. But when only a few stems are
eliminated relative to the number that get longer, the
overall length of the stem list increases.
However, even without the stem list, the grammar
with y ? i rules would still be slightly longer than
the grammar without them. In this case, the rea-
son in that under our MDL prior, it is quite efficient
to encode a signature and its suffixes. Therefore the
grammar reduction caused by removing a few signa-
tures is not enough to outweigh the increase caused
by adding a few phonological rules.
Using these observations as a guideline, we re-
designed our prior by assigning a fixed cost to each
stem and increasing the overhead cost for signa-
tures. The new overhead function is equal to the
sum of the lengths of all the suffixes in the signature,
times a constant factor. This function means there is
more incentive to collapse two signatures that share
several suffixes, such as ?e.ed.er.ing?/?.ed.er.ing?,
than to collapse signatures sharing only a single suf-
fix, such as ?ing.s?/?.ing?. This behavior is exactly
what we want, since these shorter pairs are more
likely to be accidental. Table 3 shows the effects
of adding the y ? i rules under this new prior.
The starting grammar is somewhat different from
the one in Table 2, because more rules have already
been added by the time the y ? i rules are consid-
ered. The important point, however, is that the cost
of each component of the grammar changes in the
direction we expect it to, and the total grammar cost
is reduced enough to more than make up for the loss
in likelihood.
With this new prior, our algorithm was more suc-
cessful, learning from the large corpus the three ma-
jor transformations for English (e ? ,  ? e, and
y ? i) with a total of 22 contexts. Eight of these
rules, such as ? e / V xs# and y ? i / CyeC ,
had no exceptions. Of the remaining rules, the ex-
ceptions to six of the rules were correctly analyzed
stems (for example, unhappy + ly? unhappily and
necessary + ly? necessarily but sly + ly? slyly),
while the remaining eight rules contained misan-
alyzed exceptions (such as overse + er ? over-
seer, which was listed as an exception to the rule
e? / CeeC , rather than being reanalyzed as over-
see + er). In the small corpus, no y ? i rules were
learned due to the fact that no similar signatures at-
testing to these rules were found.
Using these phonological rules, a total of 31 sig-
natures in the small corpus and 57 signatures in the
large corpus were collapsed, with subsequent re-
analysis of 225 and 528 stems, respectively. This
represents 7-10% of all the non- stems. The final
grammars are summarized in the right half of Table
1.
6 Conclusion
The work described here is clearly preliminary with
respect to learning phonological rules and using
those rules to simplify an existing morphology. Our
notion of context, for example, is somewhat impov-
erished; our system might benefit from using con-
texts with variable lengths and levels of generality,
such as those in Albright and Hayes (2003). We
also cannot handle transformations that require rule
ordering or more than one-character changes. One
reason we have not yet implemented these additions
is the difficulty of designing a heuristic search that
can handle the additional complexity required. We
are therefore working toward implementing a more
general search procedure that will allow us to ex-
plore a larger grammar space, allowing greater flex-
ibility with rules and contexts. Once some of these
improvements have been implemented, we hope to
explore the possibilities for learning in languages
with richer morphology and phonology than En-
glish.
Our point in this paper, however, is not to present
a fully general learner, but to emphasize that in a
Bayesian system, the choice of prior can be crucial
to the success of the learning task. Learning is a
trade-off between finding an explanation that fits the
current data (maximizing the likelihood) and main-
taining the ability to generalize to new data (max-
imizing the prior). The MDL framework is a way
to formalize this trade-off that is intuitively appeal-
ing and seems straightforward to implement, but we
have shown that a simple MDL approach is not the
best way to achieve our particular task. There are at
least two reasons for this. First, the obvious encod-
ing of stems actually penalizes the addition of cer-
tain types of phonological rules, even when adding
these rules reduces the number of stems in the gram-
mar. More importantly, the type of grammar we
want to learn allows two different kinds of general-
izations: the grouping of stems into signatures, and
the addition of phonological rules. Simply speci-
fying a method of encoding each type of general-
ization may not result in a linguistically appropriate
trade-off during learning. In particular, we discov-
ered that our MDL encoding for signatures was too
efficient relative to the encoding for rules, leading
the system to prefer not to add rules. Our large cor-
pus size already puts a great deal of pressure on the
system to keep signatures separate, since this leads
to a better fit of the data. In order to learn most of
the rules, we therefore had to significantly increase
the cost of signatures.
We are not the first to note that with an MDL-
style prior the choice of encoding makes a differ-
ence to the linguistic appropriateness of the result-
ing grammar. Chomsky himself (Chomsky, 1965)
points out that the reason for using certain types
of notation in grammar rules is to make clear the
types of generalizations that lead to shorter gram-
mars. However, our experience emphasizes the fact
that very little is still known about how to choose
appropriate encodings (or, more generally, priors).
As researchers continue to attempt more sophisti-
cated Bayesian learning tasks, they will encounter
more interactions between different kinds of gener-
alizations. As a result, the question of how to de-
sign a good prior will become increasingly impor-
tant. Our primary goal for the future is therefore to
investigate exactly what assumptions go into decid-
ing whether a grammar is linguistically sound, and
to determine how to specify those assumptions ex-
plicitly in a Bayesian prior.
Acknowledgements
The authors would like to thank Eugene Charniak
and the anonymous reviewers for helpful comments.
This work was supported by NSF grants 9870676
and 0085940, NIMH grant 1R0-IMH60922-01A2,
and an NDSEG fellowship.
References
A. Albright and B. Hayes. 2003. Rules vs.
analogy in english pass tenses: a computa-
tional/experimental study. Cognition, 90:119?
161.
M. Brent and T. Cartwright. 1996. Distributional
regularity and phonotactic constraints are useful
for segmentation. Cognition, 61:93?125.
N. Chomsky. 1965. Aspects of the Theory of Syn-
tax. MIT Press, Cambridge.
M. Creutz and K. Lagus. 2002. Unsupervised dis-
covery of morphemes. In Proceedings of the
Workshop on Morphological and Phonological
Learning at ACL ?02, pages 21?30.
C. de Marcken. 1996. Unsupervised Language Ac-
quisition. Ph.D. thesis, Massachusetts Institute of
Technology.
T. M. Ellison. 1993. The Machine Learning of
Phonological Structure. Ph.D. thesis, University
of Western Australia.
T. M. Ellison. 1994. The iterative learning of
phonological constraints. Computational Lin-
guistics, 20(3).
J. Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computa-
tional Linguistics, 27:153?198.
J. Goldsmith. 2004a. An algorithm for the unsuper-
vised learning of morphology. Preliminary draft
as of January 1.
J. Goldsmith. 2004b. Linguis-
tica. Executable available at
http://humanities.uchicago.edu/faculty/goldsmith/.
M. Johnson. 1984. A discovery procedure for cer-
tain phonological rules. In Proceedings of COL-
ING.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
93. Building a large annotated corpus of english:
the penn treebank. Computational Linguistics,
19(2).
Rissanen. 1989. Stochastic Complexity and Statis-
tical Inquiry. World Scientific Co., Singapore.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 112?119, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Representational Bias in Unsupervised Learning of Syllable Structure
Sharon Goldwater and Mark Johnson
Department of Cognitive and Linguistic Sciences
Brown University
Providence, RI 02912
{Sharon Goldwater,Mark Johnson}@brown.edu
Abstract
Unsupervised learning algorithms based
on Expectation Maximization (EM) are
often straightforward to implement and
provably converge on a local likelihood
maximum. However, these algorithms of-
ten do not perform well in practice. Com-
mon wisdom holds that they yield poor
results because they are overly sensitive
to initial parameter values and easily get
stuck in local (but not global) maxima.
We present a series of experiments indi-
cating that for the task of learning sylla-
ble structure, the initial parameter weights
are not crucial. Rather, it is the choice of
model class itself that makes the differ-
ence between successful and unsuccess-
ful learning. We use a language-universal
rule-based algorithm to find a good set of
parameters, and then train the parameter
weights using EM. We achieve word ac-
curacy of 95.9% on German and 97.1% on
English, as compared to 97.4% and 98.1%
respectively for supervised training.
1 Introduction
The use of statistical methods in computational lin-
guistics has produced advances in tasks such as pars-
ing, information retrieval, and machine translation.
However, most of the successful work to date has
used supervised learning techniques. Unsupervised
algorithms that can learn from raw linguistic data,
as humans can, remain a challenge. In a statistical
framework, one method that can be used for unsu-
pervised learning is to devise a probabilistic model
of the data, and then choose the values for the model
parameters that maximize the likelihood of the data
under the model.
If the model contains hidden variables, there is
often no closed-form expression for the maximum
likelihood parameter values, and some iterative ap-
proximation method must be used. Expectation
Maximization (EM) (Neal and Hinton, 1998) is
one way to find parameter values that at least lo-
cally maximize the likelihood for models with hid-
den variables. EM is attractive because at each
iteration, the likelihood of the data is guaranteed
not to decrease. In addition, there are efficient
dynamic-programming versions of the EM algo-
rithm for several classes of models that are important
in computational linguistics, such as the forward-
backward algorithm for training Hidden Markov
Models (HMMs) and the inside-outside algorithm
for training Probabilistic Context-Free Grammars
(PCFGs).
Despite the advantages of maximum likelihood
estimation and its implementation via various in-
stantiations of the EM algorithm, it is widely re-
garded as ineffective for unsupervised language
learning. Merialdo (1994) showed that with only
a tiny amount of tagged training data, supervised
training of an HMM part-of-speech tagger outper-
formed unsupervised EM training. Later results (e.g.
Brill (1995)) seemed to indicate that other methods
of unsupervised learning could be more effective (al-
though the work of Banko and Moore (2004) sug-
gests that the difference may be far less than previ-
112
ously assumed). Klein and Manning (2001; 2002)
recently achieved more encouraging results using an
EM-like algorithm to induce syntactic constituent
grammars, based on a deficient probability model.
It has been suggested that EM often yield poor
results because it is overly sensitive to initial param-
eter values and tends to converge on likelihood max-
ima that are local, but not global (Carroll and Char-
niak, 1992). In this paper, we present a series of
experiments indicating that for the task of learning
a syllable structure grammar, the initial parameter
weights are not crucial. Rather, it is the choice of
the model class, i.e., the representational bias, that
makes the difference between successful and unsuc-
cessful learning.
In the remainder of this paper, we first describe
the task itself and the structure of the two differ-
ent classes of models we experimented with. We
then present a deterministic algorithm for choosing
a good set of parameters for this task. The algo-
rithm is based on language-universal principles of
syllabification, but produces different parameters for
each language. We apply this algorithm to English
and German data, and describe the results of exper-
iments using EM to learn the parameter weights for
the resulting models. We conclude with a discussion
of the implications of our experiments.
2 Statistical Parsing of Syllable Structure
Knowledge of syllable structure is important for
correct pronunciation of spoken words, since cer-
tain phonemes may be pronounced differently de-
pending on their position in the syllable. A num-
ber of different supervised machine learning tech-
niques have been applied to the task of automatic
syllable boundary detection, including decision-tree
classifiers (van den Bosch et al, 1998), weighted
finite state transducers (Kiraz and Mo?bius, 1998),
and PCFGs (Mu?ller, 2001; Mu?ller, 2002). The re-
searchers presenting these systems have generally
argued from the engineering standpoint that sylla-
ble boundary detection is useful for pronunciation of
unknown words in text-to-speech systems. Our mo-
tivation is a more scientific one: we are interested in
the kinds of procedures and representations that can
lead to successful unsupervised language learning in
both computers and humans.
Our work has some similarity to that of Mu?ller,
who trains a PCFG of syllable structure from a
corpus of words with syllable boundaries marked.
We, too, use a model defined by a grammar to de-
scribe syllable structure.1 However, our work dif-
fers from Mu?ller?s in that it focuses on how to learn
the model?s parameters in an unsupervised manner.
Several researchers have worked on unsupervised
learning of phonotactic constraints and word seg-
mentation (Elman, 2003; Brent, 1999; Venkatara-
man, 2001), but to our knowledge there is no pre-
viously published work on unsupervised learning of
syllable structure.
In the work described here, we experimented with
two different classes of models of syllable structure.
Both of these model classes are presented as PCFGs.
The first model class, described in Mu?ller (2002),
encodes information about the positions within a
word or syllable in which each phoneme is likely
to appear. In this positional model, each syllable
is labeled as initial (I), medial (M), final (F), or as
the one syllable in a monosyllabic word (O). Syl-
lables are broken down into an optional onset (the
initial consonant or consonant cluster) followed by a
rhyme. The rhyme consists of a nucleus (the vowel)
followed by an optional coda consonant or cluster.
Each phoneme is labeled with a preterminal cate-
gory of the form CatPos.x.y, where Cat ? {Ons,
Nuc, Cod}, Pos ? {I, M, F, O}, x is the position
of a consonant within its cluster, and y is the total
number of consonants in the cluster. x and y are un-
used when Cat = Nuc, since all nuclei consist of a
single vowel. See Fig. 1 for an example parse.
Rather than directly encoding positional infor-
mation, the second model class we investigate (the
bigram model) models statistical dependencies be-
tween adjacent phonemes and adjacent syllables.
In particular, each onset or coda expands directly
into one or more terminal phonemes, thus capturing
the ordering dependencies between consonants in a
cluster. Also, the shape of each syllable (whether it
contains an onset or coda) depends on the shape of
the previous syllable, so that the model can learn,
for example, that syllables ending in a coda should
be followed by syllables with an onset.2 This kind
1We follow Mu?ller in representing our models as PCFGs be-
cause this representation is easy to present. The languages gen-
erated by these PCFGs are in fact regular, and it is straightfor-
ward to transform the PCFGs into equivalent regular grammars.
2 Many linguists believe that, cross-linguistically, a poten-
113
Word
SylI
RhyI
NucI
@
SylM
OnsM
OnsM.1.2
g
OnsM.2.2
r
RhyM
NucM
i
SylF
OnsF
OnsF.1.1
m
RhyF
NucF
@
CodF
CodF.1.2
n
CodF.2.2
t
Word
WdN
SylN
Nuc
@
WdON
SylON
Ons
g r
Nuc
i
WdON
SylONC
Ons
m
Nuc
@
Cod
n t
Figure 1: Positional analysis (left) and bigram analysis (right) of the word agreement. Groups of terminals
dominated by a Syl* node constitute syllables. Terminals appear in the SAMPA encoding of IPA used by
CELEX.
of bigram dependency between syllables is modeled
using rules of the form WdX ? SylX WdY , where
X and Y are drawn from the set of possible combi-
nations of onset, nucleus, and coda in a syllable: {N,
ON, NC, ONC}. Each SylX category has only one
expansion. See Fig. 1 for an example.
With respect to either of these two model classes,
each way of assigning syllable boundaries to a word
corresponds to exactly one parse of that word. This
makes it simple to train the models from a corpus in
which syllable boundaries are provided, as in Mu?ller
(2001). We used two different corpora for our exper-
iments, one German (from the ECI corpus of news-
paper text) and one English (from the Penn WSJ
corpus). Each corpus was created by converting
the orthographic forms in the original text into their
phonemic transcriptions using the CELEX database
(Baayen et al, 1995). CELEX includes syllable
boundaries, which we used for supervised training
and for evaluation. Any words in the original texts
that were not listed in CELEX were discarded, since
one of our goals is to compare supervised and un-
supervised training.3 From the resulting phonemic
corpora, we created a training set of 20,000 tokens
and a test set of 10,000 tokens. Using standard max-
imum likelihood supervised training procedures, we
obtained similar results for models from the two
model classes. In German, word accuracy (i.e. the
tially ambiguous consonant, such as the b in saber, is always
syllabified as the onset of the second syllable rather than the
coda of the first. We discuss this point further in Section 3.
3Due to the nature of the corpora, the percentage of words
discarded was fairly high: 35.6% of the English tokens (pri-
marily proper nouns, acronyms, and numerals, with a smaller
number of morphologically complex words) and 26.7% of the
German tokens (with compound words making up a somewhat
larger portion of these discards).
percentage of words with no syllabification errors)
was 97.4% for the bigram model and 97.2% for the
positional model,4 while in English it was 98.1%
and 97.6% respectively. These results for English
are in line with previous reported results using other
supervised learning techniques, e.g. van den Bosch
et al (1998). Since many of the words in the data are
monosyllabic (49.1% in German, 61.2% in English)
and therefore contain no ambiguous syllable bound-
aries, we also calculated the multisyllabic word ac-
curacy. This was 94.9% (bigram) and 94.5% (posi-
tional) in German, and 95.2% (bigram) and 93.8%
(positional) in English.
3 Categorical Parsing of Syllable Structure
In the previous section, we described two different
model classes and showed that the maximum like-
lihood estimates with supervised training data yield
good models of syllable structure. In moving to un-
supervised learning, however, there are two prob-
lems that need to be addressed: exactly what class of
models do we want to consider (i.e., what kinds of
rules should the model contain), and how should we
select a particular model from that class (i.e., what
weights should the rules have)? We take as our so-
lution to the latter problem the most straightforward
approach; namely, maximum likelihood estimation
using EM. This leaves us with the question of how
to choose a set of parameters in the first place. In this
section, we describe an algorithm based on two fun-
damental phonological principles that, when given a
set of data from a particular language, will produce a
4Mu?ller reports slightly lower results of 96.88% on German
using the same positional model. We have no explanation for
this discrepancy.
114
set of rules appropriate to that language. These rules
can then be trained using EM.
Given a particular rule schema, it is not imme-
diately clear which of the possible rules should ac-
tually be included in the model. For example, in
the bigram model, should we start off with the rule
Ons ? k n? This rule is unnecessary for English,
and could lead to incorrect parses of words such
as weakness. But /kn/ is a legal onset in German,
and since we want an algorithm that is prepared to
learn any language, disallowing /kn/ as an onset out
of hand is unacceptable. On the other hand, the set
of all combinatorially possible consonant clusters is
infinite, and even limiting ourselves to clusters actu-
ally seen in the data for a particular language yields
extremely unlikely-sounding onsets like /lkj/ (calcu-
late) and /bst/ (substance). Ideally, we should limit
the set of rules to ones that are likely to actually be
used in the language of interest.
The algorithm we have developed for produc-
ing a set of language-appropriate rules is essentially
a simple categorical (i.e., non-statistical) syllable
parser based on the principles of onset maximiza-
tion and sonority sequencing (Blevins, 1995). Onset
maximization is the idea that in word-medial conso-
nant clusters, as many consonants as possible (given
the phonotactics of the language) should be assigned
to onset position. This idea is widely accepted and
has been codified in Optimality Theory (Prince and
Smolensky, 1993) by proposing the existence of a
universal preference for syllables with onsets.5
In addition to onset maximization, our categorical
parser follows the principle of sonority sequencing
whenever possible. This principle states that, within
a syllable, segments that are closer to the nucleus
should be higher in sonority than segments that are
further away. Vowels are considered to be the most
sonorous segments, followed by glides (/j/, /w/), liq-
uids (/l/, /r/), nasals (/n/, /m/, /N/), fricatives (/v/,
/s/, /T/, . . . ), and stops (/b/, /t/, /k/, . . . ). Given a
5An important point, which we return to in Section 5, is
that exceptions to onset maximization may occur at morpheme
boundaries. Some linguists also believe that there are addi-
tional exceptions in certain languages (including English and
German), where stressed syllables attract codas. Under this the-
ory, the correct syllabification for saber would not be sa.ber, but
rather sab.er, or possibly sa[b]er, where the [b] is ambisyllabic.
Since the syllable annotations in the CELEX database follow
simple onset maximization, we take that as our approach as well
and do not consider stress when assigning syllable boundaries.
cluster of consonants between two syllable nuclei,
sonority sequencing states that the syllable boundary
should occur either just before or just after the con-
sonant with lowest sonority. Combining this princi-
ple with onset maximization predicts that the bound-
ary should fall before the lowest-sonority segment.
Predicting syllable boundaries in this way is not
foolproof. In some cases, clusters that are predicted
by sonority sequencing to be acceptable are in fact
illegal in some languages. The illegal English on-
set cluster kn is a good example. In other cases,
such as the English onset str, clusters are allowed
despite violating sonority sequencing. These mis-
matches between universal principles and language-
specific phonotactics lead to errors in the predic-
tions of the categorical parser, such as wea.kness and
ins.tru.ment. In addition, certain consonant clusters
like bst (as in substance) may contain more than
one minimum sonority point. To handle these cases,
the categorical parser follows onset maximization
by adding any consonants occurring between the
two minima to the onset of the second syllable:
sub.stance.
Not surprisingly, the categorical parser does not
perform as well as the supervised statistical parser:
only 92.7% of German words and 94.9% of English
words (85.7% and 86.8%, respectively, of multisyl-
labic words) are syllabified correctly. However, a
more important result of parsing the corpus using
the categorical parser is that its output can be used
to define a model class (i.e., a set of PCFG rules)
from which a model can be learned using EM.
Specifically, our model class contains the set of
rules that were proposed at least once by the cat-
egorical parser in its analysis of the training cor-
pus; in the EM experiments described below, the
rule probabilities are initialized to their frequency
in the categorical parser?s output. Due to the mis-
takes made by the categorical parser, there will be
some rules, like Ons ? k n in English, that are not
present in the model trained on the true syllabifica-
tion, but many possible but spurious rules, such as
Ons ? b s t, will be avoided. Although clusters that
violate sonority sequencing tend to be avoided by
the categorical parser, it does find examples of these
types of clusters at the beginnings and endings of
words, as well as occasionally word-medially (as in
sub.stance). This means that many legal clusters that
115
Bigram Positional
all multi all multi
CP 92.7 85.7 92.7 85.7
CP + EM 95.9 91.9 91.8 84.0
CP-U + EM 95.9 91.9 92.0 84.4
supervised 97.4 94.9 97.2 94.5
SP + EM 71.6 44.3 94.4 89.1
SP-U + EM 71.6 44.3 94.4 89.0
Table 1: Results for German: % of all words (or
multisyllabic words) correctly syllabified.
violate sonority sequencing will also be included in
the set of rules found by this procedure, although
their probabilities may be considerably lower than
those of the supervised model. In the following sec-
tion, we show that these differences in rule probabil-
ities are unimportant; in fact, it is not the rule prob-
abilities estimated from the categorical parser?s out-
put, but only the set of rules itself that matters for
successful task performance.
4 Experiments
In this section, we present a series of experiments us-
ing EM to learn a model of syllable structure. All of
our experiments use the same German and English
20,000-word training corpora and 10,000-word test-
ing corpora as described in Section 2.6
For our first experiment, we ran the categorical
parser on the training corpora and estimated a model
from the parse trees it produced, as described in the
previous section. This is essentially a single step
of Viterbi EM training. We then continued to train
the model by running (standard) EM to convergence.
Results of this experiment with Categorical Pars-
ing + EM (CP + EM) are shown in Tables 1 and
2. For both German and English, using this learn-
ing method with the bigram model yields perfor-
mance that is much better than the categorical parser
alone, though not quite as good as the fully super-
vised regime. On the other hand, training a posi-
tional model from the categorical parser?s output and
then running EM causes performance to degrade.
To determine whether the good performance of
6Of course, for unsupervised learning, it is not necessary to
use a distinct testing corpus. We did so in order to use the same
testing corpus for both supervised and unsupervised learning
experiments, to ensure fair comparison of results.
Bigram Positional
all multi all multi
CP 94.9 86.8 94.9 86.8
CP + EM 97.1 92.6 94.1 84.9
CP-U + EM 97.1 92.6 94.1 84.9
supervised 98.1 95.2 97.6 93.8
SP + EM 86.0 64.0 96.5 90.9
SP-U + EM 86.0 64.0 67.6 16.5
Table 2: Results for English.
the bigram model was simply due to good initial-
ization of the parameter weights, we performed a
second experiment. Again starting with the set of
rules output by the categorical parser, we initialized
the rule weights to the uniform distribution. The re-
sults of this experiment (CP-U + EM) show that for
the class of bigram models, the performance of the
final model found by EM does not depend on the
initial rule probabilities. Performance within the po-
sitional model framework does depend on the initial
rule probabilities, since accuracy in German is dif-
ferent for the two experiments.
As we have pointed out, the rules found by the
categorical parser are not exactly the same as the
rules found using supervised training. This raises
the question of whether the difference in perfor-
mance between the unsupervised and supervised bi-
gram models is due to differences in the rules. To
address this question, we performed two additional
experiments. First, we simply ran EM starting from
the model estimated from supervised training data.
Second, we kept the set of rules from the supervised
training data, but reinitialized the probabilities to a
uniform distribution before running EM. The results
of these experiments are shown as SP + EM and SP-
U + EM, respectively. Again, performance of the
bigram model is invariant with respect to initial pa-
rameter values, while the performance of the posi-
tional model is not. Interestingly, the performance
of the bigram model in these two experiments is far
worse than in the CP experiments. This result is
counterintuitive, since it would seem that the model
rules found by the supervised system are the opti-
mal rules for this task. In the following section, we
explain why these rules are not, in fact, the optimal
rules for unsupervised learning, as well as why we
believe the bigram model performs so much better
116
than the positional model in the unsupervised learn-
ing situation.
5 Discussion
The results of our experiments raise two interesting
questions. First, when starting from the categorical
parser?s output, why does the bigram model improve
after EM training, while the positional model does
not? And second, why does applying EM to the su-
pervised bigram model lead to worse performance
than applying it to the model induced from the cate-
gorical parser?
To answer the first question, notice that one dif-
ference between the bigram model and the posi-
tional model is that onsets and codas in the bigram
model are modeled using the same set of parame-
ters regardless of where in the word they occur. This
means that the bigram model generalizes whatever it
learns about clusters at word edges to word-medial
clusters (and, of course, vice versa). Since the cate-
gorical parser only makes errors word-medially, in-
correct clusters are only a small percentage of clus-
ters overall, and the bigram model can overcome
these errors by reanalyzing the word-medial clus-
ters. The errors that are made after EM training
are mostly due to overgeneralization from clusters
that are very common at word edges, e.g. predicting
le.gi.sla.tion instead of le.gis.la.tion.
In contrast to the bigram model, the positional
model does not generalize over different positions
of the word, which means that it learns and repeats
the word-medial errors of the categorical parser. For
example, this model predicts /E.gzE.kju.tIv/ for ex-
ecutive, just as the categorical parser does, although
/gz/ is never attested in word-initial position. In ad-
dition, each segment in a cluster is generated in-
dependently, which means clusters like /tl/ may be
placed together in an onset because /t/ is common
as the first segment of an onset, and /l/ is common
as the second. While this problem exists even in
the supervised positional model, it is compounded
in the unsupervised version because of the errors of
the categorical parser.
The differences between these two models are an
example of the bias-variance trade-off in probabilis-
tic modeling (Geman et al, 1992): models with low
bias will be able to fit a broad range of observations
fairly closely, but slight changes in the observed data
will cause relatively large changes in the induced
model. On the other hand, models with high bias
are less sensitive to changes in the observed data.
Here, the bigram model induced from the categor-
ical parser has a relatively high bias: regardless of
the parameter weights, it will be a poor model of
data where word-medial onsets and codas are very
different from those at word edges, and it cannot
model data with certain onsets such as /vp/ or /tz/
at all because the rules Ons ? v p and Ons ? t z
are simply absent. The induced positional model
can model both of these situations, and can fit the
true parses more closely as well (as evidenced by
the fact that the likelihood of the data under the su-
pervised positional model is higher than the like-
lihood under the supervised bigram model). As a
result, however, it is more sensitive to the initial
parameter weights and learns to recreate the errors
produced by the categorical parser. This sensitiv-
ity to initial parameter weights also explains the ex-
tremely poor performance of the positional model
in the SP-U + EM experiment on English. Because
the model is so unconstrained, in this case it finds a
completely different local maximum (not the global
maximum) which more or less follows coda max-
imization rather than onset maximization, yielding
syllabifications like synd.ic.ate and tent.at.ive.ly.
The concept of representational bias can also ex-
plain why applying EM to the supervised bigram
model performs so poorly. Examining the model in-
duced from the categorical parser reveals that, not
surprisingly, it contains more rules than the super-
vised bigram model. This is because the categori-
cal parser produces a wider range of onsets and co-
das than there are in the true parses. However, the
induced model is not a superset of the supervised
model. There are four rules (three in English) that
occur in the supervised model but not the induced
model. These are the rules that allow words where
one syllable contains a coda and the following syl-
lable has no onset. These are never produced by the
categorical parser because of its onset-maximization
principle. However, it turns out that a very small per-
centage of words do follow this pattern (about .14%
of English tokens and 1.1% of German tokens). In
English, these examples seem to consist entirely of
words where the unusual syllable boundary occurs at
a morpheme boundary (e.g. un.usually, dis.appoint,
117
week.end, turn.over). In German, all but a handful of
examples occur at morpheme boundaries as well.7
The fact that the induced bigram model is unable
to model words with codas followed by no onset is
a very strong bias, but these words are so infrequent
that the model can still fit the data quite well. The
missing rules have no effect on the accuracy of the
parser, because in the supervised model the proba-
bilities on the rules allowing these kinds of words
are so low that they are never used in the Viterbi
parses anyway. The problem is that if these rules are
included in the model prior to running EM, they add
several extra free parameters, and suddenly EM is
able to reanalyze many of the words in the corpus to
make better use of these parameters. It ends up pre-
ferring certain segments and clusters as onsets and
others as codas, which raises the likelihood of the
corpus but leads to very poor performance. Essen-
tially, it seems that the presence of a certain kind of
morpheme boundary is an additional parameter of
the ?true? model that the bigram model doesn?t in-
clude. Trying to account for the few cases where this
parameter matters requires introducing extra param-
eters that allow EM too much freedom of analysis.
It is far better to constrain the model, disallowing
certain rare analyses but enabling the model to learn
successfully in a way that is robust to variations in
initial conditions and idiosyncracies of the data.
6 Conclusion
We make no claims that our learning system em-
bodies a complete model of syllabification. A full
model would need to account for the effects of mor-
phological boundaries, as well as the fact that some
languages allow resyllabification over word bound-
aries. Nevertheless, we feel that the results presented
here are significant. We have shown that, despite
previous discouraging results (Carroll and Charniak,
1992; Merialdo, 1994), it is possible to achieve good
results using EM to learn linguistic structures in an
unsupervised way. However, the choice of model
parameters is crucial for successful learning. Car-
roll and Charniak, for example, generated all pos-
7The exceptions in our training data were auserkoren ?cho-
sen?, erobern ?capture?, and forms of erinnern ?remind?, all of
which were listed in CELEX as having a syllable boundary, but
no morpheme boundary, after the first consonant. Our knowl-
edge of German is not sufficient to determine whether there is
some other factor that can explain these cases.
sible rules within a particular framework and relied
on EM to remove the ?unnecessary? rules by letting
their probabilities go to zero. We suggest that this
procedure tends to yield models with low bias but
high variance, so that they are extremely sensitive
to the small variations in expected rule counts that
occur with different initialization weights.
Our work suggests that using models with higher
bias but lower variance may lead to much more
successful results. In particular, we used univer-
sal phonological principles to induce a set of rules
within a carefully chosen grammatical framework.
We found that there were several factors that en-
abled our induced bigram model to learn success-
fully where the comparison positional model did
not:
1. The bigram model encodes bigram dependen-
cies of syllable shape and disallows onset-less
syllables following syllables with codas.
2. The bigram model does not distinguish be-
tween different positions in a word, so it can
generalize onset and coda sequences from word
edges to word-medial position.
3. The bigram model learns specific sequences
of legal clusters rather than information about
which positions segments are likely to occur in.
Notice that each of these factors imposes a con-
straint on the kinds of data that can be modeled. We
have already discussed the fact that item 1 rules out
the correct syllabification of certain morphologically
complex words, but since our system currently has
no way to determine morpheme boundaries, it is bet-
ter to do so than to introduce extra free parameters.
One possible extension to this work would be to try
to incorporate morphological boundary information
(either annotated or induced) into the model.
A more interesting constraint is the one imposed
by item 2, since in fact most languages do have some
differences between the onsets and (especially) co-
das allowed at word edges and within words. How-
ever, the proper way to handle this fact is not by
introducing completely independent parameters for
initial, medial, and final positions, since this allows
far too much freedom. It would be extremely sur-
prising to find a language with one set of codas al-
lowed word-internally, and a completely disjoint set
118
allowed word-finally. In fact, the usual situation is
that word-internal onsets and codas are a subset of
those allowed at word edges, and this is exactly why
using word edges to induce our rules was successful.
Considering language more broadly, it is com-
mon to find patterns of linguistic phenomena with
many similarities but some differences as well. For
such cases, adding extra parameters to a supervised
model often yields better performance, since the
augmented model can capture both primary and sec-
ondary effects. But it seems that, at least for the
current state of unsupervised learning, it is better to
limit the number of parameters and focus on those
that capture the main effects in the data. In our task
of learning syllable structure, we were able to use
just a few simple principles to constrain the model
successfully. For more complex tasks such as syn-
tactic parsing, the space of linguistically plausible
models is much larger. We feel that a research pro-
gram integrating results from the study of linguistic
universals, human language acquisition, and compu-
tational modeling is likely to yield the most insight
into the kinds of constraints that are needed for suc-
cessful learning.
Ultimately, of course, we will want to be able to
capture not only the main effects in the data, but
some of the subtler effects as well. However, we
believe that the way to do this is not by introducing
completely free parameters, but by using a Bayesian
prior that would enforce a degree of similarity be-
tween certain parameters. In the meantime, we have
shown that employing linguistic universals to deter-
mine which set of parameters to include in a lan-
guage model for syllable parsing allows us to use
EM for learning the parameter weights in a success-
ful and robust way.
Acknowledgments
We would like to thank Eugene Charniak and our
colleagues in BLLIP for their support and helpful
suggestions. This research was partially supported
by NSF awards IGERT 9870676 and ITR 0085940
and NIMH award 1R0-IMH60922-01A2.
References
R. Baayen, R. Piepenbrock, and L. Gulikers. 1995. The
CELEX lexical database (release 2) [cd-rom].
M. Banko and R. Moore. 2004. A study of unsupervised part-
of-speech tagging. In Proceedings of COLING ?04.
J. Blevins. 1995. The syllable in phonological theory. In
J. Goldsmith, editor, the Handbook of Phonological Theory.
Blackwell, Oxford.
M. Brent. 1999. An efficient, probabilistically sound algorithm
for segmentation and word discovery. Machine Learning,
34:71?105.
E. Brill. 1995. Unsupervised learning of disambiguation rules
for part of speech tagging. In Proceedings of the 3rd Work-
shop on Very Large Corpora, pages 1?13.
G. Carroll and E. Charniak. 1992. Two experiments on learning
probabilistic dependency grammars from corpora. In Pro-
ceedings of the AAAI Workshop on Statistically-Based Natu-
ral Language Processing Techniques, San Jose, CA.
J. Elman. 2003. Generalization from sparse input. In Proceed-
ings of the 38th Annual Meeting of the Chicago Linguistic
Society.
S. Geman, E. Bienenstock, and R. Doursat. 1992. Neural net-
works and the bias/variance dilemma. Neural Computation,
4:1?58.
G. A. Kiraz and B. Mo?bius. 1998. Multilingual syllabifica-
tion using weighted finite-state transducers. In Proceedings
of the Third European Speech Communication Association
Workshop on Speech Synthesis.
D. Klein and C. Manning. 2001. Distributional phrase struc-
ture induction. In Proceedings of the Conference on Natural
Language Learning, pages 113?120.
D. Klein and C. Manning. 2002. A generative constituent-
context model for improved grammar induction. In Proceed-
ings of the ACL.
B. Merialdo. 1994. Tagging english text with a probabilistic
model. Computational Linguistics, 20(2):155?172.
K. Mu?ller. 2001. Automatic detection of syllable boundaries
combining the advantages of treebank and bracketed corpora
training. In Proceedings of the ACL.
K. Mu?ller. 2002. Probabilistic context-free grammars for
phonology. In Proceedings of the Workshop on Morpholog-
ical and Phonological Learning at ACL.
R. Neal and G. Hinton, 1998. A New View of the EM Algorithm
That Justifies Incremental and Other Variants, pages 355?
368. Kluwer.
A. Prince and P. Smolensky. 1993. Optimality theory: Con-
straint interaction in generative grammar. Technical Report
TR-2, Rutgers Center for Cognitive Science, Rutgers Univ.
A. van den Bosch, T. Weijters, and W. Daelemans. 1998. Mod-
ularity in inductively-learned word pronunciation systems.
In New Methods in Language Processing and Computational
Language Learning (NeMLaP3/CoNLL98).
A. Venkataraman. 2001. A statistical model for word dis-
covery in transcribed speech. Computational Linguistics,
27(3):351?372.
119
Proceedings of NAACL HLT 2007, pages 139?146,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Bayesian Inference for PCFGs via Markov chain Monte Carlo
Mark Johnson
Cognitive and Linguistic Sciences
Brown University
Mark Johnson@brown.edu
Thomas L. Griffiths
Department of Psychology
University of California, Berkeley
Tom Griffiths@berkeley.edu
Sharon Goldwater
Department of Linguistics
Stanford University
sgwater@stanford.edu
Abstract
This paper presents two Markov chain
Monte Carlo (MCMC) algorithms for
Bayesian inference of probabilistic con-
text free grammars (PCFGs) from ter-
minal strings, providing an alternative
to maximum-likelihood estimation using
the Inside-Outside algorithm. We illus-
trate these methods by estimating a sparse
grammar describing the morphology of
the Bantu language Sesotho, demonstrat-
ing that with suitable priors Bayesian
techniques can infer linguistic structure
in situations where maximum likelihood
methods such as the Inside-Outside algo-
rithm only produce a trivial grammar.
1 Introduction
The standard methods for inferring the parameters of
probabilistic models in computational linguistics are
based on the principle of maximum-likelihood esti-
mation; for example, the parameters of Probabilistic
Context-Free Grammars (PCFGs) are typically es-
timated from strings of terminals using the Inside-
Outside (IO) algorithm, an instance of the Ex-
pectation Maximization (EM) procedure (Lari and
Young, 1990). However, much recent work in ma-
chine learning and statistics has turned away from
maximum-likelihood in favor of Bayesian methods,
and there is increasing interest in Bayesian methods
in computational linguistics as well (Finkel et al,
2006). This paper presents two Markov chain Monte
Carlo (MCMC) algorithms for inferring PCFGs and
their parses from strings alone. These can be viewed
as Bayesian alternatives to the IO algorithm.
The goal of Bayesian inference is to compute a
distribution over plausible parameter values. This
?posterior? distribution is obtained by combining the
likelihood with a ?prior? distribution P(?) over pa-
rameter values ?. In the case of PCFG inference ? is
the vector of rule probabilities, and the prior might
assert a preference for a sparse grammar (see be-
low). The posterior probability of each value of ?
is given by Bayes? rule:
P(?|D) ? P(D|?)P(?). (1)
In principle Equation 1 defines the posterior prob-
ability of any value of ?, but computing this may
not be tractable analytically or numerically. For this
reason a variety of methods have been developed to
support approximate Bayesian inference. One of the
most popular methods is Markov chain Monte Carlo
(MCMC), in which a Markov chain is used to sam-
ple from the posterior distribution.
This paper presents two new MCMC algorithms
for inferring the posterior distribution over parses
and rule probabilities given a corpus of strings. The
first algorithm is a component-wise Gibbs sampler
which is very similar in spirit to the EM algo-
rithm, drawing parse trees conditioned on the cur-
rent parameter values and then sampling the param-
eters conditioned on the current set of parse trees.
The second algorithm is a component-wise Hastings
sampler that ?collapses? the probabilistic model, in-
tegrating over the rule probabilities of the PCFG,
with the goal of speeding convergence. Both algo-
139
rithms use an efficient dynamic programming tech-
nique to sample parse trees.
Given their usefulness in other disciplines, we
believe that Bayesian methods like these are likely
to be of general utility in computational linguis-
tics as well. As a simple illustrative example, we
use these methods to infer morphological parses for
verbs from Sesotho, a southern Bantu language with
agglutinating morphology. Our results illustrate that
Bayesian inference using a prior that favors sparsity
can produce linguistically reasonable analyses in sit-
uations in which EM does not.
The rest of this paper is structured as follows.
The next section introduces the background for our
paper, summarizing the key ideas behind PCFGs,
Bayesian inference, and MCMC. Section 3 intro-
duces our first MCMC algorithm, a Gibbs sampler
for PCFGs. Section 4 describes an algorithm for
sampling trees from the distribution over trees de-
fined by a PCFG. Section 5 shows how to integrate
out the rule weight parameters ? in a PCFG, allow-
ing us to sample directly from the posterior distribu-
tion over parses for a corpus of strings. Finally, Sec-
tion 6 illustrates these methods in learning Sesotho
morphology.
2 Background
2.1 Probabilistic context-free grammars
Let G = (T,N, S,R) be a Context-Free Grammar
in Chomsky normal form with no useless produc-
tions, where T is a finite set of terminal symbols, N
is a finite set of nonterminal symbols (disjoint from
T ), S ? N is a distinguished nonterminal called the
start symbol, and R is a finite set of productions of
the form A ? BC or A ? w, where A,B,C ? N
and w ? T . In what follows we use ? as a variable
ranging over (N ?N) ? T .
A Probabilistic Context-Free Grammar (G, ?) is
a pair consisting of a context-free grammar G and
a real-valued vector ? of length |R| indexed by pro-
ductions, where ?A?? is the production probability
associated with the production A ? ? ? R. We
require that ?A?? ? 0 and that for all nonterminals
A ? N , ?A???R ?A?? = 1.
A PCFG (G, ?) defines a probability distribution
over trees t as follows:
PG(t|?) =
?
r?R
?fr(t)r
where t is generated by G and fr(t) is the number
of times the production r = A ? ? ? R is used
in the derivation of t. If G does not generate t let
PG(t|?) = 0. The yield y(t) of a parse tree t is
the sequence of terminals labeling its leaves. The
probability of a string w ? T+ of terminals is the
sum of the probability of all trees with yield w, i.e.:
PG(w|?) =
?
t:y(t)=w
PG(t|?).
2.2 Bayesian inference for PCFGs
Given a corpus of strings w = (w1, . . . , wn), where
each wi is a string of terminals generated by a known
CFG G, we would like to be able to infer the pro-
duction probabilities ? that best describe that corpus.
Taking w to be our data, we can apply Bayes? rule
(Equation 1) to obtain:
P(?|w) ? PG(w|?)P(?), where
PG(w|?) =
n
?
i=1
PG(wi|?).
Using t to denote a sequence of parse trees for w,
we can compute the joint posterior distribution over
t and ?, and then marginalize over t, with P(?|w) =
?
t P(t, ?|w). The joint posterior distribution on t
and ? is given by:
P(t, ?|w) ? P(w|t)P(t|?)P(?)
=
( n
?
i=1
P(wi|ti)P(ti|?)
)
P(?)
with P(wi|ti) = 1 if y(ti) = wi, and 0 otherwise.
2.3 Dirichlet priors
The first step towards computing the posterior dis-
tribution is to define a prior on ?. We take P(?) to
be a product of Dirichlet distributions, with one dis-
tribution for each non-terminal A ? N . The prior
is parameterized by a positive real valued vector ?
indexed by productions R, so each production prob-
ability ?A?? has a corresponding Dirichlet param-
eter ?A??. Let RA be the set of productions in R
140
with left-hand side A, and let ?A and ?A refer to
the component subvectors of ? and ? respectively
indexed by productions in RA. The Dirichlet prior
PD(?|?) is:
PD(?|?) =
?
A?N
PD(?A|?A), where
PD(?A|?A) =
1
C(?A)
?
r?RA
??r?1r and
C(?A) =
?
r?RA ?(?r)
?(?r?RA ?r)
(2)
where ? is the generalized factorial function and
C(?) is a normalization constant that does not de-
pend on ?A.
Dirichlet priors are useful because they are con-
jugate to the distribution over trees defined by a
PCFG. This means that the posterior distribution
on ? given a set of parse trees, P(?|t, ?), is also a
Dirichlet distribution. Applying Bayes? rule,
PG(?|t, ?) ? PG(t|?) PD(?|?)
?
(
?
r?R
?fr(t)r
)(
?
r?R
??r?1r
)
=
?
r?R
?fr(t)+?r?1r
which is a Dirichlet distribution with parameters
f(t) + ?, where f(t) is the vector of production
counts in t indexed by r ? R. We can thus write:
PG(?|t, ?) = PD(?|f(t) + ?)
which makes it clear that the production counts com-
bine directly with the parameters of the prior.
2.4 Markov chain Monte Carlo
Having defined a prior on ?, the posterior distribu-
tion over t and ? is fully determined by a corpus
w. Unfortunately, computing the posterior probabil-
ity of even a single choice of t and ? is intractable,
as evaluating the normalizing constant for this dis-
tribution requires summing over all possible parses
for the entire corpus and all sets of production prob-
abilities. Nonetheless, it is possible to define al-
gorithms that sample from this distribution using
Markov chain Monte Carlo (MCMC).
MCMC algorithms construct a Markov chain
whose states s ? S are the objects we wish to sam-
ple. The state space S is typically astronomically
large ? in our case, the state space includes all pos-
sible parses of the entire training corpus w ? and
the transition probabilities P(s?|s) are specified via a
scheme guaranteed to converge to the desired distri-
bution ?(s) (in our case, the posterior distribution).
We ?run? the Markov chain (i.e., starting in initial
state s0, sample a state s1 from P(s?|s0), then sam-
ple state s2 from P(s?|s1), and so on), with the prob-
ability that the Markov chain is in a particular state,
P(si), converging to ?(si) as i ??.
After the chain has run long enough for it to ap-
proach its stationary distribution, the expectation
E?[f ] of any function f(s) of the state s will be
approximated by the average of that function over
the set of sample states produced by the algorithm.
For example, in our case, given samples (ti, ?i) for
i = 1, . . . , ? produced by an MCMC algorithm, we
can estimate ? as
E?[?] ?
1
?
?
?
i=1
?i
The remainder of this paper presents two MCMC
algorithms for PCFGs. Both algorithms proceed by
setting the initial state of the Markov chain to a guess
for (t, ?) and then sampling successive states using
a particular transition matrix. The key difference be-
twen the two algorithms is the form of the transition
matrix they assume.
3 A Gibbs sampler for P(t, ?|w, ?)
The Gibbs sampler (Geman and Geman, 1984) is
one of the simplest MCMC methods, in which tran-
sitions between states of the Markov chain result
from sampling each component of the state condi-
tioned on the current value of all other variables. In
our case, this means alternating between sampling
from two distributions:
P(t|?,w, ?) =
n
?
i=1
P(ti|wi, ?), and
P(?|t,w, ?) = PD(?|f(t) + ?)
=
?
A?N
PD(?A|fA(t) + ?A).
Thus every two steps we generate a new sample of
t and ?. This alternation between parsing and up-
dating ? is reminiscent of the EM algorithm, with
141
tit1 tn
w1 wi wn
?Aj. . .?A1 . . . ?A|N|
?A1 . . .. . . ?Aj ?A|N|
. . .
. . .. . .
. . .
Figure 1: A Bayes net representation of dependen-
cies among the variables in a PCFG.
the Expectation step replaced by sampling t and the
Maximization step replaced by sampling ?.
The dependencies among variables in a PCFG are
depicted graphically in Figure 1, which makes clear
that the Gibbs sampler is highly parallelizable (just
like the EM algorithm). Specifically, the parses ti
are independent given ? and so can be sampled in
parallel from the following distribution as described
in the next section.
PG(ti|wi, ?) =
PG(ti|?)
PG(wi|?)
We make use of the fact that the posterior is a
product of independent Dirichlet distributions in or-
der to sample ? from PD(?|t, ?). The production
probabilities ?A for each nonterminal A ? N are
sampled from a Dirchlet distibution with parameters
??A = fA(t) + ?A. There are several methods for
sampling ? = (?1, . . . , ?m) from a Dirichlet distri-
bution with parameters ? = (?1, . . . , ?m), with the
simplest being sampling xj from a Gamma(?j) dis-
tribution for j = 1, . . . ,m and then setting ?j =
xj/
?m
k=1 xk (Gentle, 2003).
4 Efficiently sampling from P(t|w, ?)
This section completes the description of the Gibbs
sampler for (t, ?) by describing a dynamic program-
ming algorithm for sampling trees from the set of
parses for a string generated by a PCFG. This al-
gorithm appears fairly widely known: it was de-
scribed by Goodman (1998) and Finkel et al(2006)
and used by Ding et al(2005), and is very simi-
lar to other dynamic programming algorithms for
CFGs, so we only summarize it here. The algo-
rithm consists of two steps. The first step con-
structs a standard ?inside? table or chart, as used in
the Inside-Outside algorithm for PCFGs (Lari and
Young, 1990). The second step involves a recursion
from larger to smaller strings, sampling from the
productions that expand each string and construct-
ing the corresponding tree in a top-down fashion.
In this section we take w to be a string of terminal
symbols w = (w1, . . . , wn) where each wi ? T ,
and define wi,k = (wi+1, . . . , wk) (i.e., the sub-
string from wi+1 up to wk). Further, let GA =
(T,N,A,R), i.e., a CFG just like G except that the
start symbol has been replaced with A, so, PGA(t|?)
is the probability of a tree t whose root node is la-
beled A and PGA(w|?) is the sum of the probabili-
ties of all trees whose root nodes are labeled A with
yield w.
The Inside algorithm takes as input a PCFG
(G, ?) and a string w = w0,n and constructs a ta-
ble with entries pA,i,k for each A ? N and 0 ?
i < k ? n, where pA,i,k = PGA(wi,k|?), i.e., the
probability of A rewriting to wi,k. The table entries
are recursively defined below, and computed by enu-
merating all feasible i, k and A in any order such that
all smaller values of k?i are enumerated before any
larger values.
pA,k?1,k = ?A?wk
pA,i,k =
?
A?B C?R
?
i<j<k
?A?B C pB,i,j pC,j,k
for all A,B,C ? N and 0 ? i < j < k ? n. At the
end of the Inside algorithm, PG(w|?) = pS,0,n.
The second step of the sampling algorithm uses
the function SAMPLE, which returns a sample from
PG(t|w, ?) given the PCFG (G, ?) and the inside
table pA,i,k. SAMPLE takes as arguments a non-
terminal A ? N and a pair of string positions
0 ? i < k ? n and returns a tree drawn from
PGA(t|wi,k, ?). It functions in a top-down fashion,
selecting the production A ? BC to expand the A,
and then recursively calling itself to expand B and
C respectively.
function SAMPLE(A, i, k) :
if k ? i = 1 then return TREE(A,wk)
(j,B,C) = MULTI(A, i, k)
return TREE(A, SAMPLE(B, i, j), SAMPLE(C, j, k))
In this pseudo-code, TREE is a function that con-
structs unary or binary tree nodes respectively, and
142
MULTI is a function that produces samples from
a multinomial distribution over the possible ?split?
positions j and nonterminal children B and C ,
where:
P(j,B,C) = ?A?BC PGB (wi,j|?) PGC (wj,k|?)PGA(wi,k|?)
5 A Hastings sampler for P(t|w, ?)
The Gibbs sampler described in Section 3 has
the disadvantage that each sample of ? re-
quires reparsing the training corpus w. In
this section, we describe a component-wise
Hastings algorithm for sampling directly from
P(t|w, ?), marginalizing over the produc-
tion probabilities ?. Transitions between
states are produced by sampling parses ti from
P(ti|wi, t?i, ?) for each string wi in turn, where
t?i = (t1, . . . , ti?1, ti+1, . . . , tn) is the current set
of parses for w?i = (w1, . . . , wi?1, wi+1, . . . , wn).
Marginalizing over ? effectively means that the
production probabilities are updated after each
sentence is parsed, so it is reasonable to expect
that this algorithm will converge faster than the
Gibbs sampler described earlier. While the sampler
does not explicitly provide samples of ?, the results
outlined in Sections 2.3 and 3 can be used to sample
the posterior distribution over ? for each sample of
t if required.
Let PD(?|?) be a Dirichlet product prior, and let
? be the probability simplex for ?. Then by inte-
grating over the posterior Dirichlet distributions we
have:
P(t|?) =
?
?
PG(t|?)PD(?|?)d?
=
?
A?N
C(?A + fA(t))
C(?A)
(3)
where C was defined in Equation 2. Because we
are marginalizing over ?, the trees ti become depen-
dent upon one another. Intuitively, this is because
wi may provide information about ? that influences
how some other string wj should be parsed.
We can use Equation 3 to compute the conditional
probability P(ti|t?i, ?) as follows:
P(ti|t?i, ?) =
P(t|?)
P(t?i|?)
=
?
A?N
C(?A + fA(t))
C(?A + fA(t?i))
Now, if we could sample from
P(ti|wi, t?i, ?) =
P(wi|ti)P(ti|t?i, ?)
P(wi|t?i, ?)
we could construct a Gibbs sampler whose states
were the parse trees t. Unfortunately, we don?t even
know if there is an efficient algorithm for calculat-
ing P(wi|t?i, ?), let alne an efficient sampling al-
gorithm for this distribution.
Fortunately, this difficulty is not fatal. A Hast-
ings sampler for a probability distribution ?(s) is
an MCMC algorithm that makes use of a proposal
distribution Q(s?|s) from which it draws samples,
and uses an acceptance/rejection scheme to define a
transition kernel with the desired distribution ?(s).
Specifically, given the current state s, a sample s? 6=
s drawn from Q(s?|s) is accepted as the next state
with probability
A(s, s?) = min
{
1, ?(s
?)Q(s|s?)
?(s)Q(s?|s)
}
and with probability 1 ?A(s, s?) the proposal is re-
jected and the next state is the current state s.
We use a component-wise proposal distribution,
generating new proposed values for ti, where i is
chosen at random. Our proposal distribution is the
posterior distribution over parse trees generated by
the PCFG with grammar G and production proba-
bilities ??, where ?? is chosen based on the current
t?i as described below. Each step of our Hastings
sampler is as follows. First, we compute ?? from
t?i as described below. Then we sample t?i from
P(ti|wi, ??) using the algorithm described in Sec-
tion 4. Finally, we accept the proposal t?i given the
old parse ti for wi with probability:
A(ti, t?i) = min
{
1, P(t
?
i|wi, t?i, ?)P(ti|wi, ??)
P(ti|wi, t?i, ?)P(t?i|wi, ??)
}
= min
{
1, P(t
?
i|t?i, ?)P(ti|wi, ??)
P(ti|t?i, ?)P(t?i|wi, ??)
}
The key advantage of the Hastings sampler over the
Gibbs sampler here is that because the acceptance
probability is a ratio of probabilities, the difficult to
143
compute P(wi|t?i, ?) is a common factor of both
the numerator and denominator, and hence is not re-
quired. The P (wi|ti) term also disappears, being 1
for both the numerator and the denominator since
our proposal distribution can only generate trees for
which wi is the yield.
All that remains is to specify the production prob-
abilities ?? of the proposal distribution P(t?i|wi, ??).
While the acceptance rule used in the Hastings
algorithm ensures that it produces samples from
P(ti|wi, t?i, ?) with any proposal grammar ?? in
which all productions have nonzero probability, the
algorithm is more efficient (i.e., fewer proposals are
rejected) if the proposal distribution is close to the
distribution to be sampled.
Given the observations above about the corre-
spondence between terms in P(ti|t?i, ?) and the
relative frequency of the corresponding productions
in t?i, we set ?? to the expected value E[?|t?i, ?] of
? given t?i and ? as follows:
??r =
fr(t?i) + ?r
?
r??RA fr?(t?i) + ?r?
6 Inferring sparse grammars
As stated in the introduction, the primary contribu-
tion of this paper is introducing MCMC methods
for Bayesian inference to computational linguistics.
Bayesian inference using MCMC is a technique of
generic utility, much like Expectation-Maximization
and other general inference techniques, and we be-
lieve that it belongs in every computational linguist?s
toolbox alongside these other techniques.
Inferring a PCFG to describe the syntac-
tic structure of a natural language is an obvi-
ous application of grammar inference techniques,
and it is well-known that PCFG inference us-
ing maximum-likelihood techniques such as the
Inside-Outside (IO) algorithm, a dynamic program-
ming Expectation-Maximization (EM) algorithm for
PCFGs, performs extremely poorly on such tasks.
We have applied the Bayesian MCMC methods de-
scribed here to such problems and obtain results
very similar to those produced using IO. We be-
lieve that the primary reason why both IO and the
Bayesian methods perform so poorly on this task
is that simple PCFGs are not accurate models of
English syntactic structure. We know that PCFGs
? = (0.1, 1.0)
? = (0.5, 1.0)
? = (1.0, 1.0)
Binomial parameter ?1
P(?1|?)
10.80.60.40.20
5
4
3
2
1
0
Figure 2: A Dirichlet prior ? on a binomial parame-
ter ?1. As ?1 ? 0, P(?1|?) is increasingly concen-
trated around 0.
that represent only major phrasal categories ignore
a wide variety of lexical and syntactic dependen-
cies in natural language. State-of-the-art systems
for unsupervised syntactic structure induction sys-
tem uses models that are very different to these kinds
of PCFGs (Klein and Manning, 2004; Smith and
Eisner, 2006).1
Our goal in this section is modest: we aim merely
to provide an illustrative example of Bayesian infer-
ence using MCMC. As Figure 2 shows, when the
Dirichlet prior parameter ?r approaches 0 the prior
probability PD(?r|?) becomes increasingly concen-
trated around 0. This ability to bias the sampler
toward sparse grammars (i.e., grammars in which
many productions have probabilities close to 0) is
useful when we attempt to identify relevant produc-
tions from a much larger set of possible productions
via parameter estimation.
The Bantu language Sesotho is a richly agglutina-
tive language, in which verbs consist of a sequence
of morphemes, including optional Subject Markers
(SM), Tense (T), Object Markers (OM), Mood (M)
and derivational affixes as well as the obligatory
Verb stem (V), as shown in the following example:
re
SM
-a
T
-di
OM
-bon
V
-a
M
?We see them?
1It is easy to demonstrate that the poor quality of the PCFG
models is the cause of these problems rather than search or other
algorithmic issues. If one initializes either the IO or Bayesian
estimation procedures with treebank parses and then runs the
procedure using the yields alone, the accuracy of the parses uni-
formly decreases while the (posterior) likelihood uniformly in-
creases with each iteration, demonstrating that improving the
(posterior) likelihood of such models does not improve parse
accuracy.
144
We used an implementation of the Hastings sampler
described in Section 5 to infer morphological parses
t for a corpus w of 2,283 unsegmented Sesotho
verb types extracted from the Sesotho corpus avail-
able from CHILDES (MacWhinney and Snow, 1985;
Demuth, 1992). We chose this corpus because the
words have been morphologically segmented manu-
ally, making it possible for us to evaluate the mor-
phological parses produced by our system. We con-
structed a CFG G containing the following produc-
tions
Word ? V
Word ? V M
Word ? SM V M
Word ? SM T V M
Word ? SM T OM V M
together with productions expanding the pretermi-
nals SM,T,OM,V and M to each of the 16,350 dis-
tinct substrings occuring anywhere in the corpus,
producting a grammar with 81,755 productions in
all. In effect, G encodes the basic morphologi-
cal structure of the Sesotho verb (ignoring factors
such as derivation morphology and irregular forms),
but provides no information about the phonological
identity of the morphemes.
Note that G actually generates a finite language.
However, G parameterizes the probability distribu-
tion over the strings it generates in a manner that
would be difficult to succintly characterize except
in terms of the productions given above. Moreover,
with approximately 20 times more productions than
training strings, each string is highly ambiguous and
estimation is highly underconstrained, so it provides
an excellent test-bed for sparse priors.
We estimated the morphological parses t in two
ways. First, we ran the IO algorithm initialized
with a uniform initial estimate ?0 for ? to produce
an estimate of the MLE ??, and then computed the
Viterbi parses t? of the training corpus w with respect
to the PCFG (G, ??). Second, we ran the Hastings
sampler initialized with trees sampled from (G, ?0)
with several different values for the parameters of
the prior. We experimented with a number of tech-
niques for speeding convergence of both the IO and
Hastings algorithms, and two of these were particu-
larly effective on this problem. Annealing, i.e., us-
ing P(t|w)1/? in place of P(t|w) where ? is a ?tem-
perature? parameter starting around 5 and slowly ad-
justed toward 1, sped the convergence of both algo-
rithms. We ran both algorithms for several thousand
iterations over the corpus, and both seemed to con-
verge fairly quickly once ? was set to 1. ?Jittering?
the initial estimate of ? used in the IO algorithm also
sped its convergence.
The IO algorithm converges to a solution where
?Word? V = 1, and every string w ? w is analysed
as a single morpheme V. (In fact, in this grammar
P(wi|?) is the empirical probability of wi, and it is
easy to prove that this ? is the MLE).
The samples t produced by the Hastings algo-
rithm depend on the parameters of the Dirichlet
prior. We set ?r to a single value ? for all pro-
ductions r. We found that for ? > 10?2 the sam-
ples produced by the Hastings algorithm were the
same trivial analyses as those produced by the IO
algorithm, but as ? was reduced below this t be-
gan to exhibit nontrivial structure. We evaluated
the quality of the segmentations in the morpholog-
ical analyses t in terms of unlabeled precision, re-
call, f-score and exact match (the fraction of words
correctly segmented into morphemes; we ignored
morpheme labels because the manual morphological
analyses contain many morpheme labels that we did
not include in G). Figure 3 contains a plot of how
these quantities vary with ?; obtaining an f-score of
0.75 and an exact word match accuracy of 0.54 at
? = 10?5 (the corresponding values for the MLE ??
are both 0). Note that we obtained good results as ?
was varied over several orders of magnitude, so the
actual value of ? is not critical. Thus in this appli-
cation the ability to prefer sparse grammars enables
us to find linguistically meaningful analyses. This
ability to find linguistically meaningful structure is
relatively rare in our experience with unsupervised
PCFG induction.
We also experimented with a version of IO modi-
fied to perform Bayesian MAP estimation, where the
Maximization step of the IO procedure is replaced
with Bayesian inference using a Dirichlet prior, i.e.,
where the rule probabilities ?(k) at iteration k are es-
timated using:
?(k)r ? max(0,E[fr|w, ?(k?1)] + ?? 1).
Clearly such an approach is very closely related to
the Bayesian procedures presented in this article,
145
Exact
Recall
Precision
F-score
Dirichlet prior parameter ?r
1 0.01 1e-04 1e-06 1e-08 1e-10
1
0.75
0.5
0.25
0
Figure 3: Accuracy of morphological segmentations
of Sesotho verbs proposed by the Hastings algo-
rithms as a function of Dirichlet prior parameter
?. F-score, precision and recall are unlabeled mor-
pheme scores, while Exact is the fraction of words
correctly segmented.
and in some circumstances this may be a useful
estimator. However, in our experiments with the
Sesotho data above we found that for the small val-
ues of ? necessary to obtain a sparse solution,the
expected rule count E[fr] for many rules r was less
than 1??. Thus on the next iteration ?r = 0, result-
ing in there being no parse whatsoever for many of
the strings in the training data. Variational Bayesian
techniques offer a systematic way of dealing with
these problems, but we leave this for further work.
7 Conclusion
This paper has described basic algorithms for per-
forming Bayesian inference over PCFGs given ter-
minal strings. We presented two Markov chain
Monte Carlo algorithms (a Gibbs and a Hastings
sampling algorithm) for sampling from the posterior
distribution over parse trees given a corpus of their
yields and a Dirichlet product prior over the produc-
tion probabilities. As a component of these algo-
rithms we described an efficient dynamic program-
ming algorithm for sampling trees from a PCFG
which is useful in its own right. We used these
sampling algorithms to infer morphological analy-
ses of Sesotho verbs given their strings (a task on
which the standard Maximum Likelihood estimator
returns a trivial and linguistically uninteresting so-
lution), achieving 0.75 unlabeled morpheme f-score
and 0.54 exact word match accuracy. Thus this
is one of the few cases we are aware of in which
a PCFG estimation procedure returns linguistically
meaningful structure. We attribute this to the ability
of the Bayesian prior to prefer sparse grammars.
We expect that these algorithms will be of inter-
est to the computational linguistics community both
because a Bayesian approach to PCFG estimation is
more flexible than the Maximum Likelihood meth-
ods that currently dominate the field (c.f., the use
of a prior as a bias towards sparse solutions), and
because these techniques provide essential building
blocks for more complex models.
References
Katherine Demuth. 1992. Acquisition of Sesotho. In Dan
Slobin, editor, The Cross-Linguistic Study of Language Ac-
quisition, volume 3, pages 557?638. Lawrence Erlbaum As-
sociates, Hillsdale, N.J.
Ye Ding, Chi Yu Chan, and Charles E. Lawrence. 2005. RNA
secondary structure prediction by centroids in a Boltzmann
weighted ensemble. RNA, 11:1157?1166.
Jenny Rose Finkel, Christopher D. Manning, and Andrew Y.
Ng. 2006. Solving the problem of cascading errors:
Approximate Bayesian inference for linguistic annotation
pipelines. In Proceedings of the 2006 Conference on Empir-
ical Methods in Natural Language Processing, pages 618?
626, Sydney, Australia. Association for Computational Lin-
guistics.
Stuart Geman and Donald Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of images.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 6:721?741.
James E. Gentle. 2003. Random number generation and Monte
Carlo methods. Springer, New York, 2nd edition.
Joshua Goodman. 1998. Parsing inside-out.
Ph.D. thesis, Harvard University. available from
http://research.microsoft.com/?joshuago/.
Dan Klein and Chris Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and con-
stituency. In Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics, pages 478?485.
K. Lari and S.J. Young. 1990. The estimation of Stochastic
Context-Free Grammars using the Inside-Outside algorithm.
Computer Speech and Language, 4(35-56).
Brian MacWhinney and Catherine Snow. 1985. The child lan-
guage data exchange system. Journal of Child Language,
12:271?296.
Noah A. Smith and Jason Eisner. 2006. Annealing structural
bias in multilingual weighted grammar induction. In Pro-
ceedings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 569?576, Sydney,
Australia. Association for Computational Linguistics.
146
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 744?751,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Fully Bayesian Approach to Unsupervised Part-of-Speech Tagging?
Sharon Goldwater
Department of Linguistics
Stanford University
sgwater@stanford.edu
Thomas L. Griffiths
Department of Psychology
UC Berkeley
tom griffiths@berkeley.edu
Abstract
Unsupervised learning of linguistic structure
is a difficult problem. A common approach
is to define a generative model and max-
imize the probability of the hidden struc-
ture given the observed data. Typically,
this is done using maximum-likelihood es-
timation (MLE) of the model parameters.
We show using part-of-speech tagging that
a fully Bayesian approach can greatly im-
prove performance. Rather than estimating
a single set of parameters, the Bayesian ap-
proach integrates over all possible parame-
ter values. This difference ensures that the
learned structure will have high probability
over a range of possible parameters, and per-
mits the use of priors favoring the sparse
distributions that are typical of natural lan-
guage. Our model has the structure of a
standard trigram HMM, yet its accuracy is
closer to that of a state-of-the-art discrimi-
native model (Smith and Eisner, 2005), up
to 14 percentage points better than MLE. We
find improvements both when training from
data alone, and using a tagging dictionary.
1 Introduction
Unsupervised learning of linguistic structure is a dif-
ficult problem. Recently, several new model-based
approaches have improved performance on a vari-
ety of tasks (Klein and Manning, 2002; Smith and
?This work was supported by grants NSF 0631518 and
ONR MURI N000140510388. We would also like to thank
Noah Smith for providing us with his data sets.
Eisner, 2005). Nearly all of these approaches have
one aspect in common: the goal of learning is to
identify the set of model parameters that maximizes
some objective function. Values for the hidden vari-
ables in the model are then chosen based on the
learned parameterization. Here, we propose a dif-
ferent approach based on Bayesian statistical prin-
ciples: rather than searching for an optimal set of
parameter values, we seek to directly maximize the
probability of the hidden variables given the ob-
served data, integrating over all possible parame-
ter values. Using part-of-speech (POS) tagging as
an example application, we show that the Bayesian
approach provides large performance improvements
over maximum-likelihood estimation (MLE) for the
same model structure. Two factors can explain the
improvement. First, integrating over parameter val-
ues leads to greater robustness in the choice of tag
sequence, since it must have high probability over
a range of parameters. Second, integration permits
the use of priors favoring sparse distributions, which
are typical of natural language. These kinds of pri-
ors can lead to degenerate solutions if the parameters
are estimated directly.
Before describing our approach in more detail,
we briefly review previous work on unsupervised
POS tagging. Perhaps the most well-known is that
of Merialdo (1994), who used MLE to train a tri-
gram hidden Markov model (HMM). More recent
work has shown that improvements can be made
by modifying the basic HMM structure (Banko and
Moore, 2004), using better smoothing techniques or
added constraints (Wang and Schuurmans, 2005), or
using a discriminative model rather than an HMM
744
(Smith and Eisner, 2005). Non-model-based ap-
proaches have also been proposed (Brill (1995); see
also discussion in Banko and Moore (2004)). All of
this work is really POS disambiguation: learning is
strongly constrained by a dictionary listing the al-
lowable tags for each word in the text. Smith and
Eisner (2005) also present results using a diluted
dictionary, where infrequent words may have any
tag. Haghighi and Klein (2006) use a small list of
labeled prototypes and no dictionary.
A different tradition treats the identification of
syntactic classes as a knowledge-free clustering
problem. Distributional clustering and dimen-
sionality reduction techniques are typically applied
when linguistically meaningful classes are desired
(Schu?tze, 1995; Clark, 2000; Finch et al, 1995);
probabilistic models have been used to find classes
that can improve smoothing and reduce perplexity
(Brown et al, 1992; Saul and Pereira, 1997). Unfor-
tunately, due to a lack of standard and informative
evaluation techniques, it is difficult to compare the
effectiveness of different clustering methods.
In this paper, we hope to unify the problems of
POS disambiguation and syntactic clustering by pre-
senting results for conditions ranging from a full tag
dictionary to no dictionary at all. We introduce the
use of a new information-theoretic criterion, varia-
tion of information (Meila?, 2002), which can be used
to compare a gold standard clustering to the clus-
tering induced from a tagger?s output, regardless of
the cluster labels. We also evaluate using tag ac-
curacy when possible. Our system outperforms an
HMM trained with MLE on both metrics in all cir-
cumstances tested, often by a wide margin. Its ac-
curacy in some cases is close to that of Smith and
Eisner?s (2005) discriminative model. Our results
show that the Bayesian approach is particularly use-
ful when learning is less constrained, either because
less evidence is available (corpus size is small) or
because the dictionary contains less information.
In the following section, we discuss the motiva-
tion for a Bayesian approach and present our model
and search procedure. Section 3 gives results illus-
trating how the parameters of the prior affect re-
sults, and Section 4 describes how to infer a good
choice of parameters from unlabeled data. Section 5
presents results for a range of corpus sizes and dic-
tionary information, and Section 6 concludes.
2 A Bayesian HMM
2.1 Motivation
In model-based approaches to unsupervised lan-
guage learning, the problem is formulated in terms
of identifying latent structure from data. We de-
fine a model with parameters ?, some observed vari-
ables w (the linguistic input), and some latent vari-
ables t (the hidden structure). The goal is to as-
sign appropriate values to the latent variables. Stan-
dard approaches do so by selecting values for the
model parameters, and then choosing the most prob-
able variable assignment based on those parame-
ters. For example, maximum-likelihood estimation
(MLE) seeks parameters ?? such that
?? = argmax
?
P (w|?), (1)
where P (w|?) = ?t P (w, t|?). Sometimes, a
non-uniform prior distribution over ? is introduced,
in which case ?? is the maximum a posteriori (MAP)
solution for ?:
?? = argmax
?
P (w|?)P (?). (2)
The values of the latent variables are then taken to
be those that maximize P (t|w, ??).
In contrast, the Bayesian approach we advocate in
this paper seeks to identify a distribution over latent
variables directly, without ever fixing particular val-
ues for the model parameters. The distribution over
latent variables given the observed data is obtained
by integrating over all possible values of ?:
P (t|w) =
?
P (t|w, ?)P (?|w)d?. (3)
This distribution can be used in various ways, in-
cluding choosing the MAP assignment to the latent
variables, or estimating expected values for them.
To see why integrating over possible parameter
values can be useful when inducing latent structure,
consider the following example. We are given a
coin, which may be biased (t = 1) or fair (t = 0),
each with probability .5. Let ? be the probability of
heads. If the coin is biased, we assume a uniform
distribution over ?, otherwise ? = .5. We observe
w, the outcomes of 10 coin flips, and we wish to de-
termine whether the coin is biased (i.e. the value of
745
t). Assume that we have a uniform prior on ?, with
p(?) = 1 for all ? ? [0, 1]. First, we apply the stan-
dard methodology of finding the MAP estimate for
? and then selecting the value of t that maximizes
P (t|w, ??). In this case, an elementary calculation
shows that the MAP estimate is ?? = nH/10, where
nH is the number of heads in w (likewise, nT is
the number of tails). Consequently, P (t|w, ??) favors
t = 1 for any sequence that does not contain exactly
five heads, and assigns equal probability to t = 1
and t = 0 for any sequence that does contain exactly
five heads ? a counterintuitive result. In contrast,
using some standard results in Bayesian analysis we
can show that applying Equation 3 yields
P (t = 1|w) = 1/
(
1 + 11!nH !nT !210
)
(4)
which is significantly less than .5 when nH = 5, and
only favors t = 1 for sequences where nH ? 8 or
nH ? 2. This intuitively sensible prediction results
from the fact that the Bayesian approach is sensitive
to the robustness of a choice of t to the value of ?,
as illustrated in Figure 1. Even though a sequence
with nH = 6 yields a MAP estimate of ?? = 0.6
(Figure 1 (a)), P (t = 1|w, ?) is only greater than
0.5 for a small range of ? around ?? (Figure 1 (b)),
meaning that the choice of t = 1 is not very robust to
variation in ?. In contrast, a sequence with nH = 8
favors t = 1 for a wide range of ? around ??. By
integrating over ?, Equation 3 takes into account the
consequences of possible variation in ?.
Another advantage of integrating over ? is that
it permits the use of linguistically appropriate pri-
ors. In many linguistic models, including HMMs,
the distributions over variables are multinomial. For
a multinomial with parameters ? = (?1, . . . , ?K), a
natural choice of prior is the K-dimensional Dirich-
let distribution, which is conjugate to the multino-
mial.1 For simplicity, we initially assume that all
K parameters (also known as hyperparameters) of
the Dirichlet distribution are equal to ?, i.e. the
Dirichlet is symmetric. The value of ? determines
which parameters ? will have high probability: when
? = 1, all parameter values are equally likely; when
? > 1, multinomials that are closer to uniform are
1A prior is conjugate to a distribution if the posterior has the
same form as the prior.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
?
 
P(
 ? 
| w
 
)
 
 
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.5
1
?
 
P(
 
t 
=
 
1 
| w
,
 
? 
)
 
 
 w = HHTHTTHHTH
 w = HHTHHHTHHH
 w = HHTHTTHHTH
 w = HHTHHHTHHH
(a)
(b)
Figure 1: The Bayesian approach to estimating the
value of a latent variable, t, from observed data, w,
chooses a value of t robust to uncertainty in ?. (a)
Posterior distribution on ? given w. (b) Probability
that t = 1 given w and ? as a function of ?.
preferred; and when ? < 1, high probability is as-
signed to sparse multinomials, where one or more
parameters are at or near 0.
Typically, linguistic structures are characterized
by sparse distributions (e.g., POS tags are followed
with high probability by only a few other tags, and
have highly skewed output distributions). Conse-
quently, it makes sense to use a Dirichlet prior with
? < 1. However, as noted by Johnson et al (2007),
this choice of ? leads to difficulties with MAP esti-
mation. For a sequence of draws x = (x1, . . . , xn)
from a multinomial distribution ? with observed
counts n1, . . . , nK , a symmetric Dirichlet(?) prior
over ? yields the MAP estimate ?k = nk+??1n+K(??1) .
When ? ? 1, standard MLE techniques such as
EM can be used to find the MAP estimate simply
by adding ?pseudocounts? of size ? ? 1 to each of
the expected counts nk at each iteration. However,
when ? < 1, the values of ? that set one or more
of the ?k equal to 0 can have infinitely high poste-
rior probability, meaning that MAP estimation can
yield degenerate solutions. If, instead of estimating
?, we integrate over all possible values, we no longer
encounter such difficulties. Instead, the probability
that outcome xi takes value k given previous out-
comes x?i = (x1, . . . , xi?1) is
P (k|x?i, ?) =
?
P (k|?)P (?|x?i, ?) d?
= nk + ?i? 1 + K? (5)
746
where nk is the number of times k occurred in x?i.
See MacKay and Peto (1995) for a derivation.
2.2 Model Definition
Our model has the structure of a standard trigram
HMM, with the addition of symmetric Dirichlet pri-
ors over the transition and output distributions:
ti|ti?1 = t, ti?2 = t?, ? (t,t
?) ? Mult(? (t,t?))
wi|ti = t, ?(t) ? Mult(?(t))
? (t,t?)|? ? Dirichlet(?)
?(t)|? ? Dirichlet(?)
where ti and wi are the ith tag and word. We assume
that sentence boundaries are marked with a distin-
guished tag. For a model with T possible tags, each
of the transition distributions ? (t,t?) has T compo-
nents, and each of the output distributions ?(t) has
Wt components, where Wt is the number of word
types that are permissible outputs for tag t. We will
use ? and ? to refer to the entire transition and out-
put parameter sets. This model assumes that the
prior over state transitions is the same for all his-
tories, and the prior over output distributions is the
same for all states. We relax the latter assumption in
Section 4.
Under this model, Equation 5 gives us
P (ti|t?i, ?) =
n(ti?2,ti?1,ti) + ?
n(ti?2,ti?1) + T?
(6)
P (wi|ti, t?i,w?i, ?) =
n(ti,wi) + ?
n(ti) + Wti?
(7)
where n(ti?2,ti?1,ti) and n(ti,wi) are the number of
occurrences of the trigram (ti?2, ti?1, ti) and the
tag-word pair (ti, wi) in the i ? 1 previously gener-
ated tags and words. Note that, by integrating out
the parameters ? and ?, we induce dependencies
between the variables in the model. The probabil-
ity of generating a particular trigram tag sequence
(likewise, output) depends on the number of times
that sequence (output) has been generated previ-
ously. Importantly, trigrams (and outputs) remain
exchangeable: the probability of a set of trigrams
(outputs) is the same regardless of the order in which
it was generated. The property of exchangeability is
crucial to the inference algorithm we describe next.
2.3 Inference
To perform inference in our model, we use Gibbs
sampling (Geman and Geman, 1984), a stochastic
procedure that produces samples from the posterior
distribution P (t|w, ?, ?) ? P (w|t, ?)P (t|?). We
initialize the tags at random, then iteratively resam-
ple each tag according to its conditional distribution
given the current values of all other tags. Exchange-
ability allows us to treat the current counts of the
other tag trigrams and outputs as ?previous? obser-
vations. The only complication is that resampling
a tag changes the identity of three trigrams at once,
and we must account for this in computing its condi-
tional distribution. The sampling distribution for ti
is given in Figure 2.
In Bayesian statistical inference, multiple samples
from the posterior are often used in order to obtain
statistics such as the expected values of model vari-
ables. For POS tagging, estimates based on multi-
ple samples might be useful if we were interested in,
for example, the probability that two words have the
same tag. However, computing such probabilities
across all pairs of words does not necessarily lead to
a consistent clustering, and the result would be diffi-
cult to evaluate. Using a single sample makes stan-
dard evaluation methods possible, but yields sub-
optimal results because the value for each tag is sam-
pled from a distribution, and some tags will be as-
signed low-probability values. Our solution is to
treat the Gibbs sampler as a stochastic search pro-
cedure with the goal of identifying the MAP tag se-
quence. This can be done using tempering (anneal-
ing), where a temperature of ? is equivalent to rais-
ing the probabilities in the sampling distribution to
the power of 1? . As ? approaches 0, even a single
sample will provide a good MAP estimate.
3 Fixed Hyperparameter Experiments
3.1 Method
Our initial experiments follow in the tradition begun
by Merialdo (1994), using a tag dictionary to con-
strain the possible parts of speech allowed for each
word. (This also fixes Wt, the number of possible
words for tag t.) The dictionary was constructed by
listing, for each word, all tags found for that word in
the entire WSJ treebank. For the experiments in this
section, we used a 24,000-word subset of the tree-
747
P (ti|t?i,w, ?, ?) ?
n(ti,wi) + ?
nti + Wti?
?
n(ti?2,ti?1,ti) + ?
n(ti?2,ti?1) + T?
?
n(ti?1,ti,ti+1) + I(ti?2 = ti?1 = ti = ti+1) + ?
n(ti?1,ti) + I(ti?2 = ti?1 = ti) + T?
?
n(ti,ti+1,ti+2) + I(ti?2 = ti = ti+2, ti?1 = ti+1) + I(ti?1 = ti = ti+1 = ti+2) + ?
n(ti,ti+1) + I(ti?2 = ti, ti?1 = ti+1) + I(ti?1 = ti = ti+1) + T?
Figure 2: Conditional distribution for ti. Here, t?i refers to the current values of all tags except for ti, I(.)
is a function that takes on the value 1 when its argument is true and 0 otherwise, and all counts nx are with
respect to the tag trigrams and tag-word pairs in (t?i,w?i).
bank as our unlabeled training corpus. 54.5% of the
tokens in this corpus have at least two possible tags,
with the average number of tags per token being 2.3.
We varied the values of the hyperparameters ? and
? and evaluated overall tagging accuracy. For com-
parison with our Bayesian HMM (BHMM) in this
and following sections, we also present results from
the Viterbi decoding of an HMM trained using MLE
by running EM to convergence (MLHMM). Where
direct comparison is possible, we list the scores re-
ported by Smith and Eisner (2005) for their condi-
tional random field model trained using contrastive
estimation (CRF/CE).2
For all experiments, we ran our Gibbs sampling
algorithm for 20,000 iterations over the entire data
set. The algorithm was initialized with a random tag
assignment and a temperature of 2, and the temper-
ature was gradually decreased to .08. Since our in-
ference procedure is stochastic, our reported results
are an average over 5 independent runs.
Results from our model for a range of hyperpa-
rameters are presented in Table 1. With the best
choice of hyperparameters (? = .003, ? = 1), we
achieve average tagging accuracy of 86.8%. This
far surpasses the MLHMM performance of 74.5%,
and is closer to the 90.1% accuracy of CRF/CE on
the same data set using oracle parameter selection.
The effects of ?, which determines the probabil-
2Results of CRF/CE depend on the set of features used and
the contrast neighborhood. In all cases, we list the best score
reported for any contrast neighborhood using trigram (but no
spelling) features. To ensure proper comparison, all corpora
used in our experiments consist of the same randomized sets of
sentences used by Smith and Eisner. Note that training on sets
of contiguous sentences from the beginning of the treebank con-
sistently improves our results, often by 1-2 percentage points or
more. MLHMM scores show less difference between random-
ized and contiguous corpora.
Value Value of ?
of ? .001 .003 .01 .03 .1 .3 1.0
.001 85.0 85.7 86.1 86.0 86.2 86.5 86.6
.003 85.5 85.5 85.8 86.6 86.7 86.7 86.8
.01 85.3 85.5 85.6 85.9 86.4 86.4 86.2
.03 85.9 85.8 86.1 86.2 86.6 86.8 86.4
.1 85.2 85.0 85.2 85.1 84.9 85.5 84.9
.3 84.4 84.4 84.6 84.4 84.5 85.7 85.3
1.0 83.1 83.0 83.2 83.3 83.5 83.7 83.9
Table 1: Percentage of words tagged correctly by
BHMM as a function of the hyperparameters ? and
?. Results are averaged over 5 runs on the 24k cor-
pus with full tag dictionary. Standard deviations in
most cases are less than .5.
ity of the transition distributions, are stronger than
the effects of ?, which determines the probability
of the output distributions. The optimal value of
.003 for ? reflects the fact that the true transition
probability matrix for this corpus is indeed sparse.
As ? grows larger, the model prefers more uniform
transition probabilities, which causes it to perform
worse. Although the true output distributions tend to
be sparse as well, the level of sparseness depends on
the tag (consider function words vs. content words
in particular). Therefore, a value of ? that accu-
rately reflects the most probable output distributions
for some tags may be a poor choice for other tags.
This leads to the smaller effect of ?, and suggests
that performance might be improved by selecting a
different ? for each tag, as we do in the next section.
A final point worth noting is that even when
? = ? = 1 (i.e., the Dirichlet priors exert no influ-
ence) the BHMM still performs much better than the
MLHMM. This result underscores the importance
of integrating over model parameters: the BHMM
identifies a sequence of tags that have high proba-
748
bility over a range of parameter values, rather than
choosing tags based on the single best set of para-
meters. The improved results of the BHMM demon-
strate that selecting a sequence that is robust to vari-
ations in the parameters leads to better performance.
4 Hyperparameter Inference
In our initial experiments, we experimented with dif-
ferent fixed values of the hyperparameters and re-
ported results based on their optimal values. How-
ever, choosing hyperparameters in this way is time-
consuming at best and impossible at worst, if there
is no gold standard available. Luckily, the Bayesian
approach allows us to automatically select values
for the hyperparameters by treating them as addi-
tional variables in the model. We augment the model
with priors over the hyperparameters (here, we as-
sume an improper uniform prior), and use a sin-
gle Metropolis-Hastings update (Gilks et al, 1996)
to resample the value of each hyperparameter after
each iteration of the Gibbs sampler. Informally, to
update the value of hyperparameter ?, we sample a
proposed new value ?? from a normal distribution
with ? = ? and ? = .1?. The probability of ac-
cepting the new value depends on the ratio between
P (t|w, ?) and P (t|w, ??) and a term correcting for
the asymmetric proposal distribution.
Performing inference on the hyperparameters al-
lows us to relax the assumption that every tag has
the same prior on its output distribution. In the ex-
periments reported in the following section, we used
two different versions of our model. The first ver-
sion (BHMM1) uses a single value of ? for all word
classes (as above); the second version (BHMM2)
uses a separate ?j for each tag class j.
5 Inferred Hyperparameter Experiments
5.1 Varying corpus size
In this set of experiments, we used the full tag dictio-
nary (as above), but performed inference on the hy-
perparameters. Following Smith and Eisner (2005),
we trained on four different corpora, consisting of
the first 12k, 24k, 48k, and 96k words of the WSJ
corpus. For all corpora, the percentage of ambigu-
ous tokens is 54%-55% and the average number of
tags per token is 2.3. Table 2 shows results for
the various models and a random baseline (averaged
Corpus size
Accuracy 12k 24k 48k 96k
random 64.8 64.6 64.6 64.6
MLHMM 71.3 74.5 76.7 78.3
CRF/CE 86.2 88.6 88.4 89.4
BHMM1 85.8 85.2 83.6 85.0
BHMM2 85.8 84.4 85.7 85.8
? < .7 .2 .6 .2
Table 2: Percentage of words tagged correctly
by the various models on different sized corpora.
BHMM1 and BHMM2 use hyperparameter infer-
ence; CRF/CE uses parameter selection based on an
unlabeled development set. Standard deviations (?)
for the BHMM results fell below those shown for
each corpus size.
over 5 random tag assignments). Hyperparameter
inference leads to slightly lower scores than are ob-
tained by oracle hyperparameter selection, but both
versions of BHMM are still far superior to MLHMM
for all corpus sizes. Not surprisingly, the advantages
of BHMM are most pronounced on the smallest cor-
pus: the effects of parameter integration and sensible
priors are stronger when less evidence is available
from the input. In the limit as corpus size goes to in-
finity, the BHMM and MLHMM will make identical
predictions.
5.2 Varying dictionary knowledge
In unsupervised learning, it is not always reasonable
to assume that a large tag dictionary is available. To
determine the effects of reduced or absent dictionary
information, we ran a set of experiments inspired
by those of Smith and Eisner (2005). First, we col-
lapsed the set of 45 treebank tags onto a smaller set
of 17 (the same set used by Smith and Eisner). We
created a full tag dictionary for this set of tags from
the entire treebank, and also created several reduced
dictionaries. Each reduced dictionary contains the
tag information only for words that appear at least
d times in the training corpus (the 24k corpus, for
these experiments). All other words are fully am-
biguous between all 17 classes. We ran tests with
d = 1, 2, 3, 5, 10, and ? (i.e., knowledge-free syn-
tactic clustering).
With standard accuracy measures, it is difficult to
749
Value of d
Accuracy 1 2 3 5 10 ?
random 69.6 56.7 51.0 45.2 38.6
MLHMM 83.2 70.6 65.5 59.0 50.9
CRF/CE 90.4 77.0 71.7
BHMM1 86.0 76.4 71.0 64.3 58.0
BHMM2 87.3 79.6 65.0 59.2 49.7
? < .2 .8 .6 .3 1.4
VI
random 2.65 3.96 4.38 4.75 5.13 7.29
MLHMM 1.13 2.51 3.00 3.41 3.89 6.50
BHMM1 1.09 2.44 2.82 3.19 3.47 4.30
BHMM2 1.04 1.78 2.31 2.49 2.97 4.04
? < .02 .03 .04 .03 .07 .17
Corpus stats
% ambig. 49.0 61.3 66.3 70.9 75.8 100
tags/token 1.9 4.4 5.5 6.8 8.3 17
Table 3: Percentage of words tagged correctly and
variation of information between clusterings in-
duced by the assigned and gold standard tags as the
amount of information in the dictionary is varied.
Standard deviations (?) for the BHMM results fell
below those shown in each column. The percentage
of ambiguous tokens and average number of tags per
token for each value of d is also shown.
evaluate the quality of a syntactic clustering when
no dictionary is used, since cluster names are inter-
changeable. We therefore introduce another evalua-
tion measure for these experiments, a distance met-
ric on clusterings known as variation of information
(Meila?, 2002). The variation of information (VI) be-
tween two clusterings C (the gold standard) and C ?
(the found clustering) of a set of data points is a sum
of the amount of information lost in moving from C
to C ?, and the amount that must be gained. It is de-
fined in terms of entropy H and mutual information
I: V I(C,C ?) = H(C)+H(C ?)? 2I(C,C ?). Even
when accuracy can be measured, VI may be more in-
formative: two different tag assignments may have
the same accuracy but different VI with respect to
the gold standard if the errors in one assignment are
less consistent than those in the other.
Table 3 gives the results for this set of experi-
ments. One or both versions of BHMM outperform
MLHMM in terms of tag accuracy for all values of
d, although the differences are not as great as in ear-
lier experiments. The differences in VI are more
striking, particularly as the amount of dictionary in-
formation is reduced. When ambiguity is greater,
both versions of BHMM show less confusion with
respect to the true tags than does MLHMM, and
BHMM2 performs the best in all circumstances. The
confusion matrices in Figure 3 provide a more intu-
itive picture of the very different sorts of clusterings
produced by MLHMM and BHMM2 when no tag
dictionary is available. Similar differences hold to a
lesser degree when a partial dictionary is provided.
With MLHMM, different tokens of the same word
type are usually assigned to the same cluster, but
types are assigned to clusters more or less at ran-
dom, and all clusters have approximately the same
number of types (542 on average, with a standard
deviation of 174). The clusters found by BHMM2
tend to be more coherent and more variable in size:
in the 5 runs of BHMM2, the average number of
types per cluster ranged from 436 to 465 (i.e., to-
kens of the same word are spread over fewer clus-
ters than in MLHMM), with a standard deviation
between 460 and 674. Determiners, prepositions,
the possessive marker, and various kinds of punc-
tuation are mostly clustered coherently. Nouns are
spread over a few clusters, partly due to a distinction
found between common and proper nouns. Like-
wise, modal verbs and the copula are mostly sep-
arated from other verbs. Errors are often sensible:
adjectives and nouns are frequently confused, as are
verbs and adverbs.
The kinds of results produced by BHMM1 and
BHMM2 are more similar to each other than to
the results of MLHMM, but the differences are still
informative. Recall that BHMM1 learns a single
value for ? that is used for all output distribu-
tions, while BHMM2 learns separate hyperparame-
ters for each cluster. This leads to different treat-
ments of difficult-to-classify low-frequency items.
In BHMM1, these items tend to be spread evenly
among all clusters, so that all clusters have simi-
larly sparse output distributions. In BHMM2, the
system creates one or two clusters consisting en-
tirely of very infrequent items, where the priors on
these clusters strongly prefer uniform outputs, and
all other clusters prefer extremely sparse outputs
(and are more coherent than in BHMM1). This
explains the difference in VI between the two sys-
tems, as well as the higher accuracy of BHMM1
for d ? 3: the single ? discourages placing low-
frequency items in their own cluster, so they are
more likely to be clustered with items that have sim-
750
1 2 3 4 5 6 7 8 9 1011121314151617
N
INPUNC
ADJ
V
DET
PREP
ENDPUNC
VBG
CONJ
VBN
ADV
TO
WH
PRT
POS
 LPUNC
RPUNC
 (a) BHMM2
Found Tags
Tr
ue
 T
ag
s
1 2 3 4 5 6 7 8 9 1011121314151617
N
INPUNC
ADJ
V
DET
PREP
ENDPUNC
VBG
CONJ
VBN
ADV
TO
WH
PRT
POS
 LPUNC
RPUNC
 (b) MLHMM
Found Tags
Tr
ue
 T
ag
s
Figure 3: Confusion matrices for the dictionary-free clusterings found by (a) BHMM2 and (b) MLHMM.
ilar transition probabilities. The problem of junk
clusters in BHMM2 might be alleviated by using a
non-uniform prior over the hyperparameters to en-
courage some degree of sparsity in all clusters.
6 Conclusion
In this paper, we have demonstrated that, for a stan-
dard trigram HMM, taking a Bayesian approach
to POS tagging dramatically improves performance
over maximum-likelihood estimation. Integrating
over possible parameter values leads to more robust
solutions and allows the use of priors favoring sparse
distributions. The Bayesian approach is particularly
helpful when learning is less constrained, either be-
cause less data is available or because dictionary
information is limited or absent. For knowledge-
free clustering, our approach can also be extended
through the use of infinite models so that the num-
ber of clusters need not be specified in advance. We
hope that our success with POS tagging will inspire
further research into Bayesian methods for other nat-
ural language learning tasks.
References
M. Banko and R. Moore. 2004. A study of unsupervised part-
of-speech tagging. In Proceedings of COLING ?04.
E. Brill. 1995. Unsupervised learning of disambiguation rules
for part of speech tagging. In Proceedings of the 3rd Work-
shop on Very Large Corpora, pages 1?13.
P. Brown, V. Della Pietra, V. de Souza, J. Lai, and R. Mer-
cer. 1992. Class-based n-gram models of natural language.
Computational Linguistics, 18:467?479.
A. Clark. 2000. Inducing syntactic categories by context dis-
tribution clustering. In Proceedings of the Conference on
Natural Language Learning (CONLL).
S. Finch, N. Chater, and M. Redington. 1995. Acquiring syn-
tactic information from distributional statistics. In J. In Levy,
D. Bairaktaris, J. Bullinaria, and P. Cairns, editors, Connec-
tionist Models of Memory and Language. UCL Press, Lon-
don.
S. Geman and D. Geman. 1984. Stochastic relaxation, Gibbs
distributions and the Bayesian restoration of images. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
6:721?741.
W.R. Gilks, S. Richardson, and D. J. Spiegelhalter, editors.
1996. Markov Chain Monte Carlo in Practice. Chapman
and Hall, Suffolk.
A. Haghighi and D. Klein. 2006. Prototype-driven learning for
sequence models. In Proceedings of HLT-NAACL.
M. Johnson, T. Griffiths, and S. Goldwater. 2007. Bayesian
inference for PCFGs via Markov chain Monte Carlo.
D. Klein and C. Manning. 2002. A generative constituent-
context model for improved grammar induction. In Proceed-
ings of the ACL.
D. MacKay and L. Bauman Peto. 1995. A hierarchical Dirich-
let language model. Natural Language Engineering, 1:289?
307.
M. Meila?. 2002. Comparing clusterings. Technical Report 418,
University of Washington Statistics Department.
B. Merialdo. 1994. Tagging English text with a probabilistic
model. Computational Linguistics, 20(2):155?172.
L. Saul and F. Pereira. 1997. Aggregate and mixed-order
markov models for statistical language processing. In Pro-
ceedings of the Second Conference on Empirical Methods in
Natural Language Processing (EMNLP).
H. Schu?tze. 1995. Distributional part-of-speech tagging. In
Proceedings of the European Chapter of the Association for
Computational Linguistics (EACL).
N. Smith and J. Eisner. 2005. Contrastive estimation: Training
log-linear models on unlabeled data. In Proceedings of ACL.
I. Wang and D. Schuurmans. 2005. Improved estimation
for unsupervised part-of-speech tagging. In Proceedings
of the IEEE International Conference on Natural Language
Processing and Knowledge Engineering (IEEE NLP-KE).
751
Proceedings of ACL-08: HLT, pages 380?388,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Which words are hard to recognize?
Prosodic, lexical, and disfluency factors that increase ASR error rates
Sharon Goldwater, Dan Jurafsky and Christopher D. Manning
Department of Linguistics and Computer Science
Stanford University
{sgwater,jurafsky,manning}@stanford.edu
Abstract
Many factors are thought to increase the
chances of misrecognizing a word in ASR,
including low frequency, nearby disfluencies,
short duration, and being at the start of a turn.
However, few of these factors have been for-
mally examined. This paper analyzes a variety
of lexical, prosodic, and disfluency factors to
determine which are likely to increase ASR er-
ror rates. Findings include the following. (1)
For disfluencies, effects depend on the type of
disfluency: errors increase by up to 15% (ab-
solute) for words near fragments, but decrease
by up to 7.2% (absolute) for words near repeti-
tions. This decrease seems to be due to longer
word duration. (2) For prosodic features, there
are more errors for words with extreme values
than words with typical values. (3) Although
our results are based on output from a system
with speaker adaptation, speaker differences
are a major factor influencing error rates, and
the effects of features such as frequency, pitch,
and intensity may vary between speakers.
1 Introduction
In order to improve the performance of automatic
speech recognition (ASR) systems on conversational
speech, it is important to understand the factors
that cause problems in recognizing words. Previous
work on recognition of spontaneous monologues
and dialogues has shown that infrequent words are
more likely to be misrecognized (Fosler-Lussier and
Morgan, 1999; Shinozaki and Furui, 2001) and that
fast speech increases error rates (Siegler and Stern,
1995; Fosler-Lussier and Morgan, 1999; Shinozaki
and Furui, 2001). Siegler and Stern (1995) and
Shinozaki and Furui (2001) also found higher er-
ror rates in very slow speech. Word length (in
phones) has also been found to be a useful pre-
dictor of higher error rates (Shinozaki and Furui,
2001). In Hirschberg et al?s (2004) analysis of
two human-computer dialogue systems, misrecog-
nized turns were found to have (on average) higher
maximum pitch and energy than correctly recog-
nized turns. Results for speech rate were ambiguous:
faster utterances had higher error rates in one corpus,
but lower error rates in the other. Finally, Adda-
Decker and Lamel (2005) demonstrated that both
French and English ASR systems had more trouble
with male speakers than female speakers, and found
several possible explanations, including higher rates
of disfluencies and more reduction.
Many questions are left unanswered by these pre-
vious studies. In the word-level analyses of Fosler-
Lussier and Morgan (1999) and Shinozaki and Fu-
rui (2001), only substitution and deletion errors were
considered, so we do not know how including inser-
tions might affect the results. Moreover, these stud-
ies primarily analyzed lexical, rather than prosodic,
factors. Hirschberg et al?s (2004) work suggests that
prosodic factors can impact error rates, but leaves
open the question of which factors are important at
the word level and how they influence recognition
of natural conversational speech. Adda-Decker and
Lamel?s (2005) suggestion that higher rates of dis-
fluency are a cause of worse recognition for male
speakers presupposes that disfluencies raise error
rates. While this assumption seems natural, it has
yet to be carefully tested, and in particular we do not
380
know whether disfluent words are associated with
errors in adjacent words, or are simply more likely to
be misrecognized themselves. Other factors that are
often thought to affect a word?s recognition, such as
its status as a content or function word, and whether
it starts a turn, also remain unexamined.
The present study is designed to address all of
these questions by analyzing the effects of a wide
range of lexical and prosodic factors on the accu-
racy of an English ASR system for conversational
telephone speech. In the remainder of this paper, we
first describe the data set used in our study and intro-
duce a new measure of error, individual word error
rate (IWER), that allows us to include insertion er-
rors in our analysis, along with deletions and substi-
tutions. Next, we present the features we collected
for each word and the effects of those features indi-
vidually on IWER. Finally, we develop a joint sta-
tistical model to examine the effects of each feature
while controlling for possible correlations.
2 Data
For our analysis, we used the output from the
SRI/ICSI/UW RT-04 CTS system (Stolcke et al,
2006) on the NIST RT-03 development set. This sys-
tem?s performance was state-of-the-art at the time of
the 2004 evaluation. The data set contains 36 tele-
phone conversations (72 speakers, 38477 reference
words), half from the Fisher corpus and half from
the Switchboard corpus.1
The standard measure of error used in ASR is
word error rate (WER), computed as 100(I + D +
S)/R, where I,D and S are the number of inser-
tions, deletions, and substitutions found by align-
ing the ASR hypotheses with the reference tran-
scriptions, and R is the number of reference words.
Since we wish to know what features of a reference
word increase the probability of an error, we need
a way to measure the errors attributable to individ-
ual words ? an individual word error rate (IWER).
We assume that a substitution or deletion error can
be assigned to its corresponding reference word, but
for insertion errors, there may be two adjacent ref-
erence words that could be responsible. Our so-
lution is to assign any insertion errors to each of
1These conversations are not part of the standard Fisher and
Switchboard corpora used to train most ASR systems.
Ins Del Sub Total % data
Full word 1.6 6.9 10.5 19.0 94.2
Filled pause 0.6 ? 16.4 17.0 2.8
Fragment 2.3 ? 17.3 19.6 2.0
Backchannel 0.3 30.7 5.0 36.0 0.6
Guess 1.6 ? 30.6 32.1 0.4
Total 1.6 6.7 10.9 19.7 100
Table 1: Individual word error rates for different word
types, and the proportion of words belonging to each
type. Deletions of filled pauses, fragments, and guesses
are not counted as errors in the standard scoring method.
the adjacent words. We could then define IWER as
100(ni + nd + ns)/R, where ni, nd, and ns are the
insertion, deletion, and substitution counts for indi-
vidual words (with nd = D and ns = S). In general,
however, ni > I , so that the IWER for a given data
set would be larger than the WER. To facilitate com-
parisons with standard WER, we therefore discount
insertions by a factor ?, such that ?ni = I . In this
study, ? = .617.
3 Analysis of individual features
3.1 Features
The reference transcriptions used in our analysis
distinguish between five different types of words:
filled pauses (um, uh), fragments (wh-, redistr-),
backchannels (uh-huh, mm-hm), guesses (where the
transcribers were unsure of the correct words), and
full words (everything else). Error rates for each
of these types can be found in Table 1. The re-
mainder of our analysis considers only the 36159 in-
vocabulary full words in the reference transcriptions
(70 OOV full words are excluded). We collected the
following features for these words:
Speaker sex Male or female.
Broad syntactic class Open class (e.g., nouns and
verbs), closed class (e.g., prepositions and articles),
or discourse marker (e.g., okay, well). Classes were
identified using a POS tagger (Ratnaparkhi, 1996)
trained on the tagged Switchboard corpus.
Log probability The unigram log probability of
each word, as listed in the system?s language model.
Word length The length of each word (in phones),
determined using the most frequent pronunciation
381
BefRep FirRep MidRep LastRep AfRep BefFP AfFP BefFr AfFr
yeah i i i think you should um ask for the ref- recommendation
Figure 1: Example illustrating disfluency features: words occurring before and after repetitions, filled pauses, and
fragments; first, middle, and last words in a repeated sequence.
found for that word in the recognition lattices.
Position near disfluency A collection of features
indicating whether a word occurred before or after a
filled pause, fragment, or repeated word; or whether
the word itself was the first, last, or other word in a
sequence of repetitions. Figure 1 illustrates. Only
identical repeated words with no intervening words
or filled pauses were considered repetitions.
First word of turn Turn boundaries were assigned
automatically at the beginning of any utterance fol-
lowing a pause of at least 100 ms during which the
other speaker spoke.
Speech rate The average speech rate (in phones per
second) was computed for each utterance using the
pronunciation dictionary extracted from the lattices
and the utterance boundary timestamps in the refer-
ence transcriptions.
In addition to the above features, we used Praat
(Boersma and Weenink, 2007) to collect the follow-
ing additional prosodic features on a subset of the
data obtained by excluding all contractions:2
Pitch The minimum, maximum, mean, and range
of pitch for each word.
Intensity The minimum, maximum, mean, and
range of intensity for each word.
Duration The duration of each word.
31017 words (85.8% of the full-word data set) re-
main in the no-contractions data set after removing
words for which pitch and/or intensity features could
not be extracted.
2Contractions were excluded before collecting prosodic fea-
tures for the following reason. In the reference transcriptions
and alignments used for scoring ASR systems, contractions are
treated as two separate words. However, aside from speech rate,
our prosodic features were collected using word-by-word times-
tamps from a forced alignment that used a transcription where
contractions are treated as single words. Thus, the start and end
times for a contraction in the forced alignment correspond to
two words in the alignments used for scoring, and it is not clear
how to assign prosodic features appropriately to those words.
3.2 Results and discussion
Results of our analysis of individual features can be
found in Table 2 (for categorical features) and Figure
2 (for numeric features). Comparing the error rates
for the full-word and the no-contractions data sets in
Table 2 verifies that removing contractions does not
create systematic changes in the patterns of errors,
although it does lower error rates (and significance
values) slightly overall. (First and middle repetitions
are combined as non-final repetitions in the table,
because only 52 words were middle repetitions, and
their error rates were similar to initial repetitions.)
3.2.1 Disfluency features
Perhaps the most interesting result in Table 2 is
that the effects of disfluencies are highly variable de-
pending on the type of disfluency and the position
of a word relative to it. Non-final repetitions and
words next to fragments have an IWER up to 15%
(absolute) higher than the average word, while fi-
nal repetitions and words following repetitions have
an IWER up to 7.2% lower. Words occurring be-
fore repetitions or next to filled pauses do not have
significantly different error rates than words not in
those positions. Our results for repetitions support
Shriberg?s (1995) hypothesis that the final word of a
repeated sequence is in fact fluent.
3.2.2 Other categorical features
Our results support the common wisdom that
open class words have lower error rates than other
words (although the effect we find is small), and that
words at the start of a turn have higher error rates.
Also, like Adda-Decker and Lamel (2005), we find
that male speakers have higher error rates than fe-
males, though in our data set the difference is more
striking (3.6% absolute, compared to their 2.0%).
3.2.3 Word probability and word length
Turning to Figure 2, we find (consistent with pre-
vious results) that low-probability words have dra-
matically higher error rates than high-probability
382
Filled Pau. Fragment Repetition Syntactic Class Sex
Bef Aft Bef Aft Bef Aft NonF Fin Clos Open Disc 1st M F All
(a) IWER 17.6 16.9 33.8 21.6 16.7 13.8 26.0 11.6 19.7 18.0 19.6 21.2 20.6 17.0 18.8
% wds 1.7 1.7 1.6 1.5 0.7 0.9 1.2 1.1 43.8 50.5 5.8 6.2 52.5 47.5 100
(b) IWER 17.6 17.2 32.0 21.5 15.8 14.2 25.1 11.6 18.8 17.8 19.0 20.3 20.0 16.4 18.3
% wds 1.9 1.8 1.6 1.5 0.8 0.8 1.4 1.1 43.9 49.6 6.6 6.4 52.2 47.8 100
Table 2: IWER by feature and percentage of words exhibiting each feature for (a) the full-word data set and (b) the no-
contractions data set. Error rates that are significantly different for words with and without a given feature (computed
using 10,000 samples in a Monte Carlo permutation test) are in bold (p < .05) or bold italics (p < .005). Features
shown are whether a word occurs before or after a filled pause, fragment, or repetition; is a non-final or final repetition;
is open class, closed class, or a discourse marker; is the first word of a turn; or is spoken by a male or female. All is
the IWER for the entire data set. (Overall IWER is slightly lower than in Table 1 due to the removal of OOV words.)
words. More surprising is that word length in
phones does not seem to have a consistent effect on
IWER. Further analysis reveals a possible explana-
tion: word length is correlated with duration, but
anti-correlated to the same degree with log proba-
bility (the Kendall ? statistics are .50 and -.49). Fig-
ure 2 shows that words with longer duration have
lower IWER. Since words with more phones tend to
have longer duration, but lower frequency, there is
no overall effect of length.
3.2.4 Prosodic features
Figure 2 shows that means of pitch and intensity
have relatively little effect except at extreme val-
ues, where more errors occur. In contrast, pitch
and intensity range show clear linear trends, with
greater range of pitch or intensity leading to lower
IWER.3 As noted above, decreased duration is as-
sociated with increased IWER, and (as in previous
work), we find that IWER increases dramatically
for fast speech. We also see a tendency towards
higher IWER for very slow speech, consistent with
Shinozaki and Furui (2001) and Siegler and Stern
(1995). The effects of pitch minimum and maximum
are not shown for reasons of space, but are similar
to pitch mean. Also not shown are intensity mini-
mum (with more errors at higher values) and inten-
sity maximum (with more errors at lower values).
For most of our prosodic features, as well as log
probability, extreme values seem to be associated
3Our decision to use the log transform of pitch range was
originally based on the distribution of pitch range values in the
data set. Exploratory data analysis also indicated that using the
transformed values would likely lead to a better model fit (Sec-
tion 4) than using the raw values.
with worse recognition than average values. We ex-
plore this possibility further in Section 4.
4 Analysis using a joint model
In the previous section, we investigated the effects
of various individual features on ASR error rates.
However, there are many correlations between these
features ? for example, words with longer duration
are likely to have a larger range of pitch and inten-
sity. In this section, we build a single model with all
of our features as potential predictors in order to de-
termine the effects of each feature after controlling
for the others. We use the no-contractions data set so
that we can include prosodic features in our model.
Since only 1% of tokens have an IWER > 1, we
simplify modeling by predicting only whether each
token is responsible for an error or not. That is, our
dependent variable is binary, taking on the value 1 if
IWER > 0 for a given token and 0 otherwise.
4.1 Model
To model data with a binary dependent variable, a
logistic regression model is an appropriate choice.
In logistic regression, we model the log odds as a
linear combination of feature values x0 . . . xn:
log p1 ? p = ?0x0 + ?1x1 + . . . + ?nxn
where p is the probability that the outcome occurs
(here, that a word is misrecognized) and ?0 . . . ?n
are coefficients (feature weights) to be estimated.
Standard logistic regression models assume that all
categorical features are fixed effects, meaning that
all possible values for these features are known in
advance, and each value may have an arbitrarily dif-
ferent effect on the outcome. However, features
383
2 4 6 8 10
0
20
40
Word length (phones)
IW
ER
100 200 300
0
20
40
Pitch mean (Hz)
50 60 70 80
0
20
40
Intensity mean (dB)
0.0 0.2 0.4 0.6 0.8 1.0
0
20
40
Duration (sec)
?5 ?4 ?3 ?2
0
20
40
Log probability
IW
ER
1 2 3 4 5
0
20
40
log(Pitch range) (Hz)
IW
ER
10 30 50
0
20
40
Intensity range (dB)
5 10 15 20
0
20
40
Speech rate (phones/sec)
Figure 2: Effects of numeric features on IWER of the SRI system for the no-contractions data set. All feature values
were binned, and the average IWER for each bin is plotted, with the area of the surrounding circle proportional to the
number of points in the bin. Dotted lines show the average IWER over the entire data set.
such as speaker identity do not fit this pattern. In-
stead, we control for speaker differences by assum-
ing that speaker identity is a random effect, mean-
ing that the speakers observed in the data are a ran-
dom sample from a larger population. The base-
line probability of error for each speaker is therefore
assumed to be a normally distributed random vari-
able, with mean equal to the population mean, and
variance to be estimated by the model. Stated dif-
ferently, a random effect allows us to add a factor
to the model for speaker identity, without allowing
arbitrary variation in error rates between speakers.
Models such as ours, with both fixed and random
effects, are known as mixed-effects models, and are
becoming a standard method for analyzing linguis-
tic data (Baayen, 2008). We fit our models using the
lme4 package (Bates, 2007) of R (R Development
Core Team, 2007).
To analyze the joint effects of all of our features,
we initially built as large a model as possible, and
used backwards elimination to remove features one
at a time whose presence did not contribute signifi-
cantly (at p ? .05) to model fit. All of the features
shown in Table 2 were converted to binary variables
and included as predictors in our initial model, along
with a binary feature controlling for corpus (Fisher
or Switchboard), and all numeric features in Figure
2. We did not include minimum and maximum val-
ues for pitch and intensity because they are highly
correlated with the mean values, making parameter
estimation in the combined model difficult. Prelimi-
nary investigation indicated that using the mean val-
ues would lead to the best overall fit to the data.
In addition to these basic fixed effects, our ini-
tial model included quadratic terms for all of the nu-
meric features, as suggested by our analysis in Sec-
tion 3, as well as random effects for speaker iden-
tity and word identity. All numeric features were
rescaled to values between 0 and 1 so that coeffi-
cients are comparable.
4.2 Results and discussion
Figure 3 shows the estimated coefficients and stan-
dard errors for each of the fixed effect categorical
features remaining in the reduced model (i.e., after
backwards elimination). Since all of the features are
binary, a coefficient of ? indicates that the corre-
sponding feature, when present, adds a weight of ?
to the log odds (i.e., multiplies the odds of an error
by a factor of e?). Thus, features with positive co-
efficients increase the odds of an error, and features
with negative coefficients decrease the odds of an er-
ror. The magnitude of the coefficient corresponds to
the size of the effect.
Interpreting the coefficients for our numeric fea-
tures is less intuitive, since most of these variables
have both linear and quadratic effects. The contribu-
tion to the log odds of a particular numeric feature
384
?1.5 ?1.0 ?0.5 0.0 0.5 1.0
corpus=SW
sex=M
starts turn
before FP
after FP
before frag
after frag
non?final rep
open class
Figure 3: Estimates and standard errors of the coefficients
for the categorical predictors in the reduced model.
xi, with linear and quadratic coefficients a and b, is
axi + bx2i . We plot these curves for each numeric
feature in Figure 4. Values on the x axes with posi-
tive y values indicate increased odds of an error, and
negative y values indicate decreased odds of an er-
ror. The x axes in these plots reflect the rescaled
values of each feature, so that 0 corresponds to the
minimum value in the data set, and 1 to the maxi-
mum value.
4.2.1 Disfluencies
In our analysis of individual features, we found
that different types of disfluencies have different ef-
fects: non-final repeated words and words near frag-
ments have higher error rates, while final repetitions
and words following repetitions have lower error
rates. After controlling for other factors, a differ-
ent picture emerges. There is no longer an effect for
final repetitions or words after repetitions; all other
disfluency features increase the odds of an error by
a factor of 1.3 to 2.9. These differences from Sec-
tion 3 can be explained by noting that words near
filled pauses and repetitions have longer durations
than other words (Bell et al, 2003). Longer duration
lowers IWER, so controlling for duration reveals the
negative effect of the nearby disfluencies. Our re-
sults are also consistent with Shriberg?s (1995) find-
ings on fluency in repeated words, since final rep-
etitions have no significant effect in our combined
model, while non-final repetitions incur a penalty.
4.2.2 Other categorical features
Without controlling for other lexical or prosodic
features, we found that a word is more likely to
be misrecognized at the beginning of a turn, and
less likely to be misrecognized if it is an open class
word. According to our joint model, these effects
still hold even after controlling for other features.
Similarly, male speakers still have higher error rates
than females. This last result sheds some light on
the work of Adda-Decker and Lamel (2005), who
suggested several factors that could explain males?
higher error rates. In particular, they showed that
males have higher rates of disfluency, produce words
with slightly shorter durations, and use more alter-
nate (?sloppy?) pronunciations. Our joint model
controls for the first two of these factors, suggesting
that the third factor or some other explanation must
account for the remaining differences between males
and females. One possibility is that female speech is
more easily recognized because females tend to have
expanded vowel spaces (Diehl et al, 1996), a factor
that is associated with greater intelligibility (Brad-
low et al, 1996) and is characteristic of genres with
lower ASR error rates (Nakamura et al, 2008).
4.2.3 Prosodic features
Examining the effects of pitch and intensity indi-
vidually, we found that increased range for these fea-
tures is associated with lower IWER, while higher
pitch and extremes of intensity are associated with
higher IWER. In the joint model, we see the same
effect of pitch mean and an even stronger effect for
intensity, with the predicted odds of an error dra-
matically higher for extreme intensity values. Mean-
while, we no longer see a benefit for increased pitch
range and intensity; rather, we see small quadratic
effects for both features, i.e. words with average
ranges of pitch and intensity are recognized more
easily than words with extreme values for these fea-
tures. As with disfluencies, we hypothesize that the
linear trends observed in Section 3 are primarily due
to effects of duration, since duration is moderately
correlated with both log pitch range (? = .35) and
intensity range (? = .41).
Our final two prosodic features, duration and
speech rate, showed strong linear and weak
quadratic trends when analyzed individually. Ac-
cording to our model, both duration and speech rate
are still important predictors of error after control-
ling for other features. However, as with the other
prosodic features, predictions of the joint model are
dominated by quadratic trends, i.e., predicted error
rates are lower for average values of duration and
speech rate than for extreme values.
Overall, the results from our joint analysis suggest
385
0.0 0.4 0.8
?
4
0
4
Word length
lo
g 
od
ds
y = ?0.8x
0.0 0.4 0.8
?
4
0
4
Pitch mean
lo
g 
od
ds
y = 1x
0.0 0.4 0.8
?
4
0
4
Intensity mean
lo
g 
od
ds
y = ?13.2x + 11.5x2
0.0 0.4 0.8
?
4
0
4
Duration
lo
g 
od
ds
y = ?12.6x + 14.6x2
0.0 0.4 0.8
?
4
0
4
Log probability
lo
g 
od
ds
y = ?0.6x + 4.1x2
0.0 0.4 0.8
?
4
0
4
log(Pitch range)
lo
g 
od
ds
y = ?2.3x + 2.2x2
0.0 0.4 0.8
?
4
0
4
Intensity range
lo
g 
od
ds
y = ?1x + 1.2x2
0.0 0.4 0.8
?
4
0
4
Speech rate
lo
g 
od
ds
y = ?3.9x + 4.4x2
Figure 4: Predicted effect on the log odds of each numeric feature, including linear and (if applicable) quadratic terms.
Model Neg. log lik. Diff. df
Full 12932 0 32
Reduced 12935 3 26
No lexical 13203 271 16
No prosodic 13387 455 20
No speaker 13432 500 31
No word 13267 335 31
Baseline 14691 1759 1
Table 3: Fit to the data of various models. Degrees of
freedom (df) for each model is the number of fixed ef-
fects plus the number of random effects plus 1 (for the
intercept). Full model contains all predictors; Reduced
contains only predictors contributing significantly to fit;
Baseline contains only intercept. Other models are ob-
tained by removing features from Full. Diff is the differ-
ence in log likelihood between each model and Full.
that, after controlling for other factors, extreme val-
ues for prosodic features are associated with worse
recognition than typical values.
4.2.4 Differences between lexical items
As discussed above, our model contains a random
effect for word identity, to control for the possibil-
ity that certain lexical items have higher error rates
that are not explained by any of the other factors
in the model. It is worth asking whether this ran-
dom effect is really necessary. To address this ques-
tion, we compared the fit to the data of two models,
each containing all of our fixed effects and a ran-
dom effect for speaker identity. One model also con-
tained a random effect for word identity. Results are
shown in Table 3. The model without a random ef-
fect for word identity is significantly worse than the
full model; in fact, this single parameter is more im-
portant than all of the lexical features combined. To
see which lexical items are causing the most diffi-
culty, we examined the items with the highest esti-
mated increases in error. The top 20 items on this
list include yup, yep, yes, buy, then, than, and r., all
of which are acoustically similar to each other or to
other high-frequency words, as well as the words af-
ter, since, now, and though, which occur in many
syntactic contexts, making them difficult to predict
based on the language model.
4.2.5 Differences between speakers
We examined the importance of the random effect
for speaker identity in a similar fashion to the ef-
fect for word identity. As shown in Table 3, speaker
identity is a very important factor in determining the
probability of error. That is, the lexical and prosodic
variables examined here are not sufficient to fully
explain the differences in error rates between speak-
ers. In fact, the speaker effect is the single most im-
portant factor in the model.
Given that the differences in error rates between
speakers are so large (average IWER for different
speakers ranges from 5% to 51%), we wondered
whether our model is sufficient to capture the kinds
of speaker variation that exist. The model assumes
that each speaker has a different baseline error rate,
but that the effects of each variable are the same for
each speaker. Determining the extent to which this
assumption is justified is beyond the scope of this
paper, however we present some suggestive results
in Figure 5. This figure illustrates some of the dif-
386
40 60 80
0.
0
0.
2
0.
4
Intensity mean (dB)
Fi
tte
d 
P(
err
)
100 250 400
0.
0
0.
2
0.
4
Pitch mean (Hz)
0.0 0.5 1.0 1.5
0.
0
0.
2
0.
4
Duration (sec)
?6 ?5 ?4 ?3 ?2
0.
0
0.
2
0.
4
Neg. log prob.
0 5 10 20
0.
0
0.
2
0.
4
Sp. rate (ph/sec)
40 60 80
0.
0
0.
2
0.
4
Intensity mean (dB)
Fi
tte
d 
P(
err
)
100 250 400
0.
0
0.
2
0.
4
Pitch mean (Hz)
0.0 0.5 1.0 1.5
0.
0
0.
2
0.
4
Duration (sec)
?6 ?5 ?4 ?3 ?2
0.
0
0.
2
0.
4
Neg. log prob.
0 5 10 20
0.
0
0.
2
0.
4
Sp. rate (ph/sec)
Figure 5: Estimated effects of various features on the error rates of two different speakers (top and bottom). Dashed
lines illustrate the baseline probability of error for each speaker. Solid lines were obtained by fitting a logistic regres-
sion model to each speaker?s data, with the variable labeled on the x-axis as the only predictor.
ferences between two speakers chosen fairly arbi-
trarily from our data set. Not only are the baseline
error rates different for the two speakers, but the ef-
fects of various features appear to be very different,
in one case even reversed. The rest of our data set
exhibits similar kinds of variability for many of the
features we examined. These differences in ASR be-
havior between speakers are particularly interesting
considering that the system we investigated here al-
ready incorporates speaker adaptation models.
5 Conclusion
In this paper, we introduced the individual word er-
ror rate (IWER) for measuring ASR performance
on individual words, including insertions as well as
deletions and substitutions. Using IWER, we ana-
lyzed the effects of various word-level lexical and
prosodic features, both individually and in a joint
model. Our analysis revealed the following effects.
(1) Words at the start of a turn have slightly higher
IWER than average, and open class (content) words
have slightly lower IWER. These effects persist even
after controlling for other lexical and prosodic fac-
tors. (2) Disfluencies heavily impact error rates:
IWER for non-final repetitions and words adjacent
to fragments rises by up to 15% absolute, while
IWER for final repetitions and words following rep-
etitions decreases by up to 7.2% absolute. Control-
ling for prosodic features eliminates the latter ben-
efit, and reveals a negative effect of adjacent filled
pauses, suggesting that the effects of these disfluen-
cies are normally obscured by the greater duration of
nearby words. (3) For most acoustic-prosodic fea-
tures, words with extreme values have worse recog-
nition than words with average values. This effect
becomes much more pronounced after controlling
for other factors. (4) After controlling for lexical
and prosodic characteristics, the lexical items with
the highest error rates are primarily homophones or
near-homophones (e.g., buy vs. by, then vs. than).
(5) Speaker differences account for much of the vari-
ance in error rates between words. Moreover, the di-
rection and strength of effects of different prosodic
features may vary between speakers.
While we plan to extend our analysis to other
ASR systems in order to determine the generality
of our findings, we have already gained important
insights into a number of factors that increase ASR
error rates. In addition, our results suggest a rich
area for future research in further analyzing the vari-
ability of both lexical and prosodic effects on ASR
behavior for different speakers.
Acknowledgments
This work was supported by the Edinburgh-Stanford
LINK and ONR MURI award N000140510388. We
thank Andreas Stolcke for providing the ASR out-
put, language model, and forced alignments used
here, and Raghunandan Kumaran and Katrin Kirch-
hoff for earlier datasets and additional help.
387
References
M. Adda-Decker and L. Lamel. 2005. Do speech rec-
ognizers prefer female speakers? In Proceedings of
INTERSPEECH, pages 2205?2208.
R. H. Baayen. 2008. Analyzing Linguistic Data. A
Practical Introduction to Statistics. Cambridge
University Press. Prepublication version available at
http://www.mpi.nl/world/persons/private/baayen/pub-
lications.html.
Douglas Bates, 2007. lme4: Linear mixed-effects models
using S4 classes. R package version 0.99875-8.
A. Bell, D. Jurafsky, E. Fosler-Lussier, C. Girand,
M. Gregory, and D. Gildea. 2003. Effects of disflu-
encies, predictability, and utterance position on word
form variation in English conversation. Journal of the
Acoustical Society of America, 113(2):1001?1024.
P. Boersma and D. Weenink. 2007. Praat:
doing phonetics by computer (version 4.5.16).
http://www.praat.org/.
A. Bradlow, G. Torretta, and D. Pisoni. 1996. Intelli-
gibility of normal speech I: Global and fine-grained
acoustic-phonetic talker characteristics. Speech Com-
munication, 20:255?272.
R. Diehl, B. Lindblom, K. Hoemeke, and R. Fahey. 1996.
On explaining certain male-female differences in the
phonetic realization of vowel categories. Journal of
Phonetics, 24:187?208.
E. Fosler-Lussier and N. Morgan. 1999. Effects of
speaking rate and word frequency on pronunciations
in conversational speech. Speech Communication,
29:137? 158.
J. Hirschberg, D. Litman, and M. Swerts. 2004. Prosodic
and other cues to speech recognition failures. Speech
Communication, 43:155? 175.
M. Nakamura, K. Iwano, and S. Furui. 2008. Differ-
ences between acoustic characteristics of spontaneous
and read speech and their effects on speech recogni-
tion performance. Computer Speech and Language,
22:171? 184.
R Development Core Team, 2007. R: A Language and
Environment for Statistical Computing. R Foundation
for Statistical Computing, Vienna, Austria. ISBN 3-
900051-07-0.
A. Ratnaparkhi. 1996. A Maximum Entropy model for
part-of-speech tagging. In Proceedings of the First
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 133?142.
T. Shinozaki and S. Furui. 2001. Error analysis using de-
cision trees in spontaneous presentation speech recog-
nition. In Proceedings of ASRU 2001.
E. Shriberg. 1995. Acoustic properties of disfluent rep-
etitions. In Proceedings of the International Congress
of Phonetic Sciences, volume 4, pages 384?387.
M. Siegler and R. Stern. 1995. On the effects of speech
rate in large vocabulary speech recognition systems.
In Proceedings of ICASSP.
A. Stolcke, B. Chen, H. Franco, V. R. R. Gadde, M. Gra-
ciarena, M.-Y. Hwang, K. Kirchhoff, A. Mandal,
N. Morgan, X. Lin, T. Ng, M. Ostendorf, K. Sonmez,
A. Venkataraman, D. Vergyri, W. Wang, J. Zheng, and
Q. Zhu. 2006. Recent innovations in speech-to-text
transcription at SRI-ICSI-UW. IEEE Transactions on
Audio, Speech and Language Processing, 14(5):1729?
1744.
388
Compi l ing  Language Mode ls  f rom a L ingu is t i ca l ly  Mot ivated  
Un i f i ca t ion  Grammar  
Manny Rayner t>, Beth Ann Hockey t, Frankie James t 
Elizabeth Owen Bratt ++, Sharon Goldwater ++ and Jean Mark Gawron ~ 
tResea.rch Inst i tute for 
Advanced Computer  Science 
Mail Stop 19-39 
NASA Ames Research Center 
Moffett Field, CA 94035-1000 
Abstract 
Systems now exist which are able to con:pile 
unification gralmnars into language models that 
can be included in a speech recognizer, but it 
is so far unclear whether non-trivial linguisti- 
cally principled gralnlnars can be used for this 
purpose. We describe a series of experiments 
which investigate the question empirica.lly, by 
incrementally constructing a grammar and dis- 
covering what prot)lems emerge when succes- 
sively larger versions are compiled into finite 
state graph representations and used as lan- 
guage models for a medium-vocabulary recog- 
nition task. 
1 Introduction ~ 
Construction of speech recognizers for n:ediuln- 
vocabulary dialogue tasks has now becolne an 
important I)ractical problem. The central task 
is usually building a suitable language model, 
and a number of standard methodologies have 
become established. Broadly speaking, these 
fall into two main classes. One approach is 
to obtain or create a domain corpus, and froln 
it induce a statistical anguage model, usually 
some kind of N-gram grammar; the alternative 
is to manually design a grammar which specifies 
the utterances the recognizer will accept. There 
are many theoretical reasons to prefer the first 
course if it is feasible, but in practice there is of- 
ten no choice. Unless a substantial domain cor- 
pus is available, the only method that stands a 
chance of working is hand-construction f an ex- 
i The majority of the research reported was performed 
at I{IACS under NASA Cooperative Agreement~ Number 
NCC 2-1006. The research described in Section 3 was 
supported by the Defense Advanced Research Projects 
Agency under Con~racl~ N66001-94 C-6046 with the 
Naval Command, Control, and Ocean Surveillance Cen- 
ter. 
SRI International  
333 Ravenswood Ave 
Menlo Park, CA 94025 
*netdecisions 
Well ington House 
East Road 
Cambr idge CB1 1BH 
England 
plicit grammar based on the grammar-writer's 
intuitions. 
If the application is simple enough, experi- 
ence shows that good grammars of this kind 
can be constructed quickly and efficiently using 
commercially available products like ViaVoice 
SDK (IBM 1999) or the Nuance Toolkit (Nu- 
ance 1999). Systems of this kind typically al- 
low specification of some restricted subset of the 
class of context-free grammars, together with 
annotations that permit the grammar-writer to
associate selnantic values with lexical entries 
and rules. This kind of framework is fl:lly ad- 
equate for small grammars. As the gran:mars 
increase in size, however, the limited expres- 
sive power of context-free language notation be- 
conies increasingly burdensome. The grainn:a,r 
tends to beconie large and unwieldy, with many 
rules appearing in multiple versions that con- 
stantly need to be kept in step with each other. 
It represents a large developn:ent cost, is hard 
to maintain, and does not usually port well to 
new applications. 
It is tempting to consider the option of mov- 
ing towards a :::ore expressive grammar tbrmal- 
isln, like unification gramnm.r, writing the orig- 
inal grammar in unification grammar form and 
coml)iling it down to the context-free notation 
required by the underlying toolkit. At least 
one such system (Gemilfi; (Moore ct al 1997)) 
has been implemented and used to build suc- 
cessful and non-trivial applications, most no- 
tably ComnmndTalk (Stent ct al 1999). Gem- 
ini accepts a slightly constrained version of the 
unification grammar formalism originally used 
in the Core Language Engine (Alshawi 1992), 
and compiles it into context-free gran:nmrs in 
the GSL formalism supported by the Nuance 
Toolkit. The Nuance Toolkit con:piles GSL 
gran:mars into sets of probabilistic finite state 
670 
gra.phs (PFSGs), which form the final bmguage 
model. 
The relative success of the Gemilfi system 
suggests a new question. Ulfification grammars 
ha.re been used many times to build substantial 
general gramlnars tbr English and other na.tu- 
ra\[ languages, but the language model oriented 
gra.mln~rs o far developed fi)r Gemini (includ- 
ing the one for ColnmandTalk) have a.ll been 
domain-sl)ecific. One naturally wonders how 
feasible it is to take yet another step in the di- 
rection of increased genera.lity; roughly, what 
we want to do is start with a completely gen- 
eral, linguistically motivated gramma.r, combine 
it with a domain-specific lexicon, and compile 
the result down to a domain-specitic context- 
free grammar that can be used as a la.nguage 
model. If this 1)tetra.mine can be rea.lized, it is 
easy to believe that the result would 1)e a.n ex- 
tremely useful methodology tbr rapid construc- 
tion of la.nguage models. It is i lnportant o note 
tha.t there are no obvious theoretical obstacles 
in our way. The clailn that English is context- 
free has been respectable since a.t least the early 
8(Is (Pullum and Gazda.r 1982) 'e, and the idea. 
of using unification grammar as a. compact wa 5, 
of tel)resenting an ulMerlying context-fl'e~e, lan- 
guage is one of the main inotivations for GPSG 
(Gazdar et al1985) and other formalislns based 
on it. The real question is whether the goal is 
practically achievable, given the resource limi- 
tations of current technology. 
In this l)a.1)er, we describe work aimed at the 
target outlined above, in which we used the 
Gemini system (described in more detail in Sec- 
tion 2) to a.ttempt o compile a. va.riety of lin- 
guistically principled unification gralnlna.rs into 
la.ngua.ge lnodels. Our first experiments (Sec- 
tion 3) were pertbrmed on a. large pre-existing 
unification gramlna.r. These were unsuccessful, 
for reasons that were not entirely obvious; in 
order to investigate the prol)lem more system- 
atically, we then conducted a second series of 
experilnents (Section 4), in which we increlnen- 
tally 1)uilt up a smMler gra.lnlna.r. By monitor- 
ing; the behavior of the compilation process and 
the resulting langua.ge model as the gra.lmnar~s 
2~1e m'e aware l, hal, this claim is most~ 1)robably not 
l;rue for natural languages ill gelmraI (lh'csnall cl al 
1987), but furl~hcr discussion of t.his point is beyond I.he 
scope of t, llC paper. 
cover~ge was expanded, we were a.ble to iden- 
tit~ the point a,t which serious problems began 
to emerge (Section 5). In the fina.1 section, we 
summarize and suggest fltrther directions. 
2 Tile Genfini Language Model  
Compi le r  
To lnake the paper nlore self-contained, this sec- 
tion provides some background on the method 
used by Gemini to compile unifica.tion grain- 
mars into CFGs, and then into language mod- 
els. The ha.sic idea. is the obvious one: enu- 
mera.te all possible instantiations of the feal;ures 
in the grammar rules and lexicon entries, and 
thus tra.nsform esch rule and entry in the ()rig- 
inal unification grammar into a set of rules in 
the derived CFG. For this to be possible, the 
relevant fe~ttul'es Inust be constrained so that 
they can only take values in a finite predefined 
range. The finite range restriction is inconve- 
nient for fea.tures used to build semantic repre- 
sentations, and the tbrmalism consequently dis- 
tinguishes syntactic and semantic features; se- 
lmmtic features axe discarded a.t the start of the 
compilation process. 
A naive iml)lelnentation of the basic lnethod 
would be iml)raetical for any but the small- 
est a.nd simplest grammars, and considera.ble 
ingemfity has been expended on various opti- 
mizations. Most importantly, categories axe ex- 
panded in a demand-driven fa.shion, with infer  
lnatiotl being percolated 1)oth t)otton>up (from 
the lexicon) and top-down (fl'om the grammar's 
start symbol). This is done in such a. way 
that potentially valid colnl)inations of feature 
instantiations in rules are successively filtered 
out if they are not licensed by the top-down 
and bottom-ul) constra.ints. Ranges of feature 
values are also kept together when possible, so 
that sets of context-free rules produced by the 
mdve algorithm may in these cases be merged 
into single rules. 
By exploiting the structure of the gram- 
mar a.nd lexicon, the demand-driven expansion 
lnethod can often effect substa.ntial reductions 
in the size of the derived CFG. (For the type 
of grammar we consider in this paper, the re- 
duction is typically by ,~ fa.etor of over 102?). 
The downside is that even an app~trently slnall 
cha.nge in the syntactic t>atures associated with 
a. rule may have a large eIfect on the size of 
671 
the CFG, if it opens up or blocks an impor- 
tant percolation path. Adding or deleting lexi- 
con entries can also have a significant effect on 
the size of the CFG, especially when there are 
only a small number of entries in a given gram- 
matical category; as usual, entries of this type 
behave from a software ngineering standpoint 
like grammar ules. 
The language model compiler also performs 
a number of other non-trivial transformations. 
The most important of these is related to the 
fact that Nuance GSL grammars are not al- 
lowed to contain left-recursive rules, and left- 
recursive unification-grammar rules must con- 
sequently be converted into a non-left-recursive 
fort::. Rules of this type do not however occur 
in the gramlnars described below, and we conse- 
quently omit further description of the method. 
3 Initial Experiments 
Our initial experiments were performed on a 
recent unification grammar in the ATIS (Air 
Travel Information System) domain, developed 
as a linguistically principled grammar with a 
domain-specific lexicon. This grammar was 
cre~ted for an experiment COl::t)aring cover- 
age and recognition performance of a hand- 
written grammar with that of a.uto:::atically de- 
rived recognition language models, as increas- 
ing amounts of data from the ATIS corpus 
were made available for each n:ethod. Exam- 
ples of sentences covered by this gralnlnar are 
"yes", "on friday", "i want to fly from boston 
to denver on united airlines on friday septem- 
ber twenty third", "is the cheapest one way 
fare from boston to denver a morning flight", 
and "what flight leaves earliest from boston to 
san francisco with the longest layover in den- 
ver". Problems obtaining a working recognition 
grammar from the unification grammar ended 
our original experiment prematurely, and led 
us to investigate the factors responsible for the 
poor recognition performance. 
We explored several ikely causes of recogni- 
tion trouble: number of rules, ::umber of vocab- 
ulary items, size of node array, perplexity, and 
complexity of the grammar, measured by aver- 
age and highest number of transitions per graph 
in the PFSG form of the grammar. 
We were able to in:mediately rule out sim- 
ple size metrics as the cause of Nuance's diffi- 
culties with recognition. Our smallest air travel 
grammar had 141 Gemini rules and 1043 words, 
producing a Nuance grammar with 368 rules. 
This compares to the Con:mandTalk grammar, 
which had 1231 Gemini rules and 1771 words, 
producing a Nuance gran:n:ar with 4096 rules. 
To determine whether the number of the 
words in the grammar or the structure of 
the phrases was responsible for the recognition 
problems, we created extreme cases of a Word+ 
grammar (i.e. a grammar that constrains the 
input to be any sequence of the words in the 
vocabulary) and a one-word-per-category gram- 
mar. We found that both of these variants 
of our gralmnar produced reasonable recogni- 
tion, though the Word+ grammar was very in- 
accurate. However, a three-words-per-category 
grammar could not produce snccessflfl speech 
recognition. 
Many thature specifications can lnake a gram- 
mar ::tore accurate, but will also result in a 
larger recognition grammar due to multiplica- 
tion of feature w~lues to derive the categories 
of the eontext-fl'ee grammar. We experimented 
with various techniques of selecting features to 
be retained in the recognition grammar. As de- 
scribed in the previous ection, Gemini's default 
method is to select only syntactic features and 
not consider semantic features in the recogni- 
tion grammar. We experimented with selecting 
a subset of syntactic features to apply and with 
applying only se:nantic sortal features, and no 
syntactic features. None of these grammars pro- 
duced successful speech recognition. 
/.Fro::: these experiments, we were unable to 
isolate any simple set of factors to explain which 
grammars would be problematic for speech 
recognition. However, the numbers of transi- 
tions per graph in a PFSG did seem suggestive 
of a factor. The ATIS grammar had a high of 
1184 transitions per graph, while the semantic 
grammar of CommandTalk had a high of 428 
transitions per graph, and produced very rea- 
sonable speech recognition. 
Still, at; the end of these attempts, it beca.me 
clear that we did not yet know the precise char- 
acteristic that makes a linguistically motivated 
grammar intractable for speech recognition, nor 
the best way to retain the advantages of the 
hand-written grammar approach while provid- 
ing reasonable speech recognition. 
672 
4 Incrementa l  Grammar  
Deve lopment  
In our second series of experiments, we in- 
crelnenta.lly developed a. new grammar front 
s('ra.tch. The new gra.mma.r is basica.lly a s('a.led- 
down and a.dapted version of tile Core Lan- 
guage Engine gramme\ for English (Puhnan 
1!)92; Rayner 1993); concrete development work 
a.nd testing were organized a.round a. speech in- 
terfa c(; to a. set; of functionalities oflhred by a 
simple simula,tion of the Space Shuttle (Rather, 
Hockey gll(l James 2000). Rules and lexical 
entries were added in sma.ll groups, typically 
2-3 rules or 5 10 lexical entries in one incre- 
ment. After each round of exl)a.nsion , we tested 
to make sure that the gramlnar could still 1)e 
compiled into a. usa.bh; recognizer, a.nd a.t sev- 
ere.1 points this suggested changes in our iln- 
1)\]ementation strategy. The rest of this section 
describes tile new grmmnar in nlore detail. 
4.1 Overv iew of  Ru les  
The current versions of the grammar and lexi- 
con contain 58 rules a.nd 30J. Ulfinflectesl entries 
respectively. They (:over the tbllowing phenom- 
el i  :~IZ 
1. Top-level utl;er~tnces: declarative clauses, 
WH-qtlestions, Y-N questions, iml)erat;ives, 
etlil)tical NPs and I)Ps, int(;rject.ions. 
~.. / \ ]  9 \,~ H-lnovement of NPs and PPs. 
3. The fbllowing verb types: intr~nsi- 
tive, silnple transitive, PP con:plen-mnt, 
lnodaJ/a.uxiliary, -ing VP con-q)len:ent, par- 
ticleq-NP complement, sentential comple- 
lnent, embedded question complement. 
4. PPs: simple PP, PP with postposition 
("ago")~ PP lnodifica,tion of VP and NP. 
5. Relat;ive clauses with both relative NP pro- 
1101111 ("tit(; telnperature th,tt I measured )
and relative PP ("the (loci: where I am"). 
6. Numeric determiners, time expressions, 
and postmodification of NP 1)y nun:eric ex- 
pressions. 
7. Constituent conjunction of NPs and 
cl~ulses. 
Tilt following examl)le sentences illustrate 
current covera,ge: 3 '-. , ':how ~d)out scenario 
three.?", "wha, t is the temperature?", "mea- 
sure the pressure a,t flight deck", "go to tile 
crew ha.tch a.nd (:lose it", "what were ten:per- 
a.tttt'e a, nd pressure a.t iifteen oh five?", "is the 
telnpera.ture going ttp'. ~', "do the fi?ed sensors 
sa.y tha.t the pressure is decreasing. , "find out 
when the pressure rea.ched fifteen p s i . . . .  wh~t 1 
is the pressure that you mea.sured?", "wha.t is 
the tempera.lure where you a.re?", ?~(:a.n you find 
out when the fixed sensors ay the temperature 
at flight deck reached thirty degrees celsius?". 
4.2 Unusua l  Features  o f  the  Grammar  
Most of the gramn:~u', as already sta.ted, is 
closely based on the Core Language Eng!ne 
gra.nlnla.r. \?e briefly sllnllna.rize the main di- 
vergences between the two gramnlars. 
4.2.1 I nvers ion  
The new gramlna, r uses a. novel trea.tment of 
inversion, which is p~trtly designed to simplify 
the l)l'ocess of compiling a, fea,ture gl'anllna, r into 
context-free form. The CLE grammar's trea.t- 
l l tent of invers ion uses a, movement account, in 
which the fronted verb is lnoved to its notional 
pla.ce in the VP through a feature. So, tbr 
example, the sentence "is pressure low?" will 
in the origina.1 CLE gramma.r ha.re the phrase- 
structure 
::\[\[iS\]l" \ [p ressure \ ]N / ,  \[\[\]V \[IO\V\]AI),\]\]V'\]'\],'g" 
in whk:h the head of th(, VP is a V gap coin- 
dexed with tile fronted main verb 1,~ . 
Our new gra.mn:ar, in contrast, hal:dles in- 
version without movement, by making the con> 
bination of inverted ver\]) and subject into a. 
VBAR constituent. A binary fea.ture invsubj  
picks o:ll; these VBARs, a.nd there is a. question- 
forma,tion rule of tilt form 
S --> VP : E invsub j=y\ ]  
Continuing the example, the new gram- 
mar a.ssigns this sentence tilt simpler phrase- 
structure 
"\[\[\[is\] v \[press:ire\] N*'\] v .A .  \[\[low\] J\] V.\] S" 
4.2.2 Sorta l  Const ra in ts  
Sortal constra,ints are coded into most gr~un:nnr 
rules as synta.ctic features in a straight-forward 
lna.nner, so they are available to the compilation 
673 
process which constructs the context-free gram- 
mar, ~nd ultimately tile language model. The 
current lexicon allows 11 possible sortal values 
tbr nouns, and 5 for PPs. 
We have taken the rather non-standard step 
of organizing tile rules for PP modification so 
that a VP or NP cannot be modified by two 
PPs  of the same sortal type. The principal mo- 
tivation is to tighten the language model with 
regard to prepositions, which tend to be pho- 
netically reduced and often hard to distinguish 
from other function words. For example, with- 
out this extra constraint we discovered that an 
utterance like 
measure temperature at flight deck 
and lower deck 
would frequently be misrecognized as 
measure temperature at flight deck in 
lower deck 
5 Exper iments  with Incremental  
G r am 111 ar  S 
Our intention when developing the new gram- 
mar was to find out just when problems began 
to emerge with respect to compilation of tan- 
gm~ge models. Our initial hypothesis was that 
these would l)robably become serious if the rules 
for clausal structure were reasonably elaborate; 
we expected that the large number of possible 
ways of combining modal and auxiliary verbs, 
question forlnation, movement, and sentential 
complements would rapidly combine to produce 
an intractably loose language model. Interest- 
ingly, this did not prove to be the case. In- 
stead, the rules which appear to be the primary 
ca.use of difficulties are those relating to relative 
clauses. We describe the main results in Sec- 
tion 5.1; quantitative results on recognizer per- 
tbrmance are presented together in Section 5.2. 
5.1 Main Findings 
We discovered that addition of the single rule 
which allowed relative clause modification of an 
NP had a dr~stic effect on recognizer perfor- 
lnance. The most obvious symptoms were that 
recognition became much slower and the size of 
the recognition process much larger, sometimes 
causing it to exceed resource bounds. The false 
reject rate (the l)roportion of utterances which 
fell below the recognizer's mininmnl confidence 
theshold) also increased substantially, though 
we were surprised to discover no significant in- 
crea.se in the word error rate tbr sentences which 
did produce a recognition result. To investi- 
gate tile cause of these effects, we examined the 
results of perfornfing compilation to GSL and 
PFSG level. The compilation processes are such 
that symbols retain mnemonic names, so that it 
is relatively easy to find GSL rules and gral)hs 
used to recognize phrases of specified gralnmat- 
ical categories. 
At the GSL level, addition of the relative 
clause rule to the original unification grammar 
only increased the number of derived Nuance 
rules by about 15%, from 4317 to 4959. The av- 
erage size of the rules however increased much 
more a. It, is easiest o measure size at the level of 
PFSGs, by counting nodes and transitions; we 
found that the total size of all the graphs had in- 
creased from 48836 nodes and 57195 tra.nsitions 
to 113166 nodes and 140640 transitions, rather 
more than doubling. The increase was not dis- 
tributed evenly between graphs. We extracted 
figures for only the graphs relating to specific 
grammatical categories; this showed that, the 
number of gra.1)hs fbr NPs had increased from 
94 to 258, and lnoreover that the average size 
of each NP graph had increased fronl 21 nodes 
and 25.5 transitions to 127 nodes and 165 tra.nsi- 
tions, a more than sixfold increase. The graphs 
for clause (S) phrases had only increased in 
number froln 53 to 68. They ha.d however also 
greatly increased in average size, from 171 nodes 
and 212 transitions to 445 nodes and 572 tran- 
sitions, or slightly less than a threefold increase. 
Since NP and S are by far the most important 
categories in the grammar, it is not strange that 
these large changes m~tke a great difference to 
the quality of the language model, and indi- 
rectly to that of speech recognition. 
Colnparing the original unification gramlnar 
and the compiled CSL version, we were able to 
make a precise diagnosis. The problem with the 
relative clause rules are that they unify feature 
values in the critical S and NP subgralnlnars; 
this means that each constrains the other, lead- 
ing to the large observed increase in the size 
and complexity of the derived Nuance grammar. 
aGSL rules are written in all notat ion which allows 
disjunction and Klccne star. 
674 
Specifically, agreement ilffbrmation and sortal 
category are shared between the two daugh- 
ter NPs in the relative clause modification rule, 
which is schematically as follows: 
Igp: \[agr=A, sort=S\]  --+ 
NP: \[agr=A, sort=S\] 
REL:\[agr=A, sort=S\]  
These feature settings ~re needed in order to get 
tile right alternation in pairs like 
the robot that *measure/measures 
the teml)erature \[agr\] 
the *deck/teml)era.ture tha.t you 
measured \[sort\] 
We tested our hypothesis by colnlnenting ()lit 
the agr and sor t  features in the above rule. 
This completely solves the main 1)robh;in of ex- 
1)lesion in the size of the PFSG representation; 
tile new version is only very slightly larger than 
tile one with no relative clause rule (50647 nodes 
and 59322 transitions against 48836 nodes and 
57195 transitions) Most inL1)ortantty, there is 
no great increase in the number or average size 
of the NP and S graphs. NP graphs increase in 
number froin 94 to 130, and stay constant in a.v- 
era ge size.; S graphs increase in number f}om 53 
to 64, and actually decrease, in aa;erage size to 
13,5 nodes and 167 transitions. Tests on st)eech 
(l~t;a. show that recognition quality is nea~rly :lie 
sa.me as for the version of the recognizer which 
does not cover relative clauses. Although speed 
is still significantly degraded, the process size 
has been reduced sufficiently that the 1)roblen:s 
with resource bounds disappear. 
It would be rea.sonal)le 1:o expect tim: remov- 
ing the explosion in the PFSG ret)resentation 
would result in mL underconstrained language 
model for the relative clause paxt of the gram- 
mar, causing degraded 1)erformance on utter- 
ances containing a, relative clause. Interestingly, 
this does not appear to hapl)en , though recog- 
nition speed under the new grammar is signif- 
icaatly worse for these utterances COml)ared to 
utterances with no relative clause. 
5.2 Recogn i t ion  Resu l ts  
This section summarizes our empirical recog- 
nition results. With the help of the Nuance 
Toolkit batchrec  tool, we evah:ated three ver- 
sions of the recognizer, which differed only with 
respect to tile language model, no_re ls  used 
the version of the language model derived fl'onI a 
granLn:a.r with the relative clause rule removed; 
re l s  is the version derived from the fltll gram- 
lnar; and un l inked  is the colnl)romise version, 
which keeps the relative clause rule but removes 
the critical features. We constructed a corpus 
of 41 utterances, of mean length 12.1 words. 
The utterances were chosen so that the first, 31 
were within the coverage of all three versions 
of the grammar; the last 10 contained relative 
clauses, and were within the coverage of re :s  
and un: inked but :tot of no_rels .  Each utter- 
anee was recorded by eight different subjects, 
none of whom had participated in development 
of the gra.mmar or recognizers. Tests were run 
on a dual-processor SUN Ultra60 with 1.5 GB 
of RAM. 
The recognizer was set, to reject uttera.nces if 
their a.ssociated confidence measure fell under 
the default threshold. Figures 1 and 2 sum- 
marize the re.suits for the first 31 utterances 
(no relative clauses) and the last 10 uttera:Lces 
(relative clauses) respectively. Under '?RT', 
we give inean recognition speed (averaged over 
subjects) e?pressed as a multiple of real time; 
'PRe.j' gives the false reject rate, the :heart l)er - 
centage of utterances which were reiected ue to 
low confidence measures; 'Me:n' gives the lnean 
1)ercentage of uttera.nces which fhiled due to the. 
recognition process exceeding inemory resource 
bounds; and 'WER,' gives the mean word er- 
ror rate on the sentences that were neither re- 
jected nor failed due to resource bound prob- 
lems. Since the distribution was highly skewed, 
all mea.ns were calculated over the six subjects 
renm.i:fing after exclusion of the extreme high 
and low values. 
Looking first at Figure 1, we see that re l s  is 
clearly inferior to no_re ls  on tile subset of the 
corpus which is within the coverage of both ver- 
sions: nea.rly twice as many utterances are re- 
jected due to low confidence values or resource 
1)roblems, and recognition speed is about five 
times slower, un l inked  is in contrast :tot sig- 
nificantly worse than no_re ls  in terms of recog- 
nition performance, though it is still two and a 
half times slower. 
Figure 2 compares re l s  and un l inked on the 
utterances containing a relative clause. It seems 
reasona.ble to say that recognition performance 
675 
I C4ran"nar I I FR .i I IWER 1 
no_rels 1.04 9.0% - 6.0% 
re l s  4.76 16.1% 1.1% 5.7% 
un l inked  2.60 9.6% - 6.5% 
Figure 1: Evaluation results for 31 utterances 
not containing relative clauses, averaged across 
8 subjects excluding extreme values. 
Grammar xRT FRej Men: WER\ ]  
re l s  4.60 26.7% 1.6% 3.5%\] 
un l inked 5.29 20.0% - 5.4%J 
Figure 2: Evaluation results for i0 utter~mces 
containing relative clauses, averaged across 8 
subjects excluding extreme values. 
is comparable for the two versions: rels has 
lower word error rate, but also rqjects more 
utterances. Recognition speed is marginally 
lower for unl inked,  though it is not clear to us 
whether the difference is significant given the 
high variability of the data. 
6 Conc lus ions  and  Fur ther  
D i rec t ions  j 
We found the results presented above surpris- 
ing and interesting. When we 1)egal: our pro- 
gramme of attempting to compile increasingly 
larger linguistically based unification grammars 
into language models, we had expected to see a 
steady combinatorial increase, which we guessed 
would be most obviously related to complex 
clause structure. This did not turn out to be the 
case. Instead, the serious problems we encoun- 
tered were caused by a small number of crit- 
ical rules, of which the one for relative clause 
modification was by the far the worst. It was 
not immediately obvious how to deal with the 
problem, but a careful analysis revealed a rea- 
sonable con:promise solution, whose only draw- 
back was a significant but undisastrous degra- 
dation in recognition speed. 
It seems optimistic to hope that the rela- 
tive clause problem is the end of the story; the 
obvious way to investigate is by continuing to 
expand the gramlnar in the same incremental 
fashion, and find out what happens next. We 
intend to do this over the next few months, and 
expect in due course to be able to l)resent fur- 
ther results. 
References  
H. Alshawi. 1992. The Core Language Engine. 
Cambridge, Massachusetts: The MIT Press. 
J. Bresnan, R.M. Kapla.n, S. Peters and A. Za- 
enen. Cross-Serial Dependencies in Dutch. 
1987. In W. J. Savitch et al(eds.), The For- 
real Complexity of Natural Languagc, Reidel, 
Dordrecht, pages 286-319. 
G. Gazdar, E. Klein, G. Pullum and I. Sag. 
1985. Generalized Phrase Structure Gram- 
mar Basil Blackwell. 
IBM. 1999. ViaVoice SDK tbr Windows, ver- 
sion 1.5. 
R. Moore, J. Dowding, H. Bratt, J.M. Gawron, 
Y. Gorfl:, and A. Cheyer. 1997. Com- 
mandTalk: A Spoken-Language Interface 
tbr Battlefield Simulations. Proceedings 
of the Fifth Conference on Applied Nat- 
uraI Languagc Processing, pages 1-7, 
Washington, DC. Available online from 
http ://www. ai. sri. com/natural-language 
/project s/arpa-sl s / commandt alk. html. 
Nuance Communications. 1999. Nuance Speech 
Recognition System Developer's Manv, aI, Ver- 
sion 6.2 
G. Pullum and G. Gazdar. 1982. Natural Lan- 
guages and Context-Free Languages. Lin- 
guistics and Philosophy, 4, pages 471-504. 
S.G. Puhnan. 1992. Unification-Based Synta.c- 
tic Analysis. In (Alshawi 1992) 
M. Rayner. 1993. English Linguistic Coverage. 
In M.S. Agn~s et al 1993. Spoken Language 
Translator: First Year Report. SRI Techni- 
cal Report CRC-043. Available online from 
http ://www. sri. com. 
M. Rayner, B.A. Hockey and F. James. 2000. 
Turning Speech into Scripts. To appear in 
P~vceedings of the 2000 AAAI Spring Sym- 
posium on Natural Language Dialogues with 
Practical Robotic Devices 
A. Stent, J. Dowding, J.M. Gawron, E.O. 
Bratt, and R. Moore. 1999. The Coin- 
mandTalk Spoken Dialogue System. P'rv- 
cecdings of the 37th Annual Meeting of the 
ACL, pages 183-190. Available online from 
ht tp  ://www. a i .  s r i .  com/natura l - language 
/p ro jec t  s /a rpa-s  :s  / commandt a:k.  html. 
676 
Building a Robust Dialogue System with Limited Data * 
Sharon  J .  Go ldwater ,  E l i zabeth  Owen Brat t ,  Jean  Mark  Gawron ,  and  John  Dowdingt  
, .  SR I  In ternat iona l  
333 Ravenswood Avenue 
Men lo  Park ,  CA 94025 
{goldwater, owen, gawron, dowding} @ai.sri.cora 
Abst ract  
We describe robustness techniques used in the Com- 
mandTalk system at: the recognition level, the pars- 
ing level, and th~ dia16gue level, and how these were 
influenced by the lack of domain data. We used 
interviews with subject matter experts (SME's) to 
develop a single grammar for recognition, under- 
standing, and generation, thus eliminating the need 
for a robust parser. We broadened the coverage of 
the recognition grammar by allowing word insertions 
and deletions, and we implemented clarification and 
correction subdialogues to increase robustness at the 
dialogue level. We discuss the applicability of these 
techniques to other domains. 
1 I n t roduct ion  
Three types of robustness must be considered when 
designing a dialogue system. First, there is robust- 
ness at the recognition level. When plentiful data 
is available, a robust n-gram language model can be 
produced, but when data is limited, producing a ro- 
bust language model for recognition can be prob- 
lematic. Second, there is robustness at the level 
of the parser. Robust parsing is often achieved by 
combining a full parser with a partial parser and 
fragment-combining rules, but even then some utter- 
ances may be correctly recognized, only to be parsed 
incorrectly or not at all. Finally, there is robustness 
at the dialogue level. Utterances may be uninter- 
pretable within the context of the dialogue due to 
errors on the part of either the system or the user, 
and the dialogue manager should be able to handle 
such problems gracefully. 
Our CommandTalk dialogue system was designed 
for a highly specialized omain with little available 
data, so finding ways to build a robust system with 
* This research was supported by the Defense Advanced Re- 
search Projects Agency under Contract N66001-94-C-6046 
with the Space and Naval Warfare Systems Center. The views 
and conclusions contained in this document are those of the 
authors and should not be interpreted asnecessarily repre- 
senting the official policies, either express or implied, of the 
Defense Advanced Research Projects Agency of the U.S. Gov- 
ernment. 
? Currently affiliated with GO.corn 
limited data was a major concern. In this paper, 
we discuss our methods and their applicability to 
other domains. Section 2 gives a brief overview of 
the CommandTalk system. In Section 3, we discuss 
the approach we took to building recognition, under- 
standing, and generaffon models for CommandTalk, 
and how it relates to the first two types of robustness 
mentioned. Section 4 discusses additional robust- 
ness techniques at the recognizer level, and Section 5 
describes dialogue-level robustness techniques. Sec- 
tion 6 discusses the applicability of our methods to 
other domains. 
2 CommandTa lk  
CommandTalk is a spoken-language interface to the 
ModSAF (Modular Semi-Automated Forces) battle- 
field simulator, developed with the goal of allow- 
ing military commanders to interact with simulated 
forces in a manner as similar as possible to the way 
they would command actual forces. CommandTalk 
allows the use of ordinary English commands and 
mouse gestures to 
? Create forces and control measures (points and 
lines) 
? Assign missions to forces 
? Modify missions during execution 
? Control ModSAF system functions, such as the 
map display 
? Get information about the state of the simula- 
tion 
CommandTalk consists of a number of indepen- 
dent, cooperating agents interacting through SRI's 
Open Agent Architecture (OAA) (Martin et al, 
1998). OAA uses a facilitator agent that plans and 
coordinates interactions among agents during dis- 
tributed computation. An introduction to the basic 
CommandTalk agents can be found in Moore et al 
(1997). CommandTalk's dialogue component is de- 
scribed in detail in Stent et al (1999), and its use 
of linguistic and situational context is described in 
Dowding et al (1999). 
61 
3 The  One-Grammar  Approach  
In a domain with limited data, the inability to col- 
lect a sufficient corpus for training a statistical lan- 
guage model can be a significant problem. For 
CommandTalk, we did not create a statistical lan- 
guage model. Instead, with information gathered 
from interviews of subject matter experts (SME's), 
we developed a handwritten grammar using Gemini 
(Dowding et al, 1993), a unification-based gram- 
mar formalism. We used this unification grammar 
for both natural language understanding and gener- 
ation, and, using a grammar compiler we developed, 
compiled it into a context-free form suitable for the 
speech recognizer as well. 
The effe~s_ of this single-grammar pproach on 
the robustness of the CommandTalk system were 
twofold. On the negative side, we presumably ended 
up with a recognition language model with less cov- 
erage than a statistical model would have had. Our 
attempts to deal with this are discussed in the next 
section. On the positive side, we eliminated the 
usual discrepancy incoverage between the recognizer 
and the natural language parser. This was advanta- 
geous, since no fragment-combining or other parsing 
robustness techniques were needed. 
Our approach ad other advantages a well. Any 
changes we made to the understanding grammar 
were automatically reflected in the recognition and 
generation grammars, making additions and modifi- 
cations efficient. Also, anecdotal evidence suggests 
that the language used by the system often influ- 
ences the language used by speakers, o maintaining 
consistency between the input and output of the sys- 
tem is desirable. 
4 Ut terance-Leve l  Robustness  
It is difficult o write a grammar that is constrained 
enough to be useful without excluding some rea- 
sonable user utterances. To alleviate this prob- 
lem, we modified the speech recognition grammar 
and natural language parser to allow certain "close- 
to-grammar" utterances. Utterances with inserted 
words, such as Center on Checkpoint 1 now or zoom 
way out (where Center on Checkpoint 1 and zoom 
out are grammatical) were permitted by allowing 
the recognizer to skip unknown words. We also al- 
lowed utterances with deleted words, as long as those 
words did not contribute to the semantics of the ut- 
terance as determined by the Gemini semantic rules 
constraining logical forms. For example, a user could 
say, Set speed, 40 kph rather than Set speed to 40 kph. 
The idea behind these modifications was to allow ut- 
terances with a slightly broader ange of wordings 
than those in the grammar, but with essentially the 
same meanings: 
We began by testing the effects of these modi- 
fications on in-grammar utterances, to ensure that 
Time, CPURT 
SRR 
AWER 
SER 
Non-Robust Robust 
0.664 : 1.05 
2.56% 1.70% 
1.68% 2.94% 
10.00% ~ 12.07% 
Table 1: In-Grammar Recognition Results 
they did not significantly decr egse recognition per- 
formance. We used a small test corpus of approxi- 
mately 800 utterances read by SRI employees. We 
collected four measures of performance: 
? Recognition time, measured, in multiples of 
CPU real time (CPURT). A recognition time 
of lxCPURT means that on,our CPU (a Sun 
Ultra2), recognition took exactly as~ long as the 
duration of the utterance. : 
? Sentence reject rate (SRR).' The percentage of 
sentences that the recognizer rejects. 
? Adjusted word error rate (A:WER). The per- 
centage of words in non:rejected sentences that 
are misrecognized. 
? Sentence rror rate (SER). The percentage of 
sentences in which some sort of error occurred, 
either a complete rejection or misrecognized 
word. 
Several parameters affected the results, most no- 
tably the numerical penalties assigned for inserting 
or deleting words, and the pruning threshold of the 
recognizer. Raising the pruning threshold caused 
both reject and error rates to go down, but slowed 
recognition. Lowering the penalties caused rejection 
rates to go down, but word and Sentence rror rates 
to go up, since some sentences which had been re- 
jected were now recognized partially correctly, and 
some sentences which had been recognized correctly 
now included some errors. Lowering the penalties 
also led to slower recognition. 
Table 1 shows recognition results for the non- 
robust and robust versions 0f the recognition gram- 
mar on in-grammar utterances: Th e pruning thresh- 
old is the same for both versions and the insertion 
and deletion penalties are set to intermediate val- 
ues. Recognition times for the robust grammar are 
about 60% slower than those of the control gram- 
mar, but still at acceptable l vels. Reject and error 
rates are fairly close for the two grammars. Overall, 
adding robustness to the recognition grammar did 
not severely penalize in-grammar recognition per- 
formance. 
We had very little out-of-grammar data for Com- 
mandTalk, and finding subjects in this highly spe- 
cialized domain would have been difficult and ex- 
pensive. To test our robustness techniques on out- 
62 
of-grammar utterances, we decided to port them 
to another domain with easily accessible users and 
data; namely, the ATIS air travel domain. We wrote 
a small grammar covering part of the ATIS data 
and ,compiled it into a recognition grammar using 
the same techniques as in CommandTalk. Unfortu- 
nately, we were unable to carry out any experiments, 
because the recognition grammar we derived yielded 
recognition times that were so slow as to be imprac- 
tical. We discuss these results further in Section 6. 
5 Diaiogue-Level Robustness 
To be considered robust at the dialogue level, a sys- 
tem must be able to deal with situations where an 
utterance is recognized and parsed, but cannot be in- 
terpreted withi~4he current system state or dialogue 
context. In addition~it must be easy for the user to 
correct faulty interpretations on the part of the sys- 
tem. Contextual interpretation problems may occur 
for a variety of reasons, including misrecognitions, 
incorrect reference resolution, and confusion or in- 
completeness on the part of the user. 
The CommandTalk dialogue manager maintains 
a Stack to ~keep 'track of the current discourse con- 
text and uses small finite-state machines to represent 
different~ types of subdialogues. Below we illustrate 
some types of  subdialogues and other techniques 
which provide robustness at the dialogue level. Note 
that for each utterance, we write what the system 
recognizes, not what the user actually says. 
5.1 Correction Subdlalogues 
Sx? 1: 
U 1 :Create a CEV at 76 53 
S 2 ?. 
U 3 Put Objective Golf here <click> 
S 4 ? I will locate Objective Golf at FQ 
? 658 583 
U 5 I said Objective Alpha 
S 6 ? I will locate Objective Alpha at FQ 
658 853 
Allowing the user to correct full or partial utterances 
can remedy interpretation problems caused by mis- 
recognitions, incorrect reference resolution, or user 
error. 
In Example 1, the system responds to the user's 
first utterance by producing a rising tone, illustrated 
by the ? symbol, to indicate successful interpreta- 
tion and execution of the command, in this case cre- 
ation of a CEV, a type of vehicle. (Unsuccessful 
interpretation is indicated by a falling tone, illus- 
trated by the ? symbol.) In utterances 3 through 
6, a misrecognition causes the system to perform 
the wrong behavior. The user initiates a correction 
subdialogue, and the system goes on to correctly re- 
interpret he full utterance. 
5.2 Impl ic i t  Conf i rmat ion  
Ex. 2: 
U 7 CEV, advance to Checkpoint 1 
S 8 ? 100All will proceed to Checkpoint 
1 
Use of implicit confirmation in combination with 
correction subdialogues makes it easy to correct 
faulty interpretations as soon as possible by alert- 
ing the user to possible sources of error. 
In utterances 7 and 8, the system must resolve 
the user's reference, "CEV", to a particular unit. 
It therefore choes the user's command using the 
CEV's unique call sign. This makes explicit the sys- 
tem's interpretation of the user's utterance, giving 
the user a chance to correct he system if necessary. 
Note that utterance 4 also contains an implicit 
confirmation, since the system has resolved the 
user's gesture to a set of coordinates. 
5.3 C lar i f i ca t ion  Subd ia logues  
Ex.  3: 
U 9 
S 10 
U 11 
Create a CEV at 72 69 
? 
CEV, conduct crater breach facing- 
south 
S 12 ? There are two CEV's. Do you mean 
100All or 100A12? 
U 13 A12 
S 14 ? Which point should 100A12 conduct 
crater breach oriented south at? 
U 15 Checkpoint 2 
S 16 ? There is no Checkpoint 2. Which 
point should 100A12 conduct crater 
breach oriented south at? 
U 17 I meant Checkpoint 1 
S 18 ? 100A12 will conduct crater breach 
oriented south at Checkpoint 1 
Clarification subdialogues are generally initiated by 
the system as a result of errors or incomplete com- 
mands on the part of the user. 
Example 3 illustrates three different ypes of prob- 
lems that can be corrected by system questions. 
First, the user's reference to "CEV" in utterance 
11 is ambiguous, so the system asks a question to 
determine which CEV the user is referring to. Next, 
the system asks the user to supply a missing piece 
of information that is required to carry out the com- 
mand. Finally, when the user makes an error by 
referring to a point that doesn't exist, the system 
prompts for a correction. 
6 Discussion and Conclusions 
CommandTalk is an example of a successful and ro- 
bust dialogue system in a domain with limited ac- 
63 
cess to both data and subjects. The pre-dialogue 
version of CommandTalk was used in the STOW 
(Synthetic Theater of War) '97 ACTD (Advanced 
Concept Technology Demonstration) exercise, an in- 
tensive 48-hour continuous military simulation by 
all four U.S. military services, and received high 
praise. The dialogue portion of the system has in- 
creased CommandTalk's usefulness and robustness. 
Nevertheless, everal questions remain, not the least 
of which is whether the robustness techniques used 
for CommandTalk can be successfully transferred to 
other domains. 
We have no doubt that our methods for adding ro- 
bustness at the dialogue level can and should be im- 
plemented in other domains, but this is not as clear 
for our parsing a-nd recognition robustness methods. 
The one-grammar approach is key to our elimi- 
nating the necessity for robust parsing, renders a 
large corpus for generating a recognition model un- 
necessary, and has other advantages as well. Yet 
our experience in the ATIS domain suggests that 
further research into this approach is needed. Our 
ATIS grammar is based on a grammar of general 
English and has a very different structure from that 
of CommandTalk's semantic grammar, but we were 
unable to isolate the factor or factors responsible for 
its poor recognition performance. Recent research 
(Rayner et al, 2000) suggests that it may be pos- 
sible to compile a useful recognition model from a 
general English unification grammar if the gram- 
mar is constructed carefully and a few compromises 
are made. We also believe that using an appropri- 
ate grammar approximation algorithm to reduce the 
complexity of the recognition model may prove fruit- 
ful. This would reintroduce some discrepancy be- 
tween the recognition and understanding language 
models, but maintain the other advantages of the 
one-grammar pproach. 
In either case, the effectiveness of our recognition 
robustness techniques remains an open question. We 
know they have no significant negative impact on in- 
grammar ecognition, but whether they are helpful 
in recognizing and~ more importantly, interpreting 
out-of-grammar utterances is unknown. We have 
been unable to evaluate them so far in the Com- 
mandTalk or any other domain, although we hope 
to do so in the future. 
Another possible solution to the problem of 
producing a workable robust recognition grammar 
would return to a statistical approach rather than 
using word insertions and deletions. Stolcke and 
Segal (1994) describe a method for combining a 
context-free grammar with an n-gram model gen- 
erated from a small corpus of a few hundred utter- 
ances to create a more accurate n-gram model. This 
method would provide a robust recognition model 
based on the context-free grammar compiled from 
64 
our unification grammar. We would'still have to 
write only one grammar for the system, it would still 
influence the recognition model, and we could still 
be sure that the system would never say anything it 
couldn't recognize. This approach Would require us- 
ing robust parsing methods, but might be the best 
solution for other domains if compiling a practical 
recognition grammar proves too difficult. 
Despite the success of the CommandTalk system, 
it is clear that more investigation is called for to 
determine how best to develop dialogue systems in 
domains with limited data. Researchers must de- 
termine which types of unification grammars can be 
compiled into practical recognition grammars using 
existing technology, whether grammar approxima- 
tions or other techniques can produce good results 
for a broader range of grammars, whether allow- 
ing word insertions and deletions is an effective ro- 
bustness technique, orwhether we should use other 
methods altogether. 
Re ferences  
J. Dowding, J. Gawron, D. Appelt, L. Cherny, 
R. Moore, and D. Moran. 1993. Gemini: A Natu- 
ral Language System for Spoken Language Under- 
standing. In Proceedings of the Thirty-First An- 
nual Meeting of the ACL, Columbus, OH. Associ- 
ation for Computational Linguistics. 
J. Dowding, E. Owen Bratt, and S. Goldwater. 
1999. Interpreting Language in Context in Com- 
mandTalk. In Communicative Agents: The Use 
of Natural Language in Embodied Systems, pages 
63-67. 
D. Martin, A. Cheyer, and D. Moran. 1998. Build- 
ing Distributed Software Systems with the Open 
Agent Architecture. In Proceedings of the Third 
International Conference on the Practical Appli- 
cation of Intelligent Agents and Multi-Agent Tech- 
nology, Blackpool, Lancashire, UK. The Practical 
Application Company Ltd. 
R. Moore, J. Dowding, H. Bratt, J. Gawron, 
Y. Gorfu, and A. Cheyer. 1997. CommandTalk: 
A Spoken-Language Interface for Battlefield Sim- 
ulations. In Proceedings of the Fifth Conference 
on Applied Natural Language Processing, pages 
1-7, Washington, DC. Association for Computa- 
tional Linguistics. 
M. Rayner, B. A. Hockey, F. James, E. Owen Bratt, 
S. Goldwater, and J. M. Gawron. 2000. Compil- 
ing Language Models from a Linquistically Moti- 
vated Unification Grammar. Submitted to COL- 
ING '00. 
A. Stent, J. Dowding, J. Gawron, E. Owen Bratt, 
and R. Moore. 1999. The CommandTalk Spoken 
Dialogu.e System. In Proceedings of the 37th An- 
nual Meeting of the A CL. Association of Compu- 
tational Linguistics. 
A. Stolcke and J. Segal. 1994. Precise N-Gram 
Probabilities from Stochastic Context-free Gram- 
mar.: In Proceedings of the 32nd Annual Meeting 
off :the ~Association for Computational Linguistics, 
pages 74~-79, 
65 
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 575?584,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Two Decades of Unsupervised POS induction: How far have we come?
Christos Christodoulopoulos
School of Informatics
University of Edinburgh
christos.c@ed.ac.uk
Sharon Goldwater
School of Informatics
University of Edinburgh
sgwater@inf.ed.ac.uk
Mark Steedman
School of Informatics
University of Edinburgh
steedman@inf.ed.ac.uk
Abstract
Part-of-speech (POS) induction is one of the
most popular tasks in research on unsuper-
vised NLP. Many different methods have been
proposed, yet comparisons are difficult to
make since there is little consensus on eval-
uation framework, and many papers evalu-
ate against only one or two competitor sys-
tems. Here we evaluate seven different POS
induction systems spanning nearly 20 years of
work, using a variety of measures. We show
that some of the oldest (and simplest) systems
stand up surprisingly well against more recent
approaches. Since most of these systems were
developed and tested using data from the WSJ
corpus, we compare their generalization abil-
ities by testing on both WSJ and the multi-
lingual Multext-East corpus. Finally, we in-
troduce the idea of evaluating systems based
on their ability to produce cluster prototypes
that are useful as input to a prototype-driven
learner. In most cases, the prototype-driven
learner outperforms the unsupervised system
used to initialize it, yielding state-of-the-art
results on WSJ and improvements on non-
English corpora.
1 Introduction
In recent years, unsupervised learning has become
a hot area in NLP, in large part due to the use of
sophisticated machine learning approaches which
promise to deliver better results than more tradi-
tional methods. Often the new approaches are tested
using part-of-speech (POS) tagging as an example
application, and usually they are shown to perform
better than one or another comparison system. How-
ever, it is difficult to draw overall conclusions about
the relative performance of unsupervised POS tag-
ging systems because of differences in evaluation
measures, and the fact that no paper includes di-
rect comparisons against more than a few other sys-
tems. In this paper, we attempt to remedy that
situation by providing a comprehensive evaluation
of seven different POS induction systems spanning
nearly 20 years of research. We focus specifically
on POS induction systems, where no prior knowl-
edge is available, in contrast to POS disambigua-
tion systems (Merialdo, 1994; Toutanova and John-
son, 2007; Naseem et al, 2009; Ravi and Knight,
2009; Smith and Eisner, 2005), which use a dic-
tionary to provide possible tags for some or all of
the words in the corpus, or prototype-driven sys-
tems (Haghighi and Klein, 2006), which use a small
set of prototypes for each tag class, but no dictio-
nary. Our motivation stems from another part of our
own research, in which we are trying to use NLP
systems on over 50 low-density languages (some of
them dead) where both tagged corpora and language
speakers are mostly unavailable. We therefore de-
sire to use these systems straight out of the box and
to know how well we can expect them to work.
One difficulty in evaluating POS induction sys-
tems is that there is no straightforward way to map
the clusters found by the algorithm onto the gold
standard tags; moreover, some systems are designed
to induce the number of clusters as well as their
contents, so the number of found clusters may not
match either the gold standard or that of another sys-
tem. Nevertheless, most recent papers have used
mapping-based performance measures (either one-
to-one or many-to-one accuracy). Here, we argue
that the entropy-based V-Measure (Rosenberg and
575
Hirschberg, 2007) is more useful in many cases, be-
ing more stable across different numbers of found
and true clusters, and avoiding several of the prob-
lems with another commonly used entropy-based
measure, Variation of Information (Meila?, 2003).
Using V-Measure along with several other evalu-
ation measures, we compare the performance of the
different induction systems on bothWSJ (the data on
which most systems were developed and tested) and
Multext East, a corpus of parallel texts in eight dif-
ferent languages. We find that for virtually all mea-
sures and datasets, older systems using relatively
simple models and algorithms (Brown et al, 1992;
Clark, 2003) work as well or better than systems
using newer and often far more sophisticated and
time-consuming machine learning methods (Gold-
water and Griffiths, 2007; Johnson, 2007; Graca et
al., 2009; Berg-Kirkpatrick et al, 2010). Thus, al-
though these newer methods have introduced po-
tentially useful machine learning techniques, they
should not be assumed to provide the best perfor-
mance for unsupervised POS induction.
In addition to our review and comparison, we in-
troduce a new way to both evaluate and potentially
improve a POS induction system. Our method is
based on the prototype-driven learning system of
Haghighi and Klein (2006), which achieves very
good performance by using a hand-selected list of
prototypes for each syntactic cluster. We instead use
the existing POS induction systems to induce proto-
types automatically, and evaluate the systems based
on the quality of their prototypes. We find that the
oldest system tested (Brown et al, 1992) produces
the best prototypes, and that using these prototypes
as input to Haghighi and Klein?s system yields state-
of-the-art performance on WSJ and improvements
on seven of the eight non-English corpora.
2 POS Induction Systems
We describe each system only briefly; for details,
see the respective papers, cited below. Each system
outputs a set of syntactic clusters C; except where
noted, the target number of clusters |C| must be
specified as an input parameter. Since we are in-
terested in out-of-the-box performance, we use the
default parameter settings for each system, except
for |C|, which is varied in some of our experiments.
The systems are as follows:1
[brown]: Class-based n-grams (Brown et al,
1992). This is the oldest and one of the simplest sys-
tems we tested. It uses a bigram model where each
word type is assigned to a latent class (a hard assign-
ment), and the probability of the corpus w1 . . . wn
is computed as P (w1|c1)
?n
i=2 P (wi|ci)P (ci|ci?1),
where ci is the class of wi. The goal is to opti-
mize the probability of the corpus under this model.
The authors use an approximate search procedure:
greedy agglomerative hierarchical clustering fol-
lowed by a step in which individual word types are
considered for movement to a different class if this
improves the corpus probability.
[clark]: Class-based n-grams with morphology
(Clark, 2003). This system uses a similar model
to the previous one, and also clusters word types
(rather than tokens, as the rest of the systems do).
The main differences between the systems are that
clark uses a slightly different approximate search
procedure, and that he augments the probabilistic
model with a prior that prefers clusterings where
morphologically similar words are clustered to-
gether. The morphology component is implemented
as a single-order letter HMM.
[cw]: Chinese Whispers graph clustering (Bie-
mann, 2006). Unlike the other systems we consider,
this one induces the value of |C| rather than taking
it as an input parameter.2 The system uses a graph
clustering algorithm called Chinese Whispers that is
based on contextual similarity. The algorithm works
in two stages. The first clusters the most frequent
10,000 words (target words) based on their context
statistics, with contexts formed from the most fre-
quent 150-250 words (feature words) that appear ei-
1Implementations were obtained from:
brown: http://www.cs.berkeley.edu/?pliang/
software/brown-cluster-1.2.zip (Percy Liang),
clark: http://www.cs.rhul.ac.uk/home/alexc/
pos2.tar.gz (Alex Clark),
cw: http://wortschatz.uni-leipzig.de/%7Ecbiemann/
software/jUnsupos1.0.zip (Chris Biemann),
bhmm, vbhmm, pr, feat: by request from the authors of the
respective papers.
2Another recent model that induces |C| is the Infinite HMM
(Van Gael et al, 2009). Unfortunately, we were unable to ob-
tain code for the IHMM in time to include it in our analysis.
Van Gael et al (2009) report results of around 59% V-Measure
on WSJ, with 194 induced clusters, which is not as good as the
best system scores in Section 4.
576
ther to the left or right of a target word. The second
stage deals with medium and low frequency words
and uses pairwise similarity scores calculated by the
number of shared neighbors between two words in
a 4-word context window. The final clustering is
a combination of the clusters obtained in the two
stages. While the number of target words, feature
words, and window size are in principle parameters
of the algorithm, they are hard-coded in the imple-
mentation we used and we did not change them.
[bhmm]: Bayesian HMM with Gibbs sampling
(Goldwater and Griffiths, 2007). This system is
based on a standard HMM for POS tagging. It dif-
fers from the standard model by placing Dirichlet
priors over the multinomial parameters defining the
state-state and state-emission distributions, and uses
a collapsed Gibbs sampler to infer the hidden tags.
The Dirichlet hyperparameters ? (which controls the
sparsity of the transition probabilities) and ? (which
controls the sparsity of the emission probabilities)
can be fixed or inferred. We used a bigram version
of this model with hyperparameter inference.
[vbhmm]: Bayesian HMM with variational
Bayes (Johnson, 2007). This system uses the
same bigram model as bhmm, but uses variational
Bayesian EM for inference. We fixed the ? and ?
parameters to 0.1, values that appeared to be reason-
able based on Johnson (2007), and which were also
used by Graca et al (2009).
[pr]: Sparsity posterior-regularization HMM
(Graca et al, 2009). The Bayesian approaches de-
scribed above encourage sparse state-state and state-
emission distributions only indirectly through the
Dirichlet priors. This system, while utilizing the
same bigram HMM, encourages sparsity directly
by constraining the posterior distributions using the
posterior regularization framework (Ganchev et al,
2009). A parameter ? controls the strengths of the
constraints (default = 25). Following Graca et al
(2009), we set ? = ? = 0.1.
[feat]: Feature-based HMM (Berg-Kirkpatrick
et al, 2010). This system uses a model that has the
structure of a standard HMM, but assumes that the
state-state and state-emission distributions are logis-
tic, rather than multinomial. The logistic distribu-
tions allow the model to incorporate local features
of the sort often used in discriminative models. The
default features are morphological, such as character
trigrams and capitalization.
3 Evaluation Measures
One difficulty in comparing POS induction meth-
ods is in finding an appropriate evaluation measure.
Many different measures have been proposed over
the years, but there is still no consensus on which is
best. In addition, some measures with supposed the-
oretical advantages, such as Variation of Information
(VI) (Meila?, 2003) have had little empirical analy-
sis. Our goal in this section is to determine which
of these measures is most sensible for evaluating
the systems presented above. We first describe each
measure before presenting empirical results. Except
for VI, all measures range from 0 to 1, with higher
scores indicating better performance.
[many-to-1]: Many-to-one mapping accuracy
(also known as cluster purity) maps each cluster to
the gold standard tag that is most common for the
words in that cluster (henceforth, the preferred tag),
and then computes the proportion of words tagged
correctly. More than one cluster may be mapped to
the same gold standard tag. This is the most com-
monly used metric across the literature as it is in-
tuitive and creates a meaningful POS sequence out
of the cluster identifiers. However, it tends to yield
higher scores as |C| increases, making comparisons
difficult when |C| can vary.
[crossval]: Cross-validation accuracy (Gao and
Johnson, 2008) is intended to address the problem
with many-to-one accuracy which is that assigning
each word to its own class yields a perfect score. In
this measure, the first half of the corpus is used to
obtain the many-to-one mapping of clusters to tags,
and this mapping is used to compute the accuracy of
the clustering on the second half of the corpus.
[1-to-1]: One-to-one mapping accuracy
(Haghighi and Klein, 2006) constrains the mapping
from clusters to tags, so that at most one cluster can
be mapped to any tag. The mapping is performed
greedily. In general, as the number of clusters
increases, fewer clusters will be mapped to their
preferred tag and scores will decrease (especially
if the number of clusters is larger than the number
of tags, so that some clusters are unassigned and
receive zero credit). Again, this makes it difficult to
577
compare solutions with different values of |C|.
[vi]: Variation of Information (Meila?, 2003) is
an information-theoretic measure that regards the
system output C and the gold standard tags T as two
separate clusterings, and evaluates the amount of in-
formation lost in going from C to T and the amount
of information gained, i.e., the sum of the condi-
tional entropy of each clustering conditioned on the
other. More formally, V I(C, T ) = H(T |C) +
H(C|T ) = H(C)+H(T )? 2I(C, T ), where H(.)
is the entropy function and I(.) is the mutual infor-
mation. VI and other entropy-based measures have
been argued to be superior to accuracy-based mea-
sures such as those above, because they consider
not only the majority tag in each cluster, but also
whether the remainder of the cluster is more or less
homogeneous. Unlike the other measures we con-
sider, lower scores are better (since VI measures the
difference between clusterings in bits).
[vm]: V-Measure (Rosenberg and Hirschberg,
2007) is another entropy-based measure that is de-
signed to be analogous to F-measure, in that it is de-
fined as the weighted harmonic mean of two values,
homogeneity (h, the precision analogue) and com-
pleteness (c, the recall analogue):
h = 1? H(T |C)
H(T )
(1)
c = 1? H(C|T )
H(C)
(2)
VM = (1 + ?)hc
(?h) + c
(3)
As with F-measure, ? is normally set to 1.
[vmb]: V-beta is an extension to V-Measure, pro-
posed by (Vlachos et al, 2009). They noted that
V-Measure favors clusterings where the number of
clusters |C| is larger than the number of POS tags
|T |. To address this issue the parameter ? in equa-
tion 3 is set to |C|/|T | in order adjust the balance
between homogeneity and completeness.
[s-fscore]: Substitutable F-score (Frank et al,
2009). One potential issue with all of the above mea-
sures is that they require a gold standard tagging to
compute. This is normally available during develop-
ment of a system, but if the system is deployed on a
novel language a gold standard may not be available.
In addition, there is the question of whether the gold
standard itself is ?correct?. Recently, Frank et al
(2009) proposed this novel evaluation measure that
requires no gold standard, instead using the concept
of substitutability to evaluate performance. Instead
of comparing the system?s clusters C to gold stan-
dard clusters T , they are compared to a set of clus-
ters S created from substitutable frames, i.e., clus-
ters of words that occur in the same syntactic en-
vironment. Ideally a substitutable frame would be
created by sentences differing in only one word (e.g.
?I want the blue ball.? and ?I want the red ball.?)
and the resulting cluster would contain the words
that change (e.g. [blue, red]). However since it is
almost impossible to find these types of sentences
in real-world corpora, the authors use frames cre-
ated by two words appearing in the corpus with ex-
actly one word between (e.g. the ?- ball). Once the
substitutable clusters have been created, they can be
used to calculate the Precision (SP ), Recall (SR)
and F-score (SF ) of the system?s clustering:
SP =
?
s?S
?
c?C |s ? c|(|s ? c| ? 1)
?
c?C |c|(|c| ? 1)
(4)
SR =
?
s?S
?
c?C |s ? c|(|s ? c| ? 1)
?
s?S |s|(|s| ? 1)
(5)
SF = 2 ? SP ? SR
SP + SR
(6)
3.1 Empirical results
We mentioned a few strengths and weaknesses of
each evaluation method above; in this section we
present some empirical results to expand on these
claims. First, we examine the effects of varying |C|
on the behavior of the evaluation measures, while
keeping the number of gold standard tags the same
(|T | = 45). Results were obtained by training and
evaluating each system on the full WSJ portion of
the Penn Treebank corpus (Marcus et al, 1993). Fig-
ure 1 shows the results from the Brown system for
|C| ranging from 1 to 200; the same trends were ob-
served for all other systems.3 In addition, Table 1
provides results for the two extremes of |C| = 1 (all
words assigned to the same cluster) and |C| equal to
the size of the corpus (a single word per cluster), as
3The results reported in this paper are only a fraction
of the total from our experiments; given the number of
parameters, models and measures tested, we obtained over
15000 results. The full set of results can be found at
http://homepages.inf.ed.ac.uk/s0787820/pos/.
578
Figure 1: Scores for all evaluation measures as a function of the number of clusters returned [model:brown, corpus:wsj,
|C|:{1-200}, |T |:45]. The right-hand y-axis shows VI scores (lower is better); the left-hand y-axis shows percentage
scores for all other measures. The vertical line indicates |T |. Many-to-1 is invisible as it tracks crossval so closely.
measure super random all single
many-to-1 97.85 13.97 13.97 100
crossval 97.59 13.98 13.98 0
1-to-1 97.86 2.42 13.97 0.01
vi 0.35 9.81 4.33 15.82
vm 95.98 0.02 0 35.42
vmb 95.98 0 0 99.99
s-fscore 7.53 0.50 0 0
Table 1: Baseline scores for the different evaluation mea-
sures on the WSJ corpus. For all measures except VI
higher is better.
well as two other baselines (a supervised tagging4
and a random clustering with |C| = 45).
These empirical results confirm that certain mea-
sures favor solutions with many clusters, while oth-
ers prefer fewer clusters. As expected, many-to-1
correlates positively with |C|, rising to almost 85%
with |C| = 200 and reaching 100% when the num-
ber of clusters is maximal (i.e., single). Recall that
crossval was proposed as a possible solution to this
problem, and it does solve the extreme case of sin-
gle, yielding 0% accuracy rather than 100%. How-
ever, it patterns just like many-to-1 for up to 200
clusters, suggesting that there is very little difference
4We used the Stanford Tagger trained on the WSJ corpus:
http://nlp.stanford.edu/software/tagger.shtml.
between the two for any reasonable number of clus-
ters, and we should be wary of using either one when
|C| may vary.
In contrast to these measures are 1-to-1 and vi: for
the most part, they yield worse performance (lower
1-to-1, higher vi) as |C| increases. However, in this
case the trend is not monotonic: there is an initial
improvement in performance before the decrease be-
gins. One might hope that the peak in performance
would occur when the number of clusters is approx-
imately equal to the number of gold standard tags;
however, the best performance for both 1-to-1 and
vi occurs with approximately 25-30 clusters, many
fewer than the gold standard 45.
Next we consider vm and vmb. Interestingly, al-
though vmbwas proposed as a way to correct for the
supposed tendency of vm to increase with increas-
ing |C|, we find that vm is actually more stable than
vmb over different values of |C|. Thus, if the goal
is to compare systems producing different numbers
of clusters (especially important for systems that in-
duce the number of clusters), then vm seems more
appropriate than any of the above measures, which
are more standard in the literature.
Finally, we analyze the behavior of the gold-
standard-independent measure, s-fscore. On the
positive side, this measure assigns scores of 0 to the
579
two extreme cases of all and single, and is relatively
stable across different values of |C| after an initial
increase. It assigns a lower score to the supervised
system than to brown, indicating that words in the
supervised clusters (which are very close to the gold
standard) are actually less substitutable than words
in the unsupervised clusters. This is probably due to
the fact that the gold standard encodes ?pure? syn-
tactic classes, while substitutability also depends on
semantic characteristics (which tend to be picked up
by unsupervised clustering systems as well). An-
other potential problem with this measure is that it
has a very small dynamic range ? while scores as
high as 1 are theoretically possible, in practice they
will never be achieved, and we see that the actual
range of scores observed are all under 20%.
We conclude that there is probably no single eval-
uation measure that is best for all purposes. If a gold
standard is available, then many-to-1 and 1-to-1 are
the most intuitive measures, but should not be used
when |C| is variable, and do not account for differ-
ences in the errors made. While vi has been popular
as an entropy-based alternative to address the latter
problem, its scores are not easy to interpret (being on
a scale of bits) and it still has the problem of incom-
parability across different |C|. Overall, vm seems to
be the best general-purpose measure that combines
an entropy-based score with an intuitive 0-1 scale
and stability over a wide range of |C|.
4 System comparison
Having provided some intuitions about the behav-
ior of different evaluation methods, we move on to
evaluating the various systems presented in Section
2. We first present results for the same WSJ cor-
pus used above. However, because most of the sys-
tems were initially developed on this corpus, and
often evaluated only on it, there is a question of
whether their methods and/or hyperparameters are
overly specific to the domain or to the English lan-
guage. This is a particularly pertinent question since
a primary argument in favor of unsupervised sys-
tems is that they are easier to port to a new language
or domain than supervised systems. To address this
question, we evaluate all the systems as well on the
multilingual Multext East corpus (Erjavec, 2004),
without changing any of the parameter settings. |C|
was set to 45 for all of the experiments reported in
this section. Based on our assessment of evaluation
Figure 2: Performance of the different systems on WSJ,
using three different measures [|C|:45, |T |:45]
system runtime
brown
?
10 min.
clark
?
40 min.
cw
?
10 min.
bhmm
?
4 hrs.
vbhmm
?
10 hrs.
pr
?
10 hrs.*
feat
?
40 hrs.*
Table 2: Runtimes for the different systems on WSJ
[|C|:45]. *pr and feat have multithreading implemen-
tations and ran on 16 cores.
measures above, we report VM scores as the most
reliable measure across different systems and clus-
ter set sizes; to facilitate comparisons with previous
papers, we also report many-to-one and one-to-one
accuracy.
4.1 Results on WSJ
Figure 2 presents results for all seven systems, with
approximate runtimes shown in Table 2. While these
algorithms have not necessarily been optimized for
speed, there is a fairly clear distinction between the
older type-clustering models (brown, clark) and the
graph-based algorithm (cw) on the one hand, and
the newer machine-learning approaches (bhmm,
vbhmm, pr, feat) on the other, with the former be-
ing much faster to run. Despite their faster run-
times and less sophisticated methods, however, these
systems perform surprisingly well in comparison to
the latter group. Even the oldest and perhaps sim-
plest method (brown) outperforms the two BHMMs
and posterior regularization on all measures. Only
580
Figure 3: VM scores for the different systems on English
Multext-East and WSJ-S corpora [|C|:45, |T |:{14,17}]
Figure 4: VM scores for the different systems on the eight
Multext-East corpora [|C|:45, |T |:14]
the very latest approach (feat) rivals clark, show-
ing slightly better performance on two of the three
measures (clark: 71.2, 53.8, 65.5 on many-to-one,
one-to-one, VM; feat: 73.9, 53.3, 67.7). The cw
system returns a total of 568 clusters on this data set,
so the many-to-one and one-to-one measures are not
strictly comparable to the other systems; on VM this
system achieves middling performance.
We note that the two best-performing systems,
clark and feat, are also the only two to use mor-
phological information. Since the clustering algo-
rithms used by brown and clark are quite similar,
the difference in performance between the two can
probably be attributed to the extra information pro-
vided by the morphology. This supports the (unsur-
prising) conclusion that incorporating morphologi-
cal features is generally helpful for POS induction.
4.2 Results on other corpora
We now examine whether either the relative or ab-
solute performance of the different systems holds up
when tested on a variety of different languages. For
these experiments, we used the 1984 portion of the
Multext-East corpus (? 7k sentences), which contains
parallel translations of Orwell?s 1984 in 8 different
languages: Bulgarian[bg], Czech[cs], Estonian[et],
Hungarian[hu], Romanian[ro], Slovene[sl], Ser-
bian[sr] and English[en]. We also included a 7k
sentence version of the WSJ corpus [wsj-s] to help
differentiate effects of corpus size from those of do-
main/language. For the WSJ corpora we experi-
mented with two standardly used tagsets: the orig-
inal PTB 45-tag gold standard and a coarser set of
17 tags previously used by several researchers work-
ing on unsupervised POS tagging (Smith and Eis-
ner, 2005; Goldwater and Griffiths, 2007; Johnson,
2007). For theMultext-East corpus only a coarse 14-
tag tagset was available.5 Finally, to facilitate direct
comparisons of genre while controlling for the size
of both the corpus and the tag set, we also created a
further collapsed 13-tag set for WSJ.6
Figure 3 illustrates the abilities of the different
systems to generalize across different genres of En-
glish text. Comparing the results for the Multext-
East English corpus and the small WSJ corpus with
13 tags (i.e., controlling as much as possible for cor-
pus size and number of gold standard tags), we see
that despite being developed on WSJ, the systems
actually perform better on Multext-East. This is en-
couraging, since it suggests that the methods and
hyperparameters of the algorithms are not strongly
tied to WSJ. It also suggests that Multext-East is in
some sense an easier corpus than WSJ. Indeed, the
distribution of vocabulary items supports this view:
the 100 most frequent words account for 48% of
the WSJ corpus, but 57% of the 1984 novel. It is
also worth pointing out that, although previous re-
searchers have reduced the 45-tag WSJ set to 17 tags
in order to create an easier task for unsupervised
learning (and to decrease training time), reducing
the tag set further to 13 tags actually decreases per-
formance, since some distinctions found by the sys-
5Out of the 14 tags only 11 are shared across all languages.
For details c.f. Appendix B in (Naseem et al, 2009).
6We tried to make the meanings of the tags as similar as
possible between the two corpora; we had to create 13 rather
than 14 WSJ tags for this reason. Our 13-tag set can be found
at http://homepages.inf.ed.ac.uk/s0787820/pos/.
581
tems (e.g., between different types of punctuation)
are collapsed in the gold standard.
Figure 4 gives the results of the different systems
on the various languages.7 Not surprisingly, all the
algorithms perform best on English, often by a wide
margin, suggesting that they are indeed tuned bet-
ter towards English syntax and/or morphology. One
might expect that the two systems with morpho-
logical features (clark and feat) would show less
difference between English and some of the other
languages (all of which have complex morphology)
than the other systems. However, although clark
and feat (along with Brown) are the best perform-
ing systems overall, they don?t show any particular
benefit for the morphologically complex languages.8
One difference between the Multext-East results
and the WSJ results is that on Multext-East, clark
clearly outperforms all the other systems. This is
true for both the English and non-English corpora,
despite the similar performance of clark and feat
on (English) WSJ. This suggests that feat benefits
more from the larger corpus size of WSJ. For the
other languages clarkmay be benefiting from some-
what more general morphological features; feat cur-
rently contains suffix features but no prefix features
(although these could be added).
Overall, our experiments on multiple languages
support our earlier claim that many of the newer
POS induction systems are not as successful as the
older methods. Moreover, these experiments under-
score the importance of testing unsupervised sys-
tems on multiple languages and domains, since both
the absolute and relative performance of systems
may change on different data sets. Ideally, some of
the corpora should be held out as unseen test data
if an effective argument is to be made regarding the
language- or domain-generality of the system.
5 Learning from induced prototypes
We now introduce a final novel method of evaluat-
ing POS induction systems and potentially improv-
ing their performance as well. Our idea is based
7Some results are missing because not all of the corpora
were successfully processed by all of the systems.
8It can be argued that lemmatization would have given a sig-
nificant gain to the performance of the systems in these lan-
guages. Although lemmatization information was included in
the corpus we chose not to use it, maintaining the fully unsu-
pervised nature of this task.
on the prototype-driven learning model of Haghighi
and Klein (2006). This model is unsupervised, but
requires as input a handful of prototypes (canonical
examples) for each word class. The system uses a
log-linear model with features that include the pro-
totype lists as well as morphological features (the
same ones used in feat). Using the most frequent
words in each gold standard class as prototypes, the
authors report 80.5% accuracy (both many-to-one
and one-to-one) on WSJ, considerably higher than
any of the induction systems seen here. This raises
two questions: If we wish to induce prototypes with-
out a tagged corpus or language-specific knowledge,
which induction system will provide the best pro-
totypes (i.e., most similar to the gold standard pro-
totypes)? And, can we use the induced prototypes
as input to the prototype-driven model (h&k) to
achieve better performance than the system the pro-
totypes were extracted from?
To explore these questions, we implemented a
simple heuristic method for inducing prototypes
from the output C of a POS induction system by
selecting a few frequent words in each cluster that
are the most similar to other words in the cluster and
also the most dissimilar to the words in other clus-
ters. For each cluster ci ? C, we retain as candi-
date prototypes the words whose frequency in ci is
at least 90% as high as the word with the highest fre-
quency (in ci). This yields about 20-30 candidates
from each cluster. For each of these, we compute
its average similarity S to the other candidates in its
cluster, and the average dissimilarity D to the candi-
dates in other clusters. Similarity is computed using
the method described by Haghighi and Klein (2006),
which uses SVD on word context vectors and cosine
similarity. Dissimilarity between a pair of words is
computed as one minus the similarity. Finally we
compute the average M = 0.5(S + D), sort the
words by their M scores, and keep as prototypes
the top ten words with M > 0.25 ? maxci(M).
The cutoff threshold results in some clusters having
less than ten prototypes, which is appropriate since
some gold standard categories have very few mem-
bers (e.g., punctuation, determiners).
Using this method, we first tested the various
base+proto systems on the WSJ corpus. Results
in Table 3 show that the brown system produces
the best prototypes. Although not as good as
using prototypes from the gold standard (h&k),
582
system many-to-1 1-to-1 vm
brown 76.1(8.3) 60.7(10.6) 68.8(5.8)
clark 74.5(3.3) 62.1(8.3) 68.6(3.0)
bhmm 71.8(8.6) 56.5(15.0) 65.7(9.5)
vbhmm 68.1(17.9) 67.2(20.7) 67.5(18.3)
pr 71.6(9.2) 60.2(17.0) 67.2(12.4)
feat 69.8(-4.1) 52.0(-1.3) 63.1(-4.6)
h&k 80.2 80.2 75.2
Table 3: Scores on WSJ for our prototype-based POS in-
duction system, with prototypes extracted from each of
the existing systems [|C|:45,|T |:45]. Numbers in paren-
theses are the improvement over the same system without
using the prototype step. Scores in bold indicate the best
performance (improvement) in each column. h&k uses
gold standard prototypes.
corpus brown clark
wsj 68.8(5.8) 68.5(3.0)
wsj-s 62.3(2.7) 67.5(3.6)
en 58.5(1.6) 57.9(-3.3)
bg 53.7(2.3) 50.2(-7.1)
cs 49.9(5.0) 48.0(-4.0)
et 45.8(4.9) 44.4(-1.9)
hu 45.8(0.1) 47.0(-5.7)
ro 53.2(0.8) 52.7(-3.3)
sl 51.2(2.9) 51.7(-4.6)
sr 48.0(2.8) 46.4(-4.9)
Table 4: VM scores for brown+proto and clark+proto
on all corpora. Numbers in parentheses indicate improve-
ment over the base systems.
brown+proto yields a large improvement over
brown, and the highest performance of any system
tested so far. In fact, the brown+proto scores are, to
our knowledge, the best reported results for an un-
supervised POS induction system on WSJ.
Next, we evaluated the two best-performing
+proto systems on Multext-East, as shown in Ta-
ble 4. We see that brown again yields the best
prototypes, and again yields improvements when
used as brown+proto (although the improvements
are not as large as those on WSJ). Interestingly,
clark+proto actually performs worse than clark on
the multilingual data, showing that although induced
prototypes can in principle improve the performance
of a system, not all systems will benefit in all situ-
ations. This suggests a need for additional investi-
gation to determine what properties of an existing
induction system allow it to produce useful proto-
types with the current method and/or to develop a
specialized system specifically targeted towards in-
ducing useful prototypes.
6 Conclusion
In this paper, we have attempted to provide a more
comprehensive review and comparison of evaluation
measures and systems for POS induction than has
been done before. We pointed out that most of the
commonly used evaluation measures are sensitive to
the number of induced clusters, and suggested that
V-measure (which is less sensitive) should be used
as an alternative or in conjunction with the standard
measures. With regard to the systems themselves,
we found that many of the newer approaches actu-
ally perform worse than older methods that are both
simpler and faster. The newer systems have intro-
duced potentially important machine learning tools,
but are not necessarily better suited to the POS in-
duction task specifically.
Since portability is a distinguishing feature for un-
supervised models, we have stressed the importance
of testing the systems on corpora that were not used
in their development, and especially on different lan-
guages. We found that on non-English languages,
Clark?s (2003) system performed best.
Finally, we introduced the idea of evaluating in-
duction systems based on their ability to produce
useful cluster prototypes. We found that the old-
est system (Brown et al, 1992) yielded the best
prototypes, and that using these prototypes gave
state-of-the-art performance on WSJ, as well as im-
provements on nearly all of the non-English corpora.
These promising results suggest a new direction for
future research: improving POS induction by de-
veloping methods targeted towards extracting better
prototypes, rather than focusing on improving clus-
tering of the entire data set.
Acknowledgments
We thank Mark Johnson, Kuzman Ganchev, and
Taylor Berg-Kirkpatrick for providing the imple-
mentations of their models, as well as Stella
Frank, Tom Kwiatkowski, Luke Zettlemoyer and the
anonymous reviewers for their comments and sug-
gestions. This work was supported by an EPSRC
graduate Fellowship, and by ERCAdvanced Fellow-
ship 249520 GRAMPLUS.
583
References
Taylor Berg-Kirkpatrick, Alexandre B. Co?te?, John DeN-
ero, and Dan Klein. 2010. Painless unsupervised
learning with features. In Proceedings of NAACL
2010, pages 582?590, Los Angeles, California, June.
Chris Biemann. 2006. Unsupervised part-of-speech tag-
ging employing efficient graph clustering. In Proceed-
ings of COLING ACL 2006, pages 7?12, Morristown,
NJ, USA.
Peter F. Brown, Vincent J. Della Pietra, Peter V. Desouza,
Jennifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of EACL 2003, pages 59?66,
Morristown, NJ, USA.
Tomaz? Erjavec. 2004. MULTEXT-East Version 3:
Multilingual Morphosyntactic Specifications, Lexi-
cons and Corpora. In Fourth International Conference
on Language Resources and Evaluation, LREC?04,
page In print, Paris. ELRA.
Stella Frank, Sharon Goldwater, and Frank Keller. 2009.
Evaluating models of syntactic category acquisition
without using a gold standard. In Proceedings of
CogSci09, July.
Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, and
Ben Taskar. 2009. Posterior regularization for struc-
tured latent variable models. Technical report, Univer-
sity of Pennsylvania.
Jianfeng Gao and Mark Johnson. 2008. A comparison of
bayesian estimators for unsupervised hidden markov
model pos taggers. In Proceedings of EMNLP 2008,
pages 344?352, Morristown, NJ, USA.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of ACL 2007, pages 744?751,
Prague, Czech Republic, June.
Joao Graca, Kuzman Ganchev, Ben Taskar, and Fernando
Pereira. 2009. Posterior vs parameter sparsity in latent
variable models. In Y. Bengio, D. Schuurmans, J. Laf-
ferty, C. K. I. Williams, and A. Culotta, editors, Ad-
vances in Neural Information Processing Systems 22,
pages 664?672.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
NAACL 2006, pages 320?327, Morristown, NJ, USA.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proceedings of EMNLP-CoNLL
2007, pages 296?305, Prague, Czech Republic, June.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn Treebank. Computational Linguistics,
19(2):331?330.
Marina Meila?. 2003. Comparing clusterings by the vari-
ation of information. In Learning Theory and Kernel
Machines, pages 173?187.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational Linguistics, 20(2):155?
172.
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and
Regina Barzilay. 2009. Multilingual part-of-speech
tagging: Two unsupervised approaches. Journal of Ar-
tificial Intelligence Research, 36:341?385.
Sujith Ravi and Kevin Knight. 2009. Minimized mod-
els for unsupervised part-of-speech tagging. In Pro-
ceedings of ACL-IJCNLP 2009, pages 504?512, Sun-
tec, Singapore, August.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of EMNLP-
CoNLL 2007, pages 410?420.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: training log-linear models on unlabeled data.
In Proceedings of ACL 2005, pages 354?362, Morris-
town, NJ, USA.
K. Toutanova and M. Johnson. 2007. A Bayesian LDA-
based model for semi-supervised part-of-speech tag-
ging. In Proceedings of NIPS 2007.
Jurgen Van Gael, Andreas Vlachos, and Zoubin Ghahra-
mani. 2009. The infinite HMM for unsupervised PoS
tagging. In Proceedings of EMLNP 2009, pages 678?
687, Singapore, August.
Andreas Vlachos, Anna Korhonen, and Zoubin Ghahra-
mani. 2009. Unsupervised and constrained dirichlet
process mixture models for verb clustering. In Pro-
ceedings of GEMS 2009, pages 74?82, Morristown,
NJ, USA.
584
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1223?1233,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Inducing Probabilistic CCG Grammars from Logical Form
with Higher-Order Unification
Tom Kwiatkowski?
t.m.kwiatkowksi@sms.ed.ac.uk
Luke Zettlemoyer?
lsz@cs.washington.edu
Sharon Goldwater?
sgwater@inf.ed.ac.uk
Mark Steedman?
steedman@inf.ed.ac.uk
?School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
?Computer Science & Engineering
University of Washington
Seattle, WA 98195
Abstract
This paper addresses the problem of learn-
ing to map sentences to logical form, given
training data consisting of natural language
sentences paired with logical representations
of their meaning. Previous approaches have
been designed for particular natural languages
or specific meaning representations; here we
present a more general method. The approach
induces a probabilistic CCG grammar that
represents the meaning of individual words
and defines how these meanings can be com-
bined to analyze complete sentences. We
use higher-order unification to define a hy-
pothesis space containing all grammars con-
sistent with the training data, and develop
an online learning algorithm that efficiently
searches this space while simultaneously es-
timating the parameters of a log-linear parsing
model. Experiments demonstrate high accu-
racy on benchmark data sets in four languages
with two different meaning representations.
1 Introduction
A key aim in natural language processing is to learn
a mapping from natural language sentences to for-
mal representations of their meaning. Recent work
has addressed this problem by learning semantic
parsers given sentences paired with logical meaning
representations (Thompson & Mooney, 2002; Kate
et al, 2005; Kate & Mooney, 2006; Wong &
Mooney, 2006, 2007; Zettlemoyer & Collins, 2005,
2007; Lu et al, 2008). For example, the training
data might consist of English sentences paired with
lambda-calculus meaning representations:
Sentence: which states border texas
Meaning: ?x.state(x) ? next to(x, tex)
Given pairs like this, the goal is to learn to map new,
unseen, sentences to their corresponding meaning.
Previous approaches to this problem have been
tailored to specific natural languages, specific mean-
ing representations, or both. Here, we develop an
approach that can learn to map any natural language
to a wide variety of logical representations of lin-
guistic meaning. In addition to data like the above,
this approach can also learn from examples such as:
Sentence: hangi eyaletin texas ye siniri vardir
Meaning: answer(state(borders(tex)))
where the sentence is in Turkish and the meaning
representation is a variable-free logical expression
of the type that has been used in recent work (Kate
et al, 2005; Kate & Mooney, 2006; Wong &
Mooney, 2006; Lu et al, 2008).
The reason for generalizing to multiple languages
is obvious. The need to learn over multiple repre-
sentations arises from the fact that there is no stan-
dard representation for logical form for natural lan-
guage. Instead, existing representations are ad hoc,
tailored to the application of interest. For example,
the variable-free representation above was designed
for building natural language interfaces to databases.
Our approach works by inducing a combinatory
categorial grammar (CCG) (Steedman, 1996, 2000).
A CCG grammar consists of a language-specific
lexicon, whose entries pair individual words and
phrases with both syntactic and semantic informa-
tion, and a universal set of combinatory rules that
1223
project that lexicon onto the sentences and meanings
of the language via syntactic derivations. The learn-
ing process starts by postulating, for each sentence
in the training data, a single multi-word lexical item
pairing that sentence with its complete logical form.
These entries are iteratively refined with a restricted
higher-order unification procedure (Huet, 1975) that
defines all possible ways to subdivide them, consis-
tent with the requirement that each training sentence
can still be parsed to yield its labeled meaning.
For the data sets we consider, the space of pos-
sible grammars is too large to explicitly enumerate.
The induced grammar is also typically highly am-
biguous, producing a large number of possible anal-
yses for each sentence. Our approach discriminates
between analyses using a log-linear CCG parsing
model, similar to those used in previous work (Clark
& Curran, 2003, 2007), but differing in that the syn-
tactic parses are treated as a hidden variable during
training, following the approach of Zettlemoyer &
Collins (2005, 2007). We present an algorithm that
incrementally learns the parameters of this model
while simultaneously exploring the space of possi-
ble grammars. The model is used to guide the pro-
cess of grammar refinement during training as well
as providing a metric for selecting the best analysis
for each new sentence.
We evaluate the approach on benchmark datasets
from a natural language interface to a database of
US Geography (Zelle & Mooney, 1996). We show
that accurate models can be learned for multiple
languages with both the variable-free and lambda-
calculus meaning representations introduced above.
We also compare performance to previous methods
(Kate & Mooney, 2006; Wong & Mooney, 2006,
2007; Zettlemoyer & Collins, 2005, 2007; Lu et al,
2008), which are designed with either language- or
representation- specific constraints that limit gener-
alization, as discussed in more detail in Section 6.
Despite being the only approach that is general
enough to run on all of the data sets, our algorithm
achieves similar performance to the others, even out-
performing them in several cases.
2 Overview of the Approach
The goal of our algorithm is to find a function
f : x ? z that maps sentences x to logical ex-
pressions z. We learn this function by inducing a
probabilistic CCG (PCCG) grammar from a train-
ing set {(xi, zi)|i = 1 . . . n} containing example
(sentence, logical-form) pairs such as (?New York
borders Vermont?, next to(ny, vt)). The induced
grammar consists of two components which the al-
gorithm must learn:
? A CCG lexicon, ?, containing lexical items
that define the space of possible parses y for
an input sentence x. Each parse contains both
syntactic and semantic information, and defines
the output logical form z.
? A parameter vector, ?, that defines a distribu-
tion over the possible parses y, conditioned on
the sentence x.
We will present the approach in two parts. The
lexical induction process (Section 4) uses a re-
stricted form of higher order unification along with
the CCG combinatory rules to propose new entries
for ?. The complete learning algorithm (Section 5)
integrates this lexical induction with a parameter es-
timation scheme that learns ?. Before presenting the
details, we first review necessary background.
3 Background
This section provides an introduction to the ways in
which we will use lambda calculus and higher-order
unification to construct meaning representations. It
also reviews the CCG grammar formalism and prob-
abilistic extensions to it, including existing parsing
and parameter estimation techniques.
3.1 Lambda Calculus and Higher-Order
Unification
We assume that sentence meanings are represented
as logical expressions, which we will construct from
the meaning of individual words by using the op-
erations defined in the lambda calculus. We use a
version of the typed lambda calculus (cf. Carpenter
(1997)), in which the basic types include e, for en-
tities; t, for truth values; and i for numbers. There
are also function types of the form ?e, t? that are as-
signed to lambda expressions, such as ?x.state(x),
which take entities and return truth values. We
represent the meaning of words and phrases using
1224
lambda-calculus expressions that can contain con-
stants, quantifiers, logical connectors, and lambda
abstractions.
The advantage of using the lambda calculus
lies in its generality. The meanings of individ-
ual words and phrases can be arbitrary lambda ex-
pressions, while the final meaning for a sentence
can take different forms. It can be a full lambda-
calculus expression, a variable-free expression such
as answer(state(borders(tex))), or any other log-
ical expression that can be built from the primitive
meanings via function application and composition.
The higher-order unification problem (Huet,
1975) involves finding a substitution for the free
variables in a pair of lambda-calculus expressions
that, when applied, makes the expressions equal
each other. This problem is notoriously complex;
in the unrestricted form (Huet, 1973), it is undecid-
able. In this paper, we will guide the grammar in-
duction process using a restricted version of higher-
order unification that is tractable. For a given ex-
pression h, we will need to find expressions for f
and g such that either h = f(g) or h = ?x.f(g(x)).
This limited form of the unification problem will al-
low us to define the ways to split h into subparts
that can be recombined with CCG parsing opera-
tions, which we will define in the next section, to
reconstruct h.
3.2 Combinatory Categorial Grammar
CCG (Steedman, 2000) is a linguistic formalism
that tightly couples syntax and semantics, and
can be used to model a wide range of language
phenomena. For present purposes a CCG grammar
includes a lexicon ? with entries like the following:
New York ` NP : ny
borders ` S\NP/NP : ?x?y.next to(y, x)
Vermont ` NP : vt
where each lexical item w`X : h has words w, a
syntactic categoryX , and a logical form h expressed
as a lambda-calculus expression. For the first exam-
ple, these are ?New York,? NP , and ny. CCG syn-
tactic categories may be atomic (such as S, NP ) or
complex (such as S\NP/NP ).
CCG combines categories using a set of com-
binatory rules. For example, the forward (>) and
backward (<) application rules are:
X/Y : f Y : g ? X : f(g) (>)
Y : g X\Y : f ? X : f(g) (<)
These rules apply to build syntactic and semantic
derivations under the control of the word order infor-
mation encoded in the slash directions of the lexical
entries. For example, given the lexicon above, the
sentence New York borders Vermont can be parsed
to produce:
New York borders Vermont
NP (S\NP )/NP NP
ny ?x?y.next to(y, x) vt
>
(S\NP )
?y.next to(y, vt)
<
S
next to(ny, vt)
where each step in the parse is labeled with the com-
binatory rule (? > or ? <) that was used.
CCG also includes combinatory rules of forward
(> B) and backward (< B) composition:
X/Y : f Y/Z : g ?X/Z : ?x.f(g(x)) (> B)
Y \Z : g X\Y : f ?X\Z : ?x.f(g(x)) (< B)
These rules provide for a relaxed notion of con-
stituency which will be useful during learning as we
reason about possible refinements of the grammar.
We also allow vertical slashes in CCG categories,
which act as wild cards. For example, with this
extension the forward application combinator (>)
could be used to combine the category S/(S|NP )
with any of S\NP , S/NP , or S|NP . Figure 1
shows two parses where the composition combina-
tors and vertical slashes are used. These parses
closely resemble the types of analyses that will be
possible under the grammars we learn in the experi-
ments described in Section 8.
3.3 Probabilistic CCGs
Given a CCG lexicon ?, there will, in general, be
many possible parses for each sentence. We select
the most likely alternative using a log-linear model,
which consists of a feature vector ? and a parame-
ter vector ?. The joint probability of a logical form
z constructed with a parse y, given a sentence x is
1225
hangi eyaletin texas ye siniri vardir
S/NP NP/NP NP NP\NP
?x.answer(x) ?x.state(x) tex ?x.border(x)
<
NP
border(tex)
>
NP
state(border(tex))
>
S
answer(state(border(tex)))
what states border texas
S/(S|NP ) S|NP/(S|NP ) S\NP/NP NP
?f?x.f(x) ?f?x.state(x)?f(x) ?y?x.next to(x, y) tex
>B
S|NP/NP
?y?x.state(x) ? next to(x, y)
>
S|NP
?x.state(x) ? next to(x, tex)
>
S
?x.state(x) ? next to(x, tex)
Figure 1: Two examples of CCG parses with different logical form representations.
defined as:
P (y, z|x; ?,?) =
e???(x,y,z)
?
(y?,z?) e
???(x,y?,z?)
(1)
Section 7 defines the features used in the experi-
ments, which include, for example, lexical features
that indicate when specific lexical items in ? are
used in the parse y. For parsing and parameter es-
timation, we use standard algorithms (Clark & Cur-
ran, 2007), as described below.
The parsing, or inference, problem is to find the
most likely logical form z given a sentence x, as-
suming the parameters ? and lexicon ? are known:
f(x) = arg max
z
p(z|x; ?,?) (2)
where the probability of the logical form is found by
summing over all parses that produce it:
p(z|x; ?,?) =
?
y
p(y, z|x; ?,?) (3)
In this approach the distribution over parse trees y
is modeled as a hidden variable. The sum over
parses in Eq. 3 can be calculated efficiently using
the inside-outside algorithm with a CKY-style pars-
ing algorithm.
To estimate the parameters themselves, we
use stochastic gradient updates (LeCun et al,
1998). Given a set of n sentence-meaning pairs
{(xi, zi) : i = 1...n}, we update the parameters ? it-
eratively, for each example i, by following the local
gradient of the conditional log-likelihood objective
Oi = logP (zi|xi; ?,?). The local gradient of the
individual parameter ?j associated with feature ?j
and training instance (xi, zi) is given by:
?Oi
??j
= Ep(y|xi,zi;?,?)[?j(xi, y, zi)]
?Ep(y,z|xi;?,?)[?j(xi, y, z)]
(4)
As with Eq. 3, all of the expectations in Eq. 4 are
calculated through the use of the inside-outside al-
gorithm on a pruned parse chart. In the experiments,
each chart cell was pruned to the top 200 entries.
4 Splitting Lexical Items
Before presenting a complete learning algorithm, we
first describe how to use higher-order unification to
define a procedure for splitting CCG lexical entries.
This splitting process is used to expand the lexicon
during learning. We seed the lexical induction with
a multi-word lexical item xi`S :zi for each training
example (xi, zi), consisting of the entire sentence xi
and its associated meaning representation zi. For ex-
ample, one initial lexical item might be:
New York borders Vermont `S:next to(ny, vt) (5)
Although these initial, sentential lexical items
can parse the training data, they will not generalize
well to unseen data. To learn effectively, we will
need to split overly specific entries of this type into
pairs of new, smaller, entries that generalize better.
For example, one possible split of the lexical entry
given in (5) would be the pair:
New York borders ` S/NP : ?x.next to(ny, x),
Vermont `NP : vt
where we broke the original logical expression into
two new ones ?x.next to(ny, x) and vt, and paired
them with syntactic categories that allow the new
lexical entries to be recombined to produce the orig-
inal analysis. The next three subsections define the
set of possible splits for any given lexical item. The
process is driven by solving a higher-order unifica-
tion problem that defines all of the ways of splitting
the logical expression into two parts, as described in
Section 4.1. Section 4.2 describes how to construct
1226
syntactic categories that are consistent with the two
new fragments of logical form and which will allow
the new lexical items to recombine. Finally, Sec-
tion 4.3 defines the full set of lexical entry pairs that
can be created by splitting a lexical entry.
As we will see, this splitting process is overly pro-
lific for any single language and will yield many
lexical items that do not generalize well. For
example, there is nothing in our original lexical
entry above that provides evidence that the split
should pair ?Vermont? with the constant vt and not
?x.next to(ny, x). Section 5 describes how we
estimate the parameters of a probabilistic parsing
model and how this parsing model can be used to
guide the selection of items to add to the lexicon.
4.1 Restricted Higher-Order Unification
The set of possible splits for a logical expression
h is defined as the solution to a pair of higher-
order unification problems. We find pairs of logi-
cal expressions (f, g) such that either f(g) = h or
?x.f(g(x)) = h. Solving these problems creates
new expressions f and g that can be recombined ac-
cording to the CCG combinators, as defined in Sec-
tion 3.2, to produce h.
In the unrestricted case, there can be infinitely
many solution pairs (f, g) for a given expression h.
For example, when h = tex and f = ?x.tex, the
expression g can be anything. Although it would be
simple enough to forbid vacuous variables in f and
g, the number of solutions would still be exponen-
tial in the size of h. For example, when h contains a
conjunction, such as h = ?x.city(x)?major(x)?
in(x, tex), any subset of the expressions in the con-
junction can be assigned to f (or g).
To limit the number of possible splits, we enforce
the following restrictions on the possible higher-
order solutions that will be used during learning:
? No Vacuous Variables: Neither g or f can be a
function of the form ?x.e where the expression
e does not contain the variable x. This rules out
functions such as ?x.tex.
? Limited Coordination Extraction: The ex-
pression g cannot contain more than N of the
conjuncts that appear in any coordination in
h. For example, with N = 1 the expression
g = ?x.city(x)?major(x) could not be used
as a solution given the h conjuction above. We
use N = 4 in our experimental evaluation.
? Limited Application: The function f can-
not contain new variables applied to any non-
variable subexpressions from h. For example,
if h = ?x.in(x, tex), the pair f = ?q.q(tex)
and g = ?y?x.in(x, y) is forbidden.
Together, these three restrictions guarantee that
the number of splits is, in the worst case, an N -
degree polynomial of the number of constants in h.
The constraints were designed to increase the effi-
ciency of the splitting algorithm without impacting
performance on the development data.
4.2 Splitting Categories
We define the set of possible splits for a category
X :h with syntax X and logical form h by enumer-
ating the solution pairs (f, g) to the higher-order
unification problems defined above and creating
syntactic categories for the resulting expressions.
For example, given X :h = S\NP :?x.in(x, tex),
f = ?y?x.in(x, y), and g = tex, we would
produce the following two pairs of new categories:
( S\NP/NP :?y?x.in(x, y) , NP :tex )
( NP :tex , S\NP\NP :?y?x.in(x, y) )
which were constructed by first choosing the syntac-
tic category for g, in this caseNP , and then enumer-
ating the possible directions for the new slash in the
category containing f . We consider each of these
two steps in more detail below.
The new syntactic category for g is determined
based on its type, T (g). For example, T (tex) = e
and T (?x.state(x)) = ?e, t?. Then, the function
C(T ) takes an input type T and returns the syntactic
category of T as follows:
C(T ) =
?
?
?
NP if T = e
S if T = t
C(T2)|C(T1) when T = ?T1, T2?
The basic types e and t are assigned syntactic
categories NP and S, and all functional types
are assigned categories recursively. For exam-
ple C(?e, t?) = S|NP and C(?e, ?e, t??) =
S|NP |NP . This definition of CCG categories is
unconventional in that it never assigns atomic cate-
gories to functional types. For example, there is no
1227
distinct syntactic category N for nouns (which have
semantic type ?e, t?). Instead, the more complex cat-
egory S|NP is used.
Now, we are ready to define the set of all category
splits. For a category A = X:h we can define
SC(A) = {FA(A) ? BA(A) ? FC(A) ? BC(A)}
which is a union of sets, each of which includes
splits for a single CCG operator. For example,
FA(X:h) is the set of category pairs
FA(X:h) = {(X/Y :f, Y :g) | h=f(g) ? Y=C(T (g))
}
where each pair can be combined with the forward
application combinator, described in Section 3.2, to
reconstruct X:h.
The remaining three sets are defined similarly,
and are associated with the backward application
and forward and backward composition operators,
respectively:
BA(X:h) = {(Y :g,X\Y :f) | h=f(g) ? Y=C(T (g))
}
FC(X/Y :h) = {(X/W :f,W/Y :g) |
h=?x.f(g(x)) ?W=C(T (g(x)))
}
BC(X\Y :h) = {(W\Y :g,X\W :f) |
h=?x.f(g(x)) ?W=C(T (g(x)))
}
where the composition sets FC and BC only accept
input categories with the appropriate outermost slash
direction, for example FC(X/Y :h).
4.3 Splitting Lexical Items
We can now define the lexical splits that will be used
during learning. For lexical entry w0:n ` A, with
word sequence w0:n = ?w0, . . . , wn? and CCG cat-
egory A, define the set SL of splits to be:
SL(w0:n`A) = {(w0:i`B,wi+1:n`C) |
0 ? i < n ? (B,C) ? SC(A)}
where we enumerate all ways of splitting the words
sequence w0:n and aligning the subsequences with
categories in SC(A), as defined in the last section.
5 Learning Algorithm
The previous section described how a splitting pro-
cedure can be used to break apart overly specific
lexical items into smaller ones that may generalize
better to unseen data. The space of possible lexi-
cal items supported by this splitting procedure is too
large to explicitly enumerate. Instead, we learn the
parameters of a PCCG, which is used both to guide
the splitting process, and also to select the best parse,
given a learned lexicon.
Figure 2 presents the unification-based learning
algorithm, UBL. This algorithm steps through the
data incrementally and performs two steps for each
training example. First, new lexical items are in-
duced for the training instance by splitting and merg-
ing nodes in the best correct parse, given the current
parameters. Next, the parameters of the PCCG are
updated by making a stochastic gradient update on
the marginal likelihood, given the updated lexicon.
Inputs and Initialization The algorithm takes as
input the training set of n (sentence, logical form)
pairs {(xi, zi) : i = 1...n} along with an NP list,
?NP , of proper noun lexical items such as Texas`
NP :tex. The lexicon, ?, is initialized with a single
lexical item xi`S :zi for each of the training pairs
along with the contents of the NP list. It is possible
to run the algorithm without the initial NP list; we
include it to allow direct comparisons with previous
approaches, which also included NP lists. Features
and initial feature weights are described in Section 7.
Step 1: Updating the Lexicon In the lexical up-
date step the algorithm first computes the best cor-
rect parse tree y? for the current training exam-
ple and then uses y? as input to the procedure
NEW-LEX, which determines which (if any) new
lexical items to add to ?. NEW-LEX begins by enu-
merating all pairs (C,wi:j), for i < j, where C is a
category occurring at a node in y? and wi:j are the
(two or more) words it spans. For example, in the
left parse in Figure 1, there would be four pairs: one
with the category C = NP\NP :?x.border(x) and
the phrase wi:j =?ye siniri vardir?, and one for each
non-leaf node in the tree.
For each pair (C,wi:j), NEW-LEX considers in-
troducing a new lexical item wi:j`C, which allows
for the possibility of a parse where the subtree rooted
at C is replaced with this new entry. (If C is a leaf
node, this item will already exist.) NEW-LEX also
considers adding each pair of new lexical items that
is obtained by splitting wi:j`C as described in Sec-
tion 4, thereby considering many different ways of
reanalyzing the node. This process creates a set of
possible new lexicons, where each lexicon expands
1228
? in a different way by adding the items from either
a single split or a single merge of a node in y?.
For each potential new lexicon ??, NEW-LEX
computes the probability p(y?|xi, zi; ??,??) of the
original parse y? under ?? and parameters ?? that are
the same as ? but have weights for the new lexical
items, as described in Section 7. It also finds the
best new parse y? = arg maxy p(y|xi, zi; ??,??).1
Finally, NEW-LEX selects the ?? with the largest
difference in log probability between y? and y?, and
returns the new entries in ??. If y? is the best parse
for every ??, NEW-LEX returns the empty set; the
lexicon will not change.
Step 2: Parameter Updates For each training ex-
ample we update the parameters ? using the stochas-
tic gradient updates given by Eq. 4.
Discussion The alternation between refining the
lexicon and updating the parameters drives the learn-
ing process. The initial model assigns a conditional
likelihood of one to each training example (there
is a single lexical item for each sentence xi, and
it contains the labeled logical form zi). Although
the splitting step often decreases the probability of
the data, the new entries it produces are less spe-
cific and should generalize better. Since we initially
assign positive weights to the parameters for new
lexical items, the overall approach prefers splitting;
trees with many lexical items will initially be much
more likely. However, if the learned lexical items
are used in too many incorrect parses, the stochastic
gradient updates will down weight them to the point
where the lexical induction step can merge or re-split
nodes in the trees that contain them. This allows the
approach to correct the lexicon and, hopefully, im-
prove future performance.
6 Related Work
Previous work has focused on a variety of different
meaning representations. Several approaches have
been designed for the variable-free logical repre-
sentations shown in examples throughout this pa-
per. For example, Kate & Mooney (2006) present a
method (KRISP) that extends an existing SVM learn-
ing algorithm to recover logical representations. The
1This computation can be performed efficiently by incre-
mentally updating the parse chart used to find y?.
Inputs: Training set {(xi, zi) : i = 1 . . . n} where each
example is a sentence xi paired with a logical form
zi. Set of NP lexical items ?NP . Number of iter-
ations T . Learning rate parameter ?0 and cooling
rate parameter c.
Definitions: The function NEW-LEX(y) takes a parse
y and returns a set of new lexical items found by
splitting and merging categories in y, as described
in Section 5. The distributions p(y|x, z; ?,?) and
p(y, z|x; ?,?) are defined by the log-linear model,
as described in Section 3.3.
Initialization:
? Set ? = {xi ` S : zi} for all i = 1 . . . n.
? Set ? = ? ? ?NP
? Initialize ? using coocurrence statistics, as de-
scribed in Section 7.
Algorithm:
For t = 1 . . . T, i = 1 . . . n :
Step 1: (Update Lexicon)
? Let y? = arg maxy p(y|xi, zi; ?,?)
? Set ? = ? ?NEW-LEX(y?) and expand the
parameter vector ? to contain entries for the
new lexical items, as described in Section 7.
Step 2: (Update Parameters)
? Let ? = ?01+c?k where k = i+ t? n.
? Let ? = Ep(y|xi,zi;?,?)[?(xi, y, zi)]
?Ep(y,z|xi;?,?)[?(xi, y, z)]
? Set ? = ? + ??
Output: Lexicon ? and parameters ?.
Figure 2: The UBL learning algorithm.
WASP system (Wong & Mooney, 2006) uses statis-
tical machine translation techniques to learn syn-
chronous context free grammars containing both
words and logic. Lu et al (2008) (Lu08) developed
a generative model that builds a single hybrid tree
of words, syntax and meaning representation. These
algorithms are all language independent but repre-
sentation specific.
Other algorithms have been designed to re-
cover lambda-calculus representations. For exam-
ple, Wong & Mooney (2007) developed a variant
of WASP (?-WASP) specifically designed for this
alternate representation. Zettlemoyer & Collins
(2005, 2007) developed CCG grammar induction
techniques where lexical items are proposed accord-
ing to a set of hand-engineered lexical templates.
1229
Our approach eliminates this need for manual effort.
Another line of work has focused on recover-
ing meaning representations that are not based on
logic. Examples include an early statistical method
for learning to fill slot-value representations (Miller
et al, 1996) and a more recent approach for recover-
ing semantic parse trees (Ge & Mooney, 2006). Ex-
ploring the extent to which these representations are
compatible with the logic-based learning approach
we developed is an important area for future work.
Finally, there is work on using categorial gram-
mars to solve other, related learning problems.
For example, Buszkowski & Penn (1990) describe
a unification-based approach for grammar discov-
ery from bracketed natural language sentences and
Villavicencio (2002) developed an approach for
modeling child language acquisition. Additionally,
Bos et al (2004) consider the challenging problem
of constructing broad-coverage semantic representa-
tions with CCG, but do not learn the lexicon.
7 Experimental Setup
Features We use two types of features in our
model. First, we include a set of lexical features:
For each lexical item L ? ?, we include a feature
?L that fires when L is used. Second, we include se-
mantic features that are functions of the output logi-
cal expression z. Each time a predicate p in z takes
an argument a with type T (a) in position i it trig-
gers two binary indicator features: ?(p,a,i) for the
predicate-argument relation; and ?(p,T (a),i) for the
predicate argument-type relation.
Initialization The weights for the semantic fea-
tures are initialized to zero. The weights for the lex-
ical features are initialized according to coocurrance
statistics estimated with the Giza++ (Och & Ney,
2003) implementation of IBM Model 1. We com-
pute translation scores for (word, constant) pairs that
cooccur in examples in the training data. The initial
weight for each ?L is set to ten times the average
score over the (word, constant) pairs in L, except for
the weights of seed lexical entries in ?NP which are
set to 10 (equivalent to the highest possible coocur-
rence score). We used the learning rate ?0 = 1.0
and cooling rate c = 10?5 in all training scenar-
ios, and ran the algorithm for T = 20 iterations.
These values were selected with cross validation on
the Geo880 development set, described below.
Data and Evaluation We evaluate our system
on the GeoQuery datasets, which contain natural-
language queries of a geographical database paired
with logical representations of each query?s mean-
ing. The full Geo880 dataset contains 880 (English-
sentence, logical-form) pairs, which we split into a
development set of 600 pairs and a test set of 280
pairs, following Zettlemoyer & Collins (2005). The
Geo250 dataset is a subset of Geo880 containing
250 sentences that have been translated into Turk-
ish, Spanish and Japanese as well as the original En-
glish. Due to the small size of this dataset we use
10-fold cross validation for evaluation. We use the
same folds as Wong & Mooney (2006, 2007) and Lu
et al (2008), allowing a direct comparison.
The GeoQuery data is annotated with both
lambda-calculus and variable-free meaning rep-
resentations, which we have seen examples of
throughout the paper. We report results for both rep-
resentations, using the standard measures of Recall
(percentage of test sentences assigned correct log-
ical forms), Precision (percentage of logical forms
returned that are correct) and F1 (the harmonic mean
of Precision and Recall).
Two-Pass Parsing To investigate the trade-off be-
tween precision and recall, we report results with a
two-pass parsing strategy. When the parser fails to
return an analysis for a test sentence due to novel
words or usage, we reparse the sentence and allow
the parser to skip words, with a fixed cost. Skip-
ping words can potentially increase recall, if the ig-
nored word is an unknown function word that does
not contribute semantic content.
8 Results and Discussion
Tables 1, 2, and 3 present the results for all of the ex-
periments. In aggregate, they demonstrate that our
algorithm, UBL, learns accurate models across lan-
guages and for both meaning representations. This
is a new result; no previous system is as general.
We also see the expected tradeoff between preci-
sion and recall that comes from the two-pass parsing
approach, which is labeled UBL-s. With the abil-
ity to skip words, UBL-s achieves the highest recall
of all reported systems for all evaluation conditions.
1230
System
English Spanish
Rec. Pre. F1 Rec. Pre. F1
WASP 70.0 95.4 80.8 72.4 91.2 81.0
Lu08 72.8 91.5 81.1 79.2 95.2 86.5
UBL 78.1 88.2 82.7 76.8 86.8 81.4
UBL-s 80.4 80.8 80.6 79.7 80.6 80.1
System
Japanese Turkish
Rec. Pre. F1 Rec. Pre. F1
WASP 74.4 92.0 82.9 62.4 97.0 75.9
Lu08 76.0 87.6 81.4 66.8 93.8 78.0
UBL 78.5 85.5 81.8 70.4 89.4 78.6
UBL-s 80.5 80.6 80.6 74.2 75.6 74.9
Table 1: Performance across languages on Geo250 with
variable-free meaning representations.
System
English Spanish
Rec. Pre. F1 Rec. Pre. F1
?-WASP 75.6 91.8 82.9 80.0 92.5 85.8
UBL 78.0 93.2 84.7 75.9 93.4 83.6
UBL-s 81.8 83.5 82.6 81.4 83.4 82.4
System
Japanese Turkish
Rec. Pre. F1 Rec. Pre. F1
?-WASP 81.2 90.1 85.8 68.8 90.4 78.1
UBL 78.9 90.9 84.4 67.4 93.4 78.1
UBL-s 83.0 83.2 83.1 71.8 77.8 74.6
Table 2: Performance across languages on Geo250 with
lambda-calculus meaning representations.
However, UBL achieves much higher precision and
better overall F1 scores, which are generally compa-
rable to the best performing systems.
The comparison to the CCG induction techniques
of ZC05 and ZC07 (Table 3) is particularly striking.
These approaches used language-specific templates
to propose new lexical items and also required as in-
put a set of hand-engineered lexical entries to model
phenomena such as quantification and determiners.
However, the use of higher-order unification allows
UBL to achieve comparable performance while au-
tomatically inducing these types of entries.
For a more qualitative evaluation, Table 4 shows a
selection of lexical items learned with high weights
for the lambda-calculus meaning representations.
Nouns such as ?state? or ?estado? are consistently
learned across languages with the category S|NP ,
which stands in for the more conventional N . The
algorithm also learns language-specific construc-
tions such as the Japanese case markers ?no? and
?wa?, which are treated as modifiers that do not add
semantic content. Language-specific word order is
also encoded, using the slash directions of the CCG
System
Variable Free Lambda Calculus
Rec. Pre. F1 Rec. Pre. F1
Cross Validation Results
KRISP 71.7 93.3 81.1 ? ? ?
WASP 74.8 87.2 80.5 ? ? ?
Lu08 81.5 89.3 85.2 ? ? ?
?-WASP ? ? ? 86.6 92.0 89.2
Independent Test Set
ZC05 ? ? ? 79.3 96.3 87.0
ZC07 ? ? ? 86.1 91.6 88.8
UBL 81.4 89.4 85.2 85.0 94.1 89.3
UBL-s 84.3 85.2 84.7 87.9 88.5 88.2
Table 3: Performance on the Geo880 data set, with varied
meaning representations.
categories. For example, ?what? and ?que? take
their arguments to the right in the wh-initial English
and Spanish. However, the Turkish wh-word ?nel-
erdir? and the Japanese question marker ?nan desu
ka? are sentence final, and therefore take their argu-
ments to the left. Learning regularities of this type
allows UBL to generalize well to unseen data.
There is less variation and complexity in the
learned lexical items for the variable-free represen-
tation. The fact that the meaning representation is
deeply nested influences the form of the induced
grammar. For example, recall that the sentence
?what states border texas? would be paired with the
meaning answer(state(borders(tex))). For this
representation, lexical items such as:
what ` S/NP : ?x.answer(x)
states `NP/NP : ?x.state(x)
border `NP/NP : ?x.borders(x)
texas `NP : tex
can be used to construct the desired output. In
practice, UBL often learns entries with only a sin-
gle slash, like those above, varying only in the di-
rection, as required for the language. Even the
more complex items, such as those for quantifiers,
are consistently simpler than those induced from
the lambda-calculus meaning representations. For
example, one of the most complex entries learned
in the experiments for English is the smallest `
NP\NP/(NP |NP ):?f?x.smallest one(f(x)).
There are also differences in the aggregate statis-
tics of the learned lexicons. For example, the aver-
age length of a learned lexical item for the (lambda-
calculus, variable-free) meaning representations is:
1231
(1.21,1.08) for Turkish, (1.34,1.19) for English,
(1.43,1.25) for Spanish and (1.63,1.42) for Japanese.
For both meaning representations the model learns
significantly more multiword lexical items for the
somewhat analytic Japanese than the agglutinative
Turkish. There are also variations in the average
number of learned lexical items in the best parses
during the final pass of training: 192 for Japanese,
206 for Spanish, 188 for English and 295 for Turk-
ish. As compared to the other languages, the mor-
pologically rich Turkish requires significantly more
lexical variation to explain the data.
Finally, there are a number of cases where the
UBL algorithm could be improved in future work.
In cases where there are multiple allowable word or-
ders, the UBL algorithm must learn individual en-
tries for each possibility. For example, the following
two categories are often learned with high weight for
the Japanese word ?chiisai?:
NP/(S|NP )\(NP |NP ):?f?g.argmin(x, g(x), f(x))
NP |(S|NP )/(NP |NP ):?f?g.argmin(x, g(x), f(x))
and are treated as distinct entries in the lexicon. Sim-
ilarly, the approach presented here does not model
morphology, and must repeatedly learn the correct
categories for the Turkish words ?nehri,? ?nehir,?
?nehirler,? and ?nehirlerin?, all of which correspond
to the logical form ?x.river(x).
9 Conclusions and Future Work
This paper has presented a method for inducing
probabilistic CCGs from sentences paired with log-
ical forms. The approach uses higher-order unifi-
cation to define the space of possible grammars in
a language- and representation-independent manner,
paired with an algorithm that learns a probabilistic
parsing model. We evaluated the approach on four
languages with two meaning representations each,
achieving high accuracy across all scenarios.
For future work, we are interested in exploring
the generality of the approach while extending it to
new understanding problems. One potential limi-
tation is in the constraints we introduced to ensure
the tractability of the higher-order unification proce-
dure. These restrictions will not allow the approach
to induce lexical items that would be used with,
among other things, many of the type-raised combi-
nators commonly employed in CCG grammars. We
English
population of ` NP/NP : ?x.population(x)
smallest ` NP/(S|NP ) : ?f.arg min(y, f(y), size(y))
what ` S|NP/(S|NP ) : ?f?x.f(x)
border ` S|NP/NP : ?x?y.next to(y, x)
state ` S|NP : ?x.state(x)
most ` NP/(S|NP )\(S|NP )\(S|NP |NP ) :
?f?g?h?x.argmax(y, g(y), count(z, f(z, y) ? h(z)))
Japanese
no ` NP |NP/(NP |NP ) : ?f?x.f(x)
shuu ` S|NP : ?x.state(x)
nan desu ka ` S\NP\(NP |NP ) : ?f?x.f(x)
wa ` NP |NP\(NP |NP ) : ?f?x.f(x)
ikutsu ` NP |(S|NP )\(S|NP |(S|NP )) :
?f?g.count(x, f(g(x)))
chiiki ` NP\NP :?x.area(x)
Turkish
nedir ` S\NP\(NP |NP ) : ?f?x.f(x)
sehir ` S|NP : ?x.city(x)
nufus yogunlugu ` NP |NP : ?x.density(x)
siniri` S|NP/NP : ?x?y.next to(y, x)
kac tane ` S\NP/(S|NP |NP )\(S|NP ) :
?f?g?x.count(y, f(y) ? g(y, x))
ya siniri ` S|NP\NP : ?x?y.next to(y, x)
Spanish
en ` S|NP/NP : ?x?y.loc(y, x)
que es la ` S/NP/(NP |NP ): ?f?x.f(x)
pequena ` NP\(S|NP )\(NP |NP ) :
?g?f.arg min(y, f(y), g(y))
estado ` S|NP : ?x.state(x)
mas ` S\(S|NP )/(S|NP )\(NP |NP |(S|NP )) :
?f?g?h.argmax(x, h(x), f(g, x))
mayores `S|NP\(S|NP ) :?f?x.f(x) ?major(x)
Table 4: Example learned lexical items for each language
on the Geo250 lambda-calculus data sets.
are also interested in developing similar grammar
induction techniques for context-dependent under-
standing problems, such as the one considered by
Zettlemoyer & Collins (2009). Such an approach
would complement ideas for using high-order unifi-
cation to model a wider range of language phenom-
ena, such as VP ellipsis (Dalrymple et al, 1991).
Acknowledgements
We thank the reviewers for useful feedback. This
work was supported by the EU under IST Cog-
nitive Systems grant IP FP6-2004-IST-4-27657
?Paco-Plus? and ERC Advanced Fellowship 249520
?GRAMPLUS? to Steedman. Kwiatkowski was
supported by an EPRSC studentship. Zettlemoyer
was supported by a US NSF International Research
Fellowship.
1232
References
Bos, J., Clark, S., Steedman, M., Curran, J. R., & Hock-
enmaier, J. (2004). Wide-coverage semantic represen-
tations from a CCG parser. In Proceedings of the In-
ternational Conference on Computational Linguistics.
Buszkowski, W. & Penn, G. (1990). Categorial grammars
determined from linguistic data by unification. Studia
Logica, 49, 431?454.
Carpenter, B. (1997). Type-Logical Semantics. The MIT
Press.
Clark, S. & Curran, J. R. (2003). Log-linear models
for wide-coverage CCG parsing. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Clark, S. & Curran, J. R. (2007). Wide-coverage effi-
cient statistical parsing with CCG and log-linear mod-
els. Computational Linguistics, 33(4), 493?552.
Dalrymple, M., Shieber, S., & Pereira, F. (1991). Ellipsis
and higher-order unification. Linguistics and Philoso-
phy, 14, 399?452.
Ge, R. & Mooney, R. J. (2006). Discriminative rerank-
ing for semantic parsing. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions.
Huet, G. (1975). A unification algorithm for typed ?-
calculus. Theoretical Computer Science, 1, 27?57.
Huet, G. P. (1973). The undecidability of unification in
third order logic. Information and Control, 22(3), 257?
267.
Kate, R. J. & Mooney, R. J. (2006). Using string-kernels
for learning semantic parsers. In Proceedings of the
44th Annual Meeting of the Association for Computa-
tional Linguistics.
Kate, R. J., Wong, Y. W., & Mooney, R. J. (2005). Learn-
ing to transform natural to formal languages. In Pro-
ceedings of the National Conference on Artificial In-
telligence.
LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998).
Gradient-based learning applied to document recogni-
tion. Proceedings of the IEEE, 86(11), 2278?2324.
Lu, W., Ng, H. T., Lee, W. S., & Zettlemoyer, L. S.
(2008). A generative model for parsing natural lan-
guage to meaning representations. In Proceedings of
The Conference on Empirical Methods in Natural Lan-
guage Processing.
Miller, S., Stallard, D., Bobrow, R. J., & Schwartz, R. L.
(1996). A fully statistical approach to natural language
interfaces. In Proc. of the Association for Computa-
tional Linguistics.
Och, F. J. & Ney, H. (2003). A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1), 19?51.
Steedman, M. (1996). Surface Structure and Interpreta-
tion. The MIT Press.
Steedman, M. (2000). The Syntactic Process. The MIT
Press.
Thompson, C. A. & Mooney, R. J. (2002). Acquiring
word-meaning mappings for natural language inter-
faces. Journal of Artificial Intelligence Research, 18.
Villavicencio, A. (2002). The acquisition of a unification-
based generalised categorial grammar. Ph.D. thesis,
University of Cambridge.
Wong, Y. W. & Mooney, R. (2006). Learning for seman-
tic parsing with statistical machine translation. In Pro-
ceedings of the Human Language Technology Confer-
ence of the NAACL.
Wong, Y. W. & Mooney, R. (2007). Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Proceedings of the Association for Com-
putational Linguistics.
Zelle, J. M. & Mooney, R. J. (1996). Learning to parse
database queries using inductive logic programming.
In Proceedings of the National Conference on Artifi-
cial Intelligence.
Zettlemoyer, L. S. & Collins, M. (2005). Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Pro-
ceedings of the Conference on Uncertainty in Artificial
Intelligence.
Zettlemoyer, L. S. & Collins, M. (2007). Online learning
of relaxed CCG grammars for parsing to logical form.
In Proc. of the Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning.
Zettlemoyer, L. S. & Collins, M. (2009). Learning
context-dependent mappings from sentences to logical
form. In Proceedings of The Joint Conference of the
Association for Computational Linguistics and Inter-
national Joint Conference on Natural Language Pro-
cessing.
1233
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 638?647,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Bayesian Mixture Model for Part-of-Speech Induction
Using Multiple Features
Christos Christodoulopoulos
School of Informatics
University of Edinburgh
christos.c@ed.ac.uk
Sharon Goldwater
School of Informatics
University of Edinburgh
sgwater@inf.ed.ac.uk
Mark Steedman
School of Informatics
University of Edinburgh
steedman@inf.ed.ac.uk
Abstract
In this paper we present a fully unsupervised
syntactic class induction system formulated as
a Bayesian multinomial mixture model, where
each word type is constrained to belong to a
single class. By using a mixture model rather
than a sequence model (e.g., HMM), we are
able to easily add multiple kinds of features,
including those at both the type level (mor-
phology features) and token level (context and
alignment features, the latter from parallel cor-
pora). Using only context features, our sys-
tem yields results comparable to state-of-the
art, far better than a similar model without the
one-class-per-type constraint. Using the addi-
tional features provides added benefit, and our
final system outperforms the best published
results on most of the 25 corpora tested.
1 Introduction
Research on unsupervised learning for NLP has be-
come widespread recently, with part-of-speech in-
duction, or syntactic class induction, being a partic-
ularly popular task.1 However, despite a recent pro-
liferation of syntactic class induction systems (Bie-
mann, 2006; Goldwater and Griffiths, 2007; John-
son, 2007; Ravi and Knight, 2009; Berg-Kirkpatrick
et al, 2010; Lee et al, 2010), careful compari-
son indicates that very few systems perform better
than some much simpler and quicker methods dating
back ten or even twenty years (Christodoulopoulos
1The task is more commonly referred to as part-of-speech
induction, but we prefer the term syntactic class induction since
the induced classes may not coincide with part-of-speech tags.
et al, 2010). This fact suggests that we should con-
sider which features of the older systems led to their
success, and attempt to combine these features with
some of the machine learning methods introduced
by the more recent systems. We pursue this strat-
egy here, developing a system based on Bayesian
methods where the probabilistic model incorporates
several insights from previous work.
Perhaps the most important property of our model
is that it is type-based, meaning that all tokens of
a given word type are assigned to the same clus-
ter. This property is not strictly true of linguistic
data, but is a good approximation: as Lee et al
(2010) note, assigning each word type to its most
frequent part of speech yields an upper bound ac-
curacy of 93% or more for most languages. Since
this is much better than the performance of cur-
rent unsupervised syntactic class induction systems,
constraining the model in this way seems likely to
improve performance by reducing the number of
parameters in the model and incorporating useful
linguistic knowledge. Both of the older systems
discussed by Christodoulopoulos et al (2010), i.e.,
Clark (2003) and Brown et al (1992), included this
constraint and achieved very good performance rel-
ative to token-based systems. More recently, Lee et
al. (2010) presented a new type-based model, and
also reported very good results.
A second property of our model, which distin-
guishes it from the type-based Bayesian model of
Lee et al (2010), is that the underlying probabilistic
model is a clustering model, (specifically, a multino-
mial mixture model) rather than a sequence model
(HMM). In this sense, our model is more closely re-
638
lated to several non-probabilistic systems that clus-
ter context vectors or lower-dimensional represen-
tations of them (Redington et al, 1998; Schu?tze,
1995; Lamar et al, 2010). Sequence models are
by far the most common method of supervised part-
of-speech tagging, and have also been widely used
for unsupervised part-of-speech tagging both with
and without a dictionary (Smith and Eisner, 2005;
Haghighi and Klein, 2006; Goldwater and Griffiths,
2007; Johnson, 2007; Ravi and Knight, 2009; Lee et
al., 2010). However, systems based on context vec-
tors have also performed well in these latter scenar-
ios (Schu?tze, 1995; Lamar et al, 2010; Toutanova
and Johnson, 2007) and present a viable alternative
to sequence models.
One advantage of using a clustering model rather
than a sequence model is that the features used for
clustering need not be restricted to context words.
Additional types of features can easily be incorpo-
rated into the model and inference procedure using
the same general framework as in the basic model
that uses only context word features. In particu-
lar, we present two extensions to the basic model.
The first uses morphological features, which serve
as cues to syntactic class and seemed to partly ex-
plain the success of two best-performing systems
analysed by Christodoulopoulos et al (2010). The
second extension to our model uses alignment fea-
tures gathered from parallel corpora. Previous work
suggests that using parallel text can improve perfor-
mance on various unsupervised NLP tasks (Naseem
et al, 2009; Snyder and Barzilay, 2008).
We evaluate our model on 25 corpora in 20 lan-
guages that vary substantially in both syntax and
morphology. As in previous work (Lee et al, 2010),
we find that the one-class-per-type restriction boosts
performance considerably over a comparable token-
based model and yields results that are comparable
to state-of-the-art even without the use of morphol-
ogy or alignment features. Including morphology
features yields the best published results on 14 or 15
of our 25 corpora (depending on the measure) and
alignment features can improve results further.
2 Models
Our model is a multinomial mixture model with
Bayesian priors over the mixing weights ? and
? ?
z
? ? f
Z
M
nj
Figure 1: Plate diagram of the basic model with a single
feature per token (the observed variable f ). M , Z, and
nj are the number of word types, syntactic classes z, and
features (= tokens) per word type, respectively.
multinomial class output parameters ?. The model
is defined so that all observations associated with
a single word type are generated from the same
mixing component (syntactic class). In the basic
model, these observations are token-level features;
the morphology model adds type-level features as
well. We begin by describing the simplest version of
our model, where each word token is associated with
a single feature, for example its left context word
(the word that occurs to its left in the corpus). We
then show how to generalise the model to multiple
token-level features and to type-level features.
2.1 Basic model
In the basic model, each word token is represented
by a single feature such as its left context word.
These features are the observed data; the model ex-
plains the data by assuming that it has been gener-
ated from some set of latent syntactic classes. The
ith class is associated with a multinomial parameter
vector ?i that defines the distribution over features
generated from that class, and with a mixing weight
?i that defines the prior probability of that class. ?
and ?i are drawn from symmetric Dirichlet distribu-
tions with parameters ? and ? respectively.
The generative story goes as follows: First, gen-
erate the prior class probabilities ?. Next, for each
639
word type j = 1 . . .M , choose a class assignment zj
from the distribution ?. For each class i = 1 . . . Z,
choose an output distribution over features ?i. Fi-
nally, for each token k = 1 . . . nj of word type j,
generate a feature fjk from ?zj , the distribution as-
sociated with the class that word type j is assigned
to. The model is illustrated graphically in Figure 1
and is defined formally as follows:
? |? ? Dirichlet(?)
zj | ? ? Multinomial(?)
?i |? ? Dirichlet(?)
fjk |?zj ? Multinomial(?zj )
In addition to the variables defined above, we will
use F to refer to the number of different possible
values a feature can take on (so that ? is a Z ? F
matrix). Thus, one way to think of the model is as a
vector-based clustering system, where word type j is
associated with a 1?F vector of feature counts rep-
resenting the features of all nj tokens of j, and these
vectors are clustered into similar classes. The differ-
ence from other vector-based syntactic class induc-
tion systems is in the method of clustering. Here,
we define a Gibbs sampler that samples from the
posterior distribution of the clusters given the ob-
served features; other systems have used various
standard distance-based vector clustering methods.
Some systems also include dimensionality reduction
(Schu?tze, 1995; Lamar et al, 2010) to reduce the
size of the context vectors; we simply use theF most
common words as context features.
2.2 Inference
At inference time we want to sample a syntactic
class assignment z from the posterior of the model.
We use a collapsed Gibbs sampler, integrating out
the parameters ? and ? and sampling from the fol-
lowing distribution:
P (z|f , ?, ?) ? P (z|?)P (f |z, ?). (1)
Rather than sampling the joint class assignment
P (z|f , ?, ?) directly, the sampler iterates over each
word type j, resampling its class assignment zj
given the current assignments z?j of all other word
types. The posterior over zj can be computed as
P (zj | z?j , f , ?, ?)
? P (zj | z?j , ?, ?)P (fj | f?j , z, ?, ?) (2)
where fj are the features associated with word type
j (one feature for each token of j). The first (prior)
factor is easy to compute due to the conjugacy be-
tween the Dirichlet and multinomial distributions,
and is equal to
P (zj = z | z?j , ?) =
nz + ?
n? + Z?
(3)
where nz is the number of types in class z and n?
is the total number of word types in all classes. All
counts in this and the following equations are com-
puted with respect to z?j (e.g., n? = M ? 1).
Computing the second (likelihood) factor is
slightly more complex due to the dependencies be-
tween the different variables in fj that are induced
by integrating out the ? parameters. Consider first a
simple case where word type j occurs exactly twice
in the corpus, so fj contains two features. The prob-
ability of the first feature fj1 is equal to
P (fj1 = f | zj = z, z?j , f?j , ?) =
nf,z + ?
n?,z + F?
(4)
where nf,z is the number of times feature f has been
seen in class z, n?,z is the total number of feature
tokens in the class, and F is the number of different
possible features.
The probability of the second feature fj2 can be
calculated similarly, except that it is conditioned on
fj1 in addition to the other variables, so the counts
for previously observed features must include the
counts due to fj1 as well as those due to f?j . Thus,
the probability is
P (fj2 = f | fj1, zj = z, z?j , f?j , ?)
= nf,z + ?(fj1, fj2) + ?n?,z + 1 + F?
(5)
where ? is the Kronecker delta function, equal to 1
if its arguments are equal and 0 otherwise.
Extending this example to the general case, the
probability of a sequence of features fj is computed
using the chain rule, where the counts used in each
factor are incremented as necessary for each addi-
tional conditioning feature, yielding the following
expression:
P (fj | f?j , zj = z, z?j , ?)
=
?F
k=1
?njk?1
i=0 (njk,z + i + ?)?nj?1
i=0 (n?,z + i + F?)
(6)
640
where njk is the number of instances of feature k in
word type j.2
2.3 Extended models
We can extend the model above in two different
ways: by adding more features at the word token
level, or by adding features at the type level. To add
more token-level features, we simply assume that
each word token generates multiple features, one
feature from each of several different kinds.3 For
example, the left context word might be one kind of
feature and the right context word another. We as-
sume conditional independence between the gener-
ated features given the syntactic class, so each kind
of feature t has its own output parameters ?(t). A
plate diagram of the model with T kinds of features
is shown in Figure 2 (a type-level feature is also in-
cluded in this diagram, as described below).
Due to the independence assumption between the
different kinds of features, the basic Gibbs sampler
is easy to extend to this case by simpling multiplying
in extra factors for the additional kinds of features,
with the prior (Equation 3) unchanged. The likeli-
hood becomes:
P (f (1)j , . . . , f
(T )
j | f
(1...T )
?j , zj = z, z?j , ?)
=
T?
t=1
P (f (t)j | f
(t)
?j , zj = z, z?j , ?) (7)
where each factor in the product is computed using
Equation 6.
In addition to monolingual context features, we
also explore the use of alignment features for those
languages where we have parallel corpora. These
features are extracted for language ? by word-
aligning ? to another language ?? (details of the
alignment procedure are described in Section 3.1).
The features used for each token e in ? are the left
and right context words of the word token that is
aligned to e (if there is one). As with the mono-
lingual context features, we use only the F most fre-
quent words in ?? as possible features.
2One could approximate this likelihood term by assuming
independence between all nj feature tokens of word type j.
This is the approach taken by Lee et al (2010).
3We use the word kind here to avoid confusion with type,
which we reserve for the type-token distinction, which can ap-
ply to features as well as words.
Note that this model with multiple context fea-
tures is deficient: it can generate data that are in-
consistent with any actual corpus, because there is
no mechanism to constrain the left context word
of token ei to be the same as the right context
word of token ei?1 (and similarly with alignment
features). However, deficient models have proven
useful in other unsupervised NLP tasks (Klein and
Manning, 2002; Toutanova and Johnson, 2007). In
particular, Toutanova and Johnson (2007) demon-
strate good performance on unsupervised part-of-
speech tagging (using a dictionary) with a Bayesian
model similar to our own. If we remove the part of
their model that relies on the dictionary (the mor-
phological ambiguity classes), their model is equiv-
alent to our own, without the restriction of one class
per type. We use this token-based version of our
model as a baseline in our experiments.
The final extension to our model introduces type-
level features, specifically morphology features.
The model is illustrated in Figure 2. We assume
conditional independence between the morphology
features and other features, so again we can simply
multiply another factor into the likelihood during in-
ference. There is only one morphological feature per
type, so this factor has the form of Equation 4. Since
frequent words will have many token-level features
contributing to the likelihood and only one morphol-
ogy feature, the morphology features will have a
greater effect for infrequent words (as appropriate,
since there is less evidence from context and align-
ments). As with the other kinds of features, we use
only a limited number Fm of morphology features,
as described below.
3 Experiments
3.1 Experimental setup
We evaluate our models using an increasing level
of complexity, starting with a model that uses only
monolingual context features. We use the F = 100
most frequent words as features, and consider two
versions of this model: one with two kinds of fea-
tures (one left and one right context word) and one
with four (two context words on each side).
For the model with morphology features we ran
the unsupervised morphological segmentation sys-
tem Morfessor (Creutz and Lagus, 2005) to get a
641
??
z
f (1) ?(1) ?(1)
. . . . . . . . .
f (T ) ?(T ) ?(T )m
?(m) ?(m)
M
nj
nj
Z
Z
Z
Figure 2: Plate diagram of the extended model with T
kinds of token-level features (f (t) variables) and a single
kind of type-level feature (morphology, m).
segmentation for each word type in the corpus. We
then extracted the suffix of each word type4 and used
it as a feature type. This process yielded on average
Fm = 110 morphological feature types5. Each word
type generates at most one of these possible features.
If there are overlapping possibilities (e.g. -ingly and
-y) we take the longest possible match.
We also explore the idea of extending the mor-
phology feature space beyond suffixes, by including
features like capitalisation and punctuation. Specif-
ically we use the features described in Haghighi
and Klein (2006), namely initial-capital, contains-
hyphen, contains-digit and we add an extra feature
contains-punctuation.
For the model with alignment features, we fol-
low (Naseem et al, 2009) in using only bidirectional
alignments: using Giza++ (Och and Ney, 2003),
we get the word alignments in both directions be-
tween all possible language pairs in our parallel cor-
pora (i.e., alternating the source and target languages
within each pair). We then use only those align-
ments that are found in both directions. As discussed
4Since Morfessor yields multiple affixes for each word we
concatenated all the suffixes into a single suffix.
5There was large variance in the number of feature types for
each language ranging from 11 in Chinese to more than 350 in
German and Czech.
above, we use two kinds of alignment features: the
left and right context words of the aligned token in
the other language. The feature space is set to the
F = 100 most frequent words in that language.
Instead of fixing the hyperparameters ? and ?, we
used the Metropolis-Hastings sampler presented by
Goldwater and Griffiths (2007) to get updated values
based on the likelihood of the data with respect to
those hyperparameters6. In order to improve conver-
gence of the sampler, we used simulated annealing
with a sigmoid-shaped cooling schedule from an ini-
tial temperature of 2 down to 1. Preliminary experi-
ments indicated that we could achieve better results
by cooling even further (approximating the MAP so-
lution rather than a sample from the posterior), so for
all experiments reported here, we ran the sampler for
a total of 2000 iterations, with the last 400 of these
decreasing the temperature from 1 to 0.66.
Finally, we investigated two different initialisa-
tion techniques: First, we use random class as-
signments to word types (referred to as method 1)
and second, we assign each of the Z most frequent
word types to a separate class and then randomly
distribute the rest of the word types to the classes
(method 2).
3.2 Datasets
Although unsupervised systems should in principle
be language- and corpus-independent, most part-of-
speech induction systems (especially in the early lit-
erature) have been developed on English. Whether
because English is simply an easier language, or be-
cause of bias introduced during development, these
systems? performance is considerably worse in other
languages (Christodoulopoulos et al, 2010)
Since we aim to use our system mostly on non-
English corpora, and ones that are significantly
smaller than the large English treebank corpora, we
developed our models using one of the languages of
the MULTEXT-East corpus (Erjavec, 2004), namely
Bulgarian. The other languages in the corpus were
used during development as a source of word align-
ments, but otherwise were only used for testing final
versions of our models. Since none of the authors
speak any of the languages in the MULTEXT col-
6For simplicity, we tied the ? parameters for the two or four
kinds of context features to the same value, and similarly the ?
parameters for the two kinds of alignment features.
642
lection, we also used the Penn Treebank WSJ cor-
pus (Marcus et al, 1993) for development. Fol-
lowing Christodoulopoulos et al (2010) we created
a smaller version of the WSJ corpus (referred to
as wsj-s) to approximate the size of the corpora in
MULTEXT-East. For comparison to other systems,
we also used the full WSJ at test time.
For further testing, we used the remaining MUL-
TEXT languages, as well as the languages of the
CONNL-X (Buchholz and Marsi, 2006) shared task.
This dataset contains 13 languages, 4 of which
are freely available (Danish, Dutch, Portuguese
and Swedish) and 9 that are used with permission
from the creators of the corpora ( Arabic7, Bul-
garian8, Czech9, German10, Chinese11, Japanese12,
Slovene13, Spanish14, Turkish15 ). Following Lee et
al. (2010) we used only the training sections for each
language.
Finally, to widen the scope of our system, we gen-
erated two more corpora in French16 and Ancient
Greek17, extracting the gold standard parts of speech
from the respective dependency treebanks.
3.3 Baselines
We chose three baselines for comparison. The first
is the basic k-means clustering algorithm, which we
applied to the same feature vectors we extracted for
our system (context + extended morphology), using
a Euclidean distance metric. This provides a very
simple vector-based clustering baseline. The second
baseline is a more recent vector-based syntactic class
induction method, the SVD approach of (Lamar et
al., 2010), which extends Schu?tze (1995)?s original
method and, like ours, enforces a one-class-per-tag
restriction. As a third baseline we use the system of
Clark (2003) since it is a type-level system that mod-
7Part of the Prague Arabic Treebank (Hajic? et al, 2003;
Smrz? and Pajas, 2004)
8Part of the BulTreeBank (Simov et al, 2004).
9Part of the Prague Dep. Treebank (Bo?hmova? et al, 2001)
10Part of the TIGER Treebank (Brants et al, 2002)
11Part of the Sinica Treebank (Keh-Jiann et al, 2003)
12Part of the Tu?bingen Treebank of Spoken Japanese (for-
merly VERMOBIL Treebank - Kawata and Bartels (2000)).
13Part of the Slovene Dep. Treebank (Dz?eroski et al, 2006)
14Part of the Cast3LB Treebank (Civit et al, 2006)
15Part of the METU-Sabanci Treebank (Oflazer et al, 2003).
16French Treebank (Abeille? et al, 2000)
17Greek Dependency Treebank (Bamman et al, 2009)
els morphology and has produced very good results
on multilingual corpora.
4 Results and Analysis
4.1 Development results
Tables 1 and 2 present the results from develop-
ment runs, which were used to decide which fea-
tures to incorporate in the final system. We used V-
Measure (Rosenberg and Hirschberg, 2007) as our
primary evaluation score, but also present many-to-
one matching accuracy (M-1) scores for better com-
parison with previously published results. We chose
V-Measure (VM) as our evaluation score because it
is less sensitive to the number of classes induced by
the model (Christodoulopoulos et al, 2010), allow-
ing us to develop our models without using the num-
ber of classes as a parameter. We fixed the number
of classes in all systems to 45 during development;
note however that the gold standard tag set for Bul-
garian contains only 12 tags, so the results in Ta-
ble 1 (especially the M-1 scores) are not comparable
to previous results. For results using the number of
gold-standard tags refer to Table 4.
The first conclusion that can be drawn from these
results is the large difference between the token-
and type-based versions of our system, which con-
firms that the one-class-per-type restriction is help-
ful for unsupervised syntactic class induction. We
also see that for both languages, the performance of
the model using 4 context words (?2 on each side) is
worse than the 2 context words model. We therefore
used only two context words for all of our additional
test languages (below).
We can clearly see that morphological features
are helpful in both languages; however the extended
features of Haghighi and Klein (2006) seem to help
only on the English data. This could be due to the
fact that Bulgarian has a much richer morphology
and thus the extra features contribute little to the
overall performance of the model.
The contribution of the alignment features on the
Bulgarian corpus (aligned with English) is less sig-
nificant than that of morphology but when com-
bined, the two sets of features yield the best per-
formance. This provides evidence in favor of using
multiple features.
Finally, initialisation method 2 does not yield
643
system ?1 words ?2 words
VM/M-1 VM/M-1
base 58.1 / 70.8 55.4 / 67.6
base(tokens) 48.3 / 62.5 37.0 / 54.4
base(init) 57.6 / 70.1 56.1 / 68.6
+morph 58.3 / 74.9 57.4 / 71.9
+morph(ext) 57.8 / 73.7 57.8 / 70.1
(init)+morph 57.8 / 74.3 57.3 / 69.5
(init)+morph(ext) 58.1 / 74.3 57.2 / 71.3
+aligns(EN) 58.1 / 72.6 56.7 / 71.1
+aligns(EN)+morph 59.0 / 75.4 57.5 / 69.7
Table 1: V-measure (VM) and many-to-one (M-1) results
on the MULTEXT-Bulgarian corpus for various mod-
els using either ?1 or ?2 context words as features.
base: context features only; (tokens): token-based model;
(init): Initialisation method 2?other results use method
1; (ext): Extended morphological features.
system ?1 words ?2 words
VM/M-1 VM/M-1
base 63.3 / 64.3 62.4 / 63.3
base(tokens) 48.6 / 57.8 49.3 / 38.3
base(init) 62.7 / 62.9 62.2 / 62.4
+morph 66.4 / 66.7 65.1 / 67.2
+morph(ext) 67.7 / 72.0 65.6 / 67.0
(init)+morph 64.8 / 66.9 64.2 / 66.0
(init)+morph(ext) 67.4 / 71.3 65.7 / 67.1
Table 2: V-measure and many-to-one results on the wsj-s
corpus for various models, as described in Table 1.
.
consistent improvements over the standard ran-
dom initialisation?if anything, it seems to perform
worse. We therefore use only method 1 in the re-
maining experiments.
4.2 Overall results
Table 3 presents the results on our parallel corpora.
We tested all possible combinations of two lan-
guages to align, and present both the average score
over all alignments, and the score under the best
choice of aligned language.18 Also shown are the
results of adding morphology features to the basic
model (context features only) and to the best align-
ment model for each language. In accord with our
18The choice of language was based on the same test data, so
the ?best-language? results should be viewed as oracle scores.
development results, adding morphology to the ba-
sic model is generally useful. The alignment results
are mixed: on the one hand, choosing the best pos-
sible language to align yields improvements, which
can be improved further by adding morphological
features, resulting in the best scores of all models
for most languages. On the other hand, without
knowing which language to choose, alignment fea-
tures do not help on average. We note, however,
that three out of the seven languages have English
as their best-aligned pair (perhaps due to its better
overall scores), which suggests that in the absence
of other knowledge, aligning with English may be a
good choice.
The low average performance of the alignment
features is disappointing, but there are many pos-
sible variations on our method for extracting these
features that we have not yet tested. For example,
we used only bidirectional alignments in an effort to
improve alignment precision, but these alignments
typically cover less than 40% of tokens. It is pos-
sible that a higher-recall set of alignments could be
more useful.
We turn now to our results on all 25 corpora,
shown in Table 4 along with corpus statistics, base-
line results, and the best published results for each
language (when available). Our system, includ-
ing morphology features in all cases, is listed as
BMMM (Bayesian Multinomial Mixture Model).
We do not include alignment features for the MUL-
TEXT languages since these features only yielded
improvements for the oracle case where we know
which aligned language to choose. Nevertheless, our
MULTEXT scores mostly outperform all other sys-
tems. Overall, we acheive the highest published re-
sults on 14 (VM) or 15 (M-1) of the 25 corpora.
One surprising discovery is the high performance
of the k-means clustering system. Despite its sim-
plicity, it is competitive with the other systems and
in a few cases even achieves the best published re-
sults.
5 Conclusion
We have presented a Bayesian model for syntactic
class induction that has two important properties.
First, it is type-based, assigning the same class to
every token of a word type. We have shown by
644
BASE ALIGNMENTS
Lang. base +morph Avg. Best +morph
VM/M-1 VM/M-1 VM/M-1 VM/M1 VM/M1
Bulgarian 54.4 / 61.5 54.5 / 64.3 53.1 / 60.5 55.2 / 64.5(EN) 55.7 / 66.0
Czech 54.2 / 58.9 53.9 / 64.2 52.6 / 58.4 53.8 / 59.7(EN) 55.4 / 66.4
English 62.9 / 72.4 63.3 / 73.3 62.5 / 72.0 63.2 / 71.9(HU) 63.5 / 73.7
Estonian 52.8 / 63.5 53.3 / 67.4 52.8 / 63.9 53.5 / 65.0(EN) 54.3 / 66.9
Hungarian 53.3 / 60.4 54.8 / 68.2 53.3 / 60.8 53.9 / 61.1(RO) 55.9 / 67.1
Romanian 53.9 / 62.4 52.3 / 61.1 56.2 / 63.7 57.5 / 64.6(ES) 54.5 / 63.4
Slovene 57.2 / 65.9 56.7 / 67.9 54.7 / 64.1 55.9 / 64.4(HU) 56.7 / 67.9
Serbian 49.1 / 56.6 49.0 / 62.0 47.3 / 55.6 48.9 / 59.4(CZ) 48.3 / 60.8
Table 3: V-measure (VM) and many-to-one (M-1) results on the languages in the MULTEXT-East corpus using
the gold standard number of classes shown in Table 4. BASE results use ?1-word context features alone or with
morphology. ALIGNMENTS adds alignment features, reporting the average score across all possible choices of paired
language and the scores under the best performing paired language (in parens), alone or with morphology features.
Language Types Tags k-means SVD2 clark Best Pub. BMMM
WS
J wsj 49,190 45 59.5 / 61.6 58.2 / 64.0 65.6 / 71.2 68.8 / 76.1? 66.1 / 72.8
wsj-s 16,850 45 56.7 / 60.1 54.3 / 60.7 63.8 / 68.8 62.3 / 70.7? 67.7 / 72.0
MU
LT
EX
T-E
ast
Bulgarian 16,352 12 50.3 / 59.3 41.7 / 51.0 55.6 / 66.5 - 54.5 / 64.4
Czech 19,115 12 48.6 / 56.7 35.5 / 50.9 52.6 / 64.1 - 53.9 / 64.2
English 9,773 12 56.5 / 65.4 52.3 / 65.5 60.5 / 70.6 - 63.3 / 73.3
Estonian 17,845 11 45.3 / 55.6 38.7 / 55.3 44.4 / 58.4 - 53.3 / 64.4
Hungarian 20,321 12 46.7 / 53.9 39.8 / 49.5 48.9 / 61.4 - 54.8 / 68.2
Romanian 15,189 14 45.2 / 55.1 42.1 / 52.6 40.9 / 49.9 - 52.3 / 61.1
Slovene 17,871 12 46.9 / 56.2 39.5 / 54.2 54.9 / 69.4 - 56.7 / 67.9
Serbian 18,095 12 41.4 / 47.0 39.1 / 54.6 51.0 / 64.1 - 49.0 / 62.0
Co
NL
L0
6S
har
ed
Ta
sk
Arabic 12,915 20 43.3 / 60.7 27.6 / 49.0 40.6 / 59.8 - 42.4 / 61.5
Bulgarian 32,439 54 53.6 / 65.6 49.0 / 65.3 59.6 / 70.4 - 58.8 / 68.9
Chinese 40,562 15 32.6 / 61.1 24.5 / 54.6 31.8 / 56.7 - 42.6 / 69.4
Czech 130,208 12 - - 47.1 / 65.5 - 48.4 / 65.7
Danish 18,356 25 51.7 / 61.6 40.8 / 57.6 52.7 / 65.3 - / 66.7? 59.0 / 71.1
Dutch 28,393 13 45.3 / 60.5 36.7 / 52.4 52.2 / 67.9 - / 67.3? 54.7 / 71.1
German 72,326 54 58.7 / 67.5 54.1 / 64.2 63.0 / 73.9 - / 68.4? 61.9 / 74.4
Japanese 3,231 80 76.1 / 76.2 74.4 / 75.5 78.6 / 77.4 - 77.4 / 78.5
Portuguese 28,931 22 51.6 / 64.4 45.9 / 63.1 57.4 / 69.2 - / 75.3? 63.9 / 76.8
Slovene 7,128 29 52.6 / 64.2 44.0 / 60.3 53.9 / 63.5 - 49.4 / 56.2
Spanish 16,458 47 59.5 / 69.2 54.8 / 68.2 61.6 / 71.9 - / 73.2? 63.2 / 71.7
Swedish 20,057 41 53.2 / 62.2 47.4 / 59.1 58.9 / 68.7 - / 60.6? 58.0 / 68.2
Turkish 17,563 30 40.8 / 62.8 27.4 / 52.4 36.8 / 58.1 - 40.2 / 58.7
French 49,964 23 48.2 / 68.6 46.3 / 68.5 57.3 / 77.8 - 55.0 / 76.6
A.Greek 15,194 15 38.6 / 44.8 24.2 / 38.5 33.3 / 45.4 - 40.5 / 45.1
Table 4: Final results on 25 corpora in 20 languages, with the number of induced classes equal to the number of gold
standard tags in all cases. k-means and SVD2 models could not produce a clustering in the Czech CoNLL corpus due
its size. Best published results are from ?Christodoulopoulos et al (2010), ?Berg-Kirkpatrick et al (2010) and ?Lee
et al (2010). The latter two papers do not report VM scores. No best published results are shown for the MULTEXT
languages; Christodoulopoulos et al (2010) report results based on 45 tags suggesting that clark performs best on
these corpora.
645
comparison with a token-based version of the model
that this restriction is very helpful. Second, it is
a clustering model rather than a sequence model.
This property makes it easy to incorporate multi-
ple kinds of features into the model at either the to-
ken or the type level. Here, we experimented with
token-level context features and alignment features
and type-level morphology features, showing that
morphology features are helpful in nearly all cases,
and alignment features can be helpful if the aligned
language is properly chosen. Our results even with-
out these extra features are competitive with state-
of-the-art; with the additional features we achieve
the best published results in the majority of the 25
corpora tested.
Since it is so easy to add extra features to our
model, one direction for future work is to explore
other possible features. For example, it could be
useful to add dependency features from an unsuper-
vised dependency parser. We are also interested in
improving our morphology features, either by con-
sidering other ways to extract features during pre-
processing (for example, including prefixes or not
concatenating together all suffixes), or by develop-
ing a joint model for inducing both morphology and
syntactic classes simultaneously. Finally, our model
could be extended by replacing the standard mixture
model with an infinite mixture model (Rasmussen,
2000) in order to induce the number of syntactic
classes automatically.
Acknowledgments
The authors would like to thank Emily Thomforde,
Ioannis Konstas, Tom Kwiatkowski and the anony-
mous reviewers for their comments and suggestions.
We would also like to thank Kiril Simov, Toni Marti,
Tomaz Erjavec, Jess Lin and Kathrin Beck for pro-
viding us with CoNLL data. This work was sup-
ported by an EPSRC graduate Fellowship, and by
ERC Advanced Fellowship 249520 GRAMPLUS.
References
Anne Abeille?, Lionel Cle?ment, and Alexandra Kinyon.
2000. Building a treebank for French. In In Proceed-
ings of the LREC 2000.
David Bamman, Francesco Mambrini, and Gregory
Crane. 2009. An ownership model of annotation: The
Ancient Greek dependency treebank. In TLT 2009-
Eighth International Workshop on Treebanks and Lin-
guistic Theories.
Taylor Berg-Kirkpatrick, Alexandre B. Co?te?, John DeN-
ero, and Dan Klein. 2010. Painless unsupervised
learning with features. In Proceedings of NAACL
2010, pages 582?590, Los Angeles, California, June.
Chris Biemann. 2006. Unsupervised part-of-speech tag-
ging employing efficient graph clustering. In Proceed-
ings of COLING ACL 2006, pages 7?12, Morristown,
NJ, USA.
Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and Barbora
Hladka?. 2001. The Prague dependency treebank:
Three-level annotation scenario. In Anne Abeille?, ed-
itor, Treebanks: Building and Using Syntactically An-
notated Corpora, pages 103 ? 126. Kluwer Academic
Publishers.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Peter F. Brown, Vincent J. Della Pietra, Peter V. Desouza,
Jennifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Compu-
tational Natural Language Learning, CoNLL-X ?06,
pages 149?164, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
POS induction: How far have we come? In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 575?584, Cam-
bridge, MA, October. Association for Computational
Linguistics.
Montserrat Civit, Ma. Mart??, and Nu?ria Buf??. 2006.
Cat3lb and cast3lb: From constituents to dependen-
cies. In Tapio Salakoski, Filip Ginter, Sampo Pyysalo,
and Tapio Pahikkala, editors, Advances in Natural
Language Processing, volume 4139 of Lecture Notes
in Computer Science, pages 141?152. Springer Berlin
/ Heidelberg.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of EACL 2003, pages 59?66,
Morristown, NJ, USA.
Mathias Creutz and Krista Lagus. 2005. Induc-
ing the morphological lexicon of a natural language
from unannotated text. In In Proceedings of the
International and Interdisciplinary Conference on
646
Adaptive Knowledge Representation and Reasoning
(AKRR?05), volume 5, pages 106?113.
Sas?o Dz?eroski, Tomaz? Erjavec, Nina Ledinek, Petr Pajas,
Zdenek Z?abokrtsky, and Andreja Z?ele. 2006. Towards
a Slovene dependency treebank. In Proceedings Int.
Conf. on Language Resources and Evaluation (LREC).
Tomaz? Erjavec. 2004. MULTEXT-East version 3: Mul-
tilingual morphosyntactic specifications, lexicons and
corpora. In Fourth International Conference on Lan-
guage Resources and Evaluation, (LREC?04), pages
1535 ? 1538, Paris. ELRA.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of ACL 2007, pages 744?751,
Prague, Czech Republic, June.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
NAACL 2006, pages 320?327, Morristown, NJ, USA.
Jan Hajic?, Jarmila Panevova?, Zden?ka Ures?ova?, Alevtina
Be?mova?, and Petr Pajas. 2003. PDTVALLEX: cre-
ating a large-coverage valency lexicon for treebank
annotation. In Proceedings of The Second Workshop
on Treebanks and Linguistic Theories, pages 57?68.
Vaxjo University Press.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proceedings of EMNLP-CoNLL
2007, pages 296?305, Prague, Czech Republic, June.
Yasushira Kawata and Julia Bartels. 2000. Stylebook
for the Japanese treebank in VERMOBIL. Technical
report, Universita?t Tu?ubingen.
Chen Keh-Jiann, Chu-Ren Huang, Feng-Yi Chen, Chi-
Ching Luo, Ming-Chung Chang, Chao-Jan Chen, and
Zhao-Ming Gao. 2003. Sinica treebank: Design cri-
teria, representational issues and implementation. In
Anne Abeille?, editor, Treebanks: Building and Us-
ing Syntactically Annotated Corpora, pages 231?248.
Kluwer Academic Publishers.
Dan Klein and Christopher D. Manning. 2002. A gener-
ative constituent-context model for improved grammar
induction. In Proceedings of ACL 40, pages 128?135.
Michael Lamar, Yariv Maron, Mark Johnson, and Elie
Bienenstock. 2010. SVD and clustering for unsuper-
vised POS tagging. In Proceedings of the ACL 2010
Conference Short Papers, pages 215?219, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised POS tagging.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 853?861, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):331?330.
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and
Regina Barzilay. 2009. Multilingual Part-of-Speech
tagging: Two unsupervised approaches. Journal of Ar-
tificial Intelligence Research, 36:341?385.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Comput. Linguist., 29(1):19?51.
Kemal Oflazer, Bilge Say, Dilek Z. Hakkani-Tu?r, and
Go?khan Tu?r, 2003. Building A Turkish Treebank,
chapter 1, pages 1?17. Kluwer Academic Publishers.
Carl Rasmussen. 2000. The infinite Gaussian mixture
model. In Advances in Neural Information Processing
Systems 12.
Sujith Ravi and Kevin Knight. 2009. Minimized mod-
els for unsupervised Part-of-Speech tagging. In Pro-
ceedings of ACL-IJCNLP 2009, pages 504?512, Sun-
tec, Singapore, August.
Martin Redington, Nick Chater, and Steven Finch. 1998.
Distributional information: a powerful cue for acquir-
ing syntactic categories. Cognitive Science, 22:425 ?
469.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of EMNLP-
CoNLL 2007, pages 410?420.
Hinrich Schu?tze. 1995. Distributional part-of-speech
tagging. In Proceedings of EACL 7, pages 141?148,
San Francisco, CA, USA.
Kiril Simov, Petya Osenova, Alexander Simov, andMilen
Kouylekov. 2004. Design and implementation of the
Bulgarian HPSG-based treebank. Research on Lan-
guage &amp; Computation, 2(4):495?522.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: training log-linear models on unlabeled data.
In Proceedings of ACL 2005, pages 354?362, Morris-
town, NJ, USA.
Otakar Smrz? and Petr Pajas. 2004. Morphotrees of Ara-
bic and their annotation in the TrEd environment. In
Proceedings of the NEMLAR International Conference
on Arabic Language Resources and Tools, pages 38?
41.
Ben Snyder and Regina Barzilay. 2008. Unsupervised
multilingual learning for morphological segmentation.
In Proceedings of ACL.
Kristina Toutanova and Mark Johnson. 2007. A
Bayesian LDA-based model for semi-supervised part-
of-speech tagging. In Proceedings of NIPS 2007.
647
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1512?1523,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Lexical Generalization in CCG Grammar Induction for Semantic Parsing
Tom Kwiatkowski?
t.m.kwiatkowksi@sms.ed.ac.uk
Luke Zettlemoyer?
lsz@cs.washington.edu
Sharon Goldwater?
sgwater@inf.ed.ac.uk
Mark Steedman?
steedman@inf.ed.ac.uk
?School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
?Computer Science & Engineering
University of Washington
Seattle, WA 98195
Abstract
We consider the problem of learning fac-
tored probabilistic CCG grammars for seman-
tic parsing from data containing sentences
paired with logical-form meaning representa-
tions. Traditional CCG lexicons list lexical
items that pair words and phrases with syntac-
tic and semantic content. Such lexicons can
be inefficient when words appear repeatedly
with closely related lexical content. In this
paper, we introduce factored lexicons, which
include both lexemes to model word meaning
and templates to model systematic variation in
word usage. We also present an algorithm for
learning factored CCG lexicons, along with a
probabilistic parse-selection model. Evalua-
tions on benchmark datasets demonstrate that
the approach learns highly accurate parsers,
whose generalization performance benefits
greatly from the lexical factoring.
1 Introduction
Semantic parsers automatically recover representa-
tions of meaning from natural language sentences.
Recent work has focused on learning such parsers
directly from corpora made up of sentences paired
with logical meaning representations (Kate et al,
2005; Kate and Mooney, 2006; Wong and Mooney,
2006, 2007; Zettlemoyer and Collins, 2005, 2007;
Lu et al, 2008; Kwiatkowski et al, 2010).
For example, in a flight booking domain we
might have access to training examples such as:
Sentence: I want flights from Boston
Meaning: ?x. f light(x)? f rom(x,bos)
and the goal is to learn a grammar that can map new,
unseen, sentences onto their corresponding mean-
ings, or logical forms.
One approach to this problem has developed al-
gorithms for leaning probabilistic CCG grammars
(Zettlemoyer and Collins, 2005, 2007; Kwiatkowski
et al, 2010). These grammars are well-suited to the
task of semantic parsing, as they closely link syn-
tax and semantics. They can be used to model a
wide range of complex linguistic phenomena and are
strongly lexicalized, storing all language-specific
grammatical information directly with the words in
the lexicon. For example, a typical learned lexicon
might include entries such as:
(1) f light `N :?x. f light(x)
(2) f light `N/(S|NP) :? f?x. f light(x)? f (x)
(3) f light `N\N :? f?x. f light(x)? f (x)
(4) f are`N :?x.cost(x)
(5) f are`N/(S|NP) :? f?x.cost(x)? f (x)
(6) f are`N\N :? f?x.cost(x)? f (x)
(7) Boston`NP :bos
(8) Boston`N\N :? f?x. f rom(x,bos)? f (x)
(9) New York `NP :nyc
(10) New York `N\N :? f?x. f rom(x,nyc)? f (x)
Although lexicalization of this kind is useful
for learning, as we will see, these grammars can
also suffer from sparsity in the training data, since
closely related entries must be repeatedly learned for
all members of a certain class of words. For exam-
ple, the list above shows a selection of lexical items
that would have to be learned separately.
In this list, the word ?flight? is paired with the
predicate flight in three separate lexical items which
are required for different syntactic contexts. Item
1512
(1) has the standard N category for entries of this
type, item (2) allows the use of the word ?flight?
with that-less relative clauses such as ?flight depart-
ing Boston?, and item (3) is useful for phrases with
unconventional word order such as ?from Boston
flight to New York?. Representing these three lexi-
cal items separately is inefficient, since each word of
this class (such as ?fare?) will require three similarly
structured lexical entries differing only in predicate
name. There may also be systemtatic semantic vari-
ation between entries for a certain class of words.
For example, in (6) ?Boston? is paired with the con-
stant bos that represents its meaning. However, item
(7) also adds the predicate from to the logical form.
This might be used to analyse somewhat elliptical,
unedited sentences such as ?Show me flights Boston
to New York,? which can be challenging for seman-
tic parsers (Zettlemoyer and Collins, 2007).
This paper builds upon the insight that a large pro-
portion of the variation between lexical items for
a given class of words is systematic. Therefore it
should be represented once and applied to a small set
of basic lexical units. 1 We develop a factored lex-
icon that captures this insight by distinguishing lex-
emes, which pair words with logical constants, from
lexical templates, which map lexemes to full lexical
items. As we will see, this can lead to a significantly
more compact lexicon that can be learned from less
data. Each word or phrase will be associated with a
few lexemes that can be combined with a shared set
of general templates.
We develop an approach to learning factored,
probabilistic CCG grammars for semantic pars-
ing. Following previous work (Kwiatkowski et al,
2010), we make use of a higher-order unification
learning scheme that defines a space of CCG gram-
mars consistent with the (sentence, logical form)
training pairs. However, instead of constructing
fully specified lexical items for the learned grammar,
we automatically generate sets of lexemes and lexi-
cal templates to model each example. This is a dif-
ficult learning problem, since the CCG analyses that
1A related tactic is commonly used in wide-coverage CCG
parsers derived from treebanks, such as work by Hockenmaier
and Steedman (2002) and Clark and Curran (2007). These
parsers make extensive use of category-changing unary rules,
to avoid data sparsity for systematically related categories (such
as those related by type-raising). We will automatically learn to
represent these types of generalizations in the factored lexicon.
are required to construct the final meaning represen-
tations are not explicitly labeled in the training data.
Instead, we model them with hidden variables and
develop an online learning approach that simultane-
ously estimates the parameters of a log-linear pars-
ing model, while inducing the factored lexicon.
We evaluate the approach on the benchmark Atis
and GeoQuery domains. This is a challenging setup,
since the GeoQuery data has complex meaning rep-
resentations and sentences in multiple languages,
while the Atis data contains spontaneous, unedited
text that can be difficult to analyze with a formal
grammar representation. Our approach achieves at
or near state-of-the-art recall across all conditions,
despite having no English or domain-specific infor-
mation built in. We believe that ours is the only sys-
tem of sufficient generality to run with this degree of
success on all of these datasets.
2 Related work
There has been significant previous work on learn-
ing semantic parsers from training sentences la-
belled with logical form meaning representations.
We extend a line of research that has addressed
this problem by developing CCG grammar induc-
tion techniques. Zettlemoyer and Collins (2005,
2007) presented approaches that use hand gener-
ated, English-language specific rules to generate lex-
ical items from logical forms as well as English
specific type-shifting rules and relaxations of the
CCG combinators to model spontaneous, unedited
sentences. Zettlemoyer and Collins (2009) extends
this work to the case of learning in context depen-
dent environments. Kwiatkowski et al (2010) de-
scribed an approach for language-independent learn-
ing that replaces the hand-specified templates with
a higher-order-unification-based lexical induction
method, but their approach does not scale well to
challenging, unedited sentences. The learning ap-
proach we develop for inducing factored lexicons is
also language independent, but scales well to these
challenging sentences.
There have been a number of other approaches
for learning semantic parsers, including ones based
on machine translation techniques (Papineni et al,
1997; Ramaswamy and Kleindienst, 2000; Wong
and Mooney, 2006), parsing models (Miller et al,
1996; Ge and Mooney, 2006; Lu et al, 2008), in-
1513
ductive logic programming algorithms (Zelle and
Mooney, 1996; Thompson and Mooney, 2002; Tang
and Mooney, 2000), probabilistic automata (He and
Young, 2005, 2006), and ideas from string kernels
and support vector machines (Kate and Mooney,
2006; Nguyen et al, 2006).
More recent work has focused on training se-
mantic parsers without supervision in the form of
logical-form annotations. Clarke et al (2010) and
Liang et al (2011) replace semantic annotations in
the training set with target answers which are more
easily available. Goldwasser et al (2011) present
work on unsupervised learning of logical form struc-
ture. However, all of these systems require signifi-
cantly more domain and language specific initializa-
tion than the approach presented here.
Other work has learnt semantic analyses from text
in the context of interactions in computational envi-
ronments (Branavan et al (2010), Vogel and Juraf-
sky (2010)); text grounded in partial observations of
a world state (Liang et al, 2009); and from raw text
alone (Poon and Domingos, 2009, 2010).
There is also related work that uses the CCG
grammar formalism. Clark and Curran (2003)
present a method for learning the parameters of a
log-linear CCG parsing model from fully annotated
normal?form parse trees. Watkinson and Manand-
har (1999) describe an unsupervised approach for
learning syntactic CCG lexicons. Bos et al (2004)
present an algorithm for building semantic represen-
tations from CCG parses but requires fully?specified
CCG derivations in the training data.
3 Overview of the Approach
Here we give a formal definition of the problem and
an overview of the learning approach.
Problem We will learn a semantic parser that
takes a sentences x and returns a logical form z repre-
senting its underlying meaning. We assume we have
input data {(xi,zi)|i = 1 . . .n} containing sentences
xi and logical forms zi, for example xi =?Show me
flights to Boston? and zi = ?x. f light(x)? to(x,bos).
Model We will represent the parser as a factored,
probabilistic CCG (PCCG) grammar. A traditional
CCG lexical item would fully specify the syntax and
semantics for a word (reviewed in Section 4). For
example, Boston`NP : bos represents the entry for
the word ?Boston? with syntactic category NP and
meaning represented by the constant bos. Where a
lexicon would usually list lexical items such as this,
we instead use a factored lexicon (L,T ) containing:
? A list of lexemes L. Each lexeme pairs a word
or phrase with a list of logical constants that can
be used to construct its meaning. For example,
one lexeme might be (Boston, [bos]).
? A list of lexical templates T . Each template
takes a lexeme and maps it on to a full lexical
item. For example, there is a single template
that can map the lexeme above to the final lex-
ical entry Boston `NP : bos.
We will make central use of this factored repre-
sentation to provide a more compact representation
of the lexicon that can be learned efficiently.
The factored PCCG will also contain a parameter
vector, ? , that defines a log-linear distribution over
the possible parses y, conditioned on the sentence x.
Learning Our approach for learning factored PC-
CGs extends the work of Kwiatkowski et al (2010),
as reviewed in Section 7. Specifically, we modify
the lexical learning, to produce lexemes and tem-
plates, as well as the feature space of the model, but
reuse the existing parameter estimation techniques
and overall learning cycle, as described in Section 7.
We present the complete approach in three parts
by describing the factored representation of the lex-
icon (Section 5), techniques for proposing potential
new lexemes and templates (Section 6), and finally
a complete learning algorithm (Section 7). How-
ever, the next section first reviews the required back-
ground on semantic parsing with CCG.
4 Background
4.1 Lambda Calculus
We represent the meanings of sentences, words
and phrases with logical expressions that can con-
tain constants, quantifiers, logical connectors and
lambda abstractions. We construct the meanings of
sentences from the meanings of words and phrases
using lambda-calculus operations. We use a version
of the typed lambda calculus (Carpenter, 1997), in
which the basic types include e, for entities; t, for
truth values; and i for numbers. We also have func-
tion types that are assigned to lambda expressions.
1514
The expression ?x. f light(x) takes an entity and re-
turns a truth value, and has the function type ?e, t?.
4.2 Combinatory Categorial Grammar
CCG (Steedman, 1996, 2000) is a linguistic formal-
ism that tightly couples syntax and semantics, and
can be used to model a wide range of language phe-
nomena. A traditional CCG grammar includes a lex-
icon ? with entries like the following:
f lights`N :?x. f light(x)
to` (N\N)/NP :?y.? f .?x. f (x)? to(x,y)
Boston`NP :bos
where each lexical item w`X : h has words w, a syn-
tactic category X , and a logical form h. For the first
example, these are ?flights,? N, and ?x. f light(x).
In this paper, we introduce a new way of represent-
ing lexical items as (lexeme, template) pairs, as de-
scribed in section 5.
CCG syntactic categories may be atomic (such
as S or NP) or complex (such as (N\N)/NP)
where the slash combinators encode word order
information. CCG uses a small set of combinatory
rules to build syntactic parses and semantic repre-
sentations concurrently. Two example combinatory
rules are forward (>) and backward (<) application:
X/Y : f Y : g ? X : f (g) (>)
Y : g X\Y : f ? X : f (g) (<)
These rules apply to build syntactic and semantic
derivations under the control of the word order infor-
mation encoded in the slash directions of the lexical
entries. For example, given the lexicon above, the
phrase ?flights to Boston? can be parsed to produce:
flights to Boston
N (N\N)/NP NP?x. f light(x) ?y? f?x. f (x)? to(x,y) bos
>
(N\N)? f?x. f (x)? to(x,bos)
<N?x. f light(x)? to(x,bos)
where each step in the parse is labeled with the com-
binatory rule (?> or ?<) that was used.
CCG also includes combinatory rules of forward
(> B) and backward (< B) composition:
X/Y : f Y/Z : g? X/Z : ?x. f (g(x)) (> B)
Y\Z : g X\Y : f ? X\Z : ?x. f (g(x)) (< B)
These rules allow a relaxed notion of constituency
which helps limit the number of distinct CCG lexical
items required.
To the standard forward and backward slashes of
CCG we also add a vertical slash for which the di-
rection of application is underspecified. We shall see
examples of this in Section 10.
4.3 Probabilistic CCGs
Due to ambiguity in both the CCG lexicon and the
order in which combinators are applied, there will
be many parses for each sentence. We discriminate
between competing parses using a log-linear model
which has a feature vector ? and a parameter vector
? . The probability of a parse y that returns logical
form z, given a sentence x is defined as:
P(y,z|x;? ,?) = e
? ??(x,y,z)
?(y?,z?) e? ??(x,y?,z?) (1)
Section 8 fully defines the set of features used in the
system presented. The most important of these con-
trol the generation of lexical items from (lexeme,
template) pairs. Each (lexeme, template) pair used
in a parse fires three features as we will see in more
detail later.
The parsing, or inference, problem done at test
time requires us to find the most likely logical form
z given a sentence x, assuming the parameters ? and
lexicon ? are known:
f (x) = argmaxz p(z|x;? ,?) (2)
where the probability of the logical form is found by
summing over all parses that produce it:
p(z|x;? ,?) =?
y
p(y,z|x;? ,?) (3)
In this approach the distribution over parse trees y
is modeled as a hidden variable. The sum over
parses in Eq. 3 can be calculated efficiently using
the inside-outside algorithm with a CKY-style pars-
ing algorithm.
To estimate the parameters themselves, we
use stochastic gradient updates (LeCun et al,
1998). Given a set of n sentence-meaning pairs
{(xi,zi) : i = 1...n}, we update the parameters ? it-
eratively, for each example i, by following the local
gradient of the conditional log-likelihood objective
1515
Oi = logP(zi|xi;? ,?). The local gradient of the in-
dividual parameter ? j associated with feature ? j and
training instance (xi,zi) is given by:
?Oi
?? j = Ep(y|xi,zi;? ,?)[? j(xi,y,zi)]
?Ep(y,z|xi;? ,?)[? j(xi,y,z)]
(4)
As with Eq. 3, all of the expectations in Eq. 4 are
calculated through the use of the inside-outside al-
gorithm on a pruned parse chart. For a sentence
of length m, each parse chart span is pruned using
a beam width proportional to m 23 , to allow larger
beams for shorter sentences.
5 Factored Lexicons
A factored lexicon includes a set L of lexemes and
a set T of lexical templates. In this section, we for-
mally define these sets, and describe how they are
used to build CCG parses. We will use a set of lex-
ical items from our running example to discuss the
details of how the following lexical items:
(1) f light `N :?x. f light(x)
(2) f light `N/(S|NP) :? f?x. f light(x)? f (x)
. . .
(6) Boston`NP :bos
(7) Boston`N\N :? f?x. f rom(x,bos)? f (x)
are constructed from specific lexemes and templates.
5.1 Lexemes
A lexeme (w,~c) pairs a word sequence w with an
ordered list of logical constants ~c = [c1 . . .cm]. For
example, item (1) and (2) above would come from
a single lexeme (flight, [ f light]). Similar lexemes
would be represented for other predicates, for exam-
ple (fare, [cost]). Lexemes also can contain multiple
constants, for example (cheapest, [argmin,cost]),
which we will see more examples of later.
5.2 Lexical Templates
A lexical template takes a lexeme and produces a
lexical item. Templates have the general form
? (?,~v).[? `X : h~v]
where h~v is a logical expression that contains vari-
ables from the list ~v. Applying this template to the
input lexeme (w,~c) gives the full lexical item w `
X :h where the variable ? has been replaced with the
wordspan w and the logical form h has been created
by replacing each of the variables in~v with the coun-
terpart constant from ~c. For example, the lexical
item (6) above would be constructed from the lex-
eme (Boston, [bos]) using the template ? (?,~v).[? `
NP :v1]. Items (1) and (2) would both be constructed
from the single lexeme (flight, [ f light]) with the two
different templates ? (?,~v).[? ` N : ?x.v1(x)] and
? (?,~v).[? `N/(S|NP) :? f?x.v1(x)? f (x)]
5.3 Parsing with a Factored Lexicon
In general, there can by many different (lexeme,
template) pairs that produce the same lexical item.
For example, lexical item (7) in our running ex-
ample above can be constructed from the lexemes
(Boston, [bos]) and (Boston, [ f rom,bos]), given ap-
propriate templates.
To model this ambiguity, we include the selection
of a (lexeme, template) pair as a decision to be made
while constructing a CCG parse tree. Given the lex-
ical item produced by the chosen lexeme and tem-
plate, parsing continues with the traditional combi-
nators, as reviewed in Section 4.2. This direct inte-
gration allows for features that signal which lexemes
and templates have been used while also allowing
for well defined marginal probabilities, by summing
over all ways of deriving a specific lexical item.
6 Learning Factored Lexicons
To induce factored lexicons, we will make use of two
procedures, presented in this section, that factor lexi-
cal items into lexemes and templates. Section 7 will
describe how this factoring operation is integrated
into the complete learning algorithm.
6.1 Maximal Factorings
Given a lexical item l of the form w `X : h with
words w, a syntactic category X , and a logical form
h, we define the maximal factoring to be the unique
(lexeme, template) pair that can be used to recon-
struct l and includes all of the constants of h in
the lexeme (listed in a fixed order based on an
ordered traversal of h). For example, the maxi-
mal factoring for the lexical item Boston ` NP :
bos is the pair we saw before: (Boston, [bos]) and
? (?,~v).[? ` NP : v1]. Similarly, the lexical item
Boston ` N\N : ? f .?x. f (x)? f rom(x,bos) would
be factored to produce (Boston, [ f rom,bos]) and
? (?,~v).[? ` N\N :? f .?x. f (x)? v1(x,v2)].
As we will see in Section 7, this notion of factor-
1516
ing can be directly incorporated into existing algo-
rithms that learn CCG lexicons. When the original
algorithm would have added an entry l to the lexi-
con, we can instead compute the factoring of l and
add the corresponding lexeme and template to the
factored lexicon.
6.2 Introducing Templates with Content
Maximal factorings, as just described, provide for
significant lexical generalization but do not handle
all of the cases needed to learn effectively. For
instance, the maximal split for the item Boston `
N\N : ? f .?x. f (x) ? f rom(x,bos) would introduce
the lexeme (Boston, [ f rom,bos]), which is subopti-
mal since each possible city would need a lexeme
of this type, with the additional from constant in-
cluded. Instead, we would ideally like to learn the
lexeme (Boston, [bos]) and have a template that in-
troduces the from constant. This would model the
desired generalization with a single lexeme per city.
In order to permit the introduction of extra con-
stants into lexical items, we allow the creation of
templates that contain logical constants through par-
tial factorings. For instance, the template below can
introduce the predicate from
? (?,~v).[? `N\N :? f .?x. f (x)? f rom(x,v1)]
The use of templates to introduce extra semantic
constants into a lexical item is similar to, but more
general than, the English-specific type-shifting rules
used in Zettlemoyer and Collins (2007), which were
introduced to model spontaneous, unedited text.
They are useful, as we will see, in learning to re-
cover semantic content that is implied, but not ex-
plicitly stated, such as our original motivating phrase
?flights Boston to New York.?
To propose templates which introduce semantic
content, during learning, we build on the intuition
that we need to recover from missing words, such
as in the example above. In this scenario, there
should also be other sentences that actually include
the word, in our example this would be something
like ?flights from Boston.? We will also assume
that we have learned a good factored lexicon for the
complete example that could produce the parse:
flights from Boston
N (N\N)/NP NP?x. f light(x) ?y? f?x. f (x)? f rom(x,y) bos
>
(N\N)? f?x. f (x)? f rom(x,bos)
<N?x. f light(x)? f rom(x,bos)
Given analyses of this form, we introduce new
templates that will allow us to recover from miss-
ing words, for example if ?from? was dropped. We
identify commonly occurring nodes in the best parse
trees found during training, in this case the non-
terminal spanning ?from Boston,? and introduce
templates that can produce the nonterminal, even if
one of the words is missing. Here, this approach
would introduce the desired template ? (?,~v).[? `
N\N : ? f .?x. f (x) ? f rom(x,v1)] for mapping the
lexeme (Boston, [bos]) directly to the intermediate
structure.
Not all templates introduced this way will model
valid generalizations. However, we will incorporate
them into a learning algorithm with indicator fea-
tures that can be weighted to control their use. The
next section presents the complete approach.
7 Learning Factored PCCGs
Our Factored Unification Based Learning (FUBL)
method extends the UBL algorithm (Kwiatkowski
et al, 2010) to induce factored lexicons, while also
simultanously estimating the parameters of a log-
linear CCG parsing model. In this section, we first
review the NEW-LEX lexical induction procedure
from UBL, and then present the FUBL algorithm.
7.1 Background: NEW-LEX
NEW-LEX generates lexical items by splitting and
merging nodes in the best parse tree of each training
example. Each parse node has a CCG category X : h
and a sequence of words w that it spans. We will
present an overview of the approach using the run-
ning example with the phrase w =?in Boston? and
the category X : h = S\NP :?x.loc(x,bos), which is
of the type commonly seen during learning. The
splitting procedure is a two step process that first
splits the logical form h, then splits the CCG syn-
tactic category X and finally splits the string w.
The first step enumerates all possible splits of
the logical form h into a pair of new expressions
1517
( f ,g) that can be used to reconstruct h by ei-
ther function application (h = f (g)) or composition
(h = ?x. f (g(x))). For example, one possible split is:
( f = ?y.?x.loc(x,y) , g = bos)
which corresponds to the function application case.
The next two steps enumerate all ways of splitting
the syntactic category X and words w to introduce
two new lexical items which can be recombined with
CCG combinators (application or composition) to
recreate the original parse node X : h spanning w. In
our example, one possibility would be:
(in` (S\NP)/NP :?y.?x.loc(x,y) , Boston`NP :bos)
which could be recombined with the forward appli-
cation combinator from Section 4.2.
To assign categories while splitting, the grammar
used by NEW-LEX only uses two atomic syntac-
tic categories S and NP. This allows NEW-LEX to
make use of a direct mapping from semantic type
to syntactic category when proposing syntactic cate-
gories. In this schema, the standard syntactic cat-
egory N is replaced by the category S|NP which
matches the type ?e, t? and uses the vertical slash in-
troduced in Section 4.2. We will see categories such
as this in the evaluation.
7.2 The FUBL Algorithm
Figure 1 shows the FUBL learning algorithm. We
assume training data {(xi,zi) : i= 1 . . .n}where each
example is a sentence xi paired with a logical form
zi. The algorithm induces a factored PCCG, includ-
ing the lexemes L, templates T , and parameters ? .
The algorithm is online, repeatedly performing
both lexical expansion (Step 1) and a parameter up-
date (Step 2) for each training example. The over-
all approach is closely related to the UBL algo-
rithm (Kwiatkowski et al, 2010), but includes exten-
sions for updating the factored lexicon, as motivated
in Section 6.
Initialization The model is initialized with a fac-
tored lexicon as follows. MAX-FAC is a function
that takes a lexical item l and returns the maximal
factoring of it, that is the unique, maximal (lexeme,
template) pair that can be combined to construct l,
as described in Section 6.1. We apply MAX-FAC to
each of the training examples (xi,zi), creating a sin-
gle way of producing the desired meaning zi from a
Inputs: Training set {(xi,zi) : i = 1 . . .n} where each
example is a sentence xi paired with a logical form
zi. Set of entity name lexemes Le. Number of itera-
tions J. Learning rate parameter ?0 and cooling rate
parameter c. Empty lexeme set L. Empty template
set T .
Definitions: NEW-LEX(y) returns a set of new lex-
ical items from a parse y as described in Sec-
tion 7.1. MAX-FAC(l) generates a (lexeme, tem-
plate) pair from a lexical item l. PART-FAC(y)
generates a set of templates from parse y. Both of
these are described in Section 7.2. The distributions
p(y|x,z;? ,(L,T )) and p(y,z|x;? ,(L,T )) are defined
by the log-linear model described in Section 4.3.
Initialization:
? For i = 1 . . .n
? (?,pi) = MAX-FAC(xi ` S : zi)
? L = L?? , T = T ?pi
? Set L = L?Le.
? Initialize ? using coocurrence statistics, as de-
scribed in Section 8.
Algorithm:
For t = 1 . . .J, i = 1 . . .n :
Step 1: (Add Lexemes and Templates)
? Let y? = argmaxy p(y|xi,zi;? ,(L,T ))
? For l ? NEW-LEX(y?)
? (?,pi) = MAX-FAC(l)
? L = L?? , T = T ?pi
? ?= PART-FAC(y?) , T = T ??
Step 2: (Update Parameters)
? Let ? = ?01+c?k where k = i+ t?n.
? Let ?= Ep(y|xi,zi;? ,(L,T ))[?(xi,y,zi)]
?Ep(y,z|xi;? ,(L,T ))[?(xi,y,z)]
? Set ? = ? + ??
Output: Lexemes L, templates T , and parameters ? .
Figure 1: The FUBL learning algorithm.
lexeme containing all of the words in xi. The lex-
emes and templates created in this way provide the
initial factored lexicon.
Step 1 The first step of the learning algorithm in
Figure 1 adds lexemes and templates to the fac-
tored model given by performing manipulations on
the highest scoring correct parse y? of the current
training example (xi,zi). First the NEW-LEX pro-
cedure is run on y? as described in Section 6.1 to
1518
generate new lexical items. We then use the func-
tion MAX-FAC to create the maximal factorings of
each of these new lexical items as described in Sec-
tion 6 and these are added to the factored represen-
tation of the lexicon. New templates can also be in-
troduced through partial factorings of internal parse
nodes as described in Section 6.2. These templates
are generated by using the function PART-FAC to
abstract over the wordspan and a subset of the con-
stants contained in the internal parse nodes of y?.
This step allows for templates that introduce new
semantic content to model elliptical language, as de-
scribed in Section 6.2.
Step 2 The second step does a stochastic gradient
descent update on the parameters ? used in the pars-
ing model. This update is described in Section 4.3
Discussion The FUBL algorithm makes use of a
direct online approach, where lexemes and tem-
plates are introduced in place while analyzing spe-
cific sentences. In general, this will overgeneralize;
not all ways of combining lexemes and templates
will produce high quality lexical items. However,
the overall approach includes features, presented in
Section 8, that can be used to learn which ones are
best in practice. The complete algorithm iterates be-
tween adding new lexical content and updating the
parameters of the parsing model with each proce-
dure guiding the other.
8 Experimental setup
Data Sets We evaluate on two benchmark seman-
tic parsing datasets: GeoQuery, which is made up of
natural language queries to a database of geograph-
ical information; and Atis, which contains natural
language queries to a flight booking system. The
Geo880 dataset has 880 (English-sentence, logical-
form) pairs split into a training set of 600 pairs and
a test set of 280. The Geo250 data is a subset of
the Geo880 sentences that have been translated into
Japanese, Spanish and Turkish as well as the original
English. We follow the standard evaluation proce-
dure for Geo250, using 10-fold cross validation ex-
periments with the same splits of the data as Wong
and Mooney (2007). The Atis dataset contains 5410
(sentence, logical-form) pairs split into a 4480 ex-
ample training set, a 480 example development set
and a 450 example test set.
Evaluation Metrics We report exact match Re-
call (percentage of sentences for which the correct
logical-form was returned), Precision (percentage of
returned logical-forms that are correct) and F1 (har-
monic mean of Precision and Recall). For Atis we
also report partial match Recall (percentage of cor-
rect literals returned), Precision (percentage of re-
turned literals that are correct) and F1, computed as
described by Zettlemoyer and Collins (2007).
Features We introduce two types of features to
discriminate between parses: lexical features and
logical-form features.
Lexical features fire on the lexemes and templates
used to build the lexical items used in a parse. For
each (lexeme,template) pair used to create a lexi-
cal item we have indicator features ?l for the lex-
eme used, ?t for the template used, and ?(l,t) for the
pair that was used. We assign the features on lexi-
cal templates a weight of 0.1 to prevent them from
swamping the far less frequent but equally informa-
tive lexeme features.
Logical-form features are computed on the
lambda-calculus expression z returned at the root of
the parse. Each time a predicate p in z takes an
argument a with type Ty(a) in position i, it trig-
gers two binary indicator features: ?(p,a,i) for the
predicate-argument relation; and ?(p,Ty(a),i) for the
predicate argument-type relation. Boolean opera-
tor features look at predicates that occurr together
in conjunctions and disjunctions. For each variable
vi that fills argument slot i in two conjoined pred-
icates p1 and p2 we introduce a binary indicator
feature ?con j(i,p1,p2). We introduce similar features?dis j(i,p1,p2) for variables vi that are shared by predi-cates in a disjunction.
Initialization The weights for lexeme features are
initialized according to coocurrance statistics be-
tween words and logical constants. These are esti-
mated with the Giza++ (Och and Ney, 2003) imple-
mentation of IBM Model 1. The initial weights for
templates are set by adding ?0.1 for each slash in
the syntactic category and ?2 if the template con-
tains logical constants. Features on lexeme-template
pairs and all parse features are initialized to zero.
Systems We compare performance to all recently-
published, directly-comparable results. For Geo-
Query, this includes the ZC05, ZC07 (Zettlemoyer
1519
System Exact MatchRec. Pre. F1
ZC07 74.4 87.3 80.4
UBL 65.6 67.1 66.3
FUBL 81.9 82.1 82.0
Table 1: Performance on the Atis development set.
System Exact Match Partial MatchRec. Pre. F1. Rec. Pre. F1
ZC07 84.6 85.8 85.2 96.7 95.1 95.9
HY06 - - - - - 90.3
UBL 71.4 72.1 71.7 78.2 98.2 87.1
FUBL 82.8 82.8 82.8 95.2 93.6 94.6
Table 2: Performance on the Atis test set.
and Collins, 2005, 2007), ? -WASP (Wong and
Mooney, 2007), UBL (Kwiatkowski et al, 2010)
systems and DCS (Liang et al, 2011). For Atis,
we report results from HY06 (He and Young, 2006),
ZC07, and UBL.
9 Results
Tables 1-4 present the results on the Atis and Geo-
query domains. In all cases, FUBL achieves at or
near state-of-the-art recall (overall number of correct
parses) when compared to directly comparable sys-
tems and it significantly outperforms UBL on Atis.
On Geo880 the only higher recall is achieved
by DCS with prototypes - which uses signifi-
cant English-specific resources, including manually
specified lexical content, but does not require train-
ing sentences annotated with logical-forms. On
Geo250, FUBL achieves the highest recall across
languages. Each individual result should be inter-
preted with care, as a single percentage point cor-
responds to 2-3 sentences, but the overall trend is
encouraging.
On the Atis development set, FUBL outperforms
ZC07 by 7.5% of recall but on the Atis test set
FUBL lags ZC07 by 2%. The reasons for this dis-
crepancy are not clear, however, it is possible that
the syntactic constructions found in the Atis test set
do not exhibit the same degree of variation as those
seen in the development set. This would negate the
need for the very general lexicon learnt by FUBL.
Across the evaluations, despite achieving high re-
call, FUBL achieves significantly lower precision
than ZC07 and ? -WASP. This illustrates the trade-
off from having a very general model of proposing
lexical structure. With the ability to skip unseen
System Rec. Pre. F1
Labelled Logical Forms
ZC05 79.3 96.3 87.0
ZC07 86.1 91.6 88.8
UBL 87.9 88.5 88.2
FUBL 88.6 88.6 88.6
Labelled Question Answers
DCS 91.1 - -
Table 3: Exact match accuracy on the Geo880 test set.
System English SpanishRec. Pre. F1 Rec. Pre. F1
? -WASP 75.6 91.8 82.9 80.0 92.5 85.8
UBL 81.8 83.5 82.6 81.4 83.4 82.4
FUBL 83.7 83.7 83.7 85.6 85.8 85.7
System Japanese TurkishRec. Pre. F1 Rec. Pre. F1
? -WASP 81.2 90.1 85.8 68.8 90.4 78.1
UBL 83.0 83.2 83.1 71.8 77.8 74.6
FUBL 83.2 83.8 83.5 72.5 73.7 73.1
Table 4: Exact-match accuracy on the Geo250 data set.
words, FUBL returns a parse for all of the Atis test
sentences, since the factored lexicons we are learn-
ing can produce a very large number of lexical items.
These parses are, however, not always correct.
10 Analysis
The Atis results in Tables 1 and 2 highlight the ad-
vantages of factored lexicons. FUBL outperforms
the UBL baseline by 16 and 11 points respectively
in exact-match recall. Without making any modi-
fication to the CCG grammars or parsing combina-
tors, we are able to induce a lexicon that is general
enough model the natural occurring variations in the
data, for example due to sloppy, unedited sentences.
Figure 2 shows a parse returned by FUBL for
a sentence on which UBL failed. While
the word ?cheapest? is seen 208 times in the
training data, in only a handful of these in-
stances is it seen in the middle of an utter-
ance. For this reason, UBL never proposes
the lexical item, cheapest ` NP\(S|NP)/(S|NP) :
? f?g.argmin(?x. f (x)? g(x),?y.cost(y)), which is
used to parse the sentence in Figure 2. In contrast,
FUBL uses a lexeme learned from the same word in
different contexts, along with a template learnt from
similar words in a similar context, to learn to per-
1520
pittsburgh to atlanta the cheapest on july twentieth
NP (S|NP)\NP/NP NP NP\(S|NP)/(S|NP) (S|NP)/NP/NP NP NP
pit ?x?y? z.to(z,x) atl ? f?g.argmin(?x. f (x)?g(x),?y.cost(y)) ?x?y? z.month(z,x) jul 20
? f rom(z,y) ?day(z,y)
> >
(S|NP)\NP (S|NP)/NP?x?y.to(y,atl)? f rom(y,x) ?x?y.month(y, jul)?day(y,x)
< >
(S|NP) (S|NP)?x.to(x,atl)? f rom(x, pit) ?x.month(x, jul)?day(x,20)
>NP\(S|NP)? f .argmin(?x. f (x)?month(x, jul)?day(x,20),?y.cost(y))
<NP
argmin(?x. f rom(x, pit)? to(x,atl)?month(x, jul)?day(x,20),?y.cost(y))
Figure 2: An example learned parse. FUBL can learn this type of analysis with novel combinations of lexemes and
templates at test time, even if the individual words, like ?cheapest,? were never seen in similar syntactic constructions
during training, as described in Section 10.
form the desired analysis.
As well as providing a new way to search the lex-
icon during training, the factored lexicon provides a
way of proposing new, unseen, lexical items at test
time. We find that new, non-NP, lexical items are
used in 6% of the development set parses.
Interestingly, the addition of templates that intro-
duce semantic content (as described in Section 6.2)
account for only 1.2% of recall on the Atis develop-
ment set. This is suprising as elliptical constructions
are found in a much larger proportion of the sen-
tences than this. In practice, FUBL learns to model
many elliptical constructions with lexemes and tem-
plates introduced through maximal factorings. For
example, the lexeme (to, [ f rom, to]) can be used
with the correct lexical template to deal with our
motivating example ?flights Boston to New York?.
Templates that introduce content are therefore only
used in truly novel elliptical constructions for which
an alternative analysis could not be learned.
Table 5 shows a selection of lexemes and tem-
plates learned for Atis. Examples 2 and 3 show that
morphological variants of the same word must still
be stored in separate lexemes. However, as these
lexemes now share templates, the total number of
lexical variants that must be learned is reduced.
11 Discussion
We argued that factored CCG lexicons, which in-
clude both lexemes and lexical templates, provide
a compact representation of lexical knowledge that
can have advantages for learning. We also described
a complete approach for inducing factored, prob-
abilistic CCGs for semantic parsing, and demon-
Most common lexemes by type of constants in~c.
1 e (Boston, [bos]) (Denver, [den])
2 ?e, t? (flight, [ f light]) (flights, [ f light])
3 ?e, i? (fare, [cost]) (fares, [cost])
4 ?e,?e, t?? (from, [ f rom]) (to, [to])
5 ?e, i?, (cheapest, [argmin,cost])?e, t? (earliest, [argmin,dep time])
6 ?i,?i, t??, (after, [>,dep time])
?e, i? (before, [<,dep time])
Most common templates matching lexemes above.
1 ? (?,~v).? `NP :v1
2 ? (?,~v).? `S|NP :?x.v1(x)
3 ? (?,~v).? `NP|NP :?x.v1(x)
4 ? (?,~v).? `S|NP/NP\(S|NP) :?x?y.v1(x,y)
5 ? (?,~v).? `NP/(S|NP) :? f .v1(?x. f (x),?y,v2(y))
6 ? (?,~v).? `S|NP\(S|NP)/NP :
?x?y? z.v1(v2(z),x)? y(x)
Table 5: Example lexemes and templates learned from
the Atis development set.
strated strong performance across a wider range of
benchmark datasets that any previous approach.
In the future, it will also be important to ex-
plore morphological models, to better model vari-
ation within the existing lexemes. The factored lex-
ical representation also has significant potential for
lexical transfer learning, where we would need to
learn new lexemes for each target application, but
much of the information in the templates could, po-
tentially, be ported across domains.
Acknowledgements
The work was supported in part by EU ERC Ad-
vanced Fellowship 249520 GRAMPLUS, and an
ESPRC PhD studentship. We would like to thank
Yoav Artzi for helpful discussions.
1521
References
Bos, Johan, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-coverage
semantic representations from a CCG parser. In Pro-
ceedings of the International Conference on Computa-
tional Linguistics.
Branavan, S.R.K., Luke Zettlemoyer, and Regina Barzi-
lay. 2010. Reading between the lines: Learning to map
high-level instructions to commands. In Association
for Computational Linguistics (ACL).
Carpenter, Bob. 1997. Type-Logical Semantics. The MIT
Press.
Clark, Stephen and James R. Curran. 2003. Log-linear
models for wide-coverage CCG parsing. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing.
Clark, Stephen and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics
33(4):493?552.
Clarke, James, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
the world?s response. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL-2010). Uppsala, Sweden,
pages 18?27.
Ge, Ruifang and Raymond J. Mooney. 2006. Discrimina-
tive reranking for semantic parsing. In Proceedings of
the COLING/ACL 2006 Main Conference Poster Ses-
sions.
Goldwasser, Dan, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised semantic
parsing. In Association for Computational Linguistics
(ACL).
He, Yulan and Steve Young. 2005. Semantic processing
using the hidden vector state model. Computer Speech
and Language .
He, Yulan and Steve Young. 2006. Spoken language
understanding using the hidden vector state model.
Speech Communication 48(3-4).
Hockenmaier, Julia and Mark Steedman. 2002. Gener-
ative models for statistical parsing with Combinatory
Categorial Grammar. In Proceedings of the 40th Meet-
ing of the ACL. Philadelphia, PA, pages 335?342.
Kate, Rohit J. and Raymond J. Mooney. 2006. Using
string-kernels for learning semantic parsers. In Pro-
ceedings of the 44th Annual Meeting of the Association
for Computational Linguistics.
Kate, Rohit J., Yuk Wah Wong, and Raymond J. Mooney.
2005. Learning to transform natural to formal lan-
guages. In Proceedings of the National Conference
on Artificial Intelligence.
Kwiatkowski, Tom, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilistic
CCG grammars from logical form with higher-order
unification. In Proceedings of the Conference on Em-
perical Methods in Natural Language Processing.
LeCun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998.
Gradient-based learning applied to document recogni-
tion. Proceedings of the IEEE 86(11):2278?2324.
Liang, P., M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
Association for Computational Linguistics and Inter-
national Joint Conference on Natural Language Pro-
cessing (ACL-IJCNLP).
Liang, P., M. I. Jordan, and D. Klein. 2011. Learning
dependency-based compositional semantics. In Asso-
ciation for Computational Linguistics (ACL).
Lu, Wei, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-
moyer. 2008. A generative model for parsing natural
language to meaning representations. In Proceedings
of The Conference on Empirical Methods in Natural
Language Processing.
Miller, Scott, David Stallard, Robert J. Bobrow, and
Richard L. Schwartz. 1996. A fully statistical approach
to natural language interfaces. In Proc. of the Associ-
ation for Computational Linguistics.
Nguyen, Le-Minh, Akira Shimazu, and Xuan-Hieu Phan.
2006. Semantic parsing with structured SVM ensem-
ble classification models. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions.
Och, Franz Josef and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics 29(1):19?51.
Papineni, K. A., S. Roukos, and T. R. Ward. 1997.
Feature-based language understanding. In Proceed-
ings of European Conference on Speech Communica-
tion and Technology.
Poon, Hoifung and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Conference on Empirical
Methods in Natural Language Processing (EMNLP).
Poon, Hoifung and Pedro Domingos. 2010. Unsuper-
vised ontology induction from text. In Association for
Computational Linguistics (ACL).
Ramaswamy, Ganesh N. and Jan Kleindienst. 2000. Hier-
archical feature-based translation for scalable natural
language understanding. In Proceedings of Interna-
tional Conference on Spoken Language Processing.
Steedman, Mark. 1996. Surface Structure and Interpre-
tation. The MIT Press.
1522
Steedman, Mark. 2000. The Syntactic Process. The MIT
Press.
Tang, Lappoon R. and Raymond J. Mooney. 2000. Au-
tomated construction of database interfaces: Integrat-
ing statistical and relational learning for semantic pars-
ing. In Proceedings of the Joint Conference on Empiri-
cal Methods in Natural Language Processing and Very
Large Corpora.
Thompson, Cynthia A. and Raymond J. Mooney. 2002.
Acquiring word-meaning mappings for natural lan-
guage interfaces. Journal of Artificial Intelligence Re-
search 18.
Vogel, Adam and Dan Jurafsky. 2010. Learning to follow
navigational directions. In Association for Computa-
tional Linguistics (ACL).
Watkinson, Stephen and Suresh Manandhar. 1999. Un-
supervised lexical learning with categorial grammars
using the LLL corpus. In Proceedings of the 1st Work-
shop on Learning Language in Logic.
Wong, Yuk Wah and Raymond Mooney. 2006. Learning
for semantic parsing with statistical machine transla-
tion. In Proceedings of the Human Language Technol-
ogy Conference of the NAACL.
Wong, Yuk Wah and Raymond Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing with
lambda calculus. In Proceedings of the Association for
Computational Linguistics.
Zelle, John M. and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic pro-
gramming. In Proceedings of the National Conference
on Artificial Intelligence.
Zettlemoyer, Luke S. and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars. In
Proceedings of the Conference on Uncertainty in Arti-
ficial Intelligence.
Zettlemoyer, Luke S. and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing to
logical form. In Proc. of the Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning.
Zettlemoyer, Luke S. and Michael Collins. 2009. Learn-
ing context-dependent mappings from sentences to
logical form. In Proceedings of The Joint Conference
of the Association for Computational Linguistics and
International Joint Conference on Natural Language
Processing.
1523
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 30?41,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Exploring the utility of joint morphological and syntactic learning
from child-directed speech
Stella Frank
sfrank@inf.ed.ac.uk
Frank Keller
keller@inf.ed.ac.uk
ILCC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
Sharon Goldwater
sgwater@inf.ed.ac.uk
Abstract
Children learn various levels of linguistic
structure concurrently, yet most existing mod-
els of language acquisition deal with only
a single level of structure, implicitly assum-
ing a sequential learning process. Developing
models that learn multiple levels simultane-
ously can provide important insights into how
these levels might interact synergistically dur-
ing learning. Here, we present a model that
jointly induces syntactic categories and mor-
phological segmentations by combining two
well-known models for the individual tasks.
We test on child-directed utterances in English
and Spanish and compare to single-task base-
lines. In the morphologically poorer language
(English), the model improves morphological
segmentation, while in the morphologically
richer language (Spanish), it leads to better
syntactic categorization. These results provide
further evidence that joint learning is useful,
but also suggest that the benefits may be dif-
ferent for typologically different languages.
1 Introduction
Models of language acquisition seek to infer lin-
guistic structure from data with minimal amounts of
prior knowledge, in order to discover which char-
acteristics of the input data are useful for learn-
ing, and thus potentially utilised by human learners.
Most previous work has focused on learning individ-
ual aspects of linguistic structure. However, children
clearly learn multiple aspects in parallel, rather than
sequentially, implying that models of language ac-
quisition should also incorporate joint learning. Joint
models investigate the interaction between different
levels of linguistic structure during learning. These
interactions are often (but not necessarily) synergis-
tic, enabling better, more robust, learning by making
use of cues from multiple sources. Recent models
using joint learning to model language acquisition
have spanned various domains including phonology,
word segmentation, syntax and semantics (Feldman
et al, 2009; Elsner et al, 2012; Doyle and Levy,
2013; Johnson, 2008; Kwiatkowski et al, 2012).
In this paper we examine the joint learning of
syntactic categories and morphology, which are ac-
quired by children at roughly the same age (Clark,
2003b), implying possible interactions in the learn-
ing process. Both morphology and word order de-
pend on categorising words based on their morpho-
syntactic function. However, previous models of
syntactic category learning have relied principally
on surrounding context, i.e., word order constraints,
whereas models of morphology use word-internal
cues. Our joint model integrates both sources of
information, allowing the model to flexibly weigh
them according to their utility.
Languages differ in the richness of their mor-
phology and strictness of word order. These char-
acteristics appear to be (anti)correlated, with rich
morphology co-occurring with free word order and
vice versa (Blake, 2001; McFadden, 2003). The
timecourse of acquisition is also influenced by lan-
guage typology: learners of morphologically rich
languages become productive in morphology ear-
lier (Xanthos et al, 2011), suggesting that richer
morphology may be more salient for learners than
impoverished morphology. Sentence comprehension
in children also shows cross-linguistic differences
in the cues used to make sense of non-canonical
sentence structure: learners of a morphologically
rich language (Turkish) disregard word order in
30
favour of morphology, whereas learners of En-
glish favour word order (Slobin, 1982; MacWhin-
ney et al, 1984). These interactions between mor-
phology and word order suggest that a joint model
will be better able to support the differences in cue
strength (rich morphology versus strict word order),
and thus be more language-general, than single-task
models.
Both syntactic category and morphology induc-
tion have been the focus of much recent work. (See
Hammarstro?m and Borin (2011) for an overview
of unsupervised morphology learning, likewise
Christodoulopoulos et al (2010) for a comparison
of part of speech/syntactic category induction sys-
tems.) However, given the tightly coupled nature of
these two tasks, there has been surprisingly little
work in joint learning of morphology and syntac-
tic categories. Systems for inducing syntactic cat-
egories often make use of morpheme-like features,
such as word-final characters (Smith and Eisner,
2005; Haghighi and Klein, 2006; Berg-Kirkpatrick
et al, 2010; Lee et al, 2010), or model words
at the character-level (Clark, 2003a; Blunsom and
Cohn, 2011), but do not include morphemes ex-
plicitly. Other systems (Dasgupta and Ng, 2007;
Christodoulopoulos et al, 2011) use morphologi-
cal segmentations learned by a separate morphology
model as features in a pipeline approach.
Models of morphology induction generally oper-
ate over a lexicon, i.e. a list of word types, rather
than token corpora (Goldsmith, 2006; Creutz and
Lagus, 2007; Kurimo et al, 2010). These models
find morphological categories on the basis of word-
internal features, without taking syntactic context
into account (which is of course not available in a
lexicon).
Lee et al (2011) and Sirts and Aluma?e (2012)
present models that infer morphological segmenta-
tions and syntactic categories jointly, although Lee
et al (2011) do not evaluate the inferred syntactic
categories. Both make use of a word-type constraint
which limits each word form to a single analysis
(i.e., all instances of ducks are assigned to a single
category and will have the same morpheme analy-
sis, ignoring the gold standard distinction between a
plural noun and third person singular verb). This can
make inference more tractable, and often increases
performance, but does not respect the ambiguity in-
herent in natural language, both over syntactic cat-
egories and morphological analyses. The degree of
ambiguity is language dependent, so that even if a
type-constraint is perhaps relatively unproblematic
in English, it will pose problems in morphologically
richer languages. Furthermore, these two models
make use of an array of heuristics that may not allow
them to be easily generalisable across languages and
datasets (e.g., likelihood scaling (Sirts and Aluma?e,
2012), sequential suffix matching (Lee et al, 2011)).
In this paper, we present a joint model composed
of two well-known individual models. This allows
us to cleanly investigate the effects of joint learning
and its potential benefits over the single task models.
The simplicity of our models also allows us to avoid
modelling and inference heuristics.
Previous models have used adult-directed written
texts, which differs significantly from the type of
language available to child learners. We test our joint
model on child-directed utterances in English (a
morphologically poor language) and Spanish (with
richer morphology)1. Our results indicate that our
joint model is able to flexibly accommodate lan-
guages with differing levels of morphological rich-
ness. The joint model matches the performance of
single task models on both tasks, demonstrating that
the additional complexity is not a problem (i.e., it
does not add noise). Moreover, the joint model im-
proves performance significantly on the task corre-
sponding to the language?s weaker cue, indicating a
transfer of information from the stronger cue. The
fact that the nature of this improvement varies by
language provides evidence that joint learning can
effectively accommodate typological diversity.
2 Model
The task is to assign word tokens to part of speech
categories and simultaneously segment the tokens
into morphemes. We assume a relatively simple yet
commonly used concatenative morphology which
models a word as a stem plus (possibly null) suffix2.
1There are languages with much richer morphology than
Spanish, but none with a child-directed corpus suitably anno-
tated for evaluation.
2Fullwood and O?Donnell (2013) recently presented a
model of non-concatenative morphology that could be inte-
grated into this model; however, it does not perform well on En-
glish (and presumably other mostly concatenative languages).
31
Since this is an unsupervised model, the inferred cat-
egories and morphemes lack meaningful labels, but
ideally will correspond to gold standard categories
and morphemes.
2.1 Word Order
We model a sequence of words as a Hidden Markov
Model (HMM) with a non-parametric emission dis-
tribution. As usual, the latent states of the HMM rep-
resent syntactic categories. The tag sequence is gen-
erated by a trigram Dirichlet-multinomial distribu-
tion, where transition parameters ? are drawn from
a symmetric Dirichlet distribution with the hyperpa-
rameter ?t . Each tag ti in the sequence is then drawn
from the transition distribution conditioned on the
previous two tags:
?(t,t ?) ? Dir(?t)
ti = t|ti?1 = t
?, ti?2 = t
??,?? Mult(?(t ?,t ??))
This model is token-based, permitting different
tokens of the same word type to have different
syntactic categories. Most recent models have in-
cluded a constraint forcing all tokens of a given
type into the same category, which improves per-
formance but often complicates inference. The
Bayesian HMM?s performance is therefore not state-
of-the-art, but is comparable to other token-based
models (Christodoulopoulos et al, 2010) and the
model is easy to extend within the Bayesian frame-
work, allowing us to compare multiple versions.
This part of the model is parametric, operat-
ing over a fixed number of tags T , and is iden-
tical to the formulation of tag transitions in the
Bayesian HMM (Goldwater and Griffiths, 2007).
However, we replace the BHMM?s emission dis-
tribution with the morphologically-informed distri-
butions described below. As in the BHMM, the
emission distributions are conditioned on the tag,
i.e., each tag has its own morphology.
2.2 Morphology
The morphology model introduced by Goldwater
et al (2006) generates morphological analyses for a
set of tokens. These analyses consist of a tag plus a
stem and suffix pair, which are concatenated to form
the observed words. Both stem s and suffix f are
generated from Dirichlet-multinomials conditioned
on the tag t:
?? Dir(??)
t|?? Mult(?)
?? Dir(?s)
s|t,?? Mult(?t)
?? Dir(? f )
f |t,?? Mult(?t)
The ?s are hyperparameters governing the Dirich-
let distributions from which the multinomials ?,?,?
are drawn. In turn, t,s, and f are drawn from these
multinomials.
The probability of a word under this model is the
sum of the probabilities of all possible analyses l =
(t,s, f ):
P0(w) =?
l
P0(l) = ?
t,s, f s.t.
s? f=w
P(s|t)P( f |t)P(t) (1)
where s? f = w denotes that the concatenation of
stem and suffix results in the word w.
On its own, this distribution over morphologi-
cal analyses makes independence assumptions that
are too strong: most word tokens of a word type
have the same analysis, but P0 will re-generate
that analysis for every token. To resolve this prob-
lem, a Pitman-Yor process (PYP) is placed over the
generating distribution above. The Pitman-Yor pro-
cess has been found to be useful for representing
the power-law distributions common in natural lan-
guage (Teh, 2006; Goldwater and Griffiths, 2007;
Blunsom and Cohn, 2011).
The distribution of draws from a Pitman-Yor pro-
cess (which, in our case, determines the distribu-
tion of word tokens with each morphological anal-
ysis) is commonly described using the metaphor of
a Chinese restaurant. A series of customers (tokens
z = z1 . . .zN) enter a restaurant with an infinite num-
ber of initially empty tables. Upon entering, each
customer is seated at a table k with probability
p(zi = k|z1 . . .zi?1,a,b) = (2)
{
nk?a
i?1+b if 1? k ? K
Ka+b
i?1+b if k = K +1
32
tk sk
fk lk
K
zi
wi
N
Figure 1: Plate diagram depicting the morphology model
(adapted from Goldwater et al (2006)). Hyperparameters
have been omitted for clarity. The left-hand plate depicts
the base distribution P0; note that the morphological anal-
yses lk are generated deterministically as (tk,sk, fk). The
observed words wi are also deterministic given zi = k and
lk, since wi = sk? fk.
where nk is the number of customers already sitting
at table k, K is the total number of tables occupied by
the i?1 previous customers, and 0? a < 1 and b? 0
are hyperparameters of the process. The probability
of being seated at a table increases with the number
of customers already seated at that table, creating a
?rich-get-richer? power-law distribution of tokens to
tables; a and b control the amount of reuse of exist-
ing tables, with smaller values leading to more reuse.
Crucially, each table serves a dish generated by
the base distribution P0?i.e., the dish is a morpho-
logical analysis lk = (t,s, f )?and all the customers
seated at the same table share the same dish, which
is generated only once (at the point when that table
is first occupied). The model can thus reuse the anal-
ysis for a particular word and avoid regenerating the
same analysis multiple times. Note that multiple ta-
bles may have identical analyses, lk = lk? . Figure 1
illustrates how the full PYP morphology model gen-
erates the observed sequence of word tokens.
2.3 Combined Model
The full model (Figure 2) combines the latent tag se-
quence with the morphology model. Tag tokens are
generated conditioned on local context, not the base
distribution, as in the morphology model. Instead of
a single PYP generating morphological analyses for
all tokens, as in the Goldwater et al (2006) model,
we have a separate PYP for each tag type, i.e., each
tag has its own restaurant with its own customers
(the tokens labeled with that tag) and its own mor-
phological analyses. The distribution of customers
ti?2 ti?1 ti zi
wi
sk fk
lk
N
Kt
T
Figure 2: Plate diagram depicting the joint model. Hyper-
parameters have been omitted for clarity. The L-shaped
plate contains the tokens, while the square plates contain
the morphological analyses. The t are latent tags, zi is an
assignment to a morphological analysis lk = (sk, fk), and
wi is the observed word. T is the number of distinct tags,
and Kt the number of tables used by tag type t.
in each of the tag-specific restaurants is still deter-
mined by Equation 2, except that all of the counts
and indices are with respect to only the tokens and
tables assigned to that tag.
Each tag-specific PYP (restaurant) also has a sep-
arate base distribution, P(t)0 , resulting in distinct dis-
tributions over stems and suffixes for each tag. The
analyses generated by the base distributions consist
of (stem, suffix) pairs; the tag is given by the identity
of the generating PYP.
P(t)0 (w) =?
l
P(t)0 (l = (s, f )) = ?
s, f s.t.
s? f=w
P(s|t)P( f |t)
(3)
The full joint posterior distribution of a sequence
of words, tags, and morpheme analyses is shown in
Figure 3. Note that all tag-specific morphology mod-
els share the same Pitman-Yor parameters a and b.
3 Inference
We use Gibbs sampling for inference over the three
sets of discrete variables: tags t, their assignments to
morphological analyses (tables) z, and the analyses
themselves l.
Each iteration of the sampler has two stages: First
the morphological analyses l are sampled, and then
each token samples a new tag and a new assignment
to an analysis/table. Because the table assignments
33
P(t, l,z|?t ,a,b,?s,? f ) =P(t|?t)P(l|t,?s,? f )P(z|a,b) (4)
P(t|?t) =
N
?
i=2
P(ti|ti?1, ti?2,t1...i?1,?t) =
T
?
t,t ?=1
?(T?t)
?(ntt ? +T?t)
T
?
t ??=1
?(ntt ?t ?? +?t)
?(?t)
(5)
P(l|t,?s,? f ) =
T
?
t=1
Kt
?
k=1
Pt(lk = (s, f )|l1...k?1,?s,? f ) (6)
=
T
?
t=1
?(S?s)
?(mt +S?s)
?(F? f )
?(mt +F? f )
S
?
s=1
?(mts +?s)
?(?s)
F
?
f=1
?(mt f +? f )
?(? f )
(7)
P(z|a,b) =
T
?
t=1
Nt
?
i=1
P(zi|t,z1...i?1,a,b) (8)
=
T
?
t=1
?(1+b)
?(nt +b)
Kt
?
k=1
(ka+b)
?(nk?a)
?(1?a)
(9)
Figure 3: The posterior distribution of our joint model. Because the sequence of words w is deterministic given
analyses l and assignments to analyses (tables) z, the joint posterior over all variables P(w,t, l,z|?t ,a,b,?s,? f ) is
equal to P(t, l,z|?t ,a,b,?s,? f ) when lzi = wi for all i, and 0 otherwise. We give equations for the non-zero case. ns
refer to token counts, ms to table counts. We add two dummy tokens at the start, end, and between sentences to pad
the context history.
are conditioned on tags (i.e., a token must be as-
signed to a table in the correct PYP restaurant) re-
sampling the tag requires immediate resampling of
the table assignment as well.
3.1 Initialization
The tags are initialized uniformly at random. For
each token, a segmentation point is chosen uni-
formly at random (we disallow segmentations with
a null stem). If this segmentation is new within the
PYP associated with that token?s tag, a new table is
created for the token in that PYP. If it matches an ex-
isting analysis, zi is sampled from the existing tables
k plus a possible new table k?.
3.2 Morphological Analyses
Each lk represents the morphological analysis for the
set of tokens assigned to table k. Resampling the
segmentation point (stem and suffix identity) of the
analysis changes the segmentation of all of the word
tokens assigned to that analysis. Note that the tag is
not included in lk in the combined model, because
the tag identity is dependent on the local contexts of
all the tokens seated at the table.
Analyses are sampled from a product of Dirichlet-
multinomial posteriors as follows:
p(lk = (s, f )|t, l
\k) =
m\ks +?s
m\k +S?s
m\kf +? f
m\k +F? f
(10)
where ms and m f are the number of analyses for
this tag that share a stem or suffix with lk, and m
is the total number of analyses for this tag. S and
F are the total number of stems and suffixes in the
model. l\k indicates that the current analysis lk has
been removed from the distribution and the appro-
priate counts, to create the correct conditioning dis-
tribution for the Gibbs sampler.
3.3 Tags
Tags are sampled from the product of posteri-
ors of the transition and emission distributions.
The transition distribution is a standard Dirichlet-
multinomial posterior. Calculating the emission dis-
tribution probability, i.e. the marginal probability of
the word given the tag, involves summing over the
probability of all the existing tables in the given PYP
that emit the correct word, plus the probability of
a new table being created, which also includes the
probability of a new analysis from P(t)0 .
34
More precisely, tags are sampled from the follow-
ing distribution:
p(ti = t|wi = w,t\i,z\i, l,?t ,a,b) (11)
? p(ti = t|ti?1, ti?2,t\i,?t)? p(w|t,z\i, l)
= p(ti = t|ti?1, ti?2,t\i,?t)
? ( ?
k s.t. lk=w
p(zi = k|t,w,z\i)+ p(zi = knew|t,w,z\i))
=
nti?2ti?1t +?t
nti?2ti?1 +T?t
? ( ?
k s.t. lk=w
nk?a
nt +b
+
Kta+b
nt +b
P(t)0 (w))
where lk = w matches tables compatible with w,
i.e., the concatenation of stem and suffix form the
word, slk ? flk = w. nk is the number of words as-
signed to the table k and Kt is the total number of
tables in the PYP for tag t. Note that all counts are
obtained after the removal of the current ti and zi,
i.e., from t\i and z\i.
3.4 Table Assignments
Once a new tag has been sampled for a token, the ta-
ble assignment must be resampled conditioned on
the new tag. The assignment zi is drawn over all
compatible tables in the tag?s PYP (that is, where
lk = w), plus a possible new table:
p(zi = k|ti = t,w,z\i,a,b) ? (12)
{
nk?a
nt+b
if 1? k ? Kt
Kt a+b
nt+b
P(t)0 (w) if k = Kt +1
P(t)0 is calculated by summing over the probability
of all possible segmentations for a new analysis for
word wi, using Equation 3. If a new table is drawn
(k > Kt) then we also sample a new analysis for that
table from P(t)0 .
4 Preliminary Experiments
An important argument for joint learning is that it
affords increased flexibility and robustness across a
wider range of input data. A model that relies on
word order cannot learn syntactic categories from a
morphologically complex language with free word
order; likewise a model attempting to categorise
words using morphology alone will fail on a lan-
guage without morphology. An effective joint model
Language A
abdc fefh pomo rtut usst
cdcc bcba gghh npop npoo
cdca aaaa fefh hfeg pnon
Language B
noom.no usrs.st bbdb.ac cbab.cc cdaa.cc
rttt.uu cbab.aa mnom.oo ccda.bc onmm.om
rruu.ts npop.mm gehg.fh trrt.uu tssu.uu
Table 1: Example sentences in the synthetic languages.
Words in Category 1 are made of characters a-d, Cate-
gory 2 e-h, Category 3 m-p, Category 4 r-u. Suffixes in
Language B are separated with periods (.) for illustrative
purposes only.
will be able to make use of the different cues in both
language types in a flexible way.
In order to test the proposed model, we run two
experiments on synthetic languages, which simulate
languages in which either word order or morphology
is the sole cue. Most natural languages fall between
these extremes, but these experiments show that our
model can capture the full spectrum.
Language A is a strict word order language lack-
ing morphology. It has a vocabulary of 200 word
types, split into four different categories. The 50
word types in each category are created by com-
bining four letters, with replacement, into four-letter
words, with a different set of letters used in each cat-
egory3. Words within a category may thus share be-
ginning or ending characters, which could be posited
as stems or suffixes by the model, but since only
50 of 256 possible strings are used, there will be
no strong evidence for consistent stem and suffixes
(i.e. stems appearing with multiple suffixes and vice
versa). Each sentence in Language A consists of five
words in one of twenty possible category sequences.
In these sequences, each category is either followed
by itself or the next category (i.e. [2,2,2,3,4] is valid
but [2,4,3,1,4] is not). Word order is thus strongly
constrained by category membership.
Language B has free word order, with category
membership signalled by suffixes. Words are cre-
3We achieved the same results with a language using the
same four characters in all categories, but using different char-
acters makes the categories human-readable. The model does
not have a orthographic/phonological component and so will
not recognise the within-category similarity, other than possi-
bly positing spurious stems or suffixes.
35
-70000
-60000
-50000
-40000
-30000
-20000
-10000
 0
 0  200  400  600  800  1000
L
o
g
 
P
r
o
b
a
b
i
l
i
t
y
Iteration
LangA TotalLangA TransitionsLangA MorphologyLangB TotalLangB TransitionsLangB Morphology
Figure 4: Log probability of the sampler state over 1000
iterations on Languages A and B.
ated by the concatenation of a stem and a suffix,
where the stems are the same as the words in lan-
guage A (50 stems in each of four categories). One
of six category-specific suffixes is appended to each
stem, resulting in 300 word types per category. Each
suffix is two letters long, created by combining three
possible letters (the same letters used to create the
stems), thus making mis-segmentation possible (for
instance, up to three of the suffixes could have the
same final letter). Sentences are again five words
long, but the sequence of categories is drawn at ran-
dom, resulting in uniformly random word order. See
Table 1 for example sentences in both languages.
We create a 5000 word corpus for each language,
and run our model on these corpora. Hyperparame-
ters are set to the same values in both languages4.
We run the sampler on each dataset for 1000 it-
erations with simulated annealing. In both cases,
the correct solution is found by iteration 500. Fig-
ure 4 shows that the morphology component con-
tinues to increase the log probability by increasing
the number of tokens seated at a table. Note that
the correct solution in Language A involves learn-
ing a very peaked transition distribution as well as an
even more extreme distribution over suffixes (where
only the null suffix has high probability), whereas
the same distributions in Language B are much flat-
ter. The fact that the same hyperparameter setting is
4The PYP parameters are set to a = 0.1,b = 1.0 and the
HMM transition parameter ?t = 1.0; the parameters in the base
distribution are ?s,? f = 0.001,?k = 0.5.
able to correctly identify the two language extremes
indicates that the model is robust to hyperparameter
values.
These experiments demonstrate that our joint
model is able to learn correctly even when only ei-
ther morphology or word order is informative in a
language. We now turn to acquisition data from nat-
ural languages in which both morphology and word
order are useful cues but to varying degrees.
5 CDS Experiments
5.1 Data
We use two corpora, Eve (Brown, 1973) and Or-
nat (Ornat, 1994), from the CHILDES database
(MacWhinney, 2000). These corpora consist of the
child-directed utterances heard by two children,
the former learning English and the latter Spanish.
These have been annotated for part of speech cate-
gories and morphemes.
The CHILDES corpora are tagged with a very rich
set of part of speech tags (74 tags), which we col-
lapse to a smaller set of tags5. The Eve corpus has
61224 tokens and is thus larger than the Spanish cor-
pus, which has 40497 tokens. However, the English
corpus has only 17 gold suffix types, while Spanish
has 83. The increased richness of Spanish morphol-
ogy also has an effect on the number of word types in
the corpus: the Spanish dataset has 3046 word types,
whereas the larger English dataset has only 1957.
Morphology is annotated using a stem-affix en-
coding which does not directly correspond to our
segmentation-based model. The word running is an-
notated as run-ING, jumping as jump-ING; the anno-
tation is thus agnostic about ortho-morphemic seg-
mentation (i.e., whether to segment as run.ning or
runn.ing), whereas the model is forced to choose
a segmentation point. Syncretic suffixes (sharing
an identical surface form) are disambiguated: sings
is annotated as sing-3S, plums as plum-PL. Con-
versely, the annotation scheme merges allomorphs
into a single suffix: infinitive verbs in Spanish,
for instance, are encoded as ending with -INF,
corresponding to -ar, -er, and -ir surface forms.
5These are 13 for English (ADJ, ADV, AUX, CONJ, DET,
INF, NOUN, NEG, OTH, PART, PREP, PRO, VERB) and 10
for Spanish, since the gold standard does not distinguish AUX,
PART or INF.
36
We ignore irregular/non-affixing forms annotated
with & (e.g. was, annotated as be&PAST) and
use only hyphen-separated suffixes to evaluate.
Where multiple suffixes are concatenated together
(e.g., dog-DIM-PL) we treat this as a single suffix
(-DIM-PL) for evaluation purposes.
In Spanish, many words are annotated as having
a suffix of effectively zero length, e.g. the imper-
ative gusta is annotated as gusta-2S&IMP. We re-
place these suffixes (where the stem is equal to the
word) with a null suffix, excluding them from eval-
uation, as they are impossible for a segmentation-
based model to find.
5.2 Evaluation
Tags are evaluated using VM (Rosenberg and
Hirschberg, 2007), as has become standard for this
task (Christodoulopoulos et al, 2010). VM is a mea-
sure of the normalised cross-entropy between gold
and proposed clusters; it ranges between 0 and 100,
with higher scores being better.
We also use VM to evaluate the morphological
segmentation: all tokens with a common suffix are
clustered together, and these clusters are compared
against the gold suffix clusters6. Using a clustering
metric avoids the need to evaluate against a gold seg-
mentation point (which the annotation lacks). Tag
membership is added to the non-null model suffixes,
so that a final -s suffix found in tag 2 is distinguished
from the same suffix found in tag 8 (creating suffixes
-s-T8 and -s-T2), analogous to the gold annotation
distinction between syncretic morphemes -PL and
-3S.
Note that ceiling performance of our model on
Suffix VM will be below 100, since our model can-
not cluster allomorphs, which are represented by a
single abstract morpheme in the gold standard.
5.3 Baselines
We test the full model, MORTAG, against a number
of variations to investigate the advantages of jointly
modelling the two tasks.
Two variants remove the transition distributions,
and thus local syntactic context, from the model.
6We also evaluated stem morpheme clusters and found near-
ceiling performance due to the high number of null-suffix words
in both corpora.
MORTAGNOTRANS is the full model without tran-
sitions between tag tokens; morphology PYP draws
remain conditioned on token tags. We add a Dirich-
let prior over tags (?t = 0.1) to encourage tag spar-
sity (analogous to the transition distribution in the
full model). MORCLUSTERS is the original model
of Goldwater et al (2006), in which tags (called
clusters in the original) are drawn by P0.
MORTAGNOSEG is a variant in which the only
available suffix is the null suffix; thus segmentations
are trivial and only tags are inferred. This model
is approximately equivalent to a simple Bayesian
HMM but with the addition of PYPs within the
emission distribution. We also evaluate against tags
found by the BHMM, with a Dirichlet-multinomial
emission distribution and no morphology.
MORTAGTRUETAGS is the full model but with all
tags fixed to their gold values. This model gives us
oracle-type results for morphology. (Due to the an-
notation scheme used in CHILDES, oracle morpho-
logical segmentations are unavailable, so we were
unable to test a model with gold morphology and in-
ferred tags.)
5.4 Experimental Procedure
Hyperparameter values for the Pitman-Yor process
were found using grid search on a development set
(Section 10 of Eve and Section 8 of Ornat; these sec-
tions are removed from the dataset we report results
on). We use the values which give the best Suffix
VM performance on the development data; however
we stress that the development results did not vary
greatly over a wide range of hyperparameter values,
and only deteriorated significantly at extreme values
of a.
There are a number of other hyperparameters in
the model which we set to fixed values. The transi-
tion hyperparameter ?t is set to 0.1 in all models.
We set the hyperparameters for the stem and suf-
fix distributions in the morphology base distribution
P0 to 0.001 for both ?s and ? f ; ?k over tags in the
MORCLUSTERS model is set to 0.5. The number of
possible stems and suffixes is given by the dataset: in
the Eve dataset there are 5339 candidate stems and
6617 candidate suffixes; in the Ornat dataset these
numbers are 8649 and 6598, respectively. The num-
ber of tags available to the model is set to the number
of gold tags in the data.
37
Tag VM Suffix VM
MORTAG 59.1(1.9) 41.9(10.0)
MORCLUSTERS 22.4(1.0)? 28.0(11.9)?
MORTAGNOTRANS 19.3(1.2)? 24.4(5.2)?
MORTAGNOSEG 59.4(1.7) ?
BHMM 56.2(2.3)? ?
MORTAGTRUETAGS ? 42.5(5.2)
Table 2: English Eve corpus results. Standard deviations
are in parentheses; ? denotes a significant difference from
the MORTAG model.
Sampling is run for 5000 iterations with anneal-
ing. Inspection of the posterior log-likelihood indi-
cates that the models converge after about 1000 it-
erations. We run inference over all models ten times
and report the average performance. Significance is
reported using the non-parametric Wilcoxon rank-
sum test with a significance level of ?< 0.05.
5.5 Results: English
Results on the English Eve corpus are shown in Ta-
ble 2. We use PYP parameters a = 0.3 and b = 10,
though we found similar performance over a wide
range of values of a and b. Our results show a clear
improvement in the morphological segmentations
found by the joint model and stable tagging perfor-
mance across all models with context information.
The syntactic clusters found by models using
only morphological patterns, MORTAGNOTRANS
and MORCLUSTERS, are clearly inferior and lead to
low Tag VM results. The models with local syntac-
tic context all perform approximately equally well
in terms of finding tags. We find no improvement on
tagging performance in English when adding mor-
phology, compared to the MORTAGNOSEG base-
line in which words are not segmented. However, we
do see a small but significant improvement over the
BHMM for both of these models, due to the replace-
ment of the multinomial emission distribution in the
BHMM with the PYP.
Morphological segmentations, as measured by
Suffix VM, clearly improve with the addition of lo-
cal contexts (and the ensuing better tags): the full
model outperforms the baselines without syntactic
contexts. On this dataset, the joint MORTAG model
even matches the performance of the model us-
Tag VM Suffix VM
MORTAG 43.4(2.6) 41.4(2.5)
MORCLUSTERS 20.3(2.5)? 46.5(3.2)
MORTAGNOTRANS 14.4(1.7)? 36.4(2.0)?
MORTAGNOSEG 39.6(3.7)? ?
BHMM 36.4(0.7)? ?
MORTAGTRUETAGS ? 59.8(0.4)?
Table 3: Spanish Ornat corpus results. Standard devia-
tions are in parentheses; ? denotes a significant difference
from the MORTAG model.
ing oracle tags. The standard deviation over Suf-
fix VM scores is quite large for MORTAG and
MORCLUSTERS; this is due to frequent words hav-
ing two high probability segmentations (most no-
tably is, which in some runs was segmented as i.s).
5.6 Results: Spanish
For the Spanish Ornat corpus, we found slightly dif-
ferent optimal PYP hyperparameters and set a = 0.1
and b = 0.1. Results are shown in Table 3.
The Spanish results pattern in the opposite way
as English. Here we see a statistically significant
improvement in tagging performance of the full
joint model over both models without morphology
(MORTAGNOSEG and BHMM). Models without
context information again find much worse tags,
mainly because (as in English) function words are
not identifiable by suffixes.
However, the full model does not find better mor-
phological segmentations than the MORCLUSTERS
model, despite better tags (the two models? Suffix
VM scores are not statistically significantly differ-
ent). We also see that the difference between the seg-
mentations found by the model using gold tags and
estimated tags is quite large. This is due to the ora-
cle model finding the rarer suffixes which were not
distinguished by the models with noisier tags. This
demonstrates the importance of syntactic categorisa-
tion for the morpheme induction task, and suggests
that a more sophisticated tagging model (with better
performance) may yet improve morpheme segmen-
tation performance in Spanish.
38
6 Conclusion
We have presented a model of joint syntactic cate-
gory and morphology induction. Operating within a
generative Bayesian framework means that combin-
ing single-task components is straightforward and
well-founded. Our model is token-based, allowing
for syntactic and morphemic ambiguity.
To our knowledge, this is the first joint model to
be tested on child-directed speech data, which is less
complex than the newswire corpora used by previ-
ous joint models. Child-directed speech may be sim-
ple enough for joint learning not to be necessary: our
results indicate the contrary, namely that joint learn-
ing is indeed helpful when learning from realistic
acquisition data.
We tested this model on two languages with dif-
ferent morphological characteristics. On English, a
language with relatively little morphology, espe-
cially in child directed speech, we found that bet-
ter categorisation of words yielded much better mor-
phology in terms of suffixes learned. Conversely, in
Spanish we saw less difference on the morphology
task between models with categories inferred solely
from morphemic patterns and models that also used
local syntactic context for categorisation. However,
in Spanish we saw an improvement in the tagging
task when morphology information was included.
This suggests that English and Spanish make dif-
ferent word-order and morphology trade-offs. In En-
glish, local context provides at least as much in-
formation as morphology in terms of determining
the correct syntactic category, but knowing a good
estimate of the correct syntactic category is use-
ful for determining a word?s morphology. In Span-
ish, a word?s morphology can more easily be deter-
mined simply by looking at frequent suffixes within
a purely morphological system. On the other hand,
word order is freer, making local syntactic context
unreliable, so taking morphological information into
account can improve tagging. These differences be-
tween languages demonstrate the benefits of joint
learning, which enables the learner to more flexibly
utilise the information available in the input data.
References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cote,
John DeNero, and Dan Klein. Painless unsuper-
vised learning with features. In Proceedings of the
North American Association for Computational
Linguistics (NAACL), 2010.
Barry J. Blake. Case. Cambridge University Press,
2001.
Phil Blunsom and Trevor Cohn. A hierarchical
Pitman-Yor process HMM for unsupervised part
of speech induction. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), 2011.
Roger Brown. A first language: The early stages.
Harvard University Press, Cambridge, MA, 1973.
Christos Christodoulopoulos, Sharon Goldwater,
and Mark Steedman. Two decades of unsuper-
vised POS induction: How far have we come? In
Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (ACL),
2010.
Christos Christodoulopoulos, Sharon Goldwater,
and Mark Steedman. A Bayesian mixture model
for part-of-speech induction using multiple fea-
tures. In Proceedings of the 16th Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), 2011.
Alexander Clark. Combining distributional and
morphological information for part of speech in-
duction. In Proceedings of the 10th annual Meet-
ing of the European Association for Computa-
tional Linguistics (EACL), 2003a.
Eve V. Clark. First Language Acquisition. Cam-
bridge University Press, 2003b.
Mathias Creutz and Krista Lagus. Unsupervised
models for morpheme segmentation and morphol-
ogy learning. ACM Transactions on Speech and
Language Processing, 4(1):1?34, 2007.
Sajib Dasgupta and Vincent Ng. Unsupervised part-
of-speech acquisition for resource-scarce lan-
guages. In Proceedings of the 12th Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), 2007.
Gabriel Doyle and Roger Levy. Combining multi-
ple information types in Bayesian word segmenta-
tion. In Proceedings of NAACL-HLT 2013, pages
117?126, 2013.
39
Micha Elsner, Sharon Goldwater, and Jacob Eisen-
stein. Bootstrapping a unified model of lexical
and phonetic acquisition. In Proceedings of the
50th Annual Meeting of the Association for Com-
putational Linguistics (ACL), 2012.
Naomi Feldman, Thomas Griffiths, and James Mor-
gan. Learning phonetic categories by learning a
lexicon. In Proceedings of the 31st Annual Con-
ference of the Cognitive Science Society (CogSci),
2009.
Michelle A. Fullwood and Timothy J. O?Donnell.
Learning non-concatenative morphology. In Pro-
ceedings of the Workshop on Cognitive Modeling
and Computational Linguistics, 2013.
John Goldsmith. An algorithm for the unsupervised
learning of morphology. Natural Language Engi-
neering, 12(4):353?371, December 2006.
Sharon Goldwater and Thomas L. Griffiths. A
fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the 45th An-
nual Meeting of the Association for Computa-
tional Linguistics (ACL), 2007.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. Interpolating between types and to-
kens by estimating power-law generators. In Ad-
vances in Neural Information Processing Systems
18, 2006.
Aria Haghighi and Dan Klein. Prototype-driven
grammar induction. In Proceedings of the 44th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), 2006.
Harald Hammarstro?m and Lars Borin. Unsupervised
learning of morphology. Computational Linguis-
tics, 37(2):309?350, 2011.
Mark Johnson. Using Adaptor Grammars to iden-
tify synergies in the unsupervised acquisition of
linguistic structure. In Proceedings of the 46th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), 2008.
Mikko Kurimo, Sami Virpioja, and Ville T. Turunen.
Proceedings of the MorphoChallenge 2010 work-
shop. Technical Report TKK-ICS-R37, Aalto
University School of Science and Technology, Es-
poo, Finland, 2010.
Tom Kwiatkowski, Sharon Goldwater, Luke Zettel-
moyer, and Mark Steedman. A probabilistic
model of syntactic and semantic acquisition from
child-directed utterances and their meanings. In
Proceedings of the 13th Conference of the Eu-
ropean Chapter of the Association for Computa-
tional Linguistics (EACL), 2012.
Yoong Keok Lee, Aria Haghighi, and Regina Barzi-
lay. Simple type-level unsupervised POS tagging.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (ACL),
2010.
Yoong Keok Lee, Aria Haghighi, and Regina Barzi-
lay. Modeling syntactic context improves mor-
phological segmentation. In Proceedings of
Fifteenth Conference on Computational Natural
Language Learning, 2011.
Brian MacWhinney. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum Asso-
ciates, Mahwah, NJ, 2000.
Brian MacWhinney, Elizabeth Bates, and Reinhold
Kliegl. Cue validity and sentence interpretation
in English, German, and Italian. Journal of Ver-
bal Learning and Verbal Behavior, 23:127?150,
1984.
Thomas McFadden. On morphological case and
word-order freedom. In Proceedings of the An-
nual Meeting of the Berkeley Linguistics Society,
volume 29, pages 295?306, 2003.
S. Lopez Ornat. La adquisicion de la lengua espag-
nola. Siglo XXI, Madrid, 1994.
Andrew Rosenberg and Julia Hirschberg. V-
measure: A conditional entropy-based external
cluster evaluation measure. In Proceedings of the
12th Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), 2007.
Kairit Sirts and Tanel Aluma?e. A hierarchical
Dirichlet process model for joint part-of-speech
and morphology induction. In Proceedings of
the Conference of the North American Chapter
of the Association for Computational Linguistics
(NAACL), 2012.
Dan Slobin. Universal and particular in the acqui-
sition of language. In Eric Wanner and Lila R.
Gleitman, editors, Language acquisition: the state
40
of the art, pages 128?170. Cambridge University
Press, 1982.
Noah A. Smith and Jason Eisner. Contrastive esti-
mation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics
(ACL), 2005.
Yee Whye Teh. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Pro-
ceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics (ACL),
2006.
Aris Xanthos, Sabine Laaha, Steven Gillis, Ursula
Stephany, Ayhan Aksu-Koc?, Anastasia Christofi-
dou, Natalia Gagarina, Gordana Hrzica, F. Ni-
han Ketrez, Marianne Kilani-Schoch, Katharina
Korecky-Kro?ll, Melita Kovac?evic?, Klaus Laalo,
Marijan Palmovic?, Barbara Pfeiler, Maria D.
Voeikova, and Wolfgang U. Dressler. On the role
of morphological richness in the early develop-
ment of noun and verb inflection. First Language,
31(4):461?479, 2011.
41
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 42?54,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Joint Learning Model of Word Segmentation, Lexical Acquisition,
and Phonetic Variability
Micha Elsner
melsner0@gmail.com
Dept. of Linguistics
The Ohio State University
Sharon Goldwater
sgwater@inf.ed.ac.uk
ILCC, School of Informatics
University of Edinburgh
Naomi H. Feldman
nhf@umd.edu
Dept. of Linguistics
University of Maryland
Frank Wood
fwood@robots.ox.ac.uk
Dept. of Engineering
University of Oxford
Abstract
We present a cognitive model of early lexi-
cal acquisition which jointly performs word
segmentation and learns an explicit model of
phonetic variation. We define the model as a
Bayesian noisy channel; we sample segmen-
tations and word forms simultaneously from
the posterior, using beam sampling to control
the size of the search space. Compared to a
pipelined approach in which segmentation is
performed first, our model is qualitatively more
similar to human learners. On data with vari-
able pronunciations, the pipelined approach
learns to treat syllables or morphemes as words.
In contrast, our joint model, like infant learners,
tends to learn multiword collocations. We also
conduct analyses of the phonetic variations that
the model learns to accept and its patterns of
word recognition errors, and relate these to de-
velopmental evidence.
1 Introduction
By the end of their first year, infants have acquired
many of the basic elements of their native language.
Their sensitivity to phonetic contrasts has become
language-specific (Werker and Tees, 1984), and they
have begun detecting words in fluent speech (Jusczyk
and Aslin, 1995; Jusczyk et al, 1999) and learn-
ing word meanings (Bergelson and Swingley, 2012).
These developmental cooccurrences lead some re-
searchers to propose that phonetic and word learning
occur jointly, each one informing the other (Swingley,
2009; Feldman et al, 2013). Previous computational
models capture some aspects of this joint learning
problem, but typically simplify the problem consid-
erably, either by assuming an unrealistic degree of
phonetic regularity for word segmentation (Goldwa-
ter et al, 2009) or assuming pre-segmented input
for phonetic and lexical acquisition (Feldman et al,
2009; Feldman et al, in press; Elsner et al, 2012).
This paper presents, to our knowledge, the first broad-
coverage model that learns to segment phonetically
variable input into words, while simultaneously learn-
ing an explicit model of phonetic variation that allows
it to cluster together segmented tokens with different
phonetic realizations (e.g., [ju] and [jI]) into lexical
items (/ju/).
We base our model on the Bayesian word segmen-
tation model of Goldwater et al (2009) (henceforth
GGJ), using a noisy-channel setup where phonetic
variation is introduced by a finite-state transducer
(Neubig et al, 2010; Elsner et al, 2012). This in-
tegrated model allows us to examine how solving
the word segmentation problem should affect infants?
strategies for learning about phonetic variability and
how phonetic learning can allow word segmentation
to proceed in ways that mimic the idealized input
used in previous models.
In particular, although the GGJ model achieves
high segmentation accuracy on phonemic (non-
variable) input and makes errors that are qualitatively
similar to human learners (tending to undersegment
the input), its accuracy drops considerably on phonet-
ically noisy data and it tends to oversegment rather
than undersegment. Here, we demonstrate that when
the model is augmented to account for phonetic vari-
ability, it is able to learn common phonetic changes
42
and by doing so, its accuracy improves and its errors
return to the more human-like undersegmentation
pattern. In addition, we find small improvements
in lexicon accuracy over a pipeline model that seg-
ments first and then performs lexical-phonetic learn-
ing (Elsner et al, 2012). We analyze the model?s
phonetic and lexical representations in detail, draw-
ing comparisons to experimental results on adult and
infant speech processing. Taken together, our results
support the idea that a Bayesian model that jointly
performs word segmentation and phonetic learning
provides a plausible explanation for many aspects of
early phonetic and word learning in infants.
2 Related Work
Nearly all computational models used to explore the
problems addressed here have treated the learning
tasks in isolation. Examples include models of word
segmentation from phonemic input (Christiansen et
al., 1998; Brent, 1999; Venkataraman, 2001; Swing-
ley, 2005) or phonetic input (Fleck, 2008; Rytting,
2007; Daland and Pierrehumbert, 2011; Boruta et
al., 2011), models of phonetic clustering (Vallabha
et al, 2007; Varadarajan et al, 2008; Dupoux et al,
2011) and phonological rule learning (Peperkamp et
al., 2006; Martin et al, 2013).
Elsner et al (2012) present a model that is similar
to ours, using a noisy channel model implemented
with a finite-state transducer to learn about phonetic
variability while clustering distinct tokens into lexi-
cal items. However (like the earlier lexical-phonetic
learning model of Feldman et al (2009; in press))
their model assumes known word boundaries, so
to perform both segmentation and lexical-phonetic
learning, they use a pipeline that first segments using
GGJ and then applies their model to the results.
Neubig et al (2010) also present a transducer-
based noisy channel model that performs joint in-
ference on two out of the three tasks we consider
here; their model assumes fixed probabilities for pho-
netic changes (the noise model) and jointly infers
the word segmentation and lexical items, as in our
?oracle? model below (though unlike our system their
model learns from phone lattices rather than a single
transcription). They evaluate only on phone recogni-
tion, not scoring the inferred lexical items.
Recently, Bo?rschinger et al (2013) did present a
? Geom a, b, ..., ju, ... want, ... juwant, ...
Generator for possible words
Probabilities for each word(sparse)p(?i) = .1, p(a) = .05, p(want) = .01...
? contexts
Conditional probabilitiesfor each word after each wordp(?i | want) = .3, p(a | want) = .1, p(want | want) = .0001...
G
Gx?1
n utterances
x1 x2 ... Intended formsju want ? kukiju want ?t...s1 s2 ...
00
Surface formsj? wan ? kukiju wand ?t...T
GGJ09
Figure 1: The graphical model for our system (Eq. 1-
4). Note that the si are not distinct observations; they
are concatenated together into a continuous sequence of
characters which constitute the observations.
joint learner for segmentation, phonetic learning, and
lexical clustering, but the model and inference are
tailored to investigate word-final /t/-deletion, rather
than aiming for a broad coverage system as we do.
3 Model
We follow several previous models of lexical acquisi-
tion in adopting a Bayesian noisy channel framework
(Eq. 1-4; Fig. 1). The model has two components:
a source distribution P (X) over utterances without
phonetic variability X , i.e., intended forms (Elsner et
al., 2012) and a channel or noise distribution T (S|X)
that translates them into the observed surface forms
S. The boundaries between surface forms are then
deterministically removed so that the actual observa-
tions are just the unsegmented string of characters in
the surface forms.
G0|?0, pstop ? DP (?0, Geom(pstop)) (1)
Gx|G0, ?1 ? DP (?1, G0) (2)
Xi|Xi?1 ? GXi?1 (3)
S|X; ? ? T (S|X; ?) (4)
The source model is an exact copy of GGJ1: to
generate the intended-form word sequences X , we
1We use their best reported parameter values: ?0 =
3000, ?1 = 100, pstop = .2 and for unigrams, ?0 = 20.
43
sample a random language model from a hierarchi-
cal Dirichlet process (Teh et al, 2006) with char-
acter strings as atoms. To do so, we first draw a
unigram distribution G0 from a Dirichlet process
prior whose base distribution generates intended form
word strings by drawing each phone in turn until the
stop character is drawn (with probability pstop). Then,
for each possible context word x, we draw a condi-
tional distribution on words following that context
Gx = P (Xi = ?|Xi?1 = x) using G0 as a prior.
Finally, we sample word sequences x1 . . . xn from
the bigram model.
The channel model is a finite transducer with pa-
rameters ? which independently rewrites single char-
acters from the intended string into characters of the
surface string. We use MAP point estimates of these
parameters; single characters (without n-gram con-
text) are used for computational efficiency. Also for
efficiency, the transducer can insert characters into
the surface string, but cannot delete characters from
the intended string. As in several previous phonolog-
ical models (Dreyer et al, 2008; Hayes and Wilson,
2008), the probabilities are learned using a feature-
based log-linear model. For features, we use all the
unigram features from Elsner et al (2012), which
check faithfulness to voicing, place and manner of
articulation (for example, for k ? g, active features
are faith-manner, faith-place, output-g and voiceless-
to-voiced).
Below, we present two methods for learning the
transducer parameters ?. The oracle transducer is es-
timated using the gold-standard word segmentations
and intended forms for the dataset; it represents the
best possible approximation under our model of the
actual phonetics of the dataset. We can also estimate
the transducer using the EM algorithm. We first ini-
tialize a simple transducer by putting small weights
on the faithfulness features to encourage phonologi-
cally plausible changes. With this initial model, we
begin running the sampler used to learn word segmen-
tations. After several hundred sampler iterations, we
start re-estimating the transducer by maximum likeli-
hood after each iteration. We regularize our estimates
by adding 200 pseudocounts for the rewrite x ? x
during training (rather than regularizing the weights
for particular features). We also show segment only
results for a model without the transducer component
(i.e., S = X); this recovers the GGJ baseline.
4 Inference
Inference for this model is complicated for two rea-
sons. First, the hypothesis space is extremely large.
Since we allow the input string to be probabilistically
lengthened, we cannot be sure how long it is, nor
which characters it contains. Second, our hypothe-
ses about nearby characters are highly correlated due
to lexical effects. When deciding how to interpret
[w@nt], if we posit that the intended vowel is /2/, the
word is likely to be /w2n/ ?one? and the next word
begins with /t/ ; if instead we posit that the vowel
is /O/, the word is probably /wOnt/ ?want?. Thus,
inference methods that change only one character at
a time are unlikely to mix well. Since they cannot
simultaneously change the vowel and resegment the
/t/, they must pass through a low-probability inter-
mediate state to get from one state to the other, so
will tend to get stuck in a bad local minimum. A
Gibbs sampler which inserts or deletes a single seg-
ment boundary in each step (Goldwater et al, 2009)
suffers from this problem.
Mochihashi et al (2009) describe an inference
method with higher mobility: a block sampler for
the GGJ model that samples from the posterior over
analyses of a whole utterance at once. This method
encodes the model as a large HMM, using dynamic
programming to select an analysis. We encode our
own model in the same way, constructing the HMM
and composing it with the transducer (Mohri, 2004)
to form a larger finite-state machine which is still
amenable to forward-backward sampling.
4.1 Finite-state encoding
Following Mochihashi et al (2009) and Neubig et
al. (2010), we can write the original GGJ model
as a Hidden Semi-Markov model. States in the
HMM, written ST:[w][C], are labeled with the
previous word w and the sequence of characters C
which have so far been incorporated into the current
word. To produce a word boundary, we transition
from ST:[w][C] to ST:[C][] with probability
P (xi = C|xi?1 = w). We can also add the next
character s to the current word, transitioning from
ST:[w][C] to ST:[w][C : s], at no cost (since
the full cost of the word is paid at its boundary, there
44
? word j?p(j?|[s])d
j u word ju[s]
word j u word up(j|[s]) p(u|j)
p(ju|[s])j/jd/j ?/u
u/u
u/u
Figure 2: A fragment of the composed finite-state machine
for word segmentation and character replacement for the
surface string ju. The start state [s] is followed by a word
boundary (filled circle); the next intended character is
probably j but can be d or others with lower probability.
After j can be a word boundary (forming the intended
word j), or another character such as u, @ or other (not
shown) alternatives.
is no cost for the individual characters)2.
In addition to analyses using known words, we
can also encode the uniform-geometric prior over
unknown words using a finite-state machine. We
can choose to select a word from the prior by tran-
sitioning to a state ST:[Geom][] with probability
P (new word|xi?1 = w) immediately after a word
boundary. While inGeom, we can transition to a new
Geom state and produce any character with uniform
probability P (c) = (1?Pstop) 1|C| ; otherwise, we can
end the word, transitioning to ST:[unk .word][],
with probability Pstop.
This construction is also approximate; it ignores
the possibility that the prior will generate a known
word w, in which case our final transition ought to
be to ST:[w][] instead of ST:[unk .word][]. This
approximation means we do not need to add context
to the Geom state to remember the sequence of char-
acters it produced, which allows us to keep only a
single Geom state on the chart at each timestep.
When we compose this model with the channel
model, the number of states expands. Each state must
now keep track of the previous word, what intended
charactersC have been posited and what surface char-
acters S have been recognized, ST:[w][C][S].
2Though not mentioned by Mochihashi et al (2009) or Neu-
big et al (2010), this construction is not exact, since transitions
in a Bayesian HMM are exchangeable but not independent (Beal
et al, 2001): if a word occurs twice in an utterance, its probabil-
ity is slightly higher the second time. For single utterances, this
bias is small and easy to correct for using a Metropolis-Hastings
acceptance check (Bo?rschinger and Johnson, 2012) using the
path probability from the HMM as the proposal.
To recognize the current word, we transition to
ST:[C][][] with probability P (xi = C|xi?1 =
w). To parse a new surface character s by positing
intended character x (note that x might be ), we
transition to ST:[w][C : x][S : s] with probabil-
ity T (s|x). (As above, we pay no cost for our choice
of x, which is paid for when we recognize the word;
however, we must pay for s.) For efficiency, we do
not allow the G0 states to hypothesize different sur-
face and intended characters, so when we initially
propose an unknown word, it must surface as itself.3
4.2 Beam sampler
This machine has too many states to fully fill the chart
before backward sampling, so we restrict the set of
trajectories under consideration using beam sampling
(Van Gael et al, 2008) and simulated annealing.
The beam sampler is closely related to the standard
beam search technique, which uses a probability cut-
off to discard parts of the FST which are unlikely to
figure in the eventual solution. Unlike conventional
beam search, the sampler explores using stochastic
cutoffs, so that all trajectories are explored, but most
of the bad ones are explored infrequently, leading to
higher efficiency.
We design our beam sampler to restrict the set
of potential intended characters at each timestep.
In particular, given a stream of input characters
S = s1 . . . sn, we introduce a set of auxiliary cutoff
variables U = u1 . . . un. The ui variables represent
limits on the probability of the emission of surface
character si; we exclude any hypothesized xi whose
probability of generating si, T (si|xi), is less than
ui. To create a beam sampling scheme, we must de-
vise a distribution for U given a state sequence Q (as
discussed above, the sequence of states encodes the
intended character sequence and the segmentation
of the surface string), Pu(U |Q) and then incorporate
the probability of U into the forward messages.
If qi is the state in Q at which si is generated, and
xi the corresponding intended character, we require
that Pu < T (si|xi); that is, the cutoffs must not
exclude any states in the sequence Q. We define Pu
3Again, this approximation is corrected for by the Metropolis-
Hastings step.
45
as a ?-mixture of two distributions:
Pu(u|si, xi) = ?U [0,min(.05, T (si|xi))]+
(1? ?)T (si|xi)Beta(5, 1e? 5)
The former distribution is quite unrestrictive, while
the latter prefers to prune away nearly all the states.
Thus, for most characters in the string, we do not
permit radical changes, while for a fraction, we do.
We follow Huggins and Wood (2013), who ex-
tended Van Gael et al (2008) to the case of a non-
uniform Pu, to define our forward message ? as:
?(qi, i) ? P (qi, S0..i, U0..i) (5)
=
?
qi?1
Pu(ui|si, xi)T (si|xi)?(qi?1, i? 1)
This is the standard HMM forward message, aug-
mented with the probability of u. Since Pu(?|si, xi)
is required to be less than T (si|xi), it will be 0 when-
ever T (si|xi) < u; this is how the u variables func-
tion as cutoffs. In practice, we use the u variables to
filter the lexical items that begin at each position i
in advance, using a simple 0/1 edit distance Markov
model which runs faster than our full model. (For ex-
ample, we can quickly check if the current U allows
want as the intended form for wOlk at i; if not, we can
avoid constructing the prefix ST:[xi?1][wa][wO]
since the continuation will fail.)
The algorithm?s speed depends on the size and
uncertainty of the inferred LM: large numbers of
plausible words mean more states to explore. When
inference starts, and the system is highly uncertain
about word boundaries, it is therefore reasonable to
limit the exploration of the character sequence. We
do so by annealing in two ways: as in Goldwater
et al (2009), we raise P (X) (Eq. 3) to a power t
which increases linearly from .3. To sample from
the posterior, we would want to end with t = 1, but
as in previous noisy-channel models (Elsner et al,
2012; Bahl et al, 1980) we get better results when we
emphasize the LM at the expense of the channel and
so end at t = 2. Meanwhile, as t rises and we explore
fewer implausible lexical sequences, we can explore
the character sequence more. We begin by setting
the ? interpolation parameter of Pu to 0 to minimize
exploration and increase it linearly to .3 (allowing
the system to change about a third of the characters
on each sweep). This is similar to the scheme for
altering Pu in Huggins and Wood (2013).
4.3 Dataset and metrics
We use the corpus released by Elsner et al (2012),
which contains 9790 child-directed English utter-
ances originally from the Bernstein-Ratner corpus
(Bernstein-Ratner, 1987) and later transcribed phone-
mically (Brent, 1999). This standard word segmenta-
tion dataset was modified by Elsner et al (2012) to
include phonetic variation by assigning each token a
pronunciation independently selected from the empir-
ical distribution of pronunciations of that word type
in the closely-transcribed Buckeye Speech Corpus
(Pitt et al, 2007). Following previous work, we hold
out the last 1790 utterances as unseen test data during
development. In the results presented here, we run
the model on all 9790 utterances but score only these
1790. We average results over 5 runs of the model
with different random seeds.
We use standard metrics for segmentation and lex-
icon recovery. For segmentation, we report precision,
recall and F-score for word boundaries (bds), and for
the positions of word tokens in the surface string (srf ;
both boundaries must be correct).
For normalization of the pronunciation variation,
we follow Elsner et al (2012) in measuring how well
the system clusters together variant pronunciations
of the same lexical item, without insisting that the
intended form the system proposes for them match
the one in our corpus. For example, if the system
correctly clusters [ju] and [jI] together but assigns
them the incorrect intended form /jI/, we can still
give credit to this cluster if it is the one that overlaps
best with the gold-standard /ju/ cluster. To compute
these scores, we find the optimal one-to-one map-
ping between our clusters of pronunciations and the
true lexical entries, then report scores for mapped to-
kens (mtk; boundaries and mapping to gold standard
cluster must be correct) and mapped types4 (mlx).
4Elsner et al (2012) calls the mlx metric lexicon F, which
is possibly confusing. We map the clusters to a gold-standard
lexicon (plus potentially some words that don?t correspond to
anything in the gold standard) and compute a type-level F-score
on this lexicon.
46
Prec Rec F-score
Pipeline (segment, then cluster): (Elsner et al, 2012)
Bds 70.4 93.5 80.3
Srf 56.5 69.7 62.4
Mtk 44.2 54.5 48.8
Mlx 48.6 43.1 45.7
Bigram model, segment only
Bds 73.9 (-0.6:0.7) 91.0 (-0.6:0.4) 81.6 (-0.5:0.6)
Srf 60.8 (-0.7:1.1) 70.8 (-0.8:0.9) 65.4 (-0.6:1.0)
Mtk 41.6 (-0.6:1.2) 48.4 (-0.5:1.2) 44.8 (-0.6:1.2)
Mlx 36.6 (-0.7:0.8) 49.8 (-1.0:0.8) 42.2 (-0.9:0.8)
Unigram model, oracle transducer
Bds 81.4 (-0.8:0.4) 72.1 (-0.9:0.8) 76.4 (-0.5:0.7)
Srf 63.6 (-1.0:1.1) 58.5 (-1.2:1.2) 60.9 (-0.9:1.2)
Mtk 46.8 (-1.0:1.1) 43.0 (-1.1:1.2) 44.8 (-1.0:1.2)
Mlx 56.7 (-1.1:1.0) 47.6 (-1.4:0.8) 51.7 (-1.2:0.8)
Bigram model, oracle transducer
Bds 76.1 (-0.6:0.6) 83.8 (-0.9:1.0) 79.8 (-0.8:0.4)
Srf 62.2 (-0.9:1.0) 66.7 (-1.2:1.1) 64.4 (-1.1:0.8)
Mtk 47.2 (-0.7:0.9) 50.6 (-1.0:0.8) 48.8 (-0.8:0.7)
Mlx 40.1 (-1.0:1.2) 43.7 (-0.6:0.7) 41.8 (-0.8:0.6)
Bigram model, EM transducer
Bds 80.1 (-0.5:0.8) 83.0 (-1.4:1.3) 81.5 (-0.5:0.7)
Srf 66.1 (-0.8:1.4) 67.8 (-1.4:1.7) 66.9 (-0.9:1.4)
Mtk 49.0 (-0.9:0.7) 50.3 (-1.1:1.4) 49.6 (-1.0:1.0)
Mlx 43.0 (-1.0:1.4) 49.5 (-1.5:1.1) 46.0 (-1.0:1.3)
Table 1: Mean segmentation (bds, srf ) and normalization
(mtk, mlx) scores on the test set over 5 runs. Parentheses
show min and max scores as differences from the mean.
5 Results and discussion
In the following sections, we analyze how our model
with variability compares to GGJ on noisy data. We
give quantitative scores and also show that qualitative
patterns of errors are often similar to those of human
learners and listeners.
5.1 Clean versus variable input
We begin by evaluating our model as a word seg-
mentation system. (Table 1 gives segmentation and
normalization scores for various models and base-
lines on the 1790 test utterances.) We first confirm
that our inference method is reasonable. The bigram
model without variability (?segment only?) should
have the same segmentation performance as the stan-
dard dpseg implementation of GGJ. This is the case:
dpseg has boundary F of 80.3 and token F of 62.4;
we get 81.6 and 65.4. Thus, our sampler is finding
good solutions, at least for the no-variability model.
We compare segmentation scores between the
?segment only? system and the two bigram models
with transducers (?oracle? and ?EM?). While these
systems all achieve similar segmentation scores, they
do so in different ways. ?Segment only? finds a so-
lution with boundary precision 73.9% and boundary
recall 91.0% for a total F of 81.6%. The low pre-
cision and high recall here indicate a tendency to
oversegment; when the analysis of a given subse-
quence is unclear, the system prefers to chop it into
small chunks. The bigram models which incorporate
transducers score P : 76.1, R: 83.8 (oracle) and P :
80.1,R: 83.0 (EM), indicating that they prefer to find
longer sequences (undersegment) more.
In previous experiments on datasets without varia-
tion, GGJ also has a strong tendency to undersegment
the data (boundary P : 90.1, R: 80.3), which Gold-
water et al argue is rational behavior for an ideal
learner seeking a parsimonious explanation for the
data. Undersegmentation occurs especially when ig-
noring lexical context (a unigram model), but to some
extent even in bigram models. Human learners also
tend to learn collocations as single words (Peters,
1983; Tomasello, 2000), and the GGJ model has been
shown to capture several other effects seen in labora-
tory segmentation tasks (Frank et al, 2010). Together,
these findings support the idea that human learners
may behave in important respects like the Bayesian
ideal learners that Goldwater et al presented.
However, experiments on data with variation have
called these conclusions into question. In particu-
lar, GGJ has previously been shown to oversegment
rather than undersegment as the input grows noisier
(Fleck, 2008), and our results replicate this finding
(oversegmentation for the ?segment only? model).
In addition, the GGJ bigram model, which achieves
much higher segmentation accuracy than the unigram
model on clean data, actually performs worse on very
noisy data (Jansen et al, 2013). Infants are known to
track statistical dependencies across words (Go?mez
and Maye, 2005), so it is worrisome that these de-
pendencies hurt GGJ?s segmentation accuracy when
learning from noisy data.
Our results show that modeling phonetic variabil-
ity reverses the problematic trends described above.
Although the models with phonetic variability show
similar overall segmentation accuracy on noisy data
to the original GGJ model, the pattern of errors
changes, with less oversegmentation and more un-
47
dersegmentation. Thus, their qualitative performance
on variable data resembles GGJ?s on clean data, and
therefore the behavior of human learners.
5.2 Phonetic variability
We next analyze the model?s ability to normalize vari-
ations in the pronunciation of tokens, by inspecting
the mtk score. The ?segment only? baseline is pre-
dictably poor, F : 44.8. The pipeline model scores
48.8, and our oracle transducer model matches this
exactly. The EM transducer scores better, F : 49.6.
Although the confidence intervals overlap slightly,
the EM system also outperforms the pipeline on the
other F -measures; altogether, these results suggest
at least a weak learning synergy (Johnson, 2008) be-
tween segmentation and phonetic learning.
It is interesting that EM can perform better than
the oracle. However, EM is more conservative about
which sound changes it will allow, and thus tends to
avoid mistakes caused by the simplicity of the trans-
ducer model. Since the transducer works segment-
by-segment, it can apply rare contextual variations
out of context. EM benefits from not learning these
variations to begin with.
We can also compare the bigram and unigram ver-
sions of the model. The unigram model is a rea-
sonable segmenter, though not quite as good as the
bigram model, with boundary F of 76.4 and token
F of 60.9 (compared to 79.8 and 64.4 using the bi-
gram model). However, it is not good at normalizing
variation; its mtk score is comparable to the baseline
at 44.8%5. Although bigram context is only moder-
ately effective for telling where words are, the model
seems heavily reliant on lexical context to decide
what words it is hearing.
5.3 Error analysis
To gain more insight into the differing behavior of
our model versus a pipelined system, we inspect the
intended word strings X proposed by each one in
detail. Below, we categorize the kinds of intended
word strings that the model might propose to span a
given gold-standard word token:
Correct Correctly segmented, mapped to the correct
lexical item (e.g., gold intended /ju/, surface
5Elsner et al (2012) show a similar result for a unigram
version of their pipelined system.
EM-learned Segment only
Correct 49.88 47.61
Wrong form 17.96 23.73
Collocation 14.25 7.59
Split 8.26 15.18
One bound 7.11 15.18
Corr. colloc. 1.35 < 0.01
Other 0.75 0.22
Corr. split 0.43 0.66
Table 2: Distribution (%) of error types (see text) in a
single run on the full dataset.
segmentation [ju], intended /ju/)
Wrong form Correctly segmented, mapped to the
wrong lexical item (/ju/, surf. [ju], int. /jEs/)
Colloc Missegmented as part of a sequence whose
boundaries correspond to real word boundaries
(/ju?want/, surf. [juwant], int. /juwant/)
Corr. colloc As above, but proposed lexical item
maps to this word (/ar?ju/, surf. [arj@] int.
/ju/)
Split Missegmented with a word-internal boundary
(/dOgiz/, surf. [dO?giz], int. /dO?giz/)
Corr. split As above, but one proposed word maps
correctly (/dOgi/, surf. [dOg?i], int. /dOgi?@/)
One boundary One boundary correct, the other
wrong (/ju?wa. . ./, surf. [juw], int. /juw/)
Other Not a collocation, both boundaries are wrong
(/du?ju?wa. . ./, surf. [ujuw], int. /ujuw/)
Table 2 shows the distribution over intended word
strings proposed by the ?segment only? baseline and
the EM-learned transducer. Both systems propose
a large number of correct forms, and the most com-
mon error category is ?wrong form? (lexical error
without segmentation error), an error which could
potentially be repaired in a pipeline system. How-
ever, the remaining errors represent segmentation
mistakes which a pipeline could not repair. Here
the two systems behave quite differently. The EM-
learned transducer analyses 14% of real tokens as
parts of multiword collocations like ?doyou?; in an-
other 1.35%, the underlying content word is even
correctly detected. The non-variable system, on the
other hand, analyses 15% of real tokens by splitting
them into pieces. Since infant learners tend to learn
collocations, this supports our analysis that the model
with variation better models human behavior.
48
EM ju: 805, duju: 239, juwan: 88, jI: 58, e~ju: 54, judu:
47, j?: 39, jul2k: 39, Su: 30, u: 23, Zu: 18, j: 17,
je~: 16, tSu: 15, aj:15, Derjugo: 12, dZu: 12
GGJ ju: 498, jI: 280, j@: 165, ji: 119, duju: 106, dujI: 44,
kInju: 39, i: 32, u: 29, kInjI: 29, jul2k: 24, juwan:
23, j: 22, Su: 19, jU: 18, e~ju: 18, I:16, Zu: 15, dZ?u:
13, jE: 12, SI: 11, T?Nkju: 11
Table 3: Forms proposed with frequency > 10 for
gold-standard tokens of ?you? in one sample from EM-
transducer and segment-only (GGJ) system.
To illustrate this behavior anecdotally, we present
the distribution of intended word strings spanning
tokens whose gold intended form is /ju/ ?you? (Table
3). The EM-learned solution proposes 805 tokens
of /ju/, which is the correct analysis6; the ?segment
only? system instead finds varying forms like /jI/,
/j?/ etc. This is unsurprising and could be repaired
by a suitable pipelined system. However, the EM
system also proposes 239 instances of ?doyou?, 88
instances of ?youwant?, 54 instances of ?areyou? and
several other collocations. The ?segment only? sys-
tem finds some of these collocations, split into dif-
ferent versions: for instance 106 instances of /duju/
and 44 of /dujI/. In a pipelined system, we could
combine these variants to find 150 instances? but
this is still 89 instances short of the 239 found when
allowing for variability. The same pattern holds for
?youlike? and ?youwant?. Because the non-variable
system must learn each variant separately, it learns
only the most common instances of these long collo-
cations, and analyzes infrequent variants differently.
We also perform this analysis specifically for
words beginning with vowels. Infants show a delay
in their ability to segment these words from continu-
ous speech (Mattys and Jusczyk, 2001; Nazzi et al,
2005; Seidl and Johnson, 2008), and Seidl and John-
son (2008) suggest a perceptual explanation? initial
vowels can be hard to hear and often exhibit variation
due to coarticulation or resyllabification. Although
our dataset does not contain coarticulation as such, it
should show this pattern of greater variation, which
we hypothesize might lead to difficulty in segmenting
and recognizing vowel-initial words.
The model?s behavior is consistent with this hy-
pothesis (Table 4). Both the ?segment only? and
EM transducer models find approximately the same
6Not all the variants are merged, however. jI, j?, Su etc. are
still occasionally analyzed as separate lexical items.
Segment only Vow. init Cons. init
Correct 47.5 51.7
Wrong form 18.6 15.7
Collocation 14.6 12.2
Split 6.2 10.8
Right bd. corr. 5.8 3.6
Left bd. corr. 4.6 3.8
EM transducer Vow. init Cons. init
Correct 41.5 52.1
Wrong form 20.4 17.3
Collocation 19.2 12.5
Split 5.2 9.1
Right bd. corr. 6.2 2.7
Left bd. corr. 2.7 3.1
Table 4: Most common error types (%; see text) for in-
tended forms beginning with vowels or consonants. Rare
error types are not shown. ?One bound? errors are split up
by which boundary is correct.
proportion of vowel-initial tokens, and both systems
do somewhat better on consonant-initial words than
vowel-initial words. The advantage is stronger for
the transducer model, which gets only 41.5% of
vowel-initial tokens correct as opposed to 52.1% of
consonant-initial words. It proposes more colloca-
tions for vowel-initial words (19.2%) than for conso-
nants (12.5%). In cases where they do not propose a
collocation, both systems are somewhat more likely
to find the right boundary of a vowel-initial token
than the left boundary (although again this difference
is larger for the EM system); this suggests that the
problem is indeed caused by the initial segment.
5.4 Phonetic Learning
We next compare phonetic variations learned by the
model to characteristics of infant speech perception.
Infants show an asymmetry between consonants and
vowels, losing sensitivity to non-native vowel con-
trasts by eight months (Kuhl et al, 1992; Bosch
and Sebastia?n-Galle?s, 2003) but to non-native con-
sonant contrasts only by 10-12 months (Werker and
Tees, 1984). The observed ordering is somewhat
puzzling when one considers the availability for dis-
tributional information (Maye et al, 2002), which is
much stronger for stop consonants than for vowels
(Lisker and Abramson, 1964; Peterson and Barney,
1952). Infants are also conservative in generalizing
across phonetic variability, showing a delayed abil-
49
ity to generalize across talkers, affects, and dialects.
They have difficulty recognizing word tokens that are
spoken by a different talker or in a different tone of
voice until 11 months (Houston and Jusczyk, 2000;
Singh et al, 2004), and the ability to adapt to unfa-
miliar dialects appears to develop even later, between
15 and 19 months (Best et al, 2009; Heugten and
Johnson, in press; White and Aslin, 2011).
Similar to infants, our model shows both a vowel-
consonant asymmetry and a reluctance to accept the
full range of adult phonetic variability. Table 5 shows
some segment-to-segment alternations learned in var-
ious transducers. The oracle learns a large amount
of variation (u surfaces as itself only 68% of the
time) involving many different segments, whereas
EM is similar to infant learners in learning a more
conservative solution with fewer alternations over-
all. Moreover, EM appears to identify patterns of
variability in vowels before consonants. It learns a
similar range of alternations for u as in the oracle,
although it treats the sound as less variable than it
actually is. It learns much less variability for con-
sonants; it picks up the alternation of D with s and
z, but predicts that D will surface as itself 91% of
the time when the true figure is only 69%. And it
fails to learn any meaningful alternations involving
k. These results suggest that patterns of variability in
vowels are more evident than patterns of variability
in consonants when infants are beginning to solve the
word segmentation problem.
To investigate the effect of data size on this con-
servativism, we ran the system on 1000 utterances
instead of 9790. This leads to an even more conser-
vative solution, with variations for u but none of the
others (although i and D still vary more than k).
5.5 Segmentation and recognition errors
A particularly interesting set of errors are those that
involve both a missegmentation and a simultaneous
misrecognition, since the joint model is prone to
such errors while the pipelined model is not. Rel-
atively little is known about infants? misrecognitions
of words in fluent speech, although it is clear that they
find words in medial position harder (Plunkett, 2005;
Seidl and Johnson, 2006). However, adults make
missegmentation/misrecognition errors fairly often,
especially when listening to noisy audio (Butterfield
and Cutler, 1988). Such errors are more common
System x top 4 outputs s
Oracle
u u .68 @ .05 a .04 U .04
i i .85 I .03 @ .03 E .02
D D .69 s .07 [?] .07 z .04
k k .93 d .02 g .02
[?] r .21 h .11 d .01 @ .07
EM
(full)
u u .75 @ .08 I .04 U .03
i i .90 I .04 E .02
D D .91 s .03 z 0.1
k k .98
[?] @ .32 I .14 n .13 t .13
EM
(only
1000
utts)
u u .82 I .04 @ .04 a .02
i i .97
D D .95
k k .99
[?] @ .21 I .18 t .12 s .12
Table 5: Learned phonetic alternations: top 4 outputs s
with p > .001 for inputs x = uw (/u/ ), iy (/i/ ), dh (/D/ ),
k (/k/) and [?], the null character. Outputs from [?] are
insertions. The oracle allows [?] as an output (deletion)
but for computational reasons, the model does not.
when the misrecognized word belongs to a prosod-
ically rare class and when the incorrectly hypothe-
sized string contains frequent words (Cutler, 1990);
phonetically ambiguous words are also more com-
monly recognized as the more frequent of two op-
tions (Connine et al, 1993). For the indefinite article
?a? (often reduced to [@]), lexical context is the main
factor in deciding between ambiguous interpretations
(Kim et al, 2012). In rapid speech, listeners have few
phonetic cues to indicate whether it is present at all
(Dilley and Pitt, 2010). Below, we analyze various
misrecognitions made by our system (using the EM
transducer), and find some similar effects.
The easiest cases to analyze are those with no mis-
segmentation: the proposed boundaries are correct,
and the proposed lexical entry corresponds to a real
word7, but not the correct one. Most of them corre-
spond to homophones (Table 6).
Common cases with a missegmentation include it
and is, a and is, it?s and is, who, who?s and whose,
that?s and what?s, and there and there?s. In general,
these errors involve words which sometimes appear
7The one-to-one mapping can be misleading, as it may map
a large cluster to a real word on the basis of one or two tokens if
all other tokens correspond to a different word already used for
another cluster. We manually filter out a few cases like this.
50
Actual proposed count
/tu/ ?two? /t@/ ?to? 95
/kin/ ?can? /k?nt/ ?can?t? 67
/En/ ?and? /?n/ ?an? 61
/hIz/ ?his? /Iz/ ?is? 57
/D@/ ?the? /@/ ?ah? 51
/w@ts/ ?what?s? /wants/ ?wants? 40
/wan/ ?want? /won/ ?won?t? 39
/yu/ ?you? /y?/ ?yeah? 39
/f@~/ ?for? /fOr/ ?four? 30
/hir/ ?here? /hil/ ?he?ll? 28
Table 6: Top ten errors involving confusion between real,
correctly segmented words: the most common pronunci-
ation of the actual token and its orthographic form, the
same for the proposed token, and the frequency.
with a morpheme or clitic (which can easily be mis-
segmented as part of something else), words which
differ by one segment, and frequent function words
which often appear in similar contexts. These tenden-
cies match those shown by adult human listeners.
A particularly distinctive set of joint recognition
and segmentation errors are those where an entire
real token is treated as phonetic ?noise?? that is, it
is segmented along with an adjacent word, and the
system clusters the whole sequence as a token of
that word. The most common examples are ?that?s a?
identified as ?that?s?, ?have a? identified as ?have?,
?sees a? identified as ?sees? and other examples in-
volving ?a?, a word which also frequently confuses
humans (Kim et al, 2012; Dilley and Pitt, 2010).
However, there are also instances of ?who?s in? as
?who?s?, ?does it? as ?does?, and ?can you? as ?can?.
6 Conclusion
We have presented a model that jointly infers word
segmentation, lexical items, and a model of phonetic
variability; we believe this is the first model to do
so on a broad-coverage naturalistic corpus8. Our re-
sults show a small improvement in both segmentation
and normalization over a pipeline model, providing
evidence for a synergistic interaction between these
learning tasks and supporting claims of interactive
learning from the developmental literature on infants.
We also reproduced several experimental findings;
our results suggest that two vowel-consonant asym-
8Software is available from the ACL archive; updated
versions may be posted at https://bitbucket.org/
melsner/beamseg.
metries, one from the word segmentation literature
and another from the phonetic learning literature, are
linked to the large variability in vowels found in nat-
ural corpora. The model?s correspondence with hu-
man behavioral results is by no means exact, but we
believe these kinds of predictions might help guide
future research on infant phonetic and word learning.
Acknowledgements
Thanks to Mary Beckman for comments. This work
was supported by EPSRC grant EP/H050442/1 to the
second author.
References
Lalit Bahl, Raimo Bakis, Frederick Jelinek, and Robert
Mercer. 1980. Language-model/acoustic-channel-
model balance mechanism. Technical disclosure bul-
letin Vol. 23, No. 7b, IBM, December.
Matthew J. Beal, Zoubin Ghahramani, and Carl Edward
Rasmussen. 2001. The infinite Hidden Markov Model.
In NIPS, pages 577?584.
Elika Bergelson and Daniel Swingley. 2012. At 6-9
months, human infants know the meanings of many
common nouns. Proceedings of the National Academy
of Sciences, 109:3253?3258.
Nan Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck, editors,
Children?s Language, volume 6. Erlbaum, Hillsdale,
NJ.
Catherine T. Best, Michael D. Tyler, Tiffany N. Good-
ing, Corey B. Orlando, and Chelsea A. Quann. 2009.
Development of phonological constancy: Toddlers? per-
ception of native- and jamaican-accented words. Psy-
chological Science, 20(5):539?542.
Benjamin Bo?rschinger and Mark Johnson. 2012. Using
rejuvenation to improve particle filtering for Bayesian
word segmentation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 85?89, Jeju Island,
Korea, July. Association for Computational Linguistics.
Benjamin Bo?rschinger, Mark Johnson, and Katherine De-
muth. 2013. A joint model of word segmentation
and phonological variation for English word-final /t/-
deletion. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics, Sofia,
Bulgaria, August. Association for Computational Lin-
guistics.
Luc Boruta, Sharon Peperkamp, Beno??t Crabbe?, and Em-
manuel Dupoux. 2011. Testing the robustness of online
word segmentation: Effects of linguistic diversity and
51
phonetic variation. In Proceedings of the 2nd Workshop
on Cognitive Modeling and Computational Linguistics,
pages 1?9.
Laura Bosch and Nu?ria Sebastia?n-Galle?s. 2003. Simulta-
neous bilingualism and the perception of a language-
specific vowel contrast in the first year of life. Lan-
guage and Speech, 46(2-3):217?243.
Michael R. Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discovery.
Machine Learning, 34:71?105, February.
Sally Butterfield and Anne Cutler. 1988. Segmentation
errors by human listeners: Evidence for a prosodic
segmentation strategy. In Proceedings of SPEECH
?88: Seventh Symposium of the Federation of Acoustic
Societies of Europe, vol. 3, pages 827?833, Edinburgh.
Morten H. Christiansen, Joseph Allen, and Mark S. Sei-
denberg. 1998. Learning to Segment Speech Using
Multiple Cues: A Connectionist Model. Language and
Cognitive Processes, 13(2/3):221?269.
C. M. Connine, D. Titone, and J. Wang. 1993. Audi-
tory word recognition: Extrinsic and intrinsic effects of
word frequency. Journal of Experimental Psychology:
Learning, Memory and Cognition, 19:81?94.
Anne Cutler. 1990. Exploiting prosodic probabilities in
speech segmentation. In G. A. Altmann, editor, Cog-
nitive models of speech processing: Psycholinguistic
and computational perspectives, pages 105?121. MIT
Press, Cambridge, MA.
Robert Daland and Janet B. Pierrehumbert. 2011. Learn-
ing diphone-based segmentation. Cognitive Science,
35(1):119?155.
Laura C. Dilley and Mark Pitt. 2010. Altering context
speech rate can cause words to appear or disappear.
Psychological Science, 21(11):1664?1670.
Markus Dreyer, Jason R. Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions with
finite-state methods. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing,
EMNLP ?08, pages 1080?1089, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Emmanuel Dupoux, Guillaume Beraud-Sudreau, and
Shigeki Sagayama. 2011. Templatic features for mod-
eling phoneme acquisition. In Proceedings of the 33rd
Annual Cognitive Science Society.
Micha Elsner, Sharon Goldwater, and Jacob Eisenstein.
2012. Bootstrapping a unified model of lexical and pho-
netic acquisition. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 184?193, Jeju
Island, Korea, July. Association for Computational Lin-
guistics.
Naomi Feldman, Thomas Griffiths, and James Morgan.
2009. Learning phonetic categories by learning a lexi-
con. In Proceedings of the 31st Annual Conference of
the Cognitive Science Society.
Naomi H. Feldman, Emily B. Myers, Katherine S. White,
Thomas L. Griffiths, and James L. Morgan. 2013.
Word-level information influences phonetic learning
in adults and infants. Cognition, 127(3):427?438.
Naomi H. Feldman, Thomas L. Griffiths, Sharon Gold-
water, and James L. Morgan. in press. A role for the
developing lexicon in phonetic category acquisition.
Psychological Review.
Margaret M. Fleck. 2008. Lexicalized phonotactic word
segmentation. In Proceedings of ACL-08: HLT, pages
130?138, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Michael C. Frank, Sharon Goldwater, Thomas L. Griffiths,
and Joshua B. Tenenbaum. 2010. Modeling human per-
formance in statistical word segmentation. Cognition,
117(2):107?125.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21?54.
Rebecca Go?mez and Jessica Maye. 2005. The develop-
mental trajectory of nonadjacent dependency learning.
Infancy, 7:183?206.
Bruce Hayes and Colin Wilson. 2008. A maximum en-
tropy model of phonotactics and phonotactic learning.
Linguistic Inquiry, 39(3):379?440.
Marieke van Heugten and Elizabeth K. Johnson. in press.
Learning to contend with accents in infancy: Benefits
of brief speaker exposure. Journal of Experimental
Psychology: General.
Derek M. Houston and Peter W. Jusczyk. 2000. The role
of talker-specific information in word segmentation by
infants. Journal of Experimental Psychology: Human
Perception and Performance, 26:1570?1582.
Jonathan Huggins and Frank Wood. 2013. Infinite struc-
tured hidden semi-Markov models. Transactions on
Pattern Analysis and Machine Intelligence (TPAMI), to
appear, September.
Aren Jansen, Emmanuel Dupoux, Sharon Goldwater,
Mark Johnson, Sanjeev Khudanpur, Kenneth Church,
Naomi Feldman, Hynek Hermansky, Florian Metze,
Richard Rose, Mike Seltzer, Pascal Clark, Ian McGraw,
Balakrishnan Varadarajan, Erin Bennett, Benjamin
Borschinger, Justin Chiu, Ewan Dunbar, Abdellah Four-
tassi, David Harwath, Chia-ying Lee, Keith Levin,
Atta Norouzian, Vijay Peddinti, Rachael Richardson,
Thomas Schatz, and Samuel Thomas. 2013. A sum-
mary of the 2012 JHU CLSP workshop on zero re-
source speech technologies and early language acqui-
sition. Proceedings of the IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing.
52
Mark Johnson. 2008. Using adaptor grammars to identify
synergies in the unsupervised acquisition of linguis-
tic structure. In Proceedings of ACL-08: HLT, pages
398?406, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Peter W. Jusczyk and Richard N. Aslin. 1995. Infants? de-
tection of the sound patterns of words in fluent speech.
Cognitive Psychology, 29:1?23.
Peter W. Jusczyk, Derek M. Houston, and Mary Newsome.
1999. The beginnings of word segmentation in English-
learning infants. Cognitive Psychology, 39:159?207.
Dahee Kim, Joseph D.W. Stephens, and Mark A. Pitt.
2012. How does context play a part in splitting words
apart? Production and perception of word boundaries
in casual speech. Journal of Memory and Language,
66(4):509 ? 529.
Patricia K. Kuhl, Karen A. Williams, Francisco Lacerda,
Kenneth N. Stevens, and Bjorn Lindblom. 1992. Lin-
guistic experience alters phonetic perception in infants
by 6 months of age. Science, 255(5044):606?608.
Leigh Lisker and Arthur S. Abramson. 1964. A cross-
language study of voicing in initial stops: Acoustical
measurements. Word, 20:384?422.
Andrew Martin, Sharon Peperkamp, and Emmanuel
Dupoux. 2013. Learning phonemes with a proto-
lexicon. Cognitive Science, 37:103?124.
Sven L. Mattys and Peter W. Jusczyk. 2001. Do infants
segment words or recurring contiguous patterns? Jour-
nal of Experimental Psychology: Human Perception
and Performance, 27(3):644?655+.
Jessica Maye, Janet F. Werker, and LouAnn Gerken. 2002.
Infant sensitivity to distributional information can affect
phonetic discrimination. Cognition, 82(3):B101?11.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested pitman-yor language modeling. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
100?108, Suntec, Singapore, August. Association for
Computational Linguistics.
Mehryar Mohri, 2004. Weighted Finite-State Transducer
Algorithms: An Overview, chapter 29, pages 551?564.
Physica-Verlag.
Thierry Nazzi, Laura C. Dilley, Ann Marie Jusczyk, Ste-
fanie Shattuck-Hufnagel, and Peter W. Jusczyk. 2005.
English-learning infants? segmentation of verbs from
fluent speech. Language and Speech, 48(3):279?298+.
Graham Neubig, Masato Mimura, Shinsuke Mori, and
Tatsuya Kawahara. 2010. Learning a language model
from continuous speech. In 11th Annual Conference
of the International Speech Communication Associa-
tion (InterSpeech 2010), pages 1053?1056, Makuhari,
Japan, 9.
Sharon Peperkamp, Rozenn Le Calvez, Jean-Pierre Nadal,
and Emmanuel Dupoux. 2006. The acquisition of
allophonic rules: Statistical learning with linguistic
constraints. Cognition, 101(3):B31?B41.
Ann M. Peters. 1983. The Units of Language Acquisi-
tion. Cambridge Monographs and Texts in Applied
Psycholinguistics. Cambridge University Press.
Gordon E. Peterson and Harold L. Barney. 1952. Control
methods used in a study of the vowels. Journal of the
Acoustical Society of America, 24(2):175?184.
Mark A. Pitt, Laura Dilley, Keith Johnson, Scott Kies-
ling, William Raymond, Elizabeth Hume, and Eric
Fosler-Lussier. 2007. Buckeye corpus of conversa-
tional speech (2nd release).
Kim Plunkett. 2005. Learning how to be flexible with
words. Attention and Performance, XXI:233?248.
Anton Rytting. 2007. Preserving Subsegmental Varia-
tion in Modeling Word Segmentation (Or, the Raising
of Baby Mondegreen). Ph.D. thesis, The Ohio State
University.
Amanda Seidl and Elizabeth Johnson. 2006. Infant word
segmentation revisited: Edge alignment facilitates tar-
get extraction. Developmental Science, 9:565?573.
Amanda Seidl and Elizabeth Johnson. 2008. Perceptual
factors influence infants? extraction of onsetless words
from continuous speech. Journal of Child Language,
34.
Leher Singh, James Morgan, and Katherine White. 2004.
Preference and processing: The role of speech affect
in early spoken word recognition. Journal of Memory
and Language, 51:173?189.
Daniel Swingley. 2005. Statistical clustering and the con-
tents of the infant vocabulary. Cognitive Psychology,
50:86?132.
Daniel Swingley. 2009. Contributions of infant word
learning to language development. Philosophical
Transactions of the Royal Society B: Biological Sci-
ences, 364(1536):3617?3632, December.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the Ameri-
can Statistical Association, 101(476):1566?1581.
Michael Tomasello. 2000. The item-based nature of chil-
dren?s early syntactic development. Trends in Cognitive
Sciences, 4(4):156 ? 163.
Gautam K. Vallabha, James L. McClelland, Ferran Pons,
Janet F. Werker, and Shigeaki Amano. 2007. Unsuper-
vised learning of vowel categories from infant-directed
speech. Proceedings of the National Academy of Sci-
ences, 104(33):13273?13278.
Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and
Zoubin Ghahramani. 2008. Beam sampling for the
infinite Hidden Markov model. In Proceedings of the
25th International Conference on Machine learning,
53
ICML ?08, pages 1088?1095, New York, NY, USA.
ACM.
Balakrishnan Varadarajan, Sanjeev Khudanpur, and Em-
manuel Dupoux. 2008. Unsupervised learning of
acoustic sub-word units. In Proceedings of the As-
sociation for Computational Linguistics: Short Papers,
pages 165?168.
Anand Venkataraman. 2001. A statistical model for word
discovery in transcribed speech. Computational Lin-
guistics, 27(3):351?372.
Janet F. Werker and Richard C. Tees. 1984. Cross-
language speech perception: Evidence for perceptual
reorganization during the first year of life. Infant Be-
havior and Development, 7(1):49 ? 63.
Katherine S. White and Richard N. Aslin. 2011. Adap-
tation to novel accents by toddlers. Developmental
Science, 14(2):372?384.
54
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 234?244,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
A Probabilistic Model of Syntactic and Semantic Acquisition from
Child-Directed Utterances and their Meanings
Tom Kwiatkowski* ?
tomk@cs.washington.edu
Sharon Goldwater?
sgwater@inf.ed.ac.uk
Luke Zettlemoyer?
lsz@cs.washington.edu
Mark Steedman?
steedman@inf.ed.ac.uk
? ILCC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
?Computer Science & Engineering
University of Washington
Seattle, WA, 98195, USA
Abstract
This paper presents an incremental prob-
abilistic learner that models the acquis-
tion of syntax and semantics from a cor-
pus of child-directed utterances paired with
possible representations of their meanings.
These meaning representations approxi-
mate the contextual input available to the
child; they do not specify the meanings of
individual words or syntactic derivations.
The learner then has to infer the meanings
and syntactic properties of the words in the
input along with a parsing model. We use
the CCG grammatical framework and train
a non-parametric Bayesian model of parse
structure with online variational Bayesian
expectation maximization. When tested on
utterances from the CHILDES corpus, our
learner outperforms a state-of-the-art se-
mantic parser. In addition, it models such
aspects of child acquisition as ?fast map-
ping,? while also countering previous crit-
icisms of statistical syntactic learners.
1 Introduction
Children learn language by mapping the utter-
ances they hear onto what they believe those ut-
terances mean. The precise nature of the child?s
prelinguistic representation of meaning is not
known. We assume for present purposes that
it can be approximated by compositional logical
representations such as (1), where the meaning is
a logical expression that describes a relationship
have between the person you refers to and the
object another(x, cookie(x)):
Utterance : you have another cookie (1)
Meaning : have(you, another(x, cookie(x)))
Most situations will support a number of plausi-
ble meanings, so the child has to learn in the face
of propositional uncertainty1, from a set of con-
textually afforded meaning candidates, as here:
Utterance : you have another cookie
Candidate
Meanings
?
?
?
have(you, another(x, cookie(x)))
eat(you, your(x, cake(x)))
want(i, another(x, cookie(x)))
The task is then to learn, from a sequence of such
(utterance, meaning-candidates) pairs, the correct
lexicon and parsing model. Here we present a
probabilistic account of this task with an empha-
sis on cognitive plausibility.
Our criteria for plausibility are that the learner
must not require any language-specific informa-
tion prior to learning and that the learning algo-
rithm must be strictly incremental: it sees each
training instance sequentially and exactly once.
We define a Bayesian model of parse structure
with Dirichlet process priors and train this on a
set of (utterance, meaning-candidates) pairs de-
rived from the CHILDES corpus (MacWhinney,
2000) using online variational Bayesian EM.
We evaluate the learnt grammar in three ways.
First, we test the accuracy of the trained model
in parsing unseen utterances onto gold standard
annotations of their meaning. We show that
it outperforms a state-of-the-art semantic parser
(Kwiatkowski et al 2010) when run with similar
training conditions (i.e., neither system is given
the corpus based initialization originally used by
Kwiatkowski et al. We then examine the learn-
ing curves of some individual words, showing that
the model can learn word meanings on the ba-
sis of a single exposure, similar to the fast map-
ping phenomenon observed in children (Carey
and Bartlett, 1978). Finally, we show that our
1Similar to referential uncertainty but relating to propo-
sitions rather than referents.
234
learner captures the step-like learning curves for
word order regularities that Thornton and Tesan
(2007) claim children show. This result coun-
ters Thornton and Tesan?s criticism of statistical
grammar learners?that they tend to exhibit grad-
ual learning curves rather than the abrupt changes
in linguistic competence observed in children.
1.1 Related Work
Models of syntactic acquisition, whether they
have addressed the task of learning both syn-
tax and semantics (Siskind, 1992; Villavicencio,
2002; Buttery, 2006) or syntax alone (Gibson
and Wexler, 1994; Sakas and Fodor, 2001; Yang,
2002) have aimed to learn a single, correct, deter-
ministic grammar. With the exception of Buttery
(2006) they also adopt the Principles and Param-
eters grammatical framework, which assumes de-
tailed knowledge of linguistic regularities2. Our
approach contrasts with all previous models in as-
suming a very general kind of linguistic knowl-
edge and a probabilistic grammar. Specifically,
we use the probabilistic Combinatory Categorial
Grammar (CCG) framework, and assume only
that the learner has access to a small set of general
combinatory schemata and a functional mapping
from semantic type to syntactic category. Further-
more, this paper is the first to evaluate a model
of child syntactic-semantic acquisition by parsing
unseen data.
Models of child word learning have focused
on semantics only, learning word meanings from
utterances paired with either sets of concept sym-
bols (Yu and Ballard, 2007; Frank et al 2008; Fa-
zly et al 2010) or a compositional meaning rep-
resentation of the type used here (Siskind, 1996).
The models of Alishahi and Stevenson (2008)
and Maurits et al(2009) learn, as well as word-
meanings, orderings for verb-argument structures
but not the full parsing model that we learn here.
Semantic parser induction as addressed by
Zettlemoyer and Collins (2005, 2007, 2009), Kate
and Mooney (2007), Wong and Mooney (2006,
2007), Lu et al(2008), Chen et al(2010),
Kwiatkowski et al(2010, 2011) and Bo?rschinger
et al(2011) has the same task definition as the
one addressed by this paper. However, the learn-
ing approaches presented in those previous pa-
2This linguistic use of the term ?parameter? is distinct
from the statistical use found elsewhere in this paper.
pers are not designed to be cognitively plausible,
using batch training algorithms, multiple passes
over the data, and language specific initialisations
(lists of noun phrases and additional corpus statis-
tics), all of which we dispense with here. In
particular, our approach is closely related that of
Kwiatkowski et al(2010) but, whereas that work
required careful initialisation and multiple passes
over the training data to learn a discriminative
parsing model, here we learn a generative parsing
model without either.
1.2 Overview of the approach
Our approach takes, as input, a corpus of (ut-
terance, meaning-candidates) pairs {(si, {m}i) :
i = 1, . . . , N}, and learns a CCG lexicon ? and
the probability of each production a ? b that
could be used in a parse. Together, these define
a probabilistic parser that can be used to find the
most probable meaning for any new sentence.
We learn both the lexicon and production prob-
abilities from allowable parses of the training
pairs. The set of allowable parses {t} for a sin-
gle (utterance, meaning-candidates) pair consists
of those parses that map the utterance onto one of
the meanings. This set is generated with the func-
tional mapping T :
{t} = T (s,m), (2)
which is defined, following Kwiatkowski et al
(2010), using only the CCG combinators and a
mapping from semantic type to syntactic category
(presented in in Section 4).
The CCG lexicon ? is learnt by reading off
the lexical items used in all parses of all training
pairs. Production probabilities are learnt in con-
junction with ? through the use of an incremen-
tal parameter estimation algorithm, online Varia-
tional Bayesian EM, as described in Section 5.
Before presenting the probabilistic model, the
mapping T , and the parameter training algorithm,
we first provide some background on the meaning
representations we use and on CCG.
2 Background
2.1 Meaning Representations
We represent the meanings of utterances in first-
order predicate logic using the lambda-calculus.
An example logical expression (henceforth also
referred to as a lambda expression) is:
like(eve,mummy) (3)
235
which expresses a logical relationship like be-
tween the entity eve and the entity mummy. In
Section 6.1 we will see how logical expressions
like this are created for a set of child-directed ut-
terances (to use in training our model).
The lambda-calculus uses ? operators to define
functions. These may be used to represent func-
tional meanings of utterances but they may also be
used as a ?glue language?, to compose elements of
first order logical expressions. For example, the
function ?x?y.like(y, x) can be combined with
the object mummy to give the phrasal mean-
ing ?y.like(y,mummy) through the lambda-
calculus operation of function application.
2.2 CCG
Combinatory Categorial Grammar (CCG; Steed-
man 2000) is a strongly lexicalised linguistic for-
malism that tightly couples syntax and seman-
tics. Each CCG lexical item in the lexicon ? is
a triple, written as word ` syntactic category :
logical expression . Examples are:
You ` NP : you
read ` S\NP/NP : ?x?y.read(y, x)
the ` NP/N : ?f.the(x, f(x))
book ` N : ?x.book(x)
A full CCG category X : h has syntactic cate-
gory X and logical expression h. Syntactic cat-
egories may be atomic (e.g., S or NP) or com-
plex (e.g., (S\NP)/NP). Slash operators in com-
plex categories define functions from the range on
the right of the slash to the result on the left in
much the same way as lambda operators do in the
lambda-calculus. The direction of the slash de-
fines the linear order of function and argument.
CCG uses a small set of combinatory rules to
concurrently build syntactic parses and semantic
representations. Two example combinatory rules
are forward (>) and backward (<) application:
X/Y : f Y : g ? X : f(g) (>)
Y : g X\Y : f ? X : f(g) (<)
Given the lexicon above, the phrase ?You read the
book? can be parsed using these rules, as illus-
trated in Figure 1 (with additional notation dis-
cussed in the following section)..
CCG also includes combinatory rules of
forward (> B) and backward (< B) composition:
X/Y : f Y/Z : g ?X/Z : ?x.f(g(x)) (> B)
Y \Z : g X\Y : f ?X\Z : ?x.f(g(x)) (< B)
3 Modelling Derivations
The objective of our learning algorithm is to
learn the correct parameterisation of a probabilis-
tic model P (s,m, t) over (utterance, meaning,
derivation) triples. This model assigns a proba-
bility to each of the grammar productions a ? b
used to build the derivation tree t. The probabil-
ity of any given CCG derivation t with sentence
s and semantics m is calculated as the product of
all of its production probabilities.
P (s,m, t) =
?
a?b?t
P (b|a) (4)
For example, the derivation in Figure 1 contains
13 productions, and its probability is the product
of the 13 production probabilities. Grammar pro-
ductions may be either syntactic?used to build a
syntactic derivation tree, or lexical?used to gen-
erate logical expressions and words at the leaves
of this tree.
A syntactic production Ch ? R expands a
head node Ch into a result R that is either an
ordered pair of syntactic parse nodes ?Cl,Cr?
(for a binary production) or a single parse node
(for a unary production). Only two unary syn-
tactic productions are allowed in the grammar:
START? A to generate A as the top syntactic
node of a parse tree and A? [A]lex to indicate
that A is a leaf node in the syntactic derivation
and should be used to generate a logical expres-
sion and word. Syntactic derivations are built by
recursively applying syntactic productions to non-
leaf nodes in the derivation tree. Each syntactic
production Ch ? R has conditional probability
P (R|Ch). There are 3 binary and 5 unary syntac-
tic productions in Figure 1.
Lexical productions have two forms. Logical
expressions are produced from leaf nodes in the
syntactic derivation tree Alex ? m with condi-
tional probability P (m|Alex). Words are then pro-
duced from these logical expressions with condi-
tional probability P (w|m). An example logical
production from Figure 1 is [NP]lex ? you. An
example word production is you? You.
Every production a ? b used in a parse tree t
is chosen from the set of productions that could
be used to expand a head node a. If there are a
finite K productions that could expand a then a
K-dimensional Multinomial distribution parame-
terised by ?a can be used to model the categorical
236
START
Sdcl
NP
[NP]lex
you
You
Sdcl\NP
(Sdcl\NP)/NP
[(Sdcl\NP)/NP]lex
?x?y.read(y, x)
read
NP
NP/N
[NP/N]lex
?f?x.the(x, f(x))
the
N
[N]lex
?x.book(x)
book
Figure 1: Derivation of sentence You read the
book with meaning read(you, the(x, book(x))).
choice of production:
b ? Multinomial(?a) (5)
However, before training a model of language ac-
quisition the dimensionality and contents of both
the syntactic grammar and lexicon are unknown.
In order to maintain a probability model with
cover over the countably infinite number of pos-
sible productions, we define a Dirichlet Process
(DP) prior for each possible production head a.
For the production head a, DP (?a, Ha) assigns
some probability mass to all possible production
targets {b} covered by the base distribution Ha.
It is possible to use the DP as an infinite prior
from which the parameter set of a finite dimen-
sional Multinomial may be drawn provided that
we can choose a suitable partition of {b}. When
calculating the probability of an (s,m, t) triple,
the choice of this partition is easy. For any given
production head a there is a finite set of usable
production targets {b1, . . . , bk?1} in t. We create
a partition that includes one entry for each of these
along with a final entry {bk, . . . } that includes all
other ways in which a could be expanded in dif-
ferent contexts. Then, by applying the distribution
Ga drawn from the DP to this partition, we get a
parameter vector ?a that is equivalent to a draw
from a k dimensional Dirichlet distribution:
Ga ? DP (?a, Ha) (6)
?a = (Ga(b1), . . . , Ga(bk?1), Ga({bk, . . . })
? Dir(?aH(b1), . . . , ?aHa(bk?1), (7)
?aHa({bk, . . . }))
Together, Equations 4-7 describe the joint distri-
bution P (X,S, ?) over the observed training data
X = {(si, {m}i) : i = 1, . . . , N}, the latent vari-
ables S (containing the productions used in each
parse t) and the parsing parameters ?.
4 Generating Parses
The previous section defined a parameterisation
over parses assuming that the CCG lexicon ? was
known. In practice ? is empty prior to training
and must be populated with the lexical items from
parses t consistent with training pairs (s, {m}).
The set of allowed parses {t} is defined by the
function T from Equation 2. Here we review the
splitting procedure of Kwiatkowski et al(2010)
that is used to generate CCG lexical items and de-
scribe how it is used by T to create a packed chart
representation of all parses {t} that are consistent
with s and at least one of the meaning represen-
tations in {m}. In this section we assume that s
is paired at each point with only a single meaning
m. Later we will show how T is used multiple
times to create the set of parses consistent with s
and a set of candidate meanings {m}.
The splitting procedure takes as input a CCG
category X :h, such as NP : a(x, cookie(x)), and
returns a set of category splits. Each category split
is a pair of CCG categories (Cl :ml,Cr :mr) that
can be recombined to give X : h using one of the
CCG combinators in Section 2.2. The CCG cat-
egory splitting procedure has two parts: logical
splitting of the category semantics h; and syntac-
tic splitting of the syntactic category X. Each logi-
cal split of h is a pair of lambda expressions (f, g)
in the following set:
{(f, g) | h = f(g) ? h = ?x.f(g(x))}, (8)
which means that f and g can be recombined us-
ing either function application or function com-
position to give the original lambda expression
h. An example split of the lambda expression
h = a(x, cookie(x)) is the pair
(?y.a(x, y(x)), ?x.cookie(x)), (9)
where ?y.a(x, y(x)) applied to ?x.cookie(x) re-
turns the original expression a(x, cookie(x)).
Syntactic splitting assigns linear order and syn-
tactic categories to the two lambda expressions f
and g. The initial syntactic category X is split by
a reversal of the CCG application combinators in
Section 2.2 if f and g can be recombined to give
237
Syntactic Category Semantic Type Example Phrase
Sdcl ?ev, t? I took it ` Sdcl :?e.took(i, it, e)
St t I?m angry ` St :angry(i)
Swh ?e, ?ev, t?? Who took it? ` Swh :?x?e.took(x, it, e)
Sq ?ev, t? Did you take it? ` Sq :?e.Q(take(you, it, e))
N ?e, t? cookie `N:?x.cookie(x)
NP e John `NP:john
PP ?ev, t? on John ` PP:?e.on(john, e)
Figure 2: Atomic Syntactic Categories.
h with function application:
{(X/Y : f Y : g), (10)
(Y : g : X\Y : f)|h = f(g)}
or by a reversal of the CCG composition combi-
nators if f and g can be recombined to give hwith
function composition:
{(X/Z : f Z/Y : g, (11)
(Z\Y : g : X\Z : f)|h = ?x.f(g(x))}
Unknown category names in the result of a
split (Y in (10) and Z in (11)) are labelled via a
functional mapping cat from semantic type T to
syntactic category:
cat(T ) =
?
?
?
Atomic(T ) if T ? Figure 2
cat(T1)/cat(T2) if T = ?T1, T2?
cat(T1)\cat(T2) if T = ?T1, T2?
?
?
?
which uses the Atomic function illustrated
in Figure 2 to map semantic-type to basic CCG
syntactic category. As an example, the logical
split in (9) supports two CCG category splits, one
for each of the CCG application rules.
(NP/N :?y.a(x, y(x)), N :?x.cookie(x)) (12)
(N :?x.cookie(x), NP\N :?y.a(x, y(x))) (13)
The parse generation algorithm T uses the func-
tion split to generate all CCG category pairs that
are an allowed split of an input category X :h:
{(Cl :ml,Cr :mr)} = split(X :h),
and then packs a chart representation of {t} in a
top-down fashion starting with a single cell entry
Cm :m for the top node shared by all parses {t}.
For the utterance and meaning in (1) the top parse
node, spanning the entire word-string, is
S :have(you, another(x, cookie(x))).
T cycles over all cell entries in increasingly small
spans and populates the chart with their splits. For
any cell entry X :h spanning more than one word
T generates a set of pairs representing the splits of
X :h. For each split (Cl :ml,Cr :mr) and every bi-
nary partition (wi:k, wk:j) of the word-span T cre-
ates two new cell entries in the chart: (Cl :ml)i:k
and (Cr :mr)k:j .
Input : Sentence [w1, . . . , wn], top node Cm :m
Output: Packed parse chart Ch containing {t}
Ch = [ [{}1, . . . , {}n]1, . . . , [{}1, . . . , {}n]n ]
Ch[1][n? 1] = Cm :m
for i = n, . . . , 2; j = 1 . . . (n? i) + 1 do
for X:h ? Ch[j][i] do
for (Cl :ml,Cr :mr) ? split(X:h) do
for k = 1, . . . , i? 1 do
Ch[j][k]? Cl :ml
Ch[j + k][i? k]? Cr :mr
Algorithm 1: Generating {t} with T .
Algorithm 1 shows how the learner uses T to
generate a packed chart representation of {t} in
the chart Ch. The function T massively overgen-
erates parses for any given natural language. The
probabilistic parsing model introduced in Sec-
tion 3 is used to choose the best parse from the
overgenerated set.
5 Training
5.1 Parameter Estimation
The probabilistic model of the grammar describes
a distribution over the observed training data X,
latent variables S, and parameters ?. The goal of
training is to estimate the posterior distribution:
p(S, ?|X) = p(S,X|?)p(?)
p(X)
(14)
which we do with online Variational Bayesian Ex-
pectation Maximisation (oVBEM; Sato (2001),
Hoffman et al(2010)). oVBEM is an online
238
Bayesian extension of the EM algorithm that
accumulates observation pseudocounts na?b for
each of the productions a ? b in the grammar.
These pseudocounts define the posterior over pro-
duction probabilities as follows:
(?a?b1 , . . . , ?a?b{k,... })) | X,S ? (15)
Dir(?H(b1) + na?b1 , . . . ,
??
j=k
?H(bj) + na?bj )
These pseudocounts are computed in two steps:
oVBE-step For the training pair (si, {m}i)
which supports the set of parses {t}, the expec-
tation E{t}[a ? b] of each production a ? b is
calculated by creating a packed chart representa-
tion of {t} and running the inside-outside algo-
rithm. This is similar to the E-step in standard
EM apart from the fact that each production is
scored with the current expectation of its parame-
ter weight ??i?1a?b, where:
??i?1a?b =
e?(?aHa(a?b)+n
i?1
a?b)
e
?
(?K
{b?} ?aHa(a?b
?)+ni?1
a?b?
) (16)
and ? is the digamma function (Beal, 2003).
oVBM-step The expectations from the oVBE
step are used to update the pseudocounts in Equa-
tion 15 as follows,
nia?b = n
i?1
a?b + ?i(N ? E{t}[a? b]? ni?1a?b)
(17)
where ?i is the learning rate and N is the size of
the dataset.
5.2 The Training Algorithm
Now the training algorithm used to learn the lex-
icon ? and pseudocounts {na?b} can be defined.
The algorithm, shown in Algorithm 2, passes over
the training data only once and one training in-
stance at a time. For each (si, {m}i) it uses the
function T |{m}i| times to generate a set of con-
sistent parses {t}?. The lexicon is populated by
using the lex function to read all of the lexical
items off from the derivations in each {t}?. In
the parameter update step, the training algorithm
updates the pseudocounts associated with each of
the productions a ? b that have ever been seen
during training according to Equation (17).
Only non-zero pseudocounts are stored in our
model. The count vector is expanded with a new
entry every time a new production is used. While
Input : Corpus D = {(si, {m}i)|i = 1, . . . , N},
Function T , Semantics to syntactic cate-
gory mapping cat, function lex to read
lexical items off derivations.
Output: Lexicon ?, Pseudocounts {na?b}.
? = {}, {t} = {}
for i = 1, . . . , N do
{t}i = {}
form? ? {m}i do
Cm? = cat(m?)
{t}? = T (si,Cm? :m?)
{t}i = {t}i ? {t}?, {t} = {t} ? {t}?
? = ? ? lex ({t}?)
for a? b ? {t} do
nia?b = n
i?1
a?b + ?i(N ? E{t}i [a? b]?
ni?1a?b)
Algorithm 2: Learning ? and {na?b}
the parameter update step cycles over all produc-
tions in {t} it is not neccessary to store {t}, just
the set of productions that it uses.
6 Experimental Setup
6.1 Data
The Eve corpus, collected by Brown (1973), con-
tains 14, 124 English utterances spoken to a sin-
gle child between the ages of 18 and 27 months.
These have been hand annotated by Sagae et al
(2004) with labelled syntactic dependency graphs.
An example annotation is shown in Figure 3.
While these annotations are designed to rep-
resent syntactic information, the parent-child re-
lationships in the parse can also be viewed as a
proxy for the predicate-argument structure of the
semantics. We developed a template based de-
terministic procedure for mapping this predicate-
argument structure onto logical expressions of the
type discussed in Section 2.1. For example, the
dependency graph in Figure 3 is automatically
transformed into the logical expression
?e.have(you,another(y, cookie(y)), e) (18)
? on(the(z, table(z)), e),
where e is a Davidsonian event variable used to
deal with adverbial and prepositional attachments.
The deterministic mapping to logical expressions
uses 19 templates, three of which are used in this
example: one for the verb and its arguments, one
for the prepositional attachment and one (used
twice) for the quantifier-noun constructions.
239
SUBJ ROOT DET OBJ JCT DET POBJ
pro|you v|have qn|another n|cookie prep|on det|the n|table
You have another cookie on the table
Figure 3: Syntactic dependency graph from Eve corpus.
This mapping from graph to logical expression
makes use of a predefined dictionary of allowed,
typed, logical constants. The mapping is success-
ful for 31% of the child-directed utterances in the
Eve corpus3. The remaining data is mostly ac-
counted for by one-word utterances that have no
straightforward interpretation in our typed logi-
cal language (e.g. what; okay; alright; no; yeah;
hmm; yes; uhhuh; mhm; thankyou), missing ver-
bal arguments that cannot be properly guessed
from the context (largely in imperative sentences
such as drink the water), and complex noun con-
structions that are hard to match with a small set
of templates (e.g. as top to a jar). We also re-
move the small number of utterances containing
more than 10 words for reasons of computational
efficiency (see discussion in Section 8).
Following Alishahi and Stevenson (2010), we
generate a context set {m}i for each utterance si
by pairing that utterance with its correct logical
expression along with the logical expressions of
the preceding and following (|{m}i|?1)/2 utter-
ances.
6.2 Base Distributions and Learning Rate
Each of the production heads a in the grammar
requires a base distribution Ha and concentration
parameter ?a. For word-productions the base dis-
tribution is a geometric distribution over character
strings and spaces. For syntactic-productions the
base distribution is defined in terms of the new
category to be named by cat and the probability
of splitting the rule by reversing either the appli-
cation or composition combinators.
Semantic-productions? base distributions are
defined by a probabilistic branching process con-
ditioned on the type of the syntactic category.
This distribution prefers less complex logical ex-
pressions. All concentration parameters are set to
1.0. The learning rate for parameter updates is
?i = (0.8 + i)?0.5.
3Data available at www.tomkwiat.com/resources.html
0.0 0.2 0.4 0.6 0.8 1.0Proportion of Data Seen0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Accu
racy
Our ApproachOur Approach + Guess UBL
1
UBL10
Figure 4: Meaning Prediction: Train on files 1, . . . , n
test on file n+ 1.
7 Experiments
7.1 Parsing Unseen Sentences
We test the parsing model that is learnt by training
on the first i files of the longitudinally ordered Eve
corpus and testing on file i + 1, for i = 1 . . . 19.
For each utterance s? in the test file we use the
parsing model to predict a meaning m? and com-
pare this to the target meaning m?. We report the
proportion of utterances for which the prediction
m? is returned correctly both with and without
word-meaning guessing. When a word has never
been seen at training time our parser has the abil-
ity to ?guess? a typed logical meaning with place-
holders for constant and predicate names.
For comparison we use the UBL semantic
parser of Kwiatkowski et al(2010) trained in
a similar setting?i.e., with no language specific
initialisation4. Figure 4 shows accuracy for our
approach with and without guessing, for UBL
4Kwiatkowski et al(2010) initialise lexical weights in
their learning algorithm using corpus-wide alignment statis-
tics across words and meaning elements. Instead we run
UBL with small positive weight for all lexical items. When
run with Giza++ parameter initialisations, UBL10 achieves
48.1% across folds compared to 49.2% for our approach.
240
when run over the training data once (UBL1) and
for UBL when run over the training data 10 times
(UBL10) as in Kwiatkowski et al(2010). Each
of the points represents accuracy on one of the
19 test files. All of these results are from parsers
trained on utterances paired with a single candi-
date meaning. The lines of best fit show the up-
ward trend in parser performance over time.
Despite only seeing each training instance
once, our approach, due to its broader lexi-
cal search strategy, outperforms both versions of
UBL which performs a greedy search in the space
of lexicons and requires initialisation with co-
occurence statistics between words and logical
constants to guide this search. These statistics are
not justified in a model of language acquisition
and so they are not used here. The low perfor-
mance of all systems is due largely to the sparsity
of the data with 32.9% of all sentences containing
a previously unseen word.
7.2 Word Learning
Due to the sparsity of the data, the training algo-
rithm needs to be able to learn word-meanings on
the basis of very few exposures. This is also a de-
sirable feature from the perspective of modelling
language acquisition as Carey and Bartlett (1978)
have shown that children have the ability to learn
word meanings on the basis of one, or very few,
exposures through the process of fast mapping.
0 500 1000 1500 20000.0
0.20.4
0.60.8
1.0
P(m|w
) 1 Meaning
0 500 1000 1500 2000
3 Meanings
0 500 1000 1500 2000Number of Utterances0.0
0.20.4
0.60.8
1.0
P(m|w
) 5 Meanings
0 500 1000 1500 2000Number of Utterances
7 Meanings
f = 168 a? ?f.a(x, f (x))f = 10 another? ?f.another(x, f (x))f = 2 any? ?f.any(x, f (x))
Figure 5: Learning quantifiers with frequency f.
Figure 5 shows the posterior probability of the
correct meanings for the quantifiers ?a?, ?another?
and ?any? over the course of training with 1, 3,
5 and 7 candidate meanings for each utterance5.
These three words are all of the same class but
have very different frequencies in the training
subset shown (168, 10 and 2 respectively). In all
training settings, the word ?a? is learnt gradually
from many observations but the rarer words ?an-
other? and ?any? are learnt (when they are learnt)
through large updates to the posterior on the ba-
sis of few observations. These large updates re-
sult from a syntactic bootstrapping effect (Gleit-
man, 1990). When the model has great confidence
about the derivation in which an unseen lexical
item occurs, the pseudocounts for that lexical item
get a large update under Equation 17. This large
update has a greater effect on rare words which
are associated with small amounts of probability
mass than it does on common ones that have al-
ready accumulated large pseudocounts. The fast
learning of rare words later in learning correlates
with observations of word learning in children.
7.3 Word Order Learning
Figure 6 shows the posterior probability of the
correct SVO word order learnt from increasing
amounts of training data. This is calculated by
summing over all lexical items containing transi-
tive verb semantics and sampling in the space of
parse trees that could have generated them. With
no propositional uncertainty in the training data
the correct word order is learnt very quickly and
stabilises. As the amount of propositional uncer-
tainty increases, the rate at which this rule is learnt
decreases. However, even in the face of ambigu-
ous training data, the model can learn the cor-
rect word-order rule. The distribution over word
orders also exhibits initial uncertainty, followed
by a sharp convergence to the correct analysis.
This ability to learn syntactic regularities abruptly
means that our system is not subject to the crit-
icisms that Thornton and Tesan (2007) levelled
at statistical models of language acquisition?that
their learning rates are too gradual.
5The term ?fast mapping? is generally used to refer to
noun learning. We chose to examine quantifier learning here
as there is a greater variation in quantifier frequencies. Fast
mapping of nouns is also achieved.
241
0 500 1000 1500 2000Number of Utterances
7 Meanings
0 500 1000 1500 2000Number of Utterances0.00.2
0.40.6
0.81.0
P(word
order)
5 Meanings 0 500 1000 1500 2000
3 Meanings
0 500 1000 1500 20000.0
0.20.4
0.60.8
1.0
P(word
order)
1 Meaning
vsosvo ovssov vososv
Figure 6: Learning SVO word order.
8 Discussion
We have presented an incremental model of lan-
guage acquisition that learns a probabilistic CCG
grammar from utterances paired with one or
more potential meanings. The model assumes
no language-specific knowledge, but does assume
that the learner has access to language-universal
correspondences between syntactic and semantic
types, as well as a Bayesian prior encouraging
grammars with heavy reuse of existing rules and
lexical items. We have shown that this model
not only outperforms a state-of-the-art semantic
parser, but also exhibits learning curves similar
to children?s: lexical items can be acquired on a
single exposure and word order is learnt suddenly
rather than gradually.
Although we use a Bayesian model, our ap-
proach is different from many of the Bayesian
models proposed in cognitive science and lan-
guage acquisition (Xu and Tenenbaum, 2007;
Goldwater et al 2009; Frank et al 2009; Grif-
fiths and Tenenbaum, 2006; Griffiths, 2005; Per-
fors et al 2011). These models are intended
as ideal observer analyses, demonstrating what
would be learned by a probabilistically optimal
learner. Our learner uses a more cognitively plau-
sible but approximate online learning algorithm.
In this way, it is similar to other cognitively plau-
sible approximate Bayesian learners (Pearl et al
2010; Sanborn et al 2010; Shi et al 2010).
Of course, despite the incremental nature of our
learning algorithm, there are still many aspects
that could be criticized as cognitively implausi-
ble. In particular, it generates all parses consistent
with each training instance, which can be both
memory- and processor-intensive. It is unlikely
that children do this once they have learnt at least
some of the target language. In future, we plan
to investigate more efficient parameter estimation
methods. One possibility would be an approxi-
mate oVBEM algorithm in which the expectations
in Equation 17 are calculated according to a high
probability subset of the parses {t}. Another op-
tion would be particle filtering, which has been
investigated as a cognitively plausible method for
approximate Bayesian inference (Shi et al 2010;
Levy et al 2009; Sanborn et al 2010).
As a crude approximation to the context in
which an utterance is heard, the logical represen-
tations of meaning that we present to the learner
are also open to criticism. However, Steedman
(2002) argues that children do have access to
structured meaning representations from a much
older apparatus used for planning actions and we
wish to eventually ground these in sensory input.
Despite the limitations listed above, our ap-
proach makes several important contributions to
the computational study of language acquisition.
It is the first model to learn syntax and seman-
tics concurrently; previous systems (Villavicen-
cio, 2002; Buttery, 2006) learnt categorial gram-
mars from sentences where all word meanings
were known. Our model is also the first to be
evaluated by parsing sentences onto their mean-
ings, in contrast to the work mentioned above and
that of Gibson and Wexler (1994), Siskind (1992)
Sakas and Fodor (2001), and Yang (2002). These
all evaluate their learners on the basis of a small
number of predefined syntactic parameters.
Finally, our work addresses a misunderstand-
ing about statistical learners?that their learn-
ing curves must be gradual (Thornton and Tesan,
2007). By demonstrating sudden learning of word
order and fast mapping, our model shows that sta-
tistical learners can account for sudden changes in
children?s grammars. In future, we hope to extend
these results by examining other learning behav-
iors and testing the model on other languages.
9 Acknowledgements
We thank Mark Johnson for suggesting an analy-
sis of learning rates. This work was funded by the
ERC Advanced Fellowship 24952 GramPlus and
EU IP grant EC-FP7-270273 Xperience.
242
References
Alishahi and Stevenson, S. (2008). A computa-
tional model for early argument structure ac-
quisition. Cognitive Science, 32:5:789?834.
Alishahi, A. and Stevenson, S. (2010). Learning
general properties of semantic roles from usage
data: a computational model. Language and
Cognitive Processes, 25:1.
Beal, M. J. (2003). Variational algorithms for ap-
proximate Bayesian inference. Technical re-
port, Gatsby Institute, UCL.
Bo?rschinger, B., Jones, B. K., and Johnson, M.
(2011). Reducing grounded learning tasks
to grammatical inference. In Proceedings of
the 2011 Conference on Empirical Methods
in Natural Language Processing, pages 1416?
1425, Edinburgh, Scotland, UK. Association
for Computational Linguistics.
Brown, R. (1973). A First Language: the Early
Stages. Harvard University Press, Cambridge
MA.
Buttery, P. J. (2006). Computational models for
first language acquisition. Technical Report
UCAM-CL-TR-675, University of Cambridge,
Computer Laboratory.
Carey, S. and Bartlett, E. (1978). Acquring a sin-
gle new word. Papers and Reports on Child
Language Development, 15.
Chen, D. L., Kim, J., and Mooney, R. J. (2010).
Training a multilingual sportscaster: Using per-
ceptual context to learn language. J. Artif. In-
tell. Res. (JAIR), 37:397?435.
Fazly, A., Alishahi, A., and Stevenson, S. (2010).
A probabilistic computational model of cross-
situational word learning. Cognitive Science,
34(6):1017?1063.
Frank, M., Goodman, S., and Tenenbaum, J.
(2009). Using speakers referential intentions
to model early cross-situational word learning.
Psychological Science, 20(5):578?585.
Frank, M. C., Goodman, N. D., and Tenenbaum,
J. B. (2008). A bayesian framework for cross-
situational word-learning. Advances in Neural
Information Processing Systems 20.
Gibson, E. and Wexler, K. (1994). Triggers. Lin-
guistic Inquiry, 25:355?407.
Gleitman, L. (1990). The structural sources of
verb meanings. Language Acquisition, 1:1?55.
Goldwater, S., Griffiths, T. L., and Johnson, M.
(2009). A Bayesian framework for word seg-
mentation: Exploring the effects of context.
Cognition, 112(1):21?54.
Griffiths, T. L., . T. J. B. (2005). Structure and
strength in causal induction. Cognitive Psy-
chology, 51:354?384.
Griffiths, T. L. and Tenenbaum, J. B. (2006). Op-
timal predictions in everyday cognition. Psy-
chological Science.
Hoffman, M., Blei, D. M., and Bach, F. (2010).
Online learning for latent dirichlet alcation.
In NIPS.
Kate, R. J. and Mooney, R. J. (2007). Learning
language semantics from ambiguous supervi-
sion. In Proceedings of the 22nd Conference
on Artificial Intelligence (AAAI-07).
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S.,
and Steedman, M. (2010). Inducing proba-
bilistic CCG grammars from logical form with
higher-order unification. In Proceedings of the
Conference on Emperical Methods in Natural
Language Processing.
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S.,
and Steedman, M. (2011). Lexical general-
ization in ccg grammar induction for semantic
parsing. In Proceedings of the Conference on
Emperical Methods in Natural Language Pro-
cessing.
Levy, R., Reali, F., and Griffiths, T. (2009). Mod-
eling the effects of memory on human online
sentence processing with particle filters. In Ad-
vances in Neural Information Processing Sys-
tems 21.
Lu, W., Ng, H. T., Lee, W. S., and Zettlemoyer,
L. S. (2008). A generative model for parsing
natural language to meaning representations. In
Proceedings of The Conference on Empirical
Methods in Natural Language Processing.
MacWhinney, B. (2000). The CHILDES project:
tools for analyzing talk. Lawrence Erlbaum,
Mahwah, NJ u.a. EN.
Maurits, L., Perfors, A., and Navarro, D. (2009).
Joint acquisition of word order and word refer-
ence. In Proceedings of the 31th Annual Con-
ference of the Cognitive Science Society.
Pearl, L., Goldwater, S., and Steyvers, M. (2010).
How ideal are we? Incorporating human limi-
243
tations into Bayesian models of word segmen-
tation. pages 315?326, Somerville, MA. Cas-
cadilla Press.
Perfors, A., Tenenbaum, J. B., and Regier, T.
(2011). The learnability of abstract syntactic
principles. Cognition, 118(3):306 ? 338.
Sagae, K., MacWhinney, B., and Lavie, A.
(2004). Adding syntactic annotations to tran-
scripts of parent-child dialogs. In Proceed-
ings of the 4th International Conference on
Language Resources and Evaluation. Lisbon,
LREC.
Sakas, W. and Fodor, J. D. (2001). The struc-
tural triggers learner. In Bertolo, S., editor,
Language Acquisition and Learnability, pages
172?233. Cambridge University Press, Cam-
bridge.
Sanborn, A. N., Griffiths, T. L., and Navarro,
D. J. (2010). Rational approximations to ratio-
nal models: Alternative algorithms for category
learning. Psychological Review.
Sato, M. (2001). Online model selection based
on the variational bayes. Neural Computation,
13(7):1649?1681.
Shi, L., Griffiths, T. L., Feldman, N. H., and San-
born, A. N. (2010). Exemplar models as a
mechanism for performing bayesian inference.
Psychonomic Bulletin & Review, 17(4):443?
464.
Siskind, J. M. (1992). Naive Physics, Event Per-
ception, Lexical Semantics, and Language Ac-
quisition. PhD thesis, Massachusetts Institute
of Technology.
Siskind, J. M. (1996). A computational study of
cross-situational techniques for learning word-
to-meaning mappings. Cognition, 61(1-2):1?
38.
Steedman, M. (2000). The Syntactic Process.
MIT Press, Cambridge, MA.
Steedman, M. (2002). Plans, affordances, and
combinatory grammar. Linguistics and Philos-
ophy, 25.
Thornton, R. and Tesan, G. (2007). Categori-
cal acquisition: Parameter setting in universal
grammar. Biolinguistics, 1.
Villavicencio, A. (2002). The acquisition of a
unification-based generalised categorial gram-
mar. Technical Report UCAM-CL-TR-533,
University of Cambridge, Computer Labora-
tory.
Wong, Y. W. and Mooney, R. (2006). Learning for
semantic parsing with statistical machine trans-
lation. In Proceedings of the Human Language
Technology Conference of the NAACL.
Wong, Y. W. and Mooney, R. (2007). Learn-
ing synchronous grammars for semantic pars-
ing with lambda calculus. In Proceedings of
the Association for Computational Linguistics.
Xu, F. and Tenenbaum, J. B. (2007). Word learn-
ing as Bayesian inference. Psychological Re-
view, 114:245?272.
Yang, C. (2002). Knowledge and Learning in Nat-
ural Language. Oxford University Press, Ox-
ford.
Yu, C. and Ballard, D. H. (2007). A unified model
of early word learning: Integrating statisti-
cal and social cues. Neurocomputing, 70(13-
15):2149 ? 2165.
Zettlemoyer, L. S. and Collins, M. (2005). Learn-
ing to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the Conference on
Uncertainty in Artificial Intelligence.
Zettlemoyer, L. S. and Collins, M. (2007). Online
learning of relaxed CCG grammars for pars-
ing to logical form. In Proc. of the Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural
Language Learning.
Zettlemoyer, L. S. and Collins, M. (2009). Learn-
ing context-dependent mappings from sen-
tences to logical form. In Proceedings of The
Joint Conference of the Association for Com-
putational Linguistics and International Joint
Conference on Natural Language Processing.
244
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 184?193,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Bootstrapping a Unified Model of Lexical and Phonetic Acquisition
Micha Elsner
melsner0@gmail.com
ILCC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
Sharon Goldwater
sgwater@inf.ed.ac.uk
ILCC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
Jacob Eisenstein
jacobe@gmail.com
School of Interactive Computing
Georgia Institute of Technology
Atlanta, GA, 30308, USA
Abstract
During early language acquisition, infants must
learn both a lexicon and a model of phonet-
ics that explains how lexical items can vary
in pronunciation?for instance ?the? might be
realized as [Di] or [D@]. Previous models of ac-
quisition have generally tackled these problems
in isolation, yet behavioral evidence suggests
infants acquire lexical and phonetic knowledge
simultaneously. We present a Bayesian model
that clusters together phonetic variants of the
same lexical item while learning both a lan-
guage model over lexical items and a log-linear
model of pronunciation variability based on ar-
ticulatory features. The model is trained on
transcribed surface pronunciations, and learns
by bootstrapping, without access to the true
lexicon. We test the model using a corpus of
child-directed speech with realistic phonetic
variation and either gold standard or automati-
cally induced word boundaries. In both cases
modeling variability improves the accuracy of
the learned lexicon over a system that assumes
each lexical item has a unique pronunciation.
1 Introduction
Infants acquiring their first language confront two
difficult cognitive problems: building a lexicon of
word forms, and learning basic phonetics and phonol-
ogy. The two tasks are closely related: knowing what
sounds can substitute for one another helps in clus-
tering together variant pronunciations of the same
word, while knowing the environments in which par-
ticular words can occur helps determine which sound
changes are meaningful and which are not (Feldman
(a) intended: /ju want w2n/ /want e kUki/
(b) surface: [j@ waP w2n] [wan @ kUki]
(c) unsegmented: [j@waPw2n] [wan@kUki]
(d) idealized: /juwantw2n/ /wantekUki/
Figure 1: The utterances you want one? want a cookie?
represented (a) using a canonical phonemic encoding for
each word and (b) as they might be pronounced phoneti-
cally. Lines (c) and (d) remove the word boundaries (but
not utterance boundaries) from (b) and (a), respectively.
et al, 2009). For instance, if an infant who already
knows the word [ju] ?you? encounters a new word
[j@], they must decide whether it is a new lexical item
or a variant of the word they already know. Evidence
for the correct conclusion comes from the pronun-
ciation (many English vowels are reduced to [@] in
unstressed positions) and the context?if the next
word is ?want?, ?you? is a plausible choice.
To date, most models of infant language learn-
ing have focused on either lexicon-building or pho-
netic learning in isolation. For example, many mod-
els of word segmentation implicitly or explicitly
build a lexicon while segmenting the input stream
of phonemes into word tokens; in nearly all cases
the phonemic input is created from an orthographic
transcription using a phonemic dictionary, thus ab-
stracting away from any phonetic variability (Brent,
1999; Venkataraman, 2001; Swingley, 2005; Gold-
water et al, 2009, among others). As illustrated
in Figure 1, these models attempt to infer line (a)
from line (d). However, (d) is an idealization: real
speech has variability, and behavioral evidence sug-
gests that infants are still learning about the phonetics
and phonology of their language even after beginning
to segment words, rather than learning to neutralize
184
the variations first and acquiring the lexicon after-
wards (Feldman et al, 2009, and references therein).
Based on this evidence, a more realistic model of
early language acquisition should propose a method
of inferring the intended forms (Figure 1a) from the
unsegmented surface forms (1c) while also learning a
model of phonetic variation relating the intended and
surface forms (a) and (b). Previous models with sim-
ilar goals have learned from an artificial corpus with
a small vocabulary (Driesen et al, 2009; Ra?sa?nen,
2011) or have modeled variability only in vowels
(Feldman et al, 2009); to our knowledge, this paper
is the first to use a naturalistic infant-directed corpus
while modeling variability in all segments, and to
incorporate word-level context (a bigram language
model). Our main contribution is a joint lexical-
phonetic model that infers intended forms from seg-
mented surface forms; we test the system using in-
put with either gold standard word boundaries or
boundaries induced by an existing unsupervised seg-
mentation model (Goldwater et al, 2009). We show
that in both cases modeling variability improves the
accuracy of the learned lexicon over a system that
assumes each intended form has a unique surface
form.
Our model is conceptually similar to those used
in speech recognition and other applications: we
assume the intended tokens are generated from a bi-
gram language model and then distorted by a noisy
channel, in particular a log-linear model of phonetic
variability. But unlike speech recognition, we have
no ?intended-form, surface-form? training pairs to
train the phonetic model, nor even a dictionary of
intended-form strings to train the language model.
Instead, we initialize the noise model using feature
weights based on universal linguistic principles (e.g.,
a surface phone is likely to share articulatory features
with the intended phone) and use a bootstrapping
process to iteratively infer the intended forms and
retrain the language model and noise model. While
we do not claim that the particular inference mech-
anism we use is cognitively plausible, our positive
results further support the claim that infants can and
do acquire phonetics and the lexicon in concert.
2 Related work
Our work is inspired by the lexical-phonetic model
of Feldman et al (2009). They extend a model for
clustering acoustic tokens into phonetic categories
(Vallabha et al, 2007) by adding a lexical level that
simultaneously clusters word tokens (which contain
the acoustic tokens) into lexical entries. Including
the lexical level improves the model?s phonetic cat-
egorization, and a follow-up study on artificial lan-
guage learning (Feldman, 2011) supports the claim
that human learners use lexical knowledge to distin-
guish meaningful from unimportant phonetic con-
trasts. Feldman et al (2009) use a real-valued rep-
resentation for vowels (formant values), but assume
no variability in consonants, and treat each word to-
ken independently. In contrast, our model uses a
symbolic representation for sounds, but models vari-
ability in all segment types and incorporates a bigram
word-level language model.
To our knowledge, the only other lexicon-building
systems that also learn about phonetic variability are
those of Driesen et al (2009) and Ra?sa?nen (2011).
These systems learn to represent lexical items and
their variability from a discretized representation of
the speech stream, but they are tested on an artifi-
cial corpus with only 80 vocabulary items that was
constructed so as to ?avoid strong word-to-word de-
pendencies? (Ra?sa?nen, 2011). Here, we use a natu-
ralistic corpus, demonstrating that lexical-phonetic
learning is possible in this more general setting and
that word-level context information is important for
doing so.
Several other related systems work directly from
the acoustic signal and many of these do use natu-
ralistic corpora. However, they do not learn at both
the lexical and phonetic/acoustic level. For example,
Park and Glass (2008), Aimetti (2009), Jansen et al
(2010), and McInnes and Goldwater (2011) present
lexicon-building systems that use hard-coded acous-
tic similarity measures rather than learning about
variability, and they only extract and cluster a few
frequent words. On the phonetic side, Varadarajan et
al. (2008) and Dupoux et al (2011) describe systems
that learn phone-like units but without the benefit of
top-down information.
A final line of related work is on word segmenta-
tion. In addition to the models mentioned in Section
1, which use phonemic input, a few models of word
segmentation have been tested using phonetic input
(Fleck, 2008; Rytting, 2007; Daland and Pierrehum-
bert, 2010). However, they do not cluster segmented
185
Figure 2: Our generative model of the surface tokens s
from intended tokens x, which occur with left and right
contexts l and r.
word tokens into lexical items (none of these mod-
els even maintains an explicit lexicon), nor do they
model or learn from phonetic variation in the input.
3 Lexical-phonetic model
Our lexical-phonetic model is defined using the stan-
dard noisy channel framework: first a sequence of
intended word tokens is generated using a language
model, and then each token is transformed by a proba-
bilistic finite-state transducer to produce the observed
surface sequence. In this section, we present the
model in a hierarchical Bayesian framework to em-
phasize its similarity to existing models, in particu-
lar those of Feldman et al (2009) and Goldwater et
al. (2009). In our actual implementation, however,
we use approximation and MAP point estimates to
make our inference process more tractable; we dis-
cuss these simplifications in Section 4.
Our observed data consists of a (segmented) se-
quence of surface words s1 . . . sn. We wish to re-
cover the corresponding sequence of intended words
x1 . . . xn. As shown in Figure 2, si is produced from
xi by a transducer T : si ? T (xi), which models
phonetic changes. Each xi is sampled from a dis-
tribution ? which represents word frequencies, and
its left and right context words, li and ri, are drawn
from distributions conditioned on xi, in order to cap-
ture information about the environments in which
xi appears: li ? PL(xi), ri ? PR(xi). Because the
number of word types is not known in advance, ? is
drawn from a Dirichlet process DP (?), and PL(x)
and PR(x) have Pitman-Yor priors with concentra-
tion parameter 0 and discount d (Teh, 2006).
Our generative model of xi is unusual for two rea-
sons. First, we treat each xi independently rather
than linking them via a Markov chain. This makes
the model deficient, since li overlaps with xi?1 and
so forth, generating each token twice. During in-
ference, however, we will never compute the joint
probability of all the data at once, only the prob-
abilities of subsets of the variables with particular
intended word forms u and v. As long as no two of
these words are adjacent, the deficiency will have no
effect. We make this independence assumption for
computational reasons?when deciding whether to
merge u and v into a single lexical entry, we compute
the change in estimated probability for their contexts,
but not the effect on other words for which u and v
themselves appear as context words.
Also unusual is that we factor the joint probabil-
ity (l, x, r) as p(x)p(l|x)p(r|x) rather than as a left-
to-right chain p(l)p(x|l)p(r|x). Given our indepen-
dence assumption above, these two quantities are
mathematically equivalent, so the difference matters
only because we are using smoothed estimates. Our
factorization leads to a symmetric treatment of left
and right contexts, which simplifies implementation:
we can store all the context parameters locally as
PL(?|x) rather than distributed over various P (x|?).
Next, we explain our transducer T . A weighted
finite-state transducer (WFST) is a variant of a finite-
state automaton (Pereira et al, 1994) that reads an
input string symbol-by-symbol and probabilistically
produces an output string; thus it can be used to
specify a conditional probability on output strings
given an input. Our WFST (Figure 3) computes a
weighted edit distance, and is implemented using
OpenFST (Allauzen et al, 2007). It contains a state
for each triplet of (previous, current, next) phones;
conditioned on this state, it emits a character out-
put which can be thought of as a possible surface
realization of current in its particular environment.
The output can be the empty string , in which case
current is deleted. The machine can also insert char-
acters at any point in the string, by transitioning to an
insert state (previous, , current) and then returning
while emitting some new character.
The transducer is parameterized by the probabil-
ities of the arcs. For instance, all arcs leaving the
state (?, D, i) consume the character D and emit some
character c with probability p(c|?, D, i). Following
186
Figure 3: The fragment of the transducer responsible for
input string [Di] ?the?. ?...? represents an output arc for
each possible character, including the empty string ; ? is
the word boundary marker.
Dreyer et al (2008), we parameterize these distribu-
tions with a log-linear model. The model features are
based on articulatory phonetics and distinguish three
dimensions of sound production: voicing, place of
articulation and manner of articulation.
Features are generated from four positional tem-
plates (Figure 4): (curr)?out, (prev, curr)?out,
(curr, next)?out and (prev, curr, next)?out. Each
template is instantiated once per articulatory dimen-
sion, with prev, curr, next and out replaced by their
values for that dimension: for instance, there are
two voicing values, voiced and unvoiced1 and the
(curr)?out template for [D] producing [d] would
be instantiated as (voiced)?voiced. To capture
trends specific to particular sounds, each template
is instantiated again using the actual symbol for
curr and articulatory values for everything else (e.g.,
[D]?unvoiced). An additional template,?out, cap-
tures the marginal frequency of the output symbol.
There are also faithfulness features, same-sound,
same-voice, same-place and same-manner which
check if curr is exactly identical to out or shares
the exact value of a particular feature.
Our choice of templates and features is based on
standard linguistic principles: we expect that chang-
ing only a single articulatory dimension will be more
acceptable than changing several, and that the artic-
ulatory dimensions of context phones are important
because of assimilatory and dissimilatory processes
(Hayes, 2011). In modern phonetics and phonology,
these generalizations are usually expressed as Opti-
mality Theory constraints; log-linear models such as
ours have previously been used to implement stochas-
1We use seven place values and five manner values (stop,
nasal stop, fricative, vowel, other). Empty segments like  and ?
are assigned a special value ?no-value? for all features.
Figure 4: Some features generated for (?, D, i)? d. Each
black factor node corresponds to a positional template.
The features instantiated for the (curr)?out and ?out
template are shown in full, and we show some of the
features for the (curr,next)?out template.
tic Optimality Theory models (Goldwater and John-
son, 2003; Hayes and Wilson, 2008).
4 Inference
Global optimization of the model posterior is diffi-
cult; instead we use Viterbi EM (Spitkovsky et al,
2010; Allahverdyan and Galstyan, 2011). We begin
with a simple initial transducer and alternate between
two phases: clustering together surface forms, and
reestimating the transducer parameters. We iterate
this procedure until convergence (when successive
clustering phases find nearly the same set of merges);
this tends to take about 5 or 6 iterations.
In our clustering phase, we improve the model
posterior as much as possible by greedily making
type merges, where, for a pair of intended word forms
u and v, we replace all instances of xi = u with
xi = v. We maintain the invariant that each intended
word form?s most common surface form must be
itself; this biases the model toward solutions with
low distortion in the transducer.
4.1 Scoring merges
We write the change in the log posterior probability
of the model resulting from a type merge of u to v as
?(u, v), which factors into two terms, one depending
on the surface string and the transducer, and the other
depending on the string of intended words. In order to
ensure that each intended word form?s most common
surface form is itself, we define ?(u, v) = ?? if u
is more common than v.
We write the log probability of x being transduced
to s as T (s|x). If we merge u into v, we no longer
187
need to produce any surface forms from u, but instead
we must derive them from v. If #(?) counts the
occurrences of some event in the current state of the
model, the transducer component of ? is:
?T =
?
s
#(xi=u, si=s)(T (s|v)? T (s|u)) (1)
This term is typically negative, voting against a
merge, since u is more similar to itself than to v.
The language modeling term relating to the in-
tended string again factors into multiple components.
The probability of a particular li, xi, ri can be broken
into p(xi)p(li|xi)p(ri|xi) according to the model.
We deal first with the p(xi) unigram term, consid-
ering all tokens where xi ? {u, v} and computing
the probability pu = p(xi = u|xi ? {u, v}). By
definition of a Dirichlet process, the marginal over a
subset of the variables will be Dirichlet, so for ? > 1
we have the MAP estimate:
pu =
#(xi=u) + ?? 1
#(xi ? {u, v}) + 2(?? 1)
(2)
pv = p(xi = v|xi ? {u, v}) is computed similarly.
If we decide to merge u into v, however, the proba-
bility p(xi = v|xi ? {u, v}) becomes 1. The change
in log-probability resulting from the merge is closely
related to the entropy of the distribution:
?U = ?#(xi=u) log(pu)?#(xi=v) log(pv) (3)
This change must be positive and favors merging.
Next, we consider the change in probability from
the left contexts (the derivations for right contexts are
equivalent). If u and v are separate words, we gen-
erate their left contexts from different distributions
p(l|u) and p(l|v), while if they are merged, we must
generate all the contexts from the same distribution
p(l|{u, v}). This change is:
?L =
?
l
#(l, u){log(p(l|{u, v}))? log(p(l|u)}
+
?
l
#(l, v){log(p(l|{u, v}))? log(p(l|v)}
In a full Bayesian model, we would integrate over
the parameters of these distributions; instead, we
use Kneser-Ney smoothing (Kneser and Ney, 1995)
which has been shown to approximate the MAP solu-
tion of a hierarchical Pitman-Yor model (Teh, 2006;
Goldwater et al, 2006). The Kneser-Ney discount2
d is a tunable parameter of our system, and con-
trols whether the term favors merging or not. If d is
small, p(l|u) and p(l|v) are close to their maximum-
likelihood estimates, and ?L is similar to a Jensen-
Shannon divergence; it is always negative and dis-
courages mergers. As d increases, however, p(l|u)
for rare words approaches the prior distribution; in
this case, merging two words may result in better
posterior parameters than estimating both separately,
since the combined estimate loses less mass to dis-
counting.
Because neither the transducer nor the language
model are perfect models of the true distribution,
they can have incompatible dynamic ranges. Often,
the transducer distribution is too peaked; to remedy
this, we downweight the transducer probability by
?, a parameter of our model, which we set to .5.
Downweighting of the acoustic model versus the LM
is typical in speech recognition (Bahl et al, 1980).
To summarize, the full change in posterior is:
?(u, v) = ?U + ?L + ?R + ??T (4)
There are four parameters. The transducer regular-
ization r = 1 and unigram prior ? = 2, which we
set ad-hoc, have little impact on performance. The
Kneser-Ney discount d = 2 and transducer down-
weight ? = .5 have more influence and were tuned
on development data.
4.2 Clustering algorithm
In the clustering phase, we start with an initial solu-
tion in which each surface form is its own intended
pronunciation and iteratively improve this solution
by merging together word types, picking (approxi-
mately) the best merger at each point.
We begin by computing a set of candidate mergers
for each surface word type u. This step saves time
by quickly rejecting mergers which are certain to get
very low transducer scores. We reject a pair u, v if
the difference in their length is greater than 4, or if
both words are longer than 4 segments, but, when
we consider them as unordered bags of segments, the
Dice coefficient between them is less than .5.
For each word u and all its candidates v, we com-
pute ?(u, v) as in Equation 4. We keep track of the
2We use one discount, rather than several as in modified KN.
188
Input: vocabulary of surface forms u
Input: C(u): candidate intended forms of u
Output: intend(u): intended form of u
foreach u ? vocab do
// initialization
v?(u)? argmaxv ?C(u) ?(u, v);
??(u)? ?(u, v?(u))
intend(u)? u
add u to queue Q with priority ??(u))
while top(Q) > ?? do
u? pop(Q)
recompute v?(u),??(u)
if ??(u) > 0 then
// merge u with best merger
intend(u)? v?(u)
update ?(x, u) ?x : v?(x) = u
remove u from C(x) ?x
update ?(x, v) ?x : v?(x) = v
update ?(v, x) ?x ? C(v)
if updated ? > ?? for any words then
reset ??, v? for those words
// (these updates can
increase a word?s priority
from ??)
else if ??(u) 6= ?? then
// reject but leave in queue
??(u)? ??
Algorithm 1: Our clustering phase.
current best target v?(u) and best score ??(u), using
a priority queue. At each step of the algorithm, we
pop the u with the current best ??(u), recompute
its scores, and then merge it with v?(u) if doing so
would improve the model posterior. In an exact al-
gorithm, we would then need to recompute most of
the other scores, since merging u and v?(u) affects
other words for which u and v?(u) are candidates,
and also words for which they appear in the context
set. However, recomputing all these scores would be
extremely time-consuming.3 Therefore, we recom-
pute scores for only those words where the previous
best merger was either u or v?(u). (If the best merge
would not improve the probability, we reject it, but
since its score might increase if we merge v?(u), we
leave u in the queue, setting its ? score to ??; this
score will be updated if we merge v?(u).)
Since we recompute the exact scores ?(u, v) im-
mediately before merging u, the algorithm is guaran-
3The transducer scores can be cached since they depend only
on surface forms, but the language model scores cannot.
teed never to reduce the posterior probability. It can
potentially make changes in the wrong order, since
not all the ?s are recomputed in each step, but most
changes do not affect one another, so performing
them out of order has no impact. Empirically, we
find that mutually exclusive changes (usually of the
form (u, v) and (v, w)) tend to differ enough in initial
score that they are evaluated in the correct order.
4.3 Training the transducer
To train the transducer on a set of mappings between
surface and intended forms, we find the maximum-
probability state sequence for each mapping (another
application of Viterbi EM) and extract features for
each state and its output. Learning weights is then
a maximum-entropy problem, which we solve using
Orthant-wise Limited-memory Quasi-Newton.4
To construct our initial transducer, we first learn
weights for the marginal distribution on surface
sounds by training the max-ent system with only the
bias features active. Next, we manually set weights
(Table 1) for insertions and deletions, which do not
appear on the surface, and for faithfulness features.
Other features get an initial weight of 0.
5 Experiments
5.1 Dataset
Our corpus is a processed version of the Bernstein-
Ratner corpus (Bernstein-Ratner, 1987) from
CHILDES (MacWhinney, 2000), which contains or-
thographic transcriptions of parent-child dyads with
infants aged 13-23 months. Brent and Cartwright
(1996) created a phonemic version of this corpus
by extracting all infant-directed utterances and con-
verted them to a phonemic transcription using a dic-
tionary. This version, which contains 9790 utterances
(33399 tokens, 1321 types), is now standard for word
segmentation, but contains no phonetic variability.
Since producing a close phonetic transcription of
this data would be impractical, we instead construct
an approximate phonetic version using information
from the Buckeye corpus (Pitt et al, 2007). Buckeye
is a corpus of adult-directed conversational Ameri-
can English, and has been phonetically transcribed
4We use the implementation of Andrew and Gao (2007) with
an l2 regularizer and regularization parameter r = 1; although
this could be tuned, in practice it has little effect on results.
189
Feature Weight
output-is-x marginal p(x)
output-is- 0
same-sound 5
same-{place,voice, manner} 2
insertion -3
Table 1: Initial transducer weights.
?about? ahbawt:15, bawt:9, ihbawt:4, ahbawd:4, ih-
bawd:4, ahbaat:2, baw:1, ahbaht:1, erbawd:1,
bawd:1, ahbaad:1, ahpaat:1, bah:1, baht:1,
ah:1, ahbahd:1, ehbaat:1, ahbaed:1, ihbaht:1,
baot:1
?wanna? waanah:94, waanih:37, wahnah:16, waan:13,
wahneh:8, wahnih:5, wahney:3, waanlih:3,
wehnih:2, waaneh:2, waonih:2, waaah:1,
wuhnih:1, wahn:1, waantah:1, waanaa:1,
wowiy:1, waaih:1, wah:1, waaniy:1
Table 2: Empirical distribution of pronunciations of
?about? and ?wanna? in our dataset.
by hand to indicate realistic pronunciation variability.
To create our phonetic corpus, we replace each phone-
mic word in the Bernstein-Ratner-Brent corpus with
a phonetic pronunciation of that word sampled from
the empirical distribution of pronunciations in Buck-
eye (Table 2). If the word never occurs in Buckeye,
we use the original phonemic version.
Our corpus is not completely realistic as a sam-
ple of child-directed speech. Since each pronuncia-
tion is sampled independently, it lacks coarticulation
and prosodic effects, and the distribution of pronun-
ciations is derived from adult-directed rather than
child-directed speech. Nonetheless, it represents pho-
netic variability more realistically than the Bernstein-
Ratner-Brent corpus, while still maintaining the lexi-
cal characteristics of infant-directed speech (as com-
pared to the Buckeye corpus, with its much larger
vocabulary and more complex language model).
We conduct our development experiments on the
first 8000 input utterances, holding out the remain-
ing 1790 for evaluation. For evaluation experiments,
we run the system on all 9790 utterances, reporting
scores on only the last 1790.
5.2 Metrics
We evaluate our results by generalizing the three
segmentation metrics from Goldwater et al (2009):
word boundary F-score, word token F-score, and
lexicon (word type) F-score.
0 1 2 3 4 5Iteration
75
76
77
78
79
80
81
82
Token F
Lexicon F
Figure 5: System scores over 5 iterations.
In our first set of experiments we evaluate how
well our system clusters together surface forms de-
rived from the same intended form, assuming gold
standard word boundaries. We do not evaluate the
induced intended forms directly against the gold stan-
dard intended forms?we want to evaluate cluster
memberships and not labels. Instead we compute
a one-to-one mapping between our induced lexical
items and the gold standard, maximizing the agree-
ment between the two (Haghighi and Klein, 2006).
Using this mapping, we compute mapped token F-
score5 and lexicon F-score.
In our second set of experiments, we use unknown
word boundaries and evaluate the segmentations. We
report the standard word boundary F and unlabeled
word token F as well as mapped F. The unlabeled to-
ken score counts correctly segmented tokens, whether
assigned a correct intended form or not.
5.3 Known word boundaries
We first run our system with known word boundaries
(Table 3). As a baseline, we treat every surface token
as its own intended form (none). This baseline has
fairly high accuracy; 65% of word tokens receive
the most common pronunciation for their intended
form.6 As an upper bound, we find the best intended
form for each surface type (type ubound). This cor-
rectly resolves 91% of tokens; the remaining error is
due to homophones (surface types corresponding to
more than one intended form). We also test our sys-
5When using the gold word boundaries, the precision and
recall are equal and this is is the same as the accuracy; in seg-
mentation experiments the two differ, because with fewer seg-
mentation boundaries, the system proposes fewer tokens. Only
correctly segmented tokens which are also mapped to the correct
form count as matches.
6The lexicon recall is not quite 100% because one rare word
appears only as a homophone of another word.
190
System Tok F Lex P Lex R Lex F
none 65.4 50.2 99.7 66.7
initializer 75.2 83.2 73.3 78.0
system 79.2 87.1 75.9 81.1
oracle trans. 82.7 88.7 83.8 86.2
type ubound 91.0 97.5 98.0 97.7
Table 3: Results on 1790 utterances (known boundaries).
Boundaries Unlabeled Tokens
P R F P R F
no var. 90.1 80.3 84.9 74.5 68.7 71.5
w/var. 70.4 93.5 80.3 56.5 69.7 62.4
Table 4: Degradation in dpseg segmentation perfor-
mance caused by pronunciation variation.
Mapped Tokens Lexicon (types)
P R F P R F
none 39.8 49.0 43.9 37.7 49.1 42.6
init 42.2 52.0 56.5 50.1 40.8 45.0
sys 44.2 54.5 48.8 48.6 43.1 45.7
Table 5: Results on 1790 utterances (induced boundaries).
tem using an oracle transducer (oracle trans.)?the
transducer estimated from the upper-bound mapping.
This scores 83%, showing that our articulatory fea-
ture set captures most, but not all, of the available
information. At the beginning of bootstrapping, our
system (init) scores 75%, but this improves to 79%
after five iterations of reestimation (system). Most
learning occurs in the first two or three iterations
(Figure 5).
To determine the importance of different parts of
our system, we run a few ablation tests on develop-
ment data. Context information is critical to obtain
a good solution; setting ?L and ?R to 0 lowers our
dev token F-score from 83% to 75%. Initializing
all feature weights to 0 yields a poor initial solution
(18% dev token F instead of 75%), but after learn-
ing the result is only slightly lower than using the
weights in Table 1 (78% rather than 80%), showing
that the system is quite robust to initialization.
5.4 Unknown word boundaries
As a simple extension of our model to the case of
unknown word boundaries, we interleave it with an
existing model of word segmentation, dpseg (Gold-
water et al, 2009).7 In each iteration, we run the
segmenter, then bootstrap our model for five itera-
tions on the segmented output. We then concatenate
the intended word sequence proposed by our model
to produce the next iteration?s segmenter input.
Phonetic variation is known to reduce the perfor-
mance of dpseg (Fleck, 2008; Boruta et al, 2011)
and our experiments confirm this (Table 4). Using
induced word boundaries also makes it harder to
recover the lexicon (Table 5), lowering the baseline
F-score from 67% to 43%. Nevertheless, our system
improves the lexicon F-score to 46%, with token F
rising from 44% to 49%, demonstrating the system?s
ability to work without gold word boundaries. Un-
fortunately, performing multiple iterations between
the segmenter and lexical-phonetic learner has little
further effect; we hope to address this issue in future.
6 Conclusion
We have presented a noisy-channel model that si-
multaneously learns a lexicon, a bigram language
model, and a model of phonetic variation, while us-
ing only the noisy surface forms as training data.
It is the first model of lexical-phonetic acquisition
to include word-level context and to be tested on an
infant-directed corpus with realistic phonetic variabil-
ity. Whether trained using gold standard or automati-
cally induced word boundaries, the model recovers
lexical items more effectively than a system that as-
sumes no phonetic variability; moreover, the use of
word-level context is key to the model?s success. Ul-
timately, we hope to extend the model to jointly infer
word boundaries along with lexical-phonetic knowl-
edge, and to work directly from acoustic input. How-
ever, we have already shown that lexical-phonetic
learning from a broad-coverage corpus is possible,
supporting the claim that infants acquire lexical and
phonetic knowledge simultaneously.
Acknowledgements
This work was supported by EPSRC grant
EP/H050442/1 to the second author.
7dpseg1.2 from http://homepages.inf.ed.ac.
uk/sgwater/resources.html
191
References
Guillaume Aimetti. 2009. Modelling early language
acquisition skills: Towards a general statistical learning
mechanism. In Proceedings of the Student Research
Workshop at EACL.
Armen Allahverdyan and Aram Galstyan. 2011. Compar-
ative analysis of Viterbi training and ML estimation for
HMMs. In Advances in Neural Information Processing
Systems (NIPS).
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst:
A general and efficient weighted finite-state trans-
ducer library. In Proceedings of the Ninth Interna-
tional Conference on Implementation and Application
of Automata, (CIAA 2007), volume 4783 of Lecture
Notes in Computer Science, pages 11?23. Springer.
http://www.openfst.org.
Galen Andrew and Jianfeng Gao. 2007. Scalable training
of L1-regularized log-linear models. In ICML ?07.
Lalit Bahl, Raimo Bakis, Frederick Jelinek, and Robert
Mercer. 1980. Language-model/acoustic-channel-
model balance mechanism. Technical disclosure bul-
letin Vol. 23, No. 7b, IBM, December.
Nan Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck, editors,
Children?s Language, volume 6. Erlbaum, Hillsdale,
NJ.
L. Boruta, S. Peperkamp, B. Crabbe?, E. Dupoux, et al
2011. Testing the robustness of online word segmenta-
tion: effects of linguistic diversity and phonetic varia-
tion. ACL HLT 2011, page 1.
Michael Brent and Timothy Cartwright. 1996. Distribu-
tional regularity and phonotactic constraints are useful
for segmentation. Cognition, 61:93?125.
Michael R. Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discovery.
Machine Learning, 34:71?105, February.
R. Daland and J.B. Pierrehumbert. 2010. Learning
diphone-based segmentation. Cognitive Science.
Markus Dreyer, Jason R. Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions with
finite-state methods. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing,
EMNLP ?08, pages 1080?1089, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Joris Driesen, Louis ten Bosch, and Hugo Van hamme.
2009. Adaptive non-negative matrix factorization in
a computational model of language acquisition. In
Proceedings of Interspeech.
E. Dupoux, G. Beraud-Sudreau, and S. Sagayama. 2011.
Templatic features for modeling phoneme acquisition.
In Proceedings of the 33rd Annual Cognitive Science
Society.
Naomi Feldman, Thomas Griffiths, and James Morgan.
2009. Learning phonetic categories by learning a lexi-
con. In Proceedings of the 31st Annual Conference of
the Cognitive Science Society (CogSci).
Naomi Feldman. 2011. Interactions between word and
speech sound categorization in language acquisition.
Ph.D. thesis, Brown University.
Margaret M. Fleck. 2008. Lexicalized phonotactic word
segmentation. In Proceedings of ACL-08: HLT, pages
130?138, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Sharon Goldwater and Mark Johnson. 2003. Learning OT
constraint rankings using a maximum entropy model.
In J. Spenader, A. Eriksson, and Osten Dahl, editors,
Proceedings of the Stockholm Workshop on Variation
within Optimality Theory, pages 111?120, Stockholm.
Stockholm University.
Sharon Goldwater, Tom Griffiths, and Mark Johnson.
2006. Interpolating between types and tokens by esti-
mating power-law generators. In Advances in Neural
Information Processing Systems (NIPS) 18.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. In In 46th
Annual Meeting of the ACL, pages 398?406.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 320?327, New York
City, USA, June. Association for Computational Lin-
guistics.
Bruce Hayes and Colin Wilson. 2008. A maximum en-
tropy model of phonotactics and phonotactic learning.
Linguistic Inquiry, 39(3):379?440.
Bruce Hayes. 2011. Introductory Phonology. John Wiley
and Sons.
A. Jansen, K. Church, and H. Hermansky. 2010. Towards
spoken term discovery at scale with zero resources. In
Proceedings of Interspeech, pages 1676?1679.
R. Kneser and H. Ney. 1995. Improved backing-off for M-
gram language modeling. In Proc. ICASSP ?95, pages
181?184, Detroit, MI, May.
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Vol 2: The Database. Lawrence
Erlbaum Associates, Mahwah, NJ, 3rd edition.
Fergus R. McInnes and Sharon Goldwater. 2011. Un-
supervised extraction of recurring words from infant-
directed speech. In Proceedings of the 33rd Annual
Conference of the Cognitive Science Society.
A. S. Park and J. R. Glass. 2008. Unsupervised pat-
tern discovery in speech. IEEE Transactions on Audio,
Speech and Language Processing, 16:186?197.
192
Fernando Pereira, Michael Riley, and Richard Sproat.
1994. Weighted rational transductions and their ap-
plication to human language processing. In HLT.
Mark A. Pitt, Laura Dilley, Keith Johnson, Scott Kies-
ling, William Raymond, Elizabeth Hume, and Eric
Fosler-Lussier. 2007. Buckeye corpus of conversa-
tional speech (2nd release).
Okko Ra?sa?nen. 2011. A computational model of word
segmentation from continuous speech using transitional
probabilities of atomic acoustic events. Cognition,
120(2):28.
Anton Rytting. 2007. Preserving Subsegmental Varia-
tion in Modeling Word Segmentation (Or, the Raising
of Baby Mondegreen). Ph.D. thesis, The Ohio State
University.
Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,
and Christopher D. Manning. 2010. Viterbi training
improves unsupervised dependency parsing. In Pro-
ceedings of the Fourteenth Conference on Computa-
tional Natural Language Learning, pages 9?17, Up-
psala, Sweden, July. Association for Computational
Linguistics.
D. Swingley. 2005. Statistical clustering and the contents
of the infant vocabulary. Cognitive Psychology, 50:86?
132.
Yee Whye Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceedings
of the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 985?992, Sydney,
Australia, July. Association for Computational Linguis-
tics.
G.K. Vallabha, J.L. McClelland, F. Pons, J.F. Werker, and
S. Amano. 2007. Unsupervised learning of vowel
categories from infant-directed speech. Proceedings
of the National Academy of Sciences, 104(33):13273?
13278.
B. Varadarajan, S. Khudanpur, and E. Dupoux. 2008. Un-
supervised learning of acoustic sub-word units. In Pro-
ceedings of the 46th Annual Meeting of the Association
for Computational Linguistics on Human Language
Technologies: Short Papers, pages 165?168. Associa-
tion for Computational Linguistics.
A. Venkataraman. 2001. A statistical model for word
discovery in transcribed speech. Computational Lin-
guistics, 27(3):351?372.
193
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 488?496,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Semantic Parsing with Bayesian Tree Transducers
Bevan Keeley Jones??
b.k.jones@sms.ed.ac.uk
Mark Johnson?
Mark.Johnson@mq.edu.au
Sharon Goldwater?
sgwater@inf.ed.ac.uk
? School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
? Department of Computing
Macquarie University
Sydney, NSW 2109, Australia
Abstract
Many semantic parsing models use tree trans-
formations to map between natural language
and meaning representation. However, while
tree transformations are central to several
state-of-the-art approaches, little use has been
made of the rich literature on tree automata.
This paper makes the connection concrete
with a tree transducer based semantic parsing
model and suggests that other models can be
interpreted in a similar framework, increasing
the generality of their contributions. In par-
ticular, this paper further introduces a varia-
tional Bayesian inference algorithm that is ap-
plicable to a wide class of tree transducers,
producing state-of-the-art semantic parsing re-
sults while remaining applicable to any do-
main employing probabilistic tree transducers.
1 Introduction
Semantic parsing is the task of mapping natural lan-
guage sentences to a formal representation of mean-
ing. Typically, a system is trained on pairs of natural
language sentences (NLs) and their meaning repre-
sentation expressions (MRs), as in figure 1(a), and
the system must generalize to novel sentences.
Most semantic parsing models rely on an assump-
tion of structural similarity between MR and NL.
Since strict isomorphism is overly restrictive, this
assumption is often relaxed by applying transforma-
tions. Several approaches assume a tree structure to
the NL, MR, or both (Ge and Mooney, 2005; Kate
and Mooney, 2006; Wong and Mooney, 2006; Lu
et al, 2008; Bo?rschinger et al, 2011), and often in-
Figure 1: (a) An example sentence/meaning pair, (b) a
tree transformation based mapping, and (c) a tree trans-
ducer that performs the mapping.
volve tree transformations either between two trees
or a tree and a string.
The tree transducer, a formalism from automata
theory which has seen interest in machine transla-
tion (Yamada and Knight, 2001; Graehl et al, 2008)
and has potential applications in many other areas,
is well suited to formalizing such tree transforma-
tion based models. Yet, while many semantic pars-
ing systems resemble the formalism, each was pro-
posed as an independent model requiring custom al-
gorithms, leaving it unclear how developments in
one line of inquiry relate to others. We argue for a
unifying theory of tree transformation based seman-
tic parsing by presenting a tree transducer model and
drawing connections to other similar systems.
We make a further contribution by bringing to
tree transducers the benefits of the Bayesian frame-
work for principled handling of data sparsity and
488
prior knowledge. Graehl et al (2008) present an EM
training procedure for top down tree transducers, but
while there are Bayesian approaches to string trans-
ducers (Chiang et al, 2010) and PCFGs (Kurihara
and Sato, 2006), there has yet to be a proposal for
Bayesian inference in tree transducers. Our vari-
ational algorithm produces better semantic parses
than EM while remaining general to a broad class
of transducers appropriate for other domains.
In short, our contributions are three-fold: we
present a new state-of-the-art semantic parsing
model, propose a broader theory for tree transforma-
tion based semantic parsing, and present a general
inference algorithm for the tree transducer frame-
work. We recommend the last of these as just one
benefit of working within a general theory: contri-
butions are more broadly applicable.
2 Meaning representations and regular
tree grammars
In semantic parsing, an MR is typically an expres-
sion from a machine interpretable language (e.g., a
database query language or a logical language like
Prolog). In this paper we assume MRs can be rep-
resented as trees, either by pre-parsing or because
they are already trees (often the case for functional
languages like LISP).1 More specifically, we assume
the MR language is a regular tree language.
A regular tree grammar (RTG) closely resembles
a context free grammar (CFG), and is a way of de-
scribing a language of trees. Formally, define T? as
the set of trees with symbols from alphabet ?, and
T?(A) as the set of all trees in T??A where symbols
from A only occur at the leaves. Then an RTG is a
tuple (Q,?, qstart,R), where Q is a set of states, ?
is an alphabet, qstart ? Q is the initial state, and R
is a set of grammar rules of the form q ? t, where q
is a state from Q and t is a tree from T?(Q).
A rule typically consists of a parent state (left) and
its child states and output symbol (right). We indi-
cate states using all capital letters:
NUM ? population(PLACE).
Intuitively, an RTG is a CFG where the yield of
every parse is itself a tree. In fact, for any CFG G, it
1See Liang et al (2011) for work in representing lambda
calculus expressions with trees.
is straightforward to produce a corresponding RTG
that generates the set of parses of G. Consequently,
while we assume we have an RTG for the MR lan-
guage, there is no loss of generality if the MR lan-
guage is actually context free.
3 Weighted root-to-frontier, linear,
non-deleting tree-to-string transducers
Tree transducers (Rounds, 1970; Thatcher, 1970) are
generalizations of finite state machines that operate
on trees. Mirroring the branching nature of its in-
put, the transducer may simultaneously transition to
several successor states, assigning a separate state to
each subtree.
There are many classes of transducer with dif-
ferent formal properties (Knight and Greahl, 2005;
Maletti et al, 2009). Figure 1(c) is an example of
a root-to-frontier, linear, non-deleting tree-to-string
transducer. It is defined using rules where the left
hand side identifies a state of the transducer and a
fragment of the input tree, and the right hand side
describes a portion of the output string. Variables
xi stand for entire sub-trees, and state-variable pairs
qj .xi stand for strings produced by applying the
transducer starting at state qj to subtree xi. Fig-
ure 1(b) illustrates an application of the transducer,
taking the tree on the left as input and outputting the
string on the right.
Formally, a weighted root-to-frontier, tree-to-
string transducer is a 5-tuple (Q,?,?, qstart,R). Q
is a finite set of states, ? and ? are the input and out-
put alphabets, qstart is the start state, and R is the
set of rules. Denote a pair of symbols, a and b by
a.b, the cross product of two sets A and B by A.B,
and let X be the set of variables {x0, x1, ...}. Then,
each rule r ? R is of the form [q.t ? u].v, where
v ? ??0 is the rule weight, q ? Q, t ? T?(X ), and
u is a string in (? ? Q.X )? such that every x ? X
in u also occurs in t.
We say q.t is the left hand side of rule r and u its
right hand side. The transducer is linear iff no vari-
able appears more than once on the right hand side.
It is non-deleting iff all variables on the left hand
side also occur on the right hand side. In this paper
we assume that every tree t on the left hand side is ei-
ther a single variable x0 or of the form ?(x0, ...xn),
where ? ? ? (i.e., it is a tree of depth ? 1).
489
A weighted tree transducer may define a probabil-
ity distribution, either a joint distribution over input
and output pairs or a conditional distribution of the
output given the input. Here, we will use joint dis-
tributions, which can be defined by ensuring that the
weights of all rules with the same state on the left-
hand side sum to one. In this case, it can be help-
ful to view the transducer as simultaneously gener-
ating both the input and output, rather than the usual
view of mapping input trees into output strings. A
joint distribution allows us to model with a single
machine both the input and output languages, which
is important during decoding when we want to infer
the input given the output.
4 A generative model of semantic parsing
Like the hybrid tree semantic parser (Lu et al, 2008)
and the synchronous grammar based WASP (Wong
and Mooney, 2006), our model simultaneously gen-
erates the input MR tree and the output NL string.
The MR tree is built up according to the provided
MR grammar, one grammar rule at a time. Coupled
with the application of the MR rule, similar CFG-
like productions are applied to the NL side, repeated
until both the MR and NL are fully generated. In
each step, we select an MR rule and then build the
NL by first choosing a pattern with which to expand
it and then filling out that pattern with words drawn
from a unigram distribution.
This kind of coupled generative process can
be naturally formalized with tree transducer rules,
where the input tree fragment on the left side of each
rule describes the derivation of the MR and the right
describes the corresponding NL derivation.
For a simple example of a tree-to-string trans-
ducer rule consider
q.population(x1) ? ?population of? q.x1 (1)
which simultaneously generates tree fragment
population(x1) on the left and sub-string ?popula-
tion of q.x1? on the right. Variable x1 stands for
an MR subtree under population, and, on the right,
state-variable pair q.x1 stands for the NL substring
generated while processing subtree x1 starting from
q. While this rule can serve as a single step of
an MR-to-NL map such as the example transducer
shown in Figure 1(c), such rules do not model the
NUM ? population(PLACE) (m)
PLACE ? cityid(CITY, STATE) (r)
CITY ? portland (u)
STATE ? maine (v)
qMRm,1.x1 ? qNLr .x1 (2)
qMRr,1 .x1 ? qNLu .x1
qMRr,2 .x1 ? qNLv .x1
qNLm .population(w1, x1, w2) ?
qWm .w1 qMRm,1.x1 qEND.w2 (3)
qNLr .cityid(w1, x1, w2, x2, w3) ?
qEND.w1 qMRr,2 .x2 qWr .w2 qMRr,1 .x1 qEND.w3 (4)
qWm .w1 ? ?population? qWm .w1 (5)
qWm .w1 ? ?of? qWm .w1
qWm .w1 ? ... qWm .w1
qWm .w1 ? ?of? qEND.w1 (6)
qWm .w1 ? ... qEND.w1
qEND.W ? ? (7)
Figure 2: Examples of transducer rules (bottom) that gen-
erate MR and NL associated with MR rules m-v (top).
Transducer rule 2 selects MR rule r from the MR gram-
mar. Rule 3 simultaneously writes the MR associated
with rule m and chooses an NL pattern (as does 4 for
r). Rules 5-7 generate the words associated with m ac-
cording to a unigram distribution specific to m.
grammaticality of the MR and lack flexibility since
sub-strings corresponding to a given tree fragment
must be completely pre-specified. Instead, we break
transductions down into a three stage process of
choosing the (i) MR grammar rule, (ii) NL expan-
sion pattern, and (iii) individual words according to
a unigram distribution. Such a decomposition in-
corporates independence assumptions that improve
generalizability. See Figure 2 for example rules
from our transducer and Figure 3 for a derivation.
To ensure that only grammatical MRs are gener-
ated, each state of our transducer encodes the iden-
tity of exactly one MR grammar rule. Transitions
between qMR and qNL states implicitly select the em-
bedded rule. For instance, rule 2 in Figure 2 selects
490
MR grammar rule r to expand the ith child of the
parent produced by rule m. Aside from ensuring
the grammaticality of the generated MR, rules of
this type also model the probability of the MR, con-
ditioning the probability of a rule both on the par-
ent rule and the index of the child being expanded.
Thus, parent state qMRm,1 encodes not only the identity
of rule m, but also the child index, 1 in this case.
Once the MR rule is selected, qNL states are ap-
plied to select among rules such as 3 and 4 to gen-
erate the MR entity and choose the NL expansion
pattern. These rules determine the word order of the
language by deciding (i) whether or not to generate
words in a given location and (ii) where to insert the
result of processing each MR subtree. Decision (i) is
made by either transitioning to state qWr to generate
words or to qEND to generate the empty string. De-
cision (ii) is made with the order of xi?s on the right
hand side. Rule 4 illustrates the case where port-
land and maine in cityid(portland, maine) would be
realized in reverse order as ?maine ... portland?.
The particular set of patterns that appear on the
right of rules such as 3 embodies the binary word at-
tachment decisions and the particular permutation of
xi in the NL. We allow words to be generated at the
beginning and end of each pattern and between the
xis. Thus, rule 4 is just one of 16 such possible pat-
terns (3 binary decisions and 2 permutations), while
rule 3 is one of 4. We instantiate all such rules and
allow the system to learn weights for them according
to the language of the training data.
Finally, the NL is filled out with words chosen ac-
cording to a unigram distribution, implemented in a
PCFG-like fashion, using a different rule for each
word which recursively chooses the next word un-
til a string termination rule is reached.2 Generating
word sequence ?population of? entails first choosing
rule 5 in Figure 2. State qWr is then recursively ap-
plied to choose rule 6, generating ?of? at the same
time as deciding to terminate the string by transi-
tioning to a new state qEND which deterministically
concludes by writing the empty string ?.
On the MR side, rules 5-7 do very little: the tree
on the left side of rules 5 and 6 consists entirely of a
2There are roughly 25,000 rules in the transducers in our
experiments, and the majority of these implement the unigram
word distributions since every entity in the MR may potentially
produce any of the words it is paired with in training.
subtree variable w1, indicating that nothing is gener-
ated in the MR. Rule 7 subsequently generates these
subtrees as W symbols, marking corresponding lo-
cations where words might be produced in the NL,
which are later removed during post processing.3
Figure 3(b) illustrates the coupled generative pro-
cess. At each step of the derivation, an MR rule is
chosen to expand a node of the MR tree, and then a
corresponding part of the NL is expanded. Step 1.1
of the example chooses MR rule m, NUM ?
population(PLACE). Transducer rule 3 then gener-
ates population in the MR (shown in the left column)
at the same time as choosing an NL expansion pat-
tern (Step 1.2) which is subsequently filled out with
specific words ?population? (1.3) and ?of? (1.4).
This coupled derivation can be represented by a
tree, shown in Figure 3(c), which explicitly repre-
sents the dependency structure of the coupled MR
and NL (a simplified version is shown in (d) for clar-
ity). In our transducer, which defines a joint distri-
bution over both the MR and NL, the probability of
a rule is conditioned on the parent state. Since each
state encodes an MR rule, MR rule specific distribu-
tions are learned for both the words and their order.
5 Relation to existing models
The tree transducer model can be viewed either as
a generative procedure for building up two separate
structures or as a transformative machine that takes
one as input and produces another as output. Dif-
ferent semantic parsing approaches have taken one
or the other view, and both can be captured in this
single framework.
WASP (Wong and Mooney, 2006) is an exam-
ple of the former perspective, coupling the genera-
tion of the MR and NL with a synchronous gram-
mar, a formalism closely related to tree transducers.
The most significant difference from our approach
is that they use machine translation techniques for
automatically extracting rules from parallel corpora;
similar techniques can be applied to tree transduc-
ers (Galley et al, 2004). In fact, synchronous gram-
mars and tree transducers can be seen as instances of
the same more general class of automata (Shieber,
3The addition of W symbols is a convenience; it is easier to
design transducer rules where every substring on the right side
corresponds to a subtree on the left.
491
Figure 3: Coupled derivation of an (MR, NL) pair. At each step an MR grammar rule is chosen to expand the MR and
the corresponding portion of the NL is then generated. Symbols W stand for locations in the tree corresponding to
substrings of the output and are removed in a post-processing step. (a) The (MR, NL) pair. (b) Step by step derivation.
(c) The same derivation shown in tree form. (d) The underlying dependency structure of the derivation.
2004). Rather than argue for one or the other, we
suggest that other approaches could also be inter-
preted in terms of general model classes, grounding
them in a broader base of theory.
The hybrid tree model (Lu et al, 2008) takes
a transformative perspective that is in some ways
more similar to our model. In fact, there is a one-
to-one relationship between the multinomial param-
eters of the two models. However, they represent the
MR and NL with a single tree and apply tree walk-
ing algorithms to extract them. Furthermore, they
implement a custom training procedure for search-
ing over the potential MR transformations. The tree
transducer, on the other hand, naturally captures the
same probabilistic dependencies while maintaining
the separation between MR and NL, and further al-
lows us to build upon a larger body of theory.
KRISP (Kate and Mooney, 2006) uses string clas-
sifiers to label substrings of the NL with entities
from the MR. To focus search, they impose an or-
dering constraint based on the structure of the MR
tree, which they relax by allowing the re-ordering
of sibling nodes and devise a procedure for recover-
ing the MR from the permuted tree. This procedure
corresponds to backward-application in tree trans-
ducers, identifying the most likely input tree given a
492
particular output string.
SCISSOR (Ge and Mooney, 2005) takes syntactic
parses rather than NL strings and attempts to trans-
late them into MR expressions. While few seman-
tic parsers attempt to exploit syntactic information,
there are techniques from machine translation for
using tree transducers to map between parsed par-
allel corpora, and these techniques could likely be
applied to semantic parsing.
Bo?rschinger et al (2011) argue for the PCFG as
an alternative model class, permitting conventional
grammar induction techniques, and tree transducers
are similar enough that many techniques are applica-
ble to both. However, the PCFG is less amenable to
conceptualizing correspondences between parallel
structures, and their model is more restrictive, only
applicable to domains with finite MR languages,
since their non-terminals encode entire MRs. The
tree transducer framework, on the other hand, allows
us to condition on individual MR rules.
6 Variational Bayes for tree transducers
As seen in the example in Figure 3(c), tree trans-
ducers not only operate on trees, their derivations
are themselves trees, making them amenable to dy-
namic programming and an EM training procedure
resembling inside-outside (Graehl et al, 2008). EM
assigns zero probability to events not seen in the
training data, however, limiting the ability to gen-
eralize to novel items. The Bayesian framework of-
fers an elegant solution to this problem, introducing
a prior over rule weights which simultaneously en-
sures that all rules receive non-zero probability and
allows the incorporation of prior knowledge and in-
tuitions. Unfortunately, the introduction of a prior
makes exact inference intractable, so we use an ap-
proximate method, variational Bayesian inference
(Bishop, 2006), deriving an algorithm similar to that
for PCFGs (Kurihara and Sato, 2006).
The tree transducer defines a joint distribution
over the input y, output w, and their derivation x
as the product of the weights of the rules appearing
in x. That is,
p(y, x, w|?) =
?
r?R
?(r)cr(x)
where ? is the set of multinomial parameters, r is a
transducer rule, ?(r) is its weight, and cr(x) is the
number of times r appears in x. In EM, we are in-
terested in the point estimate for ? that maximizes
p(Y,W|?), where Y and W are the N input-output
pairs in the training data. In the Bayesian setting,
however, we place a symmetric Dirichlet prior over
? and estimate a posterior distribution over both X
and ?.
p(?,X|Y,W) = p(Y,X ,W, ?)p(Y,W)
= p(?)
?N
i=1 p(yi, xi, wi|?)
?
p(?)?Ni=1
?
x?Xi p(yi, x, wi|?)d?
Since the integral in the denominator is in-
tractable, we look for an appropriate approximation
q(?,X ) ? p(?,X|Y,W). In particular, we assume
the rule weights and the derivations are independent,
i.e., q(?,X ) = q(?)q(X ). The basic idea is then to
define a lower bound F ? ln p(Y,W) in terms of q
and then apply the calculus of variations to find a q
that maximizes F .
ln p(Y,W|?) = lnEq[
p(Y,X ,W|?)
q(?,X ) ]
? Eq[ln
p(Y,X ,W|?)
q(?,X ) ] = F ,
Applying our independence assumption, we arrive at
the following expression for F , where ?t is the par-
ticular parameter vector corresponding to the rules
with parent state t:
F =
?
t?Q
(
Eq(?t)[ln p(?t|?t)]? Eq(?t)[ln q(?t)]
)
+
N
?
i=1
(
Eq[ln p(wi, xi, yi|?)]? Eq(xi)[ln q(xi)]
)
.
We find the q(?t) and q(xi) that maximize F by
taking derivatives of the Lagrangian, setting them to
zero, and solving, which yields:
q(?t) = Dirichlet(?t|??t)
q(xi) =
?
r?R ??(r)cr(xi)
?
x?Xi
?
r?R ??(r)cr(x)
where
??(r) = ?(r) +
?
i
Eq(xi)[cr(xi)]
??(r) = exp
?
??(??(r))??(
?
r:s(r)=t
??(r))
?
? .
493
The parameters of q(?t) are defined with respect
to q(xi) and the parameters of q(xi) with respect
to the parameters of q(?t). q(xi) can be computed
efficiently using inside-outside. Thus, we can per-
form an EM-like alternation between calculating ??
and ??.4
It is also possible to estimate the hyper-
parameters ? from data, a practice known as em-
pirical Bayes, by optimizing F . We explore learn-
ing separate hyper-parameters ?t for each ?t, us-
ing a fixed point update described by Minka (2000),
where kt is the number of rules with parent state t:
??t =
(
1
?t
+ 1kt?2t
(
?2F
??2t
)?1( ?F
??t
)
)?1
7 Training and decoding
We implement our VB training algorithm inside the
tree transducer package Tiburon (May and Knight,
2006), and experiment with both manually set and
automatically estimated priors. For our manually
set priors, we explore different hyper-parameter set-
tings for three different priors, one for each of the
main decision types: MR rule, NL pattern, and word
generation. For the automatic priors, we estimate
separate hyper-parameters for each multinomial (of
which there are hundreds). As is standard, we ini-
tialize the word distributions using a variant of IBM
model 1, and make use of NP lists (a manually cre-
ated list of the constants in the MR language paired
with the words that refer to them in the corpus).
At test time, since finding the most probable MR
for a sentence involves summing over all possible
derivations, we instead find the MR associated with
the most probable derivation.
8 Experimental setup and evaluation
We evaluate the system on GeoQuery (Wong and
Mooney, 2006), a parallel corpus of 880 English
questions and database queries about United States
geography, 250 of which were translated into Span-
ish, Japanese, and Turkish. We present here ad-
ditional translations of the full 880 sentences into
4Because of the resemblance to EM, this procedure has been
called VBEM. Unlike EM, however, this procedure alternates
between two estimation steps and has no maximization step.
German, Greek, and Thai. For evaluation, follow-
ing from Kwiatkowski et al (2010), we reserve 280
sentences for test and train on the remaining 600.
During development, we use cross-validation on the
600 sentence training set. At test, we run once on the
remaining 280 and perform 10 fold cross-validation
on the 250 sentence sets.
To judge correctness, we follow standard prac-
tice and submit each parse as a GeoQuery database
query, and say the parse is correct only if the answer
matches the gold standard. We report raw accuracy
(the percentage of sentences with correct answers),
as well as F1: the harmonic mean of precision (the
proportion of correct answers out of sentences with
a parse) and recall (the proportion of correct answers
out of all sentences).5
We run three other state-of-the-art systems for
comparison. WASP (Wong and Mooney, 2006) and
the hybrid tree (Lu et al, 2008) are chosen to rep-
resent tree transformation based approaches, and,
while this comparison is our primary focus, we also
report UBL-S (Kwiatkowski et al, 2010) as a non-
tree based top-performing system.6 The hybrid tree
is notable as the only other system based on a gen-
erative model, and uni-hybrid, a version that uses a
unigram distribution over words, is very similar to
our own model. We also report the best performing
version, re-hybrid, which incorporates a discrimina-
tive re-ranking step.
We report transducer performance under three dif-
ferent training conditions: tsEM using EM, tsVB-
auto using VB with empirical Bayes, and tsVB-hand
using hyper-parameters manually tuned on the Ger-
man training data (? of 0.3, 0.8, and 0.25 for MR
rule, NL pattern, and word choices, respectively).
Table 1 shows results for 10 fold cross-validation
on the training set. The results highlight the benefit
of the Dirichlet prior, whether manually or automat-
ically set. VB improves over EM considerably, most
likely because (1) the handling of unknown words
and MR entities allows it to return an analysis for all
sentences, and (2) the sparse Dirichlet prior favors
fewer rules, reasonable in this setting where only a
few words are likely to share the same meaning.
5Note that accuracy and f-score reduce to the same formula
if there are no parse failures.
6UBL-S is based on CCG, which can be viewed as a map-
ping between graphs more general than trees.
494
DEV geo600 - 10 fold cross-val
German Greek
Acc F1 Acc F1
UBL-S 76.7 76.9 76.2 76.5
WASP 66.3 75.0 71.2 79.7
uni-hybrid 61.7 66.1 71.0 75.4
re-hybrid 62.3 69.5 70.2 76.8
tsEM 61.7 67.9 67.3 73.2
tsVB-auto 74.0 74.0 ?79.8 ?79.8
tsVB-hand ?78.0 ?78.0 79.0 79.0
English Thai
UBL-S 85.3 85.4 74.0 74.1
WASP 73.5 79.4 69.8 73.9
uni-hybrid 76.3 79.0 71.3 73.7
re-hybrid 77.0 82.2 71.7 76.0
tsEM 73.5 78.1 69.8 72.9
tsVB-auto 81.2 81.2 74.7 74.7
tsVB-hand ?83.7 ?83.7 ?76.7 ?76.7
Table 1: Accuracy and F1 score comparisons on the
geo600 training set. Highest scores are in bold, while
the highest among the tree based models are marked with
a bullet. The dotted line separates the tree based from
non-tree based models.
On the test set (Table 2), we only run the model
variants that perform best on the training set. Test set
accuracy is consistently higher for the VB trained
tree transducer than the other tree transformation
based models (and often highest overall), while f-
score remains competitive.7
9 Conclusion
We have argued that tree transformation based se-
mantic parsing can benefit from the literature on for-
mal language theory and tree automata, and have
taken a step in this direction by presenting a tree
transducer based semantic parser. Drawing this con-
nection facilitates a greater flow of ideas in the
research community, allowing semantic parsing to
leverage ideas from other work with tree automata,
while making clearer how seemingly isolated ef-
forts might relate to one another. We demonstrate
this by both building on previous work in train-
ing tree transducers using EM (Graehl et al, 2008),
7Numbers differ slightly here from previously published re-
sults due to the fact that we have standardized the inputs to the
different systems.
TEST geo880 - 600 train/280 test
German Greek
Acc F1 Acc F1
UBL-S 75.0 75.0 73.6 73.7
WASP 65.7 ? 74.9 70.7 ? 78.6
re-hybrid 62.1 68.5 69.3 74.6
tsVB-hand ? 74.6 74.6 ?75.4 75.4
English Thai
UBL-S 82.1 82.1 66.4 66.4
WASP 71.1 77.7 71.4 75.0
re-hybrid 76.8 ? 81.0 73.6 76.7
tsVB-hand ? 79.3 79.3 ? 78.2 ? 78.2
geo250 - 10 fold cross-val
English Spanish
UBL-S 80.4 80.6 79.7 80.1
WASP 70.0 80.8 72.4 81.0
re-hybrid 74.8 82.6 78.8 ? 86.2
tsVB-hand ? 83.2 ? 83.2 ? 80.0 80.0
Japanese Turkish
UBL-S 80.5 80.6 74.2 74.9
WASP 74.4 ? 82.9 62.4 75.9
re-hybrid 76.8 82.4 66.8 ? 77.5
tsVB-hand ? 78.0 78.0 ? 75.6 75.6
Table 2: Accuracy and F1 score comparisons on the
geo880 and geo250 test sets. Highest scores are in
bold, while the highest among the tree based models are
marked with a bullet. The dotted line separates the tree
based from non-tree based models.7
and describing a general purpose variational infer-
ence algorithm for adapting tree transducers to the
Bayesian framework. The new VB algorithm re-
sults in an overall performance improvement for the
transducer over EM training, and the general effec-
tiveness of the approach is further demonstrated by
the Bayesian transducer achieving highest accuracy
among other tree transformation based approaches.
Acknowledgments
We thank Joel Lang, Michael Auli, Stella Frank,
Prachya Boonkwan, Christos Christodoulopoulos,
Ioannis Konstas, and Tom Kwiatkowski for provid-
ing the new translations of GeoQuery. This research
was supported in part under the Australian Re-
search Council?s Discovery Projects funding scheme
(project number DP110102506).
495
References
Christopher M. Bishop. Pattern Recognition and Ma-
chine Learning. Springer, 2006.
Benjamin Bo?rschinger, Bevan K. Jones, and Mark John-
son. Reducing grounded learning tasks to grammati-
cal inference. In Proc. of the Conference on Empirical
Methods in Natural Language Processing, 2011.
David Chiang, Jonathan Graehl, Kevin Knight, Adam
Pauls, and Sujith Ravi. Bayesian inference for finite-
state transducers. In Proc. of the annual meeting of
the North American Association for Computational Lin-
guistics, 2010.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. What?s in a translation rule? In Proc. of the
annual meeting of the North American Association for
Computational Linguistics, 2004.
Ruifang Ge and Raymond J. Mooney. A statistical se-
mantic parser that integrates syntax and semantics. In
Proceedings of the Conference on Computational Natu-
ral Language Learning, 2005.
Jonathon Graehl, Kevin Knight, and Jon May. Training
tree transducers. Computational Linguistics, 34:391?
427, 2008.
Rohit J. Kate and Raymond J. Mooney. Using string-
kernels for learning semantic parsers. In Proc. of the
International Conference on Computational Linguistics
and the annual meeting of the Association for Compu-
tational Linguistics, 2006.
Kevin Knight and Jonathon Greahl. An overview of prob-
abilistic tree transducers for natural language process-
ing. In Proc. of the 6th International Conference on
Intelligent Text Processing and Computational Linguis-
tics, 2005.
Kenichi Kurihara and Taisuke Sato. Variational Bayesian
grammar induction for natural language. In Proc. of
the 8th International Colloquium on Grammatical In-
ference, 2006.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. Inducing probabilistic CCG
grammars from logical form with higher-order unifica-
tion. In Proc. of the Conference on Empirical Methods
in Natural Language Processing, 2010.
Percy Liang, Michael I. Jordan, and Dan Klein. Learning
dependency-based compositional semantics. In Proc.
of the annual meeting of the Association for Computa-
tional Linguistics, 2011.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-
moyer. A generative model for parsing natural language
to meaning representations. In Proc. of the Conference
on Empirical Methods in Natural Language Processing,
2008.
Andreas Maletti, Jonathan Graehl, Mark Hopkins, and
Kevin Knight. The power of extended top-down tree
transducers. SIAM J. Comput., 39:410?430, June 2009.
Jon May and Kevin Knight. Tiburon: A weighted tree au-
tomata toolkit. In Proc. of the International Conference
on Implementation and Application of Automata, 2006.
Tom Minka. Estimating a Dirichlet distribution. Techni-
cal report, M.I.T., 2000.
W.C. Rounds. Mappings and grammars on trees. Mathe-
matical Systems Theory 4, pages 257?287, 1970.
Stuart M. Shieber. Synchronous grammars as tree trans-
ducers. In Proc. of the Seventh International Workshop
on Tree Adjoining Grammar and Related Formalisms,
2004.
J.W. Thatcher. Generalized sequential machine maps. J.
Comput. System Sci. 4, pages 339?367, 1970.
Yuk Wah Wong and Raymond J. Mooney. Learning for
semantic parsing with statistical machine translation. In
Proc. of Human Language Technology Conference and
the annual meeting of the North American Chapter of
the Association for Computational Linguistics, 2006.
Kenji Yamada and Kevin Knight. A syntax-based statis-
tical translation model. In Proc. of the annual meeting
of the Association for Computational Linguistics, 2001.
496
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1073?1083,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Weak semantic context helps phonetic learning
in a model of infant language acquisition
Stella Frank
sfrank@inf.ed.ac.uk
ILCC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
Naomi H. Feldman
nhf@umd.edu
Department of Linguistics
University of Maryland
College Park, MD, 20742, USA
Sharon Goldwater
sgwater@inf.ed.ac.uk
ILCC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
Abstract
Learning phonetic categories is one of the
first steps to learning a language, yet is hard
to do using only distributional phonetic in-
formation. Semantics could potentially be
useful, since words with different mean-
ings have distinct phonetics, but it is un-
clear how many word meanings are known
to infants learning phonetic categories. We
show that attending to a weaker source of
semantics, in the form of a distribution over
topics in the current context, can lead to
improvements in phonetic category learn-
ing. In our model, an extension of a pre-
vious model of joint word-form and pho-
netic category inference, the probability of
word-forms is topic-dependent, enabling
the model to find significantly better pho-
netic vowel categories and word-forms than
a model with no semantic knowledge.
1 Introduction
Infants begin learning the phonetic categories of
their native language in their first year (Kuhl et al,
1992; Polka and Werker, 1994; Werker and Tees,
1984). In theory, semantic information could offer
a valuable cue for phoneme induction
1
by helping
infants distinguish between minimal pairs, as lin-
guists do (Trubetzkoy, 1939). However, due to a
widespread assumption that infants do not know the
meanings of many words at the age when they are
learning phonetic categories (see Swingley, 2009
for a review), most recent models of early phonetic
category acquisition have explored the phonetic
learning problem in the absence of semantic infor-
mation (de Boer and Kuhl, 2003; Dillon et al, 2013;
1
The models in this paper do not distinguish between pho-
netic and phonemic categories, since they do not capture
phonological processes (and there are also none present in
our synthetic data). We thus use the terms interchangeably.
Feldman et al, 2013a; McMurray et al, 2009; Val-
labha et al, 2007).
Models without any semantic information are
likely to underestimate infants? ability to learn pho-
netic categories. Infants learn language in the wild,
and quickly attune to the fact that words have (pos-
sibly unknown) meanings. The extent of infants?
semantic knowledge is not yet known, but existing
evidence shows that six-month-olds can associate
some words with their referents (Bergelson and
Swingley, 2012; Tincoff and Jusczyk, 1999, 2012),
leverage non-acoustic contexts such as objects or ar-
ticulations to distinguish similar sounds (Teinonen
et al, 2008; Yeung and Werker, 2009), and map
meaning (in the form of objects or images) to new
word-forms in some laboratory settings (Friedrich
and Friederici, 2011; Gogate and Bahrick, 2001;
Shukla et al, 2011). These findings indicate that
young infants are sensitive to co-occurrences be-
tween linguistic stimuli and at least some aspects
of the world.
In this paper we explore the potential contribu-
tion of semantic information to phonetic learning
by formalizing a model in which learners attend to
the word-level context in which phones appear (as
in the lexical-phonetic learning model of Feldman
et al, 2013a) and also to the situations in which
word-forms are used. The modeled situations con-
sist of combinations of categories of salient ac-
tivities or objects, similar to the activity contexts
explored by Roy et al (2012), e.g.,?getting dressed?
or ?eating breakfast?. We assume that child learn-
ers are able to infer a representation of the situ-
ational context from their non-linguistic environ-
ment. However, in our simulations we approximate
the environmental information by running a topic
model (Blei et al, 2003) over a corpus of child-
directed speech to infer a topic distribution for each
situation. These topic distributions are then used as
input to our model to represent situational contexts.
The situational information in our model is simi-
1073
lar to that assumed by theories of cross-situational
word learning (Frank et al, 2009; Smith and Yu,
2008; Yu and Smith, 2007), but our model does not
require learners to map individual words to their ref-
erents. Even in the absence of word-meaning map-
pings, situational information is potentially useful
because similar-sounding words uttered in similar
situations are more likely to be tokens of the same
lexeme (containing the same phones) than similar-
sounding words uttered in different situations.
In simulations of vowel learning, inspired by
Vallabha et al (2007) and Feldman et al (2013a),
we show a clear improvement over previous mod-
els in both phonetic and lexical (word-form) cate-
gorization when situational context is used as an
additional source of information. This improve-
ment is especially noticeable when the word-level
context is providing less information, arguably the
more realistic setting. These results demonstrate
that relying on situational co-occurrence can im-
prove phonetic learning, even if learners do not yet
know the meanings of individual words.
2 Background and overview of models
Infants attend to distributional characteristics of
their input (Maye et al, 2002, 2008), leading to
the hypothesis that phonetic categories could be
acquired on the basis of bottom-up distributional
learning alone (de Boer and Kuhl, 2003; Vallabha
et al, 2007; McMurray et al, 2009). However, this
would require sound categories to be well sepa-
rated, which often is not the case?for example,
see Figure 1, which shows the English vowel space
that is the focus of this paper.
Recent work has investigated whether infants
could overcome such distributional ambiguity by
incorporating top-down information, in particular,
the fact that phones appear within words. At six
months, infants begin to recognize word-forms
such as their name and other frequently occurring
words (Mandel et al, 1995; Jusczyk and Hohne,
1997), without necessarily linking a meaning to
these forms. This ?protolexicon? can help differen-
tiate phonetic categories by adding word contexts
in which certain sound categories appear (Swingley,
2009; Feldman et al, 2013b). To explore this idea
further, Feldman et al (2013a) implemented the
Lexical-Distributional (LD) model, which jointly
learns a set of phonetic vowel categories and a set
of word-forms containing those categories. Simula-
tions showed that the use of lexical context greatly
500100015002000250030003500 F2
200
400
600
800
1000
1200
F1
oa
uw
aw
oo
uh
ah
er
ehae
ihei
iy
Figure 1: The English vowel space (generated from
Hillenbrand et al (1995), see Section 6.2), plotted
using the first two formants.
improved phonetic learning.
Our own Topic-Lexical-Distributional (TLD)
model extends the LD model to include an addi-
tional type of context: the situations in which words
appear. To motivate this extension and clarify the
differences between the models, we now provide
a high-level overview of both models; details are
given in Sections 3 and 4.
2.1 Overview of LD model
Both the LD and TLD models are computational-
level models of phonetic (specifically, vowel) cat-
egorization where phones (vowels) are presented
to the model in the context of words.
2
The task is
to infer a set of phonetic categories and a set of
lexical items on the basis of the data observed for
each word token x
i
. In the original LD model, the
observations for token x
i
are its frame f
i
, which
consists of a list of consonants and slots for vowels,
and the list of vowel tokensw
i
. (The TLD model
includes additional observations, described below.)
A single vowel token, w
ij
, is a two dimensional
vector representing the first two formants (peaks
in the frequency spectrum, ordered from lowest to
highest). For example, a token of the word kitty
would have the frame f
i
= k t , containing two
consonant phones, /k/ and /t/, with two vowel phone
slots in between, and two vowel formant vectors,
2
For a related model that also tackles the word segmenta-
tion problem, see Elsner et al (2013). In a model of phono-
logical learning, Fourtassi and Dupoux (submitted) show that
semantic context information similar to that used here remains
useful despite segmentation errors.
1074
wi0
= [464, 2294] and w
i1
= [412, 2760].
3
Given the data, the model must assign each
vowel token to a vowel category, w
ij
= c. Both
the LD and the TLD models do this using inter-
mediate lexemes, `, which contain vowel category
assignments, v
`j
= c, as well as a frame f
`
. If a
word token is assigned to a lexeme, x
i
= `, the
vowels within the word are assigned to that lex-
eme?s vowel categories, w
ij
= v
`j
= c.
4
The word
and lexeme frames must match, f
i
= f
`
.
Lexical information helps with phonetic catego-
rization because it can disambiguate highly over-
lapping categories, such as the ae and eh categories
in Figure 1. A purely distributional learner who ob-
serves a cluster of data points in the ae-eh region is
likely to assume all these points belong to a single
category because the distributions of the categories
are so similar. However, a learner who attends to
lexical context will notice a difference: contexts
that only occur with ae will be observed in one part
of the ae-eh region, while contexts that only oc-
cur with eh will be observed in a different (though
partially overlapping) space. The learner then has
evidence of two different categories occurring in
different sets of lexemes.
Simulations with the LD model show that using
lexical information to constrain phonetic learning
can greatly improve categorization accuracy (Feld-
man et al, 2013a), but it can also introduce errors.
When two word tokens contain the same consonant
frame but different vowels (i.e., minimal pairs),
the model is more likely to categorize those two
vowels together. Thus, the model has trouble distin-
guishing minimal pairs. Although young children
also have trouble with minimal pairs (Stager and
Werker, 1997; Thiessen, 2007), the LD model may
overestimate the degree of the problem. We hypoth-
esize that if a learner is able to associate words with
the contexts of their use (as children likely are), this
could provide a weak source of information for dis-
ambiguating minimal pairs even without knowing
their exact meanings. That is, if the learner hears
kV
1
t and kV
2
t in different situational contexts, they
are likely to be different lexical items (and V
1
and
V
2
different phones), despite the lexical similarity
between them.
3
In simulations we also experiment with frames in which
consonants are not represented perfectly.
4
The notation is overloaded: w
ij
refers both to the vowel
formants and the vowel category assignments, and x
i
refers
to both the token identity and its assignment to a lexeme.
2.2 Overview of TLD model
To demonstrate the benefit of situational informa-
tion, we develop the Topic-Lexical-Distributional
(TLD) model, which extends the LD model by as-
suming that words appear in situations analogous
to documents in a topic model. Each situation h
is associated with a mixture of topics ?
h
, which is
assumed to be observed. Thus, for the ith token in
situation h, denoted x
hi
, the observed data will be
its frame f
hi
, vowels w
hi
, and topic vector ?
h
.
From an acquisition perspective, the observed
topic distribution represents the child?s knowledge
of the context of the interaction: she can distin-
guish bathtime from dinnertime, and is able to rec-
ognize that some topics appear in certain contexts
(e.g. animals on walks, vegetables at dinnertime)
and not in others (few vegetables appear at bath-
time). We assume that the child would learn these
topics from observing the world around her and
the co-occurrences of entities and activities in the
world. Within any given situation, there might be
a mixture of different (actual or possible) topics
that are salient to the child. We assume further that
as the child learns the language, she will begin to
associate specific words with each topic as well.
Thus, in the TLD model, the words used in a sit-
uation are topic-dependent, implying meaning, but
without pinpointing specific referents. Although the
model observes the distribution of topics in each
situation (corresponding to the child observing her
non-linguistic environment), it must learn to asso-
ciate each (phonetically and lexically ambiguous)
word token with a particular topic from that distri-
bution. The occurrence of similar-sounding words
in different situations with mostly non-overlapping
topics will provide evidence that those words be-
long to different topics and that they are therefore
different lexemes. Conversely, potential minimal
pairs that occur in situations with similar topic dis-
tributions are more likely to belong to the same
topic and thus the same lexeme.
Although we assume that children infer topic
distributions from the non-linguistic environment,
we will use transcripts from CHILDES to create the
word/phone learning input for our model. These
transcripts are not annotated with environmental
context, but Roy et al (2012) found that topics
learned from similar transcript data using a topic
model were strongly correlated with immediate ac-
tivities and contexts. We therefore obtain the topic
distributions used as input to the TLD model by
1075
training an LDA topic model (Blei et al, 2003)
on a superset of the child-directed transcript data
we use for lexical-phonetic learning, dividing the
transcripts into small sections (the ?documents? in
LDA) that serve as our distinct situations h. As
noted above, the learned document-topic distribu-
tions ? are treated as observed variables in the
TLD model to represent the situational context. The
topic-word distributions learned by LDA are dis-
carded, since these are based on the (correct and
unambiguous) words in the transcript, whereas the
TLD model is presented with phonetically ambigu-
ous versions of these word tokens and must learn to
disambiguate them and associate them with topics.
3 Lexical-Distributional Model
In this section we describe more formally the gen-
erative process for the LD model (Feldman et al,
2013a), a joint Bayesian model over phonetic cat-
egories and a lexicon, before describing the TLD
extension in the following section.
The set of phonetic categories and the lexicon are
both modeled using non-parametric Dirichlet Pro-
cess priors, which return a potentially infinite num-
ber of categories or lexemes. A DP is parametrized
as DP (?,H), where ? is a real-valued hyperpa-
rameter andH is a base distribution.H may be con-
tinuous, as when it generates phonetic categories
in formant space, or discrete, as when it generates
lexemes as a list of phonetic categories.
A draw from a DP, G ? DP (?,H), returns
a distribution over a set of draws from H , i.e., a
discrete distribution over a set of categories or lex-
emes generated by H . In the mixture model setting,
the category assignments are then generated from
G, with the datapoints themselves generated by the
corresponding components fromH . IfH is infinite,
the support of the DP is likewise infinite. During
inference, we marginalize over G.
3.1 Phonetic Categories: IGMM
Following previous models of vowel learning (de
Boer and Kuhl, 2003; Vallabha et al, 2007; Mc-
Murray et al, 2009; Dillon et al, 2013) we assume
that vowel tokens are drawn from a Gaussian mix-
ture model. The Infinite Gaussian Mixture Model
(IGMM) (Rasmussen, 2000) includes a DP prior,
as described above, in which the base distribution
H
C
generates multivariate Gaussians drawn from
a Normal Inverse-Wishart prior.
5
Each observation,
a formant vector w
ij
, is drawn from the Gaussian
corresponding to its category assignment c
ij
:
?
c
,?
c
? H
C
= NIW(?
0
,?
0
, ?
0
) (1)
G
C
? DP (?
c
, H
C
) (2)
c
ij
? G
C
(3)
w
ij
|c
ij
= c ? N(?
c
,?
c
) (4)
The above model generates a category assignment
c
ij
for each vowel token w
ij
. This is the baseline
IGMM model, which clusters vowel tokens using
bottom-up distributional information only; the LD
model adds top-down information by assigning cat-
egories in the lexicon, rather than on the token
level.
3.2 Lexicon
In the LD model, vowel phones appear within
words drawn from the lexicon. Each such lexeme
is represented as a frame plus a list of vowel cate-
gories v
`
. Lexeme assignments for each token are
drawn from a DP with a lexicon-generating base
distribution H
L
. The category for each vowel to-
ken in the word is determined by the lexeme; the
formant values are drawn from the corresponding
Gaussian as in the IGMM:
G
L
? DP (?
l
, H
L
) (5)
x
i
= ` ? G
L
(6)
w
ij
|v
`j
= c ? N(?
c
,?
c
) (7)
H
L
generates lexemes by first drawing the num-
ber of phones from a geometric distribution and the
number of consonant phones from a binomial dis-
tribution. The consonants are then generated from a
DP with a uniform base distribution (but note they
are fixed at inference time, i.e., are observed cate-
gorically), while the vowel phones v
`
are generated
by the IGMM DP above, v
`j
? G
C
.
Note that two draws from H
L
may result in iden-
tical lexemes; these are nonetheless considered to
be separate (homophone) lexemes.
4 Topic-Lexical-Distributional Model
The TLD model retains the IGMM vowel phone
component, but extends the lexicon of the LD
model by adding topic-specific lexicons, which cap-
ture the notion that lexeme probabilities are topic-
dependent. Specifically, the TLD model replaces
5
This compound distribution is equivalent to
?
c
? IW(?
0
, ?
0
), ?
c
|?
c
? N(?
0
,
?
c
?
0
)
1076
the Dirichlet Process lexicon with a Hierarchical
Dirichlet Process (HDP; Teh (2006)). In the HDP
lexicon, a top-level global lexicon is generated as
in the LD model. Topic-specific lexicons are then
drawn from the global lexicon, containing a subset
of the global lexicon (but since the size of the global
lexicon is unbounded, so are the topic-specific lex-
icons). These topic-specific lexicons are used to
generate the tokens in a similar manner to the LD
model. There are a fixed number of lower level
topic-lexicons; these are matched to the number
of topics in the LDA model used to infer the topic
distributions (see Section 6.4).
More formally, the global lexicon is generated
as a top-level DP: G
L
? DP (?
l
, H
L
) (see Sec-
tion 3.2; remember H
L
includes draws from the
IGMM over vowel categories). G
L
is in turn used
as the base distribution in the topic-level DPs,
G
k
? DP (?
k
, G
L
). In the Chinese Restaurant
Franchise metaphor often used to describe HDPs,
G
L
is a global menu of dishes (lexemes). The topic-
specific lexicons are restaurants, each with its own
distribution over dishes; this distribution is defined
by seating customers (word tokens) at tables, each
of which serves a single dish from the menu: all
tokens x at the same table t are assigned to the
same lexeme `
t
. Inference (Section 5) is defined
in terms of tables rather than lexemes; if multiple
tables draw the same dish from G
L
, tokens at these
tables share a lexeme.
In the TLD model, tokens appear within situa-
tions, each of which has a distribution over topics
?
h
. Each token x
hi
has a co-indexed topic assign-
ment variable, z
hi
, drawn from ?
h
, designating the
topic-lexicon from which the table for x
hi
is to be
drawn. The formant values for w
hij
are drawn in
the same way as in the LD model, given the lexeme
assignment at x
hi
. This results in the following
model, shown in Figure 2:
G
L
? DP (?
l
, H
L
) (8)
G
k
? DP (?
k
, G
L
) (9)
z
hi
?Mult(?
h
) (10)
x
hi
= t|z
hi
= k ? G
k
(11)
w
hij
|x
hi
= t, v
`
t
j
= c ? N(?
c
,?
c
) (12)
5 Inference: Gibbs Sampling
We use Gibbs sampling to infer three sets of vari-
ables in the TLD model: assignments to vowel cat-
egories in the lexemes, assignments of tokens to
?
0
, ?
0
,?
0
, ?
0
H
C
G
C
?
c
?
c
,?
c
?
?
H
L
G
L
?
l
G
k
?
k
K
z
hi
x
hi
f
hi
w
hij
|w
hi
|
|x
h
|
D
?
h
Figure 2: TLD model, depicting, from left to right,
the IGMM component, the LD lexicon compo-
nent, the topic-specific lexicons, and finally the
token x
hi
, appearing in document h, with observed
vowel formants w
hij
and frame f
hi
. The lexeme
assignment x
hi
and the topic assignment z
hi
are
inferred, the latter using the observed document-
topic distribution ?
h
. Note that f
i
is deterministic
given the lexeme assignment. Squared nodes depict
hyperparameters. ? is the set of hyperparameters
used by H
L
when generating lexical items (see
Section 3.2).
topics, and assignments of tokens to tables (from
which the assignment to lexemes can be read off).
5.1 Sampling lexeme vowel categories
Each vowel in the lexicon must be assigned to a
category in the IGMM. The posterior probability of
a category assignment is composed of the DP prior
over categories and the likelihood of the observed
vowels belonging to that category. We use w
`j
to
denote the set of vowel formants at position j in
words that have been assigned to lexeme `. Then,
P (v
`j
= c|w,x, `
\`
)
? P (v
`j
= c|`
\`
)p(w
`j
|v
`j
= c,w
\`j
) (13)
The first (DP prior) factor is defined as:
P (v
`j
= c|v
\`j
) =
{
n
c
P
c
n
c
+?
c
if c exists
?
c
P
c
n
c
+?
c
if c new
(14)
where n
c
is the number of other vowels in the lex-
icon, v
\lj
, assigned to category c. Note that there
is always positive probability of creating a new
category.
The likelihood of the vowels is calculated by
marginalizing over all possible means and vari-
ances of the Gaussian category parameters, given
1077
the NIW prior. For a single point (if |w
`j
| = 1),
this predictive posterior is in the form of a Student-t
distribution; for the more general case see Feldman
et al (2013a), Eq. B3.
5.2 Sampling table & topic assignments
We jointly sample x and z, the variables assigning
tokens to tables and topics. Resampling the table
assignment includes the possibility of changing to
a table with a different lexeme or drawing a new
table with a previously seen or novel lexeme. The
joint conditional probability of a table and topic
assignment, given all other current token assign-
ments, is:
P (x
hi
= t, z
hi
= k|w
hi
, ?
h
, t
\hi
, `,w
\hi
)
= P (k|?
h
)P (t|k, `
t
, t
\hi
)
?
c?C
p(w
hi?
|v
`
t
?
= c,w
\hi
) (15)
The first factor, the prior probability of topic k
in document h, is given by ?
hk
obtained from the
LDA. The second factor is the prior probability of
assigning word x
i
to table t with lexeme ` given
topic k. It is given by the HDP, and depends on
whether the table t exists in the HDP topic-lexicon
for k and, likewise, whether any table in the topic-
lexicon has the lexeme `:
P (t|k, `, t
\hi
) ?
?
?
?
?
?
n
kt
n
k
+?
k
if t in k
?
k
n
k
+?
k
m
`
m+?
l
if t new, ` known
?
k
n
k
+?
k
?
`
m+?
l
if t and ` new
(16)
Here n
kt
is the number of other tokens at table t,
n
k
are the total number of tokens in topic k, m
`
is the number of tables across all topics with the
lexeme `, and m is the total number of tables.
The third factor, the likelihood of the vowel for-
mantsw
hi
in the categories given by the lexeme v
l
,
is of the same form as the likelihood of vowel cate-
gories when resampling lexeme vowel assignments.
However, here it is calculated over the set of vow-
els in the token assigned to each vowel category
(i.e., the vowels at indices where v
`
t
?
= c). For a
new lexeme, we approximate the likelihood using
100 samples drawn from the prior, each weighted
by ?/100 (Neal, 2000).
5.3 Hyperparameters
The three hyperparameters governing the HDP over
the lexicon, ?
l
and ?
k
, and the DP over vowel cate-
gories, ?
c
, are estimated using a slice sampler. The
remaining hyperparameters for the vowel category
and lexeme priors are set to the same values used
by Feldman et al (2013a).
6 Experiments
6.1 Corpus
We test our model on situated child directed speech,
taken from the C1 section of the Brent corpus in
CHILDES (Brent and Siskind, 2001; MacWhinney,
2000). This corpus consists of transcripts of speech
directed at infants between the ages of 9 and 15
months, captured in a naturalistic setting as par-
ent and child went about their day. This ensures
variability of situations.
Utterances with unintelligible words or quotes
are removed. We restrict the corpus to content
words by retaining only words tagged as adj,
n, part and v (adjectives, nouns, particles, and
verbs). This is in line with evidence that infants
distinguish content and function words on the basis
of acoustic signals (Shi and Werker, 2003). Vowel
categorization improves when attending only to
more prosodically and phonologically salient to-
kens (Adriaans and Swingley, 2012), which gen-
erally appear within content, not function words.
The final corpus consists of 13138 tokens and 1497
word types.
6.2 Hillenbrand Vowels
The transcripts do not include phonetic information,
so, following Feldman et al (2013a), we synthe-
size the formant values using data from Hillenbrand
et al (1995). This dataset consists of a set of 1669
manually gathered formant values from 139 Amer-
ican English speakers (men, women and children)
for 12 vowels. For each vowel category, we con-
struct a Gaussian from the mean and covariance of
the datapoints belonging to that category, using the
first and second formant values measured at steady
state. We also construct a second dataset using only
datapoints from adult female speakers.
Each word in the dataset is converted to a phone-
mic representation using the CMU pronunciation
dictionary, which returns a sequence of Arpabet
phoneme symbols. If there are multiple possible
pronunciations, the first one is used. Each vowel
phoneme in the word is then replaced by formant
values drawn from the corresponding Hillenbrand
Gaussian for that vowel.
1078
6.3 Merging Consonant Categories
The Arpabet encoding used in the phonemic rep-
resentation includes 24 consonants. We construct
datasets both using the full set of consonants?the
?C24? dataset?and with less fine-grained conso-
nant categories. Distinguishing all consonant cate-
gories assumes perfect learning of consonants prior
to vowel categorization and is thus somewhat unre-
alistic (Polka and Werker, 1994), but provides an
upper limit on the information that word-contexts
can give.
In the ?C15? dataset, the voicing distinction is
collapsed, leaving 15 consonant categories. The
collapsed categories are B/P, G/K, D/T, CH/JH,
V/F, TH/DH, S/Z, SH/ZH, R/L while HH, M, NG,
N, W, Y remain separate phonemes. This dataset
mirrors the finding in Mani and Plunkett (2010) that
12 month old infants are not sensitive to voicing
mispronunciations.
The ?C6? dataset distinguishes between only
6 coarse consonant phonemes, corresponding to
stops (B,P,G,K,D,T), affricates (CH,JH), fricatives
(V, F, TH, DH, S, Z, SH, ZH, HH), nasals (M,
NG, N), liquids (R, L), and semivowels/glides (W,
Y). This dataset makes minimal assumptions about
the category categories that infants could use in this
learning setting.
Decreasing the number of consonants increases
the ambiguity in the corpus: bat not only shares
a frame (b t) with boat and bite, but also, in the
C15 dataset, with put, pad and bad (b/p d/t), and
in the C6 dataset, with dog and kite, among many
others (STOP STOP). Table 1 shows the percent-
age of types and tokens that are ambiguous in each
dataset, that is, words in frames that match multiple
wordtypes. Note that we always evaluate against
the gold word identities, even when these are not
distinguished in the model?s input. These datasets
are intended to evaluate the degree of reliance on
consonant information in the LD and TLD models,
and to what extent the topics in the TLD model can
replace this information.
6.4 Topics
The input to the TLD model includes a distribution
over topics for each situation, which we infer in
advance from the full Brent corpus (not only the
C1 subset) using LDA. Each transcript in the Brent
corpus captures about 75 minutes of parent-child
interaction, and thus multiple situations will be
included in each file. The transcripts do not delimit
Dataset C24 C15 C6
Input Types 1487 1426 1203
Frames 1259 1078 702
Ambig Types % 27.2 42.0 80.4
Ambig Tokens % 41.3 56.9 77.2
Table 1: Corpus statistics showing the increasing
amount of ambiguity as consonant categories are
merged. Input types are the number of word types
with distinct input representations (as opposed to
gold orthographic word types, of which there are
1497). Ambiguous types and tokens are those with
frames that match multiple (orthographic) word
types.
situations, so we do this somewhat arbitrarily by
splitting each transcript after 50 CDS utterances,
resulting in 203 situations for the Brent C1 dataset.
As well as function words, we also remove the
five most frequent content words (be, go, get, want,
come). On average, situations are only 59 words
long, reflecting the relative lack of content words
in CDS utterances.
We infer 50 topics for this set of situations using
the mallet toolkit (McCallum, 2002). Hyperpa-
rameters are inferred, which leads to a dominant
topic that includes mainly light verbs (have, let,
see, do). The other topics are less frequent but cap-
ture stronger semantic meaning (e.g. yummy, peach,
cookie, daddy, bib in one topic, shoe, let, put, hat,
pants in another). The word-topic assignments are
used to calculate unsmoothed situation-topic distri-
butions ? used by the TLD model.
6.5 Evaluation
We evaluate against adult categories, i.e., the ?gold-
standard?, since all learners of a language even-
tually converge on similar categories. (Since our
model is not a model of the learning process, we
do not compare the infant learning process to the
learning algorithm.) We evaluate both the inferred
phonetic categories and words using the clustering
evaluation measure V-Measure (VM; Rosenberg
and Hirschberg, 2007).
6
VM is the harmonic mean
of two components, similar to F-score, where the
components (VC and VH) are measures of cross
entropy between the gold and model categorization.
6
Other clustering measures, such as 1-1 matching and
pairwise precision and recall (accuracy and completeness)
showed the same trends, but VM has been demonstrated to
be the most stable measure when comparing solutions with
varying numbers of clusters (Christodoulopoulos et al, 2010).
1079
24 Cons 15 Cons 6 Cons
75
80
85
90
Dataset
V
M
LD-all
TLD-all
LD-w
TLD-w
Figure 3: Vowel evaluation. ?all? refers to datasets
with vowels synthesized from all speakers, ?w? to
datasets with vowels synthesized from adult female
speakers? vowels. The bars show a 95% Confidence
Interval based on 5 runs. IGMM-all results in a VM
score of 53.9 (CI=0.5); IGMM-w has a VM score
of 65.0 (CI=0.2), not shown.
For vowels, VM measures how well the inferred
phonetic categorizations match the gold categories;
for lexemes, it measures whether tokens have been
assigned to the same lexemes both by the model
and the gold standard. Words are evaluated against
gold orthography, so homophones, e.g. hole and
whole, are distinct gold words.
6.6 Results
We compare all three models?TLD, LD, and
IGMM?on the vowel categorization task, and
TLD and LD on the lexical categorization task
(since IGMM does not infer a lexicon). The datasets
correspond to two sets of conditions: firstly, either
using vowel categories synthesized from all speak-
ers or only adult female speakers, and secondly,
varying the coarseness of the observed consonant
categories. Each condition (model, vowel speak-
ers, consonant set) is run five times, using 1500
iterations of Gibbs sampling with hyperparameter
sampling. Overall, we find that TLD outperforms
the other models in both tasks, across all condi-
tions.
Vowel categorization results are shown in Fig-
ure 3. IGMM performs substantially worse than
both TLD and LD, with scores more than 30 points
lower than the best results for these models, clearly
showing the value of the protolexicon and repli-
500100015002000250030003500 F2
200
400
600
800
1000
1200
F1
Figure 4: Vowels found by the TLD model; su-
pervowels are indicated in red. The gold-standard
vowels are shown in gold in the background but are
mostly overlapped by the inferred categories.
cating the results found by Feldman et al (2013a)
on this dataset. Furthermore, TLD consistently out-
performs the LD model, finding better phonetic
categories, both for vowels generated from the com-
bined categories of all speakers (?all?) and vowels
generated from adult female speakers only (?w?),
although the latter are clearly much easier for both
models to learn. Both models perform less well
when the consonant frames provide less informa-
tion, but the TLD model performance degrades less
than the LD performance.
Both the TLD and the LD models find ?super-
vowel? categories, which cover multiple vowel cat-
egories and are used to merge minimal pairs into a
single lexical item. Figure 4 shows example vowel
categories inferred by the TLD model, including
two supervowels. The TLD supervowels are used
much less frequently than the supervowels found
by the LD model, containing, on average, only two-
thirds as many tokens.
Figure 5 shows that TLD also outperforms LD
on the lexeme/word categorization task. Again per-
formance decreases as the consonant categories
become coarser, but the additional semantic infor-
mation in the TLD model compensates for the lack
of consonant information. In the individual com-
ponents of VM, TLD and LD have similar VC
(?recall?), but TLD has higher VH (?precision?),
demonstrating that the semantic information given
by the topics can separate potentially ambiguous
words, as hypothesized.
Overall, the contextual semantic information
1080
24 Cons 15 Cons 6 Cons
92
94
96
98
100
Dataset
V
M
LD-all
TLD-all
LD-w
TLD-w
Figure 5: Lexeme evaluation. ?all? refers to datasets
with vowels synthesized from all speakers, ?w? to
datasets with vowels synthesized from adult female
speakers? vowels.
added in the TLD model leads to both better pho-
netic categorization and to a better protolexicon,
especially when the input is noisier, using degraded
consonants. Since infants are not likely to have per-
fect knowledge of phonetic categories at this stage,
semantic information is a potentially rich source
of information that could be drawn upon to offset
noise from other domains. The form of the seman-
tic information added in the TLD model is itself
quite weak, so the improvements shown here are in
line with what infant learners could achieve.
7 Conclusion
Language acquisition is a complex task, in which
many heterogeneous sources of information may
be useful. In this paper, we investigated whether
contextual semantic information could be of help
when learning phonetic categories. We found that
this contextual information can improve phonetic
learning performance considerably, especially in
situations where there is a high degree of pho-
netic ambiguity in the word-forms that learners
hear. This suggests that previous models that have
ignored semantic information may have underesti-
mated the information that is available to infants.
Our model illustrates one way in which language
learners might harness the rich information that is
present in the world without first needing to acquire
a full inventory of word meanings.
The contextual semantic information that the
TLD model tracks is similar to that potentially
used in other linguistic learning tasks. Theories
of cross-situational word learning (Smith and Yu,
2008; Yu and Smith, 2007) assume that sensitivity
to situational co-occurrences between words and
non-linguistic contexts is a precursor to learning the
meanings of individual words. Under this view, con-
textual semantics is available to infants well before
they have acquired large numbers of semantic min-
imal pairs. However, recent experimental evidence
indicates that learners do not always retain detailed
information about the referents that are present in a
scene when they hear a word (Medina et al, 2011;
Trueswell et al, 2013). This evidence poses a di-
rect challenge to theories of cross-situational word
learning. Our account does not necessarily require
learners to track co-occurrences between words
and individual objects, but instead focuses on more
abstract information about salient events and topics
in the environment; it will be important to investi-
gate to what extent infants encode this information
and use it in phonetic learning.
Regardless of the specific way in which infants
encode semantic information, our method of adding
this information by using LDA topics from tran-
script data was shown to be effective. This method
is practical because it can approximate semantic
information without relying on extensive manual
annotation.
The LD model extended the phonetic catego-
rization task by adding word contexts; the TLD
model presented here goes even further, adding
larger situational contexts. Both forms of top-down
information help the low-level task of classifying
acoustic signals into phonetic categories, furthering
a holistic view of language learning with interac-
tion across multiple levels.
Acknowledgments
This work was supported by EPSRC grant
EP/H050442/1 and a James S. McDonnell Founda-
tion Scholar Award to the final author.
References
Frans Adriaans and Daniel Swingley. Distribu-
tional learning of vowel categories is supported
by prosody in infant-directed speech. In Pro-
ceedings of the 34th Annual Conference of the
Cognitive Science Society (CogSci), 2012.
E. Bergelson and D. Swingley. At 6-9 months,
human infants know the meanings of many
1081
common nouns. Proceedings of the National
Academy of Sciences, 109(9):3253?3258, Feb
2012.
David M. Blei, Thomas L. Griffiths, Michael I. Jor-
dan, and Joshua B. Tenenbaum. Hierarchical
topic models and the nested Chinese restaurant
process. In Advances in Neural Information Pro-
cessing Systems 16, 2003.
Michael R. Brent and Jeffrey M. Siskind. The role
of exposure to isolated words in early vocabulary
development. Cognition, 81(2):B33?B44, 2001.
Christos Christodoulopoulos, Sharon Goldwater,
and Mark Steedman. Two decades of unsuper-
vised POS induction: How far have we come?
In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 575?584, Cambridge, MA, Octo-
ber 2010. Association for Computational Lin-
guistics.
Bart de Boer and Patricia K. Kuhl. Investigating the
role of infant-directed speech with a computer
model. Acoustics Research Letters Online, 4(4):
129, 2003.
Brian Dillon, Ewan Dunbar, and William Idsardi. A
single-stage approach to learning phonological
categories: Insights from Inuktitut. Cognitive
Science, 37(2):344?377, Mar 2013.
Micha Elsner, Sharon Goldwater, Naomi Feldman,
and Frank Wood. A cognitive model of early
lexical acquisition with phonetic variability. In
Proceedings of the 18th Conference on Empir-
ical Methods in Natural Language Processing
(EMNLP), 2013.
Naomi H. Feldman, Thomas L. Griffiths, Sharon
Goldwater, and James L. Morgan. A role for the
developing lexicon in phonetic category acquisi-
tion. Psychological Review, 2013a.
Naomi H. Feldman, Emily B. Myers, Katherine S.
White, Thomas L. Griffiths, and James L. Mor-
gan. Word-level information influences phonetic
learning in adults and infants. Cognition, 127(3):
427?438, 2013b.
Abdellah Fourtassi and Emmanuel Dupoux. A rudi-
mentary lexicon and semantics help bootstrap
phoneme acquisition. Submitted.
Michael C. Frank, Noah D. Goodman, and
Joshua B. Tenenbaum. Using speakers? refer-
ential intentions to model early cross-situational
word learning. Psychological Science, 20(5):
578?585, 2009.
Manuela Friedrich and Angela D. Friederici. Word
learning in 6-month-olds: Fast encoding?weak
retention. Journal of Cognitive Neuroscience, 23
(11):3228?3240, Nov 2011.
Lakshmi J. Gogate and Lorraine E. Bahrick. In-
tersensory redundancy and 7-month-old infants?
memory for arbitrary syllable-object relations.
Infancy, 2(2):219?231, Apr 2001.
J. Hillenbrand, L. A. Getty, M. J. Clark, and
K. Wheeler. Acoustic characteristics of Ameri-
can English vowels. Journal of the Acoustical
Society of America, 97(5 Pt 1):3099?3111, May
1995.
P. W. Jusczyk and Elizabeth A. Hohne. Infants?
memory for spoken words. Science, 277(5334):
1984?1986, Sep 1997.
Patricia K. Kuhl, Karen A. Williams, Francisco
Lacerda, Kenneth N. Stevens, and Bjorn Lind-
blom. Linguistic experience alters phonetic per-
ception in infants by 6 months of age. Science,
255(5044):606?608, 1992.
Brian MacWhinney. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum Asso-
ciates, 2000.
D. R. Mandel, P. W. Jusczyk, and D. B. Pisoni.
Infants? recognition of the sound patterns of their
own names. Psychological Science, 6(5):314?
317, Sep 1995.
Nivedita Mani and Kim Plunkett. Twelve-month-
olds know their cups from their keps and tups.
Infancy, 15(5):445470, Sep 2010.
Jessica Maye, Daniel J. Weiss, and Richard N.
Aslin. Statistical phonetic learning in infants:
facilitation and feature generalization. Develop-
mental Science, 11(1):122?134, Jan 2008.
Jessica Maye, Janet F Werker, and LouAnn Gerken.
Infant sensitivity to distributional information
can affect phonetic discrimination. Cognition,
82(3):B101?B111, Jan 2002.
Andrew McCallum. MALLET: A machine learn-
ing for language toolkit, 2002.
Bob McMurray, Richard N. Aslin, and Joseph C.
Toscano. Statistical learning of phonetic cate-
gories: insights from a computational approach.
Developmental Science, 12(3):369?378, May
2009.
1082
Tamara Nicol Medina, Jesse Snedeker, John C.
Trueswell, and Lila R. Gleitman. How words
can and cannot be learned by observation. Pro-
ceedings of the National Academy of Sciences,
108(22):9014?9019, 2011.
Radford Neal. Markov chain sampling methods
for Dirichlet process mixture models. Journal
of Computational and Graphical Statistics, 9:
249?265, 2000.
Linda Polka and Janet F. Werker. Developmen-
tal changes in perception of nonnative vowel
contrasts. Journal of Experimental Psychology:
Human Perception and Performance, 20(2):421?
435, 1994.
Carl Rasmussen. The infinite Gaussian mixture
model. In Advances in Neural Information Pro-
cessing Systems 13, 2000.
Andrew Rosenberg and Julia Hirschberg. V-
measure: A conditional entropy-based external
cluster evaluation measure. In Proceedings of
the 12th Conference on Empirical Methods in
Natural Language Processing (EMNLP), 2007.
Brandon C. Roy, Michael C. Frank, and Deb Roy.
Relating activity contexts to early word learning
in dense longitudinal data. In Proceedings of the
34th Annual Conference of the Cognitive Science
Society (CogSci), 2012.
Rushen Shi and Janet F. Werker. The basis of pref-
erence for lexical words in 6-month-old infants.
Developmental Science, 6(5):484?488, 2003.
M. Shukla, K. S. White, and R. N. Aslin. Prosody
guides the rapid mapping of auditory word forms
onto visual objects in 6-mo-old infants. Proceed-
ings of the National Academy of Sciences, 108
(15):6038?6043, Apr 2011.
Linda B. Smith and Chen Yu. Infants rapidly learn
word-referent mappings via cross-situational
statistics. Cognition, 106(3):1558?1568, 2008.
Christine L. Stager and Janet F. Werker. Infants
listen for more phonetic detail in speech percep-
tion than in word-learning tasks. Nature, 388:
381?382, 1997.
D. Swingley. Contributions of infant word learning
to language development. Philosophical Trans-
actions of the Royal Society B: Biological Sci-
ences, 364(1536):3617?3632, Nov 2009.
Yee Whye Teh. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Pro-
ceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics (ACL),
pages 985 ? 992, Sydney, 2006.
Tuomas Teinonen, Richard N. Aslin, Paavo Alku,
and Gergely Csibra. Visual speech contributes to
phonetic learning in 6-month-old infants. Cogni-
tion, 108:850?855, 2008.
Erik D. Thiessen. The effect of distributional infor-
mation on children?s use of phonemic contrasts.
Journal of Memory and Language, 56(1):16?34,
Jan 2007.
R. Tincoff and P. W. Jusczyk. Some beginnings of
word comprehension in 6-month-olds. Psycho-
logical Science, 10(2):172?175, Mar 1999.
Ruth Tincoff and Peter W. Jusczyk. Six-month-
olds comprehend words that refer to parts of the
body. Infancy, 17(4):432444, Jul 2012.
N. S. Trubetzkoy. Grundz?uge der Phonologie. Van-
denhoeck und Ruprecht, G?ottingen, 1939.
John C. Trueswell, Tamara Nicol Medina, Alon
Hafri, and Lila R. Gleitman. Propose but ver-
ify: Fast mapping meets cross-situational word
learning. Cognitive Psychology, 66:126?156,
2013.
G. K. Vallabha, J. L. McClelland, F. Pons, J. F.
Werker, and S. Amano. Unsupervised learning
of vowel categories from infant-directed speech.
Proceedings of the National Academy of Sci-
ences, 104(33):13273?13278, Aug 2007.
Janet F. Werker and Richard C. Tees. Cross-
language speech perception: Evidence for per-
ceptual reorganization during the first year of
life. Infant Behavior and Development, 7:49?63,
1984.
H. Henny Yeung and Janet F. Werker. Learning
words? sounds before learning how words sound:
9-month-olds use distinct objects as cues to cat-
egorize speech information. Cognition, 113(2):
234?243, Nov 2009.
Chen Yu and Linda B. Smith. Rapid word learning
under uncertainty via cross-situational statistics.
Psychological Science, 18(5):414?420, 2007.
1083
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 265?271,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
POS induction with distributional and morphological information
using a distance-dependent Chinese restaurant process
Kairit Sirts
Institute of Cybernetics at
Tallinn University of Technology
sirts@ioc.ee
Jacob Eisenstein
School of Interactive Computing
Georgia Institute of Technology
jacobe@gatech.edu
Micha Elsner
Department of Linguistics
The Ohio State University
melsner0@gmail.com
Sharon Goldwater
ILCC, School of Informatics
University of Edinburgh
sgwater@inf.ed.ac.uk
Abstract
We present a new approach to inducing the
syntactic categories of words, combining
their distributional and morphological prop-
erties in a joint nonparametric Bayesian
model based on the distance-dependent
Chinese Restaurant Process. The prior
distribution over word clusterings uses a
log-linear model of morphological similar-
ity; the likelihood function is the probabil-
ity of generating vector word embeddings.
The weights of the morphology model
are learned jointly while inducing part-of-
speech clusters, encouraging them to co-
here with the distributional features. The
resulting algorithm outperforms competi-
tive alternatives on English POS induction.
1 Introduction
The morphosyntactic function of words is reflected
in two ways: their distributional properties, and
their morphological structure. Each information
source has its own advantages and disadvantages.
Distributional similarity varies smoothly with syn-
tactic function, so that words with similar syntactic
functions should have similar distributional proper-
ties. In contrast, there can be multiple paradigms
for a single morphological inflection (such as past
tense in English). But accurate computation of
distributional similarity requires large amounts of
data, which may not be available for rare words;
morphological rules can be applied to any word
regardless of how often it appears.
These observations suggest that a general ap-
proach to the induction of syntactic categories
should leverage both distributional and morpho-
logical features (Clark, 2003; Christodoulopoulos
et al, 2010). But these features are difficult to
combine because of their disparate representations.
Distributional information is typically represented
in numerical vectors, and recent work has demon-
strated the utility of continuous vector represen-
tations, or ?embeddings? (Mikolov et al, 2013;
Luong et al, 2013; Kim and de Marneffe, 2013;
Turian et al, 2010). In contrast, morphology is
often represented in terms of sparse, discrete fea-
tures (such as morphemes), or via pairwise mea-
sures such as string edit distance. Moreover, the
mapping between a surface form and morphology
is complex and nonlinear, so that simple metrics
such as edit distance will only weakly approximate
morphological similarity.
In this paper we present a new approach for in-
ducing part-of-speech (POS) classes, combining
morphological and distributional information in a
non-parametric Bayesian generative model based
on the distance-dependent Chinese restaurant pro-
cess (ddCRP; Blei and Frazier, 2011). In the dd-
CRP, each data point (word type) selects another
point to ?follow?; this chain of following links
corresponds to a partition of the data points into
clusters. The probability of word w
1
following w
2
depends on two factors: 1) the distributional simi-
larity between all words in the proposed partition
containing w
1
and w
2
, which is encoded using a
Gaussian likelihood function over the word embed-
dings; and 2) the morphological similarity between
w
1
and w
2
, which acts as a prior distribution on the
induced clustering. We use a log-linear model to
capture suffix similarities between words, and learn
the feature weights by iterating between sampling
and weight learning.
We apply our model to the English section of
the the Multext-East corpus (Erjavec, 2004) in or-
der to evaluate both against the coarse-grained and
265
fine-grained tags, where the fine-grained tags en-
code detailed morphological classes. We find that
our model effectively combines morphological fea-
tures with distributional similarity, outperforming
comparable alternative approaches.
2 Related work
Unsupervised POS tagging has a long history in
NLP. This paper focuses on the POS induction
problem (i.e., no tag dictionary is available), and
here we limit our discussion to very recent sys-
tems. A review and comparison of older systems
is provided by Christodoulopoulos et al (2010),
who found that imposing a one-tag-per-word-type
constraint to reduce model flexibility tended to
improve system performance; like other recent
systems, we impose that constraint here. Recent
work also shows that the combination of morpho-
logical and distributional information yields the
best results, especially cross-linguistically (Clark,
2003; Berg-Kirkpatrick et al, 2010). Since then,
most systems have incorporated morphology in
some way, whether as an initial step to obtain pro-
totypes for clusters (Abend et al, 2010), or as
features in a generative model (Lee et al, 2010;
Christodoulopoulos et al, 2011; Sirts and Alum?ae,
2012), or a representation-learning algorithm (Yat-
baz et al, 2012). Several of these systems use a
small fixed set of orthographic and/or suffix fea-
tures, sometimes obtained from an unsupervised
morphological segmentation system (Abend et al,
2010; Lee et al, 2010; Christodoulopoulos et al,
2011; Yatbaz et al, 2012). Blunsom and Cohn?s
(2011) model learns an n-gram character model
over the words in each cluster; we learn a log-
linear model, which can incorporate arbitrary fea-
tures. Berg-Kirkpatrick et al (2010) also include
a log-linear model of morphology in POS induc-
tion, but they use morphology in the likelihood
term of a parametric sequence model, thereby en-
couraging all elements that share a tag to have the
same morphological features. In contrast, we use
pairwise morphological similarity as a prior in a
non-parametric clustering model. This means that
the membership of a word in a cluster requires only
morphological similarity to some other element in
the cluster, not to the cluster centroid; which may
be more appropriate for languages with multiple
morphological paradigms. Another difference is
that our non-parametric formulation makes it un-
necessary to know the number of tags in advance.
3 Distance-dependent CRP
The ddCRP (Blei and Frazier, 2011) is an extension
of the CRP; like the CRP, it defines a distribution
over partitions (?table assignments?) of data points
(?customers?). Whereas in the regular CRP each
customer chooses a table with probability propor-
tional to the number of customers already sitting
there, in the ddCRP each customer chooses another
customer to follow, and sits at the same table with
that customer. By identifying the connected compo-
nents in this graph, the ddCRP equivalently defines
a prior over clusterings.
If c
i
is the index of the customer followed by
customer i, then the ddCRP prior can be written
P (c
i
= j) ?
{
f(d
ij
) if i 6= j
? if i = j,
(1)
where d
ij
is the distance between customers i and j
and f is a decay function. A ddCRP is sequential if
customers can only follow previous customers, i.e.,
d
ij
=? when i > j and f(?) = 0. In this case,
if d
ij
= 1 for all i < j then the ddCRP reduces to
the CRP.
Separating the distance and decay function
makes sense for ?natural? distances (e.g., the num-
ber of words between word i and j in a document,
or the time between two events), but they can also
be collapsed into a single similarity function. We
wish to assign higher similarities to pairs of words
that share meaningful suffixes. Because we do not
know which suffixes are meaningful a priori, we
use a maximum entropy model whose features in-
clude all suffixes up to length three that are shared
by at least one pair of words. Our prior is then:
P (c
i
= j|w, ?) ?
{
e
w
T
g(i,j)
if i 6= j
? if i = j,
(2)
where g
s
(i, j) is 1 if suffix s is shared by ith and
jth words, and 0 otherwise.
We can create an infinite mixture model by com-
bining the ddCRP prior with a likelihood function
defining the probability of the data given the cluster
assignments. Since we are using continuous-valued
vectors (word embeddings) to represent the distri-
butional characteristics of words, we use a multi-
variate Gaussian likelihood. We will marginalize
over the mean ? and covariance ? of each clus-
ter, which in turn are drawn from Gaussian and
inverse-Wishart (IW) priors respectively:
? ? IW (?
0
,?
0
) ? ? N (?
0
,
?
/
?
0
) (3)
266
The full model is then:
P (X,c,?,?|?,w, ?) (4)
=
K
?
k=1
P (?
k
|?)p(?
k
|?
k
,?)
?
n
?
i=1
(P (c
i
|w, ?)P (x
i
|?
z
i
,?
z
i
)),
where ? are the hyperparameters for (?,?) and z
i
is the (implicit) cluster assignment of the ith word
x
i
. With a CRP prior, this model would be an infi-
nite Gaussian mixture model (IGMM; Rasmussen,
2000), and we will use the IGMM as a baseline.
4 Inference
The Gibbs sampler for the ddCRP integrates over
the Gaussian parameters, sampling only follower
variables. At each step, the follower link c
i
for a
single customer i is sampled, which can implicitly
shift the entire block of n customers fol(i) who fol-
low i into a new cluster. Since we marginalize over
the cluster parameters, computing P (c
i
= j) re-
quires computing the likelihood P (fol(i),X
j
|?),
where X
j
are the k customers already clustered
with j. However, if we do not merge fol(i)
with X
j
, then we have P (X
j
|?) in the overall
joint probability. Therefore, we can decompose
P (fol(i),X
j
|?) = P (fol(i)|X
j
,?)P (X
j
|?) and
need only compute the change in likelihood due to
merging in fol(i):
1
:
P (fol(i)|X
j
,?) = pi
?nd/2
?
d/2
k
|?
k
|
?
k
/2
?
d/2
n+k
|?
n+k
|
?
n+k
/2
?
d
?
i=1
?
(
?
n+k
+1?i
2
)
?
(
?
k
+1?i
2
)
, (5)
where the hyperparameters are updated as ?
n
=
?
0
+ n, ?
n
= ?
0
+ n, and
?
n
=
?
0
?
0
+ x?
?
0
+ n
(6)
?
n
= ?
0
+Q+ ?
0
?
0
?
0
T
? ?
n
?
n
?
T
n
, (7)
where Q =
?
n
i=1
x
i
x
T
i
.
Combining this likelihood term with the prior,
the probability of customer i following j is
P (c
i
= j|X
,
?,w, ?)
? P (fol(i)|X
j
,?)P (c
i
= j|w, ?). (8)
1
http://www.stats.ox.ac.uk/
?
teh/re-
search/notes/GaussianInverseWishart.pdf
Our non-sequential ddCRP introduces cycles
into the follower structure, which are handled in the
sampler as described by Socher et al (2011). Also,
the block of customers being moved around can po-
tentially be very large, which makes it easy for the
likelihood term to swamp the prior. In practice we
found that introducing an additional parameter a
(used to exponentiate the prior) improved results?
although we report results without this exponent as
well. This technique was also used by Titov and
Klementiev (2012) and Elsner et al (2012).
Inference also includes optimizing the feature
weights for the log-linear model in the ddCRP
prior (Titov and Klementiev, 2012). We interleave
L-BFGS optimization within sampling, as in Monte
Carlo Expectation-Maximization (Wei and Tanner,
1990). We do not apply the exponentiation parame-
ter a when training the weights because this proce-
dure affects the follower structure only, and we do
not have to worry about the magnitude of the like-
lihood. Before the first iteration we initialize the
follower structure: for each word, we choose ran-
domly a word to follow from amongst those with
the longest shared suffix of up to 3 characters. The
number of clusters starts around 750, but decreases
substantially after the first sampling iteration.
5 Experiments
Data For our experiments we used the English
word embeddings from the Polyglot project (Al-
Rfou? et al, 2013)
2
, which provides embeddings
trained on Wikipedia texts for 100,000 of the most
frequent words in many languages.
We evaluate on the English part of the Multext-
East (MTE) corpus (Erjavec, 2004), which provides
both coarse-grained and fine-grained POS labels
for the text of Orwell?s ?1984?. Coarse labels con-
sist of 11 main word classes, while the fine-grained
tags (104 for English) are sequences of detailed
morphological attributes. Some of these attributes
are not well-attested in English (e.g. gender) and
some are mostly distinguishable via semantic anal-
ysis (e.g. 1st and 2nd person verbs). Many tags are
assigned only to one or a few words. Scores for the
fine-grained tags will be lower for these reasons,
but we argue below that they are still informative.
Since Wikipedia and MTE are from different
domains their lexicons do not fully overlap; we
2
https://sites.google.com/site/rmyeid/
projects/polyglot
267
Wikipedia tokens 1843M
Multext-East tokens 118K
Multext-East types 9193
Multext-East & Wiki types 7540
Table 1: Statistics for the English Polyglot word embeddings
and English part of MTE: number of Wikipedia tokens used
to train the embeddings, number of tokens/types in MTE, and
number of types shared by both datasets.
take the intersection of these two sets for training
and evaluation. Table 1 shows corpus statistics.
Evaluation With a few exceptions (Biemann,
2006; Van Gael et al, 2009), POS induction sys-
tems normally require the user to specify the num-
ber of desired clusters, and the systems are evalu-
ated with that number set to the number of tags in
the gold standard. For corpora such as MTE with
both fine-grained and coarse-grained tages, pre-
vious evaluations have scored against the coarse-
grained tags. Though coarse-grained tags have
their place (Petrov et al, 2012), in many cases
the distributional and morphological distinctions
between words are more closely aligned with the
fine-grained tagsets, which typically distinguish
between verb tenses, noun number and gender,
and adjectival scale (comparative, superlative, etc.),
so we feel that the evaluation against fine-grained
tagset is more relevant here. For better comparison
with previous work, we also evaluate against the
coarse-grained tags; however, these numbers are
not strictly comparable to other scores reported on
MTE because we are only able to train and evalu-
ate on the subset of words that also have Polyglot
embeddings. To provide some measure of the dif-
ficulty of the task, we report baseline scores using
K-means clustering, which is relatively strong base-
line in this task (Christodoulopoulos et al, 2011).
There are several measures commonly used for
unsupervised POS induction. We report greedy
one-to-one mapping accuracy (1-1) (Haghighi and
Klein, 2006) and the information-theoretic score V-
measure (V-m), which also varies from 0 to 100%
(Rosenberg and Hirschberg, 2007). In previous
work it has been common to also report many-to-
one (m-1) mapping but this measure is particularly
sensitive to the number of induced clusters (more
clusters yield higher scores), which is variable for
our models. V-m can be somewhat sensitive to the
number of clusters (Reichart and Rappoport, 2009)
but much less so than m-1 (Christodoulopoulos
et al, 2010). With different number of induced
and gold standard clusters the 1-1 measure suffers
because some induced clusters cannot be mapped
to gold clusters or vice versa. However, almost half
the gold standard clusters in MTE contain just a
few words and we do not expect our model to be
able to learn them anyway, so the 1-1 measure is
still useful for telling us how well the model learns
the bigger and more distinguishable classes.
In unsupervised POS induction it is standard to
report accuracy on tokens even when the model it-
self works on types. Here we report also type-based
measures because these can reveal differences in
model behavior even when token-based measures
are similar.
Experimental setup For baselines we use K-
means and the IGMM, which both only learn from
the word embeddings. The CRP prior in the IGMM
has one hyperparameter (the concentration param-
eter ?); we report results for ? = 5 and 20. Both
the IGMM and ddCRP have four hyperparameters
controlling the prior over the Gaussian cluster pa-
rameters: ?
0
, ?
0
, ?
0
and ?
0
. We set the prior scale
matrix ?
0
by using the average covariance from
a K-means run with K = 200. When setting the
average covariance as the expected value of the IW
distribution the suitable scale matrix can be com-
puted as ?
0
= E [X] (?
0
? d? 1), where ?
0
is the
prior degrees of freedom (which we set to d + 10)
and d is the data dimensionality (64 for the Poly-
glot embeddings). We set the prior mean ?
0
equal
to the sample mean of the data and ?
0
to 0.01.
We experiment with three different priors for the
ddCRP model. All our ddCRP models are non-
sequential (Socher et al, 2011), allowing cycles
to be formed. The simplest model, ddCRP uni-
form, uses a uniform prior that sets the distance
between any two words equal to one.
3
The second
model, ddCRP learned, uses the log-linear prior
with weights learned between each two Gibbs iter-
ations as explained in section 4. The final model,
ddCRP exp, adds the prior exponentiation. The ?
parameter for the ddCRP is set to 1 in all experi-
ments. For ddCRP exp, we report results with the
exponent a set to 5.
Results and discussion Table 2 presents all re-
sults. Each number is an average of 5 experiments
3
In the sequential case this model would be equivalent to
the IGMM (Blei and Frazier, 2011). Due to the nonsequen-
tiality this equivalence does not hold, but we do expect to see
similar results to the IGMM.
268
Fine types Fine tokens Coarse tokens
Model K Model K-means Model K-means Model K-means
K-means 104 or 11 16.1 / 47.3 - 39.2 / 62.0 - 44.4 / 45.5 -
IGMM, ? = 5 55.6 41.0 / 45.9 23.1 / 49.5 48.0 / 64.8 37.2 / 61.0 48.3 / 58.3 40.8 / 55.0
IGMM, ? = 20 121.2 35.0 / 47.1 14.7 / 46.9 50.6 / 67.8 44.7 / 65.5 48.7 / 60.0 48.3 / 57.9
ddCRP uniform 80.4 50.5 / 52.9 18.6 / 48.2 52.4 / 68.7 35.1 / 60.3 52.1 / 62.2 40.3 / 54.2
ddCRP learned 89.6 50.1 / 55.1 17.6 / 48.0 51.1 / 69.7 39.0 / 63.2 48.9 / 62.0 41.1 / 55.1
ddCRP exp, a = 5 47.2 64.0 / 60.3 25.0 / 50.3 55.1 / 66.4 33.0 / 59.1 47.8 / 55.1 36.9 / 53.1
Table 2: Results of baseline and ddCRP models evaluated on word types and tokens using fine-grained tags, and on tokens
using coarse-grained tags. For each model we present the number of induced clusters K (or fixed K for K-means) and 1-1 / V-m
scores. The second column under each evaluation setting gives the scores for K-means with K equal to the number of clusters
induced by the model in that row.
with different random initializations. For each eval-
uation setting we provide two sets of scores?first
are the 1-1 and V-m scores for the given model,
second are the comparable scores for K-means run
with the same number of clusters as induced by the
non-parametric model.
These results show that all non-parametric mod-
els perform better than K-means, which is a strong
baseline in this task (Christodoulopoulos et al,
2011). The poor performace of K-means can be
explained by the fact that it tends to find clusters
of relatively equal size, although the POS clus-
ters are rarely of similar size. The common noun
singular class is by far the largest in English, con-
taining roughly a quarter of the word types. Non-
parametric models are able to produce cluster of
different sizes when the evidence indicates so, and
this is clearly the case here.
From the token-based evaluation it is hard to
say which IGMM hyperparameter value is better
even though the number of clusters induced differs
by a factor of 2. The type-base evaluation, how-
ever, clearly prefers the smaller value with fewer
clusters. Similar effects can be seen when com-
paring IGMM and ddCRP uniform. We expected
these two models perform on the same level, and
their token-based scores are similar, but on the type-
based evaluation the ddCRP is clearly superior. The
difference could be due to the non-sequentiality,
or becuase the samplers are different?IGMM en-
abling resampling only one item at a time, ddCRP
performing blocked sampling.
Further we can see that the ddCRP uniform and
learned perform roughly the same. Although the
prior in those models is different they work mainly
using the the likelihood. The ddCRP with learned
prior does produce nice follower structures within
each cluster but the prior is in general too weak
compared to the likelihood to influence the cluster-
ing decisions. Exponentiating the prior reduces the
number of induced clusters and improves results,
as it can change the cluster assignment for some
words where the likelihood strongly prefers one
cluster but the prior clearly indicates another.
The last column shows the token-based evalua-
tion against the coarse-grained tagset. This is the
most common evaluation framework used previ-
ously in the literature. Although our scores are not
directly comparable with the previous results, our
V-m scores are similar to the best published 60.5
(Christodoulopoulos et al, 2010) and 66.7 (Sirts
and Alum?ae, 2012).
In preliminary experiments, we found that di-
rectly applying the best-performing English model
to other languages is not effective. Different lan-
guages may require different parametrizations of
the model. Further study is also needed to verify
that word embeddings effectively capture syntax
across languages, and to determine the amount of
unlabeled text necessary to learn good embeddings.
6 Conclusion
This paper demonstrates that morphology and dis-
tributional features can be combined in a flexi-
ble, joint probabilistic model, using the distance-
dependent Chinese Restaurant Process. A key ad-
vantage of this framework is the ability to include
arbitrary features in the prior distribution. Future
work may exploit this advantage more thoroughly:
for example, by using features that incorporate
prior knowledge of the language?s morphological
structure. Another important goal is the evaluation
of this method on languages beyond English.
Acknowledgments: KS was supported by the
Tiger University program of the Estonian Infor-
mation Technology Foundation for Education. JE
was supported by a visiting fellowship from the
Scottish Informatics & Computer Science Alliance.
We thank the reviewers for their helpful feedback.
269
References
Omri Abend, Roi Reichart, and Ari Rappoport. 2010.
Improved unsupervised pos induction through pro-
totype discovery. In Proceedings of the 48th An-
nual Meeting of the Association of Computational
Linguistics, pages 1298?1307.
Rami Al-Rfou?, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
for multilingual nlp. In Proceedings of the Thir-
teenth Annual Conference on Natural Language
Learning, pages 183?192, Sofia, Bulgaria. Associ-
ation for Computational Linguistics.
Taylor Berg-Kirkpatrick, Alexandre B. C?ot?e, John
DeNero, and Dan Klein. 2010. Painless unsuper-
vised learning with features. In Proceedings of Hu-
man Language Technologies: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 582?590.
Chris Biemann. 2006. Unsupervised part-of-speech
tagging employing efficient graph clustering. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 7?12.
David M Blei and Peter I Frazier. 2011. Distance
dependent chinese restaurant processes. Journal of
Machine Learning Research, 12:2461?2488.
Phil Blunsom and Trevor Cohn. 2011. A hierarchi-
cal pitman-yor process hmm for unsupervised part
of speech induction. In Proceedings of the 49th An-
nual Meeting of the Association of Computational
Linguistics, pages 865?874.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsuper-
vised POS induction: How far have we come? In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2011. A Bayesian mixture model
for part-of-speech induction using multiple features.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the European chapter of the
ACL.
Micha Elsner, Sharon Goldwater, and Jacob Eisenstein.
2012. Bootstrapping a unified model of lexical and
phonetic acquisition. In Proceedings of the 50th An-
nual Meeting of the Association of Computational
Linguistics.
Toma?z Erjavec. 2004. MULTEXT-East version 3:
Multilingual morphosyntactic specifications, lexi-
cons and corpora. In LREC.
A. Haghighi and D. Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics.
Joo-Kyung Kim and Marie-Catherine de Marneffe.
2013. Deriving adjectival scales from continuous
space word representations. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
Yoong Keok Lee, Aria Haghighi, and Regina Barzi-
lay. 2010. Simple type-level unsupervised pos tag-
ging. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
853?861.
Minh-Thang Luong, Richard Socher, and Christo-
pher D Manning. 2013. Better word representations
with recursive neural networks for morphology. In
Proceedings of the Thirteenth Annual Conference on
Natural Language Learning.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In Proceedings of Human
Language Technologies: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 746?751.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC, May.
Carl Rasmussen. 2000. The infinite Gaussian mixture
model. In Advances in Neural Information Process-
ing Systems 12, Cambridge, MA. MIT Press.
Roi Reichart and Ari Rappoport. 2009. The nvi cluster-
ing evaluation measure. In Proceedings of the Ninth
Annual Conference on Natural Language Learning,
pages 165?173.
A. Rosenberg and J. Hirschberg. 2007. V-measure:
A conditional entropy-based external cluster evalua-
tion measure. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 410?42.
Kairit Sirts and Tanel Alum?ae. 2012. A hierarchi-
cal Dirichlet process model for joint part-of-speech
and morphology induction. In Proceedings of Hu-
man Language Technologies: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 407?416.
Richard Socher, Andrew L Maas, and Christopher D
Manning. 2011. Spectral chinese restaurant pro-
cesses: Nonparametric clustering based on similar-
ities. In Proceedings of the Fifteenth International
Conference on Artificial Intelligence and Statistics,
pages 698?706.
270
Ivan Titov and Alexandre Klementiev. 2012. A
bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 384?394, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Jurgen Van Gael, Andreas Vlachos, and Zoubin
Ghahramani. 2009. The infinite HMM for unsu-
pervised PoS tagging. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 678?687, Singapore.
Greg CG Wei and Martin A Tanner. 1990. A
monte carlo implementation of the em algorithm
and the poor man?s data augmentation algorithms.
Journal of the American Statistical Association,
85(411):699?704.
Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret. 2012.
Learning syntactic categories using paradigmatic
representations of word context. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 940?951.
271
Transactions of the Association for Computational Linguistics, 1 (2013) 63?74. Action Editor: Brian Roark.
Submitted 9/2012; Published 3/2013. c?2013 Association for Computational Linguistics.
Unsupervised Dependency Parsing with Acoustic Cues
John K Pate??
j.k.pate@sms.ed.ac.uk
Sharon Goldwater?
sgwater@inf.ed.ac.uk
?ILCC, School of Informatics ?Department of Computing
University of Edinburgh Macquarie University
Edinburgh, EH8 9AB, UK Sydney, NSW 2109, Australia
Abstract
Unsupervised parsing is a difficult task that
infants readily perform. Progress has been
made on this task using text-based models, but
few computational approaches have considered
how infants might benefit from acoustic cues.
This paper explores the hypothesis that word
duration can help with learning syntax. We de-
scribe how duration information can be incor-
porated into an unsupervised Bayesian depen-
dency parser whose only other source of infor-
mation is the words themselves (without punc-
tuation or parts of speech). Our results, evalu-
ated on both adult-directed and child-directed
utterances, show that using word duration can
improve parse quality relative to words-only
baselines. These results support the idea that
acoustic cues provide useful evidence about
syntactic structure for language-learning in-
fants, and motivate the use of word duration
cues in NLP tasks with speech.
1 Introduction
Unsupervised learning of syntax is difficult for NLP
systems, yet infants perform this task routinely. Pre-
vious work in NLP has focused on using the implicit
syntactic information available in part-of-speech
(POS) tags (Klein and Manning, 2004), punctuation
(Seginer, 2007; Spitkovsky et al, 2011b; Ponvert et
al., 2011), and syntactic similarities between related
languages (Cohen and Smith, 2009; Cohen et al,
2011). However, these approaches likely use the data
in a very different way from children: neither POS
tags nor punctuation are observed during language
acquisition (although see Spitkovsky et al (2011a)
and Christodoulopoulos et al (2012) for encourag-
ing results using unsupervised POS tags), and many
children learn in a broadly monolingual environment.
This paper explores a possible source of information
that NLP systems typically ignore: word duration, or
the length of time taken to pronounce each word.
There are good reasons to think that word dura-
tion might be useful for learning syntax. First, the
well-established Prosodic Bootstrapping hypothesis
(Gleitman and Wanner, 1982) proposes that infants
use acoustic-prosodic cues (such as word duration)
to help them identify syntactic structure, because
prosodic and syntactic structures sometimes coincide.
More recently, we proposed (Pate and Goldwater,
2011) that infants might use word duration as a di-
rect cue to syntactic structure (i.e., without requir-
ing intermediate prosodic structure), because words
in high-probability syntactic structures tend to be
pronounced more quickly (Gahl and Garnsey, 2004;
Gahl et al, 2006; Tily et al, 2009).
Like most recent work on unsupervised parsing,
we focus on learning syntactic dependencies. Our
work is based on Headden et al (2009)?s Bayesian
version of the Dependency Model with Valence
(DMV) (Klein and Manning, 2004), using interpo-
lated backoff techniques to incorporate multiple infor-
mation sources per token. However, whereas Head-
den et al used words and POS tags as input, we
use words and word duration information, presenting
three variants of their model that use this information
in slightly different ways.1
1By using neither gold-standard nor learned POS tags as
input, our work differs from nearly all previous work on unsuper-
vised dependency parsing. While learned tags might be plausible
63
To our knowledge, this is the first work to incor-
porate acoustic cues into an unsupervised system for
learning full syntactic parses. The methods in this
paper were inspired by our previous approach (Pate
and Goldwater, 2011), which showed that word dura-
tion measurements could improve the performance
of an unsupervised lexicalized syntactic chunker over
a words-only baseline. However, that work was lim-
ited to HMM-like sequence models, tested on adult-
directed speech (ADS) only, and none of the models
outperformed uniform-branching baselines. Here, we
extend our results to full dependency parsing, and
experiment on transcripts of both spontaneous ADS
and child-directed speech (CDS). Our models us-
ing word duration outperform words-only baselines,
along with the Common Cover Link parser of Seginer
(2007), and the Unsupervised Partial Parser of Pon-
vert et al (2011), unsupervised lexicalized parsers
that have obtained state-of-the-art results on standard
newswire treebanks (though their performance here
is worse, as our input lacks punctuation). We also
outperform uniform-branching baselines.
2 Syntax and Word Duration
Before presenting our models and experiments, we
first discuss why word duration might be a useful cue
to syntax. This section reviews the two possible rea-
sons mentioned above: duration as a cue to prosodic
structure, or as a cue to predictability.
2.1 Prosodic Bootstrapping
Prosody is the structure of speech as conveyed by
rhythm and intonation, which are, in turn, conveyed
by such measurable phenomena as variation in fun-
damental frequency, word duration, and spectral tilt.
Prosodic structure is typically analyzed as imposing
a shallow, hierarchical grouping structure on speech,
with the ends of prosodic phrases (constituents) be-
ing cued in part by lengthening the last word of the
phrase (Beckman and Pierrehumbert, 1986).
The Prosodic Bootstrapping hypothesis (Gleit-
man and Wanner, 1982) points out that prosodic
phrases are often also syntactic phrases, and proposes
that language-acquiring infants exploit this correla-
tion. Specifically, if infants can learn about prosodic
phrase structure using word duration (and fundamen-
in a model of language acquisition, gold tags certainly are not.
tal frequency), they may be able to identify syntactic
phrases more easily using word strings and prosodic
trees than using word strings alone.
Several behavioral experiments support the con-
nection between prosody and syntax and the prosodic
bootstrapping hypothesis specifically. For example,
there is evidence that adults use prosodic information
for syntactic disambiguation (Millotte et al, 2007;
Price et al, 1991) and to help in learning the syntax
of an artificial language (Morgan et al, 1987), while
infants can use acoustic-prosodic cues for utterance-
internal clause segmentation (Seidl, 2007).
On the computational side, we are aware of only
our previous HMM-based chunkers (Pate and Gold-
water, 2011), which learned shallow syntax from
words, words and word durations, or words and hand-
annotated prosody. Using these chunkers, we found
that using words plus prosodic annotation worked
better than just words, and words plus word duration
worked even better. While these results are consistent
with the prosodic bootstrapping hypothesis, we sug-
gested that predictability bootstrapping (see below)
might be a more plausible explanation.
Other computational work has combined prosody
with syntax, but only in supervised systems, and typi-
cally using hand-annotated prosodic information. For
example, Huang and Harper (2010) used annotated
prosodic breaks as a kind of punctuation in a su-
pervised PCFG, while prosodic breaks learned in a
semi-supervised way have been used as features for
parse reranking (Kahn et al, 2005) or PCFG state-
splitting (Dreyer and Shafran, 2007). In contrast to
these methods, our approach observes neither parse
trees nor prosodic annotations.
2.2 Predictability Bootstrapping
On the basis of our HMM chunkers, we introduced
the predictability bootstrapping hypothesis (Pate and
Goldwater, 2011): the idea that word durations could
be a useful cue to syntactic structure not (or not only)
because they provide information about prosodic
structure, but because they are a direct cue to syntac-
tic predictability. It is well-established that talkers
tend to pronounce words more quickly when they
are more predictable, as measured by, e.g., word
frequency, n-gram probability, or whether or not the
word has been previously mentioned (Aylett and Turk,
2004; Bell et al, 2009). However, syntactic proba-
64
you threw it right at the basket
Figure 1: Example unlabeled dependency parse.
bility also seems to matter, with studies showing that
verbs tend to be pronounced more quickly when they
are in their preferred syntactic frame?transitive vs.
intransitive or direct object vs. sentential comple-
ment (Gahl and Garnsey, 2004; Gahl et al, 2006;
Tily et al, 2009). While this syntactic evidence is
only for verbs, together with the evidence that effects
of other notions of predictability, it suggests that such
syntactic effects may also be widespread. If so, the
duration of a word could give clues as to whether it
is being used in a high-probability or low-probability
structure, and thus what the correct structure is.
We found that our syntactic chunkers benefited
more from duration information than prosodic an-
notations, providing some preliminary evidence in
favor of predictability bootstrapping, but not ruling
out prosodic bootstrapping. So, we are left with two
plausible mechanisms by which word duration could
help with learning syntax. Slow pronunciations may
cue the end of a prosodic phrase, which is sometimes
also the end of a syntactic phrase. Alternatively, slow
pronunciations may indicate that the hidden syntactic
structure is low probability, facilitating the induc-
tion of a probabilistic grammar. This paper will not
seek to determine which mechanism is useful, instead
taking the presence of two possible mechanisms as
encouraging for the prospect of incorporating word
duration into unsupervised parsing.
3 Models2
As mentioned, we will be incorporating word dura-
tion into unsupervised dependency parsing, produc-
ing analyses like the one in Figure 1. Each arc is
between two words, with the head at the non-arrow
end of the arc, and the dependent at the arrow end.
One word, the root, depends on no word, and all
other words depend on exactly one word. Following
previous work on unsupervised dependency parsing,
we will not label the arcs.
2The implementation of these models is available at
http://github.com/jpate/predictabilityParsing
3.1 Dependency Model with Valence
All of our models are ultimately based on the De-
pendency Model with Valence (DMV) of Klein and
Manning (2004), a generative, probabilistic model
for projective (i.e. no crossing arcs), unlabeled de-
pendency parses, such as the one in Figure 1.
The DMV generates dependency parses using
three probability distributions, which together com-
prise model parameters ?. First, the root of the
sentence is drawn from Proot . Second, we decide
whether to stop generating dependents of the head
h in direction dir ? {left, right} with probability
Pstop(?|h, dir , v), where v is T if h has a dir-ward
dependent and F otherwise. If we decide to stop,
then h takes no more dependents in the direction of
dir. If we don?t stop, we use the third probability
distribution Pchoose(d|h, dir) to determine which de-
pendent d to generate. The second and third step
repeat for each generated word until all words have
stopped generating in both directions.
The DMV was the first unsupervised parsing
model to outperform a uniform-branching baseline
on the Wall Street Journal corpus. It was trained
using EM to obtain a maximum-likelihood estimate
of the parameters ?, and learned from POS tags to
avoid rare events. However, all work on syntactic
predictability effects on word duration has been lexi-
calized (looking at, e.g., the transitivity bias of par-
ticular verbs). In addition, it is unlikely that children
have access to the correct parts of speech when first
learning syntactic structure. Thus, we want a DMV
variant that learns from words rather than POS tags.
We therefore adopt several extensions to the DMV
due to Headden et al (2009), described next.
3.2 The DMV with Backoff
Headden et al (2009) sought to improve the DMV by
incorporating lexical information in addition to POS
tags. However, arcs between particular words are
rare, so they modified the DMV in two ways to deal
with this sparsity. First, they switched from MLE to a
Bayesian approach, estimating a probability distribu-
tion over model parameters ? and dependency trees
T given the training corpus C and a prior distribution
? over models: P (T, ?|C,?).
Headden et al avoided overestimating the proba-
bility of rare events that happen to occur in the train-
65
ing data by picking ? to assign low probability to
models ? which give high probability to rare events.
Accordingly, models that overcommit to rare events
will contribute little to the final average over models.
Specifically, Headden et al use Dirichlet priors, with
? being the Dirichlet hyperparameters.
Headden et al?s second innovation was to adapt in-
terpolated backoff methods from language modeling
with n-grams, where one can estimate the probabil-
ity of word wn given word wn?1 by interpolating
between unigram and bigram probability estimates:
P? (wn|wn?1) = ?P (wn|wn?1) + (1? ?)P (wn)
with ? ? [0, 1]. Ideally, ? should be large whenwn?1
is frequent, and small when wn?1 is rare. Headden et
al. (2009) apply this method to the DMV by backing
off from Choose and Stop distributions that condition
on both head word and POS to distributions that
condition on only the head POS.
In the equation above, ? is a scalar parameter.
However, it actually specifies a probability distri-
bution over the decision to back off (B) or not back
off (?B), and we can use different notation to reflect
this view. Specifically, ?stop(?) and ?choose(?) will
represent our backoff distributions for the Stop and
Choose decision, respectively. Using hp and dp to
represent head and dependent POS tag and hw and
dw to represent head and dependent word, one of the
models Headden et al explored estimates:
P? choose(dp|hw, hp, dir , val) =
?choose(?B|hw, hp, dir)Pchoose(dp|hw, hp, dir)
+?choose(B|hw, hp, dir)Pchoose(dp|hp, dir) (1)
with an analogous backoff for Pstop . We can see
from Equation 1 that P?choose backs off from a dis-
tribution that conditions on hw to a distribution that
marginalizes out hw, and that the extent of backoff
varies across hw; we can use this to back off more
when we have less evidence about hw. This model
only conditions on words; it does not generate them
in the dependents. This means it is actually a condi-
tional, rather than fully generative, model of observed
POS tags and unobserved syntax conditioned on the
observed words.
Since identifying the true posterior distribution
P (T, ?|C,?) is intractable, Headden et al use Mean-
field Variational Bayes (Kurihara and Sato, 2006;
Johnson, 2007), which finds an approximation to the
posterior using an iterative EM-like algorithm. In the
E-step of VBEM, expected countsE(ri) are gathered
for each latent variable using the Inside-Outside algo-
rithm, exactly as in the E-step of traditional EM. The
Maximization step differs from the M-Step of EM in
two ways. First, the expected counts for each value
of the latent variable ri are incremented by the hy-
perparameter ?i. Second, the numerator and denom-
inator are scaled by the function exp(?(?)), which
reduces the probability of rare events. Specifically,
the Pchoose distribution is estimated using expecta-
tions for each arc adp,h,dir from head h to dependent
POS tag dp in direction dir, and the update equation
for Pchoose from iteration n to n+ 1 is:
P?n+1choose(dp|h, dir) =
exp(?(En(adp,h,dir ) + ?dp,h,dir ))
exp(?(
?
c(En(ac,h,dir ) + ?c,h,dir )))
(2)
where h is the head POS tag for the backoff distri-
bution, and the head (word, POS) pair for the no
backoff distribution. The update equation for Pstop
is analogous.
Now consider the update equations for ?choose :
??n+1choose(?B|hw, hp, dir) =
exp(?(??B +
?
c(En(ac,hw,hp,dir ))))
exp(?(?B + ??B +
?
c(En(ac,hw,hp,dir ))))
??n+1choose(B|hw, hp, dir) =
exp(?(?B))
exp(?(?B + ??B +
?
c(En(ac,hw,hp,dir ))))
Only the ?B numerator includes the expected counts,
so as we see hw in direction dir more often, the ?B
numerator will swamp the B numerator. By picking
?B larger than ??B, we can bias our ? distribution to
prefer backing off until we expect at least ?B ? ??B
arcs out of hw with tag hp in the direction of dir.
To obtain good performance, Headden et al re-
placed each word that appeared fewer than 100 times
in the training data with the token ?UNK.? We will
also use such an UNK cutoff.
3.3 DMV with Duration
We explore three models. One is a straightforward
application of the DMV with Backoff to words and
66
(quantized) word duration, and the other two are fully-
generative variants. We also consider using words
and POS tags as input to these models. Backoff mod-
els are given two streams of information, providing
two of word identity, POS tag, or word duration for
each observed token. We call one stream the ?back-
off? stream, and the other the ?extra? stream. Backoff
models learn a probability distribution conditioning
on both streams, backing off to condition on only the
backoff stream.
Our first words and duration model takes the du-
ration as the extra stream and the word identity as
the backoff stream, and, using ha to represent the
acoustic information for the head, defines:
P? choose(dw|hw, ha, dir) =
?choose(?B|hw, ha, dir)Pchoose(dw|hw, ha, dir)
+?choose(B|hw, ha, dir)Pchoose(dw|hw, dir) (3)
with an analogous backoff scheme for Pstop . We will
refer to this conditional model as ?Cond.? in our
experiments. This equation is similar to Equation 1,
except it uses words and duration instead of words
and POS tags, and backs off to, not away from, words.
We back off to the sparse words, rather than the less
sparse duration, because duration provides almost no
information about syntax in isolation.3
Directly modelling the extra stream among the
dependents may allow us to capture selectional re-
strictions in POS and words models, or exploit ef-
fects of syntactic predictability on dependent dura-
tion. We therefore explore variants that generate both
streams in the dependents. First, we examine a model
(?Joint?) that generates them jointly:
P?choose(dw, da|hw, hp, dir) =
?choose(?B|hw, ha, dir)
Pchoose(dw, da|hw, ha, dir)
+?choose(B|hw, ha, dir)
Pchoose(dw, da|hw, dir) (4)
However, this joint model will have a very large state-
space and may suffer from the same data sparsity, so
we also explore a model (?Indep.?) that generates the
3Preliminary dev-set experiments confirmed this intuition, as
models that backed off to word duration performed poorly.
extra and backoff independently:
P?choose(dw, da|hw, hp, dir) =
?choose(?B|hw, ha, dir)
Pchoose backoff (dw|hw, ha, dir)
Pchoose extra(da|hw, ha, dir)
+ ?choose(B|hw, ha, dir)
Pchoose backoff (dw|hw, dir)
Pchoose extra(da|hw, dir) (5)
We also modified the DMV with Backoff to handle
heavily lexicalized models. In Headden et al (2009),
arcs between words that never appear in the same
sentence are given probability mass only by virtue
of the backoff distribution to POS tags, which all
appear in the same sentence at least once. We want to
avoid relying on POS tags, and we also want to use
held-out development and test sets to avoid implicitly
overfitting the data when exploring different model
structures. To this end, we add one extra ?UNK hyper-
parameter to the Dirichlet prior of Pchoose for each
combination of conditioning events. This hyperpa-
rameter reserves probability mass for a head h to take
a word dw as a dependent if h and dw never appeared
together in the training data. The amount of probabil-
ity mass reserved decreases as we see hw more often.
This is implemented in training by adding ?UNK to
the denominator of the Pchoose update equation for
each h and dir. At test time, if a word dw appears
as an unseen dependent for head h, h takes dw as a
dependent with probability:
P? choose(dw|h, dir) = (6)
exp(?(?UNK))
exp(?(?UNK +
?
c(Elast(rc,h,dir ) + ?c,h,dir )))
Here, h may be a word, (word, POS) pair, or (word,
duration) pair. Since this event by definition never
occurs in the training data, ?UNK does not appear in
the numerator during training. 4
Finally, the conditional model ignores the extra
stream in Proot , and the generative models estimate
4Note also that ?UNK is different from a global UNK cutoff,
which is imposed in preprocessing, and so effects every occur-
rence of an an UNK?d word in the model. ?UNK affects only
dependents in Pchoose , and treats a dependent as UNK iff it did
not occur on that particular side of that particular head word in
any sentence. We used both global UNK cutoffs (optimized on
the dev set) and these ?UNK hyperparameters.
67
Train Dev Test
ws
j1
0 Word tokens 42,505 1,765 2,571
Word types 7,804 818 1,134
Sentences 6,007 233 357
sw
bd
nx
t1
0 Word tokens 24,998 2,980 3,052
Word types 2,647 760 767
Sentences 3,998 488 491
br
en
t Word tokens 20,954 2,127 2,206
Word types 1,390 482 488
Sentences 6,249 424 449
Table 1: Statistics for our three corpora.
Proot over both streams jointly and independently,
respectively.
4 Experimental Setup
4.1 Datasets
We evaluate on three datasets: wsj10, sentences of
length 10 or less from the Wall Street Journal por-
tion of the Penn Treebank; swbdnxt10, sentences
of length 10 or less from the Switchboard dataset
of ADS used by Pate and Goldwater (2011); and
brent, part of the Brent corpus of CDS (Brent and
Siskind, 2001). Table 1 presents corpus statistics.
4.1.1 wsj10
We present a new evaluation of the DMV with
Backoff on wsj10, which does not have any acous-
tic information, simply to verify that ?UNK performs
sensibly on a standard corpus. Additionally, Headden
et al (2009) use an intensive initializer that relies on
dozens of random restarts, and so, strictly speaking,
only show that the backoff technology is useful for
good initializations. Our new evaluation will show
that the backoff technology provides a substantial
benefit even for harmonic initialization.
wsj10 was created in the standard way; all punc-
tuation and traces were removed, and sentences con-
taining more than ten tokens were discarded. For
our fully lexicalized version of wsj10, all words
were lowercased, and numbers were replaced with
the token ?NUMBER.?5 Following standard practice,
we used sections 2-21 for training, section 22 for
development, and section 23 for test. wsj10 con-
tains hand-annotated constituency parses, not depen-
dency parses, so we used the standard ?constituency-
5Numbers were treated in this way only in wsj10.
to-dependency? conversion tool of Johansson and
Nugues (2007) to obtain high-quality CoNLL-style
dependency parses.
4.1.2 swbdnxt10
Next, we evaluate on swbdnxt10, which con-
tains all sentences up to length 10 from the same
sections of the swbdnxt version of Switchboard
used by Pate and Goldwater (2011). Short sentences
are usually formulaic discourse responses (e.g. ?oh
ok?), so this dataset alo excludes sentences shorter
than three words. As our models successfully use
word durations, this evaluation provides an important
replication of the basic result from Pate and Goldwa-
ter (2011) with a different kind of syntactic model.
swbdnxt10 has a forced alignment of a
dictionary-based phonetic transcription of each ut-
terance to audio, providing our word duration infor-
mation. As a very simple model of hyper-articulation
and hypo-articulation, we classify a word as in the
longest third duration, shortest third, or middle third.
To minimize effects of word form, this classification
was based on vowel count (counting a diphthong as
one vowel): each word with n vowels is classified as
in the shortest, longest, or middle tercile of duration
among words with n vowels.
Like wsj10, swbdnxt10 is annotated only
with constituency parses, so to provide approximate
?gold-standard? dependencies, we used the same
constituency-to-dependency conversion tool as for
wsj10. We evaluated 200 randomly-selected sen-
tences to check the accuracy of the conversion tool,
which was designed for newspaper text. Excluding
arcs involving words with no clear role in depen-
dency structure (such as ?um?), about 86% of the
arcs were correct. While this rate is uncomfortably
low, it is still much higher than unsupervised depen-
dency parsers typically achieve, and so may provide
a reasonable measure of relative dependency parse
quality among competing systems.
4.1.3 brent
We also evaluated our models on the ?Large Brent?
dataset introduced in Rytting et al (2010), a por-
tion of the Brent corpus of child-directed speech
(Brent and Siskind, 2001). We call this corpus
brent. It consists of utterances from four of the
mothers in Brent and Siskind?s (2001) study, and, like
68
swbdnxt10, has a forced alignment from which we
obtain duration terciles. Rytting et al (2010) used
a 90%/10% train/test partition. We extracted every
ninth utterance from the original training partition to
create a dev set, producing an 80%/10%/10% parti-
tion. We also separated clitics from their base word.
This dataset only has 186 sentences longer than ten
words, with a maximum length of 22 words, so we
discarded only sentences shorter than three words
from the evaluation sets.
The Brent corpus is distributed via CHILDES
(MacWhinney, 2000) with automatic dependency an-
notations. However, these are not hand-corrected,
and rely on a different tokenization of the dataset
than is present on the transcription tier. To produce a
reliable gold-standard,6 we annotated all sentences of
length 2 or greater from the development and test sets
with dependencies drawn from the Stanford Typed
Dependency set (de Marneffe and Manning, 2008)
using the annotation tool used for the Copenhagen
Dependency Treebank (Kromann, 2003).
4.2 Parameters
In all experiments, hyperparameters for Proot , Pstop ,
and Pchoose (and their backed-off distributions, and
including ?UNK) were 1, ?B was 10, and ??B was 1.
VBEM was run on the training set until the data
log-likelihood changed by less than 0.001%, and
then the parameters were held fixed and used to
obtain Viterbi parses for the evaluation sentences.
Finally, we explored different global UNK cutoffs,
replacing each word that appeared less than c times
with the token UNK. We ran each model for each
c ? {0, 1, 25, 50, 100}, and picked the best-scoring
c on the development set for running on the test set
and presentation here. We used a harmonic initializer
similar to the one in Klein and Manning (2004).
4.3 Evaluation
In addition to evaluating the various incarnations of
the DMV with backoff and input types, we compare
to uniform branching baselines, the Common Cover
Link (CCL) parser of Seginer (2007), and the Unsu-
pervised Partial Parser (UPP) of Ponvert et al (2011).
The UPP produces a constituency parse from words
and punctuation using a series of finite-state chun-
6Available at http://homepages.inf.ed.ac.uk/s0930006/brentDep/
kers; we use the best-performing (Probabilistic Right
Linear Grammar) version. The CCL parser produces
a constituency parse using a novel ?Cover Link? rep-
resentation, scoring these links heuristically. Both
CCL and UPP rely on punctuation (though according
to Ponvert et al (2011), UPP less so), which our in-
put is missing. The left-headed ?LH? (right-headed
?RH?) baseline assumes that each word takes the first
word to its right (left) as a dependent, and corre-
sponds to a uniform right-branching (left-branching)
constituency baseline.
We evaluate the output of all models in terms
of both constituency scores and dependency accu-
racy. Our wsj10 and swbdnxt10 corpora are
originally annotated for constituency structure, with
the dependency gold standard derived as described
above, while our brent corpus is originally anno-
tated for dependency structure, with the constituency
gold standard derived by defining a constituent to
span a head and each of its dependents (ignoring
any one-word ?constituents?). As the CCL and
UPP parsers don?t produce dependencies, only con-
stituency scores are provided.
For constituency scores, we present the standard
unlabeled Precision, Recall, and F-measure scores.
For dependency scores, we present Directed attach-
ment accuracy, Undirected attachment accuracy, and
the ?Neutral Edge Detection? (NED) score intro-
duced by Schwartz et al (2011). Directed attachment
accuracy counts an arc as a true positive if it correctly
identifies both a head and a dependent, whereas undi-
rected attachment accuracy ignores arc direction in
counting true positives. NED counts an arc as a true
positive if it would be a true positive under the Undi-
rected attachment score, or if the proposed head is
the gold-standard grandparent of the proposed depen-
dent. This avoids penalizing parses for flipping an
arc, such as making determiners, rather than nouns,
the head of noun phrases.
To assess statistical significance, we carried out
stratified shuffling tests, with 10, 000 random shuf-
fles, for all measures. Tables indicate significance
differences between the backoff models and the most
competitive baseline model on that measure, indi-
cated by an italic score. A star (?) indicates p < 0.05,
and a dagger (?) indicates p < 0.01. To see the di-
rection of a significant difference (i.e. whether the
backoff model is better or worse than the baseline),
69
wsj10 swbdnxt10
Dependency Constituency Dependency Constituency
UNK Dir. Undir. NED P R F UNK Dir. Undir. NED P R F
EM
Wds 25 32.5 52.5 67.0 49.5 48.5 49.0 25 30.6 50.9 66.8 45.4 47.1 46.3
POS ? 46.4 63.8 78.1 59.2 58.1 58.6 ? 53.0 65.0 76.8 52.5 52.9 52.7
VB Wds 25 29.4 52.4 70.5 51.3 52.6 52.0 25 36.1 54.9 72.7 49.0 50.0 49.5POS ? 43.5 61.9 77.3 59.7 57.1 58.4 ? 51.3 62.5 74.3 47.1 46.6 46.8
W
ds+
PO
S Cond. 50 49.9? 66.1? 79.6? 64.2? 61.9? 63.0? 100 45.5? 62.4? 77.8 58.4? 58.9? 58.7?
Joint 50 46.0 63.7 79.0 62.0? 59.1 60.5? 1 49.4? 63.7 79.6? 60.0? 52.9 56.3?
Indep. 25 52.5? 68.0? 83.5? 63.5? 61.5? 62.5? 100 55.7? 65.8 74.6? 61.5? 57.9? 59.6?
LH ? 26.0 55.8 74.3 53.1 69.6 60.3 ? 24.1 50.8 72.7 60.8 82.5 70.0
RH ? 31.2 56.4 61.4 25.8 33.8 29.3 ? 29.2 52.0 57.9 22.2 30.1 25.5
CCL ? ? ? ? 50.8 40.7 45.2 ? ? ? ? 53.6 47.4 50.3
UPP ? ? ? ? 52.8 37.2 43.7 ? ? ? ? 60.0 46.6 52.4
Table 2: Performance on wsj10 and swbdnxt10 for models using words and POS tags only. Bold scores indicate
the best performance of all models and baselines on that measure.
? Significantly different from best non-uniform baseline (italics) by a stratified shuffling test, p < 0.01; ?: p < 0.05.
look to the scores themselves.
5 Results
In all results, when a model sees only one kind of
information, that is expressed by writing out the ab-
breviation for the relevant stream: ?Wds? for words,
?POS? for Part-Of-Speech, ?Dur? for word duration.
For baseline models that see two streams, the abbre-
viations are joined by a ??? symbol (as they treat
input pairs as atoms drawn in the cross-product of the
two streams? vocabulary). For the backoff models,
the abbreviations are joined by a ?+? symbol (as they
combine the information sources with a weighted
sum), with the ?extra? stream name first.
5.1 Results: wsj10
The left half of Table 2 presents results on wsj10.
For the baseline models, the first column with hori-
zontal text indicates the input, while for the backoff
(Wds+POS) models, the first column with horizontal
text indicates whether and how the extra stream is
modeled in dependents (as described in Section 3.3).
The EM model with POS input is largely a repli-
cation of the original DMV, differing in the use of
separate train, dev, and test sets, and possibly the
details of the harmonic initializer. Our replication
achieves an undirected attachment score of 63.8 on
the test set, similar to the score of 64.5 reported by
Klein and Manning (2004) when training and evalu-
ating on all of wsj10. Cohen et al (2008) use the
same train/dev/test partition that we do, and report
a directed attachment score of 45.8, similar to our
directed attachment score of 46.4.
The VB model which learns from POS tags does
not outperform the EM model which learns from POS
tags, suggesting that data sparsity does not hurt the
DMV when using POS tags. As expected, the words-
only models perform much worse than both the POS
input models and the uniform LH baseline. VB does
improve the words-only constituency performance.
The Cond. and Indep. backoff models outperform
the POS-only baseline on all measures, but the Joint
backoff model does not demonstrate a clear advan-
tage over the POS-only baseline on any measure. The
success of the Indep. model indicates that modelling
dependent word identity does provide enough infor-
mation to justify the increase in sparsity. The failure
of the Joint model to provide a further improvement
indicates that the extra information in the full joint
over dependents does not justify the large increase
in parameters. We also see that several models out-
perform the LH baseline on dependencies, but the
advantage is much less in F-Score, underscoring the
loss of information in the conversion of dependen-
cies to constituencies. Finally, all models outperform
CCL and UPP on F-score, emphasizing their reliance
on the punctuation we removed.
70
Dependency Constituency
UNK Dir. Undir. NED P R F
EM
Wds 25 30.6 50.9 66.8 45.4 47.1 46.3
Wds?Dur 25 26.1 46.5 62.0 45.6 48.7 47.1
VB Wds 25 36.4 55.1 73.0 49.1 50.0 49.6Wds?Dur 25 31.8 51.7 71.3 49.2 55.9 52.3
Du
r+W
ds Cond. 25 32.6? 55.1 74.5? 59.1? 71.4? 64.7?
Joint 50 31.8? 51.8? 70.8? 54.4? 60.5? 57.3?
Indep. 50 40.3? 59.1? 76.0? 56.1? 61.7? 58.8?
LH ? 24.1 50.8 72.7 60.8 82.5 70.0
RH ? 29.2 52.0 57.9 22.2 30.1 25.5
CCL ? ? ? ? 53.6 47.4 50.3
UPP ? ? ? ? 60.0 46.6 52.4
l
l
45 50 55 60
45
50
55
60
65
70
75
Switchboard Model Performance
Undirected Attachment Score
Co
nst
itue
ncy
 F?
sco
re
l
l
Wds
WdsxDur
Cond.
Joint
Indep.
LH
Table 3: Performance on swbdnxt10 for models using words and duration. The scatterplot includes a subset of the
information in the table: F-score and undirected attachment accuracy for backoff models and VB and LH baseline.
Bold, italics, and significance annotations as in Table 2.
5.2 Results: swbdnxt10
The right half of Table 2 presents performance fig-
ures on swbdnxt10 for input involving words and
POS tags. As expected, the EM and VB baselines
perform best when learning from gold-standard POS
tags, and we again see no benefit for the VB POS-
only model compared to the EM POS-only model.
The POS-only baselines far outperform the uniform-
attachment baselines on the dependency measures; to
our knowledge this is the first demonstration outside
the newspaper domain that the DMV outperforms a
uniform branching strategy on these measures.
The other comparisons among systems listed in
Table 2 are largely inconclusive. Models do com-
paratively well on either the constituency or depen-
dency evaluation, but not both. The backoff mod-
els outperform the baseline POS-only models in the
constituency evaluation, but underperform or match
those same models in the dependency evaluation.
Conversely, most models outperform the LH base-
line in the dependency evaluation, but not in the
constituency evaluation. There are probably two
causes for the ambiguity in these results. First, the
noise in the dependency gold-standard may have over-
whelmed any advantage from backoff. Second, as we
saw with wsj10, the conversion from dependencies
to constituencies removes information, which may
explain the failure of any model to outperform the
LH baseline in the constituency evaluation.
Table 3 presents performance figures on
swbdnxt10 for input involving words and duration,
including a scatter-plot of Undirected attachment
against constituency F-Score for the interesting
comparisons. In the scatter-plot, models up and
to the right performed better, and we see that the
negative correlation between the dependency and
constituency evaluations persists in words and dura-
tion input. VB substantially outperforms EM in the
baselines, indicating that good smoothing is helpful
when learning from words. Other comparisons
are again ambiguous; the dependency evaluation
is noisy, and backoff models outperform baseline
models on the constituency evaluation but not the
LH baseline. Still, the backoff models outperform
all words-only baselines in constituency score, with
two performing slightly worse in dependency score
and one performing much better. So there is some
evidence that word duration is useful, but we will
find clearer evidence on the brent corpus.
5.3 Results: brent
Table 4 presents results on the brent dataset. VB
is even more effective than in the other datasets for
improving performance among baseline models, lead-
ing to double-digit improvements on some measures.
Moreover, the best dev-set UNK cutoff drops to 1
for all VB models, indicating that, on this dataset,
VB provides good smoothing even in models without
backoff. This difference between datasets is likely
related to differences in vocabulary diversity; the
71
Dependency Constituency
UNK Dir. Undir. NED P R F
EM
Wds 25 36.9 56.3 70.7 52.4 69.5 59.8
Wds?Dur 25 31.3 51.1 66.9 50.7 64.7 56.9
VB Wds 1 51.2 64.2 77.3 63.3 68.1 66.0Wds?Dur 1 47.0 60.5 74.0 66.2 64.9 65.5
Du
r+W
ds Cond. 1 53.1? 65.5? 78.7? 65.4 68.6 67.0?
Joint 1 50.7 63.0 76.3 65.6 65.4? 65.5
Indep. 1 53.2 66.7? 79.6? 61.5? 67.9 64.5
LH ? 28.3 53.6 78.3 47.9 85.6 61.4
RH ? 27.2 48.8 61.1 26.2 46.8 33.6
CCL ? ? ? ? 41.7 58.8 48.8
UPP ? ? ? ? 56.8 63.8 60.1
l
l
50 55 60 65 70
60
62
64
66
68
70
Brent Model Performance
Undirected Attachment Score
Co
nst
itue
ncy
 F?
sco
re
l
l
Wds
WdsxDur
Cond.
Joint
Indep.
LH
Table 4: Performance on brent for models using words and duration. The scatterplot includes a subset of the
information in the table: F-score and undirected attachment accuracy for backoff models and VB and LH baseline.
Bold, italics, and significance annotations as in Table 2.
type:token ratio in the brent training set is about
1:15, compared to 1:5 and 1:9 in the wsj10 and
swbdnxt10 training sets, respectively.
More importantly for our main hypothesis, all
three backoff models using words and duration out-
perform the words-only baselines (including CCL
and UPP) on all dependency measures?the most
accurate measures on this corpus, which has hand-
annotated dependencies?and the Cond. model also
wins on F-score.
6 Conclusion
In this paper, we showed how to use the DMV with
Backoff and two fully-generative variants to explore
the utility of word duration in fully lexicalized un-
supervised dependency parsing. Although other re-
searchers have incorporated features beyond words
and POS tags into DMV-like models (e.g., semantics:
Naseem and Barzilay (2011); morphology: Berg-
Kirkpatrick et al (2009)), we believe this is the first
example based on Headden et al (2009)?s backoff
method. As far as we know, our work is also the first
test of a DMV-based model on transcribed conver-
sational speech and the first to outperform uniform-
branching baselines without using either POS tags or
punctuation in the input. Our results show that fully-
lexicalized models can do well if they are smoothed
properly and exploit multiple cues.
Our experiments also suggest that CDS is espe-
cially easy to learn from. Model performance on
the brent dataset was generally higher than on
swbdnxt10, with a much lower UNK threshold.
This latter point, and the fact that brent has a much
lower word type/token ratio than the other datasets,
suggest that CDS provides more and clearer evidence
about words? syntactic behavior.
Finally, our results provide more evidence, using
a different, more powerful syntactic model than that
of Pate and Goldwater (2011), that word duration
is a useful cue for unsupervised parsing. We found
that several ways of incorporating duration were use-
ful, although the extra sparsity of Joint emissions
was not justified in any of our investigations. Our
results are consistent with both the prosodic and pre-
dictability bootstrapping hypotheses of language ac-
quisition, providing the first computational support
for these using a full syntactic parsing model and
tested on child-directed speech. While our models do
not provide a mechanistic account of how children
might use duration information to help with learning
syntax, they do show that this information is useful
in principle, even without any knowledge of latent
prosodic structure or its relationship to syntax. In ad-
dition, our results suggest it may be useful to explore
using word duration to enrich NLP tasks in speech-
related technologies, such as syntactically-inspired
language models for text-to-speech generation. In
the future, we also hope to investigate why duration
is helpful, designing experiments to tease apart the
role of prosody and predictability in learning syntax.
72
References
Matthew Aylett and Alice Turk. 2004. The smooth signal
redundancy hypothesis: A functional explanation for re-
lationships between redundancy, prosodic prominence,
and duration in spontaneous speech. Language and
Speech, 47(1):31?56.
Mary Beckman and Janet Pierrehumbert. 1986. Intona-
tional structure in Japanese and English. Phonology
Yearbook, 3:255?309.
Alan Bell, Jason M Brenier, Michelle Gregory, Cynthia
Girand, and Dan Jurafsky. 2009. Predictability effects
on durations of content and function words in conver-
sational English. Journal of Memory and Language,
60:92?111.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?, John
DeNero, and Dan Klein. 2009. Painless unsupervised
learning with features. In Proceedings of NAACL.
Michael R Brent and Jeffrey M Siskind. 2001. The role
of exposure to isolated words in early vocabulary de-
velopment. Cognition, 81:31?44.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2012. Turning the pipeline into a loop:
Iterated unsupervised dependency parsing and PoS in-
duction. In Proceedings of the NAACL-HLT Workshop
on the Induction of Linguistic Structure, pages 96?99,
Montre?al, Canada, June. Association for Computational
Linguistics.
Shay B Cohen and Noah A Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
NAACL.
Shay B Cohen, Kevin Gimpel, and Noah A Smith. 2008.
Logistic normal priors for unsupervised probabilistic
grammar induction. In Advances in Neural Information
Processing Systems 22.
Shay B Cohen, Dipanjan Das, and Noah A Smith. 2011.
Unsupervised structure prediction with non-parallel
multilingual guidance. In Proceedings of EMNLP.
Marie-Catherine de Marneffe and Christopher D Manning.
2008. Stanford typed dependencies manual. Technical
report.
Markus Dreyer and Izhak Shafran. 2007. Exploiting
prosody for PCFGs with latent annotations. In Pro-
ceedings of Interspeech, Antwerp, Belgium, August.
Susanne Gahl and Susan M Garnsey. 2004. Knowledge of
grammar, knowledge of usage: Syntactic probabilities
affect pronunciation variation. Language, 80:748?775.
Susanne Gahl, Susan M Garnsey, Cynthia Fisher, and
Laura Matzen. 2006. ?That sounds unlikely?: Syntac-
tic probabilities affect pronunciation. In Proceedings
of the 27th meeting of the Cognitive Science Society.
Lila Gleitman and Eric Wanner. 1982. Language acqui-
sition: The state of the art. In Eric Wanner and Lila
Gleitman, editors, Language acquisition: The state of
the art, pages 3?48. Cambridge University Press, Cam-
bridge, UK.
Will Headden, Mark Johnson, and David McClosky. 2009.
Improved unsupervised dependency parsing with richer
contexts and smoothing. In Proceedings of NAACL-
HLT.
Zhongqiang Huang and Mary Harper. 2010. Appropri-
ately handled prosodic breaks help PCFG parsing. In
Proceedings of NAACL-HLT, pages 37?45, Los Ange-
les, California, June. Association for Computational
Linguistics.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proceedings of NODALIDA 2007.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers. In Proceedings of EMNLP-CoNLL, pages
296?305.
Jeremy G Kahn, Matthew Lease, Eugene Charniak, Mark
Johnson, and Mari Ostendorf. 2005. Effective use of
prosody in parsing conversational speech. In Proceed-
ings of HLT-EMNLP, pages 233?240.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of ACL,
pages 479?486.
Matthias Trautner Kromann. 2003. The Danish Depen-
dency Treebank and the DTAG treebank tool. In Pro-
ceedings of the Second Workshop on Treebanks and
Linguistic Theories, pages 217?220.
Kenichi Kurihara and Taisuke Sato. 2006. Variational
Bayesian grammar induction for natural language. In
Proceedings of the International Colloquium on Gram-
matical Inference, pages 84?96.
Brian MacWhinney. 2000. The CHILDES project: Tools
for analyzing talk. Lawrence Erlbaum Associates, Mah-
wah, NJ, third edition.
Se?verine Millotte, Roger Wales, and Anne Christophe.
2007. Phrasal prosody disambiguates syntax. Lan-
guage and Cognitive Processes, 22(6):898?909.
James L Morgan, Richard P Meier, and Elissa L Newport.
1987. Structural packaging in the input to language
learning: contributions of prosodic and morphologi-
cal marking of phrases to the acquisition of language.
Cognitive Psychology, 19:498?550.
Tahira Naseem and Regina Barzilay. 2011. Using seman-
tic cues to learn syntax. In Proceedings of AAAI.
John K Pate and Sharon Goldwater. 2011. Unsupervised
syntactic chunking with acoustic cues: computational
models for prosodic bootstrapping. In Proceedings
of the 2nd ACL workshop on Cognitive Modeling and
Computational Linguistics.
73
Elias Ponvert, Jason Baldridge, and Katrin Erk. 2011.
Simple unsupervised grammar induction from raw text
with cascaded finite state models. In Proceedings of
ACL-HLT.
Patti J Price, Mari Ostendorf, Stefanie Shattuck-Hufnagel,
and Cynthia Fong. 1991. The use of prosody in syntac-
tic disambiguation. In Proceedings of the HLT work-
shop on Speech and Natural Language, pages 372?377,
Morristown, NJ, USA. Association for Computational
Linguistics.
C Anton Rytting, Chris Brew, and Eric Fosler-Lussier.
2010. Segmenting words from natural speech: subseg-
mental variation in segmental cues. Journal of Child
Language, 37(3):513?543.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rap-
poport1. 2011. Neutralizing linguistically problematic
annotations in unsupervised dependency parsing evalu-
ation. In Proceedings of the 49th ACL, pages 663?672.
Yoav Seginer. 2007. Fast unsupervised incremental pars-
ing. In Proceedings of ACL.
Amanda Seidl. 2007. Infants? use and weighting of
prosodic cues in clause segmentation. Journal of Mem-
ory and Language, 57(1):24?48.
Valentin I Spitkovsky, Hiyan Alshawi, Angel X Chang,
and Daniel Jurafsky. 2011a. Unsupervised dependency
parsing without gold part-of-speech tags. In Proceed-
ings of EMNLP.
Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky.
2011b. Punctuation: Making a point in unsupervised
dependency parsing. In Proceedings of CoNLL.
Harry Tily, Susanne Gahl, Inbal Arnon, Neal Snider,
Anubha Kothari, and Joan Bresnan. 2009. Syntactic
probabilities affect pronunciation variation in sponta-
neous speech. Language and Cognition, 1(2):147?165.
74
Transactions of the Association for Computational Linguistics, 1 (2013) 255?266. Action Editor: Kristina Toutanova.
Submitted 11/2012; Published 5/2013. c?2013 Association for Computational Linguistics.
Minimally-Supervised Morphological Segmentation
using Adaptor Grammars
Kairit Sirts
Institute of Cybernetics
Tallinn University of Technology
sirts@phon.ioc.ee
Sharon Goldwater
School of Informatics
The University of Edinburgh
sgwater@inf.ed.ac.uk
Abstract
This paper explores the use of Adaptor Gram-
mars, a nonparametric Bayesian modelling
framework, for minimally supervised morpho-
logical segmentation. We compare three train-
ing methods: unsupervised training, semi-
supervised training, and a novel model selec-
tion method. In the model selection method,
we train unsupervised Adaptor Grammars us-
ing an over-articulated metagrammar, then use
a small labelled data set to select which poten-
tial morph boundaries identified by the meta-
grammar should be returned in the final output.
We evaluate on five languages and show that
semi-supervised training provides a boost over
unsupervised training, while the model selec-
tion method yields the best average results over
all languages and is competitive with state-of-
the-art semi-supervised systems. Moreover,
this method provides the potential to tune per-
formance according to different evaluation met-
rics or downstream tasks.
1 Introduction
Research into unsupervised learning of morphology
has a long history, starting with the work of Harris
(1951). While early research was mostly motivated
by linguistic interests, more recent work in NLP often
aims to reduce data sparsity in morphologically rich
languages for tasks such as automatic speech recogni-
tion, statistical machine translation, or automatic text
generation. For these applications, however, com-
pletely unsupervised systems may not be ideal if
even a small amount of segmented training data is
available. In this paper, we explore the use of Adap-
tor Grammars (Johnson et al, 2007) for minimally
supervised morphological segmentation.
Adaptor Grammars (AGs) are a nonparametric
Bayesian modelling framework that can learn latent
tree structures over an input corpus of strings. For
example, they can be used to define a morpholog-
ical grammar where each word consists of zero or
more prefixes, a stem, and zero or more suffixes; the
actual forms of these morphs (and the segmentation
of words into morphs) are learned from the data. In
this general approach AGs are similar to many other
unsupervised morphological segmentation systems,
such as Linguistica (Goldsmith, 2001) and the Mor-
fessor family (Creutz and Lagus, 2007). A major
difference, however, is that the morphological gram-
mar is specified as an input to the program, rather
than hard-coded, which allows different grammars
to be explored easily. For the task of segmenting
utterances into words, for example, Johnson and col-
leagues have experimented with grammars encoding
different kinds of sub-word and super-word structure
(e.g., syllables and collocations), showing that the
best grammars far outperform other systems on the
same corpora (Johnson, 2008a; Johnson and Goldwa-
ter, 2009; Johnson and Demuth, 2010).
These word segmentation papers demonstrated
both the power of the AG approach and the syner-
gistic behavior that occurs when learning multiple
levels of structure simultaneously. However, the best-
performing grammars were selected using the same
corpus that was used for final testing, and each paper
dealt with only one language. The ideal unsuper-
vised learner would use a single grammar tuned on
255
one or more development languages and still perform
well on other languages where development data is
unavailable. Indeed, this is the basic principle be-
hind Linguistica and Morfessor. However, we know
that different languages can have very different mor-
phological properties, so using a single grammar for
all languages may not be the best approach if there
is a principled way to choose between grammars.
Though AGs make it easy to try many different pos-
sible grammars, the process of proposing and testing
plausible options can still be time-consuming.
In this paper, we propose a novel method for au-
tomatically selecting good morphological grammars
for different languages (English, Finnish, Turkish,
German, and Estonian) using a small amount of
gold-segmented data (1000 word types). We use
the AG framework to specify a very general binary-
branching grammar of depth four with which we
learn a parse tree of each word that contains several
possible segmentation splits for the word. Then, we
use the gold-segmented data to learn, for each lan-
guage, which of the proposed splits from the original
grammar should actually be used in order to best
segment that language.
We evaluate our approach on both a small devel-
opment set and the full Morpho Challenge test set
for each language?up to three million word types.
In doing so, we demonstrate that using the posterior
grammar of an AG model to decode unseen data is
a feasible way to scale these models to large data
sets. We compare to several baselines which use the
annotated data to different degrees: parameter tuning,
grammar tuning, supervised training, or no use of
annotated data. In addition to existing approaches?
unsupervised and semi-supervised Morfessor, unsu-
pervised Morsel (Lignos, 2010), and unsupervised
AGs?we also show how to use the annotated data to
train semi-supervised AGs (using the data to accumu-
late rule statistics rather than for grammar selection).
The grammar selection method yields comparable
results to the best of these other approaches.
To summarize, our contributions in this paper are:
1) scaling AGs to large data sets by using the poste-
rior grammar to define an inductive model; 2) demon-
strating how to train semi-supervised AG models, and
showing that this improves morphological segmenta-
tion over unsupervised training; and 3) introducing
a novel grammar selection method for AG models
whose segmentation results are competitive with the
best existing systems.
Before providing details of our methods and re-
sults, we first briefly review Adaptor Grammars. For
a formal definition, see Johnson et al (2007).
2 Adaptor Grammars
Adaptor Grammars are a framework for specifying
nonparametric Bayesian models that can be used to
learn latent tree structures from a corpus of strings.
There are two components to an AG model: the base
distribution, which is just a PCFG, and the adaptor,
which ?adapts? the probabilities assigned to individ-
ual subtrees under the PCFG model, such that the
probability of a subtree under the complete model
may be considerably higher than the product of the
probabilities of the PCFG rules required to construct
it. Although in principle the adaptor can be any func-
tion that maps one distribution onto another, Johnson
et al (2007) use a Pitman-Yor Process (PYP) (Pit-
man and Yor, 1997) as the adaptor because it acts
as a caching model. Under a PYP AG model, the
posterior probability of a particular subtree will be
roughly proportional to the number of times that sub-
tree occurs in the current analysis of the data (with
the probability of unseen subtrees being computed
under the base PCFG distribution).
An AG model can be defined by specifying the
CFG rules (the support for the base distribution) and
indicating which non-terminals are ?adapted?, i.e.,
can serve as the root of a cached subtree. Given this
definition and an input corpus of strings, Markov
chain Monte Carlo samplers can be used to infer the
posterior distribution over trees (and all hyperparam-
eters of the model, including PCFG probabilities in
the base distribution and PYP hyperparameters). Any
frequently recurring substring (e.g., a common pre-
fix) will tend to be parsed consistently, as this permits
the model to treat the subtree spanning that string as
a cached subtree, assigning it higher probability than
under the PCFG distribution.
Adaptor Grammars have been applied to a wide
variety of tasks, including segmenting utterances
into words (Johnson, 2008a; Johnson and Goldwa-
ter, 2009; Johnson and Demuth, 2010), classifying
documents according to perspective (Hardisty et al,
2010), machine transliteration of names (Huang et
256
al., 2011), native language identification (Wong et
al., 2012), and named entity clustering (Elsner et al,
2009). There have also been AG experiments with
morphological segmentation, but more as a proof of
concept than an attempt to achieve state-of-the-art
results (Johnson et al, 2007; Johnson, 2008b).
3 Using AGs for Learning Morphology
Originally, the AG framework was designed for un-
supervised learning. This section first describes how
AGs can be used for unsupervised morphological
segmentation, and then introduces two ways to use
a small labelled data set to improve performance:
semi-supervised learning and grammar selection.
3.1 Unsupervised Adaptor Grammars
We define three AG models to use as unsupervised
baselines in our segmentation experiments. The first
of these is very simple:
Word? Morph+
Morph? Char+ (1)
The underline notation indicates an adapted non-
terminal, and + abbreviates a set of recursive rules,
e.g., Word? Morph+ is short for
Word? Morphs
Morphs? Morph Morphs
Morphs? Morph
Grammar 1 (MorphSeq) is just a unigram model
over morphs: the Morph symbol is adapted, so the
probability of each Morph will be roughly propor-
tional to its (inferred) frequency in the corpus. The
grammar specifies no further structural relationships
between morphs or inside of morphs (other than a
geometric distribution on their length in characters).
Experiments with AGs for unsupervised word seg-
mentation suggest that adding further latent structure
can help with learning. Here, we add another layer
of structure below the morphs,1 calling the resulting
1Because the nonterminal labels are arbitrary, this grammar
can also be interpreted as adding another layer on top of morphs,
allowing the model to learn morph collocations that encode de-
pendencies between morphs (which themselves have no substruc-
ture). However preliminary experiments showed that the morph-
submorph interpretation scored better than the collocation-morph
interpretation, hence we chose the corresponding nonterminal
names.
grammar SubMorphs:
Word? Morph+
Morph? SubMorph+
SubMorph? Char+
(2)
For capturing the rules of morphotactics, a gram-
mar with linguistically motivated non-terminals can
be created. There are many plausible options and
the best-performing grammar may be somewhat
language-dependent. Rather than experimenting ex-
tensively, we designed our third grammar to replicate
as closely as possible the grammar that is implicitly
implemented in the Morfessor system. This Com-
pounding grammar distinguishes between prefixes,
stems and suffixes, allows compounding, defines the
order in which the morphs can occur and also allows
the morphs to have inner latent structure:
Word? Compound+
Compound? Prefix? Stem Suffix?
Prefix? SubMorph+
Stem? SubMorph+
Suffix? SubMorph+
SubMorph? Char+
(3)
3.2 Semi-Supervised Adaptor Grammars
The first new use of AGs we introduce is the semi-
supervised AG, where we use the labelled data to ex-
tract counts of the different rules and subtrees used in
the gold-standard analyses. We then run the MCMC
sampler as usual over both the unlabelled and la-
belled data, treating the counts from the labelled data
as fixed.
We assume that the labelled data provides a con-
sistent bracketing (no two spans in the bracketing
can partially overlap) and the labels of the spans
must be compatible with the grammar. However,
the bracketing may not specify all levels of structure
in the grammar. In our case, we have morpheme
bracketings but not, e.g., submorphs. Thus, using
the SubMorphs grammar in semi-supervised mode
will constrain the sampler so that Morph spans in the
labelled data will remain fixed, while the SubMorphs
inside those Morphs will be resampled.
257
The main change made to the AG inference pro-
cess2 for implementing the semi-supervised AG was
to prune out from the sampling distribution any non-
terminals that are inconsistent with the spans/labels
in the given labelling.
3.3 AG Select
Both the unsupervised and semi-supervised methods
described above assume the definition of a grammar
that adequately captures the phenomena being mod-
elled. Although the AG framework makes it easy
to experiment with different grammars, these experi-
ments can be time-consuming and require some good
guesses as to what a plausible grammar might be.
These problems can be overcome by automating the
grammar development process to systematically eval-
uate different grammars and find the best one.
We propose a minimally supervised model selec-
tion method AG Select that uses the AG framework to
automatically identify the best grammar for different
languages and data sets. We first define a very gen-
eral binary-branching CFG grammar for AG training
that we call the metagrammar. The metagrammar
learns a parse tree for each word where each branch
contains a different structure in the word. The granu-
larity of these structures is determined by the depth of
this tree. For example, Grammar 4 generates binary
trees of depth two and can learn segmentations of up
to four segments.
Word? M1
Word? M1 M2
M1? M11
M1? M11 M12
M2? M21
M2? M21 M22
M11? Chars+
M12? Chars+
M21? Chars+
M22? Chars+
(4)
Next we introduce the notion of a morphologi-
cal template, which is an ordered sequence of non-
terminals whose concatenated yields constitute the
word and which are used to parse out a specific seg-
mentation of the word. For example, using Gram-
mar 4 the parse tree of the word saltiness is shown in
Figure 1. There are four possible templates with four
2We started with Mark Johnson?s PYAG implementa-
tion, http://web.science.mq.edu.au/?mjohnson/code/py-cfg.tgz,
which we also used for our unsupervised and grammar selection
experiments.
Word
M1
M11
s a l
M12
t
M2
M21
i
M22
n e s s
Figure 1: The parse tree generated by the metagrammar
of depth 2 for the word saltiness.
different segmentations: M1 M2 (salt iness), M11
M12 M2 (sal t iness), M1 M21 M22 (salt i ness),
and M11 M12 M21 M22 (sal t i ness).
The morphological template consisting only of
non-terminals from the lowest cached level of the
parse tree is expected to have high recall, whereas
the template containing the non-terminals just below
the Word is expected to have high precision. Our
goal is to find the optimal template by using a small
labelled data set. The grammar selection process iter-
ates over the set of all templates. For each template,
the segmentations of the words in the labelled data
set are parsed out and the value of the desired evalua-
tion metric is computed. The template that obtained
the highest score is then chosen.
For each language we use a single template to seg-
ment all the words in that language. However, even
using (say) a four-morph template such as M11 M12
M21 M22, some words may contain fewer morphs
because the metagrammar permits either unary or
binary branching rules, so some parses may not con-
tain M12 or M2 (and thus M21 M22) spans. Thus,
we can represent segmentations of different lengths
(from 1 to 2n, where n is the depth of the metagram-
mar) with a single template.3
For our experiments we use a metagrammar of
depth four. This grammar allows words to consist of
up to 16 segments, which we felt would be enough for
any word in the training data. Also, iterating over all
the templates of a grammar with bigger depth would
not be feasible as the number of different templates
increases very rapidly.4
3We also experimented with selecting different templates for
words of different length but observed no improvements over the
single template approach.
4The number of templates of each depth can be expressed
recursively as Ni = (Ni?1 + 1)2, where Ni?1 is the number of
258
3.4 Inductive Learning
Previous work on AGs has used relatively small data
sets and run the sampler on the entire input corpus
(some or all of which is also used for evaluation)?a
transductive learning scenario. However, our larger
data sets contain millions of word types, where sam-
pling over the whole set is not feasible. For example,
1000 training iterations on 50k word types took about
a week on one 2.67 GHz CPU. To solve this problem,
we need an inductive learner that can be trained on a
small set of data and then used to segment a different
larger set.
To create such a learner, we run the sampler on
up to 50k word types, and then extract the posterior
grammar as a PCFG.5 This grammar contains all the
initial CFG rules, plus rules to generate each of the
cached subtrees inferred by the sampler. The sampler
counts of all rules are normalized to obtain a PCFG,
and we can then use a standard CKY parser to decode
the remaining data using this PCFG.
4 Experiments
4.1 Data
We test on languages with a range of morphologi-
cal complexity: English, Finnish, Turkish, German,
and Estonian. For each language we use two small
sets of gold-annotated data?a labelled set for semi-
supervised training or model selection and a dev
set for development results?and one larger gold-
annotated dataset for final tests. We also have a large
unlabelled training set for each language. Table 1
gives statistics.
The data sets for English, Finnish, Turkish and
German are from the Morpho Challenge 2010 com-
petition6 (MC2010). We use the MC2010 training
set of 1000 annotated word types as our labelled data,
and for our dev sets we collate together the devel-
opment data from all years of the MC competition.
Final evaluation is done on the official MC2010 test
sets, which are not public, so we rely on the MC
organizers to perform the evaluation. The words in
templates in the grammar of depth one less and N0 = 0.
5This can be seen as a form of Structure Compilation (Liang
et al, 2008), where the solution found by a more costly model
is used to define a less costly model. However in Liang et al?s
case both models were already inductive.
6http://research.ics.aalto.fi/events/morphochallenge2010/
datasets.shtml
Unlab. Lab. Dev Test
English 0.9M 1000 1212 16K
Finnish 2.9M 1000 1494 225K
Turkish 0.6M 1000 1531 64K
German 2.3M 1000 785 62K
Estonian 2.1M 1000 1500 74K
Table 1: Number of word types in our data sets.
each test set are an unknown subset of the words in
the unlabelled corpus, so to evaluate we segmented
the entire unlabelled corpus and sent the results to
the MC team, who then computed scores on the test
words.
The Estonian wordlist is gathered from the news-
paper texts of a mixed corpus of Estonian.7 Gold
standard segmentations of some of these words are
available from the Estonian morphologically disam-
biguated corpus;8 we used these for the test set, with
small subsets selected randomly for the labelled and
dev sets.
For semi-supervised tests of the AG Compounding
grammar we annotated the morphemes in the English,
Finnish and Estonian labelled sets as prefixes, stems
or suffixes. We could not do so for Turkish because
none of the authors knows Turkish.
4.2 Evaluation
We evaluate our results with two measures: segment
border F1-score (SBF1) and EMMA (Spiegler and
Monson, 2010). SBF1 is one of the simplest and
most popular evaluation metrics for morphological
segmentations. It computes F1-score from the preci-
sion and recall of ambiguous segment boundaries?
i.e., word edges are not counted. It is easy and quick
to compute but has the drawback that it gives no
credit for one-morpheme words that have been seg-
mented correctly (i.e., are assigned no segment bor-
ders). Also it can only be used on systems and gold
standards where the output is just a segmentation of
the surface string (e.g., availabil+ity) rather than a
morpheme analysis (e.g., available+ity). For this
reason we cannot report SBF1 on our German data,
which annotations contain only analyses.
EMMA is a newer measure that addresses both
7http://www.cl.ut.ee/korpused/segakorpus/epl
8http://www.cl.ut.ee/korpused/morfkorpus/
259
of these issues?correctly segmented one-morpheme
words are reflected in the score, and it can evalu-
ate both concatenative and non-concatenative mor-
phology. EMMA works by finding the best one-to-
one mapping between the hypothesized and true seg-
ments. The induced segments are then replaced with
their mappings and based on that, F1-score on match-
ing segments is calculated. Using EMMA we can
evaluate the induced segmentations of German words
against gold standard analyses. EMMA has a freely
available implementation,9 but is slow to compute
because it uses Integer Linear Programming.
For our dev results, we computed both scores us-
ing the entire dev set, but for the large test sets, the
evaluation is done on batches of 1000 word types se-
lected randomly from the test set. This procedure is
repeated 10 times and the average is reported, just as
in the MC2010 competition (Kohonen et al, 2010a).
4.3 Baseline Models
We compare our AG models to several other mor-
phology learning systems. We were able to obtain
implementations of two of the best unsupervised sys-
tems from MC2010, Morfessor (Creutz and Lagus,
2007) and Morsel (Lignos, 2010), and we use these
for comparisons on both the dev and test sets. We
also report test results from MC2010 for the only
semi-supervised system in the competition, semi-
supervised Morfessor (Kohonen et al, 2010a; Ko-
honen et al, 2010b). No dev results are reported on
this system since we were unable to obtain an imple-
mentation. This section briefly reviews the systems.
4.3.1 Morfessor Categories-MAP
Morfessor Categories-MAP (Morfessor) is a state-
of-the-art unsupervised morphology learning system.
Its implementation is freely available10 so it is widely
used both as a preprocessing step in tasks requiring
morphological segmentations, and as a baseline for
evaluating morphology learning systems.
Morfessor uses the Minimum Description Length
(MDL) principle to choose the optimal segment lexi-
con and the corpus segmentation. Each morph in the
segment lexicon is labelled as a stem, prefix, suffix
9http://www.cs.bris.ac.uk/Research/MachineLearning/
Morphology/resources.jsp#eval
10http://www.cis.hut.fi/projects/morpho/
morfessorcatmapdownloadform.shtml
or non-morph. The morphotactic rules are encoded
as an HMM, which specifies the allowed morph se-
quences with respect to the labels (e.g., a suffix can-
not directly follow a prefix).
The morphs in the segment lexicon can have a
hierachical structure, containing submorphs which
themselves can consist of submorphs etc. We hypoth-
esize that this hierarchical structure is one of the key
reasons why Morfessor has been so successful, as the
experiments also in this paper with different gram-
mars show that the ability to learn latent structures is
crucial for learning good segmentations.
One essential difference between Morfessor and
the proposed AG Select is that while we use the la-
belled data to choose which levels of the hierarchy
are to be used as morphs, Morfessor makes this de-
cision based on the labels of the segments, choosing
the most fine-grained morph sequence that does not
contain the non-morph label.
Morfessor includes a free parameter, perplexity
threshold, which we found can affect the SBF1 score
considerably (7 points or more). The best value for
this parameter depends on the size of the training
set, characteristics of the language being learned, and
also the evaluation metric being used, as in some
cases the best SBF1 and EMMA scores are obtained
with completely different values.
Thus, we tuned the value of the perplexity thresh-
old on the labelled set for each language and evalua-
tion metric for different unlabelled training set sizes.
4.3.2 Semi-Supervised Morfessor
Recently, the Morfessor system has been adapted
to allow semi-supervised training. Four versions of
the system were evaluated in MC2010, using differ-
ent degrees of supervision. Results reported here are
from the Morfessor S+W system, which performed
best of those that use the same kind of labelled data
as we do.11 This system uses the Morfessor Base-
line model (not Cat-MAP), which incorporates a
lexicon prior and data likelihood term. The semi-
supervised version maintains separate likelihoods for
the labelled and unlabelled data, and uses the devel-
opment set to tune two parameters that weight these
terms with respect to each other and the prior.
11Morfessor S+W+L performs better, but uses training data
with morpheme analyses rather than surface segmentations.
260
4.3.3 Morsel
Morsel (Lignos, 2010) is an unsupervised mor-
phology learning system introduced in MC2010; we
obtained the implementation from the author. Morsel
learns morphological analyses rather than segmenta-
tions, so it can be evaluated only using EMMA. There
are two options provided for running Morsel: aggres-
sive and conservative. We used the development set
to choose the best in each experimental case.
The MC data sets contain gold standard morpho-
logical analyses (as well as segmentations) so we
could compute Morsel?s EMMA scores using the
analyses. However, we found that Morsel obtains
higher EMMA scores when evaluated against gold
standard segmentations and thus we used this option
in all the experiments. (EMMA scores for other sys-
tems were also computed using the segmentations.)
4.4 Method
The experiments were conducted in two parts. First,
we evaluated different aspects of the AG models and
compared to all baseline models using the dev set
data. Then we evaluated the most competitive models
on the final test data.
For the development experiments, we compiled un-
labelled training sets with sizes ranging from 10k to
50k word types (using the most frequent word types
in each case). For the AG results, we report the aver-
age of five different runs made on the same training
set. We let the sampler run for 1000 iterations. No
annealing was used as it did not seem to help. The
table label resampling option was turned on and the
hyperparameter values were inferred.
We trained all AG and baseline models on each of
these training sets. For AG Select, the words from
the labelled data set were added to the training set to
allow for template selection.12 To compute results in
transductive mode, the words from the dev set were
also added to the training data. In inductive mode,
the dev set was instead parsed with the CKY parser.
Preliminary experiments showed that the perfor-
mance of unsupervised AG and AG Select improved
with larger training sets, though the effect is small
(see Figure 2 for results of AG Select in transductive
12We also experimented with smaller sets of labelled data. In
most cases, the template selected based on only 300 word types
was the same than the one selected with 1000 word types.
mode; the trend in inductive mode is similar). Based
on these and similar results with other baseline sys-
tems, all results reported later for unsupervised mod-
els (AG and baseline) and AG Select were obtained
using training sets of 50k words.
In contrast to the above models, the semi-
supervised AG does not always improve with more
unlabelled data (see Figure 2) and in the limit, it
will match the performance of the same grammar
in the unsupervised setting. Other semi-supervised
approaches often solve this problem by weighting
the labelled data more heavily than the unlabelled
data when estimating model parameters?effectively,
assuming that each labelled item has actually been
observed more than once. However, duplicating the
labelled data does not make sense in the AG frame-
work, because duplicate items will in most cases just
be cached at the root (Word) node, providing no addi-
tional counts of Morphs (which are where the useful
information is). It might be possible to come up with
a different way to weight the labelled data more heav-
ily when larger unlabelled sets are used, however
for this paper we instead kept the labelled data the
same and tuned the amount of unlabelled data. We
used the dev set to choose the amount of unlabelled
data (in the range from 10k to 50k types); results for
semi-supervised AG are reported using the optimal
amount of unlabelled data for each experiment.
For test set experiments with semi-supervised AG,
we evaluated each language using whichever gram-
mar performed best on that language?s dev set. For
test set experiments with AG Select, we chose the
templates with a two-pass procedure. First, we
trained 5 samplers on the 50k training set with la-
belled set added, and used the labelled data to choose
the best template for each inferred grammar. Then,
we decoded the dev set using each of the 5 gram-
mar/template pairs and based on these results, chose
the best of these pairs to decode the test set.
4.5 Results
We present the dev set results in Table 2(a) for trans-
ductive and in Table 2(b) for inductive learning. In
each table, unsupervised models are shown in the
upper section and the semi-supervised models and
AG Select below. Morsel appears only in Table 2(a)
since it only works transductively. Semi-supervised
grammars cannot be trained on German, since we
261
 55
 60
 65
 70
 75
 80
 85
 90
 10000  20000  30000  40000  50000
F-s
cor
e
# of word types
EnglishEstonianFinnishTurkish
 55
 60
 65
 70
 75
 80
 85
 90
 10000  20000  30000  40000  50000
F-s
cor
e
# of word types
EnglishEstonianFinnishTurkish
Figure 2: Effect of training data size on dev set SBF1 for AG Select (left) and semi-supervised SubMorphs grammar
(right) in transductive mode.
only have gold standard analyses, not segmentations.
The SubMorphs grammar performs the best of the
unsupervised AG models, with the Compounding
grammar being only slightly worse. We also tried
the Compounding grammar without the sub-morph
structures but the results were even worse than those
of MorphSeq. This shows that the latent structures
are important for learning good segmentations.
In all cases, the semi-supervised AGs perform bet-
ter (ofen much better) than the corresponding unsu-
pervised grammars. Even though their average scores
are not as high as AG Select?s, they give the best dev
set results in many cases. This shows that although
for semi-supervised AG the grammar must be cho-
sen manually, with a suitable choice of the grammar
and only a small set of labelled data it can improve
considerably over unsupervised AG.
In transductive mode, the AG Select performs the
best in several cases. In both transductive and induc-
tive mode, the results of AG Select are close to the
best results obtained and are consistently good across
all languages?it achieves the best average scores
of all models, suggesting that the model selection
method is robust to different types of morphology
and annotation schemes.
Table 3 presents the test set results. We include
scores for unsupervised Morfessor in both transduc-
tive and inductive mode, where transductive mode
trains on the entire unlabelled corpus and inductive
mode trains on the 50k subset. The semi-supervised
Morfessor scores are taken from the MC results
page13 after verifying that the evaluation method-
13http://research.ics.aalto.fi/events/morphochallenge/
ology and labelled data used is the same as ours.14
There is a good deal of variation between devel-
opment and test results, indicating that the dev sets
may not be a representative sample. The most no-
table differences are in Turkish, where all models
perform far worse on the test than dev set. However,
AG Select performs slightly better on the test set for
the other languages. Thus its average SBF1 score ac-
tually improves on the test set and is not much worse
than semi-supervised Morfessor. While its average
performance drops somewhat on test set EMMA, it
is still as good as any other model on that measure.
Again, these results support the idea that AG Select
is robust to variations in language and data set.
We also note the surprisingly good performance
of Morfessor in transductive mode on Estonian; this
could possibly be due to the larger amount of training
data used for the test set results, but it is not clear
why this would improve performance so much on
Estonian and not on the other languages.
5 Discussion
To give a sense of what the AG Select model is learn-
ing, we provide some examples of both correctly and
incorrectly induced segmentations in Table 4. These
examples suggest that for example in English, M1 is
used to model the stem, M21 is for the suffix or the
second stem in the compound word, and the rest of
the elements in the template are for the remaining
suffixes (if any).
Table 5 presents examples of some of the most
frequently used metagrammar rules and cached rules
14Sami Virpioja, personal communication.
262
(a) Transductive mode Border F1-score EMMA
Eng Est Fin Tur Avg Eng Est Fin Tur Ger Avg
AG MorphSeq 61.5 54.0 56.9 59.5 58.0 74.7 74.1 63.7 53.5 59.4 65.1
AG SubMorphs 66.2 66.9 60.5 59.5 63.3 79.1 83.4 66.8 53.4 57.4 68.0
AG Compounding 63.0 64.8 60.9 60.9 62.4 75.4 81.6 65.5 53.7 62.2 67.7
Morfessor 69.5 55.7 65.0 69.3 64.9 81.3 75.3 67.8 62.2 62.7 69.9
Morsel - - - - - 76.8 74.4 66.1 50.1 55.9 64.7
AG ssv MorphSeq 64.4 57.3 63.0 68.9 63.4 74.4 75.9 65.6 59.6 - -
AG ssv SubMorphs 67.6 69.1 64.4 63.4 66.1 79.5 84.4 69.2 56.1 - -
AG ssv Compounding 70.0 67.5 71.8 - - 79.5 82.8 74.0 - - -
AG Select 71.9 68.5 70.2 72.6 70.8 77.5 81.8 73.2 63.0 62.4 71.6
(b) Inductive mode Border F1-score EMMA
Eng Est Fin Tur Avg Eng Est Fin Tur Ger Avg
AG MorphSeq 57.6 54.0 55.4 59.8 56.7 72.0 73.8 62.6 53.7 58.9 64.2
AG SubMorphs 66.1 67.5 61.6 59.8 63.7 78.6 83.7 67.4 53.4 56.0 67.8
AG Compounding 62.0 64.8 57.4 61.1 61.3 73.5 81.1 61.9 53.2 61.0 66.2
Morfessor 68.9 51.1 63.5 68.2 62.9 80.9 72.0 68.1 60.6 60.8 68.5
AG ssv MorphSeq 64.6 56.9 63.1 70.3 63.8 72.7 73.3 65.9 61.2 - -
AG ssv SubMorphs 70.1 69.7 66.3 67.9 68.4 80.4 83.7 70.5 59.0 - -
AG ssv Compounding 70.5 67.2 70.0 - - 77.3 81.9 70.5 - - -
AG Select 69.8 68.8 67.5 70.1 69.1 77.3 81.9 71.1 59.7 62.6 70.5
Table 2: Dev set results for all models in (a) transductive and (b) inductive mode. Unsupervised AG models and
baselines are shown in the top part of each table; semi-supervised AG models and grammar selection method are below.
Border F1-score EMMA
Eng Est Fin Tur Avg -Est Eng Est Fin Tur Ger Avg -Est/Ger
Morf. trans 67.3 73.9 61.2 57.1 64.9 61.9 78.4 78.8 61.8 49.8 65.2 66.8 63.3
Morf. ind 65.7 57.7 60.8 60.1 61.1 62.2 76.5 70.5 59.6 47.0 64.1 63.5 61.0
Morsel - - - - - - 81.9 77.2 63.3 47.8 59.0 65.8 64.3
Morf. ssv 77.8 - 71.7 68.9 - 72.8 80.6 - 62.1 49.9 - - 64.2
AG ssv best 70.3? 68.6? 64.9? 58.2? 65.5 64.5 75.9? 80.3? 61.3? 46.1? - - 61.1
AG Select 74.4 71.7 70.0 61.4 69.4 68.6 81.3 81.0 64.0 47.5 63.8 67.5 64.3
Table 3: Test set results for unsupervised baselines Morfessor CatMAP (in transductive and inductive mode) and Morsel;
semi-supervised Morfessor; and AG semi-supervised (? marks the Compounding grammar, ? denotes SubMorphs
grammar, and ? is the MorphSeq grammar) and grammar selection methods. Results are shown for each language,
averaged over all languages (when possible: Avg), and averaged over just the languages where scores are available for
all systems (-Est, -Est/Ger).
for English, together with their relative frequencies.
It shows that at the Word level the binary rule is
selected over three times more frequently than the
unary rule. Also, most of the more frequently used
grammar rules expand the first branch (rooted in M1)
into more finegrained structures. The second branch
(M2) is mostly modelled with the unary rule.
Among the frequently cached rules we see the
common English prefixes and suffixes. One of the
most frequent cached rule stores the single letter e at
the end of a word, which often causes oversegmen-
tation of words ending in e (as seen in the incorrect
examples in Table 4). This problem is common in
unsupervised morphological segmentation of English
(Goldwater et al, 2006; Goldsmith, 2001).
We also took a look at the most frequent cached
rules learned by the semi-supervised AG with the
SubMorphs grammar, and observed that Morphs
263
Correct Segmentations Incorrect Segmentations
Word Segmentation Induced Correct
treatable [tr.ea.t]M1 [a.b.le]M21 disagree s dis agree s
disciplined [dis.cip.l.i.n]M1 [e.d]M21 reduc e reduce
monogamous [mon.o.g.a.m]M1 [o.u.s]M21 revalu e re value
streakers [st.r.e.a.k]M1 [e.r]M21 [s]M2211 derid e deride
tollgate [t.o.l.l.]M1 [g.a.t.e]M21 [s]M2211 accompani ed ac compani ed
foxhunting [f.o.x]M1 [h.u.n.t]M21 [ing]M2211 war y wary
muscovites [m.u.sc.o.v]M1 [i.t.e]M21 [s]M2211 indescrib able in describ able
standardizes [st.a.n.d.a.rd]M1 [i.z.e]M21 [s]M2211 orat es orate s
slavers? [sl.a.v]M1 [e.r]M21 [s]M2211 [?]M2212 alger ian s algeri an s
earthiness? [e.ar.th]M1 [i]M2111 [ness]M2211 [?]M2212 disput e s dispute s
instinkt [in.st.in.kt]M1 meister likkust meisterlikkus t
rebis [re.b.i]M1 [s]M2 min a mina
toitsid [to.it]M1 [s.id]M2 teiste teis te
armuavaldus [a.rm.u]M11 [ava.ld.u.s]M12 kuritegu de sse kuri tegu desse
ma?a?givale [ma?a?.g.i]M11 [v.a]M12 [l.e]M2 liharoa ga liha roa ga
keskuskoulussa [kesk.us]M11 [koul.u]M12 [s.sa]M2 polte tti in polte tt i in
perusla?hteille [per.u.s]M11 [l.a?.ht.e]M12 [i]M211 [ll.e]M212 kulttuuri se lt a kin kulttuurise lta kin
perunakaupoista [per.u.n.a]M11 [k.au.p.o]M12 [i]M211 [st.a]M212 tuote palki ntoja tuote palkinto j a
yo?paikkaani [yo?]M11 [p.ai.kk.a]M12 [a]M21 [n.i]M22 veli puo lt a veli puol ta
nimetta?ko?o?n [ni.m.e]M11 [tt.a?]M12 [k.o?]M21 [o?.n]M22 ota ttava otatta va
Table 4: Examples of segmented words in English (top), Estonian (middle) and Finnish (bottom). Correctly segmented
words are in the left part of the table. The identified segments are in brackets indexed by the respective template
nonterminal; dots separate the metagrammar generated parse tree leaves. Examples of incorrectly segmented words
together with the correct segmentation are on the right.
Freq (%) Rule Freq (%) Cached Rule
9.9 Word?M1 M2 1.2 (M2 (M21 (M211 (M2111 s)))))
5.7 M1?M11 M12 0.9 (M2 (M21 (M211 (M2111 e)) (M212 (M2121 d))))
3.1 Word?M1 0.7 (M2 (M21 (M211 (M2111 i)) (M212 (M2121 n g))))
2.5 M11?M111 0.6 (M2 (M21 (M211 (M2111 e)))
1.8 M2?M21 0.4 (M2 (M21 (M211 (M2111 ?))) (M22 (M221 (M2211 s))))
1.4 M12?M121 M122 0.3 (M1112 a)
1.4 M111?M1111 M1112 0.3 (M2 (M21 (M211 (M2111 y))))
0.9 M12?M121 0.3 (M2 (M21 (M211 (M2111 e))) (M212 (M2121 r)))
0.9 M11?M111 M112 0.2 (M2 (M21 (M211 (M2111 a))))
Table 5: Examples from English most frequently used metagrammar rules and cached rules together with their relative
occurrence frequencies (in percentages).
tended to contain only a single SubMorph. This
helps to explain why the SubMorphs grammar in
semi-supervised AG improved less over the unsuper-
vised AG as compared to the MorphSeq grammar?
the rules with only a single SubMorph under the
Morph are essentially the same as they would be in
the MorphSeq grammar.
Finally, we examined the consistency of the tem-
plates chosen for each of the 5 samplers during model
selection for the test set (Section 4.4). We found that
there was some variability in the templates, but in
most experiments the same template was chosen for
the majority of the samplers (see Table 6). While this
majority template is not always the optimal one on
the dev set, we observed that it does produce con-
sistently good results. It is possible that using the
majority template, rather than the optimal template
for the dev set, would actually produce better results
264
Majority template
English M1 M21 M2211 M2212 M222
Finnish M11 M12 M211 M212 M22
Turkish M11 M12 M211 M212 M22
German M11 M121 M122 M21 M221 M222
Estonian M11 M12 M2
Table 6: Majority templates for each language. Note
that the Estonian gold standard contains less fine-grained
segmentations than some of the other languages.
on the test set, especially if (as appears to be the case
here, and may often be the case in real applications)
the dev and test sets are from somewhat different
distributions.
It must be noted that both AG Select and semi-
supervised AG are computationally more demanding
than the comparison systems. Since we do inference
over tree structures, the complexity is cubic in the
input word length, while most segmentation systems
are quadratic or linear. Even compared to the unsu-
pervised AG, AG Select is more expensive, because
of the larger grammar and number of cached symbols.
Nevertheless, our systems can feasibly be run on the
large Morpho Challenge datasets.
Other recent unsupervised systems have reported
state-of-the art results by incorporating additional in-
formation from surrounding words (Lee et al, 2011),
multilingual alignments (Snyder and Barzilay, 2008),
or overlapping context features in a log-linear model
(Poon et al, 2009), but they have only been run on
Semitic languages and English (and in the latter case,
a very small corpus). Since they explicitly enumerate
and sample from all possible segmentations of each
word (often with some heuristic constraints), they
could have trouble with the much longer words of
the agglutinative languages tested here. In any case
the results are not directly comparable to ours.
6 Conclusion
In this paper we have introduced three new meth-
ods for Adaptor Grammars and demonstrated their
usefulness for minimally supervised morphological
segmentation. First, we showed that AG models can
be scaled to large data sets by using the posterior
grammar for defining an inductive model, that on
average results in the same accuracy as compared to
full transductive training.
Second, we implemented semi-supervised AG in-
ference, which uses labelled data to constrain the
sampler, and showed that in all cases it performs
much better than the unsupervised AG on the same
grammar. Semi-supervised AG could benefit from
labelled data reweighting techniques frequently used
in semi-supervised learning, and studying the proper
ways of doing so within the AG framework would be
a potential topic for future research.
Our final contribution is the AG Select method,
where the initial model is trained using a very general
grammar that oversegments the data, and the labelled
data is used to select which granularity of segments to
use. Unlike other morphological segmentation mod-
els, this method can adapt its grammar to languages
with different structures, rather than having to use
the same grammar for every language. Indeed, we
found that AG Select performs well across a range
of languages and also seems to be less sensitive to
differences between data sets (here, dev vs. test). In
addition, it can be trained on either morphological
analyses or segmentations. Although we tuned all
results to optimize the SBF1 metric, in principle the
same method could be used to optimize other mea-
sures, including extrinsic measures on downstream
applications such as machine translation or informa-
tion retrieval. In future we hope to show that this
method can be used to improve performance on such
applications, and also to explore its use for related
segmentation tasks such as stemming or syllabifica-
tion. Also, the method itself could potentially be
improved by designing a classifier to determinine the
best template for each word based on a set of features,
rather than using a single template for all words in
the language.
Acknowledgments
This work was supported by the Tiger University pro-
gram of Estonian Information Technology Founda-
tion for the first author. We thank Constantine Lignos
for releasing his Morsel code to us, Sami Virpioja for
evaluating test set results, and Federico Sangati for
providing useful scripts.
References
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
265
learning. ACM Transactions of Speech and Language
Processing, 4(1):1?34, February.
Micha Elsner, Eugene Charniak, and Mark Johnson. 2009.
Structured generative models for unsupervised named-
entity clustering. In Proceedings of NAACL, pages
164?172. Association for Computational Linguistics.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational Lin-
guistics, 27(2):153?198, June.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens by
estimating power-law generators. In Advances in Neu-
ral Information Processing Systems 18, pages 459?466,
Cambridge, MA. MIT Press.
Eric A. Hardisty, Jordan Boyd-Graber, and Philip Resnik.
2010. Modeling perspective using adaptor grammars.
In Proceedings of EMNLP, pages 284?292. Association
for Computational Linguistics.
Zellig Harris. 1951. Structural Linguistics. University of
Chicago Press.
Yun Huang, Min Zhang, and Chew Lim Tan. 2011. Non-
parametric bayesian machine transliteration with syn-
chronous adaptor grammars. In Proceedings of ACL:
short papers - Volume 2, pages 534?539. Association
for Computational Linguistics.
Mark Johnson and Katherine Demuth. 2010. Unsuper-
vised phonemic chinese word segmentation using adap-
tor grammars. In Proceedings of COLING, pages 528?
536. Association for Computational Linguistics.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric bayesian inference: experiments on un-
supervised word segmentation with adaptor grammars.
In Proceedings of NAACL, pages 317?325. Association
for Computational Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007. Adaptor grammars: A framework for
specifying compositional nonparametric bayesian mod-
els. In B. Scho?lkopf, J. Platt, and T. Hoffman, editors,
Advances in Neural Information Processing Systems
19, pages 641?648. MIT Press, Cambridge, MA.
Mark Johnson. 2008a. Unsupervised word segmentation
for sesotho using adaptor grammars. In Proceedings
of ACL Special Interest Group on Computational Mor-
phology and Phonology, pages 20?27. Association for
Computational Linguistics.
Mark Johnson. 2008b. Using adaptor grammars to iden-
tify synergies in the unsupervised acquisition of linguis-
tic structure. In Proceedings of ACL, pages 398?406.
Association for Computational Linguistics.
Oskar Kohonen, Sami Virpioja, and Krista Lagus. 2010a.
Semi-supervised learning of concatenative morphology.
In Proceedings of ACL Special Interest Group on Com-
putational Morphology and Phonology, pages 78?86.
Association for Computational Linguistics.
Oskar Kohonen, Sami Virpioja, Laura Leppa?nen, and
Krista Lagus. 2010b. Semi-supervised extensions to
morfessor baseline. In Proceedings of the Morpho
Challenge 2010 Workshop, pages 30?34. Aalto Univer-
sity School of Science and Technology.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2011. Modeling syntactic context improves morpho-
logical segmentation. In Proceedings of CoNLL, pages
1?9. Association for Computational Linguistics.
Percy Liang, Hal Daume?, III, and Dan Klein. 2008. Struc-
ture compilation: trading structure for features. In
Proceedings of ICML, pages 592?599. Association for
Computing Machinery.
Constantine Lignos. 2010. Learning from Unseen Data.
In Mikko Kurimo, Sami Virpioja, and Ville T. Turunen,
editors, Proceedings of the Morpho Challenge 2010
Workshop, pages 35?38. Aalto University School of
Science and Technology.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable sub-
ordinator. Annals of Probability, 25(2):855?900.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation with
log-linear models. In Proceedings of NAACL, pages
209?217. Association for Computational Linguistics.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. In Proceedings of ACL, pages 737?745. Associ-
ation for Computational Linguistics.
Sebastian Spiegler and Christian Monson. 2010. Emma:
A novel evaluation metric for morphological analysis.
In Proceedings of COLING, pages 1029?1037. Associ-
ation for Computational Linguistics.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2012. Exploring adaptor grammars for native language
identification. In Proceedings of EMNLP, pages 699?
709. Association for Computational Linguistics.
266
Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 1?8,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Using Sentence Type Information for Syntactic Category Acquisition
Stella Frank (s.c.frank@sms.ed.ac.uk)
Sharon Goldwater (sgwater@inf.ed.ac.uk)
Frank Keller (keller@inf.ed.ac.uk)
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
Abstract
In this paper we investigate a new source of
information for syntactic category acquisition:
sentence type (question, declarative, impera-
tive). Sentence type correlates strongly with
intonation patterns in most languages; we hy-
pothesize that these intonation patterns are a
valuable signal to a language learner, indicat-
ing different syntactic patterns. To test this hy-
pothesis, we train a Bayesian Hidden Markov
Model (and variants) on child-directed speech.
We first show that simply training a separate
model for each sentence type decreases perfor-
mance due to sparse data. As an alternative, we
propose two new models based on the BHMM
in which sentence type is an observed variable
which influences either emission or transition
probabilities. Both models outperform a stan-
dard BHMM on data from English, Cantonese,
and Dutch. This suggests that sentence type
information available from intonational cues
may be helpful for syntactic acquisition cross-
linguistically.
1 Introduction
Children acquiring the syntax of their native language
have access to a large amount of contextual informa-
tion. Acquisition happens on the basis of speech, and
the acoustic signal carries rich prosodic and intona-
tional information that children can exploit. A key task
is to separate the acoustic properties of a word from the
underlying sentence intonation. Infants become attuned
to the pragmatic and discourse functions of utterances
as signalled by intonation extremely early; in this they
are helped by the fact that intonation contours of child
and infant directed speech are especially well differen-
tiated between sentence types (Stern et al, 1982; Fer-
nald, 1989). Children learn to use appropriate intona-
tional melodies to communicate their own intentions at
the one word stage, before overt syntax develops (Snow
and Balog, 2002).
It follows that sentence type information (whether a
sentence is declarative, imperative, or a question), as
signaled by intonation, is readily available to children
by the time they start to acquire syntactic categories.
Sentence type also has an effect on sentence structure
in many languages (most notably on word order), so
we hypothesise that sentence type is a useful cue for
syntactic category learning. We test this hypothesis by
incorporating sentence type information into an unsu-
pervised model of part of speech tagging.
We are unaware of previous work investigating the
usefulness of this kind of information for syntactic
category acquisition. In other domains, intonation has
been used to identify sentence types as a means of im-
proving speech recognition language models. Specifi-
cally, (Taylor et al, 1998) found that using intonation
to recognize dialogue acts (which to a significant extent
correspond to sentence types) and then using a special-
ized language model for each type of dialogue act led
to a significant decrease in word error rate.
In the remainder of this paper, we first present the
Bayesian Hidden Markov Model (BHMM; Goldwater
and Griffiths (2007)) that is used as the baseline model
of category acquisition, as well as our extensions to
the model, which incorporate sentence type informa-
tion. We then discuss the distinctions in sentence type
that we used and our evaluation measures, and finally
our experimental results. We perform experiments on
corpora in four different languages: English, Spanish,
Cantonese, and Dutch. Our results on Spanish show no
difference between the baseline and the models incor-
porating sentence type, possibly due to the small size of
the Spanish corpus. Results on all other corpora show
a small improvement in performance when sentence
type is included as a cue to the learner. These cross-
linguistic results suggest that sentence type may be a
useful source of information to children acquiring syn-
tactic categories.
2 BHMM Models
2.1 Standard BHMM
We use a Bayesian HMM (Goldwater and Griffiths,
2007) as our baseline model. Like a standard trigram
HMM, the BHMM assumes that the probability of tag
ti depends only on the previous two tags, and the proba-
bility of word wi depends only on ti. This can be written
as
ti|ti?1 = t, ti?2 = t
?,?(t,t
?) ? Mult(?(t,t
?)) (1)
wi|ti = t,?
(t) ? Mult(?(t)) (2)
where ?(t,t
?) are the parameters of the multinomial dis-
tribution over following tags given previous tags (t, t ?)
1
and ?(t) are the parameters of the distribution over out-
puts given tag t. The BHMM assumes that these param-
eters are in turn drawn from symmetric Dirichlet priors
with parameters ? and ?, respectively:
?(t,t
?)|? ? Dirichlet(?) (3)
?(t)|? ? Dirichlet(?) (4)
Using these Dirichlet priors allows the multinomial dis-
tributions to be integrated out, leading to the following
predictive distributions:
P(ti|t?i,?) =
C(ti?2, ti?1, ti)+?
C(ti?2, ti?1)+T?
(5)
P(wi|ti, t?i,w?i,?) =
C(ti,wi)+?
C(ti)+Wti?
(6)
where t?i = t1 . . . ti?1, w?i = w1 . . .wi?1, C(ti?2, ti?1, ti)
and C(ti,wi) are the counts of the trigram (ti?2, ti?1, ti)
and the tag-word pair (ti,wi) in t?i and w?i, T is the
size of the tagset, and Wti is the number of word types
emitted by ti.
Based on these predictive distributions, (Goldwa-
ter and Griffiths, 2007) develop a Gibbs sampler for
the model, which samples from the posterior distri-
bution over tag sequences t given word sequences w,
i.e., P(t|w,?,?)? P(w|t,?)P(t|?). This is done by us-
ing Equations 5 and 6 to iteratively resample each tag
ti given the current values of all other tags.1 The re-
sults show that the BHMM with Gibbs sampling per-
forms better than the standard HMM using expectation-
maximization. In particular, the Dirichlet priors in the
BHMM constrain the model towards sparse solutions,
i.e., solutions in which each tag emits a relatively small
number of words, and in which a tag transitions to few
following tags. This type of model constraint allows
the model to find solutions which correspond to true
syntactic parts of speech (which follow such a sparse,
Zipfian distribution), unlike the uniformly-sized clus-
ters found by standard maximum likelihood estimation
using EM.
In the experiments reported below, we use the Gibbs
sampler described by (Goldwater and Griffiths, 2007)
for the BHMM, and modify it as necessary for our own
extended models. We also follow (Goldwater and Grif-
fiths, 2007) in using Metropolis-Hastings sampling for
the hyperparameters, which are inferred automatically
in all experiments. A separate ? parameter is inferred
for each tag.
2.2 BHMM with Sentence Types
We wish to add a sentence type feature to each time-
step in the HMM, signalling the current sentence type.
We treat sentence type (s) as an observed variable, on
the assumption that it is observed via intonation or
1Slight corrections need to be made to Equation 5 to ac-
count for sampling tags from the middle of the sequence
rather than from the end; these are given in (Goldwater and
Griffiths, 2007) and are followed in our own samplers.
ti?2 ti?1 ti
wisi
?
?
N
Figure 1: Graphical model representation of the
BHMM-T, which includes sentence type as an ob-
served variable on tag transitions (but not emissions).
punctuation features (not part of our model), and these
features are informative enough to reliably distinguish
sentence types (as speech recognition tasks have found
to be the case, see Section 1).
In the BHMM, there are two obvious ways that sen-
tence type could be incorporated into the generative
model: either by affecting the transition probabilities
or by affecting the emission probabilities. The first case
can be modeled by adding si as a conditioning variable
when choosing ti, replacing line 1 from the BHMM
definition with the following:
ti|si = s, ti?1 = t, ti?2 = t
?,?(t,t
?) ? Mult(?(s,t,t
?)) (7)
We will refer to this model, illustrated graphically in
Figure 1, as the BHMM-T. It assumes that the distribu-
tion over ti depends not only on the previous two tags,
but also on the sentence type, i.e., that different sen-
tence types tend to have different sequences of tags.
In contrast, we can add si as a conditioning variable
for wi by replacing line 2 from the BHMM with
wi|si = s, ti = t,?
(t) ? Mult(?(s,t)) (8)
This model, the BHMM-E, assumes that different sen-
tence types tend to have different words emitted from
the same tag.
The predictive distributions for these models are
given in Equations 9 (BHMM-T) and 10 (BHMM-E):
P(ti|t?i,si,?) =
C(ti?2, ti?1, ti,si)+?
C(ti?2, ti?1,si)+T?
(9)
P(wi|ti,si,?) =
C(ti,wi,si)+?
C(ti,si)+Wti?
(10)
Of course, we can also create a third new model,
the BHMM-B, in which sentence type is used to con-
dition both transition and emission probabilities. This
model is equivalent to training a separate BHMM on
each type of sentence (with shared hyperparameters).
Note that introducing the extra conditioning variable
in these models has the consequence of splitting the
counts for transitions, emissions, or both. The split dis-
tributions will therefore be estimated using less data,
which could actually degrade performance if sentence
type is not a useful variable.
2
Our prediction is that sentence type is more likely
to be useful as a conditioning variable for transition
probabilities (BHMM-T) than for emission probabili-
ties (BHMM-E). For example, the auxiliary inversion
in questions is likely to increase the probability of
the AUX? PRO transition, compared to declaratives.
Knowing that the sentence is a question may also af-
fect emission probabilities, e.g., it might increase the
probability the word you given a PRO and decrease the
probability of I; one would certainly expect wh-words
to have much higher probability in wh-questions than
in declaratives. However, many other variables also af-
fect the particular words used in a sentence (princi-
pally, the current semantic and pragmatic context). We
expect that sentence type plays a relatively small role
compared to these other factors. The ordering of tags
within an utterance, on the other hand, is principally
constrained by sentences type (especially in the short
and grammatically simple utterances found in child-
directed speech).
3 Sentence Types
We experiment with a number of sentence-type cate-
gories, leading to increasingly fine grained distinctions.
The primary distinction is between questions (Q) and
declaratives (D). Questions are marked by punctuation
(in writing) or by intonation (in speech), as well as by
word order or other morpho-syntactic markers in many
languages.
Questions may be separated into categories, most
notably wh-questions and yes/no-questions. Many lan-
guages (including several English dialects) have dis-
tinct intonation patterns for wh- and yes/no-questions
(Hirst and Cristo, 1998).
Imperatives are a separate type from declaratives,
with distinct word order and intonation patterns.
Declaratives may be further subdivided into frag-
ments and full sentences. We define fragments as ut-
terances without a verb (including auxiliary verbs).
As an alternate sentence-level feature to sentence
type, we use length. Utterances are classified accord-
ing to their length, as either shorter or longer than av-
erage. Shorter utterances are more likely to be frag-
ments and may have distinct syntactic patterns. How-
ever these patterns are likely to be less strong than in
the above type-based types. In effect this condition is
a pseudo-baseline, testing the effects of less- or non-
informative sentence features on our proposed models.
4 Evaluation Measures
Evaluation of fully unsupervised part of speech tagging
is known to be problematic, due to the fact that the
part of speech clusters found by the model are unla-
beled, and do not automatically correspond to any of
the gold standard part of speech categories. We report
three evaluation measures in our experiments, in order
to avoid the weaknesses inherent in any single measure
and in an effort to be comparable to previous work.
Matched accuracy (MA), also called many-to-one
accuracy, is a commonly used measure for evaluating
unlabeled clusterings in part of speech tagging. Each
unlabeled cluster is given the label of the gold category
with which it shares the most members. Given these la-
bels, accuracy can be measured as usual, as the percent-
age of tokens correctly labeled. Note that multiple clus-
ters may have the same label if several clusters match
the same gold standard category. This can lead to a de-
generate solution if the model is allowed an unbounded
number of categories, in which each word is in a sepa-
rate cluster. In less extreme cases, it makes comparing
MA across clustering results with different numbers of
clusters difficult. Another serious issue with MA is the
?problem of matching? (Meila, 2007): matched accu-
racy only evaluates whether or not the items in the clus-
ter match the majority class label. The non-matching
items within a cluster might all be from a second gold
class, or they might be from many different classes. In-
tuitively, the former clustering should be evaluated as
better, but matched accuracy is the same for both clus-
terings.
Variation of Information (VI) (Meila, 2007) is a
clustering evaluation measure that avoids the match-
ing problem. It measures the amount of information
lost and gained when moving between two clusterings.
More precisely:
V I(C,K) = H(C)+H(K)?2I(C,K)
= H(C|K)+H(K|C)
A lower score implies closer clusterings, since each
clustering has less information not shared with the
other: two identical clusterings have a VI of zero. How-
ever, VI?s upper bound is dependent on the maximum
number of clusters in C or K, making it difficult to com-
pare clustering results with different numbers of clus-
ters.
As a third, and, in our view, most informative
measure, we use V-measure (VM; Rosenberg and
Hirschberg (2007)). Like VI, VM uses the conditional
entropy of clusters and categories to evaluate cluster-
ings. However, it also has the useful characteristic of
being analogous to the precision and recall measures
commonly used in NLP. Homogeneity, the precision
analogue, is defined as
V H = 1?
H(C|K)
H(C)
.
VH is highest when the distribution of categories
within each cluster is highly skewed towards a small
number of categories, such that the conditional entropy
is low. Completeness (recall) is defined symmetrically
to VH as:
VC = 1?
H(K|C)
H(K)
.
VC measures the conditional entropy of the clusters
within each gold standard category, and is highest if
each category maps to a single cluster so that each
3
Eve Manchester
Sentence type Counts |w| Counts |w|
Total 13494 4.39 13216 4.23
D
Total 8994 4.48 8315 3.52
I 623 4.87 757 4.22
F 2996 1.73 4146 1.51
Q
Total 4500 4.22 4901 5.44
wh 2105 4.02 1578 4.64
Short utts 5684 1.89 6486 1.74
Long utts 7810 6.21 6730 6.64
Table 1: Counts of sentence types in the Eve and
Manchester training set. (Test and dev sets are approx-
imately 10% of the size of training.) |w| is the average
length in words of utterances of this type. D: declar-
atives, I: imperatives, F: fragments, Q: questions, wh:
wh-questions.
model cluster completely contains a category. The V-
measure VM is simply the harmonic mean of VH and
VC, analogous to traditional F-score. Unlike MA and
VI, VM is invariant with regards to both the number of
items in the dataset and to the number of clusters used,
and consequently it is best suited for comparing results
across different corpora.
5 English experiments
5.1 Corpora
We use the Eve corpus (Brown, 1973) and the
Manchester corpus (Theakston et al, 2001) from the
CHILDES collection (MacWhinney, 2000). The Eve
corpus is a longitudinal study of a single US Ameri-
can child from the age of 1.5 to 2.25 years, whereas
the Manchester corpus follows a cohort of 12 British
children from the ages of 2 to 3. Using both corpora
ensures that any effect is not due to a particular child,
and is not specific to a type of English.
From both corpora we remove all utterances spoken
by a child; the remaining utterances are nearly exclu-
sively child-directed speech (CDS). We use the full Eve
corpus and a similarly sized subset of the Manchester
corpus, consisting of the first 70 CDS utterances from
each file. Files from the chronological middle of each
corpus are set aside for development and testing (Eve:
file 10 for testing, 11 for dev; Manchester: file 17 from
each child for testing, file 16 for dev).
Both corpora have been tagged using the relatively
rich CHILDES tagset, which we collapse to a smaller
set of thirteen tags: adjectives, adverbs, auxiliaries,
conjunctions, determiners, infinitival-to, nouns, nega-
tion, participles, prepositions, pronouns, verbs and
other (communicators, interjections, fillers and the
like). wh-words are tagged as adverbs (why,where,
when and how, or pronouns (who and the rest).
Table 1 show the sizes of the training sets, and
the breakdown of sentence types within them. Each
sentence type can be identified using a distinguish-
ing characteristic. Sentence-final punctuation is used to
differentiate between questions and declaratives; wh-
questions are then further differentiated by the pres-
ence of a wh-word. Imperatives are separated from the
declaratives by a heuristic (since CHILDES does not
have an imperative verb tag): if an utterance includes
a base verb within the first two words, without a pro-
noun proceeding it (with the exception of you, as in
you sit down right now), the utterance is coded as an
imperative. Fragments are also identified using the tag
annotations, namely by the lack of a verb or auxiliary
tag in an utterance.
The CHILDES annotation guide specifies that the
question mark is to be used with any utterance with ?fi-
nal rising contour?, even if syntactically the utterance
might appear to be a declarative or exclamation. The
question category consequently includes echo ques-
tions (Finger stuck?) and non-inverted questions (You
want me to have it?).
5.2 Inference and Evaluation Procedure
Unsupervised models do not suffer from overfitting,
so generally it is thought unnecessary to use separate
training and testing data, with results being reported
on the entire set of input data. However, there is still
a danger, in the course of developing a model, of over-
fitting in the sense of becoming too finely attuned to a
particular set of input data. To avoid this, we use sep-
arate test and development sets. The BHMM is trained
on (train+dev) or (train+test), but evaluation scores are
computed based on the dev or test portions of the data
only. 2
We run the Gibbs sampler for 2000 iterations, with
hyperparameter resampling and simulated annealing.
Each iteration produces an assignment of tags to the
tokens in the corpus; the final iteration is used for eval-
uation purposes. Since Gibbs sampling is a stochas-
tic algorithm, we run all models multiple times (three,
except where stated otherwise) and report average val-
ues for all evaluation measures, as well as confidence
intervals. We run our experiments using a variety of
sentence type features, ranging from the coarse ques-
tion/declarative (Q/D) distinction to the full five types.
For reasons of space we do not report all results here,
instead confining ourselves to representative samples.
5.3 BHMM-B: Type-specific Sub-Models
When separate sub-models are used for each sen-
tence type, as in the BHMM-B, where both transition
and emission probabilities are conditioned on sentence
type, the hidden states (tags) in each sub-model do
not correspond to each other, e.g., a hidden state 9 in
one sub-model is not the same state 9 in another sub-
model. Consequently, when evaluating the tagged out-
put, each sentence type must be evaluated separately
(otherwise the evaluation would equate declaratives-
tag-9 with questions-tag-9).
2The results presented in this paper are all evaluated on
the dev set; preliminary test set results on the Eve corpus
show the same patterns.
4
Model VM VC VH VI MA
wh-questions
BHMM: 63.0 (1.0) 59.8 (0.4) 66.6 (1.8) 1.63 (0.03) 70.7 (2.7)
BHMM-B: 58.7 (2.0) 58.2 (2.1) 59.2 (2.0) 1.74 (0.09) 59.7 (2.0)
Other Questions
BHMM: 65.6 (1.4) 62.7 (1.3) 68.7 (1.5) 1.62 (0.06) 74.5 (0.5)
BHMM-B: 64.4 (3.6) 62.6 (4.4) 66.2 (2.8) 1.65 (0.19) 70.8 (2.5)
Declaratives
BHMM: 60.9 (1.3) 58.7 (1.1) 63.3 (1.6) 1.84 (0.06) 73.5 (0.8)
BHMM-B: 58.0 (1.2) 55.5 (1.1) 60.7 (1.5) 1.99 (0.06) 69.0 (1.5)
Table 2: Results for BHMM-B on W/Q/D sentence types (dev set evaluation) in the Manchester corpus, compared
to the standard BHMM. The confidence interval is indicated in parentheses. Note that lower VI is better.
Model VM VC VH VI MA
BHMM: 59.4 (0.2) 56.9 (0.2) 62.3 (0.2) 1.96 (0.01) 72.2 (0.2)
Q/D: 61.2 (1.2) 58.6 (1.2) 64.0 (1.4) 1.88 (0.06) 72.1 (1.5)
W/Q/D: 61.0 (1.7) 59.0 (1.5) 63.0 (2.0) 1.86 (0.08) 69.6 (2.2)
F/I/D/Q/W: 61.7 (1.7) 58.9 (1.8) 64.8 (1.6) 1.80 (0.09) 70.5 (1.3)
Table 3: Results for BHMM-E on the Eve corpus (dev set evaluation), compared to the standard BHMM. The
confidence interval is indicated in parentheses.
Table 2 shows representative results for the W/Q/D
condition on the Manchester corpus, separated into wh-
questions, other questions, and declaratives. For each
sentence type, the BHMM-B performs significantly
worse than the BHMM. The wh-questions sub-model,
which is trained on the smallest subset of the input cor-
pus, performs the worst across all measures except VI.
This suggests that lack of data is why these sub-models
perform worse than the standard model.
5.4 BHMM-E: Type-specific Emissions
Having demonstrated that using entirely separate sub-
models does not improve tagging performance, we turn
to the BHMM-E, in which emission probability distri-
butions are sentence-type specific, but transition prob-
abilities are shared between all sentence types.
The results in Table 3 show that BHMM-E does re-
sult in slightly better tagging performance as evaluated
by VI (lower VI is better) and VM and its components.
Matched accuracy does not capture this same trend. In-
specting the clusters found by the model, we find that
clusters for the most part do match gold categories. The
tokens that do not fall into the highest matching gold
categories are not distributed randomly, however; for
instance, nouns and pronouns often end up in the same
cluster. VI and VM capture these secondary matches
while MA does not. Some small gold categories (e.g.
the single word infinitival-to and negation-not cate-
gories) are often merged by the model into a single
cluster, with the result that MA considers nearly half
the cluster as misclassified.
The largest increase in performance with regards to
the standard BHMM is obtained by adding the distinc-
tion between declaratives and questions. Thereafter,
adding the wh-question, fragment and imperative sen-
tence types does not worsen performance, but also does
not significantly improve performance on any measure.
5.5 BHMM-T: Type-specific Transitions
Lastly, the BHMM-T shares emission probabilities
among sentence types and uses sentence type specific
transition probabilities.
Results comparing the standard BHMM with the
BHMM-T with sentence-type-specific transition prob-
abilities are presented in Table 4. Once again, VM
and VI show a clear trend: the models using sen-
tence type information outperform both the standard
BHMM and models splitting according to utterance
length (shorter/longer than average). MA shows no sig-
nificant difference in performance between the differ-
ent models (aside from clearly showing that utterance
length is an unhelpful feature). The fact that the MA
measure shows no clear change in performance is likely
a fault of the measure itself; as explained above, VI and
VM take into account the distribution of words within
a category, which MA does not.
As with the BHMM-E, the improvements to VM and
VI are obtained by distinguishing between questions
and declaratives, and then between wh- and other ques-
tions. Both of these distinctions are marked by intona-
tion in English. In contrast, distinguishing fragments
and imperatives, which are less easily detected by in-
tonation, provides no obvious benefit in any case. Us-
ing sentence length as a feature degrades performance
considerably, confirming that improvements in perfor-
mance are due to sentence types capturing useful infor-
mation about the tagging task, and not simply due to
splitting the input in some arbitrary way.
One reason for the improvement when adding the
wh-question type is that the models are learning to
identify and cluster the wh-words in particular. If we
evaluate the wh-words separately, VM rises from 32.3
5
Model VM VC VH VI MA
Eve
BHMM: 59.4 (0.2) 56.9 (0.2) 62.3 (0.2) 1.96 (0.01) 72.2 (0.2)
Q/D: 60.9 (0.5) 58.3 (0.4) 63.7 (0.6) 1.89 (0.02) 72.7 (0.3)
W/Q/D: 62.5 (1.2) 60.0 (1.3) 65.2 (1.0) 1.81 (0.06) 72.9 (0.8)
F/I/D/Q/W: 62.2 (1.5) 59.5 (1.6) 65.2 (1.3) 1.77 (0.08) 71.5 (1.4)
Length: 57.9 (1.2) 55.3 (1.1) 60.7 (1.3) 2.04 (0.06) 69.7 (2.0)
Manchester
BHMM: 60.2 (0.9) 57.6 (0.9) 63.1 (1.0) 1.92 (0.05) 72.1 (0.7)
Q/D: 61.5 (0.7) 59.2 (0.6) 63.9 (0.9) 1.84 (0.03) 71.6 (1.5)
W/Q/D: 62.7 (0.2) 60.6 (0.2) 65.0 (0.3) 1.78 (0.01) 71.2 (0.6)
F/I/D/Q/W: 62.5 (0.4) 60.3 (0.5) 64.9 (0.4) 1.79 (0.02) 71.3 (0.9)
Length: 58.1 (0.7) 55.6 (0.8) 60.8 (0.6) 2.02 (0.04) 71.0 (0.6)
Table 4: Results on the Eve and Manchester corpora for the various sentence types in the BHMM and BHMM-T
models. The confidence interval is indicated in parentheses.
in the baseline BHMM to 41.5 in the W/Q/D condition
with the BHMM-T model and 46.8 with the BHMM-
E model. Performance for the non-wh-words also im-
proves in the W/Q/D condition, albeit less dramati-
cally: from 61.1 in the baseline BHMM to 63.6 with
BHMM-T and 62.0 with BHMM-E. The wh-question
type enables the models to pick up on the defining char-
acteristics of these sentences, namely wh-words.
We predicted the sentence-type specific transition
probabilities in the BHMM-T to be more useful than
the sentence-type specific emission probabilities in the
BHMM-E. The BHMM-T does perform slightly better
than the BHMM-E, however, the effect is small. Word
or tag order may be the most overt difference between
questions and declaratives in English, but word choice,
especially the use of wh-words varies sufficiently be-
tween sentence types for sentence-type specific emis-
sion probabilities to be equally useful.
6 Crosslinguistic Experiments
In the previous section we found that sentence type
information improved syntactic categorisation in En-
glish. In this section, we evaluate the BHMM?s perfor-
mance on a range of languages other than English, and
investigate whether sentence type information is use-
ful across languages. To our knowledge this is the first
application of the BHMM to non-English data.
Nearly all human languages distinguish between
yes/no-questions and declaratives in intonation; ques-
tions are most commonly marked by rising intonation
(Hirst and Cristo, 1998). wh-questions do not always
have a distinct intonation type, but they are signalled
by the presence of members of the small class of wh-
words.
The CHILDES collection includes tagged corpora
for Spanish and Cantonese: the Ornat corpus (Ornat,
1994) and the Lee Wong Leung (LWL) corpus (Lee
et al, 1994) respectively. To cover a greater variety of
word order patterns, a Dutch corpus of adult dialogue
(not CDS) is also tested. We describe each corpus in
turn below; Table 5 describes their relative sizes.
Total Ds all Qs wh-Qs
Spanish 8759 4825 3934 1507
|w| 4.29 4.41 4.14 3.72
Cantonese 12544 6689 5855 2287
|w| 4.16 3.85 4.52 4.80
Dutch 8967 7812 1155 363
|w| 6.16 6.19 6.00 7.08
Table 5: Counts of sentence types in the training sets
for Spanish. Cantonese and Dutch. (Test and dev sets
are approximately 10% of the size of training.) |w| is
the average length in words of utterances of this type.
D: declaratives, Qs: questions, wh-Qs: wh-questions.
6.1 Spanish
The Ornat corpus is a longitudinal study of a single
child between the ages of one and a half and nearly
four years, consisting of 17 files. Files 08/09 are used
testing/development.
We collapse the Spanish tagset used in the Ornat cor-
pus in a similar fashion to the English corpora. There
are 11 tags in the final set: adjectives, adverbs, con-
juncts, determiners, nouns, prepositions, pronouns, rel-
ative pronouns, auxiliaries, verbs, and other.
Spanish wh-questions are formed by fronting the
wh-word (but without the auxiliary verbs added in
English); yes/no-questions involve raising the main
verb (again without the auxiliary inversion in English).
Spanish word order in declaratives is generally freer
than English word order. Verb- and object-fronting is
more common, and pronouns may be dropped (since
verbs are marked for gender and number).
6.2 Cantonese
The LWL corpus consists of transcripts from a set of
children followed over the course of a year, totalling
128 files. The ages of the children are not matched, but
they range between one and three years old. Our train-
ing set consists of the first 500 utterances of all train-
ing files, in order to create a data set of similar size as
the other corpora used. Files from children aged two
6
years and five months are used as the test set; files from
two years and six months are the development set files
(again, the first 500 utterances from each of these make
up the test/dev corpus).
The tagset used in the LWL is larger than the En-
glish corpus. It consists of 20 tags: adjective, ad-
verb, aspectual marker, auxiliary or modal verb, clas-
sifier, communicator, connective, determiners, genitive
marker, preposition or locative, noun, negation, pro-
nouns, quantifiers, sentence final particle, verbs, wh-
words, foreign word, and other. We remove all sen-
tences that are encoded as being entirely in English but
leave single foreign, mainly English, words (generally
nouns) in a Cantonese context.
Cantonese follows the same basic SVO word order
as English, but with a much higher frequency of topic-
raising. Questions are not marked by different word or-
der. Instead, particles are inserted to signal questioning.
These particles can signal either a yes/no-question or a
wh-question; in the case of wh-questions they replace
the item being questioned (e.g., playing-you what?),
without wh-raising as in English or Spanish. Despite
the use of tones in Cantonese, questions are marked
with rising final intonation.
6.3 Dutch
The Corpus of Spoken Dutch (CGN) contains Dutch
spoken in a variety of settings. We use the ?spontaneous
conversation? component, consisting of 925 files, since
it is the most similar to CDS. However, the utterances
are longer, and there are far fewer questions, especially
wh-questions (see Table 5).
The corpus does not have any meaningful timeline,
so we designated all files with numbers ending in 0 as
test files and files ending in 9 as dev files. The first
60 utterances from each file were used, to create train-
ing/test/dev sets similar in size to the other corpora.
The coarse CGN tagset consists of 11 tags, which
we used directly: adjective, adverb, conjunction, deter-
miner, interjection, noun, number, pronoun/determiner,
preposition, and verb.
Dutch follows verb-second word order in main
clauses and SOV word order in embedded clauses.
Yes/no-questions are created by verb-fronting, as in
Spanish. wh-questions involve a wh-word at the begin-
ning of the utterance followed by the verb in second
position.
6.4 Results
We trained standard BHMM, BHMM-T and BHMM-E
models in the same manner as with the English corpora.
Given the poor performance of the BHMM-B, we did
not test it here.
Due to inconsistent annotation and lack of familiar-
ity with the languages we tested only two sentence type
distinctions, Q/D and W/Q/D. Punctuation was used
to distinguish between questions and declaratives. wh-
questions were identified by using a list of wh-words
for Spanish and Dutch; for Cantonese we relied on the
wh-word tag annotation.
Results are shown in Table 6. Since the corpora
are different sizes and use tagsets of varying sizes, VI
and MA results are not comparable between corpora.
VM (and VC and VH) are more robust, but even so
cross-corpora comparisons should be made carefully.
The English corpora VM scores are significantly higher
(around 10 points higher) than the non-English corpora
scores.
In Cantonese and Dutch, the W/Q/D BHMM-T
model performs best; in both cases significantly bet-
ter than the BHMM. In Cantonese, the separation of
wh-questions improves tagging significantly in both the
BHMM-T and BHMM-E models, whereas simply sep-
arating questions and declaratives helps far less. In
the Dutch corpus, wh-questions improved only in the
BHMM-T, not in the BHMM-E.
The Spanish models have higher variance, due to the
small size of the corpus. Due to the high variance, there
are no significant differences between any of the con-
ditions; it is also difficult to spot a trend.
7 Future Work
We have shown sentence type information to be use-
ful for syntactic tagging. However, the BHMM-E and
BHMM-T models are successful in part however be-
cause they also share information as well as split it;
the completely split BHMM-B does not perform well.
Many aspects of tagging do not change significantly
between sentence types. Within a noun phrase, the or-
dering of determiners and nouns is the same whether
it is in a question or an imperative, and to a large ex-
tent the determiners and nouns used will be the same
as well. Learning these patterns over as much input as
possible is essential. Therefore, the next step in this line
of work will be to add a general (corpus-level) model
alongside type-specific models. Ideally, the model will
learn when to use the type-specific model (when tag-
ging the beginning of questions, for instance) and when
to use the general model (when tagging NPs). Such a
model would make use of sentence-type information in
a better way, hopefully leading to further improvements
in performance. A further, more sophisticated model
could learn the useful sentence types distinctions auto-
matically, perhaps forgoing the poorly performing im-
perative or fragment types we tested here in favor of a
more useful type we did not identify.
8 Conclusions
We set out to investigate a hitherto unused source of
information for models of syntactic category learning,
namely intonation and its correlate, sentence type. We
showed that this information is in fact useful, and in-
cluding it in a Bayesian Hidden Markov Model im-
proved unsupervised tagging performance.
We found tagging performance increases if sentence
type information is used to generate either transition
probabilities or emission probabilities in the BHMM.
However, we found that performance decreases if sen-
tence type information is used to generate both transi-
7
Model VM VC VH VI MA
Spanish
BHMM: 49.4 (1.8) 47.2 (1.9) 51.8 (1.8) 2.27 (0.09) 61.5 (2.1)
BHMM-E Q/D: 49.4 (1.5) 47.0 (1.4) 52.1 (1.7) 2.28 (0.06) 60.9 (2.6)
BHMM-E W/Q/D: 48.7 (2.5) 46.4 (2.4) 51.2 (2.6) 2.31 (0.11) 60.2 (3.0)
BHMM-T Q/D: 49.0 (1.7) 46.7 (1.6) 51.6 (1.7) 2.30 (0.07) 60.9 (2.5)
BHMM-T W/Q/D: 49.5 (2.5) 47.2 (2.3) 52.1 (2.8) 2.27 (0.11) 61.0 (3.0)
Cantonese
BHMM: 49.4 (0.8) 44.5 (0.7) 55.4 (1.0) 2.60 (0.04) 67.2 (1.0)
BHMM-E Q/D: 50.7 (1.6) 45.4 (1.5) 57.5 (1.7) 2.55 (0.09) 69.0 (1.0)
BHMM-E W/Q/D: 52.3 (0.3) 46.9 (0.3) 59.3 (0.4) 2.46 (0.02) 69.4 (0.9)
BHMM-T Q/D: 50.3 (0.9) 45.0 (0.9) 57.0 (1.0) 2.57 (0.05) 68.4 (0.8)
BHMM-T W/Q/D: 52.2 (0.8) 46.8 (0.9) 59.1 (0.7) 2.47 (0.05) 69.9 (1.9)
Dutch
BHMM: 48.4 (0.7) 47.1 (0.8) 49.7 (0.7) 2.38 (0.04) 62.3 (0.3)
BHMM-E Q/D: 48.4 (0.4) 47.3 (0.4) 49.7 (0.5) 2.37 (0.02) 62.2 (0.3)
BHMM-E W/Q/D 47.6 (0.3) 46.3 (0.4) 48.8 (0.2) 2.41 (0.02) 61.2 (1.1)
BHMM-T Q/D: 47.9 (0.5) 46.7 (0.4) 49.1 (0.5) 2.40 (0.02) 61.5 (0.4)
BHMM-T W/Q/D: 49.6 (0.2) 48.5 (0.2) 50.8 (0.2) 2.31 (0.10) 64.1 (0.2)
Table 6: Results for BHMM, BHMM-E, and BHMM-T on non-English corpora.
tion and emission probabilities (which is equivalent to
training a separate BHMM for each sentence type).
To test the generality of our findings, we carried out a
series of cross-linguistic experiments, integrating sen-
tence type information in unsupervised tagging mod-
els for Spanish, Cantonese, and Dutch. The results for
Cantonese and Dutch mirrored those for English, show-
ing a small increase in tagging performance for models
that included sentence type information. For Spanish,
no improvement was observed.
References
Roger Brown. 1973. A first language: The early
stages. Harvard University Press.
Anne Fernald. 1989. Intonation and communicative
intent in mothers? speech to infants: Is the melody
the message? Child Development, 60(6):1497?
1510.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics.
Daniel Hirst and Albert Di Cristo, editors. 1998. Into-
nation systems: a survey of twenty languages. Cam-
bridge University Press.
Thomas H.T. Lee, Colleen H Wong, Samuel Leung,
Patricia Man, Alice Cheung, Kitty Szeto, and Cathy
S P Wong. 1994. The development of grammatical
competence in cantonese-speaking children. Techni-
cal report, Report of RGC earmarked grant 1991-94.
Brian MacWhinney. 2000. The CHILDES Project:
Tools for Analyzing Talk. Lawrence Erlbaum Asso-
ciates, Mahwah, NJ.
Marina Meila. 2007. Comparing clusterings ? an in-
formation based distance. Journal of Multivariate
Analysis, 98:873?895.
Susana Lopez Ornat. 1994. La adquisicion de la
lengua espagnola. Siglo XXI, Madrid.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In EMNLP.
David Snow and Heather Balog. 2002. Do chil-
dren produce the melody before the words? A re-
view of developmental intonation research. Lingua,
112:1025?1058.
Daniel N. Stern, Susan Spieker, and Kristine MacK-
ain. 1982. Intonation contours as signals in mater-
nal speech to prelinguistic infants. Developmental
Psychology, 18(5):727?735.
Paul A. Taylor, S. King, S. D. Isard, and H. Wright.
1998. Intonation and dialogue context as constraints
for speech recognition. Language and Speech,
41(3):493?512.
Anna Theakston, Elena Lieven, Julian M. Pine, and
Caroline F. Rowland. 2001. The role of per-
formance limitations in the acquisition of verb-
argument structure: an alternative account. Journal
of Child Language, 28:127?152.
8
Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 20?29,
Portland, Oregon, June 2011. c?2011 Association for Computational Linguistics
Unsupervised syntactic chunking with acoustic cues:
computational models for prosodic bootstrapping
John K Pate (j.k.pate@sms.ed.ac.uk)
Sharon Goldwater (sgwater@inf.ed.ac.uk)
School of Informatics, University of Edinburgh
10 Crichton St., Edinburgh EH8 9AB, UK
Abstract
Learning to group words into phrases with-
out supervision is a hard task for NLP sys-
tems, but infants routinely accomplish it. We
hypothesize that infants use acoustic cues to
prosody, which NLP systems typically ignore.
To evaluate the utility of prosodic information
for phrase discovery, we present an HMM-
based unsupervised chunker that learns from
only transcribed words and raw acoustic cor-
relates to prosody. Unlike previous work on
unsupervised parsing and chunking, we use
neither gold standard part-of-speech tags nor
punctuation in the input. Evaluated on the
Switchboard corpus, our model outperforms
several baselines that exploit either lexical or
prosodic information alone, and, despite pro-
ducing a flat structure, performs competitively
with a state-of-the-art unsupervised lexical-
ized parser, with a substantial advantage in
precision. Our results support the hypothesis
that acoustic-prosodic cues provide useful ev-
idence about syntactic phrases for language-
learning infants.
1 Introduction
Young children routinely learn to group words into
phrases, yet computational methods have so far
struggled to accomplish this task without supervi-
sion. Previous work on unsupervised grammar in-
duction has made progress by exploiting information
such as gold-standard part of speech tags (e.g. Klein
and Manning (2004)) or punctuation (e.g. Seginer
(2007)). While this information may be available
in some NLP contexts, our focus here is on the com-
putational problem facing language-learning infants,
who do not have access to either part of speech
tags or punctuation. However, infants do have ac-
cess to certain cues that have not been well explored
by NLP researchers focused on grammar induction
from text. In particular, we consider the cues to syn-
tactic structure that might be available from prosody
(roughly, the structure of speech conveyed through
rhythm and intonation) and its acoustic realization.
The idea that prosody provides important ini-
tial cues for grammar acquisition is known as the
prosodic bootstrapping hypothesis, and is well-
established in the field of language acquisition
(Gleitman and Wanner, 1982). Experimental work
has provided strong support for this hypothesis, for
example by showing that infants begin learning ba-
sic rhythmic properties of their language prenatally
(Mehler et al, 1988) and that 9-month-olds use
prosodic cues to distinguish verb phrases from non-
constituents (Soderstrom et al, 2003). However, as
far as we know, there has so far been no direct com-
putational evaluation of the prosodic bootstrapping
hypothesis. In this paper, we provide the first such
evaluation by exploring the utility of acoustic cues
for unsupervised syntactic chunking, i.e., grouping
words into non-hierarchical syntactic phrases.
Nearly all previous work on unsupervised gram-
mar induction has focused on learning hierarchical
phrase structure (Lari and Young, 1990; Liang et al,
2007) or dependency structure (Klein and Manning,
2004); we are aware of only one previous paper
on unsupervised syntactic chunking (Ponvert et al,
2010). Ponvert et al describe a simple method for
chunking that uses only bigram counts and punctu-
ation; when the chunks are combined using a right-
branching structure, the resulting trees achieve un-
labeled bracketing precision and recall that is com-
petitive with other unsupervised parsers. The sys-
20
tem?s dependence on punctuation renders it inappro-
priate for addressing the questions we are interested
in here, but its good performance reccommends syn-
tactic chunking as a profitable approach to the prob-
lem of grammar induction, especially since chunks
can be learned using much simpler models than are
needed for hierarchical structure.
The models used in this paper are all variants of
HMMs. Our baseline models are standard HMMs
that learn from either lexical or prosodic observa-
tions only; we also consider three types of models
(including a coupled HMM) that incorporate both
lexical and prosodic observations, but vary the de-
gree to which syntactic and prosodic variables are
tied together in the latent structure of the models.
In addition, we compare the use of hand-annotated
prosodic information (ToBI annotations) to the use
of direct acoustic measures (specifically, duration
measures) as the prosodic observations. All of our
models are unsupervised, receiving no bracketing
information during training.
The results of our experiments strongly support
the prosodic bootstrapping hypothesis: we find
that using either ToBI annotations or acoustic mea-
sures in addition to lexical observations (i.e., word
sequences) vastly improves chunking performance
over any source of information alone. Interestingly,
our best results are achieved using a combination
of words and acoustic information as input, rather
than words and ToBI annotations. Our best com-
bined model achieves an F-score of 41% when eval-
uated on the lowest level of syntactic structure in
the Switchboard corpus1, as compared to 25% for
a words-only model and only 3% for an acoustics-
only model. Although the combined model?s score
is still fairly low, additional results suggest that our
corpus of transcribed naturalistic speech is signifi-
cantly more difficult for unsupervised parsing than
the written text that is typically used for training.
Specifically, we find that a state-of the-art unsuper-
vised lexicalized parser, the Common Cover Link
1Since our interest is in child language acquisition, we
would prefer to evaluate our system on data from the CHILDES
database of child-directed speech (MacWhinney, 2000). Unfor-
tunately, there are no corpora in the database that include phrase
structure annotations. We are in the process of annotating a
small evaluation corpus with phrase structure trees, and hope to
use this for evaluation in future work.
(CCL) parser (Seginer, 2007), achieves only 38%
unlabeled bracketing F-score on our corpus, as com-
pared to published results of 76% on WSJ10 (En-
glish) and 59% on Negra10 (German). Interestingly,
we find that when evaluated against full parse trees,
our best chunker achieves an F-score comparable to
that of CCL despite positing only flat structure.
Before describing our models and experiments in
more detail, we first present a brief review of rel-
evant information about prosody and its relation-
ship to syntax, including previous work combining
prosody and syntax in supervised parsing systems.
2 Prosody and syntax
Prosody is a theoretical linguistic concept posit-
ing an abstract organizational structure for speech.2
While it is often closely associated with such mea-
surable phenomena as movement in fundamen-
tal frequency or variation in spectral tilt, these
are merely observable acoustic correlates that pro-
vide evidence of varying quality about the hidden
prosodic structure, which specifies such hidden vari-
ables as contrastive stress or question intonation.
Prosody has been hypothesized to be useful for
learning syntax because it imposes a grouping struc-
ture on word sequences that sometimes coincides
with traditional constituency analyses (Ladd, 1996;
Shattuck-Hufnagel and Turk, 1996). Moreover,
laboratory experiments have shown that adults use
prosody both for syntactic disambiguation (Millotte
et al, 2007; Price et al, 1991) and, crucially, in
learning the syntax of an artificial language (Morgan
et al, 1987). Accordingly, if prosodic structure is
sufficiently prominent in the acoustic signal, and co-
incides often enough with syntactic structure, then it
may provide children with useful information about
how to combine words into phrases.
Although there are several theories of how to rep-
resent and annotate prosodic structure, one of the
most influential is the ToBI (Tones and Break In-
dices) theory (Beckman et al, 2005), which we
will use in some of our experiments. ToBI pro-
poses, among other things, that the prosodic phras-
ing of languages can be represented in terms of se-
quences of break indices indicating the strength of
2Signed languages also exhibit prosodic phenomena, but
they are not addressed here.
21
word boundaries. In Mainstream American English
ToBI, for example, the boundary between a clitic
and its base word (e.g. ?do? and ?n?t? of ?don?t?)
is 0, representing a very weak boundary, while the
boundary following a word at the end of an intona-
tional phrase is 4, indicating a very strong boundary.
Below we examine how useful these break indices
are for identifying syntactic boundaries.
Finally, we note that our work is not the first com-
putational approach to using prosody for identifying
syntactic structure. However, previous work (Gre-
gory et al, 2004; Kahn et al, 2005; Dreyer and
Shafran, 2007; No?th et al, 2000) has focused on
supervised parsing rather than unsupervised chunk-
ing, and also makes different assumptions about
prosody. For example, Gregory et al (2004) assume
that prosody is an acoustically-realized substitute for
punctuation; our own treatment is much less con-
strained. Kahn et al (2005) and Dreyer and Shafran
(2007) use ToBI labels to represent prosodic infor-
mation, whereas we explore both ToBI and direct
acoustic measures. Finally, No?th et al (2000) do not
use ToBI, instead developing a novel prosodic anno-
tation system designed specifically to provide cues
to syntax and for annotation efficiency. However,
their system is supervised and focuses on improving
parse speed rather than accuracy.
3 Models
Following previous work (e.g. Molina and Pla
(2002) Sha and Pereira (2003)), we formulate
chunking as a tagging task. We use Hidden Markov
Models (HMMs) and their variants to perform the
tagging, with carefully specified tags and con-
strained transition distributions to allow us to inter-
pret the results as a bracketing of the input. Specif-
ically, we use four chunk tags: B (?Begin?) and
E (?End?) tags are interpreted as the first and last
words of a chunk, respectively, with I (?Inside?)
corresponding to other words inside a chunk and O
(?Outside?) to all other words. The transition ma-
trices are constrained to afford 0 probability to tran-
sitions that violate these definitions. Additionally,
the initial probabilities are constrained to forbid the
models from starting inside or at the end of a phrase.
We use this four-tag OBIE tagset rather than the
more typical three-tag IOB tagset for two reasons.
First, the OBIE set forces all chunks to be at least
two words long (the shortest chunk allowed is B E).
Imposing this requirement allows us to characterize
the task in concrete terms as ?learning when to group
words together.? Second, as we seek to incorporate
acoustic correlates of prosody into chunking, we ex-
pect edge behavior to merit explicit modeling.3
In the following subsections, we describe the var-
ious models we use. Note that input to all mod-
els is discrete, consisting of words, ToBI annota-
tions, and/or discretized acoustic measures (we de-
scribe these measures and their discretization in Sec-
tion 3.3). See Figure 1 for examples of system input
and output; different models will receive different
combinations of the three kinds of input.
3.1 Baseline Models
Our baseline models are all standard HMMs, with
the graphical structure shown in Figure 2(a). The
first baseline uses lexical information only; the ob-
servation at each time step is the phonetic transcrip-
tion of the current word in the sentence. To han-
dle unseen words at test time, we use an ?UNK.?
token to replace all words in the training and eval-
uation sets that appear less than twice in the train-
ing data. Our second baseline uses prosodic infor-
mation only; the observation at each time step is
the hand-annotated ToBI Break Index for the cur-
rent word, which takes on one of six values: { 0, 1,
2, 3, 4, X, None }.4 Our final baseline uses acous-
tic information only. The observations are one of
six automatically determined clusters in an acoustic
space, as described in Section 3.3.
We trained the HMMs using Baum-Welch, and
used Viterbi for inference.5
3Indeed, when we tried using the IOB tag set in prelimi-
nary experiments, dev-set performance dropped substantially,
supporting this latter intuition.
4The numerical break indices indicate breaks of increas-
ing strength, ?X? represents a break of uncertain strength, and
?None? indicates that the preceding word is outside one of the
fluent prosodic phrases selected for annotation. Additional dis-
tinctions marked by ?-? and ?p? were ignored.
5We actually used the junction tree algorithm from MAL-
LET, which, in the special case of an HMM, reduces to the
Forward-Backward algorithm when using Sum-Product mes-
sages, and to the Viterbi algorithm when using Max-Product
messages. Our extension of MALLET to build junction trees
efficiently for Dynamic Bayes Nets is available online, and is
being prepared for submission to the main MALLET project.
22
(a) Words g.aa dh.ae.t.s dh.ae.t s.aw.n.d.z p.r.ih.t.iy b.ae.d t.ax m.iy
Acoustics 4 4 6 4 5 4 5 6
ToBI 1 2 1 1 1 1 1 3
(b) O O B I I E B E
(c) ( ) ( )
(d) ( ( ) ( ) )
Figure 1: (a) Example input sequences for the three types of input (phonetic word transcriptions, acoustic clusters, and
ToBI break indices). (b) Example output tags. (c) The bracketing corresponding to (b). (d) The flat tree built from (b).
w1
C1
w2
C2
w3
C3
  
// //
(a) Standard HMM (HMM)
w1
C1
d1
w2
C2
d2
w3
C3
d3

OO
 
OO
// //
OO
(b) Two Output HMM (THMM)
w1
C1
P1
d1
w2
C2
P2
d2
w3
C3
P3
d3

OO

OO

OO
%%
99
//
//
%%
99
//
//
(c) Coupled HMM (CHMM)
Figure 2: Graphical structures for our various HMMs. ci nodes are constrained using the OBIE system, pi nodes
are not. wi nodes represent lexical outputs, and di nodes represent acoustic or ToBI outputs. (Rectangular nodes are
observed, circular nodes are hidden).
3.2 Combined Models
As discussed in Section 2, previous theoretical
and experimental work suggests a combined model
which models uncertainty both between prosody and
acoustics, and between prosody and syntax. To mea-
sure the importance of modeling these kinds of un-
certainty, we will evaluate a series of model struc-
tures that gradually divorce acoustic-prosodic cues
from lexical-syntactic cues.
Our first model is the standard HMM from Fig-
ure 2(a), but generates a (word, acoustics) or (word,
ToBI) pair at each time step. This model has the sim-
plest structure, but includes a separate parameter for
every unique (state, word, acoustics) triple, so may
be too unconstrained to learn anything useful.
To reduce the number of parameters, we pro-
pose a second model that assumes independence be-
tween the acoustic and lexical observations, given
the syntactic state. We call this a ?Two-output HMM
(THMM)? and present its graphical structure in Fig-
ure 2(b). It is straightforward to extend Baum-Welch
to accommodate the extra outputs of the THMM.
Finally, we consider a model that explicitly rep-
resents prosodic structure distinctly from syntactic
structure with a second sequence of tags. We use
a Coupled HMM (CHMM) (Nefian et al, 2002),
which models a set of observation sequences us-
ing a set of hidden variable sequences. Figure 2(c)
presents a two-stream Coupled HMM for three time
steps. The model consists of an initial state proba-
bility distribution pis for each stream s, a transition
matrix as for each stream s conditioning the distri-
bution of stream s at time t + 1 on the state of both
streams at time t, and an emission matrix bs for each
stream conditioning the observation of stream s at
time t on the hidden state of stream s at time t.6
Intuitively, the states emitting acoustic measures
operationalize prosodic structure, and the states
emitting words operationalize syntactic structure.
Crucially, Coupled HMMs impose no a priori cor-
respondence between variables of different streams,
allowing our ?syntactic? states to vary freely from
our ?prosodic? states. As two-stream CHMMs
maintain two emission matrices, two transition ma-
6We explored a number of minor variations on this graphical
structure, but preliminary experiments yielded no improvement.
23
trices, and two initial state distributions, they are
more complex than the other combined models, but
more closely embody intuitions inspired by previous
work on the prosody-syntax interface.
Our Coupled HMMs were also trained using EM.
Marginals for the E-step were computed using the
implementation of the junction tree algorithm avail-
able in MALLET (McCallum, 2002; Sutton, 2006).
During test, the Viterbi tag sequence for each model
is obtained by simply replacing the sum-product
messages with max-product messages.
3.3 Acoustic Cues
As explained in Section 2, prosody is an abstract hid-
den structure which only correlates with observable
features of the acoustic signal, and we seek to select
features which are both easy to measure and likely to
correlate strongly with the hidden prosodic phrasal
structure. While there are many possible cues, we
have chosen to use duration cues. These should pro-
vide good evidence about phrases due to the phe-
nomenon of pre-boundary lengthening (e.g. Beck-
man and Edwards (1990), Wightman et al (1992)),
wherein words, and their final rime, lengthen phrase-
finally. This is likely especially useful for English
due to the lack of confounding segmental duration
contrasts (although variation in duration is unpre-
dictably distributed (Klatt, 1976)), but should be
useful in varying degrees for other languages.
We gather five duration measures:
1. Log total word duration: The annotated word
end time minus the annotated word start time.
2. Log onset duration: The duration from the be-
ginning of the word to the end of the first vowel.
3. Log offset duration: The duration from the be-
ginning of the last vowel to the end of the word.
4. Onset proportion consonant: The duration of
the non-vocalic portion of the word onset di-
vided by the total onset duration.
5. Offset proportion consonant: The duration of
the non-vocalic portion of the word offset di-
vided by the total offset duration.
If a word contains no canonical vowels, then the
first and last sonorants are counted as vocalic. If a
Train Dev Test
Words 68,533 7,981 8,746
Sentences 6,420 778 802
Table 1: Data set statistics
word contains no vowels or sonorants, then the on-
set and offset are the entire word and the propor-
tion consonant for both onset and offset is 1 (this
occurred for 186 words in our corpus).
The potential utility of this acoustic space was
verified by visual inspection of the first few PCA
components, which suggested that the position of a
word in this acoustic space correlated with bracket
count. We discretize the raw (i.e. non-PCA) space
with k-means with six initially random centers for
consistency with the number of ToBI break indices.
4 Experiments
4.1 Dataset
All experiments were performed on part of the Nite
XML Toolkit edition of the Switchboard corpus
(Calhoun et al, 2010). Specifically, we gathered all
conversations which have been annotated for syn-
tax, ToBI, and Mississippi State phonetic alignments
(which lack punctuation).7 The syntactic parses,
word sequences, and ToBI break indices were hand-
annotated by trained linguists, while the Mississippi
State phonetic alignments were automatically pro-
duced by a forced alignment of the speech signal
to a pronunciation-dictionary based phone sequence,
providing an estimate of the beginning and end time
of each phone. A small number of annotation er-
rors (in which the beginning and end times of some
phones had been swapped) were corrected by hand.
This corpus has 74 conversations with two sides
each.
We split this corpus into an 80%/10%/10%
train/dev/test 8 partition by dividing the entire cor-
pus into ten-sentence chunks, assigning the first
eight to the training partition, and the ninth and tenth
to the dev and test partitions, respectively. We then
removed all sentences containing only one or two
7We threw out a small number of sentences with annotations
errors, e.g. pointing to missing words.
8The dev set was used to explore different model structures
in preliminary experiments; all reported results are on the test
set.
24
words. Sentences this short have a trivial parse, and
are usually formulaic discourse responses (Bell et
al., 2009), which may influence their prosody. The
final corpus statistics are presented in Table 1.
4.2 Evaluation
We use the Penn Treebank parsed version of Switch-
board for evaluation. This version uses a slightly
different tokenization from the Mississippi State
transcriptions that were used as input to the mod-
els, so we transformed the Penn treebank tokeniza-
tion to agree with the Mississippi State tokeniza-
tion (primarily by concatenating clitics to their base
words?i.e. ?do? and ??nt? into ?don?t??and split-
ting multi-word expressions). We also removed all
gold-standard nodes spanning only Trace or PUNC
(recall that the input to the models did not include
punctuation) and collapsed all unary productions.9
In all evaluations, we convert our models? out-
put tag sequence to a set of matched brackets by in-
serting a left bracket preceding each word tagged B
tag and a right bracket following each word tagged
E. This procedure occasionally results in a sentence
with an unmatched opening bracket. If the un-
matched opening bracket is one word from the end
of the sentence, we delete it, otherwise we insert a
closing bracket at the end of the sentence. Figure 1
shows example input sequences together with exam-
ple output tags and their corresponding bracketings.
Previous work on chunking, most notably the
2000 CONLL shared task (Tjong et al, 2000), has
defined gold standard chunks that are useful for find-
ing grammatical relations but which do not corre-
spond to any particular linguistic notion. It is not
clear that such chunks should play a role in lan-
guage acquisition, so instead we evaluate against tra-
ditional syntactic constituents from Penn Treebank-
style parses in two different ways.
Our first evaluation method compares the output
of the chunkers to what Ponvert et al (2010) call
clumps, which are just syntactic constituents that
span only terminals. We created our clump gold-
standard by taking the parse trees resulting from the
preprocessing described above and deleting nodes
that span a non-terminal. Figure 3 presents an ex-
9As we evaluate unlabeled bracketing precision and recall,
the label of the resulting nodes is irrelevant.
g.aa dh.ae.t.s dh.ae.t
s.aw.n.d.z
p.r.ih.t.iy b.ae.d t.ax m.iy
Figure 3: Example gold-standard with clumps in boxes.
ample gold-standard parse tree with the clumps in
boxes. This evaluation avoids penalizing chunkers
for not positing hierarchical structure, but rewards
chunkers only for finding very low-level structure.
In the interest of making no a priori assumptions
about the kinds of phrases our unsupervised method
recovers, we also evaluate our completely flat, non-
recursive chunks directly against the fully recursive
parses in the treebank. To do so, we turn our chun-
ked utterance into a flat tree by simply putting brack-
ets around the entire utterance as in Figure 1(d).
This evaluation penalizes chunkers for never posit-
ing hierarchical structure, but makes no assumptions
about which kinds of phrases ought to be found.
4.3 Models and training
In all, nine HMM models, two versions of the
CCL parser, and a uniform right-branching baseline
were evaluated. Three of the HMMs were standard
HMMs with chunking constraints on the four hidden
states (as described in Section 3.2) that received as
input either words, ToBI break indices, or word du-
ration cluster information, intended as baselines to
illuminate the utility of each information source in
isolation. We also ran two each of Coupled HMM
and Two-output HMM models that received words
in one observed chain and either ToBI break index or
duration cluster in the other observed chain. In the
CHMM models, chunking constraints were enforced
on the chain generating the words, while variables
generating the duration or ToBI information ranged
over four discrete states with no constraints.10 All
non-zero parameters were initialized approximately
uniformly at random,11 and we ran EM until the log
10We also tried imposing chunking constraints on the second
chain, but dev-set performance dropped slightly.
11In preliminary dev-set experiments, different random ini-
tializations performed within two points of each other.
25
Condition Prec Rec F-sc
B
as
el
in
es
H
M
M
Wds 23.5 39.9 26.3
BI 7.2 4.8 5.8
Ac 4.7 2.5 3.3
C
om
bi
ne
d
M
od
el
s H
M
M Wds+BI 24.4 22.2 23.2
Wds+Ac 20.7 22.7 21.7
T
H
M
M Wds+BI 18.2 19.6 18.9
Wds+Ac 36.1 47.8 41.2
C
H
M
M Wds+BI 25.5 36.3 29.9
Wds+Ac 33.6 48.1 39.5
C
C
L Parser 15.4 41.5 22.4
Clumper 36.8 37.9 37.3
Table 2: Scores for all models, evaluated on clumps. In-
put is words (Wds), break indices (BI), and/or acoustics.
corpus probability changed less than 0.001%, typi-
cally for 50-150 iterations.
The CCL parser was trained on the same word se-
quences provided to our models. We also evaluated
the CCL parser as a clumper (CCL Clumper) by re-
moving internal nodes spanning a non-terminal. The
right-branching baseline was generated by inserting
one opening bracket in front of all but the last word,
and closing all brackets at the end of the sentence.
4.4 Results and Discussion
Table 2 presents results for our flat chunkers evalu-
ated against Ponvert et al (2010)-style clumps. Sev-
eral points are apparent. First, all three HMM base-
lines yield very poor results, especially the prosodic
baselines, whose precision and recall are both be-
low 10%. Although the best combined models
still have relatively low performance, it is markedly
higher than either of the individual baselines, and
also higher than the clumps identified by the CCL
parser. Particularly notable is the fact that lexi-
cal and prosodic information appear to be super-
additive in some cases, yielding combined perfor-
mance that is higher than the sum of the individual
scores. Not all combined models work equally well,
however: the poor performance of the HMM com-
bined model supports our initial hypothesis that it is
over parameterized. Interestingly, our acoustic clus-
ters work better than break indices when combined
with words. Finally, we see that the THMM and
CHMM obtain similar performance using words +
acoustics, suggesting that modeling prosodic struc-
% Covered words
chunk
chunk
uttCondition Words Utts
B
as
el
in
es
H
M
M
Wds 81.9 98.4 3.16 2.82
BI 68.2 68.1 4.95 1.50
Ac 46.3 71.1 4.18 1.21
C
om
bi
ne
d
M
od
el
s H
M
M Wds+BI 79.8 98.3 4.30 2.02
Wds+Ac 83.3 98.5 3.71 2.45
T
H
M
M Wds+BI 84.6 99.0 3.84 2.40
Wds+Ac 68.0 96.1 2.52 2.94
C
H
M
M Wds+BI 83.1 99.0 2.86 3.17
Wds+Ac 76.5 97.6 2.62 3.19
CCL Clumper 48.3 99.9 2.30 2.29
Table 3: % words in a chunk, % utterances with > 0
chunks, and mean chunk length and chunks per utterance.
Condition Prec Rec F-sc
B
as
el
in
es
H
M
M
Wds 48.8(32) 26.3(15) 34.2(20)
BI 52.4(21) 18.5(5) 27.3(8)
Ac 52.5(15) 16.3(3) 24.9(5)
C
om
bi
ne
d
M
od
el
s H
M
M Wds+BI 54.4(32) 23.2(11) 32.5(16)
Wds+Ac 51.0(32) 24.7(13) 33.3(18)
T
H
M
M Wds+BI 55.9(38) 26.8(15) 36.2(21)
Wds+Ac 55.8(41) 31.0(20) 39.9(27)
C
H
M
M Wds+BI 48.4(32) 28.4(17) 35.8(22)
Wds+Ac 54.1(40) 31.9(21) 40.1(28)
C
C
L Parser 38.2(28) 37.6(28) 37.9(28)
Clumper 58.8(42) 27.3(16) 37.3(23)
Table 4: Model performance, evaluated on full trees.
Scores in parentheses were computed after removing the
full sentence bracket, which provides a free true positive.
ture separately from syntactic structure may be un-
necessary (or that the CHMM does so badly).
To provide further intuition into the kinds of
chunks recovered by the different models, we list
some relevant statistics in Table 3. These statis-
tics show that the models using lexical information
identify at least one chunk in virtually all utterances,
with the better models averaging 2-3 chunks per ut-
terance of around 3 words each. In contrast, the
unlexicalized models find longer chunks (4-5 words
each) but far fewer of them, with about 30% of ut-
terances containing none at all.
We turn now to the models? performance on full
parse trees, shown in Table 4. Two different scores
are given for each system: the first includes the
top-level bracketing of the full sentence (which is
26
standard in computing bracketing accuracy, but is a
free true positive), while the second does not (for a
more accurate picture of the system?s performance
on ambiguous brackets). Comparing the second set
of scores to the clumping evaluation, recall is much
lower for all the chunkers; the relatively small in-
crease in precision indicates that the chunkers are
most effective at finding low-level structure. For
both sets of scores, the relative F-scores of the chun-
kers are similar to the clumping evaluation, with
the words + acoustics versions of the THMM and
CHMM scoring best. Not surprisingly, the CCL
parser has much higher recall than the chunkers,
though the best chunkers have much higher preci-
sion. The result is that, using standard Parseval
scoring (first column), the best chunkers outperform
CCL on F-score; even discounting the free sentence-
level bracket (second column) they do about as well.
It is worth noting that, although CCL achieves
state-of-the-art performance on the English WSJ
and German Negra corpora (Seginer (2007) reports
75.9% F-score on WSJ10, for example), its perfor-
mance on our corpus is far lower. In fact, on this cor-
pus the CCL parser (as well as our chunkers) under-
perform a uniform right-branching baseline, which
obtains 42.2% precision and 64.8% recall (including
the top-level bracket), leading to an overall F-score
of 51.1%. This suggests that our corpus is signifi-
cantly more difficult than WSJ, probably due to dis-
fluencies and/or lack of punctuation.12 Moreover,
we stress that the use of a right-branching baseline,
while useful as a measure of overall performance,
is not plausible as a model of language acquisition
since it is highly language-specific.
5 Conclusion
Taken together, our results indicate that a purely
local model that combines lexical and acoustic-
prosodic information in an appropriate way can
identify syntactic phrases far more effectively than
a similar model using either source of information
alone. Our best combined models outperformed
the baseline individual models by a wide margin
when evaluated against the lowest level of syntac-
tic structure, and their performance was compara-
12Including punctuation improves CCL little, possibly be-
cause the punctuation in this corpus is nearly all sentence-final.
ble to CCL, a state-of-the-art unsupervised lexical-
ized parser, when evaluated against full parse trees.
It is disappointing that all of these systems scored
worse than a right-branching baseline, but this result
underscores the major differences between parsing
spoken utterances (even using transcriptions) and
parsing written text (where CCL and other unsu-
pervised parsers were developed and tested). Since
children learning language do not (at least initially)
know the head direction of their language, the right-
branching baseline for English is not available to
them. Thus, combining lexical and acoustic cues
may provide them with initial useful information
about the location of syntactic phrases, as suggested
by the prosodic bootstrapping hypothesis.
Nevertheless, we caution against assuming that
the usefulness of acoustic information must re-
sult from its relation to prosody (especially be-
cause we found that direct acoustic information was
more useful than hand-annotated prosodic labels).
The ?Smooth Signal Hypothesis? (Aylett and Turk,
2004) posits that talkers modulate their communica-
tive effort according to the predictability of their
message in order to achieve efficient communica-
tion, pronouncing more predictable parts of mes-
sages more quickly or less distinctly. If talkers con-
sider syntactic predictability in this process, then
it is possible that acoustic cues help initial gram-
mar learning not by serving as cues to prosody but
by serving as cues to the talker?s syntax-dependent
view of predictability. In this case, it may make
more sense to discuss ?predictability bootstrapping?
rather than ?prosodic bootstrapping.?
Regardless of the underlying reason, we have
shown that acoustic cues can be useful for identi-
fying syntactic structure when used in combination
with lexical information. In order to further substan-
tiate these results, we plan to replicate our experi-
ments on a corpus of child-directed speech, which
we are currently annotating for evaluation purposes.
We also hope to extend our findings to a model that
can identify hierarchical structure, and to analyze
more carefully the reasons for CCL?s poor perfor-
mance on the Switchboard corpus, in hopes of devel-
oping a model that can reach levels of performance
closer to those typical of unsupervised parsers for
written text.
27
References
Matthew Aylett and Alice Turk. 2004. The smooth sig-
nal redundancy hypothesis: A functional explanation
for relationships between redundancy, prosodic promi-
nence, and duration in spontaneous speech. Language
and Speech, 47(1):31 ? 56.
Mary E. Beckman and Jan Edwards. 1990. Lengthen-
ings and shortenings and the nature of prosodic con-
stituency. In J. Kingston and Mary E. Beckman, edi-
tors, Between the grammar and physics of speech: Pa-
pers in laboratory phonology I, pages 152?178. Cam-
bridge: Cambridge University Press.
M Beckman, J Hirschberg, and S Shattuck-Hufnagel.
2005. The original tobi system and the evolution of
the tobi framework. In S.-A. Jun, editor, Prosodic Ty-
pology ? The Phonology of Intonation and Phrasing.
Oxford University Press.
Alan Bell, Jason M. Brenier, Michelle Gregory, Cynthia
Girand, and Dan Jurafsky. 2009. Predictability effects
on durations of content and function words in conver-
sational english. Journal of Memory and Language,
60:92 ? 111.
S Calhoun, J Carletta, J Brenier, N Mayo, D Jurafsky,
M Steedman, and D Beaver. 2010. The nxt-format
switchboard corpus: A rich resource for investigat-
ing the syntax, semantics, pragmatics and prosody
of dialogue. Language Resources and Evaluation,
44(4):387 ? 419.
Markus Dreyer and Izhak Shafran. 2007. Exploiting
prosody for pcfgs with latent annotations. In Proc. of
Interspeech, Antwerp, Belgium, August.
L. Gleitman and E. Wanner. 1982. Language acquisition:
The state of the art. In E. Wanner and L. Gleitman, ed-
itors, Language acquisition: The state of the art, pages
3?48. Cambridge University Press, Cambridge, UK.
Michelle L. Gregory, Mark Johnson, and Eugene Char-
niak. 2004. Sentence-internal prosody does not help
parsing the way punctuation does. In Proceedings
of the North American Association for Computational
Linguistics (NAACL), pages 81?88.
J. G. Kahn, M. Lease, E. Charniak, M. Johnson, and
M. Ostendorf. 2005. Effective use of prosody in pars-
ing conversational speech. In Proc. of HLT/EMNLP-
05.
D H Klatt. 1976. Linguistic uses of segmental durations
in english: Acoustic and perceptual evidence. JASA,
59:1208 ? 1221.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
42nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2004), pages 479?486.
Bob Ladd. 1996. Intonational Phonology. Cambridge
University Press.
K Lari and S J Young. 1990. The estimation of stochastic
context-free grammars using the inside-outside algo-
rithm. Computer Speech and Language, 5:237 ? 257.
P. Liang, S. Petrov, M. I. Jordan, and D. Klein. 2007. The
infinite PCFG using hierarchical Dirichlet processes.
In Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP/CoNLL).
Brian MacWhinney. 2000. The CHILDES project: Tools
for analyzing talk. Lawrence Erlbaum Associates,
Mahwah, NJ, third edition.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Jacques Mehler, Peter Juszcyk, Ghislaine Lambertz,
Nilofar Halsted, Josiane Bertoncini, and Claudine
Amiele-Tison. 1988. A precursor to language acqui-
sition in young infants. Cognition, 29:143 ? 178.
Se?verine Millotte, Roger Wales, and Anne Christophe.
2007. Phrasal prosody disambiguates syntax. Lan-
guage and Cognitive Processes, 22(6):898 ? 909.
Antonio Molina and Feran Pla. 2002. Shallow parsing
using specialized HMMs. Journal of Machine Learn-
ing Research, 2:595 ? 613.
James L. Morgan, Richard P. Meier, and Elissa L. New-
port. 1987. Structural packaging in the input to lan-
guage learning: contributions of prosodic and morpho-
logical marking of phrases to the acquisition of lan-
guage. Cognitive Psychology, 19:498 ? 550.
Ara V. Nefian, Luhong Liang, Xiaobao Pi, Liu Xiaoxi-
ang, Crusoe Moe, and Kevin Murphy. 2002. A cou-
pled hmm for audiovisual speech recognition. In Inter-
national Conference on Acoustics, Speech and Signal
Processing.
Elmer No?th, Anton Batliner, Andreas Kieling, and Ralfe
Kompe. 2000. Verbmobil: The use of prosody in the
linguistic components of a speech understanding sys-
tem. IEEE Transactions on Speech and Audio Pro-
cessing, 8(5).
Elias Ponvert, Jason Baldridge, and Katrin Erk. 2010.
Simple unsupervised identification of low-level con-
stituents. In ICSC.
P J Price, M Ostendorf, S Shattuck-Hufnagel, and
C Fong. 1991. The use of prosody in syntactic dis-
ambiguation. JASA, pages 2956 ? 2970.
Yoav Seginer. 2007. Fast unsupervised incremental pars-
ing. In Proceedings of the Association of Computa-
tional Linguistics.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceedings of
HLT-NAACL 03, pages 213?220.
28
Stefanie Shattuck-Hufnagel and Alice E Turk. 1996. A
prosody tutorial for investigators of auditory sentence
processing. Journal of Psycholinguistic Research,
25(2):193 ? 247.
M. Soderstrom, A. Seidl, D. G. K. Nelson, and P. W.
Jusczyk. 2003. The prosodic bootstrapping of
phrases: Evidence from prelinguistic infants. Journal
of Memory and Language, 49:249?267.
Charles Sutton. 2006. Grmm: Graphical models in mal-
let. http://mallet.cs.umass.edu/grmm/.
Erik F. Tjong, Kim Sang, and Sabine Buchholz. 2000.
Introduction to the conll-2000 shared task: Chunking.
In Proceedings of CoNLL-2000 and LLL-2000, Lis-
bon, Portugal.
C W Wightman, S Shattuck-Hufnagel, M. Ostendorf, and
P J Price. 1992. Segmental durations in the vicinity of
prosodic phrase boundaries. Journal of the Acoustical
Society of America, 91(3):1707 ? 1717.
29
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, page 1,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Abstract for the Invited Talk
Unsupervised NLP and Human Language Acquisition: Making Connections to Make Progress
Sharon Goldwater
Natural language processing and cognitive science are two fields in which unsupervised language
learning is an important area of research. Yet there is often little crosstalk between the two fields.
In this talk, I will argue that considering the problem of unsupervised language learning from a
cognitive perspective can lead to useful insights for the NLP researcher, while also showing how
tools and methods from NLP and machine learning can shed light on human language acquisition.
I will present two case examples, both of them models inspired by cognitive questions. The first is a
model of word segmentation, which introduced new modeling and inference techniques into NLP while
also yielding a better fit than previous models to human behavioral data on word segmentation. The
second is more recent work on unsupervised grammar induction, in which prosodic cues are used to
help identify syntactic boundaries. Preliminary results indicate that such cues can be helpful, but also
reveal weaknesses in existing unsupervised grammar induction methods from NLP, suggesting possible
directions for future research.
1
NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 96?99,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Turning the pipeline into a loop: Iterated unsupervised dependency parsing
and PoS induction
Christos Christodoulopoulos?, Sharon Goldwater?, Mark Steedman?
School of Informatics
University of Edinburgh
?christos.c@ed.ac.uk ?{steedman,sgwater}@inf.ed.ac.uk
1 Motivation
Most unsupervised dependency systems rely on
gold-standard Part-of-Speech (PoS) tags, either di-
rectly, using the PoS tags instead of words, or indi-
rectly in the back-off mechanism of fully lexicalized
models (Headden et al, 2009).
It has been shown in supervised systems that us-
ing a hierarchical syntactic structure model can pro-
duce competitive sequence models; in other words
that a parser can be a good tagger (Li et al, 2011;
Auli and Lopez, 2011; Cohen et al, 2011). This
is unsurprising, as the parser uses a rich set of hi-
erarchical features that enable it to look at a less
localized environment than a PoS tagger which in
most cases relies solely on local contextual features.
However this interaction has not been shown for the
unsupervised setting. To our knowledge, this work
is the first to show that using dependencies for unsu-
pervised PoS induction is indeed useful.
2 Iterated learning
Although most unsupervised systems depend on
gold-standard PoS information, they can also be
used in a fully unsupervised pipeline. One reason for
doing so is to use dependency parsing as an extrinsic
evaluation for unsupervised PoS induction (Headden
et al, 2008). As discussed in that paper (and also by
Klein and Manning (2004)) the quality of the de-
pendencies drops with the use of induced tags. One
way of producing better PoS tags is to use the depen-
dency parser?s output to influence the PoS inducer,
thus turning the pipeline into a loop.
The main difficulty of this approach is to find
a way of incorporating dependency information
into a PoS induction system. In previous work
BMMM
DMV
BMMM
DMV
BMMM
Gen. 0 Gen. 1 Gen. 2
Figure 1: The iterated learning paradigm for induc-
ing both PoS tags and dependencies.
(Christodoulopoulos et al, 2011) we have described
BMMM: a PoS induction system that makes it is
easy to incorporate multiple features either at the
type or token level. For the dependency induction
system we chose the DMV model of Klein and Man-
ning (2004) because of its simplicity and its popular-
ity. Both systems are described briefly in section 3.
Using these two systems we performed an iter-
ated learning experiment. The term is borrowed
from the language evolution literature meaning ?the
process by which the output of one individual?s
learning becomes the input to other individuals?
learning? (Smith et al, 2003). Here we treat the
two systems as the individuals1 that influence each
other in successive generations starting from a run
of the original BMMM system without dependency
information (fig. 1). We start with a run of the basic
BMMM system using just context and morphology
features (generation 0) and use the output to train the
DMV. To complete the first generation, we then use
the induced dependencies as features (as described
in section 4) for a new run of the BMMM system.
As there is no single objective function, this setup
1This is not directly analogous to the language evolution no-
tion of iterated learning; here instead of a single type of indi-
vidual we have two separate systems that learn/model different
representations.
96
does not guarantee that either the quality of PoS tags
or the dependencies will improve after each genera-
tion. However, in practice this iterated learning ap-
proach works well (as we discuss in section 4).
3 Component models
3.1 DMV model
The basic DMV model (Klein and Manning, 2004)
generates dependency trees based on three decisions
(represented by three probability distributions) for a
given head node: whether to attach children in the
left or right direction; whether or not to stop attach-
ing more children in the specific direction given the
adjacency of the child in that direction; and finally
whether to attach a specific child node. The proba-
bility of an entire sentence is the sum of the probabil-
ities of all the possible derivations headed by ROOT.
The DMV model can be seen as (and is equiva-
lent to) a Context Free Grammar (CFG) with only a
few rules from head nodes to generated children and
therefore the model parameters can be estimated us-
ing the Inside-Outside algorithm (Baker, 1979).
3.2 BMMM model
The Bayesian Multinomial Mixture Model
(Christodoulopoulos et al, 2011), illustrated in
figure 2, assumes that all tokens of a given word
type belong to a single syntactic class, and each
type is associated with a number of features (e.g.,
morphological or contextual features), which form
the observed variables. The generative process first
chooses a hidden class z for each word type and then
chooses values for each of the observed features of
that word type, conditioned on the class. Both the
distribution over classes ? and the distributions over
each kind of feature ?(t) are multinomials drawn
from Dirichlet priors ? and ?(t) respectively. A
main advantage of this model is its ability to easily
incorporate features either at the type or token
level; as in Christodoulopoulos et al (2011) we
assume a single type-level feature m (morphology,
drawn from ?(m)) and several token-level features
f
(1)
. . . f
(T ) (e.g., left and right context words and,
in our extension, dependency features).
Inference in the model is performed using a col-
lapsed Gibbs sampler, integrating out the model pa-
rameters and sampling the class label zj for each
?
?
z
f
(1)
?
(1)
?
(1)
. . . . . . . . .
f
(T )
?
(T )
?
(T )
m
?
(m)
?
(m)
M
n
j
n
j
Z
Z
Z
Figure 2: The BMMM with T kinds of token-level
features (f (t) variables) and a single kind of type-
level feature (morphology, m). M is the total num-
ber of word types, Z the number of classes, and nj
the number of tokens of type j.
word type j from the following posterior distribu-
tion:
P (zj | z?j , f , ?, ?)
? P (zj | z?j , ?, ?)P (fj | f?j , z, ?, ?) (1)
where the first factor P (zj) is the prior distribu-
tion over classes (the mixing weights) and the sec-
ond (likelihood) factor P (fj) is the probability given
class zj of all the features associated with word type
j. Since the different kinds of features are assumed
to be independent, the likelihood can be rewritten as:
P (fj | f?j , z, ?, ?) = P (f
(m)
j | f
(m)
?j , z, ?, ?)
?
T?
t=1
P (f (t)j | f
(t)
?j , z, ?) (2)
and, as explained in Christodoulopoulos et al
(2011), the joint probability of all the token level
features of kind t for word type j is computed as:
P (f (t)j | f
(t)
?j , zj = z, z?j , ?)
=
?K(t)
k=1
?njk?1
i=0 (njk,z + i+ ?)
?nj?1
i=0 (n?,z + i+ F?)
(3)
97
30 
35 
40 
45 
50 
55 
60 
65 
0 1 2 3 4 5 6 7 8 9 10 
BMMM M-1 BMMM VM DMV Dir DMV Undir 
(a) Using only directed dependencies as features
30 
35 
40 
45 
50 
55 
60 
65 
0 1 2 3 4 5 6 7 8 9 10 
BMMM M-1 BMMM VM DMV Dir DMV Undir 
(b) Using directed and undirected dependencies as features
Figure 3: Developmental results on WSJ10. The performance of the PoS inducer is shown in terms of
many-to-1 accuracy (BMMM M1) and V-Measure accuracy (BMMM VM) and the performance of the
dependency inducer is shown using directed and undirected dependency accuracy (DMV Dir and DMV
Undir respectively).
where K(t) is the dimensionality of ?(t) and njk is
the number of instances of feature k in word type j.
4 Experimental design
Because the different kinds of features are assumed
to be independent in the BMMM, it is easy to add
more features into the model; this simply increases
the number of factors in equation 2. To incorpo-
rate dependency information, we added a feature for
word-word dependencies. In the model, this means
that for a word type j with nj tokens, we observe nj
dependency features (each being the head of one to-
ken of j). Like all other features, these are assumed
to be drawn from a class-specific multinomial ?(d)z
with a Dirichlet prior ?(d).
Using lexicalized head dependencies introduces
sparsity issues in much the same way contextual in-
formation does. In the case of context words, the
BMMM and most vector-based clustering systems
use a fixed number of most frequent words as fea-
tures; however in the case of dependencies we use
the induced PoS tags of the previous generation as
grouping labels: we aggregate the head dependency
counts of words that have the same PoS tag, so the
dimension of ?(d)z is just the number of PoS tags.
The dependency features are used in tandem with
the features used in the original BMMM system,
namely the 100 most frequent context words (?1
context window), the suffixes extracted from the
Morfessor system (Creutz and Lagus, 2005) and
the extended morphology features of Haghighi and
Klein (2006).
For designing the iterated learning experiments,
we used the 10-word version of the WSJ corpus
(WSJ10) as development data and ran the iterative
learning process for 10 generations. To evaluate the
quality of the induced PoS tags we used the many-
to-1 (M1) and V-Measure (VM) metrics and for the
induced dependencies we used directed and undi-
rected accuracy.
Figure 3a presents the developmental result of the
iterated learning experiments on WSJ10 where only
directed dependencies where used as features. We
can see that although there was some improvement
in the PoS induction score after the first generation,
the rest of the metrics show no significant improve-
ment throughout the experiment.
When we used undirected dependencies as fea-
tures (figure 3b) the improvement over iterations
was substantial: nearly 8.5% increase in M1 and
1.3% in VM after only 5 iterations. We can also see
that the results of the DMV parser are improving as
well: 7% increase in directed and 3.8% in undirected
accuracy. This is to be expected, since as Headden
et al (2008) show, there is a (weak) correlation be-
tween the intrinsic scores of a PoS inducer and the
98
performance of an unsupervised dependency parser
trained on the inducer?s output.
Using the same development set we selected the
remaining system parameters; for the BMMM we
fixed the number of induced classes to the number
of gold-standard PoS tags for each language and
used 500 sampling iterations with annealing. For
the DMV model we used 20 EM iterations. Finally
we used observed that after 5 generations the rate of
improvement seems to level, so for the rest of the
languages we use only 5 learning iterations.
References
Michael Auli and Adam Lopez. 2011. A comparison of
loopy belief propagation and dual decomposition for
integrated CCG supertagging and parsing. In Proceed-
ings of ACL-HLT, pages 470?480.
James K. Baker. 1979. Trainable grammars for speech
recognition. The Journal of the Acoustical Society of
America, 65(S1):S132, June.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2011. A Bayesian mixture model
for pos induction using multiple features. In Proceed-
ings of EMNLP, pages 638?647, Edinburgh, Scotland,
UK., July.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011.
Unsupervised structure prediction with non-parallel
multilingual guidance. In Proceedings of EMNLP,
pages 50?61.
Mathias Creutz and Krista Lagus. 2005. Inducing
the morphological lexicon of a natural language from
unannotated text. In In Proceedings of AKRR, vol-
ume 5, pages 106?113.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
NAACL, pages 320?327.
William P. Headden, David McClosky, and Eugene Char-
niak. 2008. Evaluating unsupervised part-of-speech
tagging for grammar induction. In Proceedings of
COLING, pages 329?336.
William P. Headden, Mark Johnson, and David Mc-
Closky. 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. In Pro-
ceedings of NAACL, pages 101?109.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In Proceedings of ACL.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-
liang Chen, and Haizhou Li. 2011. Joint models for
chinese POS tagging and dependency parsing. In Pro-
ceedings of EMNLP, pages 1180?1191.
Kenny Smith, Simon Kirby, and Henry Brighton. 2003.
Iterated learning: a framework for the emergence of
language. Artif. Life, 9(4):371?386.
99
