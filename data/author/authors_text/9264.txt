Improved Word Alignment Using a Symmetric Lexicon Model
Richard Zens and Evgeny Matusov and Hermann Ney
Lehrstuhl fu?r Informatik VI, Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
{zens,matusov,ney}@cs.rwth-aachen.de
Abstract
Word-aligned bilingual corpora are an
important knowledge source for many
tasks in natural language processing. We
improve the well-known IBM alignment
models, as well as the Hidden-Markov
alignment model using a symmetric lex-
icon model. This symmetrization takes
not only the standard translation direc-
tion from source to target into account,
but also the inverse translation direction
from target to source. We present a the-
oretically sound derivation of these tech-
niques. In addition to the symmetriza-
tion, we introduce a smoothed lexicon
model. The standard lexicon model is
based on full-form words only. We propose
a lexicon smoothing method that takes
the word base forms explicitly into ac-
count. Therefore, it is especially useful
for highly inflected languages such as Ger-
man. We evaluate these methods on the
German?English Verbmobil task and the
French?English Canadian Hansards task.
We show statistically significant improve-
ments of the alignment quality compared
to the best system reported so far. For
the Canadian Hansards task, we achieve
an improvement of more than 30% rela-
tive.
1 Introduction
Word-aligned bilingual corpora are an impor-
tant knowledge source for many tasks in nat-
ural language processing. Obvious applica-
tions are the extraction of bilingual word or
phrase lexica (Melamed, 2000; Och and Ney,
2000). These applications depend heavily on
the quality of the word alignment (Och and
Ney, 2000). Word alignment models were first
introduced in statistical machine translation
(Brown et al, 1993). The alignment describes
the mapping from source sentence words to
target sentence words.
Using the IBM translation models IBM-1
to IBM-5 (Brown et al, 1993), as well as
the Hidden-Markov alignment model (Vogel
et al, 1996), we can produce alignments of
good quality. In (Och and Ney, 2003), it is
shown that the statistical approach performs
very well compared to alternative approaches,
e.g. based on the Dice coefficient or the com-
petitive linking algorithm (Melamed, 2000).
A central component of the statistical trans-
lation models is the lexicon. It models the
word translation probabilities. The standard
training procedure of the statistical models
uses the EM algorithm. Typically, the models
are trained for one translation direction only.
Here, we will perform a simultaneous training
of both translation directions, source-to-target
and target-to-source. After each iteration of
the EM algorithm, we combine the two lexica
to a symmetric lexicon. This symmetric lex-
icon is then used in the next iteration of the
EM algorithm for both translation directions.
We will propose and justify linear and loglin-
ear interpolation methods.
Statistical methods often suffer from the
data sparseness problem. In our case, many
words in the bilingual sentence-aligned texts
are singletons, i.e. they occur only once. This
is especially true for the highly inflected lan-
guages such as German. It is hard to obtain
reliable estimations of the translation proba-
bilities for these rarely occurring words. To
overcome this problem (at least partially), we
will smooth the lexicon probabilities of the
full-form words using a probability distribu-
tion that is estimated using the word base
forms. Thus, we exploit that multiple full-
form words share the same base form and have
similar meanings and translations.
We will evaluate these methods on the
German?English Verbmobil task and the
French?English Canadian Hansards task. We
will show statistically significant improve-
ments compared to state-of-the-art results in
(Och and Ney, 2003). On the Canadian
Hansards task, the symmetrization methods
will result in an improvement of more than
30% relative.
2 Statistical Word Alignment Models
In this section, we will give a short description
of the commonly used statistical word align-
ment models. These alignment models stem
from the source-channel approach to statisti-
cal machine translation (Brown et al, 1993).
We are given a source language sentence fJ1 :=
f1...fj ...fJ which has to be translated into
a target language sentence eI1 := e1...ei...eI .
Among all possible target language sentences,
we will choose the sentence with the highest
probability:
e?I1 = argmax
eI1
{Pr(eI1|fJ1 )
}
= argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)
}
This decomposition into two knowledge
sources allows for an independent modeling of
target language model Pr(eI1) and translation
model Pr(fJ1 |eI1). Into the translation model,
the word alignment A is introduced as a hid-
den variable:
Pr(fJ1 |eI1) =
?
A
Pr(fJ1 , A|eI1)
Usually, we use restricted alignments in the
sense that each source word is aligned to at
most one target word, i.e. A = aJ1 . A de-
tailed description of the popular translation
models IBM-1 to IBM-5 (Brown et al, 1993),
as well as the Hidden-Markov alignment model
(HMM) (Vogel et al, 1996) can be found in
(Och and Ney, 2003). All these models include
parameters p(f |e) for the single-word based
lexicon. They differ in the alignment model.
A Viterbi alignment A? of a specific model is
an alignment for which the following equation
holds:
A? = argmax
A
{Pr(fJ1 , A|eI1)
}
We measure the quality of an alignment model
using the quality of the Viterbi alignment com-
pared to a manually produced reference align-
ment.
In Section 3, we will apply the lexicon sym-
metrization methods to the models described
previously. Therefore, we will now sketch the
standard training procedure for the lexicon
model. The EM algorithm is used to train
the free lexicon parameters p(f |e).
In the E-step, the lexical counts for each
sentence pair (fJ1 , eI1) are calculated and then
summed over all sentence pairs in the training
corpus:
N(f, e) =
?
(fJ1 ,eI1)
?
aJ1
p(aJ1 |fJ1 , eI1)
?
i,j
?(f, fj)?(e, ei)
In the M-step the lexicon probabilities are:
p(f |e) = N(f, e)?
f?
N(f? , e)
3 Symmetrized Lexicon Model
During the standard training procedure, the
lexicon parameters p(f |e) and p(e|f) were es-
timated independent of each other in strictly
separate trainings. In this section, we present
two symmetrization methods for the lexicon
model. As a starting point, we use the
joint lexicon probability p(f, e) and determine
the conditional probabilities for the source-
to-target direction p(f |e) and the target-to-
source direction p(e|f) as the corresponding
marginal distribution:
p(f |e) = p(f, e)?
f?
p(f? , e) (1)
p(e|f) = p(f, e)?
e?
p(f, e?) (2)
The nonsymmetric auxiliary Q-functions for
reestimating the lexicon probabilities during
the EM algorithm can be represented as fol-
lows. Here, NST (f, e) and NTS(f, e) denote
the lexicon counts for the source-to-target
(ST ) direction and the target-to-source (TS)
direction, respectively.
QST ({p(f |e)}) =
?
f,e
NST (f, e) ? log p(f, e)?
f?
p(f? , e)
QTS({p(e|f)}) =
?
f,e
NTS(f, e) ? log p(f, e)?
e?
p(f, e?)
3.1 Linear Interpolation
To estimate the joint probability using the EM
algorithm, we define the auxiliary Q-function
as a linear interpolation of the Q-functions for
the source-to-target and the target-to-source
direction:
Q?({p(f, e)}) = ? ?QST ({p(f |e)})
+(1? ?) ?QTS({p(e|f)})
= ? ?
?
f,e
NST (f, e) ? log p(f, e)
+(1? ?) ?
?
f,e
NTS(f, e) ? log p(f, e)
?? ?
?
e
NST (e) ? log
?
f?
p(f? , e)
?(1? ?) ?
?
f
NTS(f) ? log
?
e?
p(f, e?)
The unigram counts N(e) and N(f) are deter-
mined, for each of the two translation direc-
tions, by taking a sum of N(f, e) over f and
over e, respectively. We define the combined
lexicon count N?(f, e):
N?(f, e) := ? ?NST (f, e) + (1? ?) ?NTS(f, e)
Now, we derive the symmetrized Q-function
over p(f, e) for a certain word pair (f, e).
Then, we set this derivative to zero to deter-
mine the reestimation formula for p(f, e) and
obtain the following equation:
N?(f, e)
p(f, e) = ? ?
NST (e)?
f?
p(f? , e) + (1? ?) ?
NTS(f)?
e?
p(f, e?)
We do not know a closed form solution for this
equation. As an approximation, we use the
following term:
p?(f, e) = N?(f, e)?
f? ,e?
N?(f? , e?)
This estimate is an exact solution, if the uni-
gram counts for f and e are independent of the
translation direction, i. e. NST (f) = NTS(f)
and NST (e) = NTS(e). We make this approx-
imation and thus we interpolate the lexicon
counts linear after each iteration of the EM
algorithm. Then, we normalize these counts
(according to Equations 1 and 2) to determine
the lexicon probabilities for each of the two
translation directions.
3.2 Loglinear Interpolation
We will show in Section 5 that the linear in-
terpolation results in significant improvements
over the nonsymmetric system. Motivated by
these experiments, we investigated also the
loglinear interpolation of the lexicon counts of
the two translation directions. The combined
lexicon count N?(f, e) is now defined as:
N?(f, e) = NST (f, e)? ?NTS(f, e)1??
The normalization is done in the same way as
for the linear interpolation. The linear inter-
polation resembles more a union of the two lex-
ica whereas the loglinear interpolation is more
similar to an intersection of both lexica. Thus
for the linear interpolation, a word pair (f, e)
obtains a large combined count, if the count in
at least one direction is large. For the loglin-
ear interpolation, the combined count is large
only if both lexicon counts are large.
In the experiments, we will use the interpo-
lation weight ? = 0.5 for both the linear and
the loglinear interpolation, i. e. both transla-
tion directions are weighted equally.
3.3 Evidence Trimming
Initially, the lexicon contains all word pairs
that cooccur in the bilingual training corpus.
The majority of these word pairs are not trans-
lations of each other. Therefore, we would
like to remove those lexicon entries. Evidence
trimming is one way to do this. The evidence
of a word pair (f, e) is the estimated count
N(f, e). Now, we discard a word pair if its ev-
idence is below a certain threshold ? .1 In the
case of the symmetric lexicon, we can further
refine this method. For estimating the lex-
icon in the source-to-target direction p?(f |e),
the idea is to keep all entries from this di-
rection and to boost the entries that have a
high evidence in the target-to-source direction
NTS(f, e). We obtain the following formula:
N?ST (f, e) =
?
?
?
?NST (f, e) + (1? ?)NTS(f, e)
if NST (f, e) > ?
0 else
The count N?ST (f, e) is now used to estimate
the source-to-target lexicon p?(f |e). With this
method, we do not keep entries in the source-
to-target lexicon p?(f |e) if their evidence is low,
even if their evidence in the target-to-source
1Actually, there is always implicit evidence trim-
ming caused by the limited machine precision.
direction NTS(f, e) is high. For the target-to-
source direction, we apply this method in a
similar way.
4 Lexicon Smoothing
The lexicon model described so far is based on
full-form words. For highly inflected languages
such as German this might cause problems,
because many full-form words occur only a few
times in the training corpus. Compared to En-
glish, the token/type ratio for German is usu-
ally much lower (e.g. Verbmobil: English 99.4,
German 56.3). The information that multiple
full-form words share the same base form is
not used in the lexicon model. To take this in-
formation into account, we smooth the lexicon
model with a backing-off lexicon that is based
on word base forms. The smoothing method
we apply is absolute discounting with interpo-
lation:
p(f |e) = max {N(f, e)? d, 0}N(e) + ?(e) ? ?(f, e?)
This method is well known from language
modeling (Ney et al, 1997). Here, e? de-
notes the generalization, i.e. the base form,
of the word e. The nonnegative value d is
the discounting parameter, ?(e) is a normal-
ization constant and ?(f, e?) is the normalized
backing-off distribution.
The formula for ?(e) is:
?(e) = 1N(e)
?
? ?
f :N(f,e)>d
d+
?
f :N(f,e)?d
N(f, e)
?
?
= 1N(e)
?
f
min{d,N(f, e)}
This formula is a generalization of the one
typically used in publications on language
modeling. This generalization is necessary,
because the lexicon counts may be fractional
whereas in language modeling typically inte-
ger counts are used. Additionally, we want
to allow for discounting values d greater than
one. The backing-off distribution ?(f, e?) is es-
timated using relative frequencies:
?(f, e?) = N(f, e?)?
f?
N(f? , e?)
Here, N(f, e?) denotes the count of the event
that the source language word f and the target
language base form e? occur together. These
counts are computed by summing the lexicon
counts N(f, e) over all full-form words e which
share the same base form e?.
5 Results
5.1 Evaluation Criteria
We use the same evaluation criterion as de-
scribed in (Och and Ney, 2000). The gen-
erated word alignment is compared to a ref-
erence alignment which is produced by hu-
man experts. The annotation scheme explic-
itly takes the ambiguity of the word alignment
into account. There are two different kinds
of alignments: sure alignments (S) which are
used for alignments that are unambiguous and
possible alignments (P ) which are used for
alignments that might or might not exist. The
P relation is used especially to align words
within idiomatic expressions, free translations,
and missing function words. It is guaranteed
that the sure alignments are a subset of the
possible alignments (S ? P ). The obtained
reference alignment may contain many-to-one
and one-to-many relationships.
The quality of an alignment A is computed
as appropriately redefined precision and recall
measures. Additionally, we use the alignment
error rate (AER), which is derived from the
well-known F-measure.
recall = |A ? S||S| , precision =
|A ? P |
|A|
AER(S, P ;A) = 1? |A ? S|+ |A ? P ||A|+ |S|
With these definitions a recall error can only
occur if a S(ure) alignment is not found and a
precision error can only occur if a found align-
ment is not even P (ossible).
5.2 Experimental Setup
We evaluated the presented lexicon sym-
metrization methods on the Verbmobil and
the Canadian Hansards task. The German?
English Verbmobil task (Wahlster, 2000) is a
speech translation task in the domain of ap-
pointment scheduling, travel planning and ho-
tel reservation. The French?English Canadian
Hansards task consists of the debates in the
Canadian Parliament.
The corpus statistics are shown in Table 1
and Table 2. The number of running words
and the vocabularies are based on full-form
words including punctuation marks. As in
Table 1: Verbmobil: Corpus statistics.
German English
Train Sentences 34K
Words 329 625 343 076
Vocabulary 5 936 3 505
Singletons 2 600 1 305
Test Sentences 354
Words 3 233 3 109
Table 2: Canadian Hansards: Corpus statistics.
French English
Train Sentences 128K
Words 2.12M 1.93M
Vocabulary 37 542 29 414
Singletons 12 986 9 572
Test Sentences 500
Words 8 749 7 946
(Och and Ney, 2003), the first 100 sentences
of the test corpus are used as a development
corpus to optimize model parameters that are
not trained via the EM algorithm, e.g. the
discounting parameter for lexicon smoothing.
The remaining part of the test corpus is used
to evaluate the models.
We use the same training schemes (model
sequences) as presented in (Och and Ney,
2003). As we use the same training and test-
ing conditions as (Och and Ney, 2003), we will
refer to the results presented in that article as
the baseline results. In (Och and Ney, 2003),
the alignment quality of statistical models is
compared to alternative approaches, e.g. us-
ing the Dice coefficient or the competitive
linking algorithm. The statistical approach
showed the best performance and therefore we
report only the results for the statistical sys-
tems.
5.3 Lexicon Symmetrization
In Table 3 and Table 4, we present the follow-
ing experiments performed for both the Verb-
mobil and the Canadian Hansards task:
? Base: the system taken from (Och and
Ney, 2003) that we use as baseline system.
? Lin.: symmetrized lexicon using a lin-
ear interpolation of the lexicon counts af-
ter each training iteration as described in
Section 3.1.
? Log.: symmetrized lexicon using a log-
linear interpolation of the lexicon counts
after each training iteration as described
in Section 3.2.
Table 3: Comparison of alignment perfor-
mance for the Verbmobil task (S?T: source-
to-target direction, T?S: target-to-source di-
rection; all numbers in percent).
S?T T?S
Pre. Rec. AER Pre. Rec. AER
Base 93.5 95.3 5.7 91.4 88.7 9.9
Lin. 96.0 95.4 4.3 93.7 89.6 8.2
Log. 93.6 95.6 5.5 94.5 89.4 7.9
 4
 6
 8
 10
 12
 14
 16
 18
 100  1000  10000  100000
A
E
R
Corpus Size
baselinelinearloglinear
Figure 1: AER[%] of different alignment meth-
ods as a function of the training corpus size
for the Verbmobil task (source-to-target direc-
tion).
In Table 3, we compare both interpolation
variants for the Verbmobil task to (Och and
Ney, 2003). We observe notable improvements
in the alignment error rate using the linear in-
terpolation. For the translation direction from
German to English (S?T), an improvement of
about 25% relative is achieved from an align-
ment error rate of 5.7% for the baseline system
to 4.3% using the linear interpolation. Per-
forming the loglinear interpolation, we observe
a substantial reduction of the alignment error
rate as well. The two symmetrization methods
improve both precision and recall of the result-
ing Viterbi alignment in both translation di-
rections for the Verbmobil task. The improve-
ments with the linear interpolation is for both
translation directions statistically significant
at the 99% level. For the loglinear interpo-
lation, the target-to-source translation direc-
tion is statistically significant at the 99% level.
The statistical significance test were done us-
ing boostrap resampling.
We also performed experiments on sub-
corpora of different sizes. For the Verbmo-
bil task, the results are illustrated in Figure 1.
Table 4: Comparison of alignment perfor-
mance for the Canadian Hansards task (S?T:
source-to-target direction, T?S: target-to-
source direction; all numbers in percent).
S?T T?S
Pre. Rec. AER Pre. Rec. AER
Base 85.4 90.6 12.6 85.6 90.9 12.4
Lin. 89.3 91.4 9.9 89.0 92.0 9.8
Log. 91.0 92.0 8.6 91.2 92.1 8.4
We observe that both symmetrization variants
result in improvements for all corpus sizes.
With increasing training corpus size the per-
formance of the linear interpolation becomes
superior to the performance of the loglinear
interpolation.
In Table 4, we compare the symmetriza-
tion methods with the baseline system for the
Canadian Hansards task. Here, the loglin-
ear interpolation performs best. We achieve
a relative improvement over the baseline of
more than 30% for both translation directions.
For instance, the alignment error rate for the
translation direction from French to English
(S?T) improves from 12.6% for the baseline
system to 8.6% for the symmetrized system
with loglinear interpolation. Again, the two
symmetrization methods improve both preci-
sion and recall of the Viterbi alignment.
For the Canadian Hansards task, all the im-
provements of the alignment error rate are sta-
tistically significant at the 99% level.
5.4 Generalized Alignments
In (Och and Ney, 2003) generalized alignments
are used, thus the final Viterbi alignments of
both translation directions are combined us-
ing some heuristic. Experimentally, the best
heuristic for the Canadian Hansards task is
the intersection. For the Verbmobil task, the
refined method of (Och and Ney, 2003) is
used. The results are summarized in Table 5.
We see that both the linear and the loglinear
lexicon symmetrization methods yield an im-
provement with respect to the alignment error
rate. For the Verbmobil task, the improve-
ment with the loglinear interpolation is sta-
tistically significant at the 99% level. For the
Canadian Hansards task, both lexicon sym-
metrization methods result in statistically sig-
nificant improvements at the 95% level. Addi-
tionally, we observe that precision and recall
are more balanced for the symmetrized lexicon
variants, especially for the Canadian Hansards
Table 6: Effect of smoothing the lexicon prob-
abilities on the alignment performance for the
Verbmobil task (S?T: source-to-target direc-
tion, smooth English; T?S: target-to-source
direction, smooth German; all numbers in per-
cent).
S?T T?S
Pre. Rec. AER Pre. Rec. AER
Base 93.5 95.3 5.7 91.4 88.7 9.9
smooth 94.8 94.8 5.2 93.4 88.2 9.1
task.
5.5 Lexicon Smoothing
In Table 6, we present the results for the lex-
icon smoothing as described in Section 4 on
the Verbmobil corpus2. As expected, a no-
table improvement in the AER is reached if
the lexicon smoothing is performed for Ger-
man (i.e. for the target-to-source direction),
because many full-form words with the same
base form are present in this language. These
improvements are statistically significant at
the 95% level.
6 Related Work
The popular IBM models for statistical ma-
chine translation are described in (Brown et
al., 1993). The HMM-based alignment model
was introduced in (Vogel et al, 1996). A
good overview of these models is given in
(Och and Ney, 2003). In that article Model
6 is introduced as the loglinear interpolation
of the other models. Additionally, state-of-
the-art results are presented for the Verbmo-
bil task and the Canadian Hansards task for
various configurations. Therefore, we chose
them as baseline. Compared to our work,
these publications kept the training of the
two translation directions strictly separate
whereas we integrate both directions into one
symmetrized training. Additional linguistic
knowledge sources such as dependency trees
or parse trees were used in (Cherry and Lin,
2003) and (Gildea, 2003). In (Cherry and
Lin, 2003) a probability model Pr(aJ1 |fJ1 , eI1) is
used, which is symmetric per definition. Bilin-
gual bracketing methods were used to produce
a word alignment in (Wu, 1997). (Melamed,
2000) uses an alignment model that enforces
one-to-one alignments for nonempty words. In
2The base forms were determined using LingSoft
tools.
Table 5: Effect of different lexicon symmetrization methods on alignment performance for the
generalized alignments for the Verbmobil task and the Canadian Hansards task.
task: Verbmobil Canadian Hansards
Precision[%] Recall[%] AER[%] Precision[%] Recall[%] AER[%]
Base 93.3 96.0 5.5 96.6 86.0 8.2
Lin. 96.1 94.0 4.9 95.2 88.5 7.7
Loglin. 95.2 95.3 4.7 93.6 90.8 7.5
(Toutanova et al, 2002), extensions to the
HMM-based alignment model are presented.
7 Conclusions
We have addressed the task of automatically
generating word alignments for bilingual cor-
pora. This problem is of great importance for
many tasks in natural language processing, es-
pecially in the field of machine translation.
We have presented lexicon symmetrization
methods for statistical alignment models that
are trained using the EM algorithm, in par-
ticular the five IBM models, the HMM and
Model 6. We have evaluated these meth-
ods on the Verbmobil task and the Cana-
dian Hansards task and compared our results
to the state-of-the-art system of (Och and
Ney, 2003). We have shown that both the
linear and the loglinear interpolation of lexi-
con counts after each iteration of the EM al-
gorithm result in statistically significant im-
provements of the alignment quality. For the
Canadian Hansards task, the AER improved
by about 30% relative; for the Verbmobil task
the improvement was about 25% relative.
Additionally, we have described lexicon
smoothing using the word base forms. Es-
pecially for highly inflected languages such as
German, this smoothing resulted in statisti-
cally significant improvements.
In the future, we plan to optimize the inter-
polation weights to balance the two transla-
tion directions. We will also investigate the
possibility of generating directly an uncon-
strained alignment based on the symmetrized
lexicon probabilities.
Acknowledgment
This work has been partially funded by the
EU project LC-Star, IST-2001-32216.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra,
and R. L. Mercer. 1993. The mathematics of
statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?
311, June.
C. Cherry and D. Lin. 2003. A probability model
to improve word alignment. In Proc. of the 41th
Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 88?95, Sap-
poro, Japan, July.
D. Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proc. of the 41th An-
nual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 80?87, Sapporo,
Japan, July.
I. D. Melamed. 2000. Models of translational
equivalence among words. Computational Lin-
guistics, 26(2):221?249.
H. Ney, S. Martin, and F. Wessel. 1997. Statisti-
cal language modeling using leaving-one-out. In
S. Young and G. Bloothooft, editors, Corpus-
Based Methods in Language and Speech Process-
ing, pages 174?207. Kluwer.
F. J. Och and H. Ney. 2000. Improved statistical
alignment models. In Proc. of the 38th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 440?447, Hong Kong,
October.
F. J. Och and H. Ney. 2003. A systematic com-
parison of various statistical alignment models.
Computational Linguistics, 29(1):19?51, March.
K. Toutanova, H. T. Ilhan, and C. D. Manning.
2002. Extensions to hmm-based statistical word
alignment models. In Proc. Conf. on Empirical
Methods for Natural Language Processing, pages
87?94, Philadelphia, PA, July.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation.
In COLING ?96: The 16th Int. Conf. on Com-
putational Linguistics, pages 836?841, Copen-
hagen, Denmark, August.
W. Wahlster, editor. 2000. Verbmobil: Founda-
tions of speech-to-speech translations. Springer
Verlag, Berlin, Germany, July.
D. Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?
403, September.
Reordering Constraints for Phrase-Based
Statistical Machine Translation
Richard Zens1, Hermann Ney1, Taro Watanabe2 and Eiichiro Sumita2
1Lehrstuhl fu?r Informatik VI 2 Spoken Language Translation Research Laboratories
Computer Science Department ATR
RWTH Aachen University, Germany Kyoto, Japan
{zens,ney}@cs.rwth-aachen.de {watanabe,sumita}@slt.atr.co.jp
Abstract
In statistical machine translation, the gen-
eration of a translation hypothesis is com-
putationally expensive. If arbitrary re-
orderings are permitted, the search prob-
lem is NP-hard. On the other hand,
if we restrict the possible reorderings
in an appropriate way, we obtain a
polynomial-time search algorithm. We in-
vestigate different reordering constraints
for phrase-based statistical machine trans-
lation, namely the IBM constraints and
the ITG constraints. We present effi-
cient dynamic programming algorithms
for both constraints. We evaluate the con-
straints with respect to translation quality
on two Japanese?English tasks. We show
that the reordering constraints improve
translation quality compared to an un-
constrained search that permits arbitrary
phrase reorderings. The ITG constraints
preform best on both tasks and yield sta-
tistically significant improvements com-
pared to the unconstrained search.
1 Introduction
In statistical machine translation, we are given
a source language (?French?) sentence fJ1 =f1 . . . fj . . . fJ , which is to be translated into
a target language (?English?) sentence eI1 =e1 . . . ei . . . eI . Among all possible target lan-
guage sentences, we will choose the sentence
with the highest probability:
e?I1 = argmax
eI1
{Pr(eI1|fJ1 )
}
= argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)
}
This decomposition into two knowledge
sources is known as the source-channel ap-
proach to statistical machine translation
(Brown et al, 1990). It allows an independent
modeling of target language model Pr(eI1) and
translation model Pr(fJ1 |eI1). The target lan-guage model describes the well-formedness of
the target language sentence. The translation
model links the source language sentence to
the target language sentence. It can be fur-
ther decomposed into alignment and lexicon
model. The argmax operation denotes the
search problem, i.e. the generation of the out-
put sentence in the target language. We have
to maximize over all possible target language
sentences.
An alternative to the classical source-
channel approach is the direct modeling of the
posterior probability Pr(eI1|fJ1 ). Using a log-linear model (Och and Ney, 2002), we obtain:
Pr(eI1|fJ1 ) = exp
( M?
m=1
?mhm(eI1, fJ1 )
)
? Z(fJ1 )
Here, Z(fJ1 ) denotes the appropriate normal-ization constant. As a decision rule, we obtain:
e?I1 = argmax
eI1
{ M?
m=1
?mhm(eI1, fJ1 )
}
This approach is a generalization of the
source-channel approach. It has the advan-
tage that additional models or feature func-
tions can be easily integrated into the over-
all system. The model scaling factors ?M1 aretrained according to the maximum entropy
principle, e.g. using the GIS algorithm. Al-
ternatively, one can train them with respect
to the final translation quality measured by
some error criterion (Och, 2003).
In this paper, we will investigate the re-
ordering problem for phrase-based translation
approaches. As the word order in source and
target language may differ, the search algo-
rithm has to allow certain reorderings. If arbi-
trary reorderings are allowed, the search prob-
lem is NP-hard (Knight, 1999). To obtain an
efficient search algorithm, we can either re-
strict the possible reorderings or we have to
use an approximation algorithm. Note that in
the latter case we cannot guarantee to find an
optimal solution.
The remaining part of this work is struc-
tured as follows: in the next section, we
will review the baseline translation system,
namely the alignment template approach. Af-
terward, we will describe different reordering
constraints. We will begin with the IBM con-
straints for phrase-based translation. Then,
we will describe constraints based on inver-
sion transduction grammars (ITG). In the fol-
lowing, we will call these the ITG constraints.
In Section 4, we will present results for two
Japanese?English translation tasks.
2 Alignment Template Approach
In this section, we give a brief description of
the translation system, namely the alignment
template approach. The key elements of this
translation approach (Och et al, 1999) are the
alignment templates. These are pairs of source
and target language phrases with an alignment
within the phrases. The alignment templates
are build at the level of word classes. This
improves the generalization capability of the
alignment templates.
We use maximum entropy to train the
model scaling factors (Och and Ney, 2002).
As feature functions we use a phrase transla-
tion model as well as a word translation model.
Additionally, we use two language model fea-
ture functions: a word-based trigram model
and a class-based five-gram model. Further-
more, we use two heuristics, namely the word
penalty and the alignment template penalty.
To model the alignment template reorderings,
we use a feature function that penalizes re-
orderings linear in the jump width.
A dynamic programming beam search al-
gorithm is used to generate the translation
hypothesis with maximum probability. This
search algorithm allows for arbitrary reorder-
ings at the level of alignment templates.
Within the alignment templates, the reorder-
ing is learned in training and kept fix during
the search process. There are no constraints
on the reorderings within the alignment tem-
plates.
This is only a brief description of the align-
ment template approach. For further details,
see (Och et al, 1999; Och and Ney, 2002).
3 Reordering Constraints
Although unconstrained reordering looks per-
fect from a theoretical point of view, we find
that in practice constrained reordering shows
J
uncovered position
covered position
uncovered position for extension
1 j
Figure 1: Illustration of the IBM constraints
with k = 3, i.e. up to three positions may be
skipped.
better performance. The possible advantages
of reordering constraints are:
1. The search problem is simplified. As a
result there are fewer search errors.
2. Unconstrained reordering is only helpful
if we are able to estimate the reorder-
ing probabilities reliably, which is unfor-
tunately not the case.
In this section, we will describe two variants
of reordering constraints. The first constraints
are based on the IBM constraints for single-
word based translation models. The second
constraints are based on ITGs. In the follow-
ing, we will use the term ?phrase? to mean ei-
ther a sequence of words or a sequence of word
classes as used in the alignment templates.
3.1 IBM Constraints
In this section, we describe restrictions on the
phrase reordering in spirit of the IBM con-
straints (Berger et al, 1996).
First, we briefly review the IBM constraints
at the word level. The target sentence is pro-
duced word by word. We keep a coverage vec-
tor to mark the already translated (covered)
source positions. The next target word has to
be the translation of one of the first k uncov-
ered, i.e. not translated, source positions. The
IBM constraints are illustrated in Figure 1.
For further details see e.g. (Tillmann and Ney,
2003).
For the phrase-based translation approach,
we use the same idea. The target sentence is
produced phrase by phrase. Now, we allow
skipping of up to k phrases. If we set k = 0,
we obtain a search that is monotone at the
phrase level as a special case.
The search problem can be solved using dy-
namic programming. We define a auxiliary
function Q(j, S, e). Here, the source position
j is the first unprocessed source position; with
unprocessed, we mean this source position is
neither translated nor skipped. We use the
set S = {(jn, ln)|n = 1, ..., N} to keep track
of the skipped source phrases with lengths ln
and starting positions jn. We show the formu-
lae for a bigram language model and use the
target language word e to keep track of the
language model history. The symbol $ is used
to mark the sentence start and the sentence
end. The extension to higher-order n-gram
language models is straightforward. We use
M to denote the maximum phrase length in
the source language. We obtain the following
dynamic programming equations:
Q(1, ?, $) = 1
Q(j, S, e) = max
{
max
e?,e?
{
max
j?M?j?<j
Q(j?, S, e?) ? p(f j?1j? |e?) ? p(e?|e?),
max
(j?,l)?S?
S=S?\{(j?,l)}
Q(j, S?, e?) ? p(f j?+l?1j? |e?) ? p(e?|e?)
}
,
max
j?M?j?<j
S?:S=S??{(j?,j?j?)}?|S?|<k
Q(j?, S?, e)
}
Q(J + 2, ?, $) = maxe Q(J + 1, ?, e) ? p($|e)
In the recursion step, we have distinguished
three cases: in the first case, we translate the
next source phrase. This is the same expan-
sion that is done in monotone search. In the
second case, we translate a previously skipped
phrase and in the third case we skip a source
phrase. For notational convenience, we have
omitted one constraint in the preceding equa-
tions: the final word of the target phrase e? is
the new language model state e (using a bi-
gram language model).
Now, we analyze the complexity of this al-
gorithm. Let E denote the vocabulary size of
the target language and let E? denote the max-
imum number of phrase translation candidates
for a given source phrase. Then, J ?(J ?M)k ?E
is an upper bound for the size of the Q-table.
Once we have fixed a specific element of this
table, the maximization steps can be done in
O(E ? E? ? (M + k ? 1) + (k ? 1)). There-
fore, the complexity of this algorithm is in
O(J ?(J ?M)k ?E ?(E ?E? ?(M+k?1)+(k?1))).
Assuming k < M , this can be simplified to
O((J ?M)k+1 ?E2 ? E?). As already mentioned,
source positions
ta
rg
et
 p
os
it
io
ns
without inversion with inversion
source positions
ta
rg
et
 p
os
it
io
ns
Figure 2: Illustration of monotone and
inverted concatenation of two consecutive
blocks.
setting k = 0 results in a search algorithm that
is monotone at the phrase level.
3.2 ITG Constraints
In this section, we describe the ITG con-
straints (Wu, 1995; Wu, 1997). Here, we inter-
pret the input sentence as a sequence of blocks.
In the beginning, each alignment template is a
block of its own. Then, the reordering process
can be interpreted as follows: we select two
consecutive blocks and merge them to a single
block by choosing between two options: either
keep the target phrases in monotone order or
invert the order. This idea is illustrated in Fig-
ure 2. The dark boxes represent the two blocks
to be merged. Once two blocks are merged,
they are treated as a single block and they can
be only merged further as a whole. It is not
allowed to merge one of the subblocks again.
3.2.1 Dynamic Programming Algorithm
The ITG constraints allow for a polynomial-
time search algorithm. It is based on the fol-
lowing dynamic programming recursion equa-
tions. During the search a table Qjl,jr,eb,etis constructed. Here, Qjl,jr,eb,et denotes theprobability of the best hypothesis translating
the source words from position jl (left) to po-
sition jr (right) which begins with the target
language word eb (bottom) and ends with the
word et (top). This is illustrated in Figure 3.
The initialization is done with the phrase-
based model described in Section 2. We in-
troduce a new parameter pm (m=? monotone),
which denotes the probability of a monotone
combination of two partial hypotheses. Here,
we formulate the recursion equation for a bi-
gram language model, but of course, the same
method can also be applied for a trigram lan-
jl jr
e b
et
Figure 3: Illustration of the Q-table.
guage model.
Qjl,jr,eb,et =
max
jl?k<jr,
e?,e??
{
Q0jl,jr,eb,et ,
Qjl,k,eb,e? ?Qk+1,jr,e??,et ? p(e??|e?) ? pm,
Qk+1,jr,eb,e? ?Qjl,k,e??,et ? p(e??|e?) ? (1? pm)
}
The resulting algorithm is similar to the CYK-
parsing algorithm. It has a worst-case com-
plexity of O(J3 ?E4). Here, J is the length of
the source sentence and E is the vocabulary
size of the target language.
3.2.2 Beam Search Algorithm
For the ITG constraints a dynamic program-
ming search algorithm exists as described in
the previous section. It would be more prac-
tical with respect to language model recom-
bination to have an algorithm that generates
the target sentence word by word or phrase
by phrase. The idea is to start with the beam
search decoder for unconstrained search and
modify it in such a way that it will produce
only reorderings that do not violate the ITG
constraints. Now, we describe one way to ob-
tain such a decoder. It has been pointed out
in (Zens and Ney, 2003) that the ITG con-
straints can be characterized as follows: a re-
ordering violates the ITG constraints if and
only if it contains (3, 1, 4, 2) or (2, 4, 1, 3) as
a subsequence. This means, if we select four
columns and the corresponding rows from the
alignment matrix and we obtain one of the two
patterns illustrated in Figure 4, this reordering
cannot be generated with the ITG constraints.
Now, we have to modify the beam search
decoder such that it cannot produce these two
patterns. We implement this in the follow-
ing way. During the search, we have a cover-
age vector cov of the source sentence available
for each partial hypothesis. A coverage vec-
1
2
3
4
a b c d
1
2
3
4
a b c d
Figure 4: Illustration of the two reordering
patterns that violate the ITG constraints.
tor is a binary vector marking the source sen-
tence words that have already been translated
(covered). Additionally, we know the current
source sentence position jc and a candidate
source sentence position jn to be translated
next.
To avoid the patterns in Figure 4, we have
to constrain the placement of the third phrase,
because once we have placed the first three
phrases we also have determined the position
of the fourth phrase as the remaining uncov-
ered position. Thus, we check the following
constraints:
case a) jn < jc (1)
?jn < j < jc : cov[j] ? cov[j + 1]
case b) jc < jn (2)
?jc < j < jn : cov[j] ? cov[j ? 1]
The constraints in Equations 1 and 2 enforce
the following: imagine, we traverse the cover-
age vector cov from the current position jc to
the position to be translated next jn. Then,
it is not allowed to move from an uncovered
position to a covered one.
Now, we sketch the proof that these con-
straints are equivalent to the ITG constraints.
It is easy to see that the constraint in Equa-
tion 1 avoids the pattern on the left-hand side
in Figure 4. To be precise: after placing the
first two phrases at (b,1) and (d,2), it avoids
the placement of the third phrase at (a,3).
Similarly, the constraint in Equation 2 avoid
the pattern on the right-hand side in Fig-
ure 4. Therefore, if we enforce the constraints
in Equation 1 and Equation 2, we cannot vio-
late the ITG constraints.
We still have to show that we can gener-
ate all the reorderings that do not violate the
ITG constraints. Equivalently, we show that
any reordering that violates the constraints in
Equation 1 or Equation 2 will also violate the
ITG constraints. It is rather easy to see that
any reordering that violates the constraint in
Table 1: Statistics of the BTEC corpus.
Japanese English
train Sentences 152 K
Words 1 044 K 893 K
Vocabulary 17 047 12 020
dev sentences 500
words 3 361 2 858
test sentences 510
words 3 498 ?
Table 2: Statistics of the SLDB corpus.
Japanese English
train Sentences 15 K
Words 201 K 190 K
Vocabulary 4 757 3 663
test sentences 330
words 3 940 ?
Equation 1 will generate the pattern on the
left-hand side in Figure 4. The conditions to
violate Equation 1 are the following: the new
candidate position jn is to the left of the cur-
rent position jc, e.g. positions (a) and (d).
Somewhere in between there has to be an cov-
ered position j whose successor position j + 1
is uncovered, e.g. (b) and (c). Therefore, any
reordering that violates Equation 1 generates
the pattern on the left-hand side in Figure 4,
thus it violates the ITG constraints.
4 Results
4.1 Corpus Statistics
To investigate the effect of reordering con-
straints, we have chosen two Japanese?English
tasks, because the word order in Japanese and
English is rather different. The first task is the
Basic Travel Expression Corpus (BTEC) task
(Takezawa et al, 2002). The corpus statistics
are shown in Table 1. This corpus consists of
phrasebook entries.
The second task is the Spoken Language
DataBase (SLDB) task (Morimoto et al,
1994). This task consists of transcription of
spoken dialogs in the domain of hotel reser-
vation. Here, we use domain-specific training
data in addition to the BTEC corpus. The
corpus statistics of this additional corpus are
shown in Table 2. The development corpus is
the same for both tasks.
4.2 Evaluation Criteria
WER (word error rate). The WER is com-
puted as the minimum number of substitution,
insertion and deletion operations that have to
be performed to convert the generated sen-
tence into the reference sentence.
PER (position-independent word er-
ror rate). A shortcoming of the WER is that
it requires a perfect word order. The word or-
der of an acceptable sentence can be different
from that of the target sentence, so that the
WER measure alone could be misleading. The
PER compares the words in the two sentences
ignoring the word order.
BLEU. This score measures the precision
of unigrams, bigrams, trigrams and fourgrams
with respect to a reference translation with a
penalty for too short sentences (Papineni et
al., 2002). The BLEU score measures accu-
racy, i.e. large BLEU scores are better.
NIST. This score is similar to BLEU. It is
a weighted n-gram precision in combination
with a penalty for too short sentences (Dod-
dington, 2002). The NIST score measures ac-
curacy, i.e. large NIST scores are better.
Note that for each source sentence, we have
as many as 16 references available. We com-
pute all the preceding criteria with respect to
multiple references.
4.3 System Comparison
In Table 3 and Table 4, we show the trans-
lation results for the BTEC task. First, we
observe that the overall quality is rather high
on this task. The average length of the used
alignment templates is about five source words
in all systems. The monotone search (mon)
shows already good performance on short sen-
tences with less than 10 words. We conclude
that for short sentences the reordering is cap-
tured within the alignment templates. On the
other hand, the monotone search degrades for
long sentences with at least 10 words resulting
in a WER of 16.6% for these sentences.
We present the results for various nonmono-
tone search variants: the first one is with the
IBM constraints (skip) as described in Sec-
tion 3.1. We allow for skipping one or two
phrases. Our experiments showed that if we
set the maximum number of phrases to be
skipped to three or more the translation re-
sults are equivalent to the search without any
reordering constraints (free). The results for
the ITG constraints as described in Section 3.2
are also presented.
The unconstrained reorderings improve the
total translation quality down to a WER of
11.5%. We see that especially the long sen-
tences benefit from the reorderings resulting in
an improvement from 16.6% to 13.8%. Com-
paring the results for the free reorderings and
Table 3: Translation performance WER[%]
for the BTEC task (510 sentences). Sentence
lengths: short: < 10 words, long: ? 10 words;
times in milliseconds per sentence.
WER[%]
sentence length
reorder short long all time[ms]
mon 11.4 16.6 12.7 73
skip 1 10.8 13.5 11.4 134
2 10.8 13.4 11.4 169
free 10.8 13.8 11.5 194
ITG 10.6 12.2 11.0 164
Table 4: Translation performance for the
BTEC task (510 sentences).
error rates[%] accuracy measures
reorder WER PER BLEU[%] NIST
mon 12.7 10.6 86.8 14.14
skip 1 11.4 10.1 88.0 14.19
2 11.4 10.1 88.1 14.20
free 11.5 10.0 88.0 14.19
ITG 11.0 9.9 88.2 14.25
the ITG reorderings, we see that the ITG
system always outperforms the unconstrained
system. The improvement on the whole test
set is statistically significant at the 95% level.1
In Table 5 and Table 6, we show the re-
sults for the SLDB task. First, we observe
that the overall quality is lower than for the
BTEC task. The SLDB task is a spoken lan-
guage translation task and the training cor-
pus for spoken language is rather small. This
is also reflected in the average length of the
used alignment templates that is about three
source words compared to about five words for
the BTEC task.
The results on this task are similar to the
results on the BTEC task. Again, the ITG
constraints perform best. Here, the improve-
ment compared to the unconstrained search is
statistically significant at the 99% level. Com-
pared to the monotone search, the BLEU score
for the ITG constraints improves from 54.4%
to 57.1%.
5 Related Work
Recently, phrase-based translation approaches
became more and more popular. Marcu and
Wong (2002) present a joint probability model
for phrase-based translation. In (Koehn et
1The statistical significance test were done for the
WER using boostrap resampling.
Table 5: Translation performance WER[%]
for the SLDB task (330 sentences). Sentence
lengths: short: < 10 words, long: ? 10 words;
times in milliseconds per sentence.
WER[%]
sentence length
reorder short long all time[ms]
mon 32.0 52.6 48.1 911
skip 1 31.9 51.1 46.9 3 175
2 32.0 51.4 47.2 4 549
free 32.0 51.4 47.2 4 993
ITG 31.8 50.9 46.7 4 472
Table 6: Translation performance for the
SLDB task (330 sentences).
error rates[%] accuracy measures
reorder WER PER BLEU[%] NIST
mon 48.1 35.5 54.4 9.45
skip 1 46.9 35.0 56.8 9.71
2 47.2 35.1 57.1 9.74
free 47.2 34.9 57.1 9.75
ITG 46.7 34.6 57.1 9.76
al., 2003), various aspects of phrase-based
systems are compared, e.g. the phrase ex-
traction method, the underlying word align-
ment model, or the maximum phrase length.
In (Vogel, 2003), a phrase-based system is
used that allows reordering within a window
of up to three words. Improvements for a
Chinese?English task are reported compared
to a monotone search.
The ITG constraints were introduced in
(Wu, 1995). The applications were, for in-
stance, the segmentation of Chinese character
sequences into Chinese words and the bracket-
ing of the source sentence into sub-sentential
chunks. Investigations on the IBM constraints
(Berger et al, 1996) for single-word based sta-
tistical machine translation can be found e.g.
in (Tillmann and Ney, 2003). A comparison of
the ITG constraints and the IBM constraints
for single-word based models can be found in
(Zens and Ney, 2003). In this work, we investi-
gated these reordering constraints for phrase-
based statistical machine translation.
6 Conclusions
We have presented different reordering con-
straints for phrase-based statistical machine
translation, namely the IBM constraints and
the ITG constraints, as well as efficient dy-
namic programming algorithms. Transla-
tion results were reported for two Japanese?
English translation tasks. Both type of re-
ordering constraints resulted in improvements
compared to a monotone search. Restrict-
ing the reorderings according to the IBM con-
straints resulted already in a translation qual-
ity similar to an unconstrained search. The
translation results with the ITG constraints
even outperformed the unconstrained search
consistently on all error criteria. The improve-
ments have been found statistically significant.
The ITG constraints showed the best per-
formance on both tasks. Therefore we plan to
further improve this method. Currently, the
probability model for the ITG constraints is
very simple. More sophisticated models, such
as phrase dependent inversion probabilities,
might be promising.
Acknowledgments
This work was partially done at the Spoken
Language Translation Research Laboratories
(SLT) at the Advanced Telecommunication
Research Institute International (ATR), Ky-
oto, Japan. This research was supported in
part by the Telecommunications Advancement
Organization of Japan. This work has been
partially funded by the EU project PF-Star,
IST-2001-37599.
References
A. L. Berger, P. F. Brown, S. A. D. Pietra, V. J. D.
Pietra, J. R. Gillett, A. S. Kehler, and R. L.
Mercer. 1996. Language translation apparatus
and method of using context-based translation
models, United States patent, patent number
5510981, April.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J.
Della Pietra, F. Jelinek, J. D. Lafferty, R. L.
Mercer, and P. S. Roossin. 1990. A statisti-
cal approach to machine translation. Compu-
tational Linguistics, 16(2):79?85, June.
G. Doddington. 2002. Automatic evaluation of
machine translation quality using n-gram co-
occurrence statistics. In Proc. ARPA Workshop
on Human Language Technology.
K. Knight. 1999. Decoding complexity in word-
replacement translation models. Computational
Linguistics, 25(4):607?615, December.
P. Koehn, F. J. Och, and D. Marcu. 2003. Sta-
tistical phrase-based translation. In Proc. of
the Human Language Technology Conf. (HLT-
NAACL), pages 127?133, Edmonton, Canada,
May/June.
D. Marcu and W. Wong. 2002. A phrase-based,
joint probability model for statistical machine
translation. In Proc. Conf. on Empirical Meth-
ods for Natural Language Processing, pages 133?
139, Philadelphia, PA, July.
T. Morimoto, N. Uratani, T. Takezawa, O. Furuse,
Y. Sobashima, H. Iida, A. Nakamura, Y. Sag-
isaka, N. Higuchi, and Y. Yamazaki. 1994. A
speech and language database for speech trans-
lation research. In Proc. of the 3rd Int. Conf. on
Spoken Language Processing (ICSLP?94), pages
1791?1794, Yokohama, Japan, September.
F. J. Och and H. Ney. 2002. Discriminative train-
ing and maximum entropy models for statisti-
cal machine translation. In Proc. of the 40th
Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 295?302,
Philadelphia, PA, July.
F. J. Och, C. Tillmann, and H. Ney. 1999. Im-
proved alignment models for statistical machine
translation. In Proc. of the Joint SIGDAT Conf.
on Empirical Methods in Natural Language Pro-
cessing and Very Large Corpora, pages 20?28,
University of Maryland, College Park, MD,
June.
F. J. Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. of
the 41th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 160?
167, Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. of the 40th
Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 311?318,
Philadelphia, PA, July.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto,
and S. Yamamoto. 2002. Toward a broad-
coverage bilingual corpus for speech translation
of travel conversations in the real world. In
Proc. of the Third Int. Conf. on Language Re-
sources and Evaluation (LREC), pages 147?152,
Las Palmas, Spain, May.
C. Tillmann and H. Ney. 2003. Word reordering
and a dynamic programming beam search algo-
rithm for statistical machine translation. Com-
putational Linguistics, 29(1):97?133, March.
S. Vogel. 2003. SMT decoder dissected: Word re-
ordering. In Proc. of the Int. Conf. on Natural
Language Processing and Knowledge Engineer-
ing (NLP-KE), pages 561?566, Beijing, China,
October.
D. Wu. 1995. Stochastic inversion transduction
grammars, with application to segmentation,
bracketing, and alignment of parallel corpora.
In Proc. of the 14th International Joint Conf.
on Artificial Intelligence (IJCAI), pages 1328?
1334, Montreal, August.
D. Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?
403, September.
R. Zens and H. Ney. 2003. A comparative study
on reordering constraints in statistical machine
translation. In Proc. of the 41th Annual Meet-
ing of the Association for Computational Lin-
guistics (ACL), pages 144?151, Sapporo, Japan,
July.
Symmetric Word Alignments for Statistical Machine Translation
Evgeny Matusov and Richard Zens and Hermann Ney
Lehrstuhl fu?r Informatik VI, Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
{matusov,zens,ney}@cs.rwth-aachen.de
Abstract
In this paper, we address the word
alignment problem for statistical machine
translation. We aim at creating a sym-
metric word alignment allowing for reli-
able one-to-many and many-to-one word
relationships. We perform the iterative
alignment training in the source-to-target
and the target-to-source direction with
the well-known IBM and HMM alignment
models. Using these models, we robustly
estimate the local costs of aligning a source
word and a target word in each sentence
pair. Then, we use efficient graph algo-
rithms to determine the symmetric align-
ment with minimal total costs (i. e. max-
imal alignment probability). We evalu-
ate the automatic alignments created in
this way on the German?English Verb-
mobil task and the French?English Cana-
dian Hansards task. We show statistically
significant improvements of the alignment
quality compared to the best results re-
ported so far. On the Verbmobil task,
we achieve an improvement of more than
1% absolute over the baseline error rate of
4.7%.
1 Introduction
Word-aligned bilingual corpora provide im-
portant knowledge for many natural language
processing tasks, such as the extraction of
bilingual word or phrase lexica (Melamed,
2000; Och and Ney, 2000). The solutions of
these problems depend heavily on the quality
of the word alignment (Och and Ney, 2000).
Word alignment models were first introduced
in statistical machine translation (Brown et
al., 1993). An alignment describes a mapping
from source sentence words to target sentence
words.
Using the IBM translation models IBM-1
to IBM-5 (Brown et al, 1993), as well as
the Hidden-Markov alignment model (Vogel et
al., 1996), we can produce alignments of good
quality. However, all these models constrain
the alignments so that a source word can be
aligned to at most one target word. This con-
straint is useful to reduce the computational
complexity of the model training, but makes
it hard to align phrases in the target lan-
guage (English) such as ?the day after tomor-
row? to one word in the source language (Ger-
man) ?u?bermorgen?. We will present a word
alignment algorithm which avoids this con-
straint and produces symmetric word align-
ments. This algorithm considers the align-
ment problem as a task of finding the edge
cover with minimal costs in a bipartite graph.
The parameters of the IBM models and HMM,
in particular the state occupation probabili-
ties, will be used to determine the costs of
aligning a specific source word to a target
word.
We will evaluate the suggested alignment
methods on the German?English Verbmo-
bil task and the French?English Canadian
Hansards task. We will show statistically sig-
nificant improvements compared to state-of-
the-art results in (Och and Ney, 2003).
2 Statistical Word Alignment Models
In this section, we will give an overview of
the commonly used statistical word alignment
techniques. They are based on the source-
channel approach to statistical machine trans-
lation (Brown et al, 1993). We are given
a source language sentence fJ1 := f1...fj ...fJwhich has to be translated into a target lan-
guage sentence eI1 := e1...ei...eI . Among allpossible target language sentences, we will
choose the sentence with the highest proba-
bility:
e?I1 = argmax
eI1
{Pr(eI1|fJ1 )
}
= argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)
}
This decomposition into two knowledge
sources allows for an independent modeling of
target language model Pr(eI1) and translation
model Pr(fJ1 |eI1). Into the translation model,the word alignment A is introduced as a hid-
den variable:
Pr(fJ1 |eI1) =
?
A
Pr(fJ1 , A|eI1)
Usually, the alignment is restricted in the
sense that each source word is aligned to at
most one target word, i.e. A = aJ1 . The align-ment may contain the connection aj = 0 with
the ?empty? word e0 to account for source sen-
tence words that are not aligned to any tar-
get word at all. A detailed description of the
popular translation/alignment models IBM-1
to IBM-5 (Brown et al, 1993), as well as the
Hidden-Markov alignment model (HMM) (Vo-
gel et al, 1996) can be found in (Och and Ney,
2003). Model 6 is a loglinear combination of
the IBM-4, IBM-1, and the HMM alignment
models.
A Viterbi alignment A? of a specific model is
an alignment for which the following equation
holds:
A? = argmax
A
{Pr(fJ1 , A|eI1)
} .
3 State Occupation Probabilities
The training of all alignment models is done
using the EM-algorithm. In the E-step, the
counts for each sentence pair (fJ1 , eI1) are cal-culated. Here, we present this calculation on
the example of the HMM. For its lexicon pa-
rameters, the marginal probability of a target
word ei to occur at the target sentence posi-
tion i as the translation of the source word fj
at the source sentence position j is estimated
with the following sum:
pj(i, fJ1 |eI1) =
?
aJ1 :aj=i
Pr(fJ1 , aJ1 |eI1)
This value represents the likelihood of aligning
fj to ei via every possible alignment A = aJ1that includes the alignment connection aj = i.
By normalizing over the target sentence posi-
tions, we arrive at the state occupation proba-
bility :
pj(i|fJ1 , eI1) =
pj(i, fJ1 |eI1)
I?
i?=1
pj(i?, fJ1 |eI1)
In the M-step of the EM training, the state
occupation probabilities are aggregated for all
words in the source and target vocabularies
by taking the sum over all training sentence
pairs. After proper renormalization the lexi-
con probabilities p(f |e) are determined.
Similarly, the training can be performed
in the inverse (target-to-source) direction,
yielding the state occupation probabilities
pi(j|eI1, fJ1 ).The negated logarithms of the state occu-
pation probabilities
w(i, j; fJ1 , eI1) := ? log pj(i|fJ1 , eI1) (1)
can be viewed as costs of aligning the source
word fj with the target word ei. Thus, the
word alignment task can be formulated as the
task of finding a mapping between the source
and the target words, so that each source and
each target position is covered and the total
costs of the alignment are minimal.
Using state occupation probabilities for
word alignment modeling results in a num-
ber of advantages. First of all, in calculation
of these probabilities with the models IBM-1,
IBM-2 and HMM the EM-algorithm is per-
formed exact, i.e. the summation over all
alignments is efficiently performed in the E-
step. For the HMM this is done using the
Baum-Welch algorithm (Baum, 1972). So far,
an efficient algorithm to compute the sum over
all alignments in the fertility models IBM-3
to IBM-5 is not known. Therefore, this sum
is approximated using a subset of promising
alignments (Och and Ney, 2000). In both
cases, the resulting estimates are more pre-
cise than the ones obtained by the maximum
approximation, i. e. by considering only the
Viterbi alignment.
Instead of using the state occupation prob-
abilities from only one training direction as
costs (Equation 1), we can interpolate the
state occupation probabilities from the source-
to-target and the target-to-source training for
each pair (i,j) of positions in a sentence pair
(fJ1 , eI1). This will improve the estimation ofthe local alignment costs. Having such sym-
metrized costs, we can employ the graph align-
ment algorithms (cf. Section 4) to produce
reliable alignment connections which include
many-to-one and one-to-many alignment re-
lationships. The presence of both relation-
ship types characterizes a symmetric align-
ment that can potentially improve the trans-
lation results (Figure 1 shows an example of a
symmetric alignment).
Another important advantage is the effi-
ciency of the graph algorithms used to deter-
Figure 1: Example of a symmetric alignment
with one-to-many and many-to-one connec-
tions (Verbmobil task, spontaneous speech).
mine the final symmetric alignment. They will
be discussed in Section 4.
4 Alignment Algorithms
In this section, we describe the alignment ex-
traction algorithms. We assume that for each
sentence pair (fJ1 , eI1) we are given a cost ma-trix C.1 The elements of this matrix cij are
the local costs that result from aligning source
word fj to target word ei. For a given align-
ment A ? I ? J , we define the costs of this
alignment c(A) as the sum of the local costs
of all aligned word pairs:
c(A) =
?
(i,j)?A
cij (2)
Now, our task is to find the alignment with the
minimum costs. Obviously, the empty align-
ment has always costs of zero and would be op-
timal. To avoid this, we introduce additional
constraints. The first constraint is source sen-
tence coverage. Thus each source word has
to be aligned to at least one target word or
alternatively to the empty word. The second
constraint is target sentence coverage. Similar
to the source sentence coverage thus each tar-
get word is aligned to at least one source word
or the empty word.
Enforcing only the source sentence cover-
age, the minimum cost alignment is a mapping
from source positions j to target positions aj ,
including zero for the empty word. Each tar-
get position aj can be computed as:
aj = argmin
i
{cij}
This means, in each column we choose the
row with the minimum costs. This method re-
sembles the common IBM models in the sense
1For notational convenience, we omit the depen-
dency on the sentence pair (fJ1 , eI1) in this section.
that the IBM models are also a mapping from
source positions to target positions. There-
fore, this method is comparable to the IBM
models for the source-to-target direction. Sim-
ilarly, if we enforce only the target sentence
coverage, the minimum cost alignment is a
mapping from target positions i to source po-
sitions bi. Here, we have to choose in each
row the column with the minimum costs. The
complexity of these algorithms is in O(I ? J).
The algorithms for determining such a non-
symmetric alignment are rather simple. A
more interesting case arises, if we enforce both
constraints, i.e. each source word as well as
each target word has to be aligned at least
once. Even in this case, we can find the global
optimum in polynomial time.
The task is to find a symmetric alignment
A, for which the costs c(A) are minimal (Equa-
tion 2). This task is equivalent to finding
a minimum-weight edge cover (MWEC) in a
complete bipartite graph2. The two node
sets of this bipartite graph correspond to the
source sentence positions and the target sen-
tence positions, respectively. The costs of an
edge are the elements of the cost matrix C.
To solve the minimum-weight edge cover
problem, we reduce it to the maximum-weight
bipartite matching problem. As described
in (Keijsper and Pendavingh, 1998), this re-
duction is linear in the graph size. For the
maximum-weight bipartite matching problem,
well-known algorithm exist, e.g. the Hungar-
ian method. The complexity of this algorithm
is in O((I + J) ? I ? J). We will call the solu-
tion of the minimum-weight edge cover prob-
lem with the Hungarian method ?the MWEC
algorithm?. In contrary, we will refer to the al-
gorithm enforcing either source sentence cov-
erage or target sentence coverage as the one-
sided minimum-weight edge cover algorithm
(o-MWEC).
The cost matrix of a sentence pair (fJ1 , eI1)can be computed as a weighted linear interpo-
lation of various cost types hm:
cij =
M?
m=1
?m ? hm(i, j)
In our experiments, we will use the negated
logarithm of the state occupation probabilities
as described in Section 3. To obtain a more
symmetric estimate of the costs, we will inter-
polate both the source-to-target direction and
2An edge cover of G is a set of edges E? such that
each node of G is incident to at least one edge in E?.
the target-to-source direction (thus the state
occupation probabilities are interpolated log-
linearly). Because the alignments determined
in the source-to-target training may substan-
tially differ in quality from those produced in
the target-to-source training, we will use an
interpolation weight ?:
cij = ? ?w(i, j; fJ1 , eI1) + (1??) ?w(j, i; eI1, fJ1 )
(3)
Additional feature functions can be included
to compute cij ; for example, one could make
use of a bilingual word or phrase dictionary.
To apply the methods described in this sec-
tion, we made two assumptions: first, the costs
of an alignment can be computed as the sum
of local costs. Second, the features have to be
static in the sense that we have to fix the costs
before aligning any word. Therefore, we can-
not apply dynamic features such as the IBM-
4 distortion model in a straightforward way.
One way to overcome these restrictions lies in
using the state occupation probabilities; e.g.
for IBM-4, they contain the distortion model
to some extent.
5 Results
5.1 Evaluation Criterion
We use the same evaluation criterion as de-
scribed in (Och and Ney, 2000). We compare
the generated word alignment to a reference
alignment produced by human experts. The
annotation scheme explicitly takes the am-
biguity of the word alignment into account.
There are two different kinds of alignments:
sure alignments (S) which are used for unam-
biguous alignments and possible alignments
(P ) which are used for alignments that might
or might not exist. The P relation is used
especially to align words within idiomatic ex-
pressions and free translations. It is guaran-
teed that the sure alignments are a subset of
the possible alignments (S ? P ). The ob-
tained reference alignment may contain many-
to-one and one-to-many relationships.
The quality of an alignment A is computed
as appropriately redefined precision and recall
measures. Additionally, we use the alignment
error rate (AER), which is derived from the
well-known F-measure.
recall = |A ? S||S| , precision =
|A ? P |
|A|
AER(S, P ;A) = 1? |A ? S|+ |A ? P ||A|+ |S|
Table 1: Verbmobil task: corpus statistics.
Source/Target: German English
Train Sentences 34 446
Words 329 625 343 076
Vocabulary 5 936 3 505
Singletons 2 600 1 305
Dictionary Entries 4 404
Test Sentences 354
Words 3 233 3 109
S reference relations 2 559
P reference relations 4 596
Table 2: Canadian Hansards: corpus statistics.
Source/Target: French English
Train Sentences 128K
Words 2.12M 1.93M
Vocabulary 37 542 29 414
Singletons 12 986 9 572
Dictionary Entries 28 701
Test Sentences 500
Words 8 749 7 946
S reference relations 4 443
P reference relations 19 779
With these definitions a recall error can only
occur if a S(ure) alignment is not found and a
precision error can only occur if a found align-
ment is not even P (ossible).
5.2 Experimental Setup
We evaluated the presented lexicon sym-
metrization methods on the Verbmobil and
the Canadian Hansards task. The German?
English Verbmobil task (Wahlster, 2000) is a
speech translation task in the domain of ap-
pointment scheduling, travel planning and ho-
tel reservation. The French?English Canadian
Hansards task consists of the debates in the
Canadian Parliament.
The corpus statistics are shown in Table 1
and Table 2. The number of running words
and the vocabularies are based on full-form
words including punctuation marks. As in
(Och and Ney, 2003), the first 100 sentences
of the test corpus are used as a development
corpus to optimize model parameters that are
not trained via the EM algorithm, e.g. the
interpolation weights. The remaining part of
the test corpus is used to evaluate the models.
We use the same training schemes (model
sequences) as presented in (Och and Ney,
2003): 15H5334363 for the Verbmobil Task ,
i.e. 5 iteration of IBM-1, 5 iterations of the
HMM, 3 iteration of IBM-3, etc.; for the Cana-
dian Hansards task, we use 15H10334363. We
refer to these schemes as the Model 6 schemes.
For comparison, we also perform less sophisti-
cated trainings, to which we refer as the HMM
schemes (15H10 and 15H5, respectively), as
well as the IBM Model 4 schemes (15H103343
and 15H53343).
In all training schemes we use a conventional
dictionary (possibly containing phrases) as ad-
ditional training material. Because we use the
same training and testing conditions as (Och
and Ney, 2003), we will refer to the results pre-
sented in that article as the baseline results.
5.3 Non-symmetric Alignments
In the first experiments, we use the state oc-
cupation probabilities from only one transla-
tion direction to determine the word align-
ment. This allows for a fair comparison with
the Viterbi alignment computed as the result
of the training procedure. In the source-to-
target translation direction, we cannot esti-
mate the probability for the target words with
fertility zero and choose to set it to 0. In this
case, the minimum weight edge cover problem
is solved by the one-sided MWEC algorithm.
Like the Viterbi alignments, the alignments
produced by this algorithm satisfy the con-
straint that multiple source (target) words can
only be aligned to one target (source) word.
Tables 3 and 4 show the performance of
the one-sided MWEC algorithm in compar-
ison with the experiment reported by (Och
and Ney, 2003). We report not only the final
alignment error rates, but also the intermedi-
ate results for the HMM and IBM-4 training
schemes.
For IBM-3 to IBM-5, the Viterbi alignment
and a set of promising alignments are used
to determine the state occupation probabili-
ties. Consequently, we observe similar align-
ment quality when comparing the Viterbi and
the one-sided MWEC alignments.
We also evaluated the alignment quality af-
ter applying alignment generalization meth-
ods, i.e. we combine the alignment of both
translation directions. Experimentally, the
best generalization heuristic for the Canadian
Hansards task is the intersection of the source-
to-target and the target-to-source alignments.
For the Verbmobil task, the refined method
of (Och and Ney, 2003) is used. Again, we
observed similar alignment error rates when
merging either the Viterbi alignments or the
o-MWEC alignments.
Table 3: AER [%] for non-symmetric align-
ment methods and for various models (HMM,
IBM-4, Model 6) on the Canadian Hansards
task.
Alignment method HMM IBM4 M6
Baseline T?S 14.1 12.9 11.9
S?T 14.4 12.8 11.7
intersection 8.4 6.9 7.8
o-MWEC T?S 14.0 13.1 11.9
S?T 14.3 13.0 11.7
intersection 8.2 7.1 7.8
Table 4: AER [%] for non-symmetric align-
ment methods and for various models (HMM,
IBM-4, Model 6) on the Verbmobil task.
Alignment method HMM IBM4 M6
Baseline T?S 7.6 4.8 4.6
S?T 12.1 9.3 8.8
refined 7.1 4.7 4.7
o-MWEC T?S 7.3 4.8 4.5
S?T 12.0 9.3 8.5
refined 6.7 4.6 4.6
5.4 Symmetric Alignments
The heuristically generalized Viterbi align-
ments presented in the previous section can
potentially avoid the alignment constraints3.
However, the choice of the optimal general-
ization heuristic may depend on a particular
language pair and may require extensive man-
ual optimization. In contrast, the symmetric
MWEC algorithm is a systematic and theo-
retically well-founded approach to the task of
producing a symmetric alignment.
In the experiments with the symmetric
MWEC algorithm, the optimal interpolation
parameter ? (see Equation 3) for the Verbmo-
bil corpus was empirically determined as 0.8.
This shows that the model parameters can be
estimated more reliably in the direction from
German to English. In the inverse English-
to-German alignment training, the mappings
of many English words to one German word
are not allowed by the modeling constraints,
although such alignment mappings are signif-
icantly more frequent than mappings of many
German words to one English word.
The experimentally best interpolation pa-
rameter for the Canadian Hansards corpus was
? = 0.5. Thus the model parameters esti-
mated in the translation direction from French
to English are as reliable as the ones estimated
3Consequently, we will use them as baseline for the
experiments with symmetric alignments.
in the direction from English to French.
Lines 2a and 2b of Table 5 show the perfor-
mance of the MWEC algorithm. The align-
ment error rates are slightly lower if the HMM
or the full Model 6 training scheme is used
to train the state occupation probabilities on
the Canadian Hansards task. On the Verbmo-
bil task, the improvement is more significant,
yielding an alignment error rate of 4.1%.
Columns 4 and 5 of Table 5 contain the re-
sults of the experiments, in which the costs
cij were determined as the loglinear interpola-
tion of state occupation probabilities obtained
from the HMM training scheme with those
from IBM-4 (column 4) or from Model 6 (col-
umn 5). We set the interpolation parameters
for the two translation directions proportional
to the optimal values determined in the previ-
ous experiments. On the Verbmobil task, we
obtain a further improvement of 19% relative
over the baseline result reported in (Och and
Ney, 2003), reaching an AER as low as 3.8%.
The improvements of the alignment qual-
ity on the Canadian Hansards task are less
significant. The manual reference alignments
for this task contain many possible connec-
tions and only a few sure connections (cf. Ta-
ble 2). Thus automatic alignments consisting
of only a few reliable alignment points are fa-
vored. Because the differences in the number
of words and word order between French and
English are not as dramatic as e.g. between
German and English, the probability of the
empty word alignment is not very high. There-
fore, plenty of alignment points are produced
by the MWEC algorithm, resulting in a high
recall and low precision. To increase the preci-
sion, we replaced the empty word connection
costs (previously trained as state occupation
probabiliities using the EM algorithm) by the
global, word- and position-independent costs
depending only on one of the involved lan-
guages. The alignment error rates for these
experiments are given in lines 3a and 3b of Ta-
ble 5. The global empty word probability for
the Canadian Hansards task was empirically
set to 0.45 for French and for English, and,
for the Verbmobil task, to 0.6 for German and
0.1 for English. On the Canadian Hansards
task, we achieved further significant reduction
of the AER. In particular, we reached an AER
of 6.6% by performing only the HMM training.
In this case the effectiveness of the MWEC al-
gorithm is combined with the efficiency of the
HMM training, resulting in a fast and robust
alignment training procedure.
We also tested the more simple one-sided
MWEC algorithm. In contrast to the exper-
iments presented in Section 5.3, we used the
loglinear interpolated state occupation prob-
abilities (given by the Equation 3) as costs.
Thus, although the algorithm is not able to
produce a symmetric alignment, it operates
with symmetrized costs. In addition, we used
a combination heuristic to obtain a symmetric
alignment. The results of these experiments
are presented in Table 5, lines 4-6 a/b.
The performance of the one-sided MWEC
algorithm turned out to be quite robust on
both tasks. However, the o-MWEC align-
ments are not symmetric and the achieved low
AER depends heavily on the differences be-
tween the involved languages, which may fa-
vor many-to-one alignments in one translation
direction only. That is why on the Verbmobil
task, when determining the mininum weight in
each row for the translation direction from En-
glish to German, the alignment quality deteri-
orates, because the algorithm cannot produce
alignments which map several English words
to one German word (line 5b of Table 5).
Applying the generalization heuristics
(line 6a/b of Table 5), we achieve an AER of
6.0% on the Canadian Hansards task when
interpolating the state occupation probabil-
ities trained with the HMM and with the
IBM-4 schemes. On the Verbmobil task, the
interpolation of the HMM and the Model 6
schemes yields the best result of 3.7% AER.
In the latter experiment, we reached 97.3%
precision and 95.2% recall.
6 Related Work
A description of the IBM models for statistical
machine translation can be found in (Brown et
al., 1993). The HMM-based alignment model
was introduced in (Vogel et al, 1996). An
overview of these models is given in (Och and
Ney, 2003). That article also introduces the
Model 6; additionally, state-of-the-art results
are presented for the Verbmobil task and the
Canadian Hansards task for various configura-
tions. Therefore, we chose them as baseline.
Additional linguistic knowledge sources such
as dependeny trees or parse trees were used in
(Cherry and Lin, 2003; Gildea, 2003). Bilin-
gual bracketing methods were used to produce
a word alignment in (Wu, 1997). (Melamed,
2000) uses an alignment model that enforces
one-to-one alignments for nonempty words.
Table 5: AER[%] for different alignment symmetrization methods and for various alignment
models on the Canadian Hansards and the Verbmobil tasks (MWEC: minimum weight edge
cover, EW: empty word).
Symmetrization Method HMM IBM4 M6 HMM + IBM4 HMM + M6
Canadian 1a. Baseline (intersection) 8.4 6.9 7.8 ? ?
Hansards 2a. MWEC 7.9 9.3 7.5 8.2 7.4
3a. MWEC (global EW costs) 6.6 7.4 6.9 6.4 6.4
4a. o-MWEC T?S 7.3 7.9 7.4 6.7 7.0
5a. S?T 7.7 7.6 7.2 6.9 6.9
6a. S?T (intersection) 7.2 6.6 7.6 6.0 7.1
Symmetrization Method HMM IBM4 M6 HMM + IBM4 HMM + M6
Verbmobil 1b. Baseline (refined) 7.1 4.7 4.7 ? ?
2b. MWEC 6.4 4.4 4.1 4.3 3.8
3b. MWEC (global EW costs) 5.8 5.8 6.6 6.0 6.7
4b. o-MWEC T?S 6.8 4.4 4.1 4.5 3.7
5b. S?T 9.3 7.2 6.8 7.5 6.9
6b. S?T (refined) 6.7 4.3 4.1 4.6 3.7
7 Conclusions
In this paper, we addressed the task of au-
tomatically generating symmetric word align-
ments for statistical machine translation. We
exploited the state occupation probabilties de-
rived from the IBM and HMM translation
models. We used the negated logarithms of
these probabilities as local alignment costs and
reduced the word alignment problem to find-
ing an edge cover with minimal costs in a
bipartite graph. We presented efficient algo-
rithms for the solution of this problem. We
evaluated the performance of these algorithms
by comparing the alignment quality to man-
ual reference alignments. We showed that in-
terpolating the alignment costs of the source-
to-target and the target-to-source translation
directions can result in a significant improve-
ment of the alignment quality.
In the future, we plan to integrate the graph
algorithms into the iterative training proce-
dure. Investigating the usefulness of addi-
tional feature functions might be interesting
as well.
Acknowledgment
This work has been partially funded by the
EU project TransType 2, IST-2001-32091.
References
L. E. Baum. 1972. An inequality and associated
maximization technique in statistical estimation
for probabilistic functions of markov processes.
Inequalities, 3:1?8.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra,
and R. L. Mercer. 1993. The mathematics of
statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?
311, June.
C. Cherry and D. Lin. 2003. A probability model
to improve word alignment. In Proc. of the 41th
Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 88?95, Sap-
poro, Japan, July.
D. Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proc. of the 41th An-
nual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 80?87, Sapporo,
Japan, July.
J. Keijsper and R. Pendavingh. 1998. An effi-
cient algorithm for minimum-weight bibranch-
ing. Journal of Combinatorial Theory Series B,
73(2):130?145, July.
I. D. Melamed. 2000. Models of translational
equivalence among words. Computational Lin-
guistics, 26(2):221?249.
F. J. Och and H. Ney. 2000. Improved statistical
alignment models. In Proc. of the 38th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 440?447, Hong Kong,
October.
F. J. Och and H. Ney. 2003. A systematic com-
parison of various statistical alignment models.
Computational Linguistics, 29(1):19?51, March.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation.
In COLING ?96: The 16th Int. Conf. on Com-
putational Linguistics, pages 836?841, Copen-
hagen, Denmark, August.
W. Wahlster, editor. 2000. Verbmobil: Founda-
tions of speech-to-speech translations. Springer
Verlag, Berlin, Germany, July.
D. Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?
403, September.
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 524?532, Prague, June 2007. c?2007 Association for Computational Linguistics
A Systematic Comparison of Training Criteria
for Statistical Machine Translation
Richard Zens and Sas?a Hasan and Hermann Ney
Human Language Technology and Pattern Recognition
Lehrstuhl fu?r Informatik 6 ? Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
{zens,hasan,ney}@cs.rwth-aachen.de
Abstract
We address the problem of training the free
parameters of a statistical machine transla-
tion system. We show significant improve-
ments over a state-of-the-art minimum er-
ror rate training baseline on a large Chinese-
English translation task. We present novel
training criteria based on maximum likeli-
hood estimation and expected loss compu-
tation. Additionally, we compare the maxi-
mum a-posteriori decision rule and the min-
imum Bayes risk decision rule. We show
that, not only from a theoretical point of
view but also in terms of translation qual-
ity, the minimum Bayes risk decision rule is
preferable.
1 Introduction
Once we specified the Bayes decision rule for statis-
tical machine translation, we have to address three
problems (Ney, 2001):
? the search problem, i.e. how to find the best
translation candidate among all possible target
language sentences;
? the modeling problem, i.e. how to structure
the dependencies of source and target language
sentences;
? the training problem, i.e. how to estimate the
free parameters of the models from the training
data.
Here, the main focus is on the training problem. We
will compare a variety of training criteria for statisti-
cal machine translation. In particular, we are consid-
ering criteria for the log-linear parameters or model
scaling factors. We will introduce new training cri-
teria based on maximum likelihood estimation and
expected loss computation. We will show that some
achieve significantly better results than the standard
minimum error rate training of (Och, 2003).
Additionally, we will compare two decision rules,
the common maximum a-posteriori (MAP) deci-
sion rule and the minimum Bayes risk (MBR) de-
cision rule (Kumar and Byrne, 2004). We will show
that the minimum Bayes risk decision rule results
in better translation quality than the maximum a-
posteriori decision rule for several training criteria.
The remaining part of this paper is structured
as follows: first, we will describe related work in
Sec. 2. Then, we will briefly review the baseline
system, Bayes decision rule for statistical machine
translation and automatic evaluation metrics for ma-
chine translation in Sec. 3 and Sec. 4, respectively.
The novel training criteria are described in Sec. 5
and Sec. 6. Experimental results are reported in
Sec. 7 and conclusions are given in Sec. 8.
2 Related Work
The most common modeling approach in statistical
machine translation is to use a log-linear combina-
tion of several sub-models (Och and Ney, 2002). In
(Och and Ney, 2002), the log-linear weights were
tuned to maximize the mutual information criterion
(MMI). The current state-of-the-art is to optimize
these parameters with respect to the final evaluation
criterion; this is the so-called minimum error rate
training (Och, 2003).
Minimum Bayes risk decoding for machine trans-
524
lation was introduced in (Kumar and Byrne, 2004).
It was shown that MBR outperforms MAP decoding
for different evaluation criteria. Further experiments
using MBR for Bleu were performed in (Venugopal
et al, 2005; Ehling et al, 2007). Here, we will
present additional evidence that MBR decoding is
preferable over MAP decoding.
Tillmann and Zhang (2006) describe a percep-
tron style algorithm for training millions of features.
Here, we focus on the comparison of different train-
ing criteria.
Shen et al (2004) compared different algorithms
for tuning the log-linear weights in a reranking
framework and achieved results comparable to the
standard minimum error rate training.
An annealed minimum risk approach is presented
in (Smith and Eisner, 2006) which outperforms both
maximum likelihood and minimum error rate train-
ing. The parameters are estimated iteratively using
an annealing technique that minimizes the risk of an
expected-BLEU approximation, which is similar to
the one presented in this paper.
3 Baseline System
In statistical machine translation, we are given a
source language sentence fJ1 = f1 . . . fj . . . fJ ,
which is to be translated into a target language sen-
tence eI1 = e1 . . . ei . . . eI . Statistical decision the-
ory tells us that among all possible target language
sentences, we should choose the sentence which
minimizes the expected loss, also called Bayes risk:
e?I?1 = argmin
I,eI1
{
?
I?,e?I
?
1
Pr(e?I
?
1 |f
J
1 ) ? L(e
I
1, e
?I?
1 )
}
Here, L(eI1, e
?I?
1 ) denotes the loss function under
consideration. It measures the loss (or errors) of a
candidate translation eI1 assuming the correct trans-
lation is e?I
?
1 . In the following, we will call this de-
cision rule the MBR rule (Kumar and Byrne, 2004).
This decision rule is optimal in the sense that any
other decision rule will result (on average) in at least
as many errors as the MBR rule. Despite this, most
SMT systems do not use theMBR decision rule. The
most common approach is to use the maximum a-
posteriori (MAP) decision rule. Thus, we select the
hypothesis which maximizes the posterior probabil-
ity Pr(eI1|f
J
1 ):
e?I?1 = argmax
I,eI1
{
Pr(eI1|f
J
1 )
}
This is equivalent to the MBR decision rule under
a 0-1 loss function:
L0?1(e
I
1, e
?I?
1 ) =
{
0 if eI1 = e
?I?
1
1 else
Hence, the MAP decision rule is optimal for the
sentence or string error rate. It is not necessarily
optimal for other evaluation metrics such as the Bleu
score. One reason for the popularity of the MAP
decision rule might be that, compared to the MBR
rule, its computation is simpler.
The posterior probability Pr(eI1|f
J
1 ) is modeled
directly using a log-linear combination of several
models (Och and Ney, 2002):
p?M1 (e
I
1|f
J
1 ) =
exp
(?M
m=1 ?mhm(e
I
1, f
J
1 )
)
?
I?,e?I
?
1
exp
(?M
m=1 ?mhm(e
?I?
1 , f
J
1 )
)
(1)
This approach is a generalization of the source-
channel approach (Brown et al, 1990). It has the
advantage that additional models h(?) can be easily
integrated into the overall system.
The denominator represents a normalization fac-
tor that depends only on the source sentence fJ1 .
Therefore, we can omit it in case of the MAP de-
cision rule during the search process and obtain:
e?I?1 = argmax
I,eI1
{
M?
m=1
?mhm(e
I
1, f
J
1 )
}
Note that the denominator affects the results of the
MBR decision rule and, thus, cannot be omitted in
that case.
We use a state-of-the-art phrase-based translation
system similar to (Koehn, 2004; Mauser et al, 2006)
including the following models: an n-gram lan-
guage model, a phrase translation model and a word-
based lexicon model. The latter two models are used
for both directions: p(f |e) and p(e|f). Additionally,
we use a word penalty, phrase penalty and a distor-
tion penalty.
525
In the following, we will discuss the so-called
training problem (Ney, 2001): how do we train the
free parameters ?M1 of the model? The current
state-of-the-art is to use minimum error rate train-
ing (MERT) as described in (Och, 2003). The free
parameters are tuned to directly optimize the evalu-
ation criterion.
Except for the MERT, the training criteria that
we will consider are additive at the sentence-level.
Thus, the training problem for a development set
with S sentences can be formalized as:
??M1 = argmax
?M1
S?
s=1
F (?M1 , (e
I
1, f
J
1 )s) (2)
Here, F (?, ?) denotes the training criterion that we
would like to maximize and (eI1, f
J
1 )s denotes a sen-
tence pair in the development set. The optimization
is done using the Downhill Simplex algorithm from
the Numerical Recipes book (Press et al, 2002).
This is a general purpose optimization procedure
with the advantage that it does not require the deriva-
tive information. Before we will describe the details
of the different training criteria in Sec. 5 and 6, we
will discuss evaluation metrics in the following sec-
tion.
4 Evaluation Metrics
The automatic evaluation of machine translation is
currently an active research area. There exists a
variety of different metrics, e.g., word error rate,
position-independent word error rate, BLEU score
(Papineni et al, 2002), NIST score (Doddington,
2002), METEOR (Banerjee and Lavie, 2005), GTM
(Turian et al, 2003). Each of them has advantages
and shortcomings.
A popular metric for evaluating machine trans-
lation quality is the Bleu score (Papineni et al,
2002). It has certain shortcomings for compar-
ing different machine translation systems, especially
if comparing conceptually different systems, e.g.
phrase-based versus rule-based systems, as shown
in (Callison-Burch et al, 2006). On the other hand,
Callison-Burch concluded that the Bleu score is re-
liable for comparing variants of the same machine
translation system. As this is exactly what we will
need in our experiments and as Bleu is currently the
most popular metric, we have chosen it as our pri-
mary evaluation metric. Nevertheless, most of the
methods we will present can be easily adapted to
other automatic evaluation metrics.
In the following, we will briefly review the com-
putation of the Bleu score as some of the training
criteria are motivated by this. The Bleu score is a
combination of the geometric mean of n-gram pre-
cisions and a brevity penalty for too short translation
hypotheses. The Bleu score for a translation hypoth-
esis eI1 and a reference translation e?
I?
1 is computed as:
Bleu(eI1, e?
I?
1) = BP(I, I?) ?
4?
n=1
Precn(e
I
1, e?
I?
1)
1/4
with
BP(I, I?) =
{
1 if I ? I?
exp (1 ? I/I?) if I < I?
Precn(e
I
1, e?
I?
1) =
?
wn1
min{C(wn1 |e
I
1), C(w
n
1 |e?
I?
1)}
?
wn1
C(wn1 |e
I
1)
(3)
Here, C(wn1 |e
I
1) denotes the number of occur-
rences of an n-gram wn1 in a sentence e
I
1. The de-
nominators of the n-gram precisions evaluate to the
number of n-grams in the hypothesis, i.e. I ?n+1.
The n-gram counts for the Bleu score computa-
tion are usually collected over a whole document.
For our purposes, a sentence-level computation is
preferable. A problem with the sentence-level Bleu
score is that the score is zero if not at least one four-
gram matches. As we would like to avoid this prob-
lem, we use the smoothed sentence-level Bleu score
as suggested in (Lin and Och, 2004). Thus, we in-
crease the nominator and denominator of Precn(?, ?)
by one for n > 1. Note that we will use the
sentence-level Bleu score only during training. The
evaluation on the development and test sets will be
carried out using the standard Bleu score, i.e. at the
corpus level. As the MERT baseline does not require
the use of the sentence-level Bleu score, we use the
standard Bleu score for training the baseline system.
In the following, we will describe several crite-
ria for training the log-linear parameters ?M1 of our
model. For notational convenience, we assume that
there is just one reference translation. Nevertheless,
the methods can be easily adapted to the case of mul-
tiple references.
526
5 Maximum Likelihood
5.1 Sentence-Level Computation
A popular approach for training parameters is max-
imum likelihood estimation (MLE). Here, the goal
is to maximize the joint likelihood of the parameters
and the training data. For log-linear models, this re-
sults in a nice optimization criterion which is con-
vex and has a single optimum. It is equivalent to the
maximum mutual information (MMI) criterion. We
obtain the following training criterion:
FML?S(?
M
1 , (e
I
1, f
J
1 )) = log p?M1 (e
I
1|f
J
1 )
A problem that we often face in practice is that
the correct translation might not be among the can-
didates that our MT system produces. Therefore,
(Och and Ney, 2002; Och, 2003) defined the trans-
lation candidate with the minimum word-error rate
as pseudo reference translation. This has some bias
towards minimizing the word-error rate. Here, we
will use the translation candidate with the maximum
Bleu score as pseudo reference to bias the system
towards the Bleu score. However, as pointed out in
(Och, 2003), there is no reason to believe that the re-
sulting parameters are optimal with respect to trans-
lation quality measured with the Bleu score.
The goal of this sentence-level criterion is to dis-
criminate the single correct translation against all the
other ?incorrect? translations. This is problematic
as, even for human experts, it is very hard to define
a single best translation of a sentence. Furthermore,
the alternative target language sentences are not all
equally bad translations. Some of them might be
very close to the correct translation or even equiva-
lent whereas other sentences may have a completely
different meaning. The sentence-level MLE crite-
rion does not distinguish these cases and is therefore
a rather harsh training criterion.
5.2 N -gram Level Computation
As an alternative to the sentence-level MLE, we
performed experiments with an n-gram level MLE.
Here, we limit the order of the n-grams and assume
conditional independence among the n-gram prob-
abilities. We define the log-likelihood (LLH) of a
target language sentence eI1 given a source language
sentence fJ1 as:
FML?N (?
M
1 , (e
I
1, f
J
1 )) =
N?
n=1
?
wn1?e
I
1
log p?M1 (w
n
1 |f
J
1 )
Here, we use the n-gram posterior probability
p?M1 (w
n
1 |f
J
1 ) as defined in (Zens and Ney, 2006).
The n-gram posterior distribution is smoothed using
a uniform distribution over all possible n-grams.
p?M1 (w
n
1 |f
J
1 ) = ? ?
N?M1 (w
n
1 , f
J
1 )
?
w?n1
N?M1 (w
?n
1 , f
J
1 )
+ (1 ? ?) ?
1
V n
Here, V denotes the vocabulary size of the tar-
get language; thus, V n is the number of possi-
ble n-grams in the target language. We define
N?M1 (w
n
1 , f
J
1 ) as in (Zens and Ney, 2006):
N?M1 (w
n
1 , f
J
1 ) =
?
I,eI1
I?n+1?
i=1
p?M1 (e
I
1|f
J
1 )??(e
i+n?1
i , w
n
1 )
(4)
The sum over the target language sentences is lim-
ited to an N -best list, i.e. the N best translation
candidates according to the baseline model. In this
equation, we use the Kronecker function ?(?, ?), i.e.
the term ?(ei+n?1i , w
n
1 ) evaluates to one if and only
if the n-gram wn1 occurs in the target sentence e
I
1
starting at position i.
An advantage of the n-gram level computation
of the likelihood is that we do not have to define
pseudo-references as for the sentence-level MLE.
We can easily compute the likelihood for the human
reference translation. Furthermore, this criterion has
the desirable property that it takes partial correctness
into account, i.e. it is not as harsh as the sentence-
level criterion.
6 Expected Bleu Score
According to statistical decision theory, one should
maximize the expected gain (or equivalently mini-
mize the expected loss). For machine translation,
this means that we should optimize the expected
Bleu score, or any other preferred evaluation metric.
527
6.1 Sentence-Level Computation
The expected Bleu score for a given source sentence
fJ1 and a reference translation e?
I?
1 is defined as:
E[Bleu|e?I?1, f
J
1 ] =
?
eI1
Pr(eI1|f
J
1 ) ? Bleu(e
I
1, e?
I?
1)
Here, Pr(eI1|f
J
1 ) denotes the true probability dis-
tribution over the possible translations eI1 of the
given source sentence fJ1 . As this probability dis-
tribution is unknown, we approximate it using the
log-linear translation model p?M1 (e
I
1|f
J
1 ) from Eq. 1.
Furthermore, the computation of the expected Bleu
score involves a sum over all possible translations
eI1. This sum is approximated using an N -best list,
i.e. the N best translation hypotheses of the MT sys-
tem. Thus, the training criterion for the sentence-
level expected Bleu computation is:
FEB?S(?
M
1 , (e?
I?
1, f
J
1 )) =
?
eI1
p?M1 (e
I
1|f
J
1 )?Bleu(e
I
1, e?
I?
1)
An advantage of the sentence-level computation is
that it is straightforward to plug in alternative eval-
uation metrics instead of the Bleu score. Note that
the minimum error rate training (Och, 2003) uses
only the target sentence with the maximum posterior
probability whereas, here, the whole probability dis-
tribution is taken into account.
6.2 N -gram Level Computation
In this section, we describe a more fine grained com-
putation of the expected Bleu score by exploiting its
particular structure. Hence, this derivation is spe-
cific for the Bleu score but should be easily adapt-
able to other n-gram based metrics. We can rewrite
the expected Bleu score as:
E[Bleu|e?I?1, f
J
1 ] = E[BP|I? , f
J
1 ]
?
4?
n=1
E[Precn|e?I?1, f
J
1 ]
1/4
We assumed conditional independence between
the brevity penalty BP and the n-gram precisions
Precn. Note that although these independence as-
sumptions do not hold, the resulting parameters
might work well for translation. In fact, we will
show that this criterion is among the best perform-
ing ones in Sec. 7. This type of independence as-
sumption is typical within the naive Bayes classifier
framework. The resulting training criterion that we
will use in Eq. 2 is then:
FEB?N (?
M
1 , (e?
I?
1, f
J
1 )) = E?M1 [BP|I? , f
J
1 ]
?
4?
n=1
E?M1 [Precn|e?
I?
1, f
J
1 ]
1/4
We still have to define the estimators for the ex-
pected brevity penalty as well as the expected n-
gram precision:
E?M1 [BP|I? , f
J
1 ] =
?
I
BP(I, I?) ? p?M1 (I|f
J
1 )
E?M1 [Precn|e?
I?
1, f
J
1 ] = (5)
?
wn1
p?M1 (w
n
1 |f
J
1 )
?
c
min{c, C(wn1 |e?
I?
1)} ? p?M1 (c|w
n
1 , f
J
1 )
?
wn1
p?M1 (w
n
1 |f
J
1 )
?
c
c ? p?M1 (c|w
n
1 , f
J
1 )
Here, we use the sentence length posterior proba-
bility p?M1 (I|f
J
1 ) as defined in (Zens and Ney, 2006)
and the n-gram posterior probability p?M1 (w
n
1 |f
J
1 ) as
described in Sec. 5.2. Additionally, we predict the
number of occurrences c of an n-gram. This infor-
mation is necessary for the so-called clipping in the
Bleu score computation, i.e. the min operator in the
nominator of formulae Eq. 3 and Eq. 5. The denom-
inator of Eq. 5 is the expected number of n-grams in
the target sentence, whereas the nominator denotes
the expected number of correct n-grams.
To predict the number of occurrences within a
translation hypothesis, we use relative frequencies
smoothed with a Poisson distribution. The mean of
the Poisson distribution ?(wn1 , f
J
1 , ?
M
1 ) is chosen to
be the mean of the unsmoothed distribution.
p?M1 (c|w
n
1 , f
J
1 ) = ? ?
N?M1 (c, w
n
1 , f
J
1 )
N?M1 (w
n
1 , f
J
1 )
+ (1 ? ?) ?
?(wn1 , f
J
1 , ?
M
1 )
c ? e?c
c!
528
Table 1: Chinese-English TC-Star task: corpus
statistics.
Chinese English
Train Sentence pairs 8.3M
Running words 197M 238M
Vocabulary size 224K 389K
Dev Sentences 1 019 2 038
Running words 26K 51K
Eval 2006 Sentences 1 232 2 464
Running words 30K 62K
2007 Sentences 917 1 834
Running words 21K 45K
with
?(wn1 , f
J
1 , ?
M
1 ) =
?
c
c ?
N?M1 (c, w
n
1 , f
J
1 )
N?M1 (w
n
1 , f
J
1 )
Note that in case the mean ?(wn1 , f
J
1 , ?
M
1 ) is zero,
we do not need the distribution p?M1 (c|w
n
1 , f
J
1 ). The
smoothing parameters ? and ? are both set to 0.9.
7 Experimental Results
7.1 Task Description
We perform translation experiments on the Chinese-
English TC-Star task. This is a broadcast news
speech translation task used within the European
Union project TC-Star1. The bilingual training
data consists of virtually all publicly available LDC
Chinese-English corpora. The 6-gram language
model was trained on the English part of the bilin-
gual training data and additional monolingual En-
glish parts from the GigaWord corpus. We use the
modified Kneser-Ney discounting as implemented
in the SRILM toolkit (Stolcke, 2002).
Annual public evaluations are carried out for this
task within the TC-Star project. We will report re-
sults on manual transcriptions, i.e. the so-called ver-
batim condition, of the official evaluation test sets of
the years 2006 and 2007. There are two reference
translations available for the development and test
sets. The corpus statistics are shown in Table 1.
7.2 Translation Results
In Table 2, we present the translation results
for different training criteria for the development
1http://www.tc-star.org
set and the two blind test sets. The reported
case-sensitive Bleu scores are computed using
the mteval-v11b.pl2 tool using two reference
translations, i.e. BLEUr2n4c. Note that already the
baseline system (MERT-Bleu) would have achieved
the first rank in the official TC-Star evaluation 2006;
the best Bleu score in that evaluation was 16.1%.
The MBR hypotheses were generated using the
algorithm described in (Ehling et al, 2007) on a
10 000-best list.
On the development data, the MERT-Bleu
achieves the highest Bleu score. This seems reason-
able as it is the objective of this training criterion.
The maximum likelihood (MLE) criteria perform
somewhat worse under MAP decoding. Interest-
ingly, the MBR decoding can compensate this to
a large extent: all criteria achieve a Bleu score of
about 18.9% on the development set. The bene-
fits of MBR decoding become even more evident
on the two test sets. Here, the MAP results for the
sentence-level MLE criterion are rather poor com-
pared to the MERT-Bleu. Nevertheless, using MBR
decoding results in very similar Bleu scores for most
of the criteria on these two test sets. We can there-
fore support the claim of (Smith and Eisner, 2006)
that MBR tends to have better generalization capa-
bilities.
The n-gram level MLE criterion seems to perform
better than the sentence-level MLE criterion, espe-
cially on the test sets. The reasons might be that
there is no need for the use of pseudo references
as described in Sec. 5 and that partial correctness
is taken into account.
The best results are achieved using the expected
Bleu score criteria described in Sec. 6. Here, the sen-
tence level and n-gram level variants achieve more
or less the same results. The overall improvement
on the Eval?06 set is about 1.0% Bleu absolute for
MAP decoding and 0.9% for MBR decoding. On
the Eval?07 set, the improvements are even larger,
about 1.8% Bleu absolute for MAP and 1.1% Bleu
for MBR. All these improvements are statistically
significant at the 99% level using a pairwise signifi-
cance test3.
Given that currently the most popular approach is
to use MERT-Bleu MAP decoding, the overall im-
2http://www.nist.gov/speech/tests/mt/resources/scoring.htm
3The tool for computing the significance test was kindly pro-
vided by the National Research Council Canada.
529
Table 2: Translation results: Bleu scores [%] for the Chinese-English TC-Star task for various training
criteria (MERT: minimum error rate training; MLE: maximum likelihood estimation; E[Bleu]: expected
Bleu score) and the maximum a-posteriori (MAP) as well as the minimum Bayes risk (MBR) decision rule.
Development Eval?06 Eval?07
Decision Rule MAP MBR MAP MBR MAP MBR
Training Criterion MERT-Bleu (baseline) 19.5 19.4 16.7 17.2 22.2 23.0
MLE sentence-level 17.8 18.9 14.8 17.1 18.9 22.7
n-gram level 18.6 18.8 17.0 17.8 22.8 23.5
E[Bleu] sentence-level 19.1 18.9 17.5 18.1 23.5 24.1
n-gram level 18.6 18.8 17.7 17.6 24.0 24.0
provement is about 1.4% absolute for the Eval?06
set and 1.9% absolute on the Eval?07 set.
Note that the MBR decision rule almost always
outperforms theMAP decision rule. In the rare cases
where the MAP decision rule yields better results,
the difference in terms of Bleu score are small and
not statistically significant.
We also investigated the effect of the maximum
n-gram order for the n-gram level maximum like-
lihood estimation (MLE). The results are shown in
Figure 1. We observe an increase of the Bleu score
with increasing maximum n-gram order for the de-
velopment corpus. On the evaluation sets, however,
the maximum is achieved if the maximum n-gram
order is limited to four. This seems intuitive as the
Bleu score uses n-grams up to length four. However,
one should be careful here: the differences are rather
small, so it might be just statistical noise.
Some translation examples from the Eval?07 test
set are shown in Table 3 for different training criteria
under the maximum a-posteriori decision rule.
8 Conclusions
We have presented a systematic comparison of sev-
eral criteria for training the log-linear parameters of
a statistical machine translation system. Addition-
ally, we have compared the maximum a-posteriori
with the minimum Bayes risk decision rule.
We can conclude that the expected Bleu score
is not only a theoretically sound training criterion,
but also achieves the best results in terms of Bleu
score. The improvement over a state-of-the-art
MERT baseline is 1.3% Bleu absolute for the MAP
decision rule and 1.1% Bleu absolute for the MBR
decision rule for the large Chinese-English TC-Star
speech translation task.
1 2 3 4 5 6 7 8 9max. n-gram order
14
16
18
20
22
24
Bleu
 [%]
DevEval'06Eval'07
Figure 1: Effect of the maximum n-gram order on
the Bleu score for the n-gram level maximum like-
lihood estimation under the maximum a-posteriori
decision rule.
We presented two methods for computing the ex-
pected Bleu score: a sentence-level and an n-gram
level approach. Both yield similar results. We think
that the n-gram level computation has certain ad-
vantages: The n-gram posterior probabilities could
be computed from a word graph which would result
in more reliable estimates. Whether this pays off
in terms of translation quality is left open for future
work.
Another interesting result of our experiments is
that the MBR decision rule seems to be less affected
by sub-optimal parameter settings.
Although it is well-known that the MBR decision
rule is more appropriate than the MAP decision rule,
the latter is more popular in the SMT community
(and many other areas of natural language process-
ing). Our results show that it can be beneficial to
530
Table 3: Translation examples from the Eval?07 test set for different training criteria and the maximum a-
posteriori decision rule. (MERT: minimum error rate training, MLE-S: sentence-level maximum likelihood
estimation, E[Bleu]: sentence-level expected Bleu)
Criterion Translation
Reference 1 Saving Private Ryan ranks the third on the box office revenue list which is also a movie that is
possible to win an 1999 Oscar award
2 Saving Private Ryan ranked third in the box office income is likely to compete in the nineteen
ninety-nine Oscar Awards
MERT-Bleu Saving private Ryan in box office income is possible ranked third in 1999 Oscar a film
MLE-S Saving private Ryan box office revenue ranked third is possible in 1999 Oscar a film
E[Bleu]-S Saving private Ryan ranked third in the box office income is also likely to run for the 1999
Academy Awards a film
Reference 1 The following problem is whether people in countries like China and Japan and other countries
will choose Euros rather than US dollars in international business activities in the future
2 The next question is whether China or Japan or other countries will choose to use Euros instead
of US dollars when they conduct international business in the future
MERT-Bleu The next question is in China or Japan international business activities in the future they will not
use the Euro dollar
MLE-S The next question was either in China or Japan international business activities in the future they
will adopt the Euro instead of the dollar
E[Bleu]-S The next question was in China or Japan in the international business activities in the future they
will adopt the Euro instead of the US dollar
Reference 1 The Chairman of the European Commission Jacques Santer pointed out in this September that the
financial crisis that happened in Russia has not affected people?s confidence in adopting the Euro
2 European Commission President Jacques Santer pointed out in September this year that
Russia?s financial crisis did not shake people?s confidence for planning the use of the Euro
MERT-Bleu President of the European Commission Jacques Santer on September this year that the Russian
financial crisis has not shaken people ?s confidence in the introduction of the Euro
MLE-S President of the European Commission Jacques Santer September that the Russian financial crisis
has not affected people ?s confidence in the introduction of the Euro
E[Bleu]-S President of the European Commission Jacques Santer pointed out that Russia ?s financial crisis
last September has not shaken people ?s confidence in the introduction of the Euro
Reference 1 After many years of friction between Dutch and French speaking Belgians all of them now hope
to emphasize their European identities
2 After years of friction between Belgium?s Dutch-speaking and French-speaking people they now
all wish to emphasize their European identity
MERT-Bleu Belgium?s Dutch-speaking and French-speaking after many years of civil strife emphasized that
they now hope that Europeans
MLE-S Belgium?s Dutch-speaking and francophone after years of civil strife that they now hope that
Europeans
E[Bleu]-S Belgium?s Dutch-speaking and French-speaking after many years of civil strife it is now want
to emphasize their European identity
531
use the MBR decision rule. On the other hand, the
computation of the MBR hypotheses is more time
consuming. Therefore, it would be desirable to have
a more efficient algorithm for computing the MBR
hypotheses.
Acknowledgments
This material is partly based upon work supported
by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023,
and was partly funded by the European Union un-
der the integrated project TC-STAR (Technology
and Corpora for Speech to Speech Translation, IST-
2002-FP6-506738, http://www.tc-star.org).
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An au-
tomatic metric for MT evaluation with improved correlation
with human judgments. In Proc. Workshop on Intrinsic and
Extrinsic Evaluation Measures for MT and/or Summariza-
tion at the 43th Annual Meeting of the Association of Com-
putational Linguistics (ACL), pages 65?72, Ann Arbor, MI,
June.
Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J.
Della Pietra, Frederick Jelinek, John D. Lafferty, Robert L.
Mercer, and Paul S. Roossin. 1990. A statistical approach to
machine translation. Computational Linguistics, 16(2):79?
85, June.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn.
2006. Re-evaluating the role of BLEU in machine trans-
lation research. In Proc. 11th Conf. of the Europ. Chapter
of the Assoc. for Computational Linguistics (EACL), pages
249?256, Trento, Italy, April.
George Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statistics. In
Proc. ARPA Workshop on Human Language Technology.
Nicola Ehling, Richard Zens, and Hermann Ney. 2007. Mini-
mum Bayes risk decoding for BLEU. In Proc. 45th Annual
Meeting of the Assoc. for Computational Linguistics (ACL):
Poster Session, Prague, Czech Republic, June.
Philipp Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In Proc.
6th Conf. of the Assoc. for Machine Translation in the Amer-
icas (AMTA), pages 115?124, Washington DC, Septem-
ber/October.
Shankar Kumar and William Byrne. 2004. Minimum Bayes-
risk decoding for statistical machine translation. In Proc.
Human Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual Meet-
ing (HLT-NAACL), pages 169?176, Boston, MA, May.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a method
for evaluating automatic evaluation metrics for machine
translation. In Proc. COLING ?04: The 20th Int. Conf.
on Computational Linguistics, pages 501?507, Geneva,
Switzerland, August.
Arne Mauser, Richard Zens, Evgeny Matusov, Sas?a Hasan,
and Hermann Ney. 2006. The RWTH statistical machine
translation system for the IWSLT 2006 evaluation. In Proc.
Int. Workshop on Spoken Language Translation (IWSLT),
pages 103?110, Kyoto, Japan, November.
Hermann Ney. 2001. Stochastic modelling: from pattern
classification to language translation. In Proc. 39th Annual
Meeting of the Assoc. for Computational Linguistics (ACL):
Workshop on Data-Driven Machine Translation, pages 1?5,
Morristown, NJ, July.
Franz Josef Och and Hermann Ney. 2002. Discriminative train-
ing and maximum entropy models for statistical machine
translation. In Proc. 40th Annual Meeting of the Assoc. for
Computational Linguistics (ACL), pages 295?302, Philadel-
phia, PA, July.
Franz Josef Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. 41st Annual Meeting of the
Assoc. for Computational Linguistics (ACL), pages 160?167,
Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of ma-
chine translation. In Proc. 40th Annual Meeting of the As-
soc. for Computational Linguistics (ACL), pages 311?318,
Philadelphia, PA, July.
William H. Press, Saul A. Teukolsky, William T. Vetterling, and
Brian P. Flannery. 2002. Numerical Recipes in C++. Cam-
bridge University Press, Cambridge, UK.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004. Dis-
criminative reranking for machine translation. In Proc. Hu-
man Language Technology Conf. / North American Chapter
of the Assoc. for Computational Linguistics Annual Meeting
(HLT-NAACL), pages 177?184, Boston, MA, May.
David A. Smith and Jason Eisner. 2006. Minimum risk anneal-
ing for training log-linear models. In Proc. 21st Int. Conf.
on Computational Linguistics and 44th Annual Meeting of
the Assoc. for Computational Linguistics (COLING/ACL):
Poster Session, pages 787?794, Sydney, Australia, July.
Andreas Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Speech and Language
Processing (ICSLP), volume 2, pages 901?904, Denver, CO,
September.
Christoph Tillmann and Tong Zhang. 2006. A discriminative
global training algorithm for statistical MT. In Proc. 21st
Int. Conf. on Computational Linguistics and 44th Annual
Meeting of the Assoc. for Computational Linguistics (COL-
ING/ACL), pages 721?728, Sydney, Australia, July.
Joseph P. Turian, Luke Shen, and I. Dan Melamed. 2003. Eval-
uation of machine translation and its evaluation. Technical
Report Proteus technical report 03-005, Computer Science
Department, New York University.
Ashish Venugopal, Andreas Zollmann, and Alex Waibel. 2005.
Training and evaluating error minimization rules for statis-
tical machine translation. In Proc. 43rd Annual Meeting of
the Assoc. for Computational Linguistics (ACL): Workshop
on Building and Using Parallel Texts: Data-Driven Machine
Translation and Beyond, pages 208?215, Ann Arbor, MI,
June.
Richard Zens and Hermann Ney. 2006. N -gram posterior prob-
abilities for statistical machine translation. In Proc. Human
Language Technology Conf. / North American Chapter of the
Assoc. for Computational Linguistics Annual Meeting (HLT-
NAACL): Proc. Workshop on Statistical Machine Transla-
tion, pages 72?77, New York City, NY, June.
532
387
388
389
390
391
392
393
394
Improvements in Phrase-Based Statistical Machine Translation
Richard Zens and Hermann Ney
Chair of Computer Science VI
RWTH Aachen University
{zens,ney}@cs.rwth-aachen.de
Abstract
In statistical machine translation, the currently
best performing systems are based in some way
on phrases or word groups. We describe the
baseline phrase-based translation system and
various refinements. We describe a highly ef-
ficient monotone search algorithm with a com-
plexity linear in the input sentence length. We
present translation results for three tasks: Verb-
mobil, Xerox and the Canadian Hansards. For
the Xerox task, it takes less than 7 seconds to
translate the whole test set consisting of more
than 10K words. The translation results for
the Xerox and Canadian Hansards task are very
promising. The system even outperforms the
alignment template system.
1 Introduction
In statistical machine translation, we are given a source
language (?French?) sentence fJ1 = f1 . . . fj . . . fJ ,
which is to be translated into a target language (?English?)
sentence eI1 = e1 . . . ei . . . eI . Among all possible target
language sentences, we will choose the sentence with the
highest probability:
e?I1 = argmax
eI1
{Pr(eI1|fJ1 )
} (1)
= argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)
} (2)
The decomposition into two knowledge sources in Equa-
tion 2 is known as the source-channel approach to statisti-
cal machine translation (Brown et al, 1990). It allows an
independent modeling of target language model Pr(eI1)
and translation model Pr(fJ1 |eI1)1. The target language
1The notational convention will be as follows: we use the
symbol Pr(?) to denote general probability distributions with
(nearly) no specific assumptions. In contrast, for model-based
probability distributions, we use the generic symbol p(?).
model describes the well-formedness of the target lan-
guage sentence. The translation model links the source
language sentence to the target language sentence. It can
be further decomposed into alignment and lexicon model.
The argmax operation denotes the search problem, i.e.
the generation of the output sentence in the target lan-
guage. We have to maximize over all possible target lan-
guage sentences.
An alternative to the classical source-channel ap-
proach is the direct modeling of the posterior probabil-
ity Pr(eI1|fJ1 ). Using a log-linear model (Och and Ney,
2002), we obtain:
Pr(eI1|fJ1 ) = exp
( M?
m=1
?mhm(eI1, fJ1 )
)
? Z(fJ1 )
Here, Z(fJ1 ) denotes the appropriate normalization con-
stant. As a decision rule, we obtain:
e?I1 = argmax
eI1
{ M?
m=1
?mhm(eI1, fJ1 )
}
This approach is a generalization of the source-channel
approach. It has the advantage that additional models or
feature functions can be easily integrated into the overall
system. The model scaling factors ?M1 are trained accord-
ing to the maximum entropy principle, e.g. using the GIS
algorithm. Alternatively, one can train them with respect
to the final translation quality measured by some error
criterion (Och, 2003).
The remaining part of this work is structured as fol-
lows: in the next section, we will describe the base-
line phrase-based translation model and the extraction of
bilingual phrases. Then, we will describe refinements
of the baseline model. In Section 4, we will describe a
monotone search algorithm. Its complexity is linear in
the sentence length. The next section contains the statis-
tics of the corpora that were used. Then, we will inves-
tigate the degree of monotonicity and present the transla-
tion results for three tasks: Verbmobil, Xerox and Cana-
dian Hansards.
2 Phrase-Based Translation
2.1 Motivation
One major disadvantage of single-word based approaches
is that contextual information is not taken into account.
The lexicon probabilities are based only on single words.
For many words, the translation depends heavily on the
surrounding words. In the single-word based translation
approach, this disambiguation is addressed by the lan-
guage model only, which is often not capable of doing
this.
One way to incorporate the context into the translation
model is to learn translations for whole phrases instead
of single words. Here, a phrase is simply a sequence of
words. So, the basic idea of phrase-based translation is
to segment the given source sentence into phrases, then
translate each phrase and finally compose the target sen-
tence from these phrase translations.
2.2 Phrase Extraction
The system somehow has to learn which phrases are
translations of each other. Therefore, we use the follow-
ing approach: first, we train statistical alignment models
using GIZA++ and compute the Viterbi word alignment of
the training corpus. This is done for both translation di-
rections. We take the union of both alignments to obtain a
symmetrized word alignment matrix. This alignment ma-
trix is the starting point for the phrase extraction. The fol-
lowing criterion defines the set of bilingual phrases BP
of the sentence pair (fJ1 ; eI1) and the alignment matrix
A ? J ? I that is used in the translation system.
BP(fJ1 , eI1, A) =
{(
f j2j1 , ei2i1
)
:
?(j, i) ? A : j1 ? j ? j2 ? i1 ? i ? i2
??(j, i) ? A : j1 ? j ? j2 ? i1 ? i ? i2
}
This criterion is identical to the alignment template cri-
terion described in (Och et al, 1999). It means that two
phrases are considered to be translations of each other, if
the words are aligned only within the phrase pair and not
to words outside. The phrases have to be contiguous.
2.3 Translation Model
To use phrases in the translation model, we introduce the
hidden variable S. This is a segmentation of the sentence
pair (fJ1 ; eI1) into K phrases (f?K1 ; e?K1 ). We use a one-to-
one phrase alignment, i.e. one source phrase is translated
by exactly one target phrase. Thus, we obtain:
Pr(fJ1 |eI1) =
?
S
Pr(fJ1 , S|eI1) (3)
=
?
S
Pr(S|eI1) ? Pr(fJ1 |S, eI1) (4)
? max
S
{
Pr(S|eI1) ? Pr(f?K1 |e?K1 )
}
(5)
In the preceding step, we used the maximum approxima-
tion for the sum over all segmentations. Next, we allow
only translations that are monotone at the phrase level.
So, the phrase f?1 is produced by e?1, the phrase f?2 is
produced by e?2, and so on. Within the phrases, the re-
ordering is learned during training. Therefore, there is no
constraint on the reordering within the phrases.
Pr(f?K1 |e?K1 ) =
K?
k=1
Pr(f?k|f?k?11 , e?K1 ) (6)
=
K?
k=1
p(f?k|e?k) (7)
Here, we have assumed a zero-order model at the phrase
level. Finally, we have to estimate the phrase translation
probabilities p(f? |e?). This is done via relative frequencies:
p(f? |e?) = N(f? , e?)?
f? ? N(f? ?, e?)
(8)
Here, N(f? , e?) denotes the count of the event that f? has
been seen as a translation of e?. If one occurrence of e? has
N > 1 possible translations, each of them contributes to
N(f? , e?) with 1/N . These counts are calculated from the
training corpus.
Using a bigram language model and assuming Bayes
decision rule, Equation (2), we obtain the following
search criterion:
e?I1 = argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)
} (9)
= argmax
eI1
{ I?
i=1
p(ei|ei?1) (10)
?max
S
p(S|eI1) ?
K?
k=1
p(f?k|e?k)
}
? argmax
eI1,S
{ I?
i=1
p(ei|ei?1)
K?
k=1
p(f?k|e?k)
}
(11)
For the preceding equation, we assumed the segmentation
probability p(S|eI1) to be constant. The result is a simple
translation model. If we interpret this model as a feature
function in the direct approach, we obtain:
hphr(fJ1 , eI1, S,K) = log
K?
k=1
p(f?k|e?k)
We use the maximum approximation for the hidden vari-
able S. Therefore, the feature functions are dependent on
S. Although the number of phrases K is implicitly given
by the segmentation S, we used both S and K to make
this dependency more obvious.
3 Refinements
In this section, we will describe refinements of the
phrase-based translation model. First, we will describe
two heuristics: word penalty and phrase penalty. Sec-
ond, we will describe a single-word based lexicon model.
This will be used to smooth the phrase translation proba-
bilities.
3.1 Simple Heuristics
In addition to the baseline model, we use two simple
heuristics, namely word penalty and phrase penalty:
hwp(fJ1 , eI1, S,K) = I (12)
hpp(fJ1 , eI1, S,K) = K (13)
The word penalty feature is simply the target sentence
length. In combination with the scaling factor this re-
sults in a constant cost per produced target language
word. With this feature, we are able to adjust the sentence
length. If we set a negative scaling factor, longer sen-
tences are more penalized than shorter ones, and the sys-
tem will favor shorter translations. Alternatively, by us-
ing a positive scaling factors, the system will favor longer
translations.
Similar to the word penalty, the phrase penalty feature
results in a constant cost per produced phrase. The phrase
penalty is used to adjust the average length of the phrases.
A negative weight, meaning real costs per phrase, results
in a preference for longer phrases. A positive weight,
meaning a bonus per phrase, results in a preference for
shorter phrases.
3.2 Word-based Lexicon
We are using relative frequencies to estimate the phrase
translation probabilities. Most of the longer phrases are
seen only once in the training corpus. Therefore, pure
relative frequencies overestimate the probability of those
phrases. To overcome this problem, we use a word-based
lexicon model to smooth the phrase translation probabili-
ties. For a source word f and a target phrase e? = ei2i1 , we
use the following approximation:
p(f |ei2i1) ? 1?
i2?
i=i1
(1? p(f |ei))
This models a disjunctive interaction, also called noisy-
OR gate (Pearl, 1988). The idea is that there are multiple
independent causes ei2i1 that can generate an event f . It
can be easily integrated into the search algorithm. The
corresponding feature function is:
hlex(fJ1 , eI1, S,K) = log
K?
k=1
jk?
j=jk?1+1
p(fj |e?k)
Here, jk and ik denote the final position of phrase number
k in the source and the target sentence, respectively, and
we define j0 := 0 and i0 := 0.
To estimate the single-word based translation probabil-
ities p(f |e), we use smoothed relative frequencies. The
smoothing method we apply is absolute discounting with
interpolation:
p(f |e) = max {N(f, e)? d, 0}N(e) + ?(e) ? ?(f)
This method is well known from language modeling (Ney
et al, 1997). Here, d is the nonnegative discounting pa-
rameter, ?(e) is a normalization constant and ? is the nor-
malized backing-off distribution. To compute the counts,
we use the same word alignment matrix as for the ex-
traction of the bilingual phrases. The symbol N(e) de-
notes the unigram count of a word e and N(f, e) denotes
the count of the event that the target language word e is
aligned to the source language word f . If one occurrence
of e has N > 1 aligned source words, each of them con-
tributes with a count of 1/N . The formula for ?(e) is:
?(e) = 1N(e)
?
? ?
f :N(f,e)>d
d+
?
f :N(f,e)?d
N(f, e)
?
?
= 1N(e)
?
f
min{d,N(f, e)}
This formula is a generalization of the one typically used
in publications on language modeling. This generaliza-
tion is necessary, because the lexicon counts may be frac-
tional whereas in language modeling typically integer
counts are used. Additionally, we want to allow discount-
ing values d greater than one. One effect of the discount-
ing parameter d is that all lexicon entries with a count
less than d are discarded and the freed probability mass
is redistributed among the other entries.
As backing-off distribution ?(f), we consider two al-
ternatives. The first one is a uniform distribution and the
second one is the unigram distribution:
?1(f) = 1Vf (14)
?2(f) = N(f)?
f ? N(f ?)
(15)
Here, Vf denotes the vocabulary size of the source lan-
guage and N(f) denotes the unigram count of a source
word f .
4 Monotone Search
The monotone search can be efficiently computed with
dynamic programming. The resulting complexity is lin-
ear in the sentence length. We present the formulae for a
bigram language model. This is only for notational con-
venience. The generalization to a higher order language
model is straightforward. For the maximization problem
in (11), we define the quantity Q(j, e) as the maximum
probability of a phrase sequence that ends with the lan-
guage word e and covers positions 1 to j of the source
sentence. Q(J + 1, $) is the probability of the opti-
mum translation. The $ symbol is the sentence boundary
marker. We obtain the following dynamic programming
recursion.
Q(0, $) = 1
Q(j, e) = max
e?,e?,
j?M?j?<j
{
p(f jj?+1|e?) ? p(e?|e?) ?Q(j?, e?)
}
Q(J + 1, $) = max
e?
{Q(J, e?) ? p($|e?)}
Here, M denotes the maximum phrase length in the
source language. During the search, we store back-
pointers to the maximizing arguments. After perform-
ing the search, we can generate the optimum translation.
The resulting algorithm has a worst-case complexity of
O(J ?M ? Ve ? E). Here, Ve denotes the vocabulary size
of the target language and E denotes the maximum num-
ber of phrase translation candidates for a source language
phrase. Using efficient data structures and taking into ac-
count that not all possible target language phrases can oc-
cur in translating a specific source language sentence, we
can perform a very efficient search.
This monotone algorithm is especially useful for lan-
guage pairs that have a similar word order, e.g. Spanish-
English or French-English.
5 Corpus Statistics
In the following sections, we will present results on three
tasks: Verbmobil, Xerox and Canadian Hansards. There-
fore, we will show the corpus statistics for each of these
tasks in this section. The training corpus (Train) of each
task is used to train a word alignment and then extract the
bilingual phrases and the word-based lexicon. The re-
maining free parameters, e.g. the model scaling factors,
are optimized on the development corpus (Dev). The re-
sulting system is then evaluated on the test corpus (Test).
Verbmobil Task. The first task we will present re-
sults on is the German?English Verbmobil task (Wahlster,
2000). The domain of this corpus is appointment schedul-
ing, travel planning, and hotel reservation. It consists of
transcriptions of spontaneous speech. Table 1 shows the
corpus statistics of this task.
Xerox task. Additionally, we carried out experiments
on the Spanish?English Xerox task. The corpus consists
of technical manuals. This is a rather limited domain task.
Table 2 shows the training, development and test corpus
statistics.
Canadian Hansards task. Further experiments were
carried out on the French?English Canadian Hansards
Table 1: Statistics of training and test corpus for the Verb-
mobil task (PP=perplexity).
German English
Train Sentences 58 073
Words 519 523 549 921
Vocabulary 7 939 4 672
Dev Sentences 276
Words 3 159 3 438
Trigram PP - 28.1
Test Sentences 251
Words 2 628 2 871
Trigram PP - 30.5
Table 2: Statistics of training and test corpus for the Xe-
rox task (PP=perplexity).
Spanish English
Train Sentences 55 761
Words 752 606 665 399
Vocabulary 11 050 7 956
Dev Sentences 1012
Words 15 957 14 278
Trigram PP ? 28.1
Test Sentences 1125
Words 10 106 8 370
Trigram PP ? 48.3
task. This task contains the proceedings of the Cana-
dian parliament. About 3 million parallel sentences of
this bilingual data have been made available by the Lin-
guistic Data Consortium (LDC). Here, we use a subset
of the data containing only sentences with a maximum
length of 30 words. This task covers a large variety of
topics, so this is an open-domain corpus. This is also re-
flected by the large vocabulary size. Table 3 shows the
training and test corpus statistics.
6 Degree of Monotonicity
In this section, we will investigate the effect of the mono-
tonicity constraint. Therefore, we compute how many of
the training corpus sentence pairs can be produced with
the monotone phrase-based search. We compare this to
the number of sentence pairs that can be produced with a
nonmonotone phrase-based search. To make these num-
bers more realistic, we use leaving-one-out. Thus phrases
that are extracted from a specific sentence pair are not
used to check its monotonicity. With leaving-one-out it is
possible that even the nonmonotone search cannot gen-
erate a sentence pair. This happens if a sentence pair
contains a word that occurs only once in the training cor-
pus. All phrases that might produce this singleton are
excluded because of the leaving-one-out principle. Note
Table 3: Statistics of training and test corpus for the
Canadian Hansards task (PP=perplexity).
French English
Train Sentences 1.5M
Words 24M 22M
Vocabulary 100 269 78 332
Dev Sentences 500
Words 9 043 8 195
Trigram PP ? 57.7
Test Sentences 5432
Words 97 646 88 773
Trigram PP ? 56.7
that all these monotonicity consideration are done at the
phrase level. Within the phrases arbitrary reorderings are
allowed. The only restriction is that they occur in the
training corpus.
Table 4 shows the percentage of the training corpus
that can be generated with monotone and nonmonotone
phrase-based search. The number of sentence pairs that
can be produced with the nonmonotone search gives an
estimate of the upper bound for the sentence error rate of
the phrase-based system that is trained on the given data.
The same considerations hold for the monotone search.
The maximum source phrase length for the Verbmobil
task and the Xerox task is 12, whereas for the Canadian
Hansards task we use a maximum of 4, because of the
large corpus size. This explains the rather low coverage
on the Canadian Hansards task for both the nonmonotone
and the monotone search.
For the Xerox task, the nonmonotone search can pro-
duce 75.1% of the sentence pairs whereas the mono-
tone can produce 65.3%. The ratio of the two numbers
measures how much the system deteriorates by using the
monotone search and will be called the degree of mono-
tonicity. For the Xerox task, the degree of monotonicity
is 87.0%. This means the monotone search can produce
87.0% of the sentence pairs that can be produced with
the nonmonotone search. We see that for the Spanish-
English Xerox task and for the French-English Canadian
Hansards task, the degree of monotonicity is rather high.
For the German-English Verbmobil task it is significantly
lower. This may be caused by the rather free word order
in German and the long range reorderings that are neces-
sary to translate the verb group.
It should be pointed out that in practice the monotone
search will perform better than what the preceding esti-
mates indicate. The reason is that we assumed a perfect
nonmonotone search, which is difficult to achieve in prac-
tice. This is not only a hard search problem, but also a
complicated modeling problem. We will see in the next
section that the monotone search will perform very well
on both the Xerox task and the Canadian Hansards task.
Table 4: Degree of monotonicity in the training corpora
for all three tasks (numbers in percent).
Verbmobil Xerox Hansards
nonmonotone 76.3 75.1 59.7
monotone 55.4 65.3 51.5
deg. of mon. 72.6 87.0 86.3
7 Translation Results
7.1 Evaluation Criteria
So far, in machine translation research a single generally
accepted criterion for the evaluation of the experimental
results does not exist. Therefore, we use a variety of dif-
ferent criteria.
? WER (word error rate):
The WER is computed as the minimum number of
substitution, insertion and deletion operations that
have to be performed to convert the generated sen-
tence into the reference sentence.
? PER (position-independent word error rate):
A shortcoming of the WER is that it requires a per-
fect word order. The word order of an acceptable
sentence can be different from that of the target sen-
tence, so that the WER measure alone could be mis-
leading. The PER compares the words in the two
sentences ignoring the word order.
? BLEU score:
This score measures the precision of unigrams, bi-
grams, trigrams and fourgrams with respect to a ref-
erence translation with a penalty for too short sen-
tences (Papineni et al, 2001). BLEU measures ac-
curacy, i.e. large BLEU scores are better.
? NIST score:
This score is similar to BLEU. It is a weighted n-
gram precision in combination with a penalty for
too short sentences (Doddington, 2002). NIST mea-
sures accuracy, i.e. large NIST scores are better.
For the Verbmobil task, we have multiple references
available. Therefore on this task, we compute all the pre-
ceding criteria with respect to multiple references. To
indicate this, we will precede the acronyms with an m
(multiple) if multiple references are used. For the other
two tasks, only single references are used.
7.2 Translation Systems
In this section, we will describe the systems that were
used. On the one hand, we have three different variants
of the single-word based model IBM4. On the other hand,
we have two phrase-based systems, namely the alignment
templates and the one described in this work.
Single-Word Based Systems (SWB). First, there is a
monotone search variant (Mon) that translates each word
of the source sentence from left to right. The second vari-
ant allows reordering according to the so-called IBM con-
straints (Berger et al, 1996). Thus up to three words
may be skipped and translated later. This system will
be denoted by IBM. The third variant implements spe-
cial German-English reordering constraints. These con-
straints are represented by a finite state automaton and
optimized to handle the reorderings of the German verb
group. The abbreviation for this variant is GE. It is only
used for the German-English Verbmobil task. This is just
an extremely brief description of these systems. For de-
tails, see (Tillmann and Ney, 2003).
Phrase-Based System (PB). For the phrase-based sys-
tem, we use the following feature functions: a trigram
language model, the phrase translation model and the
word-based lexicon model. The latter two feature func-
tions are used for both directions: p(f |e) and p(e|f).
Additionally, we use the word and phrase penalty fea-
ture functions. The model scaling factors are optimized
on the development corpus with respect to mWER sim-
ilar to (Och, 2003). We use the Downhill Simplex al-
gorithm from (Press et al, 2002). We do not perform
the optimization on N -best lists but we retranslate the
whole development corpus for each iteration of the op-
timization algorithm. This is feasible because this system
is extremely fast. It takes only a few seconds to translate
the whole development corpus for the Verbmobil task and
the Xerox task; for details see Section 8. In the experi-
ments, the Downhill Simplex algorithm converged after
about 200 iterations. This method has the advantage that
it is not limited to the model scaling factors as the method
described in (Och, 2003). It is also possible to optimize
any other parameter, e.g. the discounting parameter for
the lexicon smoothing.
Alignment Template System (AT). The alignment
template system (Och et al, 1999) is similar to the sys-
tem described in this work. One difference is that the
alignment templates are not defined at the word level but
at a word class level. In addition to the word-based tri-
gram model, the alignment template system uses a class-
based fivegram language model. The search algorithm of
the alignment templates allows arbitrary reorderings of
the templates. It penalizes reorderings with costs that are
linear in the jump width. To make the results as compa-
rable as possible, the alignment template system and the
phrase-based system start from the same word alignment.
The alignment template system uses discriminative train-
ing of the model scaling factors as described in (Och and
Ney, 2002).
7.3 Verbmobil Task
We start with the Verbmobil results. We studied smooth-
ing the lexicon probabilities as described in Section 3.2.
The results are summarized in Table 5. We see that the
Table 5: Effect of lexicon smoothing on the translation
performance [%] for the German-English Verbmobil task.
system mWER mPER BLEU NIST
unsmoothed 37.3 21.1 46.6 7.96
uniform 37.0 20.7 47.0 7.99
unigram 38.2 22.3 45.5 7.79
uniform smoothing method improves translation quality.
There is only a minor improvement, but it is consistent
among all evaluation criteria. It is statistically signifi-
cant at the 94% level. The unigram method hurts perfor-
mance. There is a degradation of the mWER of 0.9%. In
the following, all phrase-based systems use the uniform
smoothing method.
The translation results of the different systems are
shown in Table 6. Obviously, the monotone phrase-based
system outperforms the monotone single-word based sys-
tem. The result of the phrase-based system is comparable
to the nonmonotone single-word based search with the
IBM constraints. With respect to the mPER, the PB sys-
tem clearly outperforms all single-word based systems.
If we compare the monotone phrase-based system with
the nonmonotone alignment template system, we see that
the mPERs are similar. Thus the lexical choice of words
is of the same quality. Regarding the other evaluation
criteria, which take the word order into account, the non-
monotone search of the alignment templates has a clear
advantage. This was already indicated by the low degree
of monotonicity on this task. The rather free word order
in German and the long range dependencies of the verb
group make reorderings necessary.
Table 6: Translation performance [%] for the German-
English Verbmobil task (251 sentences).
system variant mWER mPER BLEU NIST
SWB Mon 42.8 29.3 38.0 7.07
IBM 37.1 25.0 47.8 7.84
GE 35.4 25.3 48.5 7.83
PB 37.0 20.7 47.0 7.99
AT 30.3 20.6 56.8 8.57
7.4 Xerox task
The translation results for the Xerox task are shown in
Table 7. Here, we see that both phrase-based systems
clearly outperform the single-word based systems. The
PB system performs best on this task. Compared to the
AT system, the BLEU score improves by 4.1% absolute.
The improvement of the PB system with respect to the
AT system is statistically significant at the 99% level.
Table 7: Translation performance [%] for the Spanish-
English Xerox task (1125 sentences).
System WER PER BLEU NIST
SWB IBM 38.8 27.6 55.3 8.00
PB 26.5 18.1 67.9 9.07
AT 28.9 20.1 63.8 8.76
7.5 Canadian Hansards task
The translation results for the Canadian Hansards task are
shown in Table 8. As on the Xerox task, the phrase-based
systems perform better than the single-word based sys-
tems. The monotone phrase-based system yields even
better results than the alignment template system. This
improvement is consistent among all evaluation criteria
and it is statistically significant at the 99% level.
Table 8: Translation performance [%] for the French-
English Canadian Hansards task (5432 sentences).
System Variant WER PER BLEU NIST
SWB Mon 65.2 53.0 19.8 5.96
IBM 64.5 51.3 20.7 6.21
PB 57.8 46.6 27.8 7.15
AT 61.1 49.1 26.0 6.71
8 Efficiency
In this section, we analyze the translation speed of the
phrase-based translation system. All experiments were
carried out on an AMD Athlon with 2.2GHz. Note that
the systems were not optimized for speed. We used the
best performing systems to measure the translation times.
The translation speed of the monotone phrase-based
system for all three tasks is shown in Table 9. For the
Xerox task, the translation process takes less than 7 sec-
onds for the whole 10K words test set. For the Verbmobil
task, the system is even slightly faster. It takes about 1.6
seconds to translate the whole test set. For the Canadian
Hansards task, the translation process is much slower, but
the average time per sentence is still less than 1 second.
We think that this slowdown can be attributed to the large
training corpus. The system loads only phrase pairs into
memory if the source phrase occurs in the test corpus.
Therefore, the large test corpus size for this task also af-
fects the translation speed.
In Fig. 1, we see the average translation time per sen-
tence as a function of the sentence length. The translation
times were measured for the translation of the 5432 test
sentences of the Canadian Hansards task. We see a clear
linear dependency. Even for sentences of thirty words,
the translation takes only about 1.5 seconds.
Table 9: Translation Speed for all tasks on a AMD Athlon
2.2GHz.
Verbmobil Xerox Hansards
avg. sentence length 10.5 13.5 18.0
seconds / sentence 0.006 0.007 0.794
words / second 1642 1448 22.8
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 0  5  10  15  20  25  30
t
i
m
e
sentence lengthFigure 1: Average translation time per sentence as a func-
tion of the sentence length for the Canadian Hansards task
(5432 test sentences).
9 Related Work
Recently, phrase-based translation approaches became
more and more popular. Some examples are the align-
ment template system in (Och et al, 1999; Och and Ney,
2002) that we used for comparison. In (Zens et al, 2002),
a simple phrase-based approach is described that served
as starting point for the system in this work. (Marcu
and Wong, 2002) presents a joint probability model for
phrase-based translation. It does not use the word align-
ment for extracting the phrases, but directly generates a
phrase alignment. In (Koehn et al, 2003), various aspects
of phrase-based systems are compared, e.g. the phrase
extraction method, the underlying word alignment model,
or the maximum phrase length. (Tomas and Casacuberta,
2003) describes a linear interpolation of a phrase-based
and an alignment template-based approach.
10 Conclusions
We described a phrase-based translation approach. The
basic idea of this approach is to remember all bilingual
phrases that have been seen in the word-aligned train-
ing corpus. As refinements of the baseline model, we
described two simple heuristics: the word penalty fea-
ture and the phrase penalty feature. Additionally, we pre-
sented a single-word based lexicon with two smoothing
methods. The model scaling factors were optimized with
respect to the mWER on the development corpus.
We described a highly efficient monotone search al-
gorithm. The worst-case complexity of this algorithm is
linear in the sentence length. This leads to an impressive
translation speed of more than 1000 words per second for
the Verbmobil task and for the Xerox task. Even for the
Canadian Hansards task the translation of sentences of
length 30 takes only about 1.5 seconds.
The described search is monotone at the phrase level.
Within the phrases, there are no constraints on the re-
orderings. Therefore, this method is best suited for lan-
guage pairs that have a similar order at the level of the
phrases learned by the system. Thus, the translation pro-
cess should require only local reorderings. As the exper-
iments have shown, Spanish-English and French-English
are examples of such language pairs. For these pairs,
the monotone search was found to be sufficient. The
phrase-based approach clearly outperformed the single-
word based systems. It showed even better performance
than the alignment template system.
The experiments on the German-English Verbmobil
task outlined the limitations of the monotone search.
As the low degree of monotonicity indicated, reordering
plays an important role on this task. The rather free word
order in German as well as the verb group seems to be dif-
ficult to translate. Nevertheless, when ignoring the word
order and looking at the mPER only, the monotone search
is competitive with the best performing system.
For further improvements, we will investigate the use-
fulness of additional models, e.g. modeling the segmen-
tation probability. Also, slightly relaxing the monotonic-
ity constraint in a way that still allows an efficient search
is of high interest. In spirit of the IBM reordering con-
straints of the single-word based models, we could allow
a phrase to be skipped and to be translated later.
Acknowledgment
This work has been partially funded by the EU project
TransType 2, IST-2001-32091.
References
A. L. Berger, P. F. Brown, S. A. D. Pietra, V. J. D. Pietra,
J. R. Gillett, A. S. Kehler, and R. L. Mercer. 1996.
Language translation apparatus and method of using
context-based translation models, United States patent,
patent number 5510981, April.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79?85,
June.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proc. ARPA Workshop on Human Language
Technology.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of the Human Lan-
guage Technology Conf. (HLT-NAACL), pages 127?
133, Edmonton, Canada, May/June.
D. Marcu and W. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Proc. Conf. on Empirical Methods for Natural Lan-
guage Processing, pages 133?139, Philadelphia, PA,
July.
H. Ney, S. Martin, and F. Wessel. 1997. Statistical lan-
guage modeling using leaving-one-out. In S. Young
and G. Bloothooft, editors, Corpus-Based Methods
in Language and Speech Processing, pages 174?207.
Kluwer.
F. J. Och and H. Ney. 2002. Discriminative training and
maximum entropy models for statistical machine trans-
lation. In Proc. of the 40th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
295?302, Philadelphia, PA, July.
F. J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proc. of the Joint SIGDAT Conf. on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora, pages 20?28, University of Maryland, Col-
lege Park, MD, June.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. of the 41th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 160?167, Sapporo, Japan, July.
K. A. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Technical Report RC22176 (W0109-022),
IBM Research Division, Thomas J. Watson Research
Center, September.
J. Pearl. 1988. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann Publishers, Inc., San Mateo, CA. Revised
second printing.
W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P.
Flannery. 2002. Numerical Recipes in C++. Cam-
bridge University Press, Cambridge, UK.
C. Tillmann and H. Ney. 2003. Word reordering and a
dynamic programming beam search algorithm for sta-
tistical machine translation. Computational Linguis-
tics, 29(1):97?133, March.
J. Tomas and F. Casacuberta. 2003. Combining phrase-
based and template-based aligned models in statisti-
cal translation. In Proc. of the First Iberian Conf. on
Pattern Recognition and Image Analysis, pages 1020?
1031, Mallorca, Spain, June.
W. Wahlster, editor. 2000. Verbmobil: Foundations
of speech-to-speech translations. Springer Verlag,
Berlin, Germany, July.
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In 25th German Confer-
ence on Artificial Intelligence (KI2002), pages 18?32,
Aachen, Germany, September. Springer Verlag.
Proceedings of NAACL HLT 2007, pages 492?499,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Efficient Phrase-table Representation for Machine Translation with
Applications to Online MT and Speech Translation
Richard Zens and Hermann Ney
Human Language Technology and Pattern Recognition
Lehrstuhl fu?r Informatik 6 ? Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
{zens,ney}@cs.rwth-aachen.de
Abstract
In phrase-based statistical machine transla-
tion, the phrase-table requires a large amount
of memory. We will present an efficient repre-
sentation with two key properties: on-demand
loading and a prefix tree structure for the
source phrases.
We will show that this representation scales
well to large data tasks and that we are able
to store hundreds of millions of phrase pairs
in the phrase-table. For the large Chinese?
English NIST task, the memory requirements
of the phrase-table are reduced to less than
20MB using the new representation with no
loss in translation quality and speed. Addi-
tionally, the new representation is not limited
to a specific test set, which is important for
online or real-time machine translation.
One problem in speech translation is the
matching of phrases in the input word graph
and the phrase-table. We will describe a novel
algorithm that effectively solves this com-
binatorial problem exploiting the prefix tree
data structure of the phrase-table. This algo-
rithm enables the use of significantly larger
input word graphs in a more efficient way re-
sulting in improved translation quality.
1 Introduction
In phrase-based statistical machine translation, a
huge number of source and target phrase pairs
is memorized in the so-called phrase-table. For
medium sized tasks and phrase lengths, these
phrase-tables already require several GBs of mem-
ory or even do not fit at all. If the source text, which
is to be translated, is known in advance, a common
trick is to filter the phrase-table and keep a phrase
pair only if the source phrase occurs in the text. This
filtering is a time-consuming task, as we have to
go over the whole phrase-table. Furthermore, we
have to repeat this filtering step whenever we want
to translate a new source text.
To address these problems, we will use an ef-
ficient representation of the phrase-table with two
key properties: on-demand loading and a prefix tree
structure for the source phrases. The prefix tree
structure exploits the redundancy among the source
phrases. Using on-demand loading, we will load
only a small fraction of the overall phrase-table into
memory. The majority will remain on disk.
The on-demand loading is employed on a per sen-
tence basis, i.e. we load only the phrase pairs that
are required for one sentence into memory. There-
fore, the memory requirements are low, e.g. less than
20MB for the Chin.-Eng. NIST task. Another ad-
vantage of the on-demand loading is that we are able
to translate new source sentences without filtering.
A potential problem is that this on-demand load-
ing might be too slow. To overcome this, we use a
binary format which is a memory map of the internal
representation used during decoding. Additionally,
we load coherent chunks of the tree structure instead
of individual phrases, i.e. we have only few disk ac-
cess operations. In our experiments, the on-demand
loading is not slower than the traditional approach.
As pointed out in (Mathias and Byrne, 2006),
one problem in speech translation is that we have
to match the phrases of our phrase-table against a
word graph representing the alternative ASR tran-
492
scriptions. We will present a phrase matching algo-
rithm that effectively solves this combinatorial prob-
lem exploiting the prefix tree data structure of the
phrase-table. This algorithm enables the use of sig-
nificantly larger input word graphs in a more effi-
cient way resulting in improved translation quality.
The remaining part is structured as follows: we
will first discuss related work in Sec. 2. Then, in
Sec. 3, we will describe the phrase-table represen-
tation. Afterwards, we will present applications in
speech translation and online MT in Sec. 4 and 5,
respectively. Experimental results will be presented
in Sec. 6 followed by the conclusions in Sec. 7.
2 Related Work
(Callison-Burch et al, 2005) and (Zhang and Vogel,
2005) presented data structures for a compact rep-
resentation of the word-aligned bilingual data, such
that on-the-fly extraction of long phrases is possi-
ble. The motivation in (Callison-Burch et al, 2005)
is that there are some long source phrases in the
test data that also occur in the training data. How-
ever, the more interesting question is if these long
phrases really help to improve the translation qual-
ity. We have investigated this and our results are in
line with (Koehn et al, 2003) showing that the trans-
lation quality does not improve if we utilize phrases
beyond a certain length. Furthermore, the suffix ar-
ray data structure of (Callison-Burch et al, 2005) re-
quires a fair amount of memory, about 2GB in their
example, whereas our implementation will use only
a tiny amount of memory, e.g. less than 20MB for
the large Chinese-English NIST task.
3 Efficient Phrase-table Representation
In this section, we will describe the proposed rep-
resentation of the phrase-table. A prefix tree, also
called trie, is an ordered tree data structure used to
store an associative array where the keys are symbol
sequences. In the case of phrase-based MT, the keys
are source phrases, i.e. sequences of source words
and the associated values are the possible transla-
tions of these source phrases. In a prefix tree, all
descendants of any node have a common prefix,
namely the source phrase associated with that node.
The root node is associated with the empty phrase.
The prefix tree data structure is quite common in
automatic speech translation. There, the lexicon, i.e.
the mapping of phoneme sequences to words, is usu-
ally organized as a prefix tree (Ney et al, 1992).
We convert the list of source phrases into a pre-
fix tree and, thus, exploit that many of them share
the same prefix. This is illustrated in Fig. 1 (left).
Within each node of the tree, we store a sorted ar-
ray of possible successor words along with pointers
to the corresponding successor nodes. Additionally,
we store a pointer to the possible translations.
One property of the tree structure is that we can
efficiently access the successor words of a given pre-
fix. This will be a key point to achieve an efficient
phrase matching algorithm in Sec. 4. When looking
for a specific successor word, we perform a binary
search in the sorted array. Alternatively, we could
use hashing to speed up this lookup. We have chosen
an array representation as this can be read very fast
from disk. Additionally, with the exception of the
root node, the branching factor of the tree is small,
i.e. the potential benefit from hashing is limited. At
the root node, however, the branching factor is close
to the vocabulary size of the source language, which
can be large. As we store the words internally as in-
tegers and virtually all words occur as the first word
of some phrase, we can use the integers directly as
the position in the array of the root node. Hence, the
search for the successors at the root node is a simple
table lookup with direct access, i.e. in O(1).
If not filtered for a specific test set, the phrase-
table becomes huge even for medium-sized tasks.
Therefore, we store the tree structure on disk
and load only the required parts into memory on-
demand. This is illustrated in Fig. 1 (right). Here,
we show the matching phrases for the source sen-
tence ?c a a c?, where the matching phrases are set in
bold and the phrases that are loaded into memory are
set in italics. The dashed part of the tree structure is
not loaded into memory. Note that some nodes of the
tree are loaded even if there is no matching phrase in
that node. These are required to actually verify that
there is no matching phrase. An example is the ?bc?
node in the lower right part of the figure. This node
is loaded to check if the phrase ?c a a? occurs in the
phrase-table. The translations, however, are loaded
only for matching source phrases.
In the following sections, we will describe appli-
cations of this phrase-table representation for speech
translation and online MT.
4 Speech Translation
In speech translation, the input to the MT system is
not a sentence, but a word graph representing alter-
493
a b a c
a a b b
a b b c
a b c c
b c a
b a c a b
b a a c a c
b a b
a
b
c
a
b
a
b
c
a
c
a
b
c
b
c
a b a c
a a b b
a b b c
a b c c
b c a
b a c a b
b a a c a c
b a b
a
b
c
a
b
a
b
c
a
c
a
b
c
b
c
Figure 1: Illustration of the prefix tree. Left: list of source phrases and the corresponding prefix tree. Right:
list of matching source phrases for sentence ?c a a c? (bold phrases match, phrases in italics are loaded in
memory) and the corresponding partially loaded prefix tree (the dashed part is not in memory).
jffifl
fi
sGj,nffifl
fi
sGj,1ffifl
fi
sGj,Njffifl
fi
-  
  
  
 
@@@@@@@R
fGj,1
fGj,n
fGj,Nj
.
.
.
.
.
.
.
.
.
.
.
.
Figure 2: Illustration for graph G: node j with suc-
cessor nodes sGj,1, ..., s
G
j,n..., s
G
j,Nj
and corresponding
edge labels fGj,1, ..., f
G
j,n, ..., f
G
j,Nj
.
native ASR transcriptions. As pointed out in (Math-
ias and Byrne, 2006), one problem in speech trans-
lation is that we have to match the phrases of our
phrase-table against the input word graph. This re-
sults in a combinatorial problem as the number of
phrases in a word graph increases exponentially with
the phrase length.
4.1 Problem Definition
In this section, we will introduce the notation and
state the problem of matching source phrases of
an input graph G and the phrase-table, represented
as prefix tree T . The input graph G has nodes
1, ..., j, ..., J . The outgoing edges of a graph node
j are numbered with 1, ..., n, ..., Nj , i.e. an edge in
the input graph is identified by a pair (j, n). The
source word labeling the nth outgoing edge of graph
node j is denoted as fGj,n and the successor node of
this edge is denoted as sGj,n ? {1, ..., J}. This nota-
tion is illustrated in Fig. 2.
We use a similar notation for the prefix tree T with
nodes 1, ..., k, ...,K. The outgoing edges of a tree
node k are numbered with 1, ...,m, ...,Mk, i.e. an
edge in the prefix tree is identified by a pair (k,m).
The source word labeling the mth outgoing edge of
tree node k is denoted as fTk,m and the successor
node of this edge is denoted as sTk,m ? {1, ...,K}.
Due to the tree structure, the successor nodes of a
tree node k are all distinct:
sTk,m = s
T
k,m? ? m = m
? (1)
Let k0 denote the root node of the prefix tree and
let f?k denote the prefix that leads to tree node k.
Furthermore, we define E(k) as the set of possible
translations of the source phrase f?k. These are the
entries of the phrase-table, i.e.
E(k) =
{
e?
?
?
? p(e?|f?k) > 0
}
(2)
We will need similar symbols for the input graph.
Therefore, we define F (j?, j) as the set of source
phrases of all paths from graph node j? to node j, or
formally:
F (j?, j) =
{
f?
?
?
? ?(ji, ni)Ii=1 : f? = f
G
j1,n1 , ..., f
G
jI ,nI
? j1 = j? ?
?I?1
i=1 s
G
ji,ni = ji+1 ? sjI ,nI = j
}
Here, the conditions ensure that the edge sequence
(ji, ni)Ii=1 is a proper path from node j
? to node j
in the input graph and that the corresponding source
phrase is f? = fGj1,n1 , ..., f
G
jI ,nI . This definition can
be expressed in a recursive way; the idea is to extend
the phrases of the predecessor nodes by one word:
F (j?, j) =
?
(j??,n):sG
j??,n
=j
{
f?fGj??,n
?
?
?f? ? F (j?, j??)
}
(3)
494
Here, the set is expressed as a union over all in-
bound edges (j??, n) of node j. We concatenate each
source phrase f? that ends at the start node of such
an edge, i.e. f? ? F (j?, j??), with the corresponding
edge label fGj??,n. Additionally, we define E(j
?, j)
as the set of possible translations of all paths from
graph node j? to graph node j, or formally:
E(j?, j) =
{
e?
?
?
? ?f? ? F (j?, j) : p(e?|f?) > 0
}
(4)
=
?
k:f?k?F (j?,j)
E(k) (5)
=
?
(j??,n):sG
j??,n
=j
?
k:f?k?F (j
?,j??)
m:fG
j??,n
=fTk,m
E(sTk,m) (6)
Here, the definition was first rewritten using Eq. 2
and then using Eq. 3. Again, the set is expressed
recursively as a union over the inbound edges. For
each inbound edge (j??, n), the inner union verifies
that there exists a corresponding edge (k,m) in the
prefix tree with the same label, i.e. fGj??,n = f
T
k,m.
Our goal is to find all non-empty sets of trans-
lation options E(j?, j). The naive approach would
be to enumerate all paths in the input graph from
node j? to node j, then lookup the corresponding
source phrase in the phrase-table and add the trans-
lations, if there are any, to the set of translation
options E(j?, j). This solution has some obvious
weaknesses: the number of paths between two nodes
is typically huge and the majority of the correspond-
ing source phrases do not occur in the phrase-table.
We omitted the probabilities for notational conve-
nience. The extensions are straightforward. Note
that we store only the target phrases e? in the set
of possible translations E(j?, j) and not the source
phrases f? . This is based on the assumption that the
models which are conditioned on the source phrase
f? are independent of the context outside the phrase
pair (f? , e?). This assumption holds for the standard
phrase and word translation models. Thus, we have
to keep only the target phrase with the highest prob-
ability. It might be violated by lexicalized distor-
tion models (dependent on the configuration); in that
case we have to store the source phrase along with
the target phrase and the probability, which is again
straightforward.
4.2 Algorithm
The algorithm for matching the source phrases of the
input graph G and the prefix tree T is presented in
Figure 3: Algorithm phrase-match for match-
ing source phrases of input graph G and prefix tree
T . Input: graph G, prefix tree T , translation options
E(k) for all tree nodes k; output: translation options
E(j?, j) for all graph nodes j? and j.
0 FOR j? = 1 TO J DO
1 stack.push(j?, k0)
2 WHILE not stack.empty() DO
3 (j, k) = stack.pop()
4 E(j?, j) = E(j?, j) ? E(k)
5 FOR n = 1 TO Nj DO
6 IF (fGj,n = )
7 THEN stack.push(sGj,n, k)
8 ELSE IF (?m : fGj,n = f
T
k,m)
9 THEN stack.push(sGj,n, s
T
k,m)
Fig. 3. Starting from a graph node j?, we explore the
part of the graph which corresponds to known source
phrase prefixes and generate the sets E(j?, j) incre-
mentally based on Eq. 6. The intermediate states
are represented as pairs (j, k) meaning that there ex-
ists a path in the input graph from node j? to node j
which is labeled with the source phrase f?k, i.e. the
source phrase that leads to node k in the prefix tree.
These intermediate states are stored on a stack. After
the initialization in line 1, the main loop starts. We
take one item from the stack and update the transla-
tion options E(j?, j) in line 4. Then, we loop over
all outgoing edges of the current graph node j. For
each edge, we first check if the edge is labeled with
an  in line 6. In this special case, we go to the suc-
cessor node in the input graph sGj,n, but remain in the
current node k of the prefix tree. In the regular case,
i.e. the graph edge label is a regular word, we check
in line 8 if the current prefix tree node k has an out-
going edge labeled with that word. If such an edge
is found, we put a new state on the stack with the
two successor nodes in the input graph sGj,n and the
prefix tree sTk,m, respectively.
4.3 Computational Complexity
In this section, we will analyze the computational
complexity of the algorithm. The computational
complexity of lines 5-9 is in O(Nj logMk), i.e. it
depends on the branching factors of the input graph
and the prefix tree. Both are typically small. An ex-
ception is the branching factor of the root node k0 of
the prefix tree, which can be rather large, typically it
is the vocabulary size of the source language. But,
as described in Sec. 3, we can access the successor
495
nodes of the root node of the prefix tree in O(1), i.e.
in constant time. So, if we are at the root node of the
prefix tree, the computational complexity of lines 5-
9 is inO(Nj). Using hashing at the interior nodes of
the prefix tree would result in a constant time lookup
at these nodes as well. Nevertheless, the sorted ar-
ray implementation that we chose has the advantage
of faster loading from disk which seems to be more
important in practice.
An alternative interpretation of lines 5-9 is that we
have to compute the intersection of the two sets fGj
and fTk , with
fGj =
{
fGj,n
?
? n = 1, ..., Nj
}
(7)
fTk =
{
fTk,m
?
?m = 1, ...,Mk
}
. (8)
Assuming both sets are sorted, this could be done in
linear time, i.e. in O(Nj + Mk). In our case, only
the edges in the prefix tree are sorted. Obviously, we
could sort the edges in the input graph and then ap-
ply the linear algorithm, resulting in an overall com-
plexity of O(Nj logNj + Mk). As the algorithm
visits nodes multiple times, we could do even better
by sorting all edges of the graph during the initial-
ization. Then, we could always apply the linear time
method. On the other hand, it is unclear if this pays
off in practice and an experimental comparison has
to be done which we will leave for future work.
The overall complexity of the algorithm depends
on how many phrases of the input graph occur in the
phrase-table. In the worst case, i.e. if all phrases oc-
cur in the phrase-table, the described algorithm is
not more efficient than the naive algorithm which
simply enumerates all phrases. Nevertheless, this
does not happen in practice and we observe an ex-
ponential speed up compared to the naive algorithm,
as will be shown in Sec. 6.3.
5 Online Machine Translation
Beside speech translation, the presented phrase-
table data structure has other interesting applica-
tions. One of them is online MT, i.e. an MT sys-
tem that is able to translate unseen sentences with-
out significant delay. These online MT systems are
typically required if there is some interaction with
human users, e.g. if the MT system acts as an in-
terpreter in a conversation, or in real-time systems.
This situation is different from the usual research
environment where typically a fair amount of time
is spent to prepare the MT system to translate a cer-
tain set of source sentences. In the research scenario,
Table 1: NIST task: corpus statistics.
Chinese English
Train Sentence pairs 7M
Running words 199M 213M
Vocabulary size 222K 351K
Test 2002 Sentences 878 3 512
Running words 25K 105K
2005 Sentences 1 082 4 328
Running words 33K 148K
this preparation usually pays off as the same set of
sentences is translated multiple times. In contrast,
an online MT system translates each sentence just
once. One of the more time-consuming parts of this
preparation is the filtering of the phrase-table. Us-
ing the on-demand loading technique we described
in Sec. 3, we can avoid the filtering step and di-
rectly translate the source sentence. An additional
advantage is that we load only small parts of the full
phrase-table into memory. This reduces the mem-
ory requirements significantly, e.g. for the Chinese?
English NIST task, the memory requirement of the
phrase-table is reduced to less than 20MB using on-
demand loading. This makes the MT system usable
on devices with limited hardware resources.
6 Experimental Results
6.1 Translation System
For the experiments, we use a state-of-the-art
phrase-based statistical machine translation system
as described in (Zens and Ney, 2004). We use a
log-linear combination of several models: a four-
gram language model, phrase-based and word-based
translation models, word, phrase and distortion
penalty and a lexicalized distortion model. The
model scaling factors are optimized using minimum
error rate training (Och, 2003).
6.2 Empirical Analysis for a Large Data Task
In this section, we present an empirical analysis of
the described data structure for the large data track
of the Chinese-English NIST task. The corpus statis-
tics are shown in Tab. 1.
The translation quality is measured using two ac-
curacy measures: the BLEU and the NIST score.
Additionally, we use the two error rates: the word
error rate (WER) and the position-independent word
error rate (PER). These evaluation criteria are com-
puted with respect to four reference translations.
In Tab. 2, we present the translation quality as a
496
Table 2: NIST task: translation quality as a function of the maximum source phrase length.
src NIST 2002 set (dev) NIST 2005 set (test)
len WER[%] PER[%] BLEU[%] NIST WER[%] PER[%] BLEU[%] NIST
1 71.9 46.8 27.07 8.37 78.0 49.0 23.11 7.62
2 62.4 41.2 34.36 9.39 68.5 42.2 30.32 8.74
3 62.0 41.1 34.89 9.33 67.7 42.1 30.90 8.74
4 61.7 41.1 35.05 9.27 67.6 41.9 30.99 8.75
5 61.8 41.2 34.95 9.25 67.6 41.9 30.93 8.72
? 61.8 41.2 34.99 9.25 67.5 41.8 30.90 8.73
Table 3: NIST task: phrase-table statistics.
src number of distinct avg. tgt
len src phrases src-tgt pairs candidates
1 221 505 17 456 415 78.8
2 5 000 041 39 436 617 7.9
3 20 649 699 58 503 904 2.8
4 31 383 549 58 436 271 1.9
5 32 679 145 51 255 866 1.6
total 89 933 939 225 089 073 2.5
function of the maximum source phrase length. We
observe a large improvement when going beyond
length 1, but this flattens out very fast. Using phrases
of lengths larger than 4 or 5 does not result in fur-
ther improvement. Note that the minor differences
in the evaluation results for length 4 and beyond are
merely statistical noise. Even a length limit of 3, as
proposed by (Koehn et al, 2003), would result in
almost optimal translation quality. In the following
experiments on this task, we will use a limit of 5 for
the source phrase length.
In Tab. 3, we present statistics about the extracted
phrase pairs for the Chinese?English NIST task as
a function of the source phrase length, in this case
for length 1-5. The phrases are not limited to a spe-
cific test set. We show the number of distinct source
phrases, the number of distinct source-target phrase
pairs and the average number of target phrases (or
translation candidates) per source phrase. In the ex-
periments, we limit the number of translation can-
didates per source phrase to 200. We store a to-
tal of almost 90 million distinct source phrases and
more than 225 million distinct source-target phrase
pairs in the described data structure. Obviously, it
would be infeasible to load this huge phrase-table
completely into memory. Nevertheless, using on-
demand loading, we are able to utilize all these
phrase pairs with minimal memory usage.
In Fig. 4, we show the memory usage of the de-
scribed phrase-table data structure per sentence for
0 20 40 60 80 100percentage of test set6
8
10
12
14
16
18
20
mem
ory u
sage
 [Me
gaBy
te]
Figure 4: NIST task: phrase-table memory usage
per sentence (sorted).
the NIST 2002 test set. The sentences were sorted
according to the memory usage. The maximum
amount of memory for the phrase-table is 19MB;
for more than 95% of the sentences no more than
15MB are required. Storing all phrase pairs for this
test set in memory requires about 1.7GB of mem-
ory, i.e. using the described data structures, we not
only avoid the limitation to a specific test set, but we
also reduce the memory requirements by about two
orders of a magnitude.
Another important aspect that should be consid-
ered is translation speed. In our experiments, the
described data structure is not slower than the tradi-
tional approach. We attribute this to the fact that we
use a binary format that is a memory map of the data
structure used internally and that we load the data in
rather large, coherent chunks. Additionally, there is
virtually no initialization time for the phrase-table
which decreases the overhead of parallelization and
therefore speeds up the development cycle.
6.3 Speech Translation
The experiments for speech translation were con-
ducted on the European Parliament Plenary Sessions
(EPPS) task. This is a Spanish-English speech-to-
speech translation task collected within the TC-Star
497
Table 4: EPPS task: corpus statistics.
Train Spanish English
Sentence pairs 1.2 M
Running words 31 M 30 M
Vocabulary size 140 K 94 K
Test confusion networks Full Pruned
Sentences 1 071
Avg. length 23.6
Avg. / max. depth 2.7 / 136 1.3 / 11
Avg. number of paths 1075 264K
project. The training corpus statistics are presented
in Tab. 4. The phrase-tables for this task were kindly
provided by ITC-IRST.
We evaluate the phrase-match algorithm in
the context of confusion network (CN) decoding
(Bertoldi and Federico, 2005), which is one ap-
proach to speech translation. CNs (Mangu et al,
2000) are interesting for MT because the reordering
can be done similar to single best input. For more
details on CN decoding, please refer to (Bertoldi et
al., 2007). Note that the phrase-match algo-
rithm is not limited to CNs, but can work on arbi-
trary word graphs.
Statistics of the CNs are also presented in Tab. 4.
We distinguish between the full CNs and pruned
CNs. The pruning parameters were chosen such that
the resulting CNs are similar in size to the largest
ones in (Bertoldi and Federico, 2005). The average
depth of the full CNs, i.e. the average number of al-
ternatives per position, is about 2.7 words whereas
the maximum is as high as 136 alternatives.
In Fig. 5, we present the average number of
phrase-table look-ups for the full EPPS CNs as a
function of the source phrase length. The curve ?CN
total? represents the total number of source phrases
in the CNs for a given length. This is the number
of phrase-table look-ups using the naive algorithm.
Note the exponential growth with increasing phrase
length. Therefore, the naive algorithm is only appli-
cable for very short phrases and heavily pruned CNs,
as e.g. in (Bertoldi and Federico, 2005).
The curve ?CN explored? is the number of phrase-
table look-ups using the phrase-match algo-
rithm described in Fig. 3. We do not observe the
exponential explosion as for the naive algorithm.
Thus, the presented algorithm effectively solves the
combinatorial problem of matching phrases of the
input CNs and the phrase-table. For comparison,
we plotted also the number of look-ups using the
phrase-match algorithm in the case of single-
0 2 4 6 8 10 12 14source phrase length
0.1
1
10
100
1000
10000
100000
1000000
10000000
100000000
phra
se ta
ble l
ook-
ups CN totalCN exploredsingle-best explored
Figure 5: EPPS task: avg. number of phrase-table
look-ups per sentence as a function of the source
phrase length.
Table 5: EPPS task: translation quality and time for
different input conditions (CN=confusion network,
time in seconds per sentence).
Input type BLEU[%] Time [sec]
Single best 37.6 2.7
CN pruned 38.5 4.8
full 38.9 9.2
best input, labeled ?single-best explored?. The maxi-
mum phrase length for these experiments is seven.
For CN input, this length can be exceeded as the
CNs may contain -transitions.
In Tab. 5, we present the translation results and
the translation times for different input conditions.
We observe a significant improvement in translation
quality as more ASR alternatives are taken into ac-
count. The best results are achieved for the full
CNs. On the other hand, the decoding time in-
creases only moderately. Using the new algorithm,
the ratio of the time for decoding the CNs and the
time for decoding the single best input is 3.4 for the
full CNs and 1.8 for the pruned CNs. In previous
work (Bertoldi and Federico, 2005), the ratio for the
pruned CNs was about 25 and the full CNs could not
be handled.
To summarize, the presented algorithm has two
main advantages for speech translation: first, it
enables us to utilize large CNs, which was pro-
hibitively expensive beforehand and second, the ef-
ficiency is improved significantly.
Whereas the previous approaches required care-
ful pruning of the CNs, we are able to utilize the un-
pruned CNs. Experiments on other tasks have shown
that even larger CNs are unproblematic.
498
7 Conclusions
We proposed an efficient phrase-table data structure
which has two key properties:
1. On-demand loading.
We are able to store hundreds of millions of
phrase pairs and require only a very small
amount of memory during decoding, e.g. less
than 20MB for the Chinese-English NIST task.
This enables us to run the MT system on devices
with limited hardware resources or alternatively
to utilize the freed memory for other models. Ad-
ditionally, the usual phrase-table filtering is obso-
lete, which is important for online MT systems.
2. Prefix tree data structure.
Utilizing the prefix tree structure enables us to ef-
ficiently match source phrases against the phrase-
table. This is especially important for speech
translation where the input is a graph represent-
ing a huge number of alternative sentences. Us-
ing the novel algorithm, we are able to handle
large CNs, which was prohibitively expensive
beforehand. This results in more efficient decod-
ing and improved translation quality.
We have shown that this data structure scales very
well to large data tasks like the Chinese-English
NIST task. The implementation of the described
data structure as well as the phrase-match al-
gorithm for confusion networks is available as open
source software in the MOSES toolkit1.
Not only standard phrase-based systems can ben-
efit from this data structure. It should be rather
straightforward to apply this data structure as well as
the phrase-match algorithm to the hierarchical
approach of (Chiang, 2005). As the number of rules
in this approach is typically larger than the number
of phrases in a standard phrase-based system, the
gains should be even larger.
The language model is another model with high
memory requirements. It would be interesting to in-
vestigate if the described techniques and data struc-
tures are applicable for reducing the memory re-
quirements of language models.
Some aspects of the phrase-match algorithm
are similar to the composition of finite-state au-
tomata. An efficient implementation of on-demand
loading (not only on-demand computation) for a
1http://www.statmt.org/moses
finite-state toolkit would make the whole range of
finite-state operations applicable to large data tasks.
Acknowledgments
This material is partly based upon work supported by the
DARPA under Contract No. HR0011-06-C-0023, and was
partly funded by the European Union under the integrated
project TC-STAR (IST-2002-FP6-506738, http://www.tc-
star.org). Additionally, we would like to thank all group
members of the JHU 2006 summer research workshop Open
Source Toolkit for Statistical Machine Translation.
References
N. Bertoldi and M. Federico. 2005. A new decoder for spo-
ken language translation based on confusion networks. In
Proc. IEEE Automatic Speech Recognition and Understand-
ing Workshop, pages 86?91, Mexico, November/December.
N. Bertoldi, R. Zens, and M. Federico. 2007. Speech trans-
lation by confusion networks decoding. In Proc. IEEE
Int. Conf. on Acoustics, Speech, and Signal Processing
(ICASSP), Honolulu, Hawaii, April.
C. Callison-Burch, C. Bannard, and J. Schroeder. 2005. Scal-
ing phrase-based statistical machine translation to larger cor-
pora and longer phrases. In Proc. 43rd Annual Meeting of the
Assoc. for Computational Linguistics (ACL), pages 255?262,
Ann Arbor, MI, June.
D. Chiang. 2005. A hierarchical phrase-based model for statis-
tical machine translation. In Proc. 43rd Annual Meeting of
the Assoc. for Computational Linguistics (ACL), pages 263?
270, Ann Arbor, MI, June.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. Human Language Technology
Conf. / North American Chapter of the Assoc. for Compu-
tational Linguistics Annual Meeting (HLT-NAACL), pages
127?133, Edmonton, Canada, May/June.
L. Mangu, E. Brill, and A. Stolcke. 2000. Finding consensus
in speech recognition: Word error minimization and other
applications of confusion networks. Computer, Speech and
Language, 14(4):373?400, October.
L. Mathias and W. Byrne. 2006. Statistical phrase-based
speech translation. In Proc. IEEE Int. Conf. on Acoustics,
Speech, and Signal Processing (ICASSP), volume 1, pages
561?564, Toulouse, France, May.
H. Ney, R. Haeb-Umbach, B. H. Tran, and M. Oerder. 1992.
Improvements in beam search for 10000-word continuous
speech recognition. In Proc. IEEE Int. Conf. on Acoustics,
Speech, and Signal Processing (ICASSP), volume 1, pages
9?12, San Francisco, CA, March.
F. J. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proc. 41st Annual Meeting of the As-
soc. for Computational Linguistics (ACL), pages 160?167,
Sapporo, Japan, July.
R. Zens and H. Ney. 2004. Improvements in phrase-based sta-
tistical machine translation. In Proc.Human Language Tech-
nology Conf. / North American Chapter of the Assoc. for
Computational Linguistics Annual Meeting (HLT-NAACL),
pages 257?264, Boston, MA, May.
Y. Zhang and S. Vogel. 2005. An efficient phrase-to-phrase
alignment model for arbitrarily long phrases and large cor-
pora. In Proc. 10th Annual Conf. of the European Assoc. for
Machine Translation (EAMT), pages 294?301, Budapest,
Hungary, May.
499
Proceedings of NAACL HLT 2007, Companion Volume, pages 57?60,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Are Very Large N-best Lists Useful for SMT?
Sas?a Hasan, Richard Zens, Hermann Ney
Human Language Technology and Pattern Recognition
Lehrstuhl fu?r Informatik 6 ? Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
{hasan,zens,ney}@cs.rwth-aachen.de
Abstract
This paper describes an efficient method
to extract large n-best lists from a word
graph produced by a statistical machine
translation system. The extraction is based
on the k shortest paths algorithm which
is efficient even for very large k. We
show that, although we can generate large
amounts of distinct translation hypothe-
ses, these numerous candidates are not
able to significantly improve overall sys-
tem performance. We conclude that large
n-best lists would benefit from better dis-
criminating models.
1 Introduction
This paper investigates the properties of large n-
best lists in the context of statistical machine trans-
lation (SMT). We present a method that allows for
fast extraction of very large n-best lists based on
the k shortest paths algorithm by (Eppstein, 1998).
We will argue that, despite being able to generate a
much larger amount of hypotheses than previously
reported in the literature, there is no significant gain
of such a method in terms of translation quality.
In recent years, phrase-based approaches evolved
as the dominating method for feasible machine
translation systems. Many research groups use a de-
coder based on a log-linear approach incorporating
phrases as main paradigm (Koehn et al, 2003). As a
by-product of the decoding process, one can extract
n-best translations from a word graph and use these
fully generated hypotheses for additional reranking.
In the past, several groups report on using n-best
lists with n ranging from 1 000 to 10 000. The ad-
vantage of n-best reranking is clear: we can apply
complex reranking techniques, based e.g. on syntac-
tic analyses of the candidates or using huge addi-
tional language models, since the whole sentence is
already generated. During the generation process,
these models would either need hard-to-implement
algorithms or large memory requirements.
1.1 Related work
The idea of n-best list extraction from a word graph
for SMT was presented in (Ueffing et al, 2002). In
(Zens and Ney, 2005), an improved method is re-
ported that overcomes some shortcomings, such as
duplicate removal by determinization of the word
graph (represented as a weighted finite state automa-
ton) and efficient rest-cost estimation with linear
time complexity.
There are several research groups that use a two-
pass approach in their MT systems. First, they gen-
erate n-best translation hypotheses with the decoder.
Second, they apply additional models to the out-
put and rerank the candidates (see e.g. (Chen et al,
2006)).
Syntactic features were investigated in (Och et al,
2004) with moderate success. Although complex
models, such as features based on shallow parsing or
treebank-based syntactic analyses, were applied to
the n-best candidates, the ?simpler? ones were more
promising (e.g. IBM model 1 on sentence-level).
In the following section 2, we describe our SMT
system and explain how an improved n-best extrac-
tion method is capable of generating a very large
number of distinct candidates from the word graph.
In section 3, we show our experiments related to
n-best list reranking with various sizes and the cor-
responding performance in terms of MT evaluation
measures. Finally, we discuss the results in section 4
and give some conclusive remarks.
57
2 Generating N-best lists
We use a phrase-based SMT system (Mauser et al,
2006) and enhance the n-best list extraction with
Eppstein?s k shortest path algorithm which allows
for generating a very large number of translation
candidates in an efficient way.
2.1 Baseline SMT system
The baseline system uses phrases automatically ex-
tracted from a word-aligned corpus (trained with
GIZA++) and generates the best translations using
weighted log-linear model combination with several
features, such as word lexicon, phrase translation
and language models. This direct approach is cur-
rently used by most state-of-the-art decoders. The
model scaling factors are trained discriminatively on
some evaluation measure, e.g. BLEU or WER, using
the simplex method.
2.2 N-best list extraction
We incorporated an efficient extraction of n best
translations using the k shortest path algorithm
(Eppstein, 1998) into a state-of-the-art SMT system.
The implementation is partly based on code that is
publicly available.1
Starting point for the extraction is a word graph,
generated separately by the decoder for each sen-
tence. Since these word graphs are directed and
acyclic, it is possible to construct a shortest path tree
spanning from the sentence begin node to the end
node. The efficiency of finding the k shortest paths
in this tree lies in the book-keeping of edges through
a binary heap that allows for an implicit representa-
tion of paths. The overall performance of the algo-
rithm is efficient even for large k. Thus, it is feasi-
ble to use in situations where we want to generate a
large number of paths, i.e. translation hypotheses in
this context.
There is another issue that has to be addressed.
In phrase-based SMT, we have to deal with differ-
ent phrase segmentations for each sentence. Due to
the large number of phrases, it is possible that we
have paths through the word graph representing the
same sentence but internally having different phrase
boundaries. In n-best list generation, we want to get
rid of these duplicates. Due to the efficiency of the
k shortest paths algorithm, we allow for generating
a very large number of hypotheses (e.g. 100 ? n) and
1http://www.ics.uci.edu/?eppstein/pubs/
graehl.zip
then filter the output via a prefix tree (also called
trie) until we get n distinct translations.
With this method, it is feasible to generate
100 000-best lists without much hassle. In gen-
eral, the file input/output operations are more time-
consuming than the actual n-best list extraction.
The average generation time of n-best candidates
for each of the sentences of the development list
is approximately 30 seconds on a 2.2GHz Opteron
machine, whereas 7.4 million hypotheses are com-
puted per sentence on average. The overall extrac-
tion time including filtering and writing to hard-disk
takes around 100 seconds per sentence. Note that
this value could be optimized drastically if checking
for how many duplicates are generated on average
beforehand and adjusting the initial number of hy-
potheses before applying the filtering. We only use
the k = 100 ? n as a proof of concept.
2.3 Rescoring models
After having generated the 100 000-best lists, we
have to apply additional rescoring models to all hy-
potheses. We select the models that have shown
to improve overall translation performance as used
for recent NIST MT evaluations. In addition to the
main decoder score (which is already a combination
of several models and constitutes a strong baseline),
these include several large language models trained
on up to 2.5 billion running words, a sentence-level
IBM model 1 score, m-gram posterior probabilities
and an additional sentence length model.
3 Experiments
The experiments in this section are carried out on n-
best lists with n going up to 100 000. We will show
that, although we are capable of generating this large
amount of hypotheses, the overall performance does
not seem to improve significantly beyond a certain
threshold. Or to put it simple: although we generate
lots of hypotheses, most of them are not very useful.
As experimental background, we choose the large
data track of the Chinese-to-English NIST task,
since the length of the sentences and the large vo-
cabulary of the task allow for large n-best lists. For
smaller tasks, e.g. the IWSLT campaign, the domain
is rather limited such that it does not make sense
to generate lists reaching beyond several thousand
hypotheses. As development data, we use the 2002
eval set, whereas for test, the 2005 eval set is chosen.
The corpus statistics are shown in Table 1.
58
Chinese English
Train Sentence Pairs 7M
Running Words 199M 213M
Vocabulary Size 222K 351K
Dev Sentence Pairs 878 3 512
Running Words 25K 105K
Test Sentence Pairs 1 082 4 328
Running Words 33K 148K
Table 1: Corpus statistics for the Chinese-English
NIST MT task.
3.1 Oracle-best hypotheses
In the first experiment, we examined the oracle-best
hypotheses in the n-best lists for several list sizes.
For an efficient calculation of the true BLEU oracle
(the hypothesis which has a maximum BLEU score
when compared to the reference translations), we
use approximations based on WER/PER-oracles, i.e.
we extract the hypotheses that have the lowest edit
distance (WER, word error rate) to the references.
The same is applied by disregarding the word or-
der (leading to PER, position-independent word er-
ror rate).
As can be seen in Table 2, the improvements are
steadily decreasing, i.e. with increasing number of
generated hypotheses, there are less and less use-
ful candidates among them. For the first 10 000
candidates, we therefore have the possibility to find
hypotheses that could increase the BLEU score by
at least 8.3% absolute if our models discriminated
them properly. For the next 90 000 hypotheses, there
is only a small potential to improve the whole sys-
tem by around 1%. This means that most of the
generated hypotheses are not very useful in terms of
oracle-WER and likely distracting the ?search? for
the needle(s) in the haystack. It has been shown in
(Och et al, 2004) that true BLEU oracle scores on
lists with much smaller n ? 4096 are more or less
linear in log(n). Our results support this claim since
the oracle-WER/PER is a lower bound of the real
BLEU oracle. For the PER criterion, the behavior of
the oracle-best hypotheses is similar. Here we can
notice that after 10,000 hypotheses, the BLEU score
of the oracle-PER hypotheses stays the same.
These observations already impair the alleged
usefulness of a large amount of translation hypothe-
ses by showing that the overall possible gain with in-
creasing n gets disproportionately small if one puts
it in relation to the exponential growth of the n.
Oracle-WER [%] Oracle-PER [%]
N BLEU abs. imp. BLEU abs. imp.
1 36.1 36.1
10 38.8 +2.7 38.0 +1.9
100 41.3 +2.5 39.8 +1.8
1000 43.3 +2.0 41.0 +1.2
10000 44.4 +1.1 42.0 +1.0
100000 45.3 +0.9 42.0 +0.0
Table 2: Dev BLEU scores of oracle-best hypothe-
ses based on minimum WER/PER.
3.2 Rescoring performance
As a next step, we show the performance of tuning
the model scaling factors towards best translation
performance. In our experiments, we use the BLEU
score as objective function of the simplex method.
Figure 1 shows the graphs for the development
(on the left) and test set (on the right). The up-
per graphs depict the oracle-WER BLEU scores (cf.
also Table 2) for comparison. As was already stated,
these are a lower bound since the real oracle-BLEU
hypotheses might have even higher scores. Still, it is
an indicator of what could be achieved if the models
discriminated good from bad hypotheses properly.
The lower two graphs show the behavior when
(a) optimizing and extracting hypotheses on a sub-
set (the first n) of the 100k-best hypotheses and (b)
optimizing on a subset but extracting from the full
100k set. As can be seen, extracting from the full
set does not even help for the development data on
which the scaling factors were tuned. Experiments
on the test list show similar results. We can also
observe that the improvement declines rapidly with
higher n. Note that an optimization on the full 100k
list was not possible due to huge memory require-
ments. The highest n that fit into the 16GB machine
was 60 000. Thus, this setting was used for extrac-
tion on the full 100k set.
The results so far indicate that it is not very use-
ful to go beyond n = 10000. For the development
set, the baseline of 36.1% BLEU can be improved
by 1.6% absolute to 37.7% for the first 10k entries,
whereas for the 60k setting, the absolute improve-
ment is only increased by a marginal 0.1%. For the
chosen setting, whose focus was on various list sizes
for optimization and extraction, the improvements
on the development lists do not carry over to the test
list. From the baseline of 31.5%, we only get a mod-
erate improvement of approximately 0.5% BLEU.
59
 36
 37
 38
 39
 40
 41
 42
 43
 44
 45
 46
 1  10  100  1000  10000  100000
BL
EU
[%
]
N
Oracle-WER
(a) opt. on N, extr. on N
(b) opt. on N, extr. on 100k
 31
 32
 33
 34
 35
 36
 37
 38
 1  10  100  1000  10000  100000
BL
EU
[%
]
N
Oracle-WER
(a) extraction on N
(b) extraction on 100k
Figure 1: BLEU scores of the reranked system. Development set (left) vs. Test set (right).
One possible explanation for this lies in the poor
performance of the rescoring models. A short test
was carried out in which we added the reference
translations to the n-best list and determined the cor-
responding scores of the additional models, such as
the large LM and the IBM model 1. Interestingly,
only less than 1/4 of the references was ranked as
the best hypothesis. Thus, most reference transla-
tions would never have been selected as final candi-
dates. This strongly indicates that we have to come
up with better models in order to make significant
improvements from large n-best lists. Furthermore,
it seems that the exponential growth of n-best hy-
potheses for maintaining a quasilinear improvement
in oracle BLEU score has a strong impact on the
overall system performance. This is in contrast to a
word graph, where a linear increment of its density
yields disproportionately high improvements in ora-
cle BLEU for lower densities (Zens and Ney, 2005).
4 Conclusion
We described an efficient n-best list extraction
method that is based on the k shortest paths algo-
rithm. Experiments with large 100 000-best lists in-
dicate that the models do not have the discriminating
power to separate the good from the bad candidates.
The oracle-best BLEU scores stay linear in log(n),
whereas the reranked system performance seems to
saturate at around 10k best translations given the ac-
tual models. Using more hypotheses currently does
not help to significantly improve translation quality.
Given the current results, one should balance the
advantages of n-best lists, e.g. easily testing com-
plex rescoring models, and word graphs, e.g. repre-
sentation of a much larger hypotheses space. How-
ever, as long as the models are not able to correctly
fire on good candidates, both approaches will stay
beneath their capabilities.
Acknowledgments
This material is partly based upon work supported by the De-
fense Advanced Research Projects Agency (DARPA) under
Contract No. HR0011-06-C-0023, and was partly funded by
the Deutsche Forschungsgemeinschaft (DFG) under the project
?Statistische Textu?bersetzung? (NE 572/5-3).
References
B. Chen, R. Cattoni, N. Bertoldi, M. Cettolo, and M. Federico.
2006. The ITC-irst SMT system for IWSLT 2006. In Proc.
of the International Workshop on Spoken Language Transla-
tion, pages 53?58, Kyoto, Japan, November.
D. Eppstein. 1998. Finding the k shortest paths. SIAM J. Com-
puting, 28(2):652?673.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of the Human Language Tech-
nology Conf. (HLT-NAACL), pages 127?133, Edmonton,
Canada, May/June.
A. Mauser, R. Zens, E. Matusov, S. Hasan, and H. Ney. 2006.
The RWTH statistical machine translation system for the
IWSLT 2006 evaluation. In Proc. of the International Work-
shop on Spoken Language Translation, pages 103?110, Ky-
oto, Japan, November.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada, A.
Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain, Z. Jin,
and D. Radev. 2004. A smorgasbord of features for statisti-
cal machine translation. In Proc. 2004 Meeting of the North
American chapter of the Association for Computational Lin-
guistics (HLT-NAACL), pages 161?168, Boston, MA, May.
N. Ueffing, F. J. Och, and H. Ney. 2002. Generation of word
graphs in statistical machine translation. In Proc. of the
Conf. on Empirical Methods for Natural Language Process-
ing (EMNLP), pages 156?163, Philadelphia, PA, July.
R. Zens and H. Ney. 2005. Word graphs for statistical ma-
chine translation. In 43rd Annual Meeting of the Assoc. for
Computational Linguistics: Proc. Workshop on Building and
Using Parallel Texts, pages 191?198, Ann Arbor, MI, June.
60
A Comparative Study on Reordering Constraints in Statistical Machine
Translation
Richard Zens and Hermann Ney
Chair of Computer Science VI
RWTH Aachen - University of Technology
{zens,ney}@cs.rwth-aachen.de
Abstract
In statistical machine translation, the gen-
eration of a translation hypothesis is com-
putationally expensive. If arbitrary word-
reorderings are permitted, the search prob-
lem is NP-hard. On the other hand, if
we restrict the possible word-reorderings
in an appropriate way, we obtain a
polynomial-time search algorithm.
In this paper, we compare two different re-
ordering constraints, namely the ITG con-
straints and the IBM constraints. This
comparison includes a theoretical dis-
cussion on the permitted number of re-
orderings for each of these constraints.
We show a connection between the ITG
constraints and the since 1870 known
Schro?der numbers.
We evaluate these constraints on two
tasks: the Verbmobil task and the Cana-
dian Hansards task. The evaluation con-
sists of two parts: First, we check how
many of the Viterbi alignments of the
training corpus satisfy each of these con-
straints. Second, we restrict the search to
each of these constraints and compare the
resulting translation hypotheses.
The experiments will show that the base-
line ITG constraints are not sufficient
on the Canadian Hansards task. There-
fore, we present an extension to the ITG
constraints. These extended ITG con-
straints increase the alignment coverage
from about 87% to 96%.
1 Introduction
In statistical machine translation, we are given
a source language (?French?) sentence fJ1 =
f1 . . . fj . . . fJ , which is to be translated into a target
language (?English?) sentence eI1 = e1 . . . ei . . . eI .
Among all possible target language sentences, we
will choose the sentence with the highest probabil-
ity:
e?I1 = argmax
eI1
{Pr(eI1|fJ1 )} (1)
= argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)} (2)
The decomposition into two knowledge sources
in Eq. 2 is the so-called source-channel approach
to statistical machine translation (Brown et al,
1990). It allows an independent modeling of tar-
get language model Pr(eI1) and translation model
Pr(fJ1 |eI1). The target language model describes
the well-formedness of the target language sentence.
The translation model links the source language sen-
tence to the target language sentence. It can be fur-
ther decomposed into alignment and lexicon model.
The argmax operation denotes the search problem,
i.e. the generation of the output sentence in the tar-
get language. We have to maximize over all possible
target language sentences.
In this paper, we will focus on the alignment
problem, i.e. the mapping between source sen-
tence positions and target sentence positions. As
the word order in source and target language may
differ, the search algorithm has to allow certain
word-reorderings. If arbitrary word-reorderings are
allowed, the search problem is NP-hard (Knight,
1999). Therefore, we have to restrict the possible
reorderings in some way to make the search prob-
lem feasible. Here, we will discuss two such con-
straints in detail. The first constraints are based on
inversion transduction grammars (ITG) (Wu, 1995;
Wu, 1997). In the following, we will call these the
ITG constraints. The second constraints are the IBM
constraints (Berger et al, 1996). In the next section,
we will describe these constraints from a theoretical
point of view. Then, we will describe the resulting
search algorithm and its extension for word graph
generation. Afterwards, we will analyze the Viterbi
alignments produced during the training of the align-
ment models. Then, we will compare the translation
results when restricting the search to either of these
constraints.
2 Theoretical Discussion
In this section, we will discuss the reordering con-
straints from a theoretical point of view. We will
answer the question of how many word-reorderings
are permitted for the ITG constraints as well as for
the IBM constraints. Since we are only interested
in the number of possible reorderings, the specific
word identities are of no importance here. Further-
more, we assume a one-to-one correspondence be-
tween source and target words. Thus, we are inter-
ested in the number of word-reorderings, i.e. permu-
tations, that satisfy the chosen constraints. First, we
will consider the ITG constraints. Afterwards, we
will describe the IBM constraints.
2.1 ITG Constraints
Let us now consider the ITG constraints. Here, we
interpret the input sentence as a sequence of blocks.
In the beginning, each position is a block of its own.
Then, the permutation process can be seen as fol-
lows: we select two consecutive blocks and merge
them to a single block by choosing between two op-
tions: either keep them in monotone order or invert
the order. This idea is illustrated in Fig. 1. The white
boxes represent the two blocks to be merged.
Now, we investigate, how many permutations are
obtainable with this method. A permutation derived
by the above method can be represented as a binary
tree where the inner nodes are colored either black or
white. At black nodes the resulting sequences of the
children are inverted. At white nodes they are kept in
monotone order. This representation is equivalent to
source positions
ta
rg
et
 p
os
it
io
ns
without inversion with inversion
Figure 1: Illustration of monotone and inverted con-
catenation of two consecutive blocks.
the parse trees of the simple grammar in (Wu, 1997).
We observe that a given permutation may be con-
structed in several ways by the above method. For
instance, let us consider the identity permutation of
1, 2, ..., n. Any binary tree with n nodes and all in-
ner nodes colored white (monotone order) is a pos-
sible representation of this permutation. To obtain
a unique representation, we pose an additional con-
straint on the binary trees: if the right son of a node
is an inner node, it has to be colored with the oppo-
site color. With this constraint, each of these binary
trees is unique and equivalent to a parse tree of the
?canonical-form? grammar in (Wu, 1997).
In (Shapiro and Stephens, 1991), it is shown that
the number of such binary trees with n nodes is
the (n ? 1)th large Schro?der number Sn?1. The
(small) Schro?der numbers have been first described
in (Schro?der, 1870) as the number of bracketings of
a given sequence (Schro?der?s second problem). The
large Schro?der numbers are just twice the Schro?der
numbers. Schro?der remarked that the ratio between
two consecutive Schro?der numbers approaches 3 +
2?2 = 5.8284... . A second-order recurrence for
the large Schro?der numbers is:
(n+ 1)Sn = 3(2n? 1)Sn?1 ? (n? 2)Sn?2
with n ? 2 and S0 = 1, S1 = 2.
The Schro?der numbers have many combinatori-
cal interpretations. Here, we will mention only two
of them. The first one is another way of view-
ing at the ITG constraints. The number of permu-
tations of the sequence 1, 2, ..., n, which avoid the
subsequences (3, 1, 4, 2) and (2, 4, 1, 3), is the large
Schro?der number Sn?1. More details on forbidden
subsequences can be found in (West, 1995). The
interesting point is that a search with the ITG con-
straints cannot generate a word-reordering that con-
tains one of these two subsequences. In (Wu, 1997),
these forbidden subsequences are called ?inside-out?
transpositions.
Another interpretation of the Schro?der numbers is
given in (Knuth, 1973): The number of permutations
that can be sorted with an output-restricted double-
ended queue (deque) is exactly the large Schro?der
number. Additionally, Knuth presents an approxi-
mation for the large Schro?der numbers:
Sn ? c ? (3 +
?
8)n ? n? 32 (3)
where c is set to 12
?
(3?2? 4)/pi. This approxi-
mation function confirms the result of Schro?der, and
we obtain Sn ? ?((3 +
?8)n), i.e. the Schro?der
numbers grow like (3 +?8)n ? 5.83n.
2.2 IBM Constraints
In this section, we will describe the IBM constraints
(Berger et al, 1996). Here, we mark each position in
the source sentence either as covered or uncovered.
In the beginning, all source positions are uncovered.
Now, the target sentence is produced from bottom to
top. A target position must be aligned to one of the
first k uncovered source positions. The IBM con-
straints are illustrated in Fig. 2.
J
uncovered position
covered positionuncovered position for extension
1 j
Figure 2: Illustration of the IBM constraints.
For most of the target positions there are k per-
mitted source positions. Only towards the end of the
sentence this is reduced to the number of remaining
uncovered source positions. Let n denote the length
of the input sequence and let rn denote the permitted
number of permutations with the IBM constraints.
Then, we obtain:
rn =
{ kn?k ? k! n > k
n! n ? k (4)
Typically, k is set to 4. In this case, we obtain an
asymptotic upper and lower bound of 4n, i.e. rn ?
?(4n).
In Tab. 1, the ratio of the number of permitted re-
orderings for the discussed constraints is listed as
a function of the sentence length. We see that for
longer sentences the ITG constraints allow for more
reorderings than the IBM constraints. For sentences
of length 10 words, there are about twice as many
reorderings for the ITG constraints than for the IBM
constraints. This ratio steadily increases. For longer
sentences, the ITG constraints allow for much more
flexibility than the IBM constraints.
3 Search
Now, let us get back to more practical aspects. Re-
ordering constraints are more or less useless, if they
do not allow the maximization of Eq. 2 to be per-
formed in an efficient way. Therefore, in this sec-
tion, we will describe different aspects of the search
algorithm for the ITG constraints. First, we will
present the dynamic programming equations and the
resulting complexity. Then, we will describe prun-
ing techniques to accelerate the search. Finally, we
will extend the basic algorithm for the generation of
word graphs.
3.1 Algorithm
The ITG constraints allow for a polynomial-time
search algorithm. It is based on the following dy-
namic programming recursion equations. During
the search a table Qjl,jr,eb,et is constructed. Here,
Qjl,jr,eb,et denotes the probability of the best hy-
pothesis translating the source words from position
jl (left) to position jr (right) which begins with the
target language word eb (bottom) and ends with the
word et (top). This is illustrated in Fig. 3.
Here, we initialize this table with monotone trans-
lations of IBM Model 4. Therefore, Q0jl,jr,eb,et de-
notes the probability of the best monotone hypothe-
sis of IBM Model 4. Alternatively, we could use any
other single-word based lexicon as well as phrase-
based models for this initialization. Our choice is
the IBM Model4 to make the results as comparable
Table 1: Ratio of the number of permitted reorderings with the ITG constraints Sn?1 and the IBM constraints
rn for different sentence lengths n.
n 1 ... 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Sn?1/rn ? 1.0 1.2 1.4 1.7 2.1 2.6 3.4 4.3 5.6 7.4 9.8 13.0 17.4 23.3 31.4
jl jr
e b
et
Figure 3: Illustration of the Q-table.
as possible to the search with the IBM constraints.
We introduce a new parameter pm (m=? monotone),
which denotes the probability of a monotone combi-
nation of two partial hypotheses.
Qjl,jr,eb,et = (5)
max
jl?k<jr,
e?,e??
{
Q0jl,jr,eb,et ,
Qjl,k,eb,e? ?Qk+1,jr,e??,et ? p(e??|e?) ? pm,
Qk+1,jr,eb,e? ?Qjl,k,e??,et ? p(e??|e?) ? (1? pm)
}
We formulated this equation for a bigram lan-
guage model, but of course, the same method can
also be applied for a trigram language model. The
resulting algorithm is similar to the CYK-parsing al-
gorithm. It has a worst-case complexity of O(J3 ?
E4). Here, J is the length of the source sentence
and E is the vocabulary size of the target language.
3.2 Pruning
Although the described search algorithm has a
polynomial-time complexity, even with a bigram
language model the search space is very large. A full
search is possible but time consuming. The situation
gets even worse when a trigram language model is
used. Therefore, pruning techniques are obligatory
to reduce the translation time.
Pruning is applied to hypotheses that translate the
same subsequence f jrjl of the source sentence. We
use pruning in the following two ways. The first
pruning technique is histogram pruning: we restrict
the number of translation hypotheses per sequence
f jrjl . For each sequence f
jr
jl , we keep only a fixed
number of translation hypotheses. The second prun-
ing technique is threshold pruning: the idea is to re-
move all hypotheses that have a low probability rela-
tive to the best hypothesis. Therefore, we introduce
a threshold pruning parameter q, with 0 ? q ? 1.
Let Q?jl,jr denote the maximum probability of all
translation hypotheses for f jrjl . Then, we prune a
hypothesis iff:
Qjl,jr,eb,et < q ?Q?jl,jr
Applying these pruning techniques the computa-
tional costs can be reduced significantly with almost
no loss in translation quality.
3.3 Generation of Word Graphs
The generation of word graphs for a bottom-top
search with the IBM constraints is described in
(Ueffing et al, 2002). These methods cannot be
applied to the CYK-style search for the ITG con-
straints. Here, the idea for the generation of word
graphs is the following: assuming we already have
word graphs for the source sequences fkjl and f
jr
k+1,
then we can construct a word graph for the sequence
f jrjl by concatenating the partial word graphs either
in monotone or inverted order.
Now, we describe this idea in a more formal way.
A word graph is a directed acyclic graph (dag) with
one start and one end node. The edges are annotated
with target language words or phrases. We also al-
low ?-transitions. These are edges annotated with
the empty word. Additionally, edges may be anno-
tated with probabilities of the language or translation
model. Each path from start node to end node rep-
resents one translation hypothesis. The probability
of this hypothesis is calculated by multiplying the
probabilities along the path.
During the search, we have to combine two word
graphs in either monotone or inverted order. This
is done in the following way: we are given two
word graphs w1 and w2 with start and end nodes
(s1, g1) and (s2, g2), respectively. First, we add
an ?-transition (g1, s2) from the end node of the
first graph w1 to the start node of the second graph
w2 and annotate this edge with the probability of a
monotone concatenation pm. Second, we create a
copy of each of the original word graphs w1 and w2.
Then, we add an ?-transition (g2, s1) from the end
node of the copied second graph to the start node of
the copied first graph. This edge is annotated with
the probability of a inverted concatenation 1 ? pm.
Now, we have obtained two word graphs: one for a
monotone and one for a inverted concatenation. The
final word graphs is constructed by merging the two
start nodes and the two end nodes, respectively.
Let W (jl, jr) denote the word graph for the
source sequence f jrjl . This graph is constructed
from the word graphs of all subsequences of (jl, jr).
Therefore, we assume, these word graphs have al-
ready been produced. For all source positions k with
jl ? k < jr, we combine the word graphs W (jl, k)
and W (k + 1, jr) as described above. Finally, we
merge all start nodes of these graphs as well as all
end nodes. Now, we have obtained the word graph
W (jl, jr) for the source sequence f jrjl . As initializa-
tion, we use the word graphs of the monotone IBM4
search.
3.4 Extended ITG constraints
In this section, we will extend the ITG constraints
described in Sec. 2.1. This extension will go beyond
basic reordering constraints.
We already mentioned that the use of consecutive
phrases within the ITG approach is straightforward.
The only thing we have to change is the initializa-
tion of the Q-table. Now, we will extend this idea to
phrases that are non-consecutive in the source lan-
guage. For this purpose, we adopt the view of the
ITG constraints as a bilingual grammar as, e.g., in
(Wu, 1997). For the baseline ITG constraints, the
resulting grammar is:
A ? [AA] | ?AA? | f/e | f/? | ?/e
Here, [AA] denotes a monotone concatenation and
?AA? denotes an inverted concatenation.
Let us now consider the case of a source phrase
consisting of two parts f1 and f2. Let e denote the
corresponding target phrase. We add the productions
A ? [e/f1 A ?/f2] | ?e/f1 A ?/f2?
to the grammar. The probabilities of these pro-
ductions are, dependent on the translation direction,
p(e|f1, f2) or p(f1, f2|e), respectively. Obviously,
these productions are not in the normal form of an
ITG, but with the method described in (Wu, 1997),
they can be normalized.
4 Corpus Statistics
In the following sections we will present results on
two tasks. Therefore, in this section we will show
the corpus statistics for each of these tasks.
4.1 Verbmobil
The first task we will present results on is the Verb-
mobil task (Wahlster, 2000). The domain of this
corpus is appointment scheduling, travel planning,
and hotel reservation. It consists of transcriptions
of spontaneous speech. Table 2 shows the corpus
statistics of this corpus. The training corpus (Train)
was used to train the IBM model parameters. The
remaining free parameters, i.e. pm and the model
scaling factors (Och and Ney, 2002), were adjusted
on the development corpus (Dev). The resulting sys-
tem was evaluated on the test corpus (Test).
Table 2: Statistics of training and test corpus for
the Verbmobil task (PP=perplexity, SL=sentence
length).
German English
Train Sentences 58 073
Words 519 523 549 921
Vocabulary 7 939 4 672
Singletons 3 453 1 698
average SL 8.9 9.5
Dev Sentences 276
Words 3 159 3 438
Trigram PP - 28.1
average SL 11.5 12.5
Test Sentences 251
Words 2 628 2 871
Trigram PP - 30.5
average SL 10.5 11.4
Table 3: Statistics of training and test corpus
for the Canadian Hansards task (PP=perplexity,
SL=sentence length).
French English
Train Sentences 1.5M
Words 24M 22M
Vocabulary 100 269 78 332
Singletons 40 199 31 319
average SL 16.6 15.1
Test Sentences 5432
Words 97 646 88 773
Trigram PP ? 179.8
average SL 18.0 16.3
4.2 Canadian Hansards
Additionally, we carried out experiments on the
Canadian Hansards task. This task contains the pro-
ceedings of the Canadian parliament, which are kept
by law in both French and English. About 3 million
parallel sentences of this bilingual data have been
made available by the Linguistic Data Consortium
(LDC). Here, we use a subset of the data containing
only sentences with a maximum length of 30 words.
Table 3 shows the training and test corpus statistics.
5 Evaluation in Training
In this section, we will investigate for each of the
constraints the coverage of the training corpus align-
ment. For this purpose, we compute the Viterbi
alignment of IBM Model 5 with GIZA++ (Och and
Ney, 2000). This alignment is produced without any
restrictions on word-reorderings. Then, we check
for every sentence if the alignment satisfies each of
the constraints. The ratio of the number of satisfied
alignments and the total number of sentences is re-
ferred to as coverage. Tab. 4 shows the results for
the Verbmobil task and for the Canadian Hansards
task. It contains the results for both translation direc-
tions German-English (S?T) and English-German
(T?S) for the Verbmobil task and French-English
(S?T) and English-French (T?S) for the Canadian
Hansards task, respectively.
For the Verbmobil task, the baseline ITG con-
straints and the IBM constraints result in a similar
coverage. It is about 91% for the German-English
translation direction and about 88% for the English-
German translation direction. A significantly higher
Table 4: Coverage on the training corpus for align-
ment constraints for the Verbmobil task (VM) and
for the Canadian Hansards task (CH).
coverage [%]
task constraint S?T T?S
VM IBM 91.0 88.1
ITG baseline 91.6 87.0
extended 96.5 96.9
CH IBM 87.1 86.7
ITG baseline 81.3 73.6
extended 96.1 95.6
coverage of about 96% is obtained with the extended
ITG constraints. Thus with the extended ITG con-
straints, the coverage increases by about 8% abso-
lute.
For the Canadian Hansards task, the baseline ITG
constraints yield a worse coverage than the IBM
constraints. Especially for the English-French trans-
lation direction, the ITG coverage of 73.6% is very
low. Again, the extended ITG constraints obtained
the best results. Here, the coverage increases from
about 87% for the IBM constraints to about 96% for
the extended ITG constraints.
6 Translation Experiments
6.1 Evaluation Criteria
In our experiments, we use the following error crite-
ria:
? WER (word error rate):
The WER is computed as the minimum num-
ber of substitution, insertion and deletion oper-
ations that have to be performed to convert the
generated sentence into the target sentence.
? PER (position-independent word error rate):
A shortcoming of the WER is the fact that it
requires a perfect word order. The PER com-
pares the words in the two sentences ignoring
the word order.
? mWER (multi-reference word error rate):
For each test sentence, not only a single refer-
ence translation is used, as for the WER, but a
whole set of reference translations. For each
translation hypothesis, the WER to the most
similar sentence is calculated (Nie?en et al,
2000).
? BLEU score:
This score measures the precision of unigrams,
bigrams, trigrams and fourgrams with respect
to a whole set of reference translations with a
penalty for too short sentences (Papineni et al,
2001). BLEU measures accuracy, i.e. large
BLEU scores are better.
? SSER (subjective sentence error rate):
For a more detailed analysis, subjective judg-
ments by test persons are necessary. Each
translated sentence was judged by a human ex-
aminer according to an error scale from 0.0 to
1.0 (Nie?en et al, 2000).
6.2 Translation Results
In this section, we will present the translation results
for both the IBM constraints and the baseline ITG
constraints. We used a single-word based search
with IBM Model 4. The initialization for the ITG
constraints was done with monotone IBM Model 4
translations. So, the only difference between the two
systems are the reordering constraints.
In Tab. 5 the results for the Verbmobil task are
shown. We see that the results on this task are sim-
ilar. The search with the ITG constraints yields
slightly lower error rates.
Some translation examples of the Verbmobil task
are shown in Tab. 6. We have to keep in mind,
that the Verbmobil task consists of transcriptions
of spontaneous speech. Therefore, the source sen-
tences as well as the reference translations may have
an unorthodox grammatical structure. In the first
example, the German verb-group (?wu?rde vorschla-
gen?) is split into two parts. The search with the
ITG constraints is able to produce a correct transla-
tion. With the IBM constraints, it is not possible to
translate this verb-group correctly, because the dis-
tance between the two parts is too large (more than
four words). As we see in the second example, in
German the verb of a subordinate clause is placed at
the end (?u?bernachten?). The IBM search is not able
to perform the necessary long-range reordering, as it
is done with the ITG search.
7 Related Work
The ITG constraints were introduced in (Wu, 1995).
The applications were, for instance, the segmenta-
tion of Chinese character sequences into Chinese
?words? and the bracketing of the source sentence
into sub-sentential chunks. In (Wu, 1996) the base-
line ITG constraints were used for statistical ma-
chine translation. The resulting algorithm is simi-
lar to the one presented in Sect. 3.1, but here, we
use monotone translation hypotheses of the full IBM
Model 4 as initialization, whereas in (Wu, 1996) a
single-word based lexicon model is used. In (Vilar,
1998) a model similar to Wu?s method was consid-
ered.
8 Conclusions
We have described the ITG constraints in detail and
compared them to the IBM constraints. We draw the
following conclusions: especially for long sentences
the ITG constraints allow for higher flexibility in
word-reordering than the IBM constraints. Regard-
ing the Viterbi alignment in training, the baseline
ITG constraints yield a similar coverage as the IBM
constraints on the Verbmobil task. On the Canadian
Hansards task the baseline ITG constraints were not
sufficient. With the extended ITG constraints the
coverage improves significantly on both tasks. On
the Canadian Hansards task the coverage increases
from about 87% to about 96%.
We have presented a polynomial-time search al-
gorithm for statistical machine translation based on
the ITG constraints and its extension for the gen-
eration of word graphs. We have shown the trans-
lation results for the Verbmobil task. On this task,
the translation quality of the search with the base-
line ITG constraints is already competitive with the
results for the IBM constraints. Therefore, we ex-
pect the search with the extended ITG constraints to
outperform the search with the IBM constraints.
Future work will include the automatic extraction
of the bilingual grammar as well as the use of this
grammar for the translation process.
References
A. L. Berger, P. F. Brown, S. A. D. Pietra, V. J. D. Pietra,
J. R. Gillett, A. S. Kehler, and R. L. Mercer. 1996.
Language translation apparatus and method of using
context-based translation models, United States patent,
patent number 5510981, April.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
Table 5: Translation results on the Verbmobil task.
type automatic human
System WER [%] PER [%] mWER [%] BLEU [%] SSER [%]
IBM 46.2 33.3 40.0 42.5 40.8
ITG 45.6 33.9 40.0 37.1 42.0
Table 6: Verbmobil: translation examples.
source ja, ich wu?rde den Flug um viertel nach sieben vorschlagen.
reference yes, I would suggest the flight at a quarter past seven.
ITG yes, I would suggest the flight at seven fifteen.
IBM yes, I would be the flight at quarter to seven suggestion.
source ich schlage vor, dass wir in Hannover im Hotel Gru?nschnabel u?bernachten.
reference I suggest to stay at the hotel Gru?nschnabel in Hanover.
ITG I suggest that we stay in Hanover at hotel Gru?nschnabel.
IBM I suggest that we are in Hanover at hotel Gru?nschnabel stay.
translation. Computational Linguistics, 16(2):79?85,
June.
K. Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607?615, December.
D. E. Knuth. 1973. The Art of Computer Program-
ming, volume 1 - Fundamental Algorithms. Addison-
Wesley, Reading, MA, 2nd edition.
S. Nie?en, F. J. Och, G. Leusch, and H. Ney. 2000.
An evaluation tool for machine translation: Fast eval-
uation for MT research. In Proc. of the Second Int.
Conf. on Language Resources and Evaluation (LREC),
pages 39?45, Athens, Greece, May.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proc. of the 38th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 440?447, Hong Kong, October.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 295?302, July.
K. A. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Technical Report RC22176 (W0109-022),
IBM Research Division, Thomas J. Watson Research
Center, September.
E. Schro?der. 1870. Vier combinatorische Probleme.
Zeitschrift fu?r Mathematik und Physik, 15:361?376.
L. Shapiro and A. B. Stephens. 1991. Boostrap percola-
tion, the Schro?der numbers, and the n-kings problem.
SIAM Journal on Discrete Mathematics, 4(2):275?
280, May.
N. Ueffing, F. J. Och, and H. Ney. 2002. Generation
of word graphs in statistical machine translation. In
Proc. Conf. on Empirical Methods for Natural Lan-
guage Processing, pages 156?163, Philadelphia, PA,
July.
J. M. Vilar. 1998. Aprendizaje de Transductores Subse-
cuenciales para su empleo en tareas de Dominio Re-
stringido. Ph.D. thesis, Universidad Politecnica de Va-
lencia.
W. Wahlster, editor. 2000. Verbmobil: Foundations
of speech-to-speech translations. Springer Verlag,
Berlin, Germany, July.
J. West. 1995. Generating trees and the Catalan and
Schro?der numbers. Discrete Mathematics, 146:247?
262, November.
D. Wu. 1995. Stochastic inversion transduction gram-
mars, with application to segmentation, bracketing,
and alignment of parallel corpora. In Proc. of the 14th
International Joint Conf. on Artificial Intelligence (IJ-
CAI), pages 1328?1334, Montreal, August.
D. Wu. 1996. A polynomial-time algorithm for statis-
tical machine translation. In Proc. of the 34th Annual
Conf. of the Association for Computational Linguistics
(ACL ?96), pages 152?158, Santa Cruz, CA, June.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403, September.
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 467?474,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Integration of Speech to Computer-Assisted Translation Using
Finite-State Automata
Shahram Khadivi Richard Zens
Lehrstuhl fu?r Informatik 6 ? Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
{khadivi,zens,ney}@cs.rwth-aachen.de
Hermann Ney
Abstract
State-of-the-art computer-assisted transla-
tion engines are based on a statistical pre-
diction engine, which interactively pro-
vides completions to what a human trans-
lator types. The integration of human
speech into a computer-assisted system is
also a challenging area and is the aim of
this paper. So far, only a few methods
for integrating statistical machine transla-
tion (MT) models with automatic speech
recognition (ASR) models have been stud-
ied. They were mainly based on N -
best rescoring approach. N -best rescor-
ing is not an appropriate search method
for building a real-time prediction engine.
In this paper, we study the incorporation
of MT models and ASR models using
finite-state automata. We also propose
some transducers based on MT models for
rescoring the ASR word graphs.
1 Introduction
A desired feature of computer-assisted transla-
tion (CAT) systems is the integration of the hu-
man speech into the system, as skilled human
translators are faster at dictating than typing the
translations (Brown et al, 1994). Additionally,
incorporation of a statistical prediction engine, i.e.
a statistical interactive machine translation system,
to the CAT system is another useful feature. A sta-
tistical prediction engine provides the completions
to what a human translator types (Foster et al,
1997; Och et al, 2003). Then, one possible proce-
dure for skilled human translators is to provide the
oral translation of a given source text and then to
post-edit the recognized text. In the post-editing
step, a prediction engine helps to decrease the
amount of human interaction (Och et al, 2003).
In a CAT system with integrated speech, two
sources of information are available to recognize
the speech input: the target language speech
and the given source language text. The target
language speech is a human-produced translation
of the source language text. Statistical machine
translation (MT) models are employed to take into
account the source text for increasing the accuracy
of automatic speech recognition (ASR) models.
Related Work
The idea of incorporating ASR and MT models
was independently initiated by two groups:
researchers at IBM (Brown et al, 1994),
and researchers involved in the TransTalk
project (Dymetman et al, 1994; Brousseau
et al, 1995). In (Brown et al, 1994), the
authors proposed a method to integrate the IBM
translation model 2 (Brown et al, 1993) with
an ASR system. The main idea was to design
a language model (LM) to combine the trigram
language model probability with the translation
probability for each target word. They reported a
perplexity reduction, but no recognition results.
In the TransTalk project, the authors improved
the ASR performance by rescoring the ASR
N -best lists with a translation model. They also
introduced the idea of a dynamic vocabulary for
a speech recognition system where translation
models were generated for each source language
sentence. The better performing of the two is the
N -best rescoring.
Recently, (Khadivi et al, 2005) and (Paulik et
al., 2005a; Paulik et al, 2005b) have studied the
integration of ASR and MT models. The first
work showed a detailed analysis of the effect of
different MT models on rescoring the ASR N -best
lists. The other two works considered two parallel
N -best lists, generated by MT and ASR systems,
467
respectively. They showed improvement in the
ASR N -best rescoring when some proposed fea-
tures are extracted from the MT N -best list. The
main concept among all features was to generate
different kinds of language models from the MT
N -best list.
All of the above methods are based on an N -
best rescoring approach. In this paper, we study
different methods for integrating MT models to
ASR word graphs instead of N -best list. We
consider ASR word graphs as finite-state automata
(FSA), then the integration of MT models to ASR
word graphs can benefit from FSA algorithms.
The ASR word graphs are a compact representa-
tion of possible recognition hypotheses. Thus, the
integration of MT models to ASR word graphs can
be considered as an N -best rescoring but with very
large value for N . Another advantage of working
with ASR word graphs is the capability to pass
on the word graphs for further processing. For
instance, the resulting word graph can be used in
the prediction engine of a CAT system (Och et al,
2003).
The remaining part is structured as follows: in
Section 2, a general model for an automatic text
dictation system in the computer-assisted transla-
tion framework will be described. In Section 3,
the details of the machine translation system and
the speech recognition system along with the lan-
guage model will be explained. In Section 4,
different methods for integrating MT models into
ASR models will be described, and also the exper-
imental results will be shown in the same section.
2 Speech-Enabled CAT Models
In a speech-enabled computer-assisted translation
system, we are given a source language sentence
fJ1 = f1 . . . fj . . . fJ , which is to be translated into
a target language sentence eI1 = e1 . . . ei . . . eI ,
and an acoustic signal xT1 = x1 . . . xt . . . xT ,
which is the spoken target language sentence.
Among all possible target language sentences, we
will choose the sentence with the highest probabil-
ity:
e?I?1= argmax
I,eI1
{Pr(eI1|fJ1 , xT1 )} (1)
?= argmax
I,eI1
{Pr(eI1)Pr(fJ1 |eI1)Pr(xT1 |eI1)}(2)
Eq. 1 is decomposed into Eq. 2 by assuming
conditional independency between xT1 and fJ1 .
The decomposition into three knowledge sources
allows for an independent modeling of the target
language model Pr(eI1), the translation model
Pr(fJ1 |eI1) and the acoustic model Pr(xT1 |eI1).
Another approach for modeling the posterior
probability Pr(eI1|fJ1 , xT1 ) is direct modeling us-
ing a log-linear model. The decision rule is given
by:
e?I?1 = argmax
I,eI1
{ M?
m=1
?mhm(eI1, fJ1 , xT1 )
}
(3)
Each of the terms hm(eI1, fJ1 , xT1 ) denotes one
of the various models which are involved in the
recognition procedure. Each individual model is
weighted by its scaling factor ?m. As there is
no direct dependence between fJ1 and xT1 , the
hm(eI1, fJ1 , xT1 ) is in one of these two forms:
hm(eI1, xT1 ) and hm(eI1, fJ1 ). Due to the argmax
operator which denotes the search, no renormal-
ization is considered in Eq. 3. This approach has
been suggested by (Papineni et al, 1997; Papineni
et al, 1998) for a natural language understanding
task, by (Beyerlein, 1998) for an ASR task, and
by (Och and Ney, 2002) for an MT task. This
approach is a generalization of Eq. 2. The di-
rect modeling has the advantage that additional
models can be easily integrated into the overall
system. The model scaling factors ?M1 are trained
on a development corpus according to the final
recognition quality measured by the word error
rate (WER)(Och, 2003).
Search
The search in the MT and the ASR systems is
already very complex, therefore a fully integrated
search to combine ASR and MT models will
considerably increase the complexity. To reduce
the complexity of the search, we perform two
independent searches with the MT and the ASR
systems, the search result of each system will be
represented as a large word graph. We consider
MT and ASR word graphs as FSA. Then, we are
able to use FSA algorithms to integrate MT and
ASR word graphs. The FSA implementation of
the search allows us to use standard optimized
algorithms, e.g. available from an open source
toolkit (Kanthak and Ney, 2004).
The recognition process is performed in two
steps. First, the baseline ASR system generates a
word graph in the FSA format for a given utterance
xT1 . Second, the translation models rescore each
word graph based on the corresponding source
language sentence. For each utterance, the deci-
sion about the best sentence is made according to
the recognition and the translation models.
468
3 Baseline Components
In this section, we briefly describe the basic sys-
tem components, namely the MT and the ASR
systems.
3.1 Machine Translation System
We make use of the RWTH phrase-based statis-
tical machine translation system for the English
to German automatic translation. The system in-
cludes the following models: an n-gram language
model, a phrase translation model and a word-
based lexicon model. The latter two models are
used for both directions: German to English and
English to German. Additionally, a word penalty
and a phrase penalty are included. The reordering
model of the baseline system is distance-based, i.e.
it assigns costs based on the distance from the end
position of a phrase to the start position of the next
phrase. More details about the baseline system
can be found in (Zens and Ney, 2004; Zens et al,
2005).
3.2 Automatic Speech Recognition System
The acoustic model of the ASR system is trained
on the VerbMobil II corpus (Sixtus et al, 2000).
The corpus consists of German large-vocabulary
conversational speech: 36k training sentences
(61.5h) from 857 speakers. The test corpus is
created from the German part of the bilingual
English-German XEROX corpus (Khadivi et al,
2005): 1562 sentences including 18k running
words (2.6h) from 10 speakers. The test cor-
pus contains 114 out-of-vocabulary (OOV) words.
The remaining part of the XEROX corpus is used
to train a back off trigram language model us-
ing the SRI language modeling toolkit (Stolcke,
2002). The LM perplexity of the speech recogni-
tion test corpus is about 83. The acoustic model of
the ASR system can be characterized as follows:
? recognition vocabulary of 16716 words;
? 3-state-HMM topology with skip;
? 2500 decision tree based generalized within-
word triphone states including noise plus one
state for silence;
? 237k gender independent Gaussian densities
with global pooled diagonal covariance;
? 16 MFCC features;
? 33 acoustic features after applying LDA;
? LDA is fed with 11 subsequent MFCC vec-
tors;
? maximum likelihood training using Viterbi
approximation.
Table 1: Statistics of the machine translation cor-
pus.
English German
Train: Sentences 47 619
Running Words 528 779 467 633
Vocabulary 9 816 16 716
Singletons 2 302 6 064
Dev: Sentences 700
Running Words 8 823 8 050
Unknown words 56 108
Eval: Sentences 862
Running Words 11 019 10 094
Unknown words 58 100
The test corpus recognition word error rate is
20.4%. Compared to the previous system (Khadivi
et al, 2005), which has a WER of 21.2%, we
obtain a 3.8% relative improvement in WER. This
improvement is due to a better and complete opti-
mization of the overall ASR system.
4 Integration Approaches
In this section, we will introduce several ap-
proaches to integrate the MT models with the ASR
models. To present the content of this section in a
more reader-friendly way, we will first explain the
task and corpus statistics, then we will present the
results of N -best rescoring. Afterwards, we will
describe the new methods for integrating the MT
models with the ASR models. In each sub-section,
we will also present the recognition results.
4.1 Task
The translation models are trained on the part of
the English-German XEROX corpus which was
not used in the speech recognition test corpus. We
divide the speech recognition test corpus into two
parts, the first 700 utterances as the development
corpus and the rest as the evaluation corpus. The
development corpus is used to optimize the scal-
ing factors of different models (explained in Sec-
tion 2). The statistics of the corpus are depicted in
Table 1. The German part of the training corpus is
also used to train the language model.
4.2 N -best Rescoring
To rescore the N -best lists, we use the method
of (Khadivi et al, 2005). But the results shown
here are different from that work due to a better
optimization of the overall ASR system, using a
469
Table 2: Recognition WER [%] using N -best
rescoring method.
Models Dev Eval
MT 47.1 50.5
ASR 19.3 21.3
ASR+MT IBM-1 17.8 19.0
HMM 18.2 19.2
IBM-3 17.1 18.4
IBM-4 17.1 18.3
IBM-5 16.6 18.2
Phrase
-based 18.8 20.3
better MT system, and generating a larger N -best
list from the ASR word graphs. We rescore the
ASR N -best lists with the standard HMM (Vogel
et al, 1996) and IBM (Brown et al, 1993) MT
models. The development and evaluation sets N -
best lists sizes are sufficiently large to achieve
almost the best possible results, on average 1738
hypotheses per each source sentence are extracted
from the ASR word graphs.
The recognition results are summarized in Ta-
ble 2. In this table, the translation results of the
MT system are shown first, which are obtained
using the phrase-based approach. Then the recog-
nition results of the ASR system are shown. After-
wards, the results of combined speech recognition
and translation models are presented.
For each translation model, the N -best lists
are rescored based on the translation probability
p(eI1|fJ1 ) of that model and the probabilities of
speech recognition and language models. In the
last row of Table 2, the N -best lists are rescored
based on the full machine translation system ex-
plained in Section 3.1.
The best possible hypothesis achievable from
the N -best list has the WER (oracle WER) of
11.2% and 12.4% for development and test sets,
respectively.
4.3 Direct Integration
At the first glance, an obvious method to combine
the ASR and MT systems is the integration at the
level of word graphs. This means the ASR system
generates a large word graph for the input target
language speech, and the MT system also gener-
ates a large word graph for the source language
text. Both MT and ASR word graphs are in the
target language. These two word graphs can be
considered as two FSA, then using FSA theory,
we can integrate two word graphs by applying the
composition algorithm.
We conducted a set of experiments to integrate
the ASR and MT systems using this method. We
obtain a WER of 19.0% and 20.9% for devel-
opment and evaluation sets, respectively. The
results are comparable to N -best rescoring results
for the phrase-based model which is presented in
Table 2. The achieved improvements over the
ASR baseline are statistically significant at the
99% level (Bisani and Ney, 2004). However, the
results are not promising compared to the results
of the rescoring method presented in Table 2 for
HMM and IBM translation models. A detailed
analysis revealed that only 31.8% and 26.7% of
sentences in the development and evaluation sets
have identical paths in both FSA, respectively. In
other words, the search algorithm was not able to
find any identical paths in two given FSA for the
remaining sentences. Thus, the two FSA are very
different from each other. One explanation for
the failure of this method is the large difference
between the WERs of two systems, as shown in
Table 2 the WER for the MT system is more than
twice as high as for the ASR system.
4.4 Integrated Search
In Section 4.3, two separate word graphs are
generated using the MT and the ASR systems.
Another explanation for the failure of the direct
integration method is the independent search to
generate the word graphs. The search in the MT
and the ASR systems is already very complex,
therefore a full integrated search to combine ASR
and MT models will considerably increase the
complexity.
However, it is possible to reduce this problem
by integrating the ASR word graphs into the gen-
eration process of the MT word graphs. This
means, the ASR word graph is used in addition to
the usual language model. This kind of integration
forces the MT system to generate identical paths to
those in the ASR word graph. Using this approach,
the number of identical paths in MT and ASR
word graphs are increased to 39.7% and 34.4%
of the sentences in development and evaluation
sets, respectively. The WER of the integrated
system are 19.0% and 20.7% for development and
evaluation sets.
4.5 Lexicon-Based Transducer
The idea of a dynamic vocabulary, restricting and
weighting the word lexicon of the ASR was first
470
introduced in (Brousseau et al, 1995). The idea
was also seen later in (Paulik et al, 2005b), they
extract the words of the MT N -best list to restrict
the vocabulary of the ASR system. But they both
reported a negative effect from this method on
the recognition accuracy. Here, we extend the
dynamic vocabulary idea by weighting the ASR
vocabulary based on the source language text and
the translation models. We use the lexicon model
of the HMM and the IBM MT models. Based on
these lexicon models, we assign to each possible
target word e the probability Pr(e|fJ1 ). One way
to compute this probability is inspired by IBM
Model 1:
Pr(e|fJ1 ) =
1
J + 1
J?
j=0
p(e|fj)
We can design a simple transducer (or more pre-
cisely an acceptor) using probability in Eq. 4 to
efficiently rescore all paths (hypotheses) in the
word graph with IBM Model 1:
PIBM-1(eI1|fJ1 ) =
1
(J + 1)I
I?
i=1
J?
j=0
p(ei|fj)
=
I?
i=1
1
(J + 1) ? p(ei|f
J
1 )
The transducer is formed by one node and a num-
ber of self loops for each target language word. In
each arc of this transducer, the input label is target
word e and the weight is ? log 1J+1 ? p(e|fJ1 ).
We conducted experiments using the proposed
transducer. We built different transducers with the
lexicons of HMM and IBM translation models. In
Table 3, the recognition results of the rescored
word graphs are shown. The results are very
promising compared to the N -best list rescoring,
especially as the designed transducer is very sim-
ple. Similar to the results for the N -best rescoring
approach, these experiments also show the benefit
of using HMM and IBM Models to rescore the
ASR word graphs.
Due to its simplicity, this model can be easily
integrated into the ASR search. It is a sentence
specific unigram LM.
4.6 Phrase-Based Transducer
The phrase-based translation model is the main
component of our translation system. The pairs
of source and corresponding target phrases are
extracted from the word-aligned bilingual training
Table 3: Recognition WER [%] using lexicon-
based transducer to rescore ASR word graphs.
Models Dev Eval
ASR 19.3 21.3
ASR+MT IBM-1 17.5 19.0
HMM 17.8 19.2
IBM-3 17.7 18.8
IBM-4 17.8 18.8
IBM-5 17.6 18.9
corpus (Zens and Ney, 2004). In this section, we
design a transducer to rescore the ASR word graph
using the phrase-based model of the MT system.
For each source language sentence, we extract all
possible phrases from the word-aligned training
corpus. Using the target part of these phrases
we build a transducer similar to the lexicon-based
transducer. But instead of a target word on each
arc, we have the target part of a phrase. The weight
of each arc is the negative logarithm of the phrase
translation probability.
This transducer is a good approximation of non-
monotone phrase-based-lexicon score. Using the
designed transducer it is possible that some parts
of the source texts are not covered or covered more
than once. Then, this model can be compared
to the IBM-3 and IBM-4 models, as they also
have the same characteristic in covering the source
words. The above assumption is not critical for
rescoring the ASR word graphs, as we are con-
fident that the word order is correct in the ASR
output. In addition, we assume low probability for
the existence of phrase pairs that have the same
target phrase but different source phrases within a
particular source language sentence.
Using the phrase-based transducer to rescore
the ASR word graph results in WER of 18.8%
and 20.2% for development and evaluation sets,
respectively. The improvements are statistically
significant at the 99% level compared to the ASR
system. The results are very similar to the results
obtained using N -best rescoring method. But
the transducer implementation is much simpler
because it does not consider the word-based lex-
icon, the word penalty, the phrase penalty, and
the reordering models, it just makes use of phrase
translation model. The designed transducer is
much faster in rescoring the word graph than the
MT system in rescoring the N -best list. The av-
erage speed to rescore the ASR word graphs with
this transducer is 49.4 words/sec (source language
471
text words), while the average speed to translate
the source language text using the MT system is
8.3 words/sec. The average speed for rescoring
the N -best list is even slower and it depends on
the size of N -best list.
A surprising result of the experiments as has
also been observed in (Khadivi et al, 2005), is that
the phrase-based model, which performs the best
in MT, has the least contribution in improving the
recognition results. The phrase-based model uses
more context in the source language to generate
better translations by means of better word selec-
tion and better word order. In a CAT system, the
ASR system has much better recognition quality
than MT system, and the word order of the ASR
output is correct. On the other hand, the ASR
recognition errors are usually single word errors
and they are independent from the context. There-
fore, the task of the MT models in a CAT system is
to enhance the confidence of the recognized words
based on the source language text, and it seems
that the single word based MT models are more
suitable than phrase-based model in this task.
4.7 Fertility-Based Transducer
In (Brown et al, 1993), three alignment models
are described that include fertility models, these
are IBM Models 3, 4, and 5. The fertility-based
alignment models have a more complicated struc-
ture than the simple IBM Model 1. The fertility
model estimates the probability distribution for
aligning multiple source words to a single target
word. The fertility model provides the probabili-
ties p(?|e) for aligning a target word e to ? source
words. In this section, we propose a method for
rescoring ASR word graphs based on the lexicon
and fertility models.
In (Knight and Al-Onaizan, 1998), some trans-
ducers are described to build a finite-state based
translation system. We use the same transduc-
ers for rescoring ASR word graphs. Here, we
have three transducers: lexicon, null-emitter, and
fertility. The lexicon transducer is formed by
one node and a number of self loops for each
target language word, similar to IBM Model 1
transducer in Section 4.5. On each arc of the
lexicon transducer, there is a lexicon entry: the
input label is a target word e, the output label is
a source word f , and the weight is ? log p(f |e).
The null-emitter transducer, as its name states,
emits the null word with a pre-defined probability
after each input word. The fertility transducer is
also a simple transducer to map zero or several
instances of a source word to one instance of the
source word.
The ASR word graphs are composed succes-
sively with the lexicon, null-emitter, fertility trans-
ducers and finally with the source language sen-
tence. In the resulting transducer, the input labels
of the best path represent the best hypothesis.
The mathematical description of the proposed
method is as follows. We can decompose Eq. 1
using Bayes? decision rule:
e?I?1= argmax
I,eI1
{Pr(eI1|fJ1 , xT1 )} (4)
?= argmax
I,eI1
{Pr(fJ1 )Pr(eI1|fJ1 )Pr(xT1 |eI1)}(5)
In Eq. 5, the term Pr(xT1 |eI1) is the acoustic model
and can be represented with the ASR word graph1,
the term Pr(eI1|fJ1 ) is the translation model of
the target language text to the source language
text. The translation model can be represented
by lexicon, fertility, and null-emitter transducers.
Finally, the term Pr(fJ1 ) is a very simple language
model, it is the source language sentence.
The source language model in Eq. 5 can be
formed into the acceptor form in two different
ways:
1. a linear acceptor, i.e. a sequence of nodes
with one incoming arc and one outgoing arc,
the words of source language text are placed
consecutively in the arcs of the acceptor,
2. an acceptor containing possible permuta-
tions. To limit the permutations, we used an
approach as in (Kanthak et al, 2005).
Each of these two acceptors results in different
constraints for the generation of the hypotheses.
The first acceptor restricts the system to generate
exactly the same source language sentence, while
the second acceptor forces the system to generate
the hypotheses that are a reordered variant of
the source language sentence. The experiments
conducted do not show any significant difference
in the recognition results among the two source
language acceptors, except that the second accep-
tor is much slower than the first acceptor. There-
fore, we use the first model in our experiments.
Table 4 shows the results of rescoring the ASR
word graphs using the fertility-based transducers.
1Actually, the ASR word graph is obtained by using
Pr(xT1 |eI1) and Pr(eI1) models. However, It does not cause
any problem in the modeling, especially when we make use
of the direct modeling, Eq. 3
472
Table 4: Recognition WER [%] using fertility-
based transducer to rescore ASR word graphs.
Models Dev Eval
ASR 19.3 21.3
ASR+MT IBM-3 17.4 18.6
IBM-4 17.4 18.5
IBM-5 17.6 18.7
As Table 4 shows, we get alost the same
or slightly better results when compared to the
lexicon-based transducers.
Another interesting point about Eq. 5 is its simi-
larity to speech translation (translation from target
spoken language to source language text). Then,
we can describe a speech-enabled CAT system
as similar to a speech translation system, except
that we aim to get the best ASR output (the best
path in the ASR word graph) rather than the best
translation. This is because the best translation,
which is the source language sentence, is already
given.
5 Conclusion
We have studied different approaches to integrate
MT with ASR models, mainly using finite-state
automata. We have proposed three types of trans-
ducers to rescore the ASR word graphs: lexicon-
based, phrase-based and fertility-based transduc-
ers. All improvements of the combined models
are statistically significant at the 99% level with
respect to the baseline system, i.e. ASR only.
In general, N -best rescoring is a simplification
of word graph rescoring. As the size of N -best
list is increased, the results obtained by N -best
list rescoring approach the results of the word
graph rescoring. But we should consider that the
statement is correct when we use exactly the same
model and the same implementation to rescore the
N -best list and word graph. Figure 1 shows the
effect of the N -best list size on the recognition
WER of the evaluation set. As we expected, the
recognition results of N -best rescoring improve
as N becomes larger, until the point that the
recognition result converges to its optimum value.
As shown in Figure 1, we should not expect that
word graph rescoring methods outperform the N -
best rescoring method, when the size of N -best
lists are large enough. In Table 2, the recognition
results are calculated using a large enough size for
N -best lists, a maximum of 5,000 per sentence,
which results in the average of 1738 hypotheses
 18
 18.5
 19
 19.5
 20
 20.5
 21
 21.5
 1  10  100  1000  10000
W
E
R
 
[
%
]
Size of N-best list (N), in log scale
IBM-1HMMIBM-3IBM-4IBM-5
Figure 1: The N -best rescoring results for differ-
ent N -best sizes on the evaluation set.
per sentence. An advantage of the word graph
rescoring is the confidence of achieving the best
possible results based on a given rescoring model.
The word graph rescoring methods presented in
this paper improve the baseline ASR system with
statistical significance. The results are competitive
with the best results of N -best rescoring. For the
simple models like IBM-1, the transducer-based
integration generates similar or better results than
N -best rescoring approach. For the more com-
plex translation models, IBM-3 to IBM-5, the
N -best rescoring produces better results than the
transducer-based approach, especially for IBM-
5. The main reason is due to exact estimation
of IBM-5 model scores on the N -best list, while
the transducer-based implementation of IBM-3 to
IBM-5 is not exact and simplified. However, we
observe that the fertility-based transducer which
can be considered as a simplified version of IBM-
3 to IBM-5 models can still obtain good results,
especially if we compare the results on the evalu-
ation set.
Acknowledgement
This work has been funded by the European
Union under the RTD project TransType2 (IST
2001 32091) and the integrated project TC-
STAR - Technology and Corpora for Speech
to Speech Translation -(IST-2002-FP6-506738,
http://www.tc-star.org).
References
P. Beyerlein. 1998. Discriminative model combina-
tion. In Proc. IEEE Int. Conf. on Acoustics, Speech,
and Signal Processing (ICASSP), volume 1, pages
481 ? 484, Seattle, WA, May.
473
M. Bisani and H. Ney. 2004. Bootstrap estimates
for confidence intervals in ASR performance evalu-
ationx. In IEEE International Conference on Acous-
tics, Speech, and Signal Processing, pages 409?412,
Montreal, Canada, May.
J. Brousseau, C. Drouin, G. Foster, P. Isabelle,
R. Kuhn, Y. Normandin, and P. Plamondon. 1995.
French speech recognition in an automatic dictation
system for translators: the transtalk project. In Pro-
ceedings of Eurospeech, pages 193?196, Madrid,
Spain.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311, June.
P. F. Brown, S. F. Chen, S. A. Della Pietra, V. J. Della
Pietra, A. S. Kehler, and R. L. Mercer. 1994. Au-
tomatic speech recognition in machine-aided trans-
lation. Computer Speech and Language, 8(3):177?
187, July.
M. Dymetman, J. Brousseau, G. Foster, P. Isabelle,
Y. Normandin, and P. Plamondon. 1994. Towards
an automatic dictation system for translators: the
TransTalk project. In Proceedings of ICSLP-94,
pages 193?196, Yokohama, Japan.
G. Foster, P. Isabelle, and P. Plamondon. 1997. Target-
text mediated interactive machine translation. Ma-
chine Translation, 12(1):175?194.
S. Kanthak and H. Ney. 2004. FSA: An efficient
and flexible C++ toolkit for finite state automata
using on-demand computation. In Proc. of the 42nd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 510?517, Barcelona,
Spain, July.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and
H. Ney. 2005. Novel reordering approaches in
phrase-based statistical machine translation. In 43rd
Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Building and Using
Parallel Texts: Data-Driven Machine Translation
and Beyond, pages 167?174, Ann Arbor, Michigan,
June.
S. Khadivi, A. Zolnay, and H. Ney. 2005. Automatic
text dictation in computer-assisted translation. In
Interspeech?2005 - Eurospeech, 9th European Con-
ference on Speech Communication and Technology,
pages 2265?2268, Portugal, Lisbon.
K. Knight and Y. Al-Onaizan. 1998. Translation
with finite-state devices. In D. Farwell, L. Gerber,
and E. H. Hovy, editors, AMTA, volume 1529 of
Lecture Notes in Computer Science, pages 421?437.
Springer Verlag.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical ma-
chine translation. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 295?302, Philadelphia, PA, July.
F. J. Och, R. Zens, and H. Ney. 2003. Efficient search
for interactive statistical machine translation. In
EACL03: 10th Conf. of the Europ. Chapter of the
Association for Computational Linguistics, pages
387?393, Budapest, Hungary, April.
F. J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the 41th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
K. A. Papineni, S. Roukos, and R. T. Ward. 1997.
Feature-based language understanding. In EU-
ROSPEECH, pages 1435?1438, Rhodes, Greece,
September.
K. A. Papineni, S. Roukos, and R. T. Ward. 1998.
Maximum likelihood and discriminative training
of direct translation models. In Proc. IEEE Int.
Conf. on Acoustics, Speech, and Signal Processing
(ICASSP), volume 1, pages 189?192, Seattle, WA,
May.
M. Paulik, S. Stu?ker, C. Fu?gen, , T. Schultz, T. Schaaf,
and A. Waibel. 2005a. Speech translation enhanced
automatic speech recognition. In Automatic Speech
Recognition and Understanding Workshop (ASRU),
pages 121?126, Puerto Rico, San Juan.
M. Paulik, C. Fu?gen, S. Stu?ker, T. Schultz, T. Schaaf,
and A. Waibel. 2005b. Document driven machine
translation enhanced ASR. In Interspeech?2005 -
Eurospeech, 9th European Conference on Speech
Communication and Technology, pages 2261?2264,
Portugal, Lisbon.
A. Sixtus, S. Molau, S.Kanthak, R. Schlu?ter, and
H. Ney. 2000. Recent improvements of the
RWTH large vocabulary speech recognition system
on spontaneous speech. In Proc. IEEE Int. Conf. on
Acoustics, Speech, and Signal Processing (ICASSP),
pages 1671 ? 1674, Istanbul, Turkey, June.
A. Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, CO, September.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
COLING ?96: The 16th Int. Conf. on Computational
Linguistics, pages 836?841, Copenhagen, Denmark,
August.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proc. of the
Human Language Technology Conf. (HLT-NAACL),
pages 257?264, Boston, MA, May.
R. Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov,
J. Xu, Y. Zhang, and H. Ney. 2005. The RWTH
phrase-based statistical machine translation system.
In Proceedings of the International Workshop on
Spoken Language Translation (IWSLT), pages 155?
162, Pittsburgh, PA, October.
474
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 101?104,
Prague, June 2007. c?2007 Association for Computational Linguistics
Minimum Bayes Risk Decoding for BLEU
Nicola Ehling and Richard Zens and Hermann Ney
Human Language Technology and Pattern Recognition
Lehrstuhl fu?r Informatik 6 ? Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
{ehling,zens,ney}@cs.rwth-aachen.de
Abstract
We present a Minimum Bayes Risk (MBR)
decoder for statistical machine translation.
The approach aims to minimize the expected
loss of translation errors with regard to the
BLEU score. We show that MBR decoding
on N -best lists leads to an improvement of
translation quality.
We report the performance of the MBR
decoder on four different tasks: the TC-
STAR EPPS Spanish-English task 2006, the
NIST Chinese-English task 2005 and the
GALE Arabic-English and Chinese-English
task 2006. The absolute improvement of the
BLEU score is between 0.2% for the TC-
STAR task and 1.1% for the GALE Chinese-
English task.
1 Introduction
In recent years, statistical machine translation
(SMT) systems have achieved substantial progress
regarding their perfomance in international transla-
tion tasks (TC-STAR, NIST, GALE).
Statistical approaches to machine translation were
proposed at the beginning of the nineties and found
widespread use in the last years. The ?standard? ver-
sion of the Bayes decision rule, which aims at a min-
imization of the sentence error rate is used in vir-
tually all approaches to statistical machine transla-
tion. However, most translation systems are judged
by their ability to minimize the error rate on the word
level or n-gram level. Common error measures are
the Word Error Rate (WER) and the Position Inde-
pendent Word Error Rate (PER) as well as evalua-
tion metric on the n-gram level like the BLEU and
NIST score that measure precision and fluency of a
given translation hypothesis.
The remaining part of this paper is structured as
follows: after a short overview of related work in
Sec. 2, we describe the MBR decoder in Sec. 3. We
present the experimental results in Sec. 4 and con-
clude in Sec. 5.
2 Related Work
MBR decoder for automatic speech recognition
(ASR) have been reported to yield improvement
over the widely used maximum a-posteriori prob-
ability (MAP) decoder (Goel and Byrne, 2003;
Mangu et al, 2000; Stolcke et al, 1997).
For MT, MBR decoding was introduced in (Ku-
mar and Byrne, 2004). It was shown that MBR is
preferable over MAP decoding for different evalu-
ation criteria. Here, we focus on the performance
of MBR decoding for the BLEU score on various
translation tasks.
3 Implementation of Minimum Bayes Risk
Decoding for the BLEU Score
3.1 Bayes Decision Rule
In statistical machine translation, we are given a
source language sentence fJ1 = f1 . . . fj . . . fJ ,
which is to be translated into a target language sen-
tence eI1 = e1 . . . ei . . . eI . Statistical decision the-
ory tells us that among all possible target language
sentences, we should choose the sentence which
minimizes the Bayes risk:
e?I?1 = argmin
I,eI1
{
?
I?,e?I
?
1
Pr(e?I
?
1 |f
J
1 ) ? L(e
I
1, e
?I?
1 )
}
Here, L(?, ?) denotes the loss function under con-
sideration. In the following, we will call this deci-
sion rule the MBR rule (Kumar and Byrne, 2004).
101
Although it is well known that this decision rule is
optimal, most SMT systems do not use it. The most
common approach is to use the MAP decision rule.
Thus, we select the hypothesis which maximizes the
posterior probability Pr(eI1|f
J
1 ):
e?I?1 = argmax
I,eI1
{
Pr(eI1|f
J
1 )
}
This decision rule is equivalent to the MBR crite-
rion under a 0-1 loss function:
L0?1(e
I
1, e
?I?
1 ) =
{
1 if eI1 = e
?I?
1
0 else
Hence, the MAP decision rule is optimal for the
sentence or string error rate. It is not necessarily
optimal for other evaluation metrics as for example
the BLEU score. One reason for the popularity of
the MAP decision rule might be that, compared to
the MBR rule, its computation is simpler.
3.2 Baseline System
The posterior probability Pr(eI1|f
J
1 ) is modeled di-
rectly using a log-linear combination of several
models (Och and Ney, 2002):
Pr(eI1|f
J
1 ) =
exp
(?M
m=1 ?mhm(e
I
1, f
J
1 )
)
?
I?,e?I
?
1
exp
(?M
m=1 ?mhm(e
?I?
1 , f
J
1 )
)
(1)
This approach is a generalization of the source-
channel approach (Brown et al, 1990). It has the
advantage that additional models h(?) can be easily
integrated into the overall system.
The denominator represents a normalization fac-
tor that depends only on the source sentence fJ1 .
Therefore, we can omit it in case of the MAP de-
cision rule during the search process. Note that the
denominator affects the results of the MBR decision
rule and, thus, cannot be omitted in that case.
We use a state-of-the-art phrase-based translation
system similar to (Matusov et al, 2006) including
the following models: an n-gram language model,
a phrase translation model and a word-based lex-
icon model. The latter two models are used for
both directions: p(f |e) and p(e|f). Additionally,
we use a word penalty, phrase penalty and a distor-
tion penalty. The model scaling factors ?M1 are opti-
mized with respect to the BLEU score as described
in (Och, 2003).
3.3 BLEU Score
The BLEU score (Papineni et al, 2002) measures
the agreement between a hypothesis eI1 generated by
the MT system and a reference translation e?I?1. It is
the geometric mean of n-gram precisions Precn(?, ?)
in combination with a brevity penalty BP(?, ?) for too
short translation hypotheses.
BLEU(eI1, e?
I?
1) = BP(I, I?) ?
4?
n=1
Precn(e
I
1, e?
I?
1)
1/4
BP(I, I?) =
{
1 if I? ? I
exp (1 ? I/I?) if I? < I
Precn(e
I
1, e?
I?
1) =
?
wn1
min{C(wn1 |e
I
1), C(w
n
1 |e?
I?
1)}
?
wn1
C(wn1 |e
I
1)
Here, C(wn1 |e
I
1) denotes the number of occur-
rences of an n-gram wn1 in a sentence e
I
1. The de-
nominator of the n-gram precisions evaluate to the
number of n-grams in the hypothesis, i.e. I ?n+1.
As loss function for the MBR decoder, we use:
L[eI1, e?
I?
1] = 1 ? BLEU(e
I
1, e?
I?
1) .
While the original BLEU score was intended to be
used only for aggregate counts over a whole test set,
we use the BLEU score at the sentence-level during
the selection of the MBR hypotheses. Note that we
will use this sentence-level BLEU score only during
decoding. The translation results that we will report
later are computed using the standard BLEU score.
3.4 Hypothesis Selection
We select the MBR hypothesis among the N best
translation candidates of the MAP system. For each
entry, we have to compute its expected BLEU score,
i.e. the weighted sum over all entries in the N -best
list. Therefore, finding the MBR hypothesis has a
quadratic complexity in the size of the N -best list.
To reduce this large work load, we stop the summa-
tion over the translation candidates as soon as the
risk of the regarded hypothesis exceeds the current
minimum risk, i.e. the risk of the current best hy-
pothesis. Additionally, the hypotheses are processed
according to the posterior probabilities. Thus, we
can hope to find a good candidate soon. This allows
for an early stopping of the computation for each of
the remaining candidates.
102
3.5 Global Model Scaling Factor
During the translation process, the different sub-
models hm(?) get different weights ?m. These scal-
ing factors are optimized with regard to a specific
evaluation criteria, here: BLEU. This optimization
describes the relation between the different models
but does not define the absolute values for the scal-
ing factors. Because search is performed using the
maximum approximation, these absolute values are
not needed during the translation process. In con-
trast to this, using the MBR decision rule, we per-
form a summation over all sentence probabilities
contained in the N -best list. Therefore, we use a
global scaling factor ?0 > 0 to modify the individ-
ual scaling factors ?m:
??m = ?0 ? ?m ,m = 1, ...,M.
For the MBR decision rule the modified scaling fac-
tors ??m are used instead of the original model scal-
ing factors ?m to compute the sentence probabilities
as in Eq. 1. The global scaling factor ?0 is tuned on
the development set. Note that under the MAP deci-
sion rule any global scaling factor ?0 > 0 yields the
same result. Similar tests were reported by (Mangu
et al, 2000; Goel and Byrne, 2003) for ASR.
4 Experimental Results
4.1 Corpus Statistics
We tested the MBR decoder on four translation
tasks: the TC-STAR EPPS Spanish-English task of
2006, the NIST Chinese-English evaluation test set
of 2005 and the GALE Arabic-English and Chinese-
English evaluation test set of 2006. The TC-STAR
EPPS corpus is a spoken language translation corpus
containing the verbatim transcriptions of speeches
of the European Parliament. The NIST Chinese-
English test sets consists of news stories. The GALE
project text track consists of two parts: newswire
(?news?) and newsgroups (?ng?). The newswire part
is similar to the NIST task. The newsgroups part
covers posts to electronic bulletin boards, Usenet
newsgroups, discussion groups and similar forums.
The corpus statistics of the training corpora are
shown in Tab. 1 to Tab. 3. To measure the trans-
lation quality, we use the BLEU score. With ex-
ception of the TC-STAR EPPS task, all scores are
computed case-insensitive. As BLEU measures ac-
curacy, higher scores are better.
Table 1: NIST Chinese-English: corpus statistics.
Chinese English
Train Sentences 9M
Words 232M 250M
Vocabulary 238K 412K
NIST 02 Sentences 878
Words 26 431 24 352
NIST 05 Sentences 1 082
Words 34 908 36 027
GALE 06 Sentences 460
news Words 9 979 11 493
GALE 06 Sentences 461
ng Words 9 606 11 689
Table 2: TC-Star Spanish-English: corpus statistics.
Spanish English
Train Sentences 1.2M
Words 35M 33M
Vocabulary 159K 110K
Dev Sentences 1 452
Words 51 982 54 857
Test Sentences 1 780
Words 56 515 58 295
4.2 Translation Results
The translation results for all tasks are presented
in Tab. 4. For each translation task, we tested the
decoder on N -best lists of size N=10 000, i.e. the
10 000 best translation candidates. Note that in some
cases the list is smaller because the translation sys-
tem did not produce more candidates. To analyze
the improvement that can be gained through rescor-
ing with MBR, we start from a system that has al-
ready been rescored with additional models like an
n-gram language model, HMM, IBM-1 and IBM-4.
It turned out that the use of 1 000 best candidates
for the MBR decoding is sufficient, and leads to ex-
actly the same results as the use of 10 000 best lists.
Similar experiences were reported by (Mangu et al,
2000; Stolcke et al, 1997) for ASR.
We observe that the improvement is larger for
Table 3: GALE Arabic-English: corpus statistics.
Arabic English
Train Sentences 4M
Words 125M 124M
Vocabulary 421K 337K
news Sentences 566
Words 14 160 15 320
ng Sentences 615
Words 11 195 14 493
103
Table 4: Translation results BLEU [%] for the NIST task, GALE task and TC-STAR task (S-E: Spanish-
English; C-E: Chinese-English; A-E: Arabic-English).
TC-STAR S-E NIST C-E GALE A-E GALE C-E
decision rule test 2002 (dev) 2005 news ng news ng
MAP 52.6 32.8 31.2 23.6 12.2 14.6 9.4
MBR 52.8 33.3 31.9 24.2 13.3 15.4 10.5
Table 5: Translation examples for the GALE Arabic-English newswire task.
Reference the saudi interior ministry announced in a report the implementation of the death penalty
today, tuesday, in the area of medina (west) of a saudi citizen convicted of murdering a
fellow citizen.
MAP-Hyp saudi interior ministry in a statement to carry out the death sentence today in the area of
medina (west) in saudi citizen found guilty of killing one of its citizens.
MBR-Hyp the saudi interior ministry announced in a statement to carry out the death sentence today
in the area of medina (west) in saudi citizen was killed one of its citizens.
Reference faruq al-shar?a takes the constitutional oath of office before the syrian president
MAP-Hyp farouk al-shara leads sworn in by the syrian president
MBR-Hyp farouk al-shara lead the constitutional oath before the syrian president
low-scoring translations, as can be seen in the GALE
task. For an ASR task, similar results were reported
by (Stolcke et al, 1997).
Some translation examples for the GALE Arabic-
English newswire task are shown in Tab. 5. The dif-
ferences between the MAP and the MBR hypotheses
are set in italics.
5 Conclusions
We have shown that Minimum Bayes Risk decod-
ing on N -best lists improves the BLEU score con-
siderably. The achieved results are promising. The
improvements were consistent among several eval-
uation sets. Even if the improvement is sometimes
small, e.g. TC-STAR, it is statistically significant:
the absolute improvement of the BLEU score is be-
tween 0.2% for the TC-STAR task and 1.1% for the
GALE Chinese-English task. Note, that MBR de-
coding is never worse than MAP decoding, and is
therefore promising for SMT. It is easy to integrate
and can improve even well-trained systems by tun-
ing them for a particular evaluation criterion.
Acknowledgments
This material is partly based upon work supported
by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023,
and was partly funded by the European Union un-
der the integrated project TC-STAR (Technology
and Corpora for Speech to Speech Translation, IST-
2002-FP6-506738, http://www.tc-star.org).
References
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra,
F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin.
1990. A statistical approach to machine translation. Com-
putational Linguistics, 16(2):79?85, June.
V. Goel and W. Byrne. 2003. Minimum bayes-risk automatic
speech recognition. Pattern Recognition in Speech and Lan-
guage Processing.
S. Kumar and W. Byrne. 2004. Minimum bayes-risk decod-
ing for statistical machine translation. In Proc. Human Lan-
guage Technology Conf. / North American Chapter of the
Assoc. for Computational Linguistics Annual Meeting (HLT-
NAACL), pages 169?176, Boston, MA, May.
L. Mangu, E. Brill, and A. Stolcke. 2000. Finding consensus
in speech recognition: Word error minimization and other
applications of confusion networks. Computer, Speech and
Language, 14(4):373?400, October.
E. Matusov, R. Zens, D. Vilar, A. Mauser, M. Popovic?,
S. Hasan, and H. Ney. 2006. The RWTH machine trans-
lation system. In Proc. TC-Star Workshop on Speech-to-
Speech Translation, pages 31?36, Barcelona, Spain, June.
F. J. Och and H. Ney. 2002. Discriminative training and max-
imum entropy models for statistical machine translation. In
Proc. 40th Annual Meeting of the Assoc. for Computational
Linguistics (ACL), pages 295?302, Philadelphia, PA, July.
F. J. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proc. 41st Annual Meeting of the As-
soc. for Computational Linguistics (ACL), pages 160?167,
Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. Bleu: a
method for automatic evaluation of machine translation. In
Proc. 40th Annual Meeting of the Assoc. for Computational
Linguistics (ACL), pages 311?318, Philadelphia, PA, July.
A. Stolcke, Y. Konig, and M. Weintraub. 1997. Explicit word
error minimization in N-best list rescoring. In Proc. Eu-
ropean Conf. on Speech Communication and Technology,
pages 163?166, Rhodes, Greece, September.
104
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177?180,
Prague, June 2007. c?2007 Association for Computational Linguistics 
Moses: Open Source Toolkit for Statistical Machine Translation 
Philipp Koehn 
Hieu Hoang  
Alexandra Birch 
Chris Callison-Burch 
University of Edin-
burgh1 
Marcello Federico 
Nicola Bertoldi 
ITC-irst2 
Brooke Cowan 
Wade Shen 
Christine Moran 
MIT3 
Richard Zens 
RWTH Aachen4 
Chris Dyer 
University of Maryland5 
 
Ond?ej Bojar 
Charles University6 
Alexandra Constantin 
Williams College7 
Evan Herbst 
Cornell8 
1 pkoehn@inf.ed.ac.uk, {h.hoang, A.C.Birch-Mayne}@sms.ed.ac.uk, callison-burch@ed.ac.uk. 
2{federico, bertoldi}@itc.it. 3 brooke@csail.mit.edu, swade@ll.mit.edu, weezer@mit.edu. 4 
zens@i6.informatik.rwth-aachen.de. 5 redpony@umd.edu. 6 bojar@ufal.ms.mff.cuni.cz. 7 
07aec_2@williams.edu. 8 evh4@cornell.edu 
 
Abstract 
We describe an open-source toolkit for sta-
tistical machine translation whose novel 
contributions are (a) support for linguisti-
cally motivated factors, (b) confusion net-
work decoding, and (c) efficient data for-
mats for translation models and language 
models. In addition to the SMT decoder, 
the toolkit also includes a wide variety of 
tools for training, tuning and applying the 
system to many translation tasks.  
1 Motivation 
Phrase-based statistical machine translation 
(Koehn et al 2003) has emerged as the dominant 
paradigm in machine translation research. How-
ever, until now, most work in this field has been 
carried out on proprietary and in-house research 
systems. This lack of openness has created a high 
barrier to entry for researchers as many of the 
components required have had to be duplicated. 
This has also hindered effective comparisons of the 
different elements of the systems. 
By providing a free and complete toolkit, we 
hope that this will stimulate the development of the 
field. For this system to be adopted by the commu-
nity, it must demonstrate performance that is com-
parable to the best available systems. Moses has 
shown that it achieves results comparable to the 
most competitive and widely used statistical ma-
chine translation systems in translation quality and 
run-time (Shen et al 2006). It features all the ca-
pabilities of the closed sourced Pharaoh decoder 
(Koehn 2004). 
Apart from providing an open-source toolkit 
for SMT, a further motivation for Moses is to ex-
tend phrase-based translation with factors and con-
fusion network decoding. 
The current phrase-based approach to statisti-
cal machine translation is limited to the mapping of 
small text chunks without any explicit use of lin-
guistic information, be it morphological, syntactic, 
or semantic. These additional sources of informa-
tion have been shown to be valuable when inte-
grated into pre-processing or post-processing steps. 
Moses also integrates confusion network de-
coding, which allows the translation of ambiguous 
input. This enables, for instance, the tighter inte-
gration of speech recognition and machine transla-
tion. Instead of passing along the one-best output 
of the recognizer, a network of different word 
choices may be examined by the machine transla-
tion system. 
Efficient data structures in Moses for the 
memory-intensive translation model and language 
model allow the exploitation of much larger data 
resources with limited hardware. 
177
 2 Toolkit 
The toolkit is a complete out-of-the-box trans-
lation system for academic research. It consists of 
all the components needed to preprocess data, train 
the language models and the translation models. It 
also contains tools for tuning these models using 
minimum error rate training (Och 2003) and evalu-
ating the resulting translations using the BLEU 
score (Papineni et al 2002).  
Moses uses standard external tools for some of 
the tasks to avoid duplication, such as GIZA++ 
(Och and Ney 2003) for word alignments and 
SRILM for language modeling.  Also, since these 
tasks are often CPU intensive, the toolkit has been 
designed to work with Sun Grid Engine parallel 
environment to increase throughput.  
In order to unify the experimental stages, a 
utility has been developed to run repeatable ex-
periments. This uses the tools contained in Moses 
and requires minimal changes to set up and cus-
tomize. 
The toolkit has been hosted and developed un-
der sourceforge.net since inception. Moses has an 
active research community and has reached over 
1000 downloads as of 1st March 2007.  
The main online presence is at  
http://www.statmt.org/moses/ 
where many sources of information about the 
project can be found. Moses was the subject of this 
year?s Johns Hopkins University Workshop on 
Machine Translation (Koehn et al 2006). 
The decoder is the core component of Moses. 
To minimize the learning curve for many research-
ers, the decoder was developed as a drop-in re-
placement for Pharaoh, the popular phrase-based 
decoder. 
In order for the toolkit to be adopted by the 
community, and to make it easy for others to con-
tribute to the project, we kept to the following 
principles when developing the decoder: 
? Accessibility 
? Easy to Maintain 
? Flexibility 
? Easy for distributed team development 
? Portability 
It was developed in C++ for efficiency and fol-
lowed modular, object-oriented design. 
3 Factored Translation Model 
Non-factored SMT typically deals only with 
the surface form of words and has one phrase table, 
as shown in Figure 1. 
i am buying you a green cat
using phrase dictionary:
i
 am buying
you
a
green
cat
je
ach?te
vous
un
vert
chat
a une
je vous ach?te un chat vert
Translate:
 
In factored translation models, the surface 
forms may be augmented with different factors, 
such as POS tags or lemma. This creates a factored 
representation of each word, Figure 2.  
1 1 1 / sing /
                                 
je vous achet un chat
PRO PRO VB ART NN
je vous acheter un chat
st st st present masc masc
? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?
1 1 / 1 sing sing
i buy you a cat
PRO VB PRO ART NN
i tobuy you a cat
st st present st
? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?  
 
Mapping of source phrases to target phrases 
may be decomposed into several steps. Decompo-
sition of the decoding process into various steps 
means that different factors can be modeled sepa-
rately. Modeling factors in isolation allows for 
flexibility in their application. It can also increase 
accuracy and reduce sparsity by minimizing the 
number dependencies for each step. 
For example, we can decompose translating 
from surface forms to surface forms and lemma, as 
shown in Figure 3. 
Figure 2. Factored translation 
Figure 1. Non-factored translation 
178
  
Figure 3. Example of graph of decoding steps 
By allowing the graph to be user definable, we 
can experiment to find the optimum configuration 
for a given language pair and available data.  
The factors on the source sentence are consid-
ered fixed, therefore, there is no decoding step 
which create source factors from other source fac-
tors. However, Moses can have ambiguous input in 
the form of confusion networks. This input type 
has been used successfully for speech to text 
translation (Shen et al 2006). 
Every factor on the target language can have its 
own language model. Since many factors, like 
lemmas and POS tags, are less sparse than surface 
forms, it is possible to create a higher order lan-
guage models for these factors. This may encour-
age more syntactically correct output. In Figure 3 
we apply two language models, indicated by the 
shaded arrows, one over the words and another 
over the lemmas. Moses is also able to integrate 
factored language models, such as those described 
in (Bilmes and Kirchhoff 2003) and (Axelrod 
2006). 
4 Confusion Network Decoding 
Machine translation input currently takes the 
form of simple sequences of words. However, 
there are increasing demands to integrate machine 
translation technology into larger information 
processing systems with upstream NLP/speech 
processing tools (such as named entity recognizers, 
speech recognizers, morphological analyzers, etc.). 
These upstream processes tend to generate multiple, 
erroneous hypotheses with varying confidence. 
Current MT systems are designed to process only 
one input hypothesis, making them vulnerable to 
errors in the input.  
In experiments with confusion networks, we 
have focused so far on the speech translation case, 
where the input is generated by a speech recog-
nizer. Namely, our goal is to improve performance 
of spoken language translation by better integrating 
speech recognition and machine translation models. 
Translation from speech input is considered more 
difficult than translation from text for several rea-
sons. Spoken language has many styles and genres, 
such as, formal read speech, unplanned speeches, 
interviews, spontaneous conversations; it produces 
less controlled language, presenting more relaxed 
syntax and spontaneous speech phenomena. Fi-
nally, translation of spoken language is prone to 
speech recognition errors, which can possibly cor-
rupt the syntax and the meaning of the input. 
There is also empirical evidence that better 
translations can be obtained from transcriptions of 
the speech recognizer which resulted in lower 
scores. This suggests that improvements can be 
achieved by applying machine translation on a 
large set of transcription hypotheses generated by 
the speech recognizers and by combining scores of 
acoustic models, language models, and translation 
models. 
Recently, approaches have been proposed for 
improving translation quality through the process-
ing of multiple input hypotheses. We have imple-
mented in Moses confusion network decoding as 
discussed in (Bertoldi and Federico 2005), and de-
veloped a simpler translation model and a more 
efficient implementation of the search algorithm. 
Remarkably, the confusion network decoder re-
sulted in an extension of the standard text decoder. 
5 Efficient Data Structures for Transla-
tion Model and Language Models 
With the availability of ever-increasing 
amounts of training data, it has become a challenge 
for machine translation systems to cope with the 
resulting strain on computational resources. Instead 
of simply buying larger machines with, say, 12 GB 
of main memory, the implementation of more effi-
cient data structures in Moses makes it possible to 
exploit larger data resources with limited hardware 
infrastructure. 
A phrase translation table easily takes up giga-
bytes of disk space, but for the translation of a sin-
gle sentence only a tiny fraction of this table is 
needed. Moses implements an efficient representa-
tion of the phrase translation table. Its key proper-
ties are a prefix tree structure for source words and 
on demand loading, i.e. only the fraction of the 
phrase table that is needed to translate a sentence is 
loaded into the working memory of the decoder. 
179
 For the Chinese-English NIST  task, the mem-
ory requirement of the phrase table is reduced from 
1.7 gigabytes to less than 20 mega bytes, with no 
loss in translation quality and speed (Zens and Ney 
2007). 
The other large data resource for statistical ma-
chine translation is the language model. Almost 
unlimited text resources can be collected from the 
Internet and used as training data for language 
modeling. This results in language models that are 
too large to easily fit into memory. 
The Moses system implements a data structure 
for language models that is more efficient than the 
canonical SRILM (Stolcke 2002) implementation 
used in most systems. The language model on disk 
is also converted into this binary format, resulting 
in a minimal loading time during start-up of the 
decoder.  
An even more compact representation of the 
language model is the result of the quantization of 
the word prediction and back-off probabilities of 
the language model. Instead of representing these 
probabilities with 4 byte or 8 byte floats, they are 
sorted into bins, resulting in (typically) 256 bins 
which can be referenced with a single 1 byte index. 
This quantized language model, albeit being less 
accurate, has only minimal impact on translation 
performance (Federico and Bertoldi 2006). 
6 Conclusion and Future Work 
This paper has presented a suite of open-source 
tools which we believe will be of value to the MT 
research community. 
We have also described a new SMT decoder 
which can incorporate some linguistic features in a 
consistent and flexible framework. This new direc-
tion in research opens up many possibilities and 
issues that require further research and experimen-
tation. Initial results show the potential benefit of 
factors for statistical machine translation, (Koehn 
et al 2006) and (Koehn and Hoang 2007). 
References 
Axelrod, Amittai. "Factored Language Model for Sta-
tistical Machine Translation." MRes Thesis. 
Edinburgh University, 2006. 
Bertoldi, Nicola, and Marcello Federico. "A New De-
coder for Spoken Language Translation Based 
on Confusion Networks." Automatic Speech 
Recognition and Understanding Workshop 
(ASRU), 2005. 
Bilmes, Jeff A, and Katrin Kirchhoff. "Factored Lan-
guage Models and Generalized Parallel Back-
off." HLT/NACCL, 2003. 
Koehn, Philipp. "Pharaoh: A Beam Search Decoder for 
Phrase-Based Statistical Machine Translation 
Models." AMTA, 2004. 
Koehn, Philipp, Marcello Federico, Wade Shen, Nicola 
Bertoldi, Ondrej Bojar, Chris Callison-Burch, 
Brooke Cowan, Chris Dyer, Hieu Hoang, 
Richard Zens, Alexandra Constantin, Christine 
Corbett Moran, and Evan Herbst. "Open 
Source Toolkit for Statistical Machine Transla-
tion". Report of the 2006 Summer Workshop at 
Johns Hopkins University, 2006. 
Koehn, Philipp, and Hieu Hoang. "Factored Translation 
Models." EMNLP, 2007. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
"Statistical Phrase-Based Translation." 
HLT/NAACL, 2003. 
Och, Franz Josef. "Minimum Error Rate Training for 
Statistical Machine Translation." ACL, 2003. 
Och, Franz Josef, and Hermann Ney. "A Systematic 
Comparison of Various Statistical Alignment 
Models." Computational Linguistics 29.1 
(2003): 19-51. 
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. "BLEU: A Method for Automatic 
Evaluation of Machine Translation." ACL, 
2002. 
Shen, Wade, Richard Zens, Nicola Bertoldi, and 
Marcello Federico. "The JHU Workshop 2006 
Iwslt System." International Workshop on Spo-
ken Language Translation, 2006. 
Stolcke, Andreas. "SRILM an Extensible Language 
Modeling Toolkit." Intl. Conf. on Spoken Lan-
guage Processing, 2002. 
Zens, Richard, and Hermann Ney. "Efficient Phrase-
Table Representation for Machine Translation 
with Applications to Online MT and Speech 
Recognition." HLT/NAACL, 2007. 
180
Do We Need Chinese Word Segmentation
for Statistical Machine Translation?
Jia Xu and Richard Zens and Hermann Ney
Chair of Computer Science VI
Computer Science Department
RWTH Aachen University, Germany
{xujia,zens,ney}@cs.rwth-aachen.de
Abstract
In Chinese texts, words are not separated by
white spaces. This is problematic for many nat-
ural language processing tasks. The standard
approach is to segment the Chinese character
sequence into words. Here, we investigate Chi-
nese word segmentation for statistical machine
translation. We pursue two goals: the first one
is the maximization of the final translation qual-
ity; the second is the minimization of the man-
ual effort for building a translation system.
The commonly used method for getting the
word boundaries is based on a word segmenta-
tion tool and a predefined monolingual dictio-
nary. To avoid the dependence of the trans-
lation system on an external dictionary, we
have developed a system that learns a domain-
specific dictionary from the parallel training
corpus. This method produces results that are
comparable with the predefined dictionary.
Further more, our translation system is able
to work without word segmentation with only a
minor loss in translation quality.
1 Introduction
In Chinese texts, words composed of single or
multiple characters, are not separated by white
spaces, which is different from most of the west-
ern languages. This is problematic for many
natural language processing tasks. Therefore,
the usual method is to segment a Chinese char-
acter sequence into Chinese ?words?.
Many investigations have been performed
concerning Chinese word segmentation. For
example, (Palmer, 1997) developed a Chinese
word segmenter using a manually segmented
corpus. The segmentation rules were learned
automatically from this corpus. (Sproat and
Shih, 1990) and (Sun et al, 1998) used a
method that does not rely on a dictionary or a
manually segmented corpus. The characters of
the unsegmented Chinese text are grouped into
pairs with the highest value of mutual informa-
tion. This mutual information can be learned
from an unsegmented Chinese corpus.
We will present a new method for segment-
ing the Chinese text without using a manually
segmented corpus or a predefined dictionary. In
statistical machine translation, we have a bilin-
gual corpus available, which is used to obtain
a segmentation of the Chinese text in the fol-
lowing way. First, we train the statistical trans-
lation models with the unsegmented bilingual
corpus. As a result, we obtain a mapping of
Chinese characters to the corresponding English
words for each sentence pair. By using this map-
ping, we can extract a dictionary automatically.
With this self-learned dictionary, we use a seg-
mentation tool to obtain a segmented Chinese
text. Finally, we retrain our translation system
with the segmented corpus.
Additionally, we have performed experiments
without explicit word segmentation. In this
case, each Chinese character is interpreted as
one ?word?. Based on word groups, our ma-
chine translation system is able to work without
a word segmentation, while having only a minor
translation quality relative loss of less than 5%.
2 Review of the Baseline System for
Statistical Machine Translation
2.1 Principle
In statistical machine translation, we are given
a source language (?French?) sentence fJ1 =
f1 . . . fj . . . fJ , which is to be translated into
a target language (?English?) sentence eI1 =
e1 . . . ei . . . eI . Among all possible target lan-
guage sentences, we will choose the sentence
with the highest probability:
e?I1 = argmax
eI1
{Pr(eI1|fJ1 )
} (1)
= argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)
} (2)
The decomposition into two knowledge sources
in Equation 2 is known as the source-channel
approach to statistical machine translation
(Brown et al, 1990). It allows an independent
modeling of target language model Pr(eI1) and
translation model Pr(fJ1 |eI1)1. The target lan-
guage model describes the well-formedness of
the target language sentence. The translation
model links the source language sentence to the
target language sentence. The argmax opera-
tion denotes the search problem, i.e. the gener-
ation of the output sentence in the target lan-
guage. We have to maximize over all possible
target language sentences.
The resulting architecture for the statistical
machine translation approach is shown in Fig-
ure 1 with the translation model further decom-
posed into lexicon and alignment model.
Source Language Text
Transformation
 Lexicon Model
Language Model
Global Search:
 
 
Target Language Text
 
over
 
 Pr(f1  J  |e1I )
 
 
 Pr(   e1I )
 
 
 Pr(f1  J  |e1I )   Pr(   e1I )
  
e1I
f1 J
maximize  Alignment Model
Transformation
Figure 1: Architecture of the translation ap-
proach based on Bayes decision rule.
2.2 Alignment Models
The alignment model Pr(fJ1 , aJ1 |eI1) introduces
a ?hidden? alignment a = aJ1 , which describes
1The notational convention will be as follows: we use
the symbol Pr(?) to denote general probability distri-
butions with (nearly) no specific assumptions. In con-
trast, for model-based probability distributions, we use
the generic symbol p(?).
a mapping from a source position j to a target
position aj . The relationship between the trans-
lation model and the alignment model is given
by:
Pr(fJ1 |eI1) =
?
aJ1
Pr(fJ1 , aJ1 |eI1) (3)
In this paper, we use the models IBM-1, IBM-
4 from (Brown et al, 1993) and the Hidden-
Markov alignment model (HMM) from (Vogel et
al., 1996). All these models provide different de-
compositions of the probability Pr(fJ1 , aJ1 |eI1).
A detailed description of these models can be
found in (Och and Ney, 2003).
A Viterbi alignment a?J1 of a specific model is
an alignment for which the following equation
holds:
a?J1 = argmax
aJ1
Pr(fJ1 , aJ1 |eI1). (4)
The alignment models are trained on a bilin-
gual corpus using GIZA++(Och et al, 1999;
Och and Ney, 2003). The training is done it-
eratively in succession on the same data, where
the final parameter estimates of a simpler model
serve as starting point for a more complex
model. The result of the training procedure is
the Viterbi alignment of the final training iter-
ation for the whole training corpus.
2.3 Alignment Template Approach
In the translation approach from Section 2.1,
one disadvantage is that the contextual informa-
tion is only taken into account by the language
model. The single-word based lexicon model
does not consider the surrounding words. One
way to incorporate the context into the trans-
lation model is to learn translations for whole
word groups instead of single words. The key
elements of this translation approach (Och et
al., 1999) are the alignment templates. These
are pairs of source and target language phrases
with an alignment within the phrases.
The alignment templates are extracted from
the bilingual training corpus. The extraction al-
gorithm (Och et al, 1999) uses the word align-
ment information obtained from the models in
Section 2.2. Figure 2 shows an example of a
word aligned sentence pair. The word align-
ment is represented with the black boxes. The
figure also includes some of the possible align-
ment templates, represented as the larger, un-
filled rectangles. Note that the extraction algo-
rithm would extract many more alignment tem-
plates from this sentence pair. In this example,
the system input was the sequence of Chinese
characters without any word segmentation. As
can be seen, a translation approach that is based
on phrases circumvents the problem of word seg-
mentation to a certain degree. This method will
be referred to as ?translation with no segmen-
tation? (see Section 5.2).
they
will
also
go
to
hangzhou
for
a
visit
Figure 2: Example of a word aligned sentence
pair and some possible alignment templates.
In the Chinese?English DARPA TIDES eval-
uations in June 2002 and May 2003, carried out
by NIST (NIST, 2003), the alignment template
approach performed very well and was ranked
among the best translation systems.
Further details on the alignment template ap-
proach are described in (Och et al, 1999; Och
and Ney, 2002).
3 Task and Corpus Statistics
In Section 5.3, we will present results for a
Chinese?English translation task. The domain
of this task is news articles. As bilingual train-
ing data, we use a corpus composed of the En-
glish translations of a Chinese Treebank. This
corpus is provided by the Linguistic Data Con-
sortium (LDC), catalog number LDC2002E17.
In addition, we use a bilingual dictionary with
10K Chinese word entries provided by Stephan
Vogel (LDC, 2003b).
Table 1 shows the corpus statistics of this
task. We have calculated both the number of
words and the number of characters in the cor-
pus. In average, a Chinese word is composed
of 1.49 characters. For each of the two lan-
guages, there is a set of 20 special characters,
such as digits, punctuation marks and symbols
like ?()%$...?
The training corpus will be used to train a
word alignment and then extract the alignment
templates and the word-based lexicon. The re-
sulting translation system will be evaluated on
the test corpus.
Table 1: Statistics of training and test corpus.
For each of the two languages, there is a set of 20
special characters, such as digits, punctuation
marks and symbols like ?()%$...?
Chinese English
Train Sentences 4 172
Characters 172 874 832 760
Words 116 090 145 422
Char. Vocab. 3 419 + 20 26 + 20
Word Vocab. 9 391 9 505
Test Sentences 993
Characters 42 100 167 101
Words 28 247 26 225
4 Segmentation Methods
4.1 Conventional Method
The commonly used segmentation method is
based on a segmentation tool and a monolingual
Chinese dictionary. Typically, this dictionary
has been produced beforehand and is indepen-
dent of the Chinese text to be segmented. The
dictionary contains Chinese words and their fre-
quencies. This information is used by the seg-
mentation tool to find the word boundaries. In
the LDC method (see Section 5.2) we have used
the dictionary and segmenter provided by the
LDC. More details can be found on the LDC
web pages (LDC, 2003a). This segmenter is
based on two ideas: it prefers long words over
short words and it prefers high frequency words
over low frequency words.
4.2 Dictionary Learning from Alignments
In this section, we will describe our method of
learning a dictionary from a bilingual corpus.
As mentioned before, the bilingual training
corpus listed in Section 3 is the only input to the
system. We firstly divide every Chinese charac-
ters in the corpus by white spaces, then train
the statistical translation models with this un-
segmented Chinese text and its English trans-
lation, details of the training method are de-
scribed in Section 2.2.
To extract Chinese words instead of phrases
as in Figure 2, we configure the training pa-
rameters in GIZA++, the alignment is then re-
stricted to a multi-source-single-target relation-
ship, i.e. one or more Chinese characters are
translated to one English word.
The result of this training procedure is an
alignment for each sentence pair. Such an align-
ment is represented as a binary matrix with J ?I
elements.
An example is shown in Figure 3. The un-
segmented Chinese training sentence is plotted
along the horizontal axes and the corresponding
English sentence along the vertical axes. The
black boxes show the Viterbi alignment for this
sentence pair. Here, for example the first two
Chinese characters are aligned to ?industry?,
the next four characters are aligned to ?restruc-
turing?.
industry
restructuring
made
vigorous
progress
Figure 3: Example of an alignment without
word segmentation.
The central idea of our dictionary learning
method is: a contiguous sequence of Chinese
characters constitute a Chinese word, if they
are aligned to the same English word. Using
this idea and the bilingual corpus, we can au-
tomatically generate a Chinese word dictionary.
Table 2 shows the Chinese words that are ex-
tracted from the alignment in Figure 3.
Table 2: Word entries in Chinese dictionary
learned from the alignment in Figure 3.
We extract Chinese words from all sentence
pairs in the training corpus. Therefore, it is
straightforward to collect word frequency statis-
tics that are needed for the segmentation tool.
Once, we have generated the dictionary, we can
produce a segmented Chinese corpus using the
method described in Section 4.1. Then, we
retrain the translation system using the seg-
mented Chinese text.
4.3 Word Length Statistics
In this section, we present statistics of the word
lengths in the LDC dictionary as well as in the
self-learned dictionary extracted from the align-
ment.
Table 3 shows the statistics of the word
lengths in the LDC dictionary as well as in
the learned dictionary. For example, there are
2 368 words consisting of a single character in
learned dictionary and 2 511 words in the LDC
dictionary. These single character words rep-
resent 16.9% of the total number of entries in
the learned dictionary and 18.6% in the LDC
dictionary.
We see that in the LDC dictionary more than
65% of the words consist of two characters and
about 30% of the words consist of a single char-
acter or three or four characters. Longer words
with more than four characters constitute less
than 1% of the dictionary. In the learned dic-
tionary, there are many more long words, about
15%. A subjective analysis showed that many
of these entries are either named entities or
idiomatic expressions. Often, these idiomatic
expressions should be segmented into shorter
words. Therefore, we will investigate methods
to overcome this problem in the future. Some
suggestions will be discussed in Section 6.
Table 3: Statistics of word lengths in the LDC
dictionary and in the learned dictionary.
word LDC dictionary learned dictionary
length frequency [%] frequency [%]
1 2 334 18.6 2 368 16.9
2 8 149 65.1 5 486 39.2
3 1 188 9.5 1 899 13.6
4 759 6.1 2 084 14.9
5 70 0.6 791 5.7
6 20 0.2 617 4.4
7 6 0.0 327 2.3
?8 11 0.0 424 3.0
total 12 527 100 13 996 100
5 Translation Experiments
5.1 Evaluation Criteria
So far, in machine translation research, a sin-
gle generally accepted criterion for the evalu-
ation of the experimental results does not ex-
ist. We have used three automatic criteria. For
the test corpus, we have four references avail-
able. Hence, we compute all the following cri-
teria with respect to multiple references.
? WER (word error rate):
The WER is computed as the minimum
number of substitution, insertion and dele-
tion operations that have to be performed
to convert the generated sentence into the
reference sentence.
? PER (position-independent word error
rate):
A shortcoming of the WER is that it re-
quires a perfect word order. The word or-
der of an acceptable sentence can be dif-
ferent from that of the target sentence, so
that the WER measure alone could be mis-
leading. The PER compares the words in
the two sentences ignoring the word order.
? BLEU score:
This score measures the precision of un-
igrams, bigrams, trigrams and fourgrams
with respect to a reference translation with
a penalty for too short sentences (Papineni
et al, 2001). The BLEU score measures
accuracy, i.e. large BLEU scores are bet-
ter.
5.2 Summary: Three Translation
Methods
In the experiments, we compare the following
three translation methods:
? Translation with no segmentation: Each
Chinese character is interpreted as a single
word.
? Translation with learned segmentation:
It uses the self-learned dictionary.
? Translation with LDC segmentation:
The predefined LDC dictionary is used.
The core contribution of this paper is the
method we called ?translation with learned seg-
mentation?, which consists of three steps:
? The input is a sequence of Chinese charac-
ters without segmentation. After the train-
ing using GIZA++, we extract a mono-
lingual Chinese dictionary from the align-
ment. This is discussed in Section 4.2, and
an example is given in Figure 3 and Table 2.
? Using this learned dictionary, we segment
the sequence of Chinese characters into
words. In other words, the LDC method
is used, but the LDC dictionary is replaced
by the learned dictionary (see Section 4.1).
? Based on this word segmentation, we
perform another training using GIZA++.
Then, after training the models IBM1,
HMM and IBM4, we extract bilingual word
groups, which are referred as alignment
templates.
5.3 Evaluation Results
The evaluation is performed on the LDC corpus
described in Section 3. The translation perfor-
mance of the three systems is summarized in
Table 4 for the three evaluation criteria WER,
PER and BLEU. We observe that the trans-
lation quality with the learned segmentation is
similar to that with the LDC segmentation. The
WER of the system with the learned segmenta-
tion is somewhat better, but PER and BLEU
are slightly worse. We conclude that it is possi-
ble to learn a domain-specific dictionary for Chi-
nese word segmentation from a bilingual corpus.
Therefore the translation system is independent
of a predefined dictionary, which may be unsuit-
able for a certain task.
The translation system using no segmenta-
tion performs slightly worse. For example, for
the WER there is a loss of about 2% relative
compared to the system with the LDC segmen-
tation.
Table 4: Translation performance of different
segmentation methods (all numbers in percent).
method error rates accuracy
WER PER BLEU
no segment. 73.3 56.5 27.6
learned segment. 70.4 54.6 29.1
LDC segment. 71.9 54.4 29.2
5.4 Effect of Segmentation on
Translation Results
In this section, we present three examples of the
effect that segmentation may have on transla-
tion quality. For each of the three examples in
Figure 4, we show the segmented Chinese source
sentence using either the LDC dictionary or the
self-learned dictionary, the corresponding trans-
lation and the human reference translation.
In the first example, the LDC dictionary
leads to a correct segmentation, whereas with
the learned dictionary the segmentation is erro-
neous. The second and third token should be
combined (?Hong Kong?), whereas the fifth to-
ken should be separated (?stabilize in the long
term?). In this case, the wrong segmentation of
the Chinese source sentence does not result in a
wrong translation. A possible reason is that the
translation system is based on word groups and
can recover from these segmentation errors.
In the second example, the segmentation with
the LDC dictionary produces at least one error.
The second and third token should be combined
(?this?). It is possible to combine the seventh
and eighth token to a single word because the
eighth token shows only the tense. The segmen-
tation with the learned dictionary is correct.
Here, the two segmentations result in different
translations.
In the third example, both segmentations are
incorrect and these segmentation errors affect
the translation results. In the segmentation
with the LDC dictionary, the first Chinese char-
acters should be segmented as a separate word.
The second and third character and maybe even
the fourth character should be combined to one
word.2 The fifth and sixth character should be
combined to a single word. In the segmentation
with the learned dictionary, the fifth and sixth
token (seventh and eighth character) should be
combined (?isolated?). We see that this term is
missing in the translation. Here, the segmenta-
tion errors result in translation errors.
6 Discussion and Future Work
We have presented a new method for Chinese
word segmentation. It avoids the use of a pre-
defined dictionary and instead learns a corpus-
specific dictionary from the bilingual training
corpus.
The idea is extracting a self-learned dictio-
nary from the trained alignment models. This
method has the advantage that the word entries
in the dictionary all occur in the training data,
and its content is much closer to the training
text as a predefined dictionary, which can never
cover all possible word occurrences. Here, if the
content of the test corpus is closer to that of the
2This is an example of an ambiguous segmentation.
Example 1
LDC dictionary:
                              
It will benefit Hong Kong's economy to prosper
and stabilize in the long term.
Learned dictionary:
                          
It will benefit Hong Kong's economy to prosper
and stabilize in the long term.
Reference:
It will be benificial for the stability and
prosperity of Hong Kong in the long run.
Example 2
LDC dictionary:
                               
         
but this meeting down or achieved certain
progress.
Learned dictionary:
                             
    
however, this meeting straight down still
achieved certain progress.
Reference:
Neverless, this meeting has achieved some
progress.
Example 3
LDC dictionary:
...                                 
 ...
... the unification of the world carried adjacent
isolate of proof, ...
Learned dictionary:
...                                
 ...
... in the world faced with a became another
proof, ...
Reference:
... another proof that ... is facing isolation in the
world ...
Figure 4: Translation examples using the
learned dictionary and the LDC dictionary.
training corpus, the quality of the dictionary is
higher and the translation performance would
be better.
The experiments showed that the transla-
tion quality with the learned segmentation is
competitive with the LDC segmentation. Ad-
ditionally, we have shown the feasibility of a
Chinese?English statistical machine translation
system that works without any word segmenta-
tion. There is only a minor loss in translation
performance. Further improvements could be
possible by tuning the system toward this spe-
cific task.
We expect that our method could be im-
proved by considering the word length as dis-
cussed in Section 4.3. As shown in the word
length statistics, long words with more than
four characters occur only occasionally. Most of
them are named entity words, which are writ-
ten in English in upper case. Therefore, we can
apply a simple rule: we accept a long Chinese
word only if the corresponding English word is
in upper case. This should result in an improved
dictionary. An alternative way is to use the
word length statistics in Table 3 as a prior dis-
tribution. In this case, long words would get a
penalty, because their prior probability is low.
Because the extraction of our dictionary is
based on bilingual information, it might be in-
teresting to combine it with methods that use
monolingual information only.
For Chinese?English, there is a large num-
ber of bilingual corpora available at the LDC.
Therefore using additional corpora, we can ex-
pect to get an improved dictionary.
References
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J.
Della Pietra, F. Jelinek, J. D. Lafferty, R. L.
Mercer, and P. S. Roossin. 1990. A statisti-
cal approach to machine translation. Compu-
tational Linguistics, 16(2):79?85, June.
P. F. Brown, S. A. Della Pietra, V. J. Della
Pietra, and R. L. Mercer. 1993. The mathe-
matics of statistical machine translation: Pa-
rameter estimation. Computational Linguis-
tics, 19(2):263?311, June.
LDC. 2003a. LDC Chinese resources home
page. http://www.ldc.upenn.edu/Projects/
Chinese/LDC ch.htm.
LDC. 2003b. LDC resources home page.
http://www.ldc.upenn.edu/Projects/TIDES/
mt2004cn.htm.
NIST. 2003. Machine translation home page.
http://www.nist.gov/speech/tests/mt/
index.htm.
F. J. Och and H. Ney. 2002. Discriminative
training and maximum entropy models for
statistical machine translation. In Proc. of
the 40th Annual Meeting of the Association
for Computational Linguistics (ACL), pages
295?302, Philadelphia, PA, July.
F. J. Och and H. Ney. 2003. A systematic com-
parison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51,
March.
F. J. Och, C. Tillmann, and H. Ney. 1999. Im-
proved alignment models for statistical ma-
chine translation. In Proc. of the Joint SIG-
DAT Conf. on Empirical Methods in Natu-
ral Language Processing and Very Large Cor-
pora, pages 20?28, University of Maryland,
College Park, MD, June.
D. D. Palmer. 1997. A trainable rule-based
algorithm for word segmentation. In Proc.
of the 35th Annual Meeting of ACL and 8th
Conference of the European Chapter of ACL,
pages 321?328, Madrid, Spain, August.
K. A. Papineni, S. Roukos, T. Ward, and W. J.
Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Techni-
cal Report RC22176 (W0109-022), IBM Re-
search Division, Thomas J. Watson Research
Center, September.
R. W. Sproat and C. Shih. 1990. A statistical
method for finding word boundaries in Chi-
nese text. Computer Processing of Chinese
and Oriental Languages, 4:336?351.
M. Sun, D. Shen, and B. K. Tsou. 1998. Chi-
nese word segmentation without using lexi-
con and hand-crafted training data. In Proc.
of the 36th Annual Meeting of ACL and
17th Int. Conf. on Computational Linguistics
(COLING-ACL 98), pages 1265?1271, Mon-
treal, Quebec, Canada, August.
S. Vogel, H. Ney, and C. Tillmann. 1996.
HMM-based word alignment in statistical
translation. In COLING ?96: The 16th Int.
Conf. on Computational Linguistics, pages
836?841, Copenhagen, Denmark, August.
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 167?174,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Novel Reordering Approaches in Phrase-Based Statistical Machine
Translation
Stephan Kanthak, David Vilar, Evgeny Matusov, Richard Zens, and Hermann Ney
The authors are with the Lehrstuhl fu?r Informatik VI,
Computer Science Department, RWTH Aachen University,
D-52056 Aachen, Germany.
E-mail: {kanthak,vilar,matusov,zens,ney}@informatik.rwth-aachen.de.
Abstract
This paper presents novel approaches to
reordering in phrase-based statistical ma-
chine translation. We perform consistent
reordering of source sentences in train-
ing and estimate a statistical translation
model. Using this model, we follow a
phrase-based monotonic machine transla-
tion approach, for which we develop an ef-
ficient and flexible reordering framework
that allows to easily introduce different re-
ordering constraints. In translation, we
apply source sentence reordering on word
level and use a reordering automaton as in-
put. We show how to compute reordering
automata on-demand using IBM or ITG
constraints, and also introduce two new
types of reordering constraints. We further
add weights to the reordering automata.
We present detailed experimental results
and show that reordering significantly im-
proves translation quality.
1 Introduction
Reordering is of crucial importance for machine
translation. Already (Knight et al, 1998) use full un-
weighted permutations on the level of source words
in their early weighted finite-state transducer ap-
proach which implemented single-word based trans-
lation using conditional probabilities. In a refine-
ment with additional phrase-based models, (Kumar
et al, 2003) define a probability distribution over
all possible permutations of source sentence phrases
and prune the resulting automaton to reduce com-
plexity.
A second category of finite-state translation ap-
proaches uses joint instead of conditional probabili-
ties. Many joint probability approaches originate in
speech-to-speech translation as they are the natural
choice in combination with speech recognition mod-
els. The automated transducer inference techniques
OMEGA (Vilar, 2000) and GIATI (Casacuberta et
al., 2004) work on phrase level, but ignore the re-
ordering problem from the view of the model. With-
out reordering both in training and during search,
sentences can only be translated properly into a lan-
guage with similar word order. In (Bangalore et al,
2000) weighted reordering has been applied to tar-
get sentences since defining a permutation model on
the source side is impractical in combination with
speech recognition. In order to reduce the computa-
tional complexity, this approach considers only a set
of plausible reorderings seen on training data.
Most other phrase-based statistical approaches
like the Alignment Template system of Bender
et al (2004) rely on (local) reorderings which are
implicitly memorized with each pair of source and
target phrases in training. Additional reorderings on
phrase level are fully integrated into the decoding
process, which increases the complexity of the sys-
tem and makes it hard to modify. Zens et al (2003)
reviewed two types of reordering constraints for this
type of translation systems.
In our work we follow a phrase-based transla-
tion approach, applying source sentence reordering
on word level. We compute a reordering graph on-
demand and take it as input for monotonic trans-
lation. This approach is modular and allows easy
introduction of different reordering constraints and
probabilistic dependencies. We will show that it per-
forms at least as well as the best statistical machine
translation system at the IWSLT Evaluation.
167
In the next section we briefly review the basic
theory of our translation system based on weighted
finite-state transducers (WFST). In Sec. 3 we in-
troduce new methods for reordering and alignment
monotonization in training. To compare differ-
ent reordering constraints used in the translation
search process we develop an on-demand com-
putable framework for permutation models in Sec. 4.
In the same section we also define and analyze un-
restricted and restricted permutations with some of
them being first published in this paper. We con-
clude the paper by presenting and discussing a rich
set of experimental results.
2 Machine Translation using WFSTs
Let fJ1 and eIi be two sentences from a source and
target language. Assume that we have word level
alignments A of all sentence pairs from a bilingual
training corpus. We denote with e?J1 the segmenta-
tion of a target sentence eI1 into J phrases such that
fJ1 and e?J1 can be aligned to form bilingual tuples
(fj , e?j). If alignments are only functions of target
words A? : {1, . . . , I} ? {1, . . . , J}, the bilingual
tuples (fj , e?j) can be inferred with e. g. the GIATI
method of (Casacuberta et al, 2004), or with our
novel monotonization technique (see Sec. 3). Each
source word will be mapped to a target phrase of one
or more words or an ?empty? phrase ?. In particular,
the source words which will remain non-aligned due
to the alignment functionality restriction are paired
with the empty phrase.
We can then formulate the problem of finding the
best translation e?I1 of a source sentence fJ1 :
e?I1 = argmax
eI1
Pr(fJ1 , e
I
1)
= argmax
e?J1
?
A?A
Pr(fJ1 , e?
J
1 , A)
?= argmax
e?J1
max
A?A
Pr(A) ? Pr(fJ1 , e?
J
1 |A)
?= argmax
e?J1
max
A?A
?
fj :j=1...J
Pr(fj , e?j |f
j?1
1 , e?
j?1
1 , A)
= argmax
e?J1
max
A?A
?
fj :j=1...J
p(fj , e?j |f
j?1
j?m, e?
j?1
j?m, A)
In other words: if we assume a uniform distri-
bution for Pr(A), the translation problem can be
mapped to the problem of estimating an m-gram lan-
guage model over a learned set of bilingual tuples
(fj , e?j). Mapping the bilingual language model to a
WFST T is canonical and it has been shown in (Kan-
thak et al, 2004) that the search problem can then be
rewritten using finite-state terminology:
e?I1 = project-output(best(fJ1 ? T )) .
This implementation of the problem as WFSTs may
be used to efficiently solve the search problem in
machine translation.
3 Reordering in Training
When the alignment function A? is not monotonic,
target language phrases e? can become very long.
For example in a completely non-monotonic align-
ment all target words are paired with the last aligned
source word, whereas all other source words form
tuples with the empty phrase. Therefore, for lan-
guage pairs with big differences in word order, prob-
ability estimates may be poor.
This problem can be solved by reordering either
source or target training sentences such that align-
ments become monotonic for all sentences. We
suggest the following consistent source sentence re-
ordering and alignment monotonization approach in
which we compute optimal, minimum-cost align-
ments.
First, we estimate a cost matrix C for each sen-
tence pair (fJ1 , eI1). The elements of this matrix cij
are the local costs of aligning a source word fj to a
target word ei. Following (Matusov et al, 2004), we
compute these local costs by interpolating state oc-
cupation probabilities from the source-to-target and
target-to-source training of the HMM and IBM-4
models as trained by the GIZA++ toolkit (Och et al,
2003). For a given alignment A ? I ? J , we define
the costs of this alignment c(A) as the sum of the
local costs of all aligned word pairs:
c(A) =
?
(i,j)?A
cij (1)
The goal is to find an alignment with the minimum
costs which fulfills certain constraints.
3.1 Source Sentence Reordering
To reorder a source sentence, we require the
alignment to be a function of source words A1:
{1, . . . , J} ? {1, . . . , I}, easily computed from the
cost matrix C as:
A1(j) = argmini cij (2)
168
We do not allow for non-aligned source words. A1
naturally defines a new order of the source words fJ1
which we denote by f?J1 . By computing this permu-
tation for each pair of sentences in training and ap-
plying it to each source sentence, we create a corpus
of reordered sentences.
3.2 Alignment Monotonization
In order to create a ?sentence? of bilingual tuples
(f?J1 , e?
J
1 ) we required alignments between reordered
source and target words to be a function of target
words A2 : {1, . . . , I} ? {1, . . . , J}. This align-
ment can be computed in analogy to Eq. 2 as:
A2(i) = argminj c?ij (3)
where c?ij are the elements of the new cost matrix
C? which corresponds to the reordered source sen-
tence. We can optionally re-estimate this matrix by
repeating EM training of state occupation probabili-
ties with GIZA++ using the reordered source corpus
and the original target corpus. Alternatively, we can
get the cost matrix C? by reordering the columns of
the cost matrix C according to the permutation given
by alignment A1.
In alignment A2 some target words that were pre-
viously unaligned in A1 (like ?the? in Fig. 1) may
now still violate the alignment monotonicity. The
monotonicity of this alignment can not be guaran-
teed for all words if re-estimation of the cost matri-
ces had been performed using GIZA++.
The general GIATI technique (Casacuberta et al,
2004) is applicable and can be used to monotonize
the alignment A2. However, in our experiments
the following method performs better. We make
use of the cost matrix representation and compute
a monotonic minimum-cost alignment with a dy-
namic programming algorithm similar to the Lev-
enshtein string edit distance algorithm. As costs of
each ?edit? operation we consider the local align-
ment costs. The resulting alignment A3 represents
a minimum-cost monotonic ?path? through the cost
matrix. To make A3 a function of target words we
do not consider the source words non-aligned in A2
and also forbid ?deletions? (?many-to-one? source
word alignments) in the DP search.
An example of such consistent reordering and
monotonization is given in Fig. 1. Here, we re-
order the German source sentence based on the ini-
tial alignment A1, then compute the function of tar-
get words A2, and monotonize this alignment to A3
the very beginning of May would suit me .
the very beginning of May would suit me .
sehr gut Anfang Mai w?rde passen mir .
sehr gut Anfang Mai w?rde passen mir .
the very beginning of May would suit me .
mir sehrw?rde gut Anfang Mai passen .
.Mai|of_May w?rde|would passen|suit mir|me |.sehr|the_very gut|$ Anfang|beginning
A
A
A1
2
3
Figure 1: Example of alignment, source sentence re-
ordering, monotonization, and construction of bilin-
gual tuples.
with the dynamic programming algorithm. Fig. 1
also shows the resulting bilingual tuples (f?j , e?j).
4 Reordering in Search
When searching the best translation e?J1 for a given
source sentence fJ1 , we permute the source sentence
as described in (Knight et al, 1998):
e?I1 = project-output(best(permute(fJ1 ) ? T ))
Permuting an input sequence of J symbols re-
sults in J ! possible permutations and representing
the permutations as a finite-state automaton requires
at least 2J states. Therefore, we opt for computing
the permutation automaton on-demand while apply-
ing beam pruning in the search.
4.1 Lazy Permutation Automata
For on-demand computation of an automaton in the
flavor described in (Kanthak et al, 2004) it is suffi-
cient to specify a state description and an algorithm
that calculates all outgoing arcs of a state from the
state description. In our case, each state represents
a permutation of a subset of the source words fJ1 ,
which are already translated.
This can be described by a bit vector bJ1 (Zens
et al, 2002). Each bit of the state bit vector corre-
sponds to an arc of the linear input automaton and is
set to one if the arc has been used on any path from
the initial to the current state. The bit vectors of two
states connected by an arc differ only in a single bit.
Note that bit vectors elegantly solve the problem of
recombining paths in the automaton as states with
169
the same bit vectors can be merged. As a result, a
fully minimized permutation automaton has only a
single initial and final state.
Even with on-demand computation, complexity
using full permutations is unmanagable for long sen-
tences. We further reduce complexity by addition-
ally constraining permutations. Refer to Figure 2 for
visualizations of the permutation constraints which
we describe in the following.
4.2 IBM Constraints
The IBM reordering constraints are well-known in
the field of machine translation and were first de-
scribed in (Berger et al, 1996). The idea behind
these constraints is to deviate from monotonic trans-
lation by postponing translations of a limited num-
ber of words. More specifically, at each state we
can translate any of the first l yet uncovered word
positions. The implementation using a bit vector is
straightforward. For consistency, we associate win-
dow size with the parameter l for all constraints pre-
sented here.
4.3 Inverse IBM Constraints
The original IBM constraints are useful for a large
number of language pairs where the ability to skip
some words reflects the differences in word order
between the two languages. For some other pairs,
it is beneficial to translate some words at the end of
the sentence first and to translate the rest of the sen-
tence nearly monotonically. Following this idea we
can define the inverse IBM constraints. Let j be the
first uncovered position. We can choose any posi-
tion for translation, unless l ? 1 words on positions
j? > j have been translated. If this is the case we
must translate the word in position j. The inverse
IBM constraints can also be expressed by
invIBM(x) = transpose(IBM(transpose(x))) .
As the transpose operation can not be computed
on-demand, our specialized implementation uses bit
vectors bJ1 similar to the IBM constraints.
4.4 Local Constraints
For some language pairs, e.g. Italian ? English,
words are moved only a few words to the left or
right. The IBM constraints provide too many alter-
native permutations to chose from as each word can
be moved to the end of the sentence. A solution that
allows only for local permutations and therefore has
a)
0000 10001 11002 11103 11114
b)
0000
10001
01002 1100
2
10103
1
0110
3
11103
110141
01114
1111
4
13
2
10114 2
c)
0000
10001
01002
0010
3
0001
4 10014
1010
3 11002
1
1
1
1101
2
11113
11102 4
43
d)
0000
10001
01002 1100
2
10103
1
11103
11014 1111
43
2
Figure 2: Permutations of a) positions j = 1, 2, 3, 4
of a source sentence f1f2f3f4 using a window size
of 2 for b) IBM constraints, c) inverse IBM con-
straints and d) local constraints.
very low complexity is given by the following per-
mutation rule: the next word for translation comes
from the window of l positions1 counting from the
first yet uncovered position. Note, that the local con-
straints define a true subset of the permutations de-
fined by the IBM constraints.
4.5 ITG Constraints
Another type of reordering can be obtained using In-
version Transduction Grammars (ITG) (Wu, 1997).
These constraints are inspired by bilingual bracket-
ing. They proved to be quite useful for machine
translation, e.g. see (Bender et al, 2004). Here,
we interpret the input sentence as a sequence of seg-
ments. In the beginning, each word is a segment of
its own. Longer segments are constructed by recur-
sively combining two adjacent segments. At each
1both covered and uncovered
170
Chinese English Japanese English Italian English
train sentences 20 000 20 000 66107
words 182 904 160 523 209 012 160 427 410 275 427 402
singletons 3 525 2 948 4 108 2 956 6 386 3 974
vocabulary 7 643 6 982 9 277 6 932 15 983 10 971
dev sentences 506 506 500
words 3 515 3 595 4 374 3 595 3 155 3 253
sentence length (avg/max) 6.95 / 24 7.01 / 29 8.64 / 30 7.01 / 29 5.79 / 24 6.51 / 25
test sentences 500 500 506
words 3 794 ? 4 370 ? 2 931 3 595
sentence length (avg/max) 7.59 / 62 7.16 / 71 8.74 / 75 7.16 / 71 6.31 / 27 6.84 / 28
Table 1: Statistics of the Basic Travel Expression (BTEC) corpora.
combination step, we either keep the two segments
in monotonic order or invert the order. This pro-
cess continues until only one segment for the whole
sentence remains. The on-demand computation is
implemented in spirit of Earley parsing.
We can modify the original ITG constraints to
further limit the number of reorderings by forbid-
ding segment inversions which violate IBM con-
straints with a certain window size. Thus, the re-
sulting reordering graph contains the intersection of
the reorderings with IBM and the original ITG con-
straints.
4.6 Weighted Permutations
So far, we have discussed how to generate the per-
mutation graphs under different constraints, but per-
mutations were equally probable. Especially for the
case of nearly monotonic translation it is make sense
to restrict the degree of non-monotonicity that we
allow when translating a sentence. We propose a
simple approach which gives a higher probability
to the monotone transitions and penalizes the non-
monotonic ones.
A state description bJ1 , for which the following
condition holds:
Mon(j) : bj? = ?(j
? ? j) ? 1 ? j? ? J
represents the monotonic path up to the word fj . At
each state we assign the probability ? to that out-
going arc where the target state description fullfills
Mon(j+1) and distribute the remaining probability
mass 1? ? uniformly among the remaining arcs. In
case there is no such arc, all outgoing arcs get the
same uniform probability. This weighting scheme
clearly depends on the state description and the out-
going arcs only and can be computed on-demand.
5 Experimental Results
5.1 Corpus Statistics
The translation experiments were carried out on the
Basic Travel Expression Corpus (BTEC), a multilin-
gual speech corpus which contains tourism-related
sentences usually found in travel phrase books.
We tested our system on the so called Chinese-to-
English (CE) and Japanese-to-English (JE) Supplied
Tasks, the corpora which were provided during the
International Workshop on Spoken Language Trans-
lation (IWSLT 2004) (Akiba et al, 2004). In ad-
dition, we performed experiments on the Italian-to-
English (IE) task, for which a larger corpus was
kindly provided to us by ITC/IRST. The corpus
statistics for the three BTEC corpora are given in
Tab. 1. The development corpus for the Italian-to-
English translation had only one reference transla-
tion of each Italian sentence. A set of 506 source
sentences and 16 reference translations is used as
a development corpus for Chinese-to-English and
Japanese-to-English and as a test corpus for Italian-
to-English tasks. The 500 sentence Chinese and
Japanese test sets of the IWSLT 2004 evaluation
campaign were translated and automatically scored
against 16 reference translations after the end of the
campaign using the IWSLT evaluation server.
5.2 Evaluation Criteria
For the automatic evaluation, we used the crite-
ria from the IWSLT evaluation campaign (Akiba et
al., 2004), namely word error rate (WER), position-
independent word error rate (PER), and the BLEU
and NIST scores (Papineni et al, 2002; Doddington,
2002). The two scores measure accuracy, i. e. larger
scores are better. The error rates and scores were
computed with respect to multiple reference transla-
171
 40
 42
 44
 46
 48
 50
 52
 54
 56
 58
 60
 1  2  3  4  5  6  7  8  9
reordering constraints window size
INV-IBMIBMITGLOCAL
 46
 47
 48
 49
 50
 51
 52
 53
 54
 55
 1  2  3  4  5  6  7  8  9
reordering constraints window size
INV-IBMIBMITGLOCAL
Figure 3: Word error rate [%] as a function of the reordering window size for different reordering constraints:
Japanese-to-English (left) and Chinese-to-English (right) translation.
tions, when they were available. To indicate this, we
will label the error rate acronyms with an m. Both
training and evaluation were performed using cor-
pora and references in lowercase and without punc-
tuation marks.
5.3 Experiments
We used reordering and alignment monotonization
in training as described in Sec. 3. To estimate the
matrices of local alignment costs for the sentence
pairs in the training corpus we used the state occupa-
tion probabilities of GIZA++ IBM-4 model training
and interpolated the probabilities of source-to-target
and target-to-source training directions. After that
we estimated a smoothed 4-gram language model on
the level of bilingual tuples fj , e?j and represented it
as a finite-state transducer.
When translating, we applied moderate beam
pruning to the search automaton only when using re-
ordering constraints with window sizes larger than 3.
For very large window sizes we also varied the prun-
ing thresholds depending on the length of the input
sentence. Pruning allowed for fast translations and
reasonable memory consumption without a signifi-
cant negative impact on performance.
In our first experiments, we tested the four re-
ordering constraints with various window sizes. We
aimed at improving the translation results on the de-
velopment corpora and compared the results with
two baselines: reordering only the source training
sentences and translation of the unreordered test sen-
tences; and the GIATI technique for creating bilin-
gual tuples (fj , e?j) without reordering of the source
sentences, neither in training nor during translation.
5.3.1 Highly Non-Monotonic Translation (JE)
Fig. 3 (left) shows word error rate on the
Japanese-to-English task as a function of the win-
dow size for different reordering constraints. For
each of the constraints, good results are achieved
using a window size of 9 and larger. This can be
attributed to the Japanese word order which is very
different from English and often follows a subject-
object-verb structure. For small window sizes, ITG
or IBM constraints are better suited for this task, for
larger window sizes, inverse IBM constraints per-
form best. The local constraints perform worst and
require very large window sizes to capture the main
word order differences between Japanese and En-
glish. However, their computational complexity is
low; for instance, a system with local constraints
and window size of 9 is as fast (25 words per sec-
ond) as the same system with IBM constraints and
window size of 5. Using window sizes larger than
10 is computationally expensive and does not sig-
nificantly improve the translation quality under any
of the constraints.
Tab. 2 presents the overall improvements in trans-
lation quality when using the best setting: inverse
IBM constraints, window size 9. The baseline with-
out reordering in training and testing failed com-
pletely for this task, producing empty translations
for 37 % of the sentences2. Most of the original
alignments in training were non-monotonic which
resulted in mapping of almost all Japanese words to
? when using only the GIATI monotonization tech-
nique. Thus, the proposed reordering methods are of
crucial importance for this task.
2Hence a NIST score of 0 due to the brevity penalty.
172
mWER mPER BLEU NIST
Reordering: [%] [%] [%]
BTEC Japanese-to-English (JE) dev
none 59.7 58.8 13.0 0.00
in training 57.8 39.4 14.7 3.27
+ 9-inv-ibm 40.3 32.1 45.1 8.59
+ rescoring* 39.1 30.9 53.2 9.93
BTEC Chinese-to-English (CE) dev
none 55.2 52.1 24.9 1.34
in training 54.0 42.3 23.0 4.18
+ 7-inv-ibm 47.1 39.4 34.5 6.53
+ rescoring* 48.3 40.7 39.1 8.11
Table 2: Translation results with optimal reorder-
ing constraints and window sizes for the BTEC
Japanese-to-English and Chinese-to-English devel-
opment corpora. *Optimized for the NIST score.
mWER mPER BLEU NIST
[%] [%] [%]
BTEC Japanese-to-English (JE) test
AT 41.9 33.8 45.3 9.49
WFST 42.1 35.6 47.3 9.50
BTEC Chinese-to-English (CE) test
AT 45.6 39.0 40.9 8.55
WFST 46.4 38.8 40.8 8.73
Table 3: Comparison of the IWSLT-2004 automatic
evaluation results for the described system (WFST)
with those of the best submitted system (AT).
Further improvements were obtained with a
rescoring procedure. For rescoring, we produced
a k-best list of translation hypotheses and used the
word penalty and deletion model features, the IBM
Model 1 lexicon score, and target language n-gram
models of the order up to 9. The scaling factors for
all features were optimized on the development cor-
pus for the NIST score, as described in (Bender et
al., 2004).
5.3.2 Moderately Non-Mon. Translation (CE)
Word order in Chinese and English is usually sim-
ilar. However, a few word reorderings over quite
large distances may be necessary. This is especially
true in case of questions, in which question words
like ?where? and ?when? are placed at the end of
a sentence in Chinese. The BTEC corpora contain
many sentences with questions.
The inverse IBM constraints are designed to per-
form this type of reordering (see Sec. 4.3). As shown
in Fig. 3, the system performs well under these con-
mWER mPER BLEU NIST
Reordering: [%] [%] [%]
none 25.6 22.0 62.1 10.46
in training 28.0 22.3 58.1 10.32
+ 4-local 26.3 20.3 62.2 10.81
+ weights 25.3 20.3 62.6 10.79
+ 3-ibm 27.2 20.5 61.4 10.76
+ weights 25.2 20.3 62.9 10.80
+ rescoring* 22.2 19.0 69.2 10.47
Table 4: Translation results with optimal reordering
constraints and window sizes for the test corpus of
the BTEC IE task. *Optimized for WER.
straints already with relatively small window sizes.
Increasing the window size beyond 4 for these con-
straints only marginally improves the translation er-
ror measures for both short (under 8 words) and long
sentences. Thus, a suitable language-pair-specific
choice of reordering constraints can avoid the huge
computational complexity required for permutations
of long sentences.
Tab. 2 includes error measures for the best setup
with inverse IBM constraints with window size of 7,
as well as additional improvements obtained by a k-
best list rescoring.
The best settings for reordering constraints and
model scaling factors on the development corpora
were then used to produce translations of the IWSLT
Japanese and Chinese test corpora. These trans-
lations were evaluated against multiple references
which were unknown to the authors. Our system
(denoted with WFST, see Tab. 3) produced results
competitive with the results of the best system at the
evaluation campaign (denoted with AT (Bender et
al., 2004)) and, according to some of the error mea-
sures, even outperformed this system.
5.3.3 Almost Monotonic Translation (IE)
The word order in the Italian language does not
differ much from the English. Therefore, the abso-
lute translation error rates are quite low and translat-
ing without reordering in training and search already
results in a relatively good performance. This is re-
flected in Tab. 4. However, even for this language
pair it is possible to improve translation quality by
performing reordering both in training and during
translation. The best performance on the develop-
ment corpus is obtained when we constrain the re-
odering with relatively small window sizes of 3 to 4
and use either IBM or local reordering constraints.
173
On the test corpus, as shown in Tab. 4, all error mea-
sures can be improved with these settings.
Especially for languages with similar word order
it is important to use weighted reorderings (Sec. 4.6)
in order to prefer the original word order. Introduc-
tion of reordering weights for this task results in no-
table improvement of most error measures using ei-
ther the IBM or local constraints. The optimal prob-
ability ? for the unreordered path was determined
on the development corpus as 0.5 for both of these
constraints. The results on the test corpus using this
setting are also given in Tab. 4.
6 Conclusion
In this paper, we described a reordering framework
which performs source sentence reordering on word
level. We suggested to use optimal alignment func-
tions for monotonization and improvement of trans-
lation model training. This allowed us to translate
monotonically taking a reordering graph as input.
We then described known and novel reordering con-
straints and their efficient finite-state implementa-
tions in which the reordering graph is computed on-
demand. We also utilized weighted permutations.
We showed that our monotonic phrase-based trans-
lation approach effectively makes use of the reorder-
ing framework to produce quality translations even
from languages with significantly different word or-
der. On the Japanese-to-English and Chinese-to-
English IWSLT tasks, our system performed at least
as well as the best machine translation system.
Acknowledgement
This work was partially funded by the Deutsche
Forschungsgemeinschaft (DFG) under the project
?Statistische Textu?bersetzung? (Ne572/5) and by the
European Union under the integrated project TC-
STAR ? Technology and Corpora for Speech to
Speech Translation (IST-2002-FP6-506738).
References
Y. Akiba, M. Federico, N. Kando, H. Nakaiwa, M. Paul,
and J. Tsujii. 2004. Overview of the IWSLT04 Evalu-
ation Campaign. Proc. Int. Workshop on Spoken Lan-
guage Translation, pp. 1?12, Kyoto, Japan.
S. Bangalore and G. Riccardi. 2000. Stochastic Finite-
State Models for Spoken Language Machine Transla-
tion. Proc. Workshop on Embedded Machine Transla-
tion Systems, pp. 52?59.
O. Bender, R. Zens, E. Matusov, and H. Ney. 2004.
Alignment Templates: the RWTH SMT System. Proc.
Int. Workshop on Spoken Language Translation, pp.
79?84, Kyoto, Japan.
A. L. Berger, P. F. Brown, S. A. Della Pietra, V. J. Della
Pietra, J. R. Gillett, A. S. Kehler, and R. L. Mercer.
1996. Language Translation Apparatus and Method
of Using Context-based Translation Models. United
States Patent 5510981.
F. Casacuberta and E. Vidal. 2004. Machine Transla-
tion with Inferred Stochastic Finite-State Transducers.
Computational Linguistics, vol. 30(2):205-225.
G. Doddington. 2002. Automatic Evaluation of Machine
Translation Quality Using n-gram Co-Occurrence
Statistics. Proc. Human Language Technology Conf.,
San Diego, CA.
S. Kanthak and H. Ney. 2004. FSA: an Efficient and
Flexible C++ Toolkit for Finite State Automata using
On-demand Computation. Proc. 42nd Annual Meet-
ing of the Association for Computational Linguistics,
pp. 510?517, Barcelona, Spain.
K. Knight and Y. Al-Onaizan. 1998. Translation with
Finite-State Devices. Lecture Notes in Artificial Intel-
ligence, Springer-Verlag, vol. 1529, pp. 421?437.
S. Kumar and W. Byrne. 2003. A Weighted Finite State
Transducer Implementation of the Alignment Template
Model for Statistical Machine Translation. Proc. Hu-
man Language Technology Conf. NAACL, pp. 142?
149, Edmonton, Canada.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric Word
Alignments for Statistical Machine Translation. Proc.
20th Int. Conf. on Computational Linguistics, pp. 219?
225, Geneva, Switzerland.
F. J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, vol. 29, number 1, pp. 19?51.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Machine
Translation. Proc. 40th Annual Meeting of the Associ-
ation for Computational Linguistics, Philadelphia, PA,
pp. 311?318.
J. M. Vilar, 2000. Improve the Learning of Sub-
sequential Transducers by Using Alignments and Dic-
tionaries. Lecture Notes in Artificial Intelligence,
Springer-Verlag, vol. 1891, pp. 298?312.
D. Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3):377?403.
R. Zens, F. J. Och and H. Ney. 2002. Phrase-Based Sta-
tistical Machine Translation. In: M. Jarke, J. Koehler,
G. Lakemeyer (Eds.): KI - Conference on AI, KI 2002,
Vol. LNAI 2479, pp. 18-32, Springer Verlag.
R. Zens and H. Ney. 2003. A Comparative Study on
Reordering Constraints in Statistical Machine Trans-
lation. Proc. Annual Meeting of the Association
for Computational Linguistics, pp. 144?151, Sapporo,
Japan.
174
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 191?198,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Word Graphs for Statistical Machine Translation
Richard Zens and Hermann Ney
Chair of Computer Science VI
RWTH Aachen University
{zens,ney}@cs.rwth-aachen.de
Abstract
Word graphs have various applications in
the field of machine translation. Therefore
it is important for machine translation sys-
tems to produce compact word graphs of
high quality. We will describe the gen-
eration of word graphs for state of the
art phrase-based statistical machine trans-
lation. We will use these word graph
to provide an analysis of the search pro-
cess. We will evaluate the quality of the
word graphs using the well-known graph
word error rate. Additionally, we intro-
duce the two novel graph-to-string crite-
ria: the position-independent graph word
error rate and the graph BLEU score.
Experimental results are presented for two
Chinese?English tasks: the small IWSLT
task and the NIST large data track task.
For both tasks, we achieve significant re-
ductions of the graph error rate already
with compact word graphs.
1 Introduction
A statistical machine translation system usually pro-
duces the single-best translation hypotheses for a
source sentence. For some applications, we are also
interested in alternative translations. The simplest
way to represent these alternatives is a list with the
N -best translation candidates. These N -best lists
have one major disadvantage: the high redundancy.
The translation alternatives may differ only by a sin-
gle word, but still both are listed completely. Usu-
ally, the size of the N -best list is in the range of a few
hundred up to a few thousand candidate translations
per source sentence. If we want to use larger N -best
lists the processing time gets very soon infeasible.
Word graphs are a much more compact represen-
tation that avoid these redundancies as much as pos-
sible. The number of alternatives in a word graph is
usually an order of magnitude larger than in an N -
best list. The graph representation avoids the com-
binatorial explosion that make large N -best lists in-
feasible.
Word graphs are an important data structure with
various applications:
? Word Filter.
The word graph is used as a compact repre-
sentation of a large number of sentences. The
score information is not contained.
? Rescoring.
We can use word graphs for rescoring with
more sophisticated models, e.g. higher-order
language models.
? Discriminative Training.
The training of the model scaling factors as de-
scribed in (Och and Ney, 2002) was done on
N -best lists. Using word graphs instead could
further improve the results. Also, the phrase
translation probabilities could be trained dis-
crimatively, rather than only the scaling factors.
? Confidence Measures.
Word graphs can be used to derive confidence
measures, such as the posterior probability
(Ueffing and Ney, 2004).
191
? Interactive Machine Translation.
Some interactive machine translation systems
make use of word graphs, e.g. (Och et al,
2003).
State Of The Art. Although there are these many
applications, there are only few publications directly
devoted to word graphs. The only publication, we
are aware of, is (Ueffing et al, 2002). The short-
comings of (Ueffing et al, 2002) are:
? They use single-word based models only. Cur-
rent state of the art statistical machine transla-
tion systems are phrase-based.
? Their graph pruning method is suboptimal as it
considers only partial scores and not full path
scores.
? The N -best list extraction does not eliminate
duplicates, i.e. different paths that represent the
same translation candidate.
? The rest cost estimation is not efficient. It has
an exponential worst-case time complexity. We
will describe an algorithm with linear worst-
case complexity.
Apart from (Ueffing et al, 2002), publications on
weighted finite state transducer approaches to ma-
chine translation, e.g. (Bangalore and Riccardi,
2001; Kumar and Byrne, 2003), deal with word
graphs. But to our knowledge, there are no publica-
tions that give a detailed analysis and evaluation of
the quality of word graphs for machine translation.
We will fill this gap and give a systematic descrip-
tion and an assessment of the quality of word graphs
for phrase-based machine translation. We will show
that even for hard tasks with very large vocabulary
and long sentences the graph error rate drops signif-
icantly.
The remaining part is structured as follows: first
we will give a brief description of the translation sys-
tem in Section 2. In Section 3, we will give a def-
inition of word graphs and describe the generation.
We will also present efficient pruning and N -best
list extraction techniques. In Section 4, we will de-
scribe evaluation criteria for word graphs. We will
use the graph word error rate, which is well known
from speech recognition. Additionally, we introduce
the novel position-independent word graph error rate
and the graph BLEU score. These are generaliza-
tions of the commonly used string-to-string evalua-
tion criteria in machine translation. We will present
experimental results in Section 5 for two Chinese?
English tasks: the first one, the IWSLT task, is in the
domain of basic travel expression found in phrase-
books. The vocabulary is limited and the sentences
are short. The second task is the NIST Chinese?
English large data track task. Here, the domain is
news and therefore the vocabulary is very large and
the sentences are with an average of 30 words quite
long.
2 Translation System
In this section, we give a brief description of the
translation system. We use a phrase-based transla-
tion approach as described in (Zens and Ney, 2004).
The posterior probability Pr(eI1|fJ1 ) is modeled di-
rectly using a weighted log-linear combination of
a trigram language model and various translation
models: a phrase translation model and a word-
based lexicon model. These translation models are
used for both directions: p(f |e) and p(e|f). Addi-
tionally, we use a word penalty and a phrase penalty.
With the exception of the language model, all mod-
els can be considered as within-phrase models as
they depend only on a single phrase pair, but not on
the context outside of the phrase. The model scaling
factors are optimized with respect to some evalua-
tion criterion (Och, 2003).
We extended the monotone search algorithm from
(Zens and Ney, 2004) such that reorderings are pos-
sible. In our case, we assume that local reorder-
ings are sufficient. Within a certain window, all
possible permutations of the source positions are al-
lowed. These permutations are represented as a re-
ordering graph, similar to (Zens et al, 2002). Once
we have this reordering graph, we perform a mono-
tone phrase-based translation of this graph. More
details of this reordering approach are described in
(Kanthak et al, 2005).
3 Word Graphs
3.1 Definition
A word graph is a directed acyclic graph G = (V, E)
with one designated root node n0 ? V . The edges
are labeled with words and optionally with scores.
We will use (n, n?, w) to denote an edge from node
192
n to node n? with word label w. Each path through
the word graph represents a translation candidate. If
the word graph contains scores, we accumulate the
edge scores along a path to get the sentence or string
score.
The score information the word graph has to con-
tain depends on the application.
If we want to use the word graph as a word fil-
ter, we do not need any score information at all. If
we want to extract the single- or N -best hypotheses,
we have to retain the string or sentence score infor-
mation. The information about the hidden variables
of the search, e.g. the phrase segmentation, is not
needed for this purpose. For discriminative training
of the phrase translation probabilities, we need all
the information, even about the hidden variables.
3.2 Generation
In this section, we analyze the search process in de-
tail. Later, in Section 5, we will show the (experi-
mental) complexity of each step. We start with the
source language sentence that is represented as a lin-
ear graph. Then, we introduce reorderings into this
graph as described in (Kanthak et al, 2005). The
type of reordering should depend on the language
pair. In our case, we assume that only local reorder-
ings are required. Within a certain window, all pos-
sible reorderings of the source positions are allowed.
These permutations are represented as a reordering
graph, similar to (Knight and Al-Onaizan, 1998) and
(Zens et al, 2002).
Once we have this reordering graph, we perform
a monotone phrase-based translation of this graph.
This translation process consists of the following
steps that will be described afterward:
1. segment into phrase
2. translate the individual phrases
3. split the phrases into words
4. apply the language model
Now, we will describe each step. The first step is
the segmentation into phrases. This can be imag-
ined as introducing ?short-cuts? into the graph. The
phrase segmentation does not affect the number of
nodes, because only additional edges are added to
the graph.
In the segmented graph, each edge represents a
source phrase. Now, we replace each edge with one
edge for each possible phrase translation. The edge
scores are the combination of the different transla-
tion probabilities, namely the within-phrase models
mentioned in Section 2. Again, this step does not
increase the number of nodes, but only the number
of edges.
So far, the edge labels of our graph are phrases. In
the final word graph, we want to have words as edge
labels. Therefore, we replace each edge representing
a multi-word target phrase with a sequence of edges
that represent the target word sequence. Obviously,
edges representing a single-word phrase do not have
to be changed.
As we will show in the results section, the word
graphs up to this point are rather compact. The
score information in the word graph so far consists
of the reordering model scores and the phrase trans-
lation model scores. To obtain the sentence posterior
probability p(eI1|fJ1 ), we apply the target language
model. To do this, we have to separate paths accord-
ing to the language model history. This increases the
word graph size by an order of magnitude.
Finally, we have generated a word graph with full
sentence scores. Note that the word graph may con-
tain a word sequence multiple times with different
hidden variables. For instance, two different seg-
mentations into source phrases may result in the
same target sentence translation.
The described steps can be implemented using
weighted finite state transducer, similar to (Kumar
and Byrne, 2003).
3.3 Pruning
To adjust the size of the word graph to the desired
density, we can reduce the word graph size using
forward-backward pruning, which is well-known in
the speech recognition community, e.g. see (Mangu
et al, 2000). This pruning method guarantees that
the good strings (with respect to the model scores)
remain in the word graph, whereas the bad ones are
removed. The important point is that we compare
the full path scores and not only partial scores as, for
instance, in the beam pruning method in (Ueffing et
al., 2002).
The forward probabilities F (n) and backward
probabilities B(n) of a node n are defined by the
193
following recursive equations:
F (n) =
?
(n?,n,w)?E
F (n?) ? p(n?, n, w)
B(n) =
?
(n,n?,w)?E
B(n?) ? p(n, n?, w)
The forward probability of the root node and the
backward probabilities of the final nodes are initial-
ized with one. Using a topological sorting of the
nodes, the forward and backward probabilities can
be computed with linear time complexity. The pos-
terior probability q(n, n?, w) of an edge is defined
as:
q(n, n?, w) = F (n) ? p(n, n
?, w) ? B(n?)
B(n0)
The posterior probability of an edge is identical to
the sum over the probabilities of all full paths that
contain this edge. Note that the backward probabil-
ity of the root node B(n0) is identical to the sum
over all sentence probabilities in the word graph.
Let q? denoted the maximum posterior probability
of all edges and let ? be a pruning threshold, then
we prune an edge (n, n?, w) if:
q(n, n?, w) < q? ? ?
3.4 N -Best List Extraction
In this section, we describe the extraction of the N -
best translation candidates from a word graph.
(Ueffing et al, 2002) and (Mohri and Riley, 2002)
both present an algorithm based on the same idea:
use a modified A* algorithm with an optimal rest
cost estimation. As rest cost estimation, the negated
logarithm of the backward probabilities is used. The
algorithm in (Ueffing et al, 2002) has two disadvan-
tages: it does not care about duplicates and the rest
cost computation is suboptimal as the described al-
gorithm has an exponential worst-case complexity.
As mentioned in the previous section, the backward
probabilities can be computed in linear time.
In (Mohri and Riley, 2002) the word graph is rep-
resented as a weighted finite state automaton. The
word graph is first determinized, i.e. the nondeter-
ministic automaton is transformed in an equivalent
deterministic automaton. This process removes the
duplicates from the word graph. Out of this deter-
minized word graph, the N best candidates are ex-
tracted. In (Mohri and Riley, 2002), ?-transitions are
ignored, i.e. transitions that do not produce a word.
These ?-transitions usually occur in the backing-off
case of language models. The ?-transitions have to
be removed before using the algorithm of (Mohri
and Riley, 2002). In the presence of ?-transitions,
two path representing the same string are considered
equal only if the ?-transitions are identical as well.
4 Evaluation Criteria
4.1 String-To-String Criteria
To evaluate the single-best translation hypotheses,
we use the following string-to-string criteria: word
error rate (WER), position-independent word error
rate (PER) and the BLEU score. More details on
these standard criteria can be found for instance in
(Och, 2003).
4.2 Graph-To-String Criteria
To evaluate the quality of the word graphs, we
generalize the string-to-string criteria to work on
word graphs. We will use the well-known graph
word error rate (GWER), see also (Ueffing et al,
2002). Additionally, we introduce two novel graph-
to-string criteria, namely the position-independent
graph word error rate (GPER) and the graph BLEU
score (GBLEU). The idea of these graph-to-string
criteria is to choose a sequence from the word graph
and compute the corresponding string-to-string cri-
terion for this specific sequence. The choice of the
sequence is such that the criterion is the optimum
over all possible sequences in the word graph, i.e.
the minimum for GWER/GPER and the maximum
for GBLEU.
The GWER is a generalization of the word er-
ror rate. It is a lower bound for the WER. It can be
computed using a dynamic programming algorithm
which is quite similar to the usual edit distance com-
putation. Visiting the nodes of the word graph in
topological order helps to avoid repeated computa-
tions.
The GPER is a generalization of the position-
independent word error rate. It is a lower bound for
the PER. The computation is not as straightforward
as for the GWER.
In (Ueffing and Ney, 2004), a method for com-
puting the string-to-string PER is presented. This
method cannot be generalized for the graph-to-string
computation in a straightforward way. Therefore,
194
we will first describe an alternative computation for
the string-to-string PER and then use this idea for
the graph-to-string PER.
Now, we want to compute the number of position-
independent errors for two strings. As the word or-
der of the strings does not matter, we represent them
as multisets1 A and B. To do this, it is sufficient to
know how many words are in A but not in B, i.e.
a := |A?B|, and how many words are in B but not
in A, i.e. b := |B?A|. The number of substitutions,
insertions and deletions are then:
sub = min{a, b}
ins = a ? sub
del = b ? sub
error = sub + ins + del
= a + b ? min{a, b}
= max{a, b}
It is obvious that there are either no insertions or no
deletions. The PER is then computed as the num-
ber of errors divided by the length of the reference
string.
Now, back to the graph-to-string PER computa-
tion. The information we need at each node of the
word graph are the following: the remaining multi-
set of words of the reference string that are not yet
produced. We denote this multiset C. The cardinal-
ity of this multiset will become the value a in the
preceding notation. In addition to this multiset, we
also need to count the number of words that we have
produced on the way to this node but which are not
in the reference string. The identity of these words is
not important, we simply have to count them. This
count will become the value b in the preceding nota-
tion.
If we make a transition to a successor node along
an edge labeled w, we remove that word w from the
set of remaining reference words C or, if the word
w is not in this set, we increase the count of words
that are in the hypothesis but not in the reference.
To compute the number of errors on a graph, we
use the auxiliary quantity Q(n, C), which is the
count of the produced words that are not in the refer-
ence. We use the following dynamic programming
recursion equations:
1A multiset is a set that may contain elements multiple
times.
Q(n0, C0) = 0
Q(n, C) = min
n?,w:(n?,n,w)?E
{
Q(n?, C ? {w}),
Q(n?, C) + 1
}
Here, n0 denote the root node of the word graph,
C0 denotes the multiset representation of the refer-
ence string. As already mentioned in Section 3.1,
(n?, n, w) denotes an edge from node n? to node n
with word label w.
In the implementation, we use a bit vector to rep-
resent the set C for efficiency reasons. Note that in
the worst-case the size of the Q-table is exponen-
tial in the length of the reference string. However, in
practice we found that in most cases the computation
is quite fast.
The GBLEU score is a generalization of the
BLEU score. It is an upper bound for the BLEU
score. The computation is similar to the GPER com-
putation. We traverse the word graph in topologi-
cal order and store the following information: the
counts of the matching n-grams and the length of the
hypothesis, i.e. the depth in the word graph. Addi-
tionally, we need the multiset of reference n-grams
that are not yet produced.
To compute the BLEU score, the n-gram counts
are collected over the whole test set. This results in
a combinatorial problem for the computation of the
GBLEU score. We process the test set sentence-wise
and accumulate the n-gram counts. After each sen-
tence, we take a greedy decision and choose the n-
gram counts that, if combined with the accumulated
n-gram counts, result is the largest BLEU score.
This gives a conservative approximation of the true
GBLEU score.
4.3 Word Graph Size
To measure the word graph size we use the word
graph density, which we define as the number of
edges in the graph divided by the source sentence
length.
5 Experimental Results
5.1 Tasks
We will show experimental results for two Chinese?
English translation tasks.
195
Table 1: IWSLT Chinese?English Task: corpus
statistics of the bilingual training data.
Chinese English
Train Sentences 20 000
Running Words 182 904 160 523
Vocabulary 7 643 6 982
Test Sentences 506
Running Words 3 515 3 595
avg. SentLen 6.9 7.1
Table 2: NIST Chinese English task: corpus statis-
tics of the bilingual training data.
Chinese English
Train Sentences 3.2M
Running Words 51.4M 55.5M
Vocabulary 80 010 170 758
Lexicon Entries 81 968
Test Sentences 878
Running Words 26 431 23 694
avg. SentLen 30.1 27.0
IWSLT Chinese?English Task. The first task is
the Chinese?English supplied data track task of the
International Workshop on Spoken Language Trans-
lation (IWSLT 2004) (Akiba et al, 2004). The do-
main is travel expressions from phrase-books. This
is a small task with a clean training and test corpus.
The vocabulary is limited and the sentences are rel-
atively short. The corpus statistics are shown in Ta-
ble 1. The Chinese part of this corpus is already
segmented into words.
NIST Chinese?English Task. The second task
is the NIST Chinese?English large data track task.
For this task, there are many bilingual corpora avail-
able. The domain is news, the vocabulary is very
large and the sentences have an average length of 30
words. We train our statistical models on various
corpora provided by LDC. The Chinese part is seg-
mented using the LDC segmentation tool. After the
preprocessing, our training corpus consists of about
three million sentences with somewhat more than 50
million running words. The corpus statistics of the
preprocessed training corpus are shown in Table 2.
We use the NIST 2002 evaluation data as test set.
 15
 20
 25
 30
 35
 40
 45
 50
 0  200  400  600  800  1000  1200
gr
ap
h 
wo
rd
 e
rro
r r
at
e 
[%
]
word graph density
window-size-1
window-size-2
window-size-3
window-size-4
window-size-5
Figure 1: IWSLT Chinese?English: Graph error rate
as a function of the word graph density for different
window sizes.
5.2 Search Space Analysis
In Table 3, we show the search space statistics of the
IWSLT task for different reordering window sizes.
Each line shows the resulting graph densities after
the corresponding step in our search as described in
Section 3.2. Our search process starts with the re-
ordering graph. The segmentation into phrases in-
creases the graph densities by a factor of two. Doing
the phrase translation results in an increase of the
densities by a factor of twenty. Unsegmenting the
phrases, i.e. replacing the phrase edges with a se-
quence of word edges doubles the graph sizes. Ap-
plying the language model results in a significant in-
crease of the word graphs.
Another interesting aspect is that increasing the
window size by one roughly doubles the search
space.
5.3 Word Graph Error Rates
In Figure 1, we show the graph word error rate for
the IWSLT task as a function of the word graph den-
sity. This is done for different window sizes for
the reordering. We see that the curves start with a
single-best word error rate of about 50%. For the
monotone search, the graph word error rate goes
down to about 31%. Using local reordering during
the search, we can further decrease the graph word
error rate down to less than 17% for a window size
of 5. This is almost one third of the single-best word
error rate. If we aim at halving the single-best word
error rate, word graphs with a density of less than
196
Table 3: IWSLT Chinese?English: Word graph densities for different window sizes and different stages of
the search process.
language level graph type window size
1 2 3 4 5
source word reordering 1.0 2.7 6.2 12.8 24.4
phrase segmented 2.0 5.0 12.1 26.8 55.6
target translated 40.8 99.3 229.0 479.9 932.8
word TM scores 78.6 184.6 419.2 869.1 1 670.4
+ LM scores 958.2 2874.2 7649.7 18 029.7 39 030.1
 20
 25
 30
 35
 40
 45
 50
 55
 60
 65
 70
 0  200  400  600  800  1000  1200  1400
gr
ap
h 
wo
rd
 e
rro
r r
at
e 
[%
]
word graph density
window-size-1
window-size-2
window-size-3
window-size-4
window-size-5
Figure 2: NIST Chinese?English: Graph error rate
as a function of the word graph density for different
window sizes.
200 would already be sufficient.
In Figure 2, we show the same curves for the
NIST task. Here, the curves start from a single-best
word error rate of about 64%. Again, dependent on
the amount of reordering the graph word error rate
goes down to about 36% for the monotone search
and even down to 23% for the search with a window
of size 5. Again, the reduction of the graph word er-
ror rate compare to the single-best error rate is dra-
matic. For comparison we produced an N -best list
of size 10 000. The N -best list error rate (or oracle-
best WER) is still 50.8%. A word graph with a den-
sity of only 8 has about the same GWER.
In Figure 3, we show the graph position-
independent word error rate for the IWSLT task. As
this error criterion ignores the word order it is not
affected by reordering and we show only one curve.
We see that already for small word graph densities
the GPER drops significantly from about 42% down
to less than 14%.
 10
 15
 20
 25
 30
 35
 40
 45
 0  50  100  150  200  250  300  350
po
s.
-in
de
p.
gr
ap
h 
wo
rd
 e
rro
r r
at
e 
[%
]
word graph density
Figure 3: IWSLT Chinese?English: Graph position-
independent word error rate as a function of the
word graph density.
In Figure 4, we show the graph BLEU scores for
the IWSLT task. We observe that, similar to the
GPER, the GBLEU score increases significantly al-
ready for small word graph densities. We attribute
this to the fact that the BLEU score and especially
the PER are less affected by errors of the word or-
der than the WER. This also indicates that produc-
ing translations with correct word order, i.e. syntac-
tically well-formed sentences, is one of the major
problems of current statistical machine translation
systems.
6 Conclusion
We have described word graphs for statistical ma-
chine translation. The generation of word graphs
during the search process has been described in de-
tail. We have shown detailed statistics of the in-
dividual steps of the translation process and have
given insight in the experimental complexity of each
step. We have described an efficient and optimal
197
 30
 35
 40
 45
 50
 55
 60
 65
 70
 0  50  100  150  200  250
gr
ap
h 
BL
EU
 s
co
re
 [%
]
word graph density
window size = 1
window size = 2
window size = 3
window size = 4
window size = 5
Figure 4: IWSLT Chinese?English: Graph BLEU
score as a function of the word graph density.
pruning method for word graphs. Using these tech-
nique, we have generated compact word graphs for
two Chinese?English tasks. For the IWSLT task, the
graph error rate drops from about 50% for the single-
best hypotheses to 17% of the word graph. Even for
the NIST task, with its very large vocabulary and
long sentences, we were able to reduce the graph er-
ror rate significantly from about 64% down to 23%.
Acknowledgment
This work was partly funded by the European Union
under the integrated project TC-Star (Technology
and Corpora for Speech to Speech Translation, IST-
2002-FP6-506738, http://www.tc-star.org).
References
Y. Akiba, M. Federico, N. Kando, H. Nakaiwa, M. Paul, and
J. Tsujii. 2004. Overview of the IWSLT04 evaluation cam-
paign. In Proc. of the Int. Workshop on Spoken Language
Translation (IWSLT), pages 1?12, Kyoto, Japan, Septem-
ber/October.
S. Bangalore and G. Riccardi. 2001. A finite-state approach to
machine translation. In Proc. Conf. of the North American
Association of Computational Linguistics (NAACL), Pitts-
burgh, May.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney. 2005.
Novel reordering approaches in phrase-based statistical ma-
chine translation. In 43rd Annual Meeting of the Assoc. for
Computational Linguistics: Proc. Workshop on Building and
Using Parallel Texts: Data-Driven Machine Translation and
Beyond, Ann Arbor, MI, June.
K. Knight and Y. Al-Onaizan. 1998. Translation with finite-
state devices. In D. Farwell, L. Gerber, and E. H. Hovy,
editors, AMTA, volume 1529 of Lecture Notes in Computer
Science, pages 421?437. Springer Verlag.
S. Kumar and W. Byrne. 2003. A weighted finite state trans-
ducer implementation of the alignment template model for
statistical machine translation. In Proc. of the Human Lan-
guage Technology Conf. (HLT-NAACL), pages 63?70, Ed-
monton, Canada, May/June.
L. Mangu, E. Brill, and A. Stolcke. 2000. Finding consensus
in speech recognition: Word error minimization and other
applications of confusion networks. Computer, Speech and
Language, 14(4):373?400, October.
M. Mohri and M. Riley. 2002. An efficient algorithm for the n-
best-strings problem. In Proc. of the 7th Int. Conf. on Spoken
Language Processing (ICSLP?02), pages 1313?1316, Den-
ver, CO, September.
F. J. Och and H. Ney. 2002. Discriminative training and max-
imum entropy models for statistical machine translation. In
Proc. of the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 295?302, Philadelphia,
PA, July.
F. J. Och, R. Zens, and H. Ney. 2003. Efficient search for in-
teractive statistical machine translation. In EACL03: 10th
Conf. of the Europ. Chapter of the Association for Com-
putational Linguistics, pages 387?393, Budapest, Hungary,
April.
F. J. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proc. of the 41th Annual Meeting of
the Association for Computational Linguistics (ACL), pages
160?167, Sapporo, Japan, July.
N. Ueffing and H. Ney. 2004. Bayes decision rule and
confidence measures for statistical machine translation. In
Proc. EsTAL - Espan?a for Natural Language Processing,
pages 70?81, Alicante, Spain, October.
N. Ueffing, F. J. Och, and H. Ney. 2002. Generation of word
graphs in statistical machine translation. In Proc. of the
Conf. on Empirical Methods for Natural Language Process-
ing (EMNLP), pages 156?163, Philadelphia, PA, July.
R. Zens and H. Ney. 2004. Improvements in phrase-based
statistical machine translation. In Proc. of the Human
Language Technology Conf. (HLT-NAACL), pages 257?264,
Boston, MA, May.
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based statistical
machine translation. In M. Jarke, J. Koehler, and G. Lake-
meyer, editors, 25th German Conf. on Artificial Intelligence
(KI2002), volume 2479 of Lecture Notes in Artificial Intel-
ligence (LNAI), pages 18?32, Aachen, Germany, September.
Springer Verlag.
198
Proceedings of the Workshop on Statistical Machine Translation, pages 55?63,
New York City, June 2006. c?2006 Association for Computational Linguistics
Discriminative Reordering Models for Statistical Machine Translation
Richard Zens and Hermann Ney
Human Language Technology and Pattern Recognition
Lehrstuhl fu?r Informatik 6 ? Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
{zens,ney}@cs.rwth-aachen.de
Abstract
We present discriminative reordering
models for phrase-based statistical ma-
chine translation. The models are trained
using the maximum entropy principle.
We use several types of features: based on
words, based on word classes, based on
the local context. We evaluate the overall
performance of the reordering models as
well as the contribution of the individual
feature types on a word-aligned corpus.
Additionally, we show improved transla-
tion performance using these reordering
models compared to a state-of-the-art
baseline system.
1 Introduction
In recent evaluations, phrase-based statistical ma-
chine translation systems have achieved good per-
formance. Still the fluency of the machine transla-
tion output leaves much to desire. One reason is
that most phrase-based systems use a very simple re-
ordering model. Usually, the costs for phrase move-
ments are linear in the distance, e.g. see (Och et al,
1999; Koehn, 2004; Zens et al, 2005).
Recently, in (Tillmann and Zhang, 2005) and in
(Koehn et al, 2005), a reordering model has been
described that tries to predict the orientation of a
phrase, i.e. it answers the question ?should the next
phrase be to the left or to the right of the current
phrase?? This phrase orientation probability is con-
ditioned on the current source and target phrase and
relative frequencies are used to estimate the proba-
bilities.
We adopt the idea of predicting the orientation,
but we propose to use a maximum-entropy based
model. The relative-frequency based approach may
suffer from the data sparseness problem, because
most of the phrases occur only once in the training
corpus. Our approach circumvents this problem by
using a combination of phrase-level and word-level
features and by using word-classes or part-of-speech
information. Maximum entropy is a suitable frame-
work for combining these different features with a
well-defined training criterion.
In (Koehn et al, 2005) several variants of the ori-
entation model have been tried. It turned out that for
different tasks, different models show the best per-
formance. Here, we let the maximum entropy train-
ing decide which features are important and which
features can be neglected. We will see that addi-
tional features do not hurt performance and can be
safely added to the model.
The remaining part is structured as follows: first
we will describe the related work in Section 2 and
give a brief description of the baseline system in
Section 3. Then, we will present the discriminative
reordering model in Section 4. Afterwards, we will
evaluate the performance of this new model in Sec-
tion 5. This evaluation consists of two parts: first we
will evaluate the prediction capabilities of the model
on a word-aligned corpus and second we will show
improved translation quality compared to the base-
line system. Finally, we will conclude in Section 6.
2 Related Work
As already mentioned in Section 1, many current
phrase-based statistical machine translation systems
use a very simple reordering model: the costs
55
for phrase movements are linear in the distance.
This approach is also used in the publicly available
Pharaoh decoder (Koehn, 2004). The idea of pre-
dicting the orientation is adopted from (Tillmann
and Zhang, 2005) and (Koehn et al, 2005). Here,
we use the maximum entropy principle to combine
a variety of different features.
A reordering model in the framework of weighted
finite state transducers is described in (Kumar and
Byrne, 2005). There, the movements are defined at
the phrase level, but the window for reordering is
very limited. The parameters are estimated using an
EM-style method.
None of these methods try to generalize from the
words or phrases by using word classes or part-of-
speech information.
The approach presented here has some resem-
blance to the bracketing transduction grammars
(BTG) of (Wu, 1997), which have been applied to
a phrase-based machine translation system in (Zens
et al, 2004). The difference is that, here, we do
not constrain the phrase reordering. Nevertheless
the inverted/monotone concatenation of phrases in
the BTG framework is similar to the left/right phrase
orientation used here.
3 Baseline System
In statistical machine translation, we are given a
source language sentence fJ1 = f1 . . . fj . . . fJ ,
which is to be translated into a target language sen-
tence eI1 = e1 . . . ei . . . eI . Among all possible tar-
get language sentences, we will choose the sentence
with the highest probability:
e?I?1 = argmax
I,eI1
{
Pr(eI1|fJ1 )
} (1)
The posterior probability Pr(eI1|fJ1 ) is modeled di-
rectly using a log-linear combination of several
models (Och and Ney, 2002):
Pr(eI1|fJ1 ) =
exp
(
?M
m=1 ?mhm(eI1, fJ1 )
)
?
I?,e?I?1
exp
(
?M
m=1 ?mhm(e?I
?
1 , fJ1 )
)
(2)
The denominator represents a normalization factor
that depends only on the source sentence fJ1 . There-
fore, we can omit it during the search process. As a
decision rule, we obtain:
e?I?1 = argmax
I,eI1
{ M
?
m=1
?mhm(eI1, fJ1 )
}
(3)
This approach is a generalization of the source-
channel approach (Brown et al, 1990). It has the
advantage that additional models h(?) can be eas-
ily integrated into the overall system. The model
scaling factors ?M1 are trained with respect to the fi-
nal translation quality measured by an error criterion
(Och, 2003).
We use a state-of-the-art phrase-based translation
system (Zens and Ney, 2004; Zens et al, 2005) in-
cluding the following models: an n-gram language
model, a phrase translation model and a word-based
lexicon model. The latter two models are used for
both directions: p(f |e) and p(e|f). Additionally,
we use a word penalty and a phrase penalty. The
reordering model of the baseline system is distance-
based, i.e. it assigns costs based on the distance from
the end position of a phrase to the start position of
the next phrase. This very simple reordering model
is widely used, for instance in (Och et al, 1999;
Koehn, 2004; Zens et al, 2005).
4 The Reordering Model
4.1 Idea
In this section, we will describe the proposed dis-
criminative reordering model.
To make use of word level information, we need
the word alignment within the phrase pairs. This can
be easily stored during the extraction of the phrase
pairs from the bilingual training corpus. If there are
multiple possible alignments for a phrase pair, we
use the most frequent one.
The notation is introduced using the illustration in
Figure 1. There is an example of a left and a right
phrase orientation. We assume that we have already
produced the three-word phrase in the lower part.
Now, the model has to predict if the start position
of the next phrase j? is to the left or to the right of
the current phrase. The reordering model is applied
only at the phrase boundaries. We assume that the
reordering within the phrases is correct.
In the remaining part of this section, we will de-
scribe the details of this reordering model. The
classes our model predicts will be defined in Sec-
tion 4.2. Then, the feature functions will be defined
56
ta
rg
et
 p
os
iti
on
s
source positions
j j?
i
ta
rg
et
 p
os
iti
on
s
source positions
i
right phrase orientationleft phrase orientation
jj?
Figure 1: Illustration of the phrase orientation.
in Section 4.3. The training criterion and the train-
ing events of the maximum entropy model will be
described in Section 4.4.
4.2 Class Definition
Ideally, this model predicts the start position of the
next phrase. But as predicting the exact position is
rather difficult, we group the possible start positions
into classes. In the simplest case, we use only two
classes. One class for the positions to the left and
one class for the positions to the right. As a refine-
ment, we can use four classes instead of two: 1) one
position to the left, 2) more than one positions to the
left, 3) one position to the right, 4) more than one
positions to the right.
In general, we use a parameter D to specify 2 ? D
classes of the types:
? exactly d positions to the left, d = 1, ...,D ? 1
? at least D positions to the left
? exactly d positions to the right, d = 1, ...,D?1
? at least D positions to the right
Let cj,j? denote the orientation class for a move-
ment from source position j to source position j? as
illustrated in Figure 1. In the case of two orientation
classes, cj,j? is defined as:
cj,j? =
{
left, if j? < j
right, if j? > j (4)
Then, the reordering model has the form
p(cj,j?|fJ1 , eI1, i, j)
A well-founded framework for directly modeling the
probability p(cj,j?|fJ1 , eI1, i, j) is maximum entropy
(Berger et al, 1996). In this framework, we have a
set of N feature functions hn(fJ1 , eI1, i, j, cj,j?), n =
1, . . . ,N . Each feature function hn is weighted with
a factor ?n. The resulting model is:
p?N1 (cj,j?|f
J
1 , eI1, i, j)
=
exp
( N
?
n=1
?nhn(fJ1 , eI1, i, j, cj,j?)
)
?
c?
exp
( N
?
n=1
?nhn(fJ1 , eI1, i, j, c?)
) (5)
The functional form is identical to Equation 2,
but here we will use a large number of binary
features, whereas in Equation 2 usually only a
very small number of real-valued features is used.
More precisely, the resulting reordering model
p?N1 (cj,j?|f
J
1 , eI1, i, j) is used as an additional com-
ponent in the log-linear combination of Equation 2.
4.3 Feature Definition
The feature functions of the reordering model de-
pend on the last alignment link (j, i) of a phrase.
Note that the source position j is not necessarily the
57
end position of the source phrase. We use the source
position j which is aligned to the last word of the
target phrase in target position i. The illustration in
Figure 1 contains such an example.
To introduce generalization capabilities, some of
the features will depend on word classes or part-
of-speech information. Let F J1 denote the word
class sequence that corresponds to the source lan-
guage sentence fJ1 and let EI1 denote the target word
class sequence that corresponds to the target lan-
guage sentence eI1. Then, the feature functions are
of the form hn(fJ1 , eI1, F J1 , EI1 , i, j, j?). We consider
the following binary features:
1. source words within a window around the cur-
rent source position j
hf,d,c(fJ1 , eI1, F J1 , EI1 , i, j, j?) (6)
= ?(fj+d, f) ? ?(c, cj,j?)
2. target words within a window around the cur-
rent target position i
he,d,c(fJ1 , eI1, F J1 , EI1 , i, j, j?) (7)
= ?(ei+d, e) ? ?(c, cj,j?)
3. word classes or part-of-speech within a window
around the current source position j
hF,d,c(fJ1 , eI1, F J1 , EI1 , i, j, j?) (8)
= ?(Fj+d, F ) ? ?(c, cj,j?)
4. word classes or part-of-speech within a window
around the current target position i
hE,d,c(fJ1 , eI1, F J1 , EI1 , i, j, j?) (9)
= ?(Ei+d, E) ? ?(c, cj,j?)
Here, ?(?, ?) denotes the Kronecker-function. In the
experiments, we will use d ? {?1, 0, 1}. Many
other feature functions are imaginable, e.g. combi-
nations of the described feature functions, n-gram
or multi-word features, joint source and target lan-
guage feature functions.
4.4 Training
As training criterion, we use the maximum class
posterior probability. This corresponds to maximiz-
ing the likelihood of the maximum entropy model.
Since the optimization criterion is convex, there is
only a single optimum and no convergence problems
occur. To train the model parameters ?N1 , we use the
Generalized Iterative Scaling (GIS) algorithm (Dar-
roch and Ratcliff, 1972).
In practice, the training procedure tends to result
in an overfitted model. To avoid overfitting, (Chen
and Rosenfeld, 1999) have suggested a smoothing
method where a Gaussian prior distribution of the
parameters is assumed.
This method tried to avoid very large lambda val-
ues and prevents features that occur only once for a
specific class from getting a value of infinity.
We train IBM Model 4 with GIZA++ (Och and
Ney, 2003) in both translation directions. Then the
alignments are symmetrized using a refined heuris-
tic as described in (Och and Ney, 2003). This word-
aligned bilingual corpus is used to train the reorder-
ing model parameters, i.e. the feature weights ?N1 .
Each alignment link defines an event for the max-
imum entropy training. An exception are the one-
to-many alignments, i.e. one source word is aligned
to multiple target words. In this case, only the top-
most alignment link is considered because the other
ones cannot occur at a phrase boundary. Many-to-
one and many-to-many alignments are handled in a
similar way.
5 Experimental Results
5.1 Statistics
The experiments were carried out on the Basic
Travel Expression Corpus (BTEC) task (Takezawa
et al, 2002). This is a multilingual speech cor-
pus which contains tourism-related sentences sim-
ilar to those that are found in phrase books. We
use the Arabic-English, the Chinese-English and the
Japanese-English data. The corpus statistics are
shown in Table 1.
As the BTEC is a rather clean corpus, the prepro-
cessing consisted mainly of tokenization, i.e., sep-
arating punctuation marks from words. Addition-
ally, we replaced contractions such as it?s or I?m in
the English corpus and we removed the case infor-
mation. For Arabic, we removed the diacritics and
we split common prefixes: Al, w, f, b, l. There
was no special preprocessing for the Chinese and the
Japanese training corpora.
To train and evaluate the reordering model, we
58
Table 1: Corpus statistics after preprocessing for the BTEC task.
Arabic Chinese Japanese English
Train Sentences 20 000
Running Words 180 075 176 199 198 453 189 927
Vocabulary 15 371 8 687 9 277 6 870
C-Star?03 Sentences 506
Running Words 3 552 3 630 4 130 3 823
Table 2: Statistics of the training and test word align-
ment links.
Ara-Eng Chi-Eng Jap-Eng
Training 144K 140K 119K
Test 16.2K 15.7K 13.2K
use the word aligned bilingual training corpus. For
evaluating the classification power of the reordering
model, we partition the corpus into a training part
and a test part. In our experiments, we use about
10% of the corpus for testing and the remaining
part for training the feature weights of the reorder-
ing model with the GIS algorithm using YASMET
(Och, 2001). The statistics of the training and test
alignment links is shown in Table 2. The number
of training events ranges from 119K for Japanese-
English to 144K for Arabic-English.
The word classes for the class-based features are
trained using the mkcls tool (Och, 1999). In the
experiments, we use 50 word classes. Alternatively,
one could use part-of-speech information for this
purpose.
Additional experiments were carried out on the
large data track of the Chinese-English NIST task.
The corpus statistics of the bilingual training cor-
pus are shown in Table 3. The language model was
trained on the English part of the bilingual train-
ing corpus and additional monolingual English data
from the GigaWord corpus. The total amount of lan-
guage model training data was about 600M running
words. We use a fourgram language model with
modified Kneser-Ney smoothing as implemented in
the SRILM toolkit (Stolcke, 2002). For the four En-
glish reference translations of the evaluation sets, the
accumulated statistics are presented.
Table 3: Chinese-English NIST task: corpus statis-
tics for the bilingual training data and the NIST eval-
uation sets of the years 2002 to 2005.
Chinese English
Train Sentence Pairs 7M
Running Words 199M 213M
Vocabulary Size 223K 351K
Dictionary Entry Pairs 82K
Eval 2002 Sentences 878 3 512
Running Words 25K 105K
2003 Sentences 919 3 676
Running Words 26K 122K
2004 Sentences 1788 7 152
Running Words 52K 245K
2005 Sentences 1082 4 328
Running Words 33K 148K
5.2 Classification Results
In this section, we present the classification results
for the three language pairs. In Table 4, we present
the classification results for two orientation classes.
As baseline we always choose the most frequent
orientation class. For Arabic-English, the baseline
is with 6.3% already very low. This means that the
word order in Arabic is very similar to the word or-
der in English. For Chinese-English, the baseline
is with 12.7% about twice as large. The most dif-
ferences in word order occur for Japanese-English.
This seems to be reasonable as Japanese has usu-
ally a different sentence structure, subject-object-
verb compared to subject-verb-object in English.
For each language pair, we present results for sev-
eral combination of features. The three columns per
language pair indicate if the features are based on the
words (column label ?Words?), on the word classes
(column label ?Classes?) or on both (column label
59
Table 4: Classification error rates [%] using two orientation classes.
Arabic-English Chinese-English Japanese-English
Baseline 6.3 12.7 26.2
Lang. Window Words Classes W+C Words Classes W+C Words Classes W+C
Tgt d = 0 4.7 5.3 4.4 9.3 10.4 8.9 13.6 15.1 13.4
d ? {0, 1} 4.5 5.0 4.3 8.9 9.9 8.6 13.7 14.9 13.4
d ? {?1, 0, 1} 4.5 4.9 4.3 8.6 9.5 8.3 13.5 14.6 13.3
Src d = 0 5.6 5.0 3.9 7.9 8.3 7.2 12.2 11.8 11.0
d ? {0, 1} 3.2 3.0 2.6 4.7 4.7 4.2 10.1 9.7 9.4
d ? {?1, 0, 1} 2.9 2.5 2.3 3.9 3.5 3.3 9.0 8.0 7.8
Src d = 0 4.3 3.9 3.7 7.1 7.8 6.5 10.8 10.9 9.8
+ d ? {0, 1} 2.9 2.6 2.5 4.6 4.5 4.1 9.3 9.1 8.6
Tgt d ? {?1, 0, 1} 2.8 2.1 2.1 3.9 3.4 3.3 8.7 7.7 7.7
?W+C?). We also distinguish if the features depend
on the target sentence (?Tgt?), on the source sentence
(?Src?) or on both (?Src+Tgt?).
For Arabic-English, using features based only on
words of the target sentence the classification er-
ror rate can be reduced to 4.5%. If the features are
based only on the source sentence words, a classifi-
cation error rate of 2.9% is reached. Combining the
features based on source and target sentence words,
a classification error rate of 2.8% can be achieved.
Adding the features based on word classes, the clas-
sification error rate can be further improved to 2.1%.
For the other language pairs, the results are similar
except that the absolute values of the classification
error rates are higher.
We observe the following:
? The features based on the source sentence per-
form better than features based on the target
sentence.
? Combining source and target sentence features
performs best.
? Increasing the window always helps, i.e. addi-
tional context information is useful.
? Often the word-class based features outperform
the word-based features.
? Combining word-based and word-class based
features performs best.
? In general, adding features does not hurt the
performance.
These are desirable properties of an appropriate
reordering model. The main point is that these are
fulfilled not only on the training data, but on unseen
test data. There seems to be no overfitting problem.
In Table 5, we present the results for four orien-
tation classes. The final error rates are a factor 2-4
larger than for two orientation classes. Despite that
we observe the same tendencies as for two orien-
tation classes. Again, using more features always
helps to improve the performance.
5.3 Translation Results
For the translation experiments on the BTEC task,
we report the two accuracy measures BLEU (Pap-
ineni et al, 2002) and NIST (Doddington, 2002) as
well as the two error rates: word error rate (WER)
and position-independent word error rate (PER).
These criteria are computed with respect to 16 refer-
ences.
In Table 6, we show the translation results for
the BTEC task. In these experiments, the reorder-
ing model uses two orientation classes, i.e. it pre-
dicts either a left or a right orientation. The fea-
tures for the maximum-entropy based reordering
model are based on the source and target language
words within a window of one. The word-class
based features are not used for the translation ex-
periments. The maximum-entropy based reordering
model achieves small but consistent improvement
for all the evaluation criteria. Note that the baseline
system, i.e. using the distance-based reordering, was
among the best systems in the IWSLT 2005 evalua-
60
Table 5: Classification error rates [%] using four orientation classes.
Arabic-English Chinese-English Japanese-English
Baseline 31.4 44.9 59.0
Lang. Window Words Classes W+C Words Classes W+C Words Classes W+C
Tgt d = 0 24.5 27.7 24.2 30.0 34.4 29.7 28.9 31.4 28.7
d ? {0, 1} 23.9 27.2 23.7 29.2 32.9 28.9 28.7 30.6 28.3
d ? {?1, 0, 1} 22.1 25.3 21.9 27.6 31.4 27.4 28.3 30.1 28.2
Src d = 0 22.1 23.2 20.4 25.9 27.7 20.4 24.1 24.9 22.3
d ? {0, 1} 11.9 12.0 10.8 14.0 14.9 13.2 18.6 19.5 17.7
d ? {?1, 0, 1} 10.1 8.7 8.0 11.4 11.1 10.5 15.6 15.6 14.5
Src d = 0 20.9 21.8 19.6 24.1 26.8 19.6 22.3 23.4 21.1
+ d ? {0, 1} 11.8 11.5 10.6 13.5 14.5 12.8 18.6 18.8 17.1
Tgt d ? {?1, 0, 1} 9.6 7.7 7.6 11.3 10.1 10.1 15.6 15.2 14.2
Table 6: Translation Results for the BTEC task.
Language Pair Reordering WER [%] PER [%] NIST BLEU [%]
Arabic-English Distance-based 24.1 20.9 10.0 63.8
Max-Ent based 23.6 20.7 10.1 64.8
Chinese-English Distance-based 50.4 43.0 7.67 44.4
Max-Ent based 49.3 42.4 7.36 45.8
Japanese-English Distance-based 32.1 25.2 8.96 56.2
Max-Ent based 31.2 25.2 9.00 56.8
tion campaign (Eck and Hori, 2005).
Some translation examples are presented in Ta-
ble 7. We observe that the system using the
maximum-entropy based reordering model produces
more fluent translations.
Additional translation experiments were carried
out on the large data track of the Chinese-English
NIST task. For this task, we use only the BLEU
and NIST scores. Both scores are computed case-
insensitive with respect to four reference translations
using the mteval-v11b tool1.
For the NIST task, we use the BLEU score as pri-
mary criterion which is optimized on the NIST 2002
evaluation set using the Downhill Simplex algorithm
(Press et al, 2002). Note that only the eight or nine
model scaling factors of Equation 2 are optimized
using the Downhill Simplex algorithm. The feature
weights of the reordering model are trained using
the GIS algorithm as described in Section 4.4. We
use a state-of-the-art baseline system which would
have obtained a good rank in the last NIST evalua-
1http://www.nist.gov/speech/tests/mt/resources/scoring.htm
tion (NIST, 2005).
The translation results for the NIST task are pre-
sented in Table 8. We observe consistent improve-
ments of the BLEU score on all evaluation sets. The
overall improvement due to reordering ranges from
1.2% to 2.0% absolute. The contribution of the
maximum-entropy based reordering model to this
improvement is in the range of 25% to 58%, e.g. for
the NIST 2003 evaluation set about 58% of the im-
provement using reordering can be attributed to the
maximum-entropy based reordering model.
We also measured the classification performance
for the NIST task. The general tendencies are iden-
tical to the BTEC task.
6 Conclusions
We have presented a novel discriminative reorder-
ing model for statistical machine translation. This
model is trained on the word aligned bilingual cor-
pus using the maximum entropy principle. Several
types of features have been used:
? based on the source and target sentence
61
Table 7: Translation examples for the BTEC task.
System Translation
Distance-based I would like to check out time one day before.
Max-Ent based I would like to check out one day before the time.
Reference I would like to check out one day earlier.
Distance-based I hate pepper green.
Max-Ent based I hate the green pepper.
Reference I hate green peppers.
Distance-based Is there a subway map where?
Max-Ent based Where is the subway route map?
Reference Where do they have a subway map?
Table 8: Translation results for several evaluation sets of the Chinese-English NIST task.
Evaluation set 2002 (dev) 2003 2004 2005
Reordering NIST BLEU[%] NIST BLEU[%] NIST BLEU[%] NIST BLEU[%]
None 8.96 33.5 8.67 32.7 8.76 32.0 8.62 30.8
Distance-based 9.19 34.6 8.85 33.2 9.05 33.2 8.79 31.6
Max-Ent based 9.24 35.5 8.87 33.9 9.04 33.6 8.78 32.1
? based on words and word classes
? using local context information
We have evaluated the performance of the re-
ordering model on a held-out word-aligned corpus.
We have shown that the model is able to predict the
orientation very well, e.g. for Arabic-English the
classification error rate is only 2.1%.
We presented improved translation results for
three language pairs on the BTEC task and for the
large data track of the Chinese-English NIST task.
In none of the cases additional features have hurt
the classification performance on the held-out test
corpus. This is a strong evidence that the maximum
entropy framework is suitable for this task.
Another advantage of our approach is the gener-
alization capability via the use of word classes or
part-of-speech information. Furthermore, additional
features can be easily integrated into the maximum
entropy framework.
So far, the word classes were not used for the
translation experiments. As the word classes help
for the classification task, we might expect further
improvements of the translation results. Using part-
of-speech information instead (or in addition) to the
automatically computed word classes might also be
beneficial. More fine-tuning of the reordering model
toward translation quality might also result in im-
provements. As already mentioned in Section 4.3, a
richer feature set could be helpful.
Acknowledgments
This material is partly based upon work supported
by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023,
and was partly funded by the European Union un-
der the integrated project TC-STAR (Technology
and Corpora for Speech to Speech Translation, IST-
2002-FP6-506738, http://www.tc-star.org).
References
A. L. Berger, S. A. Della Pietra, and V. J. Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39?72, March.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79?85,
June.
S. F. Chen and R. Rosenfeld. 1999. A gaussian prior
for smoothing maximum entropy models. Technical
Report CMUCS-99-108, Carnegie Mellon University,
Pittsburgh, PA.
62
J. N. Darroch and D. Ratcliff. 1972. Generalized itera-
tive scaling for log-linear models. Annals of Mathe-
matical Statistics, 43:1470?1480.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proc. ARPA Workshop on Human Language
Technology.
M. Eck and C. Hori. 2005. Overview of the IWSLT 2005
evaluation campaign. In Proc. International Workshop
on Spoken Language Translation (IWSLT), Pittsburgh,
PA, October.
P. Koehn, A. Axelrod, A. B. Mayne, C. Callison-Burch,
M. Osborne, and D. Talbot. 2005. Edinburgh sys-
tem description for the 2005 IWSLT speech translation
evaluation. In Proc. International Workshop on Spo-
ken Language Translation (IWSLT), Pittsburgh, PA,
October.
P. Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models.
In Proc. 6th Conf. of the Assoc. for Machine Transla-
tion in the Americas (AMTA), pages 115?124, Wash-
ington DC, September/October.
S. Kumar and W. Byrne. 2005. Local phrase reorder-
ing models for statistical machine translation. In
Proc. of the Human Language Technology Conf./Conf.
on Empirical Methods in Natural Language Pro-
cessing (HLT/EMNLP), pages 161?168, Vancouver,
Canada, October.
NIST. 2005. NIST 2005 machine
translation evaluation official results.
http://www.nist.gov/speech/tests/mt/
mt05eval official results release
20050801 v3.html, August.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. of the 40th Annual Meeting of
the Assoc. for Computational Linguistics (ACL), pages
295?302, Philadelphia, PA, July.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51, March.
F. J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proc. Joint SIGDAT Conf. on Empirical Methods
in Natural Language Processing and Very Large Cor-
pora, pages 20?28, University of Maryland, College
Park, MD, June.
F. J. Och. 1999. An efficient method for determining
bilingual word classes. In Proc. 9th Conf. of the Europ.
Chapter of the Assoc. for Computational Linguistics
(EACL), pages 71?76, Bergen, Norway, June.
F. J. Och. 2001. YASMET: Toolkit for conditional maxi-
mum entropy models. http://www-i6.informatik.rwth-
aachen.de/web/Software/YASMET.html.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. of the 41th Annual
Meeting of the Assoc. for Computational Linguistics
(ACL), pages 160?167, Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of the 40th Annual Meeting of
the Assoc. for Computational Linguistics (ACL), pages
311?318, Philadelphia, PA, July.
W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P.
Flannery. 2002. Numerical Recipes in C++. Cam-
bridge University Press, Cambridge, UK.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing (ICSLP), volume 2, pages 901?904, Den-
ver, CO.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a broad-coverage bilin-
gual corpus for speech translation of travel conver-
sations in the real world. In Proc. of the Third Int.
Conf. on Language Resources and Evaluation (LREC),
pages 147?152, Las Palmas, Spain, May.
C. Tillmann and T. Zhang. 2005. A localized prediction
model for statistical machine translation. In Proc. of
the 43rd Annual Meeting of the Assoc. for Computa-
tional Linguistics (ACL), pages 557?564, Ann Arbor,
MI, June.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403, September.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proc. Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual
Meeting (HLT-NAACL), pages 257?264, Boston, MA,
May.
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004.
Reordering constraints for phrase-based statistical ma-
chine translation. In Proc. 20th Int. Conf. on Computa-
tional Linguistics (COLING), pages 205?211, Geneva,
Switzerland, August.
R. Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov,
J. Xu, Y. Zhang, and H. Ney. 2005. The RWTH
phrase-based statistical machine translation system. In
Proc. International Workshop on Spoken Language
Translation (IWSLT), pages 155?162, Pittsburgh, PA,
October.
63
Proceedings of the Workshop on Statistical Machine Translation, pages 72?77,
New York City, June 2006. c?2006 Association for Computational Linguistics
N -Gram Posterior Probabilities for Statistical Machine Translation
Richard Zens and Hermann Ney
Human Language Technology and Pattern Recognition
Lehrstuhl fu?r Informatik 6 ? Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
{zens,ney}@cs.rwth-aachen.de
Abstract
Word posterior probabilities are a com-
mon approach for confidence estimation
in automatic speech recognition and ma-
chine translation. We will generalize this
idea and introduce n-gram posterior prob-
abilities and show how these can be used
to improve translation quality. Addition-
ally, we will introduce a sentence length
model based on posterior probabilities.
We will show significant improvements on
the Chinese-English NIST task. The abso-
lute improvements of the BLEU score is
between 1.1% and 1.6%.
1 Introduction
The use of word posterior probabilities is a com-
mon approach for confidence estimation in auto-
matic speech recognition, e.g. see (Wessel, 2002).
This idea has been adopted to estimate confidences
for machine translation, e.g. see (Blatz et al, 2003;
Ueffing et al, 2003; Blatz et al, 2004). These confi-
dence measures were used in the computer assisted
translation (CAT) framework, e.g. (Gandrabur and
Foster, 2003). The (simplified) idea is that the con-
fidence measure is used to decide if the machine-
generated prediction should be suggested to the hu-
man translator or not.
There is only few work on how to improve
machine translation performance using confidence
measures. The only work, we are aware of, is
(Blatz et al, 2003). The outcome was that the con-
fidence measures did not result in improvements of
the translation quality measured with the BLEU and
NIST scores. Here, we focus on how the ideas and
methods commonly used for confidence estimation
can be adapted and/or extended to improve transla-
tion quality.
So far, always word-level posterior probabilities
were used. Here, we will generalize this idea to n-
grams.
In addition to the n-gram posterior probabili-
ties, we introduce a sentence-length model based
on posterior probabilities. The common phrase-
based translation systems, such as (Och et al, 1999;
Koehn, 2004), do not use an explicit sentence length
model. Only the simple word penalty goes into that
direction. It can be adjusted to prefer longer or
shorter translations. Here, we will explicitly model
the sentence length.
The novel contributions of this work are to in-
troduce n-gram posterior probabilities and sentence
length posterior probabilities. Using these methods,
we achieve significant improvements of translation
quality.
The remaining part of this paper is structured as
follows: first, we will briefly describe the baseline
system, which is a state-of-the-art phrase-based sta-
tistical machine translation system. Then, in Sec-
tion 3, we will introduce the n-gram posterior prob-
abilities. In Section 4, we will define the sentence
length model. Afterwards, in Section 5, we will
describe how these novel models can be used for
rescoring/reranking. The experimental results will
be presented in Section 6. Future applications will
be described in Section 7. Finally, we will conclude
in Section 8.
72
2 Baseline System
In statistical machine translation, we are given a
source language sentence fJ1 = f1 . . . fj . . . fJ ,
which is to be translated into a target language sen-
tence eI1 = e1 . . . ei . . . eI . Among all possible tar-
get language sentences, we will choose the sentence
with the highest probability:
e?I?1 = argmax
I,eI1
{
Pr(eI1|fJ1 )
} (1)
The posterior probability Pr(eI1|fJ1 ) is modeled di-
rectly using a log-linear combination of several
models (Och and Ney, 2002):
Pr(eI1|fJ1 ) =
exp
(
?M
m=1 ?mhm(eI1, fJ1 )
)
?
I?,e?I?1
exp
(
?M
m=1 ?mhm(e?I
?
1 , fJ1 )
)
(2)
The denominator is a normalization factor that de-
pends only on the source sentence fJ1 . Therefore,
we can omit it during the search process. As a deci-
sion rule, we obtain:
e?I?1 = argmax
I,eI1
{ M
?
m=1
?mhm(eI1, fJ1 )
}
(3)
This approach is a generalization of the source-
channel approach (Brown et al, 1990). It has the
advantage that additional models h(?) can be eas-
ily integrated into the overall system. The model
scaling factors ?M1 are trained with respect to the fi-
nal translation quality measured by an error criterion
(Och, 2003).
We use a state-of-the-art phrase-based translation
system as described in (Zens and Ney, 2004; Zens
et al, 2005). The baseline system includes the fol-
lowing models: an n-gram language model, a phrase
translation model and a word-based lexicon model.
The latter two models are used for both directions:
p(f |e) and p(e|f). Additionally, we use a word
penalty and a phrase penalty.
3 N-Gram Posterior Probabilities
The idea is similar to the word posterior probabili-
ties: we sum the sentence posterior probabilities for
each occurrence of an n-gram.
Let ?(?, ?) denote the Kronecker function. Then,
we define the fractional count C(en1 , fJ1 ) of an n-
gram en1 for a source sentence fJ1 as:
C(en1 , fJ1 ) =
?
I,e?I1
I?n+1
?
i=1
p(e?I1|fJ1 ) ? ?(e?
i+n?1
i , en1 )
(4)
The sums over the target language sentences are lim-
ited to an N -best list, i.e. the N best translation
candidates according to the baseline model. In this
equation, the term ?(e?i+n?1i , en1 ) is one if and only
if the n-gram en1 occurs in the target sentence e?
I
1
starting at position i.
Then, the posterior probability of an n-gram is ob-
tained as:
p(en1 |fJ1 ) =
C(en1 , fJ1 )
?
e?n1
C(e?n1 , fJ1 )
(5)
Note that the widely used word posterior proba-
bility is obtained as a special case, namely if n is set
to one.
4 Sentence Length Posterior Probability
The common phrase-based translation systems, such
as (Och et al, 1999; Koehn, 2004), do not use an ex-
plicit sentence length model. Only the simple word
penalty goes into that direction. It can be adjusted to
prefer longer or shorter translations.
Here, we will use the posterior probability of a
specific target sentence length I as length model:
p(I|fJ1 ) =
?
eI1
p(eI1|fJ1 ) (6)
Note that the sum is carried out only over target sen-
tences eI1 with the a specific length I . Again, the
candidate target language sentences are limited to an
N -best list.
5 Rescoring/Reranking
A straightforward application of the posterior prob-
abilities is to use them as additional features in
a rescoring/reranking approach (Och et al, 2004).
The use of N -best lists in machine translation has
several advantages. It alleviates the effects of the
huge search space which is represented in word
73
graphs by using a compact excerpt of the N best hy-
potheses generated by the system. N -best lists are
suitable for easily applying several rescoring tech-
niques since the hypotheses are already fully gen-
erated. In comparison, word graph rescoring tech-
niques need specialized tools which can traverse the
graph accordingly.
The n-gram posterior probabilities can be used
similar to an n-gram language model:
hn(fJ1 , eI1) =
1
I log
( I
?
i=1
p(ei|ei?1i?n+1, fJ1 )
)
(7)
with:
p(ei|ei?1i?n+1, fJ1 ) =
C(eii?n+1, fJ1 )
C(ei?1i?n+1, fJ1 )
(8)
Note that the models do not require smoothing as
long as they are applied to the same N -best list they
are trained on.
If the models are used for unseen sentences,
smoothing is important to avoid zero probabilities.
We use a linear interpolation with weights ?n and
the smoothed (n ? 1)-gram model as generalized
distribution.
pn(ei|ei?1i?n+1, fJ1 ) = ?n ?
C(eii?n+1, fJ1 )
C(ei?1i?n+1, fJ1 )
(9)
+(1 ? ?n) ? pn?1(ei|ei?1i?n+2, fJ1 )
Note that absolute discounting techniques that are
often used in language modeling cannot be applied
in a straightforward way, because here we have frac-
tional counts.
The usage of the sentence length posterior prob-
ability for rescoring is even simpler. The resulting
feature is:
hL(fJ1 , eI1) = log p(I|fJ1 ) (10)
Again, the model does not require smoothing as long
as it is applied to the same N -best list it is trained
on. If it is applied to other sentences, smoothing
becomes important. We propose to smooth the sen-
tence length model with a Poisson distribution.
p?(I|fJ1 ) = ??p(I|fJ1 )+(1??)?
?I exp(??)
I! (11)
We use a linear interpolation with weight ?. The
mean ? of the Poisson distribution is chosen to
be identical to the mean of the unsmoothed length
model:
? =
?
I
I ? p(I|fJ1 ) (12)
6 Experimental Results
6.1 Corpus Statistics
The experiments were carried out on the large data
track of the Chinese-English NIST task. The cor-
pus statistics of the bilingual training corpus are
shown in Table 1. The language model was trained
on the English part of the bilingual training cor-
pus and additional monolingual English data from
the GigaWord corpus. The total amount of lan-
guage model training data was about 600M running
words. We use a fourgram language model with
modified Kneser-Ney smoothing as implemented in
the SRILM toolkit (Stolcke, 2002).
To measure the translation quality, we use the
BLEU score (Papineni et al, 2002) and the NIST
score (Doddington, 2002). The BLEU score is the
geometric mean of the n-gram precision in com-
bination with a brevity penalty for too short sen-
tences. The NIST score is the arithmetic mean of
a weighted n-gram precision in combination with a
brevity penalty for too short sentences. Both scores
are computed case-sensitive with respect to four ref-
erence translations using the mteval-v11b tool1. As
the BLEU and NIST scores measure accuracy higher
scores are better.
We use the BLEU score as primary criterion
which is optimized on the development set using the
Downhill Simplex algorithm (Press et al, 2002). As
development set, we use the NIST 2002 evaluation
set. Note that the baseline system is already well-
tuned and would have obtained a high rank in the
last NIST evaluation (NIST, 2005).
6.2 Translation Results
The translation results for the Chinese-English NIST
task are presented in Table 2. We carried out experi-
ments for evaluation sets of several years. For these
rescoring experiments, we use the 10 000 best trans-
lation candidates, i.e. N -best lists of size N=10 000.
1http://www.nist.gov/speech/tests/mt/resources/scoring.htm
74
Table 1: Chinese-English NIST task: corpus statis-
tics for the bilingual training data and the NIST eval-
uation sets of the years 2002 to 2005.
Chinese English
Train Sentence Pairs 7M
Running Words 199M 213M
Vocabulary Size 223K 351K
Dictionary Entry Pairs 82K
Eval 2002 Sentences 878 3 512
Running Words 25K 105K
2003 Sentences 919 3 676
Running Words 26K 122K
2004 Sentences 1788 7 152
Running Words 52K 245K
2005 Sentences 1082 4 328
Running Words 33K 148K
Using the 1-gram posterior probabilities, i.e. the
conventional word posterior probabilities, there is
only a very small improvement, or no improvement
at all. This is consistent with the findings of the
JHU workshop on confidence estimation for statis-
tical machine translation 2003 (Blatz et al, 2003),
where the word-level confidence measures also did
not help to improve the BLEU or NIST scores.
Successively adding higher order n-gram poste-
rior probabilities, the translation quality improves
consistently across all evaluation sets. We also
performed experiments with n-gram orders beyond
four, but these did not result in further improve-
ments.
Adding the sentence length posterior probability
feature is also helpful for all evaluation sets. For the
development set, the overall improvement is 1.5%
for the BLEU score. On the blind evaluation sets,
the overall improvement of the translation quality
ranges between 1.1% and 1.6% BLEU.
Some translation examples are shown in Table 3.
7 Future Applications
We have shown that the n-gram posterior probabil-
ities are very useful in a rescoring/reranking frame-
work. In addition, there are several other potential
applications. In this section, we will describe two of
them.
7.1 Iterative Search
The n-gram posterior probability can be used for
rescoring as described in Section 5. An alternative is
to use them directly during the search. In this second
search pass, we use the models from the first pass,
i.e. the baseline system, and additionally the n-gram
and sentence length posterior probabilities. As the
n-gram posterior probabilities are basically a kind
of sentence-specific language model, it is straight-
forward to integrate them. This process can also be
iterated. Thus, using the N -best list of the second
pass to recompute the n-gram and sentence length
posterior probabilities and do a third search pass,
etc..
7.2 Computer Assisted Translation
In the computer assisted translation (CAT) frame-
work, the goal is to improve the productivity of hu-
man translators. The machine translation system
takes not only the current source language sentence
but also the already typed partial translation into ac-
count. Based on this information, the system suggest
completions of the sentence. Word-level posterior
probabilities have been used to select the most ap-
propriate completion of the system, for more details
see e.g. (Gandrabur and Foster, 2003; Ueffing and
Ney, 2005). The n-gram based posterior probabili-
ties as described in this work, might be better suited
for this task as they explicitly model the dependency
on the previous words, i.e. the given prefix.
8 Conclusions
We introduced n-gram and sentence length poste-
rior probabilities and demonstrated their usefulness
for rescoring purposes. We performed systematic
experiments on the Chinese-English NIST task and
showed significant improvements of the translation
quality. The improvements were consistent among
several evaluation sets.
An interesting property of the introduced meth-
ods is that they do not require additional knowledge
sources. Thus the given knowledge sources are bet-
ter exploited. Our intuition is that the posterior mod-
els prefer hypotheses with n-grams that are common
in the N -best list.
The achieved results are promising. Despite that,
there are several ways to improve the approach.
75
Table 2: Case-sensitive translation results for several evaluation sets of the Chinese-English NIST task.
Evaluation set 2002 (dev) 2003 2004 2005
System NIST BLEU[%] NIST BLEU[%] NIST BLEU[%] NIST BLEU[%]
Baseline 8.49 30.5 8.04 29.5 8.14 29.0 8.01 28.2
+ 1-grams 8.51 30.5 8.08 29.5 8.17 29.0 8.03 28.2
+ 2-grams 8.47 30.8 8.03 29.7 8.12 29.2 7.98 28.1
+ 3-grams 8.73 31.6 8.25 30.1 8.45 30.0 8.20 28.6
+ 4-grams 8.74 31.7 8.26 30.1 8.47 30.1 8.20 28.6
+ length 8.87 32.0 8.42 30.9 8.60 30.6 8.34 29.3
Table 3: Translation examples for the Chinese-English NIST task.
Baseline At present, there is no organization claimed the attack.
Rescored At present, there is no organization claimed responsibility for the attack.
Reference So far, no organization whatsoever has claimed responsibility for the attack.
Baseline FIFA to severely punish football fraud
Rescored The International Football Federation (FIFA) will severely punish football?s deception
Reference FIFA will severely punish all cheating acts in the football field
Baseline In more than three months of unrest, a total of more than 60 dead and 2000 injured.
Rescored In more than three months of unrest, a total of more than 60 people were killed and more
than 2000 injured.
Reference During the unrest that lasted more than three months, a total of more than 60 people died
and over 2,000 were wounded.
For the decision rule in Equation 3, the model
scaling factors ?M1 can be multiplied with a constant
factor without changing the result. This global fac-
tor would affect the proposed posterior probabilities.
So far, we have not tuned this parameter, but a proper
adjustment might result in further improvements.
Currently, the posterior probabilities are com-
puted on an N -best list. Using word graphs instead
should result in more reliable estimates, as the num-
ber of hypotheses in a word graph is some orders of
a magnitude larger than in an N -best list.
Acknowledgments
This material is partly based upon work supported
by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023,
and was partly funded by the European Union un-
der the integrated project TC-STAR (Technology
and Corpora for Speech to Speech Translation, IST-
2002-FP6-506738, http://www.tc-star.org).
References
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur,
C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.
2003. Confidence estimation for machine transla-
tion. Final report, JHU/CLSP Summer Workshop.
http://www.clsp.jhu.edu/ws2003/groups/
estimate/.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte,
A. Kulesza, A. Sanchis, and N. Ueffing. 2004. Con-
fidence estimation for machine translation. In Proc.
20th Int. Conf. on Computational Linguistics (COL-
ING), pages 315?321, Geneva, Switzerland, August.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79?85,
June.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proc. ARPA Workshop on Human Language
Technology.
S. Gandrabur and G. Foster. 2003. Confidence estima-
tion for text prediction. In Proc. Conf. on Natural Lan-
76
guage Learning (CoNLL), pages 95?102, Edmonton,
Canada, May.
P. Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models.
In 6th Conf. of the Association for Machine Trans-
lation in the Americas (AMTA 04), pages 115?124,
Washington DC, September/October.
NIST. 2005. NIST 2005 machine
translation evaluation official results.
http://www.nist.gov/speech/tests/mt/
mt05eval official results release
20050801 v3.html, August.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 295?302, Philadelphia, PA, July.
F. J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proc. Joint SIGDAT Conf. on Empirical Methods
in Natural Language Processing and Very Large Cor-
pora, pages 20?28, University of Maryland, College
Park, MD, June.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord
of features for statistical machine translation. In Proc.
Human Language Technology Conf. / North American
Chapter of the Association for Computational Linguis-
tics Annual Meeting (HLT-NAACL), pages 161?168,
Boston,MA.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. of the 41th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 160?167, Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 311?318, Philadelphia, PA, July.
W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P.
Flannery. 2002. Numerical Recipes in C++. Cam-
bridge University Press, Cambridge, UK.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Speech and Lan-
guage Processing (ICSLP), volume 2, pages 901?904,
Denver, CO, September.
N. Ueffing and H. Ney. 2005. Application of word-
level confidence measures in interactive statistical ma-
chine translation. In Proc. of the 10th Annual Conf. of
the European Association for Machine Translation
(EAMT), pages 262?270, Budapest, Hungary, May.
N. Ueffing, K. Macherey, and H. Ney. 2003. Confi-
dence Measures for Statistical Machine Translation.
In Proc. MT Summit IX, pages 394?401, New Orleans,
LA, September.
F. Wessel. 2002. Word Posterior Probabilities for Large
Vocabulary Continuous Speech Recognition. Ph.D.
thesis, RWTH Aachen University, Aachen, Germany,
January.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proc. Human
Language Technology Conf. / North American Chapter
of the Association for Computational Linguistics An-
nual Meeting (HLT-NAACL), pages 257?264, Boston,
MA, May.
R. Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov,
J. Xu, Y. Zhang, and H. Ney. 2005. The RWTH
phrase-based statistical machine translation system. In
Proceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 155?162, Pitts-
burgh, PA, October.
77
Proceedings of the Workshop on Statistical Machine Translation, pages 78?85,
New York City, June 2006. c?2006 Association for Computational Linguistics
Partitioning Parallel Documents Using Binary Segmentation
Jia Xu and Richard Zens and Hermann Ney
Chair of Computer Science 6
Computer Science Department
RWTH Aachen University
D-52056 Aachen Germany
{xujia,zens,ney}@cs.rwth-aachen.de
Abstract
In statistical machine translation, large
numbers of parallel sentences are required
to train the model parameters. However,
plenty of the bilingual language resources
available on web are aligned only at the
document level. To exploit this data,
we have to extract the bilingual sentences
from these documents.
The common method is to break the doc-
uments into segments using predefined
anchor words, then these segments are
aligned. This approach is not error free,
incorrect alignments may decrease the
translation quality.
We present an alternative approach to ex-
tract the parallel sentences by partitioning
a bilingual document into two pairs. This
process is performed recursively until all
the sub-pairs are short enough.
In experiments on the Chinese-English
FBIS data, our method was capable of
producing translation results comparable
to those of a state-of-the-art sentence
aligner. Using a combination of the two
approaches leads to better translation per-
formance.
1 Introduction
Current statistical machine translation systems use
bilingual sentences to train the parameters of the
translation models. The exploitation of more bilin-
gual sentences automatically and accurately as well
as the use of these data with the limited computa-
tional requirements become crucial problems.
The conventional method for producing parallel
sentences is to break the documents into sentences
and to align these sentences using dynamic program-
ming. Previous investigations can be found in works
such as (Gale and Church, 1993) and (Ma, 2006).
A disadvantage is that only the monotone sentence
alignments are allowed.
Another approach is the binary segmentation
method described in (Simard and Langlais, 2003),
(Xu et al, 2005) and (Deng et al, 2006), which
separates a long sentence pair into two sub-pairs re-
cursively. The binary reordering in alignment is al-
lowed but the segmentation decision is only opti-
mum in each recursion step.
Hence, a combination of both methods is ex-
pected to produce a more satisfying result. (Deng
et al, 2006) performs a two-stage procedure. The
documents are first aligned at level using dynamic
programming, the initial alignments are then refined
to produce shorter segments using binary segmen-
tation. But on the Chinese-English FBIS training
corpus, the alignment accuracy and recall are lower
than with Champollion (Ma, 2006).
We refine the model in (Xu et al, 2005) using
a log-linar combination of different feature func-
tions and combine it with the approach of (Ma,
2006). Here the corpora produced using both ap-
proaches are concatenated, and each corpus is as-
signed a weight. During the training of the word
alignment models, the counts of the lexicon entries
78
are linear interpolated using the corpus weights. In
the experiments on the Chinese-English FBIS cor-
pus the translation performance is improved by 0.4%
of the BLEU score compared to the performance
only with Champollion.
The remainder of this paper is structured as fol-
lows: First we will briefly review the baseline statis-
tical machine translation system in Section 2. Then,
in Section 3, we will describe the refined binary seg-
mentation method. In Section 4.1, we will introduce
the methods to extract bilingual sentences from doc-
ument aligned texts. The experimental results will
be presented in Section 4.
2 Review of the Baseline Statistical
Machine Translation System
In this section, we briefly review our translation sys-
tem and introduce the word alignment models.
In statistical machine translation, we are given
a source language sentence fJ1 = f1 . . . fj . . . fJ ,
which is to be translated into a target language sen-
tence eI1 = e1 . . . ei . . . eI . Among all possible tar-
get language sentences, we will choose the sentence
with the highest probability:
e?I?1 = argmax
I,eI1
{Pr(eI1|fJ1 )
}
= argmax
I,eI1
{Pr(eI1) ? Pr(fJ1 |eI1)
} (1)
The decomposition into two knowledge sources in
Equation 1 allows independent modeling of tar-
get language model Pr(eI1) and translation model
Pr(fJ1 |eI1)1. The translation model can be further
extended to a statistical alignment model with the
following equation:
Pr(fJ1 |eI1) =
?
aJ1
Pr(fJ1 , aJ1 |eI1)
The alignment model Pr(fJ1 , aJ1 |eI1) introduces a
?hidden? word alignment a = aJ1 , which describes a
mapping from a source position j to a target position
aj .
1The notational convention will be as follows: we use the
symbol Pr(?) to denote general probability distributions with
(nearly) no specific assumptions. In contrast, for model-based
probability distributions, we use the generic symbol p(?).
Monotone Non-
monotone
Target B A
Positions C D
Source Positions
Figure 1: Two Types of Alignment
The IBM model 1 (IBM-1) (Brown et al, 1993)
assumes that all alignments have the same probabil-
ity by using a uniform distribution:
p(fJ1 |eI1) =
1
IJ ?
J?
j=1
I?
i=1
p(fj |ei) (2)
We use the IBM-1 to train the lexicon parameters
p(f |e), the training software is GIZA++ (Och and
Ney, 2003).
To incorporate the context into the translation
model, the phrase-based translation approach (Zens
et al, 2005) is applied. Pairs of source and tar-
get language phrases are extracted from the bilin-
gual training corpus and a beam search algorithm is
implemented to generate the translation hypothesis
with maximum probability.
3 Binary Segmentation Method
3.1 Approach
Here a document or sentence pair (fJ1 , eI1) 2 is repre-
sented as a matrix. Every element in the matrix con-
tains a lexicon probability p(fj |ei), which is trained
on the original parallel corpora. Each position di-
vides a matrix into four parts as shown in Figure 1:
the bottom left (C), the upper left (A), the bottom
right (D) and the upper right (B). We use m to de-
note the alignment direction, m = 1 means that the
alignment is monotone, i.e. the bottom left part is
connected with the upper right part, and m = 0
means the alignment is non-monotone, i.e. the upper
left part is connected with the bottom right part, as
shown in Figure 1.
3.2 Log-Linear Model
We use a log-linear interpolation to combine differ-
ent models: the IBM-1, the inverse IBM-1, the an-
2Sentences are equivalent to segments in this paper.
79
chor words model as well as the IBM-4. K denotes
the total number of models.
We go through all positions in the bilingual sen-
tences and find the best position for segmenting the
sentence:
(?i, j?, m?) = argmax
i,j,m
{ K?
k=1
?khk(j, i,m|fJ1 , eI1)
}
,
where i ? [1, I ? 1] and j ? [1, J ? 1] are posi-
tions in the source and target sentences respectively.
The feature functions are described in the follow-
ing sections. In most cases, the sentence pairs are
quite long and even after one segmentation we may
still have long sub-segments. Therefore, we separate
the sub-segment pairs recursively until the length of
each new segment is less than a defined value.
3.3 Normalized IBM-1
The function in Equation 2 can be normalized by
the source sentence length with a weighting ? as de-
scribed in (Xu et al, 2005):
The monotone alignment is calculated as
h1(j, i, 1|fJ1 , eI1) = log(p(f j1 |ei1)??
1
j+(1??) (3)
?p(fJj+1|eIi+1)??
1
J?j+(1??)),
and the non-monotone alignment is formulated in
the same way.
We also use the inverse IBM-1 as a feature, by ex-
changing the place of ei1 and f j1 its monotone align-
ment is calculated as:
h2(j, i, 1|fJ1 , eI1) = log(p(ei1|f j1 )??
1
i+(1??) (4)
?p(eIi+1|fJj+1)??
1
I?i+(1??))
3.4 Anchor Words
In the task of extracting parallel sentences from
the paragraph-aligned corpus, selecting some anchor
words as preferred segmentation positions can ef-
fectively avoid the extraction of incomplete segment
pairs. Therefore we use an anchor words model to
prefer the segmentation at the punctuation marks,
where the source and target words are identical:
h3(j, i,m|fJ1 , eI1) =
{ 1 : fj = ei ? ei ? A
0 : otherwise
A is a user defined anchor word list, here we use
A={.,??;}. If the corresponding model scaling factor
?3 is assigned a high value, the segmentation posi-
tions are mostly after anchor words.
3.5 IBM-4 Word Alignment
If we already have the IBM-4 Viterbi word align-
ments for the parallel sentences and need to retrain
the system, for example to optimize the training pa-
rameters, we can include the Viterbi word align-
ments trained on the original corpora into the binary
segmentation. In the monotone case, the model is
represented as
h4(j, i, 1|fJ1 , eI1) =
log
(
N(f j1 , ei1) +N(fJj+1, eIi+1)
N(fJ1 , eI1)
)
,
where N(f j1 , ei1) denotes the number of the align-
ment links inside the matrix (1, 1) and (j, i). In the
non-monotone case the model is formulated in the
same way.
3.6 Word Alignment Concatenation
As described in Section 2, our translation is based on
phrases, that means for an input sentence we extract
all phrases matched in the training corpus and trans-
late with these phrase pairs. Although the aim of
segmentation is to split parallel text into translated
segment pairs, but the segmentation is still not per-
fect. During sentence segmentation we might sep-
arate a phrase into two segments, so that the whole
phrase pair can not be extracted.
To avoid this, we concatenate the word align-
ments trained with the segmentations of one sen-
tence pair. During the segmentation, the position of
each segmentation point in the sentence is memo-
rized. After training the word alignment model with
the segmented sentence pairs, the word alignments
are concatenated again according to the positions of
their segments in the sentences. The original sen-
tence pairs and the concatenated alignments are then
used for the phrase extraction.
80
Table 1: Corpus Statistics: NIST
Chinese English
Train Sentences 8.64 M
Running Words 210 M 226 M
Average Sentence Length 24.4 26.3
Vocabulary 224 268 359 623
Singletons 98 842 156 493
Segmentation Sentences 17.9 M
Running Words 210 M 226 M
Average Sentence Length 11.7 12.6
Vocabulary 221 517 353 148
Singletons 97 062 152 965
Segmentation with Additional Data Sentences 19.5 M
Running Words 230 M 248 M
Added Running Words 8.0% 8.2%
Evaluation Sentences 878 3 512
Running Words 24 111 105 516
Vocabulary 4 095 6 802
OOVs (Running Words) 8 658
4 Translation Experiments
4.1 Bilingual Sentences Extraction Methods
In this section, we describe the different methods to
extract the bilingual sentence pairs from the docu-
ment aligned corpus.
Given each document pair, we assume that the
paragraphs are aligned one to one monotone if both
the source and target language documents contain
the same number of paragraphs; otherwise the para-
graphs are aligned with the Champollion tool.
Starting from the parallel paragraphs we extract
the sentences using three methods:
1. Binary segmentation
The segmentation method described in Sec-
tion 3 is applied by treating the paragraph pairs
as long sentence pairs. We can use the anchor
words model described in Section 3.4 to prefer
splitting at punctuation marks.
The lexicon parameters p(f |e) in Equation 2
are estimated as follows: First the sentences are
aligned roughly using the dynamic program-
ming algorithm. Training on these aligned sen-
tences, we get the initial lexicon parameters.
Then the binary segmentation algorithm is ap-
plied to extract the sentences again.
2. Champollion
After a paragraph is divided into sentences at
punctuation marks, the Champollion tool (Ma,
2006) is used, which applies dynamic program-
ming for the sentence alignment.
3. Combination
The bilingual corpora produced by the binary
segmentation and Champollion methods are
concatenated and are used in the training of the
translation model. Each corpus is assigned a
weight. During the training of the word align-
ment models, the counts of the lexicon en-
tries are linearly interpolated using the corpus
weights.
4.2 Translation Tasks
We will present the translation results on two
Chinese-English tasks.
1. On the large data track NIST task (NIST,
2005), we will show improvements using the
refined binary segmentation method.
81
Table 2: Corpus Statistics: FBIS
Segmentation Champollion
Chinese English Chinese English
Train Sentences 739 899 177 798
Running Words 8 588 477 10 111 752 7 659 776 9 801 257
Average Sentence Length 11.6 13.7 43.1 55.1
Vocabulary 34 896 56 573 34 377 55 775
Singletons 4 775 19 283 4 588 19 004
Evaluation Sentences 878 3 513 878 3 513
Running Words 24 111 105 516 24 111 105 516
Vocabulary 4 095 6 802 4 095 6 802
OOVs (Running Words) 109 2 257 119 2 309
2. On the FBIS corpus, we will compare the dif-
ferent sentence extraction methods described in
Section 4.1 with respect to translation perfor-
mance. We do not apply the extraction meth-
ods on the whole NIST corpora, because some
corpora provided by the LDC (LDC, 2005) are
sentence aligned but not document aligned.
4.3 Corpus Statistics
The training corpora used in NIST task are a set of
individual corpora including the FBIS corpus. These
corpora are provided by the Linguistic Data Consor-
tium (LDC, 2005), the domains are news articles.
The translation experiments are carried out on the
NIST 2002 evaluation set.
As shown in Table 1, there are 8.6 million sen-
tence pairs in the original corpora of the NIST task.
The average sentence length is about 25. After seg-
mentation, there are twice as many sentence pairs,
i.e. 17.9 million, and the average sentence length
is around 12. Due to a limitation of GIZA++, sen-
tences consisting of more than one hundred words
are filtered out. Segmentation of long sentences cir-
cumvents this restriction and allows us include more
data. Here we were able to add 8% more Chinese
and 8.2% more English running words to the train-
ing data. The training time is also reduced.
Table 2 presents statistics of the FBIS data. Af-
ter the paragraph alignment described in Section 4.1
we have nearly 81 thousand paragraphs, 8.6 million
Chinese and 10.1 million English running words.
One of the advantages of the binary segmentation is
that we do not loose words during the bilingual sen-
tences extraction. However, we produce sentence
pairs with very different lengths. Using Champol-
lion we loose 10.8% of the Chinese and 3.1% of the
English words.
4.4 Segmentation Parameters
We did not optimize the log-linear model scaling
factors for the binary segmentation but used the fol-
lowing fixed values: ?1 = ?2 = 0.5 for the IBM-1
models in both directions; ?3 = 108, if the anchor
words model is is used; ?4 = 30, if the IBM-4 model
is used. The maximum sentence length is 25.
4.5 Evaluation Criteria
We use four different criteria to evaluate the transla-
tion results automatically:
? WER (word error rate):
The WER is computed as the minimum num-
ber of substitution, insertion and deletion oper-
ations that have to be performed to convert the
generated sentence into the reference sentence,
divided by the reference sentence length.
? PER (position-independent word error rate):
A shortcoming of the WER is that it requires a
perfect word order. The word order of an ac-
ceptable sentence can be differ from that of the
target sentence, so that the WER measure alone
could be misleading. The PER compares the
words in the two sentences ignoring the word
order.
? BLEU score:
This score measures the precision of unigrams,
82
0 0.2 0.4 0.6 0.8 131.8
31.9
32
32.1
32.2
Weight for the Binary Segmentation
BLE
U[%
]
Figure 2: Translation performance as a function of
the weight for the binary segmentation ? ( weight
for Champollion: 1? ? )
bigrams, trigrams and fourgrams with a penalty
for too short sentences. (Papineni et al, 2002).
? NIST score:
This score is similar to BLEU, but it uses
an arithmetic average of N-gram counts rather
than a geometric average, and it weights more
heavily those N-grams that are more informa-
tive. (Doddington, 2002).
The BLEU and NIST scores measure accuracy,
i.e. larger scores are better. In our evaluation the
scores are measured as case insensitive and with re-
spect to multiple references.
4.6 Translation Results
For the segmentation of long sentences into short
segments, we performed the experiments on the
NIST task. Both in the baseline and the segmenta-
tion systems we obtain 4.7 million bilingual phrases
during the translation. The method of alignment
concatenation increases the number of the extracted
bilingual phrase pairs from 4.7 million to 4.9 mil-
lion, the BLEU score is improved by 0.1%. By
including the IBM-4 Viterbi word alignment, the
NIST score is improved. The training of the base-
line system requires 5.9 days, after the sentence seg-
mentation it requires only 1.5 days. Moreover, the
segmentation allows the inclusion of long sentences
that are filtered out in the baseline system. Using
the added data, the translation performance is en-
hanced by 0.3% in the BLEU score. Because of
the long translation period, the translation parame-
ters are only optimized on the baseline system with
respect to the BLEU score, we could expect a further
improvement if the parameters were also optimized
on the segmentation system.
Our major objective here is to introduce another
approach to parallel sentence extraction: binary seg-
mentation of the bilingual texts recursively. We use
the paragraph-aligned corpus as a starting point. Ta-
ble 4 presents the translation results on the train-
ing corpora generated by the different methods de-
scribed in Section 4.1. The translation parameters
are optimized with the respect to the BLEU score.
We observe that the binary segmentation methods
are comparable to Champollion and the segmenta-
tion with anchors outperforms the one without an-
chors. By combining the methods of Champol-
lion and the binary segmentation with anchors, the
BLEU score is improved by 0.4% absolutely.
We optimized the weightings for the binary seg-
mentation method, the sum of the weightings for
both methods is one. As shown in Figure 2, using
one of the methods alone does not produce the best
result. The maximum BLEU score is attained when
both methods are combined with equal weightings.
5 Discussion and Future Work
We successfully applied the binary sentence seg-
mentation method to extract bilingual sentence pairs
from the document aligned texts. The experiments
on the FBIS data show an enhancement of 0.4% of
the BLEU score compared to the score obtained us-
ing a state-of-art sentence aligner. In addition to the
encouraging results obtained, further improvements
could be achieved in the following ways:
1. By extracting bilingual paragraphs from the
documents, we lost running words using Cham-
pollion. Applying the segmentation approach
to paragraph alignment might avoid the loss of
this data.
2. We combined a number of different models in
the binary segmentation, such as IBM-1, and
anchor words. The model weightings could be
optimized with respect to translation quality.
83
Table 3: Translation Results using Refined Segmentation Methods on NIST task
Error Rate[%] Accuracy
WER PER NIST BLEU[%]
Baseline 62.7 42.1 8.95 33.5
Segmentation 62.6 42.4 8.80 33.5
Segmentation + concatenation 62.4 42.3 8.84 33.6
Segmentation + concatenation + IBM-4 62.8 42.4 8.91 33.6
Segmentation + added data 62.9 42.5 9.00 33.9
Table 4: Translation Results on Sentence Alignment Task with FBIS Training Corpus
Error Rate[%] Accuracy
WER PER NIST BLEU[%]
Champollion 64.2 43.7 8.61 31.8
Segmentation without Anchors 64.3 44.4 8.57 31.8
Segmentation with Anchors 64.0 43.9 8.58 31.9
Champollion + Segmentation with Anchors 64.3 44.2 8.57 32.2
3. In the binary segmentation method, an incor-
rect segmentation results in further mistakes
in the segmentation decisions of all its sub-
segments. An alternative method (Wu, 1997)
makes decisions at the end but has a high com-
putational requirement. A restricted expansion
of the search space might better balance seg-
mentation accuracy and the efficiency.
6 Acknowledgments
This work was supported by the European Union
under the integrated project TC-Star (Technology
and Corpora for Speech to Speech Translation,
IST-2002-FP6-506738, http://www.tc-star.org) and
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311, June.
Y. Deng, S. Kumar, and W. Byrne. 2006. Segmenta-
tion and alignment of parallel text for statistical ma-
chine translation. Natural Language Engineering, Ac-
cepted. To appear.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proceedings of Human Language Technology,
pages 128?132, San Diego, California, March.
W. A. Gale and K. W. Church. 1993. A program for
aligning sentences in bilingual corpora. Computa-
tional Linguistics, 19(1):75?90.
LDC. 2005. Linguistic data consortium resource home
page. http://www.ldc.upenn.edu/Projects/TIDES.
X. Ma. 2006. Champollion: A robust parallel text
sentence aligner. In Proceedings of the fifth interna-
tional conference on Language Resources and Evalu-
ation (LREC), Genoa, Italy, Accepted. To appear.
NIST. 2005. Machine translation home page.
http://www.nist.gov/speech/tests/mt/index.htm.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51, March.
K. A. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318, Philadelphia, July.
M. Simard and P. Langlais. 2003. Statistical transla-
tion alignment with compositionality constraints. In
NAACL 2003 Workshop on Building and Using Paral-
lel Texts: Data Driven Machine Translation and Be-
yond, Edmonton, Canada, May.
84
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403, September.
J. Xu, R. Zens, and H. Ney. 2005. Sentence segmentation
using IBM word alignment model 1. In Proceedings of
EAMT 2005 (10th Annual Conference of the European
Association for Machine Translation), pages 280?287,
Budapest, Hungary, May.
R. Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov,
J. Xu, Y. Zhang, and H. Ney. 2005. The RWTH
phrase-based statistical machine translation system. In
Proceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 155?162, Pitts-
burgh, PA, October.
85
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 1?8,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Chunk-Level Reordering of Source Language Sentences with
Automatically Learned Rules for Statistical Machine Translation
Yuqi Zhang and Richard Zens and Hermann Ney
Human Language Technology and Pattern Recognition
Lehrstuhl fu?r Informatik 6 ? Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
{yzhang,zens,ney}@cs.rwth-aachen.de
Abstract
In this paper, we describe a source-
side reordering method based on syntac-
tic chunks for phrase-based statistical ma-
chine translation. First, we shallow parse
the source language sentences. Then, re-
ordering rules are automatically learned
from source-side chunks and word align-
ments. During translation, the rules are
used to generate a reordering lattice for
each sentence. Experimental results are
reported for a Chinese-to-English task,
showing an improvement of 0.5%?1.8%
BLEU score absolute on various test sets
and better computational efficiency than
reordering during decoding. The exper-
iments also show that the reordering at
the chunk-level performs better than at the
POS-level.
1 Introduction
In machine translation, reordering is one of the ma-
jor problems, since different languages have differ-
ent word order requirements. Many reordering con-
straints have been used for word reorderings, such
as ITG constraints (Wu, 1996), IBM constraints
(Berger et al, 1996) and local constraints (Kanthak
et al, 2005). These approaches do not make use of
any linguistic knowledge.
Several methods have been proposed to use syn-
tactic information to handle the reordering problem,
e.g. (Wu, 1997; Yamada and Knight, 2001; Gildea,
2003; Melamed, 2004; Graehl and Knight, 2004;
Galley et al, 2006). One approach makes use of
bitext grammars to parse both the source and tar-
get languages. Another approach makes use of syn-
tactic information only in the target language. Note
that these models have radically different structures
and parameterizations than phrase-based models for
SMT.
Another kind of approaches is to use syntactic in-
formation in rescoring methods. (Koehn and Knight,
2003) apply a reranking approach to the sub-task
of noun-phrase translation. (Och et al, 2004) and
(Shen et al, 2004) describe the use of syntactic fea-
tures in reranking the output of a full translation sys-
tem, but the syntactic features give very small gains.
In this paper, we present a strategy to reorder
a source sentence using rules based on syntactic
chunks. It is possible to integrate reordering rules di-
rectly into the search process, but here, we consider
a more modular approach: easy to exchange reorder-
ing strategy. To avoid hard decisions before SMT,
we generate a source-reordering lattice instead of a
single reordered source sentence as input to the SMT
system. Then, the decoder uses the reordered source
language model as an additional feature function. A
language model trained on the reordered source-side
chunks gives a score for each path in the lattice. The
novel ideas in this paper are:
? reordering of the source sentence at the chunk
level,
? representing linguistic chunks-reorderings in a
lattice.
1
The rest of this paper is organized as follows. Sec-
tion 2 presents a review of related work. In Sec-
tions 3, we review the phrase-based translation sys-
tem used in this work and propose the framework
of the new reordering method. In Section 4, we in-
troduce the details of the reordering rules, how they
are defined and how to extract them. In Section 5,
we explain how to apply the rules and how to gen-
erate reordering lattice. In Section 6, we present
some results that show that the chunk-level source
reordering is helpful for phrase-based statistical ma-
chine translation. Finally, we conclude this paper
and discuss future work in Section 7.
2 Related Work
Beside the reordering methods during decoding, an
alternative approach is to reorder the input source
sentence to match the word order of the target sen-
tence.
Some reordering methods are carried out on syn-
tactic source trees. (Collins et al, 2005) describe
a method for reordering German for German-to-
English translation, where six transformations are
applied to the surface string of the parsed source
sentence. (Xia and McCord, 2004) propose an ap-
proach for translation from French-to-English. This
approach automatically extracts rewrite patterns by
parsing the source and target sides of the training
corpus. These rewrite patterns can be applied to any
input source sentence so that the rewritten source
and target sentences have similar word order. Both
methods need a parser to generate trees of source
sentences and are applied only as a preprocessing
step.
Another kind of source reordering methods be-
sides full parsing is based on Part-Of-Speech (POS)
tags or word classes. (Costa-jussa` and Fonollosa,
2006) view the source reordering as a translation
task that translate the source language into a re-
ordered source language. Then, the reordered source
sentence is taken as the single input to the standard
SMT system.
(Chen et al, 2006) automatically extract rules
from word alignments. These rules are defined at
the POS level and the scores of matching rules are
used as additional feature functions during rescor-
ing. (Crego and Marin?o, 2006) integrate source-side
reordering into SMT decoding. They automatically
learn rewrite patterns from word alignment and rep-
resent the patterns with POS tags. To our knowledge
no work is reported on the reordering with shallow
parsing.
Decoding lattices were already used in (Zens et
al., 2002; Kanthak et al, 2005). Those approaches
used linguistically uninformed word-level reorder-
ings.
3 System Overview
In this section, we will describe the phrase-based
SMT system which we use for the experiments.
Then, we will give an outline of the extentions with
the chunk-level source reordering model.
3.1 The Baseline Phrase-based SMT System
In statistical machine translation, we are given a
source language sentence fJ1 = f1 . . . fj . . . fJ ,
which is to be translated into a target language sen-
tence eI1 = e1 . . . ei . . . eI . Among all possible tar-
get language sentences, we will choose the sentence
with the highest probability:
e?I?1 = argmax
I,eI1
{
Pr(eI1|fJ1 )
} (1)
= argmax
I,eI1
{
Pr(eI1) ? Pr(fJ1 |eI1)
} (2)
This decomposition into two knowledge sources
is known as the source-channel approach to sta-
tistical machine translation (Brown et al, 1990).
It allows an independent modeling of the target
language model Pr(eI1) and the translation model
Pr(fJ1 |eI1). The target language model describes
the well-formedness of the target language sentence.
The translation model links the source language sen-
tence to the target language sentence. The argmax
operation denotes the search problem, i.e., the gen-
eration of the output sentence in the target language.
A generalization of the classical source-channel
approach is the direct modeling of the posterior
probability Pr(eI1|fJ1 ). Using a log-linear model
2
(Och and Ney, 2002), we obtain:
Pr(eI1|fJ1 ) =
exp
(
?M
m=1 ?mhm(eI1, fJ1 )
)
?
I?,e?I?1
exp
(
?M
m=1 ?mhm(e?I
?
1 , fJ1 )
)
(3)
The denominator represents a normalization factor
that depends only on the source sentence fJ1 . There-
fore, we can omit it during the search process. As a
decision rule, we obtain:
e?I?1 = argmax
I,eI1
{ M
?
m=1
?mhm(eI1, fJ1 )
}
(4)
The log-linear model has the advantage that addi-
tional models h(?) can be easily integrated into the
overall system. The model scaling factors ?M1 are
trained according to the maximum entropy principle,
e.g., using the GIS algorithm. Alternatively, one can
train them with respect to the final translation quality
measured by an error criterion (Och, 2003).
The log-linear model is a natural framework to in-
tegrate many models. The baseline system uses the
following models:
? phrase translation model
? phrase count features
? word-based translation model
? word and phrase penalty
? target language model (6-gram)
? distortion model (assigning costs based on the
jump width)
All the experiments in the paper are evaluated with-
out rescoring. More details about the baseline sys-
tem can be found in (Mauser et al, 2006)
3.2 Source Sentence Reordering Framework
Encouraged by the work of (Xia and McCord, 2004)
and (Crego and Marin?o, 2006), we also reorder the
source language side. Compared to reordering on
the target language side, one advantage is the effi-
ciency since the reordering lattice can be translated
monotonically as in (Zens et al, 2002). Another ad-
vantage is that there is correct sentence information
POS tagging
shallow chunking
Translation Process
Standard Translation Proces
with Source Reordering
source text sentences
reordering rules
SMT system
translation output translation output
source text sentences
SMT system
source reordering lattice
Figure 1: Illustration of the translation process with
and without source reordering.
for the reordering methods, because the source sen-
tences are always given. Syntactic reordering on tar-
get language is difficult, since the methods will de-
grade much because of the errors in hypothesis.
We apply reordering at the syntactic chunk level
which can been seen as an intermediate level be-
tween full parsing and POS tagging. Figure 1 shows
the differences between the new translation frame-
work and the standard translation process. A re-
ordering lattice replaces the original source sentence
as the input to the translation system. The use of a
lattice avoids hard decisions before translation. To
generate the reordering lattice, the source sentence is
first POS tagged and chunk parsed. Then, reorder-
ing rules are applied to the chunks to generate the
reordering lattice.
Reordering rules are the key information for
source reordering. They are automatically learned
from the training data. The details of these two mod-
ules will be introduced in Section 5.
4 Reordering Rules
There has been much work on learning and apply-
ing reordering rules on source language, such as
(Nie?en and Ney, 2001; Xia and McCord, 2004;
Collins et al, 2005; Chen et al, 2006; Crego and
Marin?o, 2006; Popovic? and Ney, 2006). The re-
ordering rules could be composed of words, POS
tags or syntactic tags of phrases. In our work, a rule
is composed of chunk tags and POS tags. There is
3
Table 1: Examples of reordering rules. (lhs: chunk
and POS tag sequence, rhs: permutation )
no. lhs rhs
1. NP0 PP1 u2 n3 0 1 2 3
2. NP0 PP1 u2 n3 3 0 1 2
3. DNP0 NP1 V P2 0 1 2
4. DNP0 NP1 V P2 1 0 2
5. DNP0 NP1 m2 0 1 2
6. DNP0 NP1 m2 ad3 3 0 1 2
7. DNP0 NP1 m2 ad3 v4 4 3 0 1 2
no hierarchical structure in a rule.
4.1 Definition of Reordering Rules
First, we show some rule examples in Table 1. A re-
ordering rule consists of a left-hand-side (lhs) and a
right-hand-side (rhs). The left-hand-side is a syn-
tactic rule (chunk or POS tags), while the right-
hand-side is the reordering positions of the rule. Dif-
ferent rules can share the same left-hand-side, such
as rules no. 1, 2 and no. 3, 4. The rules record
not only the real reordered chunk sequence, but also
the monotone chunk sequences, like no. 1, 3 and
5. Note that the same tag sequence can appear mul-
tiple times according to different contexts, such as
DNP0 NP1 m2 # 0 1 2 in rules no. 5, 6, 7.
4.2 Extraction of Reordering Rules
The extraction of reordering rules is based on the
word alignment and the source sentence chunks.
Here, we train word alignments in both directions
with GIZA++ (Och and Ney, 2003). To get algn-
ment with high accuracy, we use the intersection
alignment here.
For a given word-aligned sentence pair
(fJ1 , eI1, aJ1 ), the source word sequence fJ1 is
first parsed into a chunk sequence FK1 . Accord-
ingly, the word-to-word alignment aJ1 is changed
to a chunk-to-word alignment a?K1 which is the
combination of the target words aligned to the
source words in a chunk. It is defined as:
a?k = {i|i = aj ? j ? [jk, jk+1 ? 1]}
Figure 2: Illustration of three kinds of phrases:
(a)monotone phrase, (b)reordering phrase, (c)cross
phrase. The black box is a word-to-word alignment.
The gray box is a chunk-to-word alignment.
Here, jk denotes the position of the first source word
in kth chunk. The new alignment is 1 : m from
source chunks to target words. It also means a?k is a
set of positions of target words.
We apply the standard phrase extraction algorithm
(Zens et al, 2002) to (FK1 , eI1, a?K1 ). Discarding the
cross phrases, we keep the other phrases as rules. In
a cross phrase, at least two chunk-word alignments
overlap on the target language side. An example
of a cross phrase is illustrated in Figure 2(c). Fig-
ure 2(a) and (b) illustrate the phrases for reordering
rules, which could be monotone phrases or reorder-
ing phrases.
5 Reordering Lattice Generation
5.1 Parsing the Source Sentence
The first step of chunk parsing is word segmentation.
Then, a POS tagger is usually needed for further
syntactic analysis. In our experiments, we use the
tool of ?Inst. of Computing Tech., Chinese Lexical
Analysis System (ICTCLAS)? (Zhang et al, 2003),
which does the two tasks in one pass.
Referring to the description of the chunking task
in CoNLL-20001, instead of English, a Chinese
chunker is processed and evaluated. Each word is
assigned a chunk tag, which contains the name of the
chunk type and ?B? for the first word of the chunk
and ?I? for each other word in the chunk. The ?O?
chunk tag is used for tokens which are not part of
any chunk. We use the maximum entropy tool YAS-
1http://www.cnts.ua.ac.be/conll2000/chunking/
4
Figure 3: Example of applying rules. The left part is the used rules. The right part is the generated new
orders of source words.
MET2 to learn the chunking model. The model is
based on a combination of word and POS tags. Since
specific training and test data are not available for
Chinese chunking, we convert subtrees of the Chi-
nese treebank (LDC2005T01) into chunks. As there
are many ways to choose a subtree, we uses the min-
imum subtree with the following constraints:
? a subtree has more than one child,
? the children of a subtree are all leaves.
Compared to chunking of English as in CoNLL-
2000, there are more chunk types (24 instead of 6)
and no single-word chunks. These two aspects make
chunking for Chinese harder.
5.2 Applying Reordering Rules
First, we search the reordering rules, in which the
chunk sequence matches any tag sequence in the in-
put sentence. A source sentence has many paths
generated by the rules . For a word uncovered by any
rules, its POS tag is used. Each path corresponds to
one sentence permutation.
The left part of the Figure 3 shows seven possible
coverages, the right part is the reordering for each
coverage. Some of the reorderings are identical, like
the permutations in line 1, 3 and 5. That is because
one word sequence is memorized by several rules in
different contexts.
5.3 Lattice Weighting
All reorderings of an input sentence S are com-
pressed and stored in a lattice. Each path is a possi-
2http://www-i6.informatik.rwth-aachen.de/web/Software
/index.html
ble reordering S? and is given a weight W . In this
paper, the weight is computed using a source lan-
guage model p(S?). The weight is used directly in
the decoder, integrated into Equation (4). There is
also a scaling factor for this weight, which is op-
timized together with other scaling factors on the
development data. The probability of the reordered
source sentence is calculated as follows: for a re-
ordered source sentence w1w2...wn, the trigram lan-
guage model is:
p(S?) =
N
?
n=1
p(wn|wn?2, wn?1) (5)
Beside a word N-gram language model, a POS tag
N-gram model or a chunk tag N-gram model could
be used as well.
In this paper, we use a word trigram model. The
model is trained on reordered training source sen-
tences. A training source sentence is parsed into
chunks. In the same way as described in Section
4.2, word-to-word alignments is converted to chunk-
to-word alignments. We reorder the source chunks
to monotonize the chunk-to-word alignments. The
chunk boundaries are kept when this reordering is
done.
6 Experiments
6.1 Chunking Result
In this section, we report results for chunk parsing.
The annotation of the data is derived from the Chi-
nese treebank (LDC2005T01). The corpus is split
into two parts: 1000 sentences are randomly se-
5
Table 2: Statistics of training and test corpus for
chunk parsing.
train test
sentences 17 785 1 000
words 486 468 21 851
chunks 105 773 4 680
words out of chunks 244 416 10 282
Table 3: Chunk parsing result on 1000 sentences.
accuracy precision recall F-measure
74.51% 65.2% 61.5% 63.3
lected as test data. The remaining part is used for
training. The corpus is from the newswire domain.
Table 2 shows the corpus statistics. For the 4 680
chunks in the test set, the chunker has found 4 414
chunks, of which 2 879 are correct. Following the
criteria of CoNLL-2000, the chunker is evaluated
using the F-score, which is a combination of pre-
cision and recall. The result is shown in Table 3.
The accuracy is evaluated at the word level, the
other three metrics are evaluated at the chunk level.
The results at the chunk level are worse than at the
word level, because a chunk is counted as correct
only if the chunk tag and the chunk boundaries are
both correct.
6.2 Translation Results
For the translation experiments, we report the two
accuracy measures BLEU (Papineni et al, 2002)
and NIST (Doddington, 2002) as well as the two
error rates word error rate (WER) and position-
independent word error rate (PER).
We perform translation experiments on the Ba-
sic Traveling Expression Corpus (BTEC) for the
Chinese-English task. It is a speech translation task
in the domain of tourism-related information. We
report results on the IWSLT 2004, 2005 and 2006
evaluation test sets. There are 16 reference trans-
lations for the IWSLT 2004 and 2005 tasks and 7
reference translations for the IWSLT 2006 task.
Table 4 shows the corpus statistics of the task. A
training corpus is used to train the translation model,
the language model and to obtain the reordering
Table 4: Statistics of training and test corpora for the
IWSLT tasks.
Chinese English
Train Sentences 40k
Words 308k 377k
Dev Sentences 489
Words 5 478 6 008
Test Sentences 500
IWSLT04 Words 3 866 3 581
Test Sentences 506
IWSLT05 Words 3 652 3 579
Test Sentences 500
IWSLT06 Words 5 846 ?
rules. A development corpus is used to optimize the
scaling factors for the BLEU score. The English text
is processed using a tokenizer. The Chinese text pro-
cessing uses word segmentation with the ICTCLAS
segmenter (Zhang et al, 2003). The translation is
evaluated case-insensitive and without punctuation
marks.
The translation results are presented in Table 5.
The baseline system is a non-monotone translation
system, in which the decoder does reordering on
the target language side. Compared to the base-
line system, the source reordering method improves
the BLEU score by 0.5% ? 1.8% absolute. It also
achieves a better WER. Note that the used chun-
ker here is out-of-domain 3. An improvement is
achieved even with a low F-measure for chunking.
So, we could hope that larger improvement is possi-
ble using a high-accuracy chunker.
Though the input is a lattice, the source reordering
is still faster than the reordering during decoding,
e.g. for the IWSLT 2006 test set, the baseline system
took 17.5 minutes and the source reordering system
took 12.3 minutes. The result also indicates that the
non-monotone decoding hurts the performance in a
source reordering framework. A similar conclusion
is also presented in (Xia and McCord, 2004).
Additional experiments we carried out to compare
POS-level and chunk-level reorderings. We delete
the chunk information and keep the POS tags. Then,
3The chunker is trained on newswire data, but the test data
is from the tourism domain.
6
Table 5: Translation performance for the Chinese-English IWSLT task
WER[%] PER[%] NIST BLEU[%]
IWSLT04 baseline 47.3 38.2 7.78 39.1
source reordering 46.3 37.2 7.70 40.9
IWSLT05 baseline 45.0 37.3 7.40 41.8
source reordering 44.6 36.8 7.51 42.3
IWSLT06 baseline 67.4 50.0 6.65 22.4
source reordering 65.6 50.4 6.46 23.3
source reordering+non-monotone decoder 66.5 50.3 6.52 22.4
Table 6: Translation performance of reordering
methods on IWSLT 2004 test set
WER PER NIST BLEU
[%] [%] [%]
Baseline 47.3 38.2 7.78 39.1
POS 46.9 37.5 7.38 39.7
Chunk 46.3 37.2 7.70 40.9
Table 7: Lattice information for the Chinese-English
IWSLT 2004 test data
avg. density used translation
pro sent rules time [min/sec]
POS 15.7 6 868 7:08
Chunk 8.2 3 685 3:47
we rerun the source reordering system on the IWSLT
2004 test set. The translation results are shown in
Table 6. Though the accuracy of chunking is low,
the chunk-level method gets better results than POS-
level method. With POS tags, we get more reorder-
ing rules and more paths in the lattice, since the sen-
tence length is longer than with chunks. The statis-
tics are shown in Table 7.
7 Conclusions and Future Work
This paper presents a source-side reordering method
which is based on syntactic chunks. The reordering
rules are automatically learned from bilingual data.
To avoid hard decision before decoding, a reorder-
ing lattice representing all possible reorderings is
used instead of single source sentence for decoding.
The experiments demonstrate that even with a very
poor chunker, the chunk-level source reordering is
still helpful for a state-of-the-art statistical transla-
tion system and it has better performance than the
POS-level source reordering and target-side reorder-
ing.
There are some directions for future work. First,
we would like to try this method on larger data sets
and other language pairs. Second, we are going to
improve the chunking accuracy. Third, we would
reduce the number of rules and prune the lattice.
Acknowledgments
This material is partly based upon work sup-
ported by the Defense Advanced Research Projects
Agency (DARPA) under Contract No. HR0011-06-
C-0023, and was partially funded by the Deutsche
Forschungsgemeinschaft (DFG) under the project
?Statistische Textu?bersetzung? (Ne572/5)
References
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A
maximum entropy approach to natural language processing.
Computational Linguistics, 22(1):39?72, March.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra,
F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin.
1990. A statistical approach to machine translation. Com-
putational Linguistics, 16(2):79?85, June.
B. Chen, M. Cettolo, and M. Federico. 2006. Reordering
rules for phrase-based statistical machine translation. In
Int. Workshop on Spoken Language Translation Evaluation
Campaign on Spoken Language Translation, pages 1?15,
Kyoto, Japan, November.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause restructur-
ing for statistical machine translation. In Proc. of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics (ACL), pages 531?540, Ann Arbor, Michigan, June.
7
M. R. Costa-jussa` and J. A. R. Fonollosa. 2006. Statistical ma-
chine reordering. In Proc. of the Conf. on Empirical Meth-
ods in Natural Language Processing, pages 70?76, Sydney,
Australia, July.
J. M. Crego and J. B. Marin?o. 2006. Integration of postag-
based source reordering into SMT decoding by an extended
search graph. In Proc. of AMTA06, pages 29?36, Mas-
sachusetts, USA, August.
G. Doddington. 2002. Automatic evaluation of machine trans-
lation quality using n-gram co-occurrence statistics. In Proc.
ARPA Workshop on Human Language Technology.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable inference and train-
ing of context-rich syntactic translation models. In Proc. of
the 21st Int. Conf. on Computational Linguistics and 44th
Annual Meeting of the Association for Computational Lin-
guistics, pages 961?968, Sydney, Australia, July.
D. Gildea. 2003. Loosely tree-based alignment for machine
translation. In Proc. of the 41th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages 80?87,
Sapporo, Japan, July.
J. Graehl and K. Knight. 2004. Training tree transducers.
In HLT-NAACL 2004: Main Proc., pages 105?112, Boston,
Massachusetts, USA, May 2 - May 7.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney. 2005.
Novel reordering approaches in phrase-based statistical ma-
chine translation. In 43rd Annual Meeting of the Assoc. for
Computational Linguistics: Proc. Workshop on Building and
Using Parallel Texts: Data-Driven Machine Translation and
Beyond, pages 167?174, Ann Arbor, Michigan, June.
P. Koehn and K. Knight. 2003. Empirical methods for com-
pound splitting. In Proc. 10th Conf. of the Europ. Chapter
of the Assoc. for Computational Linguistics (EACL), pages
347?354, Budapest, Hungary, April.
A. Mauser, R. Zens, E. Matusov, S. Hasan, and H. Ney. 2006.
The RWTH Statistical Machine Translation System for the
IWSLT 2006 Evaluation. In Proc. of the Int. Workshop
on Spoken Language Translation, pages 103?110, Kyoto,
Japan.
I. Melamed. 2004. Statistical machine translation by parsing.
In The Companion Volume to the Proc. of 42nd Annual Meet-
ing of the Association for Computational Linguistics, pages
653?660.
S. Nie?en and H. Ney. 2001. Morpho-syntactic analysis for
reordering in statistical machine translation. In Proc. of MT
Summit VIII, pages 247?252.
F. J. Och and H. Ney. 2002. Discriminative training and max-
imum entropy models for statistical machine translation. In
Proc. of the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 295?302, Philadelphia,
PA, July.
F. J. Och and H. Ney. 2003. A systematic comparison of vari-
ous statistical alignment models. Computational Linguistics,
29(1):19?51, March.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,
A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain,
Z. Jin, and D. Radev. 2004. A smorgasbord of features for
statistical machine translation. In Proc. 2004 Human Lan-
guage Technology Conf. / North American Chapter of the
Association for Computational Linguistics Annual Meeting
(HLT-NAACL), pages 161?168, Boston,MA.
F. J. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proc. of the 41th Annual Meeting of
the Association for Computational Linguistics (ACL), pages
160?167, Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. Bleu: a
method for automatic evaluation of machine translation. In
Proc. of the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 311?318, Philadelphia,
PA, July.
M. Popovic? and H. Ney. 2006. POS-based word reorderings
for statistical machine translation. In Proc. of the Fifth Int.
Conf. on Language Resources and Evaluation (LREC).
L. Shen, A. Sarkar, and F. J. Och. 2004. Discriminative rerank-
ing for machine translation. In HLT-NAACL 2004: Main
Proc., pages 177?184, Boston, Massachusetts, USA, May 2
- May 7.
C. Tillmann, S. Vogel, H. Ney, and A. Zubiaga. 1997. A DP-
based search using monotone alignments in statistical trans-
lation. In Proc. 35th Annual Conf. of the Association for
Computational Linguistics, pages 289?296, Madrid, Spain,
July.
D. Wu. 1996. A polynomial-time algorithm for statistical ma-
chine translation. In Proc. 34th Annual Meeting of the As-
soc. for Computational Linguistics, pages 152?158, Santa
Cruz, CA, June.
D. Wu. 1997. Stochastic inversion transduction grammars and
bilingual parsing of parallel corpora. Computational Lin-
guistics, 23(3):377?403, September.
F. Xia and M. McCord. 2004. Improving a statistical MT sys-
tem with automatically learned rewrite patterns. In Proc. of
COLING04, pages 508?514, Geneva, Switzerland, Aug 23?
Aug 27.
K. Yamada and K. Knight. 2001. A syntax-based statistical
translation model. In Proc. of the 39th Annual Meeting of
the Association for Computational Linguistics (ACL), pages
523?530, Toulouse, France, July.
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based statistical
machine translation. In M. Jarke, J. Koehler, and G. Lake-
meyer, editors, 25th German Conf. on Artificial Intelligence
(KI2002), volume 2479 of Lecture Notes in Artificial Intel-
ligence (LNAI), pages 18?32, Aachen, Germany, September.
Springer Verlag.
H. P. Zhang, Q. Liu, X. Q. Cheng, H. Zhang, and H. K. Yu.
2003. Chinese lexical analysis using hierarchical hidden
markov model. In Proc. of the second SIGHAN workshop
on Chinese language processing, pages 63?70, Morristown,
NJ, USA.
8
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 972?983, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Systematic Comparison of Phrase Table Pruning Techniques
Richard Zens and Daisy Stanton and Peng Xu
Google Inc.
{zens,daisy,xp}@google.com
Abstract
When trained on very large parallel corpora,
the phrase table component of a machine
translation system grows to consume vast
computational resources. In this paper, we in-
troduce a novel pruning criterion that places
phrase table pruning on a sound theoretical
foundation. Systematic experiments on four
language pairs under various data conditions
show that our principled approach is superior
to existing ad hoc pruning methods.
1 Introduction
Over the last years, statistical machine translation
has become the dominant approach to machine
translation. This is not only due to improved mod-
eling, but also due to a significant increase in the
availability of monolingual and bilingual data. Here
are just two examples of very large data resources
that are publicly available:
? The Google Web 1T 5-gram corpus available
from the Linguistic Data Consortium consist-
ing of the 5-gram counts of about one trillion
words of web data.1
? The 109-French-English bilingual corpus with
about one billion tokens from the Workshop on
Statistical Machine Translation (WMT).2
These enormous data sets yield translation models
that are expensive to store and process. Even with
1LDC catalog No. LDC2006T13
2http://www.statmt.org/wmt11/translation-task.html
modern computers, these large models lead to a long
experiment cycle that hinders progress. The situa-
tion is even more severe if computational resources
are limited, for instance when translating on hand-
held devices. Then, reducing the model size is of
the utmost importance.
The most resource-intensive components of a sta-
tistical machine translation system are the language
model and the phrase table. Recently, compact rep-
resentations of the language model have attracted
the attention of the research community, for instance
in Talbot and Osborne (2007), Brants et al(2007),
Pauls and Klein (2011) or Heafield (2011), to name
a few. In this paper, we address the other problem
of any statistical machine translation system: large
phrase tables.
Johnson et al(2007) has shown that large por-
tions of the phrase table can be removed without loss
in translation quality. This motivated us to perform
a systematic comparison of different pruning meth-
ods. However, we found that many existing methods
employ ad-hoc heuristics without theoretical foun-
dation.
The pruning criterion introduced in this work is
inspired by the very successful and still state-of-the-
art language model pruning criterion based on en-
tropy measures (Stolcke, 1998). We motivate its
derivation by stating the desiderata for a good phrase
table pruning criterion:
? Soundness: The criterion should optimize
some well-understood information-theoretic
measure of translation model quality.
972
? Efficiency: Pruning should be fast, i. e., run lin-
early in the size of the phrase table.
? Self-containedness: As a practical considera-
tion, we want to prune phrases from an existing
phrase table. This means pruning should use
only information contained in the model itself.
? Good empirical behavior: We would like to
be able to prune large parts of the phrase table
without significant loss in translation quality.
Analyzing existing pruning techniques based on
these objectives, we found that they are commonly
deficient in at least one of them. We thus designed
a novel pruning criterion that not only meets these
objectives, it also performs very well in empirical
evaluations.
The novel contributions of this paper are:
1. a systematic description of existing phrase table
pruning methods.
2. a new, theoretically sound phrase table pruning
criterion.
3. an experimental comparison of several pruning
methods for several language pairs.
2 Related Work
The most basic pruning methods rely on probabil-
ity and count cutoffs. We will cover the techniques
that are implemented in the Moses toolkit (Koehn et
al., 2007) and the Pharaoh decoder (Koehn, 2004) in
Section 3. We are not aware of any work that ana-
lyzes their efficacy in a systematic way. It is thus not
surprising that some of them perform poorly, as our
experimental results will show.
The work of Johnson et al(2007) is promis-
ing as it shows that large parts of the phrase ta-
ble can be removed without affecting translation
quality. Their pruning criterion relies on statisti-
cal significance tests. However, it is unclear how
this significance-based pruning criterion is related to
translation model quality. Furthermore, a compari-
son to other methods is missing. Here we close this
gap and perform a systematic comparison. The same
idea of significance-based pruning was exploited in
(Yang and Zheng, 2009; Tomeh et al 2009) for hi-
erarchical statistical machine translation.
A different approach to phrase table pruning was
undertaken by Eck et al(2007a; 2007b). They rely
on usage statistics from translating sample data, so it
is not self-contained. However, it could be combined
with the methods proposed here.
Another approach to phrase table pruning is trian-
gulation (Chen et al 2008; Chen et al 2009). This
requires additional bilingual corpora, namely from
the source language as well as from the target lan-
guage to a third bridge language. In many situations
this does not exist or would be costly to generate.
Duan et al(2011), Sanchis-Trilles et al(2011)
and Tomeh et al(2011) modify the phrase extrac-
tion methods in order to reduce the phrase table size.
The work in this paper is independent of the way the
phrase extraction is done, so those approaches are
complementary to our work.
3 Pruning Using Simple Statistics
In this section, we will review existing pruning
methods based on simple phrase table statistics.
There are two common classes of these methods: ab-
solute phrase table pruning and relative phrase table
pruning.
3.1 Absolute pruning
Absolute pruning methods rely only on the statistics
of a single phrase pair (f? , e?). Hence, they are in-
dependent of other phrases in the phrase table. As
opposed to relative pruning methods (Section 3.2),
they may prune all translations of a source phrase.
Their application is easy and efficient.
? Count-based pruning. This method prunes
a phrase pair (f? , e?) if its observation count
N(f? , e?) is below a threshold ?c:
N(f? , e?) < ?c (1)
? Probability-based pruning. This method
prunes a phrase pair (f? , e?) if its probability is
below a threshold ?p:
p(e?|f?) < ?p (2)
Here the probability p(e?|f?) is estimated via rel-
ative frequencies.
973
3.2 Relative pruning
A potential problem with the absolute pruning meth-
ods is that it can prune all occurrences of a source
phrase f? .3 Relative pruning methods avoid this by
considering the full set of target phrases for a spe-
cific source phrase f? .
? Threshold pruning. This method discards
those phrases that are far worse than the best
target phrase for a given source phrase f? . Given
a pruning threshold ?t, a phrase pair (f? , e?) is
discarded if:
p(e?|f?) < ?t ?max
e?
{
p(e?|f?)
}
(3)
? Histogram pruning. An alternative to thresh-
old pruning is histogram pruning. For each
source phrase f? , this method preserves the K
target phrases with highest probability p(e?|f?)
or, equivalently, their count N(f? , e?).
Note that, except for count-based pruning, none of
the methods take the frequency of the source phrase
into account. As we will confirm in the empirical
evaluation, this will likely cause drops in translation
quality, since frequent source phrases are more use-
ful than the infrequent ones.
4 Significance Pruning
In this section, we briefly review significance prun-
ing following Johnson et al(2007). The idea of sig-
nificance pruning is to test whether a source phrase
f? and a target phrase e? co-occur more frequently in
a bilingual corpus than they should just by chance.
Using some simple statistics derived from the bilin-
gual corpus, namely
? N(f?) the count of the source phrase f?
? N(e?) the count of the target phrase e?
? N(f? , e?) the co-occurence count of the source
phrase f? and the target phrase e?
? N the number of sentences in the bilingual cor-
pus
3Note that it has never been systematically investigated
whether this is a real problem or just speculation.
we can compute the two-by-two contingency table
in Table 1.
Following Fisher?s exact test, we can calculate the
probability of the contingency table via the hyperge-
ometric distribution:
ph(N(f? , e?)) =
(
N(f?)
N(f? ,e?)
)
?
(
N?N(f?)
N(e?)?N(f? ,e?)
)
(
N
N(e?)
) (4)
The p-value is then calculated as the sum of all
probabilities that are at least as extreme. The lower
the p-value, the less likely this phrase pair occurred
with the observed frequency by chance; we thus
prune a phrase pair (f? , e?) if:
?
?
??
k=N(f? ,e?)
ph(k)
?
? > ?F (5)
for some pruning threshold ?F . More details of this
approach can be found in Johnson et al(2007). The
idea of using Fisher?s exact test was first explored by
Moore (2004) in the context of word alignment.
5 Entropy-based Pruning
In this section, we will derive a novel entropy-based
pruning criterion.
5.1 Motivational Example
In general, pruning the phrase table can be consid-
ered as selecting a subset of the original phrase table.
When doing so, we would like to alter the original
translation model distribution as little as possible.
This is a key difference to previous approaches: Our
goal is to remove redundant phrases, whereas previ-
ous approaches usually try to remove low-quality or
unreliable phrases. We believe this to be an advan-
tage of our method as it is certainly easier to measure
the redundancy of phrases than it is to estimate their
quality.
In Table 2, we show some example phrases
from the learned French-English WMT phrase table,
along with their counts and probabilities. For the
French phrase le gouvernement franc?ais, we have,
among others, two translations: the French govern-
ment and the government of France. If we have
to prune one of those translations, we can ask our-
selves: how would the translation cost change if the
974
N(f? , e?) N(f?)?N(f? , e?) N(f?)
N(e?)?N(f? , e?) N ?N(f?)?N(e?) +N(f? , e?) N ?N(f?)
N(e?) N ?N(e?) N
Table 1: Two-by-two contingency table for a phrase pair (f? , e?).
Source Phrase f? Target Phrase e? N(f? , e?) p(e?|f?)
le the 7.6 M 0.7189
gouvernement government 245 K 0.4106
franc?ais French 51 K 0.6440
of France 695 0.0046
le gouvernement franc?ais the French government 148 0.1686
the government of France 11 0.0128
Table 2: Example phrases from the French-English phrase table (K=thousands, M=millions).
same translation were generated from the remain-
ing, shorter, phrases? Removing the phrase the gov-
ernment of France would increase this cost dramat-
ically. Given the shorter phrases from the table, the
probability would be 0.7189 ? 0.4106 ? 0.0046 =
0.0014?, which is about an order of a magnitude
smaller than the original probability of 0.0128.
On the other hand, composing the phrase the
French government out of shorter phrases has prob-
ability 0.7189 ? 0.4106 ? 0.6440 = 0.1901, which is
very close to the original probability of 0.1686. This
means it is safe to discard the phrase the French gov-
ernment, since the translation cost remains essen-
tially unchanged. By contrast, discarding the phrase
the government of France does not have this effect:
it leads to a large change in translation cost.
Note that here the pruning criterion only considers
redundancy of the phrases, not the quality. Thus, we
are not saying that the government of France is a
better translation than the French government, only
that it is less redundant.
?We use the assumption that we can simply multiply the
probabilities of the shorter phrases.
5.2 Entropy Criterion
Now, we are going to formalize the notion of re-
dundancy. We would like the pruned model p?(e?|f?)
to be as similar as possible to the original model
p(e?|f?). We use conditional Kullback-Leibler di-
vergence, also called conditional relative entropy
(Cover and Thomas, 2006), to measure the model
similarity:
D(p(e?|f?)||p?(e?|f?))
=
?
f?
p(f?)
?
e?
p(e?|f?) log
[
p(e?|f?)
p?(e?|f?)
]
(6)
=
?
f? ,e?
p(e?, f?)
[
log p(e?|f?)? log p?(e?|f?)
]
(7)
Computing the best pruned model of a given size
would require optimizing over all subsets with that
size. Since that is computationally infeasible, we in-
stead apply the equivalent approximation that Stol-
cke (1998) uses for language modeling. This as-
sumes that phrase pairs affect the relative entropy
roughly independently.
We can then choose a pruning threshold ?E and
prune those phrase pairs with a contribution to the
relative entropy below that threshold. Thus, we
975
prune a phrase pair (f? , e?), if
p(e?, f?)
[
log p(e?|f?)? log p?(e?|f?)
]
< ?E (8)
We now address how to assign the probability
p?(e?|f?) under the pruned model. A phrase-based
system selects among different segmentations of the
source language sentence into phrases. If a segmen-
tation into longer phrases does not exist, the system
has to compose a translation out of shorter phrases.
Thus, if a phrase pair (f? , e?) is no longer available,
the decoder has to use shorter phrases to produce
the same translation. We can therefore decompose
the pruned model score p?(e?|f?) by summing over all
segmentations sK1 and all reorderings pi
K
1 :
p?(e?|f?) =
?
sK1 ,pi
K
1
p(sK1 , pi
K
1 |f?) ? p(e?|s
K
1 , pi
K
1 , f?) (9)
Here the segmentation sK1 divides both the source
and target phrases into K sub-phrases:
f? = f?pi1 ...f?piK and e? = e?1...e?K (10)
The permutation piK1 describes the alignment of
those sub-phrases, such that the sub-phrase e?k is
aligned to f?pik . Using the normal phrase translation
model, we obtain:
p?(e?|f?) =
?
sK1 ,pi
K
1
p(sK1 , pi
K
1 |f?)
K?
k=1
p(e?k|f?pik) (11)
Virtually all phrase-based decoders use the so-
called maximum-approximation, i. e. the sum is re-
placed with the maximum. As we would like the
pruning criterion to be similar to the search criterion
used during decoding, we do the same and obtain:
p?(e?|f?) ? max
sK1 ,pi
K
1
K?
k=1
p(e?k|f?pik) (12)
Note that we also drop the segmentation probabil-
ity, as this is not used at decoding time. This leaves
the pruning criterion a function only of the model
p(e?|f?) as stored in the phrase table. There is no need
for a special development or adaptation set. We can
determine the best segmentation using dynamic pro-
gramming, similar to decoding with a phrase-based
model. However, here the target side is constrained
to the given phrase e?.
It can happen that a phrase is not compositional,
i. e., we cannot find a segmentation into shorter
phrases. In these cases, we assign a small, constant
probability:
p?(e?|f?) = pc (13)
We found that the value pc = e?10 works well for
many language pairs.
5.3 Computation
In our experiments, it was more efficient to vary the
pruning threshold ?E without having to re-compute
the entire phrase table. Therefore, we computed the
entropy criterion in Equation (8) once for the whole
phrase table. This introduces an approximation for
the pruned model score p?(e?|f?). It might happen
that we prune short phrases that were used as part
of the best segmentation of longer phrases. As these
shorter phrases should not be available, the pruned
model score might be inaccurate. Although we be-
lieve this effect is minor, we leave a detailed experi-
mental analysis for future work.
One way to avoid this approximation would be
to perform entropy pruning with increasing phrase
length. Starting with one-word phrases, which are
trivially non-compositional, the entropy criterion
would be straightforward to compute. Proceed-
ing to two-word phrases, one would decompose the
phrases into sub-phrases by looking up the proba-
bilities of some of the unpruned one-word phrases.
Once the set of unpruned two-word phrases was ob-
tained, one would continue with three-word phrases,
etc.
6 Experimental Evaluation
6.1 Data Sets
In this section, we describe the data sets used for the
experiments. We perform experiments on the pub-
licly available WMT shared translation task for the
following four language pairs:
? German-English
? Czech-English
? Spanish-English
976
Number of Words
Language Pair Foreign English
German - English 42 M 45 M
Czech - English 56 M 65 M
Spanish - English 232 M 210 M
French - English 962 M 827 M
Table 3: Training data statistics. Number of words in the
training data (M=millions).
? French-English
For each pair, we train two separate system, one for
each direction. Thus it can happen that a phrase is
pruned for X-to-Y, but not for Y-to-X.
These four language pairs represent a nice range
of training corpora sizes, as shown in Table 3.
6.2 Baseline System
Pruning experiments were performed on top of the
following baseline system. We used a phrase-
based statistical machine translation system similar
to (Zens et al 2002; Koehn et al 2003; Och and
Ney, 2004; Zens and Ney, 2008). We trained a 4-
gram language model on the target side of the bilin-
gual corpora and a second 4-gram language model
on the provided monolingual news data. All lan-
guage models used Kneser-Ney smoothing.
The baseline system uses the common phrase
translation models, such as p(e?|f?) and p(f? |e?), lex-
ical models, word and phrase penalty, distortion
penalty as well as a lexicalized reordering model
(Zens and Ney, 2006).
The word alignment was trained with six itera-
tions of IBM model 1 (Brown et al 1993) and 6 it-
erations of the HMM alignment model (Vogel et al
1996) using a symmetric lexicon (Zens et al 2004).
The feature weights were tuned on a development
set by applying minimum error rate training (MERT)
under the Bleu criterion (Och, 2003; Macherey et al
2008). We ran MERT once with the full phrase table
and then kept the feature weights fixed, i. e., we did
not rerun MERT after pruning to avoid adding un-
necessary noise. We extract phrases up to a length
of six words. The baseline system already includes
phrase table pruning by removing singletons and
keeping up to 30 target language phrases per source
phrase. We found that this does not affect transla-
 8
 10
 12
 14
 16
 18
 20
 22
 24
 26
 28
 1  2  4  8
BL
EU
[%]
Number of Phrases [millions]
ProbThresHist
Figure 1: Comparison of probability-based pruning
methods for German-English.
tion quality significantly4. All pruning experiments
are done on top of this.
6.3 Results
In this section, we present the experimental results.
Translation results are reported on the WMT?07
news commentary blind set.
We will show translation quality measured with
the Bleu score (Papineni et al 2002) as a function
of the phrase table size (number of phrases). Being
in the upper left corner of these figures is desirable.
First, we show a comparison of several
probability-based pruning methods in Figure 1.
We compare
? Prob. Absolute pruning based on Eq. (2).
? Thres. Threshold pruning based on Eq. (3).
? Hist. Histogram pruning as described in Sec-
tion 3.2.5
We observe that these three methods perform
equally well. There is no difference between abso-
lute and relative pruning methods, except that the
two relative methods (Thres and Hist) are limited by
4The Bleu score drops are as follows: English-French 0.3%,
French-English 0.4%, Czech-English 0.3%, all other are less
than 0.1%.
5Instead of using p(e?|f?) one could use the weighted model
score including p(f? |e?), lexical weightings etc.; however, we
found that this does not give significantly different results; but
it does introduce a undesirable dependance between feature
weights and phrase table pruning.
977
the number of source phrases. Thus, they reach a
point where they cannot prune the phrase table any
further. The results shown are for German-English;
the results for the other languages are very similar.
The results that follow use only the absolute prun-
ing method as a representative for probability-based
pruning.
In Figures 2 through 5, we show the transla-
tion quality as a function of the phrase table size.
We vary the pruning thresholds to obtain different
phrase table sizes. We compare four pruning meth-
ods:
? Count. Pruning based on the frequency of a
phrase pair, c.f. Equation (1).
? Prob. Pruning based on the absolute probabil-
ity of a phrase pair, c.f. Equation (2).
? Fisher. Pruning using significance tests, c.f.
Equation (5).
? Entropy. Pruning using the novel entropy cri-
terion, c.f. Equation (8).
Note that the x-axis of these figures is on a logarith-
mic scale, so the differences between the methods
can be quite dramatic. For instance, entropy pruning
requires less than a quarter of the number of phrases
needed by count- or significance-based pruning to
achieve a Spanish-English Bleu score of 34 (0.4 mil-
lion phrases compared to 1.7 million phrases).
These results clearly show how the pruning meth-
ods compare:
1. Probability-based pruning performs poorly. It
should be used only to prune small fractions of
the phrase table.
2. Count-based pruning and significance-based
pruning perform equally well. They are much
better than probability-based pruning.
3. Entropy pruning consistently outperforms the
other methods across translation directions and
language pairs.
Figures 6 and 7 show compositionality statistics
for the pruned Spanish-English phrase table (we ob-
served similar results for the other language pairs).
Total number of phrases 4 137 M
Compositional 3 970 M
Non-compositional 167 M
of those: one-word phrases 85 M
no segmentation 82 M
Table 4: Statistics of phrase compositionality
(M=millions).
Each figure shows the composition of the phrase ta-
ble for a type of pruning for different phrase tables
sizes. Along the x-axis, we plotted the phrase ta-
ble size. These are the same phrase tables used to
obtain the Bleu scores in Figure 2 (left). The dif-
ferent shades of grey correspond to different phrase
lengths. For instance, in case of the smallest phrase
table for count-based pruning, the 1-word phrases
account for about 30% of all phrases, the 2-word
phrases account for about 35% of all phrases, etc.
With the exception of the probability-based prun-
ing, the plots look comparable. The more aggres-
sive the pruning, the larger the percentage of short
phrases. We observe that entropy-based pruning re-
moves many more long phrases than any of the other
methods. The plot for probability-based pruning is
different in that the percentage of long phrases ac-
tually increases with more aggressive pruning (i. e.
smaller phrase tables). A possible explanation is
that probability-based pruning does not take the fre-
quency of the source phrase into account. This
difference might explain the poor performance of
probability-based pruning.
To analyze how many phrases are compositional,
we collect statistics during the computation of the
entropy criterion. These are shown in Table 4, ac-
cumulated across all language pairs and all phrases,
i. e., including singleton phrases. We see that 96%
of all phrases are compositional (3 970 million out
of 4 137 million phrases). Furthermore, out of
the 167 million non-compositional phrases, more
than half (85 million phrases), are trivially non-
compositional: they consist only of a single source
or target language word. The number of non-trivial
non-compositional phrases is, with 82 million or 2%
of the total number of phrases, very small.
In Figure 8, we show the effect of the constant
978
 18
 20
 22
 24
 26
 28
 30
 32
 34
 36
 38
 0.01  0.1  1  10  100
BLE
U[%
]
Number of Phrases [M]
ProbCountFisherEntropy
 10
 15
 20
 25
 30
 35
 40
 0.01  0.1  1  10  100
BLE
U[%
]
Number of Phrases [M]
ProbCountFisherEntropy
Figure 2: Translation quality as a function of the phrase table size for Spanish-English (left) and English-Spanish
(right).
 10
 15
 20
 25
 30
 35
 40
 0.1  1  10  100  1000
BLE
U[%
]
Number of Phrases [M]
ProbCountFisherEntropy
 5
 10
 15
 20
 25
 30
 35
 40
 0.1  1  10  100  1000
BLE
U[%
]
Number of Phrases [M]
ProbCountFisherEntropy
Figure 3: Translation quality as a function of the phrase table size for French-English (left) and English-French (right).
 6
 8
 10
 12
 14
 16
 18
 20
 22
 24
 26
 0.001  0.01  0.1  1  10  100
BLE
U[%
]
Number of Phrases [M]
ProbCountFisherEntropy
 4
 6
 8
 10
 12
 14
 16
 0.001  0.01  0.1  1  10  100
BLE
U[%
]
Number of Phrases [M]
ProbCountFisherEntropy
Figure 4: Translation quality as a function of the phrase table size for Czech-English (left) and English-Czech (right).
 8
 10
 12
 14
 16
 18
 20
 22
 24
 26
 28
 0.001  0.01  0.1  1  10
BLE
U[%
]
Number of Phrases [M]
ProbCountFisherEntropy
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 0.001  0.01  0.1  1  10
BLE
U[%
]
Number of Phrases [M]
ProbCountFisherEntropy
Figure 5: Translation quality as a function of the phrase table size for German-English (left) and English-German
(right).
979
 0
 20
 40
 60
 80
 100
 10  100
Per
cen
tag
e o
f Ph
ras
es 
[%]
Number of Phrases [millions]
Prob
6-word5-word
4-word3-word
2-word1-word
 0
 20
 40
 60
 80
 100
 0.01  0.1  1  10  100
Per
cen
tag
e o
f Ph
ras
es 
[%]
Number of Phrases [millions]
Count
6-word5-word
4-word3-word
2-word1-word
Figure 6: Phrase length statistics for Spanish-English for probability-based (left) and count-based pruning (right).
 0
 20
 40
 60
 80
 100
 0.01  0.1  1  10  100
Per
cen
tag
e o
f Ph
ras
es 
[%]
Number of Phrases [millions]
Fisher
6-word5-word
4-word3-word
2-word1-word
 0
 20
 40
 60
 80
 100
 0.01  0.1  1  10  100
Per
cen
tag
e o
f Ph
ras
es 
[%]
Number of Phrases [millions]
Entropy
6-word5-word
4-word3-word
2-word1-word
Figure 7: Phrase length statistics for Spanish-English for significance-based (left) and entropy-based pruning (right).
pc for non-compositional phrases.6 The results
shown are for Spanish-English; additional experi-
ments for the other languages and translation direc-
tions showed very similar results. Overall, there is
no big difference between the values. Hence, we
chose a value of 10 for all experiments.
The results in Figure 2 to Figure 5 show that
entropy-based pruning clearly outperforms the al-
ternative pruning methods. However, it is a bit
hard to see from the graphs exactly how much ad-
ditional savings it offers over other methods. In Ta-
ble 5, we show how much of the phrase table we
have to retain under various pruning criteria with-
out losing more than one Bleu point in translation
quality. We see that probability-based pruning al-
lows only for marginal savings. Count-based and
significance-based pruning results in larger savings
between 70% and 90%, albeit with fairly high vari-
6The values are in neg-log-space, i. e., a value of 10 corre-
sponds to pc = e?10.
ability. Entropy-based pruning achieves consistently
high savings between 85% and 95% of the phrase ta-
ble. It always outperforms the other pruning meth-
ods and yields significant savings on top of count-
based or significance-based pruning methods. Of-
ten, we can cut the required phrase table size in half
compared to count or significance based pruning.
As a last experiment, we want to confirm that
phrase-table pruning methods are actually better
than simply reducing the maximum phrase length.
In Figure 9, we show a comparison of different
pruning methods and a length-based approach for
Spanish-English. For the ?Length? curve, we first
drop all 6-word phrases, then all 5-word phrases, etc.
until we are left with only single-word phrases; the
phrase length is measured as the number of source
language words. We observe that entropy-based,
count-based and significance-based pruning indeed
outperform the length-based approach. We obtained
similar results for the other languages.
980
Method ES-EN EN-ES DE-EN EN-DE FR-EN EN-FR CS-EN EN-CS
Prob 77.3 % 82.7 % 61.2 % 67.3 % 84.8 % 94.1 % 85.6 % 86.3 %
Count 24.9 % 11.9 % 19.9 % 14.3 % 11.4 % 9.0 % 20.2 % 10.4 %
Fisher 23.5 % 12.6 % 21.7 % 14.0 % 14.5 % 13.6 % 31.9 % 9.9 %
Entropy 7.2 % 6.0 % 10.2 % 11.1 % 7.1 % 8.1 % 14.8 % 6.4 %
Table 5: To what degree can we prune the phrase table without losing more than 1 Bleu point? The table shows
percentage of phrases that we have to retain. ES=Spanish, EN=English, FR=French, CS=Czech, DE=German.
 24
 26
 28
 30
 32
 34
 36
 38
 0.01  0.1  1  10  100
BL
EU
[%]
Number of Phrases [M]
5101520253050
Figure 8: Translation quality (Bleu) as a function of the
phrase table size for Spanish-English for entropy pruning
with different constants pc.
7 Conclusions
Phrase table pruning is often addressed in an ad-hoc
way using the heuristics described in Section 3. We
have shown that some of those do not work well.
Choosing the wrong technique can result in sig-
nificant drops in translation quality without saving
much in terms of phrase table size. We introduced
a novel entropy-based criterion and put phrase ta-
ble pruning on a sound theoretical foundation. Fur-
thermore, we performed a systematic experimental
comparison of existing methods and the new entropy
criterion. The experiments were carried out for four
language pairs under small, medium and large data
conditions. We can summarize our conclusions as
follows:
? Probability-based pruning performs poorly
when pruning large parts of the phrase table.
This might be because it does not take the fre-
quency of the source phrase into account.
? Count-based pruning performs as well as
 18
 20
 22
 24
 26
 28
 30
 32
 34
 36
 38
 0.01  0.1  1  10  100
BL
EU
[%]
Number of Phrases [M]
LengthProbCountFisherEntropy
Figure 9: Translation quality (Bleu) as a function of the
phrase table size for Spanish-English.
significance-based pruning.
? Entropy-based pruning gives significantly
larger savings in phrase table size than any
other pruning method.
? Compared to previous work, the novel entropy-
based pruning often achieves the same Bleu
score with only half the number of phrases.
8 Future Work
Currently, we take only the model p(e?|f?) into ac-
count when looking for the best segmentation. We
might obtain a better estimate by also consider-
ing the distortion costs, which penalize reordering.
We could also include other phrase models such as
p(f? |e?) and the language model.
The entropy pruning criterion could be applied
to hierarchical machine translation systems (Chiang,
2007). Here, we might observe even larger reduc-
tions in phrase table size as there are many more en-
tries.
981
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 858?
867, Prague, Czech Republic, June. Association for
Computational Linguistics.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
Yu Chen, Andreas Eisele, and Martin Kay. 2008.
Improving statistical machine translation efficiency
by triangulation. In Proceedings of the Sixth In-
ternational Conference on Language Resources and
Evaluation (LREC?08), Marrakech, Morocco, May.
European Language Resources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.
Yu Chen, Martin Kay, and Andreas Eisele. 2009. In-
tersecting multilingual data for faster and better sta-
tistical translations. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 128?136, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228,
June.
Thomas M. Cover and Joy A. Thomas. 2006. Elements
of information theory. Wiley-Interscience, New York,
NY, USA.
Nan Duan, Mu Li, and Ming Zhou. 2011. Improving
phrase extraction via MBR phrase scoring and prun-
ing. In Proceedings of MT Summit XIII, pages 189?
197, Xiamen, China, September.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2007a.
Estimating phrase pair relevance for machine transla-
tion pruning. In Proceedings of MT Summit XI, pages
159?165, Copenhagen, Denmark, September.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2007b.
Translation model pruning via usage statistics for sta-
tistical machine translation. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Companion Volume, Short Papers,
pages 21?24, Rochester, New York, April. Association
for Computational Linguistics.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975, Prague, Czech Republic, June. Association for
Computational Linguistics.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics An-
nual Meeting (HLT-NAACL), pages 127?133, Edmon-
ton, Canada, May/June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Constan-
tine, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In 45th An-
nual Meeting of the Assoc. for Computational Linguis-
tics (ACL): Poster Session, pages 177?180, Prague,
Czech Republic, June.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In 6th Conf. of the Assoc. for Machine Translation
in the Americas (AMTA), pages 115?124, Washington
DC, September/October.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum error
rate training for statistical machine translation. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 725?734,
Honolulu, HI, October. Association for Computational
Linguistics.
Robert C. Moore. 2004. On log-likelihood-ratios and
the significance of rare events. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing, pages 333?340.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449, De-
cember.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In 41st Annual Meet-
ing of the Assoc. for Computational Linguistics (ACL),
pages 160?167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In 40th Annual Meeting of
982
the Assoc. for Computational Linguistics (ACL), pages
311?318, Philadelphia, PA, July.
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
258?267, Portland, Oregon, USA, June. Association
for Computational Linguistics.
German Sanchis-Trilles, Daniel Ortiz-Martinez, Je-
sus Gonzalez-Rubio, Jorge Gonzalez, and Francisco
Casacuberta. 2011. Bilingual segmentation for
phrasetable pruning in statistical machine translation.
In Proceedings of the 15th Conference of the European
Association for Machine Translation, pages 257?264,
Leuven, Belgium, May.
Andreas Stolcke. 1998. Entropy-based pruning of back-
off language models. In Proc. DARPA Broadcast News
Transcription and Understanding Workshop, pages
270?274.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom filter language models: Tera-scale LMs on the
cheap. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 468?476, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
Nadi Tomeh, Nicola Cancedda, and Marc Dymetman.
2009. Complexity-based phrase-table filtering for sta-
tistical machine translation. In Proceedings of MT
Summit XII, Ottawa, Ontario, Canada, August.
Nadi Tomeh, Marco Turchi, Guillaume Wisniewski,
Alexandre Allauzen, and Franc?ois Yvon. 2011. How
good are your phrases? Assessing phrase quality with
single class classification. In Proceedings of the Inter-
national Workshop on Spoken Language Translation,
pages 261?268, San Francisco, California, December.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In 16th Int. Conf. on Computational Linguistics
(COLING), pages 836?841, Copenhagen, Denmark,
August.
Mei Yang and Jing Zheng. 2009. Toward smaller, faster,
and better hierarchical phrase-based SMT. In Pro-
ceedings of the ACL-IJCNLP 2009 Conference Short
Papers, pages 237?240, Suntec, Singapore, August.
Association for Computational Linguistics.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Human Language Technology Conf. / North Ameri-
can Chapter of the Assoc. for Computational Linguis-
tics Annual Meeting (HLT-NAACL): Workshop on Sta-
tistical Machine Translation, pages 55?63, New York
City, NY, June.
Richard Zens and Hermann Ney. 2008. Improvements in
dynamic programming beam search for phrase-based
statistical machine translation. In Proceedings of the
International Workshop on Spoken Language Transla-
tion, pages 195?205, Honolulu, Hawaii, October.
Richard Zens, Franz Josef Och, and Hermann Ney.
2002. Phrase-based statistical machine translation. In
M. Jarke, J. Koehler, and G. Lakemeyer, editors, 25th
German Conf. on Artificial Intelligence (KI2002), vol-
ume 2479 of Lecture Notes in Artificial Intelligence
(LNAI), pages 18?32, Aachen, Germany, September.
Springer Verlag.
Richard Zens, Evgeny Matusov, and Hermann Ney.
2004. Improved word alignment using a symmetric
lexicon model. In 20th Int. Conf. on Computational
Linguistics (COLING), pages 36?42, Geneva, Switzer-
land, August.
983
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 28?32,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Fast and Scalable Decoding with Language Model Look-Ahead
for Phrase-based Statistical Machine Translation
Joern Wuebker, Hermann Ney
Human Language Technology
and Pattern Recognition Group
Computer Science Department
RWTH Aachen University, Germany
surname@cs.rwth-aachen.de
Richard Zens*
Google, Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94043
zens@google.com
Abstract
In this work we present two extensions to
the well-known dynamic programming beam
search in phrase-based statistical machine
translation (SMT), aiming at increased effi-
ciency of decoding by minimizing the number
of language model computations and hypothe-
sis expansions. Our results show that language
model based pre-sorting yields a small im-
provement in translation quality and a speedup
by a factor of 2. Two look-ahead methods are
shown to further increase translation speed by
a factor of 2 without changing the search space
and a factor of 4 with the side-effect of some
additional search errors. We compare our ap-
proach with Moses and observe the same per-
formance, but a substantially better trade-off
between translation quality and speed. At a
speed of roughly 70 words per second, Moses
reaches 17.2% BLEU, whereas our approach
yields 20.0% with identical models.
1 Introduction
Research efforts to increase search efficiency for
phrase-based MT (Koehn et al, 2003) have ex-
plored several directions, ranging from generalizing
the stack decoding algorithm (Ortiz et al, 2006) to
additional early pruning techniques (Delaney et al,
2006), (Moore and Quirk, 2007) and more efficient
language model (LM) querying (Heafield, 2011).
This work extends the approach by (Zens and
Ney, 2008) with two techniques to increase trans-
lation speed and scalability. We show that taking
a heuristic LM score estimate for pre-sorting the
phrase translation candidates has a positive effect on
both translation quality and speed. Further, we intro-
duce two novel LM look-ahead methods. The idea
of LM look-ahead is to incorporate the LM proba-
bilities into the pruning process of the beam search
as early as possible. In speech recognition it has
been used for many years (Steinbiss et al, 1994;
Ortmanns et al, 1998). First-word LM look-ahead
exploits the search structure to use the LM costs of
the first word of a new phrase as a lower bound for
the full LM costs of the phrase. Phrase-only LM
look-ahead makes use of a pre-computed estimate
of the full LM costs for each phrase. We detail the
implementation of these methods and analyze their
effect with respect to the number of LM computa-
tions and hypothesis expansions as well as on trans-
lation speed and quality. We also run comparisons
with the Moses decoder (Koehn et al, 2007), which
yields the same performance in BLEU, but is outper-
formed significantly in terms of scalability for faster
translation. Our implementation is available under
a non-commercial open source licence?.
2 Search Algorithm Extensions
We apply the decoding algorithm described in (Zens
and Ney, 2008). Hypotheses are scored by a
weighted log-linear combination of models. A beam
search strategy is used to find the best hypothesis.
During search we perform pruning controlled by the
parameters coverage histogram size? Nc and lexical
?Richard Zens?s contribution was during his time at RWTH.
?www-i6.informatik.rwth-aachen.de/jane
?number of hypothesized coverage vectors per cardinality
28
histogram size? Nl .
2.1 Phrase candidate pre-sorting
In addition to the source sentence f J1 , the beam
search algorithm takes a matrix E(?, ?) as input,
where for each contiguous phrase f? = f j . . . f j?
within the source sentence, E( j, j?) contains a list of
all candidate translations for f? . The candidate lists
are sorted according to their model score, which was
observed to speed up translation by Delaney et al
(2006). In addition to sorting according to the purely
phrase-internal scores, which is common practice,
we compute an estimate qLME(e?) for the LM score
of each target phrase e?. qLME(e?) is the weighted
LM score we receive by assuming e? to be a com-
plete sentence without using sentence start and end
markers. We limit the number of translation options
per source phrase to the No top scoring candidates
(observation histogram pruning).
The pre-sorting during phrase matching has two
effects on the search algorithm. Firstly, it defines
the order in which the hypothesis expansions take
place. As higher scoring phrases are considered first,
it is less likely that already created partial hypothe-
ses will have to be replaced, thus effectively reduc-
ing the expected number of hypothesis expansions.
Secondly, due to the observation pruning the sorting
affects the considered phrase candidates and conse-
quently the search space. A better pre-selection can
be expected to improve translation quality.
2.2 Language Model Look-Ahead
LM score computations are among the most expen-
sive in decoding. Delaney et al (2006) report signif-
icant improvements in runtime by removing unnec-
essary LM lookups via early pruning. Here we de-
scribe an LM look-ahead technique, which is aimed
at further reducing the number of LM computations.
The innermost loop of the search algorithm iter-
ates over all translation options for a single source
phrase to consider them for expanding the current
hypothesis. We introduce an LM look-ahead score
qLMLA(e?|e??), which is computed for each of the
translation options. This score is added to the over-
all hypothesis score, and if the pruning threshold is
?number of lexical hypotheses per coverage vector
exceeded, we discard the expansion without com-
puting the full LM score.
First-word LM look-ahead pruning defines the
LM look-ahead score qLMLA(e?|e??) = qLM(e?1|e??) to
be the LM score of the first word of target phrase e?
given history e??. As qLM(e?1|e??) is an upper bound for
the full LM score, the technique does not introduce
additional seach errors. The score can be reused, if
the LM score of the full phrase e? needs to be com-
puted afterwards.
We can exploit the structure of the search to speed
up the LM lookups for the first word. The LM prob-
abilities are stored in a trie, where each node cor-
responds to a specific LM history. Usually, each
LM lookup consists of first traversing the trie to find
the node corresponding to the current LM history
and then retrieving the probability for the next word.
If the n-gram is not present, we have to repeat this
procedure with the next lower-order history, until a
probability is found. However, the LM history for
the first words of all phrases within the innermost
loop of the search algorithm is identical. Just be-
fore the loop we can therefore traverse the trie once
for the current history and each of its lower order n-
grams and store the pointers to the resulting nodes.
To retrieve the LM look-ahead scores, we can then
directly access the nodes without the need to traverse
the trie again. This implementational detail was con-
firmed to increase translation speed by roughly 20%
in a short experiment.
Phrase-only LM look-ahead pruning defines the
look-ahead score qLMLA(e?|e??) = qLME(e?) to be the
LM score of phrase e?, assuming e? to be the full sen-
tence. It was already used for sorting the phrases,
is therefore pre-computed and does not require ad-
ditional LM lookups. As it is not a lower bound for
the real LM score, this pruning technique can intro-
duce additional search errors. Our results show that
it radically reduces the number of LM lookups.
3 Experimental Evaluation
3.1 Setup
The experiments are carried out on the
German?English task provided for WMT 2011?.
?http://www.statmt.org/wmt11
29
system BLEU[%] #HYP #LM w/s
No = ?
baseline 20.1 3.0K 322K 2.2
+pre-sort 20.1 2.5K 183K 3.6
No = 100
baseline 19.9 2.3K 119K 7.1
+pre-sort 20.1 1.9K 52K 15.8
+first-word 20.1 1.9K 40K 31.4
+phrase-only 19.8 1.6K 6K 69.2
Table 1: Comparison of the number of hypothesis expan-
sions per source word (#HYP) and LM computations per
source word (#LM) with respect to LM pre-sorting, first-
word LM look-ahead and phrase-only LM look-ahead on
newstest2009. Speed is given in words per second.
Results are given with (No = 100) and without (No = ?)
observation pruning.
The English language model is a 4-gram LM
created with the SRILM toolkit (Stolcke, 2002) on
all bilingual and parts of the provided monolingual
data. newstest2008 is used for parameter
optimization, newstest2009 as a blind test
set. To confirm our results, we run the final set of
experiments also on the English?French task of
IWSLT 2011?. We evaluate with BLEU (Papineni et
al., 2002) and TER (Snover et al, 2006).
We use identical phrase tables and scaling fac-
tors for Moses and our decoder. The phrase table
is pruned to a maximum of 400 target candidates per
source phrase before decoding. The phrase table and
LM are loaded into memory before translating and
loading time is eliminated for speed measurements.
3.2 Methodological analysis
To observe the effect of the proposed search al-
gorithm extensions, we ran experiments with fixed
pruning parameters, keeping track of the number of
hypothesis expansions and LM computations. The
LM score pre-sorting affects both the set of phrase
candidates due to observation histogram pruning and
the order in which they are considered. To sepa-
rate these effects, experiments were run both with
histogram pruning (No = 100) and without. From
Table 1 we can see that in terms of efficiency both
cases show similar improvements over the baseline,
?http://iwslt2011.org
 16
 17
 18
 19
 20
 1  4  16  64  256  1024  4096
B
L
E
U
[
%
]
words/sec
Mosesbaseline
+pre-sort
+first-word
+phrase-only
Figure 1: Translation performance in BLEU [%] on the
newstest2009 set vs. speed on a logarithmic scale.
We compare Moses with our approach without LM look-
ahead and LM score pre-sorting (baseline), with added
LM pre-sorting and with either first-word or phrase-only
LM look-ahead on top of +pre-sort. Observation his-
togram size is fixed to No = 100 for both decoders.
which performs pre-sorting with respect to the trans-
lation model scores only. The number of hypothesis
expansions is reduced by ?20% and the number of
LM lookups by ?50%. When observation pruning
is applied, we additionally observe a small increase
by 0.2% in BLEU.
Application of first-word LM look-ahead further
reduces the number of LM lookups by 23%, result-
ing in doubled translation speed, part of which de-
rives from fewer trie node searches. The heuristic
phrase-only LM look-ahead method introduces ad-
ditional search errors, resulting in a BLEU drop by
0.3%, but yields another 85% reduction in LM com-
putations and increases throughput by a factor of 2.2.
3.3 Performance evaluation
In this section we evaluate the proposed extensions
to the original beam search algorithm in terms of
scalability and their usefulness for different appli-
cation constraints. We compare Moses and four dif-
ferent setups of our decoder: LM score pre-sorting
switched on or off without LM look-ahead and both
LM look-ahead methods with LM score pre-sorting.
We translated the test set with the beam sizes set to
Nc = Nl = {1,2,4,8,16,24,32,48,64}. For Moses
we used the beam sizes 2i, i ? {1, . . . ,9}. Transla-
30
setup system WMT 2011 German?English IWSLT 2011 English?French
beam size speed BLEU TER beam size speed BLEU TER
(Nc,Nl) w/s [%] [%] (Nc,Nl) w/s [%] [%]
best Moses 256 0.7 20.2 63.2 16 10 29.5 52.8
this work: first-word (48,48) 1.1 20.2 63.3 (8,8) 23 29.5 52.9
phrase-only (64,64) 1.4 20.1 63.2 (16,16) 18 29.5 52.8
BLEU: Moses 16 12 19.6 63.7 4 40 29.1 53.2
? -1% this work: first-word (4,4) 67 20.0 63.2 (2,2) 165 29.1 53.1
phrase-only (8,8) 69 19.8 63.0 (4,4) 258 29.3 52.9
BLEU: Moses 8 25 19.1 64.2 2 66 28.1 54.3
? -2% this work: first-word (2,2) 233 19.5 63.4 (1,1) 525 28.4 53.9
phrase-only (4,4) 280 19.3 63.0 (2,2) 771 28.5 53.2
fastest Moses 1 126 15.6 68.3 1 116 26.7 55.9
this work: first-word (1,1) 444 18.4 64.6 (1,1) 525 28.4 53.9
phrase-only (1,1) 2.8K 16.8 64.4 (1,1) 2.2K 26.4 54.7
Table 2: Comparison of Moses with this work. Either first-word or phrase-only LM look-ahead is applied. We consider
both the best and the fastest possible translation, as well as the fastest settings resulting in no more than 1% and 2%
BLEU loss on the development set. Results are given on the test set (newstest2009).
tion performance in BLEU is plotted against speed
in Figure 1. Without the proposed extensions, Moses
slightly outperforms our decoder in terms of BLEU.
However, the latter already scales better for higher
speed. With LM score pre-sorting, the best BLEU
value is similar to Moses while further accelerat-
ing translation, yielding identical performance at 16
words/sec as Moses at 1.8 words/sec. Application
of first-word LM look-ahead shifts the graph to the
right, now reaching the same performance at 31
words/sec. At a fixed translation speed of roughly
70 words/sec, our approach yields 20.0% BLEU,
whereas Moses reaches 17.2%. For phrase-only LM
look-ahead the graph is somewhat flatter. It yields
nearly the same top performance with an even better
trade-off between translation quality and speed.
The final set of experiments is performed on both
the WMT and the IWSLT task. We directly com-
pare our decoder with the two LM look-ahead meth-
ods with Moses in four scenarios: the best possi-
ble translation, the fastest possible translation with-
out performance constraint and the fastest possible
translation with no more than 1% and 2% loss in
BLEU on the dev set compared to the best value.
Table 2 shows that on the WMT data, the top per-
formance is similar for both decoders. However, if
we allow for a small degradation in translation per-
formance, our approaches clearly outperform Moses
in terms of translation speed. With phrase-only LM
look-ahead, our decoder is faster by a factor of 6
for no more than 1% BLEU loss, a factor of 11 for
2% BLEU loss and a factor of 22 in the fastest set-
ting. The results on the IWSLT data are very similar.
Here, the speed difference reaches a factor of 19 in
the fastest setting.
4 Conclusions
This work introduces two extensions to the well-
known beam search algorithm for phrase-based ma-
chine translation. Both pre-sorting the phrase trans-
lation candidates with an LM score estimate and LM
look-ahead during search are shown to have a pos-
itive effect on translation speed. We compare our
decoder to Moses, reaching a similar highest BLEU
score, but clearly outperforming it in terms of scal-
ability with respect to the trade-off ratio between
translation quality and speed. In our experiments,
the fastest settings of our decoder and Moses differ
in translation speed by a factor of 22 on the WMT
data and a factor of 19 on the IWSLT data. Our soft-
ware is part of the open source toolkit Jane.
Acknowledgments
This work was partially realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for innovation.
31
References
[Delaney et al2006] Brian Delaney, Wade Shen, and
Timothy Anderson. 2006. An efficient graph search
decoder for phrase-based statistical machine transla-
tion. In International Workshop on Spoken Language
Translation, Kyoto, Japan, November.
[Heafield2011] Kenneth Heafield. 2011. KenLM: Faster
and Smaller Language Model Queries. In Proceedings
of the 6th Workshop on Statistical Machine Transla-
tion, pages 187?197, Edinburgh, Scotland, UK, July.
[Koehn et al2003] P. Koehn, F. J. Och, and D. Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Meeting of the North American
chapter of the Association for Computational Linguis-
tics (NAACL-03), pages 127?133, Edmonton, Alberta.
[Koehn et al2007] Philipp Koehn, Hieu Hoang, Alexan-
dra Birch, Chris Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondr?ej Bo-
jar, Alexandra Constantine, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Machine
Translation. In Annual Meeting of the Association for
Computational Linguistics (ACL), demonstration ses-
sion, pages 177?180, Prague, Czech Republic, June.
[Moore and Quirk2007] Robert C. Moore and Chris
Quirk. 2007. Faster beam-search decoding for phrasal
statistical machine translation. In Proceedings of MT
Summit XI.
[Ortiz et al2006] Daniel Ortiz, Ismael Garcia-Varea, and
Francisco Casacuberta. 2006. Generalized stack de-
coding algorithms for statistical machine translation.
In Proceedings of the Workshop on Statistical Machine
Translation, pages 64?71, New York City, June.
[Ortmanns et al1998] S. Ortmanns, H. Ney, and A. Ei-
den. 1998. Language-model look-ahead for large vo-
cabulary speech recognition. In International Confer-
ence on Spoken Language Processing, pages 2095?
2098, Sydney, Australia, October.
[Papineni et al2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method
for Automatic Evaluation of Machine Translation. In
Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 311?318,
Philadelphia, Pennsylvania, USA, July.
[Snover et al2006] Matthew Snover, Bonnie Dorr,
Richard Schwartz, Linnea Micciulla, and John
Makhoul. 2006. A Study of Translation Edit Rate
with Targeted Human Annotation. In Proceedings
of the 7th Conference of the Association for Ma-
chine Translation in the Americas, pages 223?231,
Cambridge, Massachusetts, USA, August.
[Steinbiss et al1994] V. Steinbiss, B. Tran, and Hermann
Ney. 1994. Improvements in Beam Search. In Proc.
of the Int. Conf. on Spoken Language Processing (IC-
SLP?94), pages 2143?2146, September.
[Stolcke2002] Andreas Stolcke. 2002. SRILM ? An
Extensible Language Modeling Toolkit. In Proceed-
ings of the Seventh International Conference on Spoken
Language Processing, pages 901?904. ISCA, Septem-
ber.
[Zens and Ney2008] Richard Zens and Hermann Ney.
2008. Improvements in Dynamic Programming Beam
Search for Phrase-based Statistical Machine Transla-
tion. In International Workshop on Spoken Language
Translation, pages 195?205, Honolulu, Hawaii, Octo-
ber.
32
