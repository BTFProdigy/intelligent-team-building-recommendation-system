Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 209?216, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Minimum Sample Risk Methods for Language Modeling1  
Jianfeng Gao  
Microsoft Research Asia 
jfgao@microsoft.com 
Hao Yu,  Wei Yuan 
 Shanghai Jiaotong Univ., China 
Peng Xu 
John Hopkins Univ., U.S.A. 
xp@clsp.jhu.edu 
                                                     
1
  The work was done while the second, third and fourth authors were visiting Microsoft Research Asia. Thanks to Hisami Suzuki for 
her valuable comments. 
Abstract 
This paper proposes a new discriminative 
training method, called minimum sample risk 
(MSR), of estimating parameters of language 
models for text input. While most existing 
discriminative training methods use a loss 
function that can be optimized easily but 
approaches only approximately to the objec-
tive of minimum error rate, MSR minimizes 
the training error directly using a heuristic 
training procedure. Evaluations on the task 
of Japanese text input show that MSR can 
handle a large number of features and train-
ing samples; it significantly outperforms a 
regular trigram model trained using maxi-
mum likelihood estimation, and it also out-
performs the two widely applied discrimi-
native methods, the boosting and the per-
ceptron algorithms, by a small but statisti-
cally significant margin. 
1 Introduction 
Language modeling (LM) is fundamental to a wide 
range of applications, such as speech recognition 
and Asian language text input (Jelinek 1997; Gao et 
al. 2002). The traditional approach uses a paramet-
ric model with maximum likelihood estimation (MLE), 
usually with smoothing methods to deal with data 
sparseness problems. This approach is optimal 
under the assumption that the true distribution of 
data on which the parametric model is based is 
known. Unfortunately, such an assumption rarely 
holds in realistic applications. 
An alternative approach to LM is based on the 
framework of discriminative training, which uses a 
much weaker assumption that training and test 
data are generated from the same distribution but 
the form of the distribution is unknown. Unlike the 
traditional approach that maximizes the function 
(i.e. likelihood of training data) that is loosely as-
sociated with error rate, discriminative training 
methods aim to directly minimize the error rate on 
training data even if they reduce the likelihood. So, 
they potentially lead to better solutions. However, 
the error rate of a finite set of training samples is 
usually a step function of model parameters, and 
cannot be easily minimized. To address this prob-
lem, previous research has concentrated on the 
development of a loss function that approximates 
the exact error rate and can be easily optimized. 
Though these methods (e.g. the boosting method) 
have theoretically appealing properties, such as 
convergence and bounded generalization error, we 
argue that the approximated loss function may 
prevent them from attaining the original objective 
of minimizing the error rate. 
In this paper we present a new estimation pro-
cedure for LM, called minimum sample risk (MSR). It 
differs from most existing discriminative training 
methods in that instead of searching on an ap-
proximated loss function, MSR employs a simple 
heuristic training algorithm that minimizes the 
error rate on training samples directly. MSR oper-
ates like a multidimensional function optimization 
algorithm: first, it selects a subset of features that 
are the most effective among all candidate features. 
The parameters of the model are then optimized 
iteratively: in each iteration, only the parameter of 
one feature is adjusted. Both feature selection and 
parameter optimization are based on the criterion 
of minimizing the error on training samples. Our 
evaluation on the task of Japanese text input shows 
that MSR achieves more than 20% error rate reduc-
tion over MLE on two newswire data sets, and it 
also outperforms the other two widely applied 
discriminative methods, the boosting method and 
the perceptron algorithm, by a small but statisti-
cally significant margin. 
Although it has not been proved in theory that 
MSR is always robust, our experiments of cross- 
domain LM adaptation show that it is. MSR can 
effectively adapt a model trained on one domain to 
209
different domains. It outperforms the traditional 
LM adaptation method significantly, and achieves 
at least comparable or slightly better results to the 
boosting method and the perceptron algorithm. 
2 IME Task and LM 
This paper studies LM on the task of Asian lan-
guage (e.g. Chinese or Japanese) text input. This is 
the standard method of inputting Chinese or 
Japanese text by converting the input phonetic 
symbols into the appropriate word string. In this 
paper we call the task IME, which stands for input 
method editor, based on the name of the commonly 
used Windows-based application. 
Performance on IME is measured in terms of the 
character error rate (CER), which is the number of 
characters wrongly converted from the phonetic 
string divided by the number of characters in the 
correct transcript. Current IME systems make 
about 5-15% CER in conversion of real data in a 
wide variety of domains (e.g. Gao et al 2002).  
Similar to speech recognition, IME is viewed as 
a Bayes decision problem. Let A be the input pho-
netic string. An IME system?s task is to choose the 
most likely word string W* among those candidates 
that could be converted from A: 
)|()(maxarg)|(maxarg
(A))(
* WAPWPAWPW
WAW GENGEN ??
==  (1) 
where GEN(A) denotes the candidate set given A. 
Unlike speech recognition, however, there is no 
acoustic ambiguity since the phonetic string is 
inputted by users. Moreover, if we do not take into 
account typing errors, it is reasonable to assume a  
unique mapping from W and A in IME, i.e. P(A|W) 
= 1. So the decision of Equation (1) depends solely 
upon P(W), making IME a more direct evaluation 
test bed for LM than speech recognition. Another 
advantage is that it is easy to convert W to A (for 
Chinese and Japanese), which enables us to obtain 
a large number of training data for discriminative 
learning, as described later.  
The values of P(W) in Equation (1) are tradi-
tionally calculated by MLE: the optimal model 
parameters ?* are chosen in such a way that 
P(W|?*) is maximized on training data. The argu-
ments in favor of MLE are based on the assumption 
that the form of the underlying distributions is 
known, and that only the values of the parameters 
characterizing those distributions are unknown. In 
using MLE for LM, one always assumes a multi-
nomial distribution of language. For example, a 
trigram model makes the assumption that the next 
word is predicted depending only on two preced-
ing words. However, there are many cases in 
natural language where words over an arbitrary 
distance can be related. MLE is therefore not opti-
mal because the assumed model form is incorrect. 
What are the best estimators when the model is 
known to be false then? In IME, we can tackle this 
question empirically. Best IME systems achieve the 
least CER. Therefore, the best estimators are those 
which minimize the expected error rate on unseen 
test data. Since the distribution of test data is un-
known, we can approximately minimize the error 
rate on some given training data (Vapnik 1999). 
Toward this end, we have developed a very simple 
heuristic training procedure called minimum sample 
risk, as presented in the next section. 
3 Minimum Sample Risk 
3.1 Problem Definition 
We follow the general framework of linear dis-
criminant models described in (Duda et al 2001). In 
the rest of the paper we use the following notation, 
adapted from Collins (2002). 
? Training data is a set of example input/output 
pairs. In LM for IME, training samples are repre-
sented as {Ai, WiR}, for i = 1?M, where each Ai is an 
input phonetic string and WiR is the reference tran-
script of Ai. 
? We assume some way of generating a set of 
candidate word strings given A, denoted by 
GEN(A).  In our experiments, GEN(A) consists of 
top N word strings converted from A using a base-
line IME system that uses only a word trigram 
model. 
? We assume a set of D+1 features fd(W), for d = 
0?D. The features could be arbitrary functions that 
map W to real values. Using vector notation, we 
have f(W)??D+1, where f(W) = [f0(W), f1(W), ?, 
fD(W)]T. Without loss of generality, f0(W) is called 
the base feature, and is defined in our case as the 
log probability that the word trigram model as-
signs to W. Other features (fd(W), for d = 1?D) are 
defined as the counts of word n-grams (n = 1 and 2 
in our experiments) in W. 
? Finally, the parameters of the model form a 
vector of D+1 dimensions, each for one feature 
function, ? = [?0, ?1, ?, ?D]. The score of a word 
string W can be written as  
210
)(),( WWScore ?f? = ?
=
=
D
d
dd Wf?
0
)( . (2)
The decision rule of Equation (1) is rewritten as 
),(maxarg),(
(A)
* ??
GEN
WScoreAW
W?
= . (3)
Equation (3) views IME as a ranking problem, 
where the model gives the ranking score, not 
probabilities. We therefore do not evaluate the 
model via perplexity. 
Now, assume that we can measure the number 
of conversion errors in W by comparing it with a 
reference transcript WR using an error function 
Er(WR,W) (i.e.  the string edit distance function in 
our case). We call the sum of error counts over the 
training samples sample risk. Our goal is to mini-
mize the sample risk while searching for the pa-
rameters as defined in Equation (4), hence the name 
minimum sample risk (MSR). Wi* in Equation (4) is 
determined by Equation (3), 
?
=
=
Mi
ii
R
i
def
MSR AWW
...1
* )),(,Er(minarg ??
?
. (4)
We first present the basic MSR training algorithm, 
and then the two improvements we made. 
3.2 Training Algorithm 
The MSR training algorithm is cast as a multidi-
mensional function optimization approach (Press 
et al 1992): taking the feature vector as a set of 
directions; the first direction (i.e. feature) is selected 
and the objective function (i.e. sample risk) is 
minimized along that direction using a line search; 
then from there along the second direction to its 
minimum, and so on, cycling through the whole set 
of directions as many times as necessary, until the 
objective function stops decreasing.  
This simple method can work properly under 
two assumptions. First, there exists an implemen-
tation of line search that optimizes the function 
along one direction efficiently. Second, the number 
of candidate features is not too large, and these 
features are not highly correlated. However, nei-
ther of the assumptions holds in our case. First of 
all, Er(.) in Equation (4) is a step function of ?, thus 
cannot be optimized directly by regular gradient- 
based procedures ? a grid search has to be used 
instead. However, there are problems with simple 
grid search: using a large grid could miss the op-
timal solution whereas using a fine-grained grid 
would lead to a very slow algorithm. Secondly, in 
the case of LM, there are millions of candidate 
features, some of which are highly correlated. We 
address these issues respectively in the next two 
subsections. 
3.3 Grid Line Search 
Our implementation of a grid search is a modified 
version of that proposed in (Och 2003). The modi-
fications are made to deal with the efficiency issue 
due to the fact that there is a very large number of 
features and training samples in our task, compared 
to only 8 features used in (Och 2003). Unlike a 
simple grid search where the intervals between any 
two adjacent grids are equal and fixed, we deter-
mine for each feature a sequence of grids with 
differently sized intervals, each corresponding to a 
different value of sample risk. 
As shown in Equation (4), the loss function (i.e. 
sample risk) over all training samples is the sum of 
the loss function (i.e. Er(.)) of each training sample. 
Therefore, in what follows, we begin with a discus-
sion on minimizing Er(.) of a training sample using 
the line search.  
Let ? be the current model parameter vector, 
and fd be the selected feature. The line search aims to 
find the optimal parameter ?d* so as to minimize 
Er(.). For a training sample (A, WR), the score of each 
candidate word string W?GEN(A), as in Equation 
(2), can be decomposed into two terms: 
)()()(),(
'0'
'' WfWfWWScore dd
D
ddd
dd ?? +== ?
??=
?f? , 
where the first term on the right hand side does not 
change with ?d. Note that if several candidate word 
strings have the same feature value fd(W), their 
relative rank will remain the same for any ?d. Since 
fd(W) takes integer values in our case (fd(W) is the 
count of a particular n-gram in W), we can group the 
candidates using fd(W) so that candidates in each 
group have the same value of fd(W). In each group, 
we define the candidate with the highest value of  
? ??=D ddd dd Wf'0' '' )(?  
as the active candidate of the group because no 
matter what value ?d takes, only this candidate 
could be selected according to Equation (3). 
Now, we reduce GEN(A) to a much smaller list 
of active candidates. We can find a set of intervals 
for ?d, within each of which a particular active 
candidate will be selected as W*. We can compute 
the Er(.) value of that candidate as the Er(.) value for 
the corresponding interval. As a result, for each 
211
training sample, we obtain a sequence of intervals 
and their corresponding Er(.) values. The optimal 
value ?d* can then be found by traversing the se-
quence and taking the midpoint of the interval with 
the lowest Er(.) value.  
305
306
307
308
309
310
311
312
0. 85 0. 9 0. 95 1 1.05 1.1 1. 15 1. 2lambda
SR
(.)
sample risk
smoothed sample risk
 
Figure 1. Examples of line search.  
This process can be extended to the whole 
training set as follows. By merging the sequence of 
intervals of each training sample in the training set, 
we obtain a global sequence of intervals as well as 
their corresponding sample risk. We can then find 
the optimal value ?d* as well as the minimal sample 
risk by traversing the global interval sequence. An 
example is shown in Figure 1. 
The line search can be unstable, however. In 
some cases when some of the intervals are very 
narrow (e.g. the interval A in Figure 1), moving the 
optimal value ?d* slightly can lead to much larger 
sample risk. Intuitively, we prefer a stable solution 
which is also known as a robust solution (with even 
slightly higher sample risk, e.g. the interval B in 
Figure 1). Following Quirk et al (2004), we evaluate 
each interval in the sequence by its corresponding 
smoothed sample risk. Let ? be the midpoint of an 
interval and SR(?) be the corresponding sample risk 
of the interval. The smoothed sample risk of the 
interval is defined as 
???? d
b
b
 )SR(? +?   
where b is a smoothing factor whose value is de-
termined empirically  (0.06 in our experiments). As 
shown in Figure 1, a more stable interval B is se-
lected according to the smoothed sample risk. 
In addition to reducing GEN(A) to an active 
candidate list described above, the efficiency of the 
line search can be further improved. We find that 
the line search only needs to traverse a small subset 
of training samples because the distribution of 
features among training samples are very sparse. 
Therefore, we built an inverted index that lists for 
each feature all training samples that contain it. As 
will be shown in Section 4.2, the line search is very 
efficient even for a large training set with millions of 
candidate features. 
3.4 Feature Subset Selection 
This section describes our method of selecting 
among millions of features a small subset of highly 
effective features for MSR learning. Reducing the 
number of features is essential for two reasons: to 
reduce computational complexity and to ensure the 
generalization property of the linear model. A large 
number of features lead to a large number of pa-
rameters of the resulting linear model, as described 
in Section 3.1. For a limited number of training 
samples, keeping the number of features suffi-
ciently small should lead to a simpler model that is 
less likely to overfit to the training data. 
The first step of our feature selection algorithm 
treats the features independently. The effectiveness 
of a feature is measured in terms of the reduction of 
the sample risk on top of the base feature f0. For-
mally, let SR(f0) be the sample risk of using the base 
feature only, and SR(f0 + ?dfd) be the sample risk of 
using both f0 and fd and the parameter ?d that has 
been optimized using the line search. Then the 
effectiveness of fd, denoted by E(fd), is given by 
))SR()(SR(max
)SR()SR(
)(
00
...1,
00
ii
Dif
dd
d fff
fff
fE
i
?
?
+?
+?=
=
, (5)
where the denominator is a normalization term to 
ensure that E(f) ? [0, 1]. 
The feature selection procedure can be stated as 
follows: The value of E(.) is computed according to 
Equation (5) for each of the candidate features. 
Features are then ranked in the order of descending 
values of E(.). The top l features are selected to form 
the feature vector in the linear model. 
Treating features independently has the ad-
vantage of computational simplicity, but may not 
be effective for features with high correlation. For 
instance, although two features may carry rich 
discriminative information when treated sepa-
rately, there may be very little gain if they are com-
bined in a feature vector, because of the high cor-
relation between them. Therefore, in what follows, 
we describe a technique of incorporating correla-
tion information in the feature selection criterion.  
Let xmd, m = 1?M and d = 1?D, be a Boolean 
value: xmd = 1 if the sample risk reduction of using 
the d-th feature on the m-th training sample, com-
B 
A
212
puted by Equation (5), is larger than zero, and 0 
otherwise. The cross correlation coefficient be-
tween two features fi and fj is estimated as 
??
?
==
==
M
m mj
M
m mi
M
m mjmi
xx
xx
jiC
1
2
1
2
1),( . (6)
It can be shown that C(i, j) ? [0, 1]. Now, similar to  
(Theodoridis and Koutroumbas 2003), the feature 
selection procedure consists of the following steps, 
where fi denotes any selected feature and fj denotes 
any candidate feature to be selected. 
Step 1. For each of the candidate features (fd, for d = 
1?D), compute the value of E(f) according to 
Equation (5). Rank them in a descending order and 
choose the one with the highest E(.) value. Let us 
denote this feature as f1. 
Step 2. To select the second feature, compute the 
cross correlation coefficient between the selected 
feature f1 and each of the remaining M-1 features, 
according to Equation (6). 
Step 3. Select the second feature f according to { } ),1()1()(maxarg*
...2
jCfEj j
Dj
?? ??=
=
 
where ? is the weight that determines the relative 
importance we give to the two terms. The value of 
? is optimized on held-out data (0.8 in our experi-
ments). This means that for the selection of the 
second feature, we take into account not only its 
impact of reducing the sample risk but also the 
correlation with the previously selected feature. It 
is expected that choosing features with less corre-
lation gives better sample risk minimization. 
Step 4. Select k-th features, k = 3?K, according to 
??
?
??
?
?
??= ??
=
1
1
),(
1
1
)(maxarg*
k
i
j
j
jiC
k
fEj
??  (7)
That is, we select the next feature by taking into 
account its average correlation with all previously 
selected features. The optimal number of features, l, 
is determined on held-out data. 
Similarly to the case of line search, we need to 
deal with the efficiency issue in the feature selec-
tion method. As shown in Equation (7), the esti-
mates of E(.) and C(.) need to be computed. Let D 
and K (K << D) be the number of all candidate 
features and the number of features in the resulting 
model, respectively. According to the feature se-
lection method described above, we need to esti-
mate E(.) for each of the D candidate features only 
once in Step 1. This is not very costly due to the 
efficiency of our line search algorithm. Unlike the 
case of E(.), O(K?D) estimates of C(.) are required in 
Step 4. This is computationally expensive even for a 
medium-sized K. Therefore, every time a new fea-
ture is selected (in Step 4), we only estimate the 
value of C(.) between each of the selected features 
and each of the top N remaining features with the 
highest value of E(.). This reduces the number of 
estimates of C(.) to O(K?N). In our experiments we 
set N = 1000, much smaller than D. This reduces the 
computational cost significantly without producing 
any noticeable quality loss in the resulting model. 
The MSR algorithm used in our experiments is 
summarized in Figure 2. It consists of feature se-
lection (line 2) and optimization (lines 3 - 5) steps. 
1 Set ?0 = 1 and ?d = 0 for d=1?D 
2 Rank all features and select the top K features, using 
the feature selection method described in Section 3.4
3 For t = 1?T (T= total number of iterations) 
4 For each k = 1?K  
5    Update the parameter of fk using line search.  
Figure 2: The MSR algorithm 
4 Evaluation 
4.1 Settings 
We evaluated MSR on the task of Japanese IME. 
Two newspaper corpora are used as training and 
test data: Nikkei and Yomiuri Newspapers. Both 
corpora have been pre-word-segmented using a 
lexicon containing 167,107 entries. A 5,000-sentence 
subset of the Yomiuri Newspaper corpus  was used 
as held-out data (e.g. to determine learning rate, 
number of iterations and features etc.). We tested 
our models on another  5,000-sentence subset of the 
Yomiuri Newspaper corpus.  
We used an 80,000-sentence subset of the Nikkei 
Newspaper corpus as the training set. For each A, 
we produced a word lattice using the baseline 
system described in (Gao et al 2002), which uses a 
word trigram model trained via MLE on anther 
400,000-sentence subset of the Nikkei Newspaper 
corpus. The two subsets do not overlap so as to 
simulate the case where unseen phonetic symbol 
strings are converted by the baseline system. For 
efficiency, we kept for each training sample the 
best 20 hypotheses in its candidate conversion set 
GEN(A) for discriminative training. The oracle best 
hypothesis, which gives the minimum number of 
errors, was used as the reference transcript of A. 
213
4.2 Results 
We used unigrams and bigrams that occurred more 
than once in the training set as features. We did not 
use trigram features because they did not result in a 
significant improvement in our pilot study. The 
total number of candidate features we used was 
around 860,000.  
Our main experimental results are shown in 
Table 1. Row 1 is our baseline result using the word 
trigram model. Notice that the result is much better 
than the state-of-the-art performance currently 
available in the marketplace (e.g. Gao et al 2002), 
presumably due to the large amount of training 
data we used, and to the similarity between the 
training and the test data. Row 2 is the result of the 
model trained using the MSR algorithm described 
in Section 3. We also compared the MSR algorithm 
to two of the state-of-the-art discriminative training 
methods: Boosting in Row 3 is an implementation 
of the improved algorithm for the boosting loss 
function proposed in (Collins 2000), and Percep-
tron in Row 4 is an implementation of the averaged 
perceptron algorithm described in (Collins 2002).  
We see that all discriminative training methods 
outperform MLE significantly (p-value < 0.01). In 
particular, MSR outperforms MLE by more than 
20% CER reduction. Notice that we used only uni-
gram and bigram features that have been included 
in the baseline trigram model, so the improvement 
is solely attributed to the high performance of MSR. 
We also find that MSR outperforms the perceptron 
and boosting methods by a small but statistically 
significant margin. 
The MSR algorithm is also very efficient: using a 
subset of 20,000 features, it takes less than 20 min-
utes to converge on an XEON(TM) MP 1.90GHz 
machine. It is as efficient as the perceptron algo-
rithm and slightly faster than the boosting method. 
4.3 Robustness Issues 
Most theorems that justify the robustness of dis-
criminative training algorithms concern two ques-
tions. First, is there a guarantee that a given algo-
rithm converges even if the training samples are 
not linearly separable? This is called the convergence 
problem. Second, how well is the training error 
reduction preserved when the algorithm is applied 
to unseen test samples? This is called the generali-
zation problem. Though we currently cannot give a 
theoretical justification, we present empirical evi-
dence here for the robustness of the MSR approach. 
As Vapnik (1999) pointed out, the most robust 
linear models are the ones that achieve the least 
training errors with the least number of features. 
Therefore, the robustness of the MSR algorithm are 
mainly affected by the feature selection method. To 
verify this, we created four different subsets of 
features using different settings of the feature se-
lection method described in Section 3.4. We se-
lected different numbers of features (i.e. 500 and 
2000) with and without taking into account the 
correlation between features (i.e. ? in Equation (7) 
is set to 0.8 and 1, respectively). For each of the four 
feature subsets, we used the MSR algorithm to 
generate a set of models. The CER curves of these 
models on training and test data sets are shown in 
Figures 3 and 4, respectively.  
2.08
2.10
2.12
2.14
2.16
2.18
2.20
2.22
2.24
2.26
2.28
1 250 500 750 1000 1250 1500 1750 2000
# of rounds
C
E
R
(%
)
MSR(?=1)-2000
MSR(?=1)-500
MSR(?=0.8)-2000
MSR(?=0.8)-500
 
Figure 3. Training error curves of the MSR algorithm 
2.94
2.99
3.04
3.09
3.14
3.19
3.24
1 250 500 750 1000 1250 1500 1750 2000
# of rounds
C
E
R
(%
)
MSR(?=1)-2000
MSR(?=1)-500
MSR(?=0.8)-2000
MSR(?=0.8)-500
 
Figure 4. Test error curves of the MSR algorithm 
The results reveal several facts. First, the con-
vergence properties of MSR are shown in Figure 3 
where in all cases, training errors drop consistently 
with more iterations. Secondly, as expected, using 
more features leads to overfitting, For example, 
MSR(? =1)-2000 makes fewer errors than MSR(? 
=1)-500 on training data but more errors on test 
data. Finally, taking into account the correlation 
between features (e.g. ? = 0.8 in Equation (7)) re-
 Model CER (%) % over MLE 
1. MLE  3.70 -- 
2. MSR (K=2000) 2.95 20.9 
3. Boosting  3.06 18.0 
4. Perceptron 3.07 17.8 
Table 1. Comparison of CER results. 
214
sults in a better subset of features that lead to not 
only fewer training errors, as shown in Figure 3, 
but also better generalization properties (fewer test 
errors), as shown in Figure 4. 
4.4 Domain Adaptation Results  
Though MSR achieves impressive performance in 
CER reduction over the comparison methods, as 
described in Section 4.2, the experiments are all 
performed using newspaper text for both training 
and testing, which is not a realistic scenario if we 
are to deploy the model in an application. This 
section reports the results of additional experi-
ments in which we adapt a model trained on one 
domain to a different domain, i.e., in a so-called 
cross-domain LM adaptation paradigm. See (Su-
zuki and Gao 2005) for a detailed report. 
The data sets we used stem from five distinct 
sources of text. The Nikkei newspaper corpus de-
scribed in Section 4.1 was used as the background 
domain, on which the word trigram model was 
trained. We used four adaptation domains: Yomi-
uri (newspaper corpus), TuneUp (balanced corpus 
containing newspapers and other sources of text), 
Encarta (encyclopedia) and Shincho (collection of 
novels). For each of the four domains, we used an 
72,000-sentence subset as adaptation training data, 
a 5,000-sentence subset as held-out data and an-
other 5,000-sentence subset as test data. Similarly, 
all corpora have been word-segmented, and we 
kept for each training sample, in the four adapta-
tion domains, the best 20 hypotheses in its candi-
date conversion set for discriminative training.  
We compared MSR with three other LM adap-
tation methods:  
Baseline is the background word trigram model, 
as described in Section 4.1. 
MAP (maximum a posteriori) is a traditional LM 
adaptation method where the parameters of the 
background model are adjusted in such a way that 
maximizes the likelihood of the adaptation data. 
Our implementation takes the form of linear in-
terpolation as P(wi|h) = ?Pb(wi|h) + (1-?)Pa(wi|h), 
where Pb is the probability of the background 
model, Pa is the probability trained on adaptation 
data using MLE and the history h corresponds to 
two preceding words (i.e. Pb and Pa are trigram 
probabilities). ? is the interpolation weight opti-
mized on held-out data.  
Perceptron, Boosting and MSR are the three 
discriminative methods described in the previous 
sections.  For each of them, the base feature was 
Model Yomiuri TuneUp Encarta Shincho 
Baseline 3.70 5.81 10.24 12.18 
MAP  3.69 5.47 7.98 10.76 
MSR  2.73 5.15 7.40 10.16 
Boosting  2.78 5.33 7.53 10.25 
Perceptron 2.78 5.20 7.44 10.18 
Table 2. CER(%) results on four adaptation test sets . 
derived from the word trigram model trained on 
the background data, and other n-gram features (i.e. 
fd, d = 1?D in Equation (2)) were trained on adap-
tation data. That is, the parameters of the back-
ground model are adjusted in such a way that 
minimizes the errors on adaptation data made by 
background model. 
Results are summarized in Table 2. First of all, 
in all four adaptation domains, discriminative 
methods outperform MAP significantly. Secondly, 
the improvement margins of discriminative 
methods over MAP correspond to the similarities 
between background domain and adaptation do-
mains. When the two domains are very similar to 
the background domain (such as Yomiuri), dis-
criminative methods outperform MAP by a large 
margin. However, the margin is smaller when the 
two domains are substantially different (such as 
Encarta and Shincho). The phenomenon is attrib-
uted to the underlying difference between the two 
adaptation methods: MAP aims to improve the 
likelihood of a distribution, so if the adaptation 
domain is very similar to the background domain, 
the difference between the two underlying distri-
butions is so small that MAP cannot adjust the 
model effectively. However, discriminative meth-
ods do not have this limitation for they aim to 
reduce errors directly. Finally, we find that in most 
adaptation test sets, MSR achieves slightly better 
CER results than the two competing discriminative 
methods. Specifically, the improvements of MSR 
are statistically significant over the boosting 
method in three out of four domains, and over the 
perceptron algorithm in the Yomiuri domain. The 
results demonstrate again that MSR is robust. 
5 Related Work 
Discriminative models have recently been proved 
to be more effective than generative models in 
some NLP tasks, e.g., parsing (Collins 2000), POS 
tagging (Collins 2002) and LM for speech recogni-
tion (Roark et al 2004). In particular, the linear 
models, though simple and non-probabilistic in 
nature, are preferred to their probabilistic coun-
215
terpart such as logistic regression. One of the rea-
sons, as pointed out by Ng and Jordan (2002), is 
that the parameters of a discriminative model can 
be fit either to maximize the conditional likelihood 
on training data, or to minimize the training errors. 
Since the latter optimizes the objective function that 
the system is graded on, it is viewed as being more 
truly in the spirit of discriminative learning. 
The MSR method shares the same motivation: to 
minimize the errors directly as much as possible. 
Because the error function on a finite data set is a 
step function, and cannot be optimized easily, 
previous research approximates the error function 
by loss functions that are suitable for optimization 
(e.g. Collins 2000; Freund et al 1998; Juang et al 
1997; Duda et al 2001). MSR uses an alternative 
approach. It is a simple heuristic training proce-
dure to minimize training errors directly without 
applying any approximated loss function. 
MSR shares many similarities with previous 
methods. The basic training algorithm described in 
Section 3.2 follows the general framework of multi- 
dimensional optimization (e.g., Press et al 1992). 
The line search is an extension of that described in 
(Och 2003; Quirk et al 2005. The extension lies in 
the way of handling large number of features and 
training samples. Previous algorithms were used to 
optimize linear models with less than 10 features. 
The feature selection method described in Section 
3.4 is a particular implementation of the feature 
selection methods described in (e.g., Theodoridis 
and Koutroumbas 2003). The major difference 
between the MSR and other methods is that it es-
timates the effectiveness of each feature in terms of 
its expected training error reduction while previ-
ous methods used metrics that are loosely coupled 
with reducing training errors. The way of dealing 
with feature correlations in feature selection in 
Equation (7), was suggested by Finette et al (1983). 
6 Conclusion and Future Work 
We show that MSR is a very successful discrimina-
tive training algorithm for LM. Our experiments 
suggest that it leads to significantly better conver-
sion performance on the IME task than either the 
MLE method or the two widely applied discrimi-
native methods, the boosting and perceptron 
methods. However, due to the lack of theoretical 
underpinnings, we are unable to prove that MSR 
will always succeed. This forms one area of our 
future work. 
One of the most interesting properties of MSR is 
that it can optimize any objective function (whether 
its gradient is computable or not), such as error rate 
in IME or speech, BLEU score in MT, precision and 
recall in IR (Gao et al 2005). In particular, MSR can 
be performed on large-scale training set with mil-
lions of candidate features. Thus, another area of 
our future work is to test MSR on wider varieties of 
NLP tasks such as parsing and tagging. 
References 
Collins, Michael. 2002. Discriminative training methods 
for Hidden Markov Models: theory and experiments 
with the perceptron algorithm. In EMNLP 2002. 
Collins, Michael. 2000. Discriminative reranking for 
natural language parsing. In ICML 2000. 
Duda, Richard O, Hart, Peter E. and Stork, David G. 2001. 
Pattern classification. John Wiley & Sons, Inc. 
Finette S., Blerer A., Swindel W. 1983. Breast tissue clas-
sification using diagnostic ultrasound and pattern rec-
ognition techniques: I. Methods of pattern recognition. 
Ultrasonic Imaging, Vol. 5, pp. 55-70. 
Freund, Y, R. Iyer, R. E. Schapire, and Y. Singer. 1998. An 
efficient boosting algorithm for combining preferences. 
In ICML?98.  
Gao, Jianfeng, Hisami Suzuki and Yang Wen. 2002. 
Exploiting headword dependency and predictive clus-
tering for language modeling. In EMNLP 2002. 
Gao, J, H. Qin, X. Xiao and J.-Y. Nie. 2005. Linear dis-
criminative model for information retrieval. In SIGIR.  
Jelinek, Fred. 1997. Statistical methods for speech recognition. 
MIT Press, Cambridge, Mass. 
Juang, B.-H., W.Chou and C.-H. Lee. 1997. Minimum 
classification error rate methods for speech recognition. 
IEEE Tran. Speech and Audio Processing 5-3: 257-265.  
Ng, A. N. and M. I. Jordan. 2002. On discriminative vs. 
generative classifiers: a comparison of logistic regres-
sion and na?ve Bayes. In NIPS 2002: 841-848. 
Och, Franz Josef. 2003. Minimum error rate training in 
statistical machine translation. In ACL 2003 
Press, W. H., S. A. Teukolsky, W. T. Vetterling and B. P. 
Flannery. 1992. Numerical Recipes In C: The Art of Scien-
tific Computing. New York: Cambridge Univ. Press. 
Quirk, Chris, Arul Menezes, and Colin Cherry. 2005. 
Dependency treelet translation: syntactically informed 
phrasal SMT. In ACL 2005: 271-279.  
Roark, Brian, Murat Saraclar and Michael Collins. 2004. 
Corrective language modeling for large vocabulary ASR 
with the perceptron algorithm. In ICASSP 2004. 
Suzuki, Hisami and Jianfeng Gao. 2005. A comparative 
study on language model adaptation using new 
evaluation metrics. In HLT/EMNLP 2005. 
Theodoridis, Sergios and Konstantinos Koutroumbas. 
2003. Pattern Recognition. Elsevier. 
Vapnik, V. N. 1999. The nature of statistical learning theory. 
Springer-Verlag, New York. 
216
A Study on Richer Syntactic Dependencies for Structured Language
Modeling
Peng Xu
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
xp@clsp.jhu.edu
Ciprian Chelba
Microsoft Research
One Microsoft Way
Redmond, WA 98052
chelba@microsoft.com
Frederick Jelinek
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
jelinek@clsp.jhu.edu
Abstract
We study the impact of richer syntac-
tic dependencies on the performance of
the structured language model (SLM)
along three dimensions: parsing accu-
racy (LP/LR), perplexity (PPL) and word-
error-rate (WER, N-best re-scoring). We
show that our models achieve an im-
provement in LP/LR, PPL and/or WER
over the reported baseline results us-
ing the SLM on the UPenn Treebank
and Wall Street Journal (WSJ) corpora,
respectively. Analysis of parsing per-
formance shows correlation between the
quality of the parser (as measured by pre-
cision/recall) and the language model per-
formance (PPL and WER). A remarkable
fact is that the enriched SLM outperforms
the baseline 3-gram model in terms of
WER by 10% when used in isolation as a
second pass (N-best re-scoring) language
model.
1 Introduction
The structured language model uses hidden parse
trees to assign conditional word-level language
model probabilities. As explained in (Chelba and
Jelinek, 2000), Section 4.4.1, if the final best parse
is used to be the only parse, the reduction in PPL
?relative to a 3-gram baseline? using the SLM?s
headword parametrization for word prediction is
about 40%. The key to achieving this reduction is
a good guess of the final best parse for a given sen-
tence as it is being traversed left-to-right, which is
much harder than finding the final best parse for the
entire sentence, as it is sought by a regular statistical
parser. Nevertheless, it is expected that techniques
developed in the statistical parsing community that
aim at recovering the best parse for an entire sen-
tence, i.e. as judged by a human annotator, should
also be productive in enhancing the performance of
a language model that uses syntactic structure.
The statistical parsing community has used var-
ious ways of enriching the dependency structure
underlying the parametrization of the probabilistic
model used for scoring a given parse tree (Charniak,
2000) (Collins, 1999). Recently, such models (Char-
niak, 2001) (Roark, 2001) have been shown to out-
perform the SLM in terms of both PPL and WER on
the UPenn Treebank and WSJ corpora, respectively.
In (Chelba and Xu, 2001), a simple way of enriching
the probabilistic dependencies in the CONSTRUC-
TOR component of the SLM also showed better
PPL and WER performance; the simple modifica-
tion to the training procedure brought the WER per-
formance of the SLM to the same level with the best
as reported in (Roark, 2001).
In this paper, we present three simple ways of
enriching the syntactic dependency structure in the
SLM, extending the work in (Chelba and Xu, 2001).
The results show that an improved parser (as mea-
sured by LP/LR) is indeed helpful in reducing the
PPL and WER. Another remarkable fact is that for
the first time a language model exploiting elemen-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 191-198.
                         Proceedings of the 40th Annual Meeting of the Association for
tary syntactic dependencies obviates the need for
interpolation with a 3-gram model in N-best re-
scoring.
2 SLM Review
An extensive presentation of the SLM can be found
in (Chelba and Jelinek, 2000). The model assigns a
probability  
	 to every sentence  and ev-
ery possible binary parse  . The terminals of 
are the words of  with POS tags, and the nodes
of

are annotated with phrase headwords and non-
terminal labels. Let  be a sentence of length 
(<s>, SB)   .......   (w_p, t_p) (w_{p+1}, t_{p+1}) ........ (w_k, t_k) w_{k+1}.... </s>
h_0 = (h_0.word, h_0.tag)h_{-1}h_{-m} = (<s>, SB)
Figure 1: A word-parse  -prefix
words to which we have prepended the sentence be-
ginning marker <s> and appended the sentence end
marker </s> so that  <s> and  </s>.
Let

Training Connectionist Models for the Structured Language Model  
Peng Xu, Ahmad Emami and Frederick Jelinek
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218

xp,emami,jelinek  @jhu.edu
Abstract
We investigate the performance of the
Structured Language Model (SLM) in
terms of perplexity (PPL) when its compo-
nents are modeled by connectionist mod-
els. The connectionist models use a dis-
tributed representation of the items in
the history and make much better use of
contexts than currently used interpolated
or back-off models, not only because of
the inherent capability of the connection-
ist model in fighting the data sparseness
problem, but also because of the sub-
linear growth in the model size when the
context length is increased. The connec-
tionist models can be further trained by an
EM procedure, similar to the previously
used procedure for training the SLM. Our
experiments show that the connectionist
models can significantly improve the PPL
over the interpolated and back-off mod-
els on the UPENN Treebank corpora, after
interpolating with a baseline trigram lan-
guage model. The EM training procedure
can improve the connectionist models fur-
ther, by using hidden events obtained by
the SLM parser.
1 Introduction
In many systems dealing with natural speech or lan-
guage such as Automatic Speech Recognition and

This work was supported by the National Science Founda-
tion under grants No.IIS-9982329 and No.IIS-0085940.
Statistical Machine Translation, a language model
is a crucial component for searching in the often
prohibitively large hypothesis space. Most of the
state-of-the-art systems use n-gram language mod-
els, which are simple and effective most of the
time. Many smoothing techniques that improve lan-
guage model probability estimation have been pro-
posed and studied in the n-gram literature (Chen and
Goodman, 1998).
Recent efforts have studied various ways of us-
ing information from a longer context span than that
usually captured by normal n-gram language mod-
els, as well as ways of using syntactical informa-
tion that is not available to the word-based n-gram
models (Chelba and Jelinek, 2000; Charniak, 2001;
Roark, 2001; Uystel et al, 2001). All these language
models are based on stochastic parsing techniques
that build up parse trees for the input word sequence
and condition the generation of words on syntactical
and lexical information available in the parse trees.
Since these language models capture useful hierar-
chical characteristics of language, they can improve
the PPL significantly for various tasks. Although
more improvement can be achieved by enriching the
syntactical dependencies in the structured language
model (SLM) (Xu et al, 2002), a severe data sparse-
ness problem was observed in (Xu et al, 2002) when
the number of conditioning features was increased.
There has been recent promising work in us-
ing distributional representation of words and neu-
ral networks for language modeling (Bengio et al,
2001) and parsing (Henderson, 2003). One great ad-
vantage of this approach is its ability to fight data
sparseness. The model size grows only sub-linearly
with the number of predicting features used. It has
been shown that this method improves significantly
on regular n-gram models in perplexity (Bengio et
al., 2001). The ability of the method to accommo-
date longer contexts is most appealing, since exper-
iments have shown consistent improvements in PPL
when the context of one of the components of the
SLM is increased in length (Emami et al, 2003).
Moreover, because the SLM provides an EM train-
ing procedure for its components, the connectionist
models can also be improved by the EM training.
In this paper, we will study the impact of neural
network modeling on the SLM, when all of its three
components are modeled with this approach. An EM
training procedure will be outlined and applied to
further training of the neural network models.
2 A Probabilistic Neural Network Model
Recently, a relatively new type of language model
has been introduced where words are represented
by points in a multi-dimensional feature space and
the probability of a sequence of words is computed
by means of a neural network. The neural network,
having the feature vectors of the preceding words as
its input, estimates the probability of the next word
(Bengio et al, 2001). The main idea behind this
model is to fight the curse of dimensionality by inter-
polating the seen sequences in the training data. The
generalization this model aims at is to assign to an
unseen word sequence a probability similar to that
of a seen word sequence whose words are similar to
those of the unseen word sequence. The similarity
is defined as being close in the multi-dimensional
space mentioned above.
In brief, this model can be described as follows.
A feature vector is associated with each token in the
input vocabulary, that is, the vocabulary of all the
items that can be used for conditioning. Then the
conditional probability of the next word is expressed
as a function of the input feature vectors by means
of a neural network. This probability is produced
for every possible next word from the output vocab-
ulary. In general, there does not need to be any rela-
tionship between the input and output vocabularies.
The feature vectors and the parameters of the neural
network are learned simultaneously during training.
The input to the neural network are the feature vec-
tors for all the inputs concatenated, and the output
is the conditional probability distribution over the
output vocabulary. The idea here is that the words
which are close to each other (close in the sense of
their role in predicting words to follow) would have
similar (close) feature vectors and since the proba-
bility function is a smooth function of these feature
values, a small change in the features should only
lead to a small change in the probability.
2.1 The Architecture of the Neural Network
Model
The conditional probability function
  
		
	
where

and  are from the
input and output vocabularies Random Forests in Language Modeling
Peng Xu and Frederick Jelinek
Center for Language and Speech Processing
the Johns Hopkins University
Baltimore, MD 21218, USA
 
xp,jelinek  @jhu.edu
Abstract
In this paper, we explore the use of Random Forests
(RFs) (Amit and Geman, 1997; Breiman, 2001) in
language modeling, the problem of predicting the
next word based on words already seen before. The
goal in this work is to develop a new language mod-
eling approach based on randomly grown Decision
Trees (DTs) and apply it to automatic speech recog-
nition. We study our RF approach in the context
of  -gram type language modeling. Unlike regu-
lar  -gram language models, RF language models
have the potential to generalize well to unseen data,
even when a complicated history is used. We show
that our RF language models are superior to regular
 -gram language models in reducing both the per-
plexity (PPL) and word error rate (WER) in a large
vocabulary speech recognition system.
1 Introduction
In many systems dealing with natural speech or lan-
guage, such as Automatic Speech Recognition and
Statistical Machine Translation, a language model
is a crucial component for searching in the often
prohibitively large hypothesis space. Most state-of-
the-art systems use  -gram language models, which
are simple and effective most of the time. Many
smoothing techniques that improve language model
probability estimation have been proposed and stud-
ied in the  -gram literature (Chen and Goodman,
1998). There has also been work in exploring Deci-
sion Tree (DT) language models (Bahl et al, 1989;
Potamianos and Jelinek, 1998), which attempt to
cluster similar histories together to achieve better
probability estimation. However, the results were
not promising (Potamianos and Jelinek, 1998): in
a fair comparison, decision tree language models
failed to improve upon the baseline  -gram models
with the same order  .
The aim of DT language models is to alleviate
the data sparseness problem encountered in  -gram
language models. However, the cause of the neg-
ative results is exactly the same: data sparseness,
coupled with the fact that the DT construction al-
gorithms decide on tree splits solely on the basis
of seen data (Potamianos and Jelinek, 1998). Al-
though various smoothing techniques were studied
in the context of DT language models, none of them
resulted in significant improvements over  -gram
models.
Recently, a neural network based language mod-
eling approach has been applied to trigram lan-
guage models to deal with the curse of dimension-
ality (Bengio et al, 2001; Schwenk and Gauvain,
2002). Significant improvements in both perplex-
ity (PPL) and word error rate (WER) over backoff
smoothing were reported after interpolating the neu-
ral network models with the baseline backoff mod-
els. However, the neural network models rely on
interpolation with  -gram models, and use  -gram
models exclusively for low frequency words. We
believe improvements in  -gram models should also
improve the performance of neural network models.
We propose a new Random Forest (RF) approach
for language modeling. The idea of using RFs for
language modeling comes from the recent success
of RFs in classification and regression (Amit and
Geman, 1997; Breiman, 2001; Ho, 1998). By defi-
nition, RFs are collections of Decision Trees (DTs)
that have been constructed randomly. Therefore, we
also propose a new DT language model which can
be randomized to construct RFs efficiently. Once
constructed, the RFs function as a randomized his-
tory clustering which can help in dealing with the
data sparseness problem. Although they do not per-
form well on unseen test data individually, the col-
lective contribution of all DTs makes the RFs gen-
eralize well to unseen data. We show that our RF
approach for  -gram language modeling can result
in a significant improvement in both PPL and WER
in a large vocabulary speech recognition system.
The paper is organized as follows: In Section 2,
we review the basics about language modeling and
smoothing. In Section 3, we briefly review DT
based language models and describe our new DT
and RF approach for language modeling. In Sec-
tion 4, we show the performance of our RF based
language models as measured by both PPL and
WER. After some discussion and analysis, we fi-
nally summarize our work and propose some future
directions in Section 5.
2 Basic Language Modeling
The purpose of a language model is to estimate the
probability of a word string. Let   denote a string
of  words, that is,   	
	 . Then,
by the chain rule of probability, we have
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 858?867, Prague, June 2007. c?2007 Association for Computational Linguistics
Large Language Models in Machine Translation
Thorsten Brants Ashok C. Popat Peng Xu Franz J. Och Jeffrey Dean
Google, Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94303, USA
{brants,popat,xp,och,jeff}@google.com
Abstract
This paper reports on the benefits of large-
scale statistical language modeling in ma-
chine translation. A distributed infrastruc-
ture is proposed which we use to train on
up to 2 trillion tokens, resulting in language
models having up to 300 billion n-grams. It
is capable of providing smoothed probabil-
ities for fast, single-pass decoding. We in-
troduce a new smoothing method, dubbed
Stupid Backoff, that is inexpensive to train
on large data sets and approaches the quality
of Kneser-Ney Smoothing as the amount of
training data increases.
1 Introduction
Given a source-language (e.g., French) sentence f ,
the problem of machine translation is to automati-
cally produce a target-language (e.g., English) trans-
lation e?. The mathematics of the problem were for-
malized by (Brown et al, 1993), and re-formulated
by (Och and Ney, 2004) in terms of the optimization
e? = arg max
e
M
?
m=1
?mhm(e, f) (1)
where {hm(e, f)} is a set of M feature functions and
{?m} a set of weights. One or more feature func-
tions may be of the form h(e, f) = h(e), in which
case it is referred to as a language model.
We focus on n-gram language models, which are
trained on unlabeled monolingual text. As a general
rule, more data tends to yield better language mod-
els. Questions that arise in this context include: (1)
How might one build a language model that allows
scaling to very large amounts of training data? (2)
How much does translation performance improve as
the size of the language model increases? (3) Is there
a point of diminishing returns in performance as a
function of language model size?
This paper proposes one possible answer to the
first question, explores the second by providing
learning curves in the context of a particular statis-
tical machine translation system, and hints that the
third may yet be some time in answering. In particu-
lar, it proposes a distributed language model training
and deployment infrastructure, which allows direct
and efficient integration into the hypothesis-search
algorithm rather than a follow-on re-scoring phase.
While it is generally recognized that two-pass de-
coding can be very effective in practice, single-pass
decoding remains conceptually attractive because it
eliminates a source of potential information loss.
2 N -gram Language Models
Traditionally, statistical language models have been
designed to assign probabilities to strings of words
(or tokens, which may include punctuation, etc.).
Let wL1 = (w1, . . . , wL) denote a string of L tokens
over a fixed vocabulary. An n-gram language model
assigns a probability to wL1 according to
P (wL1 ) =
L
?
i=1
P (wi|wi?11 ) ?
L
?
i=1
P? (wi|wi?1i?n+1)
(2)
where the approximation reflects a Markov assump-
tion that only the most recent n ? 1 tokens are rele-
vant when predicting the next word.
858
For any substring wji of wL1 , let f(w
j
i ) denote the
frequency of occurrence of that substring in another
given, fixed, usually very long target-language string
called the training data. The maximum-likelihood
(ML) probability estimates for the n-grams are given
by their relative frequencies
r(wi|wi?1i?n+1) =
f(wii?n+1)
f(wi?1i?n+1)
. (3)
While intuitively appealing, Eq. (3) is problematic
because the denominator and / or numerator might
be zero, leading to inaccurate or undefined probabil-
ity estimates. This is termed the sparse data prob-
lem. For this reason, the ML estimate must be mod-
ified for use in practice; see (Goodman, 2001) for a
discussion of n-gram models and smoothing.
In principle, the predictive accuracy of the lan-
guage model can be improved by increasing the or-
der of the n-gram. However, doing so further exac-
erbates the sparse data problem. The present work
addresses the challenges of processing an amount
of training data sufficient for higher-order n-gram
models and of storing and managing the resulting
values for efficient use by the decoder.
3 Related Work on Distributed Language
Models
The topic of large, distributed language models is
relatively new. Recently a two-pass approach has
been proposed (Zhang et al, 2006), wherein a lower-
order n-gram is used in a hypothesis-generation
phase, then later the K-best of these hypotheses are
re-scored using a large-scale distributed language
model. The resulting translation performance was
shown to improve appreciably over the hypothesis
deemed best by the first-stage system. The amount
of data used was 3 billion words.
More recently, a large-scale distributed language
model has been proposed in the contexts of speech
recognition and machine translation (Emami et al,
2007). The underlying architecture is similar to
(Zhang et al, 2006). The difference is that they in-
tegrate the distributed language model into their ma-
chine translation decoder. However, they don?t re-
port details of the integration or the efficiency of the
approach. The largest amount of data used in the
experiments is 4 billion words.
Both approaches differ from ours in that they store
corpora in suffix arrays, one sub-corpus per worker,
and serve raw counts. This implies that all work-
ers need to be contacted for each n-gram request.
In our approach, smoothed probabilities are stored
and served, resulting in exactly one worker being
contacted per n-gram for simple smoothing tech-
niques, and in exactly two workers for smoothing
techniques that require context-dependent backoff.
Furthermore, suffix arrays require on the order of 8
bytes per token. Directly storing 5-grams is more
efficient (see Section 7.2) and allows applying count
cutoffs, further reducing the size of the model.
4 Stupid Backoff
State-of-the-art smoothing uses variations of con-
text-dependent backoff with the following scheme:
P (wi|wi?1i?k+1) =
{
?(wii?k+1) if (wii?k+1) is found
?(wi?1i?k+1)P (wii?k+2) otherwise
(4)
where ?(?) are pre-computed and stored probabili-
ties, and ?(?) are back-off weights. As examples,
Kneser-Ney Smoothing (Kneser and Ney, 1995),
Katz Backoff (Katz, 1987) and linear interpola-
tion (Jelinek and Mercer, 1980) can be expressed in
this scheme (Chen and Goodman, 1998). The recur-
sion ends at either unigrams or at the uniform distri-
bution for zero-grams.
We introduce a similar but simpler scheme,
named Stupid Backoff 1 , that does not generate nor-
malized probabilities. The main difference is that
we don?t apply any discounting and instead directly
use the relative frequencies (S is used instead of
P to emphasize that these are not probabilities but
scores):
S(wi|wi?1i?k+1) =
?
?
?
?
?
f(wii?k+1)
f(wi?1i?k+1)
if f(wii?k+1) > 0
?S(wi|wi?1i?k+2) otherwise
(5)
1The name originated at a time when we thought that such
a simple scheme cannot possibly be good. Our view of the
scheme changed, but the name stuck.
859
In general, the backoff factor ? may be made to de-
pend on k. Here, a single value is used and heuris-
tically set to ? = 0.4 in all our experiments2 . The
recursion ends at unigrams:
S(wi) =
f(wi)
N (6)
with N being the size of the training corpus.
Stupid Backoff is inexpensive to calculate in a dis-
tributed environment while approaching the quality
of Kneser-Ney smoothing for large amounts of data.
The lack of normalization in Eq. (5) does not affect
the functioning of the language model in the present
setting, as Eq. (1) depends on relative rather than ab-
solute feature-function values.
5 Distributed Training
We use the MapReduce programming model (Dean
and Ghemawat, 2004) to train on terabytes of data
and to generate terabytes of language models. In this
programming model, a user-specified map function
processes an input key/value pair to generate a set of
intermediate key/value pairs, and a reduce function
aggregates all intermediate values associated with
the same key. Typically, multiple map tasks oper-
ate independently on different machines and on dif-
ferent parts of the input data. Similarly, multiple re-
duce tasks operate independently on a fraction of the
intermediate data, which is partitioned according to
the intermediate keys to ensure that the same reducer
sees all values for a given key. For additional details,
such as communication among machines, data struc-
tures and application examples, the reader is referred
to (Dean and Ghemawat, 2004).
Our system generates language models in three
main steps, as described in the following sections.
5.1 Vocabulary Generation
Vocabulary generation determines a mapping of
terms to integer IDs, so n-grams can be stored us-
ing IDs. This allows better compression than the
original terms. We assign IDs according to term fre-
quency, with frequent terms receiving small IDs for
efficient variable-length encoding. All words that
2The value of 0.4 was chosen empirically based on good
results in earlier experiments. Using multiple values depending
on the n-gram order slightly improves results.
occur less often than a pre-determined threshold are
mapped to a special id marking the unknown word.
The vocabulary generation map function reads
training text as input. Keys are irrelevant; values are
text. It emits intermediate data where keys are terms
and values are their counts in the current section
of the text. A sharding function determines which
shard (chunk of data in the MapReduce framework)
the pair is sent to. This ensures that all pairs with
the same key are sent to the same shard. The re-
duce function receives all pairs that share the same
key and sums up the counts. Simplified, the map,
sharding and reduce functions do the following:
Map(string key, string value) {
// key=docid, ignored; value=document
array words = Tokenize(value);
hash_map<string, int> histo;
for i = 1 .. #words
histo[words[i]]++;
for iter in histo
Emit(iter.first, iter.second);
}
int ShardForKey(string key, int nshards) {
return Hash(key) % nshards;
}
Reduce(string key, iterator values) {
// key=term; values=counts
int sum = 0;
for each v in values
sum += ParseInt(v);
Emit(AsString(sum));
}
Note that the Reduce function emits only the aggre-
gated value. The output key is the same as the inter-
mediate key and automatically written by MapRe-
duce. The computation of counts in the map func-
tion is a minor optimization over the alternative of
simply emitting a count of one for each tokenized
word in the array. Figure 1 shows an example for
3 input documents and 2 reduce shards. Which re-
ducer a particular term is sent to is determined by a
hash function, indicated by text color. The exact par-
titioning of the keys is irrelevant; important is that all
pairs with the same key are sent to the same reducer.
5.2 Generation of n-Grams
The process of n-gram generation is similar to vo-
cabulary generation. The main differences are that
now words are converted to IDs, and we emit n-
grams up to some maximum order instead of single
860
Figure 1: Distributed vocabulary generation.
words. A simplified map function does the follow-
ing:
Map(string key, string value) {
// key=docid, ignored; value=document
array ids = ToIds(Tokenize(value));
for i = 1 .. #ids
for j = 0 .. maxorder-1
Emit(ids[i-j .. i], "1");
}
Again, one may optimize the Map function by first
aggregating counts over some section of the data and
then emit the aggregated counts instead of emitting
?1? each time an n-gram is encountered.
The reduce function is the same as for vocabu-
lary generation. The subsequent step of language
model generation will calculate relative frequencies
r(wi|wi?1i?k+1) (see Eq. 3). In order to make that step
efficient we use a sharding function that places the
values needed for the numerator and denominator
into the same shard.
Computing a hash function on just the first words
of n-grams achieves this goal. The required n-
grams wii?n+1 and wi?1i?n+1 always share the same
first word wi?n+1, except for unigrams. For that we
need to communicate the total count N to all shards.
Unfortunately, sharding based on the first word
only may make the shards very imbalanced. Some
terms can be found at the beginning of a huge num-
ber of n-grams, e.g. stopwords, some punctuation
marks, or the beginning-of-sentence marker. As an
example, the shard receiving n-grams starting with
the beginning-of-sentence marker tends to be several
times the average size. Making the shards evenly
sized is desirable because the total runtime of the
process is determined by the largest shard.
The shards are made more balanced by hashing
based on the first two words:
int ShardForKey(string key, int nshards) {
string prefix = FirstTwoWords(key);
return Hash(prefix) % nshards;
}
This requires redundantly storing unigram counts in
all shards in order to be able to calculate relative fre-
quencies within shards. That is a relatively small
amount of information (a few million entries, com-
pared to up to hundreds of billions of n-grams).
5.3 Language Model Generation
The input to the language model generation step is
the output of the n-gram generation step: n-grams
and their counts. All information necessary to calcu-
late relative frequencies is available within individ-
ual shards because of the sharding function. That is
everything we need to generate models with Stupid
Backoff. More complex smoothing methods require
additional steps (see below).
Backoff operations are needed when the full n-
gram is not found. If r(wi|wi?1i?n+1) is not found,
then we will successively look for r(wi|wi?1i?n+2),
r(wi|wi?1i?n+3), etc. The language model generation
step shards n-grams on their last two words (with
unigrams duplicated), so all backoff operations can
be done within the same shard (note that the required
n-grams all share the same last word wi).
5.4 Other Smoothing Methods
State-of-the-art techniques like Kneser-Ney
Smoothing or Katz Backoff require additional,
more expensive steps. At runtime, the client needs
to additionally request up to 4 backoff factors for
each 5-gram requested from the servers, thereby
multiplying network traffic. We are not aware of
a method that always stores the history backoff
factors on the same shard as the longer n-gram
without duplicating a large fraction of the entries.
This means one needs to contact two shards per
n-gram instead of just one for Stupid Backoff.
Training requires additional iterations over the data.
861
Step 0 Step 1 Step 2
context counting unsmoothed probs and interpol. weights interpolated probabilities
Input key wii?n+1 (same as Step 0 output) (same as Step 1 output)
Input value f(wii?n+1) (same as Step 0 output) (same as Step 1 output)
Intermediate key wii?n+1 wi?1i?n+1 wi?n+1i
Sharding wii?n+1 wi?1i?n+1 w
i?n+2
i?n+1 , unigrams duplicated
Intermediate value fKN (wii?n+1) wi,fKN (wii?n+1)
fKN (wii?n+1)?D
fKN (wi?1i?n+1)
,?(wi?1i?n+1)
Output value fKN (wii?n+1) wi,
fKN (wii?n+1)?D
fKN (wi?1i?n+1)
,?(wi?1i?n+1) PKN (wi|wi?1i?n+1), ?(wi?1i?n+1)
Table 1: Extra steps needed for training Interpolated Kneser-Ney Smoothing
Kneser-Ney Smoothing counts lower-order n-
grams differently. Instead of the frequency of the
(n? 1)-gram, it uses the number of unique single
word contexts the (n?1)-gram appears in. We use
fKN(?) to jointly denote original frequencies for the
highest order and context counts for lower orders.
After the n-gram counting step, we process the n-
grams again to produce these quantities. This can
be done similarly to the n-gram counting using a
MapReduce (Step 0 in Table 1).
The most commonly used variant of Kneser-Ney
smoothing is interpolated Kneser-Ney smoothing,
defined recursively as (Chen and Goodman, 1998):
PKN (wi|wi?1i?n+1) =
max(fKN(wii?n+1) ? D, 0)
fKN(wi?1i?n+1)
+ ?(wi?1i?n+1)PKN (wi|wi?1i?n+2),
where D is a discount constant and {?(wi?1i?n+1)} are
interpolation weights that ensure probabilities sum
to one. Two additional major MapReduces are re-
quired to compute these values efficiently. Table 1
describes their input, intermediate and output keys
and values. Note that output keys are always the
same as intermediate keys.
The map function of MapReduce 1 emits n-gram
histories as intermediate keys, so the reduce func-
tion gets all n-grams with the same history at the
same time, generating unsmoothed probabilities and
interpolation weights. MapReduce 2 computes the
interpolation. Its map function emits reversed n-
grams as intermediate keys (hence we use wi?n+1i
in the table). All unigrams are duplicated in ev-
ery reduce shard. Because the reducer function re-
ceives intermediate keys in sorted order it can com-
pute smoothed probabilities for all n-gram orders
with simple book-keeping.
Katz Backoff requires similar additional steps.
The largest models reported here with Kneser-Ney
Smoothing were trained on 31 billion tokens. For
Stupid Backoff, we were able to use more than 60
times of that amount.
6 Distributed Application
Our goal is to use distributed language models in-
tegrated into the first pass of a decoder. This may
yield better results than n-best list or lattice rescor-
ing (Ney and Ortmanns, 1999). Doing that for lan-
guage models that reside in the same machine as the
decoder is straight-forward. The decoder accesses
n-grams whenever necessary. This is inefficient in a
distributed system because network latency causes a
constant overhead on the order of milliseconds. On-
board memory is around 10,000 times faster.
We therefore implemented a new decoder archi-
tecture. The decoder first queues some number of
requests, e.g. 1,000 or 10,000 n-grams, and then
sends them together to the servers, thereby exploit-
ing the fact that network requests with large numbers
of n-grams take roughly the same time to complete
as requests with single n-grams.
The n-best search of our machine translation de-
coder proceeds as follows. It maintains a graph of
the search space up to some point. It then extends
each hypothesis by advancing one word position in
the source language, resulting in a candidate exten-
sion of the hypothesis of zero, one, or more addi-
tional target-language words (accounting for the fact
that variable-length source-language fragments can
correspond to variable-length target-language frag-
ments). In a traditional setting with a local language
model, the decoder immediately obtains the nec-
essary probabilities and then (together with scores
862
Figure 2: Illustration of decoder graph and batch-
querying of the language model.
from other features) decides which hypotheses to
keep in the search graph. When using a distributed
language model, the decoder first tentatively extends
all current hypotheses, taking note of which n-grams
are required to score them. These are queued up for
transmission as a batch request. When the scores are
returned, the decoder re-visits all of these tentative
hypotheses, assigns scores, and re-prunes the search
graph. It is then ready for the next round of exten-
sions, again involving queuing the n-grams, waiting
for the servers, and pruning.
The process is illustrated in Figure 2 assuming a
trigram model and a decoder policy of pruning to
the four most promising hypotheses. The four ac-
tive hypotheses (indicated by black disks) at time t
are: There is, There may, There are, and There were.
The decoder extends these to form eight new nodes
at time t + 1. Note that one of the arcs is labeled ,
indicating that no target-language word was gener-
ated when the source-language word was consumed.
The n-grams necessary to score these eight hypothe-
ses are There is lots, There is many, There may be,
There are lots, are lots of, etc. These are queued up
and their language-model scores requested in a batch
manner. After scoring, the decoder prunes this set as
indicated by the four black disks at time t + 1, then
extends these to form five new nodes (one is shared)
at time t + 2. The n-grams necessary to score these
hypotheses are lots of people, lots of reasons, There
are onlookers, etc. Again, these are sent to the server
together, and again after scoring the graph is pruned
to four active (most promising) hypotheses.
The alternating processes of queuing, waiting and
scoring/pruning are done once per word position in
a source sentence. The average sentence length in
our test data is 22 words (see section 7.1), thus we
have 23 rounds3 per sentence on average. The num-
ber of n-grams requested per sentence depends on
the decoder settings for beam size, re-ordering win-
dow, etc. As an example for larger runs reported in
the experiments section, we typically request around
150,000 n-grams per sentence. The average net-
work latency per batch is 35 milliseconds, yield-
ing a total latency of 0.8 seconds caused by the dis-
tributed language model for an average sentence of
22 words. If a slight reduction in translation qual-
ity is allowed, then the average network latency per
batch can be brought down to 7 milliseconds by re-
ducing the number of n-grams requested per sen-
tence to around 10,000. As a result, our system can
efficiently use the large distributed language model
at decoding time. There is no need for a second pass
nor for n-best list rescoring.
We focused on machine translation when describ-
ing the queued language model access. However,
it is general enough that it may also be applicable
to speech decoders and optical character recognition
systems.
7 Experiments
We trained 5-gram language models on amounts of
text varying from 13 million to 2 trillion tokens.
The data is divided into four sets; language mod-
els are trained for each set separately4 . For each
training data size, we report the size of the result-
ing language model, the fraction of 5-grams from
the test data that is present in the language model,
and the BLEU score (Papineni et al, 2002) obtained
by the machine translation system. For smaller train-
ing sizes, we have also computed test-set perplexity
using Kneser-Ney Smoothing, and report it for com-
parison.
7.1 Data Sets
We compiled four language model training data sets,
listed in order of increasing size:
3One additional round for the sentence end marker.
4Experience has shown that using multiple, separately
trained language models as feature functions in Eq (1) yields
better results than using a single model trained on all data.
863
 1e+07
 1e+08
 1e+09
 1e+10
 1e+11
 1e+12
 10  100  1000  10000  100000  1e+06
 0.1
 1
 10
 100
 1000
N
um
be
r o
f n
-g
ra
m
s
Ap
pr
ox
. L
M
 s
ize
 in
 G
B
LM training data size in million tokens
x1.8/x2
x1.8/x2
x1.8/x2
x1.6/x2
target
+ldcnews
+webnews
+web
Figure 3: Number of n-grams (sum of unigrams to
5-grams) for varying amounts of training data.
target: The English side of Arabic-English parallel
data provided by LDC5 (237 million tokens).
ldcnews: This is a concatenation of several English
news data sets provided by LDC6 (5 billion tokens).
webnews: Data collected over several years, up to
December 2005, from web pages containing pre-
dominantly English news articles (31 billion to-
kens).
web: General web data, which was collected in Jan-
uary 2006 (2 trillion tokens).
For testing we use the ?NIST? part of the 2006
Arabic-English NIST MT evaluation set, which is
not included in the training data listed above7. It
consists of 1797 sentences of newswire, broadcast
news and newsgroup texts with 4 reference transla-
tions each. The test set is used to calculate transla-
tion BLEU scores. The English side of the set is also
used to calculate perplexities and n-gram coverage.
7.2 Size of the Language Models
We measure the size of language models in total
number of n-grams, summed over all orders from
1 to 5. There is no frequency cutoff on the n-grams.
5http://www.nist.gov/speech/tests/mt/doc/
LDCLicense-mt06.pdf contains a list of parallel resources
provided by LDC.
6The bigger sets included are LDC2005T12 (Gigaword,
2.5B tokens), LDC93T3A (Tipster, 500M tokens) and
LDC2002T31 (Acquaint, 400M tokens), plus many smaller
sets.
7The test data was generated after 1-Feb-2006; all training
data was generated before that date.
target webnews web
# tokens 237M 31G 1.8T
vocab size 200k 5M 16M
# n-grams 257M 21G 300G
LM size (SB) 2G 89G 1.8T
time (SB) 20 min 8 hours 1 day
time (KN) 2.5 hours 2 days ?
# machines 100 400 1500
Table 2: Sizes and approximate training times for
3 language models with Stupid Backoff (SB) and
Kneser-Ney Smoothing (KN).
There is, however, a frequency cutoff on the vocab-
ulary. The minimum frequency for a term to be in-
cluded in the vocabulary is 2 for the target, ldcnews
and webnews data sets, and 200 for the web data set.
All terms below the threshold are mapped to a spe-
cial term UNK, representing the unknown word.
Figure 3 shows the number of n-grams for lan-
guage models trained on 13 million to 2 trillion to-
kens. Both axes are on a logarithmic scale. The
right scale shows the approximate size of the served
language models in gigabytes. The numbers above
the lines indicate the relative increase in language
model size: x1.8/x2 means that the number of n-
grams grows by a factor of 1.8 each time we double
the amount of training data. The values are simi-
lar across all data sets and data sizes, ranging from
1.6 to 1.8. The plots are very close to straight lines
in the log/log space; linear least-squares regression
finds r2 > 0.99 for all four data sets.
The web data set has the smallest relative increase.
This can be at least partially explained by the higher
vocabulary cutoff. The largest language model gen-
erated contains approx. 300 billion n-grams.
Table 2 shows sizes and approximate training
times when training on the full target, webnews, and
web data sets. The processes run on standard current
hardware with the Linux operating system. Gen-
erating models with Kneser-Ney Smoothing takes
6 ? 7 times longer than generating models with
Stupid Backoff. We deemed generation of Kneser-
Ney models on the web data as too expensive and
therefore excluded it from our experiments. The es-
timated runtime for that is approximately one week
on 1500 machines.
864
 50
 100
 150
 200
 250
 300
 350
 10  100  1000  10000  100000  1e+06
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
Pe
rp
le
xit
y
Fr
ac
tio
n 
of
 c
ov
er
ed
 5
-g
ra
m
s
LM training data size in million tokens
+.022/x2
+.035/x2
+.038/x2
+.026/x2
target KN PP
ldcnews KN PP
webnews KN PP
target C5
+ldcnews C5
+webnews C5
+web C5
Figure 4: Perplexities with Kneser-Ney Smoothing
(KN PP) and fraction of covered 5-grams (C5).
7.3 Perplexity and n-Gram Coverage
A standard measure for language model quality is
perplexity. It is measured on test data T = w|T |1 :
PP (T ) = e
? 1|T |
|T |
 
i=1
log p(wi|wi?1i?n+1) (7)
This is the inverse of the average conditional prob-
ability of a next word; lower perplexities are bet-
ter. Figure 4 shows perplexities for models with
Kneser-Ney smoothing. Values range from 280.96
for 13 million to 222.98 for 237 million tokens tar-
get data and drop nearly linearly with data size (r2 =
0.998). Perplexities for ldcnews range from 351.97
to 210.93 and are also close to linear (r2 = 0.987),
while those for webnews data range from 221.85 to
164.15 and flatten out near the end. Perplexities are
generally high and may be explained by the mix-
ture of genres in the test data (newswire, broadcast
news, newsgroups) while our training data is pre-
dominantly written news articles. Other held-out
sets consisting predominantly of newswire texts re-
ceive lower perplexities by the same language mod-
els, e.g., using the full ldcnews model we find per-
plexities of 143.91 for the NIST MT 2005 evaluation
set, and 149.95 for the NIST MT 2004 set.
Note that the perplexities of the different language
models are not directly comparable because they use
different vocabularies. We used a fixed frequency
cutoff, which leads to larger vocabularies as the
training data grows. Perplexities tend to be higher
with larger vocabularies.
 0.34
 0.36
 0.38
 0.4
 0.42
 0.44
 10  100  1000  10000  100000  1e+06
Te
st
 d
at
a 
BL
EU
LM training data size in million tokens
+0.62BP/x2
+0.56BP/x2
+0.51BP/x2
+0.66BP/x2
+0.70BP/x2
+0.39BP/x2
+0.15BP/x2
target KN
+ldcnews KN
+webnews KN
target SB
+ldcnews SB
+webnews SB
+web SB
Figure 5: BLEU scores for varying amounts of data
using Kneser-Ney (KN) and Stupid Backoff (SB).
Perplexities cannot be calculated for language
models with Stupid Backoff because their scores are
not normalized probabilities. In order to neverthe-
less get an indication of potential quality improve-
ments with increased training sizes we looked at the
5-gram coverage instead. This is the fraction of 5-
grams in the test data set that can be found in the
language model training data. A higher coverage
will result in a better language model if (as we hy-
pothesize) estimates for seen events tend to be bet-
ter than estimates for unseen events. This fraction
grows from 0.06 for 13 million tokens to 0.56 for 2
trillion tokens, meaning 56% of all 5-grams in the
test data are known to the language model.
Increase in coverage depends on the training data
set. Within each set, we observe an almost constant
growth (correlation r2 ? 0.989 for all sets) with
each doubling of the training data as indicated by
numbers next to the lines. The fastest growth oc-
curs for webnews data (+0.038 for each doubling),
the slowest growth for target data (+0.022/x2).
7.4 Machine Translation Results
We use a state-of-the-art machine translation system
for translating from Arabic to English that achieved
a competitive BLEU score of 0.4535 on the Arabic-
English NIST subset in the 2006 NIST machine
translation evaluation8 . Beam size and re-ordering
window were reduced in order to facilitate a large
8See http://www.nist.gov/speech/tests/mt/
mt06eval official results.html for more results.
865
number of experiments. Additionally, our NIST
evaluation system used a mixture of 5, 6, and 7-gram
models with optimized stupid backoff factors for
each order, while the learning curve presented here
uses a fixed order of 5 and a single fixed backoff fac-
tor. Together, these modifications reduce the BLEU
score by 1.49 BLEU points (BP)9 at the largest train-
ing size. We then varied the amount of language
model training data from 13 million to 2 trillion to-
kens. All other parts of the system are kept the same.
Results are shown in Figure 5. The first part
of the curve uses target data for training the lan-
guage model. With Kneser-Ney smoothing (KN),
the BLEU score improves from 0.3559 for 13 mil-
lion tokens to 0.3832 for 237 million tokens. At
such data sizes, Stupid Backoff (SB) with a constant
backoff parameter ? = 0.4 is around 1 BP worse
than KN. On average, one gains 0.62 BP for each
doubling of the training data with KN, and 0.66 BP
per doubling with SB. Differences of more than 0.51
BP are statistically significant at the 0.05 level using
bootstrap resampling (Noreen, 1989; Koehn, 2004).
We then add a second language model using ldc-
news data. The first point for ldcnews shows a large
improvement of around 1.4 BP over the last point
for target for both KN and SB, which is approxi-
mately twice the improvement expected from dou-
bling the amount of data. This seems to be caused
by adding a new domain and combining two models.
After that, we find an improvement of 0.56?0.70 BP
for each doubling of the ldcnews data. The gap be-
tween Kneser-Ney Smoothing and Stupid Backoff
narrows, starting with a difference of 0.85 BP and
ending with a not significant difference of 0.24 BP.
Adding a third language models based on web-
news data does not show a jump at the start of the
curve. We see, however, steady increases of 0.39?
0.51 BP per doubling. The gap between Kneser-Ney
and Stupid Backoff is gone, all results with Stupid
Backoff are actually better than Kneser-Ney, but the
differences are not significant.
We then add a fourth language model based on
web data and Stupid Backoff. Generating Kneser-
Ney models for these data sizes is extremely ex-
pensive and is therefore omitted. The fourth model
91 BP = 0.01 BLEU. We show system scores as BLEU, dif-
ferences as BP.
shows a small but steady increase of 0.15 BP per
doubling, surpassing the best Kneser-Ney model
(trained on less data) by 0.82 BP at the largest
size. Goodman (2001) observed that Kneser-Ney
Smoothing dominates other schemes over a broad
range of conditions. Our experiments confirm this
advantage at smaller language model sizes, but show
the advantage disappears at larger data sizes.
The amount of benefit from doubling the training
size is partly determined by the domains of the data
sets10. The improvements are almost linear on the
log scale within the sets. Linear least-squares regres-
sion shows correlations r2 > 0.96 for all sets and
both smoothing methods, thus we expect to see sim-
ilar improvements when further increasing the sizes.
8 Conclusion
A distributed infrastructure has been described to
train and apply large-scale language models to ma-
chine translation. Experimental results were pre-
sented showing the effect of increasing the amount
of training data to up to 2 trillion tokens, resulting
in a 5-gram language model size of up to 300 billion
n-grams. This represents a gain of about two orders
of magnitude in the amount of training data that can
be handled over that reported previously in the liter-
ature (or three-to-four orders of magnitude, if one
considers only single-pass decoding). The infra-
structure is capable of scaling to larger amounts of
training data and higher n-gram orders.
The technique is made efficient by judicious
batching of score requests by the decoder in a server-
client architecture. A new, simple smoothing tech-
nique well-suited to distributed computation was
proposed, and shown to perform as well as more
sophisticated methods as the size of the language
model increases.
Significantly, we found that translation quality as
indicated by BLEU score continues to improve with
increasing language model size, at even the largest
sizes considered. This finding underscores the value
of being able to train and apply very large language
models, and suggests that further performance gains
may be had by pursuing this direction further.
10There is also an effect of the order in which we add the
models. As an example, web data yields +0.43 BP/x2 when
added as the second model. A discussion of this effect is omit-
ted due to space limitations.
866
References
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Stanley F. Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language model-
ing. Technical Report TR-10-98, Harvard, Cambridge,
MA, USA.
Jeffrey Dean and Sanjay Ghemawat. 2004. Mapreduce:
Simplified data processing on large clusters. In Sixth
Symposium on Operating System Design and Imple-
mentation (OSDI-04), San Francisco, CA, USA.
Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen.
2007. Large-scale distributed language modeling. In
Proceedings of ICASSP-2007, Honolulu, HI, USA.
Joshua Goodman. 2001. A bit of progress in language
modeling. Technical Report MSR-TR-2001-72, Mi-
crosoft Research, Redmond, WA, USA.
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of Markov source parameters from
sparse data. In Pattern Recognition in Practice, pages
381?397. North Holland.
Slava Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speech, and Signal Processing, 35(3).
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, pages 181?
184.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP-04, Barcelona, Spain.
Hermann Ney and Stefan Ortmanns. 1999. Dynamic
programming search for continuous speech recogni-
tion. IEEE Signal Processing Magazine, 16(5):64?83.
Eric W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley & Sons.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL-
02, pages 311?318, Philadelphia, PA, USA.
Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel.
2006. Distributed language modeling for n-best list
re-ranking. In Proceedings of EMNLP-2006, pages
216?223, Sydney, Australia.
867
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 245?253,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Using a Dependency Parser to Improve SMT for Subject-Object-Verb
Languages
Peng Xu, Jaeho Kang, Michael Ringgaard and Franz Och
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94043, USA
{xp,jhkang,ringgaard,och}@google.com
Abstract
We introduce a novel precedence reordering
approach based on a dependency parser to sta-
tistical machine translation systems. Similar
to other preprocessing reordering approaches,
our method can efficiently incorporate linguis-
tic knowledge into SMT systems without in-
creasing the complexity of decoding. For a set
of five subject-object-verb (SOV) order lan-
guages, we show significant improvements in
BLEU scores when translating from English,
compared to other reordering approaches, in
state-of-the-art phrase-based SMT systems.
1 Introduction
Over the past ten years, statistical machine transla-
tion has seen many exciting developments. Phrase-
based systems (Och, 2002; Koehn et.al., 2003;
Och and Ney, 2004) advanced the machine transla-
tion field by allowing translations of word sequences
(a.k.a., phrases) instead of single words. This ap-
proach has since been the state-of-the-art because of
its robustness in modeling local word reordering and
the existence of an efficient dynamic programming
decoding algorithm.
However, when phrase-based systems are used
between languages with very different word or-
ders, such as between subject-verb-object (SVO)
and subject-object-verb (SOV) languages, long dis-
tance reordering becomes one of the key weak-
nesses. Many reordering methods have been pro-
posed in recent years to address this problem in dif-
ferent aspects.
The first class of approaches tries to explicitly
model phrase reordering distances. Distance based
distortion model (Och, 2002; Koehn et.al., 2003) is
a simple way of modeling phrase level reordering.
It penalizes non-monotonicity by applying a weight
to the number of words between two source phrases
corresponding to two consecutive target phrases.
Later on, this model was extended to lexicalized
phrase reordering (Tillmann, 2004; Koehn, et.al.,
2005; Al-Onaizan and Papineni, 2006) by applying
different weights to different phrases. Most recently,
a hierarchical phrase reordering model (Galley and
Manning, 2008) was proposed to dynamically deter-
mine phrase boundaries using efficient shift-reduce
parsing. Along this line of research, discrimina-
tive reordering models based on a maximum entropy
classifier (Zens and Ney, 2006; Xiong, et.al., 2006)
also showed improvements over the distance based
distortion model. None of these reordering models
changes the word alignment step in SMT systems,
therefore, they can not recover from the word align-
ment errors. These models are also limited by a
maximum allowed reordering distance often used in
decoding.
The second class of approaches puts syntactic
analysis of the target language into both modeling
and decoding. It has been shown that direct model-
ing of target language constituents movement in ei-
ther constituency trees (Yamada and Knight, 2001;
Galley et.al., 2006; Zollmann et.al., 2008) or depen-
dency trees (Quirk, et.al., 2005) can result in signifi-
cant improvements in translation quality for translat-
ing languages like Chinese and Arabic into English.
A simpler alternative, the hierarchical phrase-based
245
approach (Chiang, 2005; Wu, 1997) also showed
promising results for translating Chinese to English.
Similar to the distance based reordering models, the
syntactical or hierarchical approaches also rely on
other models to get word alignments. These mod-
els typically combine machine translation decoding
with chart parsing, therefore significantly increase
the decoding complexity. Even though some re-
cent work has shown great improvements in decod-
ing efficiency for syntactical and hierarchical ap-
proaches (Huang and Chiang, 2007), they are still
not as efficient as phrase-based systems, especially
when higher order language models are used.
Finally, researchers have also tried to put source
language syntax into reordering in machine trans-
lation. Syntactical analysis of source language
can be used to deterministically reorder input sen-
tences (Xia and McCord, 2004; Collins et.al., 2005;
Wang et.al., 2007; Habash, 2007), or to provide mul-
tiple orderings as weighted options (Zhang et.al.,
2007; Li et.al., 2007; Elming, 2008). In these
approaches, input source sentences are reordered
based on syntactic analysis and some reordering
rules at preprocessing step. The reordering rules
can be either manually written or automatically ex-
tracted from data. Deterministic reordering based on
syntactic analysis for the input sentences provides
a good way of resolving long distance reordering,
without introducing complexity to the decoding pro-
cess. Therefore, it can be efficiently incorporated
into phrase-based systems. Furthermore, when the
same preprocessing reordering is performed for the
training data, we can still apply other reordering ap-
proaches, such as distance based reordering and hi-
erarchical phrase reordering, to capture additional
local reordering phenomena that are not captured by
the preprocessing reordering. The work presented in
this paper is largely motivated by the preprocessing
reordering approaches.
In the rest of the paper, we first introduce our de-
pendency parser based reordering approach based on
the analysis of the key issues when translating SVO
languages to SOV languages. Then, we show exper-
imental results of applying this approach to phrase-
based SMT systems for translating from English to
five SOV languages (Korean, Japanese, Hindi, Urdu
and Turkish). After showing that this approach can
also be beneficial for hierarchical phrase-based sys-
John can hit ballthe
?? ? ??
? ????? 
.
.
Figure 1: Example Alignment Between an English and a
Korean Sentence
tems, we will conclude the paper with future re-
search directions.
2 Translation between SVO and SOV
Languages
In linguistics, it is possible to define a basic word
order in terms of the verb (V) and its arguments,
subject (S) and object (O). Among all six possible
permutations, SVO and SOV are the most common.
Therefore, translating between SVO and SOV lan-
guages is a very important area to study. We use
English as a representative of SVO languages and
Korean as a representative for SOV languages in our
discussion about the word orders.
Figure 1 gives an example sentence in English and
its corresponding translation in Korean, along with
the alignments between the words. Assume that we
split the sentences into four phrases: (John , t@),
(can hit , `  ????), (the ball , ? ?D)
and (. , .). Since a phrase-based decoder generates
the translation from left to right, the following steps
need to happen when we translate from English to
Korean:
? Starts from the beginning of the sentence,
translates ?John? to ?t@?;
? Jumps to the right by two words, translates ?the
ball? to ???D?;
? Jumps to the left by four words, translates ?can
hit? to ?`?????;
? Finally, jumps to the right by two words, trans-
lates ?.? to ?.?.
It is clear that in order for the phrase-based decoder
to successfully carry out all of the reordering steps, a
very strong reordering model is required. When the
sentence gets longer with more complex structure,
the number of words to move over during decod-
ing can be quite high. Imagine when we translate
246
Figure 2: Dependency Parse Tree of an Example English
Sentence
the sentence ?English is used as the first or second
language in many countries around the world .?.
The decoder needs to make a jump of 13 words in
order to put the translation of ?is used? at the end
of the translation. Normally in a phrase-based de-
coder, very long distance reordering is not allowed
because of efficiency considerations. Therefore, it
is very difficult in general to translate English into
Korean with proper word order.
However, knowing the dependency parse trees of
the English sentences may simplify the reordering
problem significantly. In the simple example in Fig-
ure 1, if we analyze the English sentence and know
that ?John? is the subject, ?can hit? is the verb and
?the ball? is the object, we can reorder the English
into SOV order. The resulting sentence ?John the
ball can hit .? will only need monotonic translation.
This motivates us to use a dependency parser for En-
glish to perform the reordering.
3 Precedence Reordering Based on a
Dependency Parser
Figure 2 shows the dependency tree for the example
sentence in the previous section. In this parse, the
verb ?hit? has four children: a subject noun ?John?,
an auxiliary verb ?can?, an object noun ?ball? and a
punctuation ?.?. When transforming the sentence to
SOV order, we need to move the object noun and the
subtree rooted at it to the front of the head verb, but
after the subject noun. We can have a simple rule to
achieve this.
However, in reality, there are many possible chil-
dren for a verb. These children have some relative
ordering that is typically fixed for SOV languages.
In order to describe this kind of ordering, we pro-
pose precedence reordering rules based on a depen-
dency parse tree. All rules here are based English
and Korean examples, but they also apply to other
SOV languages, as we will show later empirically.
A precedence reordering rule is a mapping from
T to a set of tuples {(L,W,O)}, where T is the
part-of-speech (POS) tag of the head in a depen-
dency parse tree node, L is a dependency label for
a child node, W is a weight indicating the order of
that child node and O is the type of order (either
NORMAL or REVERSE). The type of order is only
used when we have multiple children with the same
weight, while the weight is used to determine the
relative order of the children, going from largest to
smallest. The weight can be any real valued num-
ber. The order type NORMAL means we preserve
the original order of the children, while REVERSE
means we flip the order. We reserve a special label
self to refer to the head node itself so that we can
apply a weight to the head, too. We will call this
tuple a precedence tuple in later discussions. In this
study, we use manually created rules only.
Suppose we have a precedence rule: VB ?
(nsubj, 2, NORMAL), (dobj, 1, NORMAL), (self,
0, NORMAL). For the example shown in Figure 2,
we would apply it to the ROOT node and result in
?John the ball can hit .?.
Given a set of rules, we apply them in a depen-
dency tree recursively starting from the root node. If
the POS tag of a node matches the left-hand-side of
a rule, the rule is applied and the order of the sen-
tence is changed. We go through all children of the
node and get the precedence weights for them from
the set of precedence tuples. If we encounter a child
node that has a dependency label not listed in the set
of tuples, we give it a default weight of 0 and de-
fault order type of NORMAL. The children nodes
are sorted according to their weights from highest to
lowest, and nodes with the same weights are ordered
according to the type of order defined in the rule.
3.1 Verb Precedence Rules
Verb movement is the most important movement
when translating from English (SVO) to Korean
(SOV). In a dependency parse tree, a verb node can
potentially have many children. For example, aux-
iliary and passive auxiliary verbs are often grouped
together with the main verb and moved together with
it. The order, however, is reversed after the move-
ment. In the example of Figure 2, the correct Korean
247
     
??
? ??
? ????? 
   
??? ?
.
Figure 3: Dependency Parse Tree with Alignment for a
Sentence with Preposition Modifier
word order is ?` (hit)  ????(can) . Other
categories that are in the same group are phrasal verb
particle and negation.
If the verb in an English sentence has a preposi-
tional phrase as a child, the prepositional phrase is
often placed before the direct object in the Korean
counterpart. As shown in Figure 3, ?)?t \?
(?with a bat?) is actually between ?t@? (?John?)
and ???D? (?the ball?).
Another common reordering phenomenon is
when a verb has an adverbial clause modifier. In that
case, the whole adverbial clause is moved together to
be in front of the subject of the main sentence. Inside
the adverbial clause, the ordering follows the same
verb reordering rules, so we recursively reorder the
clause.
Our verb precedence rule, as in Table 1, can cover
all of the above reordering phenomena. One way
to interpret this rule set is as follows: for any node
whose POS tag is matches VB* (VB, VBZ, VBD,
VBP, VBN, VBG), we group the children node that
are phrasal verb particle (prt), auxiliary verb (aux),
passive auxiliary verb (auxpass), negation (neg) and
the verb itself (self) together and reverse them. This
verb group is moved to the end of the sentence. We
move adverbial clause modifier to the beginning of
the sentence, followed by a group of noun subject
(nsubj), preposition modifier and anything else not
listed in the table, in their original order. Right be-
fore the verb group, we put the direct object (dobj).
Note that all of the children are optional.
3.2 Adjective Precedence Rules
Similar to the verbs, adjectives can also take an aux-
iliary verb, a passive auxiliary verb and a negation
T (L, W, O)
VB*
(advcl, 1, NORMAL)
(nsubj, 0, NORMAL)
(prep, 0, NORMAL)
(dobj, -1, NORMAL)
(prt, -2, REVERSE)
(aux, -2, REVERSE)
(auxpass, -2, REVERSE)
(neg, -2, REVERSE)
(self, -2, REVERSE)
JJ or JJS or JJR
(advcl, 1, NORMAL)
(self, -1, NORMAL)
(aux, -2, REVERSE)
(auxpass, -2, REVERSE)
(neg, -2, REVERSE)
(cop, -2, REVERSE)
NN or NNS
(prep, 2, NORMAL)
(rcmod, 1, NORMAL)
(self, 0, NORMAL)
IN or TO (pobj, 1, NORMAL)(self, -1, NORMAL)
Table 1: Precedence Rules to Reorder English to SOV
Language Order (These rules were extracted manually by
a bilingual speaker after looking at some text book exam-
ples in English and Korean, and the dependency parse
trees of the English examples.)
as modifiers. In such cases, the change in order from
English to Korean is similar to the verb rule, except
that the head adjective itself should be in front of the
verbs. Therefore, in our adjective precedence rule in
the second panel of Table 1, we group the auxiliary
verb, the passive auxiliary verb and the negation and
move them together after reversing their order. They
are moved to right after the head adjective, which is
put after any other modifiers.
For both verb and adjective precedence rules,
we also apply some heuristics to prevent exces-
sive movements. In order to do this, we disallow
any movement across punctuation and conjunctions.
Therefore, for sentences like ?John hit the ball but
Sam threw the ball?, the reordering result would be
?John the ball hit but Sam the ball threw?, instead
of ?John the ball but Sam the ball threw hit?.
3.3 Noun and Preposition Precedence Rules
In Korean, when a noun is modified by a preposi-
tional phrase, such as in ?the way to happiness?,
the prepositional phrase is usually moved in front of
the noun, resulting in ??? (happiness)<\ ?
8 (to the way)? . Similarly for relative clause mod-
ifier, it is also reordered to the front of the head noun.
For preposition head node with an object modifier,
248
the order is the object first and the preposition last.
One example is ?with a bat? in Figure 3. It corre-
sponds to ?)?t (a bat) \(with)?. We handle
these types of reordering by the noun and preposi-
tion precedence rules in the third and fourth panel of
Table 1.
With the rules defined in Table 1, we now show a
more complex example in Figure 4. First, the ROOT
node matches an adjective rule, with four children
nodes labeled as (csubj, cop, advcl, p), and with
precedence weights of (0, -2, 1, 0). The ROOT node
itself has a weight of -1. After reordering, the sen-
tence becomes: ?because we do n?t know what the
future has Living exciting is .?. Note that the whole
adverbial phrase rooted at ?know? is moved to the
beginning of the sentence. After that, we see that
the child node rooted at ?know? matches a verb rule,
with five children nodes labeled as (mark, nsubj,
aux, neg, ccomp), with weights (0, 0, -2, -2, 0). In
this case, the verb itself also has weight -2. Now
we have two groups of nodes, with weight 0 and -2,
respectively. The first group has a NORMAL order
and the second group has a REVERSE order. Af-
ter reordering, the sentence becomes: ?because we
what the future has know n?t do Living exciting
is .?. Finally, we have another node rooted at ?has?
that matches the verb rule again. After the final re-
ordering, we end up with the sentence: ?because we
the future what has know n?t do Living exciting
is .?. We can see in Figure 4 that this sentence has an
almost monotonic alignment with a reasonable Ko-
rean translation shown in the figure1.
4 Related Work
As we mentioned in our introduction, there have
been several studies in applying source sentence re-
ordering using syntactical analysis for statistical ma-
chine translation. Our precedence reordering ap-
proach based on a dependency parser is motivated by
those previous works, but we also distinguish from
their studies in various ways.
Several approaches use syntactical analysis to
provide multiple source sentence reordering options
through word lattices (Zhang et.al., 2007; Li et.al.,
2007; Elming, 2008). A key difference between
1We could have improved the rules by using a weight of -3
for the label ?mark?, but it was not in our original set of rules.
their approaches and ours is that they do not perform
reordering during training. Therefore, they would
need to rely on reorder units that are likely not vio-
lating ?phrase? boundaries. However, since we re-
order both training and test data, our system oper-
ates in a matched condition. They also focus on ei-
ther Chinese to English (Zhang et.al., 2007; Li et.al.,
2007) or English to Danish (Elming, 2008), which
arguably have less long distance reordering than be-
tween English and SOV languages.
Studies most similar to ours are those preprocess-
ing reordering approaches (Xia and McCord, 2004;
Collins et.al., 2005; Wang et.al., 2007; Habash,
2007). They all perform reordering during prepro-
cessing based on either automatically extracted syn-
tactic rules (Xia and McCord, 2004; Habash, 2007)
or manually written rules (Collins et.al., 2005; Wang
et.al., 2007). Compared to these approaches, our
work has a few differences. First of all, we study
a wide range of SOV languages using manually ex-
tracted precedence rules, not just for one language
like in these studies. Second, as we will show in
the next section, we compare our approach to a
very strong baseline with more advanced distance
based reordering model, not just the simplest distor-
tion model. Third, our precedence reordering rules,
like those in Habash, 2007, are more flexible than
those other rules. Using just one verb rule, we can
perform the reordering of subject, object, preposi-
tion modifier, auxiliary verb, negation and the head
verb. Although we use manually written rules in
this study, it is possible to learn our rules automat-
ically from alignments, similarly to Habash, 2007.
However, unlike Habash, 2007, our manually writ-
ten rules handle unseen children and their order nat-
urally because we have a default precedence weight
and order type, and we do not need to match an often
too specific condition, but rather just treat all chil-
dren independently. Therefore, we do not need to
use any backoff scheme in order to have a broad cov-
erage. Fourth, we use dependency parse trees rather
than constituency trees.
There has been some work on syntactic word or-
der model for English to Japanese machine transla-
tion (Chang and Toutanova, 2007). In this work, a
global word order model is proposed based on fea-
tures including word bigram of the target sentence,
displacements and POS tags on both source and tar-
249
                  
??? ???
  ?????????? ? ?
?
? ??
??? ???
.
we the Livingwhatfuture knowhas n't do excitingbecause is .
csubj cop detmarkROOT auxnsubj neg advcl nsubjdobj ccomp p
Living is thebecauseexciting dowe n't know futurewhat has
.
VBG VBZ DTINJJ VBPPRP RB VB NNWP VBZ
.
Label
Token
POS
Figure 4: A Complex Reordering Example (Reordered English sentence and alignments are at the bottom.)
get sides. They build a log-linear model using these
features and apply the model to re-rank N -best lists
from a baseline decoder. Although we also study the
reordering problem in English to Japanese transla-
tion, our approach is to incorporate the linguistically
motivated reordering directly into modeling and de-
coding.
5 Experiments
We carried out all our experiments based on a state-
of-the-art phrase-based statistical machine transla-
tion system. When training a system for English
to any of the 5 SOV languages, the word alignment
step includes 3 iterations of IBM Model-1 training
and 2 iterations of HMM training. We do not use
Model-4 because it is slow and it does not add much
value to our systems in a pilot study. We use the
standard phrase extraction algorithm (Koehn et.al.,
2003) to get al phrases up to length 5. In addition
to the regular distance distortion model, we incor-
porate a maximum entropy based lexicalized phrase
reordering model (Zens and Ney, 2006) as a fea-
ture used in decoding. In this model, we use 4 re-
ordering classes (+1, > 1, ?1, < ?1) and words
from both source and target as features. For source
words, we use the current aligned word, the word
before the current aligned word and the next aligned
word; for target words, we use the previous two
words in the immediate history. Using this type of
features makes it possible to directly use the maxi-
mum entropy model in the decoding process (Zens
and Ney, 2006). The maximum entropy models are
trained on all events extracted from training data
word alignments using the LBFGS algorithm (Mal-
ouf, 2002). Overall for decoding, we use between 20
System Source Target
English?Korean 303M 267M
English?Japanese 316M 350M
English?Hindi 16M 17M
English?Urdu 17M 19M
English?Turkish 83M 76M
Table 2: Training Corpus Statistics (#words) of Systems
for 5 SOV Languages
to 30 features, whose weights are optimized using
MERT (Och, 2003), with an implementation based
on the lattice MERT (Macherey et.al., 2008).
For parallel training data, we use an in-house col-
lection of parallel documents. They come from var-
ious sources with a substantial portion coming from
the web after using simple heuristics to identify po-
tential document pairs. Therefore, for some doc-
uments in the training data, we do not necessarily
have the exact clean translations. Table 2 shows the
actual statistics about the training data for all five
languages we study. For all 5 SOV languages, we
use the target side of the parallel data and some more
monolingual text from crawling the web to build 4-
gram language models.
We also collected about 10K English sentences
from the web randomly. Among them, 9.5K are used
as evaluation data. Those sentences were translated
by humans to all 5 SOV languages studied in this
paper. Each sentence has only one reference trans-
lation. We split them into 3 subsets: dev contains
3,500 sentences, test contains 1,000 sentences and
the rest of 5,000 sentences are used in a blindtest
set. The dev set is used to perform MERT training,
while the test set is used to select trained weights
due to some nondeterminism of MERT training. We
use IBM BLEU (Papineni et al, 2002) to evaluate
250
our translations and use character level BLEU for
Korean and Japanese.
5.1 Preprocessing Reordering and Reordering
Models
We first compare our precedence rules based prepro-
cessing reordering with the maximum entropy based
lexicalized reordering model. In Table 3, Baseline
is our system with both a distance distortion model
and the maximum entropy based lexicalized reorder-
ing model. For all results reported in this section,
we used a maximum allowed reordering distance of
10. In order to see how the lexicalized reordering
model performs, we also included systems with and
without it (-LR means without it). PR is our pro-
posed approach in this paper. Note that since we ap-
ply precedence reordering rules during preprocess-
ing, we can combine this approach with any other
reordering models used during decoding. The only
difference is that with the precedence reordering, we
would have a different phrase table and in the case
of LR, different maximum entropy models.
In order to implement the precedence rules, we
need a dependency parser. We choose to use a
deterministic inductive dependency parser (Nivre
and Scholz, 2004) for its efficiency and good ac-
curacy. Our implementation of the deterministic
dependency parser using maximum entropy models
as the underlying classifiers achieves 87.8% labeled
attachment score and 88.8% unlabeled attachment
score on standard Penn Treebank evaluation.
As our results in Table 3 show, for all 5 lan-
guages, by using the precedence reordering rules as
described in Table 1, we achieve significantly bet-
ter BLEU scores compared to the baseline system.
In the table, We use two stars (??) to mean that
the statistical significance test using the bootstrap
method (Koehn, 2004) gives an above 95% signif-
icance level when compared to the baselie. We mea-
sured the statistical significance level only for the
blindtest data.
Note that for Korean and Japanese, our prece-
dence reordering rules achieve better absolute
BLEU score improvements than for Hindi, Urdu and
Turkish. Since we only analyzed English and Ko-
rean sentences, it is possible that our rules are more
geared toward Korean. Japanese has almost exactly
the same word order as Korean, so we could assume
Language System dev test blind
Korean
BL 25.8 27.0 26.2
-LR 24.7 25.6 25.1
-LR+PR 27.3 28.3 27.5**
+PR 27.8 28.7 27.9**
Japanese
BL 29.5 29.3 29.3
-LR 29.2 29.0 29.0
-LR+PR 30.3 31.0 30.6**
+PR 30.7 31.2 31.1**
Hindi
BL 19.1 18.9 18.3
-LR 17.4 17.1 16.4
-LR+PR 19.6 18.8 18.7**
+PR 19.9 18.9 18.8**
Urdu
BL 9.7 9.5 8.9
-LR 9.1 8.6 8.2
-LR+PR 10.0 9.6 9.6**
+PR 10.0 9.8 9.6**
Turkish
BL 10.0 10.5 9.8
-LR 9.1 10.0 9.0
-LR+PR 10.5 11.0 10.3**
+PR 10.5 10.9 10.4**
Table 3: BLEU Scores on Dev, Test and Blindtest for En-
glish to 5 SOV Languages with Various Reordering Op-
tions (BL means baseline, LR means maximum entropy
based lexialized phrase reordering model, PR means
precedence rules based preprocessing reordering.)
the benefits can carry over to Japanese.
5.2 Reordering Constraints
One of our motivations of using the precedence re-
ordering rules is that English will look like SOV lan-
guages in word order after reordering. Therefore,
even monotone decoding should be able to produce
better translations. To see this, we carried out a con-
trolled experiment, using Korean as an example.
Clearly, after applying the precedence reordering
rules, our English to Korean system is not sensitive
to the maximum allowed reordering distance any-
more. As shown in Figure 5, without the rules, the
blindtest BLEU scores improve monotonically as
the allowed reordering distance increases. This indi-
cates that the order difference between English and
Korean is very significant. Since smaller allowed
reordering distance directly corresponds to decod-
ing time, we can see that with the same decoding
speed, our proposed approach can achieve almost
5% BLEU score improvements on blindtest set.
5.3 Preprocessing Reordering and
Hierarchical Model
The hierarchical phrase-based approach has been
successfully applied to several systems (Chiang,
251
1 2 4 6 8 10Maximum Allowed Reordering Distance
0.23
0.24
0.25
0.26
0.27
0.28
Blindt
est BL
EU Sc
ore No LexReorderBaselineNo LexReorder, with ParserReorderWith ParserReorder
Figure 5: Blindtest BLEU Score for Different Maximum
Allowed Reordering Distance for English to Korean Sys-
tems with Different Reordering Options
2005; Zollmann et.al., 2008). Since hierarchical
phrase-based systems can capture long distance re-
ordering by using a PSCFG model, we expect it to
perform well in English to SOV language systems.
We use the same training data as described in the
previous sections for building hierarchical systems.
The same 4-gram language models are also used for
the 5 SOV languages. We adopt the SAMT pack-
age (Zollmann and Venugopal, 2006) and follow
similar settings as Zollmann et.al., 2008. We allow
each rule to have at most 6 items on the source side,
including nonterminals and extract rules from initial
phrases of maximum length 12. During decoding,
we allow application of all rules of the grammar for
chart items spanning up to 12 source words.
Since our precedence reordering applies at pre-
processing step, we can train a hierarchical system
after applying the reordering rules. When doing so,
we use exactly the same settings as a regular hier-
archical system. The results for both hierarchical
systems and those combined with the precedence re-
ordering are shown in Table 4, together with the best
normal phrase-based systems we copy from Table 3.
Here again, we mark any blindtest BLEU score that
is better than the corresponding hierarchical system
with confidence level above 95%. Note that the hier-
archical systems can not use the maximum entropy
based lexicalized phrase reordering models.
Except for Hindi, applying the precedence re-
ordering rules in a hierarchical system can achieve
statistically significant improvements over a normal
hierarchical system. We conjecture that this may be
because of the simplicity of our reordering rules.
Language System dev test blind
Korean
PR 27.8 28.7 27.9
Hier 27.4 27.7 27.9
PR+Hier 28.5 29.1 28.8**
Japanese
PR 30.7 31.2 31.1**
Hier 30.5 30.6 30.5
PR+Hier 31.0 31.3 31.1**
Hindi
PR 19.9 18.9 18.8
Hier 20.3 20.3 19.3
PR+Hier 20.0 19.7 19.3
Urdu
PR 10.0 9.8 9.6
Hier 10.4 10.3 10.0
PR+Hier 11.2 10.7 10.7**
Turkish
PR 10.5 10.9 10.4
Hier 11.0 11.8 10.5
PR+Hier 11.1 11.6 10.9**
Table 4: BLEU Scores on Dev, Test and Blindtest for En-
glish to 5 SOV Languages in Hierarchical Phrase-based
Systems (PR is precedence rules based preprocessing re-
ordering, same as in Table 3, while Hier is the hierarchi-
cal system.)
Other than the reordering phenomena covered by
our rules in Table 1, there could be still some local or
long distance reordering. Therefore, using a hierar-
chical phrase-based system can improve those cases.
Another possible reason is that after the reordering
rules apply in preprocessing, English sentences in
the training data are very close to the SOV order. As
a result, EM training becomes much easier and word
alignment quality becomes better. Therefore, a hier-
archical phrase-based system can extract better rules
and hence achievesbetter translation quality.
We also point out that hierarchical phrase-based
systems require a chart parsing algorithm during de-
coding. Compared to the efficient dynamic pro-
gramming in phrase-based systems, it is much
slower. This makes our approach more appealing
in a realtime statistical machine translation system.
6 Conclusion
In this paper, we present a novel precedence re-
ordering approach based on a dependency parser.
We successfully applied this approach to systems
translating English to 5 SOV languages: Korean,
Japanese, Hindi, Urdu and Turkish. For all 5 lan-
guages, we achieve statistically significant improve-
ments in BLEU scores over a state-of-the-art phrase-
based baseline system. The amount of training data
for the 5 languages varies from around 17M to more
than 350M words, including some noisy data from
252
the web. Our proposed approach has shown to be
robust and versatile. For 4 out of the 5 languages,
our approach can even significantly improve over a
hierarchical phrase-based baseline system. As far as
we know, we are the first to show that such reorder-
ing rules benefit several SOV languages.
We believe our rules are flexible and can cover
many linguistic reordering phenomena. The format
of our rules also makes it possible to automatically
extract rules from word aligned corpora. In the fu-
ture, we plan to investigate along this direction and
extend the rules to languages other than SOV.
The preprocessing reordering like ours is known
to be sensitive to parser errors. Some preliminary
error analysis already show that indeed some sen-
tences suffer from parser errors. In the recent years,
several studies have tried to address this issue by us-
ing a word lattice instead of one reordering as in-
put (Zhang et.al., 2007; Li et.al., 2007; Elming,
2008). Although there is clearly room for improve-
ments, we also feel that using one reordering during
training may not be good enough either. It would be
very interesting to investigate ways to have efficient
procedure for training EM models and getting word
alignments using word lattices on the source side of
the parallel data. Along this line of research, we
think some kind of tree-to-string model (Liu et.al.,
2006) could be interesting directions to pursue.
References
Yaser Al-Onaizan and Kishore Papineni 2006. Distortion Models for
Statistical Machine Translation In Proceedings of ACL
Pi-Chuan Chang and Kristina Toutanova 2007. A Discriminative Syn-
tactic Word Order Model for Machine Translation In Proceedings
of ACL
David Chiang 2005. A Hierarchical Phrase-based Model for Statistical
Machine Translation In Proceedings of ACL
Michael Collins, Philipp Koehn and Ivona Kucerova 2005. Clause
Restructuring for Statistical Machine Translation In Proceedings of
ACL
Jakob Elming 2008. Syntactic Reordering Integrated with Phrase-
based SMT In Proceedings of COLING
Michel Galley and Christopher D. Manning 2008. A Simple and Ef-
fective Hierarchical Phrase Reordering Model In Proceedings of
EMNLP
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve
DeNeefe, Wei Wang and Ignacio Thayer 2006. Scalable Inference
and Training of Context-Rich Syntactic Translation Models In Pro-
ceedings of COLING-ACL
Nizar Habash 2007. Syntactic Preprocessing for Statistical Machine
Translation In Proceedings of 11th MT Summit
Liang Huang and David Chiang 2007. Forest Rescoring: Faster De-
coding with Integrated Language Models, In Proceedings of ACL
Philipp Koehn 2004. Statistical Significance Tests for Machine Trans-
lation Evaluation In Proceedings of EMNLP
Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris
Callison-Burch, Miles Osborne and David Talbot 2005. Edinborgh
System Description for the 2005 IWSLT Speech Translation Evalu-
ation In International Workshop on Spoken Language Translation
Philipp Koehn, Franz J. Och and Daniel Marcu 2003. Statistical
Phrase-based Translation, In Proceedings of HLT-NAACL
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou, Minghui Li and Yi
Guan 2007. A Probabilistic Approach to Syntax-based Reordering
for Statistical Machine Translation, In Proceedings of ACL
Yang Liu, Qun Liu and Shouxun Lin 2006. Tree-to-string Alignment
Template for Statistical Machine Translation, In Proceedings of
COLING-ACL
Wolfgang Macherey, Franz J. Och, Ignacio Thayer and Jakob Uszkoreit
2008. Lattice-based Minimum Error Rate Training for Statistical
Machine Translation In Proceedings of EMNLP
Robert Malouf 2002. A comparison of algorithms for maximum en-
tropy parameter estimation In Proceedings of the Sixth Workshop
on Computational Language Learning (CoNLL-2002)
Joakim Nivre and Mario Scholz 2004. Deterministic Dependency Pars-
ing for English Text. In Proceedings of COLING
Franz J. Och 2002. Statistical Machine Translation: From Single Word
Models to Alignment Template Ph.D. Thesis, RWTH Aachen, Ger-
many
Franz J. Och. 2003. Minimum Error Rate Training in Statistical Ma-
chine Translation. In Proceedings of ACL
Franz J. Och and Hermann Ney 2004. The Alignment Template Ap-
proach to Statistical Machine Translation. Computational Linguis-
tics, 30:417-449
Kishore Papineni, Roukos, Salim et al 2002. BLEU: A Method for
Automatic Evaluation of Machine Translation. In Proceedings of
ACL
Chris Quirk, Arul Menezes and Colin Cherry 2005. Dependency Tree
Translation: Syntactically Informed Phrasal SMT In Proceedings of
ACL
Christoph Tillmann 2004. A Block Orientation Model for Statistical
Machine Translation In Proceedings of HLT-NAACL
Chao Wang, Michael Collins and Philipp Koehn 2007. Chinese Syntac-
tic Reordering for Statistical Machine Translation In Proceedings of
EMNLP-CoNLL
Dekai Wu 1997. Stochastic Inversion Transduction Grammars and
Bilingual Parsing of Parallel Corpus In Computational Linguistics
23(3):377-403
Fei Xia and Michael McCord 2004. Improving a Statistical MT Sys-
tem with Automatically Learned Rewrite Patterns In Proceedings of
COLING
Deyi Xiong, Qun Liu and Shouxun Lin 2006. Maximum Entropy
Based Phrase Reordering Model for Statistical Machine Translation
In Proceedings of COLING-ACL
Kenji Yamada and Kevin Knight 2001. A Syntax-based Statistical
Translation Model In Proceedings of ACL
Yuqi Zhang, Richard Zens and Hermann Ney 2007. Improve Chunk-
level Reordering for Statistical Machine Translation In Proceedings
of IWSLT
Richard Zens and Hermann Ney 2006. Discriminative Reordering
Models for Statistical Machine Translation In Proceedings of the
Workshop on Statistical Machine Translation, HLT-NAACL pages
55-63
Andreas Zollmann and Ashish Venugopal 2006. Syntax Augmented
Machine Translation via Chart Parsing In Proceedings of NAACL
2006 - Workshop on Statistical Machine Translation
Andreas Zollmann, Ashish Venugopal, Franz Och and Jay Ponte
2008. A Systematic Comparison of Phrase-Based, Hierarchical and
Syntax-Augmented Statistical MT In Proceedings of COLING
253
Proceedings of NAACL HLT 2009: Tutorials, pages 3?4,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Distributed Language Models  Thorsten Brants and Peng Xu, Google Inc.    Language models are used in a wide variety of natural language applications, including machine translation, speech recognition, spelling correction, optical character recognition, etc.  Recent studies have shown that more data is better data, and bigger language models are better language models: the authors found nearly constant machine translation improvements with each doubling of the training data size even at 2 trillion tokens (resulting in 400 billion n-grams). Training and using such large models is a challenge. This tutorial shows efficient methods for distributed training of large language models based on the MapReduce computing model. We also show efficient ways of using distributed models in which requesting individual n-grams is expensive because they require communication between different machines.   Tutorial Outline  1) Training Distributed Models  * N-gram collection    Use of the MapReduce model; compressing intermediate data; minimizing    communication overhead with good sharding functions.  * Smoothing    Challenges of Katz Backoff and Kneser-Ney Smoothing in a distributed system;    Smoothing techniques that are easy to compute in a distributed system:    Stupid Backoff, Linear Interpolation; minimizing communication by sharding    and aggregation.  2) Model Size Reduction  * Pruning    Reducing the size of the model by removing n-grams that don't have much impact.    Entropy pruning is simple to compute for Stupid Backoff, requires some effort for Katz    and Kneser-Ney in a distributed system. Effects of extreme pruning.  * Quantization    Reducing the memory size of the model by storing approximations of the values. We    discuss several quantizers; typically 4 to 8 bits are sufficient to store a floating point    value.  * Randomized Data Structures    Reducing the memory size of the model by changing the set of n-grams that is stored.    This typically lets us store models in 3 bytes per n-gram, independent of the n-gram 
3
   order without significant impact on quality. At the same time it provides very fast    access to the n-grams.  3) Using Distributed Models  * Serving    Requesting a single n-gram in a distributed setup is expensive because it requires    communication between machines. We show how to use a distributed language model    in the first-pass of a decoder by batching up n-gram request.   Target Audience  Target audience are researchers in all areas that focus on or use large n-gram language models.   Presenters  Thorsten Brants received his Ph.D. in 1999 at the Saarland University, Germany, on part-of-speech tagging and parsing. From 2000 to 2003, he worked at the Palo Alto Research Center (PARC) on statistical methods for topic and event detection. Thorsten is now a Research Scientist at Google working on large, distributed language models with focus on applications in machine translation. Other research interests include information retrieval, named entity detection, and speech recognition.  Peng Xu joined Google as a Research Scientist shortly after getting a Ph.D. in April 2005 from the Johns Hopkins University. While his research is focused on statistical machine translation at Google, he is also interested in statistical machine learning, information retrieval, and speech recognition.  
4
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 972?983, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Systematic Comparison of Phrase Table Pruning Techniques
Richard Zens and Daisy Stanton and Peng Xu
Google Inc.
{zens,daisy,xp}@google.com
Abstract
When trained on very large parallel corpora,
the phrase table component of a machine
translation system grows to consume vast
computational resources. In this paper, we in-
troduce a novel pruning criterion that places
phrase table pruning on a sound theoretical
foundation. Systematic experiments on four
language pairs under various data conditions
show that our principled approach is superior
to existing ad hoc pruning methods.
1 Introduction
Over the last years, statistical machine translation
has become the dominant approach to machine
translation. This is not only due to improved mod-
eling, but also due to a significant increase in the
availability of monolingual and bilingual data. Here
are just two examples of very large data resources
that are publicly available:
? The Google Web 1T 5-gram corpus available
from the Linguistic Data Consortium consist-
ing of the 5-gram counts of about one trillion
words of web data.1
? The 109-French-English bilingual corpus with
about one billion tokens from the Workshop on
Statistical Machine Translation (WMT).2
These enormous data sets yield translation models
that are expensive to store and process. Even with
1LDC catalog No. LDC2006T13
2http://www.statmt.org/wmt11/translation-task.html
modern computers, these large models lead to a long
experiment cycle that hinders progress. The situa-
tion is even more severe if computational resources
are limited, for instance when translating on hand-
held devices. Then, reducing the model size is of
the utmost importance.
The most resource-intensive components of a sta-
tistical machine translation system are the language
model and the phrase table. Recently, compact rep-
resentations of the language model have attracted
the attention of the research community, for instance
in Talbot and Osborne (2007), Brants et al(2007),
Pauls and Klein (2011) or Heafield (2011), to name
a few. In this paper, we address the other problem
of any statistical machine translation system: large
phrase tables.
Johnson et al(2007) has shown that large por-
tions of the phrase table can be removed without loss
in translation quality. This motivated us to perform
a systematic comparison of different pruning meth-
ods. However, we found that many existing methods
employ ad-hoc heuristics without theoretical foun-
dation.
The pruning criterion introduced in this work is
inspired by the very successful and still state-of-the-
art language model pruning criterion based on en-
tropy measures (Stolcke, 1998). We motivate its
derivation by stating the desiderata for a good phrase
table pruning criterion:
? Soundness: The criterion should optimize
some well-understood information-theoretic
measure of translation model quality.
972
? Efficiency: Pruning should be fast, i. e., run lin-
early in the size of the phrase table.
? Self-containedness: As a practical considera-
tion, we want to prune phrases from an existing
phrase table. This means pruning should use
only information contained in the model itself.
? Good empirical behavior: We would like to
be able to prune large parts of the phrase table
without significant loss in translation quality.
Analyzing existing pruning techniques based on
these objectives, we found that they are commonly
deficient in at least one of them. We thus designed
a novel pruning criterion that not only meets these
objectives, it also performs very well in empirical
evaluations.
The novel contributions of this paper are:
1. a systematic description of existing phrase table
pruning methods.
2. a new, theoretically sound phrase table pruning
criterion.
3. an experimental comparison of several pruning
methods for several language pairs.
2 Related Work
The most basic pruning methods rely on probabil-
ity and count cutoffs. We will cover the techniques
that are implemented in the Moses toolkit (Koehn et
al., 2007) and the Pharaoh decoder (Koehn, 2004) in
Section 3. We are not aware of any work that ana-
lyzes their efficacy in a systematic way. It is thus not
surprising that some of them perform poorly, as our
experimental results will show.
The work of Johnson et al(2007) is promis-
ing as it shows that large parts of the phrase ta-
ble can be removed without affecting translation
quality. Their pruning criterion relies on statisti-
cal significance tests. However, it is unclear how
this significance-based pruning criterion is related to
translation model quality. Furthermore, a compari-
son to other methods is missing. Here we close this
gap and perform a systematic comparison. The same
idea of significance-based pruning was exploited in
(Yang and Zheng, 2009; Tomeh et al 2009) for hi-
erarchical statistical machine translation.
A different approach to phrase table pruning was
undertaken by Eck et al(2007a; 2007b). They rely
on usage statistics from translating sample data, so it
is not self-contained. However, it could be combined
with the methods proposed here.
Another approach to phrase table pruning is trian-
gulation (Chen et al 2008; Chen et al 2009). This
requires additional bilingual corpora, namely from
the source language as well as from the target lan-
guage to a third bridge language. In many situations
this does not exist or would be costly to generate.
Duan et al(2011), Sanchis-Trilles et al(2011)
and Tomeh et al(2011) modify the phrase extrac-
tion methods in order to reduce the phrase table size.
The work in this paper is independent of the way the
phrase extraction is done, so those approaches are
complementary to our work.
3 Pruning Using Simple Statistics
In this section, we will review existing pruning
methods based on simple phrase table statistics.
There are two common classes of these methods: ab-
solute phrase table pruning and relative phrase table
pruning.
3.1 Absolute pruning
Absolute pruning methods rely only on the statistics
of a single phrase pair (f? , e?). Hence, they are in-
dependent of other phrases in the phrase table. As
opposed to relative pruning methods (Section 3.2),
they may prune all translations of a source phrase.
Their application is easy and efficient.
? Count-based pruning. This method prunes
a phrase pair (f? , e?) if its observation count
N(f? , e?) is below a threshold ?c:
N(f? , e?) < ?c (1)
? Probability-based pruning. This method
prunes a phrase pair (f? , e?) if its probability is
below a threshold ?p:
p(e?|f?) < ?p (2)
Here the probability p(e?|f?) is estimated via rel-
ative frequencies.
973
3.2 Relative pruning
A potential problem with the absolute pruning meth-
ods is that it can prune all occurrences of a source
phrase f? .3 Relative pruning methods avoid this by
considering the full set of target phrases for a spe-
cific source phrase f? .
? Threshold pruning. This method discards
those phrases that are far worse than the best
target phrase for a given source phrase f? . Given
a pruning threshold ?t, a phrase pair (f? , e?) is
discarded if:
p(e?|f?) < ?t ?max
e?
{
p(e?|f?)
}
(3)
? Histogram pruning. An alternative to thresh-
old pruning is histogram pruning. For each
source phrase f? , this method preserves the K
target phrases with highest probability p(e?|f?)
or, equivalently, their count N(f? , e?).
Note that, except for count-based pruning, none of
the methods take the frequency of the source phrase
into account. As we will confirm in the empirical
evaluation, this will likely cause drops in translation
quality, since frequent source phrases are more use-
ful than the infrequent ones.
4 Significance Pruning
In this section, we briefly review significance prun-
ing following Johnson et al(2007). The idea of sig-
nificance pruning is to test whether a source phrase
f? and a target phrase e? co-occur more frequently in
a bilingual corpus than they should just by chance.
Using some simple statistics derived from the bilin-
gual corpus, namely
? N(f?) the count of the source phrase f?
? N(e?) the count of the target phrase e?
? N(f? , e?) the co-occurence count of the source
phrase f? and the target phrase e?
? N the number of sentences in the bilingual cor-
pus
3Note that it has never been systematically investigated
whether this is a real problem or just speculation.
we can compute the two-by-two contingency table
in Table 1.
Following Fisher?s exact test, we can calculate the
probability of the contingency table via the hyperge-
ometric distribution:
ph(N(f? , e?)) =
(
N(f?)
N(f? ,e?)
)
?
(
N?N(f?)
N(e?)?N(f? ,e?)
)
(
N
N(e?)
) (4)
The p-value is then calculated as the sum of all
probabilities that are at least as extreme. The lower
the p-value, the less likely this phrase pair occurred
with the observed frequency by chance; we thus
prune a phrase pair (f? , e?) if:
?
?
??
k=N(f? ,e?)
ph(k)
?
? > ?F (5)
for some pruning threshold ?F . More details of this
approach can be found in Johnson et al(2007). The
idea of using Fisher?s exact test was first explored by
Moore (2004) in the context of word alignment.
5 Entropy-based Pruning
In this section, we will derive a novel entropy-based
pruning criterion.
5.1 Motivational Example
In general, pruning the phrase table can be consid-
ered as selecting a subset of the original phrase table.
When doing so, we would like to alter the original
translation model distribution as little as possible.
This is a key difference to previous approaches: Our
goal is to remove redundant phrases, whereas previ-
ous approaches usually try to remove low-quality or
unreliable phrases. We believe this to be an advan-
tage of our method as it is certainly easier to measure
the redundancy of phrases than it is to estimate their
quality.
In Table 2, we show some example phrases
from the learned French-English WMT phrase table,
along with their counts and probabilities. For the
French phrase le gouvernement franc?ais, we have,
among others, two translations: the French govern-
ment and the government of France. If we have
to prune one of those translations, we can ask our-
selves: how would the translation cost change if the
974
N(f? , e?) N(f?)?N(f? , e?) N(f?)
N(e?)?N(f? , e?) N ?N(f?)?N(e?) +N(f? , e?) N ?N(f?)
N(e?) N ?N(e?) N
Table 1: Two-by-two contingency table for a phrase pair (f? , e?).
Source Phrase f? Target Phrase e? N(f? , e?) p(e?|f?)
le the 7.6 M 0.7189
gouvernement government 245 K 0.4106
franc?ais French 51 K 0.6440
of France 695 0.0046
le gouvernement franc?ais the French government 148 0.1686
the government of France 11 0.0128
Table 2: Example phrases from the French-English phrase table (K=thousands, M=millions).
same translation were generated from the remain-
ing, shorter, phrases? Removing the phrase the gov-
ernment of France would increase this cost dramat-
ically. Given the shorter phrases from the table, the
probability would be 0.7189 ? 0.4106 ? 0.0046 =
0.0014?, which is about an order of a magnitude
smaller than the original probability of 0.0128.
On the other hand, composing the phrase the
French government out of shorter phrases has prob-
ability 0.7189 ? 0.4106 ? 0.6440 = 0.1901, which is
very close to the original probability of 0.1686. This
means it is safe to discard the phrase the French gov-
ernment, since the translation cost remains essen-
tially unchanged. By contrast, discarding the phrase
the government of France does not have this effect:
it leads to a large change in translation cost.
Note that here the pruning criterion only considers
redundancy of the phrases, not the quality. Thus, we
are not saying that the government of France is a
better translation than the French government, only
that it is less redundant.
?We use the assumption that we can simply multiply the
probabilities of the shorter phrases.
5.2 Entropy Criterion
Now, we are going to formalize the notion of re-
dundancy. We would like the pruned model p?(e?|f?)
to be as similar as possible to the original model
p(e?|f?). We use conditional Kullback-Leibler di-
vergence, also called conditional relative entropy
(Cover and Thomas, 2006), to measure the model
similarity:
D(p(e?|f?)||p?(e?|f?))
=
?
f?
p(f?)
?
e?
p(e?|f?) log
[
p(e?|f?)
p?(e?|f?)
]
(6)
=
?
f? ,e?
p(e?, f?)
[
log p(e?|f?)? log p?(e?|f?)
]
(7)
Computing the best pruned model of a given size
would require optimizing over all subsets with that
size. Since that is computationally infeasible, we in-
stead apply the equivalent approximation that Stol-
cke (1998) uses for language modeling. This as-
sumes that phrase pairs affect the relative entropy
roughly independently.
We can then choose a pruning threshold ?E and
prune those phrase pairs with a contribution to the
relative entropy below that threshold. Thus, we
975
prune a phrase pair (f? , e?), if
p(e?, f?)
[
log p(e?|f?)? log p?(e?|f?)
]
< ?E (8)
We now address how to assign the probability
p?(e?|f?) under the pruned model. A phrase-based
system selects among different segmentations of the
source language sentence into phrases. If a segmen-
tation into longer phrases does not exist, the system
has to compose a translation out of shorter phrases.
Thus, if a phrase pair (f? , e?) is no longer available,
the decoder has to use shorter phrases to produce
the same translation. We can therefore decompose
the pruned model score p?(e?|f?) by summing over all
segmentations sK1 and all reorderings pi
K
1 :
p?(e?|f?) =
?
sK1 ,pi
K
1
p(sK1 , pi
K
1 |f?) ? p(e?|s
K
1 , pi
K
1 , f?) (9)
Here the segmentation sK1 divides both the source
and target phrases into K sub-phrases:
f? = f?pi1 ...f?piK and e? = e?1...e?K (10)
The permutation piK1 describes the alignment of
those sub-phrases, such that the sub-phrase e?k is
aligned to f?pik . Using the normal phrase translation
model, we obtain:
p?(e?|f?) =
?
sK1 ,pi
K
1
p(sK1 , pi
K
1 |f?)
K?
k=1
p(e?k|f?pik) (11)
Virtually all phrase-based decoders use the so-
called maximum-approximation, i. e. the sum is re-
placed with the maximum. As we would like the
pruning criterion to be similar to the search criterion
used during decoding, we do the same and obtain:
p?(e?|f?) ? max
sK1 ,pi
K
1
K?
k=1
p(e?k|f?pik) (12)
Note that we also drop the segmentation probabil-
ity, as this is not used at decoding time. This leaves
the pruning criterion a function only of the model
p(e?|f?) as stored in the phrase table. There is no need
for a special development or adaptation set. We can
determine the best segmentation using dynamic pro-
gramming, similar to decoding with a phrase-based
model. However, here the target side is constrained
to the given phrase e?.
It can happen that a phrase is not compositional,
i. e., we cannot find a segmentation into shorter
phrases. In these cases, we assign a small, constant
probability:
p?(e?|f?) = pc (13)
We found that the value pc = e?10 works well for
many language pairs.
5.3 Computation
In our experiments, it was more efficient to vary the
pruning threshold ?E without having to re-compute
the entire phrase table. Therefore, we computed the
entropy criterion in Equation (8) once for the whole
phrase table. This introduces an approximation for
the pruned model score p?(e?|f?). It might happen
that we prune short phrases that were used as part
of the best segmentation of longer phrases. As these
shorter phrases should not be available, the pruned
model score might be inaccurate. Although we be-
lieve this effect is minor, we leave a detailed experi-
mental analysis for future work.
One way to avoid this approximation would be
to perform entropy pruning with increasing phrase
length. Starting with one-word phrases, which are
trivially non-compositional, the entropy criterion
would be straightforward to compute. Proceed-
ing to two-word phrases, one would decompose the
phrases into sub-phrases by looking up the proba-
bilities of some of the unpruned one-word phrases.
Once the set of unpruned two-word phrases was ob-
tained, one would continue with three-word phrases,
etc.
6 Experimental Evaluation
6.1 Data Sets
In this section, we describe the data sets used for the
experiments. We perform experiments on the pub-
licly available WMT shared translation task for the
following four language pairs:
? German-English
? Czech-English
? Spanish-English
976
Number of Words
Language Pair Foreign English
German - English 42 M 45 M
Czech - English 56 M 65 M
Spanish - English 232 M 210 M
French - English 962 M 827 M
Table 3: Training data statistics. Number of words in the
training data (M=millions).
? French-English
For each pair, we train two separate system, one for
each direction. Thus it can happen that a phrase is
pruned for X-to-Y, but not for Y-to-X.
These four language pairs represent a nice range
of training corpora sizes, as shown in Table 3.
6.2 Baseline System
Pruning experiments were performed on top of the
following baseline system. We used a phrase-
based statistical machine translation system similar
to (Zens et al 2002; Koehn et al 2003; Och and
Ney, 2004; Zens and Ney, 2008). We trained a 4-
gram language model on the target side of the bilin-
gual corpora and a second 4-gram language model
on the provided monolingual news data. All lan-
guage models used Kneser-Ney smoothing.
The baseline system uses the common phrase
translation models, such as p(e?|f?) and p(f? |e?), lex-
ical models, word and phrase penalty, distortion
penalty as well as a lexicalized reordering model
(Zens and Ney, 2006).
The word alignment was trained with six itera-
tions of IBM model 1 (Brown et al 1993) and 6 it-
erations of the HMM alignment model (Vogel et al
1996) using a symmetric lexicon (Zens et al 2004).
The feature weights were tuned on a development
set by applying minimum error rate training (MERT)
under the Bleu criterion (Och, 2003; Macherey et al
2008). We ran MERT once with the full phrase table
and then kept the feature weights fixed, i. e., we did
not rerun MERT after pruning to avoid adding un-
necessary noise. We extract phrases up to a length
of six words. The baseline system already includes
phrase table pruning by removing singletons and
keeping up to 30 target language phrases per source
phrase. We found that this does not affect transla-
 8
 10
 12
 14
 16
 18
 20
 22
 24
 26
 28
 1  2  4  8
BL
EU
[%]
Number of Phrases [millions]
ProbThresHist
Figure 1: Comparison of probability-based pruning
methods for German-English.
tion quality significantly4. All pruning experiments
are done on top of this.
6.3 Results
In this section, we present the experimental results.
Translation results are reported on the WMT?07
news commentary blind set.
We will show translation quality measured with
the Bleu score (Papineni et al 2002) as a function
of the phrase table size (number of phrases). Being
in the upper left corner of these figures is desirable.
First, we show a comparison of several
probability-based pruning methods in Figure 1.
We compare
? Prob. Absolute pruning based on Eq. (2).
? Thres. Threshold pruning based on Eq. (3).
? Hist. Histogram pruning as described in Sec-
tion 3.2.5
We observe that these three methods perform
equally well. There is no difference between abso-
lute and relative pruning methods, except that the
two relative methods (Thres and Hist) are limited by
4The Bleu score drops are as follows: English-French 0.3%,
French-English 0.4%, Czech-English 0.3%, all other are less
than 0.1%.
5Instead of using p(e?|f?) one could use the weighted model
score including p(f? |e?), lexical weightings etc.; however, we
found that this does not give significantly different results; but
it does introduce a undesirable dependance between feature
weights and phrase table pruning.
977
the number of source phrases. Thus, they reach a
point where they cannot prune the phrase table any
further. The results shown are for German-English;
the results for the other languages are very similar.
The results that follow use only the absolute prun-
ing method as a representative for probability-based
pruning.
In Figures 2 through 5, we show the transla-
tion quality as a function of the phrase table size.
We vary the pruning thresholds to obtain different
phrase table sizes. We compare four pruning meth-
ods:
? Count. Pruning based on the frequency of a
phrase pair, c.f. Equation (1).
? Prob. Pruning based on the absolute probabil-
ity of a phrase pair, c.f. Equation (2).
? Fisher. Pruning using significance tests, c.f.
Equation (5).
? Entropy. Pruning using the novel entropy cri-
terion, c.f. Equation (8).
Note that the x-axis of these figures is on a logarith-
mic scale, so the differences between the methods
can be quite dramatic. For instance, entropy pruning
requires less than a quarter of the number of phrases
needed by count- or significance-based pruning to
achieve a Spanish-English Bleu score of 34 (0.4 mil-
lion phrases compared to 1.7 million phrases).
These results clearly show how the pruning meth-
ods compare:
1. Probability-based pruning performs poorly. It
should be used only to prune small fractions of
the phrase table.
2. Count-based pruning and significance-based
pruning perform equally well. They are much
better than probability-based pruning.
3. Entropy pruning consistently outperforms the
other methods across translation directions and
language pairs.
Figures 6 and 7 show compositionality statistics
for the pruned Spanish-English phrase table (we ob-
served similar results for the other language pairs).
Total number of phrases 4 137 M
Compositional 3 970 M
Non-compositional 167 M
of those: one-word phrases 85 M
no segmentation 82 M
Table 4: Statistics of phrase compositionality
(M=millions).
Each figure shows the composition of the phrase ta-
ble for a type of pruning for different phrase tables
sizes. Along the x-axis, we plotted the phrase ta-
ble size. These are the same phrase tables used to
obtain the Bleu scores in Figure 2 (left). The dif-
ferent shades of grey correspond to different phrase
lengths. For instance, in case of the smallest phrase
table for count-based pruning, the 1-word phrases
account for about 30% of all phrases, the 2-word
phrases account for about 35% of all phrases, etc.
With the exception of the probability-based prun-
ing, the plots look comparable. The more aggres-
sive the pruning, the larger the percentage of short
phrases. We observe that entropy-based pruning re-
moves many more long phrases than any of the other
methods. The plot for probability-based pruning is
different in that the percentage of long phrases ac-
tually increases with more aggressive pruning (i. e.
smaller phrase tables). A possible explanation is
that probability-based pruning does not take the fre-
quency of the source phrase into account. This
difference might explain the poor performance of
probability-based pruning.
To analyze how many phrases are compositional,
we collect statistics during the computation of the
entropy criterion. These are shown in Table 4, ac-
cumulated across all language pairs and all phrases,
i. e., including singleton phrases. We see that 96%
of all phrases are compositional (3 970 million out
of 4 137 million phrases). Furthermore, out of
the 167 million non-compositional phrases, more
than half (85 million phrases), are trivially non-
compositional: they consist only of a single source
or target language word. The number of non-trivial
non-compositional phrases is, with 82 million or 2%
of the total number of phrases, very small.
In Figure 8, we show the effect of the constant
978
 18
 20
 22
 24
 26
 28
 30
 32
 34
 36
 38
 0.01  0.1  1  10  100
BLE
U[%
]
Number of Phrases [M]
ProbCountFisherEntropy
 10
 15
 20
 25
 30
 35
 40
 0.01  0.1  1  10  100
BLE
U[%
]
Number of Phrases [M]
ProbCountFisherEntropy
Figure 2: Translation quality as a function of the phrase table size for Spanish-English (left) and English-Spanish
(right).
 10
 15
 20
 25
 30
 35
 40
 0.1  1  10  100  1000
BLE
U[%
]
Number of Phrases [M]
ProbCountFisherEntropy
 5
 10
 15
 20
 25
 30
 35
 40
 0.1  1  10  100  1000
BLE
U[%
]
Number of Phrases [M]
ProbCountFisherEntropy
Figure 3: Translation quality as a function of the phrase table size for French-English (left) and English-French (right).
 6
 8
 10
 12
 14
 16
 18
 20
 22
 24
 26
 0.001  0.01  0.1  1  10  100
BLE
U[%
]
Number of Phrases [M]
ProbCountFisherEntropy
 4
 6
 8
 10
 12
 14
 16
 0.001  0.01  0.1  1  10  100
BLE
U[%
]
Number of Phrases [M]
ProbCountFisherEntropy
Figure 4: Translation quality as a function of the phrase table size for Czech-English (left) and English-Czech (right).
 8
 10
 12
 14
 16
 18
 20
 22
 24
 26
 28
 0.001  0.01  0.1  1  10
BLE
U[%
]
Number of Phrases [M]
ProbCountFisherEntropy
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 0.001  0.01  0.1  1  10
BLE
U[%
]
Number of Phrases [M]
ProbCountFisherEntropy
Figure 5: Translation quality as a function of the phrase table size for German-English (left) and English-German
(right).
979
 0
 20
 40
 60
 80
 100
 10  100
Per
cen
tag
e o
f Ph
ras
es 
[%]
Number of Phrases [millions]
Prob
6-word5-word
4-word3-word
2-word1-word
 0
 20
 40
 60
 80
 100
 0.01  0.1  1  10  100
Per
cen
tag
e o
f Ph
ras
es 
[%]
Number of Phrases [millions]
Count
6-word5-word
4-word3-word
2-word1-word
Figure 6: Phrase length statistics for Spanish-English for probability-based (left) and count-based pruning (right).
 0
 20
 40
 60
 80
 100
 0.01  0.1  1  10  100
Per
cen
tag
e o
f Ph
ras
es 
[%]
Number of Phrases [millions]
Fisher
6-word5-word
4-word3-word
2-word1-word
 0
 20
 40
 60
 80
 100
 0.01  0.1  1  10  100
Per
cen
tag
e o
f Ph
ras
es 
[%]
Number of Phrases [millions]
Entropy
6-word5-word
4-word3-word
2-word1-word
Figure 7: Phrase length statistics for Spanish-English for significance-based (left) and entropy-based pruning (right).
pc for non-compositional phrases.6 The results
shown are for Spanish-English; additional experi-
ments for the other languages and translation direc-
tions showed very similar results. Overall, there is
no big difference between the values. Hence, we
chose a value of 10 for all experiments.
The results in Figure 2 to Figure 5 show that
entropy-based pruning clearly outperforms the al-
ternative pruning methods. However, it is a bit
hard to see from the graphs exactly how much ad-
ditional savings it offers over other methods. In Ta-
ble 5, we show how much of the phrase table we
have to retain under various pruning criteria with-
out losing more than one Bleu point in translation
quality. We see that probability-based pruning al-
lows only for marginal savings. Count-based and
significance-based pruning results in larger savings
between 70% and 90%, albeit with fairly high vari-
6The values are in neg-log-space, i. e., a value of 10 corre-
sponds to pc = e?10.
ability. Entropy-based pruning achieves consistently
high savings between 85% and 95% of the phrase ta-
ble. It always outperforms the other pruning meth-
ods and yields significant savings on top of count-
based or significance-based pruning methods. Of-
ten, we can cut the required phrase table size in half
compared to count or significance based pruning.
As a last experiment, we want to confirm that
phrase-table pruning methods are actually better
than simply reducing the maximum phrase length.
In Figure 9, we show a comparison of different
pruning methods and a length-based approach for
Spanish-English. For the ?Length? curve, we first
drop all 6-word phrases, then all 5-word phrases, etc.
until we are left with only single-word phrases; the
phrase length is measured as the number of source
language words. We observe that entropy-based,
count-based and significance-based pruning indeed
outperform the length-based approach. We obtained
similar results for the other languages.
980
Method ES-EN EN-ES DE-EN EN-DE FR-EN EN-FR CS-EN EN-CS
Prob 77.3 % 82.7 % 61.2 % 67.3 % 84.8 % 94.1 % 85.6 % 86.3 %
Count 24.9 % 11.9 % 19.9 % 14.3 % 11.4 % 9.0 % 20.2 % 10.4 %
Fisher 23.5 % 12.6 % 21.7 % 14.0 % 14.5 % 13.6 % 31.9 % 9.9 %
Entropy 7.2 % 6.0 % 10.2 % 11.1 % 7.1 % 8.1 % 14.8 % 6.4 %
Table 5: To what degree can we prune the phrase table without losing more than 1 Bleu point? The table shows
percentage of phrases that we have to retain. ES=Spanish, EN=English, FR=French, CS=Czech, DE=German.
 24
 26
 28
 30
 32
 34
 36
 38
 0.01  0.1  1  10  100
BL
EU
[%]
Number of Phrases [M]
5101520253050
Figure 8: Translation quality (Bleu) as a function of the
phrase table size for Spanish-English for entropy pruning
with different constants pc.
7 Conclusions
Phrase table pruning is often addressed in an ad-hoc
way using the heuristics described in Section 3. We
have shown that some of those do not work well.
Choosing the wrong technique can result in sig-
nificant drops in translation quality without saving
much in terms of phrase table size. We introduced
a novel entropy-based criterion and put phrase ta-
ble pruning on a sound theoretical foundation. Fur-
thermore, we performed a systematic experimental
comparison of existing methods and the new entropy
criterion. The experiments were carried out for four
language pairs under small, medium and large data
conditions. We can summarize our conclusions as
follows:
? Probability-based pruning performs poorly
when pruning large parts of the phrase table.
This might be because it does not take the fre-
quency of the source phrase into account.
? Count-based pruning performs as well as
 18
 20
 22
 24
 26
 28
 30
 32
 34
 36
 38
 0.01  0.1  1  10  100
BL
EU
[%]
Number of Phrases [M]
LengthProbCountFisherEntropy
Figure 9: Translation quality (Bleu) as a function of the
phrase table size for Spanish-English.
significance-based pruning.
? Entropy-based pruning gives significantly
larger savings in phrase table size than any
other pruning method.
? Compared to previous work, the novel entropy-
based pruning often achieves the same Bleu
score with only half the number of phrases.
8 Future Work
Currently, we take only the model p(e?|f?) into ac-
count when looking for the best segmentation. We
might obtain a better estimate by also consider-
ing the distortion costs, which penalize reordering.
We could also include other phrase models such as
p(f? |e?) and the language model.
The entropy pruning criterion could be applied
to hierarchical machine translation systems (Chiang,
2007). Here, we might observe even larger reduc-
tions in phrase table size as there are many more en-
tries.
981
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 858?
867, Prague, Czech Republic, June. Association for
Computational Linguistics.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
Yu Chen, Andreas Eisele, and Martin Kay. 2008.
Improving statistical machine translation efficiency
by triangulation. In Proceedings of the Sixth In-
ternational Conference on Language Resources and
Evaluation (LREC?08), Marrakech, Morocco, May.
European Language Resources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.
Yu Chen, Martin Kay, and Andreas Eisele. 2009. In-
tersecting multilingual data for faster and better sta-
tistical translations. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 128?136, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228,
June.
Thomas M. Cover and Joy A. Thomas. 2006. Elements
of information theory. Wiley-Interscience, New York,
NY, USA.
Nan Duan, Mu Li, and Ming Zhou. 2011. Improving
phrase extraction via MBR phrase scoring and prun-
ing. In Proceedings of MT Summit XIII, pages 189?
197, Xiamen, China, September.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2007a.
Estimating phrase pair relevance for machine transla-
tion pruning. In Proceedings of MT Summit XI, pages
159?165, Copenhagen, Denmark, September.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2007b.
Translation model pruning via usage statistics for sta-
tistical machine translation. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Companion Volume, Short Papers,
pages 21?24, Rochester, New York, April. Association
for Computational Linguistics.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975, Prague, Czech Republic, June. Association for
Computational Linguistics.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics An-
nual Meeting (HLT-NAACL), pages 127?133, Edmon-
ton, Canada, May/June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Constan-
tine, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In 45th An-
nual Meeting of the Assoc. for Computational Linguis-
tics (ACL): Poster Session, pages 177?180, Prague,
Czech Republic, June.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In 6th Conf. of the Assoc. for Machine Translation
in the Americas (AMTA), pages 115?124, Washington
DC, September/October.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum error
rate training for statistical machine translation. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 725?734,
Honolulu, HI, October. Association for Computational
Linguistics.
Robert C. Moore. 2004. On log-likelihood-ratios and
the significance of rare events. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing, pages 333?340.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449, De-
cember.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In 41st Annual Meet-
ing of the Assoc. for Computational Linguistics (ACL),
pages 160?167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In 40th Annual Meeting of
982
the Assoc. for Computational Linguistics (ACL), pages
311?318, Philadelphia, PA, July.
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
258?267, Portland, Oregon, USA, June. Association
for Computational Linguistics.
German Sanchis-Trilles, Daniel Ortiz-Martinez, Je-
sus Gonzalez-Rubio, Jorge Gonzalez, and Francisco
Casacuberta. 2011. Bilingual segmentation for
phrasetable pruning in statistical machine translation.
In Proceedings of the 15th Conference of the European
Association for Machine Translation, pages 257?264,
Leuven, Belgium, May.
Andreas Stolcke. 1998. Entropy-based pruning of back-
off language models. In Proc. DARPA Broadcast News
Transcription and Understanding Workshop, pages
270?274.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom filter language models: Tera-scale LMs on the
cheap. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 468?476, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
Nadi Tomeh, Nicola Cancedda, and Marc Dymetman.
2009. Complexity-based phrase-table filtering for sta-
tistical machine translation. In Proceedings of MT
Summit XII, Ottawa, Ontario, Canada, August.
Nadi Tomeh, Marco Turchi, Guillaume Wisniewski,
Alexandre Allauzen, and Franc?ois Yvon. 2011. How
good are your phrases? Assessing phrase quality with
single class classification. In Proceedings of the Inter-
national Workshop on Spoken Language Translation,
pages 261?268, San Francisco, California, December.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In 16th Int. Conf. on Computational Linguistics
(COLING), pages 836?841, Copenhagen, Denmark,
August.
Mei Yang and Jing Zheng. 2009. Toward smaller, faster,
and better hierarchical phrase-based SMT. In Pro-
ceedings of the ACL-IJCNLP 2009 Conference Short
Papers, pages 237?240, Suntec, Singapore, August.
Association for Computational Linguistics.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Human Language Technology Conf. / North Ameri-
can Chapter of the Assoc. for Computational Linguis-
tics Annual Meeting (HLT-NAACL): Workshop on Sta-
tistical Machine Translation, pages 55?63, New York
City, NY, June.
Richard Zens and Hermann Ney. 2008. Improvements in
dynamic programming beam search for phrase-based
statistical machine translation. In Proceedings of the
International Workshop on Spoken Language Transla-
tion, pages 195?205, Honolulu, Hawaii, October.
Richard Zens, Franz Josef Och, and Hermann Ney.
2002. Phrase-based statistical machine translation. In
M. Jarke, J. Koehler, and G. Lakemeyer, editors, 25th
German Conf. on Artificial Intelligence (KI2002), vol-
ume 2479 of Lecture Notes in Artificial Intelligence
(LNAI), pages 18?32, Aachen, Germany, September.
Springer Verlag.
Richard Zens, Evgeny Matusov, and Hermann Ney.
2004. Improved word alignment using a symmetric
lexicon model. In 20th Int. Conf. on Computational
Linguistics (COLING), pages 36?42, Geneva, Switzer-
land, August.
983
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 835?845,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Binarized Forest to String Translation
Hao Zhang
Google Research
haozhang@google.com
Licheng Fang
Computer Science Department
University of Rochester
lfang@cs.rochester.edu
Peng Xu
Google Research
xp@google.com
Xiaoyun Wu
Google Research
xiaoyunwu@google.com
Abstract
Tree-to-string translation is syntax-aware and
efficient but sensitive to parsing errors. Forest-
to-string translation approaches mitigate the
risk of propagating parser errors into transla-
tion errors by considering a forest of alterna-
tive trees, as generated by a source language
parser. We propose an alternative approach to
generating forests that is based on combining
sub-trees within the first best parse through
binarization. Provably, our binarization for-
est can cover any non-consitituent phrases in
a sentence but maintains the desirable prop-
erty that for each span there is at most one
nonterminal so that the grammar constant for
decoding is relatively small. For the purpose
of reducing search errors, we apply the syn-
chronous binarization technique to forest-to-
string decoding. Combining the two tech-
niques, we show that using a fast shift-reduce
parser we can achieve significant quality gains
in NIST 2008 English-to-Chinese track (1.3
BLEU points over a phrase-based system, 0.8
BLEU points over a hierarchical phrase-based
system). Consistent and significant gains are
also shown in WMT 2010 in the English to
German, French, Spanish and Czech tracks.
1 Introduction
In recent years, researchers have explored a wide
spectrum of approaches to incorporate syntax and
structure into machine translation models. The uni-
fying framework for these models is synchronous
grammars (Chiang, 2005) or tree transducers
(Graehl and Knight, 2004). Depending on whether
or not monolingual parsing is carried out on the
source side or the target side for inference, there are
four general categories within the framework:
? string-to-string (Chiang, 2005; Zollmann and
Venugopal, 2006)
? string-to-tree (Galley et al, 2006; Shen et al,
2008)
? tree-to-string (Lin, 2004; Quirk et al, 2005;
Liu et al, 2006; Huang et al, 2006; Mi et al,
2008)
? tree-to-tree (Eisner, 2003; Zhang et al, 2008)
In terms of search, the string-to-x models explore all
possible source parses and map them to the target
side, while the tree-to-x models search over the sub-
space of structures of the source side constrained
by an input tree or trees. Hence, tree-to-x mod-
els are more constrained but more efficient. Mod-
els such as Huang et al (2006) can match multi-
level tree fragments on the source side which means
larger contexts are taken into account for transla-
tion (Poutsma, 2000), which is a modeling advan-
tage. To balance efficiency and accuracy, forest-to-
string models (Mi et al, 2008; Mi and Huang, 2008)
use a compact representation of exponentially many
trees to improve tree-to-string models. Tradition-
ally, such forests are obtained through hyper-edge
pruning in the k-best search space of a monolin-
gual parser (Huang, 2008). The pruning parameters
that control the size of forests are normally hand-
tuned. Such forests encode both syntactic variants
and structural variants. By syntactic variants, we re-
fer to the fact that a parser can parse a substring into
either a noun phrase or verb phrase in certain cases.
835
We believe that structural variants which allow more
source spans to be explored during translation are
more important (DeNeefe et al, 2007), while syn-
tactic variants might improve word sense disam-
biguation but also introduce more spurious ambi-
guities (Chiang, 2005) during decoding. To focus
on structural variants, we propose a family of bina-
rization algorithms to expand one single constituent
tree into a packed forest of binary trees containing
combinations of adjacent tree nodes. We control the
freedom of tree node binary combination by restrict-
ing the distance to the lowest common ancestor of
two tree nodes. We show that the best results are
achieved when the distance is two, i.e., when com-
bining tree nodes sharing a common grand-parent.
In contrast to conventional parser-produced-forest-
to-string models, in our model:
? Forests are not generated by a parser but by
combining sub-structures using a tree binarizer.
? Instead of using arbitary pruning parameters,
we control forest size by an integer number that
defines the degree of tree structure violation.
? There is at most one nonterminal per span so
that the grammar constant is small.
Since GHKM rules (Galley et al, 2004) can cover
multi-level tree fragments, a synchronous grammar
extracted using the GHKM algorithm can have syn-
chronous translation rules with more than two non-
terminals regardless of the branching factor of the
source trees. For the first time, we show that simi-
lar to string-to-tree decoding, synchronous binariza-
tion significantly reduces search errors and improves
translation quality for forest-to-string decoding.
To summarize, the whole pipeline is as follows.
First, a parser produces the highest-scored tree for
an input sentence. Second, the parse tree is re-
structured using our binarization algorithm, result-
ing in a binary packed forest. Third, we apply the
forest-based variant of the GHKM algorithm (Mi
and Huang, 2008) on the new forest for rule extrac-
tion. Fourth, on the translation forest generated by
all applicable translation rules, which is not neces-
sarily binary, we apply the synchronous binarization
algorithm (Zhang et al, 2006) to generate a binary
translation forest. Finally, we use a bottom-up de-
coding algorithm with intergrated LM intersection
using the cube pruning technique (Chiang, 2005).
The rest of the paper is organized as follows. In
Section 2, we give an overview of the forest-to-
string models. In Section 2.1, we introduce a more
efficient and flexible algorithm for extracting com-
posed GHKM rules based on the same principle as
cube pruning (Chiang, 2007). In Section 3, we in-
troduce our source tree binarization algorithm for
producing binarized forests. In Section 4, we ex-
plain how to do synchronous rule factorization in a
forest-to-string decoder. Experimental results are in
Section 5.
2 Forest-to-string Translation
Forest-to-string models can be described as
e = Y( arg max
d?D(T ), T?F (f)
P (d|T ) ) (1)
where f stands for a source string, e stands for a tar-
get string, F stands for a forest, D stands for a set
of synchronous derivations on a given tree T , and
Y stands for the target side yield of a derivation.
The search problem is finding the derivation with
the highest probability in the space of all deriva-
tions for all parse trees for an input sentence. The
log probability of a derivation is normally a lin-
ear combination of local features which enables dy-
namic programming to find the optimal combination
efficiently. In this paper, we focus on the models
based on the Synchronous Tree Substitution Gram-
mars (STSG) defined by Galley et al (2004). In con-
trast to a tree-to-string model, the introduction of F
augments the search space systematically. When the
first-best parse is wrong or no good translation rules
are applicable to the first-best parse, the model can
recover good translations from alternative parses.
In STSG, local features are defined on tree-to-
string rules, which are synchronous grammar rules
defining how a sequence of terminals and nontermi-
nals on the source side translates to a sequence of
target terminals and nonterminals. One-to-one map-
ping of nonterminals is assumed. But terminals do
not necessarily need to be aligned. Figure 1 shows a
typical English-Chinese tree-to-string rule with a re-
ordering pattern consisting of two nonterminals and
different numbers of terminals on the two sides.
836
VP
VBD
was
VP-C
.x1:VBN PP
P
by
.x2:NP-C
? bei? x2 x1
Figure 1: An example tree-to-string rule.
Forest-to-string translation has two stages. The
first stage is rule extraction on word-aligned parallel
texts with source forests. The second stage is rule
enumeration and DP decoding on forests of input
strings. In both stages, at each tree node, the task on
the source side is to generate a list of tree fragments
by composing the tree fragments of its children. We
propose a cube-pruning style algorithm that is suit-
able for both rule extraction during training and rule
enumeration during decoding.
At the highest level, our algorithm involves three
steps. In the first step, we label each node in the in-
put forest by a boolean variable indicating whether it
is a site of interest for tree fragment generation. If it
is marked true, it is an admissible node. In the case
of rule extraction, a node is admissible if and only if
it corresponds to a phrase pair according to the un-
derlying word alignment. In the case of decoding,
every node is admissible for the sake of complete-
ness of search. An initial one-node tree fragment is
placed at each admissible node for seeding the tree
fragment generation process. In the second step,
we do cube-pruning style bottom-up combinations
to enumerate a pruned list of tree fragments at each
tree node. In the third step, we extract or enumerate-
and-match tree-to-string rules for the tree fragments
at the admissible nodes.
2.1 A Cube-pruning-inspired Algorithm for
Tree Fragment Composition
Galley et al (2004) defined minimal tree-to-string
rules. Galley et al (2006) showed that tree-to-string
rules made by composing smaller ones are impor-
tant to translation. It can be understood by the anal-
ogy of going from word-based models to phrase-
based models. We relate composed rule extraction
to cube-pruning (Chiang, 2007). In cube-pruning,
the process is to keep track of the k-best sorted lan-
guage model states at each node and combine them
bottom-up with the help of a priority queue. We
can imagine substituting k-best LM states with k
composed rules at each node and composing them
bottom-up. We can also borrow the cube pruning
trick to compose multiple lists of rules using a pri-
ority queue to lazily explore the space of combina-
tions starting from the top-most element in the cube
formed by the lists.
We need to define a ranking function for com-
posed rules. To simulate the breadth-first expansion
heuristics of Galley et al (2006), we define the fig-
ure of merit of a tree-to-string rule as a tuple m =
(h, s, t), where h is the height of a tree fragment,
s is the number of frontier nodes, i.e., bottom-level
nodes including both terminals and non-terminals,
and t is the number of terminals in the set of frontier
nodes. We define an additive operator +:
m1 + m2
= ( max{h1, h2} + 1, s1 + s2, t1 + t2 )
and a min operator based on the order <:
m1 < m2 ??
?
?
?
h1 < h2 ?
h1 = h2 ? s1 < s2 ?
h1 = h2 ? s1 = s2 ? t1 < t2
The + operator corresponds to rule compositions.
The < operator corresponds to ranking rules by their
sizes. A concrete example is shown in Figure 2,
in which case the monotonicity property of (+, <)
holds: if ma < mb, ma +mc < mb +mc. However,
this is not true in general for the operators in our def-
inition, which implies that our algorithm is indeed
like cube-pruning: an approximate k-shortest-path
algorithm.
3 Source Tree Binarization
The motivation of tree binarization is to factorize
large and rare structures into smaller but frequent
ones to improve generalization. For example, Penn
Treebank annotations are often flat at the phrase
level. Translation rules involving flat phrases are un-
likely to generalize. If long sequences are binarized,
837
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
VBD (1, 1, 0)
VBD
was
(2, 1, 1)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
VP-C (1, 1, 0)
VP-C
VPB PP
(2, 2, 0)
VP-C
VPB PP
P NP-C
(3, 3, 1)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
=
(1, 1, 0) (2, 2, 0) (3, 3, 1)
(1, 1, 0) VP
VBD VP-C
(2, 2, 0) VP
VBD VP-C
VPB PP
(3, 3, 0) VP
VBD VP-C
VPB PP
P NP-C
(4, 4, 1)
(2, 1, 1) VP
VBD
was
VP-C
(3, 2, 1) VP
VBD
was
VP-C
VPB PP
(3, 3, 1) VP
VBD
was
VP-C
VPB PP
P NP-C
(4, 4, 2)
Figure 2: Tree-to-string rule composition as cube-pruning. The left shows two lists of composed rules sorted by their
geometric measures (height, # frontiers,# frontier terminals), under the gluing rule of VP ? VBD VP?C.
The right part shows a cube view of the combination space. We explore the space from the top-left corner to the
neighbors.
the commonality of subsequences can be discov-
ered. For example, the simplest binarization meth-
ods left-to-right, right-to-left, and head-out explore
sharing of prefixes or suffixes. Among exponentially
many binarization choices, these algorithms pick a
single bracketing structure for a sequence of sibling
nodes. To explore all possible binarizations, we use
a CYK algorithm to produce a packed forest of bi-
nary trees for a given sibling sequence.
With CYK binarization, we can explore any span
that is nested within the original tree structure, but
still miss all cross-bracket spans. For example,
translating from English to Chinese, The phrase
?There is? should often be translated into one verb
in Chinese. In a correct English parse tree, however,
the subject-verb boundary is between ?There? and
?is?. As a result, tree-to-string translation based on
constituent phrases misses the good translation rule.
The CYK-n binarization algorithm shown in Al-
gorithm 1 is a parameterization of the basic CYK
binarization algorithm we just outlined. The idea is
that binarization can go beyond the scope of parent
nodes to more distant ancestors. The CYK-n algo-
rithm first annotates each node with its n nearest
ancestors in the source tree, then generates a bina-
rization forest that allows combining any two nodes
with common ancestors. The ancestor chain labeled
at each node licenses the node to only combine with
nodes having common ancestors in the past n gener-
ations.
The algorithm creates new tree nodes on the fly.
New tree nodes need to have their own states in-
dicated by a node label representing what is cov-
ered internally by the node and an ancestor chain
representing which nodes the node attaches to ex-
ternally. Line 22 and Line 23 of Algorithm 1 up-
date the label and ancestor annotations of new tree
nodes. Using the parsing semiring notations (Good-
man, 1999), the ancestor computation can be sum-
marized by the (?,?) pair. ? produces the ances-
tor chain of a hyper-edge. ? produces the ancestor
chain of a hyper-node. The node label computation
can be summarized by the (concatenate, min) pair.
concatenate produces a concatenation of node la-
bels. min yields the label with the shortest length.
A tree-sequence (Liu et al, 2007) is a sequence of
sub-trees covering adjacent spans. It can be proved
that the final label of each new node in the forest
corresponds to the tree sequence which has the min-
imum length among all sequences covered by the
node span. The ancestor chain of a new node is the
common ancestors of the nodes in its minimum tree
sequence.
For clarity, we do full CYK loops over all O(|w|2)
spans and O(|w|3) potential hyper-edges, where |w|
is the length of a source string. In reality, only de-
scendants under a shared ancestor can combine. If
we assume trees have a bounded branching factor
b, the number of descendants after n generations is
still bounded by a constant c = bn. The algorithm is
O(c3 ? |w|), which is still linear to the size of input
sentence when the parameter n is a constant.
838
VP
VBD+VBN
VBD
was
VBN
PP
P
by
NP-C
VP
VBD
was
VP-C
VBN+P
VBN P
by
NP-C
(a) (b)
VP
VBD+VBN+P
VBD+VBN
VBD
was
VBN
P
by
NP-C
VP
VBD+VBN+P
VBD
was
VBN+P
VBN P
by
NP-C
(c) (d)
1 2 3 4
0 VBD VBD+VBN VBD+VBN+P VP
1 VBN VBN+P VP-C
2 P PP
3 NP-C
Figure 3: Alternative binary parses created for the origi-
nal tree fragment in Figure 1 through CYK-2 binarization
(a and b) and CYK-3 binarization (c and d). In the chart
representation at the bottom, cells with labels containing
the concatenation symbol + hold nodes created through
binarization.
Figure 3 shows some examples of alternative trees
generated by the CYK-n algorithm. In this example,
standard CYK binarization will not create any new
trees since the input is already binary. The CYK-2
and CYK-3 algorithms discover new trees with an
increasing degree of freedom.
4 Synchronous Binarization for
Forest-to-string Decoding
In this section, we deal with binarization of transla-
tion forests, also known as translation hypergraphs
(Mi et al, 2008). A translation forest is a packed
forest representation of all synchronous derivations
composed of tree-to-string rules that match the
source forest. Tree-to-string decoding algorithms
work on a translation forest, rather than a source for-
est. A binary source forest does not necessarily al-
ways result in a binary translation forest. In the tree-
to-string rule in Figure 4, the source tree is already
ADJP
RB+JJ
x0:RB JJ
responsible
PP
IN
for
NP-C
NPB
DT
the
x1:NN
x2:PP
? x0
fuze
?? x2
de
? x1
ADJP
RB+JJ
x0:RB JJ
responsible
x1:PP
? x0
fuze
?? x1
PP
IN
for
NP-C
NPB
DT
the
x0:NN
x1:PP
? x1
de
? x0
Figure 4: Synchronous binarization for a tree-to-string
rule. The top rule can be binarized into two smaller rules.
binary with the help of source tree binarization, but
the translation rule involves three variables in the set
of frontier nodes. If we apply synchronous binariza-
tion (Zhang et al, 2006), we can factorize it into
two smaller translation rules each having two vari-
ables. Obviously, the second rule, which is a com-
mon pattern, is likely to be shared by many transla-
tion rules in the derivation forest. When beams are
fixed, search goes deeper in a factorized translation
forest.
The challenge of synchronous binarization for a
forest-to-string system is that we need to first match
large tree fragments in the input forest as the first
step of decoding. Our solution is to do the matching
using the original rules and then run synchronous
binarization to break matching rules down to factor
rules which can be shared in the derivation forest.
This is different from the offline binarization scheme
described in (Zhang et al, 2006), although the core
algorithm stays the same.
5 Experiments
We ran experiments on public data sets for English
to Chinese, Czech, French, German, and Spanish
839
Algorithm 1 The CYK-n Binarization Algorithm
1: function CYKBINARIZER(T,n)
2: for each tree node ? T in bottom-up topological order do
3: Make a copy of node in the forest output F
4: Ancestors[node] = the nearest n ancestors of node
5: Label [node] = the label of node in T
6: L? the length of the yield of T
7: for k = 2...L do
8: for i = 0, ..., L? k do
9: for j = i + 1, ..., i + k ? 1 do
10: lnode ? Node[i, j]; rnode ? Node[j, i + k]
11: if Ancestors[lnode] ? Ancestors[rnode] 6= ? then
12: pnode ? GETNODE(i, i + k)
13: ADDEDGE(pnode, lnode, rnode)
return F
14: function GETNODE(begin, end)
15: if Node[begin, end] /? F then
16: Create a new node for the span (begin, end)
17: Ancestors[node] = ?
18: Label [node] = the sequence of terminals in the span (begin, end) in T
19:
return Node[begin, end]
20: function ADDEDGE(pnode, lnode, rnode)
21: Add a hyper-edge from lnode and rnode to pnode
22: Ancestors[pnode] = Ancestors[pnode] ? (Ancestors[lnode] ?Ancestors[rnode])
23: Label [pnode] = min{Label[pnode], CONCATENATE(Label[lnode], Label[rnode])}
translation to evaluate our methods.
5.1 Setup
For English-to-Chinese translation, we used all the
allowed training sets in the NIST 2008 constrained
track. For English to the European languages, we
used the training data sets for WMT 2010 (Callison-
Burch et al, 2010). For NIST, we filtered out sen-
tences exceeding 80 words in the parallel texts. For
WMT, the filtering limit is 60. There is no filtering
on the test data set. Table 1 shows the corpus statis-
tics of our bilingual training data sets.
Source Words Target Words
English-Chinese 287M 254M
English-Czech 66M 57M
English-French 857M 996M
English-German 45M 43M
English-Spanish 216M 238M
Table 1: The Sizes of Parallel Texts.
At the word alignment step, we did 6 iterations
of IBM Model-1 and 6 iterations of HMM. For
English-Chinese, we ran 2 iterations of IBM Model-
4 in addition to Model-1 and HMM. The word align-
ments are symmetrized using the ?union? heuris-
tics. Then, the standard phrase extraction heuristics
(Koehn et al, 2003) were applied to extract phrase
pairs with a length limit of 6. We ran the hierar-
chical phrase extraction algorithm with the standard
heuristics of Chiang (2005). The phrase-length limit
is interpreted as the maximum number of symbols
on either the source side or the target side of a given
rule. On the same aligned data sets, we also ran the
tree-to-string rule extraction algorithm described in
Section 2.1 with a limit of 16 rules per tree node.
The default parser in the experiments is a shift-
reduce dependency parser (Nivre and Scholz, 2004).
It achieves 87.8% labelled attachment score and
88.8% unlabeled attachment score on the standard
Penn Treebank test set. We convert dependency
parses to constituent trees by propagating the part-
of-speech tags of the head words to the correspond-
ing phrase structures.
We compare three systems: a phrase-based sys-
tem (Och and Ney, 2004), a hierarchical phrase-
based system (Chiang, 2005), and our forest-to-
string systemwith different binarization schemes. In
the phrase-based decoder, jump width is set to 8. In
the hierarchical decoder, only the glue rule is applied
840
to spans longer than 10. For the forest-to-string sys-
tem, we do not have such length-based reordering
constraints.
We trained two 5-gram language models with
Kneser-Ney smoothing for each of the target lan-
guages. One is trained on the target side of the par-
allel text, the other is on a corpus provided by the
evaluation: the Gigaword corpus for Chinese and
news corpora for the others. Besides standard fea-
tures (Och and Ney, 2004), the phrase-based decoder
also uses a Maximum Entropy phrasal reordering
model (Zens and Ney, 2006). Both the hierarchi-
cal decoder and the forest-to-string decoder only use
the standard features. For feature weight tuning, we
do Minimum Error Rate Training (Och, 2003). To
explore a larger n-best list more efficiently in train-
ing, we adopt the hypergraph-based MERT (Kumar
et al, 2009).
To evaluate the translation results, we use BLEU
(Papineni et al, 2002).
5.2 Translation Results
Table 2 shows the scores of our system with the
best binarization scheme compared to the phrase-
based system and the hierarchical phrase-based sys-
tem. Our system is consistently better than the other
two systems in all data sets. On the English-Chinese
data set, the improvement over the phrase-based sys-
tem is 1.3 BLEU points, and 0.8 over the hierarchi-
cal phrase-based system. In the tasks of translat-
ing to European languages, the improvements over
the phrase-based baseline are in the range of 0.5 to
1.0 BLEU points, and 0.3 to 0.5 over the hierar-
chical phrase-based system. All improvements ex-
cept the bf2s and hier difference in English-Czech
are significant with confidence level above 99% us-
ing the bootstrap method (Koehn, 2004). To demon-
strate the strength of our systems including the two
baseline systems, we also show the reported best re-
sults on these data sets from the 2010 WMT work-
shop. Our forest-to-string system (bf2s) outperforms
or ties with the best ones in three out of four lan-
guage pairs.
5.3 Different Binarization Methods
The translation results for the bf2s system in Ta-
ble 2 are based on the cyk binarization algorithm
with bracket violation degree 2. In this section, we
BLEU
dev test
English-Chinese pb 29.7 39.4
hier 31.7 38.9
bf2s 31.9 40.7??
English-Czech wmt best - 15.4
pb 14.3 15.5
hier 14.7 16.0
bf2s 14.8 16.3?
English-French wmt best - 27.6
pb 24.1 26.1
hier 23.9 26.1
bf2s 24.5 26.6??
English-German wmt best - 16.3
pb 14.5 15.5
hier 14.9 15.9
bf2s 15.2 16.3??
English-Spanish wmt best - 28.4
pb 24.1 27.9
hier 24.2 28.4
bf2s 24.9 28.9??
Table 2: Translation results comparing bf2s, the
binarized-forest-to-string system, pb, the phrase-based
system, and hier, the hierarchical phrase-based system.
For comparison, the best scores from WMT 2010 are also
shown. ?? indicates the result is significantly better than
both pb and hier. ? indicates the result is significantly
better than pb only.
vary the degree to generate forests that are incremen-
tally augmented from a single tree. Table 3 shows
the scores of different tree binarization methods for
the English-Chinese task.
It is clear from reading the table that cyk-2 is the
optimal binarization parameter. We have verified
this is true for other language pairs on non-standard
data sets. We can explain it from two angles. At
degree 2, we allow phrases crossing at most one
bracket in the original tree. If the parser is reason-
ably good, crossing just one bracket is likely to cover
most interesting phrases that can be translation units.
From another point of view, enlarging the forests
entails more parameters in the resulting translation
model, making over-fitting likely to happen.
5.4 Binarizer or Parser?
A natural question is how the binarizer-generated
forests compare with parser-generated forests in
translation. To answer this question, we need a
841
BLEU
rules dev test
no binarization 378M 28.0 36.3
head-out 408M 30.0 38.2
cyk-1 527M 31.6 40.5
cyk-2 803M 31.9 40.7
cyk-3 1053M 32.0 40.6
cyk-? 1441M 32.0 40.3
Table 3: Comparing different source tree binarization
schemes for English-Chinese translation, showing both
BLEU scores and model sizes. The rule counts include
normal phrases which are used at the leaf level during
decoding.
parser that can generate a packed forest. Our fast
deterministic dependency parser does not generate
a packed forest. Instead, we use a CRF constituent
parser (Finkel et al, 2008) with state-of-the-art ac-
curacy. On the standard Penn Treebank test set, it
achieves an F-score of 89.5%. It uses a CYK algo-
rithm to do full dynamic programming inference, so
is much slower. We modified the parser to do hyper-
edge pruning based on posterior probabilities. The
parser preprocesses the Penn Treebank training data
through binarization. So the packed forest it pro-
duces is also a binarized forest. We compare two
systems: one is using the cyk-2 binarizer to generate
forests; the other is using the CRF parser with prun-
ing threshold e?p, where p = 2 to generate forests.1
Although the parser outputs binary trees, we found
cross-bracket cyk-2 binarization is still helpful.
BLEU
dev test
cyk-2 14.9 16.0
parser 14.7 15.7
Table 4: Binarized forests versus parser-generated forests
for forest-to-string English-German translation.
Table 4 shows the comparison of binarization for-
est and parser forest on English-German translation.
The results show that cyk-2 forest performs slightly
1All hyper-edges with negative log posterior probability
larger than p are pruned. In Mi and Huang (2008), the thresh-
old is p = 10. The difference is that they do the forest pruning
on a forest generated by a k-best algorithm, while we do the
forest-pruning on the full CYK chart. As a result, we need more
aggressive pruning to control forest size.
better than the parser forest. We have not done full
exploration of forest pruning parameters to fine-tune
the parser-forest. The speed of the constituent parser
is the efficiency bottleneck. This actually demon-
strates the advantage of the binarizer plus forest-to-
string scheme. It is flexible, and works with any
parser that generates projective parses. It does not
require hand-tuning of forest pruning parameters for
training.
5.5 Synchronous Binarization
In this section, we demonstrate the effect of syn-
chronous binarization for both tree-to-string and
forest-to-string translation. The experiments are on
the English-Chinese data set. The baseline systems
use k-way cube pruning, where k is the branching
factor, i.e., the maximum number of nonterminals on
the right-hand side of any synchronous translation
rule in an input grammar. The competing system
does online synchronous binarization as described in
Section 4 to transform the grammar intersected with
the input sentence to the minimum branching factor
k? (k? < k), and then applies k?-way cube pruning.
Typically, k? is 2.
BLEU
dev test
head-out cube pruning 29.2 37.0
+ synch. binarization 30.0 38.2
cyk-2 cube pruning 31.7 40.5
+ synch. binarization 31.9 40.7
Table 5: The effect of synchronous binarization for tree-
to-string and forest-to-string systems, on the English-
Chinese task.
Table 5 shows that synchronous binarization does
help reduce search errors and find better translations
consistently in all settings.
6 Related Work
The idea of concatenating adjacent syntactic cate-
gories has been explored in various syntax-based
models. Zollmann and Venugopal (2006) aug-
mented hierarchial phrase based systems with joint
syntactic categories. Liu et al (2007) proposed tree-
sequence-to-string translation rules but did not pro-
vide a good solution to place joint subtrees into con-
nection with the rest of the tree structure. Zhang et
842
al. (2009) is the closest to our work. But their goal
was to augment a k-best forest. They did not bina-
rize the tree sequences. They also did not put con-
straint on the tree-sequence nodes according to how
many brackets are crossed.
Wang et al (2007) used target tree binarization to
improve rule extraction for their string-to-tree sys-
tem. Their binarization forest is equivalent to our
cyk-1 forest. In contrast to theirs, our binarization
scheme affects decoding directly because we match
tree-to-string rules on a binarized forest.
Different methods of translation rule binarization
have been discussed in Huang (2007). Their argu-
ment is that for tree-to-string decoding target side
binarization is simpler than synchronous binariza-
tion and works well because creating discontinous
source spans does not explode the state space. The
forest-to-string senario is more similar to string-to-
tree decoding in which state-sharing is important.
Our experiments show that synchronous binariza-
tion helps significantly in the forest-to-string case.
7 Conclusion
We have presented a new approach to tree-to-string
translation. It involves a source tree binarization
step and a standard forest-to-string translation step.
The method renders it unnecessary to have a k-best
parser to generate a packed forest. We have demon-
strated state-of-the-art results using a fast parser and
a simple tree binarizer that allows crossing at most
one bracket in each binarized node. We have also
shown that reducing search errors is important for
forest-to-string translation. We adapted the syn-
chronous binarization technqiue to improve search
and have shown significant gains. In addition, we
also presented a new cube-pruning-style algorithm
for rule extraction. In the new algorithm, it is easy to
adjust the figure-of-merit of rules for extraction. In
the future, we plan to improve the learning of trans-
lation rules with binarized forests.
Acknowledgments
We would like to thank the members of the MT team
at Google, especially Ashish Venugopal, Zhifei Li,
John DeNero, and Franz Och, for their help and dis-
cussions. We would also like to thank Daniel Gildea
for his suggestions on improving the paper.
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on statisti-
cal machine translation and metrics for machine trans-
lation. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and Metrics(MATR),
pages 17?53, Uppsala, Sweden, July. Association for
Computational Linguistics. Revised August 2010.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Conference of the Association for
Computational Linguistics (ACL-05), pages 263?270,
Ann Arbor, MI.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 755?763,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics, companion volume, pages 205?208, Sap-
poro, Japan.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL-08:
HLT, pages 959?967, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of the 2004 Meeting of the North American
chapter of the Association for Computational Linguis-
tics (NAACL-04), pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the International Conference on Computational
Linguistics/Association for Computational Linguistics
(COLING/ACL-06), pages 961?968, July.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proceedings of the 2004 Meeting of the
North American chapter of the Association for Compu-
tational Linguistics (NAACL-04).
843
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Biennial
Conference of the Association for Machine Translation
in the Americas (AMTA), Boston, MA.
Liang Huang. 2007. Binarization, synchronous bina-
rization, and target-side binarization. In Proceedings
of the NAACL/AMTA Workshop on Syntax and Struc-
ture in Statistical Translation (SSST), pages 33?40,
Rochester, NY.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of the
46th Annual Conference of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL-08:HLT), Columbus, OH. ACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Meeting of the North American chap-
ter of the Association for Computational Linguistics
(NAACL-03), Edmonton, Alberta.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In 2004 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 388?395, Barcelona, Spain, July.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 163?171, Sun-
tec, Singapore, August. Association for Computational
Linguistics.
Dekang Lin. 2004. A path-based transfer model for
machine translation. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics
(COLING-04), pages 625?630, Geneva, Switzerland.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the International Conference
on Computational Linguistics/Association for Compu-
tational Linguistics (COLING/ACL-06), Sydney, Aus-
tralia, July.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In Pro-
ceedings of the 45th Annual Conference of the Associ-
ation for Computational Linguistics (ACL-07), Prague.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 206?214, Honolulu, Hawaii, Octo-
ber. Association for Computational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of the 46th An-
nual Conference of the Association for Computational
Linguistics: Human Language Technologies (ACL-
08:HLT), pages 192?199.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings of
Coling 2004, pages 64?70, Geneva, Switzerland, Aug
23?Aug 27. COLING.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Conference of the Association for Com-
putational Linguistics (ACL-03).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Conference of the Association for Com-
putational Linguistics (ACL-02).
Arjen Poutsma. 2000. Data-oriented translation. In
Proceedings of the 18th International Conference on
Computational Linguistics (COLING-00).
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual Con-
ference of the Association for Computational Linguis-
tics (ACL-05), pages 271?279, Ann Arbor, Michigan.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Conference of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (ACL-08:HLT), Columbus, OH.
ACL.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based ma-
chine translation accuracy. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 746?
754, Prague, Czech Republic, June. Association for
Computational Linguistics.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 55?63, New York City, June.
Association for Computational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the 2006 Meeting of the
844
North American chapter of the Association for Compu-
tational Linguistics (NAACL-06), pages 256?263, New
York, NY.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree sequence
alignment-based tree-to-tree translation model. In
Proceedings of ACL-08: HLT, pages 559?567, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and
Chew Lim Tan. 2009. Forest-based tree sequence to
string translation model. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 172?180,
Suntec, Singapore, August. Association for Computa-
tional Linguistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation, pages 138?141, New York City, June. As-
sociation for Computational Linguistics.
845
