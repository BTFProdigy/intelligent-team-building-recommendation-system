Coling 2010: Poster Volume, pages 162?170,
Beijing, August 2010
Global topology of word co-occurrence networks:
Beyond the two-regime power-law
Monojit Choudhury
Microsoft Research Lab India
monojitc@microsoft.com
Diptesh Chatterjee
Indian Institute of Technology Kharagpur
diptesh.chh.1987@gmail.com
Animesh Mukherjee
Complex Systems Lagrange Lab, ISI Foundation
animesh.mukherjee@isi.it
Abstract
Word co-occurrence networks are one
of the most common linguistic networks
studied in the past and they are known
to exhibit several interesting topological
characteristics. In this article, we inves-
tigate the global topological properties of
word co-occurrence networks and, in par-
ticular, present a detailed study of their
spectrum. Our experiments reveal cer-
tain universal trends found across the net-
works for seven different languages from
three different language families, which
are neither reported nor explained by any
of the previous studies and models of
word-cooccurrence networks. We hy-
pothesize that since word co-occurrences
are governed by syntactic properties of
a language, the network has much con-
strained topology than that predicted by
the previously proposed growth model. A
deeper empirical and theoretical investiga-
tion into the evolution of these networks
further suggests that they have a core-
periphery structure, where the core hardly
evolves with time and new words are only
attached to the periphery of the network.
These properties are fundamental to the
nature of word co-occurrence across lan-
guages.
1 Introduction
In a natural language, words interact among them-
selves in different ways ? some words co-occur
with certain words at a very high probability
than other words. These co-occurrences are non-
trivial, as in their patterns cannot be inferred from
the frequency distribution of the individual words.
Understanding the structure and the emergence of
these patterns can present us with important clues
and insights about how we evolved this extremely
complex phenomenon, that is language.
In this paper, we present an in-depth study of
the word co-occurrence patterns of a language in
the framework of complex networks. The choice
of this framework is strongly motivated by its
success in explaining various properties of word
co-occurrences previously (Ferrer-i-Cancho and
Sole?, 2001; Ferrer-i-Cancho et al 2007; Kapustin
and Jamsen, 2007). Local properties, such as
the degree distribution and clustering coefficient
of the word co-occurrence networks, have been
thoroughly studied for a few languages (Ferrer-
i-Cancho and Sole?, 2001; Ferrer-i-Cancho et al
2007; Kapustin and Jamsen, 2007) and many in-
teresting conclusions have been drawn. For in-
stance, it has been found that these networks are
small-world in nature and are characterized by a
two regime power-law degree distribution. Efforts
have also been made to explain the emergence of
such a two regime degree distribution through net-
work growth models (Dorogovstev and Mendes,
2001). Although it is tempting to believe that a
lot is known about word co-occurrences, in or-
der to obtain a deeper insight into how these co-
occurrence patterns emerged there are many other
interesting properties that need to be investigated.
One such property is the spectrum of the word co-
162
occurrence network which can provide important
information about its global organization. In fact,
the application of this powerful mathematical ma-
chinery to infer global patterns in linguistic net-
works is rarely found in the literature (few excep-
tions are (Belkin and Goldsmith, 2002; Mukher-
jee et al 2009)). However, note that spectral anal-
ysis has been quite successfully applied in the
analysis of biological and social networks (Baner-
jee and Jost, 2007; Farkas et al 2001).
The aim of the present work is to investigate
the spectral properties of a word co-occurrence
network in order to understand its global struc-
ture. In particular, we study the properties of
seven different languages namely Bangla (Indo-
European family), English (Indo-European fam-
ily), Estonian (Finno-Ugric family), French (Indo-
European family), German (Indo-European fam-
ily), Hindi (Indo-European family) and Tamil
(Dravidian family). Quite importantly, as we shall
see, the most popular growth model proposed by
Dorogovtsev and Mendes (DM) (Dorogovstev and
Mendes, 2001) for explaining the degree distribu-
tion of such a network is not adequate to repro-
duce the spectrum of the network. This observa-
tion holds for all the seven different languages un-
der investigation. We shall further attempt to iden-
tify the precise (linguistic) reasons behind this dif-
ference in the spectrum of the empirical network
and the one reproduced by the model. Finally, as
an additional objective, we shall present a hitherto
unreported deeper analysis of this popular model
and show how its most important parameter is cor-
related to the size of the corpus from which the
empirical network is constructed.
The rest of the paper is laid out as follows.
In section 2, we shall present a brief review of
the previous works on word co-occurrence net-
works. This is followed by a short primer to spec-
tral analysis. In section 4, we outline the construc-
tion methodology of the word co-occurrence net-
works and present the experiments comparing the
spectrum of these real networks with those gen-
erated by the DM model. Section 5 shows how
the most important parameter of the DM model
varies with the size of the corpus from which the
co-occurrence networks are constructed. Finally,
we conclude in section 6 by summarizing our con-
tributions and pointing out some of the implica-
tions of the current work.
2 Word Co-occurrence Networks
In this section, we present a short review of the
earlier works on word co-occurrence networks,
where the nodes are the words and an edge be-
tween two words indicate that the words have co-
occurred in a language in certain context(s). The
most basic and well studied form of word co-
occurrence networks are the word collocation net-
works, where two words are linked by an edge if
they are neighbors (i.e., they collocate) in a sen-
tence (Ferrer-i-Cancho and Sole?, 2001).
In (Ferrer-i-Cancho and Sole?, 2001), the au-
thors study the properties of two types of col-
location networks for English, namely the unre-
stricted and the restricted ones. While in the unre-
stricted network, all the collocation edges are pre-
served, in the restricted one only those edges are
preserved for which the probability of occurrence
of the edge is higher than the case when the two
words collocate independently. They found that
both the networks exhibit small-world properties;
while the average path length between any two
nodes in these networks is small (between 2 and
3), the clustering coefficients are high (0.69 for the
unrestricted and 0.44 for the restricted networks).
Nevertheless, the most striking observation about
these networks is that the degree distributions fol-
low a two regime power-law. The degree distribu-
tion of the 5000 most connected words (i.e., the
kernel lexicon) follow a power-law with an expo-
nent ?3.07, which is very close to that predicted
by the Baraba?si-Albert growth model (Baraba?si
and Albert, 1999). These findings led the au-
thors to argue that the word usage of the human
languages is preferential in nature, where the fre-
quency of a word defines the comprehensibility
and production capability. Thus, higher the us-
age frequency of a word, higher is the probability
that the speakers will be able to produce it eas-
ily and the listeners will comprehend it fast. This
idea is closely related to the recency effect in lin-
guistics (Akmajian, 1995).
Properties of word collocation networks have
also been studied for languages other than En-
glish (Ferrer-i-Cancho et al 2007; Kapustin and
163
Jamsen, 2007). The basic topological characteris-
tics of all these networks (e.g., scale-free, small
world, assortative) are similar across languages
and thus, point to the fact that like Zipf?s law,
these are also linguistic universals whose emer-
gence and existence call for a non-trivial psycho-
linguistic account.
In order to explain the two regime power-
law in word collocation networks, Dorogovtsev
and Mendes (Dorogovstev and Mendes, 2001)
proposed a preferential attachment based growth
model (henceforth referred to as the DM model).
In this model, at every time step t, a new word
(i.e., a node) enters the language (i.e., the net-
work) and connects itself preferentially to one of
the pre-existing nodes. Simultaneously, ct (where
c is a positive constant and a parameter of the
model) new edges are grown between pairs of
old nodes that are chosen preferentially. Through
mathematical analysis and simulations, the au-
thors successfully establish that this model gives
rise to a two regime power-law with exponents
very close to those observed in (Ferrer-i-Cancho
and Sole?, 2001). In fact, for English, the val-
ues kcross (i.e., the point where the two power
law regimes intersect) and kcut (i.e., the point
where the degree distribution cuts the x-axis) ob-
tained from the model are in perfect agreement
with those observed for the empirical network.
Although the DM model is capable of explain-
ing the local topological properties of the word
collocation network, as we shall see in the forth-
coming sections, it is unable to reproduce the
global properties (e.g., the spectrum) of the net-
work.
3 A Primer to Spectral Analysis
Spectral analysis1 is a powerful mathematical
method capable of revealing the global structural
patterns underlying an enormous and complicated
environment of interacting entities. Essentially, it
refers to the systematic investigation of the eigen-
values and the eigenvectors of the adjacency ma-
trix of the network of these interacting entities.
In this section, we shall briefly outline the basic
1The term spectral analysis is also used in the context
of signal processing, where it refers to the study of the fre-
quency spectrum of a signal.
concepts involved in spectral analysis and discuss
some of its applications (see (Chung, 1994) for
details).
A network consisting of n nodes (labeled as
1 through n) can be represented by an n ? n
square matrix A, where the entry aij represents
the weight of the edge from node i to node j. Note
that A, which is known as the adjacency matrix,
is symmetric for an undirected graph and have
binary entries for an unweighted graph. ? is an
eigenvalue of A if there is an n-dimensional vec-
tor x such that
Ax = ?x
Any real symmetric matrix A has n (possibly non-
distinct) eigenvalues ?0 ? ?1 ? . . . ? ?n?1,
and corresponding n eigenvectors that are mutu-
ally orthogonal. The spectrum of a network is
the set of the distinct eigenvalues of the graph and
their corresponding multiplicities. It is a distribu-
tion usually represented in the form of a plot with
the eigenvalues in x-axis and their multiplicities in
the y-axis.
The spectrum of real and random networks dis-
play several interesting properties. Banerjee and
Jost (Banerjee and Jost, 2007) report the spectrum
of several biological networks and show that these
are significantly different from the spectrum of ar-
tificially generated networks. It is worthwhile to
mention here that spectral analysis is also closely
related to Principal Component Analysis and Mul-
tidimensional Scaling. If the first few (say d)
eigenvalues of a matrix are much higher than the
rest of the eigenvalues, then one can conclude that
the rows of the matrix can be approximately rep-
resented as linear combinations of d orthogonal
vectors. This further implies that the correspond-
ing graph has a few motifs (subgraphs) that are re-
peated a large number of time to obtain the global
structure of the graph (Banerjee and Jost, 2009).
In the next section, we shall present a thorough
study of the spectrum of the word co-occurrence
networks across various languages.
4 Experiments and Results
For the purpose of our experiments, we con-
struct word collocation networks for seven dif-
ferent languages namely, Bangla, English, Esto-
164
Figure 1: Cumulative degree distributions for Bangla, English, Estonian, French, German, Hindi and
Tamil respectively. Each red line signifies the degree distribution for the empirical network while each
blue line signifies the one obtained from the DM model.
Lang. Tokens (Mill.) Words KLD c Max. Eig. (Real) Max. Eig. (DM)
English 32.5 97144 0.21 5.0e-4 849.1 756.8Hindi 20.2 99210 0.32 2.3e-4 472.5 329.5Bangla 12.7 100000 0.29 2.0e-3 326.2 245.0German 5.0 159842 0.19 6.3e-5 192.3 110.7Estonian 4.0 100000 0.25 1.1e-4 158.6 124.0Tamil 2.3 75929 0.24 9.9e-4 116.4 73.06French 1.8 100006 0.44 8.0e-5 236.1 170.1
Table 1: Summary of results comparing the structural properties of the empirical networks for the seven
languages and the corresponding best fits (in terms of KLD) obtained from the DM model.
nian, French, German, Hindi and Tamil. We used
the corpora available in the Lipezig Corpora Col-
lection (http://corpora.informatik.uni-leipzig.de/)
for English, Estonian, French and German. The
Hindi, Bangla and Tamil corpora were collected
by crawling some online newspapers. In these net-
works, each distinct word corresponds to a ver-
tex and two vertices are connected by an edge
if the corresponding two words are adjacent in
one or more sentences in the corpus. We assume
the network to be undirected and unweighted (as
in (Ferrer-i-Cancho and Sole?, 2001)).
As a following step, we simulate the DM model
and reproduce the degree distribution of the col-
location networks for the seven languages. We
vary the parameter c in order to minimize the KL
165
divergence (KLD) (Kullback and Leibler, 1951)
between the empirical and the synthesized dis-
tributions and, thereby, obtain the best match.
The results of these experiments are summarized
through Figure 1 and Table 1. The results clearly
show that the DM model is indeed capable of gen-
erating the degree distribution of the collocation
networks to a very close approximation for cer-
tain values of the parameter c (see Table 1 for the
values of c and the corresponding KLD).
Subsequently, for the purpose of spectral anal-
ysis, we construct subgraphs induced by the top
5000 nodes for each of the seven empirical net-
works as well as those generated by the DM model
(i.e., those for which the degree distribution fits
best in terms of KLD with the real data). We then
compute and compare the spectrum of the real
and the synthesized networks (see Figure 2 and
Table 1). It is quite apparent from these results
that the spectra of the empirical networks are sig-
nificantly different from those obtained using the
DM model. In general, the spectral plots indicate
that the adjacency matrices for networks obtained
from the DM model have a higher rank than those
for the empirical networks. Further, in case of the
synthesized networks, the first eigenvalue is sig-
nificantly larger than the second whereas for the
empirical networks the top 3 to 4 eigenvalues are
found to dominate. Interestingly, this property is
observed across all the languages under investiga-
tion.
We believe that the difference in the spectra is
due to the fact that the ordering of the words in
a sentence are strongly governed by the grammar
or the syntax of the language. Words belong to
a smaller set of lexico-syntactic categories, which
are more commonly known as the parts-of-speech
(POS). The co-occurrence patterns of the words
are influenced, primarily, by its POS category. For
instance, nouns are typically preceded by articles
or adjectives, whereas verbs might be preceded by
auxiliary verbs, adverbs or nouns, but never ar-
ticles or adjectives. Therefore, the words ?car?
and ?camera? are more likely to be structurally
similar in the word co-occurrence network, than
?car? and ?jumped?. In general, the local neigh-
borhoods of the words belonging to a particular
POS is expected to be very similar, which means
that several rows in the adjacency matrix will be
very similar to each other. Thus, the matrix is ex-
pected to have low rank.
In fact, this property is not only applicable to
syntax, but also semantics. For instance, even
though adjectives are typically followed by nouns,
semantic constraints make certain adjective-noun
co-occurrences (e.g., ?green leaves?) much more
likely than some others (e.g., ?green dreams? or
?happy leaves?). These notions are at the core of
latent semantics and vector space models of se-
mantics (see, for instance, Turney and Pantel (Tur-
ney and Pantel, 2010) for a recent study). The DM
model, on the other hand, is based on the recency
effect that says that the words which are produced
most recently are easier to remember and there-
fore, easier to produce in the future. Preferential
attachment models the recency effect in word pro-
duction, which perhaps is sufficient to replicate
the degree distribution of the networks. However,
the model fails to explain the global properties,
precisely because it does not take into account
the constraints that govern the distribution of the
words.
It is quite well known that the spectrum of a net-
work can be usually obtained by iteratively pow-
ering the adjacency matrix of the network (aka
power iteration method). Note that if the adja-
cency matrices of the empirical and the synthe-
sized networks are powered even once (i.e., they
are squared)2, their degree distributions match no
longer (see Figure 3). This result further cor-
roborates that although the degree distribution of
a word co-occurrence network is quite appropri-
ately reproduced by the DM model, more global
structural properties remain unexplained. We be-
lieve that word association in human languages
is not arbitrary and therefore, a model which ac-
counts for the clustering of words around their
POS categories might possibly turn out to present
a more accurate explanation of the spectral prop-
erties of the co-occurrence networks.
166
Figure 2: The spectrum for Bangla, English, Estonian, French, German, Hindi and Tamil respectively.
The last plot shows a portion of the spectrum for English magnified around 0 for better visualization.
All the curves are binned distributions with bin size = 100. The blue line in each case is the spectrum
for the network obtained from the DM model while each red line corresponds to the spectrum for the
empirical network.
5 Reinvestigating the DM Model
In this section, we shall delve deeper into explor-
ing the properties of the DM model since it is one
of the most popular and well accepted models for
explaining the emergence of word associations in
a language. In particular, we shall investigate the
influence of the model parameter c on the emer-
gent results.
If we plot the value of the parameter c (from
Table 1) versus the size of the corpora (from Ta-
ble 1) used to construct the empirical networks for
the different languages we find that the two are
highly correlated (see Figure 4).
2Note that this squared network is weighted in nature. We
threshold all edges below the weight 0.07 so that the resultant
network is neither too dense nor too sparse. The value of the
threshold is chosen based on the inspection of the data.
In order to further check the dependence of c
on the corpus size we perform the following ex-
periment. We draw samples of varying corpus
size and construct empirical networks from each
of them. We then simulate the DM model and at-
tempt to reproduce the degree distribution for each
of these empirical networks. In each case, we note
the value c for which the KLD between the empir-
ical and the corresponding synthesized network is
minimum. Figure 5 shows the result of the above
experiment for English. The figure clearly indi-
cates that as the corpus size increases the value of
the parameter c decreases. Similar trends are ob-
served for all the other languages.
In general, one can mathematically prove that
the parameter c is equal to the rate of change of
the average degree of the network with respect to
167
Figure 3: Cumulative degree distribution for the
squared version of the networks for English. The
red line is the degree distribution for the squared
version of the empirical network while the blue
line is degree distribution of the squared version
of the network obtained from the DM model. The
trends are similar for all the other languages.
the time t. The proof is as follows.
At every time step t, the number of new edges
formed is (1+ct). Since each edge contributes to
a total degree of 2 to the network, the sum of the
degrees of all the nodes in the network (ktot) is
ktot = 2
T?
t=1
(1 + ct) = 2T + cT (T + 1) (1)
At every time step, only one new node is added
to the network and therefore the total number of
nodes at the end of time T is exactly equal to T .
Thus the average degree of the network is
?k? = 2T + cT (T + 1)T = 2 + c(T + 1) (2)
The rate of change of average degree is
d?k?
dT = c (3)
and this completes the proof.
In fact, it is also possible to make a precise
empirical estimate of the value of the parameter
c. One can express the average degree of the co-
occurrence networks as the ratio of twice the bi-
gram frequency (i.e., twice the number of edges
in the network) to the unigram frequency (i.e., the
0 5 10 15 20 25 30 350.5
1
1.5
2
2.5
3
3.5
4
4.5
5 x 10?4
Corpus Size(Across Languages)
c
Figure 4: The parameter c versus the corpus size
for the seven languages.
Figure 5: The parameter c versus the corpus size
for English.
number of nodes or unique words in the network).
Therefore, if we can estimate this ratio we can eas-
ily estimate the value of c using equation 3. Let
us denote the total number of distinct bigrams and
unigrams after processing a corpus of size N by
B(N) and W (N) respectively. Hence we have
?k? = 2B(N)W (N) (4)
Further, the number of distinct new unigrams after
Language B(N) W (N) c
English 29.2N.67 59.3N.43 .009N?.20
Hindi 26.2N.66 49.7N.46 .009N?.26
Tamil 1.9N.91 6.4N.71 .207N?.50
Table 2: Summary of expressions for B(N),
W (N) and c for English, Hindi and Tamil.
168
Figure 6: Variation of B(N) and W (N) with N
for English (in doubly-logarithmic scale). The
blue dots correspond to variation of B(N) while
the red dots correspond to the variation of W (N).
processing a corpus of size N is equivalent to T
and therefore
T = W (N) (5)
Sampling experiments across different languages
demonstrate that W (N) and B(N) are of the form
?N? (? < 1) where ? and ? are constants. For
instance, Figure 6 shows in doubly-logarithmic
scale how B(N) and W (N) varies with N for
English. The R2 values obtained as a result of
fitting the B(N) versus N and the W (N) ver-
sus N plots using equations of the form ?N? for
English, Hindi and Tamil are greater than 0.99.
This reflects the high accuracy of the fits. Similar
trends are observed for all the other languages.
Finally, using equations 3, 4 and 5 we have
c = d?k?dT =
d?k?
dN
dN
dT (6)
and plugging the values of B(N) and W (N) in
equation 6 we find that c has the form ?N?? (? <
1) where ? and ? are language dependent positive
constants. The values of c obtained in this way
for three different languages English, Hindi and
Tamil are noted in Table 5.
Thus, we find that as N ? ?, c ? 0. In
other words, as the corpus size grows the number
of distinct new bigrams goes on decreasing and
ultimately reaches (almost) zero for a very large
sized corpus. Now, if one plugs in the values of c
and T obtained above in the expressions for kcross
and kcut in (Dorogovstev and Mendes, 2001), one
observes that limN?? kcrosskcut = 0. This impliesthat as the corpus size becomes very large, the
two-regime power law (almost) converges to a sin-
gle regime with an exponent equal to -3 as is ex-
hibited by the Baraba?si-Albert model (Baraba?si
and Albert, 1999). Therefore, it is reasonable to
conclude that although the DM model provides a
good explanation of the degree distribution of a
word co-occurrence network built from a medium
sized corpora, it does not perform well for very
small or very large sized corpora.
6 Conclusions
In this paper, we have tried to investigate in de-
tail the co-occurrence properties of words in a
language. Some of our important observations
are: (a) while the DM model is able to reproduce
the degree distributions of the word co-occurrence
networks, it is not quite appropriate for explaining
the spectrum of these networks; (b) the parameter
c in the DM model signifies the rate of change of
the average degree of the network with respect to
time; and (c) the DM model does not perform well
in explaining the degree distribution of a word co-
occurrence network when the corpus size is very
large.
It is worthwhile to mention here that our analy-
sis of the DM model leads us to a very important
observation. As N grows, the value of kcut grows
at a much faster rate than the value of kcross and
in the limit N ?? the value of kcut is so high as
compared to kcross that the ratio kcrosskcut becomes(almost) zero. In other words, the kernel lexicon,
formed of the words in the first regime of the two
regime power-law and required to ?say everything
or almost everything? (Ferrer-i-Cancho and Sole?,
2001) in a language, grows quite slowly as new
words creep into the language. In contrast, the pe-
ripheral lexicon making the other part of the two
regime grows very fast as new words enter the lan-
guage. Consequently, it may be argued that since
the kernel lexicon remains almost unaffected, the
effort to learn and retain a language by its speak-
ers increases only negligibly as new words creep
into the language.
169
References
A. Akmajian. Linguistics: An introduction to Lan-
guage and Communication. MIT Press, Cambridge,
MA, 1995.
A. Banerjee and J. Jost. Spectral plots and the repre-
sentation and interpretation of biological data. The-
ory in Biosciences, 126(1), 15-21, 2007.
A. Banerjee and J. Jost. Graph spectra as a system-
atic tool in computational biology. Discrete Applied
Mathematics, 157(10), 2425?2431, 2009.
A.-L. Baraba?si and R. Albert. Emergence of scaling in
random networks. Science, 286, 509-512, 1999.
M. Belkin and J. Goldsmith. Using eigenvectors of
the bigram graph to infer morpheme identity. In
Proceedings of Morphological and Phonological
Learning, Association for Computational Linguis-
tics, 41-47, 2002.
F. R. K. Chung. Spectral Graph Theory. Number 2 in
CBMS Regional Conference Series in Mathematics,
American Mathematical Society, 1994.
S. N. Dorogovstev and J. F .F. Mendes. Language as an
evolving word Web. Proceedings of the Royal Soci-
ety of London B, 268, 2603-2606, 2001.
I. J. Farkas, I. Dere?nyi, A. -L. Baraba?si and T. Vicsek.
Spectra of ?real-world? graphs: Beyond the semi-
circle law, Physical Review E, 64, 026704, 2001.
R. Ferrer-i-Cancho and R. V. Sole?. The small-world of
human language. Proceedings of the Royal Society
of London B, 268, 2261?2266, 2001.
R. Ferrer-i-Cancho, A. Mehler, O. Pustylnikov and
A. D??az-Guilera. Correlations in the organization
of large-scale syntactic dependency networks. In
Proceedings of TextGraphs-2: Graph-Based Algo-
rithms for Natural Language Processing, 65-72, As-
sociation for Computational Linguistics, 2007.
V. Kapustin and A. Jamsen. Vertex degree distribution
for the graph of word co-occurrences in Russian. In
Proceedings of TextGraphs-2: Graph-Based Algo-
rithms for Natural Language Processing, 89-92, As-
sociation for Computational Linguistics, 2007.
S. Kullback and R. A. Leibler. On information and
sufficiency. Annals of Mathematical Statistics 22(1),
79-86, 1951.
A. Mukerjee, M. Choudhury and R. Kannan. Discov-
ering global patterns in linguistic networks through
spectral analysis: A case study of the consonant in-
ventories. In Proceedings of EACL, 585?593, Asso-
ciation for Computational Linguistics, 2009.
P. D. Turney and P. Pantel. From frequency to meaning:
Vector space models of semantics. In JAIR, 37, 141-
188, 2010.
170
Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 35?42,
Beijing, August 2010
Co-occurrence Graph Based Iterative Bilingual Lexicon Extraction From
Comparable Corpora
Diptesh Chatterjee and Sudeshna Sarkar and Arpit Mishra
Department of Computer Science and Engineering
Indian Institute of Technology Kharagpur
{diptesh,sudeshna,arpit}@cse.iitkgp.ernet.in
Abstract
This paper presents an iterative algorithm
for bilingual lexicon extraction from com-
parable corpora. It is based on a bag-
of-words model generated at the level of
sentences. We present our results of ex-
perimentation on corpora of multiple de-
grees of comparability derived from the
FIRE 2010 dataset. Evaluation results on
100 nouns shows that this method outper-
forms the standard context-vector based
approaches.
1 Introduction
Bilingual dictionaries play a pivotal role in a num-
ber of Natural Language Processing tasks like
Machine Translation and Cross Lingual Informa-
tion Retrieval(CLIR). Machine Translation sys-
tems often use bilingual dictionaries in order to
augment word and phrase alignment (Och and
Ney, 2003). CLIR systems use bilingual dictio-
naries in the query translation step (Grefenstette,
1998). However, high coverage electronic bilin-
gual dictionaries are not available for all language
pairs. So a major research area in Machine Trans-
lation and CLIR is bilingual dictionary extraction.
The most common approach for extracting bilin-
gual dictionary is applying some statistical align-
ment algorithm on a parallel corpus. However,
parallel corpora are not readily available for most
language pairs. Also, it takes a lot of effort to ac-
tually get the accurate translations of sentences.
Hence, constructing parallel corpora involves a lot
of effort and time. So in recent years, extract-
ing bilingual dictionaries from comparable cor-
pora has become an important area of research.
Comparable corpora consist of documents on sim-
ilar topics in different languages. Unlike parallel
corpora, they are not sentence aligned. In fact,
the sentences in one language do not have to be
the exact translations of the sentence in the other
language. However, the two corpora must be on
the same domain or topic. Comparable corpora
can be obtained more easily than parallel corpora.
For example, a collection of news articles from
the same time period but in different languages
can form a comparable corpora. But after care-
ful study of news articles in English and Hindi
published on same days at the same city, we have
observed that along with articles on similar top-
ics, the corpora also contain a lot of articles which
have no topical similarity. Thus, the corpora are
quite noisy, which makes it unsuitable for lexicon
extraction. Thus another important factor in com-
parable corpora construction is the degree of sim-
ilarity of the corpora.
Approaches for lexicon extraction from compara-
ble corpora have been proposed that use the bag-
of-words model to find words that occur in similar
lexical contexts (Rapp, 1995). There have been
approaches proposed which improve upon this
model by using some linguistic information (Yuu
and Tsujii, 2009). However, these require some
linguistic tool like dependency parsers which are
not commonly obtainable for resource-poor lan-
guages. For example, in case of Indian languages
like Hindi and Bengali, we still do not have good
enough dependency parsers. In this paper, we
propose a word co-occurrence based approach for
lexicon extraction from comparable corpora using
English and Hindi as the source and target lan-
guages respectively. We do not use any language-
35
specific resource in our approach.
We did experiments with 100 words in En-
glish,and show that our approach performs signif-
icantly better than the the Context Heterogeneity
approach (Fung, 1995). We show the results over
corpora with varying degrees of comparability.
The outline of the paper is as follows. In section
2, we analyze the different approaches for lexicon
extraction from comparable corpora. In section 3,
we present our algorithm and the experimental re-
sults. In section 4, we present an analysis of the
results followed by the conclusion and future re-
search directions in section 5.
2 Previous Work
One of the first works in the area of comparable
corpora mining was based on word co-occurrence
based approach (Rapp, 1995). The basic assump-
tion behind this approach was two words are likely
to occur together in the same context if their joint
probability of occurrence in a corpus exceeds the
probability that the words occur randomly. In his
paper, Rapp made use of a similarity matrix and
using a joint probability estimate determined the
word maps. However this approach did not yield
significantly good results.
The ?Context Heterogeneity? approach was one
of the pioneering works in this area. It uses a 2-
dimensional context vector for each word based
on the right and left context. The context vector
depended on how many distinct words occur in the
particular context and also the unigram frequency
of the word to be translated. Euclidean distance
between context vectors was used as a similarity
measure.
Another approach used Distributed Clustering of
Translational Equivalents for word sense acqui-
sition from bilingual comparable corpora (Kaji,
2003). However, the major drawback of this paper
is the assumption that translation equivalents usu-
ally represent only one sense of the target word.
This may not be the case for languages having
similar origin, for example, Hindi and Bengali.
Approaches using context information for extract-
ing lexical translations from comparable corpora
have also been proposed (Fung and Yee, 1998;
Rapp, 1999). But they resulted in very poor cov-
erage. These approaches were improved upon
by extracting phrasal alignments from comparable
corpora using joint probability SMT model (Ku-
mano et al, 2007) .
Another proposed method uses dependency pars-
ing and Dependency Heterogeneity for extracting
bilingual lexicon (Yuu and Tsujii, 2009) . This
approach was similar to that of Fung, except they
used a dependency parser to get the tags for each
word and depending on the frequency of each tag
they defined a vector to represent each word in
question. Here too, Euclidean similarity was used
to compute the similarity between two words us-
ing their context vectors. However, this method is
dependent on availability of a dependency parser
for the languages and is not feasible for languages
for which resources are scarce.
3 Bilingual Dictionary Extraction Using
Co-occurrence Information
3.1 Motivation
The Context Heterogeneity and Dependency Het-
erogeneity approaches suffer from one major
drawback. They do not use any kind of infor-
mation about how individual words combine in a
particular context to form a meaningful sentence.
They only use some statistics about the number of
words that co-occur in a particular context or the
number of times a word receives a particular tag
in dependency parsing. So, we wished to study if
the quality of dictionary extracted would improve
if we consider how individual words co-occur in
text and store that information in the form of a
vector, with one dimension representing one word
in the corpus. One important point to note here
is that the function words in a language are usu-
ally very small in number. If we need to construct
a dictionary of function words in two languages,
that can be done without much effort manually.
Also, the function words do not play an impor-
tant role in CLIR applications, as they are usually
stripped off.
Our algorithm is based on the intuition that words
having similar semantic connotations occur to-
gether. For example, the words ?bread? is more
likely to occur with ?eat? than with ?play?. Our
algorithm uses this distribution of co-occurrence
frequency along with a small initial seed dictio-
36
nary to extract words that are translations of one
another. We define a co-occurrence vector of
words in both the languages, and also record the
number of times two words co-occur. To find
the translation for word Wx, we check for the
words co-occurring with Wx such that this word
already has a map in the other language, and com-
pute a scoring function using all such words co-
occurring with Wx. In short, we use the already
existing information to find new translations and
add them to the existing lexicon to grow it. Be-
low is a snapshot of a part of the data from one
of our experiments using the FIRE 20101 cor-
pus. For each word in English and Hindi, the co-
occurrence data is expressed as a list of tuples.
Each tuple has the form (word, co-occurrence
frequency). For the Hindi words, the English
meaning has been provided in parenthesis. For
the seed lexicon and final lexicon, the format is
(source word, target word, strength).
English:
1. teacher:{(training,49),(colleges,138),
(man,22)}
2. car:{(drive,238),(place,21)}
3. drive:{(car,238),(steer,125),(city,12),
(road,123)}
Hindi:
1. ghar(home):{(khidki(window),133),(makAn
(house),172), (rAstA(road),6)}
2. gAdi(car):{(rAsta,92),(chAlak(driver),121),
(signal,17)}
3. shikshaka(teacher):{(vidyalaya(school),312),
(makAn(house),6)}
Seed lexicon:
1. (colleges,vidyalaya,0.4)
2. (colleges,mahavidyalaya(college),0.6)
3. (car,gAdi,1.0)
The following is a snapshot from the final results
given by the algorithm:
1Forum For Information Retrieval
http://www.isical.ac.in/?clia/index.html
1. (car,gAdi,1.0)
2. (teacher,shikshak,0.62)
3. (teacher, vidyalaya,0.19)
4. (road, rAsta, 0.55)
3.2 The Algorithm
For extracting bilingual lexicon, we have not con-
sidered the function words of the two languages.
In order to filter out the function words, we have
made use of the assumption that content words
usually have low frequency in the corpus, whereas
function words have very high frequency. First,
we define some quantities:
Let the languages be E and H.
We = Set of words in E = {e1, e2, ...., eN}
Wh = Set of words in H = {h1, h2, ...., hM}
|We| = N
|Wh| = M
MAP = Initial map given
= {(ei, hj , wij)|wij = wt(ei, hj), ei ? We, hj ? Wh}
EM = Set of words in E which are included in
entries of MAP
HM = Set of words in H which are included in
entries of MAP
Co occ(x) = Set of words which co-occur with word x
Co occ?(x) =
(
Co occ(x) ? EM if x ? We
Co occ(x) ?HM if x ? Wh
Wte(x) = {Wey|y ? We and y ? Co occ(x)}
Wth(x) = {Why|y ? Wh and y ? Co occ(x)}
Given a comparable corpus, we follow the fol-
lowing steps of processing:
1. A sentence segmentation code is run to seg-
ment the corpus into sentences.
2. The sentence-segmented corpus is cleaned of
all punctuation marks and special symbols by
replacing them with spaces.
37
Algorithm 1 Algorithm to Extract Bilingual Dictionary by using word Co-occurrence Information
repeat
for ei ? We do
for hj ? Wh do
if (ei, hj , 0) ? MAP then
wt(ei, hj) =
P
e?Co occ?(ei)
P
h?Co occ?(hj)(WijWeeiWhhj )P
e?Co occ?(ei)
P
h?Co occ?(hj)(WeeiWhhj )end if
end for
end for
Select the pair with highest value of wt(ei, bj) and add it to the existing map and normalize
until termination
3. The collection frequency of all the terms are
computed and based on a threshold, the func-
tion words are filtered out.
4. The co-occurrence information is computed
at sentence-level for the remaining terms. In
a sentence, if words wi and wj both occur,
then wi ? Co occ(wj) and vice versa.
5. Since we can visualize the co-occurrence in-
formation in the form of a graph, we next
cluster the graph into C clusters.
6. From each cluster Ci, we choose some fixed
number number of words and manually find
out their translation in the target language.
This constitutes the initial map.
7. Next we apply Algorithm 1 to compute the
word maps.
The time complexity of the algorithm is
O(IM2N2), where I is the number of itera-
tions of the algorithm.
3.3 Corpus Construction
The corpora used for evaluating our algorithm
were derived from the FIRE 2010 English and
Hindi corpora for the ad-hoc retrieval task. These
corpora contained news articles spanning over a
time period of three years from two Indian news-
papers, ?The Dainik Jagaran? in Hindi and ?The
Telegraph? in English. However, due to the ex-
treme level of variation of the topics in these cor-
pora, we applied a filtering algorithm to select a
subset of the corpora.
Our approach to make the text similar involved
reducing the corora based on matching Named
Entities. Named Entities of English and Hindi
corpus were listed using LingPipe2 and a Hindi
NER system built at IIT Kharagpur(Saha et al,
1999). The listed Named Entities of the two cor-
pora were compared to find the matching Named
Entities. Named Entities in Hindi Unicode were
converted to iTRANS3 format and matched with
English Named Entities using edit distance. Unit
cost was defined for each insert and delete opera-
tion. Similar sounding characters like ?s?, ?c?,?a?,
?e? etc were assigned a replacement cost of 1 and
other characters were assigned a replacement cost
of 2. Two Named Entities were adjudged match-
ing if:
(2 ? Cost)/(WLh +WLe) < 0.5
where,
WLh = Length of Hindi word
WLe = Length of English word
Using this matching scheme, accuracy of match-
ing of Hindi and English Named Entities was
found to be > 95%. It was observed that there
are large number of Named Entities with small
frequency and few Named Entities with large fre-
quency. So a matching list was prepared which
contained only those Named Entities which had
frequency larger than a ?MaxFreq . This en-
sured that matching list had words with high fre-
quency in both corpus.So English words with fre-
quency larger than 368 and Hindi words with
frequency larger than 223 were considered for
matching. Based on this matching list, the two
2http://alias-i.com/lingpipe/
3http://www.aczoom.com/itrans/
38
Language Total NE Unique
NE
NE with freq
larger than?MaxFreq
NE
Matched
Total No
of docs
% of NE covered
According
to Zipf?s
Law
In the
actual
corpus
Hindi 1195474 37606 686 360 54271 63.0% 74.3%
English 5723292 137252 2258 360 87387 65.2% 71.0%
Table 1: Statistics of the main corpora used for extraction
Corpus Max Freq
Word
Max
Freq
?MaxFreq
Hindi bharat 50072 223
English calcutta 135780 368
Table 2: Criteria used for thresholding in the two
corpora
Matching
% of
NE per
document
Total documents in
corpora
Hindi English
> 10% 34694 16950
> 20% 14872 4927
> 30% 2938 1650
Table 3: Statistics of extracted corpora
corpora were reduced by including only those files
each of which contained more than a certain fixed
percentage of total matching Named Entities. The
corpus statistics are provided in tables 1, 2 and 3.
We assume that distribution of Named Entities
follows Zipf?s law (Zipf, 1949). And analysis
shows that Named Entities with frequency greater
than the chosen threshold lead to high cover-
age both theoretically and in practice (Table 1).
Hence, the threshold was chosen as ?MaxFreq.
The differences in the theoretical and actual val-
ues can be attributed to the poor performance of
the NER systems, especially the Hindi NER sys-
tem, whose output contained a number of false
positives.
3.4 Experimental Setup
The languages we used for our experiments were
English and Hindi. English was the source lan-
guage and Hindi was chosen as the target. For
our experiments, we used a collection frequency
threshold of 400 to filter out the function words.
The words having a collection frequency more
than 400 were discarded. This threshold was ob-
tained manually by ?Trial and Error? method in
order to perform an effective function word fil-
tering. For each corpora, we extracted the co-
occurrence information and then clustered the co-
occurrence graph into 20 clusters. From each
cluster we chose 15 words, thus giving us an over-
all initial seed dictionary size of 300. We ran the
algorithm for 3000 iterations.
For graph clustering, we used the Graclus system
(Dhillon et al, 2007) which uses a weighted ker-
nel k-means clustering algorithm at various levels
of coarseness of the input graph.
3.5 Evaluation Method and Results
For evaluation, we have used the Accuracy and
MMR measure (Voorhees, 1999). The measures
are defined as follows:
Accuracy = 1N
PN
i=1 ti
where, ti =
(
1 if correct translation in top n
0 otherwise
MMR = 1N
PN
i=1
1
ranki
where, ranki =
(
ri if ri ? n
0 otherwise
n means top n evaluation
ri means rank of correct translation in top n ranking
N means total number of words used for evaluation
For our experiments, we have used:
39
Corpus Context Het-
erogeneity
Co-
occurrence
Acc MMR Acc MMR
> 10% 0.14 0.112 0.16 0.135
> 20% 0.21 0.205 0.27 0.265
> 30% 0.31 0.285 0.35 0.333
Table 4: Comparison of performance between
Context Heterogeneity and Co-occurrence Ap-
proach for manual evaluation
n = 5
N = 100
The 100 words used for evaluation were chosen
randomly from the source language.
Two evaluation methods were followed - manual
and automated. In the manual evaluation, a
person who knows both English and Hindi was
asked to find the candidate translation in the target
language for the words in the source language.
Using this gold standard map, the Accuracy and
MMR values were computed.
In the second phase (automated), lexicon ex-
tracted is evaluated against English to Hindi
wordnet4. The evaluation process proceeds as
follows:
1. Hashmap is created with English words as
keys and Hindi meanings as values.
2. English words in the extracted lexicon are
crudely stemmed so that inflected words
match the root words in the dictionary. Stem-
ming is done by removing the last 4 charac-
ters, one at a time and checking if word found
in dictionary.
3. Accuracy and MMR are computed.
As a reference measure, we have used Fung?s
method of Context Heterogeneity with a context
window size of 4. The results are tabulated in
Tables 4 and 6. We can see that our proposed
algorithm shows a significant improvement over
the Context Heterogeneity method. The degree
of improvement over the Context Heterogeneity
4Downloadable from
http://sanskritdocuments.org/hindi/dict/eng-hin-itrans.html
Corpus Accuracy MMR
> 10% ? 14.28% ? 20.53%
> 20% ? 28.57% ? 29.27%
> 30% ? 12.9% ? 16.84%
Table 5: Degree of improvement shown by Co-
occurrence approach over Context Heterogeneity
for manual evaluation
Corpus Context Het-
erogeneity
Co-
occurrence
Acc MMR Acc MMR
> 10% 0.05 0.08 0.05 0.08
> 20% 0.06 0.06 0.11 0.10
> 30% 0.13 0.11 0.15 0.13
Table 6: Comparison of performance between
Context Heterogeneity and Co-occurrence Ap-
proach for auto-evaluation
is summarized in Tables 5 and 7. For auto
evaluation, We see that the proposed approach
shows the maximum improvement (83.33% in
Accuracy and 66.67% in MMR) in performance
when the corpus size is medium. For very large
(too general) corpora, both the approaches give
identical result while for very small (too specific)
corpora, the proposed approach gives slightly
better results than the reference.
The trends are similar for manual evaluation.
Once again, the maximum improvement is
observed for the medium sized corpus (> 20%).
However, in this evaluation system, the proposed
approach performs much better than the reference
even for the large (more general) corpora.
Corpus Accuracy MMR
> 10% 0.0% 0.0%
> 20% ? 83.33% ? 66.67%
> 30% ? 15.38% ? 18.18%
Table 7: Degree of improvement shown by Co-
occurrence approach over Context Heterogeneity
for auto-evaluation
40
4 Discussion
The co-occurrence based approach used in this
paper is quite a simple approach in the sense that
it does not make use of any kind of linguistic
information. From the aforementioned results
we can see that a model based on simple word
co-occurrence highly outperforms the ?Context
Heterogeneity? model in almost all the cases.
One possible reason behind this is the amount of
information captured by our model is more than
that captured by the ?Context Heterogeneity?
model. ?Context Heterogeneity? does not model
actual word-word interactions. Each word is
represented by a function of the number of
different contexts it can occur in. However, we
represent the word by a co-occurrence vector.
This captures all possible contexts of the word.
Also, we can actually determine which are the
words which co-occur with any other word. So
our model captures more semantics of the word in
question than the ?Context Heterogeneity? model,
thereby leading to better results. Another possible
factor is the nature in which we compute the
translation scores. Due to the iterative nature of
the algorithm and since we normalize after each
iteration, some of the word pairs that received
unduly high score in an earlier iteration end up
having a substantially low score. However, since
the ?Context Heterogeneity? does only a single
pass over the set of words, it fails to tackle this
problem.
The seed dictionary plays an important role in
our algorithm. A good seed dictionary gives us
some initial information to work with. However,
since ?Context Heterogeneity? does not use a
seed dictionary, it loses out on the amount of
information initially available to it. Since the seed
dictionary size for our approach is quite small,
it can be easily constructed manually. However,
how the seed dictionary size varies with corpus
size is an issue that remains to be seen.
Another important factor in our algorithm is the
way in which we have defined the co-occurrence
vectors. This is not the same as the context vector
that we define in case of Context Heterogeneity.
In a windowed context vector, we fail to capture a
lot of dependencies that might be captured using
a sentence-level co-occurrence. This problem is
especially more visible in case of free-word-order
languages like the Indo-European group of lan-
guages. For these languages, a windowed context
vector is also likely to introduce many spurious
dependencies. Since Hindi is a language of this
family, our algorithm captures many more correct
semantic dependencies than Context Heterogene-
ity algorithm, resulting in better preformance.
Another strong point of our proposed approach
is the closeness of the values of Accuracy and
MMR. This shows that the translation candidates
extracted by our algorithm are not only correct,
but also the best translation candidate gets the
highest score with high probability. This is a very
important factor in Machine Translation systems,
where a more accurate dictionary would give us
an improved performance.
A noticeable point about the evaluation scores is
the difference in scores given by the automated
system and the manual system. This can be
attributed to synonymy and spelling errors. In
the target language Hindi, synonymy plays a
very important part. It is not expected that all
synonyms of a particular word may be present
in an online dictionary. In such cases, the
manual evaluator marks a translation pair as
True, whereas the automated system marks it as
False. Instances of spelling errors have also been
found. For example, for the word ?neighbors?,
the top translation provided by the system was
?paDosana?(female neighbor). If we consider
root form of words, this is correct. But the actual
translation should be ?paDosiyAn?(neighbors,
may refer to both male and female). Thus the
auto evaluation system tags it as False, whereas
the manual evaluator tags it as True. There are
many more such occurrences throughout.
Apart from that, the manual evaluation process
has been quite relaxed. Even if the properties like
tense, number of words does not match, as long
as the root forms match the manual evaluator has
marked it as True. But this is not the case for
the automated evaluator. Although stemming has
been done, but problems still persist which can be
only solved by lemmatization, because Hindi is a
highly inflected language.
41
5 Conclusion and Future Work
In this paper we present a completely new ap-
proach for extracting bilingual lexicon from com-
parable corpora. We show the results of experi-
mentation on corpora of different levels of com-
parability. The basic feature of this approach is
that it is language independent and needs no ad-
ditional resource. We could not compare its per-
formance with the Dependency Heterogeneity al-
gorithm due to the lack of resources for Hindi.
So this can be taken up as a future work. Also,
the algorithm is quite inefficient. Another direc-
tion of research can be in trying to explore ways
to reduce the complexity of this algorithm. We
can also try to incorporate more linguistic infor-
mation into this model instead of just word co-
occurrence. It remains to be seen how these fac-
tors affect the performance of the algorithm. An-
other important question is what should be the size
of the seed dictionary for optimum performance
of the algorithm. This too can be taken up as a
future research direction.
References
Dhillon, I., Y. Guan, and B. Kulis. 2007. Weighted
graph cuts without eigenvectors: A multilevel ap-
proach. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence (PAMI), 29:11:1944?
1957, November.
Fung, Pascale and Lo Yuen Yee. 1998. An ir ap-
proach for translating new words from nonparallel,
comparable texts. In Proceedings of the 36th An-
nual Meeting of the Association for Computational
Linguistics / the 17th International Conference on
Computational Linguistics, pages 414?420.
Fung, Pascale. 1995. Compiling bilingual lexicon
entries from a non-parallel english-chinese corpus.
In Third Annual Workshop on Very Large Corpora,
Boston, Massachusetts, June.
Grefenstette, G. 1998. The problem of cross-language
information retrieval. Cross-language Information
Retrieval.
Kaji, H. 2003. Word sense acquisition from bilingual
comparable corpora. In Proc. of HLT-NAACL 2003
Main papers, pages 32?39.
Kumano, T., H. Takana, and T. Tokunaga. 2007. Ex-
tracting phrasal alignments from comparable cor-
pora by using joint probability smt model. In Proc.
of TMI.
Och, F. and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51, March.
Rapp, Reinhard. 1995. Identifying word translations
in non-parallel texts. In Proc. of TMI.
Rapp, Reinhard. 1999. Automatic identification of
word translations from unrelated english and ger-
man corpora. In Proceedings of the 37th Annual
Meeting of the Association for Computational Lin-
guistics, pages 519?526.
Saha, Sujan Kumar, Sudeshna Sarkar, and Pabitra Mi-
tra. 1999. A hybrid feature set based maximum
entropy hindi named entity recognition. In Proceed-
ings of the Third International Joint Conference on
Natural Language Processing, pages 343?349, Hy-
derabad, India, January.
Voorhees, E.M. 1999. The trec-8 question answer-
ing track report. In Proceedings of the 8th Text Re-
trieval Conference.
Yuu, K. and J. Tsujii. 2009. Extracting bilingual dic-
tionary from comparable corpora with dependency
heterogeneity. In Proc. of NAACL-HLT, short pa-
pers, pages 121?124.
Zipf, George Kingsley. 1949. Human Behaviour and
the Principle of Least Effort: an Introduction to Hu-
man Ecology. Addison-Wesley.
42
