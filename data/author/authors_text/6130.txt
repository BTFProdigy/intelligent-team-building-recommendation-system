Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 273?280, Vancouver, October 2005. c?2005 Association for Computational Linguistics
PP-attachment disambiguation using large context
Marian Olteanu and Dan Moldovan
Human Language Technology Research Institute
The University of Texas at Dallas
Richardson, TX 75080
marian@hlt.utdallas.edu
moldovan@utdallas.edu
Abstract
Prepositional Phrase-attachment is a com-
mon source of ambiguity in natural lan-
guage. The previous approaches use lim-
ited information to solve the ambiguity
? four lexical heads ? although humans
disambiguate much better when the full
sentence is available. We propose to
solve the PP-attachment ambiguity with a
Support Vector Machines learning model
that uses complex syntactic and seman-
tic features as well as unsupervised in-
formation obtained from the World Wide
Web. The system was tested on several
datasets obtaining an accuracy of 93.62%
on a Penn Treebank-II dataset; 91.79% on
a FrameNet dataset when no manually-
annotated semantic information is pro-
vided and 92.85% when semantic infor-
mation is provided.
1 Problem description
1.1 PP-attachment ambiguity problem
Prepositional Phrase-attachment is a source of ambi-
guity in natural language that generates a significant
number of errors in syntactic parsing. For example
the sentence ?I saw yesterday the man in the park
with a telescope? has 5 different semantic interpre-
tations based on the way the prepositional phrases
?in the park? and ?with the telescope? are attached:
I saw yesterday [the man [in the park [with a tele-
scope]]]; I saw yesterday [the man [in the park]
[with a telescope]]; I saw yesterday [the man [in the
park]] [with a telescope]; I saw yesterday [the man]
[in the park [with a telescope]] and I saw yesterday
[the man] [in the park] [with a telescope].
The problem can be viewed as a decision of at-
taching a prepositional phrase (PP) to one of the
preceding head nouns or verbs. The ambiguity ex-
pressed by the number of potential parse trees gener-
ated by Context-Free Grammars increases exponen-
tially with the number of PPs. For a PP that follows
the object of a verb there are 2 parse trees, for a chain
of 2, 3, 4 and 5 PPs there are respectively 5, 14, 42
and 132 parse trees. Usually the average number of
consecutive PPs in a sentence increases linearly with
the length of the sentence.
Lexical and syntactic information alone is not suf-
ficient to resolve the PP-attachment problem; of-
ten semantic and/or contextual information is nec-
essary. For example, in ?I ate a pizza with an-
chovies?, ?with anchovies? attaches to the noun
?pizza?, where as in ?I ate a pizza with friends.?,
?with friends? attaches to the verb ?eat? ? example
found in (McLauchlan, 2001). There are instances
of PP-attachment, like the one in ?I saw the car in
the picture? that can be disambiguated only by using
contextual discourse information.
Usually, people don?t have much trouble in find-
ing the right way to attach PPs. But if one limits
the information used for disambiguation of the PP-
attachment to include only the verb, the noun repre-
senting its object, the preposition and the main noun
in the PP, the accuracy for human decision degrades
from 93.2% to 88.2% (Ratnaparkhi et al, 1994) on
a dataset extracted from Penn Treebank (Marcus et
273
al., 1993).
1.2 Motivation
Syntactic parsing is essential for many natural lan-
guage applications such as Machine Translation,
Question Answering, Information Extraction, Infor-
mation Retrieval, Automatic Speech Recognition.
Since parsing occurs early in the chain of NLP
processing steps it has a large impact on the over-
all system performance.
2 Approach
Our approach to solve the PP-attachment ambigu-
ity is based on a Support Vector Machines learner
(Cortes and Vapnik, 1995). The feature set contains
complex information extracted automatically from
candidate syntax trees generated by parsing (Char-
niak, 2000), trees that will be improved by more ac-
curate PP-attachment decisions. Some of these fea-
tures were proven efficient for semantic information
labeling (Gildea and Jurafsky, 2002). The feature
set alo includes unsupervised information obtained
from a very large corpus (World Wide Web). Fea-
tures containing manually annotated semantic infor-
mation about the verb and about the objects of the
verb have also been used. We adopted the standard
approach to distinguish between verb and noun at-
tachment; thus the classifier has to choose between
two classes: V when the prepositional phrase is at-
tached to the verb and N when the prepositional
phrase is attached to the preceding head noun.
3 Data
To be able to extract the required features from a
dataset instance, one must identify the verb, the
phrase identifying the object of the verb that pre-
cedes the prepositional phrase in question (np1)
which usually is part of the predicate-argument
structure of the verb, its head noun, the prepositional
phrase (np2), its preposition and its head noun (the
second most important word in the PP).
We have adopted the notation from (Collins and
Brooks, 1995), where v is the verb, n1 is the head
noun of object phrase, p is the preposition and n2 is
the head noun of the prepositional phrase.
Compared to our datasets, Ratnaparkhi?s dataset
(Ratnaparkhi et al, 1994) contains only the lexical
heads v, n1, p and n2. Thus, our methodology can-
not be applied to Ratnaparkhi?s dataset (RRR).
In our experiments we used two datasets:
? FN ? extracted from FrameNet II 1.1 (Baker et
al., 1998)
? TB2 ? extracted from Penn Treebank-II
Table 1 presents the datasets1. The creation of the
datasets is described in details in (Olteanu, 2004).
4 Features
The experiments described in this paper use a set
of discrete (alphanumeric) and continuous (numeric)
features. All features are fully deterministic, except
the features count-ratio and pp-count that are based
on information provided by an external resource
- Google search engine (http://www.google.
com).
In describing the features, we will use the Penn
Treebank-II parse tree associated with the sentence
?The Lorillard spokeswoman said asbestos was
used in ?very modest amounts? in making paper for
the filters in the early 1950s and replaced with a dif-
ferent type of filter in 1956?.
Table 2 describes the features and the origin of
each feature. The preposition is the feature with
the most discriminative power, because of prefer-
ences of particular prepositions to attach to verbs
or nouns. Table 3 shows the distribution of top
10 most frequently used prepositions in the FN and
TB2 datasets.
The features were carefully designed so that,
when they are extracted from gold parse trees, they
don?t provide more information useful for disam-
biguation than when they are automatically gener-
ated using a parser. This claim is validated by the
experimental results that show a strong correlation
between the results on the two datasets ? one based
on automatically generated parse trees (FN) and one
based on gold parse trees (TB2).
Next, we describe in further detail the features
presented in Table 2.
v-frame represents the frame of the verb ? the
frame to which the verb belongs, as it is present in
FrameNet (manually annotated). We used this fea-
ture because the frame of the verb describes very
well the semantic behavior of the verb including the
predicate-argument structure of the verb, which en-
tails the affinity of the verb for certain prepositions.
1The datasets are available at http://www.utdallas.
edu/?mgo031000/ppa/
274
FN TB2
Source FrameNet annotation samples (British National
Corpus)
Penn Treebank-II
(WSJ articles)
Instance identifica-
tion
Semantic-centered (related to Frame Elements) Syntactic-centered (related to the structure of the
parse tree)
Parse trees Automatically generated (Charniak) Gold standard
Total size 27,421 instances 60,699 instances
Distribution statistics 70.28% ambiguous verb attachments
2.36:1 v-attch:n-attch
35.71% ambiguous verb attachments
1:1.8 v-attch:n-attch
Training / test sets 90% - 10% ? homogenously distributed (one in every 10 instances is selected for the test set)
Location of PP Both before and after verb Only after verb
Other properties ? Partial identification of ambiguous PP-
attachment instances in the corpus, derived from
manual annotation of FEs (Olteanu, 2004)
? Semantic information readily available
Table 1: The datasets and their characteristics
Feature: description [origin]
v-surface: surface form of the verb [Hindle?93, ...]
n1-surface: surface form of n1. May be morphologically
processed [Hindle?93, ...]
p: the preposition, lower-cased [Hindle?93, ...]
n2-surface: surface form of n2. May be morphologically
processed [Ratnaparkhi?94, Collins?95, ...]
n1-mp/n1-mpf: morph. processing of n1 [Collins?95]
n2-mp/n2-mpf: morph. processing of n2 [Collins?95]
v-lemma: lemma of the verb [Collins?95]
path: path in the candidate parse tree between the verb and
np1 [Gildea?02]
subcategorization: subcategorization of the verb [modified
from Pradhan?03]
v-pos: part-of-speech of the verb
v-voice: voice of the verb
n1-pos: part-of-speech of n1
n1-lemma: lemma of n1. May be morphologically
processed
n2-pos: part-of-speech of n2
n2-lemma: lemma of n2. May be morphologically
processed
position: position of np1 relative to the verb [new]
v-frame: frame of the verb [new in PPA]
n1-sr: semantic role of np1 [new in PPA]
n1-tr: thematic role of np1 [new in PPA]
n1-preposition: preposition that heads np1, if np1 is a PP
[new]
n1-parent: label of the parent of np1 in the candidate parse
tree [new in PPA]
n1-np-label: label of np1 in the candidate parse tree [new in
PPA]
n2-det: determination of np2 [new]
parser-vote: choice of the automatic parser in attaching PP
[new in PPA]
count-ratio: WWW statistics about verb-attachment vs.
noun-attachment for that particular instance [new]
pp-count: WWW statistics about co-occurrence of v and n2
[new]
n1-p-distance: the distance between n1 and p [new]
Table 2: Features
% of % v-att % of % v-att
Prep. FN FN TB2 TB2
of 13.47% 6.17% 30.14% 2.74%
to 13.27% 80.14% 9.55% 60.49%
in 12.42% 73.64% 16.94% 42.58%
for 6.87% 82.44% 8.95% 39.72%
on 6.21% 75.51% 5.16% 47.73%
with 6.17% 86.30% 3.79% 46.92%
from 5.37% 75.90% 5.76% 52.76%
at 4.09% 76.63% 3.21% 66.02%
as 3.95% 86.51% 2.49% 51.69%
by 3.53% 88.02% 3.27% 68.11%
Table 3: Distribution of the first 10 most-frequent
prepositions in the FN and TB2 datasets
n1-sr represents the semantic role of the object
phrase np1 ? the label attached to the Frame Ele-
ment (manual semantic annotation that can be found
in FrameNet). This feature was introduced because
of the relation between the underlying meaning of
np1 and its semantic role.
n1-tr represents the thematic role of the object
phrase np1 ? a coarse-grained role based on the la-
bel attached to the Frame Element (manual semantic
annotation that can be found in FrameNet). It was
introduced to reduce data sparseness for the n1-sr
feature. The conversion from fine-grained semantic
role to coarse-grained semantic role is done auto-
matically using a table that maps a pair of a frame-
level semantic role (FE label) and a frame to a the-
matic role.
subcategorization contains a semi-lexicalized
description of the structure of the verb phrase. A
subcategorization frame is closely related to the
275
predicate argument structure and to the underlying
meaning of the verb. It contains an ordered set of all
the phrase labels that are siblings of the verb, plus a
marker for the verb. If the child phrase of the verb
is a PP, then the label will also contain the prepo-
sition (the headword of the PP). This feature is a
modified form of the sub-categorization feature de-
scribed in (Pradhan et al, 2003): the differences in
various part-of-speeches for the verb were ignored
and the preposition that heads a prepositional phrase
is also attached to the label. Therefore, for the sen-
tence ?The stock declined in June by 4%?, the value
for this feature is *-PPin-PPby.
In the TB2 dataset the parse trees are gold stan-
dard (contain the expected output value for PP-
ambiguity resolution). In the case of a verb attach-
ment, if the selected PP is a child of the selected VP,
then by applying the algorithm, the value of the fea-
ture will contain the PP label plus the preposition.
This clearly is a clue for the learner that the instance
is a verb attachment. To overcome this problem for
datasets based on gold-standard parse trees, when
computing the value of the subcategorization fea-
ture the selected PP will not be used. Figure 1 shows
the subcategorization for the phrase ?replaced with
a different type of filter in 1956?.
VP
replaced PP
with NP
NP
differenta type
PP
of NP
filter
PP
in NP
1956
Figure 1: Subcategorization feature: *-PPin-PPby
path expresses the syntactic relation between the
verb v and the object phrase np1. Its purpose is
to describe the syntactic relation of np1 to the rest
of the clause by the syntactic relation of np1 with
the head of the clause ? v. We adopted this feature
from (Gildea and Jurafsky, 2002). path describes
the chain of labels in the tree from v to np1, includ-
ing the label of v and np1. Ascending movements
and descending movements are depicted separately.
We used two variants of this feature to determine
the optimum version for our problem ? one with full
POS of the verb and one with POS reduced to ?VB?.
The experiments proved that the second variant pro-
vides a better performance. Figure 2 depicts the path
between ?replaced? and ?a different type of filter?:
VBN?VP?PP?NP or VB?VP?PP?NP.
VP
replaced PP
with NP
NP
differenta type
PP
of NP
filter
PP
in NP
1956
Figure 2: Example of a path feature
position indicates the position of the n1-p-n2 con-
struction relative to the verb, i.e. whether the prepo-
sitional phrase in question lies before the verb or af-
ter the verb in the sentence. Position is very impor-
tant in deciding the type of attachment, considering
the totally different distribution of PPs constructions
preceding the verb and PPs constructions following
the verb.
Morphological processing applied to n1 and n2
was inspired by the algorithm described in (Collins
and Brooks, 1995). We analyzed the impact of dif-
ferent levels of morphological processing by using
two types: partial morphological processing (only
numbers and years are converted) ? identified by
adding -mp as a suffix to the name of this feature ?
and full morphological processing (numbers, years
and capitalized names) ? identified by adding -mpf
as a suffix to the name of this feature. The purpose
of morphological processing is data sparseness re-
duction by clustering similar values for this feature.
n1-parent represents the phrase label of the par-
ent of np1 and it cannot be used on gold parse trees
(TB2 dataset) because it will provide a clue about
the correct attachment type.
276
n2-det is called the determination of the preposi-
tional phrase np2. This novel feature tells if n2 is
preceded in np2 by a possessive pronoun or by a de-
terminer. This is used to differentiate between ?buy
books for children? (which is probably a noun at-
tachment) and ?buy books for her children? (which
very probably is a verb attachment).
parser-vote feature represents the choice of the
parser (Charniak?s parser) in the PP-attachment res-
olution. It cannot be used with gold-standard parse
trees because it will provide the right answer.
count-ratio represents the estimated ratio be-
tween the frequency of an unambiguous verb attach-
ment construction based on v, p and n2 and the fre-
quency of a probably unambiguous noun attachment
construction based on n1, p and n2 in a very large
corpus. A very large corpus is required to overcome
the data sparseness inherent for complex construc-
tions like those described above.
We chose the World Wide Web as a corpus and
Google as a query interface (see (Olteanu, 2004) for
details).
Let?s consider the estimated frequency of un-
ambiguous verb-attachments and respectively noun-
attachments defined as:
fv = cv?p?n2cv ? cp?n2
fn = cn1?p?n2cn1 ? cp?n2
where:
? cv?p?n2 is the number of occurrences of the
phrase ?v p n2?, ?v p?n2? (where * symbolizes
any word), ?v-lemma p n2? or ?v-lemma p * n2?
in World Wide Web, as reported by Google
? cv is the number of occurrences of the word ?v?
or ?v-lemma? in WWW
? cp?n2 is the number of occurrences of the
phrase ?p n2? or ?p ? n2? in WWW
? cn1?p?n2 is the number of occurrences of the
phrase ?n1 p n2? or ?v p ? n2? in WWW
? cn1 is the number of occurrences of the word
?n1? in WWW
The value for this feature is:
count? ratio = log10
fv
fn = log10
cv?p?n2 ? cn1
cn1?p?n2 ? cv
We chose logarithmic values for this feature be-
cause experiments showed that logarithmic values
provide a higher accuracy than linear values. Also,
by experimentation we concluded that value bound-
ing is helpful, and the feature was bounded to values
between -3 and 3 on the logarithmic scale, unless
specified otherwise in the experiment description.
This feature resembles the approach adopted in
(Volk, 2001).
pp-count depicts the estimated count of occur-
rences in World Wide Web of the prepositional
phrases based on p and n2. The count is estimated
by cp?n2. Therefore pp-count = log10(cp?n2 +
cp???n2).
n1-p-distance depicts the distance (in tokens) be-
tween n1 and p. Let dn1?p be the distance be-
tween n1 and p (d = 1 if there is no other to-
ken between n1 and p). Thus n1-p-distance =
log10(1 + log10 dn1?p).
5 Learning model and procedure
We used in our experiments a Support Vector
Machines learner with Radial Basis Function
kernel as implemented in the LIBSVM toolkit
(http://www.csie.ntu.edu.tw/?cjlin/
libsvm/).
We converted the feature tuples (containing dis-
crete alphanumeric and continuous values) to multi-
dimensional vectors using the following procedure:
? Discrete features: assign to each possible value
of each feature a dimension in the vector space,
and to each feature value in each training or test
example put 1 in the dimension corresponding
to the feature value and 0 in all other dimen-
sions associated with that feature.
? Continuous features: assign a dimension and
put the scaled value in the multi-dimensional
vector (all examples in training data will span
between 0 and 1 for that particular dimension).
SVM training was preceded by finding the opti-
mal ? and C parameters required for training using
2-fold cross validation, which was found to be supe-
rior in model accuracy and training time over higher
folds cross-validations (Olteanu, 2004).
The criterion for selecting the best set of features
was the accuracy on the cross-validation. Thus, the
development of the models was performed entirely
277
on the training set, which acted also as a develop-
ment set. We later computed the accuracy on the
test set on some representative models.
6 Experiments, results and analysis
For each dataset, we conducted experiments to de-
termine an efficient combination of features and the
accuracy on test data for the best combination of fea-
tures. We also run the experimental procedure on
the original Ratnaparkhi?s dataset in order to com-
pare SVM with other machine learning techniques
applied to PP-attachment problem. Table 4 summa-
rizes the experiments performed on all datasets.
% on dev % on test
Experiment / x-val
FN-basic-flw 86.25 86.44
FN-lex-syn-flw 88.55 89.61
FN-best-no-sem 90.93 91.79
FN-best-sem 91.87 92.85
TB2-basic 85.75 87.47
TB2-best-no-www 92.06 92.81
TB2-best 92.92 93.62
RRR-basic 84.32 84.60
RRR-basic-mpf 84.34 85.14
Table 4: Results
FN-basic-flw uses v-surface, n1-surface, p and
n2-surface on examples that follow the verb. FN-
lex-syn-flw uses v-surface, v-pos, v-lemma, sub-
categorization, path (full POS), position, n1-
preposition, n1-surface, n1-pos, n1-lemma, n1-
parent, p, n2-surface, n2-pos, n2-lemma, n2-
det and parser-vote on examples that follow the
verb. FN-best-no-sem uses v-surface, v-pos, v-
lemma, subcategorization, path (reduced POS),
position, n1-preposition, n1-surface, n1-pos, n1-
lemma-mpf, n1-parent, p, n2-surface, n2-pos,
n2-lemma-mpf, n2-det, parser-vote, count-ratio
and pp-count on all examples. FN-best-sem uses
the same set of features as FN-best-no-sem plus v-
frame and n1-sr.
TB2-basic uses v-surface, n1-surface-mpf, p
and n2-surface-mpf. TB2-best-no-www uses v-
surface, v-pos, v-lemma, subcategorization, path
(reduced POS), n1-preposition, n1-surface, n1-
mpf, n1-pos, n1-lemma, n1-np-label, p, n2-
surface, n2-mpf and n1-p-distance. TB2-best also
uses count-ratio and pp-count.
RRR-basic uses v-surface, n1-surface, p and
n2-surface. RRR-basic-mpf uses v-surface, n1-
surface-mpf, p and n2-surface-mpf.
On the FN dataset, all features except v-voice
have a positive contribution to the system (n2-det,
choice between semantic vs. thematic role and how
should morphological processing be applied is ques-
tionable). The negative impact for the v-voice fea-
ture may be explained by the fact that the only sit-
uation in which it may potentially help is extremely
rare: passive voice and the agent headed by ?by? ap-
pears after another argument of the verb (i.e.: ?The
painting was presented to the audience by its au-
thor.?). Moreover the PP-attachment based on the
preposition ?by? is not highly ambiguous; as seen
in Table 3 in the FrameNet dataset, 88% of the ?by?
ambiguity instances are verb-attachments.
The experiment with the highest cross-validation
accuracy has an accuracy of 92.85% on the test data.
The equivalent experiment that doesn?t include man-
ually annotated semantic information has an accu-
racy of 91.79% on the test data.
On TB2 dataset, the results are close to the results
obtained on the FrameNet corpus, although the dis-
tribution of noun and verb attachment differs consid-
erably between the two data sets (70.28% are verb-
attachments in FN2 and 35.71% in TB2). The best
accuracy in cross-validation is 92.92%, which leads
to an accuracy on test set of 93.62%.
7 Comparison with previous work
Because we couldn?t use the standard dataset used
in PP-attachment resolution (Ratnaparkhi?s), we im-
plemented back-off algorithm developed by Collins
and Brooks (1995) and applied it to our TB2 dataset.
Both RRR and TB2 datasets are extracted from Penn
Treebank. This algorithm, trained on TB2 training
set, obtains an accuracy on TB2 test set of 86.1%
(85.8% when no morphological processing is ap-
plied). The same algorithm provides an accuracy on
RRR dataset of 84.5% (84.1% without morphologi-
cal processing). The difference in accuracy between
the two datasets is 1.6% (1.7% without morpholog-
ical processing when using Collins and Brooks?s al-
gorithm.
The difference in accuracy between a SVM model
applied to RRR dataset (RRR-basic experiment) and
the same experiment applied to TB2 dataset (TB2-
278
Description Accuracy Data Extra Supervision
Always noun 55.0 RRR
Most likely for each P 72.19 RRR
Most likely for each P 72.30 TB2
Most likely for each P 81.73 FN
Average human, headwords (Ratnaparkhi et al, 1994) 88.2 RRR
Average human, whole sentence (Ratnaparkhi et al, 1994) 93.2 RRR
Maximum Likelihood-based (Hindle and Rooth, 1993) 79.7 AP
Maximum entropy, words (Ratnaparkhi et al, 1994) 77.7 RRR
Maximum entropy, words & classes (Ratnaparkhi et al, 1994) 81.6 RRR
Decision trees (Ratnaparkhi et al, 1994) 77.7 RRR
Transformation-Based Learning (Brill and Resnik, 1994) 81.8 WordNet
Maximum-Likelihood based (Collins and Brooks, 1995) 84.5 RRR
Maximum-Likelihood based (Collins and Brooks, 1995) 86.1 TB2
Decision trees & WSD (Stetina and Nagao, 1997) 88.1 RRR WordNet
Memory-based Learning (Zavrel et al, 1997) 84.4 RRR LexSpace
Maximum entropy, unsupervised (Ratnaparkhi, 1998) 81.9
Maximum entropy, supervised (Ratnaparkhi, 1998) 83.7 RRR
Neural Nets (Alegre et al, 1999) 86.0 RRR WordNet
Boosting (Abney et al, 1999) 84.4 RRR
Semi-probabilistic (Pantel and Lin, 2000) 84.31 RRR
Maximum entropy, ensemble (McLauchlan, 2001) 85.5 RRR LSA
SVM (Vanschoenwinkel and Manderick, 2003) 84.8 RRR
Nearest-neighbor (Zhao and Lin, 2004) 86.5 RRR DWS
FN dataset, w/o semantic features (FN-best-no-sem) 91.79 FN PR-WWW
FN dataset, w/ semantic features (FN-best-sem) 92.85 FN PR-WWW
TB2 dataset, best feature set (TB2-best) 93.62 TB2 PR-WWW
Table 5: Accuracy of PP-attachment ambiguity resolution (our results in bold)
basic experiment) is 2.9%. Also, the baseline ? the
most probable PP type for each preposition ? is ap-
proximately the same for the two datasets (72.19%
on RRR and 72.30% on TB2).
One may hypothesize that the majority of the al-
gorithms for PP-attachment disambiguation obtain
no more than 4% increase in accuracy on the TB2
compared to the results on the RRR dataset. One
important difference between the two datasets is the
size ? 20,801 training examples in RRR vs. 54,629
training examples in TB2. We plan to implement
more algorithms described in literature in order to
verify this statement.
Table 5 summarizes the results in PP-attachment
ambiguity resolution found in literature along with
our best results.
Other acronyms used in this table:
? AP ? dataset of 13 million word sample of As-
sociated Press news stories from 1999 (Hindle
and Rooth, 1993).
? LexSpace - Lexical Space ? a method to mea-
sure the similarity of the words (Zavrel et al,
1997).
? LSA ? Latent Semantic Analysis ? measure the
lexical preferences between a preposition and a
noun or a verb (McLauchlan, 2001)
? DWS ? Distributional Word Similarity. Words
that tend to appear in the same contexts tend to
have similar meanings (Zhao and Lin, 2004)
? PR-WWW ? the probability ratio between
verb-preposition-noun and noun-preposition-
noun constructs measured using World Wide
Web searching.
8 Conclusions
The Penn Treebank-II results indicate that the
new features used for the disambiguation of PP-
attachment provide a very substantial improvement
in accuracy over the base line (from 87.48% to
93.62%). This represents an absolute improvement
of approximately 6.14%, equivalent to a 49% er-
ror drop. The performance of the system on Penn
Treebank-II exceeds the reported human expert per-
formance on Penn Treebank-I (Ratnaparkhi et al,
1994) by about 0.4%. A significant improvement
comes from the unsupervised information collected
279
from a very large corpus; this method proved to be
efficient to overcome the data sparseness problem.
By analyzing the results from the FrameNet
dataset, we conclude that the contribution of the gold
semantic features (frame and semantic role) is sig-
nificant (1.05% difference in accuracy; 12.8% re-
duction in the error). We will further investigate this
issue by replacing gold semantic information with
automatically detected semantic information. Our
additional lexico-syntactic features increase the ac-
curacy of the system from 86.44% to 89.61% for
PPs following the verb. This suggests that on the
FrameNet dataset the proposed syntactic features
have a considerable impact on the accuracy.
The best TB2 feature set is approximately the
same as the best FN feature set in spite of the dif-
ferences between the datasets (Parse trees: TB2 ?
gold standard; FN ? automatically generated. PP-
attachment ambiguity identification: TB2 ? parse
trees; FN ? a combination of trees and FE annota-
tion. Data source: TB2 ? WSJ articles; FN ? BNC).
This fact suggests that the selected feature sets do
not exploit particularities of the datasets and that the
features are relevant to the PP-attachment ambiguity
problem.
References
Steven Abney, Robert E. Schapire, and Yoram Singer. 1999.
Boosting applied to tagging and PP Attachment. In Proceed-
ings of EMNLP/VLC-99, pages 38?45.
Martha A. Alegre, Josep M. Sopena, and Agusti Lloberas.
1999. Pp-attachment: A committee machine approach. In
Proceedings of EMNLP/VLC-99, pages 231?238.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998.
The Berkeley FrameNet Project. In Proceedings of the
17th international conference on Computational Linguistics,
pages 86?90.
Eric Brill and Philip Resnik. 1994. A rule-based approach
to prepositional phrase attachment disambiguation. In Pro-
ceedings of the 15th conference on Computational Linguis-
tics, pages 1198?1204.
Eugene Charniak. 2000. A Maximum-Entropy-Inspired Parser.
In Proceedings of NAACL-2000, pages 132?139.
Michael Collins and James Brooks. 1995. Prepositional Phrase
Attachment through a Backed-Off Model. In Proceedings of
the Thirds Workshop on Very Large Corpora, pages 27?38.
Corinna Cortes and Vladimir Vapnik. 1995. Support-Vector
Networks. Machine Learning, 20(3):273?297.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic Labeling
of Semantic Roles. Computational Linguistics, 28(3):245?
288.
Donald Hindle and Mats Rooth. 1993. Structural Ambi-
guity and Lexical Relations. Computational Linguistics,
19(1):103?120.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Mark McLauchlan. 2001. Maximum Entropy Models and
Prepositional Phrase Ambiguity. Master?s thesis, University
of Edinburgh.
Marian G. Olteanu. 2004. Prepositional Phrase Attachment
ambiguity resolution through a rich syntactic, lexical and
semantic set of features applied in support vector machines
learner. Master?s thesis, University of Texas at Dallas.
Patrick Pantel and Dekang Lin. 2000. An unsupervised ap-
proach to Prepositional Phrase Attachment using contextu-
ally similar words. In Proceedings of the 38th Meeting of the
Association for Computational Linguistic, pages 101?108.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H. Mar-
tin, and Daniel Jurafsky. 2003. Semantic Role Parsing:
Adding Semantic Structure to Unstructured Text. In Pro-
ceedings of the International Conference on Data Mining,
pages 629?632.
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos. 1994. A
Maximum Entropy Model for Prepositional Phrase Attach-
ment. In Proceedings of the Human Language Technology
Workshop, pages 250?255.
Adwait Ratnaparkhi. 1998. Statistical Models for Unsuper-
vised Prepositional Phrase Attachment. In Proceedings of
the 36th conference on Association for Computational Lin-
guistics, pages 1079?1085.
Jiri Stetina and Makoto Nagao. 1997. Corpus based PP attach-
ment ambiguity resolution with a semantic dictionary. In
Proceedings of the Fifth Workshop on Very Large Corpora,
pages 66?80.
Bram Vanschoenwinkel and Bernard Manderick. 2003. A
weighted polynomial information gain kernel for resolving
Prepositional Phrase attachment ambiguities with Support
Vector Machines. In Proceedings of the Eighteenth Inter-
national Joint Conference on Artificial Intelligence, pages
133?140.
Martin Volk. 2001. Exploiting the WWW as a corpus to re-
solve PP attachment ambiguities. In Proceedings of Corpus
Linguistics, pages 601?606.
Jakub Zavrel, Walter Daelemans, and Jorn Veenstra. 1997.
Resolving PP attachment Ambiguities with Memory-Based
Learning. In Proceedings of CoNLL-97, pages 136?144.
Shaojun Zhao and Dekang Lin. 2004. A Nearest-Neighbor
Method for Resolving PP-Attachment Ambiguity. In Pro-
ceedings of IJCNLP-04.
280
  	

Support Vector Machines Applied to the Classification of Semantic Relations
in Nominalized Noun Phrases
Roxana Girju
Computer Science Department
Baylor University
Waco, Texas
girju@cs.baylor.edu
Ana-Maria Giuglea, Marian Olteanu,
Ovidiu Fortu, Orest Bolohan, and
Dan Moldovan
Department of Computing Science
University of Texas at Dallas
Dallas, Texas
moldovan@utdallas.edu
Abstract
The discovery of semantic relations in text
plays an important role in many NLP appli-
cations. This paper presents a method for the
automatic classification of semantic relations
in nominalized noun phrases. Nominalizations
represent a subclass of NP constructions in
which either the head or the modifier noun is
derived from a verb while the other noun is an
argument of this verb. Especially designed fea-
tures are extracted automatically and used in a
Support Vector Machine learning model. The
paper presents preliminary results for the se-
mantic classification of the most representative
NP patterns using four distinct learning mod-
els.
1 Introduction
1.1 Problem description
The automatic identification of semantic relations in text
has become increasingly important in Information Ex-
traction, Question Answering, Summarization, Text Un-
derstanding, and other NLP applications. This paper dis-
cusses the automatic labeling of semantic relations in
nominalized noun phrases (NPs) using a support vector
machines learning algorithm.
Based on the classification provided by the New Web-
ster?s Grammar Guide (Semmelmeyer and Bolander
1992) and our observations of noun phrase patterns on
large text collections, the most frequently occurring NP
level constructions are: (1) Compound Nominals consist-
ing of two consecutive nouns (eg pump drainage - an IN-
STRUMENT relation), (2) Adjective Noun constructions
where the adjectival modifier is derived from a noun (eg
parental refusal - AGENT), (3) Genitives (eg tone of con-
versation - a PROPERTY relation), (4) Adjective phrases
in which the modifier noun is expressed by a preposi-
tional phrase which functions as an adjective (eg amuse-
ment in the park - a LOCATION relation), and (5) Adjec-
tive clauses where the head noun is modified by a relative
clause (eg the man who was driving the car - an AGENT
relation between man and driving).
1.2 Previous work on the discovery of semantic
relations
The development of large semantically annotated cor-
pora, such as Penn Treebank2 and, more recently, Prop-
Bank (Kingsbury, et al 2002), as well as semantic
knowledge bases, such as FrameNet (Baker, Fillmore,
and Lowe 1998), have stimulated a high interest in the
automatic acquisition of semantic relations, and espe-
cially of semantic roles. In the last few years, many re-
searchers (Blaheta and Charniak 2000), (Gildea and Ju-
rafsky 2002), (Gildea and Palmer 2002), (Pradhan et
al. 2003) have focused on the automatic prediction of se-
mantic roles using statistical techniques. These statistical
techniques operate on the output of probabilistic parsers
and take advantage of the characteristic features of the
semantic roles that are then employed in a learning algo-
rithm.
While these systems focus on verb-argument semantic
relations, called semantic roles, in this paper we inves-
tigate predicate-argument semantic relations in nominal-
ized noun phrases and present a method for their auto-
matic detection in open-text.
1.3 Approach
We approach the problem top-down, namely identify and
study first the characteristics or feature vectors of each
noun phrase linguistic pattern and then develop models
for their semantic classification. The distribution of the
semantic relations is studied across different NP patterns
and the similarities and differences among resulting se-
mantic spaces are analyzed. A thorough understanding
of the syntactic and semantic characteristics of NPs pro-
vides valuable insights into defining the most representa-
tive feature vectors that ultimately drive the discriminat-
ing learning models.
An important characteristic of this work is that it re-
lies heavily on state-of-the-art natural language process-
ing and machine learning methods. Prior to the discovery
of semantic relations, the text is syntactically parsed with
Charniak?s parser (Charniak 2001) and words are seman-
tically disambiguated and mapped into their appropriate
WordNet senses. The word sense disambiguation is done
manually for training and automatically for testing with
a state-of-the-art WSD module, an improved version of
a system with which we have participated successfully
in Senseval 2 and which has an accuracy of 81% when
disambiguating nouns in open-domain. The discovery of
semantic relations is based on learning lexical, syntactic,
semantic and contextual constraints that effectively iden-
tify the most probable relation for each NP construction
considered.
2 Semantic Relations in Nominalized Noun
Phrases
In this paper we study the behavior of semantic relations
at the noun phrase level when one of the nouns is nom-
inalized. The following NP level constructions are con-
sidered: complex nominals, genitives, adjective phrases,
and adjective clauses.
Complex Nominals
Levi (Levi 1979) defines complex nominals (CNs) as ex-
pressions that have a head noun preceded by one or more
modifying nouns, or by adjectives derived from nouns
(usually called denominal adjectives). Each sequence of
nouns, or possibly adjectives and nouns, has a particular
meaning as a whole carrying an implicit semantic rela-
tion; for example, ?parental refusal? (AGENT).
The main tasks are the recognition, and the interpre-
tation of complex nominals. The recognition task deals
with the identification of CN constructions in text, while
the interpretation of CNs focuses on the detection and
classification of a comprehensive set of semantic rela-
tions between the noun constituents.
Genitives
In English there are two kinds of genitives; in one, the
modifier is morphologically linked to the possessive clitic
?s and precedes the head noun (s-genitive e.g. ?John?s
conclusion?), and in the second one the modifier is syn-
tactically marked by the preposition of and follows the
head noun (of-genitive, e.g. ?declaration of indepen-
dence?).
Adjective Phrases are prepositional phrases attached to
nouns and act as adjectives (cf. (Semmelmeyer and
Bolander 1992)). Prepositions play an important role
both syntactically and semantically ( (Dorr 1997). Prepo-
sitional constructions can encode various semantic re-
lations, their interpretations being provided most of the
time by the underlying context. For instance, the preposi-
tion ?with? can encode different semantic relations: (1) It
was the girl with blue eyes (MERONYMY), (2) The baby
with the red ribbon is cute (POSSESSION), (3) The woman
with triplets received a lot of attention (KINSHIP).
The conclusion for us is that in addition to the nouns se-
mantic classes, the preposition and the context play im-
portant roles here.
Adjective Clauses are subordinate clauses attached to
nouns (cf. (Semmelmeyer and Bolander 1992)). Often
they are introduced by a relative pronoun/adverb (ie that,
which, who, whom, whose, where) as in the following ex-
amples: (1) Here is the book which I am reading (book
is the THEME of reading) (2) The man who was driving
the car was a spy (man is the AGENT of driving). Adjec-
tive clauses are inherently verb-argument structures, thus
their interpretation consists of detecting the semantic role
between the head noun and the main verb in the relative
clause. This is addressed below.
3 Nominalizations and Mapping of NPs
into Grammatical Role Structures
3.1 Nominalizations
A further analysis of various examples of noun - noun
pairs encoded by the first three major types of NP-level
constructions shows the need for a different taxonomy
based on the syntactic and grammatical roles the con-
stituents have in relation to each other. The criterion in
this classification splits the noun - noun examples (re-
spectively, adjective - noun examples in complex nom-
inals) into nominalizations and non-nominalizations.
Nominalizations represent a particular subclass of NP
constructions that in general have ?a systematic corre-
spondence with a clause structure? (Quirk et al1985).
The head or modifier noun is derived from a verb while
the other noun (the modifier, or respectively, the head) is
interpreted as an argument of this verb. For example, the
noun phrase ?car owner? corresponds to ?he owns a car?.
The head noun owner is morphologically related to the
verb own. Otherwise said, the interpretation of this class
of NPs is reduced to the automatic detection and inter-
pretation of semantic roles mapped on the corresponding
verb-argument structure.
As in (Hull and Gomez 1996), in this paper we use
the term nominalization to refer only to those senses of
the nominalized nouns which are derived from verbs.
For example, the noun ?decoration? has three senses in
WordNet 2.0: an ornament (#1), a medal (#2), and the act
of decorating (#3). Only the last sense is a nominaliza-
tion. However, there are more complex situations when
the underlying verb has more than one sense that refers to
an action/event. This is the case of ?examination? which
has five senses of which four are action-related. In this
case, the selection of the correct sense is provided by the
context.
We are interested in answering the following ques-
tions: (1) What is the best set of features that can capture
the meaning of noun - noun nominalization pairs for each
NP-level construction? and (2) What is the semantic be-
havior of nominalization constructions across NP levels?
3.2 Taxonomy of nominalizations
Deverbal vs verbal noun.
(Quirk et al1985) generally classify nominalizations
based on the morphological formation of the nominal-
ized noun. They distinguish between deverbal nouns, i.e.
those derived from the underlying verb through word for-
mation; e.g., ?student examination?, and verbal nouns,
i.e. those derived from the verb by adding the gerund
suffix ?-ing?; e.g.: ?cleaning woman?. Most of the time,
verbal nouns are derived from verbs which don?t have a
deverbal correspondent.
Table 1 shows the mapping of the first three major syn-
tactic NP constructions to the grammatical role level. By
analyzing a large corpus, we have observed that Quirk?s
grammatical roles shown in Table 1 are not uniformly dis-
tributed over the types of NP-constructions. For example,
the ?
 	 
Proceedings of the Workshop on Statistical Machine Translation, pages 146?149,
New York City, June 2006. c?2006 Association for Computational Linguistics
Phramer - An Open Source Statistical Phrase-Based Translator
Marian Olteanu, Chris Davis, Ionut Volosen and Dan Moldovan
Human Language Technology Research Institute
The University of Texas at Dallas
Richardson, TX 75080
{marian,phoo,volosen,moldovan}@hlt.utdallas.edu
Abstract
This paper describes the open-source
Phrase-Based Statistical Machine Transla-
tion Decoder - Phramer. The paper also
presents the UTD (HLTRI) system build
for the WMT06 shared task. Our goal was
to improve the translation quality by en-
hancing the translation table and by pre-
processing the source language text
1 Introduction
Despite the fact that the research in Statistical
Machine Translation (SMT) is very active, there
isn?t an abundance of open-source tools available
to the community. In this paper, we present
Phramer, an open-source system that embeds a
phrase-based decoder, a minimum error rate train-
ing (Och, 2003) module and various tools related
to Machine Translation (MT). The software is re-
leased under BSD license and it is available at
http://www.phramer.org/.
We also describe our Phramer-based system
that we build for the WMT06 shared task.
2 Phramer
Phramer is a phrase-based SMT system written in
Java. It includes:
? A decoder that is compatible with Pharaoh
(Koehn, 2004),
? A minimum error rate training (MERT) mod-
ule, compatible with Phramer?s decoder, with
Pharaoh and easily adaptable to other SMT
or non-SMT tasks and
? various tools.
The decoder is fully compatible with Pharaoh
1.2 in the algorithms that are implemented, input
files (configuration file, translation table, language
models) and command line. Some of the advantages
of Phramer over Pharaoh are: (1) source code
availability and its permissive license; (2) it is very
fast (1.5?3 times faster for most of the configura-
tions); (3) it can work with various storage layers for
the translation table (TT) and the language models
(LMs): memory, remote (access through TCP/IP),
disk (using SQLite databases1). Extensions for other
storage layers can be very easily implemented; (4) it
is more configurable; (5) it accepts compressed data
files (TTs and LMs); (6) it is very easy to extend; an
example is provided in the package ? part-of-speech
decoding on either source language, target language
or both; support for POS-based language models;
(7) it can internally generate n-best lists. Thus no
external tools are required.
The MERT module is a highly modular, efficient
and customizable implementation of the algorithm
described in (Och, 2003). The release has imple-
mentations for BLEU (Papineni et al, 2002), WER
and PER error criteria and it has decoding interfaces
for Phramer and Pharaoh. It can be used to
search parameters over more than one million vari-
ables. It offers features as resume search, reuse hy-
potheses from previous runs and various strategies
to search for optimal ? weight vectors.
1http://www.sqlite.org/
146
The package contains a set of tools that include:
? Distributed decoding (compatible with both
Phramer and Pharaoh) ? it automatically
splits decoding jobs and distributes them to
workers and assembles the results. It is compat-
ible with lattice generation, therefore it can also
be used during weights search (using MERT).
? Tools to process translation tables ? filter the
TT based on the input file, flip TT to reuse it
for English-to-Foreign translation, filter the TT
by phrase length, convert the TT to a database.
3 WMT06 Shared Task
We have assembled a system for participation in the
WMT 2006 shared task based on Phramer and
other tools. We participated in 5 subtasks: DE?EN,
FR?EN, ES?EN, EN?FR and EN?ES.
3.1 Baseline system
3.1.1 Translation table generation
To generate a translation table for each pair of lan-
guages starting from a sentence-aligned parallel cor-
pus, we used a modified version of the Pharaoh
training software 2. The software also required
GIZA++ word alignment tool(Och and Ney, 2003).
We generated for each phrase pair in the trans-
lation table 5 features: phrase translation probabil-
ity (both directions), lexical weighting (Koehn et al,
2003) (both directions) and phrase penalty (constant
value).
3.1.2 Decoder
The Phramer decoder was used to translate the
devtest2006 and test2006 files. We accelerated the
decoding process by using the distributed decoding
tool.
3.1.3 Minimum Error Rate Training
We determined the weights to combine the mod-
els using the MERT component in Phramer. Be-
cause of the time constrains for the shared task sub-
mission3, we used Pharaoh + Carmel4 as the de-
2http://www.iccs.inf.ed.ac.uk/?pkoehn/training.tgz
3After the shared task submission, we optimized a lot our
decoder. Before the optimizations (LM optimizations, fixing
bugs that affected performance), Phramer was 5 to 15 times
slower than Pharaoh.
4http://www.isi.edu/licensed-sw/carmel/
coder for the MERT algorithm.
3.1.4 Preprocessing
We removed from the source text the words that
don?t appear either in the source side of the train-
ing corpus (thus we know that the translation table
will not be able to translate them) or in the lan-
guage model for the target language (and we esti-
mate that there is a low chance that the untranslated
word might actually be part of the reference transla-
tion). The purpose of this procedure is to minimize
the risk of inserting words into the automatic trans-
lation that are not in the reference translation.
We applied this preprocessing step only when the
target language was English.
3.2 Enhancements to the baseline systems
Our goal was to improve the translation quality by
enhancing the the translation table.
The following enhancements were implemented:
? reduce the vocabulary size perceived by the
GIZA++ and preset algnment for certain
words
? ?normalize? distortion between pairs of lan-
guages by reordering noun-adjective construc-
tions
The first enhancement identifies pairs of tokens in
the parallel sentences that, with a very high proba-
bility, align together and they don?t align with other
tokens in the sentence. These tokens are replaced
with a special identifier, chosen so that GIZA++ will
learn the alignment between them easier than before
replacement. The targeted token types are proper
nouns (detected when the same upper-cased token
were present in both the foreign sentence and the
English sentence) and numbers, also taking into ac-
count the differences between number representa-
tion in different languages (i.e.: 399.99 vs. 399,99).
Each distinct proper noun to be replaced in the sen-
tence was replaced with a specific identifier, distinct
from other replacement identifiers already used in
the sentence. The same procedure was applied also
for numbers. The specific identifiers were reused in
other sentences. This has the effect of reducing the
vocabulary, thus it provides a large number of in-
stances for the special token forms. The change in
147
yo
I
was
the
rapporteur
on
romania
for
the
parliamentary
assembly
of
the
council
of
fui
ponente
de
la asamblea
parlamentaria
del
consejo
de europa
para
ruman?a
.
.
europe
yo
I
was
the
rapporteur
on
romania
for
the
parliamentary
assembly
of
the
council
of
fui
ponente
de
la asamblea
parlamentaria
del
consejo
de europa
para
ruman?a
.
.
europe
before reordering after reordering
Figure 1: NN-ADJ reordering
Corpus Before After
DE 195,290 184,754
FR 80,348 70,623
ES 102,885 92,827
Table 1: Vocabulary size change due to forced align-
ment
the vocabulary size is shown in Table 1. To simplify
the process, we limited the replacement of tokens
to one-to-one (one real token to one special token),
so that the word alignment file can be directly used
together with the original parallel corpus to extract
phrases required for the generation of the translation
table. Table 2 shows an example of the output.
The second enhancement tries to improve the
quality of the translation by rearranging the words in
the source sentence to better match the correct word
order in the target language (Collins et al, 2005).
We focused on a very specific pattern ? based on the
part-of-speech tags, changing the order of NN-ADJ
phrases in the non-English sentences. This process
was also applied to the input dev/test files, when the
target language was English. Figure 1 shows the re-
ordering process and its effect on the alignment.
The expected benefits are:
? Better word alignment due to an alignment
closer to the expected alignment (monotone).
? More phrases extracted from the word aligned
corpus. Monotone alignment tends to generate
more phrases than a random alignment.
? Higher mixture weight for the monotone dis-
tortion model because of fewer reordering con-
straints during MERT, thus the value of the
monotone distortion model increases, ?tighten-
ing? the translation.
3.3 Experimental Setup
We implemented the first enhancement on ES?EN
subtask by part-of-speech tagging the Spanish text
using TreeTagger5 followed by a NN-ADJ inver-
sion heuristic.
The language models provided for the task was
used.
We used the 1,000 out of the 2,000 sentences
in each of the dev2006 datasets to determine
weights for the 8 models used during decoding (one
monotone distortion mode, one language model,
five translation models, one sentence length model)
through MERT. The weights were determined in-
dividually for each pair of source-target languages.
5http://www.ims.uni-stuttgart.de/projekte/corplex/
TreeTagger/DecisionTreeTagger.html
148
There are 145 settlements in the West Bank , 16 in Gaza , 9 in East Jerusalem ; 400,000 people live in them .
Existen 145 asentamientos en Cisjordania , 16 en Gaza y 9 en Jerusaln Este ; en ellos viven 400.000 personas .
There are [x1] settlements in the West Bank , [x2] in [y1] , [x3] in East Jerusalem ; [x4] people live in them .
Existen [x1] asentamientos en Cisjordania , [x2] en [y1] y [x3] en Jerusaln Este ; en ellos viven [x4] personas .
Table 2: Forced alignment example
OOV forced NN-ADJ BLEU
Subtask filtering alignment inversion score
DE?EN
?
? ? 25.45? ?
? 25.53
FR?EN
?
? ? 30.70? ?
? 30.70
ES?EN
?
? ? 30.77? ?
? 30.84? ? ?
30.92
EN?FR ? ? ? 31.67
?
?
? 31.79
EN?ES ? ? ? 30.17
?
?
? 30.11
Table 3: Results on the devtest2006 files
Subtask BLEU 1/2/3/4-gram precision (bp)
DE?EN 22.96 58.8/28.8/16.5/9.9 (1.000)
FR?EN 27.78 61.8/33.6/21.0/13.7 (1.000)
ES?EN 29.93 63.5/36.0/23.0/15.2 (1.000)
EN?FR 28.87 60.0/34.7/22.7/15.2 (0.991)
EN?ES 29.00 62.9/35.8/23.0/15.1 (0.975)
Table 4: Results on the test2006 files
Using these weights, we measured the BLEU score
on the devtest2006 datasets. Based on the model
chosen, we decoded the test2006 datasets using the
same weights as for devtest2006.
3.4 Results
Table 3 presents the results on the devtest2006 files
using different settings. Bold values represent the
result for the settings that were also chosen for the
final test. Table 4 shows the results on the submitted
files (test2006).
3.5 Conclusions
The enhancements that we proposed provide small
improvements on the devtest2006 files. As expected,
when we used the NN-ADJ inversion the ratio ?D?LM
increased from 0.545 to 0.675. The LM is the only
model that opposes the tendency of the distortion
model towards monotone phrase order.
Phramer delivers a very good baseline system.
Using only the baseline system, we obtain +0.68 on
DE?EN, +0.43 on FR?EN and -0.18 on ES?EN
difference in BLEU score compared to WPT05?s
best system (Koehn and Monz, 2005). This fact is
caused by the MERT module. This module is capa-
ble of estimating parameters over a large develop-
ment corpus in a reasonable time, thus it is able to
generate highly relevant parameters.
References
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL?05), pages 531?540, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Philipp Koehn and Christof Monz. 2005. Shared task:
Statistical machine translation between European lan-
guages. In Proceedings of the ACL Workshop on
Building and Using Parallel Texts, pages 119?124,
Ann Arbor, Michigan, June. Association for Compu-
tational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL 2003, Edmonton, Canada.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Erhard Hinrichs
and Dan Roth, editors, Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 311?318.
149
Proceedings of the Workshop on Statistical Machine Translation, pages 150?153,
New York City, June 2006. c?2006 Association for Computational Linguistics
Language Models and Reranking for Machine Translation
Marian Olteanu, Pasin Suriyentrakorn and Dan Moldovan
Language Computer Corp.
Richardson, TX 75080
{marian,psuri,moldovan}@languagecomputer.com
Abstract
Complex Language Models cannot be eas-
ily integrated in the first pass decoding of
a Statistical Machine Translation system ?
the decoder queries the LM a very large
number of times; the search process in the
decoding builds the hypotheses incremen-
tally and cannot make use of LMs that
analyze the whole sentence. We present
in this paper the Language Computer?s
system for WMT06 that employs LM-
powered reranking on hypotheses gener-
ated by phrase-based SMT systems
1 Introduction
Statistical machine translation (SMT) systems com-
bine a number of translation models with one or
more language models. Adding complex language
models in the incremental process of decoding is a
very challenging task. Some language models can
only score sentences as a whole. Also, SMT de-
coders generate during the search process a very
large number of partial hypotheses and query the
language model/models 1.
The solution to these problems is either to use
multiple iterations for decoding, to make use of the
complex LMs only for complete hypotheses in the
search space or to generate n-best lists and to rescore
the hypotheses using also the additional LMs. For
1During the translation of the first 10 sentences of the de-
vtest2006.de dataset using Phramer and the configuration de-
scribed in Section 3, the 3-gram LM was queried 27 million
times (3 million distinct queries).
the WMT 2006 shared task we opted for the rerank-
ing solution. This paper describes our solution and
results.
2 System Description
We developed for the WMT 2006 shared task a sys-
tem that is trained on a (a) word-aligned bilingual
corpus, (b) a large monolingual (English) corpus and
(c) an English treebank and it is capable of translat-
ing from a source language (German, Spanish and
French) into English.
Our system embeds Phramer2 (used for mini-
mum error rate training, decoding, decoding tools),
Pharaoh (Koehn, 2004) (decoding), Carmel 3
(helper for Pharaoh in n-best generation), Char-
niak?s parser (Charniak, 2001) (language model) and
SRILM4 (n-gram LM construction).
2.1 Translation table construction
We developed a component that builds a translation
table from a word-aligned parallel corpus. The com-
ponent generates the translation table according to
the process described in the Pharaoh training man-
ual5. It generates a vector of 5 numeric values for
each phrase pair:
? phrase translation probability:
?(f? |e?) = count(f? , e?)count(e?) , ?(e?|f?) =
count(f? , e?)
count(f?)
2http://www.phramer.org/ ? Java-based open-source phrase
based SMT system
3http://www.isi.edu/licensed-sw/carmel/
4http://www.speech.sri.com/projects/srilm/
5http://www.iccs.inf.ed.ac.uk/?pkoehn/training.tgz
150
? lexical weighting (Koehn et al, 2003):
lex(f? |e?, a) =
n
?
i=1
1
|{j|(i, j) ? a}|
?
?(i,j)?a
w(fi|ej)
lex(e?|f? , a) =
m
?
j=1
1
|{i|(i, j) ? a}|
?
?(i,j)?a
w(ej |fi)
? phrase penalty: ?(f? |e?) = e; log(?(f? |e?)) = 1
2.2 Decoding
We used the Pharaoh decoder for both the Min-
imum Error Rate Training (Och, 2003) and test
dataset decoding. Although Phramer provides de-
coding functionality equivalent to Pharaoh?s, we
preferred to use Pharaoh for this task because it
is much faster than Phramer ? between 2 and 15
times faster, depending on the configuration ? and
preliminary tests showed that there is no noticeable
difference between the output of these two in terms
of BLEU (Papineni et al, 2002) score.
The log-linear model uses 8 features: one distor-
tion feature, one basic LM feature, 5 features from
the translation table and one sentence length feature.
2.3 Minimum Error Rate Training
To determine the best coefficients of the log-linear
model (?) for both the initial stage decoding and
the second stage reranking, we used the unsmoothed
Minimum Error Rate Training (MERT) component
present in the Phramer package. The MERT com-
ponent is highly efficient; the time required to search
a set of 200,000 hypotheses is less than 30 seconds
per iteration (search from a previous/random ? to
a local maximum) on a 3GHz P4 machine. We
also used the distributed decoding component from
Phramer to speed up the search process.
We generated the n-best lists required for MERT
using the Carmel toolkit. Pharaoh outputs a lat-
tice for each input sentence, from which Carmel
extracts a specific number of hypotheses. We used
the europarl.en.srilm language model for decoding
the n-best lists.
The weighting vector is calculated individually
for each subtask (pair of source and target lan-
guages).
No. of sentences 96.7 M
No. of tokens 2.3 B
Vocabulary size 1.6 M
Distinct grams 1 B
Table 1: English Gigaword LM statistics
2.4 Language Models for reranking
We employed both syntactic language models and
n-gram based language models extracted from very
large corpora for improving the quality of the trans-
lation through reranking of the n-best list. These lan-
guage models add a total of 13 new features to the
log-linear model.
2.4.1 English Gigaword
We created large-scale n-gram language models
using English Gigaword Second Edition6 (EGW).
We split the corpus into sentences, tokenized the
corpus, lower-cased the sentences, replaced every
digit with ?9? to cluster different numbers into the
same unigram entry, filtered noisy sentences and we
collected n-gram counts (up to 4-grams). Table 1
presents the statistics related to this process.
We pruned the unigrams that appeared less than
15 times in the corpus and all the n-grams that con-
tain the pruned unigrams. We also pruned 3-grams
and 4-grams that appear only once in the corpus.
Based on these counts, we calculated 4 features for
each sentence: the logarithm of the probability of
the sentence based on unigrams, on bigrams, on 3-
grams and on 4-grams. The probabilities of each
word in the analyzed translation hypotheses were
bounded by 10?5 (to avoid overall zero probability
of a sentence caused by zero-counts).
Based on the unpruned counts, we calculated 8
additional features: how many of the n-grams in the
the hypothesis appear in the EGW corpus and also
how many of the n-grams in the hypotheses don?t
appear in the Gigaword corpus (n = 1..4). The
two types of counts will have different behavior only
when they are used to discriminate between two hy-
potheses with different length.
The number of n-grams in each of the two cases
is presented in Table 2.
6http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2005T12
151
sentence probability n-gram hit/miss
model model
1-grams 310 K 310 K
2-grams 45 M 45 M
3-grams 123 M 283 M
4-grams 235 M 675 M
Table 2: Number of n-gram entries in the EGW LM
2.4.2 Charniak parsing
We used Charniak?s parser as an additional LM
(Charniak, 2001) in reranking. The parser pro-
vides one feature for our model ? the log-grammar-
probability of the sentence.
We retrained the parser on lowercased Penn Tree-
bank II (Marcus et al, 1993), to match the lower-
cased output of the MT decoder.
Considering the huge number of hypotheses that
needed to be parsed for this task, we set it to parse
very fast (using the command-line parameter -T107).
2.5 Reranking and voting
A ? weights vector trained over the 8 basic features
(?1) is used to decode a n-best list. Then, a ? vector
trained over all 21 features (?2) is used to rerank
the n-best list, potentially generating a new first-best
hypothesis.
To improve the results, we generated during train-
ing a set of distinct ?2 weight vectors (4-10 different
weight vectors). Each ?2 picks a preferred hypoth-
esis. The final hypothesis is chosen using a voting
mechanism. The computational cost of the voting
process is very low - each of the ?2 is applied on the
same set of hypotheses - generated by a single ?1.
2.6 Preprocessing
The vocabulary of languages like English, French
and Spanish is relatively small. Most of the new
words that appear in a text and didn?t appear in a pre-
defined large text (i.e.: translation table) are abbre-
viations and proper nouns, that usually don?t change
their form when they are translated into another lan-
guage. Thus Pharaoh and Phramer deal with
out-of-vocabulary (OOV) words ? words that don?t
appear in the translation table ? by copying them
into the output translation. German is a compound-
ing language, thus the German vocabulary is virtu-
7Time factor. Higher is better. Default: 210
ally infinite. In order to avoid OOV issues for new
text, we applied a heuristic to improve the probabil-
ity of properly translating compound words that are
not present in the translation table. We extracted the
German vocabulary from the translation table. Then,
for each word in a text to be translated (development
set or test set), we checked if it is present in the trans-
lation dictionary. If it was not present, we checked
if it can be obtained by concatenating two words in
the dictionary. If we found at least one variant of
splitting the unknown word, we altered the text by
dividing the word into the corresponding pieces. If
there are multiple ways of splitting, we randomly
took one. The minimum length for the generated
word is 3 letters.
In order to minimize the risk of inserting words
that are not in the reference translation into the out-
put translation, we applied a OOV pruning algorithm
(Koehn et al, 2005) ? we removed every word in the
text to be translated that we know we cannot trans-
late (doesn?t appear either in the foreign part of the
parallel corpus used for training) or in what we ex-
pect to be present in an English text (doesn?t appear
in the English Gigaword corpus). This method was
applied to all the input text that was automatically
translated ? development and test; German, French
and Spanish.
For the German-to-English translation, the com-
pound word splitting algorithm was applied before
the unknown word removal process.
3 Experimental Setup
We generated the translation tables for each pair
of languages using the alignment provided for this
shared task.
We split the dev2006 files into two halves. The
first half was used to determine ?1. Using ?1, we
created a 500-best list for each sentence in the sec-
ond half. We calculated the value of the enhanced
features (EGW and Charniak) for each of these hy-
potheses. Over this set of almost 500 K hypothe-
ses, we computed 10 different ?2 using MERT. The
search process was seeded using ?1 padded with 0
for the new 13 features. We sorted the ?2s by the
BLEU score estimated by the MERT algorithm. We
pruned manually the ?2s that diverge too much from
the overall set of ?2s (based on the observation that
152
500-best best voting WPT05
oracle ?1 ?2 ?2 best
DE-EN
? no split 25.70
? split 33.63 25.81 26.29 26.28 24.77
FR-EN 37.33 30.90 31.21 31.21 30.27
ES-EN 38.06 31.13 31.15 31.22 30.95
Table 3: BLEU scores on the devtest2006 datasets.
Comparison with WPT05 results
500-best oracle ?1 voting ?2
DE-EN (split) 30.93 23.03 23.55
FR-EN 34.71 27.83 28.00
ES-EN 37.68 29.97 30.12
Table 4: BLEU scores on the test2006 datasets. Sub-
mitted results are bolded.
these weights are overfitting). We picked from the
remaining set the best ?2 and a preferred subset of
?2s to be used in voting.
The ?1 was also used to decode a 500-best list for
each sentence in the devtest2006 and test2006 sets.
After computing value of the enhanced features for
each of these hypotheses, we applied the reranking
algorithm to pick a new first-best hypothesis ? the
output of our system.
We used the following parameters for decoding:
-dl 5 -b 0.0001 -ttable-limit 30 -s 200 for French and
Spanish and -dl 9 -b 0.00001 -ttable-limit 30 -s 200
for German.
4 Results
Table 3 presents the detailed results of our system on
the devtest2006 datasets and comparison with WMT
2006 best results 8. The final results, on the test set
of the shared task, are reported in Table 4.
5 Conclusions
By analyzing the results, we observe that a very
powerful component of our system is the MERT
component of Phramer. It provided a very high
baseline for the devtest2006 sets (WPT05 test sets).
The additional language models seem to consis-
tently improve the results, although the increase is
not very significant on FR-EN and ES-EN subtasks.
The cause might be the specifics of the data involved
8http://www.statmt.org/wpt05/mt-shared-task/
in this shared task ? mostly European Parliament
proceedings, which is different than the domain of
both Treebank and English Gigaword ? newswire.
The enhanced LMs compete with the default LM
(which is also part of the model) that is trained on
European Parliament data.
The word splitting heuristics offers also a small
improvement for the performance on DE-EN sub-
task.
Voting seems to slightly improve the results in
some cases (ES-EN subtask). We believe that the
voting implementation reduces ? weights overfit-
ting, by combining the output of multiple local max-
ima of the development set. The size of the de-
velopment set used to generate ?1 and ?2 (1000
sentences) compensates the tendency of the un-
smoothed MERT algorithm to overfit (Och, 2003)
by providing a high ratio between number of vari-
ables and number of parameters to be estimated.
References
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of 39th Annual
Meeting of the Association for Computational Linguis-
tics, pages 124?131.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL 2003, Edmonton, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, David
Talbot, and Michael White. 2005. Edinburgh system
description for the 2005 NIST MT Evaluation.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Erhard Hinrichs
and Dan Roth, editors, Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 311?318.
153
