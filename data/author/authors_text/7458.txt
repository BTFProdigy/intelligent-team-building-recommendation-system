Proceedings of the 10th Conference on Parsing Technologies, pages 80?82,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Nbest Dependency Parsing with linguistically rich models 
Xiaodong Shi 
Institute of Artificial Intelligence 
Department of Computer Science 
Xiamen University, Xiamen 361005 
mandel@xmu.edu.cn 
Yidong Chen 
Institute of Artificial Intelligence 
Department of Computer Science 
Xiamen University, Xiamen 361005 
ydchen@xmu.edu.cn 
 
Abstract 
We try to improve the classifier-based de-
terministic dependency parsing in two 
ways: by introducing a better search 
method based on a non-deterministic nbest  
algorithm and by devising a series of lin-
guistically richer models. It is experimen-
tally shown on a ConLL 2007 shared task 
that this results in a system with higher per-
formance while still keeping it simple 
enough for an efficient implementation. 
1 Introduction 
This work tries to improve the deterministic de-
pendency parsing paradigm introduced in (Coving-
ton 2001, Nivre 2003, Nivre and Hall, 2005) where 
parsing is performed incrementally in a strict left-
to-right order and a machine learned classifier is 
used to predict deterministically the next parser 
action. Although this approach is very simple, it 
achieved the state-of-art parsing accuracy. How-
ever, there are still some problems that leave fur-
ther room for improvement: 
(1) A greedy algorithm without backtracking 
cannot ensure to find the optimal solution. In the 
course of left-to-right parsing, when further con-
text is seen, the previous decisions may be wrong 
but a deterministic parser cannot correct it. The 
usual way of preventing early error ?commitment? 
is to enable a k-best or beam-search strategy 
(Huang and Chiang 2005, Sagae and Lavie 2006). 
(2) A classifier based approach (e.g. using SVM 
or memory based learning) is usually linguistically 
na?ve, to make it applicable to multiple languages. 
However, a few studies (Collins 1999, Charniak et 
al 2003, Galley et al2006) have shown that lin-
guistically sophisticated models can have a better 
accuracy at parsing, language modeling, and ma-
chine translation, among others. 
In this paper we explore ways to improve on the 
above-mentioned deterministic parsing model to 
overcome the two problems. The rest of the paper 
is organized as follows. Section 2 argues for a 
search strategy better at finding the optimal solu-
tion. In section 3 we built a series of linguistically 
richer models and show experimental results dem-
onstrating their practical consequences. Finally we 
draw our conclusions and point out areas to be ex-
plored further. 
2 Dependency Parsing Enhancements 
In the classifier-based approach as in Nivre (2003) 
a parse tree is produced by a series of actions 
similar to a left-to-right shift-reduce parser. The 
main source of errors in this method is the 
irrevocability of the parsing action and a wrong 
decision can therefore lead to further inaccuracies 
in later stages. So it cannot usually handle garden-
path sentences. Moreover, each action is usually 
predicted using only the local features of the words 
in a limited window, although dynamic features of 
the local context can be exploited (Carreras 2006).  
To remedy this situation, we just add a scoring 
function and a priority queue which records nbest 
partial parses. The scoring function is defined on 
the parsing actions and the features of a partial 
parse. It can be decomposed into two subfunctions: 
score(a,y)=parsing_cost(a,y) + lm(y) 
where a is parsing actions and y is partial parses, 
and parsing cost (parsing_cost) is used to imple-
ment certain parsing preferences while the lingus-
tic model score (lm) is usually modeled in the lin-
guistic (in our case, dependency model) framework. 
80
In the usual nbest or beam-search implementation 
(e.g. Huang and Chiang 2005, Sagae and Lavie 
2006), only lm is present. 
We give justification of the first term as follows: 
Many probability functions need to know the de-
pendency label and relative distance between the 
dependent and the head. However, during parsing 
sometimes this head-binding can be very late. This 
means a right-headed word may need to wait very 
long for its right head, and so a big partial-parse 
queue is needed, while psychological evidence 
suggests that there is some overhead involved in 
processing every word and a word tends to attach 
locally. By modeling parsing cost we can first use 
a coarse probability model to guide the nbest par-
tial results in order not to defer the probability cal-
culation. As parsing progresses, more information 
becomes available; we can have a better estimation 
of our linguistic probability model to rectify the 
inaccuracy.  
This use of a coarse scoring mechanism to guide 
the early parsing for possible later rectification of 
the decision is a novel feature of our parsing 
framework and enables better searching of the so-
lution space. To implement it, we just remember 
the exact score of the every major decision (wait, 
add a dependent or attach a head) in parsing, and 
re-score when more context is available. Compared 
with (Charniak 2005), our parsing process requires 
only one pass. 
Thus, we can strike a balance between accuracy, 
memory and speed. With a moderately-sized n 
(best partial results), we can reduce memory use 
and get higher speed to get a same accuracy. An 
added advantage is that this idea is also useful in 
other bottom-up parsing paradigms (not only in a 
dependency framework). 
In a word, our main innovation is the use of a 
parsing cost to influence the search paths, and the 
use of an evolving lm function to enable progres-
sively better modeling. The nbest framework is 
general enough to make this a very simple modifi-
cation to the basic algorithm of Nivre (2003).  
3 Better Linguistic Modeling 
In our modeling we combine different linguistic 
models by using many probability functions: 
lm(y)=?logP(wi,wj,x,y) =?W*log P 
where w are the trained weight vector and P is a 
vector of probability functions. In our system we 
considered the following functions: 
P1: function measuring the probability of a head 
and a dependent. This is the base function in most 
dependency parsing framework. 
P2: function calculating the subcategorization 
frame probability; 
P3:  function calculating the semantic frame us-
ing a Chinese FrameNet (Liu 2006).  
P4: function measuring the semantic affinity be-
tween a head and a dependent using resources such 
as Hownet (Dong 2003). 
P5: Other Chinese specific probability functions 
defined on the features of the head, the dependents, 
the partial parse and the input. 
Model P2 is a probability function on pseudo 
subcategorization frames (as a concatenation of all 
the dependents? labels) as we don?t know the dis-
tinction of arguments and adjuncts in the depend-
ency Treebank. We used a Markovian subcategori-
zation scheme with left and right STOP delimiters 
to ease the data sparseness. And as a first approxi-
mation, we also experimented with a model where 
each label can only be used a certain times in a 
direction. This model is called P2? in Table 4. 
Other functions (P3-P5) are also very useful 
with its different linguistic content. Model P5 actu-
ally contains a lot of Chinese-specific functions, 
e.g. between a sentence-final particle and a verb. 
We designed a series of experiments to show to 
effectiveness of each model. We use the Chinese 
training data of the ConLL 2007 shared task. We 
divided the training data by a 9:1 split. Table 1 
shows the statistics. 
 Training testing 
sentences 51777 5180 
Words 302943 34232 
Table 1. Experimental data 
In the baseline model, we train a simple probability 
function between a head and a dependent using 
deleted interpolation. For nbest=1, we have a 
deterministic model. 
 LAS UAS time 
Deterministic 41.64 % 44.11 % 8s 
nbest = 50 71.30 % 76.34 % 72s 
nbest  = 500 71.90 % 76.99 % 827s 
Table 2. baseline systems 
It can be seen (Table 3) that combing different 
linguistic information can lead to significant in-
81
crease of the accuracy. However, different models 
have different contributions. Our experiments con-
firm with Collins?s result in that subcategorization 
carries very important linguistic content.  
 LAS UAS time 
P1 71.90 % 76.99 % 827s 
P1 + P2? 73.45 % 78.44 % 832s 
P1 + P2? + P2 77.92 % 82.42 % 855s 
P1 + P2 + P3 79.13% 83.57% 1003s 
P1-4 81.21% 85.78% 1597s 
P1-5 83.12% 87.03% 2100s 
Verb valency  85.32 % 89.12 % - 
DE refinement 85.98% 90.20% - 
Table 3. systems with different linguistic models 
3.1 Relabeling of the parse treebank 
Sometimes the information needed in the modeling 
is not in the data explicitly. Implicit information 
can be made explicit and accessible to the parser. 
In the Chinese Treebank the relation label is 
often determined by the head word?s semantic type. 
We tried the relabeling of coarse POS info of the 
verb in a effort to detect its valency; and refine-
ment of the auxiliary word  ? DE (as error analy-
sis shows it is the where the most errors occur). 
Results are in Table 3. 
We also tried refinement of the relation label by 
using the two connected words. However, this does 
not improve the result. Automatic linguistic model-
ing using latent label (Matsuzaki 2005) can also be 
attempted but is not yet done.  
4 Conclusions 
In this paper we showed that simple classifier-
based deterministic dependency parsing can be 
improved using a more flexible search strategy 
over an nbest parsing framework and a variety of 
linguistically richer models. By incorporating dif-
ferent linguistic knowledge, the parsing model can 
be made more accurate and thus achieves better 
results. 
Further work to be done includes ways to com-
bine machine learning based on the automatic fea-
ture selection with manual linguistic modeling: an 
interactive approach for better synergistic model-
ing (where the machine proposes and the human 
guides). Various a priori models can be tried by the 
machine and patterns inherent in the data can be 
revealed to the human who can then explore more 
complex models. 
References 
Xavier Carreras, Mihai Surdeanu, and Llu?s M?rquez. 
2006. Projective Dependency Parsing with Percep-
tron. In Proceedings of CoNLL-X. 181-185. 
Liang Huang and David Chiang. 2005. Better k-best 
parsing. In Proceedings of IWPT.  
Eugene Charniak; K. Knight, and K.Yamada. 2003. 
Syntax-based language models for statistical ma-
chine translation. In MT Summit IX. Intl. Assoc. for 
Machine Translation. 
Eugene Charniak and Mark Johnson. 2005. Coarse-
tofine n-best parsing and maxent discriminative 
reranking. In Proceedings of ACL. 
Michael Collins. 1999. Head-Driven Statistical Models-
for Natural Language Parsing. PhD Dissertation, 
University of Pennsylvania.  
Michael Collins. 2004. Parameter Estimation for Statis-
tical Parsing Models: Theory and Practice of Distri-
bution-Free Methods. In Harry Bunt el al, New De-
velopments in Parsing Technology, Kluwer. 
Michael A. Covington. 2001. A fundamental algorithm 
for dependency parsing. Proceedings of the 39th An-
nual ACM Southeast Conference, pp. 95-102. 
Zhendong Dong  and Qiang Dong. 2003. HowNet - a 
hybrid language and knowledge resource. In Pro-
ceeding of Natural Language Processing and Knowl-
edge Engineering.  
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe, 
W. Wang, and I. Thayer. 2006. Scalable Inference 
and Training of Context-Rich Syntactic Models. In 
Proc. ACL-COLING. 
Kaiying Liu. 2006. Building a Chinese FrameNet. In 
Proceeding of 25th anniversary of Chinese Informa-
tion Processing Society of China. 
Takuya Matsuzaki, Yusuke Miyao, Jun'ichi Tsujii. 2005. 
Probabilistic CFG with latent annotations. In Pro-
ceedings of ACL-2005. 
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of IWPT. 
149-160. 
Joakim Nivre and Johan Hall. 2005. MaltParser: A Lan-
guage-Independent System for Data-Driven Depend-
ency Parsing. In Proceedings of the Fourth Work-
shop on Treebanks and Linguistic. Theories, Barce-
lona, 9-10 December 2005. 137-148. 
Sagae, K. and Lavie, A. 2006 A best-first probabilistic 
shift-reduce parser. In Proceedings of ACL. 
 
82
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459?468,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Translation Model Adaptation for Statistical Machine Translation with
Monolingual Topic Information?
Jinsong Su1,2, Hua Wu3, Haifeng Wang3, Yidong Chen1, Xiaodong Shi1,
Huailin Dong1, and Qun Liu2
Xiamen University, Xiamen, China1
Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China2
Baidu Inc., Beijing, China3
{jssu, ydchen, mandel, hldong}@xmu.edu.cn
{wu hua, wanghaifeng}@baicu.com
liuqun@ict.ac.cn
Abstract
To adapt a translation model trained from
the data in one domain to another, previous
works paid more attention to the studies of
parallel corpus while ignoring the in-domain
monolingual corpora which can be obtained
more easily. In this paper, we propose a
novel approach for translation model adapta-
tion by utilizing in-domain monolingual top-
ic information instead of the in-domain bilin-
gual corpora, which incorporates the topic in-
formation into translation probability estima-
tion. Our method establishes the relationship
between the out-of-domain bilingual corpus
and the in-domain monolingual corpora vi-
a topic mapping and phrase-topic distribution
probability estimation from in-domain mono-
lingual corpora. Experimental result on the
NIST Chinese-English translation task shows
that our approach significantly outperforms
the baseline system.
1 Introduction
In recent years, statistical machine translation(SMT)
has been rapidly developing with more and more
novel translation models being proposed and put in-
to practice (Koehn et al, 2003; Och and Ney, 2004;
Galley et al, 2006; Liu et al, 2006; Chiang, 2007;
Chiang, 2010). However, similar to other natural
language processing(NLP) tasks, SMT systems of-
ten suffer from domain adaptation problem during
practical applications. The simple reason is that the
underlying statistical models always tend to closely
?Part of this work was done during the first author?s intern-
ship at Baidu.
approximate the empirical distributions of the train-
ing data, which typically consist of bilingual sen-
tences and monolingual target language sentences.
When the translated texts and the training data come
from the same domain, SMT systems can achieve
good performance, otherwise the translation quality
degrades dramatically. Therefore, it is of significant
importance to develop translation systems which can
be effectively transferred from one domain to anoth-
er, for example, from newswire to weblog.
According to adaptation emphases, domain adap-
tation in SMT can be classified into translation mod-
el adaptation and language model adaptation. Here
we focus on how to adapt a translation model, which
is trained from the large-scale out-of-domain bilin-
gual corpus, for domain-specific translation task,
leaving others for future work. In this aspect, pre-
vious methods can be divided into two categories:
one paid attention to collecting more sentence pairs
by information retrieval technology (Hildebrand et
al., 2005) or synthesized parallel sentences (Ueffing
et al, 2008; Wu et al, 2008; Bertoldi and Federico,
2009; Schwenk and Senellart, 2009), and the other
exploited the full potential of existing parallel cor-
pus in a mixture-modeling (Foster and Kuhn, 2007;
Civera and Juan, 2007; Lv et al, 2007) framework.
However, these approaches focused on the studies of
bilingual corpus synthesis and exploitation while ig-
noring the monolingual corpora, therefore limiting
the potential of further translation quality improve-
ment.
In this paper, we propose a novel adaptation
method to adapt the translation model for domain-
specific translation task by utilizing in-domain
459
monolingual corpora. Our approach is inspired by
the recent studies (Zhao and Xing, 2006; Zhao and
Xing, 2007; Tam et al, 2007; Gong and Zhou, 2010;
Ruiz and Federico, 2011) which have shown that a
particular translation always appears in some spe-
cific topical contexts, and the topical context infor-
mation has a great effect on translation selection.
For example, ?bank? often occurs in the sentences
related to the economy topic when translated into
?y?inha?ng?, and occurs in the sentences related to the
geography topic when translated to ?he?a`n?. There-
fore, the co-occurrence frequency of the phrases in
some specific context can be used to constrain the
translation candidates of phrases. In a monolingual
corpus, if ?bank? occurs more often in the sentences
related to the economy topic than the ones related
to the geography topic, it is more likely that ?bank?
is translated to ?y?inha?ng? than to ?he?a`n?. With the
out-of-domain bilingual corpus, we first incorporate
the topic information into translation probability es-
timation, aiming to quantify the effect of the topical
context information on translation selection. Then,
we rescore all phrase pairs according to the phrase-
topic and the word-topic posterior distributions of
the additional in-domain monolingual corpora. As
compared to the previous works, our method takes
advantage of both the in-domain monolingual cor-
pora and the out-of-domain bilingual corpus to in-
corporate the topic information into our translation
model, thus breaking down the corpus barrier for
translation quality improvement. The experimental
results on the NIST data set demonstrate the effec-
tiveness of our method.
The reminder of this paper is organized as fol-
lows: Section 2 provides a brief description of trans-
lation probability estimation. Section 3 introduces
the adaptation method which incorporates the top-
ic information into the translation model; Section
4 describes and discusses the experimental results;
Section 5 briefly summarizes the recent related work
about translation model adaptation. Finally, we end
with a conclusion and the future work in Section 6.
2 Background
The statistical translation model, which contains
phrase pairs with bi-directional phrase probabilities
and bi-directional lexical probabilities, has a great
effect on the performance of SMT system. Phrase
probability measures the co-occurrence frequency of
a phrase pair, and lexical probability is used to vali-
date the quality of the phrase pair by checking how
well its words are translated to each other.
According to the definition proposed by (Koehn
et al, 2003), given a source sentence f = fJ1 =
f1, . . . , fj , . . . , fJ , a target sentence e = eI1 =
e1, . . . , ei, . . . , eI , and its word alignment a which
is a subset of the Cartesian product of word position-
s: a ? (j, i) : j = 1, . . . , J ; i = 1, . . . , I , the phrase
pair (f? , e?) is said to be consistent (Och and Ney,
2004) with the alignment if and only if: (1) there
must be at least one word inside one phrase aligned
to a word inside the other phrase and (2) no words
inside one phrase can be aligned to a word outside
the other phrase. After all consistent phrase pairs are
extracted from training corpus, the phrase probabil-
ities are estimated as relative frequencies (Och and
Ney, 2004):
?(e?|f?) =
count(f? , e?)
?
e??
count(f? , e??)
(1)
Here count(f? , e?) indicates how often the phrase pair
(f? , e?) occurs in the training corpus.
To obtain the corresponding lexical weight, we
first estimate a lexical translation probability distri-
bution w(e|f) by relative frequency from the train-
ing corpus:
w(e|f) =
count(f, e)
?
e?
count(f, e?)
(2)
Retaining the alignment a? between the phrase pair
(f? , e?), the corresponding lexical weight is calculated
as
pw(e?|f? , a?) =
|e?|?
i=1
1
|{j|(j, i) ? a?}|
?
?(j,i)?a?
w(ei|fj) (3)
However, the above-mentioned method only
counts the co-occurrence frequency of bilingual
phrases, assuming that the translation probability is
independent of the context information. Thus, the
statistical model estimated from the training data is
not suitable for text translation in different domains,
resulting in a significant drop in translation quality.
460
3 Translation Model Adaptation via
Monolingual Topic Information
In this section, we first briefly review the principle
of Hidden Topic Markov Model(HTMM) which is
the basis of our method, then describe our approach
to translation model adaptation in detail.
3.1 Hidden Topic Markov Model
During the last couple of years, topic models such
as Probabilistic Latent Semantic Analysis (Hof-
mann, 1999) and Latent Dirichlet Allocation mod-
el (Blei, 2003), have drawn more and more attention
and been applied successfully in NLP community.
Based on the ?bag-of-words? assumption that the or-
der of words can be ignored, these methods model
the text corpus by using a co-occurrence matrix of
words and documents, and build generative model-
s to infer the latent aspects or topics. Using these
models, the words can be clustered into the derived
topics with a probability distribution, and the corre-
lation between words can be automatically captured
via topics.
However, the ?bag-of-words? assumption is an
unrealistic oversimplification because it ignores the
order of words. To remedy this problem, Gruber et
al.(2007) propose HTMM, which models the topics
of words in the document as a Markov chain. Based
on the assumption that all words in the same sen-
tence have the same topic and the successive sen-
tences are more likely to have the same topic, HTM-
M incorporates the local dependency between words
by Hidden Markov Model for better topic estima-
tion.
HTMM can also be viewed as a soft clustering
tool for words in training corpus. That is, HT-
MM can estimate the probability distribution of a
topic over words, i.e. the topic-word distribution
P (word|topic) during training. Besides, HTMM
derives inherent topics in sentences rather than in
documents, so we can easily obtain the sentence-
topic distribution P (topic|sentence) in training
corpus. Adopting maximum likelihood estima-
tion(MLE), this posterior distribution makes it pos-
sible to effectively calculate the word-topic distri-
bution P (topic|word) and the phrase-topic distribu-
tion P (topic|phrase) both of which are very impor-
tant in our method.
3.2 Adapted Phrase Probability Estimation
We utilize the additional in-domain monolingual
corpora to adapt the out-of-domain translation mod-
el for domain-specific translation task. In detail, we
build an adapted translation model in the following
steps:
? Build a topic-specific translation model to
quantify the effect of the topic information on
the translation probability estimation.
? Estimate the topic posterior distributions of
phrases in the in-domain monolingual corpora.
? Score the phrase pairs according to the prede-
fined topic-specific translation model and the
topic posterior distribution of phrases.
Formally, we incorporate monolingual topic in-
formation into translation probability estimation,
and decompose the phrase probability ?(e?|f?)1 as
follows:
?(e?|f?) =
?
tf
?(e?, tf |f?)
=
?
tf
?(e?|f? , tf ) ? P (tf |f?) (4)
where ?(e?|f? , tf ) indicates the probability of trans-
lating f? into e? given the source-side topic tf ,
P (tf |f?) denotes the phrase-topic distribution of f? .
To compute ?(e?|f?), we first apply HTMM to re-
spectively train two monolingual topic models with
the following corpora: one is the source part of
the out-of-domain bilingual corpus Cf out, the oth-
er is the in-domain monolingual corpus Cf in in the
source language. Then, we respectively estimate
?(e?|f? , tf ) and P (tf |f?) from these two corpora. To
avoid confusion, we further refine ?(e?|f? , tf ) and
P (tf |f?) with ?(e?|f? , tf out) and P (tf in|f?), respec-
tively. Here, tf out is the topic clustered from the
corpus Cf out, and tf in represents the topic derived
from the corpus Cf in.
However, the two above-mentioned probabilities
can not be directly multiplied in formula (4) be-
cause they are related to different topic spaces from
1Due to the limit of space, we omit the description of the cal-
culation method of the phrase probability ?(f? |e?), which can be
adjusted in a similar way to ?(e?|f?) with the help of in-domain
monolingual corpus in the target language.
461
different corpora. Besides, their topic dimension-
s are not assured to be the same. To solve this
problem, we introduce the topic mapping probabili-
ty P (tf out|tf in) to map the in-domain phrase-topic
distribution into the one in the out-domain topic s-
pace. To be specific, we obtain the out-of-domain
phrase-topic distribution P (tf out|f?) as follows:
P (tf out|f?) =
?
tf in
P (tf out|tf in) ? P (tf in|f?) (5)
Thus formula (4) can be further refined as the fol-
lowing formula:
?(e?|f?) =
?
tf out
?
tf in
?(e?|f? , tf out)
?P (tf out|tf in) ? P (tf in|f?) (6)
Next we will give detailed descriptions of the cal-
culation methods for the three probability distribu-
tions mentioned in formula (6).
3.2.1 Topic-Specific Phrase Translation
Probability ?(e?|f? , tf out)
We follow the common practice (Koehn et al,
2003) to calculate the topic-specific phrase trans-
lation probability, and the only difference is that
our method takes the topical context information in-
to account when collecting the fractional counts of
phrase pairs. With the sentence-topic distribution
P (tf out|f) from the relevant topic model of Cf out,
the conditional probability ?(e?|f? , tf out) can be eas-
ily obtained by MLE method:
?(e?|f? , tf out)
=
?
?f ,e??Cout
count?f ,e?(f? , e?) ? P (tf out|f)
?
e??
?
?f ,e??Cout
count?f ,e?(f? , e??) ? P (tf out|f)
(7)
where Cout is the out-of-domain bilingual training
corpus, and count?f ,e?(f? , e?) denotes the number of
the phrase pair (f? , e?) in sentence pair ?f , e?.
3.2.2 Topic Mapping Probability P (tf out|tf in)
Based on the two monolingual topic models re-
spectively trained from Cf in and Cf out, we com-
pute the topic mapping probability by using source
word f as the pivot variable. Noticing that there
are some words occurring in one corpus only, we
use the words belonging to both corpora during the
mapping procedure. Specifically, we decompose
P (tf out|tf in) as follows:
P (tf out|tf in)
=
?
f?Cf out
?
Cf in
P (tf out|f) ? P (f |tf in) (8)
Here we first get P (f |tf in) directly from the top-
ic model related to Cf in. Then, considering the
sentence-topic distribution P (tf out|f) from the rel-
evant topic model of Cf out, we define the word-
topic distribution P (tf out|f) as:
P (tf out|f)
=
?
f?Cf out
countf (f) ? P (tf out|f)
?
tf out
?
f?Cf out
countf (f) ? P (tf out|f)
(9)
where countf (f) denotes the number of the word f
in sentence f .
3.2.3 Phrase-Topic Distribution P (tf in|f? )
A simple way to compute the phrase-topic distri-
bution is to take the fractional counts from Cf in
and then adopt MLE to obtain relative probability.
However, it is infeasible in our model because some
phrases occur in Cf out while being absent in Cf in.
To solve this problem, we further compute this pos-
terior distribution by the interpolation of two model-
s:
P (tf in|f?) = ? ? Pmle(tf in|f?) +
(1? ?) ? Pword(tf in|f?) (10)
where Pmle(tf in|f?) indicates the phrase-topic dis-
tribution by MLE, Pword(tf in|f?) denotes the
phrase-topic distribution which is decomposed into
the topic posterior distribution at the word level, and
? is the interpolation weight that can be optimized
over the development data.
Given the number of the phrase f? in sentence f
denoted as countf (f?), we compute the in-domain
phrase-topic distribution in the following way:
Pmle(tf in|f?)
=
?
f?Cf in
countf (f?) ? P (tf in|f)
?
tf in
?
f?Cf in
countf (f?) ? P (tf in|f)
(11)
462
Under the assumption that the topics of all word-
s in the same phrase are independent, we consid-
er two methods to calculate Pword(tf in|f?). One is
a ?Noisy-OR? combination method (Zens and Ney,
2004) which has shown good performance in calcu-
lating similarities between bags-of-words in differ-
ent languages. Using this method, Pword(tf in|f?) is
defined as:
Pword(tf in|f?)
= 1? Pword(t?f in|f?)
? 1?
?
fj?f?
P (t?f in|fj)
= 1?
?
fj?f?
(1? P (tf in|fj)) (12)
where Pword(t?f in|f?) represents the probability that
tf in is not the topic of the phrase f? . Similarly,
P (t?f in|fj) indicates the probability that tf in is not
the topic of the word fj .
The other method is an ?Averaging? combination
one. With the assumption that tf in is the topic of f?
if at least one of the words in f? belongs to this topic,
we derive Pword(tf in|f?) as follows:
Pword(tf in|f?) ?
?
fj?f?
P (tf in|fj)/|f? | (13)
where |f? | denotes the number of words in phrase f? .
3.3 Adapted Lexical Probability Estimation
Now we briefly describe how to estimate the adapted
lexical weight for phrase pairs, which can be adjust-
ed in a similar way to the phrase probability.
Specifically, adopting our method, each word is
considered as one phrase consisting of only one
word, so
w(e|f) =
?
tf out
?
tf in
w(e|f, tf out)
?P (tf out|tf in) ? P (tf in|f) (14)
Here we obtain w(e|f, tf out) with a simi-
lar approach to ?(e?|f? , tf out), and calculate
P (tf out|tf in) and P (tf in|f) by resorting to
formulas (8) and (9).
With the adjusted lexical translation probability,
we resort to formula (4) to update the lexical weight
for the phrase pair (f? , e?).
4 Experiment
We evaluate our method on the Chinese-to-English
translation task for the weblog text. After a brief de-
scription of the experimental setup, we investigate
the effects of various factors on the translation sys-
tem performance.
4.1 Experimental setup
In our experiments, the out-of-domain training cor-
pus comes from the FBIS corpus and the Hansard-
s part of LDC2004T07 corpus (54.6K documents
with 1M parallel sentences, 25.2M Chinese words
and 29M English words). We use the Chinese Sohu
weblog in 20091 and the English Blog Authorship
corpus2 (Schler et al, 2006) as the in-domain mono-
lingual corpora in the source language and target
language, respectively. To obtain more accurate top-
ic information by HTMM, we firstly filter the noisy
blog documents and the ones consisting of short sen-
tences. After filtering, there are totally 85K Chinese
blog documents with 2.1M sentences and 277K En-
glish blog documents with 4.3M sentences used in
our experiments. Then, we sample equal numbers of
documents from the in-domain monolingual corpo-
ra in the source language and the target language to
respectively train two in-domain topic models. The
web part of the 2006 NIST MT evaluation test da-
ta, consisting of 27 documents with 1048 sentences,
is used as the development set, and the weblog part
of the 2008 NIST MT test data, including 33 docu-
ments with 666 sentences, is our test set.
To obtain various topic distributions for the out-
of-domain training corpus and the in-domain mono-
lingual corpora in the source language and the tar-
get language respectively, we use HTMM tool devel-
oped by Gruber et al(2007) to conduct topic model
training. During this process, we empirically set the
same parameter values for the HTMM training of d-
ifferent corpora: topics = 50, ? = 1.5, ? = 1.01,
iters = 100. See (Gruber et al, 2007) for the
meanings of these parameters. Besides, we set the
interpolation weight ? in formula (10) to 0.5 by ob-
serving the results on development set in the addi-
tional experiments.
We choose MOSES, a famous open-source
1http://blog.sohu.com/
2http://u.cs.biu.ac.il/ koppel/BlogCorpus.html
463
phrase-based machine translation system (Koehn
et al, 2007), as the experimental decoder.
GIZA++ (Och and Ney, 2003) and the heuristics
?grow-diag-final-and? are used to generate a word-
aligned corpus, from which we extract bilingual
phrases with maximum length 7. We use SRILM
Toolkits (Stolcke, 2002) to train two 4-gram lan-
guage models on the filtered English Blog Author-
ship corpus and the Xinhua portion of Gigaword
corpus, respectively. During decoding, we set the
ttable-limit as 20, the stack-size as 100, and per-
form minimum-error-rate training (Och and Ney,
2003) to tune the feature weights for the log-linear
model. The translation quality is evaluated by
case-insensitive BLEU-4 metric (Papineni et al,
2002). Finally, we conduct paired bootstrap sam-
pling (Koehn, 2004) to test the significance in BLEU
score differences.
4.2 Result and Analysis
4.2.1 Effect of Different Smoothing Methods
Our first experiments investigate the effect of dif-
ferent smoothing methods for the in-domain phrase-
topic distribution: ?Noisy-OR? and ?Averaging?.
We build adapted phrase tables with these two meth-
ods, and then respectively use them in place of the
out-of-domain phrase table to test the system perfor-
mance. For the purpose of studying the generality of
our approach, we carry out comparative experiments
on two sizes of in-domain monolingual corpora: 5K
and 40K.
Adaptation
Method
(Dev) MT06
Web
(Tst) MT08
Weblog
Baseline 30.98 20.22
Noisy-OR (5K) 31.16 20.45
Averaging (5K) 31.51 20.54
Noisy-OR (40K) 31.87 20.76
Averaging (40K) 31.89 21.11
Table 1: Experimental results using different smoothing
methods.
Table 1 reports the BLEU scores of the translation
system under various conditions. Using the out-of-
domain phrase table, the baseline system achieves
a BLEU score of 20.22. In the experiments with
the small-scale in-domain monolingual corpora, the
BLEU scores acquired by two methods are 20.45
and 20.54, achieving absolute improvements of 0.23
and 0.32 on the test set, respectively. In the exper-
iments with the large-scale monolingual in-domain
corpora, similar results are obtained, with absolute
improvements of 0.54 and 0.89 over the baseline
system.
From the above experimental results, we know
that both ?Noisy-OR? and ?Averaging? combination
methods improve the performance over the base-
line, and ?Averaging? method seems to be slight-
ly better. This finding fails to echo the promis-
ing results in the previous study (Zens and Ney,
2004). This is because the ?Noisy-OR? method in-
volves the multiplication of the word-topic distribu-
tion (shown in formula (12)), which leads to much
sharper phrase-topic distribution than ?Averaging?
method, and is more likely to introduce bias to the
translation probability estimation. Due to this rea-
son, all the following experiments only consider the
?Averaging?method.
4.2.2 Effect of Combining Two Phrase Tables
In the above experiments, we replace the out-of-
domain phrase table with the adapted phrase table.
Here we combine these two phrase tables in a log-
linear framework to see if we could obtain further
improvement. To offer a clear description, we repre-
sent the out-of-domain phrase table and the adapted
phrase table with ?OutBP? and ?AdapBP?, respec-
tively.
Used Phrase
Table
(Dev) MT06
Web
(Tst) MT08
Weblog
Baseline 30.98 20.22
AdapBp (5K) 31.51 20.54
+ OutBp 31.84 20.70
AdapBp (40K) 31.89 21.11
+ OutBp 32.05 21.20
Table 2: Experimental results using different phrase ta-
bles. OutBp: the out-of-domain phrase table. AdapBp:
the adapted phrase table.
Table 2 shows the results of experiments using d-
ifferent phrase tables. Applying our adaptation ap-
proach, both ?AdapBP? and ?OutBP + AdapBP?
consistently outperform the baseline, and the lat-
464
Figure 1: Effect of in-domain monolingual corpus size on
translation quality.
ter produces further improvements over the former.
Specifically, the BLEU scores of the ?OutBP +
AdapBP? method are 20.70 and 21.20, which ob-
tain 0.48 and 0.98 points higher than the baseline
method, and 0.16 and 0.09 points higher than the
?AdapBP? method. The underlying reason is that the
probability distribution of each in-domain sentence
often converges on some topics in the ?AdapBP?
method and some translation probabilities are over-
estimated, which leads to negative effects on the
translation quality. By using two tables together, our
approach reduces the bias introduced by ?AdapBP?,
therefore further improving the translation quality.
4.2.3 Effect of In-domain Monolingual Corpus
Size
Finally, we investigate the effect of in-domain
monolingual corpus size on translation quality. In
the experiment, we try different sizes of in-domain
documents to train different monolingual topic mod-
els: from 5K to 80K with an increment of 5K each
time. Note that here we only focus on the exper-
iments using the ?OutBP + AdapBP? method, be-
cause this method performs better in the previous
experiments.
Figure 1 shows the BLEU scores of the transla-
tion system on the test set. It can be seen that the
more data, the better translation quality when the
corpus size is less than 30K. The overall BLEU
scores corresponding to the range of great N val-
ues are generally higher than the ones correspond-
ing to the range of small N values. For example, the
BLEU scores under the condition within the range
[25K, 80K] are all higher than the ones within the
range [5K, 20K]. When N is set to 55K, the BLEU
score of our system is 21.40, with 1.18 gains on the
baseline system. This difference is statistically sig-
nificant at P < 0.01 using the significance test tool
developed by Zhang et al(2004). For this experi-
mental result, we speculate that with the increment
of in-domain monolingual data, the corresponding
topic models provide more accurate topic informa-
tion to improve the translation system. However,
this effect weakens when the monolingual corpora
continue to increase.
5 Related work
Most previous researches about translation model
adaptation focused on parallel data collection. For
example, Hildebrand et al(2005) employed infor-
mation retrieval technology to gather the bilingual
sentences, which are similar to the test set, from
available in-domain and out-of-domain training da-
ta to build an adaptive translation model. With
the same motivation, Munteanu and Marcu (2005)
extracted in-domain bilingual sentence pairs from
comparable corpora. Since large-scale monolin-
gual corpus is easier to obtain than parallel corpus,
there have been some studies on how to generate
parallel sentences with monolingual sentences. In
this respect, Ueffing et al (2008) explored semi-
supervised learning to obtain synthetic parallel sen-
tences, and Wu et al (2008) used an in-domain
translation dictionary and monolingual corpora to
adapt an out-of-domain translation model for the in-
domain text.
Differing from the above-mentioned works on
the acquirement of bilingual resource, several stud-
ies (Foster and Kuhn, 2007; Civera and Juan, 2007;
Lv et al, 2007) adopted mixture modeling frame-
work to exploit the full potential of the existing par-
allel corpus. Under this framework, the training cor-
pus is first divided into different parts, each of which
is used to train a sub translation model, then these
sub models are used together with different weights
during decoding. In addition, discriminative weight-
ing methods were proposed to assign appropriate
weights to the sentences from training corpus (Mat-
soukas et al, 2009) or the phrase pairs of phrase ta-
ble (Foster et al, 2010). Final experimental result-
s show that without using any additional resources,
these approaches all improve SMT performance sig-
465
nificantly.
Our method deals with translation model adap-
tation by making use of the topical context, so let
us take a look at the recent research developmen-
t on the application of topic models in SMT. As-
suming each bilingual sentence constitutes a mix-
ture of hidden topics and each word pair follows a
topic-specific bilingual translation model, Zhao and
Xing (2006,2007) presented a bilingual topical ad-
mixture formalism to improve word alignment by
capturing topic sharing at different levels of linguis-
tic granularity. Tam et al(2007) proposed a bilin-
gual LSA, which enforces one-to-one topic corre-
spondence and enables latent topic distributions to
be efficiently transferred across languages, to cross-
lingual language modeling and translation lexicon
adaptation. Recently, Gong and Zhou (2010) also
applied topic modeling into domain adaptation in
SMT. Their method employed one additional feature
function to capture the topic inherent in the source
phrase and help the decoder dynamically choose re-
lated target phrases according to the specific topic of
the source phrase.
Besides, our approach is also related to context-
dependent translation. Recent studies have shown
that SMT systems can benefit from the utiliza-
tion of context information. For example, trigger-
based lexicon model (Hasan et al, 2008; Mauser et
al., 2009) and context-dependent translation selec-
tion (Chan et al, 2007; Carpuat and Wu, 2007; He
et al, 2008; Liu et al, 2008). The former gener-
ated triplets to capture long-distance dependencies
that go beyond the local context of phrases, and the
latter built the classifiers which combine rich con-
text information to better select translation during
decoding. With the consideration of various local
context features, these approaches all yielded stable
improvements on different translation tasks.
As compared to the above-mentioned works, our
work has the following differences.
? We focus on how to adapt a translation mod-
el for domain-specific translation task with the
help of additional in-domain monolingual cor-
pora, which are far from full exploitation in the
parallel data collection and mixture modeling
framework.
? In addition to the utilization of in-domain
monolingual corpora, our method is differen-
t from the previous works (Zhao and Xing,
2006; Zhao and Xing, 2007; Tam et al, 2007;
Gong and Zhou, 2010) in the following aspect-
s: (1) we use a different topic model ? HTMM
which has different assumption from PLSA and
LDA; (2) rather than modeling topic-dependent
translation lexicons in the training process, we
estimate topic-specific lexical probability by
taking account of topical context when extract-
ing word pairs, so our method can also be di-
rectly applied to topic-dependent phrase proba-
bility modeling. (3) Instead of rescoring phrase
pairs online, our approach calculate the transla-
tion probabilities offline, which brings no addi-
tional burden to translation systems and is suit-
able to translate the texts without the topic dis-
tribution information.
? Different from trigger-based lexicon model and
context-dependent translation selection both of
which put emphasis on solving the translation
ambiguity by the exploitation of the context in-
formation at the sentence level, we adopt the
topical context information in our method for
the following reasons: (1) the topic informa-
tion captures the context information beyond
the scope of sentence; (2) the topical context in-
formation is integrated into the posterior prob-
ability distribution, avoiding the sparseness of
word or POS features; (3) the topical context
information allows for more fine-grained dis-
tinction of different translations than the genre
information of corpus.
6 Conclusion and future work
This paper presents a novel method for SMT sys-
tem adaptation by making use of the monolingual
corpora in new domains. Our approach first esti-
mates the translation probabilities from the out-of-
domain bilingual corpus given the topic information,
and then rescores the phrase pairs via topic mapping
and phrase-topic distribution probability estimation
from in-domain monolingual corpora. Experimental
results show that our method achieves better perfor-
mance than the baseline system, without increasing
the burden of the translation system.
In the future, we will verify our method on oth-
466
er language pairs, for example, Chinese to Japanese.
Furthermore, since the in-domain phrase-topic dis-
tribution is currently estimated with simple smooth-
ing interpolations, we expect that the translation sys-
tem could benefit from other sophisticated smooth-
ing methods. Finally, the reasonable estimation of
topic number for better translation model adaptation
will also become our study emphasis.
Acknowledgement
The authors were supported by 863 State Key
Project (Grant No. 2011AA01A207), National
Natural Science Foundation of China (Grant Nos.
61005052 and 61103101), Key Technologies R&D
Program of China (Grant No. 2012BAH14F03). We
thank the anonymous reviewers for their insightful
comments. We are also grateful to Ruiyu Fang and
Jinming Hu for their kind help in data processing.
References
Michiel Bacchiani and Brian Roark. 2003. Unsuper-
vised Language Model Adaptation. In Proc. of ICAS-
SP 2003, pages 224-227.
Michiel Bacchiani and Brian Roark. 2005. Improving
Machine Translation Performance by Exploiting Non-
Parallel Corpora. Computational Linguistics, pages
477-504.
Nicola Bertoldi and Marcello Federico. 2009. Domain
Adaptation for Statistical Machine Translation with
Monolingual Resources. In Proc. of ACL Workshop
2009, pages 182-189.
David M. Blei. 2003. Latent Dirichlet Allocation. Jour-
nal of Machine Learning, pages 993-1022.
Ivan Bulyko, Spyros Matsoukas, Richard Schwartz, Long
Nguyen and John Makhoul. 2007. Language Model
Adaptation in Machine Translation from Speech. In
Proc. of ICASSP 2007, pages 117-120.
Marine Carpuat and Dekai Wu. 2007. Improving Statis-
tical Machine Translation Using Word Sense Disam-
biguation. In Proc. of EMNLP 2007, pages 61-72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2006.
Word sense disambiguation improves statistical ma-
chine translation. In Proc. of ACL 2007, pages 33-40.
Boxing Chen, George Foster and Roland Kuhn. 2010.
Bilingual Sense Similarity for Statistical Machine
Translation. In Proc. of ACL 2010, pages 834-843.
David Chiang. 2007. Hierarchical Phrase-Based Trans-
lation. Computational Linguistics, pages 201-228.
David Chiang. 2010. Learning to Translate with Source
and Target Syntax. In Proc. of ACL 2010, pages 1443-
1452.
Jorge Civera and Alfons Juan. 2007. Domain Adaptation
in Statistical Machine Translation with Mixture Mod-
elling. In Proc. of the Second Workshop on Statistical
Machine Translation, pages 177-180.
Matthias Eck, Stephan Vogel and Alex Waibel. 2004.
Language Model Adaptation for Statistical Machine
Translation Based on Information Retrieval. In Proc.
of Fourth International Conference on Language Re-
sources and Evaluation, pages 327-330.
Matthias Eck, Stephan Vogel and Alex Waibel. 2005.
Low Cost Portability for Statistical Machine Transla-
tion Based on N-gram Coverage. In Proc. of MT Sum-
mit 2005, pages 227-234.
George Foster and Roland Kuhn. 2007. Mixture Model
Adaptation for SMT. In Proc. of the Second Workshop
on Statistical Machine Translation, pages 128-135.
George Foster, Cyril Goutte and Roland Kuhn. 2010.
Discriminative Instance Weighting for Domain Adap-
tation in Statistical Machine Translation. In Proc. of
EMNLP 2010, pages 451-459.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang and Ignacio Thay-
er. 2006. Scalable Inference and Training of Context-
Rich Syntactic Translation Models. In Proc. of ACL
2006, pages 961-968.
Zhengxian Gong and Guodong Zhou. 2010. Improve
SMT with Source-side Topic-Document Distributions.
In Proc. of MT SUMMIT 2010, pages 24-28.
Amit Gruber, Michal Rosen-Zvi and Yair Weiss. 2007.
Hidden Topic Markov Models. In Journal of Machine
Learning Research, pages 163-170.
Sas?a Hasan, Juri Ganitkevitch, Hermann Ney and Jesu?s
Andre?s-Ferrer 2008. Triplet Lexicon Models for S-
tatistical Machine Translation. In Proc. of EMNLP
2008, pages 372-381.
Zhongjun He, Qun Liu and Shouxun Lin. 2008. Improv-
ing Statistical Machine Translation using Lexicalized
Rule Selection. In Proc. of COLING 2008, pages 321-
328.
Almut Silja Hildebrand. 2005. Adaptation of the Trans-
lation Model for Statistical Machine Translation based
on Information Retrieval. In Proc. of EAMT 2005,
pages 133-142.
Thomas Hofmann. 1999. Probabilistic Latent Semantic
Indexing. In Proc. of SIGIR 1999, pages 50-57.
Franz Joseph Och and Hermann Ney. 2003. A Systemat-
ic Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics, pages 19-51.
Franz Joseph Och and Hermann Ney. 2004. The Align-
ment Template Approach to Statistical Machine Trans-
lation. Computational Linguistics, pages 417-449.
467
Philipp Koehn, Franz Josef Och and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of HLT-
NAACL 2003, pages 127-133.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proc. of EMNLP
2004, pages 388-395.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL 2007, Demonstration Session, pages 177-180.
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-
to-String Alignment Template for Statistical Machine
Translation. In Proc. of ACL 2006, pages 609-616.
Yajuan Lv, Jin Huang and Qun Liu. 2007. Improv-
ing Statistical Machine Translation Performance by
Training Data Selection and Optimization. In Proc.
of EMNLP 2007, pages 343-350.
Arne Mauser, Richard Zens and Evgeny Matusov, Sas?a
Hasan and Hermann Ney. 2006. The RWTH Statisti-
cal Machine Translation System for the IWSLT 2006
Evaluation. In Proc. of International Workshop on
Spoken Language Translation, pages 103-110.
Arne Mauser, Sas?a Hasan and Hermann Ney 2009. Ex-
tending Statistical Machine Translation with Discrimi-
native and Trigger-Based Lexicon Models. In Proc. of
ACL 2009, pages 210-218.
Spyros Matsoukas, Antti-Veikko I. Rosti and Bing Zhang
2009. Discriminative Corpus Weight Estimation for
Machine Translation. In Proc. of EMNLP 2009, pages
708-717.
Nick Ruiz and Marcello Federico. 2011. Topic Adapta-
tion for Lecture Translation through Bilingual Latent
Semantic Models. In Proc. of ACL Workshop 2011,
pages 294-302.
Kishore Papineni, Salim Roukos, Todd Ward and WeiJing
Zhu. 2002. BLEU: A Method for Automatic Evalu-
ation of Machine Translation. In Proc. of ACL 2002,
pages 311-318.
Jonathan Schler, Moshe Koppel, Shlomo Argamon and
James Pennebaker. 2006. Effects of Age and Gender
on Blogging. In Proc. of 2006 AAAI Spring Sympo-
sium on Computational Approaches for Analyzing We-
blogs.
Holger Schwenk and Jean Senellart. 2009. Translation
Model Adaptation for an Arabic/french News Transla-
tion System by Lightly-supervised Training. In Proc.
of MT Summit XII.
Andreas Stolcke. 2002. Srilm - An Extensible Language
Modeling Toolkit. In Proc. of ICSLP 2002, pages 901-
904.
Yik-Cheung Tam, Ian R. Lane and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for statistical machine
translation. Machine Translation, pages 187-207.
Nicola Ueffing, Gholamreza Haffari and Anoop Sarkar.
2008. Semi-supervised Model Adaptation for Statisti-
cal Machine Translation. Machine Translation, pages
77-94.
Hua Wu, Haifeng Wang and Chengqing Zong. 2008. Do-
main Adaptation for Statistical Machine Translation
with Domain Dictionary and Monolingual Corpora. In
Proc. of COLING 2008, pages 993-1000.
Richard Zens and Hermann Ney. 2004. Improvments in
phrase-based statistical machine translation. In Proc.
of NAACL 2004, pages 257-264.
Ying Zhang, Almut Silja Hildebrand and Stephan Vogel.
2006. Distributed Language Modeling for N-best List
Re-ranking. In Proc. of EMNLP 2006, pages 216-223.
Bing Zhao, Matthias Eck and Stephan Vogel. 2004.
Language Model Adaptation for Statistical Machine
Translation with Structured Query Models. In Proc.
of COLING 2004, pages 411-417.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
Topic AdMixture Models for Word Alignment. In
Proc. of ACL/COLING 2006, pages 969-976.
Bing Zhao and Eric P. Xing. 2007. HM-BiTAM: Bilin-
gual Topic Exploration, Word Alignment, and Trans-
lation. In Proc. of NIPS 2007, pages 1-8.
Qun Liu, Zhongjun He, Yang Liu and Shouxun Lin.
2008. Maximum Entropy based Rule Selection Model
for Syntax-based Statistical Machine Translation. In
Proc. of EMNLP 2008, pages 89-97.
468
Chinese Personal Name Disambiguation: 
 Technical Report of Natural Language Processing Lab of 
Xiamen University 
Xiang Zhu, Xiaodong Shi, Ningfeng Liu, YingMei Guo, Yidong Chen 
Natural Language Processing Lab, Department of Cognitive Science, Xiamen 
University , Xiamen 361005 
zhuxiang_sm@163.com,mandel@xmu.edu.cn,nffliu@gmail.com 
Abstract 
This report presents the work of our 
group in the Chinese personal name 
disambiguation workshop. We propose a 
system which uses a HAC algorithm to 
cluster the mentions referring to the same 
person with features extracted from the 
documents.  
1 Introduction 
Personal name disambiguation is actually a 
task to group those documents according to 
whether the given personal name appearing 
in that document refers to the same person in 
reality. It becomes an active research topic 
recently, and the evaluation campaign for the 
English personal name has been held twice. 
Chinese personal name disambiguation is 
thought to be more challenging due to the 
need for word segmentation which could 
bring errors into the subsequent processes. 
The most widely used method for personal 
name disambiguation is unsupervised 
clustering, we adopt a Hierarchical 
Agglomerative Clustering algorithm in our 
system, which is also the most popular 
clustering algorithm used by the teams of the 
second Web People Search Evaluation 
campaign. 
The remaining part of this report is 
organized as follows. Section 2 introduces 
the document preprocessing. Section 3 
describes the feature used for the clustering 
and the section 4 addresses the clustering 
algorithm. The workshop requests two 
different tests, a formal test, and a diagnosis 
test, we will discuss the difference between 
them with our system?s result in section 5. 
2 Document preprocessing  
Different from the document preprocessing 
for English, we have to use a segmentation 
tool and a part-of-speech tagger to do some 
preprocessing work. For example, without 
the segmenting process, the documents only 
contain the string ???????? could be 
clustered when the query name is ????, 
and we need the part-of-speech tagger to 
detect whether the Chinese word ??? ? 
stands for a personal name or a toponym. 
Our experiments prove that the system is 
sensitive to the tools? performance, the 
different result between the formal test and 
the diagnosis test also proves this.  
These tools are trained with the 90% data 
of The People's Daily published in 
Jan.1998,and tested with the others 10% data. 
The performances of the tools are? 
 Precision Recall F 
seg 97.746% 97.793% 97.769% 
tag 94.197% 94.242% 94.219% 
Table 1: the performances of the tools 
3 Extracted features 
As mentioned previously, our goal is to 
group those documents. In our approach, 
each document is represented by a vector of 
features extracted from it automatically. We 
use three kinds of token-based features:  
   1) Nouns occur around the ambiguous 
personal name. 
This kind of features is selected based on 
the idea that words around the ambiguous 
personal name are more relevant to it, and 
Nouns can provide a more diagnostic 
description of the person. We use a window 
to select the nouns. 
   2) Personal names (except the ambiguous 
personal name) and toponyms occur in the 
document. 
It is intuitive that the identical person 
often associated with the same personal 
names and toponyms. 
   3)  Words with high TFIDF value. In our 
final system, we use the ten words with 
highest TFIDF value. 
This kind of features can reflect the theme 
of an article, an identical person often be 
mentioned in articles with the same theme. 
Using these features simultaneously can 
alleviate the problem caused by spare data. 
The following table presents a quantitative 
analysis: 
used features highest F score on 
dev data 
Feature 1 83.76 
Feature 1,2 86.18 
Feature 1,3 84.83 
Feature 1,2,3 86.76 
Table 2:analysis of features 
These results are obtained by using a 
initial version of the preprocessing toools, 
and when we improve the performances of 
the tools, the highest F score increases from 
86.76 to 89.61 . 
The similarity between documents was 
measured with the cosine of feature vectors. 
When computing the similarity between two 
documents, we proposed a weighting method 
for the features as:  
If the token occurs in the document, the 
weight will be 1, else 0. 
We have tried another method that count 
the tokens? weight by its frequency, but 
experiments prove it is a less effective one, 
we can interpret it by this example: 
If a word "??" which refers to a person 
named " ? ? " appears twice in one 
document  while three times in another 
document, these two documents are very 
likely referring to the same person, but the 
latter weighting method decreases the 
similarity between them.   
4 Clustering algorithm 
A Hierarchical Agglomerative Clustering 
algorithm is adopted in our system, which 
determines the number of the cluster by a 
fixed similarity threshold learned from the 
train data. At each stage of the clustering, the 
two most similar clusters are merged into a 
new one, and other clusters? similarities with 
the new cluster is the larger one of their 
similarities with the two old clusters. 
We have tried some other clustering 
methods such as modified k-means 
clustering with the same features, but the 
performances are worse. 
A pre-judgment stage before clustering is 
useful in the experiment, which can be done 
as follow: 
If two documents have the same triple 
tokens ?token1 token2 PersonName? (token2 
is tagged as noun), then they are classified to 
one cluster. 
5 Result and discussion 
The difference between the formal test and 
the diagnosis test is that the ambiguous 
personal name in each document have been 
told in the latter, but you have to find it in the 
former by yourself. The method we adopt to 
detect the ambiguous personal name in the 
formal test is to find the token which is 
tagged as personal name while contains the 
query name. 
   Our system?s performances are:  
B-Cubed precision Recall F 
Formal 
test 
90.55 84.88 85.72 
Diagnosis 
test 
89.84 89.84 89.08 
Table 3: the performances in the B-Cubed 
criterion 
P-IP P IP F 
Formal 
test 
93.3 89.22 89.9 
Diagnosis 
test 
92.77 93.33 92.71 
Table 4: the performances in the P-IP 
criterion 
From the results we can know that 
Chinese personal name disambiguation can 
be affected by the segmentation tool and the 
part-of-speech tagger. 
References 
Y. Chen, S. Y. M. Lee, and C.-R. Huang. 
Polyuhk: A robust information extraction 
system for web personal names. In 2nd Web 
People Search Evaluation Workshop (WePS 
2009), 18th WWW Conference, 2009.  
J. Gong and D. Oard. Determine the entity 
number in hierarchical clustering for web 
personal name disambiguation. In 2nd Web 
People Search Evaluation Workshop (WePS 
2009), 18th WWW Conference, 2009. 
P. Kalmar and D. Freitag. Features for web 
person disambiguation. In 2nd Web People 
Search Evaluation Workshop (WePS 2009), 
18th WWW Conference, 2009. 
1 
 
Chinese Word Sense Induction based on Hierarchical Clustering 
Algorithm 
Ke Cai, Xiaodong Shi, Yidong Chen?Zhehuang Huang, Yan Gao                    
           Cognitive Science Department, Xiamen University, Xiamen, 361005, China 
 
Abstract 
Sense induction seeks to automatically identify word senses of polysemous words 
encountered in a corpus. Unsupervised word sense induction can be viewed as a clustering 
problem. In this paper, we used the Hierarchical Clustering Algorithm as the classifier for 
word sense induction. Experiments show the system can achieve 72% F-score about 
train-corpus and 65% F-score about test-corpus. 
1. Introduction 
Word sense induction is a central problem in many natural language processing tasks such 
as information extraction, information retrieval, and machine translation [Vickrey et al, 
2005]. 
   Clp 2010 launches totally 4 tasks for evaluation exercise, these are: Chinese word 
segmentation, Chinese parsing, Chinese Personal Name disambiguation and Chinese Word 
Sense Induction. We participated in task 4, which is Chinese Word Sense Induction..  
Because the contents surround an ambiguous word is related to its meaning, we solve the 
sense problem by grouping the instances of the target word into the supposed number of 
clusters according to the similarity of contexts of the instance. In this paper we used the 
hierarchical clustering algorithm to accomplish the problem. 
   The task can be defined as two stage process: Feature selection and word clustering. 
Researchers have proposed much approach to the sense induction task which involved the use 
of basic word co-occurrence features and application of classical clustering algorithms.  
   Because the meanings of unknown words can be inferred from the contexts in which they 
appear, Pantel and Lin (2002) map the senses to WordNet. More recently, the mapping has 
been used to test the system on publicly available benchmarks (Purandare and Pedersen, 2004; 
Niu et al, 2005). 
However, this approach does not generalize to multiple-sense words. Each sense of a 
polysemous word can appear in a different context, there have been many attempts in recent 
years to apply classical clustering algorithms to this problem. 
  Clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 
2004), to agglomerative clustering (Sch?utze, 1998), and the Information Bottleneck (Niu et 
al., 2007). Senses are induced by identifying highly dense subgraphs (hubs) in the 
co-occurrence graph (V?eronis, 2004).The sIB algorithm was used to estimate cluster 
structure, which measures the similarity of contexts of instances according to the similarity of 
their feature conditional distribution(Slonim, et al,2002). Each algorithm treats words as 
feature vectors, using the same similarity function based on context information. 
The remainder of this paper is organized as follows. In section 2 the Featured set and 
word similarity definition is introduced. The hierarchical clustering algorithm is presented in 
section 3. Section 4 provides the experimental results and conclusion is drawn in section 5. 
2 
 
 
2. Feature Selection and Word Similarity Definition  
 
  2.1 Feature Selection 
 
 A feature set is used designed to capture both immediate local context in our 
experiment, wider context and syntactic context. Specifically, we experimented with several 
feature categories: ?5-word window (5w), ?3-word window (3w), part-of speech n-grams and 
dependency relations. These features have been widely adopted in various word sense 
induction algorithms. The overall best scores are achieved with local (5 words) context 
windows. 
 
. 2.2 Similarity Definition 
 
We treat the context words as feature vectors, using the same similarity function. 
Suppose 
1 2( , )i i i inC w w w?  is the contexts set of sentence iS , and 1 2( , )j j j jnC w w w?  is the 
contexts set of sentence
jS .  
Then we defined ( , ) ( , )
jk i
jl j
i j kl ik jl
W C
W C
sim S S w sim w w
?
?
? ? , here klw  is variable weight, 
Where
( , )
( , )ik jl ik jl
sim w w
dis w w
?
?
?
?
, ? is an adjustable parameter with a value of 1.2, and 
( , )ik jlDis w w  is the path length between ikw  and jlw  based on the semantic tree structure 
used for TongYiCi CiLin (?????). 
     
3. The Hierarchical Clustering Algorithm Used In Word Sense Induction 
 
Sense induction is viewed as an unsupervised clustering problem where to group a 
word?s contexts into different classes, each representing a word sense. In this paper, we use 
the bottom-up clumping approach, which begin with n singleton clusters and successively 
merge clusters to produce the other ones.  
Table1: Hierarchical Clustering Algorithm: 
1. initialize number of senses n  ? number of clusters m  
and clusters 1 2( , ), 1,2i i iC w w i m?? ?  
2. Set k n?  
3. Set  1k k? ?   
3 
 
4. Find the nearest clusters iC and jC  , Merge iC and jC  
5. If  k m?  ,  go to step 3, otherwise go to step 6; 
6.  return m  clusters  
 
The merging of the two clusters in step 4 simply corresponds to adding an edge between 
the nearest pair of nodes in iC and jC .To find the nearest clusters, the following clustering 
similarity function is used: 
( , ) ( , )
jk i
jl j
i j kl ik jl
W C
W C
sim S S w sim w w
?
?
? ?
. 
Our model incorporates features based on lexical information and parts of speech.             
So we propose a improved hierarchical clustering algorithm based on parts of speech. 
Table2: improved algorithm based on parts of speech. 
1.  initialize number of senses n  ? number of clusters m  
and clusters 1 2( , ), 1,2i i iC w w i m?? ?  
2.  Part of Speech Tagging on the corpus  
3. Divided n  senses into nn  classes base on the information of parts of 
speech. 
4. If nn m? , return m  clusters 
5. If nn m?  , invoke hierarchical clustering algorithm in different 
classes?merge clusters into m cluster. 
6.if nn m?  , invoke hierarchical clustering algorithm in different 
tagging, merge clusters into m  cluster. 
7. return m  clusters  
 
4. Experimental Results 
 
The test data includes totally 100 ambiguous Chinese words,  every word have 50 
4 
 
untagged instances. Table3 show the best/worst/average F-Score of our system about 
train-corpus and test-corpus. 
   
 Best word Worst word  All  words 
Train-corpus 0.98  0.5 0.73 
Test-corpus ------ ----- 0.65 
                    Table 3 Model performance with deferent corpus 
  Table 4 shows the performance of our model about train-corpus when using 3w and 5w 
word windows, which represent more immediate, local context. 
 
 Best word Worst word  All  words 
3w(?3-word 
window) 
0.98  0.5 0.73 
5w(?5-word 
window) 
0.92 0.52 0.72 
                    Table 4 Model performance with deferent windows  
 Table 5 summarizes the F-score in our system about train-corpus when using deferent 
similarity definition.  
 Best word Worst word  All  words 
This article 0.98  0.5 0.73 
Qun LIU  0.99 0.59 0.78 
                    Table 5 Model performance with deferent similarity definition  
  
Experimental results show that the Hierarchical Clustering Algorithm can be applied to 
sense induction. Considering words to be feature vectors and applying clustering algorithm 
can improve accuracy of the task. A significant gap still exists between the results of these 
techniques and the gold standard of manually compiled word sense dictionaries. 
 
5. Conclusions 
 
Sense induction is treated as an unsupervised clustering problem. In this paper we adopt 
hierarchical clustering algorithm to accomplish the problem. Generate context words 
according to this distribution of key words and formalize the induction problem in a 
generative mode. Experiments show the system can achieved 72% F-score about train-corpus 
and 65% F-score about test-corpus. The basic cluster algorithm can sorts the word sense into 
clusters corresponding to the context. 
 
References 
 
Boyd-Graber, Jordan, David Blei, and Xiaojin Zhu. 2007.A topic model for word sense 
disambiguation. In Proceedings of the EMNLP-CoNLL. Prague, Czech Republic,pages 
1024?1033. 
David Vickrey, Luke Biewald, Marc Teyssler, and Daphne Koller. Word-sense 
disambiguation for machine translation. In Proceedings of the conference on Human 
Language Technology and Empirical Methods in Natural Language Processing, page 
5 
 
771-778, 2005. 
Qun LIU , Sujian LI. Word Similarity Computing Based on How-net. Computational 
Linguistics and Chinese Language Processing 
Niu, Zheng-Yu, Dong-Hong Ji, and Chew-Lim Tan. 2007. I2r: Three systems for word 
sense discrimination, chineseword sense disambiguation, and english word sense 
disambiguation. In Proceedings of the Fourth International Workshop on Semantic 
Evaluations (SemEval-2007). Association for Computational Linguistics, Prague, Czech 
Republic, pages 177?182. 
Niu, Z.Y., Ji, D.H., & Tan, C.L. 2005. Word Sense Disambiguation Using Label 
Propagation Based Semi-Supervised Learning. Proceedings of the 43rd Annual Meeting of 
the Association for Computational Linguistics. 
Pantel, Patrick and Dekang Lin. 2002. Discovering word senses from text. In Proceedings 
of the 8th KDD. New York, NY, pages 613?619. 
Pedersen, Ted. 2007. Umnd2 : Senseclusters applied to the sense induction task of 
senseval-4. In Proceedings of SemEval-2007. Prague, Czech Republic, pages 394?397. 
Purandare, Amruta and Ted Pedersen. 2004. Word sense discrimination by clustering 
contexts in vector and similarity spaces. In Proceedings of the CoNLL. Boston, MA, pages 
41?48 
V?eronis, Jean. 2004. Hyperlex: lexical cartography for information retrieval. Computer 
Speech & Language. 18(3):223?252. 
 
