Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 149?159, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Weakly Supervised Model for Sentence-Level Semantic Orientation
Analysis with Multiple Experts
Lizhen Qu and Rainer Gemulla and Gerhard Weikum
Max Planck Institute for Informatics
Saarbru?cken, Germany
{lqu,rgemulla,weikum}@mpi-inf.mpg.de
Abstract
We propose the weakly supervised Multi-
Experts Model (MEM) for analyzing the se-
mantic orientation of opinions expressed in
natural language reviews. In contrast to most
prior work, MEM predicts both opinion po-
larity and opinion strength at the level of in-
dividual sentences; such fine-grained analysis
helps to understand better why users like or
dislike the entity under review. A key chal-
lenge in this setting is that it is hard to ob-
tain sentence-level training data for both po-
larity and strength. For this reason, MEM is
weakly supervised: It starts with potentially
noisy indicators obtained from coarse-grained
training data (i.e., document-level ratings), a
small set of diverse base predictors, and, if
available, small amounts of fine-grained train-
ing data. We integrate these noisy indicators
into a unified probabilistic framework using
ideas from ensemble learning and graph-based
semi-supervised learning. Our experiments in-
dicate that MEM outperforms state-of-the-art
methods by a significant margin.
1 Introduction
Opinion mining is concerned with analyzing opin-
ions expressed in natural language text. For example,
many internet websites allow their users to provide
both natural language reviews and numerical ratings
to items of interest (such as products or movies).
In this context, opinion mining aims to uncover the
relationship between users and (features of) items.
Preferences of users to items can be well understood
by coarse-grained methods of opinion mining, which
focus on analyzing the semantic orientation of doc-
uments as a whole. To understand why users like or
dislike certain items, however, we need to perform
more fine-grained analysis of the review text itself.
In this paper, we focus on sentence-level analy-
sis of semantic orientation (SO) in online reviews.
The SO consists of polarity (positive, negative, or
other1) and strength (degree to which a sentence is
positive or negative). Both quantities can be ana-
lyzed jointly by mapping them to numerical ratings:
Large negative/positive ratings indicate a strong neg-
ative/positive orientation. A key challenge in fine-
grained rating prediction is that fine-grained train-
ing data for both polarity and strength is hard to
obtain. We thus focus on a weakly supervised set-
ting in which only coarse-level training data (such
as document ratings and subjectivity lexicons) and,
optionally, a small amount of fine-grained training
data (such as sentence polarities) is available.
A number of lexicon-based approaches for phrase-
level rating prediction has been proposed in the liter-
ature (Taboada et al2011; Qu et al2010). These
methods utilize a subjectivity lexicon of words along
with information about their semantic orientation;
they focus on phrases that contain words from the
lexicon. A key advantage of sentence-level methods
is that they are able to cover all sentences in a review
and that phrase identification is avoided. To the best
of our knowledge, the problem of rating prediction
at the sentence level has not been addressed in the
literature. A naive approach would be to simply aver-
age phrase-level ratings. Such an approach performs
1We assign polarity other to text fragments that are off-topic
or not directly related to the entity under review.
149
poorly, however, since (1) phrases are analyzed out
of context (e.g., modal verbs or conditional clauses),
(2) domain-dependent information about semantic
orientation is not captured in the lexicons, (3) only
phrases that contain lexicon words are covered. Here
(1) and (2) lead to low precision, (3) to low recall.
To address the challenges outlined above, we pro-
pose the weakly supervised Multi-Experts Model
(MEM) for sentence-level rating prediction. MEM
starts with a set of potentially noisy indicators of SO
including phrase-level predictions, language heuris-
tics, and co-occurrence counts. We refer to these
indicators as base predictors; they constitute the set
of experts used in our model. MEM is designed
such that new base predictors can be easily integrated.
Since the information provided by the base predictors
can be contradicting, we use ideas from ensemble
learning (Dietterichl, 2002) to learn the most con-
fident indicators and to exploit domain-dependent
information revealed by document ratings. Thus, in-
stead of averaging base predictors, MEM integrates
their features along with the available coarse-grained
training data into a unified probabilistic model.
The integrated model can be regarded as a Gaus-
sian process (GP) model (Rasmussen, 2004) with
a novel multi-expert prior. The multi-expert prior
decomposes into two component distributions. The
first component distribution integrates sentence-local
information obtained from the base predictors. It
forms a special realization of stacking (Dzeroski and
Zenko, 2004) but uses the features from the base pre-
dictors instead of the actual predictions. The second
component distribution propagates SO information
across similar sentences using techniques from graph-
based semi-supervised learning (GSSL) (Zhu et al
2003; Belkin et al2006). It aims to improve the
predictions on sentences that are not covered well
enough by our base predictors. Traditional GSSL al-
gorithms support either discrete labels (classification)
or numerical labels (regression); we extend these
techniques to support both types of labels simulta-
neously. We use a novel variant of word sequence
kernels (Cancedda et al2003) to measure sentence
similarity. Our kernel takes the relative positions of
words but also their SO and synonymity into account.
Our experiments indicate that MEM significantly
outperforms prior work in both sentence-level rating
prediction and sentence-level polarity classification.
2 Related Work
There exists a large body of work on analyzing the
semantic orientation of natural language text. Our
approach is unique in that it is weakly supervised,
predicts both polarity and strength, and operates on
the sentence level.
Supervised approaches for sentiment analysis fo-
cus mainly on opinion mining at the document
level (Pang and Lee, 2004; Pang et al2002; Pang
and Lee, 2005; Goldberg and Zhu, 2006), but have
also been applied to sentence-level polarity classifi-
cation in specific domains (Mao and Lebanon, 2006;
Pang and Lee, 2004; McDonald et al2007). In
these settings, a sufficient amount of training data is
available. In contrast, we focus on opinion mining
tasks with little or no fine-grained training data.
The weakly supervised HCRF model (Ta?ckstro?m
and McDonald, 2011b; Ta?ckstro?m and McDonald,
2011a) for sentence-level polarity classification is per-
haps closest to our work in spirit. Similar to MEM,
HCRF uses coarse-grained training data and, when
available, a small amount of fine-grained sentence
polarities. In contrast to MEM, HCRF does not pre-
dict the strength of semantic orientation and ignores
the order of words within sentences.
There exists a large number of lexicon-based meth-
ods for polarity classification (Ding et al2008; Choi
and Cardie, 2009; Hu and Liu, 2004; Zhuang et al
2006; Fu and Wang, 2010; Ku et al2008). The
lexicon-based methods of (Taboada et al2011; Qu
et al2010) also predict ratings at the phrase level;
these methods are used as experts in our model.
MEM leverages ideas from ensemble learning (Di-
etterichl, 2002; Bishop, 2006) and GSSL meth-
ods (Zhu et al2003; Zhu and Ghahramani, 2002;
Chapelle et al2006; Belkin et al2006). We extend
GSSL with support for multiple, heterogenous labels.
This allows us to integrate our base predictors as well
as the available training data into a unified model
that exploits that strengths of algorithms from both
families.
3 Base Predictors
Each of our base predictors predicts the polarity or
the rating of a single phrase. As indicated above,
we do not use these predictions directly in MEM but
instead integrate the features of the base predictors
150
(see Sec. 4.4). MEM is designed such that new base
predictors can be integrated easily.
Our base predictors use a diverse set of available
web and linguistic resources. The hope is that this di-
versity increases overall prediction performance (Di-
etterichl, 2002): The statistical polarity predictor fo-
cuses on local syntactic patterns; it is based on corpus
statistics for SO-carrying words and opinion topic
words. The heuristic polarity predictor uses manu-
ally constructed rules to achieve high precision but
low recall. Both the bag-of-opinions rating predictor
and the SO-CAL rating predictor are based on lexi-
cons. The BoO predictor uses a lexicon trained from
a large generic-domain corpus and is recall-oriented;
the SO-CAL predictor uses a different lexicon with
manually assigned weights and is precision-oriented.
3.1 Statistical Polarity Predictor
The polarity of an SO-carrying word strongly de-
pends on its target word. For example, consider the
phrase ?I began this novel with the greatest of hopes
[...]?. Here, ?greatest? has a positive semantic orien-
tation in all subjectivity lexicons, but the combination
?greatest of hopes? often indicates a negative senti-
ment. We refer to a pair of SO-carrying word (?great-
est?) and a target word (?hopes?) as an opinion-target
pair. Our statistical polarity predictor learns the po-
larity of opinions and targets jointly, which increases
the robustness of its predictions.
Syntactic dependency relations of the form
A
R
?? B are a strong indicator for opinion-target
pairs (Qiu et al2009; Zhuang et al2006); e.g.,
?great?
nmod
?????product?. To achieve high precision,
we only consider pairs connected by the follow-
ing predefined set of shortest dependency paths:
verb
subj
??? noun, verb
obj
?? noun, adj
nmod
???? noun,
adj
prd
??? verb
subj
??? noun. We only retain opinion-
target pairs that are sufficiently frequent.
For each extracted pair z, we count how often it
co-occurs with each document polarity y ? Y , where
Y = {positive, negative, other} denotes the set of po-
larities. If z occurs in a document but is preceded by
a negator, we treat it as a co-occurrence of opposite
document polarity. If z occurs in a document with po-
larity other, we count the occurrence with only half
weight, i.e., we increase both #z and #(other, z)
by 0.5. These documents are typically a mixture of
positive and negative opinions so that we want to
reduce their impact. The marginal distribution of
polarity label y given that z occurs in a sentence is
estimated as P (y | z) = #(y, z)/#z. The predictor
is trained using the text and ratings of the reviews in
the training data, i.e., without relying on fine-grained
annotations.
The statistical polarity predictor can be used to pre-
dict sentence-level polarities by averaging the phrase-
level predictions. As discussed previously, such an
approach is problematic; we use it as a baseline ap-
proach in our experimental study. We also employ
phrase-level averaging to estimate the variance of
base predictors; see Sec. 4.3. Denote by Z(x) the set
of opinion-target pairs in sentence x. To predict the
sentence polarity y ? Y , we take the Bayesian aver-
age of the phrase-level predictors: P (y | Z(x)) =
?
z?Z(x) P (y | z)P (z) =
?
z?Z(x) P (y, z). Thus
the most likely polarity is the one with the highest
co-occurrence count.
3.2 Heuristic Polarity Predictor
Heuristic patterns can also serve as base predictors.
In particular, we found that some authors list positive
and negative aspects separately after keywords such
as ?pros? and ?cons?. A heuristic that exploits such
patterns achieved a high precision (> 90%) but low
recall (< 5%) in our experiments.
3.3 Bag-of-Opinions Rating Predictor
We leverage the bag-of-opinion (BoO) model of Qu et
al. (2010) as a base predictor for phrase-level ratings.
The BoO model was trained from a large generic
corpus without fine-grained annotations.
In BoO, an opinion consists of three components:
an SO-carrying word (e.g., ?good?), a set of intensi-
fiers (e.g., ?very?) and a set of negators (e.g., ?not?).
Each opinion is scored based on these words (repre-
sented as a boolean vector b) and the polarity of the
SO-carrying word (represented as sgn(r) ? {?1, 1})
as indicated by the MPQA lexicon of Wilson et
al. (2005). In particular, the score is computed as
sgn(r)?Tb, where ? is the learned weight vector.
The sign function sgn(r) ensures consistent weight
assignment for intensifiers and negators. For exam-
ple, an intensifier like ?very? can obtain a large posi-
tive or a large negative weight depending on whether
it is used with a positive or negative SO-carrying
151
word, respectively.
3.4 SO-CAL Rating Predictor
The Semantic Orientation Calculator (SO-CAL) of
Taboada et al2011) also predicts phrase-level rat-
ings via a scoring function similar to the one of BoO.
The SO-CAL predictor uses a manually created lexi-
con, in which each word is classified as either an SO-
carrying word (associated with a numerical score), an
intensifier (associated with a modifier on the numer-
ical score), or a negator. SO-CAL employs various
heuristics to detect irrealis and to correct for the pos-
itive bias inherent in most lexicon-based classifiers.
Compared to BoO, SO-CAL has lower recall but
higher precision.
4 Multi-Experts Model
Our multi-experts model incorporates features from
the individual base predictors, coarse-grained labels
(i.e., document ratings or polarities), similarities be-
tween sentences, and optionally a small amount of
sentence polarity labels into an unified probabilistic
model. We first give an overview of MEM, and then
describe its components in detail.
4.1 Model Overview
Denote by X = {x1, . . . ,xN} a set of sentences.
We associate each sentence xi with a set of initial
labels y?i, which are strong indicators of semantic
orientation: the coarse-grained rating of the corre-
sponding document, the polarity label of our heuristic
polarity predictor, the phrase-level ratings from the
SO-CAL predictor, and optionally a manual polarity
label. Note that the number of initial labels may vary
from sentence to sentence and that initial labels are
heterogeneous in that they refer to either polarities
or ratings. Let Y? = {y?1, . . . , y?N}. Our goal is to
predict the unobserved ratings r = {r1, . . . , rN} of
each sentence.
Our multi-expert model is a probabilistic model
for X, Y?, and r. In particular, we model the rating
vector r via a multi-expert prior PE(r | X,?) with
parameter ? (Sec. 4.2). PE integrates both features
from the base predictors and sentence similarities.
We correlate ratings to initial labels via a set of con-
ditional distributions Pb(y?b | r), where b denotes the
type of initial label (Sec. 4.3). The posterior of r is
then given by
P (r | X, Y?,?) ?
?
b
Pb(y?
b | r)PE(r | X,?).
Note that the posterior is influenced by both the multi-
expert prior and the set of initial labels.
We use MAP inference to obtain the most likely
rating of each sentence, i.e., we solve
argmin
r,?
?
?
b
log(Pb(y?
b | r))? log(PE(r | X,?)),
where as before ? denotes the model parameters. We
solve the above optimization problem using cyclic
coordinate descent (Friedman et al2008).
4.2 Multi-Expert Prior
The multi-expert prior PE(r | X,?) consists of two
component distributions N1 and N2. Distribution
N1 integrates features from the base predictors, N2
incorporates sentence similarities to propagate infor-
mation across sentences.
In a slight abuse of notation, denote by xi the set of
features for the i-th sentence. Vector xi contains the
features of all the base predictors but also includes bi-
gram features for increased coverage of syntactic pat-
terns; see Sec. 4.4 for details about the feature design.
Letm(xi) = ?Txi be a linear predictor for ri, where
? is a real weight vector. Assuming Gaussian noise,
ri follows a Gaussian distribution N1(ri | mi, ?2)
with mean mi = m(xi) and variance ?2. Note that
predictor m can be regarded as a linear combination
of base predictors because both m and each of the
base predictors are linear functions. By integrating
all features into a single function, the base predictors
are trained jointly so that weight vector ? automati-
cally adapts to domain-dependent properties of the
data. This integrated approach significantly outper-
formed the alternative approach of using a weighted
vote of the individual predictions made by the base
predictors. We regularize the weight vector ? us-
ing a Laplace prior P (? | ?) with parameter ? to
encourage sparsity.
Note that the bigram features in xi partially cap-
ture sentence similarity. However, such features can-
not be extended to longer subsequences such as tri-
grams due to data sparsity: useful features become
as infrequent as noisy terms. Moreover, we would
152
like to capture sentence similarity using gapped (i.e.,
non-consecutive) subsequences. For example, the
sentences ?The book is an easy read.? and ?It is easy
to read.? are similar but do not share any consecutive
bigrams. They do share the subsequence ?easy read?,
however. To capture this similarity, we make use of a
novel sentiment-augmented variant of word sequence
kernels (Cancedda et al2003). Our kernel is used
to construct a similarity matrix W among sentences
and the corresponding regularized Laplacian L?. To
capture the intuition that similar sentences should
have similar ratings, we introduce a Gaussian prior
N2(r | 0, L??1) as a component into our multi-expert
prior; see Sec. 4.5 for details and a discussion of
why this prior encourages similar ratings for similar
sentences.
Since the two component distributions feature dif-
ferent expertise, we take their product and obtain the
multi-expert prior
PE(r | X,?) ? N1(r |m, I?
2)N2(r | 0, L??1)P (? | ?),
where m = (m1, . . . ,mN ). Note that the normal-
izing constant of PE can be ignored during MAP
inference since it does not depend on ?.
4.3 Incorporating Initial Labels
Recall that the initial labels Y? are strong indica-
tors of semantic orientation associated with each
sentence; they correspond to either discrete polarity
labels or to continuous rating labels. This hetero-
geneity constitutes the main difficulty for incorporat-
ing the initial labels via the conditional distributions
Pb(y?b | r). We assume independence throughout so
that Pb(y?b | r) =
?
i Pb(y?
b
i | ri).
Rating Labels For continuous labels, we assume
Gaussian noise and set Pb(y?bi | ri) = N (y?
b
i | ri, ?
b
i ),
where variance ?bi is a type- and sentence-dependent.
For SO-CAL labels, we simply set ?SO-CALi =
?SO-CAL, where ?SO-CAL is a hyperparameter. The
SO-CAL scores have limited influence in our overall
model; we found that more complex designs lead to
little improvement. We proceed differently for docu-
ment ratings. Our experiment suggests that document
ratings constitute the most important indicator of the
SO of a sentence. Thus sentence ratings should be
close to document ratings unless strong evidence to
the contrary exists. In other words, we want variance
?Doci to be small.
When no manually created sentence-level polar-
ity labels are available, we set the value of ?Doci de-
pending on the polarity class. In particular, we set
?Doci = 1 for both positive and negative documents,
and ?Doci = 2 for neutral documents. The reasoning
behind this choice is that sentence ratings in neu-
tral documents express higher variance because these
documents often contain a mixture of positive and
negative sentences.
When a small set of manually created sentence
polarity labels is available, we train a classifier that
predicts whether the sentence polarity coincides with
the document polarity. If so, we set the corresponding
variance ?Doci to a small value; otherwise, we choose
a larger value. In particular, we train a logistic regres-
sion classifier (Bishop, 2006) using the following
binary features: (1) an indicator variable for each
document polarity, and (2) an indicator variable for
each triple of base predictor, predicted polarity, and
document polarity (set to 1 if the polarities match).
We then set ?Doci = (?pi)
?1, where pi is the probabil-
ity of matching polarities obtained from the classifier
and ? is a hyperparameter that ensures correct scal-
ing.
Polarity Labels We now describe how to model
the correlation between the polarity of a sentence and
its rating. An simple and effective approach is to
partition the range of ratings into three consecutive
partitions, one for each polarity class. We thus consid-
ering the polarity classes {positive, other, negative}
as ordered and formulate polarity classification as an
ordinal regression problem (Chu and Ghahramani,
2006). We immediately obtain the distribution
Pb(y?
b
i = pos | ri) = ?
(
ri ? b+
?
?b
)
Pb(y?
b
i = oth | ri) = ?
(
b+ ? ri
?
?b
)
? ?
(
b? ? ri
?
?b
)
Pb(y?
b
i = neg | ri) = ?
(
b? ? ri
?
?b
)
,
where b+ and b? are the partition boundaries between
positive/other and other/negative, respectively,2 ?(x)
denotes the cumulative distribution function of the
2We set b+ = 0.3 and b? = ?0.3 to calibrate to SO-CAL,
which treats ratings in [?0.3, 0, 3] as polarity other.
153
Figure 1: Distribution of polarity given rating.
Gaussian distribution, and variance ?b is a hyper-
parameter. It is easy to verify that
?
y?bi?Y
p(y?bi |
ri) = 1. The resulting distribution is shown in Fig. 1.
We can use the same distribution to use MEM for
sentence-level polarity classification; in this case, we
pick the polarity with the highest probability.
4.4 Incorporating Base Predictors
Base predictors are integrated into MEM via compo-
nent N1(ri | mi, ?2) of the multi-expert prior (see
Sec. 4.2). Recall that mi is a linear function of the
features xi of each sentence. In this section, we dis-
cuss how xi is constructed from the features of the
base predictors. New base predictors can be inte-
grated easily by exposing their features to MEM.
Most base predictors operate on the phrase level;
our goal is to construct features for the entire sen-
tence. Denote by nbi the number of phrases in the
i-th sentence covered by base predictor b, and let
obij denote a set of associated features. Features o
b
ij
may or may not correspond directly to the features
of base predictor b; see the discussion below. A
straightforward strategy is to set xbi = (n
b
i)
?1?
j o
b
ij .
We proceed slightly differently and average the fea-
tures associated with phrases of positive prior polar-
ity separately from those of phrases with negative
prior polarity (Taboada et al2011). We then con-
catenate the averaged feature vectors, i.e., we set
xbi = (o?
b,pos
ij o?
b,neg
ij ), where o?
b,p
ij denotes the average
of the feature vectors obij associated with phrases of
prior polarity p. This procedure allows us to learn
a different weight for each feature depending on its
context (e.g., the weight of intensifier ?very? may dif-
fer for positive and negative phrases). We construct
xi by concatenating the sentence-level features xbi of
each base predictor and a feature vector of bigrams.
To integrate a base predictor, we only need to
specify the relevant features and, if applicable, prior
phrase polarities. For our choice of base predictors,
we use the following features:
SO-CAL predictor. The prior polarity of a SO-
CAL phrase is given by the polarity of its SO-
carrying word in the SO-CAL lexicon. The feature
vector oSO-CALij consists of the weight of the SO-
carrying word from the lexicon as well the set of
negator words, irrealis marker words, and intensifier
words in the phrase. Moreover, we add the first two
words preceding the SO-carrying word as context
features (skipping nouns, negators, irrealis markers,
and intensifiers, and stopping at clause boundaries).
All words are encoded as binary indicator features.
BoO predictor. Similar to SO-CAL, we deter-
mine the prior polarity of a phrase based on the BoO
dictionary. In contrast to SO-CAL, we directly use
the BoO score as a feature because the BoO predictor
weights have been trained on a very large corpus and
are thus reliable. We also add irrealis marker words
in the form of indicator features.
Statistical polarity predictor. Recall that the sta-
tistical polarity predictor is based on co-occurrence
counts of opinion-topic pairs and document polar-
ities. We treat each opinion-topic pair as a phrase
and use the most frequently co-occurring polarity
as the phrase?s prior polarity. We use the logarithm
of the co-occurrence counts with positive, negative,
and other polarity as features; this set of features per-
formed better than using the co-occurrence counts or
estimated class probabilities directly. We also add
the same type of context features as for SO-CAL, but
rescale each binary feature by the logarithm of the
occurrence count #z of the opinion-topic pair (i.e.,
the features take values in {0, log #z}).
4.5 Incorporating Sentence Similarities
The component distribution N2(r | 0, L??1) in the
multi-expert prior encourages similar sentences to
have similar ratings. The main purpose of N2 is to
propagate information from sentences on which the
base predictors perform well to sentences for which
base prediction is unreliable or unavailable (e.g., be-
154
cause they do not contain SO-carrying words). To
obtain this distribution, we first construct an N ?N
sentence similarity matrix W using a sentiment-
augmented word sequence kernel (see below). We
then compute the regularized graph Laplacian L? =
L+I/?2 based on the unnormalized graph Laplacian
L = D?W (Chapelle et al2006), where D be a
diagonal matrix with dii =
?
j wij and hyperparam-
eter ?2 controls the scale of sentence ratings.
To gain insight into distribution N2, observe that
N2(r | 0, L??1)
? exp
(
?
1
2
?
i,j
wij(ri ? rj)
2 ? ?r?22/?
2
)
.
The left term in the exponent forces the ratings of
similar sentences to be similar: the larger the sen-
tence similarity wij , the more penalty is paid for dis-
similar ratings. For this reason, N2 has a smoothing
effect. The right term is an L2 regularizer and encour-
ages small ratings; it is controlled by hyperparameter
?2.
The entries wij in the sentence similarity matrix
determine the degree of smoothing for each pair of
sentence ratings. We compute these values by a novel
sentiment-augmented word sequence kernel, which
extends the well-known word sequence kernel of Can-
cedda et al2003) by (1) BoO weights to strengthen
the correlation of sentence similarity and rating sim-
ilarity and (2) synonym resolution based on Word-
Net (Miller, 1995).
In general, a word sequence kernel computes a
similarity score of two sequences based on their
shared subsequences. In more detail, we first de-
fine a score function for a pair of shared subse-
quences, and then sum up these scores to obtain
the overall similarity score. Consider for example
the two sentences ?The book is an easy read.? (s1)
and ?It is easy to read.? (s2) along with the shared
subsequence ?is easy read? (u). Observe that the
words ?an? and ?to? serve as gaps as they are not
part of the subsequence. We represent subsequence
u in sentence s via a real-valued projection function
?u(s). In our example, ?u(s1) = ?is?
g
an?easy?read
and ?u(s2) = ?is?easy?
g
to?read. The decay factors
?w ? (0, 1] for matching words characterize the
importance of a word (large values for significant
words). On the contrary, decay factors ?gw ? (0, 1]
for gap words are penalty terms for mismatches
(small values for significant words). The score of
subsequence u is defined as ?u(s1)?u(s2). Thus
two shared subsequences have high similarity if they
share significant words and few gaps. Following Can-
cedda et al2003), we define the similarity between
two sequences as
kn(si, sj) =
?
u??n
?u(si)?u(sj),
where ? is a finite set of words and n denotes the
length of the considered subsequences. This sim-
ilarity function can be computed efficiently using
dynamic programming.
To apply the word sequence kernel, we need to
specify the decay factors. A traditional choice is
?w = log( NNw )/ log(N), where Nw is the document
frequency of the word w and N is the total number
of documents. This IDF decay factor is not well-
suited to our setting: Important opinion words such
as ?great? have a low IDF value due to their high
document frequency. To overcome this problem,
we incorporate additional weights for SO-carrying
words using the BoO lexicon. To do so, we first
rescale the BoO weights into [0, 1] using the sig-
moid g(w) = (1 + exp(?a?w + b))?1, where ?w
denotes the BoO weight of word w.3 We then set
?w = min(log( NNw )/ log(N) + g(w), 0.9). The de-
cay factor for gaps is given by ?gw = 1 ? ?w. Thus
we strongly penalize gaps that consist of infrequent
words or opinion words.
To address data sparsity, we incorporate synonyms
and hypernyms from WordNet into our kernel. In
particular, we represent words found in WordNet by
their first two synset names (for verbs, adjectives,
nouns) and their direct hypernym (nouns only). Two
words are considered the same when their synsets
overlap. Thus, for example, ?writer? has the same
representation as ?author?.
To build the similarity matrix W, we construct
a k-nearest-neighbor graph for all sentences.4 We
consider subsequences consisting of three words (i.e.,
wij = k3(si, sj)); longer subsequences are overly
sparse, shorter subsequences are covered by the bi-
grams features in N1.
3We set a = 2 and b = 1 in our experiments.
4We use k = 15 and only consider neighbors with a similar-
ity above 0.001.
155
5 Experiments
We evaluated both MEM and a number of alternative
approaches for both sentence-level polarity classifi-
cation and sentence-level strength prediction across
a number of domains. We found that MEM out-
performs state-of-the-art approaches by a significant
margin.
5.1 Experimental Setup
We implemented MEM as well as the HCRF classi-
fier of (Ta?ckstro?m and McDonald, 2011a; Ta?ckstro?m
and McDonald, 2011b), which is the best-performing
estimator of sentence-level polarity in the weakly-
supervised setting reported in the literature. We train
both methods using (1) only coarse labels (MEM-
Coarse, HCRF-Coarse) and (2) additionally a small
number of sentence polarities (MEM-Fine, HCRF-
Fine5). We also implemented a number of baselines
for both polarity classification and strength predic-
tion: a document oracle (DocOracle) that simply uses
the document label for each sentence, the BoO rat-
ing predictor (BaseBoO), and the SO-CAL rating pre-
dictor (BaseSO-CAL). For polarity classification, we
compare our methods also to the statistical polarity
predictor (Basepolarity). To judge on the effectiveness
of our multi-export prior for combining base predic-
tors, we take the majority vote of all base predic-
tors and document polarity as an additional baseline
(Majority-Vote). Similarly, for strength prediction,
we take the arithmetic mean of the document rat-
ing and the phrase-level predictions of BaseBoO and
BaseSO-CAL as a baseline (Mean-Rating). We use the
same hyperparameter setting for MEM across all our
experiments.
We evaluated all methods on Amazon reviews
from different domains using the corpus of Ding et al
(2008) and the test set of Ta?ckstro?m and McDonald
(2011a). For each domain, we constructed a large bal-
anced dataset by randomly sampling 33,000 reviews
from the corpus of Ding et al2008). We chose
the books, electronics, and music domains for our
experiments; the dvd domain was used for develop-
ment. For sentence polarity classification, we use the
test set of Ta?ckstro?m and McDonald (2011a), which
5We used the best-performing model that fuses HCRF-Coarse
and the supervised model (McDonald et al2007) by interpola-
tion.
contains roughly 60 reviews per domain (20 for each
polarity). For strength evaluation, we created a test
set of 300 pairs of sentences per domain from the
polarity test set. Each pair consisted of two sentences
of the same polarity; we manually determined which
of the sentences is more positive. We chose this pair-
wise approach because (1) we wanted the evaluation
to be invariant to the scale of the predicted ratings,
and (2) it much easier for human annotators to rank
a pair of sentences than to rank a large collection of
sentences.
We followed Ta?ckstro?m and McDonald (2011b)
and used 3-fold cross-validation, where each fold
consisted of a set of roughly 20 documents from the
test set. In each fold, we merged the test set with the
reviews from the corresponding domain. For MEM-
Fine and HCRF-Fine, we use the data from the other
two folds as fine-grained polarity annotations. For
our experiments on polarity classification, we con-
verted the predicted ratings of MEM, BaseBoO, and
BaseSO-CAL into polarities by the method described
in Sec. 4.3. We compare the performance of each
method in terms of accuracy, which is defined as the
fraction of correct predictions on the test set (correct
label for polarity / correct ranking for strength). All
reported numbers are averages over the three folds. In
our tables, boldface numbers are statistically signifi-
cant against all other methods (t-test, p-value 0.05).
5.2 Results for Polarity Classification
Table 1 summarizes the results of our experiments for
sentence polarity classification. The base predictors
perform poorly across all domains, mainly due to
the aforementioned problems associated with aver-
aging phrase-level predictions. In fact, DocOracle
performs almost always better than any of the base
predictors. However, accurracy increases when we
combine base predictors and DocOracle using ma-
jority voting, which indicates that ensemble methods
work well.
When no fine-grained annotations are available
(HCRF-Coarse, MEM-Coarse), both MEM-Coarse
and Majority-Vote outperformed HCRF-Coarse,
which in turn has been shown to outperform a num-
ber of lexicon-based methods as well as classifiers
trained on document labels (Ta?ckstro?m and McDon-
ald, 2011a). MEM-Coarse also performs better than
Majority-Vote. This is because MEM propagates
156
Book Electronics Music Avg
Basepolarity 43.7 40.3 43.8 42.6
BaseBoO 50.9 48.9 52.6 50.8
BaseSO-CAL 44.6 50.2 45.0 46.6
DocOracle 51.9 49.6 59.3 53.6
Majority-Vote 53.7 53.4 58.7 55.2
HCRF-Coarse 52.2 53.4 57.2 54.3
MEM-Coarse 54.4 54.9 64.5 57.9
HCRF-Fine 55.9 61.0 58.7 58.5
MEM-Fine 59.7 59.6 63.8 61.0
Table 1: Accuracy of polarity classification per do-
main and averaged across domains.
evidence across similar sentences, which is espe-
cially useful when no explicit SO-carrying words
exist. Also, MEM learns weights of features of base
predictors, which leads to a more adaptive integration,
and our ordinal regression formulation for polarity
prediction allows direct competition among positive
and negative evidence for improved accuracy.
When we incorporate a small amount of sentence
polarity labels (HCRF-Fine, MEM-Fine), the accu-
racy of all models greatly improves. HCRF-Fine has
been shown to outperform the strongest supervised
method on the same dataset (McDonald et al2007;
Ta?ckstro?m and McDonald, 2011b). MEM-Fine falls
short of HCRF-Fine only in the electronics domain
but performs better on all other domains. In the book
and music domains, where MEM-Fine is particularly
effective, many sentences feature complex syntac-
tic structure and SO-carrying words are often used
without reference to the quality of the product (but to
describe contents, e.g., ?a love story? or ?a horrible
accident?).
Our models perform especially well when they are
applied to sentences containing no or few opinion
words from lexicons. Table 2 reports the evaluation
results for both sentences containing SO-carrying
words from either MPQA or SO-CAL lexicons and
for sentences containing no such words. The re-
sults explain why our model falls short of HCRF-
Fine in the electronics domain: reviews of electronic
products contain many SO-carrying words, which
almost always express opinions. Nevertheless, MEM-
Fine handles sentences without explicit SO-carrying
words well across all domains; here the propagation
of information across sentences helps to learn the SO
Book Electronics Music
op fact op fact op fact
HCRF-Fine 55.7 55.9 63.3 54.6 59.0 57.4
MEM-Fine 58.9 62.4 60.7 56.7 64.5 60.8
Table 2: Accuracy of polarity classification for sen-
tences with opinion words (op) and without opinion
words (fact).
of facts (such as ?short battery life?).
We found that for all methods, most of the errors
are caused by misclassifying positive/negative sen-
tences as other and vice versa. Moreover, sentences
with polarity opposite to the document polarity are
hard cases if they do not feature frequent strong pat-
terns. Another difficulty lies in off-topic sentences,
which may contain explicit SO-carrying words but
are not related to the item under review. This is one
of the main reasons for the poor performance of the
lexicon-based methods.
Overall, we found that MEM-Fine is the method of
choice. Thus our multi-expert model can indeed bal-
ance the strength of the individual experts to obtain
better estimation accuracy.
5.3 Results for Strength Prediction
Table 3 shows the accuracy results for strength pre-
diction. Here our models outperformed all baselines
by a large margin. Although document ratings are
strong indicators in the polarity classification task,
they lead to worse performance than lexicon-based
methods. The main reason for this drop in accuracy
is that the document oracle assigns the same rating
to all sentences within a review. Thus DocOracle
cannot rank sentences from the same review, which
is a severe limitation. This shortage can be partly
compensated by averaging the base predictions and
document rating (Mean-Rating). Note that it is non-
trivial to apply existing ensemble methods for the
weights of individual base predictors because of the
absence of the sentence ratings as training labels. In
contrast, our MEM models use indirect supervision
to adaptively assign weights to the features from base
predictors. Similar to polarity classification, a small
amount of sentence polarity labels often improved
the performance of MEM.
157
Book Electronics Music Avg
BaseBoO 58.3 51.6 53.5 54.5
BaseSO-CAL 60.6 57.1 47.6 55.1
DocOracle 45.1 36.2 41.4 40.9
Mean-Rating 70.3 57.0 60.8 62.7
MEM-Coarse 68.7 60.5 69.5 66.2
MEM-Fine 72.4 63.3 67.2 67.6
Table 3: Accuracy of strength prediction.
6 Conclusion
We proposed the Multi-Experts Model for analyz-
ing both opinion polarity and opinion strength at
the sentence level. MEM is weakly supervised; it
can run without any fine-grained annotations but is
also able to leverage such annotations when avail-
able. MEM is driven by a novel multi-expert prior,
which integrates a number of diverse base predictors
and propagates information across sentences using a
sentiment-augmented word sequence kernel. Our ex-
periments indicate that MEM achieves better overall
accuracy than alternative methods.
References
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.
2006. Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled examples.
The Journal of Machine Learning Research, 7:2399?
2434.
Christopher M. Bishop. 2006. Pattern recognition and
machine learning, volume 4. Springer New York.
Nicola Cancedda, E?ric Gaussier, Cyril Goutte, and Jean-
Michel Renders. 2003. Word-sequence kernels. Jour-
nal of Machine Learning Research, 3:1059?1082.
Oliver Chapelle, Bernhard Scho?lkopf, and Alexander Zien.
2006. Semi-Supervised Learning. MIT Press.
Yejin Choi and Claire Cardie. 2009. Adapting a polarity
lexicon using integer linear programming for domain-
specific sentiment classification. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing, volume 2, pages 590?598.
Wei Chu and Zoubin Ghahramani. 2006. Gaussian pro-
cesses for ordinal regression. Journal of Machine
Learning Research, 6(1):1019.
Thomas G. Dietterichl. 2002. Ensemble learning. The
Handbook of Brain Theory and Neural Networks, pages
405?408.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining. In
Proceedings of the International Conference on Web
Search and Data Mining, pages 231?240.
Saso Dzeroski and Bernard Zenko. 2004. Is combining
classifiers with stacking better than selecting the best
one? Machine Learning, 54(3):255?273.
Jerome H. Friedman, Trevor Hastie, and Rob Tibshirani.
2008. Regularization paths for generalized linear mod-
els via coordinate descent. Technical report.
Guohong Fu and Xin Wang. 2010. Chinese sentence-
level sentiment classification based on fuzzy sets. In
Proceedings of the International Conference on Com-
putational Linguistics, pages 312?319. Association for
Computational Linguistics.
Andrew B. Goldberg and Xiaojun Zhu. 2006. Seeing
stars when there aren?t many stars: Graph-based semi-
supervised learning for sentiment categorization. In
HLT-NAACL 2006 Workshop on Textgraphs: Graph-
based Algorithms for Natural Language Processing.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of the ACM
SIGKDD Conference on Knowledge Discovery and
Data Mining, pages 168?177.
Lun-Wei Ku, I-Chien Liu, Chia-Ying Lee, Kuan hua Chen,
and Hsin-Hsi Chen. 2008. Sentence-level opinion anal-
ysis by copeopi in ntcir-7. In Proceedings of NTCIR-7
Workshop Meeting.
Yi Mao and Guy Lebanon. 2006. Isotonic Conditional
Random Fields and Local Sentiment Flow. Advances
in Neural Information Processing Systems, pages 961?
968.
Ryan T. McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeffrey C. Reynar. 2007. Structured models
for fine-to-coarse sentiment analysis. In Proceedings of
the Annual Meeting on Association for Computational
Linguistics, volume 45, page 432.
George A. Miller. 1995. WordNet: a lexical database for
English. Communications of the ACM, 38(11):39?41.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the Annual
Meeting on Association for Computational Linguistics,
pages 271?278.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics, pages 124?131.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 79?86.
158
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009.
Expanding Domain Sentiment Lexicon through Dou-
ble Propagation. In International Joint Conference on
Artificial Intelligence, pages 1199?1204.
Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum. 2010.
The bag-of-opinions method for review rating predic-
tion from sparse text patterns. In Proceedings of the
International Conference on Computational Linguis-
tics, pages 913?921.
Carl Edward Rasmussen. 2004. Gaussian processes in
machine learning. Springer.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly D. Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computational
Linguistics, 37(2):267?307.
Oscar Ta?ckstro?m and Ryan T. McDonald. 2011a. Dis-
covering Fine-Grained Sentiment with Latent Variable
Structured Prediction Models. In Proceedings of the
European Conference on Information Retrieval, pages
368?374.
Oscar Ta?ckstro?m and Ryan T. McDonald. 2011b. Semi-
supervised latent variable models for sentence-level
sentiment analysis. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics,
pages 569?574.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level senti-
ment analysis. In Proceedings of the Human Language
Technology Conference and the Conference on Empir-
ical Methods in Natural Language Processing, pages
347?354.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning
from labeled and unlabeled data with label propagation.
Technical report.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using Gaussian fields
and harmonic functions. In Proceedings of the Inter-
national Conference on Machine Learning, pages 912?
919.
Li Zhuang, Feng Jing, and Xiaoyan Zhu. 2006. Movie
review mining and summarization. In Proceedings of
the ACM international conference on Information and
knowledge management, pages 43?50.
159
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 374?385,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Werdy: Recognition and Disambiguation of Verbs and Verb Phrases
with Syntactic and Semantic Pruning
Luciano Del Corro Rainer Gemulla
Max-Planck-Institut f?ur Informatik
Saarbr?ucken, Germany
{ delcorro, rgemulla, weikum }@mpi-inf.mpg.de
Gerhard Weikum
Abstract
Word-sense recognition and disambigua-
tion (WERD) is the task of identifying
word phrases and their senses in natural
language text. Though it is well under-
stood how to disambiguate noun phrases,
this task is much less studied for verbs
and verbal phrases. We present Werdy,
a framework for WERD with particular
focus on verbs and verbal phrases. Our
framework first identifies multi-word ex-
pressions based on the syntactic structure
of the sentence; this allows us to recog-
nize both contiguous and non-contiguous
phrases. We then generate a list of can-
didate senses for each word or phrase, us-
ing novel syntactic and semantic pruning
techniques. We also construct and lever-
age a new resource of pairs of senses for
verbs and their object arguments. Finally,
we feed the so-obtained candidate senses
into standard word-sense disambiguation
(WSD) methods, and boost their precision
and recall. Our experiments indicate that
Werdy significantly increases the perfor-
mance of existing WSD methods.
1 Introduction
Understanding the semantics of words and multi-
word expressions in natural language text is an
important task for automatic knowledge acquisi-
tion. It serves as a fundamental building block
in a wide area of applications, including semantic
parsing, question answering, paraphrasing, knowl-
edge base construction, etc. In this paper, we
study the task of word-sense recognition and dis-
ambiguation (WERD) with a focus on verbs and
verbal phrases. Verbs are the central element in a
sentence, and the key to understand the relations
between sets of entities expressed in a sentence.
We propose Werdy, a method to (i) automati-
cally recognize in natural language text both sin-
gle words and multi-word phrases that match en-
tries in a lexical knowledge base (KB) like Word-
Net (Fellbaum, 1998), and (ii) disambiguate these
words or phrases by identifying their senses in the
KB. WordNet is a comprehensive lexical resource
for word-sense disambiguation (WSD), covering
nouns, verbs, adjectives, adverbs, and many multi-
word expressions. In the following, the notion of
an entry refers to a word or phrase in the KB,
whereas a sense denotes the lexical synset of the
entry?s meaning in the given sentence.
A key challenge for recognizing KB entries in
natural language text is that entries often consist of
multiple words. In WordNet-3.0 more than 40%
of the entries are multi-word. Such entries are
challenging to recognize accurately for two main
reasons: First, the components of multi-word en-
tries in the KB (such as fiscal year) often consist
of components that are themselves KB entries (fis-
cal and year). Second, multi-word entries (such
as take a breath) may not appear consecutively in
a sentence (?He takes a deep breath.?). Werdy
addresses the latter problem by (conceptually)
matching the syntactic structure of the KB entries
to the syntactic structure of the input sentence.
To address the former problem, Werdy identifies
all possible entries in a sentence and passes them
to the disambiguation phase (take, breath, take a
breath, . . . ); the disambiguation phase provides
more information about which multi-word entries
to keep. Thus, our method solves the recognition
and the disambiguation tasks jointly.
Once KB entries have been identified, Werdy
374
disambiguates each entry against its possible
senses. State-of-the-art methods for WSD (Nav-
igli, 2009) work fairly well for nouns and noun
phrases. However, the disambiguation of verbs
and verbal phrases has received much less atten-
tion in the literature.
WSD methods can be roughly categorized into
(i) methods that are based on supervised training
over sense-annotated corpora (e.g., Zhong and Ng
(2010)), and (ii) methods that harness KB?s to as-
sess the semantic relatedness among word senses
for mapping entries to senses (e.g., Ponzetto and
Navigli (2010)). For these methods, mapping
verbs to senses is a difficult task since verbs tend
to have more senses than nouns. In WordNet, in-
cluding monosemous words, there are on average
1.24 senses per noun and 2.17 per verb.
To disambiguate verbs and verbal phrases,
Werdy proceeds in multiple steps. First, Werdy
obtains the set of candidate senses for each recog-
nized entry from the KB. Second, it reduces the
set of candidate entries using novel syntactic and
semantic pruning techniques. The key insight be-
hind our syntactic pruning is that each verb sense
tends to occur in a only limited number of syn-
tactic patterns. For example, the sentence ?Al-
bert Einstein remained in Princeton? has a sub-
ject (?Albert Einstein?), a verb (?remained?) and
an adverbial (?in Princeton?), it follows an SVA
clause pattern. We can thus safely prune verb
senses that do not match the syntactic structure of
the sentence. Moreover, each verb sense is com-
patible with only a limited number of semantic
argument types (such as location, river, person,
musician, etc); this phenomena is called selec-
tional preference or selectional restriction. Senses
that are compatible only with argument types not
present in the sentence can be pruned. Our prun-
ing steps are based on the idea that a verb selects
the categories of its arguments both syntactically
(c-selection) and semantically (s-selection). In the
final step, Werdy employs a state-of-the-art gen-
eral WSD method to select the most suitable sense
from the remaining candidates. Since incorrect
senses have already been greatly pruned, this step
significantly gains accuracy and efficiency over
standard WSD.
Our semantic pruning technique builds on a
newly created resource of pairs of senses for verbs
and their object arguments. For example, the
WordNet verb sense ?play-1? (i.e., the 1st sense of
the verb entry ?play?) selects as direct object the
noun sense ?sport-1?. We refer to this novel re-
source as the VO Sense Repository, or VOS repos-
itory for short.
1
It is constructed from the Word-
Net gloss-tags corpus, the SemCor dataset, and a
small set of manually created VO sense pairs.
We evaluated Werdy on the SemEval-2007
coarse-grained WSD task (Navigli et al., 2007),
both with and without automatic recognition of en-
tries. We found that our techniques boost state-of-
the-art WSD methods and obtain high-quality re-
sults. Werdy significantly increases the precision
and recall of the best performing baselines.
The rest of the paper is organized as follows.
Section 2 gives an overview of Werdy compo-
nents. Section 3 presents the entry recognition,
and Sections 4 and 5 discuss our novel syntac-
tic and semantic pruning techniques. Section 6
presents the Semantic VO Repository and how we
constructed it. Section 7 gives the results of our
evaluation. Section 8 discusses related work.
2 Overview of Werdy
Werdy consists of four steps: (i) entry recognition,
(ii) syntactic pruning, (iii) semantic pruning, and
(iv) word-sense disambiguation. The novel con-
tribution of this paper is in the first three steps,
and in the construction of the VO sense repository.
Each of these steps operates on the clause level,
i.e., we first determine the set of clauses present
in the input sentence and then process clauses sep-
arately. A clause is a part of a sentence that ex-
presses some statement or coherent piece of infor-
mation. Clauses are thus suitable minimal units
for automatic text understanding tasks (Del Corro
and Gemulla, 2013); see Sec.3 for details.
In the entry-recognition step (Sec. 3), Werdy
obtains for the input sentence a set of potential
KB entries along with their part-of-speech tags.
The candidate senses of each entry are obtained
from WordNet. For instance, in the sentence ?He
takes a deep and long breath?, the set of potential
entries includes take (verb, 44 candidate senses),
take a breath (verb, 1 candidate sense), and breath
(noun, 5 candidate senses). Note that in contrast to
Werdy, most existing word-sense disambiguation
methods assume that entries have already been
(correctly) identified.
1
The VOS repository, Werdy?s source code, and results of
our experimental study are available at http://people.
mpi-inf.mpg.de/
?
corrogg/.
375
In the syntactic-pruning step (Sec. 4), we elim-
inate candidate senses that do not agree with
the syntactic structure of the clause. It is well-
established that the syntactic realization of a
clause is intrinsically related with the sense of
its verb (Quirk et al., 1985; Levin, 1993; Hanks,
1996; Baker et al., 1998; Palmer et al., 2005).
Quirk et al. (1985) identified seven possible clause
types in the English language (such as ?subject
verb adverbial?, SVA). We make use of techniques
inspired by Del Corro and Gemulla (2013) to iden-
tify the clause type of each clause in the sen-
tence. We then match the clause type with the set
of WordNet frames (e.g., ?somebody verb some-
thing?) that WordNet provides for each verb sense,
and prune verb senses for which there is no match.
In the semantic-pruning step (Sec. 5), we fur-
ther prune the set of candidate senses by taking the
semantic types of direct objects into account. Sim-
ilarly to the syntactic relation mentioned above,
a verb sense also imposes a (selectional) restric-
tion on the semantic type of its arguments (Quirk
et al., 1985; Levin, 1993; Hanks, 1996; Baker et
al., 1998; Palmer et al., 2005). For instance, the
verb play with sense participate in games or sports
requires an object argument of type ?game-1?
2
,
?game-3?, or ?sport-1?. Senses that do not match
the arguments found in the clause are pruned.
This step is based on the newly constructed VOS
Repository (Sec. 6). Note that when there is no di-
rect object, only the syntactic pruning step applies.
3 Entry Recognition
The key challenge in recognizing lexical KB en-
tries in text is that entries are not restricted to sin-
gle words. In addition to named entities (such as
people, places, etc.), KB?s contain multi-word ex-
pressions. For example, WordNet-3.0 contains en-
tries such as take place (verb), let down (verb),
take into account (verb), be born (verb), high
school (noun), fiscal year (noun), and Prime Min-
ister (noun). Note that each individual word in a
multi-word entry is usually also an entry by itself,
and can even be part of several multi-word en-
tries. To ensure correct disambiguation, all poten-
tial multi-word entries need to be recognized (Fin-
layson and Kulkarni, 2011), even when they do not
appear as consecutive words in a sentence.
Werdy addresses these challenges by explor-
ing the syntactic structure of both the input sen-
2
We use the notation ?WordNet entry-sense number?.
He takes my hand and a deep breath .
nsubj poss
dobj
cc
det
amod
conj
root
Figure 1: An example dependency parse
tence and the lexical KB entries. The structure
of the sentence is captured in a dependency parse
(DP). Given a word in a sentence, Werdy con-
ceptually generates all subtrees of the DP starting
at that word, and matches them against the KB.
This process can be performed efficiently as Word-
Net entries are short and can be indexed appro-
priately. To match the individual words of a sen-
tence against the words of a KB entry, we follow
the standard approach and perform lemmatization
and stemming (Finlayson, 2014). To further han-
dle personal pronouns and possessives, we follow
Arranz et al. (2005) and normalize personal pro-
nouns (I, you, my, your, . . . ) to one?s, and reflex-
ive pronouns (myself, yourself, . . . ) to oneself.
Consider the example sentence ?He takes my
hand and a deep breath?. We first identify the
clauses and their DP?s (Fig. 1) using the method
of Del Corro and Gemulla (2013), which also
processes coordinating conjunctions. We obtain
clauses ?He takes my hand? and ?He takes a deep
breath?, which we process separately. To obtain
possible entries for the first clause, we start with
its head word (take) and incrementally consider
its descendants (take hand, take one?s hand, . . . ).
The exploration is terminated as early as possible;
for example, we do not consider take one?s hand
because there is no WordNet entry that contains
both take and hand. For the second clause, we
start with take (found in WordNet), then expand
to take breath (not found but can occur together),
then take a breath (found), then take a deep breath
(not found, cannot occur together) and so on.
Note that the word ?take? in the sentence re-
fer to two different entries and senses: ?take? for
the first clause and ?take a breath? for the sec-
ond clause. In this stage no decisions are made
about selecting entries and disambiguating them;
these decisions are made in the final WSD stage
of Werdy.
We tested Werdy?s entry-recognizer on the
SemEval-2007 corpus. We detected the correct en-
376
Pattern Clause type Example WN frame example [frame number]
SV
i
SV AE died. Somebody verb [2]
SV
e
A SVA AE remained in Princeton. Somebody verb PP [22]
SV
c
C SVC AE is smart. Somebody verb adjective [6]
SV
mt
O SVO AE has won the Nobel Prize. Somebody verb something [8]
SV
dt
O
i
O SVOO RSAS gave AE the Nobel Prize. Somebody verb somebody something [14]
SV
ct
OA SVOA The doorman showed AE to his office. Somebody verb somebody PP [20]
SV
ct
OC SVOC AE declared the meeting open. Something verb something adjective/noun [5]
S: Subject, V: Verb, C: Complement, O: Direct object, O
i
: Indirect object, A: Adverbial, V
i
: Intransitive verb, V
c
: Copular verb,
V
c
: Extended-copular verb, V
mt
: Monotransitive verb, V
dt
: Ditransitive verb, V
ct
: Complex-transitive verb
Table 1: Clause types and examples of matching WordNet frames
tries for all but two verbs (out of more than 400).
The two missed entries (take up and get rolling)
resulted from incorrect dependency parses.
4 Syntactic Pruning
Once the KB entries have been recognized, Werdy
prunes the set of possible senses of each verb entry
by considering the syntactic structure of the clause
in which the entry occurs. This pruning is based
on the observation that each verb sense may occur
only in a limited number of ?clause types?, each
having specific semantic functions (Quirk et al.,
1985). When the clause type of the sentence is
incompatible with a candidate sense of an entry,
this sense is eliminated.
Werdy first detects in the input sentence the set
of clauses and their constituents. A clause con-
sists of one subject (S), one verb (V), and option-
ally an indirect object (O), a direct object (O), a
complement (C) and one or more adverbials (A).
Not all combinations of clause constituents ap-
pear in the English language. When we classify
clauses according to the grammatical function of
their constituents, we obtain only seven different
clause types (Quirk et al., 1985); see Table 1. For
example, the sentence ?He takes my hand? is of
type SVO; here ?He? is the subject, ?takes? the
verb, and ?my hand? the object. The clause type
can (in principle) be determined by observing the
verb type and its complementation (Del Corro and
Gemulla, 2013).
For instance, consider the SVA clause ?The stu-
dent remained in Princeton?. The verb remain has
four senses in WN: (1) stay the same; remain in
a certain state (e.g., ?The dress remained wet?),
(2) continue in a place, position, or situation (?He
remained dean for another year?), (3) be left; of
persons, questions, problems (?There remains the
question of who pulled the trigger?) or (4) stay be-
hind (?The hostility remained long after they made
up?). The first sense of remain requires an SVC
pattern; the other cases require either SV or SVA.
Our example clause is of type SVA so that we can
safely prune the first sense.
WordNet provides an important resource for ob-
taining the set of clause types that are compatible
with each sense of a verb. In particular, each verb
sense in WordNet is annotated with a set of frames
(e.g., ?somebody verb something?) in which they
may occur, capturing both syntactic and semantic
constraints. There are 35 different frames in to-
tal.
3
We manually assigned a set of clause types to
each frame (e.g., SVO to frame ?somebody verb
something?). Table 1 shows an example frame for
each of the seven clause types. On average, each
WordNet-3.0 verb sense is associated with 1.57
frames; the maximum number of frames per sense
is 9. The distribution of frames is highly skewed:
More than 61% of the 21,649 frame annotations
belong to one of four simple SVO frames (num-
bers 8, 9, 10 and 11), and 22 out of the 35 frames
have less than 100 instances. This skew makes
the syntactic pruning step effective for non-SVO
clauses, but less effective for SVO clauses.
Werdy directly determines a set of possible
frame types for each clause of the input sentence.
Our approach is based on the clause-type detection
method of Del Corro and Gemulla (2013), but we
also consider additional information that is cap-
tured in frames but not in clause types. For ex-
ample, we distinguish different realizations of ob-
jects (such as clausal objects from non-clausal ob-
jects), which are not captured in the clause type.
Given the DP of a clause, Werdy identifies the
3
See http://wordnet.princeton.edu/
wordnet/man/wninput.5WN.html.
377
Clause
Object?
Q
1
Complement?
Adverbial?
Q
2
Q
3
Frames
4,6,7
Frames
1-3
Frames
1,2,12,13,22,27
No
Yes
No
Yes
No
Dir. and in-
direct object?
Complement?
that-clause?
infinitive/
to-infinitive?
Adverbial?
Q
7
Q
8
Q
9
Q
10
Q
11
Frames
14,15
Frame
5
Frames
26,34
Frames
24,28,29,32,35
Frames
1,2,8-11,33
Frames
1,2,8-11,15-21, 30, 31,33
Yes No
Yes
No
Yes
No
Yes
Yes
No
No
Yes
Figure 2: Flow chart for frame detection
set of WN frames that can potentially match the
clause as outlined in the flowchart of Fig. 2. Werdy
walks through the flowchart; for each question,
we check for the presence or absence of a specific
constituent of a clause (e.g., a direct object for Q
1
)
and proceed appropriately until we obtain a set of
possible frames. This set is further reduced by
considering additional information in the frames
(not shown; e.g., that the verb must end on ?-ing?).
For our example clause ?The student remained
in Princeton?, we first identify possible frames
{ 1, 2, 12, 13, 22, 27 } using the flowchart (Q
1
no,
Q
2
no, Q
3
yes); using the additional information
in the frames, Werdy then further prunes this set
to { 1, 2, 22 }. The corresponding set of remaining
candidate sense for remain is as given above, i.e.,
{ ?remain-2?, ?remain-3?, ?remain-4? }.
Our mapping of clause types to WordNet frames
is judiciously designed for the way WordNet is or-
ganized. For instance, frames containing adver-
bials generally do not specify whether or not the
adverbial is obligatory; here we are conservative
in that we do not prune such frames if the input
clause does not contain an adverbial. As another
example, some frames overlap or subsume each
other; e.g, frame ?somebody verb something? (8)
subsumes ?somebody verb that clause? (26). In
some word senses annotated with the more general
frame, the more specific one can also apply (e.g.,
?point out-1? is annotated with 8 but not 26; 26
can apply), in others it does not (e.g., ?play-1? is
also annotated with 8 but not 26; but here 26 can-
not apply). To ensure the effectiveness of syntactic
pruning, we only consider the frames that are di-
rectly specified in WordNet. This procedure often
produces the desired results; in a few cases, how-
ever, we do prune the correct sense (e.g., frame 26
for clause ?He points out that . . . ?).
5 Semantic Pruning
A verb sense imposes a restriction on the semantic
type of the arguments it may take and vice versa
(Quirk et al., 1985; Levin, 1993; Hanks, 1996;
Baker et al., 1998; Palmer et al., 2005; Kipper et
al., 2008). This allows us to further prune the verb
candidate set by discarding verb senses whose se-
mantic argument is not present in the clause.
WordNet frames potentially allow a shallow
type pruning based on the semantics provided for
the clause constituents. However we could solely
distinguish people (?somebody?) from things
(?something?), which is too crude to obtain sub-
stantial pruning effects. Moreover, this distinction
is sometimes ambiguous.
Instead, we have developed a more powerful
approach to semantic pruning based on our VOS
repository. We remove from the verb candidate set
those senses whose semantic argument cannot be
present in the sentence. For instance, consider the
clause ?The man plays football.? Suppose that we
know that the verb entry play with sense ?play-
1? (?participate in sports?) takes an object of type
?sport-1?; i.e., we have a tuple ?play-1, sport-1?
in our repository. Then, we check whether any
of the possible senses of football?(i) sport or (ii)
ball?is of type ?sport-1?. Here the first sense has
the correct type (the second sense does not); thus
we retain ?play-1? as a possible sense for the verb
entry play. Next, suppose that we consider sense
?play-3? (?play on an instrument?), which accord-
ing to our corpus takes ?instrument-6? as argument
(i.e., there is a tuple ?play-3, instrument-6? in our
VOS repository). Since none of the senses of foot-
ball is of type ?instrument-6?, we can safely drop
378
?play-3? from our candidate set. We perform this
procedure for every verb sense in the candidate set.
Semantic pruning makes use of both VOS
repository and the hypernym structure of the noun
senses in WordNet. For each sentence, we obtain
the possible senses of the direct-object argument
of the verb. We then consider each candidate sense
of the verb (e.g., ?play-1?), and check whether any
of its compatible object-argument senses (from
our repository) is a hypernym of any of the possi-
ble senses of its actual object argument (in the sen-
tence); e.g., ?sport-1? is a hypernym of ?football-
1?. If so, we retain the verb?s candidate sense. If
not, either the candidate sense of the verb is in-
deed incompatible with the object argument in the
sentence, or our repository is incomplete. To han-
dle incompleteness to some extent, we also con-
sider hyponyms of the object-argument senses in
our repository; e.g., if we observe object sport in a
sentence and have verb-sense argument ?football-
1? in our corpus, we consider this a match. If the
hyponyms lead to a match, we retain the verb?s
candidate sense; otherwise, we discard it.
6 Verb-Object Sense Repository
We use three different methods to construct the
repository. In particular, we harness the sense-
annotated WordNet glosses
4
as well as the sense-
annotated SemCor corpus (Landes et al., 1998).
5
The major part of the VOS repository was ac-
quired from WordNet?s gloss tags. According
to Atkins and Rundell (2008), noun definitions
should be expressed in terms of the class to which
they belong, and verb definitions should refer to
the types of the subjects or objects related to the
action. Based on this rationale, we extracted all
noun senses that appear in the gloss of each verb
sense; each of these noun senses is treated as a
possible sense of the object argument of the cor-
responding verb sense. For example, the gloss of
?play-1? is ?participate in games or sports;? each
noun is annotated with its senses (2 and 3 for
?games?, 1 for ?sports?). We extract tuples ?play-
1, game-2?, ?play-1, game-3?, and ?play-1, sport-
1? from this gloss. Note that we only extract
direct-object arguments, i.e., we do not consider
the type of the subject argument of a verb sense.
Since the constituents of the predicate are much
4
http://wordnet.princeton.edu/
glosstag.shtml
5
http://web.eecs.umich.edu/
?
mihalcea/
downloads.html
more important than the subject to determine or
describe a verb sense, lexical resources rarely con-
tain information on the subject (Atkins and Run-
dell, 2008). Similarly, WordNet glosses typically
do not provide any information about adverbials.
Overall, we collected arguments for 8,657 verb
senses (out of WordNet?s 13,767 verb senses) and
a total of 13,050 ?verb-#, object-#?-pairs.
We leveraged the sense-annotated SemCor cor-
pus to further extend our VOS repository. We
parsed each sentence in the corpus to obtain
the respective pairs of verb sense and object
sense. Since sentences are often more specific
than glosses, and thus less helpful for construct-
ing our repository, we generalized the so-found
object senses using a heuristic method. In particu-
lar, we first obtained all the object senses of each
verb sense, and then repeatedly generalized sets of
at least two senses that share a direct hypernym
to this hypernym. The rationale is that we only
want to generalize if we have some evidence that
a more general sense may apply; we thus require
at least two hyponyms before we generalize. Us-
ing this method, we collected arguments for 1,516
verb senses and a total of 4,131 sense pairs.
Finally, we noticed that the most frequent
senses used in the English language are usually
so general that their glosses do not contain any
relevant semantic argument. For instance, one of
the most frequent verbs is ?see-1?, which has gloss
?perceive by ?sight-3??. The correct semantic ar-
gument ?entity-1? is so general that it is omitted
from the gloss. In fact, our gloss-tag extractor
generates tuple ?see-1, sight-3?, which is incorrect.
We thus manually annotated the 30 most frequent
verb senses with their object argument types.
Our final repository contains arguments for
9,335 verb senses and a total of 17,181 pairs. Pairs
from SemCor tend to be more specific because
they refer to text occurrences. The assumption of
taking the nouns of the glosses as arguments seems
to be mostly correct, although some errors may
be introduced. Consider the pair ?play-28, stream-
2? extracted from the gloss ?discharge or direct
or be discharged or directed as if in a continu-
ous ?stream-2??. Also, in some cases, the glosses
may refer to adverbials as in ?play-14, location-1?,
taken from gloss ?perform on a certain ?location-
1??. Note that if an argument is missing from our
repository, we may prune the correct sense of the
verb. If, however, there is an additional, incorrect
379
argument in the repository, the correct verb sense
is retained but pruning may be less effective.
7 Evaluation
Dataset. We tested Werdy on the SemEval-2007
coarse-grained dataset.
6
It consists of five sense-
annotated documents; the sense annotations refer
to a coarse-grained version of WordNet. In addi-
tion to sense annotations, the corpus also provides
the corresponding KB entries (henceforth termed
?gold entries?) as well as a POS tag. We restrict
our evaluation to verbs that act as clause heads. In
total, 461 such verbs were recognized by ClausIE
(Del Corro and Gemulla, 2013) and the Stanford
Parser (Klein and Manning, 2003).
7
WSD Algorithms. For the final step of Werdy,
we used the KB-based WSD algorithms of
Ponzetto and Navigli (2010) and It-Makes-
Sense (Zhong and Ng, 2010), a state-of-the-art
supervised system that was the best performer in
SemEval-2007. Each method only labels entries
for which it is sufficiently confident.
Simplified Extended Lesk (SimpleExtLesk). A
version of Lesk (1986). Each entry is assigned the
sense with highest term overlap between the en-
try?s context (words in the sentence) and both the
sense?s gloss (Kilgarriff and Rosenzweig, 2000)
as well as the glosses of its neighbors (Baner-
jee and Pedersen, 2003). A sense is output only
if the overlap exceeds some threshold; we used
thresholds in the range of 1?20 in our experi-
ments. There are many subtleties and details
in the implementation of SimpleExtLesk so we
used two different libraries: a Java implementation
of WordNet::Similarity (Pedersen et al., 2004),
8
which we modified to accept a context string, and
DKPro-WSD (Miller et al., 2013) version 1.1.0,
with lemmatization, removal of stop words, paired
overlap enabled and normalization disabled.
Degree Centrality. Proposed by Navigli and La-
pata (2010). The method collects all paths con-
necting each candidate sense of an entry to the set
of candidate senses of the words the entry?s con-
text. The candidate sense with the highest degree
in the resulting subgraph is selected. We imple-
mented this algorithm using the Neo4j library.
9
6
The data is annotated with WordNet 2.1 senses; we
converted the annotations to WordNet-3.0 using DKPro-
WSD (Miller et al., 2013).
7
Version 3.3.1, model englishRNN.ser.gz
8
http://www.sussex.ac.uk/Users/drh21/
9
http://www.neo4j.org/
We used a fixed threshold of 1 and vary the search
depth in range 1?20. We used the candidate senses
of all nouns and verbs in a sentence as context.
It-Makes-Sense (IMS). A state-of-the-art, pub-
licly available supervised system (Zhong and Ng,
2010) and a refined version of Chan et al. (2007),
which ranked first in the SemEval-2007 coarse
grained task. We modified the code to accept KB
entries and their candidate senses. We tested both
in WordNet-2.1 and 3.0; for the later we mapped
Werdy?s set of candidates to WordNet-2.1.
Most Frequent Sense (MFS). Selects the most
frequent sense (according to WordNet frequen-
cies) among the set of candidate senses of an en-
try. If there is a tie, we do not label. Note that
this procedure differs slightly from the standard of
picking the entry with the smallest sense id. We
do not follow this approach since it cannot handle
well overlapping entries.
MFS back-off. When one of the above meth-
ods fails to provide a sense label (or provides more
than one), we used the MFS method above with a
threshold of 1. This procedure increased the per-
formance in all cases.
Methodology. The disambiguation was per-
formed with respect to coarse-grained sense clus-
ters. The score of a cluster is the sum of the indi-
vidual scores of its senses (except for IMS which
provides only one answer per word); the cluster
with the highest score was selected. Our source
code and the results of our evaluation are publicly
available
10
.
The SemEval-2007 task was not designed for
automatic entry recognition, for each word or
multi-word expression it provides the WordNet
entry and the POS tag. We proceeded as follows
to handle multi-word entries. In the WSD step, we
considered the candidate senses of all recognized
entries that overlap with the gold entry. For exam-
ple, we considered the candidate senses of entries
take, breath, and take a breath for gold entry take
a breath.
The SemEval-2007 task uses WordNet-2.1 but
Werdy uses WordNet-3.0. We mapped both the
sense keys and clusters from WordNet-2.1 to
WordNet-3.0. All senses in WordNet-3.0 that
could not be mapped to any cluster were consider
to belong each of them to a single sense cluster.
Note that this procedure is fair: for such senses
10
http://people.mpi-inf.mpg.de/
?
corrogg/
380
Algorithm Gold Pruning MFS threshold Verbs (clause heads) F1
Entry back-off /depth P R F1 points
Degree + - + 5 73.54 73.54 73.54
Centrality + + + 11 79.61 79.61 79.61 + 6.07
+ - - 5 73.99 71.58 72.77
+ + - 8 79.91 78.52 79.21 + 6.44
- - + 5 70.41 70.41 70.41
- + + 10 76.46 76.46 76.46 + 6.05
- - - 4 71.05 68.90 69.96
- + - 10 76.81 75.81 76.30 + 6.34
SimpleExtLesk + - + 6 77.28 75.27 76.26
(DKPro) + + + 5 81.90 80.48 81.18 + 4.92
+ - - 1 73.70 52.28 61.17
+ + - 1 81.99 64.21 72.02 + 10.85
- - + 5 74.33 72.57 73.44
- + + 5 79.30 77.75 78.52 + 5.08
- - - 1 69.85 50.54 58.65
- + - 1 78.69 62.20 69.48 + 10.83
SimpleExtLesk + - + 5 77.11 75.27 76.18
(WordNet::Sim) + + + 5 80.57 79.18 79.87 + 3.69
+ - - 1 74.82 68.98 71.78
+ + - 1 79.04 75.27 77.11 + 5.33
- - + 6 74.12 72.35 73.22
- + + 7 77.97 76.46 77.21 + 3.99
- - - 1 71.36 65.66 68.39
- + - 1 76.20 71.92 74.00 + 5.61
MFS + - - 1 76.61 74.62 75.60
+ + - 1 80.35 78.96 79.65 + 4.05
- - - 1 73.67 71.92 72.79
- + - 1 77.75 76.24 76.99 + 4.20
IMS + - + n.a. 79.60 79.60 79.60
(WordNet-2.1) + + + n.a. 80.04 80.04 80.04 + 0.44
- - + n.a. 76.21 75.05 75.63
- + + n.a. 77.53 76.36 76.94 + 1.31
IMS + - + n.a. 78.96 78.96 78.96
(WordNet-3.0) + + + n.a. 79.83 79.83 79.83 + 0.87
- - + n.a. 75.77 74.62 75.19
- + + n.a. 77.53 76.36 76.94 + 1.75
Table 2: Results on SemEval-2007 coarse-grained (verbs as clause heads)
the disambiguation is equivalent to a fine-grained
disambiguation, which is harder.
Results. Our results are displayed in Table 2.
We ran each algorithm with the gold KB entries
provided by in the dataset (+ in column ?gold en-
try) as well as the entries obtained by our method
of Sec. 3 (-). We also enabled (+) and disabled
(-) the pruning steps as well as the MFS back-off
strategy. The highest F1 score was achieved by
SimpleExtLesk (DKPro) with pruning and MFS
back-off: 81.18 with gold entries and 78.52 with
automatic entry recognition. In all cases, our syn-
tactic and semantic pruning strategy increased per-
formance (up to +10.85 F1 points). We next dis-
cuss the impact of the various steps of Werdy in
detail.
Detailed Analysis. Table 3 displays step-by-
step results for DKPro?s SimpleExtLesk, for MFS,
as well as SimpleExtLesk with MFS back-off, the
best performing strategy. The table shows results
when only some Werdy?s steps are used. We start
from a direct use of the respective algorithm with
the gold entries of SemEval-2007 after each hor-
izontal line, and then successively add the Werdy
steps indicated in the table.
When no gold entries were provided, perfor-
mance dropped due to the increase of sense can-
didates for multi-word expressions, which include
the possible senses of the expression itself as well
as the senses of the entry?s parts that are them-
381
Steps Performed threshold P R F1 F1 points
SimpleExtLesk (DKPro)
Plain with gold entries 1 73.70 52.28 61.17
+ Entry Recognition 1 69.85 50.54 58.65 - 2.52
+ Syntactic Pruning 1 76.47 58.84 66.50 + 7.85
+ Semantic Pruning 1 78.69 62.20 69.48 + 2.98
+ Entry Recognition 1 69.85 50.54 58.65 - 2.52
+ Semantic Pruning 1 73.85 55.39 63.30 + 4.65
+ Syntactic Pruning 1 79.33 61.21 69.10 + 7.93
+ Semantic Pruning 1 81.99 64.21 72.02 + 2.92
+ Semantic Pruning 1 78.11 56.90 65.84 + 4.67
MFS
Plain with gold entries 1 76.61 74.62 75.60
+ Entry Recognition 1 73.67 71.92 72.79 - 2.81
+ Syntactic Pruning 1 75.77 74.14 74.95 + 2.16
+ Semantic Pruning 1 77.75 76.24 76.99 + 2.04
+ Entry Recognition 1 73.67 71.92 72.79 - 2.81
+ Semantic Pruning 1 77.09 75.43 76.25 + 3.46
+ Syntactic Pruning 1 78.46 76.94 77.69 + 2.09
+ Semantic Pruning 1 80.35 78.96 79.65 + 1.96
+ Semantic Pruning 1 79.91 78.02 78.95 + 3.35
SimpleExtLesk (DKPro) with MFS back-off
Plain with gold entries 6 77.28 75.27 76.26
+ Entry Recognition 6 74.33 72.57 73.44 - 2.82
+ Syntactic Pruning 5 76.65 75.00 75.82 + 2.38
+ Semantic Pruning 5 79.30 77.75 78.52 + 2.70
+ Entry Recognition 5 74.33 72.57 73.44 - 2.82
+ Semantic Pruning 5 78.19 76.51 77.34 +3.90
+ Syntactic Pruning 5 79.34 77.80 78.56 + 2.30
+ Semantic Pruning 5 81.90 80.48 81.18 + 2.62
+ Semantic Pruning 5 81.02 79.09 80.04 + 3.78
Table 3: Step-by-step results
selves WordNet entries. Our entry recognizer
tends to do a good job since it managed to cor-
rectly identify all the relevant entries except in two
cases (i.e. ?take up? and ?get rolling?), in which
the dependency parse was incorrect. The drop in
F1 for our automatic entry recognition was mainly
due to incorrect selection of the correct entry of a
set of alternative, overlapping entries.
Syntactic pruning did not prune the correct
sense in most cases. In 16 cases (with gold en-
tries), however, the correct sense was pruned. Five
of these senses were pruned due to incorrect de-
pendency parses, which led to incorrect frame
identification. In two cases, the sense was not
annotated with the recognized frame in WordNet,
although it seemed adequate. In the remaining
cases, a general frame from WordNet was incor-
rectly omitted. Improvements to WordNet?s frame
annotations may thus make syntactic pruning even
more effective.
Semantic pruning also improves performance.
Here the correct sense was pruned for 11 verbs,
mainly due to the noisiness and incompleteness
of our VOS repository. Without using gold en-
tries, we found in total 237 semantic matches be-
tween possible verbs senses and possible object
senses (200 with gold entries). We also found that
our manual annotations in the VOS repository (see
Sec. 6) did not affect our experiments.
The results show that syntactic and semantic
pruning are beneficial for verb sense disambigua-
tion, but also stress the necessity to improve ex-
isting resources. Ideally, each verb sense would
be annotated with both the possible clause types
or syntactic patterns in which it can occur as well
as the possible senses of its objects. Annotations
for subjects and adverbial arguments may also be
beneficial.
382
8 Related Work
WSD is a classification task where for every word
there is a set of possible senses given by some ex-
ternal resource (as a KB). Two types of methods
can be distinguished in WSD. Supervised systems
(Dang and Palmer, 2005; Dligach and Palmer,
2008; Chen and Palmer, 2009; Zhong and Ng,
2010) use a classifier to assign senses to words,
mostly relying on manually annotated data for
training. In principle, these systems suffer from
low coverage since the training data is usually
sparse. Some authors have tried to overcome this
limitation by exploiting linked resources as train-
ing data (Shen et al., 2013; Cholakov et al., 2014).
The second WSD approach corresponds to the
so-called KB methods (Agirre and Soroa, 2009;
Ponzetto and Navigli, 2010; Miller et al., 2012;
Agirre et al., 2014). They rely on a back-
ground KB (typically WordNet or extended ver-
sions (Navigli and Ponzetto, 2012)), where related
senses appear close to each other. KB-based al-
gorithms often differ in the way the KB is ex-
plored. It has been shown that a key point to en-
hance performance is the amount of semantic in-
formation in the KB (Ponzetto and Navigli, 2010;
Miller et al., 2012). Our framework fits this line of
work since it is also unsupervised and enriches the
background knowledge in order to enhance perfor-
mance of standard WSD algorithms. A compre-
hensive overview of WSD systems can be found
in Navigli (2009) and Navigli (2012).
To bring WSD to real-world applications, the
mapping between text and KB entries is a funda-
mental first step. It has been pointed that the ex-
istence of multi-word expressions imposes multi-
ple challenges to text understanding tasks (Sag et
al., 2002). The problem has been addressed by
Arranz et al. (2005) and Finlayson and Kulkarni
(2011). They find multi-word entries by match-
ing word sequences allowing some morphological
and POS variations according to predefined pat-
terns. Our method differs in that we can recognize
KB entries that appear discontinuously and in that
we do not select the correct entry but generate a
set of potential entries.
Linguists have noted the link between verb
senses and the syntactic structure and argument
types (Quirk et al., 1985; Levin, 1993; Hanks,
1996), and supervised WSD systems were devel-
oped to capture this relation (Dang and Palmer,
2005; Chen and Palmer, 2009; Dligach and
Palmer, 2008; Cholakov et al., 2014). In Dang
and Palmer (2005) and Chen and Palmer (2009),
it is shown that WSD tasks can be improved with
features that capture the syntactic structure and in-
formation about verb arguments and their types.
They use features as shallow named entity recog-
nition and the hypernyms of the possible senses
of the noun arguments. Dang and Palmer (2005)
also included features extracted from PropBank
(Palmer et al., 2005) from role labels and frames.
Dligach and Palmer (2008) generated a corpus of
verb and their arguments (both surface forms),
which was used to incorporate a semantic feature
to the supervised system.
In our work, we also incorporate syntactic and
semantic information. Instead of learning the re-
lation between the verb senses and the syntactic
structure, however we incorporate it explicitly us-
ing the WordNet frames, which provide informa-
tion about which verb sense should be consider
for a given syntactic pattern. We also incorporate
explicitly the semantic relation between each verb
sense and its arguments using our VOS repository.
Different resources of semantic arguments for
automatic text understanding tasks have been con-
structed (Baker et al., 1998; Palmer et al., 2005;
Kipper et al., 2008; Gurevych et al., 2012; Nakas-
hole et al., 2012; Flati and Navigli, 2013). In
(Baker et al., 1998; Palmer et al., 2005; Kipper
et al., 2008; Gurevych et al., 2012), the classifica-
tion of verbs and arguments is focused toward se-
mantic or thematic roles. Nakashole et al. (2012)
uses semantic types to construct a taxonomy of bi-
nary relations and Flati and Navigli (2013) col-
lected semantic arguments for given textual ex-
pressions. For instance, given the verb ?break?,
they extract a pattern ?break ?body part-1??. In
contrast to existing resources, our VOS repository
disambiguates both the verb sense and the senses
of its arguments.
9 Conclusion
We presented Werdy, a framework for word-sense
recognition and disambiguation with a particular
focus on verbs and verbal phrases. Our results
indicate that incorporating syntactic and seman-
tic constraints improves the performance of verb
sense disambiguation methods. This stresses the
necessity of extending and improving the available
syntactic and semantic resources, such as Word-
Net or our VOS repository.
383
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Pro-
ceedings of EACL, pages 33?41.
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2014. Random walks for knowledge-based word
sense disambiguation. Computational Linguistics,
40(1):57?84.
Victoria Arranz, Jordi Atserias, and Mauro Castillo.
2005. Multiwords and word sense disambiguation.
In Computational Linguistics and Intelligent Text
Processing, volume 3406 of Lecture Notes in Com-
puter Science, pages 250?262.
B. T. Sue Atkins and Michael Rundell. 2008. The Ox-
ford Guide to Practical Lexicography. Oxford Uni-
versity Press.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of ACL, pages 86?90.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of IJCAI, pages 805?810.
Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007.
Nus-pt: Exploiting parallel texts for word sense dis-
ambiguation in the english all-words tasks. In Pro-
ceedings of SemEval, pages 253?256.
Jinying Chen and Martha Palmer. 2009. Improving
english verb sense disambiguation performance with
linguistically motivated features and clear sense dis-
tinction boundaries. Language Resources and Eval-
uation, 43(2):181?208.
Kostadin Cholakov, Judith Eckle-Kohler, and Iryna
Gurevych. 2014. Automated verb sense labelling
based on linked lexical resources. In Proceedings of
EACL, pages 68?77.
Hoa Trang Dang and Martha Palmer. 2005. The role
of semantic roles in disambiguating verb senses. In
Proceedings of ACL, pages 42?49.
Luciano Del Corro and Rainer Gemulla. 2013.
Clausie: clause-based open information extraction.
In Proceedings of WWW, pages 355?366.
Dmitriy Dligach and Martha Palmer. 2008. Improv-
ing verb sense disambiguation with automatically
retrieved semantic knowledge. In Proceedings of
ICSC, pages 182?189.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Mark Alan Finlayson and Nidhi Kulkarni. 2011. De-
tecting multi-word expressions improves word sense
disambiguation. In Proceedings of MWE, pages 20?
24.
Mark Alan Finlayson. 2014. Java libraries for access-
ing the princeton wordnet: Comparison and evalua-
tion. In Proceedings of GWC.
Tiziano Flati and Roberto Navigli. 2013. Spred:
Large-scale harvesting of semantic predicates. In
Proceedings of ACL, pages 1222?1232.
Iryna Gurevych, Judith Eckle-Kohler, Silvana Hart-
mann, Michael Matuschek, Christian M. Meyer, and
Christian Wirth. 2012. Uby - a large-scale unified
lexical-semantic resource based on lmf. In Proceed-
ings of EACL, pages 580?590.
Patrick Hanks. 1996. Contextual dependency and lex-
ical sets. International Journal of Corpus Linguis-
tics, 1(1):75?98.
Adam Kilgarriff and Joseph Rosenzweig. 2000.
Framework and results for english senseval. Com-
puters and the Humanities, 34(1-2):15?48.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classification
of English verbs. Language Resources and Evalua-
tion, 42(1):21?40.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL,
pages 423?430.
Shari Landes, Claudia Leacock, and Randee I. Tengi,
1998. Building Semantic Concordances. MIT
Press.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a
pine cone from an ice cream cone. In Proceedings
of SIGDOC, pages 24?26.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press.
Tristan Miller, Chris Biemann, Torsten Zesch, and
Iryna Gurevych. 2012. Using distributional similar-
ity for lexical expansion in knowledge-based word
sense disambiguation. In Proceedings of COLING,
pages 1781?1796.
Tristan Miller, Nicolai Erbs, Hans-Peter Zorn, Torsten
Zesch, and Iryna Gurevych. 2013. Dkpro wsd: A
generalized uima-based framework for word sense
disambiguation. In Proceedings of ACL: System
Demonstrations, pages 37?42.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: A taxonomy of relational
patterns with semantic types. In Proceedings of
EMNLP, pages 1135?1145.
Roberto Navigli and Mirella Lapata. 2010. An ex-
perimental study of graph connectivity for unsuper-
vised word sense disambiguation. EEE Transac-
tions on Pattern Analysis and Machine Intelligence,
32(4):678?692.
384
Roberto Navigli and Simone Paolo Ponzetto. 2012.
Babelnet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193(0):217?
250.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-
grained english all-words task. In Proceedings of
SemEval, pages 30?35.
Roberto Navigli. 2009. Word sense disambiguation:
A survey. ACM Computing Surveys, 41(2):10:1?
10:69.
Roberto Navigli. 2012. A quick tour of word sense dis-
ambiguation, induction and related approaches. In
Proceedings of SOFSEM, pages 115?129.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: Measuring the
relatedness of concepts. In Proceedings of HLT-
NAACL: Demonstration Papers, pages 38?41.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proceedings of ACL, pages
1522?1531.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for nlp. In Proceed-
ings of CICLing, pages 1?15.
Hui Shen, Razvan Bunescu, and Rada Mihalcea. 2013.
Coarse to fine grained sense disambiguation in
wikipedia. In Proceedings of *SEM, pages 22?31.
Zhi Zhong and Hwee Tou Ng. 2010. It makes sense:
A wide-coverage word sense disambiguation sys-
tem for free text. In Proceedings of ACL: System
Demonstrations, pages 78?83.
385
Senti-LSSVM: Sentiment-Oriented Multi-Relation Extraction
with Latent Structural SVM
Lizhen Qu
Max Planck Institute
for Informatics
lqu@mpi-inf.mpg.de
Yi Zhang
Nuance Communications
yi.zhang@nuance.com
Rui Wang
DFKI GmbH
mars198356@hotmail.com
Lili Jiang
Max Planck Institute
for Informatics
ljiang@mpi-inf.mpg.de
Rainer Gemulla
Max Planck Institute
for Informatics
rgemulla@mpi-inf.mpg.de
Gerhard Weikum
Max Planck Institute
for Informatics
weikum@mpi-inf.mpg.de
Abstract
Extracting instances of sentiment-oriented re-
lations from user-generated web documents is
important for online marketing analysis. Un-
like previous work, we formulate this extrac-
tion task as a structured prediction problem
and design the corresponding inference as an
integer linear program. Our latent structural
SVM based model can learn from training cor-
pora that do not contain explicit annotations of
sentiment-bearing expressions, and it can si-
multaneously recognize instances of both bi-
nary (polarity) and ternary (comparative) re-
lations with regard to entity mentions of in-
terest. The empirical evaluation shows that
our approach significantly outperforms state-
of-the-art systems across domains (cameras
and movies) and across genres (reviews and
forum posts). The gold standard corpus that
we built will also be a valuable resource for
the community.
1 Introduction
Sentiment-oriented relation extraction (Choi et al.,
2006) is concerned with recognizing sentiment po-
larities and comparative relations between entities
from natural language text. Identifying such rela-
tions often requires syntactic and semantic analysis
at both sentence and phrase level. Most prior work
on sentiment analysis consider either i) subjective
sentence detection (Yu and K?bler, 2011), ii) po-
larity classification (Johansson and Moschitti, 2011;
Wilson et al., 2005), or iii) comparative relation
identification (Jindal and Liu, 2006; Ganapathib-
hotla and Liu, 2008). In practice, however, differ-
ent types of sentiment-oriented relations frequently
coexist in documents. In particular, we found that
more than 38% of the sentences in our test corpus
contain more than one type of relations. The iso-
lated analysis approach is inappropriate because i) it
sacrifices acuracy by ignoring the intricate interplay
among different types of relations; ii) it could lead to
conflicting predictions such as estimating a relation
candidate as both negative and comparative. There-
fore, in this paper, we identify instances of both sen-
timent polarities and comparative relations for enti-
ties of interest simultaneously. We assume that all
the mentions of entities and attributes are given, and
entities are disambiguated. It is a widely used as-
sumption when evaluating a module in a pipeline
system that the outputs of preceding modules are
error-free.
To the best of our knowledge, the only exist-
ing system capable of extracting both comparisons
and sentiment polarities is a rule-based system pro-
posed by Ding et al. (2009). We argue that it is
better to tackle the task by using a unified model
with structured outputs. It allows us to consider a
set of correlated relation instances jointly and char-
acterize their interaction through a set of soft and
hard constraints. For example, we can encode con-
straints to discourage an attribute to participate in
a polarity relation and a comparative relation at the
same time. As a result, the system extracts a set of
correlated instances of sentiment-oriented relations
from a given sentence. For example, with the sen-
tence about the camera Canon 7D, ?The sensor is
great, but the price is higher than Nikon D7000.?
the expected output is positive(Canon 7D, sensor)
155
Transactions of the Association for Computational Linguistics, 2 (2014) 155?168. Action Editor: Janyce Wiebe.
Submitted 6/2013; Revised 11/2013; Published 4/2014. c?2014 Association for Computational Linguistics.
and preferred(Nikon D7000, Canon 7D, textit-
price).
However, constructing a fully annotated train-
ing corpus for this task is labor-intensive and re-
quires strong linguistic background. We minimize
this overhead by applying a simplified annotation
scheme, in which annotators mark mentions of en-
tities and attributes, disambiguate the entities, and
label instances of relations for each sentence. Based
on the new scheme, we have created a small Senti-
ment Relation Graph (SRG) corpus for the domains
of cameras and movies, which significantly differs
from the corpora used in prior work (Wei and Gulla,
2010; Kessler et al., 2010; Toprak et al., 2010;
Wiebe et al., 2005; Hu and Liu, 2004) in the follow-
ing ways: i) both sentiment polarities and compar-
ative relations are annotated; ii) all mentioned en-
tities are disambiguated; and iii) no subjective ex-
pressions are annotated, unless they are part of entity
mentions.
The new annotation scheme raises a new chal-
lenge for learning algorithms in that they need to
automatically find textual evidences for each anno-
tated relation during training. For example, with the
sentence ?I like the Rebel a little better, but that is
another price jump?, simply assigning a sentiment-
bearing expression to the nearest relation candidate
is insufficient, especially when the sentiment is not
explicitly expressed.
In this paper, we propose SENTI-LSSVM, a latent
structural SVM based model for sentiment-oriented
relation extraction. SENTI-LSSVM is applied to find
the most likely set of the relation instances expressed
in a given sentence, where the latent variables are
used to assign the most appropriate textual evidences
to the respective instances.
In summary, the contributions of this paper are the
following:
? We propose SENTI-LSSVM: the first unified sta-
tistical model with the capability of extracting
instances of both binary and ternary sentiment-
oriented relations.
? We design a task-specific integer linear pro-
gramming (ILP) formulation for inference.
? We construct a new SRG corpus as a valuable
asset for the evaluation of sentiment relation
extraction.
? We conduct extensive experiments with on-
line reviews and forum posts, showing that
SENTI-LSSVM model can effectively learn from
a training corpus without explicitly annotated
subjective expressions and that its performance
significantly outperforms state-of-the-art sys-
tems.
2 Related Work
There are ample works on analyzing sentiment po-
larities and entity comparisons, but the majority of
them studied the two tasks in isolation.
Most prior approaches for fine-grained sentiment
analysis focus on polarity classification. Super-
vised approaches on expression-level analysis re-
quire the annotation of sentiment-bearing expres-
sions as training data (Jin et al., 2009; Choi
and Cardie, 2010; Johansson and Moschitti, 2011;
Yessenalina and Cardie, 2011; Wei and Gulla,
2010). However, the corresponding annotation pro-
cess is time-consuming. Although sentence-level
annotations are easier to obtain, the analysis at this
level cannot cope with sentences conveying relations
of multiple types (McDonald et al., 2007; T?ckstr?m
and McDonald, 2011; Socher et al., 2012). Lexicon-
based approaches require no training data (Ku et al.,
2006; Kim and Hovy, 2006; Godbole et al., 2007;
Ding et al., 2008; Popescu and Etzioni, 2005; Liu et
al., 2005) but suffer from inferior performance (Wil-
son et al., 2005; Qu et al., 2012). In contrast, our
method requires no annotation of sentiment-bearing
expressions for training and can predict both senti-
ment polarities and comparative relations.
Sentiment-oriented comparative relations have
been studied in the context of user-generated dis-
course (Jindal and Liu, 2006; Ganapathibhotla and
Liu, 2008). Approaches rely on linguistically moti-
vated rules and assume the existence of independent
keywords in sentences which indicate comparative
relations. Therefore, these methods fall short of ex-
tracting comparative relations based on domain de-
pendent information.
Both Johansson and Moschitti (2011) and Wu et
al. (2011) formulate fine-grained sentiment analy-
sis as a learning problem with structured outputs.
However, they focus only on polarity classification
156
of expressions and require annotation of sentiment-
bearing expressions for training as well.
While ILP has been previously applied for infer-
ence in sentiment analysis (Choi and Cardie, 2009;
Somasundaran and Wiebe, 2009; Wu et al., 2011),
our task requires a complete ILP reformulation due
to 1) the absence of annotated sentiment expressions
and 2) the constraints imposed by the joint extrac-
tion of both sentiment polarity and comparative re-
lations.
3 System Overview
This section gives an overview of the whole system
for extracting sentiment-oriented relation instances.
Prior to presenting the system architecture, we in-
troduce the essential concepts and the definitions of
two kinds of directed hypergraphs as the represen-
tation of correlated relation instances extracted from
sentences.
3.1 Concepts and Definitions
Entity. An entity is an abstract or concrete thing,
which needs not be of material existence. An entity
in this paper refers to either a product or a brand.
Attribute. An attribute is an object closely associ-
ated with or belonging to an entity, such as the lens
of digital camera.
Sentiment-Oriented Relation. A sentiment-
oriented relation is either a sentiment polarity or a
comparative relation, defined on tuples of entities
and attributes. A sentiment polarity relation conveys
either a positive or a negative attitude towards enti-
ties or their attributes, whereas a comparative rela-
tion indicates the preference of one entity over the
other entity w.r.t. an attribute.
Relation Instance. An instance of sentiment polar-
ity takes the form r(entity, attribute) with r ? {pos-
itive, negative}, such as positive(Canon 7D, sen-
sor). The polarity instances expressed in the form
of unary relations, such as ?Nikon D7000 is ex-
cellent.?, are denoted as binary relations r(entity,
whole), where the attribute whole indicates the en-
tity as a whole. In contrast, an instance of compar-
ative relation is in the form of preferred{entity, en-
tity, attribute}, e.g. preferred(Canon 7D, Nikon
D7000, price). For brevity, we refer to an instance
set of sentiment-oriented relations extracted from a
sentence as an sSoR. To represent the instances
of the remaining relations, we represent them as
other{entity, attribute}, such as textitpartOf{wheel,
car}. These relations include objective relations
and the subjective relations other than sentiment-
oriented relations.
Mention-Based Relation Instances. A mention-
based relation instance refers to a tuple of entity
mentions with a certain relation. This concept is in-
troduced as the representation of instances in a sen-
tence by replacing entities with the corresponding
entity mentions, such as positive(?Canon SD880i?,
?wide angle view?).
Figure 1: An example of MRG.
Mention-Based Relation Graph. A mention-based
relation graph (or MRG ) represents a collection of
mention-based relation instances expressed in a sen-
tence. As illustrated in Figure 1, an MRG is a di-
rected hypergraph G = ?M,E? with a vertex set
M and an edge set E. A vertex mi ? M denotes
a mention of an entity or an attribute occurring ei-
ther within the sentence or in its context. We say
that a mention is from the context if it is mentioned
in the previous sentence or is an attribute implied
in the current sentence. An instance of a binary re-
lation in an MRG takes the form of a binary edge
el = (mi,ma), where mi and ma denote an en-
tity mention and an attribute mention respectively,
and the type l ? {positive, negative, other}. A
ternary edge el indicating comparative relation is
represented as el = (mi,mj ,ma), where two en-
tity mentions mi and mj are compared with respect
to the attribute mention ma. We define the type
l ? {better,worse} to indicate two possible direc-
tions of the relation and assume mi occurs before
mj . As a result, we have a set L of five relation
types: positive, negative, better, worse or other. Ac-
cording to these definitions, the annotations in the
SRG corpus are actually MRGs and disambiguated
entities. If there are multiple mentions referring to
the same entity, annotators are asked to choose the
157
most obvious one because it saves annotation time
and is less demanding for the entity recognition and
diambiguation modules.
Figure 2: An example of eMRG. The textual evi-
dences are wrapped by green dashed boxes.
Evidentiary Mention-Based Relation Graph. An
evidentiary mention-based relation graph, coined
eMRG , extends an MRG by associating each edge
with a textual evidence to support the corresponding
relation assertions (see Figure 2). Consequently, an
edge in an eMRG is denoted by a pair (a, c), where
a represents a mention-based relation instance and
c is the associated textual evidence. It is also re-
ferred to as an evidentiary edge. represented as
el = (mi,mj ,ma), an MRG as an evidentiary MRG
(eMRG) and the edges of eMRGs as evidentiary
edges, as shown in Figure 2.
3.2 System Architecture
Figure 3: System architecture.
As illustrated by Figure 3, at the core of our sys-
tem is the SENTI-LSSVM model, which extracts sets
of mention-based relationships in the form of eMRGs
from sentences. For a given sentence with known
entity mentions, we select all possible mention sets
as relation candidates, where each set includes at
least one entity mention. Then we associate each
relation candidate with a set of constituents or the
whole sentence as the textual evidence candidates
(cf. Section 6.1). Subsequently, the inference com-
ponent aims to find the most likely eMRG from all
possible combinations of mention-based relation in-
stances and their textual evidences (cf. Section 6.2).
The representation eMRG is chosen because it char-
acterizes exactly the model outputs by letting each
edge correspond to an instance of mention-based re-
lation and the associated textual evidence. Finally,
the model parameters of this model are learned by
an online algorithm (cf. Section 7).
Since instance sets of sentiment-oriented relations
(sSoRs) are the expected outputs, we can obtain
sSoRs from MRGs by using a simple rule-based al-
gorithm. The algorithm essentially maps the men-
tions from an MRG into entities and attributes in an
sSoR and label the corresponding tuples with the re-
lation types of the edges from an MRG. For instances
of comparative relation, the label better or worse is
mapped to the relation type preferred.
4 SENTI-LSSVM Model
The task of sentiment-oriented relation extraction
is to determine the most likely sSoR in a sentence.
Since sSoRs are derived from the corresponding
MRGs as described in Section 3, the task is reduced
to find the most likely MRG for each sentence. Since
an MRG is created by assigning relation types to a
subset of all relation candidates, which are possible
tuples of mentions with unknown relation types, the
number of MRGs can be extremely high.
To tackle the task, one solution is to employ
an edge-factored linear model in the framework of
structural SVM (Martins et al., 2009; Tsochantaridis
et al., 2004). The model suggests that a bag of fea-
tures should be specified for each relation candidate,
and then the model predicts the most likely candi-
date sets along with their relation types to form the
optimal MRGs. As we observed, for a relation can-
didate, the most informative features are the words
near its entity mentions in the original text. How-
158
ever, if we represent a candidate by all these words,
it is very likely that the instances of different relation
types share overly similar features, because a men-
tion is often involved in more than one relation can-
didate, as shown in Figure 2. As a consequence, the
instances of different relations represented by overly
similar features can easily confuse the learning algo-
rithm. Thus, it is critical to select proper constituents
or sentences as textual evidences for each relation
candidate in both training and testing.
Consequently, we divide the task of sentiment-
oriented relation extraction into two subtasks : i)
identifying the most likely MRGs; ii) assigning
proper textual evidences to each edge of MRGs to
support their relation assertions. It is desirable to
carry out the two subtasks jointly as these two sub-
tasks could enhance each other. First, the identifi-
cation of relation types requires proper textual ev-
idences; second, the soft and hard constraints im-
posed by the correlated relation instances facilitate
the recognition of the corresponding textual evi-
dences. Since the eMRGs are created by attaching
every MRG with a set of textual evidences, tackling
the two subtasks simultaneously is equivalent to se-
lecting the most likely eMRG from a set of eMRG
candidates. It is challenging because our SRG corpus
does not contain any annotation of textual evidences.
Formally, let X denote the set of all available sen-
tences, and we define y ? Y(x)(x ? X ) as the set
of labeled edges of an MRG and Y = ?x?XY(x).
Since the assignments of textual evidences are not
observed, an assignment of evidences to y is de-
noted by a latent variable h ? H(x) and H =
?x?XH(x). Then (y, h) corresponds to an eMRG,
and (a, c) ? (y, h) is a labeled edge a attached
with a textual evidence c. Given a labeled dataset
D = {(x1, y1), ..., (xn, yn)} ? (X ? Y)n, we aim
to learn a discriminant function f : X ? Y?H that
outputs the optimal eMRG (y, h) ? Y(x)?H(x) for
a given sentence x.
Due to the introduction of latent variables, we
adopt the latent structural SVM (Yu and Joachims,
2009) for structural classification. Our discriminant
function is defined as
f(x) = argmax(y,h)?Y(x)?H(x)?>?(x, y, h) (1)
where ?(x, y, h) is the feature function of an eMRG
(y, h) and ? is the corresponding weight vector.
To ensure tractability, we also employ edge-based
factorization for our model. Let Mp denote a set of
entity mentions and yr(mi) be a set of edges labeled
with sentiment-oriented relations incident to mi, the
factorization of ?(x, y, h) is given as
?(x, y, h) =
?
(a,c)?(y,h)
?e(x, a, c) + (2)
?
mi?Mp
?
a,a??yr(mi),a 6=a?
?c(a, a?)
where ?e(x, a, c) is a local edge feature function
for a labeled edge a attached with a textual evidence
c and ?c(a, a?) is a feature function capturing co-
occurrence of two labeled edges ami and a?mi inci-dent to an entity mention mi.
5 Feature Space
The following features are used in the feature func-
tions (Equation 2):
Unigrams: As mentioned before, a textual evi-
dence attached to an edge in MRG is either a word,
phrase or sentence. We consider all lemmatized un-
igrams in the textual evidence as unigram features.
Context: Since web users usually express related
sentiments about the same entity across sentence
boundaries, we describe the sentiment flow using a
set of contextual binary features. For example, if en-
tity A is mentioned in both the previous sentence and
the current sentence, a set of contextual binary fea-
tures are used to indicate all possible combinations
of the current and the previous mentioned sentiment-
oriented relations regarding to entity A.
Co-occurrence: We have mentioned the co-
occurrence feature in Equation 2, indicated by
?c(a, a?). It captures the co-occurrence of two la-
beled edges incident to the same entity mention.
Note that the co-occurrence feature function is con-
sidered only if there is a contrast conjunction such as
?but? between the non-shared entity mentions inci-
dent to the two labeled edges.
Senti-predictors: Following the idea of (Qu et
al., 2012), we encode the prediction results from
the rule-based phrase-level multi-relation predic-
tor (Ding et al., 2009) and from the bag-of-opinions
predictor (Qu et al., 2010) as features based on the
textual evidence. The output of the first predictor
is an integer value, while the output of the second
predictor is a sentiment relation, such as ?positive?,
159
?negative?, ?better? or ?worse?. We map the rela-
tional outputs into integer values and then encode
the outputs from both predictors as senti-predictor
features.
Others: The commonly used part-of-speech tags
are also included as features. Moreover, for an edge
candidate, a set of binary features are used to denote
the types of the edge and its entity mentions. For in-
stance, a binary feature indicates whether an edge is
a binary edge related to an entity mentioned in con-
text. To characterize the syntactic dependencies be-
tween two adjacent entity mentions, we use the path
in the dependency tree between the heads of the cor-
responding constituents, the number of words and
other mentions in-between as features. Additionally,
if the textual evidence is a constituent, its feature
w.r.t. an edge is the dependency path to the clos-
est mention of the edge that does not overlap with
this constituent.
6 Structural Inference
In order to find the best eMRG for a given sentence
with a well trained model, we need to determine
the most likely relation type for each relation candi-
date and support the corresponding assertions with
proper textual evidences. We formulate this task
as an Integer Linear Programming (ILP). Instead of
considering all constituents of a sentence, we empir-
ically select a subset as textual evidences for each
relation candidate.
6.1 Textual Evidence Candidates Selection
Textual evidences are selected based on the con-
stituent trees of sentences parsed by the Stanford
parser (Klein and Manning, 2003). For each men-
tion in a sentence, we first locate a constituent in
the tree with the maximal overlap by Jaccard sim-
ilarity. Starting from this constituent, we consider
two types of candidates: type I candidates are con-
stituents at the highest level which contain neither
any word of another mention nor any contrast con-
junctions such as ?but?; type II candidates are con-
stituents at the highest level which cover exactly two
mentions of an edge and do not overlap with any
other mentions. For a binary edge connecting an en-
tity mention and an attribute mention, we consider
a type I candidate starting from the attribute men-
tion. For a binary edge connecting two entity men-
tions, we consider type I candidates starting from
both mentions. Moreover, for a comparative ternary
edge, we consider both type I and type II candidates
starting from the attribute mention. This strategy is
based on our observation that these candidates of-
ten cover the most important information w.r.t. the
covered entity mentions.
6.2 ILP Formulation
We formulate the inference problem of finding the
best eMRG as an ILP problem due to its convenient
integration of both soft and hard constraints.
Given the model parameters ?, we reformulate
the score of an eMRG in the discriminant function
(1) as follows,
?>?(x, y, h) =
?
(a,c)?(y,h)
saczac +
?
mi?Mp
?
a,a??yr(mi),a 6=a?
saa?zaa?
where sac = ?>?e(x, a, c) denotes the score of a
labeled edge a attached with a textual evidence c,
saa? = ?>?c(a, a?) is the edge co-occurrence score,
the binary variable zac indicates the presence or ab-
sence of the corresponding edge, and zaa? indicates
if two edges co-occurr. As not every edge set can
form an eMRG, we require that a valid eMRG should
satisfy a set of linear constraints, which form our
constraint space. Then function (1) is equivalent to
max
z?B s
>z + ?zd
s.t. A
?
?
z
?
?
?
? ? d
z,?, ? ? B
where B = 2S with S = {0, 1}, and ? and ? are
auxiliary binary variables that help define the con-
straint space. The above optimization problem takes
exactly the form of an ILP because both the con-
straints and the objective function are linear, and all
variables take only integer values.
In the following, we consider two types of con-
straint space, 1) an eMRG with only binary edges and
2) an eMRG with both binary and ternary edges.
160
eMRG with only Binary Edges: An eMRG has
only binary edges if a sentence contains no attribute
mention or at most one entity mention. We expect
that each edge has only one relation type and is sup-
ported by a single textual evidence. To facilitate the
formulation of constraints, we introduce ?el to de-
note the presence or absence of a labeled edge el,
and ?ec to indicate if a textual evidence c is assigned
to an unlabeled edge e. Then the binary variable for
the corresponding evidentiary edge zelc = ?ec ? ?el ,
where the ILP formulation of conjunction can be
found in (Martins et al., 2009).
Let Ce denote the set of textual evidence candi-
dates of an unlabeled edge e. The constraint of at
most one textual evidence per edge is formulated as:
?
c?Ce
?ec ? 1 (3)
Once a textual evidence is assigned to an edge,
their relation labels should match and the number
of labeled edges must agree with the number of at-
tached textual evidences. Further, we assume that a
textual evidence c conveys at most one relation so
that an evidence will not be assigned to the relations
of different types, which is the main problem for the
structural SVM based model. Let ?cl indicate that
the textual evidence c is labeled by the relation type
l. The corresponding constraints are expressed as,
?
l?Le
?el =
?
c?Ce
?ec; zelc ? ?cl;
?
l?L
?cl ? 1
where Le denotes the set of all possible labels for
an unlabeled edge e, and L is the set of all relation
types of MRGs (cf. Section 3).
In order to avoid a textual evidence being overly
reused by multiple relation candidates, we first pe-
nalize the assignment of a textual evidence c to a
labeled edge a by associating the corresponding zac
with a fixed negative cost ?? in the objective func-
tion. Then the selection of one textual evidence per
edge a is encouraged by associating ? to zdc in the
objective function, where zdc =
?
e?Sc ?ec and Sc isthe set of edges that the textual evidence c serves as
a candidate. The disjunction zdc is expressed as:
zdc ? ?e, e ? Sc
zdc ?
?
e?Sc
?e
(a) Binary edge structure
(b) Ternary edge structure
Figure 4: Alternative structures associated with an
attribute mention.
This soft constraint not only encourages one textual
evidence per edge, but also keeps it eligible for mul-
tiple assignments.
For any two labeled edge a and a? incident
to the same entity mention, the edge-to-edge co-
occurrence is described by zca,a? = za ? za? .
eMRG with both Binary and Ternary Edges: If
there are more than one entity mentions and at least
one attribute mention in a sentence, an eMRG can
potentially have both binary and ternary edges. In
this case, we assume that each mention of attributes
can participate either in binary relations or in ternary
relations. The assumption holds in more than 99.9%
of the sentences in our SRG corpus, thus we describe
it as a set of hard constraints. Geometrically, the as-
sumption can be visualized as the selection between
two alternative structures incident to the same at-
tribute mention, as shown in Figure 4. Note that,
in the binary edge structure, we include not only the
edges incident to the attribute mention but also the
edge between the two entity mentions.
Let Sbmi be the set of all possible labeled edgesin a binary edge structure of an attribute mention
mi. Variable ? bmi =
?
el?Sbmi
?el indicates whether
the attribute mention is associated with a binary
edge structure or not. In the same manner, we use
? tmi =
?
el?Stmi
?el to indicate the association of the
an attribute mention mi with an ternary edge struc-
ture from the set of all incident ternary edges Stmi .The selection between two alternative structures is
161
formulated as ? bmi + ? tmi = 1. As this influencesonly the edges incident to an attribute mention, we
keep all the constraints introduced in the previous
section unchanged except for constraint (3), which
is modified as
?
c?Ce
?ec ? ? bmi ;
?
c?Ce
?ec ? ? tmi
Therefore, we can have either binary edges or
ternary edges for an attribute mention.
7 Learning Model Parameters
Given a set of training sentences D =
{(x1, y1), . . . , (xn, yn)}, the best weight vec-
tor ? of the discriminant function (1) is found by
solving the following optimization problem:
min
?
1
n
n?
i=1
[ max
(y?,h?)?Y(x)?H(x)
(?>?(x, y?, h?)+?(h?, y?, y))
? max
h??H(x)
?>?(x, y, h?)] + ?|?|] (4)
where ?(h?, y?, y) is a loss function measuring the dis-
crepancies between an eMRG (y, h?) with gold stan-
dard edge labels y and an eMRG (y?, h?) with inferred
labeled edges y? and textual evidences h?. Due to the
sparse nature of the lexical features, we apply L1
regularizer to the weight vector ?, and the degree of
sparsity is controlled by the hyperparameter ?.
Since the L1 norm in the above optimization
problem is not differentiable at zero, we apply the
online forward-backward splitting (FOBOS) algo-
rithm (Duchi and Singer, 2009). It requires two steps
for updating the weight vector ? by using a single
training sentence x on each iteration t.
?t+ 12 = ?t ? ?t?t
?t+1 = arg min
?
1
2?? ? ?t?
2 + ?t?|?|
where ?t is the subgradient computed without con-
sidering the L1 norm and ?t is the learning rate.
For a labeled sentence x, ?t = ?(x, y??, h??) ?
?(x, y, h??), where the feature functions of the corre-
sponding eMRGs are inferred by solving (y??, h??) =
arg max(h?,y?)?H(x)?Y(x)[?
>?(x, y?, h?) + ?(h?, y?, y)]
and (y, h??) = arg maxh??H(x) ?>?(x, y, h?), as in-
dicated in the optimization problem (4).
The former inference problem is similar to the
one we considered in the previous section except
the inclusion of the loss function. We incorporate
the loss function into the ILP formulation by defin-
ing the loss between an MRG (y, h) and a gold stan-
dard MRG as the sum of per-edge costs. In our ex-
periments, we consider a positive cost ? for each
wrongly labeled edge a, so that if an edge a has a
different label from the gold standard, we add ? to
the coefficient sac of the corresponding variable zac
in the objective function of the ILP formulation.
In addition, since the non-positive weights of edge
labels in the initial learning phrase often lead to
eMRGs with many unlabeled edges, which harms the
learning performance, we fix it by adding a con-
straint for the minimal number of labeled edges in
an eMRG, ?
a?A
?
c?Ca
?ac ? ? (5)
where A is the set of all labeled edge candidates and
? denotes the minimal number of labeled edges.
Empirically, the best way to determine ? is to
make it equal to the maximal number of labeled
edges in an eMRG with the restriction that a tex-
tual evidence can be assigned to at most one edge.
By considering all the edge candidates A and all the
textual evidence candidates C as two vertex sets in a
bipartite graph G? = ?V = (A,C), E? (with edges in
E indicating which textual evidence can be assigned
to which edge), ? corresponds to exactly the size of
a maximum matching of the bipartite graph1.
To find the optimal eMRG (y, h??), for the gold la-
bel k of each edge, we consider the following set of
constraints for inference since the labels of the edges
are known for the training data,
?
c?Ce
?ec ? 1; ?ec ? lck
?
k??L
lck? ? 1;
?
e?Sc
?ec ? 1
We include also the soft constraints, which avoid
a textual evidence being overly reused by multiple
relations, and the constraints similar to (5) to ensure
a minimal number of labeled edges and a minimal
number of sentiment-oriented relations.
1It is computed by the Hopcroft-Karp algorithm (Hopcroft
and Karp, 1973) in our implementation.
162
8 SRG Corpus
For evaluation we constructed the SRG corpus,
which in total consists of 1686 manually annotated
online reviews and forum posts in the digital camera
and movie domains2. For each domain, we maintain
a set of attributes and a list of entity names.
The annotation scheme for the sentiment repre-
sentation asserts minimal linguistic knowledge from
our annotators. By focusing on the meanings of the
sentences, the annotators make decisions based on
their language intuition, not restricted by specific
syntactic structures. Taking the example in Figure
2, the annotators only need to mark the mentions of
entities and attributes from both the sentences and
the context, disambiguate them, and label (?Canon
7D?, ?Nikon D7000?, price) as worse and (?Canon
7D?, ?sensor?) as positive, whereas in prior work,
people have annotated the sentiment-bearing expres-
sions such as ?great? and link them to the respective
relation instances as well. This also enables them
to annotate instances of both sentiment polarity and
comparative relaton, which are conveyed by not only
explicit sentiment-bearing expressions like ?excel-
lent performance?, but also factual expressions im-
plying evaluations such as ?The 7V has 10x optical
zoom and the 9V has 16x.?.
Camera Movie
Reviews Forums Reviews Forums
positive 386 1539 879 905
negative 165 363 529 331
comparison 30 480 39 35
Table 1: Distribution of relation instances in SRG corpus.
14 annotators participated in the annotation
project. After a short training period, annotators
worked on randomly assigned documents one at a
time. For product reviews, the system lists all rel-
evant information about the entity and the prede-
fined attributes. For forum posts, the system shows
only the attribute list. For each sentence in a doc-
ument, the annotator first determines if it refers to
an entity of interest. If not, the sentence is marked
2The 107 camera reviews are from bestbuy.com and Ama-
zon.com; the 667 camera forum posts are downloaded from fo-
rum.digitalcamerareview.com; the 138 movie reviews and 774
forum posts are from imdb.com and boards.ie respectively
as off-topic. Otherwise, the annotator will identify
the most obvious mentions, disambiguate them, and
mark the MRGs. We evaluate the inter-annotator
agreement on sSoRs in terms of Cohen?s Kappa
(?) (Cohen, 1968). An average Kappa value of 0.698
was achieved on a randomly selected set consisting
of 412 sentences.
Table 1 shows the corpus distribution after nor-
malizing them into sSoRs. Camera forum posts con-
tain the largest proportion of comparisons because
they are mainly about the recommendation of dig-
ital cameras. In contrast, web users are much less
interested in comparing movies, in both reviews and
forums. In all subsets, positive relations play a dom-
inant role since web users intend to express more
positive attitudes online than negative ones (Pang
and Lee, 2007).
9 Experiments
This section describes the empirical evaluation of
SENTI-LSSVM together with two competitive base-
lines on the SRG corpus.
9.1 Experimental Setup
We implemented a rule-based baseline (DING-
RULE) and a structural SVM (Tsochantaridis et
al., 2004) baseline (SENTI-SSVM) for comparison.
The former system extends the work of Ding et
al. (2009), which designed several linguistically-
motivated rules based on a sentiment polarity lexi-
con for relation identification and assumes there is
only one type of sentiment relation in a sentence. In
our implementation, we keep all the rules of (Ding et
al., 2009) and add one phrase-level rule when there
are more than one mention in a sentence. The ad-
ditional rule assigns sentiment-bearing words and
negators to its nearest relation candidates based on
the absolute surface distance between the words and
the corresponding mentions. In this case, the phrase-
level sentiment-oriented relations depend only on
the assigned sentiment words and negators. The lat-
ter system is based on a structural SVM and does
not consider the assignment of textual evidences to
relation instances during inference. The textual fea-
tures of a relation candidate are all lexical and sen-
timent predictor features within a surface distance
of four words from the mentions of the candidate.
163
Thus, this baseline does not need the inference con-
straints of SENTI-LSSVM for the selection of textual
evidences. To gain more insights into the model,
we also evaluate the contribution of individual fea-
tures of SENTI-LSSVM. In addition, to show if identi-
fying sentiment polarities and comparative relations
jointly works better than tackling each task on its
own, we train SENTI-LSSVM for each task separately
and combine their predictions according to compat-
ibility rules and the corresponding graph scores.
For each domain and text genre, we withheld 15%
documents for development and use the remaining
for cross validation. The hyperparameters of all sys-
tems are tuned on the development datasets. For all
experiments of SENTI-LSSVM, we use ? = 0.0001
for the L1 regularizer in Eq.(4) and ? = 0.05 for
the loss function; and for SENTI-SSVM, ? = 0.0001
and ? = 0.01. Since the relation type of off-topic
sentences is certainly other, we evaluate all systems
with 5-fold cross-validation only on the on-topic
sentences in the evaluation dataset. Since the same
sSoR can have several equivalent MRGs and the rela-
tion type other is not of our interest, we evaluate the
sSoRs in terms of precision, recall and F-measure.
All reported numbers are averages over the 5 folds.
9.2 Results
Table 2 shows the complete results of all sys-
tems. Here our model SENTI-LSSVM outperformed
all baselines in terms of the average F-measure
scores and recalls by a large margin. The F-measure
on movie reviews is about 14% over the best base-
line. The rule-based system has higher precision
than recall in most cases. However, simply increas-
ing the coverage of the domain independent senti-
ment polarity lexicon might lead to worse perfor-
mance (Taboada et al., 2011) because many sen-
timent oriented relations are conveyed by domain
dependent expressions and factual expressions im-
plying evaluations, such as ?This camera does not
have manual control.? Compared to DING-RULE,
SENTI-SSVM performs better in the camera domain
but worse for the movies due to many misclassi-
fication of negative relation instances as other. It
also wrongly predicted more positive instances as
other than SENTI-LSSVM. We found that the recalls
of these instances are low because they often have
overly similar features with the instances of the type
other linking to the same mentions. The problem
gets worse in the movie domain since i) many sen-
tences contain no explicit sentiment-bearing words;
ii) the prior polarity of the sentiment-bearing words
do not agree with their contextual polarity in the
sentences. Consider the following example from a
forum post about the movie ?Superman Returns?:
?Have a look at Superman: the Animated Series or
Justice League Unlimited . . . that is how the char-
acters of Superman and Lex Luthor should be.?. In
contrast, our model minimizes the overlapping fea-
tures by assigning them to the most likely relation
candidates. This leads to significantly better per-
formance. Although SENTI-SSVM has low recall for
both positive and negative relations, it achieves the
highest recall for the comparative relation among all
systems in the movie domain and camera reviews.
Since less than 1% of all instances are for compara-
tive relations in these document sets and all models
are trained to optimize the overall accuracy, SENTI-
LSSVM intends to trade off the minority class for the
overall better performance. This advantage disap-
pears on the camera forum posts, where the number
of instances of comparative relation is 12 times more
than that in the other data sets.
All systems perform better in predicting positive
relations than the negative ones. This corresponds
well to the empirical findings in (Wilson, 2008) that
people intend to use more complex expressions for
negative sentiments than their affirmative counter-
parts. It is also in accordance with the distribution of
these relations in our SRG corpus which is randomly
sampled from the online documents. For learning
systems, it can also be explained by the fact that the
training data for positive relations are considerably
more than those for negative ones. The comparative
relation is the hardest one to process since we found
that many corresponding expressions do not contain
explicit keywords for comparison.
To understand the performance of the key fea-
ture groups in our model better, we remove each
group from the full SENTI-LSSVM system and eval-
uate the variations with movie reviews and camera
forum posts, which have relatively balanced distri-
bution of relation types. As shown in Table 3, the
features from the sentiment predictors make signif-
icant contributions for both datasets. The differ-
ent drops of the performance indicate that the po-
164
Positive Negative Comparison Micro-average
P R F P R F P R F P R F
Ca
me
ra
Fo
rum
DING-RULE 56.4 39.0 46.1 46.2 24.0 31.6 42.6 14.0 21.0 53.4 30.8 39.0
SENTI-SSVM 60.2 35.6 44.8 44.2 38.5 41.2 28.0 40.1 32.9 43.7 36.7 39.9
SENTI-LSSVM 69.2 38.9 49.8 50.8 39.3 44.3 42.6 35.1 38.5 56.5 38.0 45.4
Ca
me
ra
Re
-
vie
w DING-RULE 83.6 69.0 75.6 68.6 38.8 49.6 30.0 16.9 21.6 81.1 58.6 68.1SENTI-SSVM 72.6 75.4 74.0 63.9 62.5 63.2 28.0 38.9 32.5 68.1 70.4 69.3
SENTI-LSSVM 77.3 85.4 81.2 68.9 61.3 64.9 22.3 20.7 21.6 73.1 73.4 73.7
Mo
vie
Fo
rum
DING-RULE 63.7 37.4 47.1 27.6 34.3 30.6 8.9 5.6 6.8 48.2 35.9 41.2
SENTI-SSVM 66.2 30.1 41.3 25.6 17.3 20.7 44.2 56.7 49.7 53.3 27.9 36.6
SENTI-LSSVM 63.3 44.2 52.1 29.7 45.6 36.0 40.1 45.0 42.4 49.7 44.6 47.0
Mo
vie Re
-
vie
w DING-RULE 66.5 47.2 55.2 42.0 39.1 40.5 31.4 12.0 17.4 56.2 44.0 49.4SENTI-SSVM 61.3 54.0 57.4 45.2 13.7 21.1 24.5 63.3 35.3 54.6 39.2 45.7
SENTI-LSSVM 59.0 79.1 67.6 53.3 51.4 52.3 28.3 34.0 30.9 57.9 68.8 62.9
Table 2: Evaluation results for DING-RULE, SENTI-SSVM and SENTI-LSSVM. Boldface figures are statistically
significantly better than all others in the same comparison group under t-test with p = 0.05.
Feature Models Movie Reviews Camera Forums
full system 62.9 45.4
?unigram 63.2 (+0.3) 41.2 (-4.2)
?context 54.5 (-8.4) 46.0 (+0.6)
?co-occurrence 62.6 (-0.3) 44.9 (-0.5)
?senti-predictors 61.3 (-1.6) 34.3 (-11.1)
Table 3: Micro-average F-measure of SENTI-LSSVM
with different feature models
larities predicted by rules are more consistent in
camera forum posts than in movie reviews. Due
to the complexity of expressions in the movie re-
views our model cannot benefit from the unigram
features but these features are a good compensation
for the sentiment predictor features in camera fo-
rum posts. The sharp drop by removing the context
features from our model on movie reviews indicates
that the sentiments in movie reviews depend highly
on the relations of the previous sentences. In con-
trast, the sentiment-oriented relations of the previ-
ous sentences could be a reason of overfitting for
camera forum data. The edge co-occurrence fea-
tures do not play an important role in our model
since the number of co-occurred sentiment-oriented
relations in the sentences with contrast conjunctions
like ?but? is small. However, we found that allow-
ing the co-occurrence of any sentiment-oriented re-
lations would harm the performance of the model.
In addition, our experiments showed that the sep-
arated approach, which trains a model for senti-
ment polarities and comparative relations respec-
tively, leads to a decrease by almost 1% in terms of
the F-measure averaged over all four datasets. The
largest drop of F-measure is 3% on camera forum
posts, since this dataset contains the largest propor-
tion of comparative relations. We found that the er-
rors are increased when the trained models make
conflicting predictions. In this case, the joint ap-
proach can take all factors into account and make
more consistent decisions than the separated ap-
proaches.
10 Conclusion
We proposed SENTI-LSSVM model for extracting in-
stances of both sentiment polarities and comparative
relations. For evaluating and training the model, we
created an SRG corpus by using a lightweight an-
notation scheme. We showed that our model can
automatically find textual evidences to support its
relation predictions and achieves significantly bet-
ter F-measure scores than alternative state-of-the-art
methods.
References
Yejin Choi and Claire Cardie. 2009. Adapting a polarity
lexicon using integer linear programming for domain-
specific sentiment classification. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
165
Language Processing: Volume 2 - Volume 2, EMNLP
?09, pages 590?598, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Yejin Choi and Claire Cardie. 2010. Hierarchical se-
quential learning for extracting opinions and their at-
tributes. In Proceedings of the Annual meeting of
the Association for Computational Linguistics, pages
269?274. Association for Computational Linguistics.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 431?
439, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Jacob Cohen. 1968. Weighted Kappa: Nominal Scale
Agreement Provision for Scaled Disagreement or Par-
tial Credit. Psychological bulletin, 70(4):213.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining. In
Proceedings of the 2008 International Conference on
Web Search and Data Mining, pages 231?240, New
York, NY, USA. ACM.
Xiaowen Ding, Bing Liu, and Lei Zhang. 2009. Entity
discovery and assignment for opinion mining applica-
tions. In Proceedings of the ACM SIGKDD Confer-
ence on Knowledge Discovery and Data Mining, pages
1125?1134.
John Duchi and Yoram Singer. 2009. Efficient online
and batch learning using forward backward splitting.
The Journal of Machine Learning Research, 10:2899?
2934.
Murthy Ganapathibhotla and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings of
the 22nd International Conference on Computational
Linguistics - Volume 1, pages 241?248, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Namrata Godbole, Manjunath Srinivasaiah, and Steven
Skiena. 2007. Large-scale sentiment analysis for
news and blogs (system demonstration). In Proceed-
ings of the International AAAI Conference on Weblogs
and Social Media.
John E Hopcroft and Richard M Karp. 1973. An n?5/2
algorithm for maximum matchings in bipartite graphs.
SIAM Journal on computing, 2(4):225?231.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, Proceedings of the
ACM SIGKDD Conference on Knowledge Discov-
ery and Data Mining, pages 168?177, New York, NY,
USA. ACM.
Wei Jin, Hung Hay Ho, and Rohini K. Srihari. 2009.
Opinionminer: a novel machine learning system for
web opinion mining and extraction. In Proceedings
of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 1195?
1204, New York, NY, USA. ACM.
Nitin Jindal and Bing Liu. 2006. Mining comparative
sentences and relations. In Proceedings of the 21st In-
ternational Conference on Artificial Intelligence - Vol-
ume 2, AAAI?06, pages 1331?1336. AAAI Press.
Richard Johansson and Alessandro Moschitti. 2011.
Extracting opinion expressions and their polarities?
exploration of pipelines and joint models. In Proceed-
ings of the Annual meeting of the Association for Com-
putational Linguistics, volume 11, pages 101?106.
Jason S. Kessler, Miriam Eckert, Lyndsie Clark, and
Nicolas Nicolov. 2010. The 2010 icwsm jdpa sent-
ment corpus for the automotive domain. In 4th Inter-
national AAAI Conference on Weblogs and Social Me-
dia Data Workshop Challenge (ICWSM-DWC 2010).
Soo-Min Kim and Eduard Hovy. 2006. Extracting opin-
ions, opinion holders, and topics expressed in online
news media text. In Proceedings of the Workshop on
Sentiment and Subjectivity in Text, SST ?06, pages 1?8,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, ACL ?03, pages 423?430, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen. 2006.
Opinion extraction, summarization and tracking in
news and blog corpora. In AAAI Spring Sympo-
sium: Computational Approaches to Analyzing We-
blogs, pages 100?107.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opinions
on the web. In Proceedings of the 14th international
conference on World Wide Web, pages 342?351, New
York, NY, USA. ACM.
Andr? L. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formula-
tions for dependency parsing. In Proceedings of the
Annual meeting of the Association for Computational
Linguistics, pages 342?350.
Ryan T. McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeffrey C. Reynar. 2007. Structured mod-
els for fine-to-coarse sentiment analysis. In Proceed-
ings of the Annual meeting of the Association for Com-
putational Linguistics.
Bo Pang and Lillian Lee. 2007. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
166
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, HLT ?05, pages 339?346, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum.
2010. The bag-of-opinions method for review rat-
ing prediction from sparse text patterns. In Chu-Ren
Huang and Dan Jurafsky, editors, Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), ACL Anthology, pages 913?
921, Beijing, China. Tsinghua University Press.
Lizhen Qu, Rainer Gemulla, and Gerhard Weikum. 2012.
A weakly supervised model for sentence-level seman-
tic orientation analysis with multiple experts. In Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 149?159,
Jeju Island, Korea, July. Proceedings of the Annual
meeting of the Association for Computational Linguis-
tics.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 1201?1211.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings of
the Joint conference of the 47th Annual Meeting of the
Association for Computational Linguistics and the 4th
International Joint Conference on Natural Language
Processing of the Asian Federation of Natural Lan-
guage Processing, pages 226?234.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly D. Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computational
Linguistics, 37(2):267?307.
Oscar T?ckstr?m and Ryan McDonald. 2011. Discov-
ering fine-grained sentiment with latent variable struc-
tured prediction models. In Proceedings of the 33rd
European conference on Advances in information re-
trieval, ECIR?11, pages 368?374, Berlin, Heidelberg.
Springer-Verlag.
Cigdem Toprak, Niklas Jakob, and Iryna Gurevych.
2010. Sentence and expression level annotation of
opinions in user-generated discourse. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, ACL ?10, pages 575?584,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proceedings of the International
Conference on Machine Learning, pages 104?112.
Wei Wei and Jon Atle Gulla. 2010. Sentiment learn-
ing on product reviews via sentiment ontology tree. In
Proceedings of the Annual meeting of the Association
for Computational Linguistics, pages 404?413.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165?210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the confer-
ence on Human Language Technology and Empirical
Methods in Natural Language Processing, HLT ?05,
pages 347?354, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Theresa Ann Wilson. 2008. Fine-grained subjectivity
and sentiment analysis: recognizing the intensity, po-
larity, and attitudes of private states. Ph.D. thesis,
UNIVERSITY OF PITTSBURGH.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2011. Structural opinion mining for graph-based sen-
timent representation. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1332?1341.
Ainur Yessenalina and Claire Cardie. 2011. Composi-
tional matrix-space models for sentiment analysis. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 172?182.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural svms with latent variables. In Pro-
ceedings of the International Conference on Machine
Learning, page 147.
Ning Yu and Sandra K?bler. 2011. Filling the gap:
Semi-supervised learning for opinion detection across
domains. In Proceedings of the Fifteenth Conference
on Computational Natural Language Learning, pages
200?209. Association for Computational Linguistics.
167
168
