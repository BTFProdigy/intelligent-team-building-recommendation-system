The Importance of Discourse Context
for Statistical Natural Language Generation
Cassandre Creswell
Department of Linguistics
University of Toronto
creswell@cs.toronto.edu
Elsi Kaiser
Center for Language Sciences
University of Rochester
ekaiser@ling.rochester.edu
Abstract
Surface realization in statistical natural lan-
guage generation is based on the idea that when
there are many ways to say the same thing, the
most frequent option based on corpus counts
is the best. Based on data from English and
Finnish, we argue instead that all options are
not equivalent, and the most frequent one can
be incoherent in some contexts. A statistical
NLG system where word order choice is based
only on frequency counts of forms cannot cap-
ture the contextually-appropriate use of word
order. We describe an alternative method for
word order selection and show how it outper-
forms a frequency-only approach.
1 Introduction
The purpose of a natural language generation (NLG) sys-
tem is to encode semantic content in a linguistic form eas-
ily understood by humans in order to communicate it to
the user of the system. Ideally, this content should be
encoded in strings that are both grammatical and contex-
tually appropriate.
Human speakers of all natural languages have many
ways to encode the same truth-conditional meaning be-
sides a single ?canonical? word order, even when encod-
ing one predicate and its arguments as a main clause. Hu-
mans choose contextually-appropriate options from these
many ways with little conscious effort and with rather ef-
fective communicative results. Statistical approaches to
natural language generation are based on the assumption
that often many of these options will be equally good,
e.g. (Bangalore and Rambow, 2000).
In this paper, we argue that, in fact, not all options
are equivalent, based on linguistic data both from En-
glish, a language with relatively static word order, and
from Finnish, a language with much more flexible word
order. We show that a statistical NLG algorithm based
only on counts of trees cannot capture the appropriate
use of word order. We provide an alternative method
which has been implemented elsewhere and show that
it dramatically outperforms the statistical approach. Fi-
nally, we explain how the alternative method could be
used to augment present statistical approaches and draw
some lessons for future development of statistical NLG.
2 Statistical NLG: a brief summary
In recent years, a new approach to NLG has emerged,
which hopes to build on the success of the use of
probabilistic models in natural language understanding
(Langkilde and Knight, 1998; Bangalore and Rambow,
2000; Ratnaparkhi, 2000). Building an NLG system is
highly labor-intensive. For the system to be robust, large
amounts of world and linguistic knowledge must be hand-
coded. The goal of statistical approaches is to minimize
hand-coding and instead rely upon information automat-
ically extracted from linguistic corpora when selecting a
linguistic realization of some conceptual representation.
The underlying concept of these statistical approaches
is that the form generated to express a particular mean-
ing should be selected on the basis of counts of that form
(either strings or trees) in a corpus. In other words, in
generating a form f to express an input, one wants to
maximize the probability of the form, P (f), with respect
to some gold-standard corpus, and thus express the in-
put in a way that resembles the realizations in the corpus
most closely (Bangalore and Rambow, 2000). Bangalore
and Rambow?s algorithm for generating a string in the
FERGUS system begins with an underspecified concep-
tual representation which is mapped to a dependency tree
with unordered sibling nodes. To convert the dependency
tree into a surface form, a syntactic structure is chosen for
each node. In FERGUS, this structure is an elementary
tree in a tree-adjoining grammar. The choice of a tree is
stochastic, based on a tree model derived from 1,000,000
words of the Wall Street Journal. For example, the tree
chosen for a verb V will be the most frequently found
tree in the corpus headed by V .
3 Where counting forms fails
This section provides evidence from English and Finnish
that word order affects meaning and acceptability. For
each phenomenon we show how a statistical generation
technique based only on the probability of forms in a cor-
pus will fail to capture this distinction in meaning.
Speakers can use a particular form to indicate their
assumptions about the status of entities, properties, and
events in the discourse model. For example, references
to entities may appear as full NPs, pronouns, or be miss-
ing entirely, depending on whether speakers regard them
as new or old to the hearer or the discourse or as particu-
larly salient (Gundel et al, 1993; Prince, 1992). Not just
the lexical form of referential expressions, but also their
position or role within the clause may vary depending on
the information status of its referent (Birner and Ward,
1998). An example of this in English is ditransitive verbs,
which have two variants, the to-dative (I gave the book to
the manager) and the double-object (I gave the manager
the book). Without a context both forms are equally ac-
ceptable, and in context native speakers may be unable
to consciously decide which is more appropriate. How-
ever, the use of the forms is highly systematic and almost
entirely predictable from the relative information status
and the relative size of the object NPs (Snyder, 2003). In
general, older, lighter NPs precede newer, heavier NPs.
Generating the appropriate ditransitive form based
only on their relative frequencies is impossible, as can
be seen in the behavior of the ditransitive give in a corpus
of naturally occurring written and spoken English (Sny-
der, 2003).1 Of the 552 tokens of give where the indirect
and direct objects are full NPs,2 152 (27.5%) are the to-
dative and 400 (72.5%) are the double object construc-
tion. Given this ratio, only the double object construction
would be generated. If the distribution of relative infor-
mation status and heaviness of direct and indirect objects
is the same in the domain of generation as in the source
corpus, then on average, the construction chosen as a sur-
face realization will be inappropriate 3 times out of 10.
Compared to English, the evidence for the importance
of word order from a free word order language like
Finnish is even more striking. When word order is used
to encode the information status and discourse function
of NP referents, native speakers will judge the use of the
wrong form infelicitous and odd, and a text incorporating
1This corpus consists of two novels, the Switchboard corpus,
and a corpus of online newsgroup texts.
2She omits pronominal NPs because their ordering is af-
fected by additional phonological factors related to cliticization.
several wrong forms in succession rapidly becomes in-
coherent (cf. Kruijff-Korbayova? et al (2002) on Czech,
Russian, and Bulgarian).
Although Finnish is regarded as canonically subject-
verb-object (SVO), all six permutations of these three el-
ements are possible, and corpus studies reveal that SVO
order only occurs in 56% of sentences (Hakulinen and
Karlsson, 1980). Different word order variants in Finnish
realize different pragmatic structurings of the conveyed
information. For example, Finnish has no definite or in-
definite article, and the SVO/OVS variation is used to en-
code the distinction between already-mentioned entities
and new entities (e.g. Chesterman (1991)). OVS order
typically marks the object as given, and the subject as
new. SVO order is more flexible. It can be used when
the subject is given, and the object is new, and also when
both are old or both are new. In orders with more than one
preverbal argument (SOV, OSV), as well as verb-initial
orders (VOS, VSO), the initial constituent is interpreted
as being contrastive (Vilkuna (1995); and others).
Because different orders have different discourse prop-
erties, use of an inappropriate order can lead to severe
misunderstandings, including difficulty in interpreting
NPs. For example, if a speaker uses canonical SVO order
in a context where the subject is discourse-new informa-
tion but the object has already been mentioned, the hearer
will tend to have difficulty interpreting the NPs because
OVS?not SVO?is the order that usually marks the ob-
ject as discourse-old and subject as discourse-new. Psy-
cholinguistic evidence from sentence processing experi-
ments shows that humans are very sensitive to the given-
new information carried by word order (Kaiser, 2003).
Hence, it is an important factor in the quality of linguistic
output of a NLG system.
Attempts to choose the appropriate word order in
Finnish will encounter the same problem found with En-
glish ditransitives. Table 1 illustrates the frequency of the
different word orders in a 10,000 sentence corpus used
by Hakulinen and Karlsson (1980). The most frequent
order is SV(X), where X is any non-subject, non-verbal
constituent, and so this order should always be the one
selected by a statistical algorithm. Based on the counts
then, assuming that the proportion of discourse contexts
is roughly similar within a domain, in only 56% of con-
texts will the choice of SV(X) order actually match the
discourse conditions in which it is used.
Order SV(X) XVS SXV XSV Other
N 5674 1139 60 348 2928
% 56 11 1 3 29
Table 1: Finnish word order frequency
The point here is not that statistical approaches to NLG
are entirely flawed. Attempting to generate natural lan-
guage by mimicking a corpus of naturally-occurring lan-
guage may be the most practical strategy for designing ro-
bust, scalable NLG systems. However, human language
is not just a system for concatenating words (or assem-
bling trees) to create grammatical outputs. Speakers do
not put constituents in a certain order simply because the
words they are using to express the constituents have been
frequently put in that order in the past. Constituents (and
thereby words) appear in particular orders because those
orders can reliably indicate the content speakers wish to
communicate. Because of the lucky coincidence that sta-
tistical NLG has been primarily based on English, where
the effects of word order variation are subtle, the prob-
lems with selecting a form f based only on a calcula-
tion of P (f) are not obvious. It might seem as if the
most frequent tree can express a given proposition ad-
equately. However, given the English word order phe-
nomenon shown above, a model based on P (f) is prob-
lematic. Moreover, in languages like Finnish, even the
generation of simple transitive clauses may result in out-
put which is confusing for human users.
NLG must take into account not just grammaticality
but contextual appropriateness, and so statistical algo-
rithms need to be provided with an augmented represen-
tation from which to learn?not just strings or trees, but
pairings of linguistic forms, contexts, and meanings. The
probability we need to maximize for NLG is the probabil-
ity that f is used given a meaning to be expressed and the
context in which f will be used, P (f |meaning,context).
4 An alternative approach
This section describes a very simple example of how a
probability like P (f |meaning,context) could be utilized
as part of a surface realization algorithm for English di-
transitives, in particular for the verb give. This example
is only a small subset of the larger problem of surface
realization, but it illustrates well the improvement in per-
formance of using P (f |meaning,context) vs. P (f), when
evaluated against actual corpus data.
First, the corpus from which the probabilities are be-
ing taken must be annotated with the additional mean-
ing information conditioning the use of the form. For
ditransitives, this is the information status of the indirect
object NP, in particular whether it is hearer-new. Hearer-
status can be quickly and reliably annotated and has been
widely used in corpus-based pragmatic studies (Birner
and Ward, 1998). It could be applied as an additional
markup of a corpus to be used as input to a statistical gen-
eration algorithm, like the Penn Treebank, such that each
NP indirect object of a ditransitive verb would be given
an additional tag marking its hearer status. Here we use
the corpus counts presented in Snyder (2003) for the verb
give as our training data. Table 2 shows the frequency of
the properties of hearer-newness and relative heaviness
of indirect objects (IOs) and direct objects (DOs) with
respect to the two ditransitive alternations.
IO STATUS TO-DATIVE DOUBLE OBJECT
Hearer-new ? 60 0
IO heavier 79 31
Hearer-old DO heavier 7 357
IO=DO 6 12
Totals 152 400
Table 2: Corpus freq. of ditransitives (Snyder, 2003)
To demonstrate the performance of an approach which
counts only form, we use the equation P (f) to determine
the choice of double-object vs. to-dative. The relative
probabilities of each order in the Snyder (2003) corpus
are .725 and .275 for double object and to-dative, respec-
tively. As such, this method will always select the double
object form, yielding an error rate of 27.5% on the train-
ing data, as shown in the row labeled P (f) of Table 3.
An algorithm which incorporates more information
than just raw frequencies will proceed as follows: if the
IO is hearer-new, generate a to-dative because the prob-
ability in the corpus of finding a to-dative given that the
indirect object is hearer-new is 1 (60 out of 552 tokens).
In all other cases (i.e. all other information statuses of
IO and DO), the probability of finding a to-dative is now
92/400, or 18.6%, so generate a double object. This
method results in 92 incorrect forms (all cases where the
double object is generated instead of a to-dative), an error
rate of 16.7% on the training data.
If the generation algorithm is further augmented to take
into account information about the relative heaviness of
the direct and indirect object NPs?possible in a system
where NPs are generated separately from sentences as a
whole, the error rate can be reduced even more. This al-
gorithm will be as follows, if the IO is hearer-new, the
form chosen is a to-dative. If the IO is not hearer-new,
the IO and DO are compared with respect to number of
syllables. If the IO is longer, generate a to-dative; if the
DO is longer, generate a double object. As before, the
first rule applies to the 60 tokens where the IO is hearer-
new. Out of the remaining 492 tokens, 474 have IOs and
DOs of different heaviness. In 357 of the 388 double ob-
jects, the DO is heavier, and in 79 of the 86 to-datives,
the IO is heavier. This leaves 38 of 474 tokens not cov-
ered by the heaviness rule, along with 18 tokens where
the IO and DO are equal. For these 56 cases, we gener-
ate the more probable overall form, the double object. In
total then, this augmented generation rule will yield 139
to-datives (60 cases where the IO is hearer-new and 79
cases where the IO is heavier). With this algorithm, only
13 actual to-datives will be generated wrongly as double-
objects when compared to their actual form in the corpus,
an error rate of only 2.4%
DO-IO IO-DO Error
Actual counts 152 400 ?
P (f) 0 552 27.5%
Hearer-status, P (f) 60 492 16.7%
Hearer-status, heaviness, P (f) 139 413 2.4%
Table 3: Error rates with respect to choice of word order
This example shows that for some arbitrary generation
of a surface realization of the predicate GIVE, simply in-
cluding the hearer-status of the recipient as a condition on
the choice of form yields the order that matches the ?gold
standard? of human behavior in a meaningful way about
80% of the time vs. only 70% for an approach based on
counts of trees including give alone. By including addi-
tional information about the relative size of the NPs, the
surface realization will match the gold standard over 97%
of the time, a highly human-like output.
5 Implementation & implications for NLG
The approach argued for above is one where discourse
context and meaning must be taken into account when se-
lecting a construction for NLG purposes. Admittedly, the
demonstration of the error rate here is not derived from
an actual system. However, functioning NLG systems
have been implemented where exactly such information
conditions the algorithm for choice of main clause word
order (Stone et al, 2001; Kruijff-Korbayova? et al, 2002).
Additionally, an approach like Bangalore and Rambow?s
could easily be extended by annotating their corpus for
hearer-status of NPs. The necessary information could
also possibly be extracted automatically from a corpus
like the Prague Dependency Treebank which includes
discourse-level information relevant to word order. For
phenomena which have not been as closely studied as En-
glish ditransitives, machine learning could be used to find
correlations between context and forms in corpora which
could be incorporated into statistical NLG algorithms.
The primary implication of our argument here is that
counting words and trees is not enough for statistical
NLG. Meaning, semantic and pragmatic, is a crucial
component of natural language generation. Despite the
desire to lessen the need for labeled data in statistical
NLP, such data remain crucial. Efforts to create multi-
level corpora which overlay semantic annotation on top
of syntactic annotation, such as the Propbank (Kingsbury
and Palmer, 2002), should be expanded to include anno-
tations of pragmatic and discourse information and used
in the development of statistical NLG methods. We can-
not generate forms by ignoring their meaning and expect
to get meaningful output. In other words, if the input to
an NLG system does lack distinctions that play a crucial
role in human language comprehension, the system will
not be able to overcome this lack of quality and generate
high-quality output.
In addition, in the effort to push the boundaries of sta-
tistical techniques, limiting the scope of research to En-
glish may give falsely promising results. If one of the
primary benefits of statistical techniques is robust porta-
bility to other languages, presentation of results based on
experimentation on a small subset of human languages
must be accompanied by a typologically-informed exam-
ination of the assumptions underlying such experiments.
References
Bangalore, S., and O. Rambow. 2000. Exploiting a proba-
bilistic hierarchical model for generation. In COLING.
Birner, B., and G. Ward. 1998. Information status and
noncanonical word order in English. Amsterdam:
John Benjamins.
Chesterman, A. 1991. On definiteness. Cambridge: CUP.
Gundel, J., N. Hedberg, and R. Zacharski. 1993. Cogni-
tive status and the form of referring expressions. Lan-
guage 69:274?307.
Hakulinen, A., and F. Karlsson. 1980. Finnish syntax in
text. Nordic Journal of Linguistics 3:93?129.
Kaiser, E. 2003. The quest for a referent: A crosslinguis-
tic look at reference resolution. Doctoral Dissertation,
University of Pennsylvania.
Kingsbury, P., and M. Palmer. 2002. From Treebank to
Propbank. In LREC-02. Las Palmas, Spain.
Kruijff-Korbayova?, I., G. J. Kruijff, and J. Bateman.
2002. Generation of contextually appropriate word or-
der. In Information sharing, 193?222. CSLI.
Langkilde, I., and K. Knight. 1998. Generation that ex-
ploits corpus-based statistical knowledge. In COLING-
ACL.
Prince, E. F. 1992. The ZPG letter: subjects, definiteness,
and information-status. In Discourse description. Am-
sterdam: John Benjamins.
Ratnaparkhi, A. 2000. Trainable methods for surface nat-
ural language generation. In ANLPC 6?NAACL 1.
Snyder, K. 2003. On ditransitives. Doctoral Dissertation,
University of Pennsylvania.
Stone, M., C. Doran, B. Webber, T. Bleam, and
M. Palmer. 2001. Communicative-intent-based mi-
croplanning: the Sentence Planning Using Description
system. Rutgers University.
Vilkuna, M. 1995. Discourse configurationality in
Finnish. In Discourse configurational languages, ed.
K. Kiss, 244?268. New York: Oxford University Press.
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 168?175,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatically Extracting Nominal Mentions of Events with a
Bootstrapped Probabilistic Classifier?
Cassandre Creswell? and Matthew J. Beal? and John Chen?
Thomas L. Cornell? and Lars Nilsson? and Rohini K. Srihari??
?Janya, Inc.
1408 Sweet Home Road, Suite 1
Amherst NY 14228
{ccreswell,jchen,cornell,
lars,rohini}@janyainc.com
?Dept. of Computer Science and Engineering
University at Buffalo
The State University of New York
Amherst NY 14260
mbeal@cse.buffalo.edu
Abstract
Most approaches to event extraction focus
on mentions anchored in verbs. However,
many mentions of events surface as noun
phrases. Detecting them can increase the
recall of event extraction and provide the
foundation for detecting relations between
events. This paper describes a weakly-
supervised method for detecting nominal
event mentions that combines techniques
from word sense disambiguation (WSD)
and lexical acquisition to create a classifier
that labels noun phrases as denoting events
or non-events. The classifier uses boot-
strapped probabilistic generative models
of the contexts of events and non-events.
The contexts are the lexically-anchored se-
mantic dependency relations that the NPs
appear in. Our method dramatically im-
proves with bootstrapping, and comfort-
ably outperforms lexical lookup methods
which are based on very much larger hand-
crafted resources.
1 Introduction
The goal of information extraction is to generate
a set of abstract information objects that repre-
sent the entities, events, and relations of particu-
lar types mentioned in unstructured text. For ex-
ample, in a judicial domain, relevant event types
might be ARREST, CHARGING, TRIAL, etc.
Although event extraction techniques usually
focus on extracting mentions textually anchored
by verb phrases or clauses, e.g. (Aone and Ramos-
? This work was supported in part by SBIR grant
FA8750-05-C-0187 from the Air Force Research Laboratory
(AFRL)/IFED.
Santacruz, 2000), many event mentions, espe-
cially subsequent mentions of events that are the
primary topic of a document, are referred to with
nominals. Because of this, detecting nominal
event mentions, like those in (1), can increase the
recall of event extraction systems, in particular for
the most important events in a document.1
(1) The slain journalist was a main organizer of the mas-
sive demonstrations that forced Syria to withdraw its
troops from Lebanon last April, after Assad was widely
accused of planningHariri?s assassination in a Febru-
ary car bombing that was similar to today?s blast.
Detecting event nominals is also an important
step in detecting relations between event men-
tions, as in the causal relation between the demon-
strations and the withdrawal and the similarity re-
lation between the bombing and the blast in (1).
Finally, detecting nominal events can improve
detection and coreference of non-named mentions
of non-event entities (e.g. persons, locations, and
organizations) by removing event nominals from
consideration as mentions of entities.
Current extraction techniques for verbally-
anchored events rest on the assumption that most
verb phrases denote eventualities. A system to ex-
tract untyped event mentions can output all con-
stituents headed by a non-auxiliary verb with a
filter to remove instances of to be, to seem, etc.
A statistical or rule-based classifier designed to
detect event mentions of specific types can then
be applied to filter these remaining instances.
Noun phrases, in contrast, can be used to denote
anything?eventualities, entities, abstractions, and
only some are suitable for event-type filtering.
1For example, in the 2005 Automatic Content Extraction
training data, of the 5,349 event mentions, over 35% (1934)
were nominals.
168
1.1 Challenges of nominal event detection
Extraction of nominal mentions of events encom-
passes many of the fundamental challenges of
natural language processing. Creating a general
purpose lexicon of all potentially event-denoting
terms in a language is a labor-intensive task. On
top of this, even utilizing an existing lexical re-
source like WordNet requires sense disambigua-
tion at run-time because event nominals display
the full spectrum of sense distinction behaviors
(Copestake and Briscoe, 1995), including idiosyn-
cratic polysemy, as in (2); constructional poly-
semy, as in (3); coactivation, (4); and copredica-
tion, as in (5).
(2) a. On May 30 a group of Iranian mountaineers hoisted
the Iranian tricolor on the summit.
b. EU Leaders are arriving here for their two-day
summit beginning Thursday.
(3) Things are getting back to normal in the Baywood Golf
Club after a chemical spill[=event]. Clean-up crews
said the chemical spill[=result] was 99 percent water
and shouldn?t cause harm to area residents.
(4) Managing partner Naimoli said he wasn?t concerned
about recent media criticism.
(5) The construction lasted 30 years and was inaugurated
in the presence of the king in June 1684.
Given the breadth of lexical sense phenom-
ena possible with event nominals, no existing ap-
proach can address all aspects. Lexical lookup,
whether using a manually- or automatically-
constructed resource, does not take context into
consideration and so does not allow for vagueness
or unknown words. Purely word-cooccurrence-
based approaches (e.g. (Schu?tze, 1998)) are un-
suitable for cases like (3) where both senses are
possible in a single discourse. Furthermore, most
WSD techniques, whether supervised or unsuper-
vised, must be retrained for each individual lexical
item, a computationally expensive procedure both
at training and run time. To address these limita-
tions, we have developed a technique which com-
bines automatic lexical acquisition and sense dis-
ambiguation into a single-pass weakly-supervised
algorithm for detecting event nominals.
The remainder of this paper is organized as fol-
lows: Section 2 describes our probabilistic clas-
sifier. Section 3 presents experimental results of
this model, assesses its performance when boot-
strapped to increase its coverage, and compares it
to a lexical lookup technique. We describe related
work in Section 4 and present conclusions and im-
plications for future work in Section 5.
2 Weakly-supervised, simultaneous
lexical acquisition and disambiguation
In this section we present a computational method
that learns the distribution of context patterns that
correlate with event vs. non-event mentions based
on unambiguous seeds. Using these seeds we
build two Bayesian probabilistic generative mod-
els of the data, one for non-event nominals and the
other for event nominals. A classifier is then con-
structed by comparing the probability of a candi-
date instance under each model, with the winning
model determining the classification. In Section 3
we show that this classifier?s coverage can be in-
creased beyond the initial labeled seed set by au-
tomatically selecting additional seeds from a very
large unlabeled, parsed corpus.
The technique proceeds as follows. First, two
lexicons of seed terms are created by hand. One
lexicon includes nominal terms that are highly
likely to unambiguously denote events; the other
includes nominal terms that are highly likely to
unambiguously denote anything other than events.
Then, a very large corpus (>150K documents) is
parsed using a broad-coverage dependency parser
to extract all instantiations of a core set of seman-
tic dependency relations, including verb-logical
subject, verb-logical object, subject-nominal pred-
icate, noun phrase-appositive-modifier, etc.
Format of data: Each instantiation is in the
form of a dependency triple, (wa, R,wb), where
R is the relation type and where each argument is
represented just by its syntactic head, wn. Each
partial instantiation of the relation?i.e. either wa
or wb is treated as a wild card ? that can be filled
by any term?becomes a feature in the model. For
every common noun term in the corpus that ap-
pears with at least one feature (including each en-
try in the seed lexicons), the times it appears with
each feature are tabulated and stored in a matrix
of counts. Each column of the matrix represents
a feature, e.g. (occur,Verb-Subj, ?); each row rep-
resents an individual term,2 e.g. murder; and each
entry is the number of times a term appeared with
the feature in the corpus, i.e. as the instantiation of
?. For each row, if the corresponding term appears
in a lexicon it is given that designation, i.e. EVENT
or NONEVENT, or if it does not appear in either
lexicon, it is left unlabeled.
2A term is any common noun whether it is a single or
multiword expression.
169
Probabilistic model: Here we present the de-
tails of the EVENT model?the computations for
the NONEVENT model are identical. The probabil-
ity model is built using a set of seed words labeled
as EVENTs and is designed to address two desider-
ata: (I) the EVENT model should assign high prob-
ability to an unlabeled vector, v, if its features (as
recorded in the count matrix) are similar to the
vectors of the EVENT seeds; (II) each seed term
s should contribute to the model in proportion to
its prevalence in the training data.3 These desider-
ata can be incorporated naturally into a mixture
model formalism, where there are as many com-
ponents in the mixture model as there are EVENT
seed terms. Desideratum (I) is addressed by hav-
ing each component of the mixture model assign-
ing a multinomial probability to the vector, v. For
the ith mixture component built around the ith
seed, s(i), the probability is
p(v|s(i)) =
F?
f=1
(
s(i)f
)vf
,
where s(i)f is defined as the proportion of the times
the seed was seen with feature f compared to the
number of times the seed was seen with any fea-
ture f ? ? F . Thus s(i)f is simply the (i, f)th entry
in a row-sum normalized count matrix,
s(i)f =
s(i)f
?F
f ?=1 s
(i)
f ?
.
Desideratum (II) is realized using a mixture den-
sity by forming a weighted mixture of the above
multinomial distributions from all the provided
seeds i ? E . The weighting of the ith compo-
nent is fixed to be the ratio of the number of oc-
currences of the ith EVENT seed, denoted |s(i)|, to
the total number of all occurrences of event seed
words. This gives more weight to more prevalent
seed words:
p(s(i)) =
|s(i)|
?
i??E |s
(i?)|
.
The EVENT generative probability is then:
p(v|EVENT) =
?
i?E
[
p(s(i)) ? p(v|s(i))
]
.
An example of the calculation for a model with
just two event seeds and three features is given in
Figure 1. A second model is built from the non-
3The counts used here are the number of times a term is
seen with any feature in the training corpus because the in-
dexing tool used to calculate counts does not keep track of
which instances appeared simultaneously with more than one
feature. We do not expect this artifact to dramatically change
the relative seed frequencies in our model.
f1 f2 f3
event seed vector s(1) 3 1 8
event seed vector s(2) 4 6 1
unlabeled mention vector v 2 0 7
p(v|event) =
12
23
?
?
3
12
?2? 1
12
?0? 8
12
?7
+
11
23
?
?
4
11
?2? 6
11
?0? 1
11
?7
= 0.0019
Figure 1: Example of calculating the probability of unla-
beled instance v under the event distribution composed of
two event seeds s(1) and s(2).
event seeds as well, and a corresponding probabil-
ity p(v|NONEVENT) is computed. The following
difference (log odds-ratio)
d(v) = log p(v|EVENT) ? log p(v|NONEVENT)
is then calculated. An instance v encoded as the
vector v is labeled as EVENT or NONEVENT by
examining the sign of d(v). A positive difference
d(v) classifies v as EVENT; a negative value of
d(v) classifies v as NONEVENT. Should d=0 the
classifier is considered undecided and abstains.
Each test instance is composed of a term and
the dependency triples it appears with in context
in the test document. Therefore, an instance can
be classified by (i: word): Find the unlabeled fea-
ture vector in the training data corresponding to
the term and apply the classifier to that vector,
i.e. classify the instance based on the term?s be-
havior summed across many occurrences in the
training corpus; (ii: context): Classify the instance
based only on its immediate test context vector; or
(iii: word+context): For each model, multiply the
probability information from the word vector (=i)
and the context vector (=ii). In our experiments,
all terms in the test corpus appeared at least once
(80% appearing at least 500 times) in the training
corpus, so there were no cases of unseen terms?
not suprising with a training set 1,800 times larger
than the test set. However, the ability to label
an instance based only on its immediate context
means that there is a backoff method in the case of
unseen terms.
3 Experimental Results
3.1 Training, test, and seed word data
In order to train and test the model, we created
two corpora and a lexicon of event and non-event
seeds. The training corpus consisted of 156,000
newswire documents, ?100 million words, from
the Foreign Broadcast Information Service, Lexis
170
Nexis, and other online news archives. The cor-
pus was parsed using Janya?s information extrac-
tion application, Semantex, which creates both
shallow, non-recursive parsing structures and de-
pendency links, and all (wi, R,wj) statistics were
extracted as described in Section 2. From the
1.9 million patterns, (wi, R, ?) and (?, R,wj) ex-
tracted from the corpus, the 48,353 that appeared
more than 300 times were retained as features.
The test corpus was composed of 77 additional
documents (?56K words), overlapping in time
and content but not included in the training set.
These were annotated by hand to mark event nom-
inals. Specifically, every referential noun phrase
headed by a non-proper noun was considered
for whether it denoted an achievement, accom-
plishment, activity, or process (Parsons, 1990).
Noun heads denoting any of these were marked
as EVENT, and all others were left unmarked.
All documents were first marked by a junior an-
notator, and then a non-blind second pass was per-
formed by a senior annotator (first author). Sev-
eral semantic classes were difficult to annotate be-
cause they are particularly prone to coactivation,
including terms denoting financial acts, legal acts,
speech acts, and economic processes. In addition,
for terms like mission, plan, duty, tactic, policy,
it can be unclear whether they are hyponyms of
EVENT or another abstract concept. In every case,
however, the mention was labeled as an event or
non-event depending on whether its use in that
context appeared to be more or less event-like,
respectively. Tests for the ?event-y?ness of the
context included whether an unambiguous word
would be an acceptable substitute there (e.g. funds
[=only non-event] for expenditure [either]).
To create the test data, the annotated documents
were also parsed to automatically extract all com-
mon noun-headed NPs and the dependency triples
they instantiate. Those with heads that aligned
with the offsets of an event annotation were la-
beled as events; the remainder were labeled as
non-events. Because of parsing errors, about 10%
of annotated event instances were lost, that is re-
mained unlabeled or were labeled as non-events.
So, our results are based on the set of recover-
able event nominals as a subset of all common-
noun headed NPs that were extracted. In the
test corpus there were 9,381 candidate instances,
1,579 (17%) events and 7,802 (83%) non-events.
There were 2,319 unique term types; of these, 167
types (7%) appeared both as event tokens and non-
event tokens. Some sample ambiguous terms in-
clude: behavior, attempt, settlement, deal, viola-
tion, progress, sermon, expenditure.
We constructed two lexicons of nominals to use
as the seed terms. For events, we created a list of
95 terms, such as election, war, assassination, dis-
missal, primarily based on introspection combined
with some checks on individual terms in WordNet
and other dictionaries and using Google searches
to judge how ?event-y? the term was.
To create a list of non-events, we used WordNet
and the British National Corpus. First, from the
set of all lexemes that appear in only one synset
in WordNet, all nouns were extracted along with
the topmost hypernym they appear under. From
these we retained those that both appeared on a
lemmatized frequency list of the 6,318 words with
more than 800 occurrences in the whole 100M-
word BNC (Kilgarriff, 1997) and had one of the
hypernyms GROUP, PSYCHOLOGICAL, ENTITY,
POSSESSION. We also retained select terms from
the categories STATE and PHENOMENON were la-
beled non-event seeds. Examples of the 295 non-
event seeds are corpse, electronics, bureaucracy,
airport, cattle.
Of the 9,381 test instances, 641 (6.8%) had a
term that belonged to the seed list. With respect
to types, 137 (5.9%) of the 2,319 term types in the
test data also appeared on the seed lists.
3.2 Experiments
Experiments were performed to investigate the
performance of our models, both when using orig-
inal seed lists, and also when varying the content
of the seed lists using a bootstrapping technique
that relies on the probabilistic framework of the
model. A 1,000-instance subset of the 9,381 test
data instances was used as a validation set; the re-
maining 8,381 were used as evaluation data, on
which we report all results (with the exception of
Table 3 which is on the full test set).
EXP1: Results using original seed sets Prob-
abilistic models for non-events and events were
built from the full list of 295 non-event and 95
event seeds, respectively, as described above.
Table 1 (top half: original seed set) shows the
results over the 8,381 evaluation data instances
when using the three classification methods de-
scribed above: (i) word, (ii) context, and (iii)
word+context. The first row (ALL) reports scores
where all undecided responses are marked as in-
171
B
O
O
T
S
T
R
A
P
P
E
D
O
R
IG
IN
A
L
S
E
E
D
S
E
T
S
E
E
D
S
E
T
EVENT NONEVENT TOTAL AVERAGE
Input Vector Correct Acc (%) Att (%) Correct Acc (%) Att (%) Correct Acc (%) Att (%) Acc (%)
A
L
L
word 1236 87.7 100.0 4217 60.5 100.0 5453 65.1 100.0 74.1
context 627 44.5 100.0 2735 39.2 100.0 3362 40.1 100.0 41.9
word+context 1251 88.8 100.0 4226 60.6 100.0 5477 65.4 100.0 74.7
FA
IR
word 1236 89.3 98.3 4217 60.7 99.6 5453 65.5 99.4 75.0
context 627 69.4 64.2 2735 62.5 62.8 3362 63.6 63.0 65.9
word+context 1251 89.3 99.5 4226 60.7 99.9 5477 65.5 99.8 75.0
A
L
L
word 1110 78.8 100.0 5517 79.1 100.0 6627 79.1 100.0 79.0
context 561 39.8 100.0 2975 42.7 100.0 3536 42.2 100.0 41.3
word+context 1123 79.8 100.0 5539 79.4 100.0 6662 79.5 100.0 79.6
FA
IR
word 1110 80.2 98.3 5517 79.4 99.6 6627 79.5 99.4 79.8
context 561 62.1 64.2 2975 67.9 62.8 3536 66.9 63.0 65.0
word+context 1123 80.2 99.5 5539 79.5 99.9 6662 79.7 99.8 79.9
LEX 1 1114 79.1 100.0 5074 72.8 100.0 6188 73.8 100.0 75.9
total counts 1408 6973 8381
Table 1: (EXP1, EXP3) Accuracies of classifiers in terms of correct classifications, % correct, and % attempted (if allowed to
abstain), on the evaluation test set. (Row 1) Classifiers built from original seed set of size (295, 95); (Row 2) Classifiers built
from 15 iterations of bootstrapping; (Row 3) Classifier built from Lexicon 1. Accuracies in bold are those plotted in related
Figures 2, 3(a) and 3(b).
correct. In the second row (FAIR), undecided an-
swers (d = 0) are left out of the total, so the
number of correct answers stays the same, but the
percentage of correct answers increases.4 Scores
are measured in terms of accuracy on the EVENT
instances, accuracy on the NONEVENT instances,
TOTAL accuracy across all instances, and the sim-
ple AVERAGE of accuracies on non-events and
events (last column). The AVERAGE score as-
sumes that performance on non-events and events
is equally important to us.
?From EXP1, we see that the behavior of a term
across an entire corpus is a better source of infor-
mation about whether a particular instance of that
term refers to an event than its immediate context.
We can further infer that this is because the imme-
diate context only provides definitive evidence for
the models in 63.0% of cases; when the context
model is not penalized for indecision, its accuracy
improves considerably. Nonetheless, in combina-
tion with the word model, immediate context does
not appear to provide much additional information
over only the word. In other words, based only
on a term?s distribution in the past, one can make
a reasonable prediction about how it will be used
when it is seen again. Consequently, it seems that
a well-constructed, i.e. domain customized, lexi-
con can classify nearly as well as a method that
also takes context into account.
EXP2: Results on ACE 2005 event data In ad-
dition to using the data set created specifically for
this project, we also used a subset of the anno-
4Note that Att(%) does not change with bootstrapping?
an artifact of the sparsity of certain feature vectors in the
training and test data, and not the model?s constituents seeds.
Input Vector Acc (%) Att (%)
word 96.1 97.2
context 72.8 63.1
word+context 95.5 98.9
LEX 1 76.5 100.0
Table 2: (EXP2) Results on ACE event nominals: %correct
(accuracy) and %attempted, for our classifiers and LEX 1.
tated training data created for the ACE 2005 Event
Detection and Recognition (VDR) task. Because
only event mentions of specific types are marked
in the ACE data, only recall of ACE event nomi-
nals can be measured rather than overall recall of
event nominals and accuracy on non-event nom-
inals. Results on the 1,934 nominal mentions of
events (omitting cases of d = 0) are shown in Ta-
ble 2. The performance of the hand-crafted Lex-
icon 1 on the ACE data, described in Section 3.3
below, is also included.
The fact that our method performs somewhat
better on the ACE data than on our own data, while
the lexicon approach is worse (7 points higher
vs. 3 points lower, respectively) can likely be ex-
plained by the fact that in creating our introspec-
tive seed set for events, we consulted the annota-
tion manual for ACE event types and attempted
to include in our list any unambiguous seed terms
that fit those types.
EXP3: Increasing seed set via Bootstrapping
There are over 2,300 unlabeled vectors in the train-
ing data that correspond to the words that appear
as lexical heads in the test data. These unlabeled
training vectors can be powerfully leveraged us-
ing a simple bootstrapping algorithm to improve
the individual models for non-events and events,
as follows: Step 1: For each vector v in the unla-
beled portion of training data, row-sum normalize
172
100 1 5 10 15      LEX160
65
70
75
80
85
90 non?events
eventstotal
average
Figure 2: Accuracies vs. iterations of bootstrapping. Bold
symbols on left denote classifier built from initial (295, 95)
seeds; and bold (disconnected) symbols at right are LEX 1.
it to produce v? and compute a normalized mea-
sure of confidence of the algorithm?s prediction,
given by the magnitude of d(v?). Step 2: Add
those vectors most confidently classified as either
non-events or events to the seed set for non-events
or events, according to the sign of d(v?). Step 3:
Recalculate the model based on the new seed lists.
Step 4: Repeat Steps 1?3 until either no more un-
labeled vectors remain or the validation accuracy
no longer increases.
In our experiments we added vectors to each
model such that the ratio of the size of the seed
sets remained constant, i.e. 50 non-events and
16 events were added at each iteration. Using
our validation set, we determined that the boot-
strapping should stop after 15 iterations (despite
continuing for 21 iterations), at which point the
average accuracy leveled out and then began to
drop. After 15 iterations the seed set is of size
(295, 95)+(50, 16)?15 = (1045, 335). Figure 2
shows the change in the accuracy of the model as
it is bootstrapped through 15 iterations.
TOTAL accuracy improves with bootstrapping,
despite EVENT accuracy decreasing, because the
test data is heavily populated with non-events,
whose accuracy increases substantially. The AV-
ERAGE accuracy also increases, which proves that
bootstrapping is doing more than simply shifting
the bias of the classifier to the majority class. The
figure also shows that the final bootstrapped clas-
sifier comfortably outperforms Lexicon 1, impres-
sive because the lexicon contains at least 13 times
more terms than the seed lists.
EXP4: Bootstrapping with a reduced number
of seeds The size of the original seed lists were
chosen somewhat arbitrarily. In order to deter-
mine whether similar performance could be ob-
tained using fewer seeds, i.e. less human effort, we
experimented with reducing the size of the seed
lexicons used to initialize the bootstrapping.
To do this, we randomly selected a fixed frac-
tion, f%, of the (295, 95) available event and non-
event seeds, and built a classifier from this sub-
set of seeds (and discarded the remaining seeds).
We then bootstrapped the classifier?s models us-
ing the 4-step procedure described above, using
candidate seed vectors from the unlabeled train-
ing corpus, and incrementing the number of seeds
until the classifier consisted of (295, 95) seeds.
We then performed 15 additional bootstrapping it-
erations, each adding (50, 16) seeds. Since the
seeds making up the initial classifier are chosen
stochastically, we repeated this entire process 10
times and report in Figures 3(a) and 3(b) the mean
of the total and average accuracies for these 10
folds, respectively. Both plots have five traces,
with each trace corresponding the fraction f =
(20, 40, 60, 80, 100)% of labeled seeds used to
build the initial models. As a point of reference,
note that initializing with 100% of the seed lexicon
corresponds to the first point of the traces in Fig-
ure 2 (where the x-axis is marked with f =100%).
Interestingly, there is no discernible difference
in accuracy (total or average) for fractions f
greater than 20%. However, upon bootstrapping
we note the following trends. First, Figure 3(b)
shows that using a larger initial seed set increases
the maximum achievable accuracy, but this max-
imum occurs after a greater number bootstrap-
ping iterations; indeed the maximum for 100% is
achieved at 15 (or greater) iterations. This reflects
the difference in rigidity of the initial models, with
smaller initial models more easily misled by the
seeds added by bootstrapping. Second, the final
accuracies (total and average) are correlated with
the initial seed set size, which is intuitively satisfy-
ing. Third, it appears from Figure 3(a) that the to-
tal accuracy at the model size (295,95) (or 100%)
is in fact anti-correlated with the size of the ini-
tial seed set, with 20% performing best. This is
correct, but highlights the sometimes misleading
interpretation of the total accuracy: in this case
the model is defaulting to classifying anything as
a non-event (the majority class), and has a consid-
erably impoverished event model.
If one wants to do as well as Lexicon 1 after 15
iterations of bootstrapping then one needs at least
173
EVENT NONEVENT TOTAL AVERAGE
Corr (%) Corr (%) Corr (%) (%)
LEX 1 1256 79.5 5695 73.0 6951 74.1 76.3
LEX 2 1502 95.1 4495 57.6 5997 63.9 76.4
LEX 3 349 22.1 7220 92.5 7569 80.7 57.3
Total 1579 7802 9381
Table 3: Accuracy of several lexicons, showing number and
percentage of correct classifications on the full test set.
an initial seed set of size 60%. An alternative is
to perform fewer iterations, but here we see that
using 100% of the seeds comfortably achieves the
highest total and average accuracies anyway.
3.3 Comparison with existing lexicons
In order to compare our weakly-supervised proba-
bilistic method with a lexical lookup method based
on very large hand-created lexical resources, we
created three lexicons of event terms, which were
used as very simple classifiers of the test data. If
the test instance term belongs to the lexicon, it is
labeled EVENT; otherwise, it is labeled as NON-
EVENT. The results on the full test set using these
lexicons are shown in Table 3.
Lex 1 5,435 entries from NomLex (Macleod et
al., 1998), FrameNet(Baker et al, 1998), CELEX
(CEL, 1993), Timebank(Day et al, 2003).
Lex 2 13,659 entries from WordNet 2.0 hypernym
classes EVENT, ACT, PROCESS, COGNITIVE PRO-
CESS, & COMMUNICATION combined with Lex 1.
Lex 3 Combination of pre-existing lexicons in the
information extraction application from WordNet,
Oxford Advanced Learner?s Dictionary, etc.
As shown in Tables 1 and 3, the relatively
knowledge-poor method developed here using
around 400 seeds performs well compared to the
use of the much larger lexicons. For the task of
detecting nominal events, using Lexicon 1 might
be the quickest practical solution. In terms of ex-
tensibility to other semantic classes, domains, or
languages lacking appropriate existing lexical re-
sources, the advantage of our trainable method is
clear. The primary requirement of this method is
a dependency parser and a system user-developer
who can provide a set of seeds for a class of in-
terest and its complement. It should be possi-
ble in the next few years to create a dependency
parser for a language with no existing linguistic re-
sources (Klein and Manning, 2002). Rather than
having to spend the considerable person-years it
takes to create resources like FrameNet, CELEX,
and WordNet, a better alternative will be to use
weakly-supervised semantic labelers like the one
described here.
4 Related Work
In recent years an array of new approaches have
been developed using weakly-supervised tech-
niques to train classifiers or learn lexical classes
or synonyms, e.g. (Mihalcea, 2003; Riloff and
Wiebe, 2003). Several approaches make use of de-
pendency triples (Lin, 1998; Gorman and Curran,
2005). Our vector representation of the behavior
of a word type across all its instances in a corpus is
based on Lin (1998)?s DESCRIPTION OF A WORD.
Yarowsky (1995) uses a conceptually similar
technique for WSD that learns from a small set of
seed examples and then increases recall by boot-
strapping, evaluated on 12 idiosyncratically poly-
semous words. In that task, often a single disam-
biguating feature can be found in the context of a
polysemous word instance, motivating his use of
the decision list algorithm. In contrast, the goal
here is to learn how event-like or non-event-like
a set of contextual features together are. We do
not expect that many individual features correlate
unambiguously with references to events (or non-
events), only that the presence of certain features
make an event interpretation more or less likely.
This justifies our probabilistic Bayesian approach,
which performs well given its simplicity.
Thelen and Riloff (2002) use a bootstrapping al-
gorithm to learn semantic lexicons of nouns for
six semantic categories, one of which is EVENTS.
For events, only 27% of the 1,000 learned words
are correct. Their experiments were on a much
smaller scale, however, using the 1,700 document
MUC-4 data as a training corpus and using only
10 seeds per category.
Most prior work on event nominals does not try
to classify them as events or non-events, but in-
stead focuses on labeling the argument roles based
on extrapolating information about the argument
structure of the verbal root (Dahl et al, 1987; La-
pata, 2002; Pradhan et al, 2004). Meyers, et al
(1998) describe how to extend a tool for extrac-
tion of verb-based events to corresponding nomi-
nalizations. Hull and Gomez (1996) design a set
of rule-based algorithms to determine the sense of
a nominalization and identify its arguments.
5 Conclusions
We have developed a novel algorithm for label-
ing nominals as events that combines WSD and
lexical acquisition. After automatically bootstrap-
ping the seed set, it performs better than static lex-
icons many times the original seed set size. Also,
174
further bootstrap iterations?? initial seed setfraction (%) ?
20 40 60 80 100 1 5 10 15      LEX164
66
68
70
72
74
76
78
80
82
20%40%60%80%100%
(a) Total Accuracy
further bootstrap iterations?
?
initial seed set
fraction (%) ?
20 40 60 80 100 1 5 10 15      LEX164
66
68
70
72
74
76
78
80
82
20%40%60%80%100%
(b) Average Accuracy
Figure 3: Accuracies of classifiers built from different-sized initial seed sets, and then bootstrapped onwards to the equivalent
of 15 iterations as before. Total (a) and Average (b) accuracies highlight different aspects of the bootstrapping mechanism.
Just as in Figure 2, the initial model is denoted with a bold symbol in the left part of the plot. Also for reference the relevant
Lexicon 1 accuracy (LEX 1) is denoted with a ? at the far right.
it is more robust than lexical lookup as it can also
classify unknown words based on their immediate
context and can remain agnostic in the absence of
sufficient evidence.
Future directions for this work include applying
it to other semantic labeling tasks and to domains
other than general news. An important unresolved
issue is the difficulty of formulating an appropriate
seed set to give good coverage of the complement
of the class to be labeled without the use of a re-
source like WordNet.
References
C. Aone and M. Ramos-Santacruz. 2000. REES: A
large-scale relation and event extraction system. In
6th ANLP, pages 79?83.
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The
Berkeley FrameNet project. In Proc. COLING-ACL.
Centre of Lexical Information, Nijmegen, 1993.
CELEX English database, E25, online edition.
A. Copestake and T. Briscoe. 1995. Semi-productive
polysemy and sense extension. Journal of Seman-
tics, 12:15?67.
D. Dahl, M. Palmer, and R. Passonneau. 1987. Nomi-
nalizations in PUNDIT. In Proc. of the 25th ACL.
D. Day, L. Ferro, R. Gaizauskas, P. Hanks, M. Lazo,
J. Pustejovsky, R. Sauri, A. See, A. Setzer, and
B. Sundheim. 2003. The TIMEBANK corpus. In
Corpus Linguistics 2003, Lancaster UK.
J. Gorman and J. Curran. 2005. Approximate search-
ing for distributional similarity. In Proc. of the
ACL-SIGLEX Workshop on Deep Lexical Acquisi-
tion, pages 97?104.
R. Hull and F. Gomez. 1996. Semantic interpretation
of nominalizations. In Proc. of the 13th National
Conf. on Artificial Intelligence, pages 1062?1068.
A. Kilgarriff. 1997. Putting frequencies in the dictio-
nary. Int?l J. of Lexicography, 10(2):135?155.
D. Klein and C. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In Proc. of the 40th ACL.
M. Lapata. 2002. The disambiguation of nominalisa-
tions. Computational Linguistics, 28(3):357?388.
D. K. Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. of COLING-ACL ?98.
C. Macleod, R. Grishman, A. Meyers, L. Barrett, and
R. Reeves. 1998. NOMLEX: A lexicon of nominal-
izations. In Proc. of EURALEX?98.
A. Meyers, C. Macleod, R. Yangarber, R. Grishman,
L. Barrett, and R. Reeves. 1998. Using NOMLEX
to produce nominalization patterns for information
extraction. In Proc. of the COLING-ACL Workshop
on the Computational Treatment of Nominals.
R. Mihalcea. 2003. Unsupervised natural language
disambiguation using non-ambiguous words. In
Proc. of Recent Advances in Natural Language Pro-
cessing, pages 387?396.
T. Parsons. 1990. Events in the Semantics of English.
MIT Press, Boston.
S. Pradhan, H. Sun, W. Ward, J. Martin, and D. Juraf-
sky. 2004. Parsing arguments of nominalizations in
English and Chinese. In Proc. of HLT-NAACL.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proc. EMNLP.
H. Schu?tze. 1998. Automatic word sense disambigua-
tion. Computational Linguistics, 24(1):97?124.
M. Thelen and E. Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extrac-
tion pattern contexts. In Proc. of EMNLP.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. of
the 33rd ACL, pages 189?196.
175
Using a probabilistic model of discourse relations
to investigate word order variation
Cassandre Creswell
Cymfony, Inc.
600 Essjay Rd
Williamsville NY 14221
USA
ccreswell@cymfony.com
Department of Linguistics
University of Pennsylvania
Philadelphia PA 19104
USA
creswell@ling.upenn.edu
Abstract
Like speakers of any natural language, speakers of
English potentially have many different word orders
in which to encode a single meaning. One key fac-
tor in speakers? use of certain non-canonical word
orders in English is their ability to contribute infor-
mation about syntactic and semantic discourse rela-
tions. Explicit annotation of discourse relations is a
difficult and subjective task. In order to measure the
correlations between different word orders and vari-
ous discourse relations, this project utilizes a model
in which discourse relations are approximated us-
ing a set of lower-level linguistic features, which are
more easily and reliably annotated than discourse
relations themselves. The featural model provides
statistical evidence for the claim that speakers use
non-canonicals to communicate information about
discourse structure.
1 Introduction: Non-canonical main
clause word order in English
Users of natural languages have many ways to en-
code the same propositional content within a sin-
gle clause. In English, besides the ?canonical?
word order, (1), options for realizing a proposi-
tion like GROW(MYRA,EGGPLANTS), include top-
icalization, left-dislocation, it-clefts, and wh-clefts,
shown in (2?5), respectively.
(1) Myra grows eggplants.
(2) Eggplants, Myra grows.
(3) Eggplants, Myra grows them.
(4) It?s eggplants that Myra grows.
(5) What Myra grows are eggplants.
Corpus-based research has shown that these
forms are appropriate only under certain discourse
conditions (Prince, 1978; Birner and Ward, 1998);
among others. These include the membership of
referents in a salient set of entities (left-dislocations
and topicalizations) or the salience of particular
propositions (topicalizations and clefts). For exam-
ple, in (6), the topicalization is felicitous because
there is a salient set KINDS OF VEGETABLES and a
salient open proposition, that Myra stands in some
relation X with an element of that set.
(6) Myra likes most vegetables, but eggplants
she adores.
V = {KINDS OF VEGETABLES};
P = X(m1 , v2 ), SUCH THAT v2 ? V
The discourse conditions licensing the use of
these non-canonical syntactic forms are necessary
conditions. When they do not hold, native speak-
ers judge the use of the form infelicitous. They are
not, however, sufficient conditions for use because
salient sets and open propositions are ubiquitous
in any discourse context, but these non-canonical
forms are rare. Each type alone makes up < 1%
of utterances, across a variety of genres (Creswell,
2003).
In addition to their information structure func-
tions, one additional communicative goal these
word orders fulfill is providing information about
how an utterance is related to other discourse seg-
ments (Creswell, 2003). Native speaker intuitions
about the appropriateness of non-canonicals in par-
ticular contexts provide anecdotal evidence (i.e.
based on listing individual examples) for this dis-
course function. To provide broader support for this
claim, however, we need to be able to generalize
across many tokens.
Ideally, a corpus annotated with discourse rela-
tions would be used to measure the correlations be-
tween the presence of non-canonical word order and
particular discourse relations. However, explicit an-
notation of discourse relations is a difficult task, and
one heavily dependent on the specific theory from
which the set of discourse relations is chosen. In-
stead, this paper describes how a set of more easily-
annotated features can be used to create a simpli-
fied approximation of the discourse context sur-
rounding non-canonical (or canonical control) utter-
ances. These features are then used as the indepen-
dent variables in a statistical model which provides
evidence for claims about how speakers use non-
canonical word order to communicate information
about discourse relations.
The remainder of the paper is organized as fol-
lows: Section 2 describes how some non-canonical
word orders in English contribute to the establish-
ment of certain discourse relations. Section 3 de-
scribes how these relations can be approximated
with a probabilistic model composed of more eas-
ily annotated features of the discourse context. Sec-
tion 4 presents results and discussion of using such
a model to measure the correlations between dis-
course relations and word order. Section 5 con-
cludes and suggests improvements and applications
of the model.
2 Additional meaning of non-canonical
syntax: discourse relations
The meaning of a multi-utterance text is composed
not only of the meaning of each individual utterance
but also of the relations holding between the utter-
ances. These relations have syntactic aspects, such
that single utterances can be grouped together and
combined into segments recursively and are often
modeled as a hierarchical tree structure (Grosz and
Sidner, 1986; Webber et al, 1999). Discourse re-
lations may also have a semantic or meaning com-
ponent; this property, when treated in the literature,
is often referred to as coherence, subject matter, or
rhetorical relations (Kehler, 2002; Halliday, 1985;
Mann and Thompson, 1988).
The use of an utterance with non-canonical word
order helps hearers make inferences about both the
syntactic and semantic properties of discourse rela-
tions between the utterance and the rest of the dis-
course. For both aspects of discourse relations, it is
the fact that the non-canonical order marks part of
the utterance?s information as salient or discourse-
old that assists these inferences.
2.1 Syntax of discourse relations
One substructure of a coherent discourse struc-
ture is its attentional structure, which can be mod-
eled as a stack of focus spaces (Grosz and Sidner,
1986). Each segment in the discourse tree has a
corresponding focus space containing the currently
salient discourse entities. When a segment begins,
its focus space is pushed onto the stack on top of any
other incomplete segments? spaces. When the seg-
ment ends, the focus space is popped off the stack.
When an utterance continues in the same segment,
the focus stack is unchanged.
Non-canonical utterances instruct hearers about
where to attach segments to the discourse tree. Be-
cause of the necessary conditions that license the
use of a non-canonical, in most cases the open
proposition or set is part of a focus space pushed
onto the stack previously. So, the non-canonical
form evokes the old proposition or set and thus re-
activates the salience of that focus space. Reactivat-
ing the salience of the focus space in turn activates
the salience of the discourse segment. As a result,
the hearer infers that the new segment associated
with the non-canonical utterance should be attached
at the same level as this reactivated discourse seg-
ment, i.e. at a non-terminal node on the tree?s right
frontier. Any intervening segments should be closed
off, and their focus spaces should be popped off the
stack.
To illustrate, in (7) the use of the it-cleft occurs
after an intervening discussion of a separate topic.
It-clefts are used to indicate that an existential clo-
sure of an open proposition is presupposed, here
?t.YOU GOT TO MICHIGAN STATE AT TIME t. This
presupposed material allows speaker B to mark the
question as related to the prior discussion. In a tree
structure of this discourse, the cleft corresponds to
an instruction to ?pop? back to a higher level in
the tree when attaching the utterance, where speaker
G?s career at Michigan State was under discussion.
The canonical version in (8) is an abrupt and infe-
licitous continuation of the discourse, as if B is un-
aware of the previous discussion of G?s arrival at
Michigan State.1
(7) G: So for two years, I served as a project offi-
cer for grants and contracts in health economics
that that agency was funding. I decided to go to
academia after that and taught at Michigan State
in economics and community medicine. One thing
I should mention is that for my last three months
in government, I had been detailed to work on the
Price Commission which was a component of the
Economic Stabilization program. [Description of
work on Price Commission...]
B: In what year was it that you got to Michigan
State? (SSA, ginsberg)
(8) In what year did you get to Michigan State?
2.2 Semantics of discourse relations
The contribution of non-canonical utterances to the
inference of semantic aspects of discourse relations
is also related to the fact that these word orders
mark (part of) an utterance?s content as discourse-
old or presupposed. Non-canonical word order is
1Varying the placement of the primary prosodic stress may
improve the version in (8); see Delin (1995) and Creswell
(2003) for comparison of the discourse function of prosody and
syntax.
used to indicate relations of RESEMBLANCE rather
than CONTIGUITY.
A CONTIGUITY relation is the basic relation
found in narratives. According to Labov (1997), ut-
terances in a prototypical narrative describe in the
order they took place a sequence of causally-related
events which lead up to a MOST REPORTABLE
EVENT. Kehler (2002), following Hobbs (1990),
says the events should be centered around a system
of entities, and each event should correspond to a
change of state for that system. To infer a CONTI-
GUITY relation between two utterances, the hearer
must infer that their eventualities correspond to a
change of state for that system.
Inferring a RESEMBLANCE relation between two
utterances depends on a very different type of in-
formation. To establish RESEMBLANCE, the hearer
must identify a common relation R that relates the
propositional content of two utterances and also the
number and identity of their arguments (Kehler,
2002). Resemblance relations include PARAL-
LEL, CONTRAST, EXEMPLIFICATION, GENERAL-
IZATION, EXCEPTION, and ELABORATION.
Non-canonicals are useful in resemblance rela-
tions because 1) the presence of ?old? material in a
non-canonical helps overrule the default coherence
relation of CONTIGUITY by making that interpreta-
tion less likely, and 2) the use of old material and
a structured proposition assists the hearer in iden-
tifying a common relation and corresponding argu-
ments needed to establish RESEMBLANCE.
This is illustrated in (9). The use of a left-
dislocation tells the hearer that the referent of a lot
of the doctors is in a salient set. By identifying that
set as {PROFESSIONAL PEOPLE}, the hearer can re-
alize that the information being added about a lot of
the doctors is going to be in an EXEMPLIFICATION
relation with the earlier statement that professional
people in general began to think of themselves as
disabled.
(9) During the Depression an awful lot of people
began to think of themselves as disabled, es-
pecially professional people, who depended on
clients whose business was on a cash basis?there
was no credit, this was a universe without credit
cards. A lot of the doctors, they were doing an
awful lot of charity work. They couldn?t sup-
port themselves. They?d have a little heart attack.
They?d have disability insurance. They went on
the insurance company rolls. A lot of doctors had
disability insurance and a lot of others too. A
lot of the insurance companies stopped underwrit-
ing disability insurance. They couldn?t afford it.
(SSA, hboral)
Ui-1 Ui?? Ui+1??
e1?? e3??
e4??e5?? e6?? e9??
e7??e8??e2??
Mi?? Mi+1??Mi-1?? ?? R??? R???
Figure 1: Approximating discourse relations (R)
between utterances (U ) by examining lexical dis-
course cues (M ) and relations between entities (e)
(10) A lot of the doctors were doing an awful lot of
charity work.
Without the left-dislocation, identifying the in-
clusion relationship between the set of professional
people and doctors is quite difficult. The preferred
interpretation of the canonical version in (10) is only
that the doctors were doing charity work for pro-
fessional people who had no credit cards. The left-
dislocation supports the additional inference that the
exemplification described above holds too.
3 Probabilistic model of discourse
relations and non-canonical syntax
To provide evidence beyond individual examples
for the phenomena in Section 2, we need to mea-
sure the correlation between discourse relations and
syntactic form, but annotating discourse relations
directly is problematic. Annotation of hierarchi-
cal discourse structure is difficult and subjective
although efforts have been made (Creswell et al,
2002; Marcu et al, 1999). Even annotating lin-
ear segmentation is challenging, particularly in the
vicinity of segment boundaries (Passonneau and
Litman, 1997). Annotation of the semantics of dis-
course relations requires a predetermined set of re-
lation types, on which theories vary widely, making
theory-neutral generalizations about the role of non-
canonical syntax impossible.
This project attempts to overcome these difficul-
ties by indirectly deriving discourse relations by
mapping from their known correlates to the use
of certain non-canonical forms. The correlates
used here are referential relations across utterance
boundaries and the presence and type of lexical dis-
course markers or cue words. These features are
annotated with respect to a three-utterance window
centered on a target utterance Ui, shown schemati-
cally in Figure 1.
These referential and lexical features build on the
work of Passonneau and Litman (1997), who use
them in discourse segmentation. Their use here is
extended to also derive information about the se-
Discourse Relations
Discourse Markers Sentence Types Referential Patterns
R2 R1
T x yL
W
I
Cmn
p ?
Figure 2: Influence of relations on independent and
dependent variables
mantic and syntactic properties of the relations be-
tween utterances.
As illustrated in Figure 2, discourse relations (e.g.
R1 ) influence observable patterns of referential re-
lations (e.g. x) and discourse markers (e.g. m). We
want to test whether discourse relations also influ-
ence the use of certain sentence types. However, the
discourse relations themselves are not observable
directly. To measure their correlation with sentence-
level syntax, we will only look at correlations of
referential patterns and discourse markers with syn-
tactic form. In the logistic regression analysis per-
formed here, syntactic form is the dependent vari-
able; referential relations and lexical cues are the
independent variables.
This analysis only measures the direct influence
of the independent variables on the dependent vari-
able, and does not model the existence of a mediat-
ing set of (unobserved) discourse relations, the re-
sult being that it is unable to capture correlations
among the independent variables. This inherent in-
adequacy of the model will be discussed further be-
low. Despite this inadequacy, a logistic regression
analysis is used because it is a mathematically con-
venient and well-understood way to model which
features of the independent variables are significant
in predicting the occurrence of each syntactic form,
while taking into account the rare prior probabilities
of the non-canonical syntactic forms.
In order to decide whether the featural models
provide evidence to support the claims about dis-
course relations and syntactic forms, we first need
to make clear our assumptions about how refer-
ential relations and lexical markers correlate with
discourse relations. Based on those assumptions,
testable predictions can then be made about how ref-
erential relations and lexical markers should corre-
late with syntactic forms.
Ref Utterances share center of attention; Cp of
3 first utterance is Cb of second utterance.
Mary?s a vegetarian. She never eats meat.
Ref Utterances have coreferential NPs.
2 Mary likes Fred. He?s very friendly.
Ref Utterances have inferentially-related NPs.
1 I bought an old bike. The tire was flat.
Ostriches are huge. Finches are little.
Ref Utterances have no NPs that share
0 reference.
Table 1: Values of referential relations feature
The lexical discourse marker feature is annotated
for the target Ui and its preceding (Ui?1) and fol-
lowing (Ui+1) utterances and has five values: and,
but, so, other or none. The predictions about the
correlations between these lexical features and syn-
tactic forms are based on the assumed correlations
between these lexical markers and discourse rela-
tions. First, if non-canonicals are indicators of at-
tentional stack pops, they should be more likely at
segment boundaries; hence, we expect an increased
presence of cue words (Passonneau and Litman,
1997) on non-canonicals compared to canonicals.
Predictions about the type of cue words are
based on the survey of lexical cue meanings from
Hirschberg and Litman (1994). Because and is an
indicator of segment-continuation and the relation
CONTIGUITY, we expect decreased incidence on
non-canonicals. However, we expect greater inci-
dence on Ui+1 when Ui is non-canonical because
Ui should be used to start a new segment. The pres-
ence of but indicates a CONTRAST relation. Non-
canonicals should have a greater likelihood of being
in contrast with either of the utterances surround-
ing them,2 so we expect a greater incidence of but
on both Ui and Ui+1 for non-canonicals than for
canonicals. The presence of so can indicate RE-
STATEMENT or RESULT, so so should appear more
often on Ui for wh-clefts, which are often used in
ELABORATION relations.
The referential features are four-valued and an-
notated with respect to pairs of utterances, (Ui?1,
Ui) and (Ui, Ui+1). The values here, described in
Table 1, form an implicational scale from strongest
to weakest connections, and the utterance pair is la-
beled with the strongest relation that holds.
In general, the more semantic content two utter-
ances share, the more likely they are to be related.
Referential connections are the measure of shared
content used here. Discourse relations vary in their
likelihood to be associated with certain values of
2See Creswell (2003) for examples and discussion.
Ref, shown in Table 2. For example, an utterance
immediately following a discourse pop, should be
unlikely to share a center with its immediately pre-
ceding utterance and be highly likely to share no ref-
erences at all. Two utterances in a RESEMBLANCE
relation (other than ELABORATION) are likely to
have inferential connections without coreferential
connections. Note that for nearly all of these pat-
terns, the correlation between a referential feature
value and the syntax or semantics of a discourse re-
lation is not absolute but only more or less likely.
Using a probabilistic model, however, allows for
patterns of relative likelihood in the data.
Based on the assumptions in Table 2, we can
now make predictions about expected correlations
between the referential features and utterances with
non-canonical word orders. These predictions are
based primarily on how we expect non-canonical
utterances to compare with canonical utterances.
However, when we test them on our data, we will
also compare each type of non-canonical utterance
with the others.
? Non-canonicals should be more likely than
canonicals to follow a POP and begin a new seg-
ment. They should have weaker referential ties
to the preceding utterance. They should have a
higher incidence of having no referential ties to
Ui?1, and a lower incidence of having no referen-
tial ties to Ui+1.
? Non-canonicals should be less likely than canon-
icals to have a NARRATIVE relation with either
Ui?1 or Ui+1. This situation predicts that with re-
spect to both of the utterances surrounding a non-
canonical utterance, these utterances will be less
likely to share the same center of attention than
when Ui is canonical.
? Non-canonicals should be more likely than
canonicals to be in RESEMBLANCE relations with
Ui?1 and/or Ui+1. So, a greater likelihood of ref-
erence to inferentially-related entities and smaller
likelihood of reference to coreferential entities or
shared centers in both the preceding and follow-
ing utterance is expected.
4 Results and discussion
To test the predictions about non-canonicals and
discourse relations, a corpus of 799 utterances with
non-canonical word order were extracted from 58
transcribed interviews from the Social Security Ad-
ministration Oral History Archives (SSA), a cor-
pus with ?750,000 words and 44,000 sentences. In
addition to the four types of non-canonicals, 200
randomly-selected controls with canonical word or-
der were also included. Table 4 lists the breakdown
Syntactic Type No. of Tokens
It-cleft 150
Left-dislocation 258
Topicalization 111
Wh-cleft 280
Control 200
Total 999
Table 3: Corpus of utterances by syntactic type
by syntactic type. The two lexical and three referen-
tial features described in the previous section were
annotated for each of the 999 utterances.
Logistic regression models for binary compar-
isons between each of the five sentence types were
then created. For 9 of 10 comparisons, at least one
of the five features were found to be significant.3
Table 4 lists all features found to be significant
for each of the ten comparisons, i.e. features that in-
dividually have a significant effect in improving the
likelihood of a model when compared to a model
that uses no features to predict the distribution of the
two classes.4 For comparisons with multiple fea-
tures significant at the five-percent level, the p-value
of the model fit in comparison with a fully saturated
model is listed in the fourth column of Table 4.
In order to understand the most likely context in
which a form to appears, we need to examine the
weights assigned to each feature value by the re-
gression analysis. The detailed feature weights in
the best model are listed in Table 5.
Table 6 summarizes the general conclusions we
can draw from these weights about the most favor-
able discourse contexts for each of the four types
of non-canonicals. For considerations of space, we
discuss in detail only one of the four types here, wh-
clefts. Wh-clefts are particularly relevant with re-
spect to the insights they provide into the inherent
limitations of our model of discourse relations.
Overall, wh-clefts are favored in contexts where
they start a new segment, one with weak connec-
tions with the preceding utterance and strong con-
nections with the following utterance. In particu-
lar, feature 4, REF(Ui?1,Ui), is significant in the
3The comparison of it-clefts and left-dislocations is the ex-
ception here. From the lack of significant features in this com-
parison we can surmise that the it-clefts and left-dislocations
are more similar to each other than any of the other forms com-
pared here.
4In particular, the measure whose significance is tested is
the -2?(difference in log-likelihoods of the models), which is
?2 distributed, where the number of degrees of freedom is the
difference in the total number of feature values between the two
models.
Relation between 3. Shared 2. Coreferring 1. Inferentially-related 0. No shared
Uj and Uk center entities entities only reference
SY
N
TA
C
T
IC POP unlikely less likely possible likely
PUSH, BEGIN possible likely possible unlikely
EMBEDDED SEG
CONTINUE IN highly possible possible highly
SAME SEG likely unlikely
SE
M
A
N
T
IC RESEMBLANCE unlikely possible likely impossible(not ELABORATION)
ELABORATION possible likely impossible impossible
NARRATIVE highly possible unlikely highly
likely unlikely
Table 2: Predictions from ref. features to discourse relations
CLASS VS. CLASS Feat. (p < .05) Feat. (p < .2) Overall Model Fit ?2 p-value
CONTROL, IT-CLEFT 2 5 (0.097) n.a.
CONTROL, LEFT-DIS. 4,5 3 (0.161) p=0.9289
CONTROL, TOPIC. 3 2 (0.178) n.a.
CONTROL, WH-CLEFT 2,4 3 (0.151) p=0.8696
IT-CLEFT, LEFT-DIS. ? 1 (0.106), 4 (0.086) ?
IT-CLEFT, TOPIC. 3 2 (0.092) , 4 (0.099) n.a.
IT-CLEFT, WH-CLEFT 4 1 (0.184) n.a.
LEFT-DIS, TOPIC. 3,4 5 (0.129) p =0.8561
LEFT-DIS, WH-CLEFT 1,4 5 (0.147) p=0.7615
TOPIC, WH-CLEFT 2,3,4 ? p=.6935 (with 3,4)
Table 4: Features significant at p < 0.05 and p < 0.2. Features significant at p < 0.01 are in bold. Features
1, 2, and 3 are discourse marker features on Ui?1, Ui, and Ui+1, respectively. Features 4 and 5 are referential
features for the pairs (Ui?1,Ui) and (Ui,Ui+1), respectively.
Sentence type Most Favorable Contexts
Topicalizations CONTINUE with Ui?1; CON-
TRAST with Ui?1 or Ui+1
Wh-clefts POP after Ui?1; CONTRAST
or CONTINUE with Ui+1
Left-dislocations POP after Ui?1 or RESEM-
BLANCE with Ui?1; CON-
TINUE with Ui+1
It-clefts No strong tendencies for be-
gin/end of segments; pos-
sible CONTRAST relations
with Ui?1, Ui+1
Table 6: Summary: favorable discourse contexts
comparison of wh-clefts with all other classes. Wh-
clefts are much more likely to share no connec-
tions at all with Ui?1 and less likely to share only
inferential connections when compared with any
other class. In comparison with everything but left-
dislocations, wh-clefts are also less likely to share
their center of attention with Ui?1.
In terms of discourse markers, feature 2 and 3
are significant when comparing topicalizations and
controls with wh-clefts (although feature 3 is only
weakly significant in comparing wh-clefts and con-
trols.) For feature 2, MARKER(Ui), wh-clefts are
less likely than either of the other two to appear with
and and more likely to appear with so. For feature
3, however, the presence of and on Ui+1 favors wh-
clefts over topicalizations and controls.
The most likely context in which to find wh-clefts
then is one with no referential connections to the
previous utterance and marked with the discourse
adverbial, so. When the Ui+1 begins with and, as-
sumed to be a marker of continuation of the previ-
ous content, wh-clefts are also favored. This pattern
resembles most closely the descriptions of a preced-
ing discourse POP and a subsequent discourse CON-
TINUE or NARRATIVE.
One use of wh-clefts that is not borne out conclu-
sively in the data is its use in ELABORATION rela-
tions, as in (11). Kehler (2002) describes elabora-
tions as a case of RESEMBLANCE where the predi-
cate and its arguments are the same, but described
from a different perspective or level of detail. The
hearer must infer the identity of the event and en-
CONTROL CONTROL CONTROL CONTROL IT-CLEFT IT-CL EFT IT-CLEFT LEFT-DIS. LEFT-DIS. TOPIC.
IT-CLEFT LEFT-DIS. TOPIC. WH-CLEFT LEFT-DIS. TOPI C. WH-CLEFT TOPIC. WH-CL EFT WH-CLEFT
1. MARK a 0.655
(Ui?1) b 0.326
s 0.500
o 0.542
n 0.481
2. MARK a 0.548 0.600 0.665
(Ui) b 0.399 0.444 0.604
s 0.249 0.246 0.172
o 0.689 0.628 0.483
n 0.628 0.602 0.630
3. MARK a 0.514 0.595 0.603 0.343
(Ui+1) b 0.266 0.232 0.268 0.804
s 0.408 0.406 0.470 0.579
o 0.738 0.685 0.641 0.311
n 0.574 0.602 0.533 0.429
4. REF 0 0.418 0.352 0.334 0.662 0.420 0.279
(Ui?1, Ui) 1 0.443 0.575 0.639 0.566 0.634 0. 574
2 0.511 0.493 0.458 0.501 0.493 0.467
3 0.627 0.583 0.571 0.280 0.450 0.686
5. REF 0 0.785
(Ui,Ui+1) 1 0.401
2 0.370
3 0.409
Table 5: Individual feature weights in best model. Feature weights >0.5 favor the application value (class
category) listed first; weights <0.5 favor the second application value. The farther away from 0.5, the
stronger the feature value?s effect on the distinction between the two classes.
tities being described in the two segments. If wh-
clefts are associated with ELABORATIONS, then we
should see an increased incidence of close referen-
tial connections with Ui?1 and an increased inci-
dence of so, a marker of restatement. In the results,
however, we only see evidence for the latter.
(11) S: How did you develop this Resource-Based
Relative Value Scale at this point?
H: We basically treated this as a research
project because most of us involved realized
we had some past failures and we should
not over-promise. We should be prepared to
face up to the world and say, ?We cannot
make the theory operational.? So what we did
was we continued to accept the theoretical
premise, that is the rational and objective
price should be based on the cost of the
service. Then we asked, ?What constitutes
the cost of physicians? services and what are
the components of physicians? work?? (SSA,
hsiao)
A possible factor in the absence of evidence here
is that wh-clefts are also associated with discourse
pops, which increase the likelihood of having no
referential connections with the previous utterance.
The logistic regression model used here aggregates
over all possible discourse relations. So, when two
discourse relations that give rise to different lexi-
cal and referential patterns are both associated with
a single sentence type, the patterns of one may ob-
scure the patterns of the other. A more sophisticated
statistical model might take into account dependen-
cies between discourse markers and referential pat-
terns and from them posit hidden states which cor-
respond to different discourse relations. Then based
upon these hidden states, the model would predict
which sentence type would best fit the context. Such
a model would be more true to Figure 2.
Another limitation of the model shown here is
that the only indicators in this model of starting a
new segment are weak or absent referential rela-
tions, presence of a connective like so, and absence
of and. These measures will not necessarily distin-
guish between continuing in the same segment or
beginning a new segment which includes recently-
mentioned discourse-old entities.
5 Conclusions and potential applications
The statistical model here uses a combination of ref-
erential and lexical features annotated for a small
window surrounding the target utterance to repre-
sent the local discourse context surrounding utter-
ances with non-canonical and canonical word or-
ders. The primary goal was to model the correla-
tions between discourse relations and non-canonical
syntax. Due to the difficulties inherent in annotating
discourse relations directly, the featural approxima-
tion was devised as a practical alternative.
Overall, the method used here yielded some in-
teresting new insights into the contexts that favor
the use of four types of non-canonical word order.
The complexity of this approach does make it diffi-
cult to draw simple conclusions about the relation-
ship between discourse relations and non-canonical
syntactic forms. However, the strength of some of
the correlations found here merits further investiga-
tion. The data also lend support for the idea that
some aspects of discourse relations, both syntactic
and semantic, can be inferred from combinations of
lower-level linguistic features.
An important factor in improving upon the cur-
rent project is the need for larger amounts of data.
The significance of any particular feature is greatly
affected by the quantity of data. This was a par-
ticular issue for the lexical feature values, where it
prevented inclusion of several of the less frequent
connectives with better understood discourse struc-
turing properties, like well and now. In addition,
more data may also be required in order to support
the use of more complex statistical models. Auto-
matic methods of annotating the referential features
or the availability of larger corpora marked up with
coreferential and inferential relations and with a rich
variety of syntactic forms could be used to test more
accurately the predictions in Section 3.
The technique used here for approximating dis-
course relations through more easily annotated fea-
tures has at least two interesting potential applica-
tions. One, given the significant correlation of these
features with non-canonical word order variation,
the discriminative models trained here could be used
as classifiers which could label discourse contexts
(feature vectors) with the form best suited to the
context for the surface realization stage in a natu-
ral language generation system.
Secondly, the feature set used here could be ap-
plied to the problem of automatic classification of
discourse relations. In conjunction with a rela-
tively small set of pairs of sentences for which
there is high inter-annotator agreement when hand-
annotated for type of discourse relation, the lexical
and referential features here could serve as an initial
feature set for bootstrapping the development of a
statistical discourse relation classifier. This applica-
tion would require stipulation of a predetermined set
of discourse relations?a requirement the present
study wished to avoid. However, given the practical
need for a statistical relation classifier, a set of rela-
tions could be constructed suitable to the domain of
use.
References
Birner, B., and G. Ward. 1998. Information status and non-
canonical word order in English. John Benjamins.
Creswell, C. 2003. Syntactic form and discourse function in
natural language generation. Doctoral Dissertation, Uni-
versity of Pennsylvania.
Creswell, C., K. Forbes, E. Miltsakaki, R. Prasad, A. Joshi, and
B. Webber. 2002. The discourse anaphoric properties of
connectives. In Proceedings of DAARC 4, 45?50. Lisbon,
Portugal: Edicoes Colibri.
Delin, J. 1995. Presupposition and shared knowledge in it-
clefts. Language and Cognitive Processes 10:97?120.
Grosz, B. J., and C. L. Sidner. 1986. Attention, intentions,
and the structure of discourse. Computational Linguistics
12:175?204.
Halliday, M. A. K. 1985. An introduction to functional gram-
mar. Baltimore: Edward Arnold Press.
Hirschberg, J., and D. J. Litman. 1994. Empirical studies on the
disambiguation of cue phrases. Computational Linguistics
19:501?530.
Hobbs, J. R. 1990. Literature and cognition. Stanford: CSLI.
Kehler, A. 2002. Coherence, reference, and the theory of gram-
mar. CSLI Publishers.
Labov, W. 1997. Some further steps in narrative analysis. Jour-
nal of Narrative and Life History .
Mann, W. C., and S. A. Thompson. 1988. Rhetorical Structure
Theory: towards a functional theory of text organization.
Text 8:243?281.
Marcu, D., E. Amorrortu, and M. Romera. 1999. Experiments
in constructing a corpus of discourse trees. In Proceedings
of the ACL workshop: Towards standards and tools for
discourse tagging, ed. M. Walker, 48?57.
Passonneau, R., and D. Litman. 1997. Discourse segmentation
by human and automated means. Computational Linguis-
tics 23:103?139.
Prince, E. F. 1978. A comparison of wh-clefts and it-clefts in
discourse. Language 54:883?906.
Webber, B., A. Knott, M. Stone, and A. Joshi. 1999. Discourse
relations: a structural and presuppositional account using
lexicalised TAG. In ACL 37, 41?48. College Park, MD.
