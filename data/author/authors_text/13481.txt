Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 340?348,
Beijing, August 2010
Opinosis: A Graph-Based Approach to Abstractive Summarization of
Highly Redundant Opinions
Kavita Ganesan and ChengXiang Zhai and Jiawei Han
Department of Computer Science
University of Illinois at Urbana-Champaign
{kganes2,czhai,hanj}@cs.uiuc.edu
Abstract
We present a novel graph-based summa-
rization framework (Opinosis) that generates
concise abstractive summaries of highly re-
dundant opinions. Evaluation results on sum-
marizing user reviews show that Opinosis
summaries have better agreement with hu-
man summaries compared to the baseline ex-
tractive method. The summaries are readable,
reasonably well-formed and are informative
enough to convey the major opinions.
1 Introduction
Summarization is critically needed to help users
better digest the large amounts of opinions ex-
pressed on the web. Most existing work in Opin-
ion Summarization focus on predicting sentiment
orientation on an entity (Pang et al, 2002) (Pang
and Lee, 2004) or attempt to generate aspect-based
ratings for that entity (Snyder and Barzilay, 2007)
(Lu et al, 2009)(Lerman et al, 2009)(Titov and
Mcdonald, 2008). Such summaries are very infor-
mative, but it is still hard for a user to understand
why an aspect received a particular rating, forcing
a user to read many, often highly redundant sen-
tences about each aspect. To help users further di-
gest the opinions in each aspect, it is thus desirable
to generate a concise textual summary of such re-
dundant opinions.
Indeed, in many scenarios, we will face the
problem of summarizing a large number of highly
redundant opinions; other examples include sum-
marizing the ?tweets? on Twitter or comments
made about a blog or news article. Due to the sub-
tle variations of redundant opinions, typical extrac-
tive methods are often inadequate for summarizing
such opinions. Consider the following sentences:
1. The iPhone?s battery lasts long, only had to
charge it once every few days.
2. iPhone?s battery is bulky but it is cheap..
3. iPhone?s battery is bulky but it lasts long!
With extractive summarization, no matter which
single sentence of the three is chosen as a sum-
mary, the generated summary would be biased.
In such a case, an abstractive summary such as
?iPhone?s battery is cheap, lasts long but is bulky?
is a more complete summary, conveying all the
necessary information. Extractive methods also
tend to be verbose and this is especially problem-
atic when the summaries need to be viewed on
smaller screens like on a PDA. Thus, an informa-
tive and concise abstractive summary would be a
better solution.
Unfortunately, abstractive summarization is
known to be difficult. Existing work in abstractive
summarization has been quite limited and can be
categorized into two categories: (1) approaches us-
ing prior knowledge (Radev and McKeown, 1998)
(Finley and Harabagiu, 2002) (DeJong, 1982) and
(2) approaches using Natural Language Genera-
tion (NLG) systems (Saggion and Lapalme, 2002)
(Jing and McKeown, 2000). The first line of work
requires considerable amount of manual effort to
define schemas such as frames and templates that
can be filled with the use of information extraction
techniques. These systems were mainly used to
summarize news articles. The second category of
work uses deeper NLP analysis with special tech-
niques for text regeneration. Both approaches ei-
ther heavily rely on manual effort or are domain
dependent.
In this paper, we propose a novel flexible sum-
marization framework, Opinosis, that uses graphs
to produce abstractive summaries of highly redun-
dant opinions. In contrast with the previous work,
Opinosis assumes no domain knowledge and uses
shallow NLP, leveraging mostly the word order in
the existing text and its inherent redundancies to
generate informative abstractive summaries. The
key idea of Opinosis is to first construct a tex-
tual graph that represents the text to be summa-
rized. Then, three unique properties of this graph
are used to explore and score various subpaths
that help in generating candidate abstractive sum-
maries.
Evaluation results on a set of user reviews show
that Opinosis summaries have reasonable agree-
ment with human summaries. Also, the gener-
340
ated summaries are readable, concise and fairly
well-formed. Since Opinosis assumes no do-
main knowledge and is highly flexible, it can
be potentially used to summarize any highly re-
dundant content and could even be ported to
other languages. (All materials related to this
work including the dataset and demo software can
be found at http://timan.cs.uiuc.edu/
downloads.html.)
2 Opinosis-Graph
Our key idea is to use a graph data structure (called
Opinosis-Graph) to represent natural language text
and cast this abstractive summarization problem
as one of finding appropriate paths in the graph.
Graphs have been commonly used for extractive
summarization (e.g., LexRank (Erkan and Radev,
2004) and TextRank (Mihalcea and Tarau, 2004)),
but in these works the graph is often undirected
with sentences as nodes and similarity as edges.
Our graph data structure is different in that each
node represents a word unit with directed edges
representing the structure of sentences. Moreover,
we also attach positional information to nodes as
will be discussed later.
Algorithm 1 (A1): OpinosisGraph(Z)
1: Input: Topic related sentences to be summarized: Z = {zi}ni=12: Output: G = (V,E)
3: for i = 1 to n do4: w ? Tokenize(zi)5: sent size? SizeOf(w)
6: for j = 1 to sent size do
7: LABEL? wj8: PID ? j
9: SID ? i10: ifExistsNode(G,LABEL) then
11: vj ? GetExistingNode(G,LABEL)12: PRIvj ? PRIvj ? (SID, PID)13: else14: vj ? CreateNewNode(G,LABEL)15: PRIvj ? (SID, PID)16: end if
17: if notExistsEdge(vj?1 ? vj , G) then18: AddEdge(vj?1 ? vj , G)19: end if20: end for21: end for
Our graph representation is closer to that used by
Barzilay and Lee (Barzilay and Lee, 2003) for the
task of paraphrasing, wherein each node in the
graph represents a unique word. However, in their
work, such a graph is used to identify regions of
commonality and variability amongst similar sen-
tences. Thus, the positional information is not re-
quired nor is it maintained. In contrast, we main-
tain positional information at each node as this is
critical for the selection of candidate paths.
Algorithm A1 outlines the steps involved in
building an Opinosis-Graph. We start with a set
of sentences relevant to a specific topic, which can
be obtained in different ways depending on the ap-
plication. For example, they may be all sentences
related to the battery life of the iPod Nano. We de-
note these sentences as Z = {zi}ni=1 where each ziis a sentence containing part-of-speech (POS) an-
notations. (A1:4) Each zi ? Z is split into a set
of word units, where each unit, wj consists of a
word and its corresponding POS annotation (e.g.
?service:nn?, ?good:adj?). (A1:7-9) Each unique
wj will form a node, vj , in the Opinosis-Graph,
with wj being the label. Also, since we only have
one node per unique word unit, each node keeps
track of all sentences that it is a part of using a sen-
tence identifier (SID) along with its position of oc-
currence in that sentence (PID). (A1:10-16) Each
node will thus carry a Positional Reference Infor-
mation (PRI) which is a list of {SID:PID} pairs
representing the node?s membership in a sentence.
(A1:17-19) The original structure of a sentence is
recorded with the use of directed edges. Figure 1
shows a resulting Opinosis-Graph based on four
sentences.
The Opinosis-Graph has some unique proper-
ties that are crucial in generating abstractive sum-
maries. We highlight some of the core properties
by drawing examples from Figure 1:
Property 1. (Redundancy Capture). Highly re-
dundant discussions are naturally captured by sub-
graphs.
Figure 1 shows that although the phrase ?great de-
vice? was mentioned in different parts of sentences
(1) and (3), this phrase forms a relatively heavy
sub-path in the resulting graph. This is a good in-
dication of salience.
Property 2. (Gapped Subsequence Capture). Ex-
isting sentence structures introduce lexical links
that facilitate the discovery of new sentences or re-
inforce existing ones.
The main point conveyed by sentences (2) and (3)
in Figure 1 is that calls drop frequently. However,
this is expressed in slightly different ways and is
reflected in the resulting subgraph. Since sentence
(2) introduces a lexical link between ?drop? and
?frequently?, the word ?too? can be ignored for sen-
tence (3) as the same amount of information is re-
tained. This is analogous to capturing a repetitive
gapped subsequence where similar sequences with
minor variations are captured. With this, the sub-
graph calls drop frequently can be considered re-
dundant.
Property 3. (Collapsible Structures). Nodes that
resemble hubs are possibly collapsible.
In Figure 1 we see that the subgraph ?the iPhone
is?, is fairly heavy and the ?is? node acts like a
341
my
pho
ne
call
s
freq
uen
tly
too {3:8}
with
dro
p ipho
ne
is
a
my {2:1}
pho
ne
{2:2
}
call
s
{2:3
, 3:
6}
freq
uen
tly
{2:5
, 3:
9}
with {2:6
}
the
dro
p
{2:4
, 3:
7}
gre
at
{1:5
, 3:
1}
{1:2
, 2:
8, 4
:2}
{1:3
,4:3
}{1
:4}
.
{1:7
,2:
9,3
:10
}
{1:1
, 2:
7, 3
:5, 
4:1
,4:5
}
wor
th
pric
e
{4:6
}
{
,
}
, {3:3}
but {3:4}{1:7
,2:
9,3
:10
}
wor
th {4:4
} no
de 
labe
l
SID
:PID
 pa
irs
dev
ice
{1:6
, 3:
2}
Inp
ut:
SID
:1.
The
 iPh
one
is a
 gre
at d
evi
ce.
 
SID
:2.
 My
 ph
one
 ca
lls 
dro
p fr
equ
ent
ly w
ith 
the
 iPh
one
. 
SID
:3.
 Gr
eat
 de
vice
, bu
t th
e c
alls
 dro
p to
o fr
equ
ent
ly.
p ,
p
q
y
SID
:4.
 Th
e iP
hon
eis
 wo
rth
 the
 pri
ce.
Figure 1: Sample Opinosis-Graph. Thick edges
indicate salient paths.
?hub? where it connects to various other nodes.
Such a structure is naturally captured by the
Opinosis-Graph and is a good candidate for com-
pression to generate a summary such as ?The
iPhone is a great device and is worth the price?.
Also, certain word POS (e.g. linking verbs like
?is? and ?are?) often carry hub-like properties that
can be used in place of the outlink information.
3 Opinosis Summarization Framework
In this section, we describe a general framework
for generating abstractive summaries using the
Opinosis-Graph. We also describe our implemen-
tation of the components in this framework.
At a high level, we generate an abstractive sum-
mary by repeatedly searching the Opinosis graph
for appropriate subgraphs that both encode a valid
sentence (thus meaningful sentences) and have
high redundancy scores (thus representative of the
major opinions). The sentences encoded by these
subgraphs would then form an abstractive sum-
mary.
Going strictly by the definition of true abstrac-
tion (Radev et al, 2002), our problem formula-
tion is still more extractive than abstractive be-
cause the generated summary can only contain
words that occur in the text to be summarized;
our problem definition may be regarded as a word-
level (finer granularity) extractive summarization.
However, compared to the conventional sentence-
level extractive summarization, our formulation
has flavors of abstractive summarization wherein
we have elements of fusion (combining extracted
portions) and compression (squeezing out unim-
portant material from a sentence). Hence, the sen-
tences in the generated summary are generally not
the same as any original sentence. Such a ?shal-
low? abstractive summarization problem is more
tractable, enabling us to develop a general solution
to the problem. We now describe each component
in such a summarization framework.
3.1 Valid Path
A valid path intuitively refers to a path that corre-
sponds to a meaningful sentence.
Definition 1. (Valid Start Node - VSN). A node vq
is a valid start node if it is a natural starting point
of a sentence.
We use the positional information of a node to de-
termine if it is a VSN. Specifically, we check if
Average(PIDvq) ? ?vsn, where ?vsn is a pa-rameter to be empirically set. With this, we only
qualify nodes that tend to occur early on in a sen-
tence.
Definition 2. (Valid End Node - VEN). A node vs
is a valid end point if it completes a sentence.
We use the natural ending points in the text to be
summarized as hints to which node may be a valid
end point of a path (i.e., a sentence). Specifically,
a node is a valid end node if (1) the node is a
punctuation such as period and comma or (2) the
node is any coordinating conjunction (e.g., ?but?
and ?yet?).
Definition 3. (Valid Path). A path W = {vq...vs}
is valid if it is connected by a set of directed edges
such that (1) vq is a VSN, (2) vs is a VEN, and
(3) W satisfies a set of well-formedness POS con-
straints.
Since not every path starting with a VSN and end-
ing at a VEN encodes a meaningful sentence, we
further require a valid path to satisfy the following
POS constraints (expressed in regular-expression)
to ensure that a valid path encodes a well-formed
sentence:
1. . ? (/nn) + . ? (/vb) + . ? (/jj) + .?
2. . ? (/jj) + . ? (/to) + . ? (/vb).?
3. . ? (/rb) ? . ? (/jj) + . ? (/nn) + .?
4. . ? (/rb) + . ? (/in) + . ? (/nn) + .?
This also provides a way (if needed) for the appli-
cation to generate only specific type of sentences
like comparative sentences or strictly opinionated
sentences. These rules are thus application spe-
cific.
3.2 Path Scoring
Intuitively, to generate an abstractive summary, we
should select a valid path that can represent most of
the redundant opinions well. We would thus favor
a valid path with a high redundancy score.
Definition 4. (Path Redundancy). Let W =
{vq...vs} be a path from an Opinosis-Graph. The
path redundancy of W , r(q, s), is the number of
overlapping sentences covered by this path, i.e.,
342
r(q, s) = nq??nq+1...??ns,
where ni = PRIvi and ?? is the intersection be-tween two sets of SIDs such that the difference be-
tween the corresponding PIDs is no greater than
?gap, and ?gap > 0 is a parameter.
Path redundancies provide good indication of how
many sentences discuss something similar at each
point in the path. The ?gap parameter controls the
maximum allowed gaps in discovering these re-
dundancies. Thus, a common sentence X between
nodes vq and vr, will be considered a valid inter-
sect if (PIDvrx ? PIDvqx) ? ?gap.Based on path redundancy, we propose several
ways to score a path for the purpose of selecting a
good path to include in the summary:
1. Sbasic(W ) = 1|W |
?s
k=i+1,i r(i, k)
2. Swt len(W ) = 1|W |
?s
k=i+1,i |vi, vk| ? r(i, k)
3. Swt loglen(W ) = 1|W |(r(i, i+ 1) +?s
k=i+2,i+1 log2|vi, vk| ? r(i, k))
vi is the first node in the path being scored and vs
is the last node. |vi, vk| is the length from node vi
to vk. |W | is the length of the entire path being
scored. The Sbasic scoring function scores a path
purely based on the level of redundancy. One could
also argue that high redundancy on a longer path is
intuitively more valuable than high redundancy on
a shorter path as the former would provide better
coverage than the latter. This intuition is factored
in by the Swt len and Swt loglen scoring functions
where the level of redundancy is weighted by the
path length. Swt loglen is similar to Swt len only
that it scales down the path length so that it does
not entirely dominate.
3.3 Collapsed paths
In some cases, paths in the Opinosis-Graph may be
collapsible (as explained in Section 2). In such a
case, the collapse operation is performed and then
the path scores are computed. We will now ex-
plain a few concepts related to collapsible struc-
tures. Let W? = {vi...vk} be a path from the
Opinosis-Graph.
Definition 5. (Collapsible Node). Node vk is a
candidate for collapse if its POS is a verb.
We only attempt to collapse nodes that are verbs
due to the heavy usage of verbs in opinion text and
the ease with which the structures can be combined
to form a new sentence. However, as mentioned
earlier other properties like the outlink information
can be used to determine if a node is collapsible.
Definition 6. (Collapsed Candidates, Anchor).
Let vk be a collapsible node. The collapsed can-
didates of vk (denoted by CC = {cci}mi=1) are the
Canchor CC Connectora. the sound quality is cc1 : really good and
cc2 : clearb. the iphone is cc1 : great but
cc2 : expensive
Table 1: Example of anchors, collapsed candidates
and suitable connectors
remaining paths after vk in all the valid paths go-
ing through vi...vk. The prefix vi...vk is called the
anchor, denoted as Canchor = {vi...vk}. Each
path {vi...vn}, where vn is the last node in each
cci ? CC, is an individually valid path.
Table 1 shows a simplistic example of anchors and
corresponding collapsed candidates. Once the an-
chor and collapsed candidates have been identified,
the task is then to combine all of these to form a
new sentence.
Definition 7. (Stitched Sentence) A stitched sen-
tence is one that combines Canchor and CC to
form a combined, logical sentence.
We will now describe the stitching procedure that
we use, by drawing examples from Table 1. Since
we are dealing with verbs, Canchor can be com-
bined with the corresponding CC with commas
to separate each cci ? CC with one exception -
the correct sentence connector has to be used for
the last cci. For Canchora , the phrases really goodand clear can be connected by ?and? due to the
same sentiment orientation. For Canchorb , the col-lapsed candidate phrases are well connected by the
word ?but?. We use the existing Opinosis-Graph
to determine the most appropriate connector. We
do this by looking at all coordinating conjunction
(e.g. ?but?, ?yet?) nodes (vcconj) that are connected
to the first node of the last collapsed candidate,
ccm. This would be the node labeled ?clear? for
Canchora and ?expensive? for Canchorb . We denotethese nodes as v0,ccm . The vcconj , with the high-est path redundancy with v0,ccm , will be selectedas the connector.
Definition 8. (Collapsed Path Score) The final
path score after the entire collapse operation is the
average across path scores computed from vi to the
last node in each cci ? CC.
The collapsed path score essentially involves com-
puting the path scores of the individual sentences
assuming that they are not collapsed and then av-
eraging them.
3.4 Generation of summary
Once we can score all the valid paths as well as all
the collapsed paths, the generation of an abstrac-
tive summary can be done in two steps: First, we
rank all the paths (including the collapsed paths)
in descending order of their scores. Second, we
343
eliminate duplicated (or extremely similar) paths
by using a similarity measure (in our experiments,
we used Jaccard). We then take the top few re-
maining paths as the generated summary, with the
number of paths to be chosen controlled by a pa-
rameter ?ss, which represents summary size.
Although conceptually we enumerate all the
valid paths, in reality we can use a redundancy
score threshold, ?r to prune many non-promising
paths. This is reasonable because we are only in-
terested in paths with high redundancy scores.
4 Summarization Algorithm
Algorithms A2 and A3 describe the steps involved
in Opinosis Summarization. A2 is the starting
point of the Opinosis Summarization and A3 is a
subroutine where path finding takes place, invoked
from within A2.
Algorithm 2 (A2): OpinosisSummarization(Z)
1: Input: Topic related sentences to be summarized: Z = {zi}ni=12: Output: O ={Opinosis Summaries}
3: g ? OpinosisGraph(Z)
4: node size? SizeOf(g)
5: for j = 1 to node size do
6: if V SN(vj) then7: pathLen? 1
8: score? 09: cList? CreateNewList()
10: Traverse(cList, vj , score, PRIvj , labelvj , pathLen)11: candidates? {candidates ? cList}
12: end if13: end for14: C ? EliminateDuplicates(candidates)
15: C ? SortByPathScore(C)
16: for i = 1 to ?ss do17: O = {O ? PickNextBestCandidate(C)}
18: end for
(A2:3) Opinosis Summarization starts with the
construction of the Opinosis-Graph, described in
detail in Section 2. This is followed by the depth
first traversal of this graph to locate valid paths
that become candidate summaries. (A2:6-12) To
achieve this, each node vj in the Opinosis-Graph
is examined to determine if it is a VSN and, if it
is, path finding will start from this node by invok-
ing subroutine A3. A3 takes the following as in-
put: list - a list to hold candidate summaries; vi
- the node to continue traversal from; score - the
accumulated path score; PRIoverlap - the intersect
between PRIs of all nodes visited so far (see Defi-
nition 4); sentence - the summary sentence formed
so far; len - the current path length. (A2:7-10) Be-
fore invoking A3 from A2, the path length is set to
?1?, path score is set to ?0? and a new list is cre-
ated to store candidate summaries generated from
node vj . (A2:11) All candidate summaries gener-
ated from vj will be stored in a common pool of
candidate summaries.
Algorithm 3 (A3): Traverse(...)
1: Input: list, vk ? V , score, PRIoverlap, sentence, len2: Output: A set of candidate summaries
3: redundancy ? SizeOf(PRIoverlap)4: if redundancy ? ?r then5: if V EN(vk) then6: if V alidSentence(sentence) then
7: finalScore? scorelen8: AddCandidate(list, sentence, finalScore)
9: end if10: end if
11: for vn ? Neighborsvk do12: PRInew ? PRIoverlap ?? PRIvn13: redundancy ? SizeOf(PRInew)14: newSent? Concat(sentence, labelvn )15: L? len+ 116: newScore? score+ PathScore(redundancy, L)
17: if Collapsible(vn) then18: Canchor ? newSent19: tmp? CreateNewList()
20: for vx ? Neighborsvn do21: Traverse(tmp, vx, 0, PRInew, labelvx , L)22: CC ? EliminateDuplicates(tmp)
23: CCPathScore? AveragePathScore(CC)
24: finalScore? newScore+ CCPathScore
25: stitchedSent? Stitch(Canchor, CC)26: AddCandidate(list, stitchedSent, finalScore)
27: end for28: else29: Traverse(list, vn, newScore, PRInew, newSent, L)30: end if31: end for32: end if
(A3:3-4) Algorithm A3 starts with a check to
ensure that the minimum path redundancy require-
ment is satisfied (see definition 4). For the very
first node sent from A2, the path redundancy is the
size of the raw PRI . (A3:5-10) If the redundancy
requirement is satisfied, a few checks are done to
determine if a valid path has been found. If it has,
then the resulting sentence and its final score are
added to the list of candidate summaries.
(A3:11-31) Traversal proceeds recursively
through the exploration of all neighboring nodes
of the current node, vk. (A3:12-16) For every
neighboring node, vn the PRI overlap information,
path length, summary sentence and path score
are updated before the next recursion. (A3:29)
If a vn is not collapsible, then a regular traver-
sal takes place. (A3:17-27) However, if vn is
collapsible, the updated sentence in A3:14, will
now serve as an anchor in A3:18. (A3:21) A3
will then attempt to start a recursive traversal
from all neighboring nodes of vn in order to find
corresponding collapsed candidates. (A3:22-26)
After this, duplicates are eliminated from the
collapsed candidates and the collapsed path score
is computed. The resulting stitched sentence and
its final score are then added to the original list of
candidate summaries.
(A2:14-18) Once all paths have been explored
344
for candidate generation, duplicate candidates are
removed and the remaining are sorted in descend-
ing order of their path scores. The best ?ss candi-
dates are ?picked? as final Opinosis summaries.
5 Experimental Setup
We evaluate this abstractive summarization task
using reviews of hotels, cars and various prod-
ucts1. Based on these reviews, 2 humans were
asked to construct ?opinion seeking? queries which
would consist of an entity name and a topic of in-
terest. Example of such queries are: Amazon Kin-
dle:buttons, Holiday Inn, Chicago: staff, and so
on. We compiled a set of 51 such queries. We cre-
ate one review document per query by collecting
all review sentences that contain the query words
for the given entity. Each review document thus
consists of a set of unordered, redundant review
sentences related to the query. There are approxi-
mately 100 sentences per review document.
We use ROUGE (Lin, 2004b) to quantitatively
assess the agreement of Opinosis summaries with
human composed summaries. ROUGE is based on
an n-gram co-occurrence between machine sum-
maries and human summaries and is a widely ac-
cepted standard for evaluation of summarization
tasks. In our experiments, we use ROUGE-1,
ROUGE-2 and ROUGE-SU4 measures. ROUGE-
1 and ROUGE-2 have been shown to have most
correlation with human summaries (Lin and Hovy,
2003) and higher order ROUGE-N scores (N > 1)
estimate the fluency of summaries.
We use multiple reference (human) summaries
in our evaluation since it can achieve better cor-
relation with human judgment (LIN, 2004a). We
leverage Amazon?s Online Workforce2 to get 5 dif-
ferent human workers to summarize each review
document. The workers were asked to be concise
and were asked to summarize the major opinions in
the review document presented to them. We manu-
ally reviewed each set of reference summaries and
dropped summaries that had little or no correlation
with the majority. This left us with around 4 refer-
ence summaries for each review document.
To allow performance comparison between hu-
mans, Opinosis and the baseline method, we im-
plemented a Jackknifing procedure where, given K
references, the ROUGE score is computed over K
sets of K-1 references. With this, average human
performance is computed by treating each refer-
ence summary as a ?system? summary, computing
ROUGE scores over the remaining K-1 reference
1Reviews collected from Tripadvisor, Amazon, Edmunds
2https://www.mturk.com
summaries.
Due to the limited work in abstractive sum-
marization, no natural baseline could be used for
comparison. The existing work in this area is
mostly domain dependent and requires too much
manual effort (explained in Section 1). The next
best baseline is to use a state of the art extractive
method. Thus, we use MEAD (Radev et al, 2000)
as our baseline. MEAD is an extractive summa-
rizer based on cluster centroids. It uses a collection
of the most important words from the whole clus-
ter to select the best sentences for summarization.
By default, the scoring of sentences in MEAD is
based on 3 parameters - minimum sentence length,
centroid, and position in text. MEAD was ideal
for our task because a good summary in our case
would be one that could capture the most essential
information. This is exactly what centroid-based
summarization aims to achieve. Also, since the po-
sition in text parameter is irrelevant in our case, we
could easily turn this off with MEAD.
We introduce a readability test to understand if
Opinosis summaries are in fact readable. Suppose
we have N sentences from a system-generated
summary and M sentences from corresponding
human summaries. We mix all these sentences
and then ask a human assessor to pick at most N
sentences that are least readable as the prediction
of system summary.
readability(O) = 1? #CorrectPickN
If the human assessor often picks out system gen-
erated summaries as being least readable, then the
readability of system summaries is poor. If not,
then the system generated summaries are no dif-
ferent from human summaries.
6 Results
The baseline method (MEAD) selects 2 most rep-
resentative sentences as summaries. To give a fair
comparison, we fix the Opinosis summary size,
?ss = 2. We also fix ?vsn = 15. The best Opinosis
configuration with ?ss = 2 and ?vsn = 15 is
called Opinosisbest (?gap = 4, ?r = 2, Swt loglen).
ROUGE scores reported are with the use of stem-
ming and stopword removal.
Performance comparison between humans,
Opinosis and baseline. Table 2 shows the perfor-
mance comparison between humans, Opinosisbest
and the baseline method. First, we see that the
baseline method has very high recall scores com-
pared to Opinosis. This is because extractive meth-
ods that just ?select? sentences tend to be much
longer resulting in higher recall. However, these
summaries tend to carry information that may not
be significant and is clearly reflected by the poor
345
Recall
ROUGE-1 ROUGE-2 ROUGE-SU4 Avg # Words
Human 0.3184 0.1106 0.1293 17
Opinosis 0.2831 0.0853 0.0851 15
Baseline 0.4932 0.1058 0.2316 75
Precision
ROUGE-1 ROUGE-2 ROUGE-SU4 Avg # Words
Human 0.3434 0.1210 0.1596 17
Opinosis 0.4482 0.1416 0.2261 15
Baseline 0.0916 0.0184 0.0102 75
F-score
ROUGE-1 ROUGE-2 ROUGE-SU4 Avg # Words
Human 0.3088 0.1069 0.1142 17
Opinosis 0.3271 0.0998 0.1027 15
Baseline 0.1515 0.0308 0.0189 75
Table 2: Performance comparison between Hu-
mans, Opinosisbest and Baseline.
0.110
0330
0.1000.110
E-SU4
0.3100.330
GE-1
0.0900.1000.110
ROUG
basi
c
0.2900.3100.330
ROU
basi
c
00700.0800.0900.1000.110
basi
c
wt_l
ogle
n
wt_l
en
0.2700.2900.3100.330
basi
c
wt_l
ogle
n
wtl
en
0.0700.0800.0900.1000.110
1
2
3
4
5basi
c
wt_l
ogle
n
wt_l
en
0.2500.2700.2900.3100.330
1
2
3
4
5bas
ic wt_l
ogle
n
wt_l
en
?gap
?gap
0.0700.0800.0900.1000.110
1
2
3
4
5basi
c
wt_l
ogle
n
wt_l
en
0.2500.2700.2900.3100.330
1
2
3
4
5bas
ic wt_l
ogle
n
wt_l
en
?gap
?gap
Figure 2: ROUGE scores (f-measure) at different
levels of ?gap, ?r = 2.
precision scores.
Next, we see that humans have reasonable
agreement amongst themselves given that these are
independently composed summaries. This agree-
ment is especially clear with the ROUGE-2 re-
call score where the recall is better than Opinosis
but comparable to the baseline even though the
summaries are much shorter. It is also clear that
Opinosis is closer in performance to humans than
to the baseline method. The recall scores of
Opinosis summaries are slightly lower than that
achieved by humans, while the precision scores are
higher (Wilcoxon test shows that the increase in
precision is statistically more significant than the
decrease in recall). In terms of f-scores, Opinosis
has the best ROUGE-1 score and its ROUGE-2 and
ROUGE-SU4 scores are comparable with human
performance. The baseline method has the low-
est f-scores. The difference between the f-scores
of Opinosis and that of humans is statistically in-
significant.
Comparison of scoring functions. Next, we look
into the performance of the three scoring func-
tions, Sbasic, Swt len and Swt loglen described in
Section 3. Figure 2 shows ROUGE scores of these
scoring methods at varying levels of ?gap. First,
0.30
ROU
GE-
1
0.090.10
ROU
GE-
SU4
0.200.250.30
ROU
GE-
1
0.050.060.070.080.090.10
R2
R3
R4
ROU
GE-
SU4
0.200.250.30
R2
R3
R4
ROU
GE-
1
bas
ic
wt_
logl
en
wt_
bas
ic
0.050.060.070.080.090.10
R2
R3
R4
ROU
GE-
SU4
bas
ic
wt_
logl
en
wt_
bas
ic
Figure 3: ROUGE scores (f-measure) at different
levels of ?r averaged across ?gap ? [1, 5]
0.230.28
PRECISION
precis
ion-re
call cu
rve
ROUG
E-SU4
0.400.450.50
PRECISION
precis
ion-re
call cu
rve
ROUG
E-1
x coll
apse
x dup
elim x coll
apse
+dup
elim
Opino
sis
(base
line)
+ll
x coll
apse
x dup
elim x 
collap
se
+dup
elim
Opino
sis
(base
line)
0.180.230.28 0
.06
0.07
0.08
0.09
0.1
RECA
LL
precis
ion-re
call cu
rve
ROUG
E-SU4
0.350.400.450.50 0
.22
0.24
0.26
0.28
0.3
RECA
LL
precis
ion-re
call cu
rve
ROUG
E-1
x coll
apse
x dup
elim x coll
apse
+dup
elim
Opino
sis
(base
line)
+coll
apse
x dup
elim
x coll
apse
x dup
elim x 
collap
se
+dup
elim
Opino
sis
(base
line) +col
lapse x dup
elim
Figure 4: Precision-Recall comparison with differ-
ent Opinosis features turned off.
it can be observed that Swt basic which does not
use path length information, performs the worst.
This is due to the effect of heavily favoring re-
dundant paths over longer but reasonably redun-
dant ones that can provide more coverage. We also
see that Swt len and Swt loglen are similar in per-
formance with Swt loglen marginally outperform-
ing Swt len when ?gap > 2. Since Swt len uses
the raw path length in its scoring function, it may
be inflating the path scores of long but insignifi-
cant paths. Swt loglen scales down the path length,
thus providing a reasonable tradeoff between re-
dundancy and the length of the selected path. The
three scoring functions are not influenced by dif-
ferent levels of ?r as shown in Figure 3.
Effect of gap setting (?gap). Now, we will ex-
amine the effect of ?gap on the generated sum-
maries. Based on Figure 2, we see that setting
?gap=1 yields in relatively low performance. This
is because ?gap=1 implies immediate adjacency
between the PIDs of two nodes and such strict ad-
jacency enforcements prevent redundancies from
being discovered. When ?gap is increased to 2,
there is a big jump in performance, after which
improvements are observed in smaller amounts. A
very large gap setting could increase the possibility
of generating ill-formed sentences, thus we recom-
mend that ?gap is set between 2-5.
Effect of redundancy requirement (?r) . Fig-
ure 3 shows the ROUGE scores at different levels
of ?r. It is clear that when ?r > 2, the quality of
summaries is negatively impacted. Since we only
have about 100 sentences per review document,
?r > 2 severely restricts the number of paths that
can be explored, yielding in lower ROUGE scores.
Since the scoring function can account for the level
of redundancy, ?r should be set according to the
size of the input data. For our dataset, ?r = 2 was
ideal.
346
?A
bo
ut 
foo
da
t H
oli
da
y I
nn
, L
on
do
n?
Human
summaries:
[1]
Foo
d w
as
 exc
ell
en
t w
ith
 a 
wid
e r
an
ge
 of
 ch
oic
es
 an
d g
oo
d s
erv
ice
s.
[2]
The
 fo
od
 is
 go
od
, th
e s
erv
ice
 gr
ea
t. Ve
ry 
go
od
 se
lec
tio
n o
f fo
od
 fo
r b
rea
kfas
t 
bu
ffe
t.
?W
ha
t is
 fr
ee
at 
Be
stw
es
ter
nI
nn
, S
an
 Fr
an
cis
co
?
Human
summaries:
[1]
The
re 
is 
fre
e Wi
Fiin
ter
ne
t a
cc
es
s a
va
ila
ble
 in
 al
l th
e r
oo
ms
.. Fro
m 
5-6
 p.m
. th
ere
 is
 fre
e 
win
e t
as
tin
g a
nd
 ap
pe
tize
rs 
av
ail
ab
le 
to 
all
 th
e g
ue
sts
.
[2]
Eve
nin
g w
ine
 re
ce
pti
on
 an
d f
ree
 co
ffe
e i
n t
he
 m
orn
ing
. Fre
e i
nte
rne
t, f
ree
 pa
rkin
g a
nd
 
fre
em
as
sa
ge
Opinosis
abstractive summary:
The
 fo
od
 w
as
  e
xce
lle
nt,
  g
oo
d a
nd
  d
eli
cio
us
. Ver
y g
oo
d s
ele
cti
on
 of
 fo
od
.
Baseline
extractive summary:
With
in 
20
0 y
ard
s o
f le
av
ing
 th
e h
ote
l a
nd
 he
ad
ing
 to
 th
e Tu
be
 St
ati
on
 yo
u h
av
e a
 
nu
mb
er 
of 
fas
t fo
od
 ou
tle
ts,
 hi
gh
str
ee
tRe
sta
uta
nts
, P
as
try
 sh
op
s a
nd
 
su
pe
rm
arke
ts
so
ify
ou
did
wis
ht
ol
ive
in
yo
ur
ho
tel
roo
m
for
the
du
rat
ion
of
yo
ur
fre
e m
as
sa
ge
.
Opinosis
abstractive summary:
Free
 w
ine
 re
ce
pti
on
 in
 ev
en
ing
. Fre
e c
off
ee
 an
d b
isc
ott
i a
nd
 w
ine
.
Baseline
extractive summary:
The
 fre
e w
ine
 an
d n
ibb
les
 se
rve
d b
etw
ee
n 5
pm
 an
d 6
pm
 w
ere
 a 
lov
ely
 to
uc
h. Th
ere
's f
ree
 
co
ffe
e
tea
sa
tb
rea
kfas
tti
me
wit
hl
ittl
eb
isc
ott
ia
nd
be
st
of
all
fro
m
5t
ill6
pm
yo
ug
et
af
ree
su
pe
rm
arke
ts
, s
o i
f y
ou
 di
d w
ish
 to
 liv
e i
n y
ou
r h
ote
l ro
om
 fo
r th
e d
ura
tio
n o
f y
ou
r 
sta
y, 
yo
u c
ou
ld 
do
.......
co
ffe
e, 
tea
s a
t b
rea
kfas
t 
tim
e w
ith
 lit
tle
 bi
sc
ott
i a
nd
, b
es
t o
f a
ll, 
fro
m 
5 t
ill 6
pm
 yo
u g
et 
a f
ree
 
win
e 't
as
tin
g' r
ec
ep
tio
n w
hic
h, 
as
 lo
ng
 as
 yo
u d
on
't ta
ke??
Figure 5: Sample results comparing Opinosis summaries with human and baseline summaries.
Effect of collapsed structures and duplicate
elimination. So far, it has been assumed that all
features used in Opinosis are required to gener-
ate reasonable summaries. To test this hypothesis,
we use Opinosisbest as a baseline and then we turn
off different features of Opinosis. We turn off the
duplicate elimination feature, then the collapsi-
ble structure feature, and finally both. Figure 4
shows the resulting precision-recall curve. From
this graph, we see that without duplicate elimina-
tion and when collapsing is turned off, the preci-
sion is highest but recall is lowest. No collaps-
ing implies shorter sentences and thus lower recall,
which is clearly reflected in Figure 4. On top of
this, if duplicates are allowed, the overall informa-
tion coverage is low, further affecting the recall.
Notice that the presence of duplicates with the col-
lapse feature turned on results in very high recall
(even higher than the baseline). This is caused by
the presence of similar phrases that were not elim-
inated from the collapsed candidates, resulting in
long sentences that artificially boost recall. The
Opinosis baseline which uses duplicate elimina-
tion and the collapsible structure feature, offers a
reasonable tradeoff between precision and recall.
Readability of Summaries. To test the readability
of Opinosis summaries, we conducted a readabil-
ity test (described in Section 5) using summaries
generated from Opinosisbest. A human assessor
picked the 2 least readable sentences from each of
the 51 test sets (based on 51 summaries). Collec-
tively, there were 565 sentences out of which 102
were Opinosis generated. Out of these, the hu-
man assessor picked only 34 of the sentences as
being least readable, resulting in an average read-
ability score of 0.67. This shows that more than
60% of the generated sentences are indistinguish-
able from human composed sentences. Of the 34
sentences with problems, 11 contained no informa-
tion or were incomprehensible, 12 were incomplete
possibly due to false positives when the sentence
validity check was done, and 8 had conflicting in-
formation such as ?the hotel room is clean and
dirty?. This happens due to mixed feelings about
the same topic and can be resolved using sentiment
analysis. The remaining 3 sentences were found
to contain poor grammar, possibly caused by the
gaps allowed in finding redundant paths.
Sample Summaries. Finally, in Figure 5 we show
two sample summaries on two different topics.
Notice that the Opinosis summaries are concise,
fairly well-formed and have closer resemblance to
human summaries than to the baseline summaries.
7 Conclusion
In this paper, we described a novel summarization
framework (Opinosis) that uses textual graphs to
generate abstractive summaries of highly redun-
dant opinions. Evaluation results on a set of review
documents show that Opinosis summaries have
better agreement with human summaries com-
pared to the baseline extractive method. The
Opinosis summaries are concise, reasonably well-
formed and communicate essential information.
Our readability test shows that more than 60% of
the generated sentences are no different from hu-
man composed sentences.
Opinosis is a flexible framework in that many
of its modules can be easily improved or replaced
with other suitable implementation. Also, since
Opinosis is domain independent and relies on min-
imal external resources, it can be used with any
corpus containing high amounts of redundancies.
Our graph representation naturally ensures the
coherence of a summary, but such a graph empha-
sizes too much on the surface order of words. As a
result, it cannot group sentences at a deep seman-
tic level. To address this limitation, we can use a
similar idea to overlay parse trees and this would
be a very interesting future research.
8 Acknowledgments
We thank the anonymous reviewers for their use-
ful comments. This paper is based upon work sup-
ported in part by an IBM Faculty Award, an Alfred
P. Sloan Research Fellowship, an AFOSR MURI
Grant FA9550-08-1-0265, and by the National Sci-
ence Foundation under grants IIS-0347933, IIS-
0713581, IIS-0713571, and CNS-0834709.
347
References
[Barzilay and Lee2003] Barzilay, Regina and Lillian
Lee. 2003. Learning to paraphrase: an unsuper-
vised approach using multiple-sequence alignment.
In NAACL ?03: Proceedings of the 2003 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology, pages 16?23, Morristown, NJ,
USA. Association for Computational Linguistics.
[DeJong1982] DeJong, Gerald F. 1982. An overview of
the FRUMP system. In Lehnert, Wendy G. and Mar-
tin H. Ringle, editors, Strategies for Natural Lan-
guage Processing, pages 149?176. Lawrence Erl-
baum, Hillsdale, NJ.
[Erkan and Radev2004] Erkan, Gu?nes and Dragomir R.
Radev. 2004. Lexrank: graph-based lexical central-
ity as salience in text summarization. J. Artif. Int.
Res., 22(1):457?479.
[Finley and Harabagiu2002] Finley, Sanda Harabagiu
and Sanda M. Harabagiu. 2002. Generating sin-
gle and multi-document summaries with gistexter. In
Proceedings of the workshop on automatic summa-
rization, pages 30?38.
[Jing and McKeown2000] Jing, Hongyan and Kath-
leen R. McKeown. 2000. Cut and paste based
text summarization. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics conference, pages 178?185, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
[Lerman et al2009] Lerman, Kevin, Sasha Blair-
Goldensohn, and Ryan Mcdonald. 2009. Sentiment
summarization: Evaluating and learning user prefer-
ences. In 12th Conference of the European Chapter
of the Association for Computational Linguistics
(EACL-09).
[Lin and Hovy2003] Lin, Chin-Yew and Eduard Hovy.
2003. Automatic evaluation of summaries using n-
gram co-occurrence statistics. In Proc. HLT-NAACL,
page 8 pages.
[LIN2004a] LIN, Chin-Yew. 2004a. Looking for a few
good metrics : Rouge and its evaluation. proc. of the
4th NTCIR Workshops, 2004.
[Lin2004b] Lin, Chin-Yew. 2004b. Rouge: a pack-
age for automatic evaluation of summaries. In Pro-
ceedings of the Workshop on Text Summarization
Branches Out (WAS 2004), Barcelona, Spain.
[Lu et al2009] Lu, Yue, ChengXiang Zhai, and Neel
Sundaresan. 2009. Rated aspect summarization of
short comments. In 18th International World Wide
Web Conference (WWW2009), April.
[Mihalcea and Tarau2004] Mihalcea, R. and P. Tarau.
2004. TextRank: Bringing order into texts. In Pro-
ceedings of EMNLP-04and the 2004 Conference on
Empirical Methods in Natural Language Processing,
July.
[Pang and Lee2004] Pang, Bo and Lillian Lee. 2004.
A sentimental education: Sentiment analysis using
subjectivity summarization based on minimum cuts.
In Proceedings of the ACL, pages 271?278.
[Pang et al2002] Pang, Bo, Lillian Lee, and Shivaku-
mar Vaithyanathan. 2002. Thumbs up? Sentiment
classification using machine learning techniques. In
Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 79?86.
[Radev and McKeown1998] Radev, DR and K. McKe-
own. 1998. Generating natural language summaries
from multiple on-line sources. Computational Lin-
guistics, 24(3):469?500.
[Radev et al2000] Radev, Dragomir, Hongyan Jing, and
Malgorzata Budzikowska. 2000. Centroid-based
summarization of multiple documents: Sentence ex-
traction, utility-based evaluation, and user studies.
In In ANLP/NAACL Workshop on Summarization,
pages 21?29.
[Radev et al2002] Radev, Dragomir R., Eduard Hovy,
and Kathleen McKeown. 2002. Introduction to the
special issue on summarization.
[Saggion and Lapalme2002] Saggion, Horacio and Guy
Lapalme. 2002. Generating indicative-informative
summaries with sumum. Computational Linguistics,
28(4):497?526.
[Snyder and Barzilay2007] Snyder, Benjamin and
Regina Barzilay. 2007. Multiple aspect ranking
using the good grief algorithm. In In Proceedings
of the Human Language Technology Conference
of the North American Chapter of the Association
of Computational Linguistics (HLT-NAACL, pages
300?307.
[Titov and Mcdonald2008] Titov, Ivan and Ryan Mc-
donald. 2008. A joint model of text and aspect rat-
ings for sentiment summarization. In Proceedings
of ACL-08: HLT, pages 308?316, Columbus, Ohio,
June. Association for Computational Linguistics.
348
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1567?1578, Dublin, Ireland, August 23-29 2014.
The Wisdom of Minority: Unsupervised Slot Filling Validation based on
Multi-dimensional Truth-Finding
Dian Yu
1
, Hongzhao Huang
1
, Taylor Cassidy
2,3
, Heng Ji
1
Chi Wang
4
, Shi Zhi
4
, Jiawei Han
4
, Clare Voss
2
, Malik Magdon-Ismail
1
1
Computer Science Department, Rensselaer Polytechnic Institute
2
U.S. Army Research Lab
3
IBM T. J. Watson Research Center
4
Computer Science Department, Univerisity of Illinois at Urbana-Champaign
1
{yud2,huangh9,jih,magdon}@rpi.edu,
2,3
{taylor.cassidy.ctr,clare.r.voss.civ}@mail.mil
4
{chiwang1,shizhi2,hanj}@illinois.edu
Abstract
Information Extraction using multiple information sources and systems is beneficial due to multi-
source/system consolidation and challenging due to the resulting inconsistency and redundancy.
We integrate IE and truth-finding research and present a novel unsupervised multi-dimensional
truth finding framework which incorporates signals from multiple sources, multiple systems and
multiple pieces of evidence by knowledge graph construction through multi-layer deep linguistic
analysis. Experiments on the case study of Slot Filling Validation demonstrate that our approach
can find truths accurately (9.4% higher F-score than supervised methods) and efficiently (finding
90% truths with only one half the cost of a baseline without credibility estimation).
1 Introduction
Traditional Information Extraction (IE) techniques assess the ability to extract information from
individual documents in isolation. However, similar, complementary or conflicting information may
exist in multiple heterogeneous sources. We take the Slot Filling Validation (SFV) task of the NIST Text
Analysis Conference Knowledge Base Population (TAC-KBP) track (Ji et al., 2011) as a case study. The
Slot Filling (SF) task aims at collecting from a large-scale multi-source corpus the values (?slot fillers?)
for certain attributes (?slot types?) of a query entity, which is a person or some type of organization. KBP
2013 has defined 25 slot types for persons (per) (e.g., age, spouse, employing organization) and 16 slot
types for organizations (org) (e.g., founder, headquarters-location, and subsidiaries). Some slot types
take only a single slot filler (e.g., per:birthplace), whereas others take multiple slot fillers (e.g., org:top
employees).
We call a combination of query entity, slot type, and slot filler a claim. Along with each claim, each
system must provide the ID of a source document and one or more detailed context sentences as evidence
which supports the claim. A response (i.e., a claim, evidence pair) is correct if and only if the claim is
true and the evidence supports it.
Given the responses produced by multiple systems from multiple sources in the SF task, the goal of
the SFV task is to determine whether each response is true or false. Though it?s a promising line of
research, it raises two complications: (1) different information sources may generate claims that vary
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1567
in trustability; and (2) a large-scale number of SF systems using different resources and algorithms may
generate erroneous, conflicting, redundant, complementary, ambiguously worded, or inter-dependent
claims from the same set of documents. Table 1 presents responses from four SF systems for the query
entity Ronnie James Dio and the slot type per:city of death. Systems A, B and D return Los Angeles
with different pieces of evidence
1
extracted from different information sources, though the evidence of
System D does not decisively support the claim. System C returns Atlantic City, which is neither true
nor supported by the corresponding evidence.
Such complications call for ?truth finding?: determining the veracity of multiple conflicting claims
from various sources and systems. We propose a novel unsupervised multi-dimensional truth finding
framework to study credibility perceptions in rich and wide contexts. It incorporates signals from
multiple sources and systems, using linguistic indicators derived from knowledge graphs constructed
from multiple evidences using multi-layer deep linguistic analysis. Experiments demonstrate that our
approach can find truths accurately (9.4% higher F-score than supervised methods) and efficiently (find
90% truths with only one half cost of a baseline without credibility estimation).
System Source Slot Filler Evidence
A Agence France-
Presse, News
Los Angeles The statement was confirmed by publicist Maureen O?Connor, who said Dio
died in Los Angeles.
B New York
Times, News
Los Angeles Ronnie James Dio, a singer with the heavy-metal bands Rainbow, Black
Sabbath and Dio, whose semioperatic vocal style and attachment to demonic
imagery made him a mainstay of the genre, died on Sunday in Los Angeles.
C Discussion Fo-
rum
Atlantic City Dio revealed last summer that he was suffering from stomach cancer shortly
after wrapping up a tour in Atlantic City.
D Associated
Press
Worldstream,
News
Los Angeles LOS ANGELES 2010-05-16 20:31:18 UTC Ronnie James Dio, the metal god
who replaced Ozzy Osbourne in Black Sabbath and later piloted the bands
Heaven, Hell and Dio, has died, according to his wife and manager.
Table 1: Conflicting responses across different SF systems and different sources (query entity = Ronnie
James Dio, slot type = per:city of death).
2 Related Work & Our Novel Contributions
Most previous SFV work (e.g., (Tamang and Ji, 2011; Li and Grishman, 2013)) focused on filtering
incorrect claims from multiple systems by simple heuristic rules, weighted voting, or costly supervised
learning to rank algorithms. We are the first to introduce the truth finding concept to this task.
The ?truth finding? problem has been studied in the data mining and database communities (e.g., (Yin
et al., 2008; Dong et al., 2009a; Dong et al., 2009b; Galland et al., 2010; Blanco et al., 2010; Pasternack
and Roth, 2010; Yin and Tan, 2011; Pasternack and Roth, 2011; Vydiswaran et al., 2011; Ge et al.,
2012; Zhao et al., 2012; Wang et al., 2012; Pasternack and Roth, 2013)). Compared with the previous
work, our truth finding problem is defined under a unique setting: each response consists of a claim and
supporting evidence, automatically generated from unstructured natural language texts by a SF system.
The judgement of a response concerns both the truth of the claim and whether the evidence supports
the claim. This has never been modeled before. We mine and exploit rich linguistic knowledge from
multiple lexical, syntactic and semantic levels from evidence sentences for truth finding. In addition,
previous truth finding work assumed most claims are likely to be true. However, most SF systems have
hit a performance ceiling of 35% F-measure, and false responses constitute the majority class (72.02%)
due to the imperfect algorithms as well as the inconsistencies of information sources. Furthermore,
certain truths might only be discovered by a minority of good systems or from a few good sources. For
example, 62% of the true responses are produced only by 1 or 2 of the 18 SF systems.
1
Hereafter, we refer to ?pieces of evidence? with the shorthand ?evidences?. Note that SF systems may include multiple
sentences as ?evidence? within their responses.
1568
r1 
      Response 
<Claim, Evidence> 
t1 
t2 
System 
s1 
r2 
r3 
s2 
Source 
t3 
r4 t4 
s3 
r5 
Figure 1: Heterogeneous networks for MTM.
3 MTM: A Multi-dimensional Truth-Finding Model
MTM Construction
A response is trustworthy if its claim is true and its evidence supports the claim. A trusted
source always supports true claims by giving convincing evidence, and a good system tends to extract
trustworthy responses from trusted sources. We propose a multi-dimensional truth-finding model (MTM)
to incorporate and compute multi-dimensional credibility scores.
Consider a set of responses R = {r
1
, . . . , r
m
} extracted from a set of sources S = {s
1
, . . . , s
n
} and
provided by a set of systems T = {t
1
, . . . , t
l
}. A heterogeneous network is constructed as shown in
Fig. 1. Let weight matrices be W
rs
m?n
= {w
rs
ij
} and W
rt
m?l
= {w
rt
ik
}. A link w
rs
ij
= 1 is generated
between r
i
and s
j
when response r
i
is extracted from source s
j
, and a link w
rt
ik
= 1 is generated between
r
i
and t
k
when response r
i
is provided by system t
k
.
Credibility Initialization
Each source is represented as a combination of publication venue and genre. The credibility scores
of sources S are initialized uniformly as
1
n
, where n is the number of sources. Given the set of systems
T = {t
1
, . . . , t
l
}, we initialize their credibility scores c
0
(t) based on their interactions on the predicted
responses. Suppose each system t
i
generates a set of responses R
t
i
. The similarity between two systems
t
i
and t
j
is defined as similarity(t
i
, t
j
) =
|R
t
i
?R
t
j
|
log (|R
t
i
|)+log (|R
t
j
|)
(Mihalcea, 2004). Then we construct a
weighted undirected graph G = ?T,E?, where T (G) = {t
1
, . . . , t
l
} and E(G) = {?t
i
, t
j
?}, ?t
i
, t
j
? =
similarity(t
i
, t
j
), and apply the TextRank algorithm (Mihalcea, 2004) on G to obtain c
0
(t).
We got negative results by initializing system credibility scores uniformly. We also got negative results
by initializing system credibility scores using system metadata, such as the algorithms and resources the
system used at each step, its previous performance in benchmark tests, and the confidence values it
produced for its responses. We found the quality of an SF system depends on many different resources
instead of any dominant one. For example, an SF system using a better dependency parser does not
necessarily produce more truths. In addition, many systems are actively being improved, rendering
previous benchmark results unreliable. Furthermore, most SF systems still lack reliable confidence
estimation.
The initialization of the credibility scores for responses relies on deep linguistic analysis of the
evidence sentences and the exploitation of semantic clues, which will be described in Section 4.
Credibility Propagation
1569
We explore the following heuristics in MTM.
HEURISTIC 1: A response is more likely to be true if derived from many trustworthy sources. A source
is more likely to be trustworthy if many responses derived from it are true.
HEURISTIC 2: A response is more likely to be true if it is extracted by many trustworthy systems. A
system is more likely to be trustworthy if many responses generated by it are true.
Based on these two heuristics we design the following credibility propagation approach to mutually
reinforce the trustworthiness of linked objects in MTM.
By extension of Co-HITS (Deng et al., 2009), designed for bipartite graphs, we develop a propagation
method to handle heterogeneous networks with three types of objects: source, response and system. Let
the weight matrices beW
rs
(between responses and sources) andW
rt
(between responses and systems),
and their transposes beW
sr
andW
tr
. We can obtain the transition probability that vertex s
i
in S reaches
vertex r
j
in R at the next iteration, which can be formally defined as a normalized weight p
sr
ij
=
w
sr
ij?
k
w
sr
ik
such that
?
r
j
?R
p
sr
ij
= 1. We compute the transition probabilities p
rs
ji
, p
rt
jk
and p
tr
kj
in an analogous
fashion.
Given the initial credibility scores c
0
(r), c
0
(s) and c
0
(t), we aim to obtain the refined credibility scores
c(r), c(s) and c(t) for responses, sources, and systems, respectively. Starting with sources, the update
process considers both the initial score c
0
(s) and the propagation from connected responses, which we
formulated as:
c(s
i
) = (1? ?
rs
)c
0
(s
i
) + ?
rs
?
r
j
?R
p
rs
ji
c(r
j
) (1)
Similarly, the propagation from responses to systems is formulated as:
c(t
k
) = (1? ?
rt
)c
0
(t
k
) + ?
rt
?
r
j
?R
p
rt
jk
c(r
j
) (2)
Each response?s score c(r
j
) is influenced by both linked sources and systems:
c(r
j
) = (1? ?
sr
? ?
tr
)c
0
(r
j
) + ?
sr
?
s
i
?S
p
sr
ij
c(s
i
) + ?
tr
?
t
k
?T
p
tr
kj
c(t
k
) (3)
where ?
rs
, ?
rt
, ?
sr
and ?
tr
? [0, 1]. These parameters control the preference for the propagated over
initial score for every type of random walk link. The larger they are, the more we rely on link structure
2
.
The propagation algorithm converges (10 iterations in our experimental settings) and a similar theoretical
proof to HITS (Peserico and Pretto, 2009) can be constructed. Algorithm 1 summarizes MTM.
4 Response Credibility Initialization
Each evidence along with a claim is expressed as a few natural language sentences that include the query
entity and the slot filler, along with semantic content to support the claim. We analyze the evidence of
each response in order to initialize that response?s credibility score. This is done using heuristic rules
defined in terms of the binary outputs of various linguistic indicators (Section 4.1).
4.1 Linguistic Indicators
We encode linguistic indicators based on deep linguistic knowledge acquisition and use them to
determine whether responses provide supporting clues or carry negative indications (Section 4.3). These
indicators make use of linguistic features on varying levels - surface form, sentential syntax, semantics,
and pragmatics - and are defined in terms of knowledge graphs (Section 4.2). We define a heuristic rule
for each slot type in terms of the binary-valued linguistic indicator outputs to yield a single binary value
(1 or 0) for each response. If a response?s linguistic indicator value is 1, the credibility score of a response
is initialized at 1.0, and 0.5 otherwise.
2
We set ?
rs
= 0.9, ?
sr
= 0.1, ?
rt
= 0.3 and ?
tr
= 0.2, optimized from a development set. See Section 5.1.
1570
Input: A set of responses (R), sources (S) and systems (T ).
Output: Credibility scores (c(r)) for R.
1: Initialize the credibility scores c
0
(s) for S as c
0
(s
i
) =
1
|S|
;
2: Use TextRank to compute initial credibility scores c
0
(t) for T ;
3: Initialize the credibility scores c
0
(r) using linguistic indicators (Section 4);
4: Construct heterogeneous networks across R, S and T ;
5: k ? 0, diff? 10e6;
6: while k < MaxIteration and diff > MinThreshold do
7: Use Eq. (1) to compute c
k+1
(s);
8: Use Eq. (2) to compute c
k+1
(t);
9: Use Eq. (3) to compute c
k+1
(r);
10: Normalize c
k+1
(s), c
k+1
(t), and c
k+1
(r);
11: diff?
?
(|c
k+1
(r)? c
k
(r)|);
12: k ? k + 1
13: end while
Algorithm 1:Multi-dimensional Truth-Finding.
4.2 Knowledge Graph Construction
A semantically rich knowledge graph is constructed that links a query entity, all of its relevant slot
filler nodes, and nodes for other intermediate elements excerpted from evidence sentences. There is one
knowledge graph per sentence.
Fig. 2 shows a subregion of the knowledge graph built from the sentence: ?Mays, 50, died in his sleep
at his Tampa home the morning of June 28.?. It supports 3 claims: [Mays, per: city of death, Tampa],
[Mays, per: date of death, 06/28/2009] and [Mays, per: age, 50].
Formally, a knowledge graph is an annotated graph of entity mentions, phrases and their links. It must
contain one query entity node and one or more slot filler nodes. The annotation of a node includes its
entity type, subtype, mention type, referent entities, and semantic category (though not every node has
each type of annotation). The annotation of a link includes a dependency label and/or a semantic relation
between the two linked nodes.
The knowledge graph is constructed using the following procedure. First, we annotate the evidence
text using dependency parsing (Marneffe et al., 2006) and Information Extraction (entity, relation and
event) (Li et al., 2013; Li and Ji, 2014). Two nodes are linked if they are deemed related by one of the
annotation methods (e.g., [Mays, 50] is labeled with the dependency type amod, and [home, Tampa] is
labeled with the semantic relation located in). The annotation output is often in terms of syntactic heads.
Thus, we extend the boundaries of entity, time, and value mentions (e.g., people?s titles) to include an
entire phrase where possible. We then enrich each node with annotation for entity type, subtype and
mention type. Entity type and subtype refer to the role played by the entity in the world, the latter being
more fine-grained, whereas mention type is syntactic in nature (it may be pronoun, nominal, or proper
name). For example, ?Tampa? in Fig. 2 is annotated as a Geopolitical (entity type) Population-Center
(subtype) Name (mention type) mention. Every time expression node is annotated with its normalized
reference date (e.g., ?June, 28? in Fig. 2 is normalized as ?06/28/2009?).
Second, we perform co-reference resolution, which introduces implicit links between nodes that refer
to the same entity. Thus, an entity mention that is a nominal or pronoun will often be co-referentially
linked to a mention of a proper name. This is important because many queries and slot fillers are
expressed only as nominal mentions or pronouns in evidence sentences, their canonical form appearing
elsewhere in the document.
Finally, we address the fact that a given relation type may be expressed in a variety of ways. For
example, ?the face of ? indicates the membership relation in the following sentence: ?Jennifer Dunn was
the face of the Washington state Republican Party for more than two decades.? We mined a large
1571
Mays 
had 
died 
sleep 
his 
home 
Tampa 
50 
June,28 
amod 
nsubj 
aux 
prep_in 
poss 
prep_at 
prep_of 
nn 
poss 
  located_in 
{PER.Individual, NAM, Billy Mays} 
?Query? 
{NUM } 
?Per:age? 
{Death-Trigger} 
{PER.Individual.PRO, Mays} 
{GPE.Population-Center.NAM, FL-USA} 
? Per:place_of_death? 
{FAC.Building-Grounds.NOM} 
{06/28/2009, TIME-WITHIN}  
? per:date_of_death? 
Figure 2: Knowledge Graph Example.
number of trigger phrases for each slot type by mapping various knowledge bases, including Wikipedia
Infoboxes, Freebase (Bollacker et al., 2008), DBPedia (Auer et al., 2007) and YAGO (Suchanek et
al., 2007), into the Gigaword corpus
3
and Wikipedia articles via distant supervision (Mintz et al.,
2009)
4
. Each intermediate node in the knowledge graph that matches a trigger phrase is then assigned a
corresponding semantic category. For example, ?died? in Fig. 2 is labeled a Death-Trigger.
4.3 Knowledge Graph-Based Verification
We design linguistic indicators in terms of the properties of nodes and paths that are likely to be bear on
the response?s veracity. Formally, a path consists of the list of nodes and links that must be traversed
along a route from a query node to a slot filler node.
Node indicators contribute information about a query entity or slot filler node in isolation, that
may bear on the trustworthiness of the containing evidence sentence. For instance, a slot filler for the
per:date of birth slot type must be a time expression.
Node Indicators
1. Surface: Whether the slot filler includes stop words; whether it is lower cased but appears in news.
These serve as negative indicators.
2. Entity type, subtype and mention type: For example, the slot fillers for ?org:top employees? must be
person names; and fillers for ?org:website? must match the url format. Besides the entity extraction
system, we also exploited the entity attributes mined by the NELL system (Carlson et al., 2010)
from the KBP source corpus.
Each path contains syntactic and/or semantic relational information that may shed light on the manner
in which the query entity and slot filler are related, based on dependency parser output, IE output,
and trigger phrase labeling. Path indicators are used to define properties of the context in which
which query-entity and slot-filler are related in an evidence sentence. For example, whether the path
3
http://catalog.ldc.upenn.edu/LDC2011T07
4
Under the distant supervision assumption, sentences that appear to mention both entities in a binary relation contained in
the knowledge base were assumed to express that relation.
1572
associated with a claim about an organization?s top employee includes a title commonly associated with
decision-making power can be roughly represented using the trigger phrases indicator.
Path Indicators
1. Trigger phrases: Whether the path includes any trigger phrases as described in Section 4.2.
2. Relations and events: Whether the path includes semantic relations or events indicative of the slot
type. For example, a ?Start-Position? event indicates a person becomes a ?member? or ?employee?
of an organization.
3. Path length: Usually the length of the dependency path connecting a query node and a slot filler
node is within a certain range for a given slot type. For example, the path for ?per:title? is usually
no longer than 1. A long dependency path between the query entity and slot filler indicates a lack
of a relationship. In the following evidence sentence, which does not entail the ?per:religion?
relation between ?His? and the religion ?Muslim?, there is a long path (?his-poss-moment-nsubj-
came-advcl-seized-militant-acmod-Muslim?): ?His most noticeable moment in the public eye came
in 1979, when Muslim militants in Iran seized the U.S. Embassy and took the Americans stationed
there hostage.?.
Detecting and making use of interdependencies among various claims is another unique challenge in
SFV. After initial response credibility scores are calculated by combining linguistic indicator values, we
identify responses that have potentially conflicting or potentially supporting slot-filler candidates. For
such responses, their credibility scores are changed in accordance with the binary values returned by the
following indicators.
Interdependent Claims Indicators
1. Conflicting slot fillers: When fillers for two claims with the same query entity and slot type appear
in the same evidence sentence, we apply an additional heuristic rule designed for the slot type in
question. For example, the following evidence sentence indicates that compared to ?Cathleen P.
Black?, ?Susan K. Reed? is more likely to be in a ?org:top employees/members? relation with ?The
Oprah Magazine? due to the latter pair?s shorter dependency path: ?Hearst Magazine?s President
Cathleen P. Black has appointed Susan K. Reed as editor-in-chief of the U.S. edition of The
Oprah Magazine.?. The credibility scores are accordingly changed (or kept at) 0.5 for responses
associated with the former claim, and 1.0 for those associated with the latter.
2. Inter-dependent slot types: Many slot types are inter-dependent, such as ?per:title? and
?per:employee of ?, and various family slots. After determining initial credibility scores for each
response, we check whether evidence exists for any implied claims. For example, given initial
credibility scores of 1.0 for two responses supporting the claims that (1)?David? is ?per:children?
of ?Carolyn Goodman? and (2)?Andrew? is ?per:sibling? of ?David?, we check for any responses
supporting the claim that (3)?Andrew? is ?per:children? of ?Carolyn Goodman?, and set their
credibility scores to 1.0. For example, a response supporting this claim included the evidence
sentence, ?Dr. Carolyn Goodman, her husband, Robert, and their son, David, said goodbye to
David?s brother, Andrew.?.
5 Experimental Results
This section presents the experiment results and analysis of our approach.
5.1 Data
The data set we use is from the TAC-KBP2013 Slot Filling Validation (SFV) task, which consists of the
merged responses returned by 52 runs (regarded as systems in MTM) from 18 teams submitted to the Slot
1573
Methods Precision Recall F-measure Accuracy Mean Average Precision
1.Random 28.64% 50.48% 36.54% 50.54% 34%
2.Voting 42.16% 70.18% 52.68% 62.54% 62%
3.Linguistic Indicators 50.24% 70.69% 58.73% 72.29% 60%
4.SVM (3 + System + Source) 56.59% 48.72% 52.36% 75.86% 56%
5.MTM (3 + System + Source) 53.94% 72.11% 61.72% 81.57% 70%
Table 2: Overall Performance Comparison.
Filling (SF) task. The source collection has 1,000,257 newswire documents, 999,999 web documents
and 99,063 discussion forum posts, which results in 10 different sources (combinations of publication
venues and genres) in our experiment. There are 100 queries: 50 person and 50 organization entities.
After removing redundant responses within each single system run, we use 45,950 unique responses as
the input to truth-finding. Linguistic Data Consortium (LDC) human annotators manually assessed all
of these responses and produced 12,844 unique responses as ground truth. In order to compare with
state-of-the-art supervised learning methods for SFV (Tamang and Ji, 2011; Li and Grishman, 2013), we
trained a SVMs classifier
5
as a counterpart, incorporating the same set of linguistic indicators, sources
and systems as features. We picked 10% (every 10th line) to compose the development set for MTM and
the training set for the SVMs. The rest is used for blind test.
5.2 Overall Performance
Table 2 shows the overall performance of various truth finding methods on judging each response as true
or false. MTM achieves promising results and even outperforms supervised learning approach. Table 3
presents some examples ranked at the top and the bottom based on the credibility scores produced by
MTM.
We can see that majority voting across systems performs much better than random assessment, but its
accuracy is still low. For example, the true claim T5 was extracted by only one system because most
systems mistakenly identified ?Briton Stuart Rose? as a person name. In comparison, MTM obtained
much better accuracy by also incorporating multiple dimensions of source and evidence information.
Method 3 using linguistic indicators alone, already achieved promising results. For example, many
claims are judged as truths through trigger phrases (T1 and T5), event extraction (T2), coreference (T4),
and node type indicators (T3). On the other hand, many claims are correctly judged as false because
their evidence sentences did not include the slot filler (F1, F4, F5) or valid knowledge paths to connect
the query entity and the slot filler (F2, F3). The performance gain (2.99% F-score) from Method 3 to
Method 5 shows the need for incorporating system and source dimensions. For example, most truths are
from news while many false claims are from newsgroups and discussion forum posts (F1, F2, F5).
The SVMs model got very low recall because of the following two reasons: (1) It ignored the inter-
dependency between multiple dimensions; (2) the negative instances are dominant in the training data,
so the model is biased towards labeling responses as false.
5.3 Truth Finding Efficiency
Table 3 shows that some truths (T1) are produced from low-ranked systems whereas some false responses
from high-ranked systems (F1, F2). Note that systems are ranked by their performance in KBP SF task.
In order to find all the truths, human assessors need to go through all the responses returned by multiple
systems. This process was proven very tedious and costly (Ji et al., 2010; Tamang and Ji, 2011).
Our MTM approach can expedite this process by ranking responses based on their credibility scores
and asking human to assess the responses with high credibility first. Traditionally, when human assess
responses, they follow an alphabetical order or system IDs in a ?passive learning? style. This is set as
our baseline. For comparison, we also present the results using only linguistic indicators, using voting
in which the responses which get more votes across systems are assessed first, and the oracle method
assessing all correct responses first. Table 2 shows our model can successfully rank trustworthy responses
at high positions compared with other approaches.
5
We used the LIBSVM toolkit (Chang and Lin, 2011) with Gaussian radial basis function kernel.
1574
Response Ranked by MTM
Source
System
Rank
Claim
Evidence
Query Entity Slot Type Slot Filler
Top
Truths
T1 China
Banking
Regulatory
Commission
org:top
member-
s/employees
Liu
Mingkang
Liu Mingkang, the chairman of
the China Banking Regulatory
Commission
Central
News
Agency
of Taiwan
News
News 15
T2 Galleon
Group
org:founded
by
Raj
Rajaratnam
Galleon Group, founded by bil-
lionaire Raj Rajaratnam
New York
Times
News 9
T3 Mike Penner per:age 52 L.A. Times Sportswriter Mike
Penner, 52, Dies
New York
Times
News 1
T4 China
Banking
Regulatory
Commission
org:alternate
names
CBRC ...China Banking Regulatory Com-
mission said in the notice. The five
banks ... according to CBRC.
Xinhua,
News
News 5
T5 Stuart Rose per:origin Briton Bolland, 50, will replace Briton
Stuart Rose at the start of 2010.
Agence
France-
Presse
News 3
Bottom
False
Claims
F1 American
Association
for the Ad-
vancement of
Science
org:top
members
employees
Freedman erica.html &gt; American Library
Association, President: Maurice
Freedman &lt; http://www.aft.org
&gt; American Federation of
Teachers ...
Google Newsgroup4
F2 Jade Goody per:origin Britain because Jade Goody?s the only
person to ever I love Britain
Discussion Forum 3
F3 Don Hewitt per:spouse Swap ...whether ?Wife Swap? on ABC
or ?Jon &amp; Kate? on TLC
New York
Times
News 7
F4 Council of
Mortgage
Lenders
org:website www.cml.org.ukme purchases in the U.K. jumped
by 16 percent in April, suggesting
the property market slump may
have bottomed out
Associated
Press
World-
stream
News 18
F5 Don Hewitt per:alternate
names
Hewitt M-
chen
US DoMIna THOMPson LACtaTe
haVeD [3866 words]
Google Newsgroup13
Table 3: Top and Bottom Response Examples Ranked by MTM.
Fig. 3 summarizes the results from the above 6 approaches. The common end point of all curves
represents the cost and benefit of assessing all system responses. We can see that the baseline is very
inefficient at finding the truths. If we employ linguistic indicators, the process can be dramatically
expedited. MTM provides further significant gains, with performance close to the Oracle. With only half
the cost of the baseline, MTM can already find 90% truths.
5.4 Enhance Individual SF Systems
Finally, as a by-product, our MTM approach can also be exploited to validate the responses from each
individual SF system based on their credibility scores. For fair comparison with the official KBP
evaluation, we use the same ground-truth in KBP2013 and standard precision, recall and F-measure
metrics as defined in (Ji et al., 2011). To increase the chance of including truths which may be particularly
difficult for a system to find, LDC prepared a manual key which was assessed and included in the final
ground truth. According to the SF evaluation setting, F-measure is computed based on the number of
unique true claims. After removing redundancy across multiple systems, there are 1,468 unique true
claims. The cutoff criteria for determining whether a response is true or not was optimized from the
development set.
Fig. 4 presents the F-measure scores of the best run from each individual SF system. We can see that
our MTM approach consistently improves the performance of almost all SF systems, in an absolute gain
range of [-1.22%, 5.70%]. It promotes state-of-the-art SF performance from 33.51% to 35.70%. Our
MTM approach provides more gains to SF systems which mainly rely on lexical or syntactic patterns
than other systems using distant supervision or logic rules.
1575
1 
0 10000 20000 30000 40000
0
2000
4000
6000
8000
10000
12000
14000
13 2
4
5
#tr
uth
s
 6 Oracle 
 5 MTM
 4 SVM
 3 Linguistic Indicator
 2 Voting
 1 Baseline
#total responses
6
Figure 3: Truth Finding Efficiency.
0 2 4 6 8 10 12 14 16 18 20
0
5
10
15
20
25
30
35
F-m
es
au
re 
(%
)
System
 Before
 After
Figure 4: Impact on Individual SF Systems.
1576
6 Conclusions and Future Work
Truth finding has received attention from both Natural Language Processing (NLP) and Data Mining
communities. NLP work has mostly explored linguistic analysis of the content, while Data Mining
work proposed advanced models in resolving conflict information from multiple sources. They have
relative strengths and weaknesses. In this paper we leverage the strengths of these two distinct,
but complementary research paradigms and propose a novel unsupervised multi-dimensional truth-
finding framework incorporating signals both from multiple sources, multiple systems and multiple
evidences based on knowledge graph construction with multi-layer linguistic analysis. Experiments on
a challenging SFV task demonstrated that this framework can find high-quality truths efficiently. In the
future we will focus on exploring more inter-dependencies among responses such as temporal and causal
relations.
Acknowledgments
This work was supported by the U.S. Army Research Laboratory under Cooperative Agreement
No. W911NF-09-2-0053 (NS-CTA), the U.S. Army Research Office under Cooperative Agreement
No. W911NF-13-1-0193, U.S. National Science Foundation grants IIS-0953149, CNS-0931975,
IIS-1017362, IIS-1320617, IIS-1354329, U.S. DARPA Award No. FA8750-13-2-0041 in the Deep
Exploration and Filtering of Text (DEFT) Program, IBM Faculty Award, Google Research Award,
DTRA, DHS and RPI faculty start-up grant. The views and conclusions contained in this document are
those of the authors and should not be interpreted as representing the official policies, either expressed
or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute
reprints for Government purposes notwithstanding any copyright notation here on.
References
S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, and Z. Ives. 2007. Dbpedia: A nucleus for a web of open data. In
Proc. the 6th International Semantic Web Conference.
L. Blanco, V. Crescenzi, P. Merialdo, and P. Papotti. 2010. Probabilistic models to reconcile complex data
from inaccurate data sources. In Proc. Int. Conf. on Advanced Information Systems Engineering (CAiSE?10),
Hammamet, Tunisia, June.
K. Bollacker, R. Cook, and P. Tufts. 2008. Freebase: A shared database of structured general human knowledge.
In Proc. National Conference on Artificial Intelligence.
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, Estevam R Hruschka Jr, and Tom M Mitchell. 2010. Toward an
architecture for never-ending language learning. In AAAI.
C. Chang and C. Lin. 2011. Libsvm: a library for support vector machines. ACM Transactions on Intelligent
Systems and Technology (TIST), 2(3):27.
H. Deng, M. R. Lyu, and I. King. 2009. A generalized co-hits algorithm and its application to bipartite graphs. In
Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
KDD ?09, pages 239?248, New York, NY, USA. ACM.
X. L. Dong, L. Berti-Equille, and D. Srivastavas. 2009a. Integrating conflicting data: The role of source
dependence. In Proc. 2009 Int. Conf. Very Large Data Bases (VLDB?09), Lyon, France, Aug.
X. L. Dong, L. Berti-Equille, and D. Srivastavas. 2009b. Truth discovery and copying detection in a dynamic
world. In Proc. 2009 Int. Conf. Very Large Data Bases (VLDB?09), Lyon, France, Aug.
A. Galland, S. Abiteboul, A. Marian, and P. Senellart. 2010. Corroborating information from disagreeing views.
In Proc. ACM Int. Conf. on Web Search and Data Mining (WSDM?10), New York, NY, Feb.
L. Ge, J. Gao, X. Yu, W. Fan, and A. Zhang. 2012. Estimating local information trustworthiness via multi-
source joint matrix factorization. In Data Mining (ICDM), 2012 IEEE 12th International Conference on, pages
876?881. IEEE.
1577
H. Ji, R. Grishman, H. T. Dang, K. Griffitt, and J. Ellis. 2010. An overview of the tac2010 knowledge base
population track. In Proc. Text Analytics Conf. (TAC?10), Gaithersburg, Maryland, Nov.
H. Ji, R. Grishman, and H.T. Dang. 2011. Overview of the tac 2011 knowledge base population track. In Text
Analysis Conf. (TAC) 2011.
X. Li and R. Grishman. 2013. Confidence estimation for knowledge base population. In Proc. Recent Advances
in Natural Language Processing (RANLP).
Q. Li and H. Ji. 2014. Incremental joint extraction of entity mentions and relations.
Q. Li, H. Ji, and L. Huang. 2013. Joint event extraction via structured prediction with global features.
M. D. Marneffe, B. Maccartney, and C. D. Manning. 2006. Generating typed dependency parses from phrase
structure parses. In LREC, pages 449,454.
R. Mihalcea. 2004. Graph-based ranking algorithms for sentence extraction, applied to text summarization. In
Proc. ACL2004.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Distant supervision for relation extraction without labeled
data. In Proc. ACL2009.
J. Pasternack and D. Roth. 2010. Knowing what to believe (when you already know something). In
Proceedings of the 23rd International Conference on Computational Linguistics, pages 877?885. Association
for Computational Linguistics.
J. Pasternack and D. Roth. 2011. Making better informed trust decisions with generalized fact-finding. In Proc.
2011 Int. Joint Conf. on Artificial Intelligence (IJCAI?11), Barcelona, Spain, July.
J. Pasternack and D. Roth. 2013. Latent credibility analysis. In Proc. WWW 2013.
E. Peserico and L. Pretto. 2009. Score and rank convergence of hits. In Proceedings of the 32nd international
ACM SIGIR conference on Research and development in information retrieval, pages 770?771. ACM.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: A Core of Semantic Knowledge. In
16th international World Wide Web conference (WWW 2007), New York, NY, USA. ACM Press.
S. Tamang and H. Ji. 2011. Adding smarter systems instead of human annotators: Re-ranking for slot filling
system combination. In Proc. CIKM2011 Workshop on Search & Mining Entity-Relationship data, Glasgow,
Scotland, UK, Oct.
VG Vydiswaran, C.X. Zhai, and D. Roth. 2011. Content-driven trust propagation framework. In Proceedings
of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 974?982.
ACM.
D. Wang, L. Kaplan, H. Le, and T. Abdelzaher. 2012. On truth discovery in social sensing: A maximum likelihood
estimation approach. In Proc. ACM/IEEE Int. Conf. on Information Processing in Sensor Networks (IPSN?12),
pages 233?244, Beijing, China, April.
X. Yin and W. Tan. 2011. Semi-supervised truth discovery. In Proc. 2011 Int. World Wide Web Conf. (WWW?11),
Hyderabad, India, March.
X. Yin, J. Han, and P. S. Yu. 2008. Truth discovery with multiple conflicting information providers on the Web.
IEEE Trans. Knowledge and Data Engineering, 20:796?808.
B. Zhao, B. I. P. Rubinstein, J. Gemmell, and J. Han. 2012. A Bayesian approach to discovering truth from
conflicting sources for data integration. In Proc. 2012 Int. Conf. Very Large Data Bases (VLDB?12), Istanbul,
Turkey, Aug.
1578
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1083?1093,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Resolving Entity Morphs in Censored Data
Hongzhao Huang1, Zhen Wen2, Dian Yu1, Heng Ji1,
Yizhou Sun3, Jiawei Han4, He Li5
1Computer Science Department and Linguistics Department,
Queens College and Graduate Center, City University of New York, New York, NY, USA
2IBM T. J. Watson Research Center, Hawthorne, NY, USA
3College of Computer and Information Science, Northeastern University, Boston, MA, USA
4Computer Science Department, Univerisity of Illinois at Urbana-Champaign, Urbana, IL, USA
5Admaster Inc., China
{hongzhaohuang1,yudiandoris1,hengjicuny1, liheact5}@gmail.com,
zhenwen@us.ibm.com2, yzsun@ccs.neu.edu3, hanj@illinois.edu4
Abstract
In some societies, internet users have to
create information morphs (e.g. ?Peace
West King? to refer to ?Bo Xilai?) to avoid
active censorship or achieve other com-
munication goals. In this paper we aim
to solve a new problem of resolving en-
tity morphs to their real targets. We ex-
ploit temporal constraints to collect cross-
source comparable corpora relevant to any
given morph query and identify target can-
didates. Then we propose various novel
similarity measurements including surface
features, meta-path based semantic fea-
tures and social correlation features and
combine them in a learning-to-rank frame-
work. Experimental results on Chinese
Sina Weibo data demonstrate that our ap-
proach is promising and significantly out-
performs baseline methods1.
1 Introduction
Language constantly evolves to maximize com-
municative success and expressive power in daily
social interactions. The proliferation of online so-
cial media significantly expedites this evolution,
as new phrases triggered by social events may be
disseminated rapidly in social media. To automati-
cally analyze such fast evolving language in social
media, new computational models are demanded.
In this paper, we focus on one particular lan-
guage evolution that creates new ways to commu-
nicate sensitive subjects because of the existence
of internet information censorship. We call this
1Some of the resources and open source programs devel-
oped in this work are made freely available for research pur-
pose at http://nlp.cs.qc.cuny.edu/Morphing.tar.gz
phenomenon information morph. For example,
when Chinese online users talk about the former
politician ?Bo Xilai?, they use a morph ?Peace
West King? instead, a historical figure four hun-
dreds years ago who governed the same region
as Bo. Morph can be considered as a special
case of alias used for hiding true entities in ma-
licious environment (Hsiung et al, 2005; Pantel,
2006). However, social network plays an impor-
tant role in generating morphs. Usually morphs
are generated by harvesting the collective wisdom
of the crowd to achieve certain communication
goals. Aside from the purpose of avoiding cen-
sorship, other motivations for using morph include
expressing sarcasm/irony, positive/negative senti-
ment or making descriptions more vivid toward
some entities or events. Table 1 presents the wide
range of cases that are used to create the morphs.
We can see that a morph can be either a regular
term with new meaning or a newly created term.
Morph Target Motivation
Peace West King Bo Xilai Sensitive
Blind Man Chen Guangcheng Sensitive
Miracle Brother Wang Yongping Irony
Kim Fat Kim Joing-il Negative
Kimchi Country South Korea Vivid
Table 1: Morph Examples and Motivations.
We believe that successful resolution of morphs
is a crucial step for automated understanding of
the fast evolving social media language, which
is important for social media marketing (Bar-
wise and Meehan, 2010). Another application
is to help common users without enough back-
ground/cultural knowledge to understand internet
language for their daily use. Furthermore, our ap-
proaches can also be applied for satire or other im-
plicit meaning recognition, as well as information
extraction (Bollegala et al, 2011).
1083
However, morph resolution in social media is
challenging due to the following reasons. First,
the sensitive real targets that exist in the same
data source under active censorship are often au-
tomatically filtered. Table 2 presents the distribu-
tions of some examples of morphs and their tar-
gets in English Twitter and Chinese Sina Weibo.
For example, the target ?Chen Guangcheng? only
appears once in Weibo. Thus, the co-occurrence
of a morph and its target is quite low in the vast
amount of information in social media. Second,
most morphs were not created based on pronunci-
ations, spellings or other encryptions of their origi-
nal targets. Instead, they were created according to
semantically related entities in historical and cul-
tural narratives (e.g. ?Peace West King? as morph
of ?Bo Xilai?) and thus very difficult to capture
based on typical lexical features. Third, tweets
from Twitter/Chinese Weibo are short (only up to
140 characters) and noisy, resulting in difficult ex-
traction of rich and accurate evidences due to the
lack of enough contexts.
 Frequency in  Twitter Frequency in  Weibo Morph Target Morph Target Morph Target Hu Ji Hu Jintao 1 3,864 2,611 71 Blind  Man Chen  Guangcheng 18 2,743 20,941 1 Baby Wen Jiabao 2238 2021 26,279 8  
Table 2: Distributions of Morph Examples
To the best of our knowledge, this is the first
work to use NLP and social network analysis tech-
niques to automatically resolve morphed informa-
tion. To address the above challenges, our paper
offers the following novel contributions.
? We detect target candidates by exploiting the
dynamics of the social media to extract tem-
poral distribution of entities, based on the as-
sumption that the popularity of an individ-
ual is correlated between censored and un-
censored text within a certain time window.
? Our approach builds and analyzes heteroge-
neous information networks from multiple
sources, such as Twitter, Sina Weibo and web
documents in formal genre (e.g. news) be-
cause a morph and its target tend to appear in
similar contexts.
? We propose two new similarity measures, as
well as integrating temporal information into
the similarity measures to generate global se-
mantic features.
? We model social user behaviors and use so-
cial correlation to assist in measuring seman-
tic similarities because the users who posted
a morph and its corresponding target tend to
share similar interests and opinions.
Our experiments demonstrate that the pro-
posed approach significantly outperforms tradi-
tional alias detection methods (Hsiung et al,
2005).
2 Approach Overview   Morph Query   Comparable Data Acquisition   
Target Candidate Ranking 
Target
Learning to Rank 
Semantic Features
Semantic Annotation and  Target Candidate Identification
Surface Features Social Features
Censored Data Uncensored Data
Figure 1: Overview of Morph Resolution
Given a morph query m, the goal of morph res-
olution is to find its real target. Figure 1 depicts
the general procedure of our approach. It consists
of two main sub-tasks:
? Target Candidate Identification: For each
m, discover a list of target candidates E =
{e1, e2, ..., eN}. First, relevant comparable
data sets that include m are retrieved. In
this paper we collect comparable censored
data from Weibo and uncensored data from
Twitter and Web documents such as news ar-
ticles. We then apply various annotations
such as word segmentation, part-of-speech
tagging, noun phrase chunking, name tagging
and event extraction to these data sets.
? Target Candidate Ranking: Rank the target
candidates in E. We explore various features
including surface, semantic and social fea-
tures, and incorporate them into a learning to
1084
rank framework. Finally, the top ranked can-
didate is produced as the resolved target.
3 Target Candidate Identification
The general goal of the first step is to identify a list
of target candidates for each morph query from the
comparable corpora including Sina Weibo, Chi-
nese News websites and English Twitter. How-
ever, obviously we cannot consider all of the
named entities in these sources as target candi-
dates due to the sheer volume of information. In
addition, morphs are not limited to named entity
forms. In order to narrow down the scope of tar-
get candidates, we propose a Temporal Distribu-
tion Assumption as follows. The intuition is that
a morph m and its real target e should have sim-
ilar temporal distributions in terms of their occur-
rences. Suppose the data sets are separated into Z
temporal slots (e.g. by day), the assumption can
be stated as:
Let Tm = {tm1, tm2, ..., tmZm} be the set of
temporal slots each morph m occurs, and Te =
{te1, te2, ..., teZe} be the set of slots a target can-
didate e occurs. Then e is considered as a target
candidate of m if and only if, for each tmi ? Tm
(i = 1, 2, ..., Zm), there exist a j ? {1, 2, ..., Ze}
such that tmi ? tej ? ?, where ? is a threshold
value (in this paper we set the threshold to 7 days,
which is optimized from a development set). For
comparison we also attempted topic modeling ap-
proach to detect target candidates, as shown in sec-
tion 5.3.
4 Target Candidate Ranking
Next, we propose a learning-to-rank framework to
rank target candidates based on various levels of
novel features based on surface, semantic and so-
cial analysis.
4.1 Surface Features
We first extract surface features between the
morph and the candidate based on measuring or-
thographic similarity measures which were com-
monly used in entity coreference resolution (e.g.
(Ng, 2010; Hsiung et al, 2005)). The measures
we use include ?string edit distance?, ?normalized
string edit distance? (Wagner and Fischer, 1974)
and ?longest common subsequence? (Hirschberg,
1977).
4.2 Semantic Features
4.2.1 Motivations
Fortunately, although a morph and its target may
have very different orthographic forms, they tend
to be embedded in similar semantic contexts
which involve similar topics and events. Figure 2
presents some example messages under censor-
ship (Weibo) and not under censorship (Twitter
and Chinese Daily). We can see that they include
similar topics, events (e.g., ?fell from power?,
?gang crackdown?, ?sing red songs?), and se-
mantic relations (e.g., family relations with ?Bo
Guagua?). Therefore if we can automatically ex-
tract and exploit these indicative semantic con-
texts, we can narrow down the real targets effec-
tively.
?Pe
ace W
est K
ingfr
om C
hong
qing
fell fr
om p
ower
, still
 nee
d to 
sing 
red s
ongs
?
?T
here
 is no
 diffe
renc
e be
twee
n tha
t 
guy?s
 plag
iarism
 and
 Buh
ou?s
gang
 
crack
down
.
?R
eme
mbe
r tha
t Buh
ousa
id tha
t his 
famil
y wa
s not
 rich 
at th
e pre
ss 
confe
renc
e a f
ew d
ays b
efore
 he 
fell fr
om p
ower
. His 
sonB
o 
Guag
uais 
supp
orted
 by h
is 
scho
larsh
ip.
?B
o Xila
i: ten
 thou
sand
 lette
rs of
 
accu
satio
n ha
ve be
en re
ceive
d du
ring 
Chon
gqing
gang
 crac
kdow
n.
?T
he w
ebpa
ge o
f ?Tia
nzeE
cono
mic 
Stud
y Ins
titute
?own
ed b
y the
 liber
al 
party
 has 
been
 clos
ed. T
his is
 the 
first 
affec
ted w
ebsit
e of 
the li
bera
l par
ty 
after
 Bo X
ilaife
ll from
 pow
er.
?B
o Xila
igave
 an e
xplan
ation
 abo
ut th
e 
sour
ce of
 hiss
on, B
o Gu
agua
?s 
tuitio
n.
?B
o Xila
iled C
hong
qing 
city l
eade
rs 
and 
40 d
istric
t and
 coun
ty pa
rty a
nd 
gove
rnme
nt lea
ders
 to si
ng re
d son
gs.
Weib
o(ce
nsore
d)
Twitt
er an
d Ch
inese
 New
s (un
cens
ored)
Figure 2: Cross-source Comparable Data Example
(each morph and target pair is shown in the same
color)
4.2.2 Information Network Construction
We define an information network as a directed
graph G = (V, E) with an object type mapping
function ? : V ? A and a link type mapping func-
tion ? : E ? R, where each object v ? V belongs
to one particular object type ?(v) ? A, each link
e ? E belongs to a particular relation ?(e) ? R.
If two links belong to the same relation type, then
they share the same starting object type as well as
the same ending object type. An information net-
work is homogeneous if and only if there is only
one type for both objects and links, and an infor-
mation network is heterogeneous when the objects
are from multiple distinct types or there exist more
than one type of links.
In order to construct the information networks
for morphs, we apply the Standford Chinese word
1085
segmenter with Chinese Penn Treebank segmen-
tation standard (Chang et al, 2008) and Stan-
ford part-of-speech tagger (Toutanova et al, 2003)
to process each sentence in the comparable data
sets. Then we apply a hierarchical Hidden Markov
Model (HMM) based Chinese lexical analyzer
ICTCLAS (Zhang et al, 2003) to extract named
entities, noun phrases and events.
We have also attempted using the results from
Dependency Parsing, Relation Extraction and
Event Extraction tools (Ji and Grishman, 2008)
to enrich the link types. Unfortunately the state-
of-the-art techniques for these tasks still perform
poorly on social media in terms of both accuracy
and coverage of important information, these so-
phisticated semantic links all produced negative
impact on the target ranking performance. There-
fore we limited the types of vertices into: Morph
(M), Entity(E), which includes target candidates,
Event (EV), and Non-Entity Noun Phrases (NP);
and used co-occurrence as the edge type. We ex-
tract entities, events, and non-entity noun phrases
that occur in more than one tweet as neighbors.
And for two vertices xi and xj , the weight wij
of their edge is the frequency they co-occur to-
gether within the tweets. A network schema of
such networks is shown in Figure 3. Figure 4
M
E NP
EV
Figure 3: Network Schema of Morph-Related Het-
erogeneous Information Network
presents an example of a heterogeneous informa-
tion network from the motivation examples fol-
lowing the above network schema, which connects
the morphs ?Peace West King?, ?Buhou? and their
corresponding target ?Bo Xilai?.
4.2.3 Meta-Path-Based Semantic Similarity
Measurements
Given the constructed network, a straightforward
solution for finding the target for a morph is to use
link-based similarity search. However, now ob-
jects are linked to different types of neighbors, if
all neighbors are treated as the same, it may cause
information loss problems. For example, the en-
tity ??? (Chongqing)? is a very important aspect
characterizing the politician ????(Bo Xilai)?
Ga
ng 
Cra
ckd
ow
n 
Fel
l Fr
om
 
Pow
er
Ch
ong
qin
g
Sin
g R
ed 
Son
gs
Bu
hou
Pea
ce 
We
st 
Kin
g
Bo
 Xil
ai
Bo
 Gu
agu
a
Ent
ity
Ent
ity
Ent
ity
Eve
nt
Eve
nt
Eve
nt
Mo
rph
Mo
rph
Figure 4: Example of Morph-Related Heteroge-
neous Information Network
since he governed it, and if a morph m which is
also highly correlated with ??? (Chongqing)?, it
is very likely that ?Bo Xilai? is the real target ofm.
Therefore, the semantic features generated from
neighbors such as the entity ??? (Chongqing)?
should be treated differently from other types of
neighbors such as ???(talented people)? .
In this work, we propose to measure the simi-
larity of two nodes over heterogeneous networks
as shown in Figure 3, by distinguishing neighbors
into three types according to the network schema
(i.e. entities, events, non-entity noun phrases). We
then adopt meta-path-based similarity measures
(Sun et al, 2011a; Sun et al, 2011b), which are
defined over heterogeneous networks to extract se-
mantic features. A meta-path is a path defined over
a network, and composed of a sequence of rela-
tions between different object types. For example,
as shown in Figure 3, a morph and its target can-
didate can be connected by three meta-paths, in-
cluding ?M - E - E?, ?M - EV - E?, and ?M - NP
- E?. Intuitively, each meta-path provides a unique
angle to measure how similar two objects are.
For the determined meta-paths, we extract se-
mantic features using the similarity measures pro-
posed in (Sun et al, 2011a; Hsiung et al, 2005).
We denote the neighbor sets of certain type for a
morph m and a target candidate e as ?(m) and
?(e), and a meta-path as P . We now list several
meta-path-based similarity measures below.
Common neighbors (CN). It measures the num-
ber of common neighbors that m and e share as
|?(m) ? ?(e)|.
Path count (PC). It measures the number of path
instances betweenm and e following meta-pathP .
Pairwise random walk (PRW). For a meta-
path P that can be decomposed into two shorter
1086
meta-paths with the same length P = (P1P2),
pairwise random walk measures the probabil-
ity of the pairwise random walk starting from
both m and e and reaching the same mid-
dle object. More formally, it is computed as?
(p1p2)?(P1P2) prob(p1)prob(p?12 ), where p?12 isthe inverse of p2.
Kullback-Leibler distance (KLD). For m and
e, the pairwise random walk probability of their
neighbors can be represented as two probability
vectors, then Kullback-Leibler distance (Hsiung
et al, 2005) can be used to compute sim(m, e).
Beyond the above similarity measures, we also
propose to use cosine-similarity-style normaliza-
tion method to modify common neighbor and pair-
wise random walk measures so that we can ensure
the morph node and the target candidate node are
strongly connected and also have similar popular-
ity. The modified algorithms penalize features in-
volved with the highly popular objects, since they
are more likely to have accidental interactions with
each other.
Normalized common neighbors (NCN). Nor-
malized common neighbors can be measured as
sim(m, e) = |?(m)??(e)|?
|?(m)|
?
|?(e)|
. It refines the simple
counting of common neighbors by avoiding bias
to highly visible or concentrated objects.
Pairwise random walk/cosine (PRW/cosine).
Pairwise random walk measures linkage weights
disproportionately with their visibility to their
neighbors, which may be too strong. Instead, we
propose to use a tamer normalization method as?
(p1p2)?(P1P2) f(p1)f(p?12 ), where.
f(p1) = count(m,x)??
x?? count(m,x)
,
f(p2) = count(e, x)??
x?? count(e, x)
,
and ? is the set of middle objects connecting the
decomposed meta-paths p1 and p?12 , count(y, x)
is the total number of paths between y and the mid-
dle object x, y could be m or e.
The above similarity measures can also be ap-
plied to homogeneous networks that do not differ-
entiate the neighbor types.
4.2.4 Global Semantic Feafure Generation
A morph tends to have higher temporal correlation
with its real target, and share more similar topics
compared to other irrelevant targets. Therefore,
we propose to incorporate temporal information
into similarity measures to generate global seman-
tic features.
Let T = t1 ? t2 ? ... ? tN be a set of temporal
slots (i.e. by day),E be the set of target candidates
for each morphm. Then for each ti ? T , and each
e ? E, the local semantic features simti(m, e)
is extracted based only on the information posted
within ti using one of the similarity measures in-
troduced in Section 4.2.3. Then we propose two
approaches to generate global semantic features.
The first approach is adding the similarity score
between m and e in each temporal slot to attain
the first set of global features:
simglobal sum(m, e) =
?
ti?T
simti(m, e).
The second method first normalizes the similarity
score in each temporal slot ti, them sum the nor-
malized scores to generate the second set of global
features, which can be calculated as
simglobal norm(m, e) =
?
ti?T
normti(m, e).
where normti(m, e) = simti (m,e)?e?E simti (m,e) .
4.2.5 Integrate Cross Source/Cross Genre
Information
Due to internet information censorship or surveil-
lance, users may need to use morphs to post sensi-
tive information. For example, the Chinese Weibo
message ?????,??????? (Already
put in prison, still need to serve Buhou?? include
a morph ?? (Buhou). In contrast, users are less
restricted in some other uncensored social media
such as Twitter. For example, the tweet from Twit-
ter ?...?????????????????...
(...call Bo Xilai?peace west king? or ?buhou?...)?
contains both the morph and the real target ??
? (Bo Xilai). Therefore, we propose to integrate
information from another source (e.g. Twitter) to
help resolution of sensitive morphs in Weibo.
Another difficulty from morph resolution in
micro-blogging is that tweets are only allowed to
contain maximum 140 characters with a lot of
noise and diverse topics. The shortness and di-
versity of tweets may limit the power of content
analysis for semantic feature extraction. However,
formal genres such as web documents are cleaner
and contain richer contexts, thus can provide more
topically related information. In this work, we also
exploit the background web documents from the
1087
embedded URLs in tweets to enrich information
network construction. After applying the same an-
notation techniques as tweets for uncensored data
sets, sentence-level co-occurrence relations are ex-
tracted and integrated into the network as shown in
Figure 3.
4.3 Social Features
It has been shown that there exist correlation be-
tween neighbors in social networks (Anagnos-
topoulos et al, 2008; Wen and Lin, 2010). Be-
cause of such social correlation, close social
neighbors in social media such as Twitter and
Weibo may post similar information, or share sim-
ilar opinion. Therefore, we can utilize social cor-
relation to assist in resolving morphs.
As social correlation can be defined as a func-
tion of social distance between a pair of users, we
use social distance as a proxy to social correla-
tion in our approach. The social distance between
user i and j is defined by considering the degree
of separation in their interaction (e.g. retweet-
ing and mentioning) and the amount of the in-
teraction. Similar definition has been shown ef-
fective in characterizing social distance in social
networks extracted from communication data (Lin
et al, 2012; Wen and Lin, 2010). Specifically,
it is dist(i, j) = ?K?1k=1 1strength(vk,vk+1) , where
v1, ..., vk are the nodes on the shortest path from
user i to user j, and strength(vk, vk+1) measures
the strength of interactions between vk and vk+1
as: strength(i, j) = log(Xij)maxj log(Xij) , where Xij isthe total interactions between user i and j, includ-
ing both retweeting and mentioning (If Xij < 10,
we set strength(i, j) = 0).
We integrate social correlation and temporal in-
formation to define our social features. The in-
tuition is that when a morph is used by an user,
the real target may also in the posts by the user or
his/her close friends within a certain time period.
Let T be the set of temporal slots a morph m oc-
curs, Ut be the set of users whose posts include m
in slot t where t ? T , and Uc be the set of close
friends (i.e., social distance < 0.5) for Ut. The
social features are defined as
s(m, e) =
?
t?T f(e, t, Ut, Uc)
|T | .
where f(e, t, Ut, Uc) is a indicator function which
return 1 if one of the users in Ut or Uc posts tweets
include the target candidate e within 7 days before
t.
4.4 Learning-to-Rank
Similar to (Hsiung et al, 2005; Sun et al, 2011a),
we then model the probability of linkage predic-
tion between a morph m and its target candidate
e as a function incorporating the surface, semantic
and social features. Given a training pair ?m, e?,
we choose the standard logistic regression model
to learn weights for the features defined above.
The learnt model is used to predict the probabil-
ity of linking an unseen morph and its target can-
didate. Based on the descending ranking order of
the probability, we select top k candidates as the
final answers based on the answer size k.
5 Experiments
Next, we present the experiment under various set-
tings shown in Table 3, and the impacts of cross
source and cross genre information.
5.1 Data and Evaluation Metric
We collected 1, 553, 347 tweets from Chinese Sina
Weibo from May 1 to June 30 to construct the
censored data set, and retrieved 66, 559 web doc-
uments from the embedded URLs in tweets as the
initial uncensored data set. Retweets and redun-
dant web documents are filtered to ensure more
reliable frequency counting of co-occurrence rela-
tions. We asked two native Chinese annotators to
analyze the data, and construct a test set consisted
of 107 morph entities (81 persons and 26 loca-
tions) and their real targets as our references. We
verified the references by Web resources includ-
ing the summary of popular morphs in Wikipedia
2. In addition, we used 23 sensitive morphs and
the entities that appear in the tweets as queries and
retrieved 25, 128 Chinese tweets from 10% Twit-
ter feeds within the same time period, as well as
7, 473 web documents from the embedded URLs
and added them into the uncensored data set.
To evaluate the system performance, we use
leave-one-out cross validation by computing ac-
curacy as Acc@k = CkQ , where Ck is the to-tal number of correctly resolved morphs at top
k ranked answers, and Q is the total number of
morph queries. We consider a morph as correctly
resolved at the top k answers if the top k answer
set contains the real target of the morph.
2http://zh.wikipedia.org/wiki/??????????
1088
Feature sets Descriptions
Surf Surface features
HomB Semantic features extracted from homogeneous CN, PC, PRW, and KLD
HomE HomB + semantic features extracted from homogeneous NCN and PRW/cosine
HetB Semantic features extracted from heterogeneous CN, PC, PRW and KLD
HetE HetB + Semantic features extracted from heterogeneous NCN and PRW/cosine
Glob? Global semantic features
Social Social network features
Table 3: Description of feature sets. ? Glob only uses the same set of similarity measures when combined
with other semantic features.
5.2 Resolution Performance
5.2.1 Single Genre Information
We first study the contributions of each set of sur-
face and semantic features, as shown in the first
five rows in Table 4. The poor performance based
on surface features shows that morph resolution
task is very challenging since 70% of morphs are
not orthographically similar to their real targets.
Thus, capturing a morph?s semantic meaning is
crucial. Overall, the results demonstrate the ef-
fectiveness of our proposed methods. Specifi-
cally, comparing ?HomB? and ?HetB?, ?HomE?
and ?HetE?, we can see that the semantic fea-
tures based on heterogeneous networks have ad-
vantages over those based on homogeneous net-
works. This corroborates that different neighbor
sets contribute differently, and such discrepancies
should be captured. And comparisions of ?HomB?
and ?HomE?, ?HetB? and ?HetE?demonstrate the
effectiveness of our two new proposed measures.
To evaluate the importance of each similarity mea-
sures, we delete the semantic features obtained
from each measure in ?HetE? and re-evaluate the
system. We find that NCN is the most effective
measure, while KLD is the least important one.
Further adding the global semantic features signif-
icantly improves the performance. This indicates
that capturing both temporal correlations and se-
mantics of morphing simultaneously are important
for morph resolution.
Table 5 shows that combination of surface and
semantic features further improves the perfor-
mance, showing that they are complementary. For
example, using only surface features, the real tar-
get ?????Steve Jobs?? of the morph ???
? (Qiao Boss)? is not top ranked since some other
candidates such as ??? (George)? are more or-
thographically similar. However, ?Steve Jobs? is
ranked top when combined with semantic features.
Features Surf HomB HomE HetB HetE
Acc@1 0.028 0.201 0.192 0.224 0.252
Acc@5 0.159 0.313 0.369 0.393 0.421
Acc@10 0.243 0.346 0.407 0.439 0.467
Acc@20 0.313 0.411 0.467 0.50 0.523
Features + Glob + Glob + Glob + Glob
Acc@1 0.230 0.285 0.257 0.285
Acc@5 0.402 0.407 0.449 0.458
Acc@10 0.435 0.458 0.50 0.495
Acc@20 0.486 0.523 0.565 0.542
Table 4: The System Performance Based on Each
Single Feature Set.
Features Surf +
HomB
Surf +
HomE
Surf +
HetB
Surf +
HetE
Acc@1 0.234 0.238 0.262 0.276
Acc@5 0.416 0.444 0.481 0.519
Acc@10 0.477 0.505 0.533 0.570
Acc@20 0.519 0.561 0.565 0.598
Features + Glob + Glob + Glob + Glob
Acc@1 0.290 0.341 0.322 0.346
Acc@5 0.505 0.495 0.528 0.533
Acc@10 0.551 0.551 0.579 0.584
Acc@20 0.594 0.603 0.636 0.631
Table 5: The System Performance Based on Com-
binations of Surface and Semantic Features.
5.2.2 Cross Source and Cross Genre
Information
We integrate the cross source information from
Twitter, and the cross genre information from web
documents into Weibo tweets for information net-
work construction, and extract a new set of se-
mantic features. Table 6 shows that further gains
can be achieved. Notice that integrating tweets
from Twitter mainly improves the ranking for top
k where k > 1. This is because Weibo dominates
our dataset, and in Weibo many of these sensi-
tive morphs are mostly used with their traditional
meanings instead of the morph senses. Further
performance improvement is achieved by integrat-
ing information from background formal web doc-
uments which can provide richer context and rela-
tions.
1089
Features Surf +
HomB +
Glob
Surf +
HomE +
Glob
Surf +
HetB +
Glob
Surf +
HetE +
Glob
Acc@1 0.290 0.341 0.322 0.346
Acc@5 0.505 0.495 0.528 0.533
Acc@10 0.551 0.551 0.579 0.584
Acc@20 0.594 0.603 0.636 0.631
Features + Twit-
ter
+ Twit-
ter
+ Twit-
ter
+ Twit-
ter
Acc@1 0.308 0.336 0.336 0.346
Acc@5 0.514 0.519 0.547 0.565
Acc@10 0.579 0.594 0.594 0.636
Acc@20 0.631 0.640 0.668 0.668
Features + Web + Web + Web + Web
Acc@1 0.327 0.360 0.341 0.379
Acc@5 0.528 0.519 0.565 0.575
Acc@10 0.594 0.589 0.622 0.645
Acc@20 0.631 0.650 0.678 0.678
Table 6: The System Performance of Integrating
Cross Source and Cross Genre Information.
5.2.3 Effects of Social Features
Table 7 shows that adding social features can im-
prove the best performance achieved so far. This is
because a group of people with close relationships
may share similar opinion. As an example, two
tweets ?...of course the reputation of Buhou is a
little too high! //@User1: //@User2: Chongqing
event tells us...)? and ?...do not follow Bo Xi-
lai...@User1...) are from two users in the same
social group.One includes a morph ?Buhou? and
the other includes its target ?Bo Xilai?.
Features Surf +
HomB +
Glob +
Twitter
+ Web
Surf +
HomE +
Glob +
Twitter
+ Web
Surf +
HetB +
Glob +
Twitter
+ Web
Surf +
HetE +
Glob +
Twitter
+ Web
Acc@1 0.327 0.360 0.341 0.379
Acc@5 0.528 0.519 0.565 0.575
Acc@10 0.594 0.589 0.622 0.645
Acc@20 0.631 0.650 0.678 0.678
Features + Social + Social + Social + Social
Acc@1 0.336 0.369 0.365 0.379
Acc@5 0.537 0.547 0.589 0.594
Acc@10 0.594 0.601 0.645 0.659
Acc@20 0.645 0.664 0.701 0.701
Table 7: The Effects of Social Features.
5.3 Effects of Candidate Detection
The performance with and without candidate de-
tection step (using all features) is shown in Ta-
ble 8. The gain is small since the combination
of all features in the learning to rank framework
can already well capture the relationship between
a morph and a target candidate. Nevertheless, the
temporal distribution assumption is effective. It
helps to filter out 80% of unrelated targets and
speed up the system 5 times, while retain 98.5%
of the morph candidates that can be detected.
System Acc@1 Acc@5 Acc@10 Acc@20
Without 0.365 0.579 0.645 0.696
With 0.379 0.594 0.659 0.701
Table 8: The Effects of Temporal Constraint
We also attempted using topic modeling ap-
proach to detect target candidates. Due to the large
amount of data, we first split the data set on a daily
basis, then applied Probabilistic Latent Semantic
Analysis (PLSA) (Hofmann, 1999). Named enti-
ties which co-occur at least ? times with a morph
query in the same topic are selected as its target
candidates. As shown in Table9 (K is the num-
ber of predefined topics), PLSA is not quite effec-
tive mainly because traditional topic modeling ap-
proaches do not perform well on short texts from
social media. Therefore, in this paper we choose
a simple method based on temporal distribution to
detect target candidates.
Method All Temporal PLSA( PLSA(
K = 5 K = 5
? = 1) ? = 2)
Acc 0.935 0.921 0.935 0.925
No. 8, 111 1, 964 6, 380 4, 776
Method PLSA( PLSA( PLSA( PLSA(
K = 10 K = 10 K = 20 K = 20
? = 1) ? = 2) ? = 1) ? = 2)
Acc 0.935 0.907 0.888 0.757
No. 5, 117 3, 138 3, 702 1, 664
Table 9: Accuracy of Target Candidate Detection
5.4 Discussions
Compared with the standard alias detection
(?Surf+HomB?) approach (Hsiung et al, 2005),
our proposed approach achieves significantly bet-
ter performance (99.9% confidence level by the
Wilcoxon Matched-Pairs Signed-Ranks Test for
Acc@1). We further explore two types of factors
which may affect the system performance as fol-
lows.
One important aspect affecting the resolution
performance is the morph & non-morph ambigu-
ity. We categorize a morph query as ?Unique? if
the string is mainly used as a morph when it oc-
curs, such as ??? (Bodu)? which is used to re-
fer to ?Bo Xilai?; otherwise as ?Common? (e.g.
??? (Baby)? ,??? (President)? ). Table 10
presents the separate scores for these two cate-
gories. We can see that the morphs in ?Unique?
1090
category have much better resolution performance
than those in ?Common? category.
Category Number Acc@1 Acc@5 Acc@10 Acc@20
Unique 72 0.479 0.715 0.771 0.819
Common 35 0.171 0.343 0.40 0.429
Table 10: Performance of Two Categories
We also investigate the effects of popularity of
morphs on the resolution performance. We split
the queries into 5 bins with equal size based on the
non-descending frequency, and evaluate Acc@1
separately. As shown in Table11, we can see that
the popularity is not highly correlated with the per-
formance.
Rank 0 ?
20%
20% ?
40%
40% ?
60%
60% ?
80%
80% ?
100%
All 0.333 0.476 0.341 0.429 0.318
Unique 0.321 0.679 0.379 0.571 0.483
Common 0.214 0.214 0.071 0.071 0.286
Table 11: Effects of Popularity of Morphs
6 Related Work
To analyze social media behavior under active
censorship, (Bamman et al, 2012) automatically
discovered politically sensitive terms from Chi-
nese tweets based on message deletion analysis.
In contrast, our work goes beyond target idendi-
fication by resolving implicit morphs to their real
targets.
Our work is closely related to alias detec-
tion (Hsiung et al, 2005; Pantel, 2006; Bollegala
et al, 2011; Holzer et al, 2005). We demon-
strated that state-of-the-art alias detection meth-
ods did not perform well on morph resolution. In
this paper we exploit cross-genre information and
social correlation to measure semantic similarity.
(Yang et al, 2011; Huang et al, 2012) also showed
the effectiveness of exploiting information from
formal web documents to enhance tweet summa-
rization and tweet ranking.
Other similar research lines are the TAC-KBP
Entity Linking (EL) (Ji et al, 2010; Ji et al, 2011),
which links a named entity in news and web docu-
ments to an appropriate knowledge base (KB) en-
try, the task of mining name translation pairs from
comparable corpora (Udupa et al, 2009; Ji, 2009;
Fung and Yee, 1998; Rapp, 1999; Shao and Ng,
2004; Hassan et al, 2007) and the link predic-
tion problem (Adamic and Adar, 2001; Liben-
Nowell and Kleinberg, 2003; Sun et al, 2011b;
Hasan et al, 2006; Wang et al, 2007; Sun et al,
2011a). Most of the work focused on unstruc-
tured or structured data with clean and rich re-
lations (e.g. DBLP). In contrast, our work con-
structs heterogeneous information networks from
unstructured, noisy multi-genre text without ex-
plicit entity attributes.
7 Conclusion and Future Work
To the best of our knowledge, this is the first work
of resolving implicit information morphs from the
data under active censorship. Our promising re-
sults can well serve as a benchmark for this new
problem. Both of the Meta-path based and so-
cial correlation based semantic similarity mea-
surements are proven powerful and complemen-
tary.
In this paper we have focused on entity morphs.
In the future we will extend our method to dis-
cover other types of information morphs, such as
events and nominal mentions. In addition, auto-
matic identification of candidate morphs is another
challenging task, especially when the mentions are
ambiguous and can also refer to other real enti-
ties. Our ongoing work includes identifying can-
didate morphs from scratch, as well as discovering
morphs for a given target based on anomaly anal-
ysis and textual coherence modeling.
Acknowledgments
Thanks to the three anonymous reviewers for their
insightful comments. This work was supported
by the U.S. Army Research Laboratory under Co-
operative Agreement No. W911NF- 09-2-0053
(NS-CTA), the U.S. NSF CAREER Award under
Grant IIS-0953149, the U.S. NSF EAGER Award
under Grant No. IIS-1144111, the U.S. DARPA
FA8750-13-2-0041 - Deep Exploration and Filter-
ing of Text (DEFT) Program, the U.S. DARPA un-
der Agreement No. W911NF-12-C-0028, CUNY
Junior Faculty Award, NSF IIS-0905215, CNS-
0931975, CCF-0905014, and MIAS, a DHS-IDS
Center for Multimodal Information Access and
Synthesis at UIUC. The views and conclusions
contained in this document are those of the au-
thors and should not be interpreted as representing
the official policies, either expressed or implied,
of the U.S. Government. The U.S. Government is
authorized to reproduce and distribute reprints for
Government purposes notwithstanding any copy-
right notation here on.
1091
References
Lada A. Adamic and Eytan Adar. 2001. Friends
and neighbors on the web. SOCIAL NETWORKS,
25:211?230.
Aris Anagnostopoulos, Ravi Kumar, and Mohammad
Mahdian. 2008. Influence and correlation in social
networks. In KDD, pages 7?15.
David Bamman, Brendan O?Connor, and Noah A.
Smith. 2012. Censorship and deletion practices in
chinese social media. First Monday, 17(3).
Patrick Barwise and Sea?n Meehan. 2010. The one
thing you must get right when building a brand.
Harvard Business Review, 88(12):80?84.
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2011. Au-
tomatic discovery of personal name aliases from
the web. Knowledge and Data Engineering, IEEE
Transactions on, 23(6):831?844.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, StatMT ?08, pages 224?232.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, com-
parable texts. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics - Volume 1, ACL ?98, pages
414?420.
Mohammad Al Hasan, Vineet Chaoji, Saeed Salem,
and Mohammed Zaki. 2006. Link prediction using
supervised learning. In In Proc. of SDM 06 work-
shop on Link Analysis, Counterterrorism and Secu-
rity.
Ahmed Hassan, Haytham Fahmy, and Hany Has-
san. 2007. Improving named entity translation
by exploiting comparable and parallel corpora. In
RANLP.
Daniel S. Hirschberg. 1977. Algorithms for the
longest common subsequence problem. J. ACM,
24(4):664?675.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, SIGIR ?99,
pages 50?57.
Ralf Holzer, Bradley Malin, and Latanya Sweeney.
2005. Email alias detection using social network
analysis. In Conference on Knowledge Discovery
in Data: Proceedings of the 3 rd international work-
shop on Link discovery, volume 21, pages 52?57.
Paul Hsiung, Andrew Moore, Daniel Neill, and Jeff
Schneider. 2005. Alias detection in link data sets.
In Proceedings of the International Conference on
Intelligence Analysis, May.
Hongzhao Huang, Arkaitz Zubiaga, Heng Ji, Hongbo
Deng, Dong Wang, Hieu Khac Le, Tarek F. Ab-
delzaher, Jiawei Han, Alice Leung, John Hancock,
and Clare R. Voss. 2012. Tweet ranking based on
heterogeneous networks. In COLING, pages 1239?
1256.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of ACL, pages 254?262.
H. Ji, R. Grishman, H.T. Dang, K. Griffitt, and J. El-
lis. 2010. Overview of the tac 2010 knowledge base
population track. In Text Analysis Conference (TAC)
2010.
H. Ji, R. Grishman, and H.T. Dang. 2011. Overview
of the tac 2011 knowledge base population track. In
Text Analysis Conference (TAC) 2011.
Heng Ji. 2009. Mining name translations from com-
parable corpora by creating bilingual information
networks. In Proceedings of the 2nd Workshop
on Building and Using Comparable Corpora: from
Parallel to Non-parallel Corpora, BUCC ?09, pages
34?37.
David Liben-Nowell and Jon Kleinberg. 2003. The
link prediction problem for social networks. In
Proceedings of the twelfth international conference
on Information and knowledge management, CIKM
?03, pages 556?559.
Ching-Yung Lin, Lynn Wu, Zhen Wen, Hanghang
Tong, Vicky Griffiths-Fisher, Lei Shi, and David
Lubensky. 2012. Social network analysis in enter-
prise. Proceedings of the IEEE, 100(9):2759?2776.
Vincent Ng. 2010. Supervised noun phrase corefer-
ence research: the first fifteen years. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, ACL ?10, pages 1396?
1411.
Patrick Pantel. 2006. Alias detection in malicious en-
vironments. In AAAI Fall Symposium on Capturing
and Using Patterns for Evidence Detection, pages
14?20.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th annual meet-
ing of the Association for Computational Linguistics
on Computational Linguistics, ACL ?99, pages 519?
526.
Li Shao and Hwee Tou Ng. 2004. Mining new word
translations from comparable corpora. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, COLING ?04.
Yizhou Sun, Rick Barber, Manish Gupta, Charu C. Ag-
garwal, and Han Jiawei. 2011a. Co-author relation-
ship prediction in heterogeneous bibliographic net-
works. In Proceedings of the 2011 International
Conference on Advances in Social Networks Anal-
ysis and Mining, ASONAM ?11, pages 121?128.
1092
Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S. Yu, and
Tianyi Wu. 2011b. Pathsim: Meta path-based top-k
similarity search in heterogeneous information net-
works. PVLDB, 4(11):992?1003.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, NAACL ?03, pages 173?180.
Raghavendra Udupa, K. Saravanan, A. Kumaran, and
Jagadeesh Jagarlamudi. 2009. Mint: a method
for effective and scalable mining of named entity
transliterations from large comparable corpora. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, EACL ?09, pages 799?807.
Robert A. Wagner and Michael J. Fischer. 1974.
The string-to-string correction problem. J. ACM,
21(1):168?173.
Chao Wang, Venu Satuluri, and Srinivasan
Parthasarathy. 2007. Local probabilistic mod-
els for link prediction. In Proceedings of the 2007
Seventh IEEE International Conference on Data
Mining, ICDM ?07, pages 322?331.
Zhen Wen and Ching-Yung Lin. 2010. On the qual-
ity of inferring interests from social neighbors. In
KDD, pages 373?382.
Zi Yang, Keke Cai, Jie Tang, Li Zhang, Zhong Su, and
Juanzi Li. 2011. Social context summarization. In
Proceedings of the 34th international ACM SIGIR
conference on Research and development in Infor-
mation Retrieval, SIGIR ?11, pages 255?264.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. Hhmm-based chinese lexical analyzer
ictclas. In Proceedings of the second SIGHAN work-
shop on Chinese language processing - Volume 17,
SIGHAN ?03, pages 184?187.
1093
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 706?711,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Be Appropriate and Funny: Automatic Entity Morph Encoding
Boliang Zhang
1
, Hongzhao Huang
1
, Xiaoman Pan
1
, Heng Ji
1
, Kevin Knight
2
Zhen Wen
3
, Yizhou Sun
4
, Jiawei Han
5
, Bulent Yener
1
1
Computer Science Department, Rensselaer Polytechnic Institute
2
Information Sciences Institute, University of Southern California
3
IBM T. J. Watson Research Center
4
College of Computer and Information Science, Northeastern University
5
Computer Science Department, Univerisity of Illinois at Urbana-Champaign
1
{zhangb8,huangh9,panx2,jih,yener}@rpi.edu,
2
knight@isi.edu
3
zhenwen@us.ibm.com,
4
yzsun@ccs.neu.edu,
5
hanj@illinois.edu
Abstract
Internet users are keen on creating differ-
ent kinds of morphs to avoid censorship,
express strong sentiment or humor. For
example, in Chinese social media, users
often use the entity morph ???? (In-
stant Noodles)? to refer to ???? (Zhou
Yongkang)? because it shares one char-
acter ?? (Kang)? with the well-known
brand of instant noodles ???? (Master
Kang)?. We developed a wide variety of
novel approaches to automatically encode
proper and interesting morphs, which can
effectively pass decoding tests
1
.
1 Introduction
One of the most innovative linguistic forms in so-
cial media is Information Morph (Huang et al,
2013). Morph is a special case of alias to hide the
original objects (e.g., sensitive entities and events)
for different purposes, including avoiding censor-
ship (Bamman et al, 2012; Chen et al, 2013),
expressing strong sentiment, emotion or sarcasm,
and making descriptions more vivid. Morphs are
widely used in Chinese social media. Here is an
example morphs: ?????????????
????. (Because of Gua Dad?s issue, Instant
Noodles faces down with Antenna.)?, where
? ??? (Gua Dad)? refers to ???? (Bo Xilai)?
because it shares one character ?? (Gua)? with
???? (Bo Guagua)? who is the son of ???
? (Bo Xilai)?;
? ???? (Instant Noodles)? refers to ????
(Zhou Yongkang)? because it shares one char-
acter ?? (kang)? with the well-known instant
noodles brand ???? (Master Kang)?;
1
The morphing data set is available for research purposes:
http://nlp.cs.rpi.edu/data/morphencoding.tar.gz
? ??? (Antenna)? refers to ???? (Wen Ji-
abao)? because it shares one character ??
(baby)? with the famous children?s television
series ????? (Teletubbies)?;
In contrast with covert or subliminal chan-
nels studied extensively in cryptography and se-
curity, Morphing provides confidentiality against
a weaker adversary which has to make a real time
or near real time decision whether or not to block
a morph within a time interval t. It will take longer
than the duration t for a morph decoder to decide
which encoding method is used and exactly how it
is used; otherwise adversary can create a codebook
and decode the morphs with a simple look up.
We note that there are other distinct characteristics
of morphs that make them different from crypto-
graphic constructs: (1) Morphing can be consid-
ered as a way of using natural language to com-
municate confidential information without encryp-
tion. Most morphs are encoded based on seman-
tic meaning and background knowledge instead
of lexical changes, so they are closer to Jargon.
(2) There can be multiple morphs for an entity.
(3) The Shannon?s Maxim ?the enemy knows the
system? does not always hold. There is no com-
mon code-book or secret key between the sender
and the receiver of a morph. (4) Social networks
play an important role in creating morphs. One
main purpose of encoding morphs is to dissemi-
nate them widely so they can become part of the
new Internet language. Therefore morphs should
be interesting, fun, intuitive and easy to remem-
ber. (5) Morphs rapidly evolve over time, as some
morphs are discovered and blocked by censorship
and newly created morphs emerge.
We propose a brand new and challenging re-
search problem - can we automatically encode
morphs for any given entity to help users commu-
nicate in an appropriate and fun way?
706
2 Approaches
2.1 Motivation from Human Approaches
Let?s start from taking a close look at human?s
intentions and general methods to create morphs
from a social cognitive perspective. In Table 1
and Table 2, we summarize 548 randomly selected
morphs into different categories. In this paper we
automate the first seven human approaches, with-
out investigating the most challenging Method 8,
which requires deep mining of rich background
and tracking all events involving the entities.
2.2 M1: Phonetic Substitution
Given an entity name e, we obtain its pho-
netic transcription pinyin(e). Similarly, for each
unique term t extracted from Tsinghua Weibo
dataset (Zhang et al, 2013) with one billion
tweets from 1.8 million users from 8/28/2012 to
9/29/2012, we obtain pinyin(t). According to the
Chinese phonetic transcription articulation man-
ner
2
, the pairs (b, p), (d, t), (g,k), (z,c), (zh,ch),
( j,q), (sh,r), (x,h), (l,n), (c,ch), (s,sh) and (z,zh)
are mutually transformable.
If a part of pinyin(e) and pinyin(t) are identi-
cal or their initials are transformable, we substi-
tute the part of e with t to form a new morph.
For example, we can substitute the characters of
??? ?? (Bill Gates) [Bi Er Gai Ci]? with
??? (Nose and ear) [Bi Er]? and ??? (Lid)
[Gai Zi]? to form new morph ??? ?? (Nose
and ear Lid) [Bi Er Gai Zi]?. We rank the candi-
dates based on the following two criteria: (1) If the
morph includes more negative words (based on a
gazetteer including 11,729 negative words derived
from HowNet (Dong and Dong, 1999), it?s more
humorous (Valitutti et al, 2013). (2) If the morph
includes rarer terms with low frequency, it is more
interesting (Petrovic and Matthews, 2013).
2.3 M2: Spelling Decomposition
Chinese characters are ideograms, hieroglyphs
and mostly picture-based. It allows us to natu-
rally construct a virtually infinite range of combi-
nations from a finite set of basic units - radicals (Li
and Zhou, 2007). Some of these radicals them-
selves are also characters. For a given entity name
e = c
1
...c
n
, if any character c
k
can be decomposed
into two radicals c
1
k
and c
2
k
which are both char-
acters or can be converted into characters based
on their pictograms (e.g., the radical ??? can be
2
http://en.wikipedia.org/wiki/Pinyin#Initials and finals
converted into??? (grass) ), we create a morph by
replacing c
k
with c
1
k
c
2
k
in e. Here we use a charac-
ter to radical mapping table that includes 191 rad-
icals (59 of them are characters) and 1328 com-
mon characters. For example, we create a morph
???? (Person Dumb Luo)? for ??? (Paul)?
by decomposing ?? (Pau-)? into ?? (Person)?
and ?? (Dull)?. A natural alternative is to com-
posing two chracter radicals in an entity name to
form a morph. However, very few Chinese names
include two characters with single radicals.
2.4 M3: Nickname Generation
We propose a simple method to create morphs by
duplicating the last character of an entity?s first
name. For example, we create a morph ???
(Mimi)? to refer to ??? (Yang Mi)?.
2.5 M4: Translation and Transliteration
Given an entity e, we search its English translation
EN(e) based on 94,015 name translation pairs (Ji
et al, 2009). Then, if any name component in
EN(e) is a common English word, we search for
its Chinese translation based on a 94,966 word
translation pairs (Zens and Ney, 2004), and use the
Chinese translation to replace the corresponding
characters in e. For example, we create a morph
??? ?? (Larry bird)? for ??? ?? (Larry
Bird)? by replacing the last name ??? (Bird)?
with its Chinese translation ??? (bird)?.
2.6 M5: Semantic Interpretation
For each character c
k
in the first name of a given
entity name e, we search its semantic interpreta-
tion sentence from the Xinhua Chinese character
dictionary including 20,894 entries
3
. If a word
in the sentence contains c
k
, we append the word
with the last name of e to form a new morph. Sim-
ilarly to M1, we prefer positive, negative or rare
words. For example, we create a morph ????
(Bo Mess)? for ???? (Bo Xi Lai)? because the
semantic interpretation sentence for ?? (Lai)? in-
cludes a negative word ??? (Mess)?.
2.7 M6: Historical Figure Mapping
We collect a set of 38 famous historical figures
including politicians, emperors, poets, generals,
ministers and scholars from a website. For a given
entity name e, we rank these candidates by ap-
plying the resolution approach as described in our
previous work (Huang et al, 2013) to measure the
similarity between an entity and a historic figure
3
http://xh.5156edu.com/
707
Category
Frequency
Distribution
Examples
Entity Morph Comment
(1) Avoid censorship 6.56% ??? (Bo Xi-
lai)
B?? (B Secre-
tary)
?B? is the first letter of ?Bo? and ?Secretary? is
the entity?s title.
(2) Express strong
sentiment, sarcasm,
emotion
15.77% ??? (Wang
Yongping)
? ? ? (Miracle
Brother)
Sarcasm on the entity?s public speech: ?It?s a mir-
acle that the girl survived (from the 2011 train col-
lision)?.
(3) Be humorous or
make descriptions
more vivid
25.91% ?? (Yang Mi) ???? (Tender
Beef Pentagon)
The entity?s face shape looks like the shape of fa-
mous KFC food ?Tender Beef Pentagon?.
Mixture 25.32% ? ? ?
(Gaddafi)
???? (Crazy
Duck Colonel)
Sarcasm on Colonel Gaddafi?s violence.
Others 23.44% ??? (Chi-
ang Kai-shek)
??? (Peanut) Joseph Stilwell, a US general in China during
World War II, called Chiang Kai-shek ????
(Peanut)? in his diary because of his stubbornness.
Table 1: Morph Examples Categorized based on Human Intentions
No. Category
Frequency
Distribution
Example
Entity Morph Comment
M1 Phonetic Sub-
stitution
12.77% ? ? ?
(Sarkozy)
??? (Silly Po-
lite)
The entity?s phonetic transcript ?Sa Ke Qi? is
similar to the morph?s ?Sha Ke Qi?.
M2 Spelling De-
composition
0.73% ??? (Hu
Jintao)
?? (Old Moon) The entity?s last name is decomposed into the
morph ??? (Old Moon)??
M3 Nickname Gen-
eration
12.41% ??? (Jiang
Zemin)
?? (Old Jiang) The morph is a conventional name for old people
with last name ?Jiang?.
M4 Translation &
Transliteration
3.28% ?? (Bush) ?? (shrub) The morph is the Chinese translation of ?bush?.
M5 Semantic Inter-
pretation
20.26% ??? (Kim
Il Sung)
??? (Kim Sun) The character ??? in the entity name means ??
? (Sun)?.
M6 Historical Fig-
ure Mapping
3.83% ??? (Bo
Xilai)
??? (Conquer
West King)
The entity shares characteristics and political ex-
periences similar to the morph.
M7 Characteristics
Modeling
20.62% ??? (Kim
Il Sung)
??? (Kim Fat) ??? (Fat)? describes ???? (Kim Il
Sung)??s appearance.
M8
Reputation and
public perception
26.09%
? ? ?
(Obama)
?? (Staring at
the sea)
Barack Obama received a calligraphy ????
? (Staring at sea and listening to surf)? as a
present when he visited China.
??? (Ma
Jingtao)
???? (Roar
Bishop)
In the films Ma Jingtao starred, he always used
exaggerated roaring to express various emotions.
??? (Ma
Yingjiu)
??? (Ma Se-
cession)
The morph derives from Ma Yingjiu?s political
position on cross-strait relations.
Table 2: Morph Examples Categorized based on Human Generation Methods
based on their semantic contexts. For example,
this approach generates a morph ??? (the First
Emperor)? for ???? (Mao Zedong)? who is the
first chairman of P. R. China and ??? (the Sec-
ond Emperor )? for ???? (Deng Xiaoping )?
who succeeded Mao.
2.8 M7: Characteristics Modeling
Finally, we propose a novel approach to auto-
matically generate an entity?s characteristics using
Google word2vec model (Mikolov et al, 2013).
To make the vocabulary model as general as pos-
sible, we use all of the following large corpora
that we have access to: Tsinghua Weibo dataset,
Chinese Gigaword fifth edition
4
which includes
10 million news documents, TAC-KBP 2009-2013
Source Corpora (McNamee and Dang, 2009; Ji et
4
http://catalog.ldc.upenn.edu/LDC2011T13
al., 2010; Ji et al, 2011; Ji and Grishman, 2011)
which include 3 million news and web documents,
and DARPA BOLT program?s discussion forum
corpora with 300k threads. Given an entity e, we
compute the semantic relationship between e and
each word from these corpora. We then rank the
words by: (1) cosine similarity, (2) the same cri-
teria as in section 2.6. Finally we append the top
ranking word to the entity?s last name to obtain
a new morph. Using this method, we are able
to generate many vivid morphs such as ?? ??
(Yao Wizard)? for ??? (Yao Ming)?.
3 Experiments
3.1 Data
We collected 1,553,347 tweets from Chinese Sina
Weibo from May 1 to June 30, 2013. We extracted
708
187 human created morphs based on M1-M7 for
55 person entities. Our approach generated 382
new morphs in total.
3.2 Human Evaluation
We randomly asked 9 Chinese native speakers
who regularly access Chinese social media and are
not involved in this work to conduct evaluation in-
dependently. We designed the following three cri-
teria based on Table 1:
? Perceivability: Who does this morph refer to?
(i) Pretty sure, (ii) Not sure, and (iii) No clues.
? Funniness: How interesting is the morph? (i)
Funny, (ii) Somewhat funny, and (iii) Not funny.
? Appropriateness: Does the morph describe the
target entity appropriately? (i) Make sense, (ii)
Make a little sense, and (iii) Make no sense.
The three choices of each criteria account for
100% (i), 50% (ii) and 0% (iii) satisfaction rate,
respectively. If the assessor correctly predicts the
target entity with the Perceivability measure, (s)he
is asked to continue to answer the Funniness and
Appropriateness questions; otherwise the Funni-
ness and Appropriateness scores are 0. The hu-
man evaluation results are shown in Table 4. The
Fleiss?s kappa coefficient among all the human as-
sessors is 0.147 indicating slight agreement.
From Table 4 we can see that overall the sys-
tem achieves 66% of the human performance
with comparable stability as human. In partic-
ular, Method 4 based on translation and translit-
eration generates much more perceivable morphs
than human because the system may search in a
larger vocabulary. Interestingly, similar encour-
aging results - system outperforms human - have
been observed by previous back-transliteration
work (Knight and Graehl, 1998).
It?s also interesting to see that human assessors
can only comprehend 76% of the human generated
morphs because of the following reasons: (1) the
morph is newly generated or it does not describe
the characteristics of the target entity well; and (2)
the target entity itself is not well known to human
assessors who do not keep close track of news top-
ics. In fact only 64 human generated morphs and
72 system generated morphs are perceivable by all
human assessors.
For Method 2, the human created morphs are
assessed as much more and funny than the sys-
tem generated ones because human creators use
this approach only if: (1). the radicals still reflect
the meaning of the character (e.g., ?? (worry)?
is decomposed into two radicals ??? (heart au-
tumn)? instead of three ????? (grain fire heart)
because people tend to feel sad when the leaves
fall in the autumn), (2). the morph reflects some
characteristics of the entity (e.g., ???? (Jiang
Zemin)? has a morph ????? (Water Engi-
neer Zemin)? because he gave many instructions
on water conservancy construction); or (3). The
morph becomes very vivid and funny (e.g., the
morph ?????? (Muji Yue Yue Bird)? for
???? is assessed as very funny because ??
?(Muji)? looks like a Japanese name, ???(Yue
Yue)? can also refer to a famous chubby woman,
and ??? (bird man)? is a bad word referring to
bad people); or (4). The morph expresses strong
sentiment or sarcasm; or (5) The morph is the
name of another entity (e.g., the morph ???(Gu
Yue)? for ????(Hu Jintao)? is also the name
of a famous actor who often acts as Mao Zedong).
The automatic approach didn?t explore these intel-
ligent constraints and thus produced more boring
morph. Moreover, sometimes human creators fur-
ther exploit traditional Chinese characters, gener-
alize or modify the decomposition results.
Table 3 presents some good (with average score
above 80%) and bad (with average score below
20%) examples.
Good Examples
Entity Morph Method
??? (Osama bin
Laden)
??? (The silly turn-
ing off light)
M1
??? (Chiang Kai-
shek)
???? (Grass Gen-
eral Jie Shi)
M2
???? (Bill Gates) ???? (Bill Gates) M4
Bad Examples
Entity Morph Method
?? (Kobe) ?? (Arm) M1
? ? ? ? ?
(Medvedev)
??? (Mei Virtue) M5
??? (Jeremy Lin) ?? (Lao Tze) M6
Table 3: System Generated Morph Examples
To understand whether users would adopt sys-
tem generated morphs for their social media com-
munication, we also ask the assessors to recite
the morphs that they remember after the survey.
Among all the morphs that they remember cor-
rectly, 20.4% are system generated morphs, which
is encouraging.
3.3 Automatic Evaluation
Another important goal of morph encoding is to
avoid censorship and freely communicate about
709
Human System Human System Human System Human System Human System Human System Human System Human System# of morphs 17 124 4 21 10 54 9 28 64 87 9 18 74 50 187 382Perceivability 75 76 95 86 94 81 61 71 87 59 66 5 77 34 76 67Funniness 78 49 92 43 44 41 70 47 70 35 74 28 79 44 76 46Appropriateness 71 51 89 59 81 43 75 49 76 36 78 18 82 38 79 43Average 75 59 92 57 73 55 69 56 78 43 73 17 79 39 77 52Standard Deviation 12.29 21.81 7.32 11.89 13.2 9.2 17.13 20.3 18.83 17.54 10.01 21.23 15.18 15.99 15.99 18.14
h s2568 58984214.3 29691742 45712641 1153922692 26766901.8 811317052 1278447812 1E+05255.7 329.12568 58984 214.3 2969 1742 4571 2641 11539 22692 26766 901.8 8113 17052 12784
M6 M7 OverallM1 M2 M3 M4 M5
Table 4: Human Evaluation Satisfaction Rate (%)
certain entities. To evaluate how well the new
morphs can pass censorship, we simulate the cen-
sorship using an automatic morph decoder con-
sisted of a morph candidate identification system
based on Support Vector Machines incorporating
anomaly analysis and our morph resolution sys-
tem (Huang et al, 2013). We use each system gen-
erated morph to replace its corresponding human-
created morphs in Weibo tweets and obtain a new
?morphed? data set. The morph decoder is then
applied to it. We define discovery rate as the per-
centage of morphs identified by the decoder, and
the ranking accuracy Acc@k to evaluate the reso-
lution performance. We conduct this decoding ex-
periment on 247 system generated and 151 human
generated perceivable morphs with perceivability
scores > 70% from human evaluation.
Figure 1 shows that in general the decoder
achieves lower discovery rate on system gener-
ated morphs than human generated ones, because
the identification component in the decoder was
trained based on human morph related features.
This result is promising because it demonstrates
that the system generated morphs contain new and
unique characteristics which are unknown to the
decoder. In contrast, from Figure 2 we can see
that system generated morphs can be more easily
resolved into the right target entities than human
generated ones which are more implicit.
0	 ?
20	 ?
40	 ?
60	 ?
80	 ?
100	 ?
M1	 ? M2	 ? M3	 ? M4	 ? M5	 ? M6	 ? M7	 ? ALL	 ?
Human	 ?created	 ?	 ?morph	 ? System	 ?generated	 ?morph	 ?
Figure 1: Discovery Rate (%)
4 Related Work
Some recent work attempted to map between Chi-
nese formal words and informal words (Xia et al,
2005; Xia and Wong, 2006; Xia et al, 2006; Li
Figure 2: Resolution Acc@K Accuracy (%)
and Yarowsky, 2008; Wang et al, 2013; Wang and
Kan, 2013). We incorporated the pronunciation,
lexical and semantic similarity measurements pro-
posed in these approaches. Some of our basic se-
lection criteria are also similar to the constraints
used in previous work on generating humors (Val-
itutti et al, 2013; Petrovic and Matthews, 2013).
5 Conclusions and Future Work
This paper proposed a new problem of encoding
entity morphs and developed a wide variety of
novel automatic approaches. In the future we will
focus on improving the language-independent ap-
proaches based on historical figure mapping and
culture and reputation modeling. In addition, we
plan to extend our approaches to other types of in-
formation including sensitive events, satires and
metaphors so that we can generate fable stories.
We are also interested in tracking morphs over
time to study the evolution of Internet language.
Acknowledgments
This work was supported by U.S. ARL No.
W911NF-09-2-0053, DARPA No. FA8750-13-
2-0041 and No. W911NF-12-C-0028, ARO
No. W911NF-13-1-0193, NSF IIS-0953149,
CNS-0931975, IIS-1017362, IIS-1320617, IIS-
1354329, IBM, Google, DTRA, DHS and RPI.
The views and conclusions in this document are
those of the authors and should not be inter-
preted as representing the official policies, either
expressed or implied, of the U.S. Government.
The U.S. Government is authorized to reproduce
and distribute reprints for Government purposes
notwithstanding any copyright notation here on.
710
References
David Bamman, Brendan O?Connor, and Noah A.
Smith. 2012. Censorship and deletion practices in
Chinese social media. First Monday, 17(3).
Le Chen, Chi Zhang, and Christo Wilson. 2013.
Tweeting under pressure: analyzing trending topics
and evolving word choice on sina weibo. In Pro-
ceedings of the first ACM conference on Online so-
cial networks, pages 89?100.
Zhendong Dong and Qiang Dong. 1999. Hownet. In
http://www.keenage.com.
Hongzhao Huang, Zhen Wen, Dian Yu, Heng Ji,
Yizhou Sun, Jiawei Han, and He Li. 2013. Resolv-
ing entity morphs in censored data. In Proceedings
of the 51st Annual Meeting of the Association for
Computational Linguistics (ACL2013).
Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges.
In Proceedings of the Association for Computational
Linguistics (ACL2011).
Heng Ji, Ralph Grishman, Dayne Freitag, Matthias
Blume, John Wang, Shahram Khadivi, Richard
Zens, and Hermann Ney. 2009. Name extraction
and translation for distillation. Handbook of Natu-
ral Language Processing and Machine Translation:
DARPA Global Autonomous Language Exploitation.
Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2010. Overview of the tac 2010
knowledge base population track. In Text Analysis
Conference (TAC) 2010.
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011.
Overview of the tac 2011 knowledge base popula-
tion track. In Proc. Text Analysis Conference (TAC)
2011.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
Zhifei Li and David Yarowsky. 2008. Mining and
modeling relations between formal and informal chi-
nese phrases from web corpora. In Proceedings
of Conference on Empirical Methods in Natural
Language Processing (EMNLP2008), pages 1031?
1040.
Jianyu Li and Jie Zhou. 2007. Chinese character struc-
ture analysis based on complex networks. Phys-
ica A: Statistical Mechanics and its Applications,
380:629?638.
Paul McNamee and Hoa Trang Dang. 2009.
Overview of the tac 2009 knowledge base popula-
tion track. In Proceedings of Text Analysis Confer-
ence (TAC2009).
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their composition-
ality. In C.J.C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K.Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111?3119.
Sasa Petrovic and David Matthews. 2013. Unsuper-
vised joke generation from big data. In Proceed-
ings of the Association for Computational Linguis-
tics (ACL2013).
Alessandro Valitutti, Hannu Toivonen, Antoine
Doucet, and Jukka M. Toivanen. 2013. ?let every-
thing turn well in your wife?: Generation of adult
humor using lexical constraints. In Proceedings
of the Association for Computational Linguistics
(ACL2013).
Aobo Wang and Min-Yen Kan. 2013. Mining informal
language from chinese microtext: Joint word recog-
nition and segmentation. In Proceedings of the As-
sociation for Computational Linguistics (ACL2013).
Aobo Wang, Min-Yen Kan, Daniel Andrade, Takashi
Onishi, and Kai Ishikawa. 2013. Chinese informal
word normalization: an experimental study. In Pro-
ceedings of International Joint Conference on Natu-
ral Language Processing (IJCNLP2013).
Yunqing Xia and Kam-Fai Wong. 2006. Anomaly de-
tecting within dynamic chinese chat text. In Proc.
Workshop On New Text Wikis And Blogs And Other
Dynamic Text Sources.
Yunqing Xia, Kam-Fai Wong, and Wei Gao. 2005. Nil
is not nothing: Recognition of chinese network in-
formal language expressions. In 4th SIGHAN Work-
shop on Chinese Language Processing at IJCNLP,
volume 5.
Yunqing Xia, Kam-Fai Wong, and Wenjie Li. 2006.
A phonetic-based approach to chinese chat text nor-
malization. In Proceedings of COLING-ACL2006,
pages 993?1000.
Richard Zens and Hermann Ney. 2004. Improvements
in phrase-based statistical machine translation. In
Proceedings of HLT-NAACL2004.
Jing Zhang, Biao Liu, Jie Tang, Ting Chen, and Juanzi
Li. 2013. Social influence locality for modeling
retweeting behaviors. In Proceedings of the 23rd
International Joint Conference on Artificial Intelli-
gence (IJCAI?13), pages 2761?2767.
711
