Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1408?1417,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Cross-Cultural Analysis of Blogs and Forums
with Mixed-Collection Topic Models
Michael Paul and Roxana Girju
University of Illinois at Urbana Champaign
Urbana, IL 61801
{mjpaul2, girju}@illinois.edu
Abstract
This paper presents preliminary results on
the detection of cultural differences from
people?s experiences in various countries
from two perspectives: tourists and lo-
cals. Our approach is to develop proba-
bilistic models that would provide a good
framework for such studies. Thus, we pro-
pose here a new model, ccLDA, which
extends over the Latent Dirichlet Alloca-
tion (LDA) (Blei et al, 2003) and cross-
collection mixture (ccMix) (Zhai et al,
2004) models on blogs and forums. We
also provide a qualitative and quantitative
analysis of the model on the cross-cultural
data.
1 Introduction
In today?s society, people from different cultural
backgrounds have to understand each other, inter-
act on a daily base and travel to or work in more
than one country. Understanding cultural diver-
sity, as well as addressing the need to communi-
cate effectively across cultural divides, have be-
come imperative in almost every aspect of life.
These constitute an important language aspect
since the lack of such cultural awareness can lead
to misinterpretations.
This paper presents preliminary results on the
detection of cultural differences from people?s ex-
periences in various countries from two perspec-
tives: tourists and locals. Since the advent of Web
2.0, user-generated data in the form of blogs and
newsgroup messages have reached high propor-
tions. In this paper we take advantage of such
resources of blogs and forums to perform various
cross-cultural analyses.
Our approach is to develop probabilistic mod-
els that would provide a good framework for such
studies. Thus, we propose here a new model,
ccLDA, which extends over the Latent Dirichlet
Allocation (LDA) (Blei et al, 2003) and cross-
collection mixture (ccMix) (Zhai et al, 2004)
models. Our contribution is as follows:
(1) Unsupervised topic models such as LDA are
elegant and flexible approaches to clustering large
collections of unannotated data. These models,
however, have conceptually focused on one single
collection of text which is inadequate for compar-
ative analyses of text.
We thus develop an LDA-based model that can
not only discover topics but also model their simi-
larities and differences across multiple text collec-
tions.
(2) We improve on similar previous work by craft-
ing a model that can better generalize data and is
less reliant on user-defined parameters.
(3) We apply our new model on blogs and forums
to identify cross-cultural differences.
Thus, different models can be compared to re-
flect different hypotheses about the data.
The paper is organized as follows. In Section
2 we summarize relevant previous work and give
a detailed description of the model in Section 3.
Section 4 details the model?s parameter estima-
tion. Experimental results are presented in Sec-
tion 5, followed by discussion, future work, and
conclusions.
2 Previous Work
A topic model for comparing text collections
(ccMix) was previously introduced by Zhai et
al. (2004) for a problem called comparative text
mining (CTM). Given news articles from differ-
ent sources (about the same event), ccMix can ex-
tract what is common to all the sources and what
is unique to one specific source.
Our model improves over ccMix by replacing
their probabilistic latent semantic indexing (pLSI)
(Hofmann, 1999) framework with that of LDA.
1408
Under the ccMix model, the probability of gen-
erating the ith word in a document belonging to
collection c is:
P (w
i
) = (1? ?
B
)
?
z?Z
P (z)(?
C
P (w
i
|z) +
(1? ?
C
)P (w
i
|z, c)) + ?
B
P (w
i
|B),
where each topic is denoted z. ?
B
is the prob-
ability of choosing a word from the background
word distribution and is user-defined. ?
C
is also
defined by the user and is the probability of draw-
ing a word from the collection-independent word
distribution instead of the collection-specific dis-
tribution. The parameters can be estimated using
the Expectation-Maximization algorithm (Demp-
ster et al, 1977).
However, in addition to the advantages of LDA
over pLSI such as the incorporation of Dirichlet
priors and a natural way to deal with new docu-
ments, our model avoids the limitations of using a
single user-defined parameter ?
C
? this probabil-
ity is learned automatically under our model. Fur-
thermore, we allow this probability to depend on
the collection and topic, which is a less restrictive
assumption.
Our model, ccLDA, shares with the LDA-
Collocation (Griffiths et al, 2007) and Topical N-
Grams (Wang et al, 2007) models the assump-
tion that each word can come from two different
word distributions, one of which depends on an-
other observable variable. In these models, a word
can come from either its topic?s word distribution,
or it can come from a word distribution associated
with the previous word, in the case that the word
is determined to be part of a collocation. The key
difference here is that in these models, the alter-
native word distribution depends on the word pre-
ceding a token, while in ccLDA, this depends on
the document?s collection.
The model is also related to hierarchical
variants of LDA, in particular the hierarchical
Pachinko allocation (hPAM) (Mimno et al, 2007)
model, in which both a topic and hierarchy depth
are chosen, and there is a different word dis-
tribution at different levels in the hierarchy. A
natural way to view our model is as a two-
level hierarchy where the top level represents the
collection-independent distributions and the bot-
tom level represents the collection-specific distri-
butions. One of the main differences here is that
the discovered hierarchies in hPAM can be arbi-
trary, whereas the graphical structure of our model
is pre-determined such that each topic has exactly
one ?sub-topic? representing each collection.
Wang et al recently introduced Markov topic
models (MTM) (2009), a family of models which
can simultaneously learn the topic structure of a
single collection while discovering correlated top-
ics in other collections. This is promising in that
this type of model makes no assentation that each
topic is in some way shared across all collections.
However, it does not explicitly model the similar-
ities and differences between collections as we do
in this research.
In computational linguistics, topic models have
been used in various applications, such as predict-
ing response to political webposts (Yano et al,
2009), analyzing Enron and academic emails (Mc-
Callum et al, 2007a), analyzing voting records
and corresponding text of resolutions from the
U.S. Senate and the U.N. (McCallum et al,
2007b), as well as studying the history of ideas
in various research fields (Hall et al, 2008; Paul
and Girju, 2009). To our knowledge, the applica-
tion of topic models to identifying cross-cultural
differences is novel.
3 The Model
In this section we first review the basic pLSI and
LDA models. We then introduce our extension to
LDA: cross-collection LDA (ccLDA).
3.1 Basic Topic Modeling
The most basic generative model that assumes
document topicality is the standard Na??ve Bayes
model, where each document is assumed to be-
long to exactly one topic, and each topic is asso-
ciated with a probability distribution over words
(Mitchell, 1997).
While this single-topic approach can be suffi-
cient for classification tasks, it is often too limiting
for unsupervised grouping of semantically related
words into topics. A better assumption is that each
document is a mixture of topics. For example, a
news article about a natural disaster may include
topics about the causes of such disasters, the dam-
age/death toll, and relief aid/efforts. Probabilistic
latent semantic indexing (pLSI) (Hofmann, 1999)
is one such model. Under this model, the proba-
bility of seeing the ith word in a document is:
P (w
i
|d) =
?
z?Z
P (w
i
|z)P (z|d)
1409
One of the main criticisms of pLSI is that each
document is represented as a variable d and it is
not clear how to label previously unseen docu-
ments. This issue is addressed by Blei et al with
latent Dirichlet alocation (2003). Furthermore,
the probabilities under this model have Dirichlet
priors, which results in more reasonable mixtures
and less overfitting. In LDA, a document is gener-
ated as follows:
1) Draw a multinomial distribution of words ?
z
from Dirichlet(?) for each topic z
2) For each document d
1
, draw a topic mixture
distribution ?
(d)
from Dirichlet(?). Then for each
word w
i
in d:
a) Sample a topic z
i
from ?
(d)
b) Sample a word w
i
from ?
z
The Dirichlet parameters ? and ? are vectors
which represent the average of the respective dis-
tributions. In many applications, it is sufficient
to assume that these vectors are uniform and to
fix them at a value pre-defined by the user. In
this case, the Dirichlet priors simply function as
smoothing factors.
3.2 Cross-Collection LDA
In this subsection we introduce our extension
of LDA for comparing multiple text collec-
tions, which we refer to as cross-collection LDA
(ccLDA). Under this model, each topic is as-
sociated with two classes of word distributions:
one that is shared among all collections, and one
that is unique to the collection from which the
document comes. For example, when modeling
reviews of different laptops, the topic describ-
ing the preloaded software contains the words
?software?, ?application?, ?programs?, etc. in
its shared distribution with high probability, and
the Apple-specific word distribution contains the
words ?itunes?, ?appleworks?, and ?iphoto?.
When generating a document under this model,
one first samples a collection c (which is ob-
servable in the data), then chooses a topic z and
flips a coin x to determine whether to draw from
the shared topic-word distribution or the topic?s
collection-specific distribution. The probability of
x being 1 or 0 comes from a Beta distribution (the
bivariate analog of the Dirichlet distribution) and
1
One should also assume that a document length is sam-
pled from an arbitrary distribution, but this does not affect the
derivation of the model, so we ignore this here and elsewhere.
is dependent on the collection and topic of the cur-
rent token.
Figure 1: Graphical representation of ccLDA. C is the
number of collections, T is the number of topics, D is the
number of documents, and N is the length of each document.
The generative process is thus:
1) Draw a collection-independent multinomial
word distribution ?
z
from Dirichlet(?) for each
topic z
2) Draw a collection-specific multinomial word
distribution ?
z,c
from Dirichlet(?) for each topic
z and each collection c
3) Draw a Bernoulli distribution ?
z,c
from
Beta(?
0
, ?
1
) for each topic z and each collection
c
4) For each document d, choose a collection c and
draw a topic mixture ?
(d)
fromDirichlet(?
c
). Then
for each word w
i
in d:
a) Sample a topic z
i
from ?
(d)
b) Sample x
i
from ?
z,c
c) If x
i
= 0, sample a word w
i
from ?
z
;
else if x
i
= 1, sample w
i
from ?
z,c
As mentioned in section 2, this model is in
some respects an LDA-based analog of the Zhai
et al (2004) model (ccMix), and thus it offers the
same improvements that LDA offers over pLSI
(described in the previous subsection), but there
are some other differences. An obvious structural
difference between the models is that ccMix has
a special topic for background words, whereas we
simply address this by removing stop words dur-
ing preprocessing, which seems to give reasonable
performance in this respect. This could easily be
incorporated into our model such that x can take a
1410
third value that designates that a word comes from
the background, but removing stop words hugely
reduces the number of tokens in the data, and thus
very significantly improves the time needed to es-
timate the model.
In the ccMix model, the probability that a
word comes from the collection-specific distribu-
tion versus the shared distribution depends on a
single user-defined parameter ?
C
. Since it is not
clear how to set this parameter
2
, in our model, we
learn this probability automatically. Furthermore,
the nature of the ?
C
parameter is quite restrictive
in that it is the same regardless of the topic and
collection. In our model, this probability depends
on the collection and topic, which should allow for
a more accurate fitting of the data, as some topics
may be shared across the collections to a different
degree than others.
Additionally, our model allows the topic dis-
tributions for each document to come from non-
uniform Dirichlet priors (parameterized by the
vector ?
c
) that depends on the document?s collec-
tion. Because the learned Dirichlet parameters can
be interpreted as the average mixing level of each
topic in the different collections, we can easily de-
termine if a topic is not shared among all collec-
tions, and thus we can automatically remove or set
aside such topics.
4 Parameter Estimation
Exact inference is often intractable in complex
Bayesian models and approximate methods must
be used. Blei et al (2003) offer a variational EM
algorithm for LDA. Griffiths and Steyvers (2004)
show how Gibbs sampling can be used for approx-
imate inference in LDA. Gibbs sampling is a type
of Markov chain Monte Carlo algorithm and is
what we employ in this paper, as it is simple to
derive, comparable in speed to other estimators,
and it approximates a global maximum (whereas
EM algorithms may only converge to a local max-
imum).
In a Gibbs sampler, one iteratively samples new
assignments of hidden variables by drawing from
the distributions conditioned on the previous state
of the model (Gilks et al, 1995). In each Gibbs
sampling iteration we alternately sample new as-
signments of z and xwith the following equations:
2
If needed, one can effectively set this probability manu-
ally in ccLDA as well by using a large prior.
P (z
i
|x
i
= 0, z
?i
,w, ?, ?) ? (n
d
z
i
+ ?
cz
)?
n
z
i
w
i
+ ?
n
z
i
.
+ W?
(1)
P (z
i
|x
i
= 1, z
?i
,w, ?, ?) ? (n
d
z
i
+ ?
cz
)?
n
z
i
,c
w
i
+ ?
n
z
i
,c
.
+ W?
(2)
P (x
i
= 0|x
?i
, z,w, ?, ?) ?
n
z,c
x=0
+ ?
0
n
z,c
.
+ ?
0
+ ?
1
?
n
z
i
w
i
+ ?
n
z
i
.
+ W?
(3)
P (x
i
= 1|x
?i
, z,w, ?, ?) ?
n
z,c
x=1
+ ?
1
n
z,c
.
+ ?
0
+ ?
1
?
n
z
i
,c
w
i
+ ?
n
z
i
,c
.
+ W?
(4)
Because of the conjugacy of the Beta/Dirichlet
and binomial/multinomial distributions, we can
integrate out ?, ?, ? and ? to obtain these equa-
tions, a technique known as ?collapsed? Gibbs
sampling (Heinrich, 2008).
n
b
a
denotes the number of times a has been
assigned to b, excluding the assignment of the
current token i. W is the size of the vocabu-
lary. x should be initialized as 0 for all tokens;
that is, we initially assume that everything comes
from the shared word distributions, otherwise the
collection-specific word distributions will form in-
dependently.
?
c
is a non-uniform vector that is collection-
specific. A simple and efficient way to approxi-
mate this is through moment-matching such that
?
cz
?
1
N
c
?
d
n
d
z
n
d
.
, where d belongs to collection c
andN
c
is the number of documents in c (details in
(Minka, 2003); (Li and McCallum, 2006)). The
other hyperparameters can be updated similarly,
although in our research we simply keep that at
fixed, uniform values, as they do not largely affect
the sampling procedure at small values.
5 Experimental Results
Our experiments focus on discovering cultural dif-
ferences by running our model on text from or
about three countries: the UK, India, and Singa-
pore. We explore the notion of perspective by ex-
perimenting with datasets with two distinctly dif-
ferent perspectives: one in which the text is about
each country (tourists), and one in which the text
is authored by residents of each country (locals).
5.1 The Data
In our first experiment, we model 3,266 discus-
sions from the forums at lonelyplanet.com, the
largest blog website for travelers with a forum for
nearly every potential travel destination. We show
how this can be used for comparative content ag-
gregation and summarization, and we show how
1411
our model improves upon previous work on such
datasets. In the second experiment, we compare
by authorship (blogs written by locals), and we
run our model on 7,388 English-language weblogs
from the same set of three different countries
3
. We
show how this is a solid step toward automatic dis-
covery of cultural differences.
Moreover, we compare the two perspectives
on the topic of food. We show that there are
some strong similarities between the topic in each
dataset (thereby enforcing our inferences from
each experiment individually), but we also show
some differences in the foods tourists find inter-
esting and what locals actually eat.
In all of our experiments, we ran the Gibbs sam-
pler for a burn-in period of 3000 iterations, then
we collected and averaged 15 samples, each sep-
arated by a 100-iteration lag. We used ? = ? =
0.01 and ?
0
= ?
1
= 1.0.
Our implementation is loosely based on the
LDA Gibbs sampler
4
by Phan and Nguyen (2008).
5.2 Analysis Along the Tourists Dimension
In the first experiment we consider data about
three destination countries. Using the data pro-
vided by lonelyplanet.com, we crawled 1,108
threads from the UK forum, 1,112 from the India
forum, and 1,046 from the Singapore forum. Mes-
sages are predominantly written by people who
have traveled or plan to travel to that country.
Since we are not interested in the thread discus-
sions on a particular travel topic, we treated each
thread or discussion of multiple messages as a sin-
gle document. We were able to use simple pat-
tern matching to extract only the discussion text.
We removed HTML tags, stop words, and words
with a corpus frequency less than 10. There were
703,551 tokens after preprocessing.
Wemodeled this dataset with 25 topics. General
topical words were grouped into the shared word
distribution of each topic, but each collection-
specific distribution contained words in the topic
that best describe that country. For example, the
topic on weather is characterized by words like
weather, rain and snow, but each collection?s dis-
tribution might give one a sense of the weather in
each country. Table 1 shows that travelers in In-
dia, for example, should be aware of monsoon sea-
son, and travelers to Singapore can expect to be
3
The dataset is available for download at
http://apfel.ai.uiuc.edu/resources.html
4
http://gibbslda.sourceforge.net
weather time day going rain
summer month high days thanks
UK India Singapore
wind leh hot
waterproof monsoon humid
ending road humidity
rolling manali heat
walkers ladakh degree
rochdale trekking equator
layers trek sweat
snow season bring
footwear rains rain
ankle monsoons umbrella
Table 1: The topic of weather, modeled across travel forums
for three different countries.
hot and sweaty. The UK distribution suggests that
campers should prepare for potentially hazardous
weather with the appropriate clothing and gear.
As another example, let?s consider the topic
whose shared words are english, school, language,
and speak. The results show that English is com-
mon to all three, but the collection-specific word
distributions indicate that Irish language is found
in the UK region, Hindi is common in India, and
Mandarin is common in Singapore.
Other common topics include immigration re-
quirements, monetary issues, air and rail travel,
etc., all containing information specific to each
country. This could be used for automatic sum-
marization by topic which would be useful either
to travelers who are visiting multiple destinations,
or for a potential traveler in the process of choos-
ing where to go. Someone interested in shopping
for music should go to the UK while someone in-
terested in electronics should go to Singapore, for
example (at least according to one of the topics
discovered).
5.3 Analysis Along the Locals Dimension
The results of the first experiment offer an unsu-
pervised aggregation of factual information that is
important to travelers such as a destination?s cli-
mate, law, and infrastructure; however, the data
did not offer much in terms of cultural informa-
tion. We would now like to see if we can get bet-
ter insight into this problem by modeling text au-
thored by residents of these same countries. In do-
ing this we can compare what they talk about and
in what manner they talk about certain topics.
For this experiment we downloaded 2,715 blogs
from the UK, 2,630 blogs from India, and 2,043
blogs from Singapore. We found these English-
language blogs through blogcatalog.com, a blog
1412
directory which lists a blog?s language and coun-
try of origin. We downloaded only the front page
of each blog, which usually included multiple ar-
ticles or postings.
We removed HTML tags from the documents,
but we made no attempt to segment the documents
into article text ? there are efficient methods of
doing this (Pasternack and Roth, 2009) and this
may be worth experimenting with, but we found
that noise such as navigation menus and advertise-
ments would mostly get grouped into their own
topics. We removed stop words and words with
a corpus frequency less than 20. All punctua-
tion was treated as word separators. There were
8,599,751 tokens in the end.
Table 2 shows 3 topics induced from modeling
this data with 50 topics. By looking at these we
can see some clear differences between the three
groups of native bloggers. For example, Topic 1 is
about fashion, and we can compare which fashions
are popular in each country. Shoes are popular in
the UK; leather and jewelry are more popular in
India. Singapore bloggers seem to focus on prices
and the shopping aspect of apparel.
From Topic 2 (about pets) it seems that Britons
slightly prefer dogs and Singaporians slightly pre-
fer cats. In general, it seems that Singaporians
have an affinity for small animals, considering the
presence of hamster and rabbit in their word dis-
tribution.
Topic 3 is about religion, in which we see that
Christianity is common to all of them, but Hin-
duism is prominent in India as well.
There are many topics not shown here includ-
ing politics, gardening, health, etc. The health
topic is interesting in that homeopathy and herbal
medicines are discussed in Indian blogs. Smoking
is a bigger topic in the UK than the others.
It is also interesting to compare what technolo-
gies and web services people use. Twitter and
Facebook are popular in the UK whereas Orkut
is more popular in India. Blogging services like
Wordpress are popular in Singapore.
From the travel topic, shown in Table 5, we
see that people travel close to home, so to speak.
Britons travel around Europe, especially Spain,
Paris and London, while Singaporians travel to
popular destinations in that part of the world, such
as Hong Kong, Thailand and Bali.
5.4 Differences in Perspective: Tourists vs.
Locals
Having modeled the same countries from two dif-
ferent perspectives (that of travelers and that of
locals), it would be interesting to see how topics
compare between the two perspectives.
Do people have the same view of themselves as
outsiders see them? Are locals interested in the
same things as tourists?
We hope to answer these questions by examin-
ing related topics within these two datasets. While
the two datasets consist of mostly different top-
ics, there are a few that would be interesting to
compare. In particular, we examine the topic of
food and eating. The top words from this topic are
shown in Table 3.
We first examine this topic from the blog data
(that is, from the perspective of residents). By
looking at each collection-specific word distribu-
tion we can see which foods are more popular in
each country ? cheese and soup in the UK, curry in
India, and seafood in Singapore. We also noticed
that tea and coffee are more popular in Singapore,
wine and beer are more popular in the UK, while
in Indian blogs beverages are not commonly men-
tioned. Perhaps a less trivial observation is that
the words restaurant and chef are frequent in UK
blogs, but the Indian word distribution is domi-
nated by words pertaining to recipes. From this
one might infer that people in the UK (and to a
lesser extent in Singapore) eat out more often than
people in India, who do more home cooking.
Looking now at the topics induced from the
lonelyplanet.com forums (that is, from the per-
spective of travelers), we see some interesting sim-
ilarities. Most notably, the Indian distribution
again consists of words related to cooking, af-
firming our observation that dining out is not as
popular in India. The Singapore distribution also
matches that in the other dataset ? the common
words include seafood and noodles. The UK dis-
tribution, however, shows that tourists are mostly
interested in local specialties (such as fish and
chips and haggis).
To see where these perspectives on food differ
the most, we computed the ratio of the probabil-
ity of each word given the topic between the two
datasets. That is, if p = P (w|z) in the locals
data and q = P (w|z) in the tourists data, then
? = p/q gives us a measure of how much more (or
less) prominent that word is among locals than it
1413
Topic 1 Topic 2 Topic 3
fashion style look dress wear dog dogs pet animals animal god jesus lord life faith
new collection accessories black comments cat like food plant holy man christ church love
UK India Singapore UK India Singapore UK India Singapore
shoes fashion price garden water cat church krishna god
fashion women posted dog energy cats god religion sin
clothing indian earrings pet carbon dog john religious john
high designer length cat earth pet todd spiritual spirit
designer sarees item dogs green training bentley guru things
style leather sgd pets solar pets jesus lord lamb
love girls silver gardening jai hamster christ sri exodus
london china clothes cats climate cute luke shri suffering
shirts jewellery shop puppy environment hamsters bible baba cross
bag jewelry code flowers warming rabbit christian hindu lives
Table 2: A sample of topics induced on a set of blogs from 3 countries. Shown are the top 10 words from the shared
topic-word distribution P (word|x = 0, topic) and the top 10 words from P (word|x = 1, topic, class) for each collection.
Perspective of Locals Perspective of Tourists
food add chicken recipe cooking food eat restaurant restaurants tea
taste rice recipes sugar soup cheap meal eating cafe drink
UK India Singapore UK India Singapore
food recipe coffee chips cooking hawker
a
wine recipes cup haggis spices satay
restaurant powder oil fish sick stalls
coffee indian comments respectability flour noodles
cheese salt fried decent tomato roti
soup tsp add veggie batter stall
eat rice restaurant pudding ate seafood
chef masala rice photoblog cook malay
english oil tea sausages olive rochester
drink coriander seafood sandwiches recipe noodle
a
A hawker centre is an open-air complex with many
food stalls, commonly found in Singapore and Malaysia.
Table 3: A comparison of the food topic from two different
datasets, one of which comes from a travel forum and the
other of which consists of blogs authored by residents of each
respective country.
is among tourists in the food topic. Table 4 shows
the words with the highest (left) and lowest (right)
values of ?.
Preferred by Locals Preferred by Tourists
recipe bowl lemon tomato simple street cheap couple yeah crowd
spring spoon vanilla stir pour old road floor run locals
UK India Singapore UK India Singapore
food indian cup pubs mother quay
healthy recipes comments music ate coast
shop cup tea lane tree parkway
favorite chicken mins brick party reasonably
wine minutes pot fish fields air
icing kitchen note jazz base sultan
coffee mustard nice pints rock tum
leeds fried salt dancing toilet views
duck ginger tarts arms bottled plenty
extra salt fish recommend olive rochester
Table 4: This table shows words in the food topic that are
more popular in the tourists data than the locals data or vice
versa.
The prominent trend, which is largely a logis-
tical matter, is that travelers are more interested
in restaurants and locals talk more about cooking.
Most of the words that are more prominent from
the tourist perspective have to do with eating loca-
tions. We also noticed that wine and coffee rank
more prominently among the locals, whereas trav-
elers are more likely to ask about beer and liquor.
5.5 Model Evaluation
In this subsection we evaluate ccLDA against
ccMix and LDA both qualitatively, through blind
judgments of cluster quality, and quantitatively,
by measuring the likelihood of held-out data with
each model.
5.5.1 Cluster Coherence
Because our research relies on analyses of discov-
ered topics, it is important that we use a model that
gives the best empirical quality of word clusters.
We compare against ccMix (Zhai et al, 2004),
the only related model that is naturally suited to
our task. Using blind human judgments we show
that ccLDA unquestionably delivers topics that are
more coherent than those obtained with the ccMix
model.
A direct comparison with ccMix is tricky be-
cause it incorporates a model for background
words, whereas our model expects stop words to
be removed during preprocessing. So that they
are fully comparable, we set the parameter ?
B
(the probability that a word comes from the back-
ground) to 0 and fed the model the same input as
we did ccLDA. We set the parameter ?
C
, analo-
gous to P (x = 0), to 0.6, which is the average
value learned by ccLDA on this data, and it seems
quite reasonable. Using an implementation pro-
vided by the authors of ccMix, we ran the EM pro-
cedure for 20 trials and saved the model with the
best log-likelihood.
We performed human judgments of the 25 top-
ics induced by ccLDA in the first experiment
1414
above and by the ccMix model with the number of
topics set again to 25. We aligned the topics auto-
matically using a symmetric KL-divergence score
computed on the collection-independent distribu-
tions ? specifically, D(P ||Q) + D(Q||P ) where
D(P ||Q) is the KL-divergence
5
of the distribu-
tions P and Q.
Each aligned pair of topics (ordered randomly
for each topic to avoid bias) was presented to two
natural language processing researchers who were
asked to choose which one was better, based on the
following criteria: (1) semantic coherence of the
topic as a whole (e.g. are the words in the clusters
related?) and (2) coherence across collections, that
is, are the collection-specific distributions related
to each other and to the common one? The judges
were also given the option to rate a pair as ?no
opinion? in the case that the aligned topics were
too dissimilar to compare (because the two mod-
els did not discover the same topic), or that the
topics did not carry enough semantic information
to judge (i.e. topics composed mostly of function
words).
Of the 25 pairs, there were 10 that both judges
rated. Of these 10, the judges disagreed on 3. The
other 7 were all rated in favor of ccLDA.
Similarly, the 50 topics from the second exper-
iment were judged against 50 topics formed us-
ing ccMix. There were 22 topics that both judges
rated. Among these, they disagreed on only 3; of
the remaining topics they voted in favor of ccMix
for 1 topic and in favor of ccLDA for 18 topics.
It has been observed that the performance of a
model can largely depend on the estimator used
(Girolami and Kab?an, 2003), so it may be that the
weaker performance of ccMix is because the EM
algorithm is getting stuck in local maxima, even
after several trials.
Table 5 shows the topic of travel compared with
both ccMix and LDA. To compare against LDA,
we performed a post-hoc estimation of the topic?s
word distribution for each collection by consider-
ing topic assignments of documents within each
collection. We see that the ccLDA distributions
are much more coherent than that of ccMix. Fur-
thermore, the advantage over LDA is clear ? with
LDA, we do not get a separation of the words
that are common to all of the collections, and thus
it is hard to detect the important differences at a
5
Kullback-Leibler divergence is a commonly used mea-
surement of the similarity of two probability distributions.
glance.
5.5.2 Likelihood Comparison
To measure how well our model can generalize
unseen documents, we compute the likelihood of
held-out data using ccLDA compared with ccMix
and LDA. We partitioned the forum dataset from
the first experiment into a subset of 80% of the
data on which the models are learned, and an eval-
uation set of the remaining 20%.
To calculate the likelihood of the held-out doc-
uments with ccMix, we use the ?fold-in? method
(Hofmann, 1999) in which the mixing proportions
except for P (z|d) are fixed during the EM pro-
cess. As with our cluster evaluation above, we set
?
B
= 0 and ?
C
= 0.6. With LDA and ccLDA, we
approximate P (z|d) through another Gibbs sam-
pling procedure, by averaging 10 samples col-
lected after 100 iterations with a 10-iteration lag
in between each sample.
The log-likelihood of the three models is shown
at various numbers of topics in Figure 2. As ex-
pected, ccLDA generally achieves a higher like-
lihood than ccMix, although the difference be-
tween them diminishes at higher numbers of top-
ics. This appears to be because the pLSI-based
ccMix does not regularize the topic mixtures and
can thus achieve higher values of P (z|d), and the
smoothing of ccLDA has a greater effect at higher
numbers of topics.
Both cross-collection models achieve a higher
likelihood than LDA, which is not too surprising,
given that these models utilize extra information
(specifically, the document?s collection) to assign
a higher probability to words more likely to appear
in a document given that information.
It should be noted that even though the like-
lihood of both cross-collection models increases
with the number of topics up to 100, we observed
empirically that the best cluster quality in this
dataset occurs around 20 to 30 topics; more than
that results in clusters that are repeated and are
largely specific to only one collection.
6 Discussion and Future Work
While there are obvious limitations of the unigram
approach used here, our system was nevertheless
able to capture some interesting details. It is im-
portant, however, to point out some limitations for
possible future extensions.
Consider Topic 2 in Table 2. The UK and Singa-
pore word distributions are both clearly pertinent
1415
ccLDA ccMix LDA
travel hotel hotels city best travel hotel comments hotels city travel city hotel park holiday
place holiday visit trip world posted road trip labels airport hotels place beach road visit
UK India Singapore UK India Singapore UK India Singapore
holiday india singapore yang india yang travel travel travel
holidays delhi kong train delhi dan holiday city hotel
hotels indian hong london tourism ini hotel beach city
spain mumbai spa saya dubai dengan city place park
london bangalore hotel nie indian untuk london hotel place
great tour beach travel tour itu park temple beach
surf air chinese flight bangalore saya hotel road trip
breaks dubai pictures luxury mahindra orang place park hotels
train city restaurant dan hotels tidak holidays hotels spa
ski mahindra bangkok advert marathi dalam hall tourism visit
Table 5: The topic of travel as discovered by the 3 different models.
Figure 2: Comparison of the log-likelihood of held-out data
with the 3 models.
to the topic of pets, but the India distribution seems
entirely unrelated, being about energy and the en-
vironment. This could be because the environment
topic was statistically too strong to ignore, but not
found in other collections, so it made its way into
a largely unrelated topic. (In fact, the formation of
the environment cluster within this topic is not en-
tirely random, as the pets topic also includes some
words related to gardening, including ?water? and
?plant?, which are likely to also co-occur with en-
vironmental words.)
This is perhaps the main weakness of the model.
If an emerging topic is not shared among all col-
lections, it will either form as a primary topic
that is unique to only a subset of collections (and
thus some of the collection-specific distributions
will be noisy), or it will form as a collection-
specific distribution that is not strongly related
to the main collection-independent distribution.
This can make the results difficult to interpret,
although an automated solution would be to re-
move or flag topics that are not evenly shared,
which could be done by comparing the learned
collection-dependent Dirichlet parameters ?
c
.
This is also a matter of how the model performs
with different numbers of collections. It would
be interesting to see what results we would get
by modeling UK-India, UK-Singapore, and India-
Singapore as only a pair at a time. The perfor-
mance should not degrade with larger numbers of
collections if the collections are fully compara-
ble, but in practice, with more collections there
are likely to be more topics that are difficult to fit
across all collections.
In future work, we would like to enrich the
model and/or feature set to move beyond the lim-
itations of a bag-of-words analysis. For example,
by considering negation and word polarity, we can
better capture the opinions of the authors, which is
an important component of such cultural analysis.
Certainly, there are many other possible appli-
cations of this model, including product compar-
ison, media bias detection, and interdisciplinary
literature analysis. Cultural awareness is also im-
portant in marketing and we can use this model to
investigate, for example what products and what
aspects of life people in different regions focus on.
Acknowledgments
We would like to thank ChengXiang Zhai for
thoughtful discussions and for providing an imple-
mentation of the ccMix model. We would also like
to thank the reviewers for their constructive com-
ments.
1416
References
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research,
3.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. Journal of the Royal Statistical Soci-
ety. Series B (Methodological), 39(1):1?38.
W.R. Gilks, S. Richardson, and D.J. Spiegelhalter.
1995. Markov Chain Monte Carlo in Practice. CRC
Press.
M. Girolami and A. Kab?an. 2003. On an equivalence
between plsi and lda. In SIGIR ?03: Proceedings
of the 26th annual international ACM SIGIR confer-
ence on Research and development in informaion re-
trieval, pages 433?434, New York, NY, USA. ACM.
T. Griffiths and M. Steyvers. 2004. Finding scientific
topics. In Proceedings of the National Academy of
Sciences of the United States of America.
Tl Griffiths, M. Steyvers, and Jb Tenenbaum. 2007.
Topics in semantic representation. Psychological
Review, 114(2):211?244.
D. Hall, D. Jurafsky, and C. Manning. 2008. Study-
ing the history of ideas using topic models. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
363?371.
G. Heinrich. 2008. Parameter estimation for text anal-
ysis. Technical report, University of Leipzig.
T. Hofmann. 1999. Probabilistic latent semantic in-
dexing. In SIGIR ?99: Proceedings of the 22nd an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 50?57, New York, NY, USA. ACM.
W. Li and A. McCallum. 2006. Pachinko allocation:
Dag-structured mixture models of topic correlations.
In International Conference on Machine Learning.
A. McCallum, X. Wang, and A. Corrada-Emmanuel.
2007a. Topic and role discovery in social networks
with experiments on enron and academic email.
Journal of Artificial Intelligence Research (JAIR),
30:249?272.
A. McCallum, X. Wang, and N. Mohanty. 2007b. Joint
group and topic discovery from relations and text.
In Statistical Network Analysis: Models, Issues and
New Directions - Lecture Notes in Computer Science
4503, pages 28?44.
D. Mimno, W. Li, and A. McCallum. 2007. Mixtures
of hierarchical topics with pachinko allocation. In
International Conference on Machine Learning.
T. Minka. 2003. Estimating a dirichlet distribution.
T. Mitchell. 1997. Machine Learning. McGraw-Hill,
Boston.
J. Pasternack and D. Roth. 2009. Extracting article text
from the web with maximum subsequence segmen-
tation. In The International World Wide Web Con-
ference, April.
M. Paul and R. Girju. 2009. Topic modeling of re-
search fields: An interdisciplinary perspective. In
Proceedings of the the International Conference on
Recent Advances in Natural Language Processing
(RANLP) (to appear).
X. Phan, L. Nguyen, and S. Horiguchi. 2008. Learning
to classify short and sparse text & web with hidden
topics from large-scale data collections. In WWW
?08: Proceeding of the 17th international conference
on World Wide Web, pages 91?100, New York, NY,
USA. ACM.
X. Wang, A. McCallum, and X. Wei. 2007. Top-
ical n-grams: Phrase and topic discovery, with an
application to information retrieval. In ICDM ?07:
Proceedings of the 2007 Seventh IEEE International
Conference on Data Mining, pages 697?702. IEEE
Computer Society.
C. Wang, B. Thiesson, C. Meek, and D. Blei. 2009.
Markov topic models. In The Twelfth International
Conference on Artificial Intelligence and Statistics
(AISTATS), pages 583?590.
T. Yano, W. Cohen, and N. Smith. 2009. Predicting
response to political blog posts with topic models.
In The 7th Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
C. Zhai, A. Velivelli, and B. Yu. 2004. A cross-
collection mixture model for comparative text min-
ing. In Proceedings of KDD 04, pages 743?748.
1417
Automatic Discovery of Part?Whole Relations
Roxana Girju?
University of Illinois at
Urbana-Champaign
Adriana Badulescu?
Language Computer Corporation
Dan Moldovan?
Language Computer Corporation
An important problem in knowledge discovery from text is the automatic extraction of semantic
relations. This paper presents a supervised, semantically intensive, domain independent ap-
proach for the automatic detection of part?whole relations in text. First an algorithm is described
that identifies lexico-syntactic patterns that encode part?whole relations. A difficulty is that these
patterns also encode other semantic relations, and a learning method is necessary to discriminate
whether or not a pattern contains a part?whole relation. A large set of training examples have
been annotated and fed into a specialized learning system that learns classification rules. The
rules are learned through an iterative semantic specialization (ISS) method applied to noun
phrase constituents. Classification rules have been generated this way for different patterns such
as genitives, noun compounds, and noun phrases containing prepositional phrases to extract
part?whole relations from them. The applicability of these rules has been tested on a test corpus
obtaining an overall average precision of 80.95% and recall of 75.91%. The results demonstrate
the importance of word sense disambiguation for this task. They also demonstrate that different
lexico-syntactic patterns encode different semantic information and should be treated separately
in the sense that different clarification rules apply to different patterns.
1. Introduction
The identification of semantic relations in text is at the core of Natural Language
Processing and many of its applications. Detecting semantic relations between various
text segments, such as phrases, sentences, and discourse spans, is important for auto-
matic text understanding (Rosario, Hearst, and Fillmore 2002; Lapata 2002; Morris and
Hirst 2004). Furthermore, semantic relations represent the core elements in the organi-
zation of lexical semantic knowledge bases intended for inference purposes. Recently,
there has been a renewed interest in text semantics as evidenced by the international
? Computer Science Department, University of Illinois at Urbana-Champaign, Urbana, IL 61801, E-mail:
girju@uiuc.edu.
? Language Computer Corporation, 1701 N. Collins Blvd. Suite 2000, Richardson, TX 75080, E-mail:
adriana@languagecomputer.com.
? Language Computer Corporation, 1701 N. Collins Blvd. Suite 2000, Richardson, TX 75080, E-mail:
moldovan@languagecomputer.com.
Submission received: 21 October 2003; revised submission received: 7 March 2005; accepted for
publication: 1 August 2005.
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 1
participation in the Senseval 3 Semantic Roles competition,1 the associated workshops,2
and numerous other workshops.
An important semantic relation for many applications is the part?whole relation, or
meronymy. Let us notate the part?whole relation as PART(X, Y), where X is part of Y. For
example, the compound nominal door knob contains the part?whole relation PART(knob,
door). Part?whole relations occur frequently in text and are expressed by a variety of
lexical constructions as illustrated in the text below.
(1) The car?s mail messenger is busy at work in the mail car as the train moves
along. Through the open side door of the car, moving scenery can be seen.
The worker is alarmed when he hears an unusual sound. He peeks
through the door?s keyhole leading to the tender and locomotive cab and sees
the two bandits trying to break through the express car door.3
There are several part?whole relations in this text: 1) the mail car is part of the train,
2) the side door is part of the car, 3) the keyhole is part of the door, 4) the cab is part of the
locomotive, 5) the tender is part of the train, 6) the locomotive is part of the train, 7) the door
is part of the car, and 8) the car is part of the express train (in the compound noun express
car door).
This paper provides a supervised, knowledge-intensive method for the automatic
detection of part?whole relations in English texts. Based on a set of positive (encoding
meronymy) and negative (not encoding meronymy) training examples provided and
annotated by us, the algorithm creates a decision tree and a set of rules that classify
new data. The rules produce semantic conditions that the noun constituents matched
by the patterns must satisfy in order to exhibit a part?whole relation. For the dis-
covery of classification rules we used C4.5 decision tree learning (Quinlan 1993). The
learned function is represented by a decision tree transformed into a set of if?then
rules. The decision tree learning searches a complete hypothesis space from simple to
complex hypotheses until it finds a hypothesis consistent with the data. Its bias is a
preference for the shorter tree that places high information gain attributes closer to the
root.
For training purposes we used WordNet, and the LA Times (TREC9)4 and SemCor
1.75 text collections. From these we formed a large corpus of 27,963 negative examples
and 29,134 positive examples of well distributed subtypes of part?whole relationships
which provided a comprehensive set of classification rules. The rules were tested on
1 http://www.senseval.org/senseval3.
2 The Computational Lexical Semantics Workshop at the 2004 Human Language Technology
(HLT/NAACL) conference; the first and second Workshops on Text Meaning and Interpretation at the
HLT/NAACL-03 and the 2004 Association for Computational Linguistics conference (ACL), respectively;
the first and second Workshops on Multiword Expressions at ACL 2003 and 2004; the ACL 2005
Workshop on Deep Lexical Acquisition.
3 This example is an excerpt from a review of the 1903 movie ?The Great Train Robbery?
(http://filmsite.org/grea.html).
4 TREC 9 is a text collection provided by NIST for the Question Answering competition (TREC-QA) at the
TExt Retrieval Conference in 2000. It contains 3 GBytes of news articles from the Wall Street Journal,
Financial Times, LA Times, Financial Report, AP Newswire, San Jose Mercury News, and Foreign
Broadcast Information Center from 1989 to 1994.
5 The SemCor collection (Miller et al, 1993) is a subset of the Brown Corpus and consists of 352 news
articles distributed into three sets in which the nouns, verbs, adverbs, and adjectives have been manually
tagged with their corresponding WordNet senses and part-of-speech tags using Brill?s tagger (1995).
84
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
two different text collections (LA Times and Wall Street Journal) obtaining an overall
average precision of 80.95% and recall of 75.91%.
In this paper we do not distinguish between situations when whole objects consist
of parts that are always present, or parts that are only sometimes present. For example,
it might be relatively easy to pin down the parts of a car (e.g., four wheels, one engine,
as ever present parts of a car irrespective of its type) as compared to enumerating all
the components of a sandwich (e.g., two layers of cheese and/or salami, two slices of bread,
that depend on the type of sandwich). In our experiments we focus only on part?whole
instances that are mentioned in the corpus employed and on those provided by general-
purpose lexical knowledge bases such as WordNet,6 whether the parts are just sometimes
constituents of the entity considered or are always present. We do not check for the
validity of these instances (e.g., whether the instance ?wood is part of a sandwich? is true
or not). Based on a large training corpus of positive and negative part?whole examples,
our system infers what type of objects are parts and wholes. Also, our system does not
take into consideration modality information such as knowledge about the possibility,
certainty, or probability of existence of part?whole relations.
The paper is organized as follows. Section 2 presents a summary of previous
work on meronymy from several perspectives. Section 3 gives a detailed classifica-
tion of the lexico-syntactic patterns used to express meronymy in English texts and
a procedure for finding these patterns. Section 4 describes a method for learning
semantic classification rules, while Section 5 shows the results obtained for discov-
ering the part?whole relations by applying the classification rules on two distinct
test corpora. Section 6 comments on the method?s limitations and extensions, and
Section 7 discusses the relevance of the task to NLP applications. Conclusions are
offered in Section 8.
2. Previous Work on Meronymy
Historically, part?whole or meronymy relations have played an important role in lin-
guistics, philosophy, and psychology mainly because a clear understanding of part?
whole relations requires a deep interaction of logic, semantics, and pragmatics as they
provide the tools needed for our understanding of the world. The part?whole rela-
tion has been considered a fundamental ontological relation since the atomists (Plato,
Aristotle, and the Scholastics). They were the first to give a systematic characterization
of parts and wholes, the relation between them, and the inheritance properties of this
relation. However, most of the investigations of part?whole relations have been made
since the beginning of the 20th century.
The logical/philosophical studies of meronymy were concerned with formal theo-
ries of parts (mereologies), wholes, and their relation in the context of formal ontology.
This school of thought advocates a single, universal, and transitive part-of relation used
for modeling various domains such as time and space.
Simons (1986) criticized this standard extensional view and proposed a more
adequate account that offers an axiomatic representation of the part-of relation as a
strict partial-ordering relation. The axioms considered were: existence (if A is a part of
B then both A and B exist), asymmetry (if A is a part of B then B is not a part of A),
supplementarity (if A is a part of B then B has a part C disjoint of A), and transitivity (if
6 For example, in WordNet 1.7 the only part listed for the concept sandwich is bread.
85
Computational Linguistics Volume 32, Number 1
A is a part of B and B is a part of C then A is a part of C). In 1991, Simons (1991) added
two more axioms: extensionality (objects with the same parts are identical) and existence
of mereological sum (for any number of objects there exists a whole that consists exactly
of those objects).
Linguistics and cognitive psychology researchers focused on different part?whole
relations and their role as semantic primitives. Since there are different ways in which
something can be expressed as part of something else, many researchers have claimed
that meronymy is a complex relation that ?should be treated as a collection of relations,
not as a single relation? (Iris, Litowitz, and Evens 1988).
Based on psycholinguistic experiments and the way in which the parts contribute
to the structure of the wholes, Winston, Chaffin, and Hermann (1987) determined
six types of part?whole relations: (1) COMPONENT?INTEGRAL OBJECT, (2) MEMBER?
COLLECTION, (3) PORTION?MASS, (4) STUFF?OBJECT, (5) FEATURE?ACTIVITY, and (6)
PLACE?AREA.
They also proposed three relation elements ( functional, homeomerous, and separable) to
further classify the six types of meronymy relations. The functional relational element
indicates that the part has a function with respect to its whole, whereas homeomerous
means that the part is identical to the other parts making up the whole. The separable
relational element shows that the part can be separated from the whole. For example,
the relation wheel?car is a COMPONENT?INTEGRAL part?whole relation that is functional,
non-homeomerous and separable. This means that the wheel has a specific function with
respect to the car, does not resemble the other parts of the car, and can be separated from
the car.
The COMPONENT?INTEGRAL relation is the relation between components and the
objects to which they belong. Integral objects have a structure, their components are sep-
arable and have a functional relation with their wholes. For example, kitchen?apartment
and aria?opera are COMPONENT?INTEGRAL relations.
The MEMBER?COLLECTION relation represents membership in a collection. Mem-
bers are parts, but they cannot be separated from their collections and do not play any
functional role with respect to their whole. For example, soldier?army, professor?faculty,
and tree?forest are MEMBER?COLLECTION relations.
PORTION?MASS captures the relations between portions and masses, extensive ob-
jects, and physical dimensions. The parts are separable and similar to each other and
to the wholes which they comprise, and do not play any functional role with respect to
their whole. For example, slice?pie and meter?kilometer are PORTION?MASS relations.
The STUFF?OBJECT category encodes the relations between an object and the stuff
of which it is partly or entirely made. The parts are not similar to the wholes that
they comprise, cannot be separated from the whole, and have no functional role. For
example, steel?car and alcohol?wine are STUFF?OBJECT relations.
The FEATURE?ACTIVITY relation captures the semantic links within features or
phases of various activities or processes. The parts have a functional role, but they are
not similar or separable from the whole. For example, paying?shopping and chewing?
eating are FEATURE?ACTIVITY relations.
PLACE?AREA captures the relation between areas and special places and locations
within them. The parts are similar to their wholes, but they are not separable from them.
For example, oasis?desert and Guadalupe Mountains National Park?Texas are PLACE?AREA
relations.
In this paper we use the Winston, Chaffin, and Hermann classification as a criterion
for building the training corpus to provide a wide coverage of such subtypes of part?
whole relations.
86
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
In computational linguistics, although a considerable amount of work has been
done on semantic relation detection,7 the work most similar to the task of identifying
part?whole semantic relations is that of Hearst (1992) and Berland and Charniak (1999).
Hearst developed a method for the automatic acquisition of hypernymy relations by
identifying a set of frequently used and mostly unambiguous lexico-syntactic patterns.
For example, countries, such as England indicates a hypernymy relation between the
words countries and England. In her paper, she mentions that she tried applying the
same method to meronymy, but without much success, as the patterns detected also
expressed other semantic relations. This is consistent with our study of part?whole
lexico-syntactic patterns presented in this paper.
In 1999, Berland and Charniak applied statistical methods to a very large corpus8
to find part?whole relations. Using Hearst?s method, they focused on a small set of
genitive patterns and a list of six seeds representing whole objects (book, building, car,
hospital, plant, and school). Their system?s output was an ordered list of possible parts
according to some statistical metrics (e.g., the log-likelihood metric (Dunning 1993)).
Although the training corpus used is very large, the coverage of the algorithm is small
due to the limited number of patterns used and the small number of wholes allowed.
Moreover, certain words, such as those ending in -ing, -ness, or -ity, were ruled out.
Their accuracy is 55% for the first 50 ranked parts and 70% for the first 20 ranked
parts. As a baseline, they considered as potential parts the head nouns immediately
surrounding the target whole object and ranked them based on the same statistical
metric. The baseline accuracy was 8%.
While Berland and Charniak?s method focuses solely on identifying parts given a
whole, our task targets the identification of both parts and wholes.
Hearst, and Berland and Charniak observed that for ambiguous whole words, such
as plant, the method produces the weakest part list of the six seeds considered. Although
they don?t provide a one-to-one comparison, Berland and Charniak mention that their
method outperforms Hearst?s pattern matching algorithm mainly due to the very large
corpus used. However, neither approach addresses the pattern ambiguity problem,
i.e., patterns such as genitives that can express different semantic relations in different
contexts (the dress of silk encodes a part?whole relation, but the dress of my girl does not).
The ambiguity of these patterns explains our rationale for choosing an approach based
on a machine learning method to discover discriminating rules automatically.
3. Lexico-Syntactic Patterns that Express Meronymy
The automatic discovery of any semantic relation must start with a thorough un-
derstanding of the lexical and syntactic forms used to express that relation. Since
there are many ways in which something can be part of something else, there is a
variety of lexico-syntactic structures that can express a meronymy semantic relation.
7 Besides the work on semantic roles (Charniak 2000; Gildea and Jurafsky 2002; Thompson, Levy, and
Manning 2003), considerable interest has been shown in the automatic interpretation of various noun
phrase-level constructions, such as noun compounds. The focus here is to determine the semantic
relations that link the two noun constituents. The best-performing noun compound interpretation
systems have employed either symbolic (Finin 1980; Vanderwende 1994) or statistical techniques
(Pustejovsky, Bergler, and Anick 1993; Lauer and Dras 1994; Lapata 2002) relying on rather ad hoc,
domain-specific, hand-coded semantic taxonomies, or on statistical patterns in a large corpus of
examples, respectively.
8 The North American News Corpus (NANC) of 1 million words.
87
Computational Linguistics Volume 32, Number 1
There are unambiguous lexical expressions that always convey a part?whole relation.
For example:
(2) The substance consists of three ingredients.
(3) The cloud was made of dust.
(4) Iceland is a member of NATO.
In these cases the simple detection of the patterns leads to the discovery of part?whole
relations.
On the other hand, there are many ambiguous expressions that are explicit but
convey part?whole relations only in some contexts. The detection of meronymy in these
cases is based on extracting semantic features of constituents and checking whether or
not these features match the classification rules. For example, The horn is part of the car is
meronymic whereas He is part of the game is not.
In the case of meronymy, since there are numerous unambiguous and ambiguous
patterns, we devised a method to find these patterns and rank them in the order of
their frequency of use. Our intention is to detect the most frequently occurring patterns
that express meronymy and provide an algorithm for their automatic detection and
disambiguation in text.
3.1 An Algorithm for Finding Lexico-Syntactic Patterns
In order to identify lexico-syntactic forms that express part?whole relations and de-
termine their distribution over a very large corpus, we used the following algorithm
inspired by Hearst?s (1998) work:
Step 1. Pick pairs of concepts Ci, Cj among which there is a part?whole relation
For this task, we used the information provided by WordNet 1.7 (Fellbaum 1998).
In WordNet, the nouns are organized into nine hierarchies, each hierarchy being
identified by its corresponding root concept: {abstraction}, {act}, {entity}, {event},
{group}, {phenomenon}, {possession}, {psychological feature}, and {state}. The nouns
are grouped in concepts or synsets; a concept consisting of a list of synonymous
word senses. For example, {mother#1, female parent#1} is a WordNet concept. Besides
concepts, WordNet contains 11 semantic relations: HYPONYMY (IS?A), HYPERNYMY
(REVERSE IS?A), MERONYMY (PART?WHOLE), HOLONYMY (REVERSE PART?WHOLE),
ENTAIL, CAUSE?TO, ATTRIBUTE, PERTAINYMY, ANTONYMY, SYNSET (SYNONYMY), and
SIMILARITY. The part?whole relations in WordNet are further classified into three
basic types: MEMBER-OF (e.g., UK#1 IS-MEMBER-OF NATO#1), STUFF-OF (e.g., carbon#1
IS-STUFF-OF coal#1), and PART-OF (e.g., leg#3 IS-PART-OF table#2) which includes all the
other part?whole relations described in the Winston, Chaffin and Hermann (WCH)
classification.
Since the part and whole concepts provided by WordNet can belong to almost
any WordNet noun hierarchy, we randomly selected 100 pairs of part?whole concepts
that were well distributed over all nine WordNet noun hierarchies, the three WordNet
meronymic relations, and the six types of part?whole relations of WCH. Two annotators
with computational linguistic knowledge classified the WordNet meronymic relations
into the WCH?s six part?whole types. According to our annotations, the MEMBER-
88
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
OF WordNet relations correspond to Winston, Chaffin, and Hermann?s MEMBER?
COLLECTION relations, STUFF-OF relations correspond to WCH?s STUFF?OBJECT rela-
tions, and the PART-OF correspond to the other four WCH relations. The annotators
obtained a 100% agreement in mapping the MEMBER-OF to MEMBER?COLLECTION,
STUFF-OF to STUFF?OBJECT. The PART-OF relations were mapped to the other four types
of WCH relations with an average agreement of 98%. A third judge (one of the authors)
checked the correctness of all the mappings and decided on the non-agreed instances.
This mapping ensures that the 100 general-purpose WordNet pairs cover most of the
possible types of part?whole relations in text.
Table 1 shows only 50 pairs from the set of 100 WordNet part?whole pairs and
their distribution among the WordNet hierarchies and the part?whole types provided
by WordNet and the WCH taxonomy. For example, the pair Bucharest#1?Romania#1
is a PART-OF relation in WordNet, but based on the Winston, Chaffin, and Hermann
classification it can be further classified as a more specific meronymy relation, PLACE?
AREA.
For the purpose of this research, we lumped together all part?whole types in the
classification of Winston et al9 However, the method presented in the paper is applica-
ble to extracting subtypes of part?whole relations; separate annotations for each type
would be necessary.
Step 2. Search a corpus and extract lexico-syntactic patterns that link a pair of part?
whole concepts
For each pair of part?whole noun concepts determined above, search the Internet or
any other large collection of documents and retain only the sentences containing that
pair. Since our intention is to demonstrate that the automatic procedure proposed here
is domain independent, we chose two distinct text collections: SemCor 1.7 and the
LA Times from TREC-9. From each collection we randomly selected 10,000 sentences,
which were searched for the pair of concepts selected. Since the LA Times collection is
not word-sense disambiguated, we searched for sentences containing the pair of nouns
without considering their senses. Out of these sentences, only some contained the part?
whole pairs selected in Step 1. We manually inspected these sentences and picked only
those in which the pairs involved meronymy. For example, the sentence I can feel my
fingers and close my hand contains the meronymic pair finger?hand, but in this context the
relationship is not expressed. From these sentences we manually extracted meronymic
lexico-syntactic patterns. Table 2 shows for each collection the number of sentences
used, the number of sentences that contain the studied concept pairs, the number of sen-
tences that contain part?whole relations, and the number of unique patterns discovered
from those sentences. Seven of the unique patterns occurred in both SemCor and the LA
Times.
In order to extract the patterns from the SemCor collection we used its gold standard
word sense annotations to our advantage and looked for the occurrences of concepts
(word with the sense) in the corpus. This explains the large difference between the
number of sentences discovered in the two corpora. The SemCor patterns thus extracted
did not need manual validation, since the noun concept pairs were always in a part?
whole relation.
9 The WCH categories were also used by the annotators to better distinguish between positive and
negative examples.
89
Computational Linguistics Volume 32, Number 1
Ta
b
le
1
T
he
lis
to
ffi
ft
y
se
le
ct
ed
p
ar
t?
w
ho
le
re
la
ti
on
p
ai
rs
u
se
d
as
in
p
u
tf
or
th
e
le
xi
co
-s
yn
ta
ct
ic
p
at
te
rn
id
en
ti
fi
ca
ti
on
p
ro
ce
d
u
re
.W
N
Ty
pe
is
th
e
p
ar
t?
w
ho
le
ty
p
e
fr
om
W
or
d
N
et
an
d
W
C
H
Ty
pe
is
th
e
p
ar
t?
w
ho
le
ty
p
e
fr
om
th
e
W
in
st
on
,C
ha
ffi
n
an
d
H
er
m
an
n
ta
xo
no
m
y.
P
ar
tc
on
ce
p
t
W
ho
le
co
nc
ep
t
T
he
P
ar
t
T
he
W
ho
le
W
N
Ty
p
e
W
C
H
Ty
p
e
H
ie
ra
rc
hy
H
ie
ra
rc
hy
ac
t
p
la
y
ab
st
ra
ct
io
n
ab
st
ra
ct
io
n
PA
R
T
-O
F
P
O
R
T
IO
N
?M
A
SS
ai
r
w
in
d
en
ti
ty
p
he
no
m
en
on
ST
U
FF
-O
F
ST
U
FF
?O
B
JE
C
T
ar
ti
lle
ry
ba
tt
er
y
en
ti
ty
gr
ou
p
M
E
M
B
E
R
-O
F
M
E
M
B
E
R
?C
O
L
L
E
C
T
IO
N
bo
d
yg
u
ar
d
gu
ar
d
en
ti
ty
gr
ou
p
M
E
M
B
E
R
-O
F
M
E
M
B
E
R
?C
O
L
L
E
C
T
IO
N
B
u
ch
ar
es
t
R
om
an
ia
en
ti
ty
en
ti
ty
PA
R
T
-O
F
P
L
A
C
E
?A
R
E
A
ce
llu
lo
se
p
ap
er
en
ti
ty
en
ti
ty
ST
U
FF
-O
F
ST
U
FF
?O
B
JE
C
T
ch
ew
ea
ti
ng
en
ti
ty
p
he
no
m
en
on
PA
R
T
-O
F
F
E
A
T
U
R
E
?A
C
T
IV
IT
Y
ch
or
u
s
so
ng
gr
ou
p
en
ti
ty
PA
R
T
-O
F
P
O
R
T
IO
N
?M
A
SS
co
m
p
u
te
r
co
m
p
u
te
r
ne
tw
or
k
en
ti
ty
en
ti
ty
PA
R
T
-O
F
C
O
M
P
O
N
E
N
T
?I
N
T
E
G
R
A
L
d
oo
r
ca
r
en
ti
ty
en
ti
ty
PA
R
T
-O
F
C
O
M
P
O
N
E
N
T
?I
N
T
E
G
R
A
L
ep
ile
p
ti
c
se
iz
u
re
ep
ile
p
sy
ev
en
t
st
at
e
PA
R
T
-O
F
F
E
A
T
U
R
E
?A
C
T
IV
IT
Y
ex
ec
u
ti
ve
go
ve
rn
m
en
t
gr
ou
p
gr
ou
p
M
E
M
B
E
R
-O
F
M
E
M
B
E
R
?C
O
L
L
E
C
T
IO
N
fi
gh
t
w
ar
en
ti
ty
ac
t
PA
R
T
-O
F
F
E
A
T
U
R
E
?A
C
T
IV
IT
Y
fi
ng
er
ha
nd
en
ti
ty
en
ti
ty
PA
R
T
-O
F
C
O
M
P
O
N
E
N
T
?I
N
T
E
G
R
A
L
fo
ot
le
g
en
ti
ty
en
ti
ty
PA
R
T
-O
F
C
O
M
P
O
N
E
N
T
?I
N
T
E
G
R
A
L
ge
nt
le
br
ee
ze
B
ea
u
fo
rt
sc
al
e
p
he
no
m
en
on
ab
st
ra
ct
io
n
M
E
M
B
E
R
-O
F
M
E
M
B
E
R
?C
O
L
L
E
C
T
IO
N
G
ib
ra
lt
ar
E
u
ro
p
e
gr
ou
p
en
ti
ty
PA
R
T
-O
F
P
L
A
C
E
?A
R
E
A
ha
nd
ar
m
en
ti
ty
en
ti
ty
PA
R
T
-O
F
C
O
M
P
O
N
E
N
T
?I
N
T
E
G
R
A
L
in
ch
fo
ot
ab
st
ra
ct
io
n
ab
st
ra
ct
io
n
PA
R
T
-O
F
P
O
R
T
IO
N
?M
A
SS
ir
on
st
ee
l
en
ti
ty
en
ti
ty
ST
U
FF
-O
F
ST
U
FF
?O
B
JE
C
T
kn
ee
le
g
en
ti
ty
en
ti
ty
PA
R
T
-O
F
C
O
M
P
O
N
E
N
T
?I
N
T
E
G
R
A
L
le
g
ch
ai
r
en
ti
ty
en
ti
ty
PA
R
T
-O
F
C
O
M
P
O
N
E
N
T
?I
N
T
E
G
R
A
L
le
tt
er
al
p
ha
be
t
ab
st
ra
ct
io
n
ab
st
ra
ct
io
n
M
E
M
B
E
R
-O
F
M
E
M
B
E
R
?C
O
L
L
E
C
T
IO
N
liq
u
id
as
se
ts
ca
p
it
al
p
os
se
ss
io
n
p
os
se
ss
io
n
PA
R
T
-O
F
M
E
M
B
E
R
?C
O
L
L
E
C
T
IO
N
lo
ck
d
oo
r
en
ti
ty
en
ti
ty
PA
R
T
-O
F
C
O
M
P
O
N
E
N
T
?I
N
T
E
G
R
A
L
90
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
Ta
b
le
1
(c
on
t.)
P
ar
tc
on
ce
p
t
W
ho
le
co
nc
ep
t
T
he
P
ar
t
T
he
W
ho
le
W
N
Ty
p
e
W
C
H
Ty
p
e
H
ie
ra
rc
hy
H
ie
ra
rc
hy
m
em
be
r
or
ga
ni
za
ti
on
en
ti
ty
gr
ou
p
M
E
M
B
E
R
-O
F
M
E
M
B
E
R
?C
O
L
L
E
C
T
IO
N
m
em
or
y
co
m
p
u
te
r
en
ti
ty
en
ti
ty
PA
R
T
-O
F
C
O
M
P
O
N
E
N
T
?I
N
T
E
G
R
A
L
m
et
ac
ar
p
u
s
ha
nd
en
ti
ty
en
ti
ty
PA
R
T
-O
F
C
O
M
P
O
N
E
N
T
?I
N
T
E
G
R
A
L
m
et
er
ki
lo
m
et
er
ab
st
ra
ct
io
n
ab
st
ra
ct
io
n
PA
R
T
-O
F
P
O
R
T
IO
N
?M
A
SS
m
id
d
le
ag
e
ad
u
lt
ho
od
ab
st
ra
ct
io
n
ab
st
ra
ct
io
n
PA
R
T
-O
F
F
E
A
T
U
R
E
?A
C
T
IV
IT
Y
m
yo
ca
rd
ia
l
in
fa
rc
t
he
ar
t
at
ta
ck
st
at
e
ev
en
t
PA
R
T
-O
F
F
E
A
T
U
R
E
?A
C
T
IV
IT
Y
nu
m
be
r
se
ri
es
ab
st
ra
ct
io
n
en
ti
ty
M
E
M
B
E
R
-O
F
M
E
M
B
E
R
?C
O
L
L
E
C
T
IO
N
ox
yg
en
ai
r
en
ti
ty
en
ti
ty
ST
U
FF
-O
F
ST
U
FF
?O
B
JE
C
T
p
al
m
ha
nd
en
ti
ty
en
ti
ty
PA
R
T
-O
F
C
O
M
P
O
N
E
N
T
?I
N
T
E
G
R
A
L
p
av
em
en
t
ro
ad
en
ti
ty
en
ti
ty
ST
U
FF
-O
F
ST
U
FF
?O
B
JE
C
T
p
aw
ca
t
en
ti
ty
en
ti
ty
PA
R
T
-O
F
C
O
M
P
O
N
E
N
T
?I
N
T
E
G
R
A
L
p
eo
p
le
w
or
ld
gr
ou
p
gr
ou
p
M
E
M
B
E
R
-O
F
M
E
M
B
E
R
?C
O
L
L
E
C
T
IO
N
p
ro
m
en
ad
e
ba
ll
en
ti
ty
gr
ou
p
PA
R
T
-O
F
F
E
A
T
U
R
E
?A
C
T
IV
IT
Y
R
om
an
ia
E
u
ro
p
e
en
ti
ty
en
ti
ty
PA
R
T
-O
F
P
L
A
C
E
?A
R
E
A
R
om
an
ia
n
R
om
an
ia
en
ti
ty
en
ti
ty
M
E
M
B
E
R
-O
F
M
E
M
B
E
R
?C
O
L
L
E
C
T
IO
N
Sa
ha
ra
A
fr
ic
a
en
ti
ty
en
ti
ty
PA
R
T
-O
F
P
L
A
C
E
?A
R
E
A
sh
ow
er
ba
th
en
ti
ty
en
ti
ty
PA
R
T
-O
F
C
O
M
P
O
N
E
N
T
?I
N
T
E
G
R
A
L
sn
ow
sn
ow
ba
ll
en
ti
ty
en
ti
ty
ST
U
FF
-O
F
ST
U
FF
?O
B
JE
C
T
sy
m
p
to
m
d
is
ea
se
p
sy
ch
fe
at
u
re
st
at
e
PA
R
T
-O
F
F
E
A
T
U
R
E
?A
C
T
IV
IT
Y
ty
m
p
an
u
m
ea
r
en
ti
ty
en
ti
ty
PA
R
T
-O
F
C
O
M
P
O
N
E
N
T
?I
N
T
E
G
R
A
L
vo
lu
m
e
se
t
en
ti
ty
gr
ou
p
M
E
M
B
E
R
-O
F
M
E
M
B
E
R
?C
O
L
L
E
C
T
IO
N
w
at
er
ic
e
en
ti
ty
en
ti
ty
ST
U
FF
-O
F
ST
U
FF
?O
B
JE
C
T
W
at
er
lo
o
B
el
gi
u
m
en
ti
ty
en
ti
ty
PA
R
T
-O
F
P
L
A
C
E
?A
R
E
A
w
in
d
ow
ca
r
en
ti
ty
en
ti
ty
PA
R
T
-O
F
C
O
M
P
O
N
E
N
T
?I
N
T
E
G
R
A
L
w
in
g
an
ge
l
en
ti
ty
p
sy
ch
fe
at
u
re
PA
R
T
-O
F
C
O
M
P
O
N
E
N
T
?I
N
T
E
G
R
A
L
91
Computational Linguistics Volume 32, Number 1
3.2 Taxonomy of Part?Whole Patterns
From the 535 part?whole relations detected from the 20,000 SemCor and LA Times
sentences, 493 (92.15%) were expressed by phrase-level patterns and 42 (7.85%) by
sentence-level patterns. Overall, there were 42 unique meronymic lexico-syntactic pat-
terns, of which 31 were phrase-level patterns and 11 sentence-level patterns.
Recall our notation for the part?whole relation PART(X, Y), where X is part of Y.
a. Phrase-level patterns
Here, the part and whole concepts are included in the same phrase. For example, for the
pattern NPX PPY the noun phrase that contains the part and the prepositional phrase
that contains the whole are found in the same noun phrase. The engine in the car is an
instance of this pattern where X is the part (engine) and Y is the whole (car).
b. Sentence-level patterns
In these constructions, the part?whole relation is intrasentential. The patterns contain
specific verbs and the part and the whole can be found inside noun phrases or prepo-
sitional phrases that contain specific prepositions. A frequent such pattern is NPY verb
NPX, where NPX is the noun phrase that contains the part, NPY is the noun phrase that
contains the whole and the verb is restricted (see Table 2 of Appendix A). For instance,
the cars have doors is an instance of this pattern.
An extension of this pattern is NPX verb NPZ PPY, with NPZ containing the words
part or member. An example is: The engine is a part of the car; NPX ? the engine, PPY ? of
the car, and the verb ? to be.
In some instances the meronymic constructions contained combinations, conjunc-
tions and/or disjunctions, of parts and wholes. For example, NPX1X2 PPY (e.g., wheels
and engine of a car) is a form of the pattern NPX PPY. This observation enabled us to gen-
eralize the list of patterns. A summary of phrase-level and sentence-level meronymic
patterns along with their extensions and generalizations is provided in Appendix A.
Based on our observations of the corpus used for the pattern identification proce-
dure and based on the results obtained by others (Evens et al 1980), we have concluded
that the lexico-syntactic patterns encoding meronymy can be classified according to
their semantic similarity and frequency of occurrence into the clusters presented in
Table 3. The clusters contain lexico-syntactic patterns that have similar semantic be-
havior. We also noticed that more than a half of cluster 4?s patterns are very rare;
for example, X branch of Y; In Y, X1 verb X2; or In Y packed to X. Overall, this cluster
covers less than 7% of the part?whole patterns discovered. Thus, for the purpose of
this research we considered only the first three clusters of lexico-syntactic patterns
expressing meronymy.
Table 2
Number of sentences and patterns containing the 100 part?whole pairs in each text collection
considered.
Collection Number of Number of sentences Number of sentences Number of
sentences containing the pairs containing patterns
part?whole relations
SemCor 10,000 87 48 12
LA Times 10,000 1,988 487 30
92
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
Table 3
Clusters of lexico-syntactic patterns classified based on their semantic similarity and their
frequency of occurrence in the 20,000 sentence corpus used in the part?whole pattern
identification procedure.
Cluster Patterns Freq. Coverage Examples
C1. genitives NPX of NPY eyes of the baby
and NPY?s NPX 282 52.71% girl?s mouth
verb to have NPY have NPX The table has four legs.
C2. noun NPXY 86 16.07% door knob
compounds NPYX turkey pie
C3. preposition NPY PPX 133 24.86% A bird without wings cannot fly.
NPX PPY A room in the house.
C4. other others 34 6.36% The Supreme Court is a branch of
the Government.
This pattern classification criterion is justified, in part, by our desire to verify
whether or not the automatic approach proposed here is generally applicable not
only for the genitive cluster patterns (cluster 1) (Girju, Badulescu, and Moldovan
2003), but also for more complex types, such as noun compounds (cluster 2) and
prepositional constructions (cluster 3). Our intuition that the proposed patterns have
different semantic behavior, and thus have to be treated separately in distinct clusters,
is partially justified by a linguistic analysis summarized in Section 3.3 and supported
by our empirical results from Section 5.3. In the remainder of the paper, we refer to
these clusters as the genitives (cluster 1), noun compounds (cluster 2), preposition
(cluster 3), and other (cluster 4) clusters.
We also noticed that some patterns, such as the genitive and preposition clusters,
prefer the part and the whole in a certain position. For example, in of?genitives the part
is mostly in the first position (modifier), and the whole in the second (head) (e.g., door of
the car), while in s?genitives the positions are reversed (e.g., car?s door). The verb to have
requires parts in the second position, while noun compounds have a preference for them
in the second position (e.g., car has door and car door, respectively). In the preposition
cluster patterns, for the preposition in the part is usually in the first position (e.g., door
in the car) and for the preposition with the positions are reversed (e.g., car with four doors).
However, there are also exceptions. For instance, in some of?genitives the part can
occupy the second position (e.g., flock of birds) and in some noun compounds it can
be present in the first position (e.g., ham sandwich). In the corpus used for pattern
identification these exceptions are rare. Therefore, we will not consider the patterns NPY
of NPX and NPXY in our experiments. If such examples are encountered, the part and
the whole concepts are wrongly identified, representing one source of errors.
Berland and Charniak (1999) also used Hearst?s algorithm to find part?whole pat-
terns. However, they focused only on the first five patterns that occur frequently in their
corpus. These patterns are subsumed by our clusters as shown in Table 4. They noticed
that the last three patterns are ambiguous and decided to use only the first two in their
experiments.
3.3 The Ambiguity of Part?Whole Lexico-Syntactic Patterns
From the list of lexico-syntactic patterns thus extracted, we noticed that some of these
part?whole constructions always refer to meronymy, but most of them are ambiguous,
93
Computational Linguistics Volume 32, Number 1
Table 4
The patterns used by Berland and Charniak and the corresponding cluster patterns used by us.
Berland and Charniak patterns Our cluster patterns Example
NNwhole ?s NNpart NPY ?s NPX girl?s mouth
NNpart of (the|a) (JJ|NN) NNwhole NPX of NPY eyes of the baby
NNpart in (the|a) (JJ|NN) NNwhole NPX PPY ball in red box
NNparts of NNwholes NPX of NPY doors of cars
NNparts in NNwholes NPX PPY quotations in articles
in the sense that they express a part?whole relation only in some particular contexts and
only between specific pairs of nouns. For example, NP1 is member of NP2 always refers
to meronymy, but this is not true for NP1 has NP2. In most cases, the verb to have has the
sense of to possess, and only in some particular contexts refers to meronymy.
Table 5 presents a summary of some of the most frequent part?whole lexico-
syntactic patterns we observed, classified based on their ambiguity.
Below we discuss further the ambiguities encountered in the patterns of the first
three clusters.
The Semantic Ambiguity of Genitive Constructions
In English there are two kinds of genitives: the s-genitive and the of-genitive. A char-
acteristic of the genitives is that they are very ambiguous, as the constructions can be
given various interpretations (Moldovan and Badulescu 2005). For instance, genitives
can encode relations such as PART?WHOLE (Mary?s hand), POSSESSION (Mary?s car),
KINSHIP (Mary?s sister), PROPERTY/ATTRIBUTE HOLDER (Mary?s beauty), DEPICTION?
DEPICTED (Mary?s painting ? if it depicts her), SOURCE-FROM (Mary?s birth city), or
Table 5
Examples of meronymic expressions based on their ambiguity.
Types of Positive Examples (part?whole) Negative Examples (not part?whole)
Part?Whole
Expressions
Unambiguous The parts of an airplane include
the engine, ..
The substance consists of
three ingredients.
One of the air?s constituents is
oxygen.
The cloud was made of dust.
Iceland is a member of NATO.
Ambiguous The horn is part of the car. He is part of the game (PARTICIPANT?EVENT)
The table has four legs. Kate has four cats. (POSSESSION)
The girl?s mouth is sensual. Mary?s brother is cute. (KINSHIP)
The eyes of the baby are blue. The dress of my niece is blue. (POSSESSION)
Each door knob was made of silver. Dallas is a modern Texas city. (LOCATION)
It was the girl with blue eyes. The woman with triplets received a lot
of attention. (KINSHIP)
94
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
MAKE-PRODUCE (Mary?s novel ? if Mary wrote it). Thus, any attempt to interpret
genitive constructions has to deal with the semantic analysis of the two noun con-
stituents. Sometimes world knowledge or more contextual information is necessary to
identify the correct semantic relation (e.g., Mary?s novel might mean the novel written
by Mary, read by Mary, or dreamed about by Mary).
The Semantic Ambiguity of the Verb To Have
According to WordNet 1.7, the verb to have in transitive constructions has 21 different
senses, such as to possess, feature, need, get, undergo, be confronted with, accept, suffer
from, and many others. Although the senses enumerated in WordNet represent a rather
disparate set with no well defined semantic connection among them, the verb to have
can participate in many different semantic structures and has been studied extensively
in the linguistics community (Freeze 1992; Schafer 1995; Jensen and Vikner 1996).
The semantic relations encoded by the verb to have are quite similar to those realized
by genitive constructions. Some researchers (Jensen and Vikner 1996) offered a detailed
analysis for the purpose of capturing the most important semantic features of the verb
to have. Their hypothesis is based on the idea that, semantically, the verb to have has
a sense of its own derived from the semantic interpretation of the close context or
the sentence in which it occurs. Let?s consider the following sentences: (a) Kate has a sister
(KINSHIP), (b) Kate has a cat (POSSESSION), and (c) Kate has green eyes (PART?WHOLE). The
meaning of the verb to have in these situations is derived from the semantic information
encoded in both the subject and the object.
The Semantic Ambiguity of Noun Compounds
Noun compounds (NCs) are noun sequences of the type N1 N2 .. Nn that have a
particular meaning as a whole. NCs have been studied intensively in linguistics (Levi
1978), psycholinguistics (Downing 1977), and computational linguistics (Spa?rck Jones
1983; Lauer and Dras 1994; Rosario and Hearst 2001) for a long time. The interpretation
of NCs focuses on the detection and classification of a comprehensive set of semantic
relations between the noun constituents. This task has proved to be very difficult due to
the complex semantic aspect of noun compounds:
1. NCs have implicit semantic relations: for example, spoon handle
(PART?WHOLE).
2. NCs? interpretation is knowledge intensive and can be idiosyncratic: For
example, GM car (in order to correctly interpret this compound we have to
know that GM is a car-producing company).
3. There can be many possible semantic relations between a given pair of
word constituents. For example, linen bag can mean bag made of linen
(PART?WHOLE), as well as bag for linen (PURPOSE).
4. Interpretation of NCs can be highly context-dependent. For example, apple
juice seat can be defined as ?seat with apple juice on the table in front of it?
(Downing 1977).
The Semantic Ambiguity of Prepositional Constructions
In English and various other natural languages, prepositions play a very important role
both syntactically and semantically in the phrases, clauses, and sentences in which they
95
Computational Linguistics Volume 32, Number 1
occur. Semantically speaking, prepositional constructions can encode various seman-
tic relations, their interpretations being provided most of the time by the underlying
context. For instance, in the following examples the preposition with encodes different
semantic relations: (a) It was the girl with blue eyes (MERONYMY), (b) The baby with the
red ribbon is cute (POSSESSION), and (c) The woman with triplets received a lot of attention
(KINSHIP).
The variety and ambiguity of these constructions show the complexity and impor-
tance of our task. We have seen that the interpretation of these constructions depends
heavily on the meaning of the two noun constituents. To get the meaning of the nouns
we rely on a word sense disambiguation system that takes into consideration surround-
ing contexts of the words.
4. A Machine Learning Algorithm for Automatic Discovery of Classification Rules
4.1 Approach
In this section we propose a method for the automatic discovery of rules that discrimi-
nate whether or not a selected pattern instance is meronymic. First a corpus is prepared
and patterns from clusters C1?C3 are identified. The approach relies on the assumption
that the semantic relation between two noun constituents representing the part and the
whole can be detected based on nouns? semantic features.
This procedure applies to ambiguous constructions. The unambiguous construc-
tions don?t have to be processed since they lead unmistakably to part?whole relations.
The system learns automatically classification rules that check semantic features of
noun constituents. The classification rules are learned through an iterative semantic spe-
cialization (ISS) procedure applied on the noun constituents? semantic features provided
by the WordNet lexical knowledge base (Fellbaum 1998). ISS starts by mapping the
training noun pairs to the corresponding top WordNet noun concepts using hypernymy
chains. Then, it builds a learning tree by recursively splitting the training corpus into
unambiguous and ambiguous examples based on the semantic information provided
by the WordNet noun hierarchies. The learning tree is built top-down, one level at a
time, each level corresponding to a specialization iteration. The internal nodes represent
sets of ambiguous examples at various levels of specialization, while the leaves contain
unambiguous examples. The ambiguous examples are further specialized with next-
level WordNet concepts. The process is repeated recursively until there are no more
ambiguous examples. For each set of unambiguous positive and negative examples
at each level in the downward descent, we apply Quinlan?s C4.5 algorithm and learn
classification rules of the form if X is/is not of a WordNet semantic class A and Y is/is not of
WordNet semantic class B, then the instance is/is not a part?whole relation.
4.2 Preprocessing Part?Whole Lexico-Syntactic Patterns
Since our discovery procedure is based on the semantic information provided by Word-
Net, we need to preprocess the noun phrases (NPs) extracted by the three clusters
considered and identify the potential part and the whole concepts. For each NP we keep
only the largest sequence of words (from left to right) defined in WordNet. For example,
from the noun phrase brown carving knife the procedure retains only carving knife, since
this concept is defined in WordNet. For each such sequence of words, we manually
annotate it with its WordNet sense in context. For the example above we annotated
the noun phrase with sense #1 (carving knife#1), since in that context it had sense #1 in
96
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
WordNet (for this concept WordNet lists only one sense, defined as ?a large knife used
to carve cooked meat?). Table 6 shows a few examples of patterns from different clusters
and the results of this preprocessing step.
4.3 Building the Training Corpus
In order to learn the classification rules, we used the SemCor 1.7 and TREC 9 text
collections, and the part?whole information provided by WordNet. From the SemCor
collection we selected 19,000 sentences. Another 100,000 sentences were randomly ex-
tracted from the LA Times articles of TREC 9. As SemCor 1.7 is already annotated with
part-of-speech tags and WordNet senses, we part-of-speech tagged only the LA Times
collection using Brill?s tagger (1995). A corpus ?A? was thus created from the selected
sentences of each text collection. Each sentence in this corpus was then parsed using the
syntactic parser developed by Charniak (2000). Focusing only on sentences containing
the lexico-syntactic patterns in each cluster C1?C3, we manually annotated nouns in the
patterns with their corresponding WordNet senses (with the exception of those from
SemCor), as shown in Section 4.2, and marked all candidate instances that encoded a
part?whole relation as positives, and negatives otherwise. In the corpus, 66% of the
annotated instances were PART-OF relations, 14% STUFF-OF, and 20% MEMBER-OF.
Moreover, WordNet 1.7 contains 27,636 part?whole relations linking various noun
concepts. As this information is very valuable for training purposes, we tried to see
which of the selected patterns match these pairs. For each WordNet part?whole pair,
we formed inflected queries (to capture singular and plural instances) and searched
the Web, the largest on-line general purpose text collection, using Altavista. From the
first 100 retrieved documents, we selected and syntactically parsed only those sen-
tences containing pairs within cluster patterns. We manually validated those instances
and registered which cluster(s) of patterns could extract the pair. All these sentences
formed a second corpus, corpus ?B?. For instance, for the pair door#4?car#1 we searched
Altavista for documents containing both words car and door. Then, we retrieved all the
sentences that contained the two words in at least one of the target patterns. As a result,
we obtained sentences containing the pair of words linked by patterns such as door of
car, car?s door, car has door, car with four doors, car door, etc.
Overall, the 27,636 WordNet pairs were linked by the genitive cluster patterns,
while the noun compound and preposition clusters extracted only some subsets of these
pairs. Some part?whole pairs were linked by patterns that belong to more than one
cluster. For instance, door knob is a pair that usually belongs to the noun compound
cluster, but it can also be selected by the genitive cluster (e.g., knob of the door) and the
preposition cluster (e.g., the door with the iron knob).
Table 6
Examples of identifying the potential Part and Whole concepts for different clusters.
Cluster Example Potential Part(X) and Positive or
Whole(Y) concepts negative example
C1. genitives the door of the car the [door#4]X of the [car#1]Y positive
my friend?s car my [ friend#1]Y ?s [car#1]X negative
C2. noun compounds car door company [car door#1]X [company#1]Y negative
[car#1]X [door#4]Y positive
C3. prepositions window from the car [window#2]X from the [car#1]Y positive
97
Computational Linguistics Volume 32, Number 1
Corpus ?B? was used only to convince us that the part?whole pairs selected from
WordNet were representative, ie., present in the patterns considered. Indeed corpus
?B? pairs were found in at least one of the cluster patterns. While corpus ?A? consists
of positive and negative examples from LA Times and SemCor collections, corpus
?B? contains only positive instances as they are WordNet part?whole pair concepts.
Moreover, although corpus ?B? has a different distribution than corpus ?A?, the noun
pairs from WordNet are general-purpose and always encode a part?whole relation.
Table 7 shows the statistics for the positive and negative training examples for each
cluster. In the genitive cluster, for example, there were 18,936 such pattern instances, of
which 325 encoded part?whole relations, while 18,611 did not. Thus, for the genitive
cluster we used a training corpus of 27,961 positive examples (325 pairs of concepts in
a part?whole relation extracted from corpus ?A? and 27,636 extracted from WordNet
as selected pairs) and 18,611 negative examples (the non-part?whole relations extracted
from corpus ?A?).
4.4 Inter-Annotator Agreement
The part?whole relation discovery procedure proposed in this paper was trained and
tested on a large corpus of human annotated examples (a part of the LA Times collection
for both training and testing, and a part of the Wall Street Journal (WSJ) collection for
testing). The annotators, two researchers in computational semantics, decided whether
an example pair encoded a part?whole relation or not. The examples were disam-
biguated in context: the annotators were given the pairs and the sentence in which they
occurred. The two annotators? task was to determine the correct senses of the two noun
constituents and then decide if the relation is meronymic or not. A third researcher
decided on the non-agreed word senses and relations. The annotators were also pro-
vided with the list of subtypes of meronymy relations proposed by (Winston, Chaffin,
and Hermann 1987) as a guideline for detecting part?whole relations. If an example
contained one of the six meronymy subtypes, the annotators tagged that example as
positive (part?whole); otherwise they tagged it as a negative example.
The annotators? agreement was measured using the kappa statistic (Siegel and
Castellan 1988), one of the most frequently used measures of inter-annotator agreement
for classification tasks:
K =
Pr(A) ? Pr(E)
1 ? Pr(E) , (1)
where Pr(A) is the proportion of times the raters agree and Pr(E) is the probability of
agreement by chance.
Table 7
Training corpora statistics for each of the three clusters considered.
Positive examples Negative examples
Cluster from WordNet as from Corpus ?A? from Corpus ?A?
evidenced by corpus ?B?
C1. genitives 27,636 325 18,611
C2. noun compounds 142 625 6,601
C3. prepositions 111 295 2,751
98
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
The K coefficient is 1 if there is total agreement among the annotators, and 0 if
there is no agreement other than that expected to occur by chance. This coefficient
measures how well annotators agree at identifying both positive and negative instances
of meronymic relations.
Table 8 shows the inter-annotator agreement on the part?whole classification task
for each of the three clusters considered in both training and test phases of the part?
whole relation discovery procedure.
On average, the K coefficient is close to 0.85, showing a good level of agreement,
for all clusters in the training and test data. This can be explained by the instructions
the annotators received prior to annotation and by their expertise in lexical semantics.
The results also show that even for more productive genitive and noun compound
examples, the sentence-level context was enough to disambiguate the examples most
of the time.
4.5 Iterative Semantic Specialization (ISS) Learning
Iterative Semantic Specialization Learning is an iterative process that learns a decision
tree and classification rules by mapping the semantic features of the noun pairs to
the WordNet noun hierarchies. The procedure starts with a generalized version of the
training examples as pairs of top WordNet noun concepts using hypernymy chains. The
examples are then split into unambiguous and ambiguous. The ambiguous examples
are further specialized with next-level WordNet concepts. The process is repeated re-
cursively until there are no more ambiguous examples. For each set of unambiguous
positive and negative examples at each level in the downward descent, we apply
Quinlan?s C4.5 algorithm and learn classification rules. As will be shown in Section 5.3,
the algorithm is applied separately to each of the three clusters considered for optimal
results.
The Iterative Semantic Specialization (ISS) Learning Algorithm
Input: Positive and negative meronymic examples of pairs of concepts. The concepts are
WordNet words semantically disambiguated in context (tagged with their correspond-
ing WordNet senses).
Output: Classification rules in the form of semantic selectional restrictions on the modifier
and head concepts using WordNet IS?A hierarchy information.
Table 8
The inter-annotator agreement on the part?whole classification task for each of the three clusters
considered in both training and test phases of the part?whole relation discovery procedure.
Corpus Cluster Kappa agreement
LA Times genitives 0.878
(training and testing) noun compounds 0.826
prepositions 0.811
genitives 0.880
WSJ (testing) noun compounds 0.862
prepositions 0.836
99
Computational Linguistics Volume 32, Number 1
Step 1. Generalizing the training examples
Initially, the training corpus consists of examples that have the format ?part#sense;
whole#sense; target?, where target can be either Yes or No, depending whether the relation
between the part and whole is meronymy or not: for example, ?aria#1, opera#1, Yes?.
From this initial set of examples an intermediate corpus was created by expanding each
example using the following format:
?part#sense, class part#sense, whole#sense, class whole#sense; target?,
where class part and class whole correspond to the WordNet top semantic classes of
the part and whole concepts, respectively. For instance, the previous example becomes
?aria#1, entity#1, opera#1, abstraction#6, Yes?.
From this intermediate corpus a generalized set of training examples is built, retain-
ing only the semantic classes and the target value. At this point, the generalized training
corpus contains three types of examples:
1. Positive examples ?X hierarchy#sense, Y hierarchy#sense, Yes?
2. Negative examples ?X hierarchy#sense, Y hierarchy#sense, No?
3. Ambiguous examples ?X hierarchy#sense, Y hierarchy#sense, Yes/No?
The third situation occurs when the training corpus contains both positive and
negative examples for the same hierarchy types. For example, both the relationships
?apartment#1, woman#1, No? and ?hand#1, woman#1, Yes? are mapped into the more
general type ?entity#1, entity#1, Yes/No?. However, the first example is negative (a POS-
SESSION relation), while the second one is a positive example.
Step 2. Learning classification rules for unambiguous examples
For the unambiguous examples in the generalized training corpus (those that are either
positive or negative), rules are determined using C4.5. In this context, the features are
the components of the relation (the part and, respectively the whole) and the values of
the features are the corresponding WordNet semantic classes (the furthest ancestor in
WordNet of the corresponding concept).
With the first two types of examples, the unambiguous ones, a new training corpus
was created on which we applied C4.5 using a 10-fold cross validation. The corpus
is split in ten permutations, 9/10 training and 1/10 testing, and the output is rep-
resented by 10 sets of rules and default values generated from these unambiguous
examples.
The rules obtained are if?then rules with the part and whole noun semantic senses
as preconditions. The default value is the most probable value for the target value and
is used to classify unseen instances of that type when no other rule applies. It can be
either Yes or No, corresponding to the possible values of the target attribute (part?whole
relation or not).
The rules were ranked according to their frequency of occurrence and average
accuracy obtained for each particular set. In order to use the best rules, we decided
to keep only those that had a frequency above a threshold (occurring in at least 7 of the
10 sets of rules) and an average accuracy greater than or equal to 50%.
In order to minimize the redundancies that may occur during the learning process,
rules with the same classification value as the default value are ignored. The idea is that
the default rule incorporates all the rules with the same target value.
For instance, after running C4.5 on the unambiguous set for the abstraction#6?
abstraction#6 example, we obtained a list of five rules and a default value No, as shown
100
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
in Table 9. Rules 1 and 5 were discarded as they were incorporated into the default
class. Rules 3 and 4 were also discarded as their frequency did not pass the threshold of
7. Thus, rule 2 remains the only applicable rule.
After filtering the rules that have the default value or do not pass the frequency and
accuracy thresholds, there might be cases in which the set of remaining rules is empty.
Step 3. Specializing ambiguous examples
Since C4.5 cannot be applied to ambiguous examples, we recursively specialize them to
eliminate the ambiguity. The specialization procedure is based on the IS?A information
provided by WordNet. Initially, each semantic class represented the root of one of the
noun hierarchies in WordNet. By specialization, the semantic class is replaced with the
corresponding hyponym for that particular sense, i.e., the concept immediately below
in the hierarchy.
For this task, we again considered the intermediate training corpus of examples.
For instance, the examples ?leg#2, entity#1, bee#1, entity#1, Yes? and ?beehive#1,
entity#1, bee#1, entity#1, No? that caused the ambiguity ?entity#1, entity#1, Yes/No?, were
replaced with ?leg#2, thing#12, bee#1, organism#1, Yes? and ?beehive#1, object#1, bee#1,
organism#1, No?, respectively. This intermediate example is thus generalized in the less
ambiguous examples ?thing#12, organism#1, Yes? and ?object#1, organism#1, No?. This way,
we specialize the ambiguous examples with more specific values for the attributes. The
specialization process for this particular example is shown in Figure 1.
Although this specialization procedure eliminates a proportion of the ambigu-
ous examples, there is no guarantee it will work for all the ambiguous examples of
this type. This is because the specialization splits the initial hierarchy into smaller
distinct subhierarchies, with the examples distributed over this new set of subhier-
archies. For the examples described above, the procedure eliminates the ambiguity
through specialization of the semantic classes into new ones: thing#12?organism#1 and
object#1?organism#1.
However, not all the examples can be disambiguated after only one specialization.
For the examples ?leg#2, bee#1, Yes? and ?world#7, bee#1, No?, the procedure generalizes
abstraction#6?abstraction#6 into the ambiguous example ?entity#1, entity#1, Yes/No? and
then specializes it in the ambiguous example ?part#7, organism#1, Yes/No?. After one
specialization the ambiguity still remains.
Steps 2 and 3 are repeated until there are no more ambiguous examples. The general
architecture of this procedure is shown in Figure 2.
Table 9
The list of rules for the iteration generated by the unambiguous subset of the ambiguous
example ?abstraction#6, abstraction#6, yes/no?. ?Yes? means part?whole relation, while ?No?
means non-part?whole relation. The global default target value of this unambiguous node
is No. Note that rules 3 and 4 are discarded as their frequency is below 7, and rules 1 and
5 were also discarded as incorporated in the default class No.
Rule no. Part Class Whole Class Target Accuracy value Frequency
1 measure#3 abstraction#6 No 92.51 9
2 time#5 abstraction#6 Yes 79.21 9
3 abstraction#6 time#5 Yes 85.70 1
4 abstraction#6 measure#3 Yes 63.00 1
5 abstraction#6 attribute#2 No 93.00 1
Default No
101
Computational Linguistics Volume 32, Number 1
Figure 1
The specialization of examples ?leg#2, entity#1, bee#1, entity#1, Yes?, ?beehive#1, entity#1, bee#1,
entity#1, No?, and ?world#7, entity#1, bee#1, entity#1, No? with the corresponding WordNet
semantic classes.
We observed that after the first generalization, 99.72% of the examples were am-
biguous. After each specialization, the percentage decreases. For instance, after one level
of specialization, 97.36% of the examples for entity#1?entity#1, 96.05% for abstraction#6?
abstraction#6, and 97.56% for entity#1?group#1 were ambiguous.
Table 10 presents a sample of the iterations produced by the program to specialize
the genitive cluster ambiguous example abstraction#6?abstraction#6. Each indentation
corresponds to a specialization iteration.
The training corpus considered for this research required on average 2.5 and at most
five levels of specialization.
The next section describes the construction of classification rules, the experiments,
and the results obtained.
Figure 2
Diagram of the ISS system.
102
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
Table 10
A sample iteration produced by the ISS procedure for the genitive cluster
abstraction#6?abstraction#6 ambiguous example. The italicized examples are unambiguous.
abstraction#6?abstraction#6
attribute#2?attribute#2
attribute#2?measure#3
attribute#2?relation#1
relation#1?attribute#2
measure#3?measure#3
measure#3?relation#1
time#5?time#5
relation#1?time#5
relation#1?relation#1
magnitude relation#1?magnitude relation#1
part#1?part#1
social relation#1?part#1
communication#2?language unit#1
social relation#1?social relation#1
communication#2?communication#2
signal#1?message#2
signal#1?written communication#1
written communication#1?written communication#1
writing#2?writing#2
message#2?written communication#1
5. Formulating Classification Rules and Applying them to Discover
Part?Whole Relations
5.1 Building the Learning Tree
The ISS learning procedure presented in the previous section builds a learning tree by
recursively splitting the training corpus into unambiguous and ambiguous examples,
based on the semantic information provided by the WordNet noun hierarchies. The
learning tree is built top-down, one level at a time, each level corresponding to a
specialization iteration. The internal nodes represent ambiguous examples at various
levels of specialization, while the leaves contain sets of unambiguous examples. For
instance, Figure 3 shows the learning tree corresponding to the specialization from
Table 10.
Initially, the learning tree contains only a dummy root node that provides no
information. After the generalization done in step 1 of the ISS learning procedure,
all the initial examples are mapped into corresponding pairs of top noun semantic
classes in WordNet and split into unambiguous and ambiguous sets based on their
target function. All these new sets of examples form the first level of the learning
tree.
The learning tree has two types of nodes: unambiguous nodes, corresponding to the
sets of unambiguous examples from each iteration (e.g., nodes 1.1, 1.3.1, and 1.4.1 from
Figure 3) and ambiguous nodes, corresponding to each ambiguous example from each
iteration (e.g., nodes 1.2, 1.3, 1.4, and 1.4.2 from Figure 3). Each node has associated with
it a pair {R, D} representing a set of rules and a default value. The set of rules represents
the rules to be used for classifying the new instances and the default value represents
the target value (Yes if an instance is a part?whole relation and No if the instance is
103
Computational Linguistics Volume 32, Number 1
Figure 3
A snapshot of the learning subtree abstraction#6?abstraction#6 on which the combination and
propagation algorithm is exemplified. Each node has an associated set of rules and a default
value. The rule number references are for the ?No.? column from Table 11.
not a part?whole relation) that should be returned if none of the rules classify the new
instances.
After learning the classification rules in Step 2 of the ISS procedure, all the unam-
biguous nodes have default values and some have rules.
5.2 Formulating the Classification Rules
In order to generate an overall set of classification rules, we traverse the learning tree
in a bottom-up fashion, applying the rules generated at each level in this order. The
rationale of this approach is that the rules closer to the bottom are more specific,
and thus more accurate. At each level, the idea is to combine the rules associated
with each sibling node and propagate the result to the parent. The combination and
propagation steps are applied recursively until the root is reached. The combination
phase guarantees that the rules to be combined are applied in a particular order at each
level.
Figure 4 shows a typical tree corresponding to one iteration of the ISS procedure on
which we will explain the combination and propagation algorithm. Node L represents
an internal node containing an ambiguous example. Through specialization, the learn-
ing procedure generated a set of unambiguous examples represented by the leaf LU, and
a sequence of n ambiguous examples represented by the internal nodes LA1 , LA2 , .. LAn.
104
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
Table 11
The rules and default value learned for the genitive cluster for the abstraction#6?abstraction#6
ambiguous example. ?Val.? is the target value, ?Acc.? is the rules? accuracy, and ?Fr.? is their
occurrence frequency. The numbering style used in the ?No.? column is intended to indicate
rules at different specialization levels.
No. Part Class Whole Class Val. Acc. Fr. Example
abstraction#6 abstraction#6 No glory#2?past#1
1 linear measure#3 measure#3 Yes 63 9 centimeter#1?decimeter#1
2 communication#2 communication#2 Yes act#3?play#1
2.1 written comm.#1 written comm.#1 No text#1?act#3
2.1.1 writing#2 writing#2 Yes New Testament#1?Bible#1
2.1.1.1 matter#6 No 79.98 9 text#1?act#3
2.2 indication#1 message#2 No 73.25 10 copy#1?recommendation#1
2.3 message#2 communication#2 No 79.72 8 irony#1?play#1
3 time#5 abstraction#6 Yes 79.21 9 carboniferous#1?paleozoic#1
Default No glory#2?past#1
The values associated with the ambiguous nodes (rules and default values) are
generated through propagation from lower levels.
Rule combination and propagation algorithm:
Input: Pairs of rules and associated default values for each unambiguous and ambigu-
ous node: {RU, DU}, {RA1 , DA1}, {RA2 , DA2}, ..{RAn , DAn};
Output: A pair of rules and default value for parent node: {RL, DL}.
Step 1. Propagating the default value to the parent node: DL ? DU
The default value of the unambiguous examples (DU) will be directly propagated
to the parent as the global default value of the subtree L (DL). For example, the default
value for the unambiguous node 1.1 from Figure 3 is No and it will propagate to the
parent node abstraction#6?abstraction#6 (node 1 in Figure 3).
If there is no unambiguous node LU (and therefore default value DU), the default
value for the first ambiguous example is propagated to L. For instance, for the am-
biguous node 1.4.2 (social relation#1?social relation#1), there were no unambiguous ex-
Figure 4
A part of the learning tree generated by the ISS learning procedure. The pairs of rules and
default value associated with the parent node are generated through propagation of the
combined pairs of rules and default values of the children.
105
Computational Linguistics Volume 32, Number 1
amples; and therefore the default value from the node 1.4.2.1 (written communication#2?
written communication#2) will be used.
Step 2. Propagating the rules from an ambiguous node with the same default value
to the parent node: RL ? {RAi |DAi = DL, 1 ? i ? n}
The ambiguous nodes are the first to be tested. All the rules associated with the
ambiguous nodes having the same default value as the global one are applied with the
highest priority. For instance, all the ambiguous nodes for abstraction#6?abstraction#6
(nodes 1.2?1.8 in Figure 3) received a default value of No through propagation from
their descendents. Since the default value for this node is No, it will receive all their
rules (Rules 1 and 2 from Table 12).
Step 3. Propagating the rules from an ambiguous node with the opposite default
value to the parent node: RL ? RL ? {if Aj then RAj ? DAj |DAj = DL, 1 ? j ? n}
The remaining ambiguous nodes have associated with them a different default
value (a non-default value). Since the two nodes have opposite default values, the
default value (DAj ) needs to be used when the rules for the child node (Aj) do not hold.
Therefore, a new rule, specific to the example Aj, needs to be created, for handling all
the instances of Aj: if Aj then RAj ? DAj .
For example, the ambiguous node 1.4.2.1.2 (written communication#1?written
communication#1) has the default value No. Its only ambiguous node (node 1.4.2.1.2.2 :
writing#2?writing#2) has the default value Yes. Therefore, a specific rule (Rule 2.1.1 from
Table 12) needs to be created for the example (Part=writing#2 and Whole=writing#2),
Table 12
The list of rules obtained for the ambiguous example abstraction#6?abstraction#6 for the genitive
cluster.
1 if Part is linear measure#3 and Whole Class is measure#3
then It is a part?whole relation
2 if Part is communication#2 and Whole is communication#2
then
2.1 if Part is written communication#1 and Whole is written communication#1
then
2.1.1 if Part is writing#2 and Whole is writing#2
then
2.1.1.1 if Part is matter#6
then It is not a part?whole relation
else It is a part?whole relation
else It is not a part?whole relation
else
2.2 if Part is indication#1 and Whole is message#2
then It is not a part?whole relation
else
2.3 if Part is message#2 and Whole is communication#2
then It is not a part?whole relation
else It is a part?whole relation
3 if Part is time#5 and Whole is abstraction#6
then It is a part?whole relation
106
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
that applies its rule (Rule 2.1.1.1: if Part=matter#6 then No) and returns its default value
(a non-default value) for the other cases:
if Part=writing#2 and Whole=writing#2 - the ambiguous example Aj
then
if Part=matter#6 then No (Is not part?whole) - the rules RAj
else Yes (Is part?whole) - the non-default value DAj
Step 4. Propagating the rules learned from an unambiguous node to the parent node:
RL ? RL ? RU
Last, the rules learned from the unambiguous examples propagate to the parent
node. They are applied last, since they are more general than the other rules. For
example, after running C4.5 on the unambiguous set for the abstraction#6?abstraction#6
ambiguous example (Node 1 in Figure 3), and eliminating the non-satisfactory rules
(see Table 9), we obtained only Rule 3: if Part is time#5 and Whole is abstraction#6 then
Yes and the default value No (see Table 11). The rule is propagated to the parent node
abstraction#6? abstraction#6 and applied last.
In the end, the rules learned from the unambiguous examples are propagated to the
parent node L. The procedure repeats until the top node of the tree is reached. After the
combination and propagation procedure finishes, the root node contains the complete
set of rules. The default value is added as a last rule, for classifying the instances that
are not captured by the rules.
A sample of the rules obtained using the ISS procedure for the genitive cluster
is shown in Table 11 in the order in which they were applied and propagated to the
abstraction#6?abstraction#6 node. Table 12 shows a translation of these rules into if?then?
else rules.
The meaning of a rule Part Class Whole Class Val is if Part is Part Class and Whole is
Whole Class, then It is a part?whole relation (Val. = Yes) or not (Val. = No). For example,
Rule 1 is if Part is a linear measure#3 and Whole is a measure#3, then It is a part?whole
relation.
5.3 Classification Rules for Each Cluster
In this section we present the classification rules learned for each cluster using the ISS
learning procedure. We also performed various experiments to study the similarities
and differences among clusters, especially to determine whether or not the classifica-
tion rules learned for a particular cluster can be applied with high accuracy to other
clusters.
A. Experiments with the genitive cluster
The most frequently used set of part?whole lexico-syntactic patterns is represented
by the genitive cluster. Tables 13 shows some of the classification rules learned for this
cluster by the ISS learning procedure in the order provided by the combination and
propagation algorithm. The full list of classification rules is shown in Tables 1 and 2
from Appendix B. The unambiguous set at level 1 of the learning tree did not generate
any rules. The rule labeled Default in Table 13 shows the learning tree global default
value (No). The tables of classification rules show only the frequency and accuracy of
the rules generated at the unambiguous nodes.
107
Computational Linguistics Volume 32, Number 1
Ta
b
le
13
A
sa
m
p
le
of
th
e
ru
le
s
le
ar
ne
d
fo
r
th
e
ge
ni
ti
ve
cl
u
st
er
.T
he
fu
ll
lis
ti
s
p
ro
vi
d
ed
in
Ta
bl
e
1,
A
p
p
en
d
ix
B
.?
V
al
.?
m
ea
ns
ta
rg
et
va
lu
e
(N
o
or
Ye
s)
,?
A
cc
.?
is
th
e
ru
le
s?
ac
cu
ra
cy
,a
nd
?F
r.?
is
th
e
fr
eq
u
en
cy
w
it
h
w
hi
ch
th
ey
oc
cu
rr
ed
.T
he
nu
m
be
ri
ng
st
yl
e
u
se
d
in
th
e
?N
o.
?
co
lu
m
n
is
in
te
nd
ed
to
in
d
ic
at
e
ru
le
s
le
ar
ne
d
at
d
if
fe
re
nt
sp
ec
ia
liz
at
io
n
le
ve
ls
.
N
o.
P
ar
tC
la
ss
W
ho
le
C
la
ss
V
al
.
A
cc
.
Fr
.
E
xa
m
p
le
ab
st
ra
ct
io
n
#6
ab
st
ra
ct
io
n
#6
N
o
gl
or
y#
2?
pa
st
#1
1
lin
ea
r
m
ea
su
re
#3
m
ea
su
re
#3
Ye
s
63
9
ce
nt
im
et
er
#1
?d
ec
im
et
er
#1
ab
st
ra
ct
io
n
#6
en
ti
ty
#1
N
o
ag
e#
1?
ea
rt
h#
1
4
sh
ap
e#
2
ar
ti
fa
ct
#1
Ye
s
po
in
t#
8?
kn
ife
#2
4.
1
sh
ap
e#
2
st
ru
ct
u
re
#1
N
o
67
.6
2
10
di
am
et
er
#2
?p
lu
g#
1
4.
2
sh
ap
e#
2
su
rf
ac
e#
1
N
o
67
.6
2
10
sq
ua
re
#1
?p
eg
bo
ar
d#
1
ab
st
ra
ct
io
n
#6
gr
ou
p
#1
N
o
hi
st
or
y#
3?
re
gi
m
en
t#
1
9
ab
st
ra
ct
io
n#
6
bi
ol
og
ic
al
gr
ou
p
#1
Ye
s
92
.4
4
10
ye
ar
#3
?m
on
ti
a#
1
10
re
la
ti
on
#1
ar
ra
ng
em
en
t#
2
Ye
s
79
.4
0
9
m
ed
iu
m
fr
eq
ue
nc
y#
1?
el
ec
tr
om
ag
ne
ti
c
sp
ec
tr
um
#1
ab
st
ra
ct
io
n
#6
p
h
en
om
en
on
#1
N
o
ca
us
e#
2?
de
at
h#
2
11
sh
ap
e#
2
p
hy
si
ca
l
p
he
no
m
en
on
#1
Ye
s
de
w
dr
op
#1
?d
ew
#1
ab
st
ra
ct
io
n
#6
p
sy
ch
ol
og
ic
al
fe
at
u
re
#1
N
o
am
ou
nt
#1
?w
or
k#
4
12
m
ea
su
re
#3
st
ru
ct
u
re
#3
Ye
s
95
.6
4
10
A
ug
us
t#
1?
G
re
go
ri
an
ca
le
nd
ar
#1
en
ti
ty
#1
p
h
en
om
en
on
#1
N
o
ke
ep
er
#2
?fl
am
e#
1
13
p
oi
nt
#2
p
hy
si
ca
l
p
he
no
m
en
on
#1
Ye
s
st
or
m
ce
nt
er
#3
?s
to
rm
#1
14
ob
je
ct
#1
p
ro
ce
ss
#2
Ye
s
fe
rr
ic
ox
id
e#
1?
ru
st
#3
p
h
en
om
en
on
#1
en
ti
ty
#1
N
o
18
p
ro
ce
ss
#2
or
ga
ni
sm
#1
Ye
s
m
ei
os
is
#1
?a
na
ps
id
#1
108
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
Ta
b
le
13
(c
on
t.)
N
o.
P
ar
tC
la
ss
W
ho
le
C
la
ss
V
al
.
A
cc
.
Fr
.
E
xa
m
p
le
18
.1
p
ro
ce
ss
#2
p
er
so
n#
1
N
o
76
.7
0
8
gr
ow
th
#2
?c
hi
ld
#2
p
h
en
om
en
on
#1
p
h
en
om
en
on
#1
N
o
in
flu
en
ce
#4
?a
ct
io
n#
6
19
na
tu
ra
l
p
he
no
m
en
on
#1
na
tu
ra
l
p
he
no
m
en
on
#1
Ye
s
m
et
eo
r#
1?
m
et
eo
r
sh
ow
er
#1
p
os
se
ss
io
n
#2
en
ti
ty
#1
N
o
co
st
#1
?h
om
e#
2
20
te
rr
it
or
y#
2
en
ti
ty
#1
Ye
s
69
.8
4
9
un
it
ed
st
at
es
vi
rg
in
is
la
nd
s#
1?
vi
rg
in
is
la
nd
s#
1
p
sy
ch
ol
og
ic
al
fe
at
u
re
#1
p
sy
ch
ol
og
ic
al
fe
at
u
re
#1
N
o
22
kn
ow
le
d
ge
d
om
ai
n#
1
kn
ow
le
d
ge
d
om
ai
n#
1
Ye
s
ag
ro
lo
gy
#1
?a
gr
on
om
y#
1
23
en
ti
ty
#1
en
ti
ty
#1
Ye
s
do
or
#4
?c
ar
#1
23
.1
ca
u
sa
l
ag
en
t#
1
ca
u
sa
l
ag
en
t#
1
N
o
le
th
al
do
se
#1
?o
pi
um
#1
23
.2
ca
u
sa
l
ag
en
t#
1
lo
ca
ti
on
#1
N
o
ta
xi
dr
iv
er
#1
?L
os
A
ng
el
es
#1
23
.3
ca
u
sa
l
ag
en
t#
1
ob
je
ct
#1
N
o
do
se
#1
?m
al
at
hi
on
#1
23
.4
p
oi
nt
#2
bo
d
y
of
w
at
er
#1
N
o
94
.4
6
10
he
ad
w
at
er
s#
1?
ni
le
#1
23
.5
re
gi
on
#1
bo
d
y
of
w
at
er
#1
N
o
91
.7
9
10
ea
st
si
de
#1
?r
iv
er
#1
23
.6
lin
e#
11
re
gi
on
#3
N
o
di
re
ct
io
n#
1?
pa
rk
#1
23
.6
.1
ad
m
in
d
is
tr
ic
t#
1
ad
m
in
d
is
tr
ic
t#
1
Ye
s
A
la
sk
a#
1?
U
ni
te
d
St
at
es
#1
23
.
D
ef
au
lt
Ye
s
do
or
#4
?c
ar
#1
26
gr
ou
p
#1
gr
ou
p
#1
Ye
s
ge
nu
s
am
oe
ba
#1
?a
m
oe
bi
da
#1
26
.1
so
ci
al
gr
ou
p
#1
p
eo
p
le
#1
N
o
di
ct
at
or
sh
ip
#1
?p
ro
le
ta
ri
at
#1
26
.2
gr
ou
p
#1
p
eo
p
le
#1
N
o
83
.8
6
8
de
m
i-
m
on
de
#1
?h
ig
h
so
ci
et
y#
1
26
.3
ar
ra
ng
em
en
t#
2
co
lle
ct
io
n#
1
N
o
82
.2
2
10
cl
as
si
fic
at
io
n#
2?
fa
m
ily
#4
26
.4
so
ci
al
gr
ou
p
#1
co
lle
ct
io
n#
1
N
o
82
.2
2
10
ci
rc
le
#2
?l
aw
#2
26
.
D
ef
au
lt
Ye
s
ge
nu
s
am
oe
ba
#1
?a
m
oe
bi
da
#1
D
ef
au
lt
N
o
109
Computational Linguistics Volume 32, Number 1
Overall, for the genitive cluster the ISS procedure obtained 27 complex sets of
classification rules.
B. Experiments with the noun compound cluster
Taking into consideration the results already obtained for the genitive cluster, there
are three possible approaches for detecting part?whole relations using the Y X and X Y
patterns:
a. [C1] Use the classification rules obtained for the genitive cluster.
b. [C1 + C2] Determine new classification rules collectively for the genitive
and noun compound clusters (Y?s X; X of Y; Y have X; and Y X).
c. [C2] Determine classification rules only for the noun compound cluster
(Y X; X Y).
Table 14 shows the results obtained for the noun compound cluster using these
three approaches. As one can observe, the best approach is to use only the classification
rules generated by the noun compound cluster training examples. The recall increases
significantly when new classification rules are learned for both the genitive and noun
compound clusters, while the precision jumps considerably when the classification rules
are learned only from the noun compound cluster examples. These statistics indicate
that the genitive and noun compound clusters encode different semantic information,
and consequently should be treated separately.
Table 15 shows the classification rules learned only for the noun compound cluster.
C. Experiments with the preposition cluster
Taking into consideration the results obtained for the previous two clusters, there
are five possible approaches for detecting part?whole relations using X prep Y and Y
prep X patterns:
a. [C1] Use the classification rules obtained for the genitive cluster.
b. [C2] Use the classification rules obtained for the noun compound cluster.
c. [C1 + C3] Determine new classification rules for all the patterns in the
genitive and preposition clusters (Y?s X; X of Y; and Y have X; Y prep X;
and X prep Y).
Table 14
The results obtained for each of the three approaches for the Y X; X Y patterns applied on the LA
Times test corpus.
Results Genitives Genitives + Noun compounds Noun compounds
(C1) (C1 + C2) (C2)
Precision 48.43% 52.98% 79.02%
Recall for cluster 58.08% 73.46% 75.33%
F-measure 52.82% 61.56% 77.13%
110
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
Ta
b
le
15
T
he
se
m
an
ti
c
cl
as
si
fi
ca
ti
on
ru
le
s
le
ar
ne
d
fo
r
th
e
no
u
n
co
m
p
ou
nd
cl
u
st
er
.?
V
al
.?
m
ea
ns
ta
rg
et
va
lu
e
(N
o
or
Ye
s)
,?
A
cc
.?
is
th
e
ru
le
s?
ac
cu
ra
cy
,a
nd
?F
r.?
is
th
e
fr
eq
u
en
cy
w
it
h
w
hi
ch
th
ey
oc
cu
rr
ed
.
N
o.
P
ar
tC
la
ss
W
ho
le
C
la
ss
V
al
.
A
cc
.
Fr
.
E
xa
m
p
le
ab
st
ra
ct
io
n
#6
ab
st
ra
ct
io
n
#6
N
o
ar
t#
4?
ad
ve
rt
is
in
g#
1
1
ti
m
e
p
er
io
d
#1
ti
m
e
p
er
io
d
#1
Ye
s
af
te
rn
oo
n#
1?
W
ed
ne
sd
ay
#1
2
m
es
sa
ge
#2
w
ri
tt
en
co
m
m
u
ni
ca
ti
on
#1
Ye
s
in
de
x#
4?
ba
ck
m
at
te
r#
1
3
w
ri
tt
en
co
m
m
u
ni
ca
ti
on
#1
m
es
sa
ge
#2
Ye
s
zi
p
co
de
#1
?a
dd
re
ss
#6
ab
st
ra
ct
io
n
#6
en
ti
ty
#1
N
o
ad
dr
es
s#
1?
re
st
au
ra
nt
#1
4
co
m
m
u
ni
ca
ti
on
#2
m
u
si
ca
l
co
m
p
os
it
io
n#
1
Ye
s
50
7
ly
ri
c#
1?
ba
lla
d#
1
5
w
ri
tt
en
co
m
m
u
ni
ca
ti
on
#1
cr
ea
ti
on
#2
Ye
s
50
7
zi
p
co
de
#1
?a
dd
re
ss
#6
ab
st
ra
ct
io
n
#6
p
sy
ch
ol
og
ic
al
fe
at
u
re
#1
N
o
th
eo
re
m
#1
?d
ec
om
po
si
ti
on
#1
6
at
tr
ib
u
te
#2
in
fo
rm
at
io
n#
3
Ye
s
50
7
he
ad
#1
0?
ab
sc
es
s#
1
ac
t#
2
gr
ou
p
#1
N
o
co
ns
ol
id
at
io
n#
2?
sc
ho
ol
#1
7
ac
t#
2
p
eo
p
le
#1
Ye
s
pr
es
id
en
t#
6?
cl
as
s#
1
en
ti
ty
#1
ab
st
ra
ct
io
n
#6
N
o
bo
ok
#1
?r
ec
ip
e#
1
8
su
rf
ac
e#
1
co
m
m
u
ni
ca
ti
on
#2
Ye
s
67
.6
2
10
he
ad
#2
7?
co
in
#1
8.
1
ho
ri
zo
nt
al
su
rf
ac
e#
1
N
o
da
is
#1
?m
ed
al
#1
en
ti
ty
#1
en
ti
ty
#1
N
o
ad
vo
ca
te
#1
?c
hi
ld
#1
9
ob
je
ct
#1
bo
d
y
of
w
at
er
#1
Ye
s
w
at
er
#1
?p
on
d#
1
10
co
ve
ri
ng
#2
in
st
ru
m
en
ta
lit
y#
3
Ye
s
96
.5
0
10
ro
of
#?
ca
r#
1
11
w
ay
#6
st
ru
ct
u
re
#1
Ye
s
90
.6
6
10
st
ai
rw
ay
#1
?b
ui
ld
in
g#
1
12
op
en
in
g#
10
ar
ti
fa
ct
#1
Ye
s
84
.3
0
10
w
in
do
w
#2
?b
us
#1
13
co
ve
ri
ng
#2
st
ru
ct
u
re
#1
Ye
s
82
.2
2
10
ro
of
#1
?b
ui
ld
in
g#
1
14
ar
ti
fa
ct
#1
co
ve
ri
ng
#2
Ye
s
67
.0
9
10
to
p#
11
?r
oo
f#
1
15
in
st
ru
m
en
ta
lit
y#
3
co
ve
ri
ng
#2
Ye
s
lo
ck
#1
?l
id
#2
16
in
st
ru
m
en
ta
lit
y#
3
in
st
ru
m
en
ta
lit
y#
3
Ye
s
ac
ce
le
ra
to
r#
1?
ca
r#
1
16
.1
co
nv
ey
an
ce
#3
in
st
ru
m
en
ta
lit
y#
3
N
o
93
.5
3
10
al
ar
m
#2
?s
ei
sm
og
ra
ph
#1
16
.2
fu
rn
is
hi
ng
s#
1
in
st
ru
m
en
ta
lit
y#
3
N
o
83
.5
6
10
st
an
d#
4?
m
ag
az
in
e#
1
16
.3
m
ea
ns
#2
in
st
ru
m
en
ta
lit
y#
3
N
o
76
.7
9
10
m
ag
az
in
e#
1?
te
le
sc
op
e#
1
111
Computational Linguistics Volume 32, Number 1
Ta
b
le
15
(c
on
t.)
N
o.
P
ar
tC
la
ss
W
ho
le
C
la
ss
V
al
.
A
cc
.
Fr
.
E
xa
m
p
le
16
.4
eq
u
ip
m
en
t#
1
in
st
ru
m
en
ta
lit
y#
3
N
o
72
.7
3
10
re
co
rd
er
#1
?p
en
#1
17
re
gi
on
#1
lo
ca
ti
on
#1
Ye
s
67
.6
2
10
bo
un
da
ry
#1
?c
it
y#
1
18
re
gi
on
#3
d
is
tr
ic
t#
1
Ye
s
50
8
ci
ty
#?
C
al
ifo
rn
ia
#
19
re
gi
on
#1
or
ga
ni
sm
#1
Ye
s
50
8
cr
ow
n#
8?
tr
ee
#1
en
ti
ty
#1
gr
ou
p
#1
N
o
m
in
e#
1?
na
vy
#1
20
ca
u
sa
l
ag
en
t#
1
p
eo
p
le
#1
Ye
s
50
9
ad
m
in
is
tr
at
or
#1
?s
ch
oo
l#
1
21
or
ga
ni
sm
#1
p
eo
p
le
#1
Ye
s
50
9
se
cr
et
ar
y#
2?
pr
es
s#
1
22
or
ga
ni
sm
#1
so
ci
al
gr
ou
p
#1
Ye
s
ch
an
ce
llo
r#
1?
un
iv
er
si
ty
#1
23
p
la
nt
#2
so
ci
al
gr
ou
p
#1
N
o
67
.6
2
10
ri
ce
#1
?U
.S
.#
1
gr
ou
p
#1
gr
ou
p
#1
N
o
go
ve
rn
m
en
t#
1?
m
ili
ta
ry
#1
24
so
ci
al
gr
ou
p
#1
se
t#
5
Ye
s
50
8
le
ad
er
#1
?p
ar
ty
#1
D
ef
au
lt
N
o
of
fic
er
#1
?n
ar
co
ti
c#
1
112
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
Table 16
The results obtained for each of the five approaches for the Y prep X and X prep Y patterns in the
preposition cluster applied on the LA Times test corpus. C1 refers to the genitive cluster, C2 to
the noun compound cluster, and C3 to the preposition cluster.
Results C1 C2 C1 + C3 C2 + C3 C3 C1 + C2 + C3
Precision 46.98% 61.54% 4.81% 36.36% 82.56% 40.74%
Recall for cluster 61.37% 54.55% 8.26% 36.36% 62.83% 15.06%
F-measure 53.25% 57.84% 6.18% 36.36% 71.36% 22.78%
d. [C2 + C3] Determine new classification rules for all the patterns in the
noun compound and preposition clusters (Y X, Y prep X and X prep Y).
e. [C3] Determine classification rules only for the preposition cluster patterns
(Y prep X and X prep Y patterns).
f. [C1 + C2 + C3] Determine new classification rules for all the patterns in all
three clusters (Y?s X; X of Y; X Y; Y X, and Y have X; Y prep X and X prep Y).
Table 16 shows the results obtained for the preposition cluster patterns in each of
the five approaches used. One can observe that the preposition cluster alone provides
the best results over all other combinations. These statistics are also consistent with the
results obtained for the noun compound cluster experiments. The best approach is to
use only the classification rules generated by the preposition cluster training examples.
Table 17 shows the classification rules learned only for the preposition cluster
patterns.
5.4 Results for Discovering Part?Whole Relations
In order to test the classification rules for the extraction of part?whole relations, we
selected two different text collections: the LA Times news articles from TREC 9 and the
Wall Street Journal (WSJ) articles from Treebank2.10 From each collection we randomly
selected 10,000 sentences that formed two distinct test corpora. This corpus was
parsed and disambiguated using a state-of-the-art domain independent Word Sense
Disambiguation system that has an accuracy of 71% when disambiguating nouns in
texts (Novischi et al 2004). In cases in which the noun constituents were not in WordNet,
we used an in-house Named Entity Recognizer (NERD) that has a 96% F-measure on
MUC6 data.
The part?whole relations extracted by the ISS system were validated by com-
paring them with the gold standard for the test set obtained through inter-annotator
agreement.
We define the precision, recall, and F-measure performance metrics in this context:
Precision =
Number of correctly retrieved relations
Number of relations retrieved
(2)
10 Treebank2 is a text collection developed at UPenn consisting of a million words of 1989 Wall Street
Journal material.
113
Computational Linguistics Volume 32, Number 1
Ta
b
le
17
T
he
se
m
an
ti
c
cl
as
si
fi
ca
ti
on
ru
le
s
le
ar
ne
d
fo
r
th
e
p
re
p
os
it
io
n
cl
u
st
er
.?
V
al
.?
m
ea
ns
ta
rg
et
va
lu
e
(N
o
or
Ye
s)
,?
A
cc
.?
is
th
e
ru
le
s?
ac
cu
ra
cy
,a
nd
?F
r.?
is
th
e
fr
eq
u
en
cy
w
it
h
w
hi
ch
th
ey
oc
cu
rr
ed
.
N
o.
P
ar
tC
la
ss
W
ho
le
C
la
ss
V
al
.
A
cc
.
Fr
.
E
xa
m
p
le
ab
st
ra
ct
io
n
#6
ab
st
ra
ct
io
n
#6
N
o
fo
rc
e#
7?
pa
st
#1
1
st
at
em
en
t#
1
sp
ee
ch
#2
Ye
s
an
no
un
ce
m
en
t#
1?
ne
w
s
co
nf
er
en
ce
#1
2
si
gn
al
#1
m
es
sa
ge
#2
Ye
s
le
tt
er
#2
?a
lp
ha
be
t#
1
3
w
ri
ti
ng
#2
w
ri
ti
ng
#2
Ye
s
ad
de
nd
um
#1
?b
ac
k
m
at
te
r#
1
4
lin
ea
r
m
ea
su
re
#1
m
ea
su
re
#3
Ye
s
50
8
in
ch
#1
?f
oo
t#
1
en
ti
ty
#1
en
ti
ty
#1
N
o
ch
ild
#2
?h
us
ba
nd
#1
5
ar
ti
fa
ct
#1
ar
ti
fa
ct
#1
Ye
s
do
or
#1
?r
oo
m
#1
5.
1
cr
ea
ti
on
#2
ar
ti
fa
ct
#1
N
o
93
.1
7
10
bo
ok
#1
?a
ud
io
ca
ss
et
te
#1
5.
2
ar
ti
fa
ct
#1
cr
ea
ti
on
#2
N
o
87
.7
4
10
ch
ar
co
al
#2
?w
at
er
co
lo
r#
1
5.
3
fa
br
ic
#1
ar
ti
fa
ct
#1
N
o
82
.2
2
10
kn
it
#1
?t
ie
#1
5.
4
ar
ti
fa
ct
#1
fl
oa
t#
4
N
o
82
.2
0
10
ou
tb
oa
rd
m
ot
or
#1
?r
af
t#
1
5.
5
ar
ti
fa
ct
#1
ex
ca
va
ti
on
#3
N
o
76
.7
9
10
ad
it
#1
?m
in
e#
1
5.
6
ar
ti
fa
ct
#1
su
rf
ac
e#
1
N
o
68
.1
3
10
ca
rg
o
co
nt
ai
ne
r#
1?
m
ai
n
de
ck
#1
5.
7
lin
e#
18
ar
ti
fa
ct
#1
N
o
67
.6
2
10
ro
pe
#1
?w
al
kw
ay
#1
5.
8
w
ay
#6
w
ay
#6
N
o
67
.6
2
10
pa
th
#2
?d
oo
r#
2
5.
9
co
nt
ai
ne
r#
1
fu
rn
is
hi
ng
s#
1
N
o
81
.4
2
10
bo
tt
le
#1
?w
ar
dr
ob
e#
1
6
na
tu
ra
l
ob
je
ct
#1
na
tu
ra
l
ob
je
ct
#1
Ye
s
pi
st
il#
1?
flo
w
er
#1
7
re
gi
on
#1
ob
je
ct
#1
Ye
s
67
.0
9
10
fo
ot
#3
?s
ho
e#
1
8
re
gi
on
#3
ar
ti
fa
ct
#1
Ye
s
50
7
se
at
#1
?h
al
l#
3
9
p
ar
t#
4
ob
je
ct
#1
Ye
s
50
7
au
to
ac
ce
ss
or
y#
1?
ca
r#
1
en
ti
ty
#1
gr
ou
p
#1
N
o
w
ea
po
n#
1?
tr
oo
p#
2
10
ca
u
sa
l
ag
en
t#
1
so
ci
al
gr
ou
p
#1
Ye
s
m
em
be
r#
1?
as
so
ci
at
io
n#
1
gr
ou
p
#1
gr
ou
p
#1
N
o
de
le
ga
ti
on
#1
?W
as
hi
ng
to
n#
3
11
p
eo
p
le
#1
so
ci
al
gr
ou
p
#1
Ye
s
yo
ut
h#
2?
hi
gh
sc
ho
ol
#1
D
ef
au
lt
N
o
au
to
m
ob
ile
#1
?g
ar
ag
e#
1
114
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
Recall =
Number of correctly retrieved relations
Number of correct relations
(3)
F ? measure = 21
Precision +
1
Recall
(4)
Tables 18 and 19 show the overall results obtained by the ISS system on the Wall
Street Journal (WSJ) and on the LA Times collections of news articles, respectively. The
results obtained for each cluster are summarized in Tables 1 and 2 in Appendix C.
Overall, on the WSJ test set the system obtained 82.87% precision and 79.09% recall
on these three clusters. Besides the 373 relations corresponding to the three clusters, 33
other meronymy relations (406 ? 373) were found in the corpus corresponding to part?
whole lexico-syntactic patterns that were not studied in this paper, giving us a global
part?whole relation coverage (recall) of 72.66%.
The ISS system?s results were compared to four baseline measures. Baseline1 shows
the results obtained by the system with no word sense disambiguation (WSD), using
only sense#1 (the most frequent sense in WordNet) for the pair of concepts. In Baseline2,
the system considered WSD and applied the specialization algorithm, but ran C4.5 only
once on all the unambiguous sets of specialized training examples representing all the
leaves of the learning tree. Baseline3 shows the results obtained without generalizing
the concepts; and Baseline4 shows the results obtained with automatic word sense
disambiguation (WSD) on the training corpus as opposed to the manual word sense
disambiguation used for ISS training.
From the baselines? results for both the WSJ and LA Times text collections, one can
see the importance of the WSD and IS?A generalization/specialization features to the
extraction of the part?whole relations.
Table 18
The number of part?whole relations obtained and the accuracy in the WSJ collection.
Results Baseline1 Baseline2 Baseline3 Baseline4 ISS
(No WSD) (One learning) (No generalization) (Using WSD System
for training)
Precision 7.72% 7.73% 15.71% 53.57% 82.87%
Pattern recall 24% 43% 2.95% 27.87% 79.09%
Relation recall 10.81% 19.37% 2.71% 25.86% 72.66%
F-measure 3.56% 6.02% 4.97% 36.67% 82.05%
Table 19
The number of part?whole relations obtained and the accuracy in the LA Times collection.
Results Baseline1 Baseline2 Baseline3 Baseline4 ISS
(No WSD) (One learning) (No generalization) (Using WSD System
for training)
Precision 2.10% 3.24% 24.34% 48.22% 79.03%
Pattern recall 11.61% 42.86% 6.25% 20.61% 85.30%
Relation recall 3.02% 11.16% 5.80% 30.05% 79.15%
F-measure 11.68% 13.1% 9.98% 28.88% 80.94%
115
Computational Linguistics Volume 32, Number 1
Figure 5 shows the learning curve where the classifier is trained on an incrementally
increasing number of training data instances. The learning curve was determined by
applying the training rules obtained through specialization on the LA Times test corpus
annotated with automatic WSD. If for 1,000 positive and 1,000 negative examples the
F-measure is only 35%, for 5,000 it increases to 70%, for 10,000 to 74%, for 15,000 to
77%, and it stabilizes at 87% for 20,000 examples. The learning curve shows that the ISS
system obtains an F-measure of about 75% with only 16.8% of the training data.
5.5 Comparison with Previous Work
In this section we compare our work with two other approaches most similar to our task
of part?whole semantic relation detection.
Berland and Charniak (1999) limit their approach to single words denoting some
entities that have recognizable parts, such as car and building. As they also observe,
this approach causes errors, such as the detection of conditioner is part of car instead of
air conditioner is part of car. Our system is considerably more knowledge intensive, but
more general in the sense that it relies on WordNet and NERD to detect both single
word and multiple word concepts in context. Moreover, their system was tested only
on a working list of predefined highly probable wholes for their corpus based on the
genitive syntactic patterns. In contrast, the ISS system can disambiguate any pair of
concepts, provided they are in WordNet or can be classified by NERD.
In order to eliminate a part of the data ambiguities, Berland and Charniak apply
an ad hoc filtering procedure to eliminate those instances that represent properties or
qualities of objects, such as those ending in -ing, -ity, and -ness. Our procedure is general
enough to treat both positive and negative example instances.
Using the genitive patterns they find parts of a predefined list of wholes from a
large text collection. Our method, however, determines if two noun concepts are in a
part?whole relation or not. By generalizing the method to all the parts and wholes from
our testing corpus, the accuracy of the system will fall. On the other hand, to be able
to test the system on their six whole concepts we would need thousands of positive
and negative examples for each such word. For instance, for the word book, Berland
Figure 5
The learning curve for the number of learning examples.
116
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
and Charniak had almost 2,000 examples for the top 50 ranked parts. Unfortunately,
in our LA Times testing corpus we couldn?t find more than ten parts for each of their
proposed whole objects. Therefore, we are unable to replicate their work using our text
collection.
ISS algorithm is based on an iterative semantic specialization method that allows
us to go deeper into the semantic complexity problem of the patterns considered. To
the best of our knowledge, ISS is the only noun-phrase interpretation system that
uses word sense disambiguation. One other noun compound interpretation sys-
tem, SENS (Vanderwende 1995), used IS?A generalizations, and considered only the
first sense of the noun constituents. The current state-of-the-art approaches in auto-
matic detection of semantic roles (Gildea and Jurafsky 2002) have tried to use lexico-
semantic hierarchies, such as WordNet, to generalize from lexical noun features.
However, they also rely on the first sense listed for each noun occurring in the train-
ing data. Our experiments indicate the importance of WSD in extracting part?whole
semantic relations.
6. Limitations and Extensions
The difficulty of detecting part?whole relations is due to a variety of factors ranging
from syntactic analysis, to semantic and pragmatic information. In this section we
analyze the sources of errors occurring in our experiments and present some possible
improvements.
To arrive at an interpretation of the pair of words selected by the cluster patterns, it
is first necessary to identify that both words are nouns, and not other parts of speech.
For example, if Brill?s tagger mis-tags an adjective or verb as a noun, then the ISS system
will also be affected.
Our classification rule learning approach is based on the WordNet semantic classes
of the two concepts that represent the part and the whole, respectively. Thus, if the
WSD system fails to annotate the concepts with the correct senses, the ISS system
can generate wrong semantic classes, which leads to wrong conclusions. For example,
the WordNet concept end has 14 senses corresponding to 6 semantic classes (entity,
abstraction, event, psychological feature, state, and act) (see Table 20). However, not all
the senses refer to a part?whole relation (e.g., senses 4, 6, 8, 9, 11, and 14 do not).
Some senses corresponding to both positive and negative examples are mapped into
the same semantic class (e.g., senses 7 and 8). In this case, the classification error will
not affect the final result as it is eliminated in the specialization phase. However, when
a part?whole sense of end is mapped erroneously into a semantic class that is represent-
ative of negative examples, then the error might propagate to the final classification
rule.
For some words, WordNet does not have all their senses. For example, the concepts
import and export are not listed in WordNet as denoting the act of importing/exporting
commodities from a foreign country. Thus, relations such as import of sweater and export
of milk are mis-classified. Similar examples are participant and beneficiary for which
WordNet lists only the senses corresponding to people and not to other entities, such
as countries (e.g., a country can be one of the participants at a NATO meeting).
When a noun is too specific to be found in WordNet, we rely on a named entity
recognizer (NERD). NERD identifies people, organizations, and other information ex-
traction categories and annotates them accordingly. However, NERD doesn?t always
provide the correct annotation. For example, in the phrase attorney of York, it identifies
117
Computational Linguistics Volume 32, Number 1
Table 20
The semantic classes and part?whole status for all the senses of the concept end in WordNet.
Sense Semantic Class Part?Whole Examples
No. Relation?
1 entity Yes end of line
2 abstraction Yes end of year
3 event Yes end of movie
4 psychological feature No the end justifies the means
5 psychological feature Yes end of section
6 state No glorious end of the experiment
(sense of destruction, death)
7 entity Yes end of box
8 entity No end hold the pass
9 entity No both ends wrote encouraging thoughts
10 entity Yes end of town
11 act No -
12 abstraction Yes In the end of the presentation
13 entity Yes the end of the cloth
14 act No the end from the line of scrimmage
York as a name of a person and tags it with sense#1. However, York#1 is defined in
WordNet as the House of York, the English royal house that reigned from 1461 to 1485.
Consequently, the ISS system will consider York#1 a group instead of an entity, yielding
an erroneous result.
The WSD tool identifies noun compounds and annotates them with the correspond-
ing WordNet sense. For instance in the sentence ?... by/IN simply/RB/1 redesigning/VBG/1
how/WRB/1 a/DT car door/NN/1 is/VBZ assembled/VBN/1? the system annotated the
concept car door with its WordNet sense (sense 1). This way, the ISS system considers the
two words as a concept and not as a noun compound encoding a part?whole relation.
The majority of noun compounds from the test corpus are names of people (e.g., Andrea
West, Mr. Moore), dates (e.g., Oct 12, Monday afternoon), names of institutions (e.g., Bank
of America, Planters Corp., Research Inc., Johnson & Johnson), or numbers (e.g., six days, five
years). After analyzing the ambiguous pairs of nouns in noun compound instances, we
noticed that only a few of them were positive examples. This error can be easily fixed
by disabling the labeling of noun compounds with word senses.
Another class of errors involves the position of part and whole concepts. For exam-
ple, the part?whole instance band#1 of people#1 is detected by the pattern NPX of NPY
and the system classifies erroneously band as part, and people as whole. One way to
overcome this is to further classify the patterns based on selectional restrictions on their
constituent nouns (e.g., group nouns in of?genitives have different positions for the part
and whole concepts).
We present in Table 21 the types of errors and their frequency of occurrence for each
cluster and overall.
Although our approach takes context into account through the use of word sense
disambiguation, it does so in a limited way, without access to the general discourse
and pragmatic context within which a pair of nouns is embedded. Various researchers
(Spa?rck Jones 1983; Lascarides and Copestake 1998; Lapata 2002) showed that the
interpretation of noun compounds, for example, may be influenced by discourse and
pragmatic knowledge. For instance, the discourse context provided by the following
118
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
Table 21
Error types statistics measured on the Wall Street Journal corpus for the ISS system.
Clusters
Error Type Genitives Noun Preposition All Error (%)
compounds
WSD System 57 36 26 119 53.85%
NERD module 11 5 9 25 11.31%
Brill?s tagger 7 4 8 19 8.6%
Missing WordNet sense 10 1 7 18 8.14%
Part and Whole identification 5 10 0 15 6.79%
Classification rules 7 2 5 14 6.33%
Unseen examples
Noun compound annotation 0 9 2 11 4.98%
Total 97 67 57 221 100%
sentences prefers the PURPOSE interpretation (bag for cotton clothes) of the noun com-
pound cotton bag over the PART?WHOLE meaning (bag made of cotton) (cf. (Lapata 2002)):
(5) Mary sorted her clothes into various bags made from plastic.
(6) She put her skirt into the cotton bag.11
Encoding discourse knowledge is thus necessary. However, this is an open research
problem and involves considerable manual annotation effort.
Furthermore, our experiments focused on the detection of part?whole relations
in compositional constructions. A more general approach would consider lexicalized
instances as well. Pragmatic knowledge is particularly important for the interpretation
of lexicalized constructions, such as soap opera. The meaning of lexicalized instances is
usually captured by semantic lexicons and dictionaries.
Finally, the approach presented here can be extended to other semantic relations
encoded by the cluster patterns considered. The only part?whole elements used in this
algorithm were the patterns and the examples. Thus the learning and the validation
procedures are generally applicable and we intend to generalize the method for the
detection of other semantic relations, such as KINSHIP and PURPOSE. So far, we have
obtained encouraging results for a list of 35 general-purpose semantic relations encoded
by genitives (Moldovan and Badulescu 2005), by noun compounds (Girju et al 2005),
and different noun phrase-level patterns including genitives, noun compounds, and the
preposition patterns (Moldovan et al 2004).
The drawback of the method presented here, as for other very precise learning
methods, is that the number of training examples needs to be very large. If a certain
class of negative or positive examples is not seen in the training data (and therefore it is
not captured by the classification rules), the system cannot classify its instances. Thus,
the larger and more diverse the training data, the better the classification rules.
11 These sentences were introduced in (Lapata 2002).
119
Computational Linguistics Volume 32, Number 1
Table 22
The components of the AH?64A Apache Helicopter found on Web documents.
AH?64A Apache Helicopter
Hellfire air-to-surface missile
millimeter wave seeker
70mm Folding Fin Aerial rocket
30mm Cannon camera
armaments
General Electric 1700-GE engine
4-rail launchers
four-bladed main rotor
anti-tank laser guided missile
Longbow millimetre wave fire control radar
integrated radar frequency interferometer
rotating turret
tandem cockpit
Kevlar seats
7. Importance to NLP Applications
Since part?whole semantic relations occur frequently in text and have been recognized
as fundamental ontological relations since ancient times, their discovery is paramount
for applications such as Question Answering, automatic ontology construction, textual
inferencing, and others. For questions like What parts does General Electric manufacture?,
What are the components of X, What is Y made of?, and many more, the discovery of part?
whole relations is necessary to assemble the right answer.
The concepts and part?whole relations acquired from a collection of documents
can be useful in answering difficult questions that normally can not be handled based
solely on keyword matching and proximity. As the level of difficulty increases, Question
Answering systems need richer semantic resources, including ontologies and larger
knowledge bases. Consider the question What does the AH?64A Apache helicopter consist
of? For questions like this, the system must extract all the components the war helicopter
has. Unless an ontology of such army attack helicopter parts exists in the knowledge
base, which in an open domain situation is highly unlikely, the system must first acquire
from the document collection all the pieces the helicopter is made of. These parts
can be scattered all over the text collection, so the Question Answering system has to
gather together these partial answers into a single and concise hierarchy of parts. This
technique is called answer fusion (Girju 2001).
Using a state-of-the-art Question Answering system (Moldovan et al 2002) adapted
for answer fusion and including the ISS system as a module, the question presented
above was answered by searching the Internet (the website for the Defence Industries?
army at www.army-technology.com). The QA system started with the question focus
helicopter and extracted and disambiguated all the meronymy relations using the ISS
module. Table 22 shows the taxonomic ontology created for this question (presenting
all the parts of a whole).
For example, the relation ?AH?64 Apache helicopter has part Hellfire air-to-surface
missile? was determined from the sentence AH?64 Apache helicopter has a Longbow-
millimetre wave fire control radar and a Hellfire air-to-surface missile. Only the heads of the
noun phrases were considered as they occur in WordNet (i.e., helicopter and air-to-surface
missile, respectively).
120
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
Ontologies12 are used more and more as means to boost the accuracy of natural
language application systems (Moldovan and Girju 2001). Semantically richer ontolo-
gies can be built by incorporating more semantic relations in addition to the traditional
IS?A relation. Part?whole is an excellent example of such relations. Recently, Tatu
and Moldovan (2005) have shown that semantic relations such as part?whole can be
combined with other relations using a semantic calculus for the purpose of improving
the performance of a textual inference system.
8. Conclusions
In this paper we presented a supervised, knowledge-intensive approach to the auto-
matic detection of part?whole relations encoded by the three most frequent clusters of
syntactic constructions: (1) genitives and NP have NP clauses, (2) noun compounds, and
(3) other NP PP phrases. The detection of the part?whole relations is difficult due to
the highly ambiguous nature of the syntactic constructions, as they can encode other
relations than meronymy.
Our method for detection of part?whole relations discovers semi-automatically the
part?whole lexico-syntactic patterns and learns automatically the semantic classifica-
tion rules needed for the disambiguation of these patterns. We defined the task as a
binary classification problem and used an approach that relies on the assumption that
the semantic relation between two constituent nouns representing the part and the
whole can be detected based on the components? semantic classification rules. The clas-
sification rules are learned automatically through an iterative semantic specialization (ISS)
procedure applied on the noun constituents? semantic classes provided by WordNet.
We successfully combined the results of decision tree learning with the WordNet IS-A
hierarchy specialization for more accurate learning. We proved the method is domain
independent.
The classification rules learned by our method and listed in several tables can be
easily implemented to extract part?whole relations from text. However, to apply these
rules a word sense disambiguation system for nouns is necessary.
Our experiments revealed the importance of word sense disambiguation and Word-
Net IS?A specialization. We have directly compared and contrasted the results of our
system with a variety of baselines and have shown impressive results. Combination of
word sense disambiguation information with IS-A semantic information in WordNet
yields better performance over either WSD or IS-A specialization alone.
Our experiments also showed that the three cluster patterns considered are not al-
ternative ways of encoding part?whole information. This observation is very important
for various text understanding applications.
Moreover, the approach presented can be extended to other semantic relations since
the learning procedures are generally applicable and yield good results for sufficiently
large training corpora.
12 Gartner Group identified Ontologies as one of the leading IT technologies, ranked 3rd in its list of top 10
technologies forecast for 2005.
121
Computational Linguistics Volume 32, Number 1
Appendix A: Experiments with Meronymic Patterns
Tables 1, 2, and 3 present a summary of phrase-level and sentence-level meronymic
patterns and their possible extensions.
Table 1
The phrase-level patterns determined with the pattern identification procedure in Section 3.
?Fr.? means frequency.
No. Pattern Fr. Example
1 NPX PPY 173 door of his car
? PPY starts with of the executive of the new government
2 NPX PPY 61 people in the world
? PPY starts with in
3 NPX PPY 14 They organized the executive branch of government
? NPX ends with branch
? PPY begins with of
4 NPX PPY 9 oxygen from air
? PPY starts with from people from all over the world
5 NPX PPY 6 people throughout the world
? PPY starts with throughout
6 NPX PPY 5 window at the rear of the building
? PPY starts with at
7 NPX PPY 16 five fingers on one hand
? PPY starts with on
8 NPX PPY 3 the door to the house
? PPY starts with to
9 NPX PPY 2 people around the world
? PPY starts with around
10 NPX PPY 1 people all over the world
? PPY starts with all over
11 X and other Z of Y 1 in Romania and the other countries of Eastern Europe
12 NPX PPY 1 severe ligament damage to the left knee
? PPY starts with to
? NPX ends with damage
13 NPX PPY 1 pavement onto streets
? PPY starts with onto
14 NPX PPY 1 windows outside the court building
? PPY starts with outside
15 NPY PPX 7 the organization with 120 members
? PPX starts with with The spiders with 6 legs are dangerous.
16 NPY PPX 4 They amputate his leg above the knee.
? PPX starts with above
17 NPY?s NPX 71 car?s engine
organization?s membership
18 PPX PPY 2 in an abdomen, in a reclining torso
? PPX starts with in
? PPY starts with in
19 PPY PPX 1 on her car, on her window
? PPX starts with on
? PPY starts with on
20 NPX, NPY 10 Bucharest, Romania
in Atlanta ,Ga.
21 PPY WHPX 4 The club whose membership
? WHPX starts with whose
122
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
Table 1
(cont.)
No. Pattern Fr. Example
22 NPX1X2 PPY 8 between the executive and legislative branches
of government
? NPX1X2 ends with branches
? NPX1X2 contains and or or
? PPY begins with of
23 NPX1X2 PPY 1 the memory and other features of IBM-compatible
personal computers.
? NPX1X2 contains and or or
? PPY begins with of
24 NPY (NPX1X2) 1 the three states of Southern New England
(Massachusetts, Connecticut, and Rhode Island)
? NPX1X2 contains and or or
25 NPX (NPY) 1 red-bellied snake (Storeria)
26 NPY NPX 42 He sell car doors
27 NPZ?NPX NPY 26 a one?act ballet
28 NPY NPX NPZ 12 faulty garage door lock
computer memory chip
four?door compact car
29 NPX NPY 3 membership organization
power window buildings
30 NPX?NPY NPZ 3 a play?act universe
123
Computational Linguistics Volume 32, Number 1
Ta
b
le
2
T
he
se
nt
en
ce
-l
ev
el
p
at
te
rn
s
d
et
er
m
in
ed
w
it
h
th
e
p
at
te
rn
id
en
ti
fi
ca
ti
on
p
ro
ce
d
u
re
in
Se
ct
io
n
3.
?F
r.?
m
ea
ns
fr
eq
ue
nc
y.
N
o.
P
at
te
rn
Fr
.E
xa
m
p
le
1
N
P
Y
ve
rb
N
P
X
18
A
ca
r
ha
s
w
he
el
s.
-v
er
bs
:c
ar
ry
,c
om
bi
ne
,c
om
pr
eh
en
d,
co
m
pr
is
e,
T
he
ca
ke
co
nt
ai
ns
fr
es
h
fr
u
it
s.
co
ns
is
t,
co
nt
ai
n
,e
nc
lo
se
,f
ea
tu
re
,h
av
e,
ho
ld
,
A
ny
ca
r
in
cl
ud
es
a
sp
ar
e
ti
re
.
ho
ld
in
,h
ou
se
,i
nc
lu
de
,i
nc
or
po
ra
te
,i
nh
er
it
,
T
he
p
at
ie
nt
re
ce
iv
ed
a
ne
w
he
ar
t.
in
te
gr
at
e,
re
ce
iv
e,
re
ta
in
,s
ub
su
m
e
2
N
P
Z
ve
rb
N
P
X
P
P
Y
13
T
he
y
co
ns
tr
uc
te
d
th
e
ca
r
fr
om
en
gi
ne
,d
oo
rs
,w
he
el
s.
-P
P
Y
st
ar
ts
w
it
h
in
,i
nt
o,
as
or
fr
om
T
he
p
ri
ce
in
cl
ud
es
a
m
em
be
rs
hi
p
in
a
go
od
cl
u
b.
-v
er
bs
:a
ss
em
bl
e,
bu
ild
,b
ui
ld
in
,c
ar
ry
,c
om
bi
ne
,
T
he
sy
st
em
ad
m
in
is
tr
at
or
co
nn
ec
tt
he
co
m
p
u
te
rs
co
m
po
se
,c
om
po
un
d,
co
m
pr
eh
en
d,
co
m
pr
is
e,
in
to
a
co
m
p
u
te
r
ne
tw
or
k.
co
nn
ec
t,
co
ns
is
t,
co
ns
tr
uc
t,
co
nt
ai
n
,c
oo
rd
in
at
e,
T
he
st
at
e
us
es
th
es
e
so
ld
ie
rs
as
th
e
m
ai
n
ar
m
y.
cr
ea
te
,e
m
br
ac
e,
en
cl
os
e,
en
te
r,
fa
br
ic
at
e,
fe
at
ur
e,
T
he
co
lo
ne
lo
rg
an
iz
ed
th
e
so
ld
ie
rs
in
to
an
el
it
e
ar
m
y.
fil
e,
fo
rm
,h
av
e,
ho
ld
,h
ol
d
in
,h
ou
se
,i
nc
lu
de
,
T
he
u
se
r
in
se
rt
s
th
e
fi
le
in
to
hi
s
d
ir
ec
to
ry
.
in
co
rp
or
at
e,
in
fix
,i
nh
er
it
,i
ns
er
t,
in
te
gr
at
e,
T
he
p
ro
gr
am
m
er
in
cl
ud
es
th
e
m
ai
n
p
ro
ce
d
u
re
in
in
tr
od
uc
e,
jo
in
,l
in
k,
m
ak
e,
m
an
uf
ac
tu
re
,m
er
ge
,
th
e
C
so
u
rc
e
fi
le
.
ob
se
rv
e,
or
ga
ni
ze
,o
ve
rl
ap
,r
ec
ei
ve
,r
et
ai
n,
su
bs
um
e,
un
ify
,u
ni
te
,u
se
3
N
P
Z
ve
rb
P
P
X
P
P
Y
2
T
he
y
dr
ag
ge
d
hi
m
ou
to
ft
he
ca
r
-P
P
Y
st
ar
ts
w
it
h
th
ro
ug
h
th
ro
ug
h
th
e
w
in
d
ow
.
-v
er
bs
:d
ra
g,
ex
it
,l
ea
ve
4
N
P
X
ve
rb
N
P
Y
1
T
he
m
em
be
r
jo
in
ed
th
e
or
ga
ni
za
ti
on
in
19
76
.
-v
er
bs
:a
cc
om
m
od
at
e,
ad
d,
ad
m
it
,a
ffi
lia
te
,
T
he
ox
yg
en
co
m
po
se
s
th
e
ai
r.
ap
pe
rt
ai
n,
be
,b
ea
r,
be
lo
ng
,b
ui
ld
in
,c
ol
lig
at
e,
T
he
m
an
in
fil
tr
at
es
th
e
or
ga
ni
za
ti
on
in
19
99
.
co
m
po
se
,c
om
po
un
d,
co
nfi
ne
,c
on
st
it
ut
e,
dw
el
l,
em
br
ac
e,
en
co
m
pa
ss
,f
al
li
n,
fo
rm
,g
et
to
ge
th
er
,
in
fil
tr
at
e,
in
he
re
,i
nv
ol
ve
,j
oi
n,
le
ti
n,
lie
,l
ie
in
,
m
ak
e,
m
ak
e
up
,p
er
ta
in
,r
ej
oi
n,
re
po
se
,r
ep
re
se
nt
,
re
si
de
,r
es
t,
si
gn
up
,t
ak
e
124
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
Ta
b
le
2
(c
on
t.)
N
o.
P
at
te
rn
Fr
.E
xa
m
p
le
5
N
P
X
ve
rb
P
P
Y
1
T
he
cy
to
p
la
sm
in
he
re
in
a
ce
ll.
-v
er
bs
:a
tt
ac
he
d
to
,i
nh
er
e
in
(P
P
Y
st
ar
ts
w
it
h
in
or
to
)
6
N
P
X
ve
rb
N
P
Z
P
P
Y
2
T
he
en
gi
ne
is
a
pa
rt
of
a
ca
r.
-P
P
Y
st
ar
ts
w
it
h
of
A
ro
se
is
a
m
em
be
r
of
ge
nu
s
R
os
a.
-N
P
Z
is
p
ar
to
r
m
em
be
r
7
N
P
X
N
P
Y
ve
rb
1
T
he
he
ad
lig
ht
s
th
e
ca
r
ha
d
w
er
e
bl
u
e.
-v
er
bs
:c
ar
ry
,c
om
bi
ne
,c
om
pr
eh
en
d,
co
m
pr
is
e,
co
ns
is
t,
co
nt
ai
n
,e
nc
lo
se
,f
ea
tu
re
,
ha
ve
,h
ol
d,
ho
ld
in
,h
ou
se
,i
nc
lu
de
,i
nc
or
po
ra
te
,
in
he
ri
t,
in
te
gr
at
e,
re
ce
iv
e,
re
ta
in
,s
ub
su
m
e
8
N
P
Y
1
ve
rb
N
P
X
to
N
P
Y
2
1
T
he
m
an
do
na
te
d
on
e
of
hi
s
ki
d
ne
ys
to
hi
s
si
st
er
.
-v
er
bs
:d
on
at
e,
gi
ve
9
In
P
P
Y
,N
P
X
1
ve
rb
N
P
X
2
1
In
a
ca
r,
th
e
ca
r
bo
d
y
co
ve
rs
th
e
en
gi
ne
.
-v
er
bs
:c
ov
er
in
N
P
Y
p
ac
ke
d
to
N
P
X
1
..
.i
n
a
ca
r
pa
ck
ed
to
th
e
w
in
d
ow
s
w
it
h
p
er
so
na
lb
el
on
gi
ng
s.
..
10
N
P
Z
P
P
X
ve
rb
N
P
T
P
P
Y
1
In
fa
nt
m
or
ta
lit
y
in
R
om
an
ia
is
no
w
th
e
hi
gh
es
t
?P
P
X
st
ar
ts
w
it
h
in
in
E
u
ro
p
e.
?N
P
T
co
nt
ai
ns
an
ad
je
ct
iv
e
su
p
er
la
ti
ve
-v
er
b:
be
125
Computational Linguistics Volume 32, Number 1
Table 3
Extensions for lexico-syntactic patterns discovered in the 20,000 sentence corpus used in the
pattern identification procedure in Section 3.
No. Pattern Example
1 NPY PPX A bird without wings cannot fly.
- PPY starts with without
2 NPX PPY X inside Y The walls inside the building had better colors.
- PPY starts with inside
126
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
A
p
p
en
d
ix
B
:S
em
an
ti
c
C
la
ss
ifi
ca
ti
on
R
u
le
s
fo
r
th
e
G
en
it
iv
e
C
lu
st
er
Ta
bl
es
1
an
d
2
sh
ow
th
e
fu
ll
lis
to
fs
em
an
ti
c
cl
as
si
fi
ca
ti
on
ru
le
s
le
ar
ne
d
fo
r
th
e
ge
ni
ti
ve
cl
u
st
er
fr
om
al
lt
he
am
bi
gu
ou
s
no
d
es
.
Ta
b
le
1
T
he
se
m
an
ti
c
cl
as
si
fi
ca
ti
on
ru
le
s
le
ar
ne
d
fo
r
th
e
ge
ni
ti
ve
cl
u
st
er
al
la
m
bi
gu
ou
s
no
d
es
(d
ef
au
lt
va
lu
e
N
o)
.?
V
al
.?
is
th
e
ta
rg
et
va
lu
e,
?A
cc
.?
is
th
e
ru
le
s?
ac
cu
ra
cy
,a
nd
?F
r.?
is
th
ei
r
oc
cu
rr
en
ce
fr
eq
u
en
cy
.T
he
in
d
en
ta
ti
on
s
in
th
e
?N
o.
?
co
lu
m
n
re
fe
r
to
ru
le
s
at
d
if
fe
re
nt
sp
ec
ia
liz
at
io
n
le
ve
ls
.
N
o.
P
ar
tC
la
ss
W
ho
le
C
la
ss
V
al
.
A
cc
.
Fr
.
E
xa
m
p
le
ab
st
ra
ct
io
n
#6
ab
st
ra
ct
io
n
#6
N
o
gl
or
y#
2?
pa
st
#1
1
lin
ea
r
m
ea
su
re
#3
m
ea
su
re
#3
Ye
s
63
9
ce
nt
im
et
er
#1
?d
ec
im
et
er
#1
2
co
m
m
u
ni
ca
ti
on
#2
co
m
m
u
ni
ca
ti
on
#2
Ye
s
ac
t#
3?
pl
ay
#1
2.
1
w
ri
tt
en
co
m
m
u
ni
ca
ti
on
#1
w
ri
tt
en
co
m
m
u
ni
ca
ti
on
#1
N
o
te
xt
#1
?a
ct
#3
2.
1.
1
w
ri
ti
ng
#2
w
ri
ti
ng
#2
Ye
s
N
ew
Te
st
am
en
t#
1?
B
ib
le
#1
2.
1.
1.
1
m
at
te
r#
6
N
o
79
.9
8
9
te
xt
#1
?a
ct
#3
2.
2
in
d
ic
at
io
n#
1
m
es
sa
ge
#2
N
o
73
.2
5
10
co
py
#1
?r
ec
om
m
en
da
ti
on
#1
2.
3
m
es
sa
ge
#2
co
m
m
u
ni
ca
ti
on
#2
N
o
79
.7
2
8
ir
on
y#
1?
pl
ay
#1
2.
4
ti
m
e#
5
ab
st
ra
ct
io
n#
6
Ye
s
79
.2
1
9
ca
rb
on
ife
ro
us
#1
?p
al
eo
zo
ic
#1
ab
st
ra
ct
io
n
#6
en
ti
ty
#1
N
o
ag
e#
1?
ea
rt
h#
1
4
sh
ap
e#
2
ar
ti
fa
ct
#1
Ye
s
po
in
t#
8?
kn
ife
#2
4.
1
sh
ap
e#
2
st
ru
ct
u
re
#1
N
o
67
.6
2
10
di
am
et
er
#2
?p
lu
g#
1
4.
2
sh
ap
e#
2
su
rf
ac
e#
1
N
o
67
.6
2
10
sq
ua
re
#1
?p
eg
bo
ar
d#
1
5
m
ea
su
re
#3
ob
je
ct
#1
Ye
s
dr
um
st
ic
k#
1?
bi
rd
#2
5.
1
d
efi
ni
te
qu
an
ti
ty
#1
ar
ti
fa
ct
#1
N
o
do
ze
n#
1?
vi
de
ot
ap
e#
1
5.
2
in
d
efi
ni
te
qu
an
ti
ty
#1
ar
ti
fa
ct
#1
N
o
lo
t#
1?
th
ro
tt
le
#1
5.
3
lin
ea
r
m
ea
su
re
#1
ar
ti
fa
ct
#1
N
o
m
ile
#1
?q
ua
rt
er
s#
1
5.
4
sy
st
em
of
m
ea
su
re
m
en
t#
1
ob
je
ct
#1
N
o
82
.2
6
8
ba
nd
w
id
th
#1
?r
ec
ei
ve
r#
1
5.
5
re
la
ti
ve
qu
an
ti
ty
#1
ob
je
ct
#1
N
o
79
.7
2
8
no
th
in
g#
1?
re
fr
ig
er
at
or
#1
6
p
os
it
io
n#
7
ar
ti
fa
ct
#1
Ye
s
ci
rc
le
#6
?t
he
at
er
#1
6.
1
p
la
ce
m
en
t#
1
N
o
67
.6
2
10
de
ns
it
y#
2?
pa
tt
er
n#
3
7
w
ri
tt
en
co
m
m
u
ni
ca
ti
on
#1
in
st
ru
m
en
ta
lit
y#
3
Ye
s
85
.7
0
10
by
-l
in
e#
1?
w
ri
ti
ng
ar
m
#1
8
sh
ap
e#
2
lo
ca
ti
on
#1
Ye
s
66
.5
6
10
po
in
t#
8?
ar
ro
w
he
ad
#1
ab
st
ra
ct
io
n
#6
gr
ou
p
#1
N
o
hi
st
or
y#
3?
re
gi
m
en
t#
1
127
Computational Linguistics Volume 32, Number 1
Ta
b
le
1
(c
on
t.)
N
o.
P
ar
tC
la
ss
W
ho
le
C
la
ss
V
al
.
A
cc
.
Fr
.
E
xa
m
p
le
9
ab
st
ra
ct
io
n#
6
bi
ol
og
ic
al
gr
ou
p
#1
Ye
s
92
.4
4
10
ye
ar
#3
?m
on
ti
a#
1
10
re
la
ti
on
#1
ar
ra
ng
em
en
t#
2
Ye
s
79
.4
0
9
m
ed
iu
m
fr
eq
ue
nc
y#
1?
el
ec
tr
om
ag
ne
ti
c
sp
ec
tr
um
#1
ab
st
ra
ct
io
n
#6
p
h
en
om
en
on
#1
N
o
ca
us
e#
2?
de
at
h#
2
11
sh
ap
e#
2
p
hy
si
ca
l
p
he
no
m
en
on
#1
Ye
s
de
w
dr
op
#1
?d
ew
#1
ab
st
ra
ct
io
n
#6
p
sy
ch
ol
og
ic
al
fe
at
u
re
#1
N
o
am
ou
nt
#1
?w
or
k#
4
12
m
ea
su
re
#3
st
ru
ct
u
re
#3
Ye
s
95
.6
4
10
A
ug
us
t#
1?
G
re
go
ri
an
ca
le
nd
ar
#1
en
ti
ty
#1
p
h
en
om
en
on
#1
N
o
ke
ep
er
#2
?fl
am
e#
1
13
p
oi
nt
#2
p
hy
si
ca
l
p
he
no
m
en
on
#1
Ye
s
st
or
m
ce
nt
er
#3
?s
to
rm
#1
14
ob
je
ct
#1
p
ro
ce
ss
#2
Ye
s
fe
rr
ic
ox
id
e#
1?
ru
st
#3
ev
en
t#
1
en
ti
ty
#1
N
o
15
p
er
io
d
ic
ev
en
t#
1
ob
je
ct
#1
Ye
s
66
.5
6
10
w
av
e#
3?
w
av
eg
ui
de
#1
ev
en
t#
1
ev
en
t#
1
N
o
re
ru
n#
1?
te
le
vi
si
on
sh
ow
#1
16
ha
p
p
en
in
g#
1
p
er
io
d
ic
ev
en
t#
1
Ye
s
58
.1
2
8
flo
od
#6
?fl
oo
d
ti
de
#2
p
h
en
om
en
on
#1
ab
st
ra
ct
io
n
#6
N
o
om
is
si
on
#3
?p
ro
no
un
#1
17
at
m
os
p
he
ri
c
p
he
no
m
en
on
#1
co
m
m
u
ni
ca
ti
on
#2
Ye
s
ge
nt
le
br
ee
ze
#1
?
be
au
fo
rt
sc
al
e#
1
p
h
en
om
en
on
#1
en
ti
ty
#1
N
o
18
p
ro
ce
ss
#2
or
ga
ni
sm
#1
Ye
s
m
ei
os
is
#1
?a
na
ps
id
#1
18
.1
p
ro
ce
ss
#2
p
er
so
n#
1
N
o
76
.7
0
8
gr
ow
th
#2
?c
hi
ld
#2
p
h
en
om
en
on
#1
p
h
en
om
en
on
#1
N
o
in
flu
en
ce
#4
?a
ct
io
n#
6
19
na
tu
ra
l
p
he
no
m
en
on
#1
na
tu
ra
l
p
he
no
m
en
on
#1
Ye
s
m
et
eo
r#
1?
m
et
eo
r
sh
ow
er
#1
p
os
se
ss
io
n
#2
en
ti
ty
#1
N
o
co
st
#1
?h
om
e#
2
20
te
rr
it
or
y#
2
en
ti
ty
#1
Ye
s
69
.8
4
9
un
it
ed
st
at
es
vi
rg
in
is
la
nd
s#
1?
vi
rg
in
is
la
nd
s#
1
p
os
se
ss
io
n
#2
p
os
se
ss
io
n
#2
N
o
liq
ui
d
as
se
ts
#1
?c
ap
it
al
#1
21
as
se
ts
#1
tr
an
sf
er
re
d
p
ro
p
er
ty
#1
Ye
s
cu
t#
6?
lo
ot
#1
p
sy
ch
ol
og
ic
al
fe
at
u
re
#1
p
sy
ch
ol
og
ic
al
fe
at
u
re
#1
N
o
22
kn
ow
le
d
ge
d
om
ai
n#
1
kn
ow
le
d
ge
d
om
ai
n#
1
Ye
s
ag
ro
lo
gy
#1
?a
gr
on
om
y#
1
128
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
Ta
b
le
2
T
he
se
m
an
ti
c
cl
as
si
fi
ca
ti
on
ru
le
s
le
ar
ne
d
fo
r
th
e
ge
ni
ti
ve
cl
u
st
er
fr
om
al
lt
he
am
bi
gu
ou
s
no
d
es
w
it
h
d
ef
au
lt
va
lu
e
Ye
s.
?V
al
.?
m
ea
ns
ta
rg
et
va
lu
e
(N
o
or
Ye
s)
,?
A
cc
.?
is
th
e
ru
le
s?
ac
cu
ra
cy
,a
nd
?F
r.?
is
th
e
fr
eq
u
en
cy
w
it
h
w
hi
ch
th
ey
oc
cu
rr
ed
.T
he
nu
m
be
ri
ng
st
yl
e
u
se
d
in
th
e
?N
o.
?
co
lu
m
n
is
in
te
nd
ed
to
in
d
ic
at
e
ru
le
s
le
ar
ne
d
at
d
if
fe
re
nt
sp
ec
ia
liz
at
io
n
le
ve
ls
.
N
o.
P
ar
tC
la
ss
W
ho
le
C
la
ss
V
al
.
A
cc
.
Fr
.
E
xa
m
p
le
23
en
ti
ty
#1
en
ti
ty
#1
Ye
s
do
or
#4
?c
ar
#1
23
.1
ca
u
sa
l
ag
en
t#
1
ca
u
sa
l
ag
en
t#
1
N
o
le
th
al
do
se
#1
?o
pi
um
#1
23
.2
ca
u
sa
l
ag
en
t#
1
lo
ca
ti
on
#1
N
o
ta
xi
dr
iv
er
#1
?L
os
A
ng
el
es
#1
23
.3
ca
u
sa
l
ag
en
t#
1
ob
je
ct
#1
N
o
do
se
#1
?m
al
at
hi
on
#1
23
.4
p
oi
nt
#2
bo
d
y
of
w
at
er
#1
N
o
94
.4
6
10
he
ad
w
at
er
s#
1?
ni
le
#1
23
.5
re
gi
on
#1
bo
d
y
of
w
at
er
#1
N
o
91
.7
9
10
ea
st
si
de
#1
?r
iv
er
#1
23
.6
lin
e#
11
re
gi
on
#3
N
o
di
re
ct
io
n#
1?
pa
rk
#1
23
.7
ge
og
ra
p
hi
c
p
oi
nt
#1
re
gi
on
#3
N
o
89
.4
3
9
co
rn
er
#4
?w
as
hi
ng
to
n#
1
23
.8
p
oi
nt
#2
ge
og
ra
p
hi
ca
l
ar
ea
#1
N
o
79
.9
8
9
st
oc
k
ex
ch
an
ge
#1
?i
st
an
bu
l#
1
23
.9
d
is
tr
ic
t#
1
d
is
tr
ic
t#
1
N
o
co
m
m
on
w
ea
lt
h#
1?
pu
er
to
ri
co
#1
23
.9
.1
ad
m
in
d
is
tr
ic
t#
1
ad
m
in
d
is
tr
ic
t#
1
Ye
s
A
la
sk
a#
1?
U
ni
te
d
St
at
es
#1
23
.1
0
lo
ca
ti
on
#1
ob
je
ct
#1
Ye
s
R
om
an
ia
#1
?E
ur
op
e#
1
23
.1
0.
1
lo
ca
ti
on
#1
na
tu
ra
l
ob
je
ct
#1
N
o
89
.5
5
10
ne
ig
hb
or
ho
od
#1
?e
ar
th
#1
23
.1
0.
2
ar
ea
#1
na
tu
ra
l
ob
je
ct
#1
N
o
50
8
co
rn
er
#1
?e
ar
th
#1
23
.1
0.
3
lo
ca
ti
on
#1
p
er
so
n#
1
N
o
87
.9
0
10
bi
rt
hp
la
ce
#1
?N
ix
on
#1
23
.1
1
ob
je
ct
#1
ca
u
sa
l
ag
en
t#
1
N
o
bo
tt
le
#1
?p
re
sc
ri
pt
io
n
dr
ug
#1
23
.1
2
p
ar
t#
4
lo
ca
ti
on
#1
N
o
50
7
m
ap
#1
?V
ie
tn
am
#1
23
.1
3
cr
ea
ti
on
#2
ex
tr
em
it
y#
4
N
o
se
t#
4?
ed
ge
#1
23
.1
4
fa
ci
lit
y#
1
re
gi
on
#3
N
o
93
.7
4
10
ra
ilw
ay
st
at
io
n#
1?
B
ei
jin
g#
1
23
.1
5
cr
ea
ti
on
#2
re
gi
on
#3
N
o
87
.9
0
10
m
in
ia
tu
re
#2
?w
ar
sa
w
#1
23
.1
6
eq
u
ip
m
en
t#
1
ad
m
in
d
is
tr
ic
t#
1
N
o
re
ac
to
r#
2?
ir
aq
#1
23
.1
7
na
tu
ra
l
ob
je
ct
#1
p
oi
nt
#2
N
o
he
ad
la
nd
#1
?t
op
#3
23
.1
8
bl
oc
k#
1
bu
ild
in
g
m
at
er
ia
l#
1
N
o
95
.3
2
10
sl
ab
#1
?c
on
cr
et
e#
1
23
.1
9
ar
ti
fa
ct
#1
p
av
in
g
m
at
er
ia
l#
1
N
o
91
.7
6
10
sl
ab
#1
?c
on
cr
et
e#
1
23
.2
0
cr
ea
ti
on
#2
st
ru
ct
u
re
#1
N
o
95
.3
2
10
ar
t#
1?
m
us
ic
-h
al
l#
1
23
.2
1
co
m
m
od
it
y#
1
in
st
ru
m
en
ta
lit
y#
3
N
o
sh
ip
m
en
t#
1?
ca
pa
ci
to
r#
1
23
.2
2
fl
ap
#1
cl
ot
hi
ng
#1
N
o
he
m
#1
?d
re
ss
#1
23
.2
3
re
p
re
se
nt
at
io
n#
2
cr
ea
ti
on
#2
N
o
67
.6
2
10
sp
ec
ta
cl
e#
2?
sc
en
er
y#
1
129
Computational Linguistics Volume 32, Number 1
Ta
b
le
2
(c
on
t.)
N
o.
P
ar
tC
la
ss
W
ho
le
C
la
ss
V
al
.
A
cc
.
Fr
.
E
xa
m
p
le
23
.2
4
d
es
ig
n#
4
co
ve
ri
ng
#2
N
o
67
.6
2
10
co
lo
rs
#1
?p
ai
nt
#1
23
.2
5
la
nd
#3
is
la
nd
#1
N
o
91
.7
9
10
co
nt
in
en
t#
1?
A
tl
an
ti
s#
1
23
.2
6
ap
p
en
d
ag
e#
3
in
st
ru
m
en
ta
lit
y#
3
N
o
st
oc
k#
7?
ar
ti
lle
ry
#1
23
.2
7
ob
je
ct
#1
p
ar
t#
7
N
o
sl
ab
#1
?f
at
#2
23
.2
8
ob
je
ct
#1
u
ni
t#
6
N
o
ad
di
ti
on
#1
?s
od
iu
m
ni
tr
at
e#
1
23
.2
9
or
ga
ni
sm
#1
ca
u
sa
l
ag
en
t#
1
N
o
su
pp
lie
r#
1?
co
ca
in
e#
1
23
.3
0
or
ga
ni
sm
#1
lo
ca
ti
on
#1
N
o
am
ba
ss
ad
or
#1
?i
ra
q#
1
23
.3
1
or
ga
ni
sm
#1
ob
je
ct
#1
N
o
au
th
or
#1
?b
oo
k#
1
23
.3
2
or
ga
ni
sm
#1
or
ga
ni
sm
#1
N
o
as
sa
ss
in
#1
?K
en
ne
dy
#1
23
.3
3
th
in
g#
12
en
ti
ty
#1
N
o
94
.6
4
10
so
m
et
hi
ng
#1
?A
m
er
ic
a#
1
23
.3
4
bo
d
y
of
w
at
er
#1
en
ti
ty
#1
N
o
68
.1
8
8
se
a#
1?
in
te
ra
ct
io
n#
1
23
.
D
ef
au
lt
Ye
s
do
or
#4
?c
ar
#1
24
en
ti
ty
#1
gr
ou
p
#1
Ye
s
ac
ad
em
ic
ia
n#
1?
ac
ad
em
y#
2
24
.1
en
ti
ty
#1
sy
st
em
#1
N
o
87
.9
0
10
ri
ve
r#
1?
ec
os
ys
te
m
#1
24
.2
ar
ti
fa
ct
#1
ga
th
er
in
g#
1
N
o
ro
st
ru
m
#1
?c
on
gr
es
s#
2
24
.
D
ef
au
lt
Ye
s
ac
ad
em
ic
ia
n#
1?
ac
ad
em
y#
2
25
en
ti
ty
#1
p
os
se
ss
io
n
#2
Ye
s
Tu
am
ot
u
A
rc
hi
pe
la
go
#1
?
Fr
en
ch
P
ol
yn
es
ia
#1
25
.1
or
ga
ni
sm
#1
p
os
se
ss
io
n#
2
N
o
85
.5
0
8
m
an
ag
er
#1
?i
nv
es
tm
en
t
fu
nd
s#
1
25
.2
ca
u
sa
l
ag
en
t#
1
p
os
se
ss
io
n#
2
N
o
85
.3
0
8
bu
ye
r#
1?
lif
e
in
su
ra
nc
e#
1
25
.
D
ef
au
lt
Ye
s
Tu
am
ot
u
A
rc
hi
pe
la
go
#1
?
Fr
en
ch
P
ol
yn
es
ia
#1
26
gr
ou
p
#1
gr
ou
p
#1
Ye
s
ge
nu
s
am
oe
ba
#1
?a
m
oe
bi
da
#1
26
.1
so
ci
al
gr
ou
p
#1
p
eo
p
le
#1
N
o
di
ct
at
or
sh
ip
#1
?p
ro
le
ta
ri
at
#1
26
.2
gr
ou
p
#1
p
eo
p
le
#1
N
o
83
.8
6
8
de
m
i-
m
on
de
#1
?h
ig
h
so
ci
et
y#
1
26
.3
ar
ra
ng
em
en
t#
2
co
lle
ct
io
n#
1
N
o
82
.2
2
10
cl
as
si
fic
at
io
n#
2?
fa
m
ily
#4
26
.4
so
ci
al
gr
ou
p
#1
co
lle
ct
io
n#
1
N
o
82
.2
2
10
ci
rc
le
#2
?l
aw
#2
26
.
D
ef
au
lt
Ye
s
ge
nu
s
am
oe
ba
#1
?a
m
oe
bi
da
#1
D
ef
au
lt
N
o
130
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
Appendix C: Performance Results per Cluster
Tables 1 and 2 show the performance results obtained for each cluster considered on LA
Times and WSJ test corpora.
Table 1
The number of part?whole relations obtained and the accuracy for each cluster and for all the
clusters in the WSJ collection.
Results Genitives Noun compounds Preposition All
cluster cluster cluster clusters
ISS system
Number of patterns 1514 1217 1081 3812
Number of correctly retrieved 161 85 49 295
relations
Number of relations retrieved 202 90 64 356
Number of correct relations 167 141 65 373
for the pattern(s)
Number of correct relations 406
Precision 79.70% 94.44% 76.56% 82.87%
Recall for cluster(s) 96.41% 60.28% 75.38% 79.09%
Coverage 72.66%
F-measure 86.87% 77.13% 71.36% 82.05%
Baseline1 ? No WSD
Precision 6.04% 50% 45.45% 7.72%
Recall for the pattern(s) 36% 3.57% 22.73% 24%
Coverage 10.81%
F-measure 2.12% 7.41% 23.26% 3.56%
Baseline2 ? One learning
Precision 7.18% 37.5% 20% 7.73%
Recall for the pattern(s) 78% 10.71% 4.54% 43%
Coverage 19.37%
F-measure 6% 10% 3.57% 6.02%
Baseline3 ? No Generalization
Precision 28.21% 0% 0% 15.71%
Recall for the pattern(s) 6.59% 0% 0% 2.95%
Coverage 2.71%
F-measure 10.68% 1.12% 0% 4.97%
Baseline4 ? WSD using system for training
Precision 62.42% 50.74% 59.05% 53.57%
Recall for the pattern(s) 92.40% 68.67% 54.87% 27.87%
Coverage 25.86%
F-measure 74.51% 58.36% 56.88% 36.67%
131
Computational Linguistics Volume 32, Number 1
Table 2
The number of part?whole relations obtained and the accuracy for each cluster and for all the
clusters in the LA Times collection.
Results Genitives Noun compounds Preposition All
cluster cluster cluster clusters
ISS system
Number of patterns 4106 3442 2577 10125
Number of correctly retrieved 321 113 71 505
relations
Number of relations retrieved 410 143 86 639
Number of correct relations 329 150 113 592
for the pattern(s)
Number of correct relations 638
Precision 78.29% 79.02% 82.56% 79.03%
Recall for cluster(s) 97.57% 75.33% 62.83% 85.30%
Coverage 79.15%
F-measure 87.26% 73.59% 75.97% 80.94%
Baseline1 ? No WSD
Precision 1.16% 33.33% 38.46% 2.10%
Recall for the pattern(s) 12.07% 4.17% 16.67% 11.61%
Coverage 3.02%
F-measure 10.34% 6.66% 30.3% 11.68%
Baseline2 ? One learning
Precision 3.12% 12.5% 3.84% 3.24%
Recall for the pattern(s) 77.59% 8.33% 3.33% 42.86%
Coverage 11.16%
F-measure 13.15% 16.66% 7.4% 13.1%
Baseline3 ? No Generalization
Precision 34.29% 3.33% 0% 24.34%
Recall for the pattern(s) 10.94% 0.67% 0% 6.25%
Coverage 5.80%
F-measure 16.59% 1.12% 0% 9.98%
Baseline4 ? WSD using system for training
Precision 52.8% 54% 39.81% 48.22%
Recall for the pattern(s) 79.04% 57.45% 63.08% 20.61%
Coverage 30.05%
F-measure 63.31% 55.67% 48.81% 28.88%
132
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
Acknowledgments
We would like to thank Matthew Jones for
his help in providing the gold-standard
annotations for the training and test corpora
used in this research. We are grateful
for the constructive comments made by
Robert Dale and anonymous reviewers
that helped considerably clarify and
improve the presentation. This work was
partially supported by the Advanced
Research and Development Activity/
Disruptive Technology Office.
References
Berland, Matthew and Eugene Charniak.
1999. Finding parts in very large corpora.
In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics
(ACL 1999), pages 57?64, University of
Maryland.
Brill, Eric. 1995. Transformation-based
error-driven learning and natural
language processing: A case study in
part-of-speech tagging. Computational
Linguistics, 21(4):543?566.
Charniak, Eugene. 2000. A
maximum-entropy-inspired parser. In
Proceedings of the 1st Conference of the North
American Chapter of the Association for
Computational Linguistics (NAACL 2000),
pages 132?139, Seattle, WA.
Downing, Pamela. 1977. On the creation and
use of English compound nouns. Language,
53(4):810?842.
Dunning, Ted. 1993. Accurate methods for
the statistics of surprise and coincidence.
Computational Linguistics, 19:61?74.
Evens, Martha W., Bonnie C. Litowitz,
Judith A. Markowitz, Raoul N. Smith, and
Oswald Werner. 1980. Lexical-semantic
relations: A comparative survey. Linguistic
Research, pages 187?219.
Fellbaum, Christiane. 1998. WordNet?An
Electronic Lexical Database. MIT Press,
Cambridge, MA.
Finin, Timothy W. 1980. The Semantic
Interpretation of Compound Nominals. Ph.D.
thesis, University of Illinois at
Urbana-Champaign.
Freeze, Ray. 1992. Existentials and other
locatives. Language, 68:553?595.
Gildea, Daniel and Daniel Jurafsky.
2002. Automatic labeling of semantic
roles. Computational Linguistics, 28(3):
245?288.
Girju, Roxana. 2001. Answer fusion with
on-line ontology development. In
Proceedings of the 2nd Meeting of the North
American Chapter of the Association for
Computational Linguistics (NAACL 2001) -
Student Research Workshop, pages 23?28,
Pittsburgh, PA.
Girju, Roxana, Adriana Badulescu, and
Dan Moldovan. 2003. Learning semantic
constraints for the automatic discovery of
part-whole relations. In Proceedings of the
3rd Human Language Technology Conference/
4th Meeting of the North American Chapter of
the Association for Computational Linguistics
Conference (HLT-NAACL 2003),
pages 80?87, Edmonton, Canada.
Girju, Roxana, Dan Moldovan, Marta Tatu,
and Daniel Antohe. 2005. On the semantics
of noun compounds. Computer Speech and
Language?Special Issue on Multiword
Expressions (in press).
Hearst, Marti. 1992. Acquisition of hyponyms
from large text corpora. In Proceedings
of the 14th International Conference on
Computational Linguistics (COLING-92),
pages 539?545, Nantes, France.
Hearst, Marti. 1998. Automated discovery
of WordNet relations. In Christiane
Fellbaum, editor, An Electronic Lexical
Database and Some of Its Applications.
MIT Press, Cambridge, MA, pages 131?151.
Iris, Madelyn, Bonnie Litowitz, and Martha
Evens. 1988. Problems with part-whole
relation. In M. W. Evens, editor, Relational
Models of the Lexicon: Representing
Knowledge in Semantic Networks.
Cambridge University Press, Cambridge,
pages 261?288.
Jensen, Per Anker and Carl Vikner. 1996. The
double nature of the verb have. LAMBDA,
21:25?37.
Kingsbury, Paul, Martha Palmer, and Mitch
Marcus. 2002. Adding semantic annotation
to the Penn Treebank. In Proceedings of
the 2nd Human Language Technology
Conference (HLT 2002), pages 252?256,
San Diego, CA.
Lapata, Mirella. 2002. The disambiguation of
nominalisations. Computational Linguistics,
28(3):357?388.
Lascarides, Alex and Ann Copestake. 1998.
Pragmatics and word meaning. Journal
of Linguistics, 34(2):387?414.
Lauer, Mark and Mark Dras. 1994. A
probabilistic model of compound nouns.
In Proceedings of the 7th Australian Joint
Conference on Artificial Intelligence,
pages 474?481, Armidale, Australia.
Levi, Judith. 1978. The Syntax and Semantics
of Complex Nominals. Academic Press,
New York.
133
Computational Linguistics Volume 32, Number 1
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English:
The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Moldovan, Dan and Adriana Badulescu.
2005. A semantic scattering model
for the automatic interpretation of
genitives. In Proceedings of Human
Language Technology Conference and
Conference on Empirical Methods in
Natural Language Proceesing (HLT/
EMNLP 2005), pages 891?898,
Vancouver, BC, Canada.
Moldovan, Dan, Adriana Badulescu,
Marta Tatu, Daniel Antohe, and Roxana
Girju. 2004. Models for the semantic
classification of noun phrases. In
Proceedings of the Human Language
Technology Conference (HLT-NAACL)
2004, Computational Lexical Semantics
Workshop, Boston, MA.
Moldovan, Dan and Roxana Girju. 2001. An
interactive tool for the rapid development
of knowledge bases. International
Journal on Artificial Intelligence Tools,
10(1?2):65?86.
Moldovan, Dan, Sanda Harabagiu, Roxana
Girju, Paul Morarescu, Finley Lacatusu,
Adrian Novischi, Adriana Badulescu,
and Orest Bolohan. 2002. LCC tools for
question answering. In Proceedings
of the 11th Meeting of the Text Retrieval
Conference (TREC 2002), pages 388?397,
Gaithersburg, MD.
Morris, Jane and Graeme Hirst. 2004.
Non-classical lexical semantic relations.
In Proceedings of the 4th Human Language
Technology Conference / of the 5th Meeting
of the North American Chapter of the
Association for Computational Linguistics
(HLT-NAACL 2004) - Workshop on
Computational Lexical Semantics,
pages 46?51, Boston, MA.
Novischi, Adrian, Dan Moldovan, Paul
Parker, Adriana Badulescu, and Bob
Hauser. 2004. LCC?s WSD systems for
Senseval 3. In Proceedings of Senseval 3
(ACL 2004), Barcelona, Spain.
Pustejovsky, James, Sabine Bergler, and
Peter Anick. 1993. Lexical semantic
techniques for corpus analysis.
Computational Linguistics, 19(2):
331?358.
Quinlan, Ross. J. 1993. C4.5: Programs for
Machine Learning. Morgan Kaufmann,
San Francisco, CA.
Resnik, Philip. 1996. Selectional constraints:
An information-theoretic model and its
computational realization. Cognition,
61:127?159.
Resnik, Philip and Marti Hearst. 1993.
Structural ambiguity and conceptual
relations. In Proceedings of the 31st
Meeting of the Association for
Computational Linguistics (ACL 1993)-
1st Workshop on Very Large Corpora:
Academic and Industrial Perspectives,
pages 58?64, Ohio State University,
Columbus, OH.
Rosario, Barbara and Marti Hearst. 2001.
Classifying the semantic relations in
noun compounds via a domain-specific
lexical hierarchy. In Proceedings of the
Conference on Empirical Methods in
Natural Language Processing (EMNLP 2001),
pages 82?90, Pittsburgh, PA.
Rosario, Barbara, Marti Hearst, and Charles
Fillmore. 2002. The descent of hierarchy,
and selection in relational semantics. In
Proceedings of the 40th Annual Meeting
of the Association for Computational
Linguistics, pages 247?254, University
of Pennsylvania.
Schafer, Robin. 1995. The SLP/ILP
distinction in have-predication. In
M. Simons and T. Galloway, editors,
Proceedings from Semantics and
Linguistic Theory V. Cornell University
Department of Linguistics, pages 292?309,
Ithaca.
Siegel, Sidney and John Castellan. 1988.
Nonparametric Statistics for the Behavioral
Science. McGraw-Hill, New York.
Simons, Peter. 1987. Parts. A Study
in Ontology. Clarendon Press, Oxford.
Simons, Peter. 1991. Part/whole II:
Mereology since 1900. In H. Burkhardt
and B. Smith, editors, Handbook of
Metaphysics and Ontology. Philosophia,
Munich, pages 672?675.
Spa?rck Jones, K. 1983. Compound noun
interpretation problems. In F. Fallside and
W. A. Woods, editors, Computer Speech
Processing. Prentice-Hall, Englewood Cliffs,
NJ, pages 363?380.
Tatu, Marta and Dan Moldovan. 2005.
A semantic approach to recognizing
textual entailmant. In Proceedings of
Human Language Technology Conference
and Conference on Empirical Methods in
Natural Language Processing (HLT/EMNLP
2005), pages 371?378, Vancouver,
BC, Canada.
Thompson, Cynthia A., Roger Levy, and
Christopher Manning. 2003. A generative
model for Framenet semantic role
labeling. In Proceedings of the 14th
134
Girju, Badulescu, and Moldovan Automatic Discovery of Part?Whole Relations
European Conference on Machine Learning
(ECML 2003), pages 397?408,
Cavtat-Dubrovnik, Croatia.
Vanderwende, Lucy. 1994. Algorithm for
automatic interpretation of noun
sequences. In Proceedings of the 15th
International Conference on Computational
Linguistics (COLING 1994), pages 782?788,
Kyoto, Japan.
Vanderwende, Lucy. 1995. The Analysis of
Noun Sequences using Semantic
Information Extracted from On-Line
Dictionaries. Ph.D. thesis, Georgetown
University.
Winston, Morton, Roger Chaffin, and
Douglas Hermann. 1987. A taxonomy
of part-whole relations. Cognitive Science,
11(4):417?444.
135

Book Review
Mathematical Linguistics
Andra?s Kornai
(MetaCarta Inc.)
Springer (Advanced information and knowledge processing series, edited by
Lakhmi Jain), 2008, xiii+289 pp; ISBN 978-1-84628-985-9, $99.00
Reviewed by
Richard Sproat and Roxana G??rju
University of Illinois at Urbana-Champaign
For readers of traditional textbooks such as that of Partee, ter Meulen, and Wall (1990),
the term ?mathematical linguistics? denotes a rather narrowly circumscribed set of issues
including automata theory, set theory, and lambda calculus, with maybe a little formal
language theory thrown in. Kornai?s contribution is refreshingly different in that he
treats, in this relatively compact volume, practically all areas of linguistics, phonetics,
and speech and language processing.
Kornai?s motivation for writing this book is to present ?a single entry point to
the central methods and concepts of linguistics that are made largely inaccessible
to the mathematician, computer scientist, or engineer by the surprisingly adversarial
style of argumentation . . . and the proliferation of unmotivated notation and formal-
ism . . . all too often encountered in research papers and monographs in the humanities?
(page viii). There is no question that much of what passes for rigor (mathematical and
scientific) in linguistics is a joke and that there is clearly a need for any work that can
place the field on a more solid footing. It also seems likely that Kornai is the only person
who could have written this book.
The book is divided into ten chapters, including a short introductory chapter, which
lays the groundwork and identifies the potential audience, and a concluding chapter
where Kornai reveals his own views on what is important in the field, which in the
interests of balance he has largely suppressed throughout the book. Chapter 2 is also
introductory in that it presents basic concepts of generation (via a ruleset), axioms, and
string rewriting.
The main chapters (3?9) deal with a variety of topic areas relating to language
and speech, starting with phonology in Chapter 3. This chapter introduces the notion
of phonemes, distinctive features, autosegmental phonology, and computation using
finite automata. Kornai offers many details that are of course lacking in most linguistic
treatments, such as a proof that the number of well-formed association lines between
two tiers of length n is asymptotically (6+ 4
?
2)n.
Chapter 4 deals with morphology, which for Kornai includes not only word forma-
tion, but also prosody (including stress assignment and moraic structure), as well as
Optimality Theory and Zipf?s law.
The fifth chapter treats syntax, including categorial grammar, phrase structure,
dependency frameworks, valency, and weighted models of grammar, ending with a
discussion of weighted finite automata and hidden Markov models. In the context of
weighted models, Kornai implies that Chomsky?s original notion of degree of gram-
maticality fits naturally as an instance of a weighted model with a particular semiring;
of course, exactly what the ? and ? operators of that semiring map to remain to
Computational Linguistics Volume 34, Number 4
be seen insofar as the notion ?degree of grammaticality? has never been rigorously
defined.
Chapter 6, on Semantics, starts with a discussion of various standard paradoxes
such as the Liar, and then moves on to an overview of Montague?s theory, type theory,
and grammatical semantics. Throughout the discussion, Kornai underscores the fun-
damental limitations of theories of semantics that are based purely upon evaluation of
truth conditions for artificial fragments, an important point for anyone who wants to
go beyond theoretical philosophically inspired models and consider semantic interpre-
tation in the real world.
Complexity is the topic of Chapter 7. This is not the Chomsky-hierarchy notion of
complexity, but rather deals with information theory, in particular entropy, Kolmogorov
complexity, and a short section on learning, including identification in the limit and PAC
learning.
Pattern recognition is divided across two chapters, with Chapter 8 laying the es-
sential groundwork of linguistic pattern recognition, and Chapter 9 presenting details
on speech processing and handwriting recognition. This includes feature extraction: In
the case of speech recognition, Kornai reviews the frequency representation of speech
signals, and defines the cepstrum. Discussion of acoustic models leads us to phonemes
as hidden units, with a slight detour into the fine-grained distinctions between different
levels of phonemic analysis in the once popular but now largely discredited theory of
Lexical Phonology.
Each chapter ends with a section entitled ?Further Reading,? and the texts referred
to are generally quite useful as material for readers who wish to explore the issues
further.
According to Wikipedia, Kornai is a ?well-known mathematical linguist? whose
Erdo?s number is 2. Unfortunately, neither of us can claim Kornai?s mathematical so-
phistication or stature, but on the other hand this makes us good judges of the book?s
potential audience; and herein lies a problem. Kornai?s target is ?anyone with suffi-
cient general mathematical maturity? with ?[n]o prior knowledge of linguistics or lan-
guages . . . assumed on the part of the reader? (page viii). This suggests that the book
is not primarily aimed at linguists, and certainly the mathematical maturity assumed
puts this book well beyond the reach of most linguists, so that it could not easily be
used in an introductory course on mathematical linguistics in a linguistics program. It
is probably beyond the reach of many computer science students as well.
What about those who do have the mathematical maturity, but know nothing
about linguistics? The problem here is that in many cases Kornai does not give enough
background (or any background) to appreciate the significance of the particular issues
being discussed. For example, on page 77 Kornai gives weak crossover and heavy NP shift
as examples of phenomena that have ?weak? effects on grammaticality, and resumptive
pronouns as examples of phenomena that are marginal in some languages (such as Eng-
lish). But nowhere does he explain what these terms denote, which means that these are
throw-away comments for anyone who does not already know. Section 3.2 introduces
phonological features and feature geometry and sketches some of the mathematical
properties of systems with features; but very little background is given on what features
are supposed to represent. The short discussion of Optimality Theory (pages 67?69)
hardly gives enough background to give a feel for the main points of that approach.
In other cases, topics are introduced but their importance to surrounding topics is hard
to fathom. For example, in Section 6.1.3 a discussion of the Berry paradox leads into a
digression on how to implement digit-sequence-to-number-name mappings as finite-
state transducers. Apart from giving Kornai an opportunity to emphasize that this is
616
Book Review
trivial to do (something that is true in principle, but less true in practice, depending
upon the language), it is not clear what purpose this digression serves.
There are also a number of places where issues are presented in a non-standard
way, which might make sense from some points of view, but not if you are trying to
introduce someone to the way the field is practiced. It is odd, for instance, that prosody
is introduced not in the chapter on phonology but in the one on morphology. It is
also somewhat odd that Zipf?s law gets introduced in the morphology chapter. (And
why is it that nowhere does Kornai cite Baayen?s excellent book on word-frequency
distributions (Baayen 2001), which would be a very useful source of further information
on this topic to any reader of Kornai?s book?)
Some material presented is puzzling or simply wrong. It is not explained in what
sense German has a ?pure SVO construction? (page 103) in contradistinction to the
normal assumption that German is verb-second. The Cypriot syllabary does not date
from the 15th century BCE (page 54); Latin does not have two locative cases (page 90)?
indeed, it does not even have one locative case, so-called; the basic Hangul letter shapes
(introduced on page 31 to make a point about phonetic features) are, with two excep-
tions, completely incorrect?probably it would have been better to use a real Korean
font rather than trying to imitate the jamowith LATEX math symbols. There are of course a
great many places where the discussion is useful and informative, but there are enough
examples of the kinds we have outlined that the uninitiated reader should be careful.
As far as we can see, the most likely readership of this book consists of (computa-
tional) linguists and others who already know the linguistic issues, have a fairly strong
formal and mathematical background, and could benefit from the more-precise and
more-rigorous mathematical expositions that Kornai provides.
Throughout the book, Kornai pauses occasionally to present exercises to the reader.
These range from relatively simple to major research projects. As with other aspects of
this book, the distribution of topics for the exercises is somewhat erratic. Thus, on page
184, in the chapter on complexity, we are offered exercises 7.6 and 7.7 in close proximity:
Exercise 7.6 Prove that a regular language is prefix-free iff it is accepted by a DFSA
with no transitions out of accepting states. Is a prefix-free language context-free iff it
is accepted by a DPDA with the same restriction on its control?
...
Exercise 7.7 Research the role of the ascii codes 0x02 (STX), 0x03 (ETX), and 0x16
(SYN).
But variety is, after all, what keeps things interesting.
References
Baayen, R. Harald 2001. Word Frequency
Distributions. Kluwer Academic
Publishers, Dordrecht.
Partee, Barbara, Alice ter Meulen, and Robert
Wall. 1990. Mathematical Methods in
Linguistics. Kluwer Academic Publishers,
Dordrecht.
Richard Sproat is Professor of Linguistics and Electrical and Computer Engineering at the Uni-
versity of Illinois at Urbana-Champaign. He works on computational morphology, text normal-
ization, and speech processing. His Erdo?s number is 4. Roxana G??rju is Assistant Professor of
Linguistics at the University of Illinois at Urbana-Champaign. She has a Ph.D. in Computer
Science and works on computational semantics, pragmatics, and inference. Her Erdo?s number
is also 4. Their address is Department of Linguistics, University of Illinois at Urbana-Champaign,
Foreign Languages Building 4016D, 707 South Matthews Avenue, MC-168, Urbana, IL, 61801;
e-mail: rws@uiuc.edu and girju@uiuc.edu.
617

The Syntax and Semantics of Prepositions in
the Task of Automatic Interpretation of
Nominal Phrases and Compounds:
A Cross-Linguistic Study
Roxana Girju?
University of Illinois at
Urbana-Champaign
In this article we explore the syntactic and semantic properties of prepositions in the context
of the semantic interpretation of nominal phrases and compounds. We investigate the problem
based on cross-linguistic evidence from a set of six languages: English, Spanish, Italian, French,
Portuguese, and Romanian. The focus on English and Romance languages is well motivated.
Most of the time, English nominal phrases and compounds translate into constructions of the
form N P N in Romance languages, where the P (preposition) may vary in ways that correlate
with the semantics. Thus, we present empirical observations on the distribution of nominal
phrases and compounds and the distribution of their meanings on two different corpora, based
on two state-of-the-art classification tag sets: Lauer?s set of eight prepositions and our list of 22
semantic relations. A mapping between the two tag sets is also provided. Furthermore, given a
training set of English nominal phrases and compounds along with their translations in the five
Romance languages, our algorithm automatically learns classification rules and applies them
to unseen test instances for semantic interpretation. Experimental results are compared against
two state-of-the-art models reported in the literature.
1. Introduction
Prepositions are an important and frequently used category in both English and Ro-
mance languages. In a corpus study of one million English words, Fang (2000) shows
that one in ten words is a preposition. Moreover, about 10% of the 175 most frequent
words in a corpus of 20 million Spanish words were found to be prepositions (Almela
et al 2005). Studies on language acquisition (Romaine 1995; Celce-Murcia and Larsen-
Freeman 1999) have shown that the acquisition and understanding of prepositions in
languages such as English and Romance is a difficult task for native speakers, and
even more difficult for second language learners. For example, together with articles,
prepositions represent the primary source of grammatical errors for learners of English
as a foreign language (Gocsik 2004).
? Linguistics and Computer Science Departments, University of Illinois at Urbana-Champaign, Urbana, IL
61801. E-mail: girju@illinois.edu.
Submission received: 1 August 2006; revised submission received: 20 January 2008; accepted for publication:
17 March 2008.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 2
Although the complexity of preposition usage has been argued for and documented
by various scholars in linguistics, psycholinguistics, and computational linguistics,
very few studies have been done on the function of prepositions in natural language
processing (NLP) applications. The reason is that prepositions are probably the most
polysemous category and thus, their linguistic realizations are difficult to predict and
their cross-linguistic regularities difficult to identify (Saint-Dizier 2005a).
In this article we investigate the role of prepositions in the task of automatic seman-
tic interpretation of English nominal phrases and compounds. The problem is simple to
define: Given a compositional noun phrase (the meaning of the phrase derives from the
meaning of the constituents) constructed out of a pair of nouns, N1 N2, one representing
the head and the other the modifier, determine the semantic relationship between the
two nouns. For example, the noun?noun compound family estate encodes a POSSESSION
relation, while the nominal phrase the faces of the children refers to PART-WHOLE. The
problem, although simple to state, is difficult for automatic semantic interpretation.
The reason is that the meaning of these constructions is most of the time implicit (it
cannot be easily recovered from morphological analysis). Interpreting nominal phrases
and compounds correctly requires various types of information, from world knowledge
to lexico-syntactic and discourse information.
This article focuses on nominal phrases of the type N P N and noun compounds
(N N) and investigates the problem based on cross-linguistic evidence from a set of six
languages: English, Spanish, Italian, French, Portuguese, and Romanian. The choice of
these constructions is empirically motivated. In a study of 6,200 (Europarl1) and 2,100
(CLUVI2) English token nominal phrase and compound instances randomly chosen
from two English?Romance parallel text collections of different genres, we show that
over 80% of their Romance noun phrase translations are encoded by N P N and N N
constructions. For instance, beer glass, an English compound of the form N1 N2, trans-
lates into N2 P N1 instances in Romance: tarro de cerveza (?glass of beer?) in Spanish,
bicchiere da birra (?glass for beer?) in Italian, verre a` bie`re (?glass at/to beer?) in French, copo
de cerveja (?glass of beer?) in Portuguese, and pahar de bere (?glass of beer?) in Romanian.
In this article, in addition to the sense translation (in italics), when relevant we also
provide the word-by-word gloss (in ?parentheses?). Moreover, we use N1, N2 to denote
the two lexical nouns that encode a semantic relation (whereN1 is the syntactic modifier
and N2 is the syntactic head), and Arg1, Arg2 to denote the semantic arguments of the
relation encoded by the two nouns. For example, beer glass encodes a PURPOSE relation
where Arg1 (beer) is the purpose of Arg2 (?glass?; thus ?glass (used) for beer?).
We argue here that the syntactic directionality given by the head-modifier relation
(N1 N2 in noun compounds and N2 P N1 in nominal phrases) is not always the same
as the semantic directionality given by the semantic argument frame of the semantic
relation. Otherwise said, N1 does not always map to Arg1 and N2 to Arg2 for any given
relation.
Languages choose different nominal phrases and compounds to encode relation-
ships between nouns. For example, English nominal phrases and compounds of the
1 http://www.isi.edu/koehn/europarl/.
This corpus contains over 20 million words in eleven official languages of the European Union covering
the proceedings of the European Parliament from 1996 to 2001.
2 CLUVI - Linguistic Corpus of the University of Vigo Parallel Corpus 2.1; http://sli.uvigo.es/CLUVI/.
CLUVI is an open text repository of parallel corpora of contemporary oral and written texts in some of
the Romance languages (such as Galician, French, Spanish, and Portuguese) and Basque parallel text
collections.
186
Girju The Syntax and Semantics of Prepositions
form N1 N2 (e.g., wood stove) and N2 P1 N1 (e.g., book on the table) usually translate
in Romance languages as N2 P2 N1 (e.g., four a` bois in French ? ?stove at/to wood?,
and livre sur la table ? ?book on the table?). Romance languages have very few N N
compounds and they are of limited semantic categories, such as TYPE (e.g., legge quadro
in Italian ? ?law framework? ? translates as framework law). Besides the unproductive
N N and the productive N P N phrases, Romanian also uses another productive con-
struction: the genitive-marked noun?noun compounds (e.g., frumuset?ea fetei ? beauty-
the girl-GEN ? translated as the beauty of the girl). Whereas English N N compounds
are right-headed (e.g., framework/Modifier law/Head), Romance compounds are left-
headed (e.g., legge/Head quadro/Modifier). Moreover, the Romance preposition used in
the translations of English nominal phrase instances of the type N P N is one that comes
closest to having overlapping semantic range as intended in the English instance, but
may not be the exact counterpart for the whole semantic range. For example, Committee
on Culture translates as Comisio?n de la Cultura (Spanish) (?Committee of the Culture?),
Commission de la Culture (French) (?Committee of the Culture?), Commissione per la Cul-
tura (Italian) (?Committee for the Culture?), Comissa?o para Cultura (Portuguese) (?Com-
mittee for Culture?), and Comitet pentru Cultura? (Romanian) (?Committee for Culture?).
Even those Romance prepositions that are spelled ?de? are pronounced differently in
different Romance languages.
Thus, the focus on nominal phrases and compounds in English and Romance lan-
guages is also motivated linguistically. The extension of this task to natural languages
other than English brings forth both new insights and new challenges. The Romance
prepositions used in the translations of English nominal phrases and compounds, may
vary in ways that correlate with the semantics. Thus, Romance language prepositions
will give us another source of evidence for disambiguating the semantic relations in
English nominal phrases and compounds. We argue that, in languages with multiple
syntactic options such as English (N N and N P N) and Romanian (N N, genitive-
marked N N, and N P N), the choice between such constructions in context is governed
in part by semantic factors. For example, the set of semantic relations that can be
encoded by pairs of nouns such as tea?cup and sailor?suit varies with the syntactic
construction used. In English, while the noun?noun compounds tea cup and sailor suit
encode only PURPOSE, the N P N constructions cup of tea and suit of the sailor encode
CONTENT-CONTAINER (a subtype of LOCATION) and MEASURE relations and POSSES-
SION, respectively. Similarly, in Romanian both tea cup and cup of tea translate only as
the N P N instance ceas?ca? de ceai (?cup of tea?), while sailor suit translates as costum de
marinar (?suit of sailor?) and the suit of the sailor as the genitive-marked N N costumul
marinarului (?suit-the sailor-GEN?). Thus, we study the distribution of semantic relations
across different nominal phrases and compounds in one language and across all six
languages, and analyze the resulting similarities and differences. This distribution is
evaluated over the two different corpora based on two state-of-the-art classification tag
sets: Lauer?s set of eight prepositions (Lauer 1995) and our list of 22 semantic relations.
A mapping between the two tag sets is also provided.
In order to test their contribution to the task of semantic interpretation, preposi-
tions and other linguistic clues are employed as features in a supervised, knowledge-
intensive model. Furthermore, given a training set of English nominal phrases and
compounds along with their translations in the five Romance languages, our algo-
rithm automatically learns classification rules and applies them to unseen test instances
for semantic interpretation. As training and test data we used 3,124 Europarl and
2,023 CLUVI token instances. These instances were annotated with semantic relations
and analyzed for inter-annotator agreement. The results are compared against two
187
Computational Linguistics Volume 35, Number 2
state-of-the-art approaches: a supervised machine learning model, semantic scattering
(Moldovan and Badulescu 2005), and a Web-based unsupervised model (Lapata and
Keller 2005). Moreover, we show that the Romanian linguistic features contribute more
substantially to the overall performance than the features obtained for the other Ro-
mance languages. This is explained by the fact that the choice of the linguistic construc-
tions (either genitive-marked N N or N P N) in Romanian is highly correlated with their
meaning.
The article is organized as follows. Section 2 presents a summary of related work.
In Section 3 we describe the general approach to the interpretation of nominal phrases
and compounds and list the syntactic and semantic interpretation categories used
along with observations regarding their distribution in the two different cross-linguistic
corpora. Sections 4 and 5 present a learning model and experimental results. Section 6
presents linguistic observations on the behavior of English and Romanian N N and
N P N constructions. Finally, in Section 7 we provide an error analysis and in Section 8
we offer some discussion and conclusions.
2. Previous Work
2.1 Noun Phrase Semantic Interpretation
The semantic interpretation of nominal phrases and compounds in particular and noun
phrases (NPs) in general has been a long-term research topic in linguistics, computa-
tional linguistics,3 and artificial intelligence.
Noun?noun compounds in linguistics
Early studies in linguistics (Lees 1963) classified noun?noun compounds on purely
grammatical criteria using a transformational approach, criteria which failed to account
for the large variety of constraints needed to interpret these constructions. Later on, Levi
(1978) attempted to give a tight account of noun?noun interpretation, distinguishing
two types of noun?noun compounds: (a) compounds interpreted as involving one of
nine predicates (CAUSE, HAVE, MAKE, USE, BE, IN, FOR, FROM, ABOUT) (e.g., onion
tears encodes CAUSE) and (b) those involving nominalizations, namely, compounds
whose heads are nouns derived from a verb, and whose modifiers are interpreted as
arguments of the related verb (e.g., a music lover loves music). Levi?s theory was cast
in terms of the more general theory of Generative Semantics. In that theory it was
assumed that the interpretation of compounds was available because the examples were
derived from underlying relative clauses that had the same meanings. Thus, honey bee,
expressing the relation MAKE, was taken to be derived from a headed relative a bee
that makes honey. Levi was committed to the view that a very limited set of predicates
constituted all of the relations that could hold between nouns in simple noun?noun
compounds. This reductionist approach has been criticized in studies of language use
by psycholinguists (Gleitman and Gleitman 1970; Downing 1977) who claim that noun?
noun compounds, which are frequent in languages like English, encode in principle an
3 In the past few years at many workshops, tutorials, and competitions this research topic has received
considerable interest from the computational linguistics community: the Workshops on Multiword
Expressions at ACL 2003, ACL 2004 and COLING/ACL 2006; the Computational Lexical Semantics
Workshop at ACL 2004; the Tutorial on Knowledge Discovery from Text at ACL 2003; the Shared Task on
Semantic Role Labeling at CONLL 2004 and 2005 and at SemEval 2007.
188
Girju The Syntax and Semantics of Prepositions
unbounded number of possible relations. One such example is apple juice seat??a seat
in front of which an apple juice [is] placed? (Downing 1977, page 818)?which can only
be interpreted in the current discourse context.
In this article we tackle the problem using a unified framework. Although we
agree with Downing (1977) that pragmatics plays an important factor in noun?noun
interpretation, a large variety of noun?noun meanings can be captured with a well-
chosen set of semantic relations. Our proposed semantic classification set differs from
that of Levi (1978) in the sense that it contains more homogenous categories. Levi?s
categories, instead, are more heterogeneous, including both prepositions and verbs,
some of which are too general (e.g., the prepositions for, in and the verb to have), and
thus, too ambiguous. Moreover, in our approach to automatic semantic interpretation
we focus on both N N and N P N constructions and exploit a set of five Romance
languages.
Noun?noun compounds in computational linguistics
The automatic interpretation of nominal phrases and compounds is a difficult task
for both unsupervised and supervised approaches. Currently, the best-performing
noun?noun interpretation methods in computational linguistics focus mostly on two
or three-word noun?noun compounds and rely either on ad hoc, domain-specific,
hand-coded semantic taxonomies, or statistical models on large collections of unlabeled
data. Recent results have shown that symbolic noun?noun compound interpretation
systems using machine learning techniques coupled with a large lexical hierarchy
perform with very good accuracy, but they are most of the time tailored to a specific
domain (Rosario and Hearst 2001; Rosario, Hearst, and Fillmore 2002), or are general
purpose (Turney 2006) but rely on semantic similarity metrics on WordNet (Fellbaum
1998). On the other hand, the majority of corpus statistics approaches to noun?noun
compound interpretation collect statistics on the occurrence frequency of the noun
constituents and use them in a probabilistic model (Lauer 1995). The problem is that
most noun?noun compounds are rare and thus, statistics on such infrequent instances
lead in general to unreliable estimates of probabilities. More recently, Lapata and Keller
(2005) showed that simple unsupervised models applied to the noun?noun compound
interpretation task perform significantly better when the n-gram frequencies are
obtained from the Web (55.71% accuracy4), rather than from a large standard corpus.
Nakov and Hearst (2005) improve over Lapata and Keller?s method through the use of
surface features and paraphrases only for the task of noun?noun compound bracketing
(syntactic parsing of three-word noun compounds) without their interpretation.
Other researchers (Pantel and Ravichandran 2004; Pantel and Pennacchiotti 2006;
Pennacchiotti and Pantel 2006) use clustering techniques coupled with syntactic
dependency features to identify IS-A relations in large text collections. Kim and Baldwin
(2005) propose a general-purpose method that computes the lexical similarity of unseen
noun?noun compounds with those found in training. More recently Kim and Baldwin
(2006) developed an automatic method for interpreting noun?noun compounds based
on a set of 20 semantic relations. The relations are detected based on a fixed set of
constructions involving the constituent nouns and a set of seed verbs denoting the
semantic relation (e.g., to own denotes POSSESSION). Then all noun?noun instances
4 These results were obtained on AltaVista on a general and abstract set of eight prepositions (Lauer 1995)
as semantic classification categories: of, for, with, in, on, at, about, and from.
189
Computational Linguistics Volume 35, Number 2
in transitive sentential contexts (i.e., those sentences containing a transitive verb) are
mapped onto the selected set of constructions based on lexical similarity over the verbs.
However, although the Web-based solution might overcome the data sparsity prob-
lem, current probabilistic models are limited because they do not take full advantage of
the structure and the meaning of language.
From a cross-linguistic perspective, there hasn?t been much work on the automatic
interpretation of nominal phrases and compounds. Busa and Johnston (1996), Johnston
and Busa (1996), and Calzolari et al (2002), for example, focus on the differences
between English and Italian noun?noun compounds. In their work they argue that a
computational approach to the cross-linguistic interpretation of these compounds has to
rely on a rich lexical representation model, such as those provided by FrameNet frames
(Baker, Fillmore, and Lowe 1998) and qualia structure (Pustejovsky 1995). In the qualia
structure representation, for example, the meaning of a lexical concept, such as the
modifier in a noun?noun compound, is defined in terms of four elements representing
concept attributes along with their use and purpose. Thus, qualia structure provides
a relational structure that enables the compositional interpretation of the modifier in
relation to the head noun. Two implementations of such representations are provided
by the SIMPLE Project ontology (Lenci et al 2000) and the OMB ontology (Pustejovsky
et al 2006). The SIMPLE ontology, for example, is developed for 12 European languages
and defines entry words that are mapped onto high-level concepts in EuroWordNet
(Vossen 1998), a version of WordNet developed for European languages.
In this article, we use a supervised semantic interpretation model employing rich
linguistic features generated from corpus evidence coupled with word sense disam-
biguation and WordNet concept structure information. The results obtained are com-
pared against two state-of-the-art approaches: a supervised machine learning model,
semantic scattering (Moldovan and Badulescu 2005), and a Web-based unsupervised
model (Lapata and Keller 2005). In this research we do not consider extra cross-linguistic
information, such as semantic classes of Romance nouns (those provided by IS-A re-
lations; e.g., cat belongs to the class of animals) made available, for example, by the
SIMPLE ontology. However, such resources can be added at any time to further improve
the performance of noun?noun interpretation systems.
2.2 Semantics of Prepositions
Although prepositions have been studied intensively in linguistics (Herskovits 1987;
Zelinski-Wibbelt 1993; Linstromberg 1997; Tyler and Evans 2003; Evans and Chilton
2009, among others), they have only recently started to receive more attention in the
computational linguistics community.5 Moreover, the findings from these broad stud-
ies have not yet been fully integrated into NLP applications. For example, although
information retrieval, and even question answering systems, would benefit from the
incorporation of prepositions into their NLP techniques, they often discard them as stop
words.
5 The first Workshop on the Syntax and Semantics of Prepositions, Toulouse, France, 2003; the second
ACL-SIGSEM Workshop on The Linguistic Dimensions of Prepositions and their Use in Computational
Linguistics Formalisms and Applications, Colchester, UK, 2005; the third ACL-SIGSEM Workshop on
Prepositions, Trento, Italy, 2006.
190
Girju The Syntax and Semantics of Prepositions
Prepositions in linguistics
Considerable effort has been allocated to the investigation of spatial prepositions mainly
based on a cognitive approach, not only in English (Herskovits 1987; Linstromberg 1997;
Tyler and Evans 2003; Evans and Chilton 2009), but also in many of the Indo-European
languages (Casadei 1991; Vandeloise 1993; Cadiot 1997; Melis 2002; Luraghi 2003). These
studies provide a detailed analysis of such prepositions trying to give a methodologi-
cal motivated account for the range of their polysemy. These works identify special
constraints on various prepositional patterns, such as semantic restrictions on the noun
phrases occurring as complements of the preposition. For example, in prepositional
phrase constructions such as in NP, the head noun can be a container (in a cup), a
geometrical area (in a region), a geo-political area (in Paris), an atmospheric condition
(in the rain), and so on. These selectional restrictions imposed by the preposition on the
noun phrases it combines with are presented in various formats from lists (Herskovits
1987; Linstromberg 1997) to semantic networks of cluster senses (Tyler and Evans 2003).
In this article we also focus on the polysemy of such prepositions, but we identify the se-
lectional restrictions automatically based on a specialization procedure on the WordNet
IS-A hierarchy. However, unlike Herskovits, we do not consider pragmatic issues such
as relevance and tolerance. These account for the difference that pragmatic motivations
and context dependency make to how expressions are understood. Relevance has to do
with communicative goals and choice of means and is evident, for example, in instances
such as cat on the mat which is still relevant even when only the paws and not the
whole cat are on the mat. Tolerance occurs in situations in which a book, for example,
is described as on the table even though a set of files are placed between it and the
table.
The use of spatial prepositions can also trigger various inferences. For example,
the man at his desk (cf. Herskovits 1987) implies, besides a LOCATION relation, that the
man is using the desk, thus an INSTRUMENT relation. Other inferences are more subtle,
involving spatial reasoning about the actions that can be performed on the arguments
of the preposition. One such instance is infant in a playpen (cf. Tyler and Evans 2003),
where the movement of the playpen involves the movement of the infant. In order to
identify such inferences the automatic interpretation system has to rely on pragmatic
knowledge. In this research we do not deal with such inference issues, rather we identify
the meaning of N P N constructions based on the local context of the sentence.
Prepositions in computational linguistics
In order to incorporate prepositions into various resources and applications, it is neces-
sary to perform first a systematic investigation of their syntax and semantics. Various
researchers (Dorr 1993; Litkowski and Hargraves 2005; Saint-Dizier 2005b; Lersundi
and Aggire 2006) have already provided inventories of preposition senses in English
and other languages. Others have focused on the analysis of verb particles (Baldwin
2006a, 2006b; Villavicencio 2006), the distributional similarity (Baldwin 2005) and the
semantics of prepositions (Kordoni 2005) in a multilingual context, and the meaning
of prepositions in applications such as prepositional phrase attachment (O?Hara and
Wiebe 2003; Kordoni 2006; Volk 2006).
Moreover, although there is a large amount of work in linguistics and computa-
tional linguistics relating to contrastive analysis of prepositions (Busa and Johnston
(1996); Johnston and Busa (1996); Jensen and Nilsson (2005); Kordoni (2005), inter alia),
to our knowledge, there have not been any attempts to provide an investigation of the
prepositions? role in the task of automatic noun phrase interpretation in a large cross-
linguistic English?Romance framework.
191
Computational Linguistics Volume 35, Number 2
3. Linguistic Considerations of Nominal Phrases and Compounds
The meaning of nominal phrases and compounds can be compositional (e.g., spoon
handle?PART?WHOLE, kiss in the morning?TEMPORAL), or idiosyncratic, when the
meaning is a matter of convention (e.g., soap opera, sea lion). These constructions can
also encode metaphorical names (e.g., ladyfinger), proper names (e.g., John Doe), and
dvandva compounds6 in which neither noun is the head (e.g., player?coach).
Moreover, they can also be classified into synthetic (verbal, e.g., truck driver) and
root (non-verbal, e.g., tea cup) constructions.7 It is widely held (Levi 1978; Selkirk 1982b)
that the modified noun of a synthetic noun?noun compound, for example, may be
associated with a theta-role of the compound?s head noun, which is derived from a
verb. For instance, in truck driver, the noun truck satisfies the THEME relation associated
with the direct object in the corresponding argument structure of the verb to drive.
In this article we address English?Romance compositional nominal phrases and
compounds of the type N N (noun?noun compounds which can be either genitive-
marked or not genitive-marked) and N P N, and disregard metaphorical names, proper
names, and dvandva structures. In the following we present two state-of-the-art se-
mantic classification sets used in automatic noun?noun interpretation and analyze their
distribution in two different corpora.
3.1 Lists of Semantic Classification Relations
Although researchers (Jespersen 1954; Downing 1977) argued that noun?noun com-
pounds, and noun phrases in general, encode an infinite set of semantic relations,
many agree (Levi 1978; Finin 1980) there is a limited number of relations that occur
with high frequency in these constructions. However, the number and the level of
abstraction of these frequently used semantic categories are not agreed upon. They can
vary from a few prepositions (Lauer 1995) to hundreds and even thousands of more
specific semantic relations (Finin 1980). The more abstract the category, the more noun
phrases are covered, but also the larger the variation as to which category a phrase
should be assigned. Lauer, for example, classifies the relation between the head and the
modifier nouns in a noun?noun compound by making use of a set of eight frequently
used prepositions: of, for, with, in, on, at, about, and from. However, according to this
classification, the noun?noun compound love story, for instance, can be classified both
as story of love and story about love. The main problem with these abstract categories
is that much of the meaning of individual compounds is lost, and sometimes there is
no way to decide whether a form is derived from one category or another. On the other
hand, lists of very specific semantic relations are difficult to build as they usually contain
a very large number of predicates, such as the list of all possible verbs that can link the
noun constituents. Finin, for example, uses semantic categories such as dissolved in to
build interpretations of compounds such as salt water and sugar water.
In this article we experiment with two sets of semantic classification categories
defined at different levels of abstraction. The first is a core set of 22 semantic relations
(SRs), a set which was identified by us from the linguistics literature and from various
experiments after many iterations over a period of time (Moldovan and Girju 2003).
6 The term dvandva comes from Sanskrit, translates literally as ?two-and-two? and means ?pair?.
7 In the linguistic literature the words ?synthetic? and ?root? have been coined for noun?noun compounds.
Because these terms apply also to nominal phrases, we use them in relation to these constructions as well.
192
Girju The Syntax and Semantics of Prepositions
Moldovan and Girju proved empirically that this set is encoded by noun?noun pairs in
noun phrases; the set is a subset of their larger list of 35 semantic relations used in a large
set of semantics tasks. This list, presented in Table 1 along with examples and semantic
argument frames, is general enough to cover a large majority of text semantics while
keeping the semantic relations to a manageable number. A semantic argument frame is
defined for each semantic relation and indicates the position of each semantic argument
in the underlying relation. For example, ?Arg2 is part of (whole)Arg1? identifies the part
(Arg2) and the whole (Arg1) entities in this relation. This representation is important
because it allows us to distinguish between different arrangements of the arguments
for given relation instances. For example, most of the time, in N N compounds Arg1
precedes Arg2, whereas in N P N constructions the position is reversed (Arg2 P Arg1).
However, this is not always the case as shown by N N instances such as ham/Arg2
sandwich/Arg1 and spoon/Arg1 handle/Arg2, both encoding PART?WHOLE. More details
on subtypes of PART?WHOLE relations are presented in Section 6.2. A special relation
here is KINSHIP, which is encoded only by N P N constructions and whose argument
order is irrelevant. Thus, the labeling of the semantic arguments for each relation as
Arg1 and Arg2 is just a matter of convention and they were introduced to provide a
consistent guide to the annotators to easily test the goodness-of-fit of the relations. The
examples in column 4 are presented with their WordNet senses identified in context
from the CLUVI and Europarl text collections, where the specific sense is represented
as the sense number preceded by a ?#? sign.
The second set is Lauer?s list of eight prepositions (exemplified in Table 2) and can
be applied only to noun?noun compounds, because in N P N instances the preposition
is explicit. We selected these two state-of-the-art sets as they are of different size and
contain semantic classification categories at different levels of abstraction. Lauer?s list is
more abstract and thus capable of encoding a large number of noun?noun compound
instances found in a corpus (e.g., many N1 N2 instances can be paraphrased as N2 of
Table 1
The set of 22 semantic relations along with examples interpreted in context and the semantic
argument frame.
Semantic
No. relations Default argument frame Examples
1 POSSESSION Arg1 POSSESSES Arg2 family#2/Arg1 estate#2/Arg2
2 KINSHIP Arg1 IS IN KINSHIP REL. WITH Arg2 the sister#1/Arg2 of the boy#1/Arg1
3 PROPERTY Arg2 IS PROPERTY OF Arg1 lubricant#1/Arg1 viscosity#1/Arg2
4 AGENT Arg1 IS AGENT OF Arg2 investigation#2/Arg2 of the police#1/Arg1
5 TEMPORAL Arg1 IS TEMPORAL LOCATION OF Arg2 morning#1/Arg1 news#3/Arg2
6 DEPICTION-DEPICTED Arg2 DEPICTS Arg1 a picture#1Arg2 of my nice#1/Arg1
7 PART-WHOLE Arg2 IS PART OF (whole) Arg1 faces#1/Arg2 of children#1/Arg1
8 HYPERNYMY (IS-A) Arg1 IS A Arg2 daisy#1/Arg1 flower#1/Arg2
9 CAUSE Arg1 CAUSES Arg2 scream#1/Arg2 of pain#1/Arg1
10 MAKE/PRODUCE Arg1 PRODUCES Arg2 chocolate#2/Arg2 factory#1/Arg1
11 INSTRUMENT Arg1 IS INSTRUMENT OF Arg2 laser#1/Arg1 treatment#1/Arg2
12 LOCATION Arg2 IS LOCATED IN Arg1 castle#1/Arg2 in the desert#1/Arg1
13 PURPOSE Arg1 IS PURPOSE OF Arg2 cough#1/Arg1 syrup#1/Arg2
14 SOURCE Arg1 IS SOURCE OF Arg2 grapefruit#2/Arg1 oil#3/Arg2
15 TOPIC Arg1 IS TOPIC OF Arg2 weather#1/Arg1 report#2/Arg2
16 MANNER Arg1 IS MANNER OF Arg2 performance#3/Arg2 with passion#1/Arg1
17 MEANS Arg1 IS MEANS OF Arg2 bus#1/Arg1 service#1/Arg2
18 EXPERIENCER Arg1 IS EXPERIENCER OF Arg2 the fear#1/Arg2 of the girl#1/Arg1
19 MEASURE Arg2 IS MEASURE OF Arg1 inches#1/Arg2 of snow#2/Arg1
20 TYPE Arg2 IS A TYPE OF Arg1 framework#1/Arg1 law#2/Arg2
21 THEME Arg1 IS THEME OF Arg2 acquisition#1/Arg2 of stock#1/Arg1
22 BENEFICIARY Arg1 IS BENEFICIARY OF Arg2 reward#1/Arg2 for the finder#1/Arg1
OTHERS cry of death
193
Computational Linguistics Volume 35, Number 2
Table 2
Lauer?s set of prepositions along with examples interpreted in context.
No. Preposition Examples
1 of sea bottom (bottom of the sea)
2 for leisure boat (boat for leisure)
3 with spoon feeding (feeding with a spoon)
4 in London house (house in London)
5 on Saturday snowstorm (snowstorm on Saturday)
6 at night flight (flight at night)
7 about war story (story about war)
8 from almond butter (butter from almonds)
N1), whereas our list contains finer grained semantic categories (e.g., only some N1 N2
instances encode a CAUSE relation).
In the next section, we present the coverage of these semantic lists on two different
corpora, how well they solve the interpretation problem of noun phrases, and the
mapping from one list to another.
3.2 Corpus Analysis
For a better understanding of the semantic relations encoded by N N and N P N
instances, we analyzed the semantic behavior of these constructions on two large
cross-linguistic corpora of examples. Our intention is to answer questions like:
(1) What syntactic constructions are used to translate the English instances to the target
Romance languages and vice versa? (cross-linguistic syntactic mapping)
(2) What semantic relations do these constructions encode? (cross-linguistic semantic
mapping)
(3)What is the corpus distribution of the semantic relations per each syntactic construction?
(4) What is the role of English and Romance prepositions in the semantic interpretation of
nominal phrases and compounds?
For questions (1) and (2), we expand the work of Selkirk (1982b), Grimshaw
(1990), Giorgi and Longobardi (1991), and Alexiadou, Haegeman, and Stavrou (2007)
on the syntax of noun phrases in English and Romance languages by providing
cross-linguistic empirical evidence for in-context instances on two different corpora
based on the set of 22 semantic tags. Following a configurational approach, Giorgi and
Longobardi, for example, focus only on synthetic nominal phrases, such as the capture
of the soldier (THEME), where the noun capture is derived through nominalization
from the verb to capture. Besides synthetic constructions, we also consider root nominal
phrases and compounds, such as family estate (POSSESSION).
The data
In order to perform empirical investigations of the semantics of nominal phrases and
compounds, and to train and test a learning model for the interpretation of noun?noun
194
Girju The Syntax and Semantics of Prepositions
instances encoded by these constructions, we collected data from two text collections
with different distributions and of different genres, Europarl and CLUVI.
The Europarl data were assembled by combining four of the bilingual sentence-
aligned corpora made public as part of the freely available Europarl corpus. Specif-
ically, the Spanish?English, Italian?English, French?English and Portuguese?English
corpora were automatically aligned based on exact matches of English translations.8
Then, only those English sentences which appeared verbatim in all four language pairs
were considered. The resulting English corpus contained 10,000 sentences which were
syntactically parsed using Charniak?s parser (Charniak 2000). From these we extracted
6,200 token instances of N N (49.62%) and N P N (50.38%) constructions.
CLUVI (Linguistic Corpus of the University of Vigo) is an open text repository of
parallel corpora of contemporary oral and written languages, a resource that besides
Galician also contains literary text collections in other Romance languages. Because the
collection provides translations into only two of the Romance languages considered
here, Spanish and Portuguese, we focused only on the English?Portuguese and English?
Spanish literary parallel texts from the works of Agatha Christie, James Joyce, and H. G.
Wells, among others. Using the CLUVI search interfaces we created a sentence-aligned
parallel corpus of 4,800 unique English?Portuguese?Spanish sentences. The English
version was syntactically parsed using Charniak?s parser (Charniak 2000) after which
each N N and N P N instance was manually mapped to the corresponding translations.
The resulting corpus contains 2,310 English token instances with a distribution of
25.97% N N and 74.03% N P N.
Corpus annotation and inter-annotator agreement
For each corpus, each nominal phrase and compound instance was presented separately
to two experienced annotators9 in a Web interface in context along with the English
sentence and its translations. Because the corpora do not cover some of the languages
(Romanian in Europarl, and Romanian, Italian, and French in CLUVI), three other
native speakers of these languages who were fluent in English provided the translations,
which were added to the list. The two computational semantics annotators had to tag
each English constituent noun with its corresponding WordNet sense.10 If the word was
not found in WordNet the instance was not considered. The annotators were also asked
to identify the translation phrases, tag each instance with the corresponding semantic
relation, and identify the semantic arguments Arg1 and Arg2 in the semantic argument
frame of the corresponding relation. Whenever the annotators found an example encod-
ing a semantic relation or a preposition paraphrase other than those provided, or if they
did not know what interpretation to give, they had to tag it as OTHER-SR (e.g., melody
of the pearl: here the context of the sentence did not indicate the association between the
two nouns; cry of death: the cry announcing death), and OTHER-PP (e.g., box by the wall,
searches after knowledge) respectively.
Tagging each noun constituent with the corresponding WordNet sense in context is
important not only as a feature employed in the training models, but also as guidance
for the annotators to select the right semantic relation. For instance, in the follow-
ing sentences, daisy flower expresses a PART?WHOLE relation in Example (1) and an
8 This version of the Europarl text collection does not include Romanian.
9 The annotators have extensive expertise in computational semantics and are fluent in at least three of the
Romance languages considered for this task.
10 We used version 2.1 of WordNet.
195
Computational Linguistics Volume 35, Number 2
IS-A relation in Example (2) depending on the sense of the noun flower (cf. WordNet
2.1: flower#2 is a ?reproductive organ of angiosperm plants especially one having
showy or colorful parts,? whereas flower#1 is ?a plant cultivated for its blooms or
blossoms?).
(1) Usually, more than one daisy#1 flower#2 grows on top of a single stem.
(2) Try them with orange or yellow flowers of red-hot poker, solidago, or other late
daisy#1 flowers#1, such as rudbeckias and heliopsis.
In cases where noun senses were not enough for relation selection, the annotators
had to rely on a larger context provided by the sentence and its translations.
Moreover, because the order of the semantic arguments in a nominal phrase or
noun?noun compound is not fixed (Girju et al 2005), the annotators were presented
with the semantic argument frame for each of the 22 semantic relations and were
asked to tag the instances accordingly. For example, in PART?WHOLE instances such
as chair/Arg1 arm/Arg2 the part arm follows the whole chair, whereas in spoon/Arg1
handle/Arg2 the order is reversed. In the annotation process the translators also used
the five corresponding translations as additional information in selecting the semantic
relation. For instance, the context provided by the Europarl English sentence in Exam-
ple (3) does not give enough information for the disambiguation of the English nominal
phrase judgment of the presidency, where the modifier noun presidency can be either
AGENT or THEME in relation to the nominalized noun head judgment. The annotators
had to rely on the Romance translations in order to identify the correct meaning in
context (THEME): valoracio?n sobre la Presidencia (Sp. ? Spanish), avis sur la pre?sidence
(Fr. ? French), giudizio sulla Presidenza (It. ? Italian), veredicto sobre a Preside?ncia (Port. ?
Portuguese), evaluarea Pres?endint?iei (Ro. ? Romanian).
Most of the time, one instance was tagged with one semantic relation, and one
preposition paraphrase (in case of noun?noun compounds), but there were also situa-
tions in which an example could belong to more than one category in the same context.
For example, Texas city is tagged as PART?WHOLE, but also as a LOCATION relation using
the 22-SR classification set, and as of, from, in based on the 8-PP set (e.g., city of Texas,
city from Texas, and city in Texas). Overall, 8.2% CLUVI and 4.8% Europarl instances
were tagged with more than one semantic relation, and almost half of the noun?noun
compound instances were tagged with more than one preposition.
(3) En.: If you do , the final judgment of the Spanish presidency will be even more
positive than it has been so far.
Sp.: Si se hace, la valoracio?n sobre la Presidencia espan?ola del Consejo
sera? au?n mucho ma?s positiva de lo que es hasta ahora.
Fr.: Si cela arrive, notre avis sur la pre?sidence espagnole du Conseil sera
encore beaucoup plus positif que ce n?est de?ja` le cas.
It.: Se ci riuscira`, il nostro giudizio sulla Presidenza spagnola sara` ancora
piu` positivo di quanto non sia stato finora.
Port.: Se isso acontecer, o nosso veredicto sobre a Preside?ncia espanhola sera?
ainda muito mais positivo do que o actual.
Ro.: Daca? are loc, evaluarea Pres?edint?iei spaniole va fi ??nca? mai positiva
deca?t pa?na? acum.
196
Girju The Syntax and Semantics of Prepositions
Thus, the corpus instances used in the corpus analysis phase have the following
format: ?NPEn; NPEs; NPIt; NPFr; NPPort; NPRo; target?. The word target is one of the
23 (22 + OTHER-SR) semantic relations and one of the eight prepositions considered for
noun compound instances, and one of the 23 semantic relations for N P N instances. For
example, ?development cooperation; cooperacio?n para el desarrollo; cooperazione allo sviluppo;
coope?ration au de?veloppement; cooperac?a?o para o desenvolvimento; cooperare de dezvoltare;
PURPOSE / FOR?.
Inter-annotator agreement was measured using kappa, one of the most frequently
used measures of inter-annotator agreement for classification tasks: K = Pr(A)?Pr(E)1?Pr(E) ,
where Pr(A) is the proportion of times the annotators agree and Pr(E) is the probability
of agreement by chance. The K coefficient is 1 if there is a total agreement among
the annotators, and 0 if there is no agreement other than that expected to occur by
chance.
The kappa values along with percentage agreements obtained on each corpus are
shown in Table 3. We also computed the number of instances that were tagged with
OTHER by both annotators for each semantic relation and preposition paraphrase, over
the number of examples classified in that category by at least one of the judges. For
the instances that encoded more than one classification category, the agreement was
measured on the first relation on which the annotators agreed.
The agreement obtained for the Europarl corpus is higher than that for CLUVI
on both classification sets. Overall, the K coefficient shows a fair to good level of
agreement for the corpus data on the set of 22 relations, with a higher agreement for the
preposition paraphrases. However, according to Artstein (2007), kappa values can drop
significantly if the frequency distribution of the annotation categories in the text corpus
is skewed. This is the case here, as will be shown in the next section. Thus, for a better
understanding of the annotation results we also computed the percentage agreement,
which is indicated for each classification set in parentheses in Table 3.
7.8% of Europarl and 5.7% of CLUVI instances that could not be tagged with
Lauer?s prepositions were included in the OTHER-PP category. From these, 2.1% and
2.3%, respectively, could be paraphrased with prepositions other than those considered
by Lauer (e.g., bus service: service by bus), and 5.7% and 3.4%, respectively, could not be
paraphrased with prepositions (e.g., daisy flower).
In the next section we discuss the distribution of the syntactic and semantic inter-
pretation categories on the two different cross-linguistic corpora.
Table 3
The inter-annotator agreement on the annotation of the nominal phrases and compounds in the
two corpora. For the instances that encoded more than one classification category, the agreement
was measured on the first relation on which the annotators agreed. N/A = not applicable.
Kappa Agreement
(% agreement)
Corpus Classification tag sets N N N P N OTHER
Europarl 8 PPs 0.80 (85.4%) N/A 91%
22 SRs 0.61 (76.1%) 0.67 (80.8%) 78%
CLUVI 8 PPs 0.77 (84.7%) N/A 86%
22 SRs 0.56 (73.8%) 0.58 (75.1%) 69%
197
Computational Linguistics Volume 35, Number 2
3.3 Distribution of Syntactic Constructions and Semantic Relations
A. Cross-linguistic distribution and mapping of nominal phrases and compounds
Table 4 shows the distribution of various syntactic constructions used for the
translation of the 6,200 (3,076 N N and 3,124 N P N) Europarl and 2,310 (600 N N
and 1,710 N P N) CLUVI English token instances in each of the five target languages
considered. The data show that N N and N P N constructions cover over 83% of the
translation patterns for both text corpora. However, whereas the distribution of both
constructions is balanced in the Europarl corpus (about 45%, with the exception of
Romanian for which N P N constructions are less frequent), in CLUVI the N P N
constructions occur in more than 85% of the cases (again, with the exception of Ro-
manian where they represent about 56% of the data). The high percentage obtained for
N P N instances in CLUVI is explained by the fact that Romance languages have very
few N N compounds which are of limited semantic types, such as TYPE. Moreover, it
is interesting to note here that some of the English instances are translated into both
noun?noun (N N) and noun?adjective (N A) compounds in the target languages. For
example, love affair translates into either the N A construction enredo amoroso (Spanish),
aventure amoureuse (French), relazione amorosa (Italian), relac?ao amorosa (Portuguese),
and aventura? amoroasa? (Romanian), or using the more common N de N pattern aventura
de amor (Spanish), aventure d?amour (French), storia d?amore (Italian), estoria de amor
(Portuguese), and aventura? de dragoste (Romanian). There are also instances which
translate as one word in the target language, shown in Table 4, column 6. For example,
Table 4
The distribution of syntactic constructions used in the translation of 6,200 Europarl and 2,310
English NN and N P N instances. N A = noun?adjective; pph = other syntactic paraphrase.
Syntactic distribution
Corpus Language N N N P N N A word pph Total
French 2,747 2,896 372 37 148
(44.31%) (46.71%) (5.99%) (0.6%) (2.39%)
Italian 2,896 2,413 520 111 260
(46.71%) (38.92%) (8.38%) (1.8%) (4.19%)
Europarl Spanish 2,896 2,487 483 36 298
(46.71%) (40.12%) (7.79%) (0.58%) (4.80%) 6,200
Portuguese 2,858 2,301 594 75 372
(46.1%) (37.11%) (9.58%) (1.21%) (6%)
Romanian 4,010 1,596 297 74 223
(64.68%) (25.74%) (4.79%) (1.19%) (3.6%)
French 32 1,967 94 154 63
(1.39%) (85.15%) (4.07%) (6.66%) (2.73%)
Italian 25 2,046 75 113 51
(1.08%) (88.57%) (3.25%) (4.89%) (2.21%)
CLUVI Spanish 25 1,959 107 163 56
(1.08%) (84.81%) (4.63%) (7.06%) (2.42%) 2,310
Portuguese 25 1,990 163 88 44
(1.08%) (86.15%) (7.05%) (3.81%) (1.91%)
Romanian 758 1,295 88 125 44
(32.81%) (56.06%) (3.81%) (5.41%) (1.91%)
198
Girju The Syntax and Semantics of Prepositions
ankle boot is translated into bottine in French and stivaletto in Italian. The rest of the data
is encoded by other syntactic paraphrases, as shown in Table 4, column 7. For example,
bomb site is translated into Italian as luogo dove e` esplosa la bomba (?the place where
the bomb has exploded?). Moreover, Table 5 shows the distribution of the prepositions
present in the N P N translations.
Table 5
The distribution of N P N constructions used in the translation of the English noun phrase
instances on both text corpora. The preposition a is used to denote a, ad, and de to denote simple
and articulated prepositions (de, di, du, de la, della, degli, d?, etc.).
Corpus Language N P N distribution Total
English of (81.15%); for (3.27%); in (4.61%); on (2.43%); 3,124
at (1.22%); from (0.67%); with (2.85%);
by (1.5%); against (0.42%); through (0.29%);
under (0.42%); after (0.38%); before (0.85%)
French de (75.69%); a` (2.93%); pour (6.42%); par (1.42%); 2,896
en (1.62%); avec (1.6%) ; devant (1.6%);
apre`s (1.21%); dans (2.11%); sur (2.6%);
contre (0.4%); avant (0.4%)
Italian de (71.78%); a (7%); su (1.29%); a (3.11%); 2,413
da (6.59%); per (6.22%); via (0.79%); in (0.79%);
con (1.41%); contra (0.62%); davanti (0.2%);
dopo (0.2%)
Europarl Spanish de (83.39%); a (1.81%); en (1.41%); para (3.5%); 2,487
por (2.61%); con (3.18%); sobre (3.3%);
contra (0.4%); en materia de (0.4%)
Portuguese de (78.4%); a (0.8%); em (0.8%); para (3.5%); 2,301
por (1.6%); com (0.8%); sobre (1.3%);
antes de (0.4%)
Romanian de (82.2%); ??nainte de (1.82%); cu (1.82%); pentru (4.51%); 1,596
despre (1.63%); la (0.38%); datorita? (0.38%);
pe (6.08%); pe calea (0.37%); ??n (0.81%)
English of (83.80%); for (1.17%); in (5.90%); on (2.40%); 1,710
at (0.76%); with (1.99%); against (1.17%);
through (0.41%); over (0.41%); above (0.41%);
beside (0.41%); about (0.41%); behind (0.76%)
French de (82.33%); a` (6.2%); pour (1.42%); en (1.8%); 1,967
sur (7.02%); contre (0.41%); pre`s de (0.41%);
a` cote? de (0.41%)
Italian de (75.42%); a (8.07%); su (1.32%); 2,046
da (6.6%); per (6.21%); in (0.78%); con (0.4%);
contra (0.4%); sopra (0.2%); accanto a (0.2%);
dietro de (0.2%); via (0.2%)
CLUVI Spanish de (85.96%); a (2.81%); en (3.89%); para (0.71%); 1,959
por (1.74%); con (2.1%); sobre (1.38%);
contra (0.36%); detra?s (0.71%); encima (0.36%)
Portuguese de (78.4%); a (0.8%); em (0.82%); para (3.5%); 1,990
por (1.6%); com (0.8%); sobre (1.3%);
acima de (0.4%)
Romanian de (85.21%); cu (1.82%); pentru (4.5%); 1,295
la (0.4%); datorita? (0.4%); pe (5.08%);
despre (1.58%); ??n (0.79%); la?nga ( 0.2%)
199
Computational Linguistics Volume 35, Number 2
For the purposes of this research, from the 6,200 Europarl and 2,310 CLUVI
instances, we selected those which had all the translations encoded only by N N and
N P N constructions. Columns 3 and 4 in Table 4 show the number of N N and N P N
translation instances in each Romance language. Out of these, we considered only
3,124 Europarl and 2,023 CLUVI token instances representing the examples encoded
by N N and N P N in all languages considered, after inter-annotator agreement.
B. Cross-linguistic distribution of semantic relations and their mapping to nominal
phrases and compounds
A closer look at the N N and N P N translation instances in Table 4 shows that
their syntactic distribution is influenced by the text genre and the semantics of the
instances. For example, in Europarl most of the N N instances were naming noun?
noun compounds referring to entities such as member states and framework law which
were repeated in many sentences. Many of them encoded TYPE relations (e.g., member
state, framework law) which, most of the time, are encoded by N N patterns in the target
languages (stato membro and legge quadro in Italian, respectively). In the CLUVI corpus,
on the other hand, the N N Romance translations represented only 1% of the data. A
notable exception here is Romanian (64.68% of Europarl and 32.8% of CLUVI). This is
explained by the fact that, in Romanian, many noun phrases are represented as genitive-
marked noun compounds (N1 N2). In Romanian the genitive case is realized either as
a suffix attached to the modifier noun N2 or as one of the genitival articles a/al/ale. If
the modifier noun N2 is determined by an indefinite article then the genitive mark is
applied to the article, not to the noun, for example o fata? ? unei fete (?a girl ? of/to a
girl?) and un ba?iat ? unui ba?iat (?a boy ? of/to a boy?). Similarly, if the modifier noun is
determined by the definite article (which is enclitic in Romanian), the genitive mark is
added at the end of the noun together with the article. For example, fata?fetei (the girl ?
girl-GEN), cartea?ca?rt?ii (the book ? book-GEN). Thus, the noun phrase the beauty of the
girl, for instance, is translated as frumuset?ea fetei (?beauty-the girl-GEN?), and the beauty of
a girl as frumuset?ea unei fete (?beauty-the of/to a girl?).
In general, in Romanian the choice between the N de N and the genitive-marked
N N constructions depends on the specificity of the instance. Some noun?noun instances
refer to a specific entity (existential interpretation), in which case the construction
preferred is the genitive-marked N N, or they can refer in general to the category of
those entities (generic interpretation),11 thus using N de N. For example, the instance
the bite of the scorpion (AGENT) translates into mus?ca?tura scorpionului (?bite-the scorpion-
GEN?), whereas a scorpion bite (AGENT) translates into mus?ca?tura? de scorpion (?bite of
scorpion?).
Many semantic relations that allow both the generic and the existential interpre-
tations can be encoded by both N P N and genitive-marked N N constructions as
shown by the example above. However, there are situations when the generic and
the existential interpretations change the meaning of the noun?noun pair. One such
example is the suit of the sailor (POSSESSION) translated as costumul marinarului (?suit-
the sailor-GEN?), and sailor suit (PURPOSE) translated as costum de marinar (?suit of
sailor?).
11 The words existential and generic are borrowed here from the vast linguistic literature on definite and
indefinite descriptions. Here, nouns such as firemen can have different readings in various contexts:
Firemen are available (existential reading), vs. Firemen are altruistic (generic reading).
200
Girju The Syntax and Semantics of Prepositions
At the other extreme there are relations which prefer either the generic or the
existential interpretation. For example, some POSSESSION-encoding instances such as
the budget of the University translate as ?bugetul Universita?t?ii? (budget-the University-GEN)
and not as ?bugetul de Universitate? (budget-the of University). Other relations such as
PURPOSE and SOURCE identify generic instances. For example, (a) olive oil (SOURCE)
translates as ?ulei de ma?sline? (oil of olive), and not as ?uleiul ma?slinei? (oil-the olive-
GEN), and (b) the milk glass (PURPOSE) translates as ?paharul de lapte? (glass-the of milk)
and not as ?paharul laptelui? (glass-the milk-GEN). Other examples include CAUSE and
TOPIC. This observation is very valuable for the interpretation of nominal phrases and
compounds and is used in the learning model to discriminate among the possible
interpretations.
Tables 6 and 7 show the semantic distribution of the instances on both text corpora.
This distribution is represented both in number of tokens (the total number of instances
per relation) and types (the unique number of instances per relation). In Europarl,
the most frequently occurring relations are TYPE and THEME that together represent
about 50% of the data with an equal distribution. The next most frequent relations
are TOPIC, PURPOSE, AGENT, and PROPERTY with an average coverage of about 8%.
Moreover, eight relations of the 22-SR set (KINSHIP, DEPICTION, CAUSE, INSTRUMENT,
SOURCE, MANNER, MEASURE, and BENEFICIARY) did not occur in this corpus. The
9.61% of the OTHER-SR relation represents the ratio of those instances that did not
encode any of the 22 semantic relations. It is interesting to note here the large difference
between the number of types versus tokens for the TYPE relation in Europarl. This is
accounted for by various N N instances such as member states that repeat across the
corpus.
This semantic distribution contrasts with the one in CLUVI. Here, the most fre-
quent relation by far is PART?WHOLE (40.53%), followed by LOCATION (8.95%), AGENT
(6.23%), and IS-A (5.93%). The missing relations are KINSHIP, MANNER and BENEFI-
CIARY. A larger percentage of OTHER-SR instances (12.95%) did not encode any of the
22 semantic relations. Moreover, in CLUVI 256 instances were tagged with more than
one semantic relation with the following distribution: 46.8% MEASURE/PART?WHOLE
(e.g., a couple of cigarettes), 28.2% PART?WHOLE/LOCATION (e.g., bottom of the sea), 10.9%
MEASURE/LOCATION (e.g., cup of chocolate), 8.2% PURPOSE/LOCATION (e.g., waste gar-
den), and 5.9% THEME/MAKE-PRODUCE (e.g., makers of songs). In Europarl, on the other
hand, there were only 97 such cases: 81.4% THEME/MAKE-PRODUCE (e.g., bus manufac-
turers) and 18.6% MEASURE/PART?WHOLE (e.g., number of states).
One way to study the contribution of both the English and Romance prepositions
to the interpretation task is to look at their distribution over the set of semantic relations
on two reasonably large text corpora of different genres. Of course, this approach does
not provide an analysis that generates an exhaustive generalization over the properties
of the language. However, as Tables 6 and 7 show, there are dependencies between the
structure of the Romance language translations and the semantic relations encoded by
the nominal phrases and compounds, although the most frequently occurring preposi-
tions are de and its English equivalent of. Here we use the preposition de to represent a
set of translation equivalents in Romance languages (e.g., the Italian counterpart is di).
These prepositions are semantically underspecified, encoding a large set of semantic
relations. The many-to-many mappings of the prepositions to the semantic classes
adds to the complexity of the interpretation task. For example, in the Europarl corpus
LOCATION is encoded in French by de, sur, devant, and a` pre`s de, while TOPIC is encoded
in English by of, for, on, about and noun compounds, and in Spanish by de, sobre, en
materia de.
201
Computational Linguistics Volume 35, Number 2
Table 6
Mapping between the set of 22 semantic classification categories and the set of English and
Romance syntactic constructions on the Europarl corpus. The preposition de is used here to
denote simple and articulated prepositions (de, di, du, de la, della, degli, d?, etc.). Also, the dash ???
refers to noun?noun compounds where there is no connecting preposition. The mapping was
obtained on the 3,124 Europarl instance corpus. En. = English; Sp. = Spanish; It. = Italian;
Fr. = French; Port. = Portuguese; Ro. = Romanian.
Total
Token Type
Nr. SRs En. Sp. It. Fr. Port. Ro. [%] [%] Example
1 POSSESSION of, ? de, ? de, ? de de, ? 2.85 2.4 Union resources
?resursele uniunii? (Ro.)
(resource-the union-GEN)
2 KINSHIP 0 0
3 PROPERTY of, for, de de de de de, ? 6.05 6.05 traffic density
in, ? ?densita` del traffico? (It.)
(density of traffic)
4 AGENT of, for, de de,? de de de 7.47 7.08 request of a member
in, by, ? ?richiesta di uno membro? (It.)
(request of a member)
5 TEMPORAL of, in, de, con de de, de de, 0.04 0.04 year before the constitution
on, at, ? a avant acima de ??nainte de ?an?o anterior a la constitucio?n? (Sp.)
(the year previous of the)
constitution
6 DEPICTION 0 0
7 PART?WHOLE of, in, de, con de, a, de, a` de, de 3.20 2.75 Union citizen
with, ? a, ? con ?citoyen de l? Union? (Fr.)
(citizen of the Union)
8 IS?A of, ? de, ? de, ? de, ? ? ? 0.8 0.8 process of decay
(HYPERNYMY) with ?proces de descompunere? (Ro.)
(process of decay)
9 CAUSE 0 0
10 MAKE/ of, for, ? de de de de de 1.43 1.43 paper plant
PRODUCE in, from ?fa?brica de papel? (Sp.)
(plant of paper)
11 INSTRUMENT 0 0
12 LOCATION of, in, de, en, de, su, de, sur, de de, pe, 2.14 2.14 place of the meeting
on, ? sobre a, in a`, pre`s de, la, ??n ?lieu de la re?union? (Fr.)
at devant (place of the meeting)
13 PURPOSE of, ? de, por, de, da, contre, a`, de, a de, 7.48 7.23 building stone
for para, per, a, de, ? pentru ?pedras de construc?a?o? (Port.)
contra ? pour (stones of building)
14 SOURCE 0 0
15 TOPIC of, for, de, sobre, de, a, de de, de, 11.03 11.03 policy on asylum
on, ? en materia su sobre despre ?pol??tica en materia de asilo? (Sp.)
about de (policy in regard to asylum)
16 MANNER 0 0
17 MEANS by por, en, per, in, en, a`, por pe, cu, 0.07 0.07 travel by train
de, ? a, via par pe calea ?calatorie cu tenul? (Ro.)
(travel with train-the)
18 EXPERIENCER of,? de de de de de, ? 0.04 0.04 suffering of the people
in ?sofrimento das pessoas? (Port.)
in (suffering of the people)
19 MEASURE 0 0
20 TYPE ? ? ? ? ? ? 24.47 1.7 framework law
?legge quadro? (It.)
(law framework)
21 THEME of, for, de de, a de de de 23.13 19.2 conflict prevention
in, ? ?prevenire de conflict? (Ro.)
(prevention of conflict)
22 BENEFICIARY 0 0
23 OTHER?SR of, by de a, de de, a` de, a, de, 9.61 8.13 tobacco addiction
com pentru ?adiccio?n a tabaco? (Sp.)
(addiction to tobacco)
Total no. of examples 3,124 2,190
Moreover, in the Europarl corpus, 31.64% of the instances are synthetic phrases en-
coding AGENT, MEANS, LOCATION, THEME, and EXPERIENCER. Out of these instances,
98.7% use the preposition of and its Romance equivalent de. In the CLUVI corpus,
14.1% of the examples were verbal, from which the preposition of/de has a coverage
of 77.66%.
Based on the literature on prepositions (Lyons 1986; Barker 1998; Ionin,
Matushansky, and Ruys 2006) and our own observations, the preposition of/de in both
root and synthetic nominal phrases may have a functional or a semantic role, acting
as a linking device with no apparent semantic content, or with a meaning of its own.
Thus, for the interpretation of these constructions a system must rely on the meaning of
preposition and the meaning of the two constituent nouns in particular, and on context
202
Girju The Syntax and Semantics of Prepositions
Table 7
Mapping between the set of 22 semantic classification categories and the set of English and
Romance syntactic constructions on the CLUVI corpus. The preposition de is used here to denote
simple and articulated prepositions (de, di, du, de la, della, degli, d?, etc.). Also, the dash ??? refers
to noun?noun compounds where there is no connecting preposition. The mapping was obtained
on the 2,023 CLUVI instance corpus. En. = English; Sp. = Spanish; It. = Italian; Fr. = French;
Port. = Portuguese; Ro. = Romanian.
Total
Token Type
Nr. SRs En. Sp. It. Fr. Port. Ro. [%] [%] Example
1 POSSESSION of, ? de, ? de, ? de de de, ? 1.35 1.21 police car
?coche de polizia? (Sp.)
(car of police)
2 KINSHIP 0 0
3 PROPERTY of, for, de de de de de, ? 2.97 2.76 beauty of the buildings
in, ? ?belleza de los edificios? (Sp.)
(beauty of the buildings)
4 AGENT of, for, de de, ? de ? de, ? 6.23 5.78 return of the family
in, by, ? ?regresso da fam??lia? (Port.)
(return of the family)
5 TEMPORAL of, in, de, con de de de de 2.97 2.97 spring rain
on, at, ? ?pluie de printemps? (Fr.)
(rain of spring)
6 DEPICTION? of de de de de de 0.3 0.3 picture of a girl
DEPICTED ?retrato de uma rapariga? (Port.)
(picture of a girl)
7 PART?WHOLE of, in, de, con de, a, de, a` de 40.53 34.35 ruins of granite
with, ? ? com de, ? ?ruinas de granito? (Sp.)
(ruins of granite)
8 IS?A of, ? de, ? de, ? de, ? ? de 5.93 5.4 sensation of fear
(HYPERNYMY) with ?sensac?a?o de medo? (Port.)
(sensation of fear)
9 CAUSE from, ? de de, da de de de, 2.72 2.72 cries of delight
datorita? ?cri de joie? (Fr.)
(cries of delight)
10 MAKE/ of, for, de de de de de 0.29 0.29 noise of the machinery
PRODUCE in, from, ? ?ruido de la maquinaria? (Sp.)
(noise of the machinery)
11 INSTRUMENT for, with de, ? de, a, de, a` de de, cu 0.29 0.29 a finger scratch
con ?o zga?rietura? de unghie? (Ro.)
(a scratch of finger)
12 LOCATION of, in, de, en, de, su, de, sur, de, em de, pe, la, 8.65 8.01 book on the table
on, at, ? sobre, a, in, a`, pre`s de, acima de ??n, la?nga ?livre sur la table? (Fr.)
dietro de, a` cote? de (book on the table)
accanto a,
sopra
13 PURPOSE of, ? de, por, de, da, contre, a, de de, 4.45 4.45 nail brush
for para, per, a, ? de, ? pentru ?spazzolino per le unghie? (It.)
contra contra pour (brush for the nails)
14 SOURCE of, from de de de de de 0.94 0.15 oil of cloves
?o?leo de cravinho? (Port.)
(oil of cloves)
15 TOPIC of, for, on, de, de, a, de de, de, 0.79 0.79 love story
about, ? sobre su sobre despre ?histoire d?amour? (Fr.)
(story of love)
16 MANNER 0 0
17 MEANS of, by por via a` por pe 0.15 0.15 travel by car
?ca?la?torie cu mas?ina? (Ro.)
(travel by car)
18 EXPERIENCER of, in, ? de de de de de, ? 0.64 0.64 the agony of the prisoners
?l?agonia dei prigionieri? (It.)
(the agony of.the prisoners)
19 MEASURE of por de a` de de, 3.81 2.72 a cup of sugar
pentru ?o ceas?ca? de zaha?r? (Ro.)
(a cup of sugar)
20 TYPE 0 0
21 THEME for, ? de de, a de de de, a, ? 4.05 3.94 lack of intelligence
of, in ?manque d?intelligence? (Fr.)
(lack of intelligence)
22 BENEFICIARY 0 0
23 OTHER?SR of, by de de, a de de, a, de 12.95 8.81 cry of death
?cri de mort? (Fr.)
(cry of death)
Total no. of examples 2,023 1,734
in general. Because the two corpora used in this paper contain both root and synthetic
instances, we employed two semantic resources for this task: WordNet noun semantic
classes and a collection of verb classes in English that correspond to special types of
nominalizations. These resources are defined in Section 4.2. Moreover, in Section 6
we present a detailed linguistic analysis of the prepositions of in English and de in
Romance languages, and show how their selection correlates with the meaning of the
construction.
203
Computational Linguistics Volume 35, Number 2
4. Model
4.1 Mathematical Formulation
Given the syntactic constructions considered, the goal is to develop a procedure for
the automatic annotation of the semantic relations they encode. The semantic relations
derive from various lexical and semantic features of each instance.
The semantic classification of instances of nominal phrases and compounds can be
formulated as a learning problem, and thus benefits from the theoretical foundation
and experience gained with various learning paradigms. The task is a multi-class clas-
sification problem since the output can be one of the semantic relations in the set. We
cast this as a supervised learning problem where input/output pairs are available as
training data.
An important first step is to map the characteristics of each instance (i.e., list of
properties that describe the instance, usually not numerical) into feature vectors. Let us
define xi as the feature vector of an instance i and let X be the space of all instances; that
is, xi ? X.
The multi-class classification is performed by a function that maps the feature space
X into a semantic space S, f :X ? S, where S is the set of semantic relations from Table 1,
namely, rj ? S, where rj is a semantic relation.
Let T be the training set of examples or instances T = (x1r1 .. xlrl) ? (X x S)l where
l is the number of examples x each accompanied by its semantic relation label r. The
problem is to decide which semantic relation to assign to a new, unseen example xl+1.
In order to classify a given set of examples (members of X), one needs some kind of
measure of the similarity (or the difference) between any two given members of X.
Thus, the system receives as input an English nominal phrase and compound
instances along with their translations in the Romance languages, plus a set of extra-
linguistic features. The output is a set of learning rules that classify the data based on
the set of 22 semantic target categories. The learning procedure is supervised and takes
into consideration the cross-linguistic lexico-syntactic information gathered for each
instance.
4.2 Feature Space
The set of features allows a supervised machine learning algorithm to induce a function
that can be applied to accurately classify unseen instances. Based on the study of the
instances and their semantic distribution presented in Section 3, we have identified and
experimented with the following features presented subsequently for each language in-
volved. Features F1?F5 have been employed by us in our previous research (Moldovan
et al 2004; Girju et al 2005; Girju, Badulescu, and Moldovan 2006). All the other features
are novel.
A. English features
F1 and F2. Semantic class of noun specifies the WordNet sense of the head noun (F1), and
the modifier noun (F2) and implicitly points to all its hypernyms. The semantics of the
instances of nominal phrases and compounds is heavily influenced by the meaning of
the noun constituents. One such example is family#2 car#1, which encodes a POSSESSION
relation. The hypernyms of the head noun car#1 are: {motor vehicle}, {self-propelled
204
Girju The Syntax and Semantics of Prepositions
vehicle} ... {entity} (cf. WordNet 2.1). These features will help generalize over the se-
mantic classes of the two nouns in the instance corpus.
F3 and F4. WordNet derivationally related form specifies if the head noun (F3), and the
modifier noun (F4) are related to a corresponding verb in WordNet. WordNet contains
information about nouns derived from verbs (e.g., statement derived from to state; cry
from to cry; death from to die).
F5. Prepositional cues link the two nouns in a nominal phrase. These can be either simple
or complex prepositions such as of or according to. In case of N N instances (e.g., member
state), this feature is ???.
F6 and F7. Type of nominalized noun indicates the specific class of nouns the head (F6) or
modifier (F7) belongs to depending on the verb from which it derives. First, we check if
the noun is a nominalization or not. For English we used the NomLex-Plus dictionary of
nominalizations (Meyers at al. 2004) to map nouns to corresponding verbs.12 One such
example is the destruction of the city, where destruction is a nominalization. F6 and F7 may
overlap with features F3 and F4 which are used in case the noun to be checked has no
entry in the NomLex-Plus dictionary.
These features are of particular importance because they impose some constraints
on the possible set of relations the instance can encode. They take the following values:
a) active form nouns, b) unaccusative nouns, c) unergative nouns, and d) inherently
passive nouns. We present them in more detail subsequently.
a. Active form nouns are derived through nominalization from psych verbs and rep-
resent states of emotion, such as love, fear, desire, and so forth. They have an intrinsic
active voice predicate?argument structure and, thus, resist passivisation. For example,
we can say the desire of Anna, but not the desire by Anna. This is also explained by
the fact that in English the AGENT or EXPERIENCER relations are mostly expressed
by the clitic genitive ?s (e.g., Anna?s desire) and less or never by N P N constructions.
Citing Anderson (1983), Giorgi and Longobardi (1991) mention that with such nouns
that resist passivisation, the preposition introducing the internal argument, even if
it is of, has always a semantic content, and is not a bare case-marker realizing the
genitive case. Moreover, they argue that the meaning of these nouns might pattern
differently in different languages. Consider for example the Italian sentences (4) and (5)
below and their English equivalents (see Giorgi and Longobardi 1991, pages 121?
122). In English the instance Anna?s desire identifies the subject of desire (and thus
encodes an EXPERIENCER relation), whereas in Italian it can identify either the subject
(EXPERIENCER) as in Example (4), or the object of desire (THEME) as in Example (5),
the disambiguation being done at the discourse level. In Example (6) the prenominal
construction il suo desiderio encodes only EXPERIENCER.
(4) Il desiderio di Anna fu esaudito. (EXPERIENCER)
(The desire of Anna was fulfilled.)
?Anna?s desire was fulfilled.?
(5) Il desiderio di Anna lo portera` alla rovina. (THEME)
(The desire of Anna him will ruin.)
?The desire for Anna will ruin him.?
12 NomLex-Plus is a hand-coded database of 5,000 verb nominalizations, de-adjectival, and de-adverbial
nouns including the corresponding subcategorization frames (verb-argument structure information).
205
Computational Linguistics Volume 35, Number 2
(6) Il suo desiderio fu esaudito. (EXPERIENCER)
(The her desire was fulfilled.)
?Her desire was fulfilled.?
However, our observations on the Romanian training instances in Europarl and CLUVI
(captured by features F12 and F13 below) indicate that the choice of syntactic construc-
tions can help in the disambiguation of instances that include such active nouns. Thus,
whereas genitive-marked N N compounds identify only the subject (thus encoding
EXPERIENCER), the N de/pentru N constructions identify only the object (thus encoding
THEME). Such examples are dorint?a Anei (?desire-the Anna-GEN? ? Anna?s desire) (EX-
PERIENCER) and dorint?a de/pentru Ana (?desire-the of/for Anna? ? the desire for Anna)
(THEME).
Another example is the love of children and not the love by the children, where children
are the recipients of love, not its experiencers. In Italian the instance translates as l?amore
per i bambini (?the love for the children?), whereas in Romanian it translates as dragostea
pentru copii (?love-the for children?). These nouns mark their internal argument through
of in English and most of the time require prepositions such as for in Romance languages
and vice versa.
b. Unaccusative nouns are derived from ergative verbs that take only internal ar-
guments (e.g., those that indicate an object and not a subject grammatical role). For
example, the transitive verb to disband allows the subject to be deleted as in the following
sentences:
(7) The lead singer disbanded the group in 1991.
(8) The group disbanded.
Thus, the corresponding unaccusative nominalization of to disband, the disbandment of
the group, encodes THEME and not AGENT.
c. Unergative nouns are derived from intransitive verbs. They can take only AGENT
semantic relations. One such case is exemplified in the instance l?arrivo della cavalleria in
Italian which translates in English as the arrival of the cavalry and in Romanian as sorirea
cavaleriei (?arrival-the cavalry-GEN?).
d. Inherently passive nouns. These nouns, like the verbs they are derived from, assume
an implicit AGENT relation and, being transitive, associate to their internal argument
the THEME relation. One such example is the capture of the soldier which translates in
Italian as la cattura del soldato (?the capture of the soldier?), la capture du soldat in French
(?the capture of soldier?), and la captura de soldado in Spanish and Portuguese (?the
capture of soldier?), where the nominalization capture (cattura, capture, captura in Italian,
French, and Spanish and Portuguese respectively) is derived from the verb to capture.
Here, whereas English and Italian, Spanish, Portuguese, and French use the N of/de N
construction (as shown in Examples (9) and (10) for English and Italian), Romanian
uses genitive-marked noun compounds. In Romanian, however, nominalizations are
formed through suffixation, where a suffix is added to the root of the verb it comes from.
Different suffixes attached to the same verb may lead, however, to more than one nom-
inalization, producing different meanings. The verb to capture (a captura in Romanian),
for example, can result through suffixation in two nominalizations: capturare (with the
206
Girju The Syntax and Semantics of Prepositions
infinitive suffix -are and encoding an implicit AGENT relation) and captura? (through
zero derivation and encoding an implicit THEME relation) (Cornilescu 2001). Thus, the
noun phrase capturarea soldatului (?capture-the soldier-GEN?) encodes a THEME relation,
while captura soldatului (?capture-the soldier-GEN?) encodes an AGENT relation. In all the
Romance languages with the exception of Romanian, this construction is ambiguous,
unless the AGENT is explicitly stated or inferred as shown in Example (9) for Italian.
The same ambiguity might occur sometimes in English, with the difference that besides
the of-genitive, English also uses the s-genitive: the soldier?s capture (AGENT is preferred
if the context doesn?t mention otherwise), the soldier?s capture by the enemy (THEME), the
capture of the soldier (THEME is preferred if the context doesn?t mention otherwise), the
capture of the soldier by the enemy (THEME).
(9) La cattura del soldato (da parte del nemigo) e` cominciata come un atto terroristico.
(THEME)
?The capture of the soldier (by the enemy) has started as a terrorist act.?
(10) La sua cattura e` cominciata come un atto terroristico. (THEME)
?His capture has started as a terrorist act.?
These nouns have a different behavior than that of active form nouns. As shown
previously, the object of inherently passive nouns can move to the subject position as
in the soldier?s capture by the enemy, whereas it cannot do so for active form nouns (e.g.,
*Anna?s desire by John). Similarly, in Italian, although active form nouns allow only the
subject reading in prenominal constructions (e.g., il suo desiderio ? ?her desire?), inher-
ently passive nouns allow only the object reading (e.g., la sua cattura ? ?his capture?).
For Romanian, the nominalization suffixes were identified based on the morpho-
logical patterns presented in Cornilescu (2001).
We assembled a list of about 3,000 nouns that belong to classes a?d using the infor-
mation on subcategorization frames and thematic roles of the verbs in VerbNet (Kipper,
Dang, and Palmer 2000). VerbNet is a database which encodes rich lexical information
for a large number of English verbs in the form of subcategorization information,
selectional restrictions, thematic roles for each argument of the verb, and alternations
(the syntactic constructions in which the verb participates).
B. Romance features
F8, F9, F10, F11, and F12. Prepositional cues that link the two nouns are extracted from
each translation of the English instance: F8 (Sp.), F9 (Fr.), F10 (It.), F11 (Port.), and F12
(Ro.). These can be either simple or complex prepositions (e.g., de, in materia de [Sp.]) in
all five Romance languages, or the Romanian genitival article a/al/ale. For N N instances,
this feature is ???.
F13.Noun inflection is defined only for Romanian and shows if the modifier noun in N N
instances is not inflected or is inflected and modifies the head noun which is or is not
a nominalization. This feature is used to help differentiate between instances encoded
by genitive-marked N N constructions and noun?noun compounds, when the choice
of syntactic construction reflects different semantic content. Two such examples are the
noun?noun compound lege cadru (law framework) (TYPE) which translates as framework
207
Computational Linguistics Volume 35, Number 2
law and the genitive-marked N N instance frumuset?ea fetei (?beauty-the girl-GEN?) (PROP-
ERTY) meaning the beauty of the girl. It also covers examples such as capturarea soldatului
(?capture-the soldier-GEN?), where the modifier soldatului is inflected and the head noun
capturarea is a nominalization derived through infinitive suffixation.
In the following Example we present the feature vector for the instance the capture
of the soldiers.
(11) The instance the capture of the soldiers has the following Romance translations:
?capture#4/Arg2 of soldiers#1/Arg1; captura de soldados; capture du soldats; cattura dei soldati;
captura dos soldados; capturarea soldat?ilor; THEME?.
Its corresponding feature vector is:
?entity#1/Arg2; entity#1/Arg1; capture; ?; of; inherently passive noun; ?; de; de; de; de; ?;
mod-inflected-inf-nom; THEME?,
where mod-inflected-inf-nom indicates that the noun modifier soldat?ilor in the Ro-
manian translation capturarea soldat?ilor (?capture-the soldiers-GEN?) is inflected and that
the head noun capturarea is an infinitive nominalization.
4.3 Learning Models
Several learning models can be used to provide the discriminating function f. We
have experimented with the support vector machines model and compared the results
against two state-of-the-art models: semantic scattering, a supervised model described
in Moldovan et al (2004), Girju et al (2005), and Moldovan and Badulescu (2005), and
Lapata and Keller?s Web-based unsupervised model (Lapata and Keller 2005).
Each model was trained and tested on the Europarl and CLUVI corpora using a
7:3 training?testing ratio. All the test nouns were tagged with the corresponding sense
in context using a state-of-the-art WSD tool (Mihalcea and Faruque 2004). The default
semantic argument frame for each relation was used in the automatic identification of
the argument positions.
A. Support vector machines
Support vector machines (SVMs) are a set of related supervised learning methods used
for creating a learning function from a set of labeled training instances. The function
can be either a classification function, where the output is binary (is the instance of
category X?), or it can be a general regression function. For classification, SVMs operate
by finding a hypersurface in the space of possible inputs. This hypersurface will attempt
to split the positive examples from the negative examples. The split will be chosen
to have the largest distance from the hypersurface to the nearest of the positive and
negative examples. Intuitively, this makes the classification correct for testing data that
is similar but not identical to the training data.
In order to achieve classification in n semantic classes, n > 2, we built a binary
classifier for each pair of classes (a total of C2n classifiers), and then we used a voting
procedure to establish the class of a new example. For the experiments with semantic
relations, the simplest voting scheme has been chosen; each binary classifier has one
vote, which is assigned to the class it chooses when it is run. Then the class with the
largest number of votes is considered to be the answer. The software used in these
experiments is the package LIBSVM (http://www.csie.ntu.edu.tw/?cjlin/libsvm/)
which implements an SVM model. We tested with the radial-based kernel.
208
Girju The Syntax and Semantics of Prepositions
After the initial instances in the training and testing corpora were expanded with
the corresponding features, we had to prepare them for the SVM model. The set-up
procedure is now described.
Corpus set-up for the SVM model:
The processing method consists of a set of iterative procedures of specialization of
the examples on the WordNet IS-A hierarchy. Thus, after a set of necessary specializa-
tion iterations, the method produces specialized examples which through supervised
machine learning are transformed into sets of semantic rules for the semantic interpre-
tation of nominal phrases and compounds. The specialization procedure is described
subsequently.
Initially, the training corpus consists of examples that follow the format exemplified
at the end of Section 4.2 (Example [11]). Note that for the English instances, each noun
constituent was expanded with the corresponding WordNet top semantic class. At this
point, the generalized training corpus contains two types of examples: unambiguous
and ambiguous. The second situation occurs when the training corpus classifies the
same noun?noun pair into more than one semantic category. For example, both rela-
tionships chocolate#2 cake#3 (PART?WHOLE) and chocolate#2 article#1 (TOPIC) are mapped
into the more general type ?entity#1, entity#1, PART?WHOLE/TOPIC?.13 We recursively
specialize these examples to eliminate the ambiguity. By specialization, the semantic
class is replaced with the corresponding hyponym for that particular sense, that is,
the concept immediately below in the hierarchy. These steps are repeated until there
are no more ambiguous examples. For this example, the specialization stops at the
first hyponym of entity: physical entity (for cake) and abstract entity (for article). For the
unambiguous examples in the generalized training corpus (those that are classified
with a single semantic relation), constraints are determined using cross-validation on
the SVM model.
B. Semantic scattering
The semantic scattering (SS) model was initially tested on the classification of genitive
constructions, but it is also applicable to nominal phrases and compounds (Moldovan
et al 2004). SS is a supervised model which, like the SVM model described previously,
relies on WordNet?s IS-A semantic hierarchy to learn a function which separates positive
and negative examples. Essentially, it consists of using a training data set to establish a
boundary G? on WordNet noun hierarchies such that each feature pair of noun?noun
senses fij on this boundary maps uniquely into one of a predefined list of semantic
relations. The algorithm starts with the most general boundary corresponding to the
entity WordNet noun hierarchy and then specializes it based on the training data until
a good approximation is reached.14 Any feature pair above the boundary maps into
more than one semantic relation. Due to the specialization property on noun hierarchies,
feature pairs below the boundary also map into only one semantic relation. For any new
pair of noun?noun senses, the model finds the closest boundary pair which maps to one
semantic relation.
The authors define with SCm = { f mi } and SC
h = { f hj } the sets of semantic class features
for modifier noun and, respectively, head noun. A pair of<modifier, head> nouns maps
13 The specialization procedure applies only to features 1 and 2.
14 Moldovan et al (2004) used a list of 35 semantic relations ? actually only 22 of them proved to be encoded
by nominal phrases and compounds.
209
Computational Linguistics Volume 35, Number 2
uniquely into a semantic class feature pair ? f mi , f
h
j ? (henceforth fij). The probability of a
semantic relation r given the feature pair fij, P(r| fij ) =
n(r,fij )
n( fij )
is defined as the ratio between
the number of occurrences of a relation r in the presence of the feature pair fij over the
number of occurrences of the feature pair fij in the corpus. The most probable semantic
relation r? is
r? = arg max
r?R
P(r| fij) = arg max
r?R
P( fij|r)P(r) (1)
From the training corpus, one can measure the quantities n(r, fij) and n( fij). Depend-
ing on the level of abstraction of fij two cases are possible:
Case 1. The feature pair fij is specific enough such that there is only one semantic
relation r for which P(r| fij) = 1 and 0 for all the other semantic relations.
Case 2. The feature pair fij is general enough such that there are at least two semantic
relations for which P(r| fij) = 0. In this case Equation (1) is used to find the most appro-
priate r?.
Definition
A boundary G? in the WordNet noun hierarchies is a set of synset pairs such that:
a) for any feature pair on the boundary, denoted f G
?
ij ? G
?, f G
?
ij maps uniquely into
only one relation r, and
b) for any f uij  f
G?
ij , f
u
ij maps into more than one relation r, and
c) for any f lij ? f
G?
ij , f
l
ij maps uniquely into a semantic relation r.
Here relations  and ? mean ?semantically more general? and ?semantically more
specific?, respectively.
As proven by observation, there are more concept pairs under the boundary G?
than above it, that is, | {f lij} |  | {f
u
ij } |.
Boundary Detection Algorithm
Step 1. Create an initial boundary.
The initial boundary denoted G1 is formed from combinations of the entity#1 ?
entity#1 noun class pairs. For each training example a corresponding feature fij is
first determined, after which it is replaced with the most general corresponding
feature consisting of top WordNet hierarchy concepts. For example, both instances
family#2 estate#2 (POSSESSION) and the sister#1 of the boy#1 (KINSHIP) are mapped into
entity#1 ? entity#1. At this level, the noun?noun feature encodes a number of semantic
relations. For each feature, one can determine the most probable relation using Equa-
tion (1). For instance, the feature entity#1 ? entity#1 can be encoded by any of the 23
relations.
The next step is to construct a lower boundary by specializing the semantic classes
of the ambiguous features. A feature fij is ambiguous if it corresponds to more than
one relation and its most relevant relation has a conditional probability less than 0.9.
210
Girju The Syntax and Semantics of Prepositions
To eliminate irrelevant specializations, the algorithm specializes only the ambiguous
classes that occur in more than 1% of the training examples.
The specialization procedure consists of first identifying the features fij to which
correspond more than one semantic relation, then replacing these features with their
hyponym synsets. Thus one feature breaks into several new specialized features.
For example, the feature entity#1 ? entity#1 generated through generalization for
the examples family#2 estate#2 and the sister#1 of the boy#1 is specialized now as
kin group#1 ? real property#1 and female sibling#1 ? male person#1 corresponding to the
direct hyponyms of the nouns in these instances. The net effect is that the semantic
relations that were attached to fij will be ?scattered? across the new specialized features
which form the second boundary. The probability of the semantic relations that are
encoded by these specialized features is recalculated again using Equation (1). The
number of relations encoded by each of this boundary?s features is less than the one for
the features defining the previous boundary. This process continues until each feature
has only one semantic relation attached. Each iteration creates a new boundary.
Step 2. Test the new boundary.
The new boundary is more specific than the previous boundary and it is closer to the
ideal boundary. One does not know how well it behaves on unseen examples, but the
goal is to find a boundary that classifies these instances with high accuracy. Thus,
the boundary is first tested on only 10% of the annotated examples (different from
the 10% of the examples used for testing). If the accuracy is larger than the previous
boundary?s accuracy, the algorithm is converging toward the best approximation of the
boundary and thus it repeats Step 2 for the new boundary. If the accuracy is lower than
the previous boundary?s accuracy, the new boundary is too specific and the previous
boundary is a better approximation of the ideal boundary.
C. Lapata and Keller?s Web-based unsupervised model
Lauer (1995) was the first to devise and test an unsupervised probabilistic model for
noun?noun compound interpretation on Grolier?s Encyclopedia, an eight million word
corpus, based on a set of eight preposition paraphrases. His probabilistic model com-
putes the probability of a preposition p given a noun?noun pair n1 ? n2 and finds
the most likely preposition paraphrase p? = argmaxpP(p|n1,n2). However, as Lauer
noticed, this model requires a very large training corpus to estimate these proba-
bilities. More recently, Lapata and Keller (2005) replicated the model using the Web
as training corpus and showed that the best performance was obtained with the trigram
model f (n1, p,n2). In their approach, they used as the count for a given trigram the num-
ber of pages returned by using the trigram as a query. These co-occurrence frequencies
were estimated using inflected queries which are obtained by expanding a noun?noun
compound into all its morphological forms; then searching for N P N instances, for each
of the eight prepositions P in Lauer?s list. All queries are performed as exact matches
using quotation marks. For example, for the test noun?noun compound instancewar sto-
ries, all possible combinations of definite/indefinite articles and singular/plural noun
forms are tried resulting in the queries story about war, a/the story about war, story about
a/the war, stories about war, stories about the wars, story about wars, story about the wars,
and so on. These forms are then submitted as literal queries, and the resulting hits are
summed up. The query, and thus the preposition, with the largest number of hits is
selected as the correct semantic interpretation category.
211
Computational Linguistics Volume 35, Number 2
For the Europarl and CLUVI test sets, we replicated Lapata and Keller?s (2005) ex-
periments using Google.15 We formed inflected queries with the patterns they proposed
and searched the Web.
5. Experimental Results
We performed various experiments on both the Europarl and CLUVI testing corpora
using seven sets of supervised models. Table 8 shows the results obtained against SS
and Lapata and Keller?s model on both corpora and the contribution of the features
exemplified in seven versions of the SVM model. Supervised models 1 and 2 are defined
only for the English features. Here, features F1 and F2 measure the contribution of the
WordNet IS-A lexical hierarchy specialization. However, supervised model 1, which is
also the baseline, does not differentiate between unambiguous and ambiguous training
examples and thus does not specialize those that are ambiguous. These models show
the difference between SS and SVM and the contribution of the other English features,
such as preposition and nominalization (F1?F7).
The table shows that overall the performance is better for the Europarl corpus
than for CLUVI. For the supervised models 1 and 2, SS [F1 + F2] gives better re-
sults than SVM [F1 + F2]. The inclusion of the other English features (SVM [F1?F7])
adds more than 10% accuracy (with a higher increase in Europarl) for the supervised
model 1.
The results obtained are presented using the standard measure of accuracy (the
number of correctly labeled instances over the number of instances in the test set).
5.1 The Contribution of Romance Linguistic Features
Our intuition is that the more information we use from other languages for the interpre-
tation of an English instance, the better the results. Thus, we wanted to see the impact
of each Romance language on the overall performance. Supervised model 3 shows the
results obtained for English and the Romance language that contributed the least to the
performance (English and Spanish for the entire English feature subset F1?F8). Here we
computed the performance on all five English?Romance language combinations and
chose the Romance language that provided the best result. Thus, supervised models 3
through 7 add Spanish, French, Portuguese, Italian, and Romanian in this order and
show the contribution of each Romance preposition and all English features.
The language ranking in Table 8 shows that Romance languages considered here
have a different contribution to the overall performance. Whereas the addition of
Portuguese in CLUVI decreases the performance, in Europarl it increases it, if only
by a few points. However, a closer analysis of the data shows that this is mostly due
to the distribution of the corpus instances. For example, French, Italian, Spanish, and
Portuguese are consistent in the choice of preposition (e.g., if the preposition de [of ] is
used in French, then the corresponding preposition is used in the other four language
translations). A notable exception here is Romanian which provides two possible con-
structions with almost equal distribution: the N P N and the genitive-marked N N. The
table shows (in the increase in performance between supervised models 6 and 7) that
15 As Google limits the number of queries to 1,000 per day per computer, we repeated the experiment using
10 computers for a number of days. Although Keller and Lapata used AltaVista for the interpretation of
two noun?noun compounds, they showed that there is almost no difference between the correlations
achieved using Google and AltaVista counts.
212
Girju The Syntax and Semantics of Prepositions
Table 8
The performance obtained by five versions of the cross-linguistic SVM model compared against
the baseline, an English SVM model, and the SS model. The results obtained are presented
using the standard measure of accuracy (number of correctly labeled instances over the number
of instances in the test set).
Results [%]
CLUVI Europarl
Learning models 8-PP 22-SR 8-PP 22-SR
Supervised model 1: Baseline SS (F1+F2) 42.01 46.03 35.8 36.2
(English nominal features only) SVM (F1+F2) 34.17 38.11 30.02 33.01
(no WordNet specialization) SVM (F1-F7) ? 50.1 ? 43.33
Supervised model 2 SS (F1+F2) 55.20 61.02 54.12 57.01
(English features) SVM (F1+F2) 41.8 46.18 41.03 41.3
SVM (F1-F7) ? 61.04 ? 67.63
Supervised model 3 SVM (F1-F7+F8) ? 63.11 ? 68.04
(English and Spanish features)
Supervised model 4 SVM ? 65.81 ? 69.58
(English, Spanish, and French (F1-F7+F8+F9)
features)
Supervised model 5 SVM ? 64.31 ? 69.92
(English, Spanish, French, (F1-F7+F8+F9)
and Portuguese features) (+F11)
Supervised model 6 SVM ? 66.05 ? 71.25
(English, Spanish, French, (F1-F7+F8+F9+
Portuguese, and Italian features) F10+F11)
Supervised model 7 (SVM) ? 72.82 ? 76.34
(English and all Romance features: F1?F13)
Lapata and Keller?s Web-based 41.10 ? 42.12 ?
unsupervised model (English)
this choice is not random, but influenced by the meaning of the instances (features F12,
F13). This observation is also supported by the contribution of each feature to the overall
performance. For example, in Europarl, the WordNet verb and nominalization features
of the head noun (F3, F6) have a contribution of 5.12%, whereas for the modifier nouns
they decrease by about 2.7%. The English preposition (F5) contributes 6.11% (Europarl)
and 4.82% (CLUVI) to the overall performance.
The most frequently occurring preposition in both corpora is the underspecified
preposition de (of ), encoding almost all of the 22 semantic relations. The many-to-
many mappings of the preposition to the semantic classes adds to the complexity
of the interpretation task. A closer look at the Europarl and CLUVI data shows that
Lauer?s set of eight prepositions represents 88.2% (Europarl) and 91.8% (CLUVI) of the
N P N instances. From these, the most frequent preposition is of with a coverage of 79%
(Europarl) and 88% (CLUVI). Because the polysemy of this preposition is very high, we
wanted to analyze its behavior on the set of most representative semantic relations in
both corpora. Moreover, we wanted to see what prepositions were used to translate the
English nominal phrase and compound instances in the target Romance languages, and
thus to capture the semantic (ir)regularities among these languages in the two corpora
and their contribution to the semantic interpretation task.
For most of the N P N instances, we noticed consistent behavior of the target
Romance languages in terms of the prepositions used. This behavior can be classified
213
Computational Linguistics Volume 35, Number 2
roughly in four categories exemplified subsequently: Example (12) shows a combination
of the preposition of/de and more specific prepositions; Example (13) shows different
prepositions than the one corresponding to the English equivalent in the instance; and
Examples (14) and (15) show corresponding translations of the equivalent preposi-
tion in English in all Romance languages with variations in Romanian (e.g., de for of,
para/pour/par/pentru for for).
(12) Committee on Culture (En.) ? Comisio?n de la Cultura (Sp.) ? commission de la
culture (Fr.) ? commissione per la cultura (It.) ? Comissa?o para Cultura (Port.) ?
comitet pentru cultura? (Ro.) (PURPOSE)
(13) the supervision of the administration (En.) ? control sobre la administracio?n
(Sp.) ? contro?le sur l?administration (Fr.) ? controllo sull?amministrazione (It.) ?
controlo sobre administrac?a?o (Port.) ? controlul asupra administrat?iei (Ro.)
(THEME)
(14) lack of protection (En.) ? falta de proteccio?n (Sp.) ? manque de protection (Fr.) ?
mancanza di tutela (It.) ? falta de protecc?a?o (Port.) ? lipsa? de protect?ie (Ro.)
(THEME)
(15) the cry of a man (En.) ? el llanto de un hombre (Sp.) ? un cri d?homme (Fr.) ?
l?urlo di un uomo (It.) ? o choro de um be?bado (Port.) ? striga?tul unui om (Ro.)
(AGENT)
Because the last three categories are the most frequent in both corpora, we analyzed
their instances. Most of the time Spanish, French, Italian, and Portuguese make use of
specific prepositions such as those in Examples (12) and (13) to encode some semantic
relations such as PURPOSE and LOCATION, but rely on N de N constructions for almost
all the other relations. English and Romanian, however, can choose between N N and
N P N constructions. In the next section we present in more detail an analysis of the
semantic correlations between English and Romanian nominal phrases and compounds
and their role in the semantic interpretation task.
6. Linguistic Observations
In this section we present some linguistic observations derived from the analysis of
the system?s performance on the CLUVI and Europarl corpora. More specifically, we
present different types of ambiguity that can occur in the interpretation of nominal
phrases and compounds when using more abstract interpretation categories such as
Lauer?s eight prepositions. We also show that the choice of syntactic constructions
in English and Romanian can help in the identification of the correct position of the
semantic arguments in test instances.
6.1 Observations on Lapata and Keller?s Unsupervised Model
In this section we show some of the limitations of the unsupervised probabilistic ap-
proaches that rely on more abstract interpretation categories, such as Lauer?s set of
eight prepositions. For this, we used Lapata and Keller?s approach, a state-of-the-art
knowledge-poor Web-based unsupervised probabilistic model which provided a per-
formance of 42.12% on Europarl and 41.10% on CLUVI. We manually checked the first
214
Girju The Syntax and Semantics of Prepositions
Table 9
Experimental results with Lapata and Keller?s Web-based unsupervised interpretation model on
different types of test sets from the Europarl corpus.
Noun?noun compound Accuracy
test set Ambiguity of noun constituents [%]
Set#1 one part of speech, one WordNet sense 35.28%
Set#2 multiple parts of speech, one WordNet sense 31.22%
Set#3 one part of speech, multiple WordNet senses 50.63%
Set#4 multiple parts of speech, multiple WordNet senses 43.25%
five entries of the pages returned by Google for each most frequent N P N paraphrase
for 100 CLUVI and Europarl instances and noticed that about 35% of them were wrong
due to syntactic (e.g., part of speech) and/or semantic ambiguities. For example, baby cry
generated instances such as ?it will make moms cry with the baby,? where cry is a verb,
not a noun. This shows that many of the NP instances selected by Google as matching
the N P N query are incorrect, and thus the number of hits returned for the query is over-
estimated. Thus, because we wanted to measure the impact of various types of noun?
noun compound ambiguities on the interpretation performance, we further tested the
probabilistic Web-based model on four distinct test sets selected from Europarl, each
containing 30 noun?noun compounds encoding different types of ambiguity: In Set#1
the noun constituents had only one part of speech and one WordNet sense; in Set#2 the
nouns had at least two possible parts of speech and were semantically unambiguous; in
Set#3 the nouns were ambiguous only semantically; and in Set#4 they were ambiguous
both syntactically and semantically. Table 9 shows that for Set#1, the model obtained an
accuracy of 35.28%, while for more semantically ambiguous compounds it obtained an
average accuracy of about 48% (50.63% [Set#3] and 43.25% [Set#4]. This shows that for
more syntactically ambiguous instances, the Web-based probabilistic model introduces
a significant number of false positives, thus decreasing the accuracy (cf. sets #1 vs. #2
and #3 vs. #4).
Moreover, further analyses of the results obtained with Lapata and Keller?s model
showed that about 30% of the noun?noun compounds in sets #3 and #4 were ambiguous
with at least two possible readings. For example, paper bag can be interpreted out-
of-context both as bag of paper (bag made of paper?STUFF?OBJECT, a subtype of
PART?WHOLE) and as bag for papers (bag used for storing papers?PURPOSE). Simi-
larly, gingerbread bowl can be correctly paraphrased both as bowl of/with gingerbread
(CONTENT?CONTAINER) and as bowl of gingerbread (bowl made of gingerbread?STUFF?
OBJECT). The following two examples show the two readings of the noun?noun com-
pound gingerbread bowl as found on Google:
(16) Stir a bowl of gingerbread,
Smooth and spicy and brown,
Roll it with a rolling pin,
Up and up and down,
...16
16 An excerpt from the ?Gingerbread Man? song.
215
Computational Linguistics Volume 35, Number 2
(17) The gingerbread will take the shape of the glass bowl. Let it cool for a few
minutes and then carefully loosen the foil and remove the gingerbread from
the glass. And voila`: your bowl of gingerbread.
These ambiguities partially explain why the accuracy values obtained for sets #3
and #4 are higher then the ones obtained for the other two sets. The semantic ambiguity
also explains why the accuracy obtained for set #2 is higher than that for set #4. For these
sets of examples the syntactic ambiguity affected the accuracy much less than the se-
mantic ambiguity (that is, more N P N combinations were possible due to various noun
senses). This shows one more time that a large number of noun?noun compounds are
covered by more abstract categories, such as prepositions. Moreover, these categories
also allow for a large variation as to which category a compound should be assigned.
6.2 Observations on the Symmetry of Semantic Relations: A Study on English
and Romanian
Nominal phrases and compounds in English, nominal phrases in the Romance lan-
guages considered here, and genitive-marked noun?noun compounds in Romanian
have an inherent directionality imposed by their fixed syntactic structure. For example,
in English noun?noun compounds the syntactic head always follows the syntactic
modifier, whereas in English and Romance nominal phrases the order is reversed. Two
such examples are tea/Modifier cup/Head and glass/Head of wine/Modifier.
The directionality of semantic relations (i.e., the order of the semantic arguments)
however, is not fixed and thus it is not always the same as the inherent direc-
tionality imposed by the syntactic structure. Two such examples are ham/Modifier/
Arg2 sandwich/Head/Arg1 and spoon/Modifier/Arg1 handle/Head/Arg2. Although
both instances encode a PART?WHOLE relation (Arg1 is the semantic argument iden-
tifying the whole and Arg2 is the semantic argument identifying the part), their se-
mantic arguments are not listed in the same order (Arg1 Arg2 for spoon handle and
Arg2 Arg1 for ham sandwich). For a better understanding of this phenomenon, we
performed a more thorough analysis of the training instances in both CLUVI and
Europarl. Because the choice of syntactic constructions in context is governed in part
by semantic factors, we focused on English and Romanian because they are the only
languages from the set considered here with two productive syntactic options: N N
and N P N (English) and genitive-marked N N and N P N (Romanian). Thus, we
grouped the English?Romanian parallel instances per each semantic relation and each
syntactic construction and checked if the relation was symmetric or not, according to
the following definition.
Definition
We say that a semantic relation is symmetric relative to a particular syntactic
construction if there is at least one relation instance whose arguments are in a different
order than the order indicated by the relation?s default argument frame for that
construction.
For example, PART?WHOLE is symmetric with regard to nominal phrases because
the semantic arguments of the instance the building/Arg1 with parapets/Arg2 are in a
different order than the one imposed by the relation?s default argument frame (Arg2
P Arg1) for nominal phrases (cf. Table 1).
216
Girju The Syntax and Semantics of Prepositions
Because the relation distribution is skewed in both corpora, we focused only on
those relations encoded by at least 50 instances in both Europarl and CLUVI. For
example, in English the POSSESSION relation is symmetric when encoded by N P N and
noun?noun compounds. For instance, we can say the girl with three dogs and the resources
of the Union, but also family estate and land proprietor. The findings are summarized and
presented in Table 10 along with examples. Some relations such as IS-A, PURPOSE, and
MEASURE cannot be encoded by genitive-marked noun?noun compounds in Romanian
(indicated by ??? in the table). A checkmark symbol indicates if the relation is symmetric
(??) or not (?x?) for a particular syntactic construction. It is interesting to note that not
all the relations are symmetric and this behavior varies from one syntactic construction
to another and from one language to another. Although some relations such as AGENT
and THEME are not symmetric, others such as TEMPORAL, PART?WHOLE, and LOCATION
are symmetric irrespective of the syntactic construction used.
Symmetric relations pose important challenges to the automatic interpretation of
nominal phrases and compounds because the system has to know which of the nouns
is the semantic modifier and which is the semantic head. In this research, the order
of the semantic arguments has been manually identified and marked in the training
corpora. However, this information is not provided for unseen test instances. So far,
in our experiments with the test data the system used the order indicated by the
default argument frames. Another solution is to build argument frames for clusters of
prepositions which impose a particular order of the arguments in N P N constructions.
For example, in the N2 P N1 phrases the books on the table (LOCATION) and relaxation
during the summer (TEMPORAL), the semantic content of the prepositions on and during
identifies the position of the physical and temporal location (e.g., that N1 is the time
or location). This approach works most of the time for relations such as LOCATION
and TEMPORAL because in both English and Romance languages they rely mostly on
prepositions indicating location and time and less on underspecified prepositions such
as of or de. However, a closer look at these relations shows that some of the noun?noun
pairs that encode them are not symmetric and this is true for both English and Romance.
For instance, cut on the chin and house in the city cannot be reversed as chin P cut or
city P house. One notable exception here is indicated by examples such as box of/with
matches ? matches in/inside the box and vessels of/with blood ? blood in vessels17 encoding
CONTENT?CONTAINER. Another special case is when P1 and P2 are location antonyms
(e.g., the book under the folder and the folder on the book). However, even here symmetry
is not always possible, being influenced by pragmatic factors (Herskovits 1987) (e.g.,
we can say the vase on the table, but not the table under the vase?this has to do with the
difference in size of the objects indicated by the head and modifier nouns. Thus, a larger
object cannot be said to be placed under a smaller one).
It is important to stress here the fact that our definition of symmetry of semantic
relations does not focus in particular on the symmetry of an instance noun?noun pair
that encodes the relation, although it doesn?t exclude such a case. We call this lexical
symmetry and define it here.
Definition
We say that a noun?noun pair (N1 ? N2) is symmetric relative to a particular syntactic
construction and the semantic relation it encodes in that construction if the order of the
nouns in the construction can be changed provided the semantic relation is preserved.
17 Here the noun vessels refers to a type of container.
217
Computational Linguistics Volume 35, Number 2
Table 10
A summary of the symmetry properties of a set of the 12 most frequent semantic relations in
CLUVI and Europarl. ??? means the semantic relation is not encoded by the syntactic
construction, ?? and ?x? symbols indicate whether the relation is or is not symmetric.
Symmetry
English Romanian
Semantic genitive-marked
No. relations N N N P N N N N P N Examples
1 POSSESSION    x En.: family#2/Arg1 estate#2/Arg2 vs.
land#1/Arg2 proprietor#1/Arg1
Ro.: terenul/Arg2 proprietarului/Arg1
(land-the owner-GEN)
(?the owner?s land?)
proprietarul/Arg1 magazinului/Arg2
(owner-the store-GEN)
(?the owner of the store?)
2 PROPERTY x  x  En.: calm#1/Arg2 of evening#1/Arg1 vs.
spots#4/Arg1 of color#1/Arg2
Ro.: pete/Arg1 de culoare/Arg2
(?spots of color?)
miros/Arg2 de camfor/Arg1
(?odour of camphor?)
3 AGENT x x x x En.: the investigation#2/Arg2 of the police#1/Arg1
Ro.: investigat?ia/Arg2 polit?iei/Arg1
(investigation-the police-GEN)
4 TEMPORAL     En.: news#3/Arg2 in the morning#1/Arg1 vs.
the evening#1/Arg1 of her arrival#2/Arg2
Ro.: placinte/Arg2 de dimineat?a?/Arg1
(cakes of morning)
(?morning cakes?) vs.
ani/Arg1 de subjugare/Arg2
(?years of subjugation?)
5 PART?WHOLE     En: faces#1/Arg2 of children#1/Arg1 vs.
the shell#5Arg2 of the egg#2/Arg1
Ro: fet?ele/Arg2 copiilor/Arg1
(faces-the children-GEN)
(?the faces of the children?) vs.
coaja?/Arg1 de ou/Arg2
(shell of egg)
(?egg shell?)
6 HYPERNYMY x x ? x En.: daisy#1/Arg1 flower#1/Arg2
(is-a) Ro.: meci/Arg2 de fotbal/Arg1
(match of football)
(?football match?)
7 LOCATION     En.: castle#2/Arg2 in the desert#1/Arg1 vs.
point#2/Arg1 of arrival#1/Arg2
Ro.: castel/Arg2 in des?ert/Arg1
(castle in desert)
(?castle in the desert?) vs.
punct/Arg1 de sosire/Arg2
(?point of arrival?)
8 PURPOSE x x ? x En.: war#1/Arg1 canoe#1/Arg2
Ro.: piroga?/Arg2 de ra?zboi/Arg1
(canoe of war)
9 TOPIC x x x x En.: war#1/Arg1 movie#1/Arg2
Ro.: film/Arg2 despre ra?zboi/Arg1
(?movie about war?)
10 MEASURE ? x ? x En.: inches#1/Arg2 of snow#2/Arg1
Ro.: inci/Arg2 de zapada?/Arg1 (inches of snow)
11 TYPE x  x x En.: framework#1/Arg1 law#2/Arg2
Ro.: lege/Arg2 cadru/Arg1 (law framework)
12 THEME x x x x En.: examination#1/Arg2 of machinery#1/Arg1
Ro.: verificarea/Arg2 mas?inii/Arg1
(examination-the machinery-GEN)
(?the examination of the machinery?)
218
Girju The Syntax and Semantics of Prepositions
For instance, the pair building?parapets in the nominal phrases the building/Arg1 with
parapets/Arg2 and the parapets/Arg2 of the building/Arg1 encodes a PART?WHOLE relation.
Here, both the noun?noun pair and the semantic relation are symmetric relative to
N P N. However, the situation is different for instances such as the book/Arg2 under
the folder/Arg1 and the folder/Arg2 on the book/Arg1, both encoding LOCATION. Here, the
book?folder pair is symmetric in N P N constructions (in the first instance the book is the
syntactic head and the folder is the modifier, whereas in the second instance the order
is reversed). However, the LOCATION relation they encode is not symmetric (in both
instances, the order of the semantic arguments matches the default argument frame for
LOCATION). It is interesting to notice here that these two location instances are actually
paraphrases of one another. This can be explained by the fact that both the book and the
folder can act as a location with respect to the other, and that the prepositions under and
on are location antonyms. In comparison, the building with parapets is not a paraphrase
of the parapets of the building. Here, the nouns building and parapets cannot act as a
whole/part with respect to each other (e.g., the only possible whole here is the noun
building, and the only possible part here is the noun parapets). This is because parts and
wholes have an inherent semantic directionality imposed by the inclusion operation on
the set of things representing parts and wholes, respectively.
In this research we consider the identification and extraction of semantic relations
in nominal phrases and compounds, but we do not focus in particular on the acquisition
of paraphrases in these constructions. Our goal is to build an accurate semantic parser
which will automatically annotate instances of nominal phrases and compounds with
semantic relations in context. This approach promises to be very useful in applications
that require semantic inference, such as textual entailment (Tatu and Moldovan 2005).
However, a thorough analysis of the semantics of nominal phrases and compounds
should focus on both semantic relations and paraphrases. We leave this topic for future
research.
Because we wanted to study in more detail the directionality of semantic relations,
we focused on PART?WHOLE. These relations, and most of the semantic relations con-
sidered here, are encoded mostly by N of/de N constructions, genitive-marked N N
(Romanian), and noun?noun compounds (English) and thus, the task of argument order
identification becomes more challenging. For the purpose of this research we decided
to take a closer look at the PART?WHOLE relation in both CLUVI and Europarl where
together it accounted for 920 token and 636 type instances. We show subsequently a
detailed analysis of the symmetry property on a classification of PART?WHOLE relations
starting with a set of five PART?WHOLE subtypes identified by Winston, Chaffin, and
Hermann (1987):18 (1) Component?Integral object, (2) Member?Collection, (3) Portion?Mass,
(4) Stuff?Object, and (5) Place?Area.
(1) Component?Integral object
This is a relation between components and the objects to which they belong. Integral
objects have a structure with their components being separable and having a functional
relation with their wholes. This type of PART?WHOLE relation can be encoded by N of N
and less often by N N constructions. Moreover, here the existential interpretation is
preferred over the generic one. Such examples are the leg of the table and the table
leg which translate in Romanian as piciorul mesei (?leg-the table-GEN?). In Romanian a
18 Winston, Chaffin, and Hermann (1987) identified six subtypes of PART?WHOLE relations, one of which,
(Feature?Activity), is not presented here because it is not frequently encoded by N N and N P N
constructions.
219
Computational Linguistics Volume 35, Number 2
generic interpretation is also possible, but with change of construction and most of the
time of semantic relation (e.g., picior de masa? ? ?leg of table? encoding PURPOSE19).
This relation subtype is symmetric in English for both N N and N P N con-
structions. In Romanian, however, it is symmetric only when encoded by N P N.
Moreover, it is interesting to note that Modifier/Arg1 Head/Arg2 noun?noun com-
pound instances translate as genitive noun?noun compounds in Romanian, whereas
Modifier/Arg2 Head/Arg1 instances translate as N P N, with P different from of. For
example, chair/Arg1 arm/Arg2 and ham/Arg2 sandwich/Arg1 translate in Romanian as
Head/Arg2 Modifier/Arg1 ? brat?ul scaunului (?arm-the chair-GEN?) and Head/Arg1 P
Modifier/Arg2 ? sandwich cu s?unca? (?sandwich with ham?).
For N P N instances in Romanian and English both Arg1 P1 Arg2 and Arg2 P2 Arg1
argument orderings are possible, but with a different choice of preposition (with P1
different from of/de). For example, one can say the parapets/Arg2 of the building/Arg1,
but also the building/Arg1 with parapets/Arg2. A closer look at such instances shows that
symmetry is possible when the modifier (in this case the part) is not a mandatory part
of the whole, but an optional part with special features (e.g., color, shape). For example,
the car with the door is less preferred than the car with the red door which differentiates the
car from other types of cars.
(2) Stuff?Object
This category encodes the relations between an object and the material of which it
is partly or entirely made. The parts are not similar to the wholes which they compose,
cannot be separated from the whole, and have no functional role. The relation can
be encoded by both N of N and N N English and Romanian patterns and the choice
between existential and generic interpretations correlates with the relation symmetry.
For N N constructions this relation subtype is not symmetric, while for N P N it
is symmetric only in English. Such examples are brush/Arg2 hut/Arg1 in English, and
metalul/Arg2 scaunului/Arg1 (?metal-the seat-GEN? ? the metal of the seat) and scaun de metal
(?chair of metal? ? metal chair) in Romanian.
N P N instances can only be encoded by of in English or de/din (of/from) in Ro-
manian. If the position of the arguments is Arg1 of Arg2 and Arg2 is an indefinite noun
indicating the part then the instance interpretation is generic. For example, seat of metal
translates as scaun de/din metal (?chair of/from metal?) in Romanian. It is important to
note here the possible choice of the preposition from in Romanian, a preposition which
is rarely used in English for this type of relation.
When the position of the arguments changes (e.g., Arg2 of Arg1), the same preposi-
tion of is used and the semantic relation is still STUFF?OBJECT, but the instance is more
specific having an existential interpretation. For instance, the metal of the seat translates in
Romanian as metalul scaunului (?metal-the seat-GEN?) and not as metalul de scaun (?metal-
the of seat?).
(3) Portion?Mass
According to Selkirk (1982a), Ionin, Matushansky, and Ruys (2006), and our own
observations on the CLUVI and Europarl data, this type of PART?WHOLE relation can
be further classified into mass, measure, and fraction partitives. Here the parts are
separable and similar to each other and to the whole they are part of. An example of a
mass partitive is half/Arg2 of the cake/Arg1 which translates in Romanian as juma?tate/Arg2
19 This reading is possible if the leg is separated from the table.
220
Girju The Syntax and Semantics of Prepositions
de/din prajitura?/Arg1 (?half of/from cake?). Note that here the noun cake is indefinite in
Romanian, and thus the instance is generic. An existential interpretation is possible
when the noun is modified by a possessive (e.g., half of your cake).
Measure partitives are also called vague PART?WHOLE relations (Selkirk 1982b)
because they can express both PART?WHOLE and MEASURE depending on the context.
They are encoded by N1 of N2 constructions, where N2 is indefinite, and can indi-
cate both existential and generic interpretations. Two such examples are bottles/Arg1 of
wine/Arg2 and cup/Arg1 of sugar/Arg2. In Romanian, the preposition used is either de
(of ), or cu (with). For example, sticle/Arg1 de/cu vin/Arg2 (?bottles of/with wine?) and
ceas?ca?/Arg1 de/cu zaha?r/Arg2 (?cup of/with sugar?).
Fraction partitives indicate fractions of wholes, such as three quarters/Arg2 of a
pie/Arg1 (trei pa?trimi/Arg2 de pla?cinta?/Arg1 [Romanian]?[?three quarters of pie?]) and one
third/Arg2 of the nation/Arg1 (o treime/Arg2 din populat?ia/Arg1 [Romanian]?[?one third from
population-the?] and not o treime de populat?ia ? [?a third of population-the?]). Here again,
we notice the choice of the Romanian preposition din and not de when the second noun
is definite. The preposition from indicates the idea of separation of the part from the
whole, an idea which characterizes PART?WHOLE relations.
Portion?Mass relations cannot be encoded by N N structures in either English or
Romanian and they are not symmetric in N P N constructions.
(4) Member?Collection
This subtype represents membership in a collection. Members are parts, but may
not play any functional role with respect to their whole. That is, compared with
Component?Integral instances such as the knob of the door, where the knob is a round
handle one turns in order to open a door, in an example like bunch of cats, the cats don?t
play any functional role to the whole bunch.
This subtype can be further classified into a basic subtype (e.g., the member of the
team), count partitives (e.g., two of these people), fraction count partitives (e.g., two
out of three workers), and vague measure partitives (e.g., a number/lot/bunch of cats).
Although the basic Member?Collection partitives are symmetric for N N (Romanian
only) and N P N (English and Romanian), the other subtypes can be encoded only by
N P N constructions and are not symmetric in English or in Romanian. For example,
the children/Arg2 of the group/Arg1 and children/Arg2 group/Arg1 translate as copiii/Arg2
din grup/Arg1 (?children-the from group?) and as grup/Arg1 de copii/Arg2 (?group of
children?).
The second and the third subtypes translate in Romanian as doi/Arg2 din aces?ti
oameni/Arg1 (?two from these people?) and doi/Arg2 din trei lucra?tori/Arg1 (?two from three
workers?), by always using the preposition din ( from) instead of de (of ). On the other
hand, vague measure partitives translate as un numa?r/Arg1 de pisici/Arg2 (?a number of
cats?) and not as un numa?r din pisici (?a number from cats?). Although all these subtypes
need to have a plural modifier noun and are not symmetric, count partitives always
have an existential interpretation, whereas fraction count and vague measure partitives
have a generic meaning.
(5) Location?Area
This subtype captures the relation between areas and special places and locations
within them. The parts are similar to their wholes, but they are not separable from
them. Thus, this relation overlaps with the LOCATION relation. One such example is the
surface/Arg2 of the water/Arg1. Both nouns can be either definite or indefinite and the rela-
tion is not symmetric when the part is a relational noun (e.g., surface, end). In Romanian,
221
Computational Linguistics Volume 35, Number 2
both N de N and genitive-marked N N constructions are possible: suprafat?a/Arg2
apei/Arg1 (?surface-the water-GEN?) and suprafat?a?/Arg2 de apa?/Arg1 (?surface of water?).
The relation is symmetric only for N P N in both English and Romanian.
Table 11 summarizes the symmetry properties of all five PART?WHOLE subtypes
accompanied by examples.
Thus, features such as the semantic classes of the two nouns (F1, F2), and the syntac-
tic constructions in English and Romanian?more specifically, the preposition features
for English (F5) and Romanian (F12) and the inflection feature for Romanian (F13)?
can be used to train a classifier for the identification of the argument order in nominal
phrases and compounds encoding different subtypes of PART?WHOLE relations. For
example, the argument order for Portion?Mass instances can be easily identified if it is
determined that they are encoded byN2 of/de N1 in English and Romanian and the head
noun N2 is identified as a fraction in the WordNet IS-A hierarchy, thus representing Arg2
(the part). It is interesting to note here that all the other Member?Collection subtypes with
the exception of the basic one are also encoded only by N of/de N, but here the order
is reversed in both English and Romanian (N1 of/de N2), where the head noun N1, if
identified as a collection concept in WordNet, represents the whole concept (Arg1).
This approach can also be applied to other symmetric relations by classifying them
into more specific subtypes for argument order identification. Thus, local classifiers
can be trained for each subtype on features such as those mentioned herein and tested
on unseen instances. However, for training this procedure requires a sufficiently large
number of examples for each subtype of the semantic relation considered.
Table 11
A summary of the symmetry properties of the five subtypes of PART?WHOLE semantic relation in
CLUVI and Europarl. ??? means the semantic relation is not encoded by the syntactic
construction, ?? and ?x? symbols indicate whether the relation is or is not symmetric.
Symmetry
English Romanian
Semantic genitive-marked
No. relations N N N P N N N N P N Examples
1 Component ?   x  En.: chair#1/Arg1 arm#5/Arg2 vs.
Integral obj. Arg2 Arg1 ham#1/Arg2 sandwich#1/Arg1
Ro: ?brat?ul/Arg2 scaunului/Arg1?
(arm-the chair-GEN) vs.
?sandwhich/Arg1 cu s?unca?/Arg2?
(sandwich with ham)
2 Stuff ? x  x x En.: dress#1/Arg1 of silk#1/Arg2 vs.
Object Arg2 Arg1 Arg2 Arg1 Arg1 de Arg2 the silk#1/Arg2 of the dress#1/Arg1
Ro.: ?rochie/Arg1 de ma?tase/Arg2?
(dress of silk) vs.
?ma?tasea/Arg2 rochiei/Arg1?
(silk-the dress-GEN)
3 Portion ? ? x ? x En.: half#1/Arg2 of the cake#3/Arg1 vs.
Mass Arg2 of Arg1 Arg2 de Arg1 Ro: ?juma?tate/Arg2 de/din prajitura?/Arg1?
(half of/from cake)
4 Member ? ? x ? x En.: a bunch#1/Arg1 of cats#1/Arg2
Collection Arg1 of Arg2 Arg1 de Arg2 Ro.: ?o gra?mada?/Arg1 de pisici/Arg2?
(count, (a bunch of cats)
fraction count,
and vague measure
partitives)
Member ? x    En.: president#4/Arg2 of the committee#1/Arg1 vs.
Collection Arg1 Arg2 committee#1/Arg1 of idiots#1/Arg2
(basic Ro.: copiii/Arg2 din grup/Arg1
partitive) (children-the from group)
(?the children from the group?)
grup/Arg1 de copii/Arg2
(?group of children?)
5 Location ? x  x  En.: the swamps#1/Arg2 of the land#7/Arg1 vs.
Area Arg1 Arg2 Arg2 Arg1 the land#7/Arg1 with swamps#1/Arg2
Ro.: oaza? ??n des?ert
(oasis in desert) vs.
des?ert cu oaza? ??n (desert with oasis)
222
Girju The Syntax and Semantics of Prepositions
This analysis shows that the choice of lexico-syntactic structures in both English and
Romanian correlates with the meaning of the instances encoded by such structures. In
the next section we present a list of errors and situations that, currently, our system fails
to recognize, and suggest possible improvements.
7. Error Analysis and Suggested Improvements
A major part of the difficulty of interpreting nominal phrases and compounds stems
from multiple sources of ambiguity. These factors range from syntactic analysis, to
semantic, pragmatic, and contextual information and translation issues. In this section
we show various sources of error we found in our experiments and present some
possible improvements.
A. Error analysis
Two basic factors are wrong part-of-speech and word sense disambiguation tags. Thus,
if the syntactic tagger and WSD system fail to annotate the nouns with the correct senses,
the system can generate wrong semantic classes which will lead to wrong conclusions.
Moreover, there were also instances for which the nouns or the corresponding senses of
these nouns were not found in WordNet. There were 42.21% WSD and 6.7% POS tagging
errors in Europarl and 54.8% and 7.32% in CLUVI. Additionally, 6.9% (Europarl) and
4.6% (CLUVI) instances had missing senses.
There are also cases when local contextual information such as word sense disam-
biguation is not enough for relation detection and when access to a larger discourse
context is needed. Various researchers (Spa?rck Jones 1983; Lascarides and Copestake
1998; Lapata 2002) have shown that the interpretation of noun?noun compounds, for
example, may be influenced by discourse and pragmatic knowledge. This context may
be identified at the level of local nominal phrases and compounds or sentences or at the
document and even collection level. For example, a noun?noun compound modified
by a relative clause might be disambiguated in the context of another argument of the
same verb in the clause, which can limit the number of possible semantic relations. For
instance, the interpretation of the instance museum book in the subject position in the
following examples is influenced by another argument of the verbs bought in Exam-
ple (18), and informed in Example (19):
(18) the [museum book]TOPIC John bought in the bookshop at the museum
(19) the [museum book]LOCATION that informed John about the ancient art
Prepositions such as spatial ones are also amenable to visual interpretations due
to their usage in various visual contexts. For example, the instance nails in the box (cf.
Herskovits 1987) indicates two possible arrangements of the nails: either held by the
box, or hammered into it. We cannot capture these subtleties with the current procedure
even if they are mentioned in the context of the sentence or discourse.
B. Suggested improvements
In this article we investigated the contribution of English and Romance prepositions to
the task of interpreting nominal phrases and compounds, both as features employed
in a learning model and as classification categories. An interesting extension of this
approach would be to look into more detail at the functional?semantic aspect of these
prepositions and to define various tests that would classify them as pure functional
components with no semantic content or semantic devices with their own meaning.
223
Computational Linguistics Volume 35, Number 2
Moreover, our experiments focused on the detection of semantic relations encoded
by N N and N P N patterns. A more general approach would extend the investigation
to adjective?noun constructions in English and Romance languages as well.
Another direction for future work is the study of the semantic (ir)regularities among
English and Romance nominal phrases and compounds in both directions. Such an
analysis might be also useful for machine translation, especially when translating into a
language with multiple choices of syntactic constructions. One such example is tarro
de cerveza (?glass of beer?) in Spanish which can be translated as either glass of beer
(MEASURE) or beer glass (PURPOSE) in English. The current machine translation language
models do not differentiate between such options, choosing the most frequent instance
in a large training corpus.
The drawback of the approach presented in this article, as for other very precise
learning methods, is the need for a large number of training examples. If a certain
class of negative or positive examples is not seen in the training data (and therefore
it is not captured by the classification rules), the system cannot classify its instances.
Thus, the larger and more diverse the training data, the better the classification rules.
Moreover, each cross-linguistic study requires translated data, which is not easy to
obtain in electronic form, especially for most of the world?s languages. However, more
and more parallel corpora in various languages are expected to be forthcoming.
8. Discussion and Conclusions
In this article we investigated the contribution of English and Romance prepositions to
the interpretation of N N and N P N instances and presented a supervised, knowledge-
intensive interpretation model.
Our approach to the interpretation of nominal phrases and compounds is novel
in several ways. We investigated the problem based on cross-linguistic evidence from
a set of six languages: English, Spanish, Italian, French, Portuguese, and Romanian.
Thus, we presented empirical observations on the distribution of nominal phrases and
compounds and the distribution of their meanings on two different corpora, based
on two state-of-the-art classification tag sets: Lauer?s set of eight prepositions (Lauer
1995) and our list of 22 semantic relations. A mapping between the two tag sets was
also provided. A supervised learning model employing various linguistic features was
successfully compared against two state-of-the-art models reported in the literature.
It is also important to mention here the linguistic implications of this work. We
hope that the corpus investigations presented in this article provide new insight for the
machine translation and multilingual question answering communities. The translation
of nominal phrase and compound instances from one language to another is highly
correlated with the structure of each language, or set of languages. In this article we
measured the contribution of a set of five Romance languages to the task of semantic in-
terpretation of English nominal phrases and compounds. More specifically, we showed
that the Romanian linguistic features contribute more substantially to the overall per-
formance than the features obtained for the other Romance languages. The choice of
the Romanian linguistic constructions (either N N or N P N) is highly correlated with
their meaning. This distinct behavior of Romanian constructions is also explained by
the Slavic and Balkanic influences. An interesting future research direction would be to
consider other Indo- and non Indo-European languages and measure their contribution
to the task of interpreting nominal phrases and compounds in particular, and noun
phrases in general.
224
Girju The Syntax and Semantics of Prepositions
Acknowledgments
We would like to thank all our annotators
without whom this research would not have
been possible: Silvia Kunitz (Italian) and
Florence Mathieu-Conner (French). We also
thank Richard Sproat, Tania Ionin, and Brian
Drexler for their suggestions on various
versions of the article. And last but not least,
we also would like to thank the reviewers for
their very useful comments.
References
Alexiadou, Artemis, Liliane Haegeman, and
Melita Stavrou. 2007. Noun Phrases in the
Generative Perspective. Mouton de Gruyter,
Berlin.
Almela, Ramo?n, Pascual Cantos, Aquilino
Sa?nchez, Ramo?n Sarmiento, and Moise?s
Almela. 2005. Frequencias del Espan?ol.
Dicctionario de estudios le?xicos y morfolo?gicos.
Ed. Universitas, Madrid.
Anderson, Mona. 1983. Prenominal genitive
NPs. The Linguistic Review, 3:1?24.
Artstein, Ron. 2007. Quality Control of Corpus
Annotation Through Reliability Measures.
Association for Computational Linguistics
Conference (ACL), Prague, Czech
Republic.
Baker, Collin, Charles Fillmore, and John
Lowe. 1998. The Berkeley FrameNet
Project. In Proceedings of the 36th Annual
Meeting of the Association for Computational
Linguistics and 17th International Conference
on Computational Linguistics (COLING-ACL
1998), pages 86?90, Montreal.
Baldwin, Timothy. 2005. Distributional
similarity and collocational prepositional
phrases. In Patrick Saint-Dizier, editor,
Syntax and Semantics of Prepositions,
pages 197?210, Kluwer, Dordrecht.
Baldwin, Timothy. 2006a. Automatic
identification of English verb particle
constructions using linguistic features. In
Third ACL-SIGSEMWorkshop on
Prepositions, pages 65?72, Trento, Italy.
Baldwin, Timothy. 2006b. Representing
and Modeling the Lexical Semantics of
English Verb Particle Constructions.
In The European Association for
Computational Linguistics (EACL), the
ACL-SIGSEMWorkshop on Prepositions,
Trento.
Barker, Chris. 1998. Partitives, double
genitives and anti-uniqueness.
Natural Language and Linguistic Theory,
16:679?717.
Busa, Federica and Michael Johnston. 1996.
Cross-linguistic semantics for complex
nominals in the generative lexicon. In AISB
Workshop on Multilinguality in the Lexicon,
Sussex.
Cadiot, Piere. 1997. Les pre?positions abstraites
en franc?ais. Armand Colin, Paris.
Calzolari, Nicoletta, Charles J. Fillmore,
Ralph Grishman, Nancy Ide, Alessandro
Lenci, Catherine MacLeod, and Antonio
Zampolli. 2002. Towards best practice for
multiword expressions in computational
lexicons. In The International Conference on
Language Resources and Evaluation LREC,
pages 1934?1940, Las Palmas.
Casadei, Federica. 1991. Le locuzioni
preposizionali. Struttura lessicale e gradi
di lessicalizzazione. Lingua e Stile, XXXVI:
43?80.
Celce-Murcia, Marianne and Diane
Larsen-Freeman. 1999. The grammar book,
2nd edition. Heinle and Heinle, Boston,
MA.
Charniak, Eugene. 2000. A
maximum-entropy-inspired parser. In The
1st Conference of the North American Chapter
of the Association for Computational
Linguistics (NAACL), pages 132?139,
Seattle, WA.
Cornilescu, Alexandra. 2001. Romanian
nominalizations: Case and aspectual
structure. Journal of Linguistics, 37:467?501.
Dorr, Bonnie. 1993. Machine Translation: A
View from the Lexicon. MIT Press,
Cambridge, MA.
Downing, Pamela. 1977. On the creation and
use of English compound nouns. Language,
53:810?842.
Evans, Vyvyan and Paul Chilton, editors.
2009. Language, Cognition and Space: The
State of the Art and New Directions.
Advances in Cognitive Linguistics.
Equinox Publishing Company, London.
Fang, Alex C. 2000. A lexicalist approach
towards the automatic determination for
the syntactic functions of prepositional
phrases. Natural Language Engineering,
6:183?201.
Fellbaum, Christiane. 1998. WordNet?An
Electronic Lexical Database. MIT Press,
Cambridge, MA.
Finin, Timothy W. 1980. The Semantic
Interpretation of Compound Nominals.
Ph.D. thesis, University of Illinois at
Urbana-Champaign, Urbana-Champaign,
IL.
Giorgi, Alessandra and Giuseppe
Longobardi. 1991. The syntax of noun
phrases. Cambridge University Press,
London.
Girju, Roxana, Alexandra Badulescu, and
Dan Moldovan. 2006. Automatic discovery
225
Computational Linguistics Volume 35, Number 2
of part-whole relations. Computational
Linguistics, 32(1):83?135.
Girju, Roxana, Dan Moldovan, Marta Tatu,
and Daniel Antohe. 2005. On the
Semantics of Noun Compounds. Computer
Speech and Language, Special Issue on
Multiword Expressions, 19(4):479?496.
Gleitman, Lila R. and Henry Gleitman. 1970.
Phrase and Paraphrase: Some Innovative Uses
of Language. Norton, New York.
Gocsik, Karen. 2004. English as a Second
Language. Dartmouth College Press,
Hanover, NH.
Grimshaw, Jane. 1990. Argument Structure.
MIT Press, Cambridge, MA.
Herskovits, Annette. 1987. Language and
Spatial Cognition: An Interdisciplinary
Study of the Prepositions in English.
Cambridge University Press, Cambridge,
MA.
Ionin, Tania, Ora Matushansky, and Eddy
Ruys. 2006. Parts of speech: Toward a
unified semantics for partitives. In
Conference of the North East Linguistic
Society (NELS), pages 357?370,
Amherst, MA.
Jensen, Per Anker and Jo?rgen F. Nilsson.
2005. Ontology-based semantics for
prepositions. In Patrick Saint-Dizier,
editor, Syntax and Semantics of Prepositions,
volume 29 of Text, Speech and Language
Technology. Springer, Dordrecht.
Jespersen, Otto. 1954. AModern English
Grammar on Historical Principles. George
Allen & Unwin Ltd., Heidelberg and
London.
Johnston, Michael and Federica Busa. 1996.
Qualia structure and the compositional
interpretation of compounds. In Evelyne
Viegas, editor, Breadth and Depth of
Semantics Lexicon, pages 77?88, Kluwer
Academic, Dordrecht.
Kim, Su Nam and Timothy Baldwin. 2005.
Automatic interpretation of noun
compounds using WordNet similarity.
In The International Joint Conference on
Natural Language Processing (IJCNLP),
pages 945?956, Jeju.
Kim, Su Nam and Timothy Baldwin. 2006.
Interpreting semantic relations in noun
compounds via verb semantics. In The
International Conference on Computational
Linguistics / the Association for
Computational Linguistics (COLING/ACL) -
Main Conference Poster Sessions,
pages 491?498, Sydney.
Kipper, Karin, Hoa Trang Dang, and Martha
Palmer. 2000. Class-based construction
of a verb lexicon. In The National
Conference on Artificial Intelligence (AAAI),
pages 691?696, Austin, TX.
Kordoni, Valia. 2005. Prepositional
arguments in a multilingual context.
In Patrick Saint-Dizier, editor,
Syntax and Semantics of Prepositions,
volume 29 of Text, Speech and Language
Technology. Springer, Dordrecht,
pages 307?330.
Kordoni, Valia. 2006. PPs as verbal
arguments: From a computational
semantics perspective. In The European
Association for Computational Linguistics
(EACL), the ACL-SIGSEMWorkshop on
Prepositions, Trento, Italy.
Lapata, Mirella. 2002. The Disambiguation of
nominalisations. Computational Linguistics,
28(3):357?388.
Lapata, Mirella and Frank Keller. 2005.
Web-based models for natural language
processing. ACM Transactions on Speech and
Language Processing, 2:1?31.
Lascarides, Alex and Ann Copestake. 1998.
Pragmatics and word meaning. Journal of
Linguistics, 34:387?414.
Lauer, Mark. 1995. Corpus statistics meet the
noun compound: Some empirical results.
In The Association for Computational
Linguistics Conference (ACL), pages 47?54,
Cambridge, MA.
Lees, Robert B. 1963. The Grammar of English
Nominalisations. Mouton, The Hague.
Lenci, Alessandro, Nuria Bel, Federica Busa,
Nicoletta Calzolari, Elisabetta Gola,
Monica Monachini, Antoine Ogonowski,
Ivonne Peters, Wim Peters, Nilda Ruimy,
Marta Villegas, and Antonio Zampolli.
2000. SIMPLE: A general framework for
the development of multilingual lexicons.
International Journal of Lexicography,
13:249?263.
Lersundi, Mikel and Eneko Aggire. 2006.
Multilingual inventory of interpretations
for postpositions and prepositions. In
Patrick Saint-Dizier, editor, Syntax and
Semantics of Prepositions, volume 29 of Text,
Speech and Language Technology. Springer,
Dordrecht, pages 69?82.
Levi, Judith. 1978. The Syntax and Semantics
of Complex Nominals. Academic Press,
New York.
Linstromberg, Seth. 1997. English Prepositions
Explained. John Benjamins Publishing Co.,
Amsterdam/Philadelphia.
Litkowski, Kenneth C. and Orin Hargraves.
2005. The Preposition Project. In The
ACL-SIGSEMWorkshop on the Linguistic
Dimensions of Prepositions and their Use in
Computational Linguistics Formalisms and
226
Girju The Syntax and Semantics of Prepositions
Applications, pages 171?179, Colchester,
UK.
Luraghi, Silvia. 2003. Prepositions in Greek and
Indo-European. Benjamins, Amsterdam.
Lyons, Christopher. 1986. The syntax of
English genitive constructions. Journal of
Linguistics, 22:123?143.
Melis, Ludo. 2002. Les pre?positions du franc?ais.
L?essentiel franc?ais. Ophrys, Paris/Gap.
Meyers, A., R. Reeves, Catherine Maclead,
Rachel Szekely, Veronika Zielinsk, Brian
Young, and R. Grishman. 2004. The
cross-breeding of dictionaries. In
Proceedings of the 5th International
Conference on Language Resources and
Evaluation (LREC-2004), pages 1095?1098,
Lisbon.
Mihalcea, Rada and Ehsanul Faruque. 2004.
SenseLearner: Minimally supervised word
sense disambiguation for all words in
open text. In Senseval-3: Third International
Workshop on the Evaluation of Systems for the
Semantic Analysis of Text, pages 155?158,
Barcelona.
Moldovan, Dan and Adriana Badulescu.
2005. A semantic scattering model for the
automatic interpretation of genitives. In
Proceedings of Human Language Technology
Conference and Conference on Empirical
Methods in Natural Language Processing,
pages 891?898, Vancouver.
Moldovan, Dan, Adriana Badulescu, Marta
Tatu, Daniel Antohe, and Roxana Girju.
2004. Models for the semantic
classification of noun phrases. In The
Human Language Technology Conference /
North American Association for
Computational Linguistics Conference
(HLT/NAACL), Workshop on Computational
Lexical Semantics, pages 60?67,
Boston, MA.
Moldovan, Dan and Roxana Girju. 2003.
Proceedings of the Tutorial on Knowledge
Discovery from Text. Association for
Computational Linguistics, Sapporo,
Japan.
Nakov, Preslav and Marti Hearst. 2005.
Search engine statistics beyond the
n-gram: Application to noun compound
bracketing. In The 9th Conference on
Computational Natural Language Learning,
pages 835?842, Vancouver.
O?Hara, Tom and Janyce Wiebe. 2003.
Preposition semantic classification via
Treebank and FrameNet. In Conference on
Computational Natural Language Learning
(CoNLL), pages 79?86, Edmonton.
Pantel, Patrick and Marco Pennacchiotti.
2006. Espresso: Leveraging generic
patterns for automatically harvesting
semantic relations. In The International
Computational Linguistics Conference /
Association for Computational Linguistics
(COLING/ACL), pages 113?120, Sydney.
Pantel, Patrick and Deepak Ravichandran.
2004. Automatically labeling semantic
classes. In The Human Language Technology
Conference of the North American Chapter
of the Association for Computational
Linguistics (HLT/NAACL), pages 321?328,
Boston, MA.
Pennacchiotti, Marco and Patrick Pantel.
2006. Ontologizing semantic relations. In
The International Computational Linguistics
Conference / Association for Computational
Linguistics (COLING/ACL), pages 793?800,
Sydney.
Pustejovsky, James. 1995. The Generative
Lexicon. MIT Press, Cambridge, MA.
Pustejovsky, James, Catherine Havasi, Roser
Sauri, Patrick Hanks, Jessica Littman,
Anna Rumshisky, Jose Castano, and Marc
Verhagen. 2006. Towards a generative
lexical resource: The brandeis semantic
ontology. In The International Conference on
Language Resources and Evaluation (LREC),
pages 385?388, Genoa.
Romaine, Suzanne. 1995. Bilingualism.
Blackwell, Oxford.
Rosario, Barbara and Marti Hearst. 2001.
Classifying the semantic relations in noun
compounds. In Conference on Empirical
Methods in Natural Language Processing,
pages 82?90, Pittsburgh, PA.
Rosario, Barbara, Marti Hearst, and Charles
Fillmore. 2002. The descent of hierarchy,
and selection in relational semantics. In
The 40th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 247?254, Philadelphia, PA.
Saint-Dizier, Patrick. 2005a. PrepNet: A
framework for describing prepositions:
Preliminary investigation results. In The
6th International Workshop on Computational
Semantics, pages 25?34, Tilburg.
Saint-Dizier, Patrick, editor. 2005b. Syntax
and Semantics of Prepositions. Springer,
Dordrecht.
Selkirk, Elisabeth. 1982a. Some remarks on
noun phrase structure. In Peter W.
Culicover, Thomas Wasow, and Adrian
Akmajian, editors, Formal Syntax.
Academic Press, London.
Selkirk, Elisabeth. 1982b. Syntax of Words.
MIT Press, Cambridge, MA.
Spa?rck Jones, Karen. 1983. Compound
noun interpretation problems. In
Frank Fallside and William A. Woods,
227
Computational Linguistics Volume 35, Number 2
editors, Computer Speech Processing.
Prentice-Hall, Englewood Cliffs, NJ,
pages 363?381.
Tatu, Marta and Dan Moldovan. 2005. A
semantic approach to recognizing textual
entailmant. In Proceedings of Human
Language Technology Conference and
Conference on Empirical Methods in Natural
Language Proceesing (HLT/EMNLP 2005),
pages 371?378, Vancouver.
Turney, Peter. 2006. Expressing implicit
semantic relations without supervision. In
The Computational Linguistics Conference /
Association for Computational Linguistics
Conference (COLING/ACL), pages 313?320,
Sydney.
Tyler, Andrea and Vyvyan Evans. 2003.
The Semantics of English Prepositions:
Spatial Sciences, Embodied Meaning, and
Cognition. Cambridge University Press,
Cambridge, MA.
Vandeloise, Claude, editor. 1993. La couleur
des pre?positions, volume 110. Larousse,
Paris.
Villavicencio, Aline. 2006. Verb-particle
constructions in the World Wide Web. In
Patrick Saint-Dizier, editor, Syntax and
Semantics of Prepositions, volume 29 of Text,
Speech and Language Technology. Springer,
Dordrecht, pages 115?130.
Volk, Martin. 2006. How bad is the problem
of PP-attachment? A comparison of
English, German and Swedish. In The
European Association for Computational
Linguistics (EACL), the ACL-SIGSEM
Workshop on Prepositions, pages 81?88,
Trento.
Vossen, Peter. 1998. EuroWordNet: A
Multilingual Database with Lexical
Semantic Networks. Kluwer Academic
Publishers, Verlag.
Winston, Morton, Roger Chaffin, and
Douglas Hermann. 1987. A taxonomy of
part-whole relations. Cognitive Science,
11:417?444.
Zelinski-Wibbelt, Cornelia, editor. 1993. The
Semantics of Prepositions. Mouton de
Gruyter, Berlin.
228
This article has been cited by:
1. Timothy Baldwin, Valia Kordoni, Aline Villavicencio. 2009. Prepositions in Applications:
A Survey and Introduction to the Special IssuePrepositions in Applications: A Survey and
Introduction to the Special Issue. Computational Linguistics 35:2, 119-149. [Citation] [PDF]
[PDF Plus]
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 568?575,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Improving the Interpretation of Noun Phrases with Cross-linguistic
Information
Roxana Girju
University of Illinois at Urbana-Champaign
girju@uiuc.edu
Abstract
This paper addresses the automatic classifi-
cation of semantic relations in noun phrases
based on cross-linguistic evidence from a
set of five Romance languages. A set
of novel semantic and contextual English?
Romance NP features is derived based on
empirical observations on the distribution
of the syntax and meaning of noun phrases
on two corpora of different genre (Europarl
and CLUVI). The features were employed
in a Support Vector Machines algorithm
which achieved an accuracy of 77.9% (Eu-
roparl) and 74.31% (CLUVI), an improve-
ment compared with two state-of-the-art
models reported in the literature.
1 Introduction
Semantic knowledge is very important for any ap-
plication that requires a deep understanding of natu-
ral language. The automatic acquisition of semantic
information in text has become increasingly impor-
tant in ontology development, information extrac-
tion, question answering, and other advanced natural
language processing applications.
In this paper we present a model for the auto-
matic semantic interpretation of noun phrases (NPs),
which is the task of determining the semantic re-
lation among the noun constituents. For example,
family estate encodes a POSSESSION relation, while
dress of silk refers to PART-WHOLE. The problem,
while simple to state is hard to solve. The rea-
son is that the meaning of these constructions is
most of the time ambiguous or implicit. Interpreting
NPs correctly requires various types of information
from world knowledge to complex context features.
Moreover, the extension of this task to other natu-
ral languages brings forward new issues and prob-
lems. For instance, beer glass translates into tarro
de cerveza in Spanish, bicchiere da birra in Italian,
verre a` bie`re in French, and pahar de bere in Roma-
nian. Thus, an important research question is how
do the syntactic constructions in the target language
contribute to the preservation of meaning in context.
In this paper we investigate noun phrases based on
cross-linguistic evidence and present a domain inde-
pendent model for their semantic interpretation. We
aim at uncovering the general aspects that govern
the semantics of NPs in English based on a set of
five Romance languages: Spanish, Italian, French,
Portuguese, and Romanian. The focus on Romance
languages is well motivated. It is mostly true that
English noun phrases translate into constructions of
the form N P N in Romance languages where, as
we will show below, the P (preposition) varies in
ways that correlate with the semantics. Thus Ro-
mance languages will give us another source of evi-
dence for disambiguating the semantic relations in
English NPs. We also present empirical observa-
tions on the distribution of the syntax and meaning
of noun phrases on two different corpora based on
two state-of-the-art classification tag sets: Lauer?s
set of 8 prepositions (Lauer, 1995) and our list of 22
semantic relations. We show that various crosslin-
gual cues can help in the NP interpretation task when
employed in an SVM model. The results are com-
pared against two state of the art approaches: a su-
568
pervised machine learning model, Semantic Scatter-
ing (Moldovan and Badulescu, 2005), and a web-
based probabilistic model (Lapata and Keller, 2004).
The paper is organized as follows. In Section 2
we present a summary of the previous work. Sec-
tion 3 lists the syntactic and semantic interpretation
categories used along with observations regarding
their distribution on the two different cross-lingual
corpora. Sections 4 and 5 present a learning model
and results for the interpretation of English noun
phrases. Finally, in Section 6 we offer some dis-
cussion and conclusions.
2 Related Work
Currently, the best-performing NP interpretation
methods in computational linguistics focus mostly
on two consecutive noun instances (noun com-
pounds) and rely either on rather ad-hoc, domain-
specific semantic taxonomies, or on statistical mod-
els on large collections of unlabeled data. Recent
results have shown that symbolic noun compound
interpretation systems using machine learning tech-
niques coupled with a large lexical hierarchy per-
form with very good accuracy, but they are most of
the time tailored to a specific domain (Rosario and
Hearst, 2001). On the other hand, the majority of
corpus statistics approaches to noun compound in-
terpretation collect statistics on the occurrence fre-
quency of the noun constituents and use them in a
probabilistic model (Lauer, 1995). More recently,
(Lapata and Keller, 2004) showed that simple unsu-
pervised models perform significantly better when
the frequencies are obtained from the web, rather
than from a large standard corpus. Other researchers
(Pantel and Pennacchiotti, 2006), (Snow et al, 2006)
use clustering techniques coupled with syntactic de-
pendency features to identify IS-A relations in large
text collections. (Kim and Baldwin, 2006) and (Tur-
ney, 2006) focus on the lexical similarity of unseen
noun compounds with those found in training.
However, although the web-based solution might
overcome the data sparseness problem, the current
probabilistic models are limited by the lack of deep
linguistic information. In this paper we investigate
the role of cross-linguistic information in the task
of English NP semantic interpretation and show the
importance of a set of novel linguistic features.
3 Corpus Analysis
For a better understanding of the meaning of the
N N and N P N instances, we analyzed the seman-
tic behavior of these constructions on a large cross-
linguistic corpora of examples. We are interested
in what syntactic constructions are used to trans-
late the English instances to the target Romance lan-
guages and vice-versa, what semantic relations do
these constructions encode, and what is the corpus
distribution of the semantic relations.
3.1 Lists of semantic classification relations
Although the NP interpretation problem has been
studied for a long time, researchers haven?t agreed
on the number and the level of abstraction of these
semantic categories. They can vary from a few
prepositions (Lauer, 1995) to hundreds or thousands
specific semantic relations (Finin, 1980). The more
abstract the categories, the more noun phrases are
covered, but also the more room for variation as to
which category a phrase should be assigned.
In this paper we experiment with two state of the
art classification sets used in NP interpretation. The
first is a core set of 22 semantic relations (22 SRs)
identified by us from the computational linguistics
literature. This list, presented in Table 1 along with
examples is general enough to cover a large major-
ity of text semantics while keeping the semantic re-
lations to a manageable number. The second set is
Lauer?s list of 8 prepositions (8 PP) and can be ap-
plied only to noun compounds (of, for, with, in, on,
at, about, and from ? e.g., according to this classifi-
cation, love story can be classified as story about
love). We selected these sets as they are of different
size and contain semantic classification categories at
different levels of abstraction. Lauer?s list is more
abstract and, thus capable of encoding a large num-
ber of noun compound instances, while the 22-SR
list contains finer grained semantic categories. We
show below the coverage of these semantic lists on
two different corpora and how well they solve the
interpretation problem of noun phrases.
3.2 The data
The data was collected from two text collections
with different distributions and of different genre,
569
POSSESSION (family estate); KINSHIP (sister of the boy); PROPERTY (lubricant viscosity); AGENT (return of the natives);
THEME (acquisition of stock); TEMPORAL (morning news); DEPICTION-DEPICTED (a picture of my niece); PART-WHOLE
(brush hut); HYPERNYMY (IS-A) (daisy flower); CAUSE (scream of pain); MAKE/PRODUCE (chocolate factory); INSTRUMENT
(laser treatment); LOCATION (castle in the desert); PURPOSE (cough syrup); SOURCE (grapefruit oil); TOPIC (weather report);
MANNER (performance with passion); beneficiary (rights of citizens); MEANS (bus service); EXPERIENCER (fear of the girl);
MEASURE (cup of sugar); TYPE (framework law);
Table 1: The list of 22 semantic relations (22-SRs).
Europarl1 and CLUVI2. The Europarl data was as-
sembled by combining the Spanish-English, Italian-
English, French-English and Portuguese-English
corpora which were automatically aligned based on
exact matches of English translations. Then, we
considered only the English sentences which ap-
peared verbatim in all four language pairs. The re-
sulting English corpus contained 10,000 sentences
which were syntactically parsed (Charniak, 2000).
From these we extracted the first 3,000 NP instances
(N N: 48.82% and N P N: 51.18%).
CLUVI is an open text repository of parallel cor-
pora of contemporary oral and written texts in some
of the Romance languages. Here, we focused only
on the English-Portuguese and English-Spanish par-
allel texts from the works of John Steinbeck, H. G.
Wells, J. Salinger, and others. Using the CLUVI
search interface we created a sentence-aligned par-
allel corpus of 2,800 English-Spanish and English-
Portuguese sentences. The English versions were
automatically parsed after which each N N and
N P N instance thus identified was manually mapped
to the corresponding translations. The resulting cor-
pus contains 2,200 English instances with a distribu-
tion of 26.77% N N and 73.23% N P N.
3.3 Corpus Annotation
For each corpus, each NP instance was presented
separately to two experienced annotators in a web
interface in context along with the English sentence
and its translations. Since the corpora do not cover
some of the languages (Romanian in Europarl and
CLUVI, and Italian and French in CLUVI), three
other native speakers of these languages and flu-
ent in English provided the translations which were
1http://www.isi.edu/koehn/europarl/. This corpus contains
over 20 million words in eleven official languages of the Euro-
pean Union covering the proceedings of the European Parlia-
ment from 1996 to 2001.
2CLUVI - Linguistic Corpus of the University of Vigo - Par-
allel Corpus 2.1 - http://sli.uvigo.es/CLUVI/
added to the list. The two computational semantics
annotators had to tag each English constituent noun
with its corresponding WordNet sense and each in-
stance with the corresponding semantic category. If
the word was not found in WordNet the instance was
not considered. Whenever the annotators found an
example encoding a semantic category other than
those provided or they didn?t know what interpre-
tation to give, they had to tag it as ?OTHER-SR?, and
respectively ?OTHER-PP?3. The details of the anno-
tation task and the observations drawn from there are
presented in a companion paper (Girju, 2007).
The corpus instances used in the corpus analy-
sis phase have the following format: <NPEn ;NPEs;
NPIt; NPFr; NPPort; NPRo; target>. The word
target is one of the 23 (22 + OTHER-SR) seman-
tic relations and one of the eight prepositions con-
sidered or OTHER-PP (with the exception of those
N P N instances that already contain a preposi-
tion). For example, <development cooperation;
cooperacio?n para el desarrollo; cooperazione allo
sviluppo; coope?ration au de?veloppement; cooperare
pentru dezvoltare; PURPOSE / FOR>.
The annotators? agreement was measured using
Kappa statistics: K = Pr(A)?Pr(E)1?Pr(E) , where Pr(A)
is the proportion of times the annotators agree and
Pr(E) is the probability of agreement by chance.
The Kappa values were obtained on Europarl (N N:
0.80 for 8-PP and 0.61 for 22-SR; N P N: 0.67 for
22-SR) and CLUVI (N N: 0.77 for 8-PP and 0.56 for
22-SR; N P N: 0.68 for 22-SR). We also computed
the number of pairs that were tagged with OTHER
by both annotators for each semantic relation and
preposition paraphrase, over the number of exam-
ples classified in that category by at least one of the
judges (in Europarl: 91% for 8-PP and 78% for 22-
SR; in CLUVI: 86% for 8-PP and 69% for 22-SR).
The agreement obtained on the Europarl corpus is
3The annotated corpora resulted in this research is available
at http://apfel.ai.uiuc.edu.
570
higher than the one on CLUVI on both classification
sets. This is partially explained by the distribution of
semantic relations in both corpora, as will be shown
in the next subsection.
3.4 Cross-linguistic distribution of Syntactic
Constructions
From the sets of 2,954 (Europarl) and 2,168
(CLUVI) instances resulted after annotation, the
data show that over 83% of the translation patterns
for both text corpora on all languages were of the
type N N and N P N. However, while their distribu-
tion is balanced in the Europarl corpus (about 45%,
with a 64% N P N ? 26% N N ratio for Romanian),
in CLUVI the N P N constructions occur in more
than 85% of the cases (again, with the exception of
Romanian ? 50%). It is interesting to note here that
some of the English NPs are translated into both
noun?noun and noun?adjective compounds in the
target languages. For example, love affair translates
in Italian as storia d?amore or the noun?adjective
compound relazione amorosa. There are also in-
stances that have just one word correspondent in
the target language (e.g., ankle boot is bottine in
French). The rest of the data is encoded by other
syntactic paraphrases (e.g., bomb site is luogo dove
e` esplosa la bomba (It.)). 4.
From the initial corpus we considered those En-
glish instances that had all the translations encoded
only by N N and N P N. Out of these, we selected
only 1,023 Europarl and 1,008 CLUVI instances en-
coded by N N and N P N in all languages considered
and resulted after agreement.
4 Model
4.1 Feature space
We have identified and experimented with 13 NP
features presented below. With the exceptions of
features F1-F5 (Girju et al, 2005), all the other fea-
tures are novel.
A. English Features
F1 and F2. Noun semantic class specifies the Word-
Net sense of the head (F1) and modifier noun (F2)
and implicitly points to all its hypernyms. For ex-
ample, the hypernyms of car#1 are: {motor vehi-
4
?the place where the bomb is exploded? (It.)
cle}, .. {entity}. This feature helps generalize over
the semantic classes of the two nouns in the corpus.
F3 and F4. WordNet derivationally related form
specifies if the head (F3) and the modifier (F4) nouns
are related to a corresponding WordNet verb (e.g.
statement derived from to state; cry from to cry).
F5. Prepositional cues that link the two nouns in an
NP. These can be either simple or complex preposi-
tions such as ?of? or ?according to?. In case of N N
instances, this feature is ??? (e.g., framework law).
F6 and F7. Type of nominalized noun indicates the
specific class of nouns the head (F6) or modifier (F7)
belongs to depending on the verb it derives from.
First, we check if the noun is a nominalization. For
English we used NomLex-Plus (Meyers et al, 2004)
to map nouns to corresponding verbs.5 For exam-
ple, ?destruction of the city?, where destruction is
a nominalization. F6 and F7 may overlap with fea-
tures F3 and F4 which are used in case the noun to be
checked does not have an entry in the NomLex-Plus
dictionary. These features are of particular impor-
tance since they impose some constraints on the pos-
sible set of relations the instance can encode. They
take the following values (identified based on list of
verbs extracted from VerbNet (Kipper et al, 2000)):
a. Active form nouns which have an intrinsic
active voice predicate-argument structure. (Giorgi
and Longobardi, 1991) argue that in English this is a
necessary restriction. Most of the time, they rep-
resent states of emotion, such as fear, desire, etc.
These nouns mark their internal argument through
of and require most of the time prepositions like por
and not de when translated in Romance. Our obser-
vations on the Romanian translations (captured by
features F12 and F13 below) show that the possible
cases of ambiguity are solved by the type of syntac-
tic construction used. For example, N N genitive-
marked constructions are used for EXPERIENCER?
encoding instances, while N de N or N pentru N (N
for N) are used for other relations. Such examples
are the love of children ? THEME (and not the love by
the children). (Giorgi and Longobardi, 1991) men-
tion that with such nouns that resist passivisation,
5NomLex-Plus is a hand-coded database of 5,000 verb nom-
inalizations, de-adjectival, and de-adverbial nouns including the
corresponding subcategorization frames (verb-argument struc-
ture information).
571
the preposition introducing the internal argument,
even if it is of, has always a semantic content, and
is not a bare case-marker realizing the genitive case.
b. Unaccusative (ergative) nouns which are de-
rived from ergative verbs that take only internal ar-
guments (e.g., not agentive ones). For example, the
transitive verb to disband allows the subject to be
deleted as in the following sentences (1) ?The lead
singer disbanded the group in 1991.? and (2) ?The
group disbanded.?. Thus, the corresponding erga-
tive nominalization the disbandment of the group en-
codes a THEME relation and not AGENT.
c. Unergative (intransitive) nouns are derived
from intransitive verbs and take only AGENT seman-
tic relations. For example, the departure of the girl.
d. Inherently passive nouns such as the cap-
ture of the soldier. These nouns, like the verbs they
are derived from, assume a default AGENT (subject)
and being transitive, associate to their internal argu-
ment (introduced by ?of? in the example above) the
THEME relation.
B. Romance Features
F8, F9, F10, F11 and F12. Prepositional cues that
link the two nouns are extracted from each transla-
tion of the English instance: F8 (Es.), F9 (Fr.), F10
(It.), F11 (Port.), and F12 (Ro.). These can be either
simple or complex prepositions (e.g., de, in materia
de (Es.)) in all five Romance languages, or the Ro-
manian genitival article a/ai/ale. In Romanian the
genitive case is assigned by the definite article of the
first noun to the second noun, case realized as a suf-
fix if the second noun is preceded by the definite arti-
cle or as one of the genitival articles a/ai/ale. For ex-
ample, the noun phrase the beauty of the girl is trans-
lated as frumuset?ea fetei (beauty-the girl-gen), and
the beauty of a girl as frumuset?ea unei fete (beauty-
the gen girl). For N N instances, this feature is ???.
F13. Noun inflection is defined only for Romanian
and shows if the modifier noun is inflected (indicates
the genitive case). This feature is used to help differ-
entiate between instances encoding IS-A and other
semantic relations in N N compounds in Romanian.
It also helps in features F6 and F7, case a) when the
choice of syntactic construction reflects different se-
mantic content. For example, iubirea pentru copii
(N P N) (the love for children) and not iubirea copi-
ilor (N N) (love expressed by the children).
4.2 Learning Models
We have experimented with the support vector ma-
chines (SVM) model6 and compared the results
against two state-of-the-art models: a supervised
model, Semantic Scattering (SS), (Moldovan and
Badulescu, 2005), and a web-based unsupervised
model (Lapata and Keller, 2004). The SVM and SS
models were trained and tested on the Europarl and
CLUVI corpora using a 8:2 ratio. The test dataset
was randomly selected from each corpus and the test
nouns (only for English) were tagged with the cor-
responding sense in context using a state of the art
WSD tool (Mihalcea and Faruque, 2004).
After the initial NP instances in the training and
test corpora were expanded with the corresponding
features, we had to prepare them for SVM and SS.
The method consists of a set of automatic iterative
procedures of specialization of the English nouns on
the WordNet IS-A hierarchy. Thus, after a set of nec-
essary specialization iterations, the method produces
specialized examples which through supervised ma-
chine learning are transformed into sets of seman-
tic rules. This specialization procedure improves
the system?s performance since it efficiently sepa-
rates the positive and negative noun-noun pairs in
the WordNet hierarchy.
Initially, the training corpus consists of examples
in the format exemplified by the feature space. Note
that for the English NP instances, each noun con-
stituent was expanded with the corresponding Word-
Net top semantic class. At this point, the general-
ized training corpus contains two types of examples:
unambiguous and ambiguous. The second situation
occurs when the training corpus classifies the same
noun ? noun pair into more than one semantic cat-
egory. For example, both relationships ?chocolate
cake?-PART-WHOLE and ?chocolate article?-TOPIC
are mapped into the more general type <entity#1,
entity#1, PART-WHOLE/TOPIC>7. We recursively
specialize these examples to eliminate the ambigu-
ity. By specialization, the semantic class is replaced
with the corresponding hyponym for that particular
sense, i.e. the concept immediately below in the hi-
erarchy. These steps are repeated until there are no
6We used the package LIBSVM with a radial-based kernel
http://www.csie.ntu.edu.tw/?cjlin/libsvm/
7The specialization procedure applies only to features 1, 2.
572
more ambiguous examples. For the example above,
the specialization stops at the first hyponym of en-
tity: physical entity (for cake) and abstract entity
(for article). For the unambiguous examples in the
generalized training corpus (those that are classified
with a single semantic relation), constraints are de-
termined using cross validation on SVM.
A. Semantic Scattering uses a training data set
to establish a boundary G? on WordNet noun hier-
archies such that each feature pair of noun ? noun
senses fij on this boundary maps uniquely into one
of a predefined list of semantic relations, and any
feature pair above the boundary maps into more than
one semantic relation. For any new pair of noun?
noun senses, the model finds the closest WordNet
boundary pair.
The authors define with SCm = {fmi } and
SCh = {fhj } the sets of semantic class features
for modifier noun and, respectively head noun. A
pair of <modifier ? head> nouns maps uniquely
into a semantic class feature pair < fmi , fhj >,
denoted as fij . The probability of a semantic re-
lation r given feature pair fij , P (r|fij) = n(r,fij)n(fij) ,
is defined as the ratio between the number of oc-
currences of a relation r in the presence of fea-
ture pair fij over the number of occurrences of
feature pair fij in the corpus. The most proba-
ble semantic relation r? is arg maxr?R P (r|fij) =
arg maxr?R P (fij |r)P (r).
B. (Lapata and Keller, 2004)?s web-based un-
supervised model classifies noun - noun instances
based on Lauer?s list of 8 prepositions and uses
the web as training corpus. They show that the
best performance is obtained with the trigram model
f(n1, p, n2). The count used for a given trigram is
the number of pages returned by Altavista on the tri-
gram corresponding queries. For example, for the
test instance war stories, the best number of hits was
obtained with the query stories about war.
For the Europarl and CLUVI test sets, we repli-
cated Lapata & Keller?s experiments using Google8.
We formed inflected queries with the patterns they
proposed and searched the web.
8As Google limits the number of queries to 1,000 per day,
we repeated the experiment for a number of days. Although
(Lapata and Keller, 2004) used Altavista in their experiments,
they showed there is almost no difference between the correla-
tions achieved using Google and Altavista counts.
5 Experimental results
Table 2 shows the results obtained against SS and
Lapata & Keller?s model on both corpora and the
contribution the features exemplified in one baseline
and six versions of the SVM model. The baseline is
defined only for the English part of the NP feature
set and measures the the contribution of the Word-
Net IS-A lexical hierarchy specialization. The base-
line does not differentiate between unambiguous and
ambiguous training examples (after just one level
specialization) and thus, does not specialize the am-
biguous ones. Moreover, here we wanted to see what
is the difference between SS and SVM, and what is
the contribution of the other English features, such
as preposition and nominalization (F1?F7).
The table shows that, overall the performance is
better for the Europarl corpus than for CLUVI. For
the Baseline and SV M1, SS [F1 + F2] gives bet-
ter results than SVM. The inclusion of other English
features (SVM [F1?F7]) adds more than 15% (with
a higher increase in Europarl) for SV M1.
The contribution of Romance linguistic features.
Since our intuition is that the more translations are
provided for an English noun phrase instance, the
better the results, we wanted to see what is the im-
pact of each Romance language on the overall per-
formance. Thus, SV M2 shows the results obtained
for English and the Romance language that con-
tributed the least to the performance (F1?F12). Here
we computed the performance on all five English ?
Romance language combinations and chose the Ro-
mance language that provided the best result. Thus,
SVM #2, #3, #4, #5, and #6 add Spanish, French,
Italian, Portuguese, and Romanian in this order and
show the contribution of each Romance preposition
and all features for English.
The language ranking in Table 2 shows that Ro-
mance languages considered here have a different
contribution to the overall performance. While the
addition of Italian in Europarl decreases the per-
formance, Portuguese doesn?t add anything. How-
ever, a closer analysis of the data shows that this
is mostly due to the distribution of the corpus in-
stances. For example, French, Italian, Spanish, and
Portuguese are most of the time consistent in the
choice of preposition (e.g. most of the time, if the
preposition ?de? (?of?) is used in French, then the
573
Learning models Results [%]
CLUVI Europarl
8-PP 22-SR 8-PP 22-SR
Baseline (En.) (no specializ.) SS (F1+F2) 44.11 48.03 38.7 38
SVM (F1+F2) 36.37 40.67 31.18 34.81
SVM (F1-F7) ? 52.15 ? 47.37
SVM1 (En.) SS (F1+F2) 56.22 61.33 53.1 56.81
SVM (F1+F2) 45.08 46.1 40.23 42.2
SVM (F1-F7) ? 62.54 ? 74.19
SVM2 (En. + Es.) SVM (F1-F8) ? 64.18 ? 75.74
SVM3 (En.+Es.+Fr.) SVM (F1-F9) ? 67.8 ? 76.52
SVM4 (En.+Es.+Fr.+It.) SVM (F1-F10) ? 66.31 ? 75.74
SVM5 (En.+Es.+Fr.+It+Port.) SVM (F1-F11) ? 67.12 ? 75.74
SVM6 (En.+Romance: F1?F13) ? 74.31 ? 77.9
Lapata & Keller?s unsupervised model (En.) 44.15 ? 45.31 ?
Table 2: The performance of the cross-linguistic SVM models compared against one baseline, SS model and
Lapata & Keller?s unsupervised model. Accuracy (number of correctly labeled instances over the number of
instances in the test set).
corresponding preposition is used in the other four
language translations). A notable exception here
is Romanian which provides two possible construc-
tions: the N P N and the genitive-marked N N. The
table shows (in the increase in performance between
SV M5 and SV M6) that this choice is not random,
but influenced by the meaning of the instances (fea-
tures F12, F13). This observation is also supported
by the contribution of each feature to the overall per-
formance. For example, in Europarl, the WordNet
verb and nominalization features of the head noun
(F3, F6) have a contribution of 4.08%, while for the
modifier nouns it decreases by about 2%. The prepo-
sition (F5) contributes 4.41% (Europarl) and 5.24%
(CLUVI) to the overall performance.
A closer analysis of the data shows that in Eu-
roparl most of the N N instances were naming noun
compounds such as framework law (TYPE) and,
most of the time, are encoded by N N patterns in
the target languages (e.g., legge quadro (It.)). In
the CLUVI corpus, on the other hand, the N N Ro-
mance translations represented only 1% of the data.
A notable exception here is Romanian where most
NPs are represented as genitive?marked noun com-
pounds. However, there are instances that are en-
coded mostly or only as N P N constructions and this
choice correlates with the meaning of the instance.
For example, the milk glass (PURPOSE) translates
as paharul de lapte (glass-the of milk) and not as
paharul laptelui (glass-the milk-gen), the olive oil
(SOURCE) translates as uleiul de ma?sline (oil-the of
olive) and not as uleiul ma?slinei (oil-the olive-gen).
Other examples include CAUSE and TOPIC.
Lauer?s set of 8 prepositions represents 94.5%
(Europarl) and 97% (CLUVI) of the N P N in-
stances. From these, the most frequent preposition
is ?of? with a coverage of 70.31% (Europarl) and
85.08% (CLUVI). Moreover, in the Europarl cor-
pus, 26.39% of the instances are synthetic phrases
(where one of the nouns is a nominalization) encod-
ing AGENT, EXPERIENCER, THEME, BENEFICIARY.
Out of these instances, 74.81% use the preposition
of. In CLUVI, 11.71% of the examples were ver-
bal, from which the preposition of has a coverage of
82.20%. The many-to-many mappings of the prepo-
sitions (especially of/de) to the semantic classes adds
to the complexity of the interpretation task. Thus,
for the interpretation of these constructions a system
must rely on the semantic information of the prepo-
sition and two constituent nouns in particular, and
on context in general.
In Europarl, the most frequently occurring re-
lations are PURPOSE, TYPE, and THEME that to-
gether represent about 57% of the data followed by
PART-WHOLE, PROPERTY, TOPIC, AGENT, and LO-
CATION with an average coverage of about 6.23%.
Moreover, other relations such as KINSHIP, DE-
PICTION, MANNER, MEANS did not occur in this
corpus and 5.08% represented OTHER-SR relations.
This semantic distribution contrasts with the one
in CLUVI, which uses a more descriptive lan-
guage. Here, the most frequent relation by far
574
is PART-WHOLE (32.14%), followed by LOCATION
(12.40%), THEME (9.23%) and OTHER-SR (7.74%).
It is interesting to note here that only 5.70% of the
TYPE relation instances in Europarl were unique.
This is in contrast with the other relations in both
corpora, where instances were mostly unique.
We also report here our observations on Lap-
ata & Keller?s unsupervised model. An analysis
of these results showed that the order of the con-
stituent nouns in the N P N paraphrase plays an im-
portant role. For example, a search for blood ves-
sels generated similar frequency counts for vessels
of blood and blood in vessels. About 30% noun -
noun paraphrasable pairs preserved the order in the
corresponding N P N paraphrases. We also manually
checked the first five entries generated by Google for
each most frequent prepositional paraphrase for 50
instances and noticed that about 35% of them were
wrong due to syntactic and/or semantic ambiguities.
Thus, since we wanted to measure the impact of
these ambiguities of noun compounds on the inter-
pretation performance, we further tested the prob-
abilistic web-based model on four distinct test sets
selected from Europarl, each containing 30 noun -
noun pairs encoding different types of ambiguity:
in set#1 the noun constituents had only one part of
speech and one WordNet sense; in set#2 the nouns
had at least two possible parts of speech and were
semantically unambiguous, in set#3 the nouns were
ambiguous only semantically, and in set#4 they were
ambiguous both syntactically and semantically. For
unambiguous noun-noun pairs (set#1), the model
obtained an accuracy of 35.01%, while for more se-
mantically ambiguous compounds it obtained an ac-
curacy of about 48.8%. This shows that for more
semantically ambiguous noun - noun pairs, the web-
based probabilistic model introduces a significant
number of false positives. Thus, the more abstract
the categories, the more noun compounds are cov-
ered, but also the more room for variation as to
which category a compound should be assigned.
6 Discussion and Conclusions
In this paper we presented a supervised, knowledge-
intensive interpretation model which takes advan-
tage of new linguistic information from English and
a list of five Romance languages. Our approach to
NP interpretation is novel in several ways. We de-
fined the problem in a cross-linguistic framework
and provided empirical observations on the distribu-
tion of the syntax and meaning of noun phrases on
two different corpora based on two state-of-the-art
classification tag sets.
As future work we consider the inclusion of other
features such as the semantic classes of Romance
nouns from aligned EuroWordNets, and other sen-
tence features. Since the results obtained can be seen
as an upper bound on NP interpretation due to per-
fect English - Romance NP alignment, we will ex-
periment with automatic translations generated for
the test data. Moreover, we like to extend the anal-
ysis to other set of languages whose structures are
very different from English and Romance.
References
T. W. Finin. 1980. The Semantic Interpretation of Compound
Nominals. Ph.D. thesis, University of Illinois at Urbana-
Champaign.
A. Giorgi and G. Longobardi. 1991. The syntax of noun
phrases. Cambridge University Press.
R. Girju, D. Moldovan, M. Tatu, and D. Antohe. 2005. On
the semantics of noun compounds. Computer Speech and
Language, 19(4):479?496.
R. Girju. 2007. Experiments with an annotation scheme for a
knowledge-rich noun phrase interpretation system. The Lin-
guistic Annotation Workshop at ACL, Prague.
Su Nam Kim and T. Baldwin. 2006. Interpreting semantic rela-
tions in noun compounds via verb semantics. COLING-ACL.
K. Kipper, H. Dong, and M. Palmer. 2000. Class-based con-
struction of a verb lexicon. AAAI Conference, Austin.
M. Lapata and F. Keller. 2004. The Web as a baseline: Eval-
uating the performance of unsupervised Web-based models
for a range of NLP tasks. HLT-NAACL.
M. Lauer. 1995. Corpus statistics meet the noun compound:
Some empirical results. ACL, Cambridge, Mass.
A. Meyers, R. Reeves, C. Macleod, R. Szekeley V. Zielinska,
and B. Young. 2004. The cross-breeding of dictionaries.
LREC-2004, Lisbon, Portugal.
R. Mihalcea and E. Faruque. 2004. Senselearner: Minimally
supervised word sense disambiguation for all words in open
text. ACL/SIGLEX Senseval-3, Barcelona, Spain.
D. Moldovan and A. Badulescu. 2005. A semantic scat-
tering model for the automatic interpretation of genitives.
HLT/EMNLP Conference, Vancouver, Canada.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Leveraging
generic patterns for automatically harvesting semantic rela-
tions. COLING/ACL, Sydney, Australia.
B. Rosario and M. Hearst. 2001. Classifying the semantic rela-
tions in noun compounds. EMNLP Conference.
R. Snow, D. Jurafsky, and A. Ng. 2006. Semantic taxonomy
induction from heterogenous evidence. COLING-ACL.
P. Turney. 2006. Expressing implicit semantic relations without
supervision. COLING/ACL, Sydney, Australia.
575
Proceedings of the Linguistic Annotation Workshop, pages 168?175,
Prague, June 2007. c?2007 Association for Computational Linguistics
Experiments with an Annotation Scheme for a Knowledge-rich Noun Phrase
Interpretation System
Roxana Girju
University of Illinois at Urbana-Champaign
girju@uiuc.edu
Abstract
This paper presents observations on our ex-
perience with an annotation scheme that was
used in the training of a state-of-the-art noun
phrase semantic interpretation system. The
system relies on cross-linguistic evidence
from a set of five Romance languages: Span-
ish, Italian, French, Portuguese, and Roma-
nian. Given a training set of English noun
phrases in context along with their transla-
tions in the five Romance languages, our
algorithm automatically learns a classifica-
tion function that is later on applied to un-
seen test instances for semantic interpreta-
tion. As training and test data we used two
text collections of different genre: Europarl
and CLUVI. The training data was annotated
with contextual features based on two state-
of-the-art classification tag sets.
1 Introduction
Linguistically annotated corpora are valuable re-
sources for both theoretical and computational lin-
guistics. They have played an important role in any
aspect of natural language processing research, from
supervised learning to evaluation, and have been
used in many applications such as Syntactic and Se-
mantic Parsing, Information Extraction, and Ques-
tion Answering.
A long-term research topic in linguistics, compu-
tational linguistics1, and artificial intelligence has
1In the past few years at many workshops, tutorials, and
competitions this research topic has received considerable inter-
been the semantic interpretation of noun phrases
(NPs). The basic problem is simple to define: given
a noun phrase constructed out of a pair of concepts
expressed by words or phrases, c1 ? c2, one rep-
resenting the head and the other the modifier, de-
termine the semantic relationship between the two
concepts. For example, a compound family estate
should be interpreted as the estate OWNED BY the
family; an NP such as dress of silk should be inter-
preted as denoting a dress MADE FROM silk. The
problem, while simple to state is hard to solve. The
reason is that the meaning of these constructions is
most of the time ambiguous or implicit.
Currently, the best-performing English NP inter-
pretation methods in computational linguistics fo-
cus mostly on two consecutive noun instances (noun
compounds) and are either (weakly) supervised,
knowledge-intensive (Rosario and Hearst, 2001),
(Rosario et al, 2002), (Moldovan et al, 2004),
(Pantel and Pennacchiotti, 2006), (Pennacchiotti and
Pantel, 2006), (Kim and Baldwin, 2006), (Snow et
al., 2006), (Girju et al, 2005; Girju et al, 2006),
or use statistical models on large collections of un-
labeled data (Berland and Charniak, 1999), (Lap-
ata and Keller, 2004), (Nakov and Hearst, 2005),
(Turney, 2006). Unlike unsupervised models, su-
pervised knowledge-rich approaches rely heavily on
large sets of annotated training data. For example,
we previously showed (Girju et al, 2006) that, for
est from the computational linguistics community: Workshop
on Multiword Expressions at COLING/ACL 2006, 2004, 2003;
Computational Lexical Semantics Workshop at ACL 2004; Tu-
torial on Knowledge Discovery from Text at ACL 2003; Shared
task on Semantic Role Labeling at CONLL 2005, 2004 and at
SENSEVAL 2005.
168
the task of automatic detection of part-whole rela-
tions, our system?s learning curve reached a plateau
at 74% F-measure when trained on approximatively
10,000 positive and negative examples.
Interpreting NPs correctly requires various types
of information from world knowledge to complex
context features. Since the training data needs to be
as accurate as possible, many of such features are
manually identified and annotated. Thus, the anno-
tation process is an important task that requires not
only considerable amount of time, but also experi-
ence with various annotation schemas and tools, and
a good understanding of the research topic. More-
over, the extension of the noun phrase interpretation
task to other natural languages brings forward new
annotation issues.
This paper presents observations on our experi-
ence with an annotation scheme that was used in the
training of a state-of-the-art noun phrase semantic
interpretation system (Girju, 2007). The system re-
lies on cross-linguistic evidence from a set of five
Romance languages: Spanish, Italian, French, Por-
tuguese, and Romanian. Given a training set of En-
glish noun phrases in context along with their trans-
lations in the five Romance languages, our algo-
rithm automatically learns a classification function
that is later on applied to unseen test instances for
semantic interpretation. As training and test data
we used two text collections of different genre: Eu-
roparl2 and CLUVI3. The training data was anno-
tated with contextual features based on two state-of-
the-art classification tag sets: Lauer?s set of 8 prepo-
sitions (Lauer, 1995) and our list of 22 semantic re-
lations. The system achieved an accuracy of 77.9%
(Europarl) and 74.31% (CLUVI).
The paper is organized as follows. Section 2
presents a summary of linguistic considerations of
noun phrases. In Section 3 we describe the list of se-
mantic interpretation categories used along with ob-
servations regarding their distribution on the two dif-
2http://www.isi.edu/koehn/europarl/
This corpus contains over 20 million words in eleven official
languages of the European Union covering the proceedings of
the European Parliament from 1996 to 2001.
3CLUVI - Linguistic Corpus of the University of Vigo Par-
allel Corpus 2.1 - http://sli.uvigo.es/CLUVI/. CLUVI is an open
text repository of parallel corpora of contemporary oral and
written texts in some of the Romance languages, such as Gali-
cian, French, Spanish, Portuguese, Basque parallel text collec-
tions.
ferent cross-lingual corpora. Section 4 presents the
data used along with observations on corpus annota-
tion and inter-annotator agreement. Finally, Section
5 offers some discussion and conclusions.
2 Linguistic considerations of noun
phrases
The automatic discovery of semantic relations must
start with a thorough understanding of the linguistic
aspects of the underlying relations. These consider-
ations are not only employed as features in the su-
pervised noun phrase interpretation model, but they
are also used in the annotation process.
Noun phrases can be compositional when their
meaning is derived from the meaning of the con-
stituent nouns (e.g., door knob ? PART-WHOLE,
kiss in the morning ? TEMPORAL), or idiosyn-
cratic, when the meaning is a matter of conven-
tion (e.g., soap opera, sea lion). NPs can also ex-
press metaphorical names (eg, ladyfinger), proper
names (e.g., John Doe), and binomial (dvandva)
compounds in which neither noun is the head (e.g.,
player-coach).
NPs can also be classified into synthetic (verbal)
and root (non-verbal) constructions. It is widely held
(Levi, 1978), (Selkirk, 1982) that the modifier noun
of a synthetic noun compound, for example, may be
associated with a theta-role of the verbal head. For
instance, in truck driver, the noun truck satisfies the
THEME relation associated with the direct object in
the corresponding argument structure of the verb to
drive.
Studied cross-linguistically, noun phrases can ex-
press variations from one language to another. For
example, English compounds of the form N1 N2
(e.g., wood stove) usually translate in Romance lan-
guages as N2 P N1 (e.g., four a? bois (French) ?
stove at/to wood). Romance languages have very
few N N compounds and they are of limited se-
mantic categories, such as TYPE (e.g., legge quadro
(Italian) ? framework law). Moreover, while En-
glish N N compounds are right-headed (e.g., frame-
work/modifier law/head), Romance compounds are
left-headed (e.g., legge/head quadro/modifier).
For this research we focus only on English?
Romance compositional noun phrases of the type
N N and N P N and disregard metaphorical and
169
proper names. In the following section we present
two different state-of-the-art classification sets used
in NP interpretation.
3 Lists of semantic classification relations
Although researchers (Downing, 1977), (Jespersen,
1954) argued that noun compounds, and NPs in gen-
eral, encode an infinite set of semantic relations,
many agree (Finin, 1980), (Levi, 1978) there is a
limited number of relations that occur with high fre-
quency in these constructions. However, the num-
ber and the level of abstraction of these frequently
used semantic categories are not agreed upon. They
can vary from a few prepositions (Lauer, 1995) to
hundreds and even thousands more specific seman-
tic relations (Finin, 1980). The more abstract the
categories, the more noun phrases are covered, but
also the more room for variation as to which cat-
egory a phrase should be assigned. Lauer (Lauer,
1995), for example, considers a set of eight prepo-
sitions as semantic classification categories that can
link the head and the modifier nouns in a noun com-
pound: of, for, with, in, on, at, about, and from.
However, according to this classification, the noun
compound love story, for instance, can be classified
both as story of love and story about love. The main
problem with these abstract categories is that much
of the meaning of individual compounds is lost, and
sometimes there is no way to decide whether a form
is derived from one category or another. On the
other hand, lists of very specific semantic relations
are difficult to build as they usually contain a very
large number of predicates, such as the list of all
possible verbs that can link the noun constituents.
Finin (Finin, 1980), for example, uses semantic cat-
egories such as ?dissolved in? to build interpreta-
tions of compounds such as ?salt water? and ?sugar
water?.
In this research we experiment with two sets of
semantic classification categories defined at differ-
ent abstraction levels. The first is a core set of 22 se-
mantic relations (22 SRs), set which was identified
by us from the linguistics literature and from vari-
ous experiments after many iterations over a period
of time (Moldovan and Girju, 2003)4. We proved
4There are also other lists of semantic relations used by the
research community (e.g., (Barker and Szpakowicz, 1998)), but
empirically that this set is encoded by noun ? noun
pairs in noun phrases and is a subset of our larger
list of 35 semantic relations. This list, presented
in Table 1 along with examples and semantic ar-
gument frames, is general enough to cover a large
majority of text semantics while keeping the seman-
tic relations to a manageable number. A semantic
argument frame is defined for each semantic rela-
tion and indicates the position of each semantic ar-
gument in the underlying relation. For example,
?Arg1 is part of (whole) Arg2? identifies the part
(Arg1) and the whole (Arg2) entities of this rela-
tion. This representation is important since it allows
to distinguish between different arrangements of the
arguments for given relation instances. For exam-
ple, most of the time, in N N compounds Arg1 pre-
cedes Arg2, while in N P N constructions the po-
sition is reversed (Arg2 P Arg1). However, this
is not always the case as shown by N N instances
such as ?ham/Arg1 sandwich/Arg2? and ?door/Arg2
knob/Arg1?. These argument frames were intro-
duced to provide consistent guide to the annotators
to easily test the goodness-of-fit of the relations.
The second set is Lauer?s list of 8 prepositions and
can be applied only to noun?noun compounds. We
selected these two state-of-the-art sets as they are
of different size and contain semantic classification
categories at different levels of abstraction. Lauer?s
list is more abstract and, thus capable of encoding a
large number of noun compound instances found in
a corpus, while our list contains finer grained seman-
tic categories. Details about the coverage of these
semantic lists on the two different corpora (Europarl
and CLUVI), how well they solve the interpretation
problem of noun phrases, and the mapping from one
list to another are provided in a companion paper
(Girju, 2007).
4 The data
For a better understanding of the semantic relations
encoded by N N and N P N instances, we analyzed
the semantic behavior of these constructions on a
large cross-linguistic corpora of examples. Our in-
tention is to answer questions such as:
(1) What syntactic constructions are used to
translate the English instances to the target Ro-
they overlap considerably with our list of 22-SR.
170
No. Semantic Default argument frame Examples
Relations
1 POSSESSION Arg1 POSSESSES Arg2 family#2/Arg1 estate#2/Arg2
2 KINSHIP Arg1 IS IN KINSHIP REL. WITH Arg2 the boy#1/Arg1?s sister#1/Arg2
3 PROPERTY Arg2 IS PROPERTY OF Arg1 lubricant#1/Arg1 viscosity#1/Arg2
4 AGENT Arg1 IS AGENT OF Arg2 investigation#2/Arg2 of the crew#2/Arg1
5 TEMPORAL Arg2 IS TEMPORAL LOCATION OF Arg1 morning#1/Arg2 news#3/Arg1
6 DEPICTION-DEPICTED Arg1 DEPICTS Arg2 a picture#1Arg1 of the nice#1/Arg2
7 PART-WHOLE Arg2 IS PART OF (whole) Arg1 faces#1/Arg2 of children#1/Arg1
8 HYPERNYMY (IS-A) Arg2 IS A Arg1 daisy#1/Arg2 flower#1/Arg1
9 CAUSE Arg1 CAUSES Arg2 scream#1/Arg2 of pain#1/Arg1
10 MAKE/PRODUCE Arg1 PRODUCES Arg2 chocolate#2/Arg2 factory#1/Arg1
11 INSTRUMENT Arg2 IS INSTRUMENT OF Arg1 laser#1/Arg2 treatment#1/Arg1
12 LOCATION Arg2 IS LOCATED IN Arg1 castle#1/Arg2 in the desert#1/Arg1
13 PURPOSE Arg2 IS PURPOSE OF Arg1 cough#1/Arg2 syrup#1/Arg1
14 SOURCE Arg2 IS SOURCE OF Arg1 grapefruit#2/Arg2 oil#3/Arg1
15 TOPIC Arg2 IS TOPIC OF Arg1 weather#1/Arg2 report#2/Arg2
16 MANNER Arg2 IS MANNER OF Arg1 performance#3/Arg1 with passion#1/Arg2
17 MEANS Arg2 IS MEANS OF Arg1 bus#1/Arg2 service#1/Arg1
18 EXPERIENCER Arg1 IS EXPERIENCER OF Arg2 the girl#1/Arg1?s fear#1/Arg2
19 MEASURE Arg2 IS MEASURE OF Arg1 cup#2/Arg2 of sugar#1/Arg1
20 RESEMBLANCE/TYPE Arg2 RESEMBLES OR IS A TYPE OF Arg1 framework#1/Arg1 law#2/Arg2
21 THEME Arg2 IS THEME OF Arg1 acquisition#1/Arg1 of stock#1/Arg2
22 BENEFICIARY Arg1 IS BENEFICIARY OF Arg2 reward#1/Arg2 for the finder#1/Arg1
OTHERS altar#1 boys#1
Table 1: The set of 22 semantic relations along with examples interpreted in context and the semantic
argument frame.
mance languages and vice-versa? (cross-linguistic
syntactic mapping),
(2) What semantic relations do these construc-
tions encode? (cross-linguistic semantic mapping),
(3) What is the corpus distribution of the seman-
tic relations per each syntactic construction?, and
finally
(4) What is the role of English and Romance
prepositions in the NP interpretation?
Thus, we collected the data from two text col-
lections with different distributions and of different
genre, Europarl and CLUVI.
The Europarl text collection
Europarl is a parallel corpora of over 20 million
words in eleven official languages of the Euro-
pean Union covering the proceedings of the Eu-
ropean Parliament from 1996 to 2001. The cor-
pus was assembled by combining four of the bilin-
gual sentence-aligned corpora made public as part
of the freely available Europarl corpus. Specifi-
cally, the Spanish-English, Italian-English, French-
English and Portuguese-English corpora were au-
tomatically aligned based on exact matches of En-
glish translations. Then, only those English sen-
tences which appeared verbatim in all four language
pairs were considered. The resulting English cor-
pus contained 10,000 sentences which were syntac-
tically parsed (Charniak, 2000). From these we ex-
tracted the first 3,000 NP instances (N N: 48.82%
and N P N: 51.18%).
The CLUVI text collection
CLUVI (Linguistic Corpus of the University of
Vigo) is an open text repository of parallel cor-
pora of contemporary oral and written languages,
resource that besides Galician also contains literary
text collections in other Romance languages. We fo-
cused only on the English-Portuguese and English-
Spanish literary parallel texts from the works of
John Steinbeck, H. G. Wells, J. Salinger, among
others. Using the CLUVI search interface we cre-
ated a sentence-aligned parallel corpus of 2,800
English-Spanish and English-Portuguese sentences.
The English versions were automatically parsed af-
ter which each N N and N P N instance thus iden-
tified was manually mapped to the corresponding
translations. The resulting corpus contains 2,200
English instances with a distribution of 26.77% N N
and 73.23% N P N.
171
4.1 Corpus annotation
For each corpus, each NP instance was presented
separately to two experienced annotators5 in a web
interface in context along with the English sentence
and its translations. Since the corpora do not cover
some of the languages (Romanian in Europarl and
CLUVI, and Italian and French in CLUVI), three
other native speakers of these languages and flu-
ent in English provided the translations which were
added to the list.
WordNet senses
The two computational semantics annotators had
to tag each English constituent noun with its cor-
responding WordNet sense6. If the word was not
found in WordNet the instance was not considered.
Tagging each noun constituent with the corre-
sponding WordNet sense in context is important not
only as a feature employed in the training models,
but also as guidance for the annotators to select the
right semantic relation. For instance, in the fol-
lowing sentences, daisy flower expresses a PART-
WHOLE relation in (1) and a IS-A relation in (2) de-
pending on the sense of the noun flower (cf. Word-
Net 2.1: flower#2 is a ?reproductive organ of an-
giosperm plants especially one having showy or col-
orful parts?, while flower#1 is ?a plant cultivated for
its blooms or blossoms?).
(1) ?Usually, more than one daisy#1 flower#2
grows on top of a single stem.?
(2) ?Try them with orange or yellow flowers of
red-hot poker, solidago or other late daisy#1
flowers#1, such as rudbeckias and heliopsis.?
In cases where noun senses were not enough for
relation selection, the annotators had to rely on a
larger context provided by the sentence and its trans-
lations as shown below.
Semantic argument frame
The annotators were also asked to identify the trans-
lation phrases, tag each instance with the corre-
sponding semantic relation, and identify the seman-
tic arguments Arg1 and Arg2 in the semantic argu-
ment frame of the corresponding relation.
5The annotators have extensive expertise in computational
semantics and are fluent in at least two of the Romance lan-
guages considered for this task.
6For the purpose of this research we used WordNet 2.1.
Thus, since the order of the semantic arguments
in an NP is not fixed (Girju et al, 2005), the an-
notators were presented with the semantic argu-
ment frame for each of the 22 semantic relations
and were asked to tag the NP instances accord-
ingly. For example, in PART-WHOLE instances such
as chair/Arg2 arm/Arg1 the part arm follows the
whole chair, while in button/Arg1 shirt/Arg2 the or-
der is reversed.
Translation instances
In the annotation process the annotators were asked
to identify and use, if necessary, the five correspond-
ing translations as additional information in select-
ing the semantic relation. Since only N N and N P N
noun phrase constructions were considered, the an-
notators had to discard those instances encoded by
different syntactic constructions in the Romance lan-
guages.
For instance, the context provided by the Europarl
English sentence in (3) below does not give enough
information for the disambiguation of the English
noun phrase ?judgment of the presidency? which
can mean either AGENT or THEME. The annotators
had to rely on the Romance translations in order to
identify the correct meaning in context (in this case
THEME): valoracio?n sobre la Presidencia (Es.), avis
sur la pre?sidence (Fr.), giudizio sulla Presidenza
(It.), veredicto sobre a Preside?ncia (Port.), evalu-
area Presendit?iei (Ro.)7.
(3)
En.: ?If you do , our final judgment of the
Spanish presidency will be even more
positive than it has been so far.?
Es.: ?Si se hace, nuestra valoracio?n sobre
la Presidencia espan?ola del Consejo sera?
au?n mucho ma?s positiva de lo que es hasta
ahora.?
Fr.: ?Si cela arrive, notre avis sur la
pre?sidence espagnole du Conseil sera
encore beaucoup plus positif que ce n?est
de?ja` le cas.?
It.: ?Se ci riuscira` il nostro giudizio sulla
Presidenza spagnola sara` ancora piu`
positivo di quanto non sia stato finora.?
7En. means English, Es. ? Spanish, Fr. ? French, It. ?
Italian, Port. ? Portuguese, and Ro. ? Romanian.
172
Port.: ?Se isso acontecer, o nosso veredicto
sobre a Preside?ncia espanhola sera? ainda
muito mais positivo do que o actual.?
Ro.: ?Daca? are loc, evaluarea Pres?edint?iei
spaniole va fi ??nca? mai pozitiva? deca?t
pa?na? acum.?
Semantic relations
Whenever the annotators found an example encod-
ing a semantic relation or a preposition paraphrase
other than those provided or they didn?t know what
interpretation to give, they had to tag it as OTHER-
SR and OTHER-PP, respectively . For example, in
the CLUVI sentences (4) and (5) below, the noun
phrases melody of the pearl and cry of death (the cry
announcing death) were tagged as OTHER-SR since
here the context of the sentences does not indicate
the association between the two nouns. Moreover,
noun compound instances such as the corner box
and knowledge searches were tagged as OTHER-PP
(box in the corner, searches after knowledge).
(3) LPE-284: ?And because the need was great
and the desire was great, the little secret
melody of the pearl that might be was
stronger this morning.? (En.)
(4) LPE-1582: ?And then Kino?s brain cleared
from its red concentration and he knew the
sound - the keening, moaning, rising hyster-
ical cry from the little cave in the side of the
stone mountain, the cry of death.? (En.)
Moreover, most of the time one instance was
tagged with one semantic relation, and respectively
preposition paraphrase, but there were also situa-
tions in which an example could belong to more
than one classification category in the same con-
text. For example, Texas city is tagged as PART-
WHOLE/PLACE-AREA, but also as a LOCATION re-
lation using the 22-SR classification category, and
respectively as of, from, in based on the 8-PP cat-
egory (e.g., city of Texas, city from Texas, and
city in Texas). Other instances, however, can en-
code a total of three semantic relations in a par-
ticular context. One such instance is cup#2 of
hot chocolate#1 in example (6) below, which was
tagged in CLUVI as MEASURE/OTHER(CONTENT-
CONTAINER)/LOC. Sense #2 of cup in WordNet
refers to ?the quantity the cup will hold? (cf. Word-
Net 2.1), thus mostly indicating a MEASURE rela-
tion.
(5) 557-AGU: ?Wouldn?t you like a cup of hot
chocolate before you go?? (En.)
However, since most hot beverages (such as tea,
coffee, and chocolate) are served in cups, it stands
to reason that the instance can be easily paraphrased
as a cup holding hold chocolate. Although our cur-
rent NP interpretation system (Girju, 2007) does
not differentiate between LOCATION and CONTENT-
CONTAINER (as other researchers (Tyler and Evans,
2003)8, we consider CONTENT-CONTAINER as a
special type of LOCATION), we capture them in our
annotation scheme.
Other examples of multiple annotations are
MEASURE/PART-WHOLE (e.g., an abundance of
buildings, a bunch of guys), Overall, 0.5% Europarl
and 6.9% CLUVI instances were tagged with more
than one semantic relation, and almost all noun com-
pound instances were tagged with more than one
preposition.
Thus, the annotated instances used in the cor-
pus analysis and system training phases have
the following format: <NPEn ;NPEs; NPIt;
NPFr; NPPort; NPRo; target>. The word tar-
get is one of the 23 (22 + OTHER) semantic
relations or one of the eight prepositions con-
sidered. For example, <judgment#2/Arg1 of
presidency#2/Arg2; valoracio?n sobre la Presiden-
cia; avis sur la pre?sidence; giudizio sulla Pres-
idenza; veredicto sobre a Preside?ncia; evaluarea
Pres?edint?iei; THEME>.
4.2 Inter-annotator agreement
The annotators? agreement was measured using
Kappa statistics, one of the most frequently used
measure of inter-annotator agreement for classifica-
tion tasks: K = Pr(A)?Pr(E)1?Pr(E) , where Pr(A) is the
proportion of times the annotators agree and Pr(E)
is the probability of agreement by chance. The K
coefficient is 1 if there is a total agreement among
the annotators, and 0 if there is no agreement other
than that expected to occur by chance.
8(Tyler and Evans, 2003) cite child language acquisition
studies which show there is a strong cognitive relationship be-
tween LOCATION and CONTENT-CONTAINER.
173
The Kappa values obtained on each corpus are
shown in Table 2. We also computed the number
of pairs that were tagged with OTHER by both an-
notators for each semantic relation and preposition
paraphrase, over the number of examples classified
in that category by at least one of the judges. For the
noun compound instances that encoded more than
one classification category, the agreement was done
on one of the relations only.
The agreement obtained for the Europarl corpus
is higher than the one for CLUVI on both classifica-
tion sets. This is partially explained by the distribu-
tion of semantic relations in both corpora. Overall,
the K coefficient shows a fair to good level of agree-
ment for the corpus data on the set of 22-SRs, tak-
ing into consideration the task difficulty. The level
of agreement for the prepositional paraphrases was
much higher. All these can be explained by the in-
structions the annotators received prior to the anno-
tation and by their expertise in lexical semantics.
Corpus Classification Kappa Agreement
tag sets N N N P N OTHER
Europarl 8-PP 0.80 N/A 91%
22-SR 0.61 0.67 78%
CLUVI 8-PP 0.77 N/A 86%
22-SR 0.56 0.58 69%
Table 2: The inter-annotator agreement on the NP annotation
on the two corpora. For the noun compound instances that en-
coded more than one semantic classification category, the agree-
ment was done on one of the relations only. ?N/A? means not
applicable.
13.05% of Europarl9 and 1.9% of CLUVI in-
stances that could not be tagged with Lauer?s prepo-
sitions were included in OTHER-PP category. About
99% of the Europarl N N instances encode TYPE re-
lations (e.g., framework law), while in CLUVI most
of them were TYPE (e.g., nightmare sensation), fol-
lowed by OTHER-SR (e.g., altar boys), and IS-A
(e.g., Winchester carbine).
From the initial corpus we considered those En-
glish instances that had all the translations encoded
by N N and N P N. Out of these, we selected only
1,023 Europarl and 1,008 CLUVI instances encoded
by N N and N P N in all languages considered and
resulted after agreement10. We split the corpora us-
9Only 5.70% of the TYPE instances in the Europarl corpus
were unique.
10The annotated corpora resulted in this research are avail-
able at http://apfel.ai.uiuc.edu.
ing a 8:2 training - test ratio and used it to train and
test our system. Details about the experiments and
the results obtained are presented in (Girju, 2007).
5 Discussion and conclusions
In this paper we presented some observations on our
experience with an annotation scheme that was used
in the training of a state-of-the-art noun phrase se-
mantic interpretation system. These observations
are defined in the framework of a larger project. This
project is to investigate various linguistic issues and
develop specific language models for the interpreta-
tion of noun phrase constructions in Germanic, Ro-
mance, and other classes of languages.
Our approach to NP interpretation, and thus an-
notation procedure, is novel in several ways. We
define the problem in a cross-linguistic framework
and provide empirical observations on various an-
notation issues based on a set of two different cor-
pora using two state-of-the-art classification tag sets:
Lauer?s prepositions and our list of 22 relations.
The linguistic implications are also important to
mention here. The annotation investigations done in
this research provide new insights into the research
topic at hand, the semantic interpretation of noun
phrases, in particular and the identification of se-
mantic relations between nominals (irrespective of
the syntactic constructions that link the two nouns),
in general. One such linguistic aspect is the impor-
tance of context for this task. Sometimes, the local
context of the noun phrase is not enough to disam-
biguate the underlying instances. For this, the anno-
tators need to relay on world and domain specific
knowledge and the entire context of the sentence,
or consider a larger context window (from a simple
paragraph including the sentence, to the discourse of
the text) as shown below in (6), (7), and (8). In (6)
and (7), for example, neither the context of the sen-
tence, nor the context of their paragraph provide the
meaning of the NPs. Many of the CLUVI instances
tagged as OTHER-SR (such as the music of the pearl
in (6)), are naming phrases ? they were defined only
once in the text collection and later on mentioned to
refer to the initial concept.
In (8), on the other hand, the meaning of the
NP the destruction of the Palestinian Authority is
THEME and not AGENT as might be considered by
default.
174
(6) LPE-390: ?And the music of the pearl
rose like a chorus of trumpets in his ears.?
(CLUVI)
(7) ?Mr President, the violent destruction of the
State of Israel.? (Europarl)
(8) ?The spread of the settlements, the seizing
of land, the curfews, the Palestinians im-
prisoned in their own villages, the summary
executions, the ambulances prevented from
reaching their destinations, the women giv-
ing birth at check points, the destruction of
the Palestinian Authority: these are not mis-
takes or accidents.? (Europarl)
6 Acknowledgments
We would like to thank all the people who helped
with the corpus creation and annotation, and those
with whom we had nice discussions about vari-
ous semantic relations. Without them this research
wouldn?t have been possible: Archna Bhatia, Gus-
tavo Cavallin, Brian Drexler, Matt Garley, Tania
Ionin, Matt Niemi, Dustin Parr, and Chris Struven.
And last, but not least we like to thank the reviewers
for their useful comments.
References
K. Barker and S. Szpakowicz. 1998. Semi-automatic recogni-
tion of noun modifier relationships. In the Proceedings of
the Association for Computational Linguistics / Conference
on Computational Linguistics.
M. Berland and E. Charniak. 1999. Finding Parts in Very Large
Corpora. In the Proceedings of the Association for Compu-
tational Linguistics (ACL), University of Maryland.
E. Charniak. 2000. A Maximum-entropy-inspired Parser. In
the Proceedings of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL), Seattle,
Washington.
P. Downing. 1977. On the Creation and Use of English Com-
pound Nouns. Language, 53(4):810?842.
T. W. Finin. 1980. The Semantic Interpretation of Compound
Nominals. Ph.D. thesis, University of Illinois at Urbana-
Champaign.
R. Girju, D. Moldovan, M. Tatu, and D. Antohe. 2005. On
the semantics of noun compounds. Computer Speech and
Language, 19(4):479?496.
R. Girju, A. Badulescu, and D. Moldovan. 2006. Automatic
discovery of part-whole relations. Computational Linguis-
tics, 32(1).
R. Girju. 2007. Improving the interpretation of noun phrases
with cross-linguistic information. In the Proceedings of the
Association for Computational Linguistics (ACL), Prague.
O. Jespersen. 1954. A Modern English Grammar on Historical
Principles. London.
S. N. Kim and T. Baldwin. 2006. In the Proceedings of the As-
sociation for Computational Linguistics, Sydney, Australia.
M. Lapata and F. Keller. 2004. The Web as a baseline: Evaluat-
ing the performance of unsupervised Web-based models for
a range of NLP tasks. In the Proceedings of the Human Lan-
guage Technology Conference / North American Chapter of
the Association of Computational Linguistics (HLT-NAACL).
M. Lauer. 1995. Corpus statistics meet the noun compound:
Some empirical results. In the Proceedings of Association
for Computational Linguistics (ACL), Cambridge, Mass.
J. Levi. 1978. The Syntax and Semantics of Complex Nominals.
Academic Press, New York.
D. Moldovan and R. Girju. 2003. Knowledge discovery from
text. In the Tutorial Proceedings of the Association for Com-
putational Linguistics (ACL), Sapporo, Japan.
D. Moldovan, A. Badulescu, M. Tatu, D. Antohe, and R. Girju.
2004. Models for the semantic classification of noun
phrases. In the Proceedings of the HLT/NAACL Workshop
on Computational Lexical Semantics, Boston, MA.
P. Nakov and M. Hearst. 2005. Search engine statistics be-
yond the n-gram: Application to noun compo und bracket-
ing. In the Proceedings of the Computational Natural Lan-
guage Learning Conference.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Leverag-
ing generic patterns for automatically harvesting semantic
relations. In the Proceedings of the International Confer-
ence for Computational Linguistics (COLING/ACL), Syd-
ney, Australia.
M. Pennacchiotti and P. Pantel. 2006. Ontologizing semantic
relations. In the Proceedings of Conference on Computa-
tional Linguistics / Association for Computational Linguis-
tics (COLING/ACL-06), Sydney, Australia. Association for
Computational Linguistics.
B. Rosario and M. Hearst. 2001. Classifying the semantic re-
lations in noun compounds. In the Proceedings of the 2001
EMNLP Conference.
B. Rosario, M. Hearst, and C. Fillmore. 2002. The descent of
hierarchy, and selection in relational semantics. In the Pro-
ceedings of the Association for Computational Linguistics.
E. Selkirk. 1982. Syntax of words. In Linguistic Inquiry Mono-
graph. MIT Press.
R. Snow, D. Jurafsky, and A. Ng. 2006. Semantic taxonomy
induction from heterogenous evidence. In the Proceedings
of the Conference on Computational Linguistics / Associa-
tion for Computational Linguistics (COLING-ACL), Sydney,
Australia.
P. Turney. 2006. Expressing implicit semantic relations with-
out supervision. In the Proceedings of the Conference on
Computational Linguistics / Association for Computational
Linguistics (COLING/ACL), Sydney, Australia.
A. Tyler and V. Evans. 2003. Spatial Experience, Lexical Struc-
ture and Motivation: The Case of In. In G. Radden and K.
Panther. Studies in Linguistic Motivation. Berlin and New
York: Mouton de Gruyter.
175
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 13?18,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 04:
Classification of Semantic Relations between Nominals
Roxana Girju
Univ. of Illinois
at Urbana-Champaign
Urbana, IL 61801
girju@uiuc.edu
Preslav Nakov
Univ. of California at Berkeley
Berkeley, CA 94720
nakov@cs.berkeley.edu
Vivi Nastase
EML Research gGmbH
Heidelberg, Germany 69118
nastase@eml-research.de
Stan Szpakowicz
University of Ottawa
Ottawa, ON K1N 6N5
szpak@site.uottawa.ca
Peter Turney
National Research Council of Canada
Ottawa, ON K1A 0R6
peter.turney@nrc-cnrc.gc.ca
Deniz Yuret
Koc? University
Istanbul, Turkey 34450
dyuret@ku.edu.tr
Abstract
The NLP community has shown a renewed
interest in deeper semantic analyses, among
them automatic recognition of relations be-
tween pairs of words in a text. We present an
evaluation task designed to provide a frame-
work for comparing different approaches to
classifying semantic relations between nom-
inals in a sentence. This is part of SemEval,
the 4th edition of the semantic evaluation
event previously known as SensEval. We de-
fine the task, describe the training/test data
and their creation, list the participating sys-
tems and discuss their results. There were
14 teams who submitted 15 systems.
1 Task Description and Related Work
The theme of Task 4 is the classification of semantic
relations between simple nominals (nouns or base
noun phrases) other than named entities ? honey
bee, for example, shows an instance of the Product-
Producer relation. The classification occurs in the
context of a sentence in a written English text. Al-
gorithms for classifying semantic relations can be
applied in information retrieval, information extrac-
tion, text summarization, question answering and so
on. The recognition of textual entailment (Tatu and
Moldovan, 2005) is an example of successful use of
this type of deeper analysis in high-end NLP appli-
cations.
The literature shows a wide variety of methods
of nominal relation classification. They depend as
much on the training data as on the domain of ap-
plication and the available resources. Rosario and
Hearst (2001) classify noun compounds from the
domain of medicine, using 13 classes that describe
the semantic relation between the head noun and
the modifier in a given noun compound. Rosario
et al (2002) classify noun compounds using the
MeSH hierarchy and a multi-level hierarchy of se-
mantic relations, with 15 classes at the top level.
Nastase and Szpakowicz (2003) present a two-level
hierarchy for classifying noun-modifier relations in
base noun phrases from general text, with 5 classes
at the top and 30 classes at the bottom; other re-
searchers (Turney and Littman, 2005; Turney, 2005;
Nastase et al, 2006) have used their class scheme
and data set. Moldovan et al (2004) propose a 35-
class scheme to classify relations in various phrases;
the same scheme has been applied to noun com-
pounds and other noun phrases (Girju et al, 2005).
Chklovski and Pantel (2004) introduce a 5-class set,
designed specifically for characterizing verb-verb
semantic relations. Stephens et al (2001) propose
17 classes targeted to relations between genes. La-
pata (2002) presents a binary classification of rela-
tions in nominalizations.
There is little consensus on the relation sets and
algorithms for analyzing semantic relations, and it
seems unlikely that any single scheme could work
for all applications. For example, the gene-gene re-
lation scheme of Stephens et al (2001), with rela-
tions like X phosphorylates Y, is unlikely to be trans-
ferred easily to general text.
We have created a benchmark data set to allow the
evaluation of different semantic relation classifica-
tion algorithms. We do not presume to propose a sin-
gle classification scheme, however alluring it would
13
Relation Training data Test data Agreement Example
positive set size positive set size (independent tagging)
Cause-Effect 52.1% 140 51.3% 80 86.1% laugh (cause) wrinkles (effect)
Instrument-Agency 50.7% 140 48.7% 78 69.6% laser (instrument) printer (agency)
Product-Producer 60.7% 140 66.7% 93 68.5% honey (product) bee (producer)
Origin-Entity 38.6% 140 44.4% 81 77.8% message (entity) from outer-space (origin)
Theme-Tool 41.4% 140 40.8% 71 47.8% news (theme) conference(tool)
Part-Whole 46.4% 140 36.1% 72 73.2% the door (part) of the car (whole)
Content-Container 46.4% 140 51.4% 74 69.1% the apples (content) in the basket (container)
Table 1: Data set statistics
be to try to design a unified standard ? it would be
likely to have shortcomings just as any of the others
we have just reviewed. Instead, we have decided to
focus on separate semantic relations that many re-
searchers list in their relation sets. We have built an-
notated data sets for seven such relations. Every data
set supports a separate binary classification task.
2 Building the Annotated Data Sets
Ours is a new evaluation task, so we began with data
set creation and annotation guidelines. The data set
that Nastase and Szpakowicz (2003) created had re-
lation labels and part-of-speech and WordNet sense
annotations, to facilitate classification. (Moldovan
et al, 2004; Girju et al, 2005) gave the annotators
an example of each phrase in a sentence along with
WordNet senses and position of arguments. Our
annotations include all these, to support a variety
of methods (since we work with relations between
nominals, the part of speech is always noun). We
have used WordNet 3.0 on the Web and sense index
tags.
We chose the following semantic relations:
Cause-Effect, Content-Container, Instrument-
Agency, Origin-Entity, Part-Whole, Product-
Producer and Theme-Tool. We wrote seven detailed
definitions, including restrictions and conventions,
plus prototypical positive and near-miss negative
examples. For each relation separately, we based
data collection on wild-card search patterns that
Google allows. We built the patterns manually,
following Hearst (1992) and Nakov and Hearst
(2006). Instances of the relation Content-Container,
for example, come up in response to queries such as
?* contains *?, ?* holds *?, ?the * in the *?. Fol-
lowing the model of the Senseval-3 English Lexical
Sample Task, we set out to collect 140 training and
at least 70 test examples per relation, so we had a
number of different patterns to ensure variety. We
also aimed to collect a balanced number of positive
and negative examples. The use of heuristic patterns
to search for both positive and negative examples
should naturally result in negative examples that
are near misses. We believe that near misses are
more useful for supervised learning than negative
examples that are generated randomly.
?Among the contents of the <e1>vessel</e1>
were a set of carpenter?s <e2>tools</e2>, sev-
eral large storage jars, ceramic utensils, ropes and
remnants of food, as well as a heavy load of ballast
stones.?
WordNet(e1) = ?vessel%1:06:00::?,
WordNet(e2) = ?tool%1:06:00::?,
Content-Container(e2, e1) = ?true?,
Query = ?contents of the * were a?
Figure 1: Annotations illustrated
Figure 1 illustrates the annotations. We tag the
nominals, so parsing or chunking is not necessary.
For Task 4, we define a nominal as a noun or base
noun phrase, excluding names entities. A base noun
phrase, e.g., lawn or lawn mower, is a noun with pre-
modifiers. We also exclude complex noun phrases
(e.g., with attached prepositional phrases ? the en-
gine of the lawn mower).
The procedure was the same for each relation.
One person gathered the sample sentences (aim-
ing approximately for a similar number of positive
and negative examples) and tagged the entities; two
other people annotated the sentences with WordNet
senses and classified the relations. The detailed re-
lation definitions and the preliminary discussions of
positive and negative examples served to maximize
the agreement between the annotators. They first
classified the data independently, then discussed ev-
ery disagreement and looked for consensus. Only
the agreed-upon examples went into the data sets.
Next, we split each data set into 140 training and
no fewer than 70 test examples. (We published the
training set for the Content-Container relation as de-
velopment data two months before the test set.) Ta-
ble 1 shows the number of positive and negative ex-
14
amples for each relation.1
The average inter-annotator agreement on rela-
tions (true/false) after the independent annotation
step was 70.3%, and the average agreement on
WordNet sense labels was 71.9%. In the process of
arriving at a consensus between annotators, the def-
inition of each relation was revised to cover explic-
itly cases where there had been disagreement. We
expect that these revised definitions would lead to
much higher levels of agreement than the original
definitions did.
3 The Participants
The task of classifying semantic relations between
nominals has attracted the participation of 14 teams
who submitted 15 systems. Table 4 lists the sys-
tems, the authors and their affiliations, and brief de-
scriptions. The systems? performance information
in terms of precision, recall, F -measure and accu-
racy, macroaveraged over all relations, appears in
Table 3. We computed these measures as described
in Lewis (1991).
We distinguish four categories of systems based
on the type of information used ? WordNet senses
and/or Google queries:
A ? WordNet = NO & Query = NO;
B ? WordNet = YES & Query = NO;
C ? WordNet = NO & Query = YES;
D ? WordNet = YES & Query = YES.
WordNet = ?YES? or WordNet = ?NO? tells us
only whether a system uses the WordNet sense la-
bels in the data sets. A system may use WordNet
internally for varied purposes, but ignore our sense
labels; such a system would be in category A or C .
Based on the input variation, each submitted system
may have up to 4 variations ? A,B,C,D.
Table 2 presents three baselines for a relation.
Majority always guesses either ?true? or ?false?,
whichever is the majority in the test set (maximizes
accuracy). Alltrue always guesses ?true? (maxi-
mizes recall). Probmatch randomly guesses ?true?
(?false?) with the probability matching the distribu-
tion of ?true? (?false?) in the test dataset (balances
precision and recall).
We present the results in Table 3 grouped by cat-
egory, to facilitate system comparison.
1As this paper serves also as a documentation of the data set,
the order of relations in the table is the same as in the data set.
Type P R F Acc
majority 81.3 42.9 30.8 57.0
alltrue 48.5 100.0 64.8 48.5
probmatch 48.5 48.5 48.5 51.7
Table 2: Baselines: precision, recall, F -measure and
accuracy averaged over the 7 binary classifications.
Team P R F Acc
A ? WordNet = NO & Query = NO
UCD-FC 66.1 66.7 64.8 66.0
ILK 60.5 69.5 63.8 63.5
UCB? 62.7 63.0 62.7 65.4
UMELB-B 61.5 55.7 57.8 62.7
UTH 56.1 57.1 55.9 58.8
UC3M 48.2 40.3 43.1 49.9
avg?stdev 59.2?6.3 58.7?10.5 58.0?8.1 61.1?6.0
B ? WordNet = YES & Query = NO
UIUC? 79.7 69.8 72.4 76.3
FBK-IRST 70.9 73.4 71.8 72.9
ILK 72.8 70.6 71.5 73.2
UCD-S1 69.9 64.6 66.8 71.4
UCD-PN 62.0 71.7 65.4 67.0
UC3M 66.7 62.8 64.3 67.2
CMU-AT 55.7 66.7 60.4 59.1
UCD-FC 66.4 58.1 60.3 63.6
UMELB-A 61.7 56.8 58.7 62.5
UVAVU 56.8 56.3 56.1 57.7
LCC-SRN 55.9 57.8 51.4 53.7
avg ? stdev 65.3?7.7 64.4?6.5 63.6?6.9 65.9?7.2
C ? WordNet = NO & Query = YES
UCB? 64.2 66.5 65.1 67.0
UCD-FC 66.1 66.7 64.8 66.0
UC3M 49.4 43.9 45.3 50.1
avg?stdev 59.9?9.1 59.0?13.1 58.4?11.3 61.0?9.5
D ? WordNet = YES & Query = YES
UTD-HLT-CG 67.3 65.3 62.6 67.2
UCD-FC 66.4 58.1 60.3 63.6
UC3M 60.9 57.8 58.8 62.3
avg?stdev 64.9?3.5 60.4?4.2 60.6?1.9 64.4?2.5
Systems tagged with ? have a Task 4 organizer as part of the team.
Table 3: System performance grouped by category.
Precision, recall, F -measure and accuracy macro-
averaged over each system?s performance on all 7
relations.
4 Discussion
The highest average accuracy on Task 4 was 76.3%.
Therefore, the average initial agreement between an-
notators (70.3%), before revising the definitions, is
not an upper bound on the accuracy that can be
achieved. That the initial agreement between anno-
tators is not a good indicator of the accuracy that can
be achieved is also supported by the low correlation
15
System Institution Team Description System Type
UVAVU Univ. of Amsterdam
TNO Science & Industry
Free Univ. Amsterdam
Sophia Katrenko
Willem Robert van
Hage
similarity measures in WordNet; syn-
tactic dependencies; lexical patterns;
logical combination of attributes
B
CMU -AT Carnegie Mellon Univ. Alicia Tribble
Scott E. Fahlman
WordNet; manually-built ontologies;
Scone Knowledge Representation Lan-
guage; semantic distance
B
ILK Tilburg University Caroline Sporleder
Roser Morante
Antal van den Bosch
semantic clusters based on noun simi-
larity; WordNet supersenses; grammat-
ical relation between entities; head of
sentence; WEKA
A, B
FBK-IRST Fondazione Bruno
Kessler - IRST
Claudio Giuliano
Alberto Lavelli
Daniele Pighin
Lorenza Romano
shallow and deep syntactic information;
WordNet synsets and hypernyms; ker-
nel methods; SVM
B
LCC-SRN Language Computer
Corp.
Adriana Badulescu named entity recognition; lexical, se-
mantic, syntactic features; decision tree
and semantic scattering
B
UMELB-A Univ. of Melbourne Su Kim
Timothy Baldwin
sense collocations; similarity of con-
stituents; extending training and testing
data using similar words
B
UMELB-B Univ. of Melbourne Su Kim
Timothy Baldwin
similarity of nearest-neighbor matching
over the union of senses for the two
nominals; cascaded tagging with de-
creasing thresholds
A
UCB? Univ. of California at
Berkeley
Preslav Nakov
Marti Hearst
VSM; joining terms; KNN-1 A, C
UC3M Univ. Carlos III of Madrid Isabel Segura Bedmar
Doaa Sammy
Jose? Luis Mart??nez
Ferna?ndez
WordNet path; syntactic features; SVM A, B, C, D
UCD-S1 Univ. College Dublin Cristina Butnariu
Tony Veale
lexical-semantic categories from Word-
Net; syntactic patterns from corpora,
SVM
B
UCD-FC Univ. College Dublin Fintan Costello WordNet; additional noun compounds
tagged corpus; Naive Bayes
A, B, C, D
UCD-PN Univ. College Dublin Paul Nulty WordNet supersenses; web-based fre-
quency counts for specific joining
terms; WEKA (SMO)
B
UIUC? Univ. of Illinois at Urbana
Champaign
Roxana Girju
Brandon Beamer
Suma Bhat
Brant Chee
Andrew Fister
Alla Rozovskaya
features based on WordNet, NomLex-
PLUS, grammatical roles, lexico-
syntactic patterns, semantic parses
B
UTD-HLT-CG Univ. of Texas at Dallas Cristina Nicolae
Garbiel Nicolae
Sanda Harabagiu
lexico-semantic features from Word-
Net, VerbNet; semantic features from a
PropBank parser; dependency features
D
UTH Univ. of Tokio Eiji Aramaki
Takeshi Imai
Kengo Miyo
Kazuhiko Ohe
joining phrases; physical size for enti-
ties; web-mining; SVM
A
Systems tagged with ? have a Task 4 organizer as part of the team.
Table 4: Short description of the teams and the participating systems.
16
Relation Team Type P R F Acc Test size Base-F Base-Acc Avg. rank
Cause-Effect UIUC B4 69.5 100.0 82.0 77.5 80 67.8 51.2 3.4
Instrument-Agency FBK-IRST B4 76.9 78.9 77.9 78.2 78 65.5 51.3 3.4
Product-Producer UCD-S1 B4 80.6 87.1 83.7 77.4 93 80.0 66.7 1.7
Origin-Entity ILK B3 70.6 66.7 68.6 72.8 81 61.5 55.6 6.0
Theme-Tool ILK B4 69.0 69.0 69.0 74.6 71 58.0 59.2 6.0
Part-Whole UC3M B4 72.4 80.8 76.4 81.9 72 53.1 63.9 4.5
Content-Container UIUC B4 93.1 71.1 80.6 82.4 74 67.9 51.4 3.1
Table 5: The best results per relation. Precision, recall, F -measure and accuracy macro-averaged over each
system?s performance on all 7 relations. Base-F shows the baseline F -measure (alltrue), Base-Acc ? the
baseline accuracy score (majority). The last column shows the average rank for each relation.
of 0.15 between the Acc column in Table 5 and the
Agreement column in Table 1.
We performed various analyses of the results,
which we summarize here in four questions. We
write Xi to refer to four possible system categories
(Ai, Bi, Ci, and Di) with four possible amounts of
training data (X1 for training examples 1 to 35, X2
for 1 to 70, X3 for 1 to 105, and X4 for 1 to 140).
Does more training data help?
Overall, the results suggest that more training data
improves the performance. There were 17 cases in
which we had results for all four possible amounts
of training data. All average F -measure differences,
F (X4)?F (Xi) where X = A to D, i = 1 to 3, for
these 17 sets of results are statistically significant:
F (X4)?F (X1): N = 17, avg = 8.3, std = 5.8, min =
1.1, max = 19.6, t-value = ?5.9, p-value = 0.00001.
F (X4)?F (X2): N = 17, avg = 4.0, std = 3.7, min =
?3.5, max = 10.5, t-value = 4.5, p-value = 0.0002.
F (X4)?F (X3): N = 17, avg = 0.9, std = 1.7, min =
?2.6, max = 4.7, t-value = 2.1, p-value = 0.03.
Does WordNet help?
The statistics show that WordNet is important, al-
though the contribution varies across systems. Three
teams submitted altogether 12 results both for A1?
A4 and B1?B4. The average F -measure difference,
F (Bi)?F (Ai), i = 1 to 4, is significant:
F (Bi)?F (Ai): N = 12, avg = 6.1, std = 8.4, min =
?4.5, max = 21.2, t-value = ?2.5, p-value = 0.01.
The results of the UCD-FC system actually went
down when WordNet was used. The statistics for the
remaining two teams, however, are a bit better:
F (Bi)?F (Ai): N = 8, avg = 10.4, std = 6.7, min =
?1.0, max = 21.2, t-value = ?4.4, p-value = 0.002.
Does knowing the query help?
Overall, knowing the query did not seem to improve
the results. Three teams submitted 12 results both
for A1?A4 and C1?C4. The average F -measure dif-
ference, F (Ci)?F (Ai) , i = 1 to 4, is not significant:
F (Ci)?F (Ai): N = 12, avg = 0.9, std = 1.8, min =
?2.0, max = 5.0, t-value = ?1.6, p-value = 0.06.
Again, the UCD-FC system differed from the
other systems in that the A and C scores were iden-
tical, but even averaging over the remaining two sys-
tems and 8 cases does not show a statistically signif-
icant advantage:
F (Ci)?F (Ai): N = 8, avg = 1.3, std = 2.2, min =
?2.0, max = 5.0, t-value = ?1.7, p-value = 0.07.
Are some relations harder to classify?
Table 5 shows the best results for each relation in
terms of precision, recall, and F -measure, per team
and system category. Column Base-F presents the
baseline F -measure (alltrue), while Base-Acc the
baseline accuracy score (majority). For all seven re-
lations, the best team significantly outperforms the
baseline. The category of the best-scoring system
in almost every case is B4 (only the ILK B4 system
scored second on the Origin-Entity relation).
Table 5 suggests that some relations are more dif-
ficult to classify than others. The best F -measure
ranges from 83.7 for Product?Producer to 68.6 for
Origin?Entity. The difference between the best F -
measure and the baseline F -measure ranges from
23.3 for Part-Whole to 3.7 for Product-Producer.
The difference between the best accuracy and the
baseline accuracy ranges from 31.0 for Content-
Container to 10.7 for Product-Producer.
The F column shows the best result for each rela-
tion, but similar differences among the relations may
be observed when all results are pooled. The Avg.
rank column computes the average rank of each re-
lation in the ordered list of relations generated by
each system. For example, Product?Producer is of-
ten listed as the first or the second easiest relation
(with an average rank of 1.7), while Origin?Entity
and Theme?Tool are identified as the most difficult
17
relations to classify (with average ranks of 6.0).
5 Conclusion
This paper describes a new semantic evaluation task,
Classification of Semantic Relations between Nom-
inals. We have accomplished our goal of providing
a framework and a benchmark data set to allow for
comparisons of methods for this task. The data in-
cluded different types of information ? lexical se-
mantic information, context, query used ? meant to
facilitate the analysis of useful sources of informa-
tion for determining the semantic relation between
nominals. The results that the participating systems
have reported show successful approaches to this
difficult task, and the advantages of using lexical se-
mantic information.
The success of the task ? measured in the inter-
est of the community and the results of the partici-
pating systems ? shows that the framework and the
data are useful resources. By making this collection
freely accessible, we encourage further research into
this domain and integration of semantic relation al-
gorithms in high-end applications.
Acknowledgments
We thank Eneko Agirre, Llu??s Ma`rquez and Richard
Wicentowski, the organizers of SemEval 2007, for
their guidance and prompt support in all organiza-
tional matters. We thank Marti Hearst for valu-
able advice throughout the task description and de-
bates on semantic relation definitions. We thank the
anonymous reviewers for their helpful comments.
References
T. Chklovski and P. Pantel. 2004. Verbocean: Mining the
web for fine-grained semantic verb relations. In Proc.
Conf. on Empirical Methods in Natural Language Pro-
cessing, EMNLP-04, pages 33?40, Barcelona, Spain.
R. Girju, D. Moldovan, M. Tatu, and D. Antohe. 2005.
On the semantics of noun compounds. Computer
Speech and Language, 19:479?496.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. 14th International
Conf. on Computational Linguistics (COLING-92),
pages 539?545.
M. Lapata. 2002. The disambiguation of nominaliza-
tions. Computational Linguistics, 28(3):357?388.
D.D. Lewis. 1991. Evaluating text categorization.
In Proceedings of the Speech and Natural Language
Workshop, pages 312?318, Asilomar.
D. Moldovan, A. Badulescu, M. Tatu, D. Antohe, and
R. Girju. 2004. Models for the semantic classification
of noun phrases. In Proc. Computational Lexical Se-
mantics Workshop at HLT-NAACL 2004, pages 60?67,
Boston, MA.
P. Nakov and M. Hearst. 2006. Using verbs to char-
acterize noun-noun relations. In Proc. Twelfth Inter-
national Conf. in Artificial Intelligence (AIMSA-06),
pages 233?244, Varna,Bulgaria.
V. Nastase and S. Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5), pages 285?301, Tilburg, The Netherlands.
V. Nastase, J. Sayyad-Shirabad, M. Sokolova, and S. Sz-
pakowicz. 2006. Learning noun-modifier semantic
relations with corpus-based and WordNet-based fea-
tures. In Proc. 21st National Conf. on Artificial Intel-
ligence (AAAI 2006), pages 781?787, Boston, MA.
B. Rosario and M. Hearst. 2001. Classifying the seman-
tic relations in noun-compounds via domain-specific
lexical hierarchy. In Proc. 2001 Conf. on Empirical
Methods in Natural Language Processing (EMNLP-
01), pages 82?90.
B. Rosario, M. Hearst, and C. Fillmore. 2002. The de-
scent of hierarchy, and selection in relational seman-
tics. In Proc. 40th Annual Meeting of the Association
for Computational Linguistics (ACL-02), pages 417?
424, Philadelphia, PA.
M. Stephens, M. Palakal, S. Mukhopadhyay, and R. Raje.
2001. Detecting gene relations from MEDLINE ab-
stracts. In Proc. Sixth Annual Pacific Symposium on
Biocomputing, pages 483?496.
M. Tatu and D. Moldovan. 2005. A semantic approach to
recognizing textual entailment. In Proc. Human Lan-
guage Technology Conf. and Conf. on Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP
2005), pages 371?378, Vancouver, Canada.
P.D. Turney and M.L. Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning, 60(1-3):251?278.
P.D. Turney. 2005. Measuring semantic similarity by
latent relational analysis. In Proc. Nineteenth Interna-
tional Joint Conf. on Artificial Intelligence (IJCAI-05),
pages 1136?1141, Edinburgh, Scotland.
18
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 386?389,
Prague, June 2007. c?2007 Association for Computational Linguistics
UIUC: A Knowledge-rich Approach to Identifying Semantic Relations
between Nominals
Brandon Beamer,1,4 Suma Bhat,2,4 Brant Chee,3,4 Andrew Fister,1,4 Alla Rozovskaya,1,4
Roxana Girju1,4
Department of Linguistics1,
Department of Electrical and Computer Engineering2,
Department of Library and Information Science3,
Beckman Institute4,
University of Illinois at Urbana-Champaign
{bbeamer, spbhat2, chee, afister2, rozovska, girju}@uiuc.edu
Abstract
This paper describes a supervised,
knowledge-intensive approach to the auto-
matic identification of semantic relations
between nominals in English sentences.
The system employs different sets of new
and previously used lexical, syntactic, and
semantic features extracted from various
knowledge sources. At SemEval 2007 the
system achieved an F-measure of 72.4% and
an accuracy of 76.3%.
1 Introduction
The SemEval 2007 task on Semantic Relations be-
tween Nominals is to identify the underlying se-
mantic relation between two nouns in the context
of a sentence. The dataset provided consists of a
definition file and 140 training and about 70 test
sentences for each of the seven relations consid-
ered: Cause-Effect, Instrument-Agency, Product-
Producer, Origin-Entity, Theme-Tool, Part-Whole,
and Content-Container. The task is defined as a
binary classification problem. Thus, given a pair
of nouns and their sentential context, the classifier
decides whether the nouns are linked by the target
semantic relation. In each training and test exam-
ple sentence, the nouns are identified and manu-
ally labeled with their corresponding WordNet 3.0
senses. Moreover, each example is accompanied by
the heuristic pattern (query) the annotators used to
extract the sentence from the web and the position
of the arguments in the relation.
(1) 041 ?He derives great joy and <e1>happiness</e1>
from <e2>cycling</e2>.? WordNet(e1) =
?happiness%1:12:00::?, WordNet(e2) = ?cy-
cling%1:04:00::?, Cause-Effect(e2,e1) = ?true?,
Query = ?happiness from *?
Based on the information employed, systems can
be classified in four types of classes: (A) systems
that use neither the given WordNet synsets nor the
queries, (B) systems that use only WordNet senses,
(C) systems that use only the queries, and (D) sys-
tems that use both.
In this paper we present a type-B system that re-
lies on various sets of new and previously used lin-
guistic features employed in a supervised learning
model.
2 Classification of Semantic Relations
Semantic relations between nominals can be en-
coded by different syntactic constructions. We
extend here over previous work that has focused
mainly on noun compounds and other noun phrases,
and noun?verb?noun constructions.
We selected a list of 18 lexico-syntactic and se-
mantic features split here into three sets: feature set
#1 (core features), feature set #2 (context features),
and the feature set #3 (special features). Table 1
shows all three sets of features along with their defi-
nitions; a detailed description is presented next. For
some features, we list previous works where they
proved useful. While features F1 ? F4 were selected
from our previous experiments, all the other features
are entirely the contribution of this research.
Feature set #1: Core features
This set contains six features that were employed
in all seven relation classifiers. The features take
into consideration only lexico-semantic information
386
No. Feature Definition
Feature Set #1: Core features
F1 Argument position indicates the position of the arguments in the semantic relation
(Girju et al, 2005; Girju et al, 2006) (e.g., Part-Whole(e1, e2), where e1 is the part and e2 is the whole).
F2 Semantic specialization this is the prediction returned by the automatic WordNet IS-A semantic
(Girju et al, 2005; Girju et al, 2006) specialization procedure.
F3, F4 Nominalization indicates whether the nouns e1 (F3) and e2 (F4) are nominalizations
(Girju et al, 2004) or not. Specifically, we distinguish here between agential nouns,
other nominalizations, and neither.
F5, F6 Spatio-Temporal features indicate if e1 (F5) or e2 (F6) encode time or location.
Feature Set #2: Context features
F7, F8 Grammatical role describes the grammatical role of e1 (F7) and e2 (F8). There are three
possible values: subject, direct object, or neither.
F9 PP Attachment applies to NP PP constructions and indicates if the prepositional phrase
containing e2 attaches to the NP containing e1.
F10, F11 Semantic Role is concerned with the semantic role of the phrase containing
either e1 (F10) or e2 (F11). In particular, we focused on three semantic
roles: Time, Location, Manner. The feature is set to 1 if the target noun
is part of a phrase of that type and to 0 otherwise.
F12, F13, Inter-noun context sequence is a set of three features. F12 captures the sequence of stemmed
F14 words between e1 and e2, while F13 lists the part of speech sequence in
between the target nouns. F14 is a scoring weight (with possible values
1, 0.5, 0.25, and 0.125) which measures the similarity of an unseen
sequence to the set of sequence patterns associated with a relation.
Feature Set #3: Special features
F15, F16 Psychological feature is used in the Theme-Tool classifier; indicates if e1 (F15) or e2 (F16)
belong or not to a predefined set of psychological features.
F17 Instrument semantic role is used for the Instrument-Agency relation and indicates whether
the phrase containing e1 is labeled as em Instrument or not.
F18 Syntactic attachment is used for the Instrument-Agent relation and indicates whether the phrase
containing the Instrument role attaches to a noun or a verb
Table 1: The three sets of features used for the automatic semantic relation classification.
about the two target nouns.
Argument position (F1) indicates the position of
the semantic arguments in the relation. This infor-
mation is very valuable, since some relations have a
particular argument arrangement depending on the
lexico-syntactic construction in which they occur.
For example, most of the noun compounds encod-
ing Stuff-Object / Part-Whole relations have e1 as
the part and e2 as the whole (e.g., silk dress).
Semantic specialization (F2) is a binary feature
representing the prediction of a semantic specializa-
tion learning model. The method consists of a set
of iterative procedures of specialization of the train-
ing examples on the WordNet IS-A hierarchy. Thus,
after all the initial noun?noun pairs are mapped
through generalization to entity ? entity pairs in
WordNet, a set of necessary specialization iterations
is applied until it finds a boundary that separates pos-
itive and negative examples. This boundary is tested
on new examples for relation prediction.
The nominalization features (F3, F4) indicate if
the target noun is a nominalization and, if yes, of
what type. We distinguish here between agential
nouns, other nominalizations, and neither. The
features were identified based on WordNet and
NomLex-Plus1 and were introduced to filter some
of negative examples, such as car owner/THEME.
Spatio?Temporal features (F5, F6) were also in-
troduced to recognize some near miss examples,
such as Temporal and Location relations. For in-
stance, activation by summer (near-miss for Cause-
Effect) and mouse in the field (near-miss for Content-
Container). Similarly, for Theme-Tool, a word act-
ing as a Theme should not indicate a period of time,
as in <e1>the appointment</e1> was for more
than one <e2>year</e2>. For this we used the in-
formation provided by WordNet and special classes
generated from the works of (Herskovits, 1987),
(Linstromberg, 1997), and (Tyler and Evans, 2003).
1NomLex-Plus is a hand-coded database of 5,000 verb nom-
inalizations, de-adjectival, and de-adverbial nouns.
http://nlp.cs.nyu.edu/nomlex/index.html
387
Feature set #2: Context features
This set takes advantage of the sentence context to
identify features at different linguistic levels.
The grammatical role features (F7, F8) determine
if e1 or e2 is the subject, direct object, or neither.
This feature helps filter out some instances with poor
context, such as noun compounds and identify some
near-miss examples. For example, a restriction im-
posed by the definition of Theme-Tool indicates that
in constructions such as Y/Tool is used for V-ing
X/Theme, neither X nor Y can be the subject of
the sentence, and hence Theme-Tool(X, Y) would be
false. This restriction is also captured by the nomi-
nalization feature in case X or Y is an agential noun.
PP attachment (F9) is defined for NP PP construc-
tions, where the prepositional phrase containing the
noun e2 attaches or not to the NP (containing e1).
The rationale is to identify negative instances where
the PP attaches to any other word before NP in the
sentence. For example, eat <e1>pizza</e1> with
<e2>a fork</e2>, where with a fork attaches to
the verb to eat (cf. (Charniak, 2000)).
Furthermore, we implemented and used two se-
mantic role features which identify the semantic role
of the phrase in a verb?argument structure, phrase
containing either e1 (F10) or e2 (F11). In particular,
we focus on three semantic roles: Time, Location,
Manner. The feature is set to 1 if the target noun
is part of a semantic role phrase and to 0 otherwise.
The idea is to filter out near-miss examples, expe-
cially for the Instrument-Agency relation. For this,
we used ASSERT, a semantic role labeler developed
at the University of Colorado at Boulder2 which was
queried through a web interface.
Inter-noun context sequence features (F12, F13)
encode the sequence of lexical and part of speech
information between the two target nouns. Feature
F14 is a weight feature on the values of F12 and
F13 and indicates how similar a new sequence is to
the already observed inter-noun context associated
with the relation. If there is a direct match, then the
weight is set to 1. If the part-of-speech pattern of the
new substring matches that of an already seen sub-
string, then the weight is set to 0.5. Weights 0.25
and 0.125 are given to those sequences that overlap
entirely or partially with patterns encoding other se-
2http://oak.colorado.edu/assert/
mantic relations in the same contingency set (e.g.,
semantic relations that share syntactic pattern se-
quences). The value of the feature is the summation
of the weights thus obtained. The rationale is that
the greater the weight, the more representative is the
context sequence for that relation.
Feature set #3: Special features
This set includes features that help identify specific
information about some semantic relations.
Psychological feature was defined for the Theme-
Tool relation and indicates if the target noun (F15,
F16) belongs to a list of special concepts. This fea-
ture was obtained from the restrictions listed in the
definition of Theme-Tool. In the example need for
money, the noun need is a psychological feature, and
thus the instance cannot encode a Theme-Tool rela-
tion. A list of synsets from WordNet subhierarchy
of motivation and cognition constituted the psycho-
logical factors. This was augmented with precondi-
tions such as foundation and requirement since they
would not be allowed as tools for the theme.
The Instrument semantic role is used for the
Instrument-Agency relation as a boolean feature
(F17) indicating whether the argument identified as
Instrument in the relation (e.g., e1 if Instrument-
Agency(e1, e2)) belongs to an instrument phrase as
identified by a semantic role tool, such as ASSERT.
The syntactic attachment feature (F18) is a fea-
ture that indicates whether the argument identified
as Instrument in the relation attaches to a verb or to
a noun in the syntactically parsed sentence.
3 Learning Model and Experimental
Setting
For our experiments we chose libSVM, an open
source SVM package3. Since some of our features
are nominal, we followed the standard practice of
representing a nominal feature with n discrete val-
ues as n binary features. We used the RBF kernel.
We built a binary classifier for each of the seven
relations. Since the size of the task training data per
relation is small, we expanded it with new examples
from various sources. We added a new corpus of
3,000 sentences of news articles from the TREC-9
text collection (Girju, 2003) encoding Cause-Effect
(1,320) and Product-Producer (721). Another col-
3http://www.csie.ntu.edu.tw/?cjlin/libsvm/
388
Relation P R F Acc Total Base-F Base-Acc Best features
Cause-Effect 69.5 100.0 82.0 77.5 80 67.8 51.2 F1, F2, F5, F6, F12?F14
Instrument-Agency 68.2 78.9 73.2 71.8 78 65.5 51.3 F7, F8, F10, F11, F15?F18
Product-Producer 84.5 79.0 81.7 76.3 93 80.0 66.7 F1?F4, F12?F14
Origin-Entity 86.4 52.8 65.5 75.3 81 61.5 55.6 F1, F2, F5, F6, F12?F14
Theme-Tool 85.7 41.4 55.8 73.2 71 58.0 59.2 F1?F6, F15, F16
Part-Whole 70.8 65.4 68.0 77.8 72 53.1 63.9 F1?F4
Content-Container 93.1 71.1 80.6 82.4 74 67.9 51.4 F1?F6, F12?F14
Average 79.7 69.8 72.4 76.3 78.4
Table 2: Performance obtained per relation. Precision, Recall, F-measure, Accuracy, and Total (number of examples) are macro-
averaged for system?s performance on all 7 relations. Base-F shows the baseline F measure (all true), while Base-Acc shows the
baseline accuracy score (majority).
lection of 3,129 sentences from Wall Street Journal
(Moldovan et al, 2004; Girju et al, 2004) was con-
sidered for Part-Whole (1,003), Origin-Entity (167),
Product-Producer (112), and Theme-Tool (91). We
also extracted 552 Product-Producer instances from
eXtended WordNet4 (noun entries and their gloss
definition). Moreover, for Theme-Tool and Content-
Container we used special lists of constraints5. Be-
sides the selectional restrictions imposed on the
nouns by special features such as F15 and F16 (psy-
chological feature), we created lists of containers
from various thesauri6 and identified selectional re-
strictions that differentiate between containers and
locations relying on taxonomies of spatial entities
discussed in detail in (Herskovits, 1987) and (Tyler
and Evans, 2003).
Each instance in this text collection had the tar-
get nouns identified and annotated with WordNet
senses. Since the annotations used different Word-
Net versions, senses were mapped to sense keys.
4 Experimental Results
Table 2 shows the performance of our system for
each semantic relation. Base-F indicates the base-
line F-measure (all true), while Base-Acc shows the
baseline accuracy score (majority). The Average
score of precision, recall, F-measure, and accuracy
is macroaveraged over all seven relations. Overall,
all features contributed to the performance, with a
different contribution per relation (cf. Table 2).
5 Conclusions
This paper describes a method for the automatic
identification of a set of seven semantic relations
4http://xwn.hlt.utdallas.edu/
5The Instrument-Agency classifier was trained only on the
task dataset.
6Thesauri such as TheFreeDictionary.com.
based on support vector machines (SVMs). The ap-
proach benefits from an extended dataset on which
binary classifiers were trained for each relation. The
feature sets fed into the SVMs produced very good
results.
Acknowledgments
We would like to thank Brian Drexler for his valu-
able suggestions on the set of semantic relations.
References
E. Charniak. 2000. A Maximum-entropy-inspired Parser. In
the Proceedings of the 1st NAACL Conference.
R. Girju, A. Giuglea, M. Olteanu, O. Fortu, O. Bolohan, and
D. Moldovan. 2004. Support vector machines applied to
the classification of semantic relations in nominalized noun
phrases. In the Proceedings of the HLT/NAACL Workshop
on Computational Lexical Semantics.
R. Girju, D. Moldovan, M. Tatu, and D. Antohe. 2005. On
the semantics of noun compounds. Computer Speech and
Language, 19(4):479?496.
R. Girju, A. Badulescu, and D. Moldovan. 2006. Automatic
discovery of part-whole relations. Computational Linguis-
tics, 32(1).
R. Girju. 2003. Automatic detection of causal relations for
question answering. In the Proceedings of the ACL Work-
shop on ?Multilingual Summarization and Question Answer-
ing - Machine Learning and Beyond?.
A. Herskovits. 1987. Language and spatial cognition: An in-
terdisciplinary study of the prepositions in English. Cam-
bridge University Press.
S. Linstromberg. 1997. English Prepositions Explained. John
Benjamins Publishing Co., Amsterdam/Philaderphia.
D. Moldovan, A. Badulescu, M. Tatu, D. Antohe, and R. Girju.
2004. Models for the semantic classification of noun
phrases. In the Proceedings of the HLT/NAACL Workshop
on Computational Lexical Semantics.
A. Tyler and V. Evans. 2003. The Semantics of English Prepo-
sitions: Spatial Sciences, Embodied Meaning, and Cogni-
tion. Cambridge University Press.
389
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 75?83,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Mining the Web for Reciprocal Relationships
Michael Paul, Roxana Girju, and Chen Li
Linguistics and Computer Science Departments and Beckman Institute,
University of Illinois at Urbana-Champaign
{mjpaul2, girju, chenli}@illinois.edu
Abstract
In this paper we address the problem of
identifying reciprocal relationships in English.
In particular we introduce an algorithm that
semi-automatically discovers patterns encod-
ing reciprocity based on a set of simple but
effective pronoun templates. Using a set of
most frequently occurring patterns, we extract
pairs of reciprocal pattern instances by search-
ing the web. Then we apply two unsuper-
vised clustering procedures to form meaning-
ful clusters of such reciprocal instances. The
pattern discovery procedure yields an accu-
racy of 97%, while the clustering procedures
indicate accuracies of 91% and 82%. More-
over, the resulting set of 10,882 reciprocal in-
stances represent a broad-coverage resource.
1 Introduction
Reciprocity is a pervasive concept which has been
studied a lot in a wide variety of fields from ethics
to game theory where it is analyzed as a highly ef-
fective ?tit for tat? strategy. The ethic of reciprocity
(also known as the golden rule), for example, is a
moral code born from social interaction: ?Do onto
others as you would wish them do onto you?. The
golden rule appears in most religions and cultures as
a standard used to resolve conflicts.
According to sociologists and philosophers, the
concept of reciprocity lies at the foundation of social
organization. It strengthens and maintains social re-
lations among people, beyond the basic exchange of
useful goods. Thus, the way people conceptualize
reciprocity and the way it is expressed in language
play an important role in governing people?s behav-
ior, judgments, and thus their social interactions.
In this paper we present an analysis of the concept
of reciprocity as expressed in English and present a
way to model it. In particular we introduce an al-
gorithm that semi-automatically discovers patterns
encoding reciprocity based on a set of simple but ef-
fective pronoun templates. We then rank the identi-
fied patterns according to a scoring function and se-
lect the most frequent ones. Using these patterns we
query the web and run two unsupervised clustering
procedures to form meaningful clusters of reciprocal
pattern instances. The pattern discovery procedure
yields an accuracy of 97%, while the clustering pro-
cedures indicate accuracies of 91% and 82%. More-
over, the resulting set of 10,882 reciprocal instances
represent a broad-coverage resource.
Next we define the concept of reciprocity as ex-
pressed in English.
Reciprocity in language
The Oxford English Dictionary Online1 defines
reciprocity as ?a state or relationship in which there
is mutual action, influence, giving and taking, cor-
respondence, etc., between two parties?, while in
WordNet the verb to reciprocate means ?to act, feel,
or give mutually or in return?.
Reciprocity is defined as a relation between two
eventualities eo (original eventuality) and er (recip-
rocated eventuality), which can occur in various re-
ciprocal constructions. Each eventuality is an event2
or a state between two participants. Thus, the rela-
1http://www.oed.com/
2We use the term ?event? to denote all those actions or ac-
tivities performed by people.
75
tion of reciprocity <(eo(X, Y), er(Z, W)) describes
a situation where the eventuality er is performed ?in
return? for eo. Thus, reciprocity can be seen as a
special type of causal relation.
The two arguments of each eventuality represent
the subject and the object (direct or indirect), in this
order, and they might not all be explicitely stated
in the sentence, but can be inferred. Moreover, the
participants of the two eventualities might or might
not be the same. A few such examples are presented
below with the corresponding reciprocity relations:
(1) Mary argued with Paul at the station.
<(argue with(Mary, Paul), argue with(Paul Mary)) &
<(argue with(Paul, Mary), argue with(Mary, Paul))
(2) Paul and Mary hate each other.
<(hate(Paul, Mary), hate(Mary, Paul)) &
<(hate(Mary, Paul), hate(Paul, Mary))
(3) Mary likes Paul and he likes her, too.
<(like(Mary, Paul), like(Paul, Mary)) &
<(like(Paul, Mary), like(Mary, Paul))
(4) Mary likes Paul for helping her sister.
<(help(Paul, Mary?s sister), like(Mary,Paul))3
As shown in the examples above, in English
there are two basic types of reciprocal construc-
tions: mono-clausal reciprocals (involving words
such as (to) hug, to agree/argue with, partner of, mu-
tual(ly), together, each other ? examples (1) and (2))
or sentence-level reciprocals (involving two consec-
utive clauses ? examples (3) and (4)). Most of the
sentence-level reciprocals are paraphrased by coor-
dinations or subordinations of two clauses with the
same or different predicate and most of the time in-
verted arguments. They might also manifest various
markers as shown in bold in the examples.
In this paper we focus only on sentence-level con-
structions when the eventualities occur in different
consecutive clauses, and when the subject ? object
arguments of each eventuality are personal pronoun
pairs which occur in reverse order in each eventual-
ity. One such example is ?She likes him for help-
ing her?. Here the two eventualities are like(she,
he) and help(he, she). In this example, although the
subject of the second verb is not explicitely stated,
it is easily inferred. These simplifying assumptions
3We assume here that the subject of the verb help has been
recovered and the coreference solved.
will prove very useful in the semi-supervised pat-
tern discovery procedure to ensure the accuracy of
the discovered patterns and their matched instances.
Such a resource of reciprocal event pairs can
be very useful in a number of applications, rang-
ing from question answering and textual entailment
(since reciprocal event pairs encode a type of causal
relation), to behavior analysis of social groups (to
monitor cooperation, trustworthiness and personal-
ity), and behavior prediction in negotiations.
The paper is organized as follows. In the next sec-
tion we present relevant previous work. In Section
3 we detail a semi-supervised approach of extract-
ing patterns which encode reciprocity in English. In
section 4 we extract pairs of reciprocal instances and
cluster them in meaningful clusters. In section 5 we
present the experimental data and results. Discus-
sions and conclusion are presented in Section 6.
2 Previous work
Although the concept of reciprocity has been studied
a lot in different disciplines such as social sciences
(Gergen et al, 1980), anthropology (Sahlins, 1972),
economics (Fehr and Gachter, 2000), and philoso-
phy (Becker, 1990), linguists have started to look
deeper into this problem only more recently. More-
over, to the best of our knowledge, in computational
linguistics the problem is novel.
In linguistics, most of the work on reciprocity fo-
cuses on mono-clausal reciprocal constructions, in
particular on the quantifiers each other and one an-
other (Dalrymple et al, 1998; Heim, 1991; Ko?nig,
2005). Most of this work has been done by lan-
guage typologists (Maslova and Nedjalkov, 2005;
Haspelmath, 2007) who are interested in how recip-
rocal constructions of these types vary from one lan-
guage to another and they do this through compara-
tive studies of large sets of world?s languages.
In computational linguistics, our pattern discov-
ery procedure extends over previous approaches
that use surface patterns as indicators of semantic
relations between nouns or verbs ((Hearst, 1998;
Chklovski and Pantel, 2004; Etzioni et al, 2004;
Turney, 2006; Davidov and Rappoport, 2008) inter
alia). We extend over these approaches in two ways:
(i) our patterns indicate a new type of relation be-
tween verbs, (ii) instead of seed or hook words we
76
use a set of simple but effective pronoun templates
which ensure the validity of the patterns extracted.
To the best of our knowledge, the rest of our
reciprocity model is novel. In particular, we use a
novel procedure which extracts pairs of reciprocal
instances and present two novel unsupervised clus-
tering methods which group the instance pairs in
meaningful ways. We also present some interesting
observations on the data thus obtained and suggest
future research directions.
3 Pattern discovery procedure
Our algorithm first discovers clusters of patterns in-
dicating reciprocity in English, and then merges the
resulting clusters to identify the final set of recipro-
cal constructions. In this section we detail the algo-
rithm and evaluate it in subsection 5.2.
3.1 Pronoun templates
In this paper we focus on reciprocal eventualities
which occur in two consecutive clauses and have
two arguments: a subject and an object. One way
to do this is to fully parse each sentence of a corpus
and identify coordinations or subordinations of two
clauses. Then identify the subject and object argu-
ments of each verb in each clause with the help of
a PropBank-style grammatical or semantic role la-
beler (Kingsbury et al, 2002) and make sure they
represent people named entities (as indicated by
proper names, personal pronouns, etc.). Since our
focus is on reciprocal constructions, we also have to
keep in mind that the verbs have to have the same
set of arguments (subject-object) in reverse order.
Thus, noun and pronoun coreference should also be
resolved at this point.
Instead of starting with such a complex and error-
prone preprocessing procedure, our algorithm con-
siders a set of pronoun templates, where personal
pronouns are anchor words (they have to be matched
as such). Each template consists of four personal
pronouns corresponding to a subject - object pair in
one clause, and a subject - object pair in the other
clause. Two such examples are
?[Part1] I [Part2] him [Part3] he [Part4] me [Part5]? and
?[Part1] they [Part2] us [Part3] we [Part4] them [Part5]?,
where [Part1] - [Part5] are partitions identifying
any sequence of words. This is an elegant proce-
dure since in English, pronouns have different cases
such as nominative and accusative4 which identify
the subject, and respectively the object of an event.
This saves us the trouble of parsing a sentence to
find the grammatical roles of each verb. In English,
there are 30 possible arrangements of nominative -
accusative case personal pronoun pairs. Thus we
built 30 pronoun templates.
This approach is similar to that of seed words
(e.g., (Hearst, 1998)) or hook words (e.g., (Davidov
and Rappoport, 2008)) in previous work. However,
in our case they are fixed and rich in grammatical in-
formation in the sense that they have to correspond
to subject - object pairs in consecutive clauses.
Since the first two pronouns in each pronoun tem-
plate belong to the first clause (C1), and the last two
to the second clause (C2), the templates can be re-
stated as [Part1] C1 [Part3] C2 [Part5], with the re-
striction that partition 3 should not contain any of
the four pronouns in the template. C1 denotes ?Pro-
noun1 [Part2] Pronoun2? and C2 denotes ?Pronoun3
[Part4] Pronoun4?. Partitions 2 and 4 contain the
verb phrases (and thus the eventualities) we would
like to extract. For speed and memory reasons, we
limit their size to no more than 5 words.
Moreover, since the two clauses are consecutive,
we hypothesize that they should be very close to
each other. Thus, we restrict the size of each par-
tition 1, 3, and 5 to no more than 5 words. We then
consider all possible variations of the pattern where
the size of each partition varies from 0 to 5. This re-
sults in 216 possible combinations (63). Moreover,
to ensure the accuracy of the procedure, partitions 1
and 5 should be bounded to the left and respectively
to the right by punctuation marks, parentheses, or
paragraph boundaries. An example of an instance
matched by one such pattern is ?, I cooked dinner
for her and she loves me for that .?
3.2 Scoring function
One way to compute the prominence of the discov-
ered patterns would be to consider the frequency of
each of the five partitions. However, as our pre-
liminary experiments suggest, although individual
4In English, the pronouns you has the same form in nomina-
tive and accusative.
77
patterns within each partition do often repeat, rank-
ing patterns spanning all three partitions (PART1,
PART3, and PART5) is problematic. Patterns with
relatively long partitions (more than 2 words each)
seldomly occur more than once in the entire corpus.
Thus frequency would produce very little differenti-
ation in ranking the patterns.
Thus we developed an alternative scoring system
in lieu of frequencies. A sequence of size n (seq(n))
is an instance of a pronoun template and a subse-
quence of size k (seq(k)) is simply a substring of the
sequence with k < n. For example, for the instance
?I love her and she loves me , too? of length 9, there
will be two subsequences of length 8: ?love her and
she loves me , too? and ?I love her and she loves me
,?. Taking into account the frequencies of the subse-
quences occurring within instances of each partition,
we use the following recursive scoring function (n is
the length of each subsequence of size n):
Score(seq(n)) =
8
><
>:
Disc(freq(seq(n)))+
P
seq(n?1) Disc(Score(seq(n ? 1))), if n> 1
freq(seq(n)), if n= 1
(1)
In addition, in order to ensure a valid ranking
over the extracted templates with different lengths
for each partition, we need to normalize the scores
obtained for PART1, PART3, and PART5. In other
words, we need to scale the scores obtained for each
partition to discount the scores of longer partitions,
so that the maximum possible score would remain
the same regardless of how long the partition is.
So we use the following formula to compute the
discount for each of PART1, PART3, and PART5,
where n is the length of the subsequence:
Disc(Score(seq(n))) =
{
(1.0? fraction) ? fractionm?nm?n+1 , if n> 1
fractionm?n
m?n+1 , if n= 1
(2)
Fraction is an empirically predetermined parame-
ter - here set to 0.5. The variable m is the length of
the entire PART1, PART3, or PART5 in question.
This allows not only the frequency of the exact
pattern to contribute to the score, but also occur-
rences of similar patterns, although to a lesser ex-
tent. And since partitions 1, 3, and 5 constitute the
salient parts of the pattern as the environment for the
two reciprocal clauses C1 and C2, we take the score
to be ranked as Score(PART1)?Score(PART3)?
Score(PART5).
We searched the 30 pronoun templates with var-
ious partition sizes on a 20 million word English
corpus obtained from Project Gutenberg, the largest
single collection of free electronic books (over
27,000) (http://www.gutenberg.org) and British Na-
tional Corpus (BNC), an 100 million word collec-
tion of English from spoken and written sources.
There were 2,750 instances matched which were
ranked by the scoring function. There were 1,613
distinct types of patterns which generated 1,866 dis-
tinct pattern instances. Thus, we selected the top
15 patterns, after manual validation. These patterns
represent 56% of the data (Table 1). All the other
patterns were discarded as having very low frequen-
cies and being very specific.
The manual validation was necessary in order to
collapse some of the identified instances into more
general classes. For example, the patterns ?C1 and
C2 to? (e.g., ?He could not hurt me and I would not
wish him to.?), ?C1 and C2 in? (e.g., ?I give you and
you take me in.?), and ?C1 and C2 fast said Aunt
Jane? (e.g., ?He will come to her and she can hold
him fast said Aunt Jane.?) were collapsed into ?C1
and C2?. This procedure can be partially solved by
identifying complex verbs such as ?take in?. How-
ever, we leave this improvement for future work.
Patterns Examples
C1 [, |; |.] C2 I help him; he helps me.
C1 and C2 He understands her and she understands
him.
C1 and C2 [right] back I kissed him and and he kissed me back.
C1 and C2 for that They helped us and we appreciate them
for that.
C1 and C2, too I love her and she loves me, too.
C1 when C2 He ignores her when she scolds him.
C1 whenever C2 He is there for her whenever she needs
him.
C1 because C2 They tolerate us because we helped them.
C1 as much as C2 He loves her as much as she loves him.
C1 for C2 (vb-ing) He thanked her for being patient with him.
C1 but C2 I loved her but she dumped me.
C1 for what C2 They will punish him for what he did to
them.
C1 and thus C2 She rejected him and thus he killed her.
when C1, C2 When he confronted them, they arrested
him.
C1 as long as C2 She will stay with him as long as
he doesn?t hurt her.
Table 1: The top 15 reciprocal patterns along with examples.
78
4 Clustering of Reciprocal Eventualities
It seems reasonable to expect that certain reciproc-
ities could be grouped together. For example, the
language used in convincing a person of some-
thing could be characterized by verbs such as
eo = {convince, promise, assure, beg} and er =
{believe, trust, choose, forgive}.
There are many potential uses for this sort of
grouping. Having a single group label for multiple
reciprocal eventuality pairs would allow us to iden-
tify certain language patterns as a particular speech
act. Also, such clusters could be useful if one wants
to perform a macro-level analysis of reciprocity in a
specific domain. For example, examining reciprocal
language could be useful in analyzing the nature of
a social community or the theme of a literary work.
Generalizing over many similar instances, will give
us better insight into how people communicate ? as
reactions (effects) to other people?s actions (causes).
Thus, in this section we present a model for clus-
tering the eventualities we extract through the pro-
cess described in the previous sections. Experimen-
tal results are presented in Section 5.
4.1 Representing the data
After obtaining these patterns, we must extract pairs
of eventualities of the form (eo, er). This involves
both reducing the clauses into a form that is seman-
tically representative of some eventuality, as well as
determining the order of the two eventualities (i.e.,
if they are asymmetric).
As shown in the previous sections, each pat-
tern contains two clauses of the form ?Pronouni
[Part2/4] Pronounj?, where the first pronouns is
the subject and the second is the object. From
each clause we extract only the non-auxiliary verb,
as it carries the most meaning. We first stem the
verb and then negate it if it is preceded by not or
n?t. For example, ?They do not like him because
he snubbed them? is represented as the eventualities
(eo, er) = (snub,?like).
Certainly, we are missing important information
by excluding phrases and ignoring modality. How-
ever, these features can be difficult to capture accu-
rately, and since inaccurate input could degrade the
clustering accuracy, in this research we stick with
the important and easily-obtainable features.
4.2 Ordering the eventualities
Most patterns entail a particular ordering of the two
eventualities, corresponding to symmetric (e.g., ?He
loves her and she loves him?) or asymmetric eventu-
alities (e.g., ?He ignores her when she scolds him?).
In ambiguous situations (e.g., He loves her and she
loves him? and ?He cheated on her and she still
loves him!?), we determine the order through clues
such as the relative temporal ordering of the verbs as
determined by their tense (e.g., past or present tense
happens before future tense) and whether the verbs
denote an action (e.g., ?to chase?) or a state (e.g.,
?to love?). For this we rely on our previous work
(Girju, 2009) where we identified the order of even-
tualities based on a set of such features employed in
a semi-supervised model whose accuracy is 90.2%.
4.3 Modeling the relationships
The extracted eventuality pairs can be represented
as a bipartite graph with a node for all eo values
in one partition, a node for all er values in another
partition, and an edge between these nodes for each
(eo, er) pair. An intuitive way to cluster these even-
tualities is to find groups of nodes such that each
node in one partition has an edge to every node in
the other partition and vice versa. This is a form of
hard-clustering, as membership in a cluster is strictly
yes or no. The goal is that one could randomly pull
an eo and an er from a given cluster and the reci-
procity would be valid. For example, ?help? and
?give? could both be reciprocated by either ?thank?
or ?like?. Thus, given a cluster, not only is there a
reciprocal relationship between verbs in the eo group
with the verbs in the er group, but there is often
a kind of similarity relationship between the verbs
within each eo or er group.
This approach gives precise and concrete relations
between verbs, but while it could be well-suited
to some applications (such as knowledge base con-
struction or automatic verb classification (Joanis et
al., 2008)5) it has disadvantages in the context of
grouping these verbs together. The clusters are small
and sparse, and the results are difficult to interpret,
as there are many overlapping clusters.
5These verb classes correspond to some extent to the Verb-
Net (Kipper et al, 2000) or FrameNet-style (Baker et al, 1998)
verb classes such as admire, judgment.
79
..
.
.
.
.
.
.
.
.
.
.
cheat
hurt
forgive
despise
hate
betray
Figure 1: A sample of our data as a bipartite graph. Some edges have
been omitted for readability. The nodes {eo=?betray?, eo=?cheat?,
er=?despise?, er=?hate?} form a cluster with our hard-clustering ap-
proach.
We instead adopt a probabilistic framework,
which allows us to relax the restrictiveness of
the clusters while retaining information about the
strength of the pairwise relations. Thus, we design
a bimodal mixture model in which we assume that
each pair of eventualities (eo, er) belongs to a latent
class z, and each class is associated with two distinct
multinomial distributions from which the two even-
tualities are independently drawn. Thus, the proba-
bility of generating a particular pair is:
P (eo, er) =
|Z|?
k
P (eo|z = k)P (er|z = k) (3)
Each class can be thought of as a general type of
reciprocity, such as an action followed by apprecia-
tion, or an attack followed by retaliation. We should
be clear that each class is characterized not by a dis-
tribution of specific pairs, but by a distribution of
eo verbs and a distribution of er verbs. This allows
for the classification of (eo, er) pairs that do not ap-
pear in the corpus. For example, if we have not seen
the pair (slap, punch), but we know that (slap, hit)
and (kick, punch) belong to the same class, then it
is likely that (slap, punch) is in the same group.
This model can be used in a fully supervised as
well as a semi-/unsupervised setting. If some or
all of the class labels are unknown, we can learn
the model parameters using an estimator such as
Expectation-Maximization (EM) (Dempster et al,
1977). For each eventuality pair ci in a collection
C, we update P (z = k|ci) with the following equa-
tion, which represents the E-step:
P (z|ci) ? P (z)P (e(ci)o |z)P (e(ci)r |z) (4)
In the M-step, we use the following update equa-
tions:
P (z = k) ? ? +
|C|?
i
P (z = k|ci) (5)
P (eo = j|z) = ? +
?|C|
i I(e(ci)o = j)P (z|ci)
|Eo|? +?j?
?
i I(e(ci)o = j?)P (z|ci)(6)
where I is a binary indicator function. The equa-
tion for P (er = j|z) is identical to that for eo, but
with er instead6.
? and ? are the hyperparameters of the uniform
Dirichlet priors of P (z) and P (e?|z). They can
be tuned to control the level of smoothing; a value
of 1.0 is equivalent to the commonly-used Laplace
smoothing (Nigam et al, 2000).
4.4 Identifying polarity words
Since we are interested in analyzing how people in-
teract, we would also like to identify the polarity
(affective value) associated with each eventuality.
Thus, we automatically identify polarity words in
both clauses. For this we consider the standard po-
larity values: Good, Bad, and Neutral.
In the next section we present in detail the results
of the evaluation.
5 Experimental data and results
5.1 Data collection
While the Gutenberg and BNC collections are use-
ful in obtaining the frequent patterns, they do not
contain a very large number of eventuality pairs
to do meaningful clustering. We thus query the
web through Google to easily obtain thousands of
examples. We queried each of the top 15 pat-
terns and all pronoun combinations thereof (e.g.
?they * us because we * them?) and took the top
500 results for each pattern/pronoun combination
(15*30*500)7. We then extracted the clauses from
the result snippets using the procedure outlined in
the previous section and ended up with 10,882 pairs
6We sometimes use the shorthand P (z) to represent P (z =
k), which is updated for each particular value of z.
7This is because Google limits traffic. However, in the future
we can acquire more instances.
80
(4,403 unique pairs) since some of the queries had
less than 500 matched instances8.
5.2 Pattern discovery procedure
Since we wanted to see to what extent the 15 most
frequently occurring patterns encode reciprocity, we
selected a sample of 10 pattern instances matched
by each pattern in the text collection obtained from
the web. We presented the resulting 130 sentences
(a few patterns were not frequent on the web, so we
obtained a few less than 10 instances) to 2 judges
who evaluated them as encoding reciprocity (?yes?)
or not (?no?). The judges agreed 97% of the time.
Moreover, only 2.3% of the 130 pattern instances
did not encode reciprocity as agreed by both judges.
These statistics show that these patterns are highly
accurate indicators of reciprocity in English.
5.3 Unsupervised clustering
We can capture pattern instance clusters with no
prior labeling by initializing the EM parameters ran-
domly. In our experiments we used ? = 1.0 and
? = 0.01, with varying numbers of clusters (which
we denote as k). EM is sensitive to the initial pa-
rameters and can perform poorly due to many local
maxima. We thus ran the algorithm several times,
and saved the output with the best log-likelihood.
Results from clustering with k = 6 are shown
in Table 2. The examples shown correspond to a
random sample of 10 pairs within the top 10% of
P (eo, er|cluster) within each cluster. We find that
with larger values of k such as 30 or 50, some of the
clusters become noisier, but we can capture finer-
grained clusters such as eo = {libel, defame} and
er = {sue,?sue}.
Upon a close look at the clusters in Table 2, one
can see that each one seems to have a central theme.
Cluster 1 seems to contain mostly positive actions
reciprocated by verbs describing gratitude and ap-
preciation. Cluster 2 has to do with cognition; Clus-
ter 3 has to do with the way people communicate and
interact. Cluster 4 captures relationships of need and
desire. Cluster 5 is about love and adoration, while
Cluster 6 is about hate and other negative events, and
how they are reciprocated.
8The reciprocity dataset is available for download at
http://apfel.ai.uiuc.edu/resources.html.
Accuracy
No. instances 6 clusters 9 clusters
Top 20 90.8% 82.2%
20/100 71.7% 66.1%
20/All 34.2% 26.1%
Table 3: Cluster membership accuracy for 6 and 9 clusters.
Cluster membership is defined as argmaxc
P (eo|c) P (er|c). We took three samples of pairs:
(1) the top 20 pairs with the highest P (eo, er|c) val-
ues, (2) a random 20 of the top 10%, and (3) a ran-
dom 20 of all pairs assigned to each cluster. We pre-
sented the pairs to two judges who were asked to
identify each pair as belonging to the cluster or not
based on coherence; that is, all pairs labeled ?yes?
appear to be related in some way.
Because we fix the number of clusters, we are
making the assumption that each reciprocal pair
could be put into one of k groups, which is obviously
an assumption that will not hold true. However, if a
pair does not fit well into any of the clusters, this
should be reflected by a low probability. Thus we
can achieve decently high accuracy if we consider
only the highest-ranked pairs. The accuracy when
considering all pairs is only 34% which means that
34% of reciprocal pairs can be meaningfully placed
into only 6 groups, which is actually fairly high.
A big source of inter-annotator disagreement
comes from the ambiguity of certain verbs, which
is a weakness of our limited representation. For ex-
ample, without additional information it is not clear
how a pair like (know, ask) might relate to others.
5.4 Polarity word identification
For this procedure we used the Subjectivity Clues
(Wilson et al, 2005) which provides 8,220 entries.
From all the 10,882 eventuality pairs, 40.1% of the
total number of words were in the subjectivity lexi-
con, while 36.9% of the pairs had both words in the
subjectivity lexicon.
Table 4 shows all possible combinations of pairs
of affective values and their associated probabilities
in the corpus. These values are computed for those
pairs where both words have known polarity.
As one might expect, each polarity class is most
likely to be reciprocated by itself: Good for Good
(altruism) and Bad for Bad (retaliation). Further-
more, it is more likely that Good follows Bad (?turn
81
eo er eo er eo er eo er eo er eo er
help thank know respect call tell need need love love hate hate
allow thank trust know ask give need trust adore love attack hate
invite thank tell trust tell help want need understand love attack forgive
rescue thank tell know tell tell want trust love adore slap hate
join thank know know contact tell want want teach love hurt attack
inform thank know trust meet hear help need protect love betray punish
join admire know follow follow see offer need feed love kill hate
send thank give let watch send help help challenge love hit curse
support thank let like tell ignore help trust need love treat dislike
teach owe help marry confront tell love need give love ruin shoot
Table 2: The clusters induced after running our unsupervised algorithm with k = 6 clusters. The pairs correspond to a sample of the top 10% of
pairs with the highest value of P (eo, er|cluster) for each cluster.
Good Bad Neutral Total
Good 0.90 0.18 0.29 0.63
Bad 0.09 0.82 0.08 0.29
Neutral 0.01 0.002 0.63 0.09
Table 4: All possible combinations of pairs of affective values and
their associated probabilities as found in the corpus. The numbers in the
table correspond to conditional probabilities P(rowi|colj ). The Total
column indicates the probability of each affective class (P(rowi)).
the other cheek?) than that Bad follows Good.
We experimented with incorporating polarity into
our clustering process. We defined 9 clusters for
each combination of polarity pairs, and initialized
the model by labeling the eventuality pairs where
the polarity of both words was known. We then
ran the EM process on all of the pairs, and since
the model parameters were initialized with these 9
groups, their pairs were more likely to fit into clus-
ters that matched their polarity. We found, how-
ever, that it had trouble clustering the less-common
classes ? essentially, everything but (Good, Good)
and (Bad, Bad). For example, the cluster that was
initialized as (Bad, Good) ended up being dominated
by er = thanks and mostly positive-polarity words
as eo. This seems to be due to the fact that many of
these pairs included er = thanks (often in sarcasm,
as in ?he thanked them for embarrassing him?). But
there are many more words associated with thanks
that are Good, thus those pairs were put into the
same group, and the Good verbs eventually overtook
the cluster. Problems such as this could perhaps be
avoided with more varied labeled data.
We selected a sample of the top 20 pair instances
for each of the 9 clusters of polarity pairs and gave
them to 2 judges who agreed 82% of the time.
6 Discussion and Conclusions
In this paper we presented an analysis of the concept
of reciprocity as expressed in English and a way to
model it. The experimental results provided nice in-
sights into the problem, but can be further improved.
We noticed that the identification of polarity
words is not always enough to capture the affect of
each eventuality. Thus, the text needs to be further
processed to identify speech acts corresponding to
each clause in the reciprocal patterns. For exam-
ple, words such as ?sorry? can be classified as neg-
ative, while the entire clause ?I am sorry? captures
the speech act of APOLOGY which is associated with
good intentions. As future work, we will recluster
the reciprocity pairs.
Another observation concerns the reciprocity
property of magnitude (cf. (Jackendoff, 2005))
or equivalence of value between two eventualities.
Most of the time reciprocal eventualities have the
same or similar magnitude, as the patterns identified
indicate a more or less equivalence of value ? i.e.,
hugs for kisses, thanks for help. And most of these
constructions do not focus so much on the magni-
tude, but on the order in which one eventuality (the
effect) is a reaction to the other (the cause). How-
ever, a closer look at our data shows that there are
also constructions which indicate this property more
precisely. One such example is ?C1 as much as C2?
where even a negation in C1 or C2 might destroy the
magnitude balance (e.g., ?She does not love him as
much as he loves her.?).
We would like to study this property in more de-
tail as well. This kind of study is very important
in the analysis of people?s behavior, judgments, and
thus their social interactions.
82
References
C. Baker, Ch. Fillmore, and J. Lowe. 1998. The Berkeley
FrameNet Project. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics (COLING-ACL 1998), pages 86?90,
Montreal, Canada.
L. Becker, editor. 1990. Reciprocity. University of
Chicago Press, Chicago.
T. Chklovski and P. Pantel. 2004. Verbocean: Mining
the web for fine-grained semantic verb relations. In
Proceedings of the Empirical Methods in Natural Lan-
guage Processing (EMNLP) Conference.
M. Dalrymple, M. Kazanawa, Y. Kim, S. Mchombo,
and S. Peters. 1998. Reciprocal expressions and the
concept of reciprocity. Linguistics and Philosophy,
21:159?210.
D. Davidov and A. Rappoport. 2008. Unsupervised
discovery of generic relationships using pattern clus-
ters and its evaluation by automaticaly generated sat
analogy questions. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics (ACL).
A. P. Dempster, N.M. Laird, and D. B. Rdin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39:1?38.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates. 2004.
Methods for domain-independent information extrac-
tion from the web: An experimental comparison. In
Proceedings of the National Conference on Artificial
Intelligence (AAAI) Conference.
E. Fehr and S. Gachter. 2000. Cooperation and Punish-
ment in Public Goods Experiments. American Eco-
nomic Review, 90:980?994.
K. Gergen, M. Greenberg, and R. Willis, editors. 1980.
Social Exchange: Advances in Theory and Research.
New York: Plenum.
R. Girju. 2009. Reciprocity in language. In Technical
Report. University of Illinois at Urbana-Champaign.
M. Haspelmath. 2007. Further remarks on reciprocal
constructions. In Vladimir P. Nedjalkov, editor, Re-
ciprocal Constructions, pages 2087?2115.
M. Hearst. 1998. Automated Discovery of WordNet Re-
lations. In Christiane Fellbaum, editor, An Electronic
Lexical Database and Some of its Applications, pages
131?151. MIT Press, Cambridge, MA.
I. Heim. 1991. Reciprocity and plurality. Linguistic In-
quiry, 22:63?101.
R. Jackendoff. 2005. The peculiar logic of value. Jour-
nal of Cognition and Culture, 6:375?407.
E. Joanis, S. Stevenson, and D. James. 2008. A general
feature space for automatic verb classification. Natu-
ral Language Engineering, 14(3).
P. Kingsbury, M. Palmer, and M. Marcus. 2002. Adding
Semantic Annotation to the Penn Treebank. In Pro-
ceedings of the 2nd Human Language Technology
Conference (HLT 2002), pages 252?256, San Diego,
California.
K. Kipper, H. Trang Dang, and M. Palmer. 2000. Class-
based construction of a verb lexicon. In Proceedings
of the National Conference on Artificial Intelligence
(AAAI), pages 691?696, Austin, TX.
E. Ko?nig. 2005. Reciprocity in language: Cultural con-
cepts and patterns of encoding. Uhlenbeck Lecture,
23.
E. Maslova and V. Nedjalkov. 2005. Reciprocal con-
structions. In M. Haspelmath, M. Dryer, D. Gill,
and B. Comrie, editors, The World Atlas of Language
Structures, pages 430?433. New York: Oxford Univer-
sity Press.
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell.
2000. Text classification from labeled and unlabeled
documents using EM. Machine Learning, 39:103?
134.
M. Sahlins, editor. 1972. Stone Age Economics.
Chicago: Aldine-Atherton.
P. Turney. 2006. Similarity of semantic relations. Com-
putational Linguistics, 32(3):379?416.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. In Proceedings of the Human Language Technol-
ogy (HLT/EMNLP) Conference.
83
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 111?119,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Investigating Automatic Alignment Methods for Slide Generation from
Academic Papers
Brandon Beamer and Roxana Girju
Department of Linguistics
University of Illinois
Urbana, IL
{bbeamer,girju}@illinos.edu
Abstract
In this paper we investigate the task of auto-
matic generation of slide presentations from
academic papers, focusing initially on slide
to paper alignment. We compare and eval-
uate four different alignment systems which
utilize various combinations of methods used
widely in other alignment and question an-
swering approaches, such as TF-IDF term
weighting and query expansion. Our best
aligner achieves an accuracy of 75% and our
findings show that for this application, av-
erage TF-IDF scoring performs more poorly
than a simpler method based on the number of
matched terms, and query expansion degrades
aligner performance.
1 Introduction
Automatic generation of slide presentations is a task
the Computational Linguistics community has not
yet pursued in much depth. A robust system capable
of generating slide presentations from papers would
save the author much tedium when organizing her
presentations. In this paper we investigate this task
from a novel perspective. While others have devel-
oped interesting approaches to slide generation from
documents by modeling the problem in a unique
way (Utiyama and Hasida, 1999; Shibata and Kuro-
hashi, 2005), the aim of the research this paper initi-
ates is to discover how humans create slide presen-
tations, focusing more specifically on academic pa-
pers. Thus we take a corpus-based approach to the
problem, and as a first step focus on the task of au-
tomatically aligning slide presentations to academic
papers.
We built a corpus of 296 slide-paper pairs and im-
plemented four slide to paper aligners which utilize
popular information retrieval methods such as TF-
IDF term weighting and query expansion. In this
paper we show that, in this application, TF-IDF term
weighting is inferior to a simpler scoring mecha-
nism based only on the number of matched terms
and query expansion degrades aligner performance.
Our best aligner achieves an accuracy of 75%.
2 Related Work
Automatic slide generation from documents is a thus
far under-investigated topic. Utiyama and Hasida
(1999) generate slides from GDA1 (global document
annotation) tagged documents. They detect topics
within the documents by analyzing GDA corefer-
ence links, modeled each slide as a topic and item-
ized elaborations (which were also tagged with the
GDA tag set). Shibata and Kurohashi (2005) convert
Japanese documents to slide representation by pars-
ing their discourse structures and representing the
resulting tree in an outline format. While (Utiyama
and Hasida, 1999) and (Shibata and Kurohashi,
2005) generate slides from documents by modeling
the task in creative ways, we aim to learn something
deeper regarding how humans actually go about the
task. Creating a corpus of slide/paper pairs will en-
able us to study the intricacies involved in how real
humans approach this task.
Our current focus is slide to paper (region) align-
ment, which can be categorized best as align-
ment between monolingual comparable corpora, but
1The GDA tag set is designed to allow machines to auto-
matically infer the underlying structure of documents. More
information is available at http://i-content.org/gda.
111
could also be easily construed as document passage
retrieval, which is a well-researched topic in the In-
formation Retrieval community. Barzilay and El-
hadad (2003) incorporate context to facilitate align-
ment between monolingual comparable corpora by
first learning paragraph matching rules in a super-
vised way, and then refining the alignment at the sen-
tence level within paragraphs. Nelken and Shieber
(2008) used TF-IDF term weighting with logistic
regression to align sentences from pericopes in the
gospels of the new testament. Callan (1994) ana-
lyzed various ways to define document passages and
identified three main passage types, discourse (based
on physical structure of the document), semantic
(based on topic boundaries), and window (based on
token distance) and suggests that while discourse
passages may be an attractive way to define and re-
trieve document passages, due to reasons related to
sloppy writing, visual aids, or other factors, para-
graph boundaries may not be the best indicators of
content boundaries. Our alignment task differs from
that of (Barzilay and Elhadad, 2003) and (Nelken
and Shieber, 2008) in two ways. First, Barzilay
and Elhadad (2003) and Nelken and Shieber (2008)
align like-chunks between the two documents. That
is, they are either aligning sentences to sentences or
paragraphs to paragraphs. In our task we are align-
ing slide regions which are usually bullets spanning
at most a couple lines, to paper regions which can
be a whole paragraph long. Second, Barzilay and
Elhadad (2003) and (Nelken and Shieber, 2008) are
working with comparable corpora in which the same
information is assumed to be present in each docu-
ment, but expressed in a different way. We are not
able to necessarily make this assumption, in fact we
show in this paper that as much as half of the infor-
mation in slide presentations may not be present in
the corresponding paper.
The concept of query expansion that we im-
plement in some of our aligners is also not new.
Voorhees (1994) suggests that query expansion
tends to help performance with short, incomplete
queries but degrades performance with longer, more
complete queries. van der Plas and Tiedemann
(2008) investigated several types of lexico-semantic
information for query expansion in their question
answering system. They found that expansions that
bridge the terminology gap (synonyms, etc.) did not
result in improvement but expansions that bridge the
knowledge gap (words belonging to the same subject
field) did. In this paper, to get an idea of the base-
line performance of query expansion with regard to
our unique task, we implement a more rudimentary
form of query expansion which only expands syn-
onyms of terms. Since our slide regions don?t vary
much in length, it?s hard to say how our results relate
to the findings of Voorhees (1994). Our results par-
tially support (van der Plas and Tiedemann, 2008) in
that our implementation only bridges the terminol-
ogy gap, and isn?t very successful.
3 The Corpus
The first step to understanding how humans generate
slides from papers is to collect real-world examples
of academic papers and corresponding slide presen-
tations. To build our corpus, we searched the in-
ternet for web pages containing workshop proceed-
ings from various fields using generic queries such
as ?workshop slide paper?. The collected papers and
presentations come from a variety of fields but tend
to be focused generally on science and technology.
Workshop proceedings are an ideal source for our
data because they often provide the papers and slide
presentations side-by-side. Using this strategy, we
manually extracted 296 slide-paper pairs. The pa-
pers were downloaded in PDF format and the slides
were a mixture of PDF and Powerpoint formats. Be-
fore working with these files, we converted them to a
custom XML format which represents relevant parts
of the original data as logical regions. In the case of
slides, regions include bullets, headings, and other
text spans. In the case of papers, regions include re-
gions (or passages) which correspond to paragraphs,
section headings, and list items.
To work with PDF data, we convert it to a cus-
tom XML format which represents logical chunks
or regions of the paper. In our approach we delimit
regions by orthographic boundaries. Orthographic
boundaries delimit the physical structure of a paper
and describe the paper in a physical fashion in terms
of paragraphs, headings, bullets, etc. We do recog-
nize that there are other ways to define paper regions
though. As Callan (1994) observes, academic papers
could also be represented via semantic boundaries
which delimit the topical structure of papers and de-
112
scribe them in terms of where new topics are intro-
duced and where old ones are no longer discussed.
We prefer using orthographic boundaries in our ap-
proach for two reasons. First, detecting orthographic
boundaries can be accomplished with simple heuris-
tics while topic boundary detection requires more
sophisticated methods2, thus implementation is eas-
ier. Second, because orthographic boundaries are far
less subjective than topic boundaries, it?s easier to
verify the validity of orthographic boundaries than
semantic ones.
Preprocessing Powerpoint files is significantly
simpler than PDF files. To convert the Powerpoint
data to our custom XML, we first convert the Pow-
erpoint file to an OpenOffice.org3 ODP file via the
document converter tool that comes standard with
OpenOffice. ODP files are already encoded with a
rich XML which already describes physical regions
such as list items, bullets, and other text, so region
identification is unnecessary. We only needed to im-
plement a filter that translates the available data to
the custom XML format.
4 Alignment Methods
Discovering how humans generate slide presenta-
tions from papers starts with observing where slide
regions originate from. We make the general as-
sumption that a slide region either a) is a summa-
rization (excerpt or abstract) from the associated pa-
per, or b) comes from other sources including but
not limited to the author?s personal (world and/or
specific) knowledge. A complete alignment module
would thus need to be able to discern if the informa-
tion in a region comes from the target paper or if it
does not. When it does, the task of the aligner is then
to choose the region in the paper that is summarized
or from which the excerpt is taken. Our original hy-
pothesis was that the vast majority of the data in a
given slide presentation would come from the target
paper and concluded that a reasonable first attempt
at building an aligner could be made under this as-
sumption.
We approach the task of aligning slide regions to
paper regions with methods popular in information
2Reynar (1998) provides a detailed overview of the basic
topic detection and segmentation methods
3OpenOffice.org is a freely available office suite available at
http://www.openoffice.org.
Aligner Scoring Query Expansion
A Method 1 No
B Method 1 Yes
C Method 2 No
D Method 2 Yes
Table 1: Features implemented by each aligner.
retrieval. When aligning a slide region to a paper
region, we treat the slide region as a search query
and the target regions as documents in the informa-
tion retrieval sense. We compare two TF-IDF based
scoring methods and the effect of query expansion
by building four different aligners, each of which
corresponds to one combination of scoring type and
usage of query expansion. Table 1 shows a diagram
indicating which aligners have which features.
To prepare both the slide region and paper for
alignment, certain preprocessing tasks are executed
by all our aligners. The general procedure all our
aligners follow is outlined below:
1. For each token in each region in the paper, the to-
ken?s TF-IDF score is calculated, where the token?s
term frequency is the frequency of the token?s stem
in the region and the term?s document frequency is
the number of regions containing the token?s stem.
2. The slide region is tokenized and part-of-speech
tagged with the SNoW tagger (Roth, 1998) and non-
content words are removed. We consider content
words to be any token which is either a noun, adjec-
tive, verb, adverb, or cardinal number.
3. Each token in the slide region is stemmed and, in
the case of aligners B and D, query expansion is
performed.
4. A score is calculated for each region in the target pa-
per according to the scoring function implemented
by the aligner?method 1 for aligners A and B and
method 2 for aligners C and D.
These methods are presented in detail below.
4.1 Scoring Methods
In this paper we investigate two scoring methods,
which we?ll refer to as scoring method 1 and scor-
ing method 2. Scoring method 1 is implemented by
aligners A and B and is equivalent to the average TF-
IDF score of the search terms relative to the target
region. I.e. to calculate the score for a slide region
relative to a target paper region with method 1, the
TF-IDF scores of all the search terms are added and
the sum is divided by the number of terms, and the
113
target region with the highest average score wins.
Scoring method 2 is implemented by aligners C and
D and is based on the quantity of matched terms, re-
verting to scoring method 1 only in the case of a tie.
Thus, to calculate the score for a slide region rela-
tive to a target paper region with method 2, the num-
ber of search terms with non-zero TF-IDF scores for
the paper region is counted and the region with the
largest number of such search terms wins. In the
case of a tie, the average score is calculated as it is
in method 1 and the region with the highest average
score wins the tie.
With either scoring method, a zero score results
in the system predicting that the slide region is not
derived from any paper region.
4.2 Query Expansion
One common problem with rudimentary TF-IDF
based information retrieval systems is that match-
ing tokens must have a form identical to the search
terms. Hence, synonyms and other semantically-
related words that probably should match do not.
Query expansion is one way to consider terms which
are semantically near, but orthographically differ-
ent from the search terms. The general principle of
query expansion is that, via an external knowledge
base, semantic neighbors of search terms are added
to the search query before the score is calculated.
Our implementation of query expansion is utilized
by aligners B and D and uses Wordnet (Fellbaum,
1998) to extract synonyms of search terms. When a
slide region undergoes query expansion our aligner
executes the following steps:
1. The search terms are part-of-speech tagged using
the SNoW part-of-speech tagger (Roth, 1998) and
lemmatized with a morphological analyzer4.
2. The resulting lemmas and parts of speech are used
to query Wordnet for matching synsets.
3. Synonyms for all retrieved synsets are recorded.
4. When scoring occurs, the TF-IDF score of a search
term changes from the score of the stem to the maxi-
mum score among the stem and all its synonyms. In
the case of scoring method 2, a search term matches
if it stem is found in the target region or if any of its
synonyms? stems are found.
4The morphological analyzer we use is called mor-
pha and is freely available and can be downloaded at
http://www.informatics.susx.ac.uk/research/groups/nlp/carroll
/morph.html
5 Evaluation
To evaluate our aligners, we manually checked the
alignment of each on four randomly chosen slide
presentation-paper pairs. We refer to these presen-
tations here as P1, P2, P3, and P4. Collectively,
these four presentations with their respective papers
amount to 587 alignment decisions which were eval-
uated according to the following guidelines. If the
slide region is either an excerpt from the chosen pa-
per region or if the slide region is an abstract of
the chosen paper region, the alignment is judged as
good. In cases where the matching excerpt or ab-
stract text spans more than one paper region, the
alignment is judged as good if the aligner selected
any of the involved regions. Otherwise, the align-
ment is judged as bad and an error code is recorded.
The three error codes we utilize are BR, NR, and ER.
BR is short for ?better region? and indicates that the
alignment is bad because the chosen paper region is
not the paper region from which the slide region is
extracted or generated, but such a region does in-
deed exist. NR is short for ?no region? and indi-
cates that the alignment is bad because there is no
region in the paper to which the slide region should
be aligned. ER is short for ?existing region? and in-
dicates that the alignment is bad because the aligner
decided there was no paper region to which the slide
region should be aligned, but in fact there was. Also,
the type of each slide region was recorded as either
frontmatter (which covers text spans such as titles,
authors, dates, and addresses), outline, heading, bul-
let, or diagram. Table 2 illustrates the composition
of the four presentations insofar as slide region type
is concerned.
The distribution of slide region types is not sur-
prising. Table 2 shows that two of our presentations
included diagrams and the other two did not, and
that bullets not surprisingly account for more slide
regions than any other region type.
5.1 Alignability of Slide Regions
Table 3 shows the percentage of slide regions which
have a target paper region (i.e. the percentage of
alignable slide regions). One surprising observation
is that only about half (57%) of the slide bullets were
alignable. This goes against our initial hypothesis
that the vast majority of slide regions would come
114
Presentation Frontmatter Outline Heading Bullet Diagram
P1 3/174 (1.7%) 0/174 (0.0%) 5/174 (2.9%) 74/174 (42.5%) 92/174 (52.9%)
P2 9/181 (5.0%) 9/181 (5.0%) 34/181 (18.8%) 129/181 (71.3%) 0/181 (0.0%)
P3 5/114 (4.4%) 1/114 (0.9%) 52/114 (45.6%) 55/114 (48.2%) 0/114 (0.0%)
P4 5/118 (4.2%) 1/118 (0.8%) 13/118 (11.0%) 47/118 (39.8%) 52/118 (44.0%)
Total 22/587 (3.7%) 11/587 (1.9%) 104/587 (17.7%) 305/587 (52.0%) 144/587 (24.5%)
Table 2: Breakdown of slide text spans by type. Columns correspond to slide text span types. Percentages in each column measure the fraction
of text spans which are of the given type.
from the associated paper, and not from the author?s
knowledge.
Another important observation from the data in
table 3 is that the fraction of slide regions which are
alignable for any given presentation can vary wildly.
82% of P4?s regions were alignable while 60% of
P3?s and only 14% of P1?s regions were alignable.
5.2 Aligner Accuracy
Tables 4 and 5 show the raw accuracy and alignable
accuracy of the four aligners respectively. Raw
accuracy is the number of slide regions correctly
aligned out of the total number of slide regions.
Alignable accuracy is the percentage of alignable
slide regions which were aligned correctly.
Given the surprising results that a large percent-
age of slide regions need not come from the paper,
any fully fledged slide to paper aligner would need
a module which first filters out the unalignable slide
regions. Because such a module is not implemented
in our aligners, as our aligners make the assumption
that each slide region has a corresponding paper re-
gion, we limit most of our accuracy evaluation to
alignable accuracy rather than raw accuracy.
From tables 4 and 5 we can easily see the im-
portance of such a filtering module. As our best
aligner, which achieves an average alignable accu-
racy of 75%, only achieves an average raw accuracy
of 50%.
5.3 Error Analysis
Tables 6 and 7 show what percentage of an aligner?s
errors correspond to which error types. Because our
aligners are based on term matching, the only way
for them to predict no alignment is for the average
TF-IDF score of the terms to be zero (no matching
terms anywhere). Because this is a very rare event,
ER-type errors are also extremely rare, and are ex-
cluded from our error analysis.
We can see from tables 6 and 7 that our poorer
aligners (A and B) have a fairly even split between
BR-type and NR-type errors, while our better align-
ers (C and D) have a far greater percentage of NR-
type errors, indicating that the features we are in-
vestigating can only reduce BR-type errors. This
verifies the importance of the proposed alignability
module which first filters out unalignable slide re-
gions.
5.4 Error Reduction
Tables 8 and 9 analyze how well query expansion
and scoring method 2 reduce errors by measuring
the percentage of errors made by one aligner, which
were not made by another. Four pairings of align-
ers are considered: A and B, A and C, B and D, and
C and D. By comparing aligner A to B and C to D,
we have one measure of the error reduction achieved
by adding query expansion to an aligner. If the ad-
dition of query expansion enables an aligner to cor-
rectly align slide regions which its query expansion-
less counterpart could not, then we should see large
percentages of errors being corrected when compar-
ing aligner A to B and C to D. By comparing aligner
A to C and B to D, we have a measure of the error
reduction achieved by implementing scoring method
2 instead of method 1.
Tables 8 and 9 show that aligner D significantly
reduced aligner B?s errors and aligner C significantly
reduced aligner A?s errors, but aligner B did not im-
prove much on A, nor did D on C. In other words,
adding query expansion did not significantly reduce
errors, but using scoring method 2 instead of 1 did.
6 Discussion
6.1 On Alignability
Before mentioning alignment performance, it is im-
portant to notice from our data that there is great va-
riety among slide presentations. For example, ta-
115
Presentation Frontmatter Outline Heading Bullet Diagram Overall
P1 3/3 (100.0%) 0/0 0/5 (0.0%) 21/74 (28.4%) 0/92 (0.0%) 24/174 (13.8%)
P2 9/9 (100.0%) 8/9 (88.9%) 24/34 (70.6%) 104/129 (80.6%) 0/0 145/181 (80.1%)
P3 5/5 (100.0%) 0/1 (0.0%) 48/52 (92.3%) 15/55 (27.3%) 0/0 68/114 (59.5%)
P4 4/5 (80.0%) 0/1 (0.0%) 11/13 (74.6%) 33/47 (70.2%) 49/52 (94.2%) 97/118 (82.2%)
Total 21/22 (95.5%) 8/11 (72.7%) 83/104 (79.8%) 173/305 (56.7%) 49/144 (34.0%) 334/587 (56.9%)
Table 3: Breakdown of alignable slide text spans by type. Columns correspond to slide text span types. Percentages in each column measure the
fraction of text spans of that type which are alignable. E.g. of the 129 bullets in presentation P2, 104 are alignable. The ?Overall? column measures
the fraction of all text spans which are alignable. E.g. of the 181 text spans in presentation P2, 145 are alignable.
Presentation Aligner A Aligner B Aligner C Aligner D
P1 34/174 (19.5%) 129/174 (16.7%) 37/174 (21.3%) 35/174 (20.1%)
P2 71/181 (39.2%) 64/181 (35.4%) 101/181 (55.8%) 97/181 (53.6%)
P3 66/114 (57.9%) 64/114 (56.1%) 77/114 (67.5%) 77/114 (67.5%)
P4 50/118 (42.4%) 48/118 (40.7%) 78/118 (66.1%) 77/118 (65.3%)
Total 221/587 (37.6%) 205/587 (34.9%) 293/587 (49.9%) 286/587 (48.7%)
Table 4: Raw accuracy. Each column corresponds to one of the four aligners evaluated. Percentages measure the fraction of text spans which
were aligned correctly.
Presentation Aligner A Aligner B Aligner C Aligner D
P1 12/24 (50.0%) 9/24 (37.5%) 15/24 (62.5%) 15/24 (62.5%)
P2 63/145 (43.4%) 56/145 (38.6%) 93/145 (64.1%) 90/145 (62.1%)
P3 55/68 (80.9%) 54/68 (79.4%) 66/68 (97.1%) 67/68 (98.5%)
P4 49/97 (50.5%) 47/97 (48.5%) 77/97 (79.4%) 76/97 (78.4%)
Total 179/334 (53.6%) 166/334 (49.7%) 251/334 (75.1%) 248/334 (74.3%)
Table 5: Alignable accuracy. Each column corresponds to one of the four aligners evaluated. Percentages measure the fraction of alignable text
spans which were aligned correctly.
Aligner A Aligner B
Presentation BR NR BR NR
P1 11/140 (7.9%) 128/140 (91.4%) 14/145 (9.7%) 130/145 (89.7%)
P2 82/110 (74.5%) 28/110 (25.5%) 89/117 (76.1%) 28/117 (23.9%)
P3 13/48 (27.1%) 35/48 (72.9%) 14/50 (28.0%) 36/50 (72.0%)
P4 48/68 (70.6%) 20/68 (29.4%) 50/70 (71.4%) 20/70 (28.6%)
Total 154/366 (42.1%) 211/366 (57.7%) 167/382 (43.7%) 214/382 (56.0%)
Table 6: Error type breakdown for aligners A and B. Columns correspond to specific types of alignment errors. ?BR? is short for ?better region?
and ?NR? is short for ?no region?. An error of type ?BR? means that the aligner choose an incorrect region in the paper, and a better region existed.
An error of type ?NR? means the aligner choose an incorrect region, and there was no correct region.
Aligner C Aligner D
Presentation BR NR BR NR
P1 8/137 (5.8%) 128/137 (93.4%) 8/139 (5.8%) 130/139 (93.5%)
P2 52/80 (65.0%) 28/80 (35.0%) 55/84 (65.5%) 29/84 (34.5%)
P3 2/37 (5.4%) 35/37 (94.6%) 1/37 (2.7%) 36/37 (97.3%)
P4 20/40 (50.0%) 20/40 (50.0%) 21/41 (51.2%) 20/41 (48.8%)
Total 82/294 (27.9%) 211/294 (71.8%) 85/301 (28.2%) 215/301 (71.4%)
Table 7: Error type breakdown for aligners C and D. Columns correspond to specific types of alignment errors. ?BR? is short for ?better region?
and ?NR? is short for ?no region?. An error of type ?BR? means that the aligner choose an incorrect region in the paper, and a better region existed.
An error of type ?NR? means the aligner choose an incorrect region, and there was no correct region.
116
Aligner A? B Aligner A? C
Presentation BR NR Overall BR NR Overall
P1 0/11 (0.0%) 0/128 (0.0%) 0/140 (0.0%) 4/11 (36.4%) 0/128 (0.0%) 4/140 (2.9%)
P2 0/82 (0.0%) 0/28 (0.0%) 0/110 (0.0%) 38/82 (46.3%) 0/28 (0.0%) 38/110 (34.5%)
P3 0/13 (0.0%) 0/35 (0.0%) 0/48 (0.0%) 11/13 (84.6%) 0/35 (0.0%) 11/48 (22.9%)
P4 0/48 (0.0%) 0/20 (0.0%) 0/68 (0.0%) 31/48 (64.6%) 0/20 (0.0%) 31/68 (45.6%)
Total 0/154 (0.0%) 0/211 (0.0%) 0/366 (0.0%) 84/154 (54.5%) 0/211 (0.0%) 84/366 (23.0%)
Table 8: Error reduction between aligners A and B, and between aligners A and C. Major columns correspond to aligner pairs and minor columns
correspond to error types. A pair denoted by X ? Y indicates that the corresponding percentages are measuring the fraction of slide text spans
aligned incorrectly by aligner X , which were aligned correctly by aligner Y . E.g. from this table you can see that in presentation P1, aligner A
incorrectly aligned 140 text spans. 11 of them were BR-type errors and 128 of them were NR-type errors. Four of aligner A?s BR-type errors were
aligned correctly by aligner C.
Aligner B? D Aligner C? D
Presentation BR NR Overall BR NR Overall
P1 7/14 (50.0%) 0/130 (0.0%) 7/145 (4.8%) 0/8 (0.0%) 0/128 (0.0%) 0/137 (0.0%)
P2 42/89 (47.2%) 0/28 (0.0%) 42/117 (35.9%) 1/52 (1.9%) 0/28 (0.0%) 1/80 (1.2%)
P3 13/14 (92.9%) 0/36 (0.0%) 13/50 (26.0%) 1/2 (50.0%) 0/35 (0.0%) 1/37 (2.7%)
P4 32/50 (64.0%) 0/20 (0.0%) 32/70 (45.7%) 1/20 (5.0%) 0/20 (0.0%) 1/40 (2.5%)
Total 94/167 (56.3%) 0/214 (0.0%) 94/382 (24.6%) 3/82 (3.7%) 0/211 (0.0%) 3/294 (1.0%)
Table 9: Error reduction between aligners B and D, and between aligners C and D. Major columns correspond to aligner pairs and minor columns
correspond to error types. A pair denoted by X ? Y indicates that the corresponding percentages are measuring the fraction of slide text spans
aligned incorrectly by aligner X , which were aligned correctly by aligner Y . E.g. from this table you can see that in presentation P1, aligner B
incorrectly aligned 145 text spans. 14 of them were BR-type errors and 130 of them were NR-type errors. 7 of aligners B?s BR-type errors were
correctly aligned by aligner D.
ble 3 shows that 28% of P1?s bullets were alignable,
while 81% of P2?s were alignable. P1 and P4 both
contained diagrams, but only P4?s diagram existed
in the paper. Our initial hypothesis was that the vast
majority of slide regions would either be excerpts or
abstracts from/of the paper regions. Table 3 shows
that a nontrivial amount of slide regions does not
map to the paper at all. Also, tables 6 and 7 show
that as a result, NR-type errors make up the majority
of the errors made by the better aligners. Thus, the
data indicates that the task of slide-presentation gen-
eration is highly dependent on the end purpose the
presentation will serve, as well as the target audience
and other factors. We will focus more on identify-
ing these factors in future research. Once identified,
these factors should be quantified and controlled in
future corpora of presentation-paper pairs used for
this task.
6.2 On Scoring Methods and Query Expansion
Our results clearly show that, for this task, query
expansion has little or negative impact on aligners
and that scoring method 2 is indeed superior to scor-
ing method 1. Tables 4 and 5 show that aligner
C consistently outperforms aligner A and aligner D
consistently outperforms aligner B, especially when
limited to alignable slide regions. Hence, scoring
method 2 is better than method 1. We can also
see from tables 4 and 5 that aligner B consistently
under-performs A and aligner D consistently under-
performs C, which shows that query expansion does
not improve performance and in fact, it degrades it.
Tables 8 and 9 show the same results from a differ-
ent perspective: aligner C correctly aligned 55% of
the aligner A?s erroneous alignable slide regions and
aligner D correctly aligned 56% of aligner B?s erro-
neous alignable slide regions. But aligner B did not
catch any of aligner A?s errors and aligner D only
caught 4% of aligner C?s errors ? but ended up mak-
ing more in the end anyway.
With regard to query expansion, there are two
possibilities. Query expansion was not very help-
ful here because either (a) slide authors tend to use
wording identical to that in the paper, or (b) using
synonyms from Wordnet is not aggressive enough
and we should consider expanding our query expan-
sion approach to include hypernyms, immediate hy-
ponyms, and other semantically related terms. We
think the data suggests that (a) is more the case than
(b). If (b) were the case, including synonyms in our
search should have improved the performance, just
not by a lot. In actually, aligner B performed worse
117
on average than aligner A, and likewise with aligner
D when compared to C. Synonyms are semanti-
cally closer to the original term than hypernyms,
hyponyms, or other semantically related terms, and
our results show that introducing this small amount
of semantic distance is (a little bit) detrimental. By
adding hypernyms and other relations, only a wider,
less focused group of terms will be introduced which
will probably just result in more false positives.
One possible criticism against our argument for
(a) could be that our implementation of query expan-
sion performed poorly because we don?t word sense
disambiguate, and thus we introduce synonyms from
incorrect senses of each term. This probably isn?t
the case because the search terms are not in isolation,
but are part of a larger query. For an incorrect paper
region to be select based on an error of this type,
it would have to contain many of the terms in the
query as well as the semantically inaccurate sense of
the one in question. This situation is unlikely due to
one of the most basic assumptions made when sense
disambiguating: that context restricts the possible
senses of any word. So, if a paper region contains
many of the terms in a slide region, it is unlikely
that it will also contain the off-topic, semantically
awkward term pertaining to a bad sense of one of
them.
With regard to scoring methods. Average TF-IDF
scoring is probably ineffective in this application be-
cause of the nature of paper regions. When retriev-
ing whole documents given a search query, one doc-
ument?s contents are probably independent of any
other, so terms related to the document?s topic are
stated explicitly. Paper regions, however, are in the
context of each other. The topic of one can be very
similar to another, only because it?s nearby, not be-
cause of the terms explicitly mentioned in the re-
gion. Add to this the fact that paper regions are ex-
tremely non-uniform in length and TF-IDF scores
end up skewed.
6.3 On Improvement
There is a lot of room for improvement on slide
to paper alignment. As mentioned previously in
section 6.1, unalignable slide regions account for a
much larger portion of the slide presentations than
our initial hypothesis predicted; around 70% of the
errors made by our better aligners (C and D) were
NR-type errors, meaning the alignment was bad be-
cause the system selected a paper region when in fact
there was no correct paper region. A robust slide to
paper aligner would need to have a module capable
of filtering out unalignable slide regions. If this task
were solved and implemented on our better aligners,
raw accuracy would raise from 50% to about 75%
on average which is nearing the level of robustness
necessary for real-world applications.
We also suggest that, in regard to alignable slide
regions, performance would be significantly boosted
by taking context into account, both on the slide and
paper side. We noticed during evaluation that many
of the BR-type errors occurred when the slide region
in question lacked the necessary terms, but the terms
existed in nearby slide regions. Examples of this in-
clude when for instance, the title is broken across
two lines and the second line only has a word or
two in it, or when a heading is rather non-descriptive
but the sub-bullets beneath it contain many relevant
terms to the topic. Incorporating terms of nearby
slide regions (perhaps in query-expansion fashion),
rather than just treating each one as an independent
search query will certainly boost performance.
Likewise on the paper end, it is reasonable to as-
sume that in most cases, the topic of one region is
similar to the topics of adjacent regions. And just as
terms from nearby slide regions could supplement
term-poor slide regions, terms from nearby paper re-
gions could supplement term-poor paper regions.
7 Conclusion
In this paper we investigated the task of automatic
slide to paper alignment. We built a corpus of
slide-paper pairs and used four presentations from
it to evaluate four aligners which utilize methods
such as TF-IDF term weighting and query expan-
sion. We showed that query expansion does not im-
prove performance in our application and that TF-
IDF term weighting is inferior to a much simpler
scoring mechanism based on the number of matched
terms. For future improvements, we suggest that a
module capable of robustly filtering out unalignable
slide regions is necessary. We also suggest that per-
formance can be improved by taking context into ac-
count and using terms in nearby regions to supple-
ment both slide regions and paper regions.
118
References
Regina Barzilay and Noemie Elhadad. 2003. Sen-
tence alignment for monolingual comparable corpora.
In Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing (EMNLP).
James P. Callan. 1994. Passage-level evidence in docu-
ment retrieval. In Proceedings of the 17th Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval.
Christiane Fellbaum. 1998. WordNet - An Electronic
Lexical Database. Cambridge MA: MIT Press.
Rani Nelken and Stuart M. Shieber. 2008. Towards ro-
bust context-sensitive sentence alignment for monolin-
gua corpora. In Proceedings of the 11th Conference of
the European Chapter of the Association for Compu-
tational Linguistics.
Jeffrey C. Reynar. 1998. Topic Segmentation: Algo-
rithms and Applications. Ph.D. thesis, University of
Pennsylvania.
Dan Roth. 1998. Learning to resolve natural lanuage
ambiguities: A unified approach. In Proceedings of
the 15th Conference of the American Association for
Artificial Intelligence (AAAI).
Tomohide Shibata and Sadao Kurohashi. 2005. Au-
tomatic slide generation based on discourse structure
analysis. In Proceedings of the second international
joint conference on natural language processing (IJC-
NLP).
Masao Utiyama and Koiti Hasida. 1999. Automatic slide
presentation from semantically annotated documents.
In Proceedings of the workshop held in conjunction
with the 37th annual meeting of the Association for
Computational Linguistics (ACL).
Lonneke van der Plas and Jo?rg Tiedemann. 2008. Us-
ing lexico-semantic information for query expansion
in passage retrieval for question answering. In Pro-
ceedings of the 9th SIGdial Workshop on Discourse
and Dialogue.
Ellen M. Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of the 17th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval.
119
Domain-Specific Knowledge Acquisition from Text 
Dan Mo ldovan ,  Roxana  G i r ju  and  Vas i le  Rus  
Department  of Computer  Science and Engineer ing 
University of Southern Methodist  University 
Dallas, Texas, 75275-0122 
{ moldovan, roxana, rus} @seas.smu.edu 
Abst rac t  
In many knowledge intensive applications, it is nec- 
essary to have extensive domain-specific knowledge 
in addition to general-purpose knowledge bases. 
This paper presents a methodology for discovering 
domain-specific concepts and relationships in an at- 
tempt o extend WordNet. The method was tested 
on five seed concepts elected from the financial 
domain: interest rate, stock market, inflation, eco- 
nomic growth, and employment. 
1 Des iderata  fo r  Automated  
Knowledge  Acqu is i t ion  
The need for knowledge 
The knowledge is infinite and no matter how large a 
knowledge base is, it is not possible to store all the 
concepts and procedures for all domains. Even if 
that were possible, the knowledge is generative and 
there are no guarantees that a system will have the 
latest information all the time. And yet, if we are to 
build common-sense knowledge processing systems 
in the future, it is necessary tohave general-purpose 
and domain-specific knowledge that is up to date. 
Our inability to build large knowledge bases without 
much effort has impeded many ANLP developments. 
The most successful current Information Extrac- 
tion systems rely on hand coded linguistic rules rep- 
resenting lexico-syntactic patterns capable of match- 
ing natural anguage xpressions of events. Since 
the rules are hand-coded it is difficult to port sys- 
tems across domains. Question answering, inference, 
summarization, and other applications can benefit 
from large linguistic knowledge bases. 
The basic idea 
A possible solution to the problem of rapid develop- 
ment of flexible knowledge bases is to design an au- 
tomatic knowledge acquisition system that extracts 
knowledge from texts for the purpose of merging it 
with a core ontological knowledge base. The attempt 
to create a knowledge base manually is time con- 
suming and error prone, even for small application 
domains, and we believe that automatic knowledge 
acquisition and classification is the only viable solu- 
tion to large-scale, knowledge intensive applications. 
This paper presents an interactive method that ac- 
quires new concepts and connections a sociated with 
user-selected seed concepts, and adds them to the 
WordNet linguistic knowledge structure (Fellbaum 
1998). The sources of the new knowledge are texts 
acquired from the Internet or other corpora. At the 
present time, our system works in a semi-automatic 
mode, in the sense that it acquires concepts and re- 
lations automatically, but their validation isdone by 
the user. 
We believe that domain knowledge should not be 
acquired in a vacuum; it should expand an existent 
ontology with a skeletal structure built on consistent 
and acceptable principles. The method presented in
this paper is applicable to any Machine Readable 
Dictionary. However, we chose WordNet because it 
is freely available and widely used. 
Re la ted  work  
This work was inspired in part by Marti Hearst's 
paper (Hearst 1998) where she discovers manually 
lexico-syntactic patterns for the HYPERNYMY rela- 
tion in WordNet. 
Much of the work in pattern extraction from texts 
was done for improving the performance of Infor- 
mation Extraction systems. Research in this area 
was done by (Kim and Moldovan 1995) (Riloff 1996), 
(Soderland 1997) and others. 
The MindNet (Richardson 1998) project at Mi- 
crosoft is an attempt to transform the Longman Dic- 
tionary of Contemporary English (LDOCE) into a 
form of knowledge base for text processing. 
Woods studied knowledge representation a d clas- 
sification for long time (Woods 1991), and more re- 
cently is trying to automate the construction oftax- 
onomies by extracting concepts directly from texts 
(Woods 1997). 
The Knowledge Acquisition from Text (KAT) sys- 
tem is presented next. It consists of four parts: (1) 
discovery of new concepts, (2) discovery of new lex- 
ical patterns, (3) discovery of new relationships re- 
flected by the lexical patterns, and (4) the classifi- 
cation and integration of the knowledge discovered 
with a WordNet - like knowledge base. 
268 
2 KAT System 
2.1 D iscover  new concepts  
Select  seed concepts .  New domain knowledge can 
be acquired around some seed concepts that a user 
considers important. In this paper we focus on the 
financial domain, and use: interest rate, stock mar- 
ket, inflation, economic growth, and employment as 
seed concepts. The knowledge we seek to acquire re- 
lates to one or more of these concepts, and consists 
of new concepts not defined in WordNet and new re- 
lations that link these concepts with other concepts, 
some of which are in WordNet. 
For example, from the sentence: When the US 
economy enters a boom, mortgage interest rates rise, 
the system discovers: (1) the new concept mortgage 
interest rate not defined in WordNet but related to 
the seed concept interest rate, and (2) the state of 
the US economy and the value of mortgage interest 
rate are in a DIRECT RELATIONSHIP. 
In WordNet, a concept is represented as a synset 
that contains words sharing the same meaning. In 
our experiments, we extend the seed words to their 
corresponding synset. For example, stock market is 
synonym with stock exchange and securities market, 
and we aim to learn concepts related to all these 
terms, not only to stock market. 
Ext ract  sentences.  Queries are formed with each 
seed concept o extract documents from the Internet 
and other possible sources. The documents retrieved 
are further processed such that only the sentences 
that contain the seed concepts are retained. This 
way, an arbitrarily large corpus .4 is formed of sen- 
tences containing the seed concepts. We limit the 
size of this corpus to 1000 sentences per seed con- 
cept. 
Parse  sentences.  Each sentence in this corpus is 
first part-of-speech (POS) tagged then parsed. We 
use Brill's POS tagger and our own parser. The out- 
put of the POS tagger for the example above is: 
When/WRB the/DW U.~./NNP economy/NN en- 
ters/VBZ a/DT boom/NN ,/, mortgage/NN inter- 
est_rates/NNS rise/vBP ./. 
The syntactic parser output is: 
TOP (S (SBAR (WHADVP (WRB When)) (S (NP (DT 
the) (NNP U.S.) (NN economy)) (VP (VBZ enters) (NP 
(DT a) (NN boom) (, ,))))) (NP (NN mortgage) (NNS 
interest_rates)) (VP (VI3P rise))) 
Ext ract  new concepts .  In this paper only noun 
concepts are considered. Since, most likely, one- 
word nouns are already defined in WordNet, the fo- 
cus here is on compound nouns and nouns with mod- 
ifiers that have meaning but are not in WordNet. 
The new concepts directly related to the seeds are 
extracted from the noun phrases (NPs) that contain 
the seeds. In the example above, we see that the 
seed belongs to the NP: mortgage interest rate. 
This way, a list of NPs containing the seeds is 
assembled automatically from the parsed texts. Ev- 
ery such NP is considered a potential new concept. 
This is only the "raw material" from which actual 
concepts are discovered. 
In some noun phrases the seed is the head noun, 
i.e. \[word, word,..see~, where word can be a noun or 
an adjective. For example, \[interest rate\] is in Word- 
Net, but \[short erm nominal interest rate\] is not in 
WordNet. Most of the new concepts related to a 
seed are generated this way. In other cases the seed 
is not the head noun i.e. \[word, word,..seed, word, 
wor~. For example \[interest rate peg\], or \[interna- 
tional interest rate differentia~. 
The following procedures are used to discover con- 
cepts, and are applicable in both cases: 
Procedure 1.1. WordNet reduction. Search NP for 
words collocations that are defined in WordNet as 
concepts. Thus \[long term interest rate\] becomes 
\[long_term interest_rate\], \[prime interest rate\] be- 
comes \[prime_interest_rate\], as all hyphenated con- 
cepts are in WordNet. 
Procedure 1.2. Dictionary reduction. For each 
NP, search further in other on-line dictionaries for 
more compound concepts, and if found, hyphen- 
ate the words. Many domain-specific dictionaries 
are available on-line. For example, \[mortgage inter- 
est_rate\] becomes \[mortgage_interest_rate\], since it is 
defined in the on-line dictionary OneLook Dictionar- 
ies (http://www.onelook.com). 
Procedure 1.3. User validation. Since currently we 
lack a formal definition of a concept, it is not possible 
to completely automate the discovery of concepts. 
The human inspects the list of noun phrases and 
decides whether to accept or decline each concept. 
2.2 D iscover  lex lco -syntact i c  pat terns  
Texts represent a rich source of information from 
which in addition to concepts we can also discover 
relations between concepts. We are interested in dis- 
covering semantic relationships that link the con- 
cepts extracted above with other concepts, some of 
which may be in WordNet. The approach is to 
search for lexico-syntactic patterns comprising the 
concepts of interest. The semantic relations from 
WordNet are the first we search for, as it is only 
natural to add more of these relations to enhance 
the WordNet knowledge base. However, since the 
focus is on the acquisition of domain-specific knowl- 
edge, there are semantic relations between concepts 
other than the WordNet relations that are impor- 
tant. These new relations can be discovered auto- 
matically from the clauses and sentences in which 
the seeds occur. 
269 
Pick a semantic relation R. These can be Word- 
Net semantic relations or any other elations defined 
by the user. So far, we have experimented with the 
WordNet HYPERNYMY (or so-called IS-A) relation, 
and three other relations. By inspecting a few sen- 
tences containing interest rate one can notice that 
INFLUENCE is a frequently used relation. The two 
other relations are CAUSE and EQUIVALENT. 
Pick a pair of concepts Ci, C# among which 
R holds. These may be any noun concepts. In the 
context of finance domain, some examples of con- 
cepts linked by the INFLUENCE relation are: 
interest rate INFLUENCES earnings, or 
credit worthiness INFLUENCES interest rate. 
Extract lexico-syntactic patterns Ci :P Cj. 
Search any corpus B, different from ,4 for all in- 
stances where Ci and Cj occur in the same sentence. 
Extract the lexico-syntactic patterns that link the 
two concepts. For example~ from the sentence : The 
graph indicates the impact on earnings from several 
different interest rate scenarios, the generally appli- 
cable pattern extracted is: 
impact on NP2 from NP1 
This pattern corresponds unambiguously to the re- 
lation R we started with, namely INFLUENCE. Thus 
we conclude: INFLUENCE(NPI, NP2). 
Another example is: As the credit worthiness de- 
creases, the interest rate increases. From this sen- 
tence we extract another lexical pattern that ex- 
presses the INFLUENCE relation: 
\[as NP1 vbl, NP2 vb$\] & \[vbl and vb2 are antonyms\] 
This pattern is rather complex since it contains not 
only the lexical part but also the verb condition that 
needs to be satisfied. 
This procedure repeats for all relations R. 
2.3 Discover new relationships between 
concepts 
Let us denote with Cs the seed-related concepts 
found with Procedures 1.1 through 1.3. We search 
now corpus ,4 for the occurrence of patterns ~ dis- 
covered above such that one of their two concepts i
a concept Cs. 
Search corpus ,4 for a pattern ~. Using a lexico- 
syntactic pattern P, one at a time, search corpus ,4 
for its occurrence. If found, search further whether 
or not one of the NPs is a seed-related concept Cs. 
Identify new concepts Cn. Part of the pattern 7~ 
are two noun phrases, one of which is Cs. The head 
noun from the other noun phrase is a concept Cn we 
are looking for. This may be a WordNet concept, 
and if it is not it will be added to the list of concepts 
discovered. 
Form relation R(Cs, Cn). Since each pattern 7~ is 
a linguistic expression of its corresponding seman- 
tic relation R, we conclude R(Cs,Cn) (this is in- 
terpreted "C8 is relation R Cn)'). These steps are 
repeated for all patterns. 
User intervention to accept or reject relationships 
is necessary mainly due to our system inability of 
handling coreference r solution and other complex 
linguistic phenomena. 
2.4 Knowledge classification and 
integration 
Next, a taxonomy needs to be created that is con- 
sistent with WordNet. In addition to creating a 
taxonomy, this step is also useful for validating the 
concepts acquired above. The classification is based 
on the subsumption principle (Schmolze and Lipkis 
1983), (Woods 1991). 
This algorithm provides the overall steps for the 
classification ofconcepts within the context of Word- 
Net. Figure 1 shows the inputs of the Classification 
Algorithm and suggests that the classification is an 
iterative process. In addition to WordNet, the in- 
puts consist of the corpus ,4, the sets of concepts Cs 
and Cn, and the relationships 7~. Let's denote with 
C = Cs U Cn the union of the seed related concepts 
with the new concepts. All these concepts need to 
be classified. 
Wo,aN=l C?~Tr~ A Co.=i= ~. C?, V.=~tio.~=a~\[ I R \[ I 
t Knowledge Classification ?--k 
Algorithm '1 ..... ~i;\] 
Figure 1: The knowledge classification diagram 
Step 1. From the set of relationships 7"~ discovered 
in Part 3, pick all the HYPERNYMY relations. From 
the way these relations were developed, there are 
two possibilities: 
(1) A HYPERNYMY relation links a WordNet concept 
Cw with another concept from the set C denoted 
with CAw , or 
(2) A HYPERNYMY relation links a concept Cs with 
a concept Cn. 
Concepts C~w are immediately linked to Word- 
Net and added to the knowledge base. The concepts 
from case (2) are also added to the knowledge base 
but they form at this point only some isolated islands 
since are not yet linked to the rest of the knowledge 
base. 
Step 2. Search corpus `4 for all the patterns asso- 
ciated with the HYPERNYMY relation that may link 
270 
\[Asian_country interest_rate \] 
IIS-A TIS-A 
\[Japan discount_rate \] 
a) 
\[country interest_rate \] 
\[Japan discount_rate \] \[Germany prime interest_rate \]
b) 
Figure 2: Relative classification of two concepts 
concepts in the set Cn with any WordNet concepts. 
Altough concepts C ,  are not seed-based concepts, 
they are related to at least one Cs concept via a re- 
lationship (as found in Task 3). Here we seek to find 
HYPERNYMY links between them and WordNet con- 
cepts. If such C,~ concepts exist, denote them with 
C~w. The union Chw = C~w LJ C2w represents all 
concepts from the set C that are linked to WordNet 
without any further effort. We focus now on the rest 
of concepts, Cc -- C N Chw, that are not yet linked 
to any WordNet concepts. 
Step 3. Classify all concepts in set Ce using Pro- 
cedures 4.1 through 4.5 below. 
Step 4. Repeat Step 3 for all the concepts in set 
Cc several times till no more changes occur. This 
reclassification is necessary since the insertion of a 
concept into the knowledge base may perturb the 
ordering of other surrounding concepts in the hier- 
archy. 
Step 5. Add the rest of relationships 7~ other 
than the HYPERNYMY to the new knowledge base. 
The HYPERNYMY relations have already been used 
in the Classification Algorithm, but the other rela- 
tions, i.e. INFLUENCE, CAUSE and EQUIVALENT need 
to be added to the knowledge base. 
Concept  classif ication procedures 
Procedure 4.1. Classify a concept of the form \[word, 
head\] with respect o concept \[head\]. 
It is assumed here that the \[head\] concept exists 
in WordNet simply because in many instances the 
"head" is the "seed" concept, and because frequently 
the head is a single word common oun usually de- 
fined in WordNet. In this procedure we consider only 
those head nouns that do not have any hyponyms 
since the other case when the head has other con- 
cepts under it is more complex and is treated by 
Procedure 4.4. Here "word" is a noun or an adjec- 
tive. 
The classification is based on the simple idea 
that a compound concept \[word, head\] is onto- 
logically subsumed by concept \[head\]. For exam- 
ple, mortgage_interest_rate is a kind of interest_rate, 
thus linked by a relation nYPERNYMY(interest_rate, 
mortgage_interest_rate). 
Procedure 4.2. Classify a concept \[wordx, headx\] 
with respect o another concept \[words, head2\]. 
For a relative classification of two such concepts, the 
ontological relations between headz and head2 and 
between word1 and words, if exist, are extended to 
the two concepts. We distinguish ere three possi- 
bilities: 
1. heady subsumes heads and word1 subsumes 
word2. In this case \[wordz, headl\] subsumes 
\[word2, heads\]. The subsumption may not al- 
ways be a direct connection; sometimes it may 
consist of a chain of subsumption relations ince 
subsumption is (usually) a transitive relation 
(Woods 1991). An example is shown in Fig- 
ure 2a; in WordNet, Asian_country subsumes 
Japan and interest_rate subsumes discount_rate. 
A particular case of this is when head1 is iden- 
tical with head2. 
2. Another case is when there is no direct sub- 
sumption relation in WordNet between word1 
and words, and/or head1 and heads, but there 
are a common subsuming concepts, for each 
pair. When such concepts are found, pick 
the most specific common subsumer (MSCS) 
concepts of word1 and words, and of head1 
and head2, respectively. Then form a concept 
\[MSCS(wordz, words), MSCS(headl, head2)\] 
and place \[word1 headz\] and \[words heads\] un- 
der it. This is exemplified in Figure 2b. In 
WordNet, country Subsumes Japan and Ger- 
many, and interest_rate subsumes discount_rate 
and prime_interest_rate. 
3. In all other cases, no subsumption relation is es- 
tablished between the two concepts. For exam- 
ple, we cannot say whether Asian_country dis- 
count_rate is more or less abstract hen Japan 
interest_rate. 
Procedure 4.3. Classify concept \[word1 words head\]. 
Several poss!bilities exist: 
1. When there is already a concept \[words head\] 
in the knowledge base under the \[head\], then 
place \[wordl words head\] under concept \[words 
head\]. 
2. When there is already a concept \[wordz head\] 
in the knowledge base under the \[head\], then 
place \[wordl word2 head\] under concept \[wordl 
head\]. 
3. When both cases 1 and 2 are true then place 
\[wordz word2 head\] under both concepts. 
271 
4. When neither \[wordl head\] nor \[words head\] are 
in the knowledge base, then place \[wordl word~ 
head\] under the \[head\]. The example in Figure 
3 corresponds to case 3. 
components ;y/ 
radio components automobile components / 
automobile radio components 
Figure 3: Classification of a compound concept with respect o 
its ~ concepts 
Since we do not deal here with the sentence seman- 
tics, it is not possible to completely determine the 
meaning of \[word1 word2 head\], as it may be either 
\[((word1 word2) head)\] or \[(word1 (words head))\] of- 
ten depending on the sentence context. 
In the example of Figure 3 there is only one mean- 
ing, i.e. \[(automobile radio) components\]. However, 
in the case of ~erformance skiing equipment\] here 
are two valid interpretations, namely \[(performance 
skiing) equipment\] and ~erformance (skiing equip- 
ment)\]. 
Procedure 4.4 Classify a concept \[word1, head\] with 
respect o a concept h/erarchy under the ~aead\]. 
The task here is to identify the most specific sub- 
sumer (MSS) from all the concepts under the head 
that subsumes \[wordx, head\]. By default, \[wordl 
head\] is placed under \[head\], however, since it may 
be more specific than other hyponyms of \[head\], a 
more complex classification analysis needs to be im- 
plemented. 
In the previous work on knowledge classification 
it was assumed that the concepts were accompanied 
by rolesets and values (Schmolze and Lipkis 1983), 
(Woods 1991), and others. Knowledge classifiers are 
part of almost any knowledge representation system. 
However, the problem we face here is more diffi- 
cult. While in build-by-hand knowledge representa- 
tion systems, the relations and values defining con- 
cepts are readily available, here we have to extract 
them from text. Fortunately, one can take advantage 
of the glossary definitions that are associated with 
concepts in WordNet and other dictionaries. One 
approach is to identify a set of semantic relations 
into which the verbs used in the gloss definitions are 
mapped into for the purpose of working with a man- 
ageable set of relations that may describe the con- 
cepts restrictions. In WordNet these basic relations 
are already identified and it is easy to map every 
verb into such a semantic relation. 
As far as the newly discovered concepts are con- 
cerned, their defining relations need to be retrieved 
from texts. Human assistance is required, at least 
for now, to pinpoint he most characteristic relations 
that define a concept. 
Below is a two step algorithm that we envision for 
the relative classification of two concepts A and B. 
Let's us denote with ARaCa and BRbCb the rela- 
tionships that define concepts A and B respectively. 
These are similar to rolesets and values. 
1. Extract relations (denoted by verbs) be- 
tween concept and other gloss concepts. 
ARalC~I BRblCbl 
ARa2Ca2 BRb2Cb2 
AR,~Cam B Rbn Cb,, 
2. A subsumes B ff and only if 
(a) Relations Rai subsume Rbl, for 1 < i < m. 
(b) Col subsumes or is a meronym of Cbi. 
(c) Concept B has more relations than concept 
A, i.e. m<n.  
Example: In Figure 4 it is shown the classification 
of concept monetary policy that has been discovered. 
By default his concept is placed under policy. How- 
ever in WordNet there is a hierarchy fiscal policy - 
IS-A - economic policy - IS-A - policy. The question 
is where exactly to place monetary policy in this hi- 
erarchy. 
The gloss of economic policy indicates that it is 
MADE BY Government, and that it CONTROLS eco- 
nomic growth- (here we simplified the explanation 
and used economy instead of economic growth). The 
gloss of fiscal policy leads to relations MADE BY Gov- 
ernment, CONTROLS budget, and CONTROLS taxa- 
tion. The concept money supply was found by Pro- 
cedure 1.2 in several dictionaries, and its dictionary 
definition leads to relations MADE BY Federal Gov- 
ernment, and CONTROLS money supply. In Word- 
Net Government subsumes Federal Government, and 
economy HAS PART money. All necessary conditions 
are satisfied for economic policy to subsume mone- 
tary policy. However, fiscal policy does not subsume 
monetary policy since monetary policy does not con- 
trol budget or taxation, or any of their hyponyms. 
Procedure 4.5 Merge a structure of concepts with 
the rest of the knowledge base. 
It is possible that structures consisting of several 
inter-connected concepts are formed in isolation of 
the main knowledge base as a result of some proce- 
dures. The task here is to merge such structures with 
the main knowledge base such that the new knowl- 
edge base will be consistent with both the struc- 
ture and the main knowledge base. This is done by 
272 
po~cy 
| IS-A 
economic policy .~ . . . . . . . . . . .  >" made by . . . . . . . .  :" government 
monetary policy = - - -> madeby 
fiscal policy : ~ . . . . . . . . . .  > made by . . . . . . . .  >- government 
" ' .  "~" controls . . . . . . . .  ~" budget 
"-k controls . . . . . . . .  : "  taxation 
WordNet 
before merger 
work place 
t lS-A 
exchange 
I IS-A 
stock market 
industry 
t lS-A 
market 
IS-A 
money market 
Figure 4: Classification of the new concept monetary policy 
WordNet 
The new structure from text after merger 
 Taltarke' 
eapital~market money market 
\[IS-A 
stock market 
work place 
t lS-A 
exchange 
A~ capital market 
IS- 
stock market 
ind~su'y 
\[ IS-A 
ma\[ket 
IS-A 
financial market 
money market 
Figure 5: Merging a structure of concepts with WordNet 
bridging whenever possible the structure concepts 
and the main knowledge base concepts. It is possi- 
ble that as a result of this merging procedure, some 
HYPERNYMY relations either from the structure or 
the main knowledge base will be destroyed to keep 
the consistency. An example is shown in Figure 5. 
Example : The following HYPERNYMY relation- 
ships were discovered in Part 3: 
HYPERNYMY(financial market,capital market) 
HYPERNYMY(fInancial market,money market) 
HYPERNYMY(capital market,stock market) 
The structure obtained from these relationships 
along with a part of WordNet hierarchy is shown 
in Figure 5. An attempt is made to merge the new 
structure with WordNet. To these relations it cor- 
responds a structure as shown in Figure 5. An at- 
tempt is made to merge this structure with Word- 
Net. Searching WordNet for all concepts in the 
structure we find money market and stock market 
in WordNet where as capital market and financial 
market are not. Figure 5 shows how the structure 
merges with WordNet and moreover how concepts 
that were unrelated in WordNet (i.e. stock market 
and money market) become connected through the 
new structure. It is also interesting to notice that the 
IS-A link in WordNet from money market o market 
is interrupted by the insertion of financial market 
in-between them. 
3 Imp lementat ion  and  Resu l ts  
The KAT Algorithm has been implemented, and 
when given some seed concepts, it produces new con- 
cepts, patterns and relationships between concepts 
in an interactive mode. Table 1 shows the number 
of concepts extracted from a 5000 sentence corpus, 
in which each sentence contains at least one of the 
five seed concepts. 
The NPs were automatically searched in Word- 
Net and other on-line dictionaries. There were 3745 
distinct noun phrases of interest extracted; the rest 
contained only the seeds or repetitions. Most of the 
273 
\[I Re la t ions  I Lex ico -syntact i c  Pat te rns  Examples  
H WordNet  Relations 
HYPERNYMY I NP1 \ [<be>\]  a kind of NP2 Thus, LIBOR is a kind of interest rate, as it  is charged 
I ::~ HYPERNYMY(NPI,NP2) on deposits between banks in the Eurodolar market. 
New Re la t ions  
CAUSE 
INFLUENCE 
NPI  \[<be>\] cause NP2 
=~ CAUSE(NPI,NP2) 
NP1 impact on NP2 
INFLU~NCZ(NP1,NP2) 
As NP1 vb, so <do> NP2 
=> INFLUENCE(NPI,NP2) 
NP1  <be> associated with NP2  
=> INFLUENCE(NP1,NP2) 
INFLUENCE(NP2,NPI) 
As/if/when NP1 vbl, NP2  vb2. -{- 
vbl, vb2 ---- antonyms / go in 
opposite directions 
::~ INFLUENCE(NPI,NP2) 
the effect(s) of NP1 on/upon NP2 
::> INFLUENCE(NPI,NP2) 
inverse relationship between 
NP I  and NP2 
=> INFLUENCE(NP1,NP2) 
=~ INFLUENCE(NP2,NP1) 
NP2  <be> function of NP1  
=# INFLUENCZ(NP1,NP2) 
NP1  (and thus NP2) 
:~ INFLUENCE(NPI,NP2) 
Phi l l ips,  a Brit ish economist, stated in 1958 that  high inflation 
causes low unemployment rates. 
The Bank of Israel governor said that  the t i ;h t  economic policy 
would have an immediate impact on inflation th is  year. 
As the economy picks up steam, so does inflation. 
Higher interest rates are normal ly associated with weaker bond markets. 
On the other hand, if interest rates go down, bonds go up, 
and your bond becomes more valuable. 
The effects of inflation on debtors and creditors varies as the 
actual  inflation is compared to the expected one. 
There exists an inverse relat ionship between unemployment rates 
and inflation, best i l lustrated by the Phi l l ips  Curve. 
Irish employment is also largely a funct ion of the past 
high birth rate. 
We believe that  the Treasury bonds (and thus interest rates) 
are in a downward cycle. 
Table 2: Examples of lexico-syntactic pat terns  and semant ic  relations derived from the 5000 sentence corpus 
l a  I b l  c Id  I e II 
concepts  (NPs)  773 382 833 921 . 
Tota l  concepts  ext rac ted  with Procedurel 
Concepts found 
in WordNet 2 0 1 0 2 
Concepts Concepts 
found in with seed 6 0 3 0 0 
on-line head 
dictionaries, Concepts 
but not in with seed 7 0 1 1 1 
WordNet not head 
I C?ncepts accepted I 
by human 78 62 58 60 37 
Table 1: Results showing the number of new concepts learned 
from the corpus related to (a) interest rate, (b) stock market, (c) 
inflation, (d) economic 9rowth, a~ld (e) employment. 
processing in Part 1 is taken by the parser. The hu- 
man intervention to accept or decline concepts takes 
about 4 min./seed. 
The next step was to search for lexico-syntactic 
patterns. We considered one WordNet semantic re- 
lation, HYPERNYMY and three other relations that 
we found relevant for the domain, namely INFLU- 
ENCE, CAUSE and EQUIVALENT. For each relation, 
a pair of related words was selected and searched 
for on the Internet. The first 500 sentences/relation 
were retained. A human selected and validated semi- 
automatically the patterns for each sentence. A sam- 
ple of the results is shown in Table 2. A total of 22 
patterns were obtained and their selection and vali- 
dation took approximately 35 minutes/relation. 
Next, the patterns are searched for on the 5000 
sentence corpus (Part 3). The procedure provided 
a total of 43 new concepts and 166 relationships 
in which at least one of the seeds occurred. From 
these relationships, by inspection, we have accepted 
63 and rejected 102, procedure which took about 7 
minutes. Table 3 lists some of the 63 relationships 
discovered. 
Relationships. Examples 
HYPEaNYMY(interest rate, LIBOR) 
HYPERNYMY(leading stock market ,  
New York Stock Exchange) 
HYPERNYMY(market risks, interest  rate risk) 
HYPERNYMY(Capital markets,  stock markets)  
CAUSE(inflation, unemployment)  
CAUSE(labour shortage, wage inflation) 
CAUSE(excessive demand,  inflation 
INFLUENCE_DIRECT_PROPORTIONALYI economy, inflation) 
INFLUENCE_DIRECT_PROPORT1ONALY sett lements,  interest rate) 
INFLUENCE..DIRECT..PROPORTIONALY~ U.S. interest rates, dollars) 
INFLUENCE_DIRECT_PROPORTIONALY~ oil prices, inflation) 
INFLUENCE_DIRECT_PROPORTIONALY' inflation, nominal  interest rates) 
INFLUENCE..DIRECT_PROPORTIONALY~ deflation, real interest rates) 
INFLUENCE-DIRECT-PROPORTIONALY currencles, lnf lat ion) 
INFLUENCE_INVERSE_PROPORTIONALY unemployment  rates, inflation) 
INFLUENCE_INVERSE-PKOPOKTIONALY monetary  policies, inflation) 
INFLUENCE_INVERSE_PROPORTIONALY economy, interest rates) 
INFLUENCE_INVERSE..PROPORTIONALY inflation, unemployment  rates) 
INFLUENCE.JNVERSE-PROPORTIONALY credit  worthiness, interest rate) 
INFLUENCE_INVERSE-PROPORTIONALYlinterest rates, bonds) 
INFLUENCE(Internal Revenue Service, interest rates) 
INFLUENCE(economic growth, share prices) 
EQUIVALENT(big mistakes, high inf lation rates of 1970s) 
EQUIVALENT(fixed interest rate, coupon) 
Table 3: A part  of the relat ionships derived from the 5000 
sentence corpus 
274 
4 App l i ca t ions  
An application in need of domain-specific knowledge 
is Question Answering. The concepts and the rela- 
tionships acquired can be useful in answering dif- 
ficult questions that normally cannot be easily an- 
swered just by using the information from WordNet. 
Consider the processing of the following questions af- 
ter the new domain knowledge has been acquired: 
QI: What factors have an impact on the interest 
rate? 
Q2: What happens with the employment when the 
economic growth rises? 
Q3: How does deflation influence prices? 
"'"'"-... J 
ISA " ~ 
Figure 6: A sample of concepts and relations acquired from the 
5000 sentence corpus. Legend: continue lines represent influence 
inverse proportionally, dashed lines represent influence direct 
proportionally, and dotted lines represent influence (the direction 
of the relationship was not specified in the text). 
Figure 6 shows a portion of the new domain 
knowledge that is relevant o these questions. The 
first question can be easily answered by extracting 
the relationships that point to the concept interest 
rate. The factors that influence the interest rate are 
Fed, inflation, economic growth, and employment. 
The last two questions ask for more detailed infor- 
mation about the complex relationship among these 
concepts. Following the path from the deflation con- 
cept up to prices, the system learns that deflation in- 
fluences direct proportionally real interest rate, and 
real interest rate has an inverse proportional impact 
on prices. Both these relationships came from the 
sentence: Thus, the deflation and the real interest 
rate are positively correlated, and so a higher real 
interest rate leads to falling prices. 
This method may be adapted to acquire infor- 
mation when the question concepts are not in the 
knowledge base. Procedures may be invoked to dis- 
cover these concepts and the relations in which they 
may be used. 
5 Conclus ions 
The knowledge acquisition technology described 
above is applicable to any domain, by simply select- 
ing appropriate seed concepts. We started with five 
concepts interest rate, stock market, inflation, eco- 
nomic growth, and employment and from a corpus 
of 5000 sentences we acquired a total of 362 con- 
cepts of which 319 contain the seeds and 43 relate 
to these via selected relations. There were 22 dis- 
tinct le:dco-syntactic patterns discovered used in 63 
instances. Most importantly, the new concepts can 
be integrated with an existing ontology. 
The method works in an interactive mode where 
the user accepts or declines concepts, patterns and 
relationships. The manual operation took on aver- 
age 40 minutes per seed for the 5000 sentence corpus. 
KAT is useful considering that most of the knowl- 
edge base construction today is done manually. 
Complex linguistic phenomena such as corefer- 
ence resolution, word sense disambiguation, and oth- 
ers have to be dealt with in order to increase the 
automation of the knowledge acquisition system. 
Without a good handling of these problems the re- 
sults are not always accurate and human interven- 
tion is necessary. 
Re ferences  
Christiane Fellbaum. WordNet - An Electronic Lezical 
Database, MIT Press, Cambridge, MA, 1998. 
Marti Hearst. Automated Discovery of WordNet Rela- 
tions. In WordNet: An Electronic Lezical Database 
and Some of its Applications, editor Fellbaum, C., 
MIT Press, Cambridge, MA, 1998. 
J. Kim and D. Moldovan. Acquisition of Linguistic 
Patterns for knowledge-based information extraction. 
IEEE Transactions on Knowledge and Data Engineer- 
ing 7(5): pages 713-724. 
R. MacGregor. A Description Classifier for the Predicate 
Calculus. Proceedings of the 12th National Conference 
on Artificial Intelligence (AAAI94), pp. 213-220, 1994. 
Stephen D. Richardson, William B. Dolan, Lucy Vander- 
wende. MindNet: acquiring and structuring seman- 
tic information from text. Proceedings of ACL-Coling 
1998, pages 1098-1102. 
Ellen Riloff. Automatically'Generating Extraction Pat- 
terns from Untagged Text. In Proceedings of the Thir- 
teenth National Conference on Artificial Intelligence, 
1044-1049. The AAAI Press/MIT Press. 
J.G. Schmolze and T. Lipkis. Classification in the KL- 
ONE knowledge representation system. Proceedings 
of 8th Int'l Joint Conference on Artificial Intelligence 
(IJCAI83), 1983. 
S. Soderland. Learning to extract ext-based informa- 
tion from the world wide web. In the Proceedings of 
the Third International Conference on Knowledge Dis- 
cover# and Data Mining (KDD-97). 
Text REtrieval Conference. http://trec.nist.gov 1999 
W.A. Woods. Understanding Subsumption and Taxon- 
omy: A Framework for Progress. In the Principles 
of Semantic Networks: Explorations in the Represen- 
tation of Knowledge, Morgan Kaufmann, San Mateo, 
Calif. 1991, pages 45-94. 
W.A. Woods. A Better way to Organize Knowledge. 
Technical Report of Sun Microsystems Inc., 1997. 
275 
 
			ffThe Role of Lexico-Semantic Feedback in Open-Domain Textual
Question-Answering
Sanda Harabagiu, Dan Moldovan
Marius Pas?ca, Rada Mihalcea, Mihai Surdeanu,
Ra?zvan Bunescu, Roxana G??rju, Vasile Rus and Paul Mora?rescu
Department of Computer Science and Engineering
Southern Methodist University
Dallas, TX 75275-0122
 
sanda  @engr.smu.edu
Abstract
This paper presents an open-domain
textual Question-Answering system
that uses several feedback loops to en-
hance its performance. These feedback
loops combine in a new way statistical
results with syntactic, semantic or
pragmatic information derived from
texts and lexical databases. The paper
presents the contribution of each feed-
back loop to the overall performance of
76% human-assessed precise answers.
1 Introduction
Open-domain textual Question-Answering
(Q&A), as defined by the TREC competitions1 ,
is the task of identifying in large collections of
documents a text snippet where the answer to
a natural language question lies. The answer
is constrained to be found either in a short (50
bytes) or a long (250 bytes) text span. Frequently,
keywords extracted from the natural language
question are either within the text span or in
its immediate vicinity, forming a text para-
graph. Since such paragraphs must be identified
throughout voluminous collections, automatic
and autonomous Q&A systems incorporate an
index of the collection as well as a paragraph
retrieval mechanism.
Recent results from the TREC evaluations
((Kwok et al, 2000) (Radev et al, 2000) (Allen
1The Text REtrieval Conference (TREC) is a series of
workshops organized by the National Institute of Standards
and Technology (NIST), designed to advance the state-of-
the-art in information retrieval (IR)
et al, 2000)) show that Information Retrieval (IR)
techniques alone are not sufficient for finding an-
swers with high precision. In fact, more and more
systems adopt architectures in which the seman-
tics of the questions are captured prior to para-
graph retrieval (e.g. (Gaizauskas and Humphreys,
2000) (Harabagiu et al, 2000)) and used later in
extracting the answer (cf. (Abney et al, 2000)).
When processing a natural language question two
goals must be achieved. First we need to know
what is the expected answer type; in other words,
we need to know what we are looking for. Sec-
ond, we need to know where to look for the an-
swer, e.g. we must identify the question keywords
to be used in the paragraph retrieval.
The expected answer type is determined based
on the question stem, e.g. who, where or how
much and eventually one of the question concepts,
when the stem is ambiguous (for example what),
as described in (Harabagiu et al, 2000) (Radev et
al., 2000) (Srihari and Li, 2000). However finding
question keywords that retrieve all candidate an-
swers cannot be achieved only by deriving some
of the words used in the question. Frequently,
question reformulations use different words, but
imply the same answer. Moreover, many equiv-
alent answers are phrased differently. In this pa-
per we argue that the answer to complex natural
language questions cannot be extracted with sig-
nificant precision from large collections of texts
unless several lexico-semantic feedback loops are
allowed.
In Section 2 we survey the related work
whereas in Section 3 we describe the feedback
loops that refine the search for correct answers.
Section 4 presents the approach of devising key-
word alternations whereas Section 5 details the
recognition of question reformulations. Section 6
evaluates the results of the Q&A system and Sec-
tion 7 summarizes the conclusions.
2 Related work
Mechanisms for open-domain textual Q&A were
not discovered in the vacuum. The 90s witnessed
a constant improvement of IR systems, deter-
mined by the availability of large collections of
texts and the TREC evaluations. In parallel, In-
formation Extraction (IE) techniques were devel-
oped under the TIPSTER Message Understand-
ing Conference (MUC) competitions. Typically,
IE systems identify information of interest in a
text and map it to a predefined, target represen-
tation, known as template. Although simple com-
binations of IR and IE techniques are not practical
solutions for open-domain textual Q&A because
IE systems are based on domain-specific knowl-
edge, their contribution to current open-domain
Q&A methods is significant. For example, state-
of-the-art Named Entity (NE) recognizers devel-
oped for IE systems were readily available to be
incorporated in Q&A systems and helped recog-
nize names of people, organizations, locations or
dates.
Assuming that it is very likely that the answer
is a named entity, (Srihari and Li, 2000) describes
a NE-supported Q&A system that functions quite
well when the expected answer type is one of the
categories covered by the NE recognizer. Un-
fortunately this system is not fully autonomous,
as it depends on IR results provided by exter-
nal search engines. Answer extractions based on
NE recognizers were also developed in the Q&A
presented in (Abney et al, 2000) (Radev et al,
2000) (Gaizauskas and Humphreys, 2000). As
noted in (Voorhees and Tice, 2000), Q&A sys-
tems that did not include NE recognizers per-
formed poorly in the TREC evaluations, espe-
cially in the short answer category. Some Q&A
systems, like (Moldovan et al, 2000) relied both
on NE recognizers and some empirical indicators.
However, the answer does not always belong
to a category covered by the NE recognizer. For
such cases several approaches have been devel-
oped. The first one, presented in (Harabagiu et
al., 2000), the answer type is derived from a large
answer taxonomy. A different approach, based on
statistical techniques was proposed in (Radev et
al., 2000). (Cardie et al, 2000) presents a method
of extracting answers as noun phrases in a novel
way. Answer extraction based on grammatical
information is also promoted by the system de-
scribed in (Clarke et al, 2000).
One of the few Q&A systems that takes into
account morphological, lexical and semantic al-
ternations of terms is described in (Ferret et al,
2000). To our knowledge, none of the cur-
rent open-domain Q&A systems use any feed-
back loops to generate lexico-semantic alterna-
tions. This paper shows that such feedback loops
enhance significantly the performance of open-
domain textual Q&A systems.
3 Textual Q&A Feedback Loops
Before initiating the search for the answer to a
natural language question we take into account
the fact that it is very likely that the same ques-
tion or a very similar one has been posed to the
system before, and thus those results can be used
again. To find such cached questions, we measure
the similarity to the previously processed ques-
tions and when a reformulation is identified, the
system returns the corresponding cached correct
answer, as illustrated in Figure 1.
When no reformulations are detected, the
search for answers is based on the conjecture that
the eventual answer is likely to be found in a
text paragraph that (a) contains the most repre-
sentative question concepts and (b) includes a tex-
tual concept of the same category as the expected
answer. Since the current retrieval technology
does not model semantic knowledge, we break
down this search into a boolean retrieval, based
on some question keywords and a filtering mech-
anism, that retains only those passages containing
the expected answer type. Both the question key-
words and the expected answer type are identified
by using the dependencies derived from the ques-
tion parse.
By implementing our own version of the pub-
licly available Collins parser (Collins, 1996), we
also learned a dependency model that enables the
mapping of parse trees into sets of binary rela-
tions between the head-word of each constituent
and its sibling-words. For example, the parse tree
of TREC-9 question Q210: ?How many dogs pull
a sled in the Iditarod ?? is:
JJ
S
Iditarod
VP
NP
PP
NP
NNPDTINNN
NP
DTVBPNNS
NP
manyHow
WRB
dogs pull a sled in the
For each possible constituent in a parse tree,
rules first described in (Magerman, 1995) and
(Jelinek et al, 1994) identify the head-child and
propagate the head-word to its parent. For the
parse of question Q210 the propagation is:
NP (sled)
DT NN DTIN
manyHow
WRB
dogs
NNSJJ
NP (dogs)
VBP
pull a sled in the Iditarod
NNP (Iditarod)
NP (Iditarod)
PP (Iditarod)
NP (sled)
VP (pull)
S (pull)
When the propagation is over, head-modifier
relations are extracted, generating the following
dependency structure, called question semantic
form in (Harabagiu et al, 2000).
dogs IditarodCOUNT pull sled
In the structure above, COUNT represents the
expected answer type, replacing the question stem
?how many?. Few question stems are unambigu-
ous (e.g. who, when). If the question stem is am-
biguous, the expected answer type is determined
by the concept from the question semantic form
that modifies the stem. This concept is searched
in an ANSWER TAXONOMY comprising several
tops linked to a significant number of WordNet
noun and verb hierarchies. Each top represents
one of the possible expected answer types imple-
mented in our system (e.g. PERSON, PRODUCT,
NUMERICAL VALUE, COUNT, LOCATION). We
encoded a total of 38 possible answer types.
In addition, the question keywords used for
paragraph retrieval are also derived from the ques-
tion semantic form. The question keywords are
organized in an ordered list which first enumer-
ates the named entities and the question quota-
tions, then the concepts that triggered the recogni-
tion of the expected answer type followed by all
adjuncts, in a left-to-right order, and finally the
question head. The conjunction of the keywords
represents the boolean query applied to the doc-
ument index. (Moldovan et al, 2000) details the
empirical methods used in our system for trans-
forming a natural language question into an IR
query.
Answer Semantic Form
No
No
Yes
Lexical 
Alternations
Semantic
Alternations
Question Semantic Form
Answer Logical Form
S-UNIFICATIONS
Expected Answer Type 
Question Logical Form
ABDUCTIVE   PROOF
in paragraph
No
Yes
No
Yes
LOOP 2
Filter out paragraph
Expected Answer Type
Question Keywords
Min<Number Paragraphs<Max No
LOOP 1Index
Yes LOOP 3
Yes
PARSE
	



Retrieval
Cached Questions
Cached Answers
    
Question
REFORMULATION
Figure 1: Feedbacks for the Answer Search.
It is well known that one of the disadvantages
of boolean retrieval is that it returns either too
many or too few documents. However, for ques-
tion answering, this is an advantage, exploited by
the first feedback loop represented in Figure 1.
Feedback loop 1 is triggered when the number of
retrieved paragraphs is either smaller than a min-
imal value or larger than a maximal value deter-
mined beforehand for each answer type. Alterna-
tively, when the number of paragraphs is within
limits, those paragraphs that do not contain at
least one concept of the same semantic category
as the expected answer type are filtered out. The
remaining paragraphs are parsed and their depen-
dency structures, called answer semantic forms,
are derived.
Feedback loop 2 illustrated in Figure 1 is acti-
vated when the question semantic form and the
answer semantic form cannot by unified. The uni-
fication involves three steps:
 Step 1: The recognition of the expected answer
type. The first step marks all possible concepts
that are answer candidates. For example, in the
case of TREC -9 question Q243: ?Where did the
ukulele originate ??, the expected answer type is
LOCATION. In the paragraph ?the ukulele intro-
duced from Portugal into the Hawaiian islands?
contains two named entities of the category LO-
CATION and both are marked accordingly.
 Step 2: The identification of the question con-
cepts. The second step identifies the question
words, their synonyms, morphological deriva-
tions or WordNet hypernyms in the answer se-
mantic form.
 Step 3: The assessment of the similarities of
dependencies. In the third step, two classes of
similar dependencies are considered, generating
unifications of the question and answer semantic
forms:Automatic Detection of Causal Relations for Question Answering
Roxana Girju
Computer Science Department
Baylor University
Waco, Texas
roxana@cs.baylor.edu
Abstract
Causation relations are a pervasive fea-
ture of human language. Despite this, the
automatic acquisition of causal informa-
tion in text has proved to be a difficult
task in NLP. This paper provides a method
for the automatic detection and extraction
of causal relations. We also present an
inductive learning approach to the auto-
matic discovery of lexical and semantic
constraints necessary in the disambigua-
tion of causal relations that are then used
in question answering. We devised a clas-
sification of causal questions and tested
the procedure on a QA system.
1 Introduction
The automatic detection of semantic information in
English texts is a very difficult task, as English is
highly ambiguous. However, there are many appli-
cations which can greatly benefit from in depth se-
mantic analysis of text. Question Answering is one
of them.
An important semantic relation for many applica-
tions is the causation relation. Although many com-
putational linguists focused their attention on this
semantic relation, they used hand-coded patterns to
extract causation information from text.
This work has been motivated by our desire to
analyze cause-effect questions that are currently be-
yond the state-of-the-art in QA technology. This pa-
per provides an inductive learning approach to the
automatic discovery of lexical and semantic con-
straints necessary in the disambiguation of verbal
causal relations. After a brief review of the previ-
ous work in Computational Linguistics on causation
in section 2, we present in section 3 a classification
of lexico-syntactic patterns that are used to express
causation in English texts and show the difficulties
involved in the automatic detection and extraction of
these patterns. A method for automatic detection of
causation patterns and validation of ambiguous ver-
bal lexico-syntactic patterns referring to causation is
proposed in section 4. Results are discussed in sec-
tion 5, and in section 6 the application of causal re-
lations in Question Answering is demonstrated.
2 Previous Work in Computational
Linguistics
Computational linguists have tried to tackle the no-
tion of causality in natural language focusing on lex-
ical and semantic constructions that can express this
relation.
Many previous studies have attempted to extract
implicit inter-sentential cause-effect relations from
text using knowledge-based inferences (Joskowiscz
et al 1989), (Kaplan 1991). These studies were
based on hand-coded, domain-specific knowledge
bases difficult to scale up for realistic applications.
More recently, other researchers (Garcia 1997),
(Khoo et al 2000) used linguistic patterns to iden-
tify explicit causation relations in text without any
knowledge-based inference. Garcia used French
texts to capture causation relationships through lin-
guistic indicators organized in a semantic model
which classifies causative verbal patterns. She found
25 causal relations with an approach based on the
?Force Dynamics? of Leonard Talmy claiming a pre-
cision of 85%.
Khoo at al. used predefined verbal linguistic pat-
terns to extract cause-effect information from busi-
ness and medical newspaper texts. They presented
a simple computational method based on a set of
partially parsed linguistic patterns that usually indi-
cate the presence of a causal relationship. The rela-
tionships were determined by exact matching on text
with a precision of about 68%.
3 How are causation relations expressed in
English?
Any causative construction involves two compo-
nents, the cause and its effect. For example:
?The bus fails to turn up. As a result, I am late
for a meeting?.(Comrie 1981)
Here the cause is represented by the bus?s failing
to turn up, and the effect by my being late for the
meeting.
In English, the causative constructions can be ex-
plicit or implicit. Usually, explicit causation pat-
terns can contain relevant keywords such as cause,
effect, consequence, but also ambiguous ones such
as generate, induce, etc. The implicit causative con-
structions are more complex, involving inference
based on semantic analysis and background knowl-
edge. The English language provides a multitude of
cause-effect expressions that are very productive. In
this paper we focus on explicit but ambiguous verbal
causation patterns and provide a detailed computa-
tional analysis. A list of other causation expressions
were presented in detail elsewhere (Girju 2002).
Causation verbs
Many linguists focused their attention on causative
verbal constructions that can be classified based on
a lexical decomposition. This decomposition builds
a taxonomy of causative verbs according to whether
they define only the causal link or the causal link
plus other components of the two entities that are
causally related (Nedjalkov and Silnickij 1969):
1. Simple causatives (cause, lead to, bring about,
generate, make, force, allow, etc.)
Here the linking verb refers only to the causal
link, being synonymous with the verb cause. E.g.,
?Earthquakes generate tidal waves.?
2. Resultative causatives (kill, melt, dry, etc.)
These verbs refer to the causal link plus a part of the
resulting situation.
3. Instrumental causatives (poison (killing by poi-
soning), hang, punch, clean, etc.)
These causatives express a part of the causing event
as well as the result.
4 Automatic detection of causation
relationships
In this section we describe a method for automatic
detection of lexico-syntactic patterns that express
causation.
The algorithm consists of two major procedures.
The first procedure discovers lexico-syntactic pat-
terns that can express the causation relation, and the
second procedure presents an inductive learning ap-
proach to the automatic detection of syntactic and
semantic constraints on the constituent components.
4.1 Automatic discovery of lexico-syntactic
patterns referring to causation
One of the most frequent explicit intra-sentential
patterns that can express causation is  
	

. In this paper we focus on this kind of pat-
terns, where the verb is a simple causative.
In order to catch the most frequently used lexico-
syntactic patterns referring to causation, we used the
following procedure (Hearst 1998):
Discovery of lexico-syntactic patterns:
Input: semantic relation R
Output: lexico-syntactic patterns expressing R
STEP 1. Pick a semantic relation R (in this paper,
CAUSATION)
STEP 2. Pick a pair of noun phrases  ,  among
which R holds.
Since CAUSE-TO is one of the semantic relations
explicitly used in WordNet, this is an excellent re-
source for picking  and  . The CAUSE-TO rela-
tion is a transitive relation between verb synsets. For
example, in WordNet the second sense of the verb
develop is ?causes to grow?. Although WordNet
contains numerous causation relationships between
nouns that are always true, they are not directly men-
tioned. One way to determine such relationships is
to look for all patterns   ff  	

Models for the Semantic Classification of Noun Phrases
Dan Moldovan, Adriana Badulescu,
Marta Tatu, and Daniel Antohe
Computer Science Department
University of Texas at Dallas
Dallas, Texas
moldovan@utdallas.edu
Roxana Girju
Department of Computer Science
Baylor University
Waco, Texas
girju@cs.baylor.edu
Abstract
This paper presents an approach for detecting
semantic relations in noun phrases. A learning
algorithm, called semantic scattering, is used
to automatically label complex nominals, gen-
itives and adjectival noun phrases with the cor-
responding semantic relation.
1 Problem description
This paper is about the automatic labeling of semantic
relations in noun phrases (NPs).
The semantic relations are the underlying relations be-
tween two concepts expressed by words or phrases. We
distinguish here between semantic relations and semantic
roles. Semantic roles are always between verbs (or nouns
derived from verbs) and other constituents (run quickly,
went to the store, computer maker), whereas semantic
relations can occur between any constituents, for exam-
ple in complex nominals (malaria mosquito (CAUSE)),
genitives (girl?s mouth (PART-WHOLE)), prepositional
phrases attached to nouns (man at the store (LOCATIVE)),
or discourse level (The bus was late. As a result, I missed
my appointment (CAUSE)). Thus, in a sense, semantic
relations are more general than semantic roles and many
semantic role types will appear on our list of semantic
relations.
The following NP level constructions are consid-
ered here (cf. the classifications provided by (Quirk
et al1985) and (Semmelmeyer and Bolander 1992)):
(1) Compound Nominals consisting of two consecutive
nouns (eg night club - a TEMPORAL relation - indicat-
ing that club functions at night), (2) Adjective Noun con-
structions where the adjectival modifier is derived from a
noun (eg musical clock - a MAKE/PRODUCE relation), (3)
Genitives (eg the door of the car - a PART-WHOLE rela-
tion), and (4) Adjective phrases (cf. (Semmelmeyer and
Bolander 1992)) in which the modifier noun is expressed
by a prepositional phrase which functions as an adjective
(eg toy in the box - a LOCATION relation).
Example: ?Saturday?s snowfall topped a one-day record
in Hartford, Connecticut, with the total of 12.5 inches,
the weather service said. The storm claimed its fatal-
ity Thursday, when a car which was driven by a college
student skidded on an interstate overpass in the moun-
tains of Virginia and hit a concrete barrier, police said?.
(www.cnn.com - ?Record-setting Northeast snowstorm
winding down?, Sunday, December 7, 2003).
There are several semantic relations at the noun phrase
level: (1) Saturday?s snowfall is a genitive encoding a
TEMPORAL relation, (2) one-day record is a TOPIC noun
compound indicating that record is about one-day snow-
ing - an ellipsis here, (3) record in Hartford is an adjective
phrase in a LOCATION relation, (4) total of 12.5 inches
is an of-genitive that expresses MEASURE, (5) weather
service is a noun compound in a TOPIC relation, (6) car
which was driven by a college student encodes a THEME
semantic role in an adjectival clause, (7) college student is
a compound nominal in a PART-WHOLE/MEMBER-OF re-
lation, (8) interstate overpass is a LOCATION noun com-
pound, (9) mountains of Virginia is an of-genitive show-
ing a PART-WHOLE/PLACE-AREA and LOCATION rela-
tion, (10) concrete barrier is a noun compound encoding
PART-WHOLE/STUFF-OF.
1.1 List of Semantic Relations
After many iterations over a period of time we identified a
set of semantic relations that cover a large majority of text
semantics. Table 1 lists these relations, their definitions,
examples, and some references. Most of the time, the
semantic relations are encoded by lexico-syntactic pat-
terns that are highly ambiguous. One pattern can express
a number of semantic relations, its disambiguation be-
ing provided by the context or world knowledge. Often
semantic relations are not disjoint or mutually exclusive,
two or more appearing in the same lexical construct. This
is called semantic blend (Quirk et al1985). For example,
the expression ?Texas city? contains both a LOCATION as
well as a PART-WHOLE relation.
Other researchers have identified other sets of seman-
tic relations (Levi 1979), (Vanderwende 1994), (Sowa
1994), (Baker, Fillmore, and Lowe 1998), (Rosario and
Hearst 2001), (Kingsbury, et al 2002), (Blaheta and
Charniak 2000), (Gildea and Jurafsky 2002), (Gildea
and Palmer 2002). Our list contains the most frequently
used semantic relations we have observed on a large cor-
pus.
Besides the work on semantic roles, considerable in-
terest has been shown in the automatic interpretation of
complex nominals, and especially of compound nomi-
nals. The focus here is to determine the semantic re-
lations that hold between different concepts within the
same phrase, and to analyze the meaning of these com-
pounds. Several approaches have been proposed for em-
pirical noun-compound interpretation, such as syntactic
analysis based on statistical techniques (Lauer and Dras
1994), (Pustejovsky et al 1993). Another popular ap-
proach focuses on the interpretation of the underlying se-
mantics. Many researchers that followed this approach
relied mostly on hand-coded rules (Finin 1980), (Van-
derwende 1994). More recently, (Rosario and Hearst
2001), (Rosario, Hearst, and Fillmore 2002), (Lapata
2002) have proposed automatic methods that analyze and
detect noun compounds relations from text. (Rosario and
Hearst 2001) focused on the medical domain making use
of a lexical ontology and standard machine learning tech-
niques.
2 Approach
2.1 Basic Approach
We approach the problem top-down, namely identify
and study first the characteristics or feature vectors of
each noun phrase linguistic pattern, then develop mod-
els for their semantic classification. This is in contrast to
our prior approach ( (Girju, Badulescu, and Moldovan
2003a)) when we studied one relation at a time, and
learned constraints to identify only that relation. We
study the distribution of the semantic relations across dif-
ferent NP patterns and analyze the similarities and dif-
ferences among resulting semantic spaces. We define a
semantic space as the set of semantic relations an NP con-
struction can encode. We aim at uncovering the general
aspects that govern the NP semantics, and thus delineate
the semantic space within clusters of semantic relations.
This process has the advantage of reducing the annotation
effort, a time consuming activity. Instead of manually an-
notating a corpus for each semantic relation, we do it only
for each syntactic pattern and get a clear view of its se-
mantic space. This syntactico-semantic approach allows
us to explore various NP semantic classification models
in a unified way.
This approach stemmed from our desire to answer
questions such as:
1. What influences the semantic interpretation of various
linguistic constructions?
2. Is there only one interpretation system/model that
works best for all types of expressions at all syntactic lev-
els? and
3. What parameters govern the models capable of seman-
tic interpretation of various syntactic constructions?
2.2 Semantic Relations at NP level
It is well understood and agreed in linguistics that con-
cepts can be represented in many ways using various con-
structions at different syntactic levels. This is in part why
we decided to take the syntactico-semantic approach that
analyzes semantic relations at different syntactic levels
of representation. In this paper we focus only on the be-
havior of semantic relations at NP level. A thorough un-
derstanding of the syntactic and semantic characteristics
of NPs provides valuable insights into defining the most
representative feature vectors that ultimately drive the
discriminating learning models.
Complex Nominals
Levi (Levi 1979) defines complex nominals (CNs) as ex-
pressions that have a head noun preceded by one or more
modifying nouns, or by adjectives derived from nouns
(usually called denominal adjectives). Most importantly
for us, each sequence of nouns, or possibly adjectives and
nouns, has a particular meaning as a whole carrying an
implicit semantic relation; for example, ?spoon handle?
(PART-WHOLE) or ?musical clock? (MAKE/PRODUCE).
CNs have been studied intensively in linguistics,
psycho-linguistics, philosophy, and computational lin-
guistics for a long time. The semantic interpretation
of CNs proves to be very difficult for a number of rea-
sons. (1) Sometimes the meaning changes with the
head (eg ?musical clock? MAKE/PRODUCE, ?musical cre-
ation? THEME), other times with the modifier (eg ?GM
car? MAKE/PRODUCE, ?family car? POSSESSION). (2)
CNs? interpretation is knowledge intensive and can be id-
iosyncratic. For example, in order to interpret correctly
?GM car? we have to know that GM is a car-producing
company. (3) There can be many possible semantic re-
lations between a given pair of word constituents. For
example, ?USA city? can be regarded as a LOCATION as
well as a PART-WHOLE relation. (4) Interpretation of CNs
can be highly context-dependent. For example, ?apple
juice seat? can be defined as ?seat with apple juice on the
table in front of it? (cf. (Downing 1977)).
Genitives
The semantic interpretation of genitive constructions
No. Semantic Definition / Example
Relation
1 POSSESSION an animate entity possesses (owns) another entity; (family estate; the girl has a new car.), (Vanderwende 1994)
2 KINSHIP an animated entity related by blood, marriage, adoption or strong affinity to another animated entity; (Mary?s daughter;
my sister); (Levi 1979)
3 PROPERTY/ characteristic or quality of an entity/event/state; (red rose; The thunderstorm was awful.); (Levi 1979)
ATTRIBUTE-HOLDER
4 AGENT the doer or instigator of the action denoted by the predicate;
(employee protest; parental approval; The king banished the general.); (Baker, Fillmore, and Lowe 1998)
5 TEMPORAL time associated with an event; (5-o?clock tea; winter training; the store opens at 9 am),
includes DURATION (Navigli and Velardi 2003),
6 DEPICTION- an event/action/entity depicting another event/action/entity; (A picture of my niece.),
DEPICTED
7 PART-WHOLE an entity/event/state is part of another entity/event/state (door knob; door of the car),
(MERONYMY) (Levi 1979), (Dolan et al 1993),
8 HYPERNYMY an entity/event/state is a subclass of another; (daisy flower; Virginia state; large company, such as Microsoft)
(IS-A) (Levi 1979), (Dolan et al 1993)
9 ENTAIL an event/state is a logical consequence of another; (snoring entails sleeping)
10 CAUSE an event/state makes another event/state to take place; (malaria mosquitoes; to die of hunger; The earthquake
generated a Tsunami), (Levi 1979)
11 MAKE/PRODUCE an animated entity creates or manufactures another entity; (honey bees; nuclear power plant; GM makes cars) (Levi 1979)
12 INSTRUMENT an entity used in an event/action as instrument; (pump drainage; the hammer broke the box) (Levi 1979)
13 LOCATION/SPACE spatial relation between two entities or between an event and an entity; includes DIRECTION; (field mouse;
street show; I left the keys in the car), (Levi 1979), (Dolan et al 1993)
14 PURPOSE a state/action intended to result from a another state/event; (migraine drug; wine glass; rescue mission;
He was quiet in order not to disturb her.) (Navigli and Velardi 2003)
15 SOURCE/FROM place where an entity comes from; (olive oil; I got it from China) (Levi 1979)
16 TOPIC an object is a topic of another object; (weather report; construction plan; article about terrorism); (Rosario and Hearst 2001)
17 MANNER a way in which an event is performed or takes place; (hard-working immigrants; enjoy immensely; he died of
cancer); (Blaheta and Charniak 2000)
18 MEANS the means by which an event is performed or takes place; (bus service; I go to school by bus.) (Quirk et al1985)
19 ACCOMPANIMENT one/more entities accompanying another entity involved in an event; (meeting with friends; She came with us) (Quirk et al1985)
20 EXPERIENCER an animated entity experiencing a state/feeling; (Mary was in a state of panic.); (Sowa 1994)
21 RECIPIENT an animated entity for which an event is performed; (The eggs are for you) ; includes BENEFICIARY; (Sowa 1994)
22 FREQUENCY number of occurrences of an event; (bi-annual meeting; I take the bus every day); (Sowa 1994)
23 INFLUENCE an entity/event that affects other entity/event; (drug-affected families; The war has an impact on the economy.);
24 ASSOCIATED WITH an entity/event/state that is in an (undefined) relation with another entity/event/state; (Jazz-associated company;)
25 MEASURE an entity expressing quantity of another entity/event; (cup of sugar;
70-km distance; centennial rite; The jacket cost $60.)
26 SYNONYMY a word/concept that means the same or nearly the same as another word/concept;
(NAME) (Marry is called Minnie); (Sowa 1994)
27 ANTONYMY a word/concept that is the opposite of another word/concept; (empty is the opposite of full); (Sowa 1994)
28 PROBABILITY OF the quality/state of being probable; likelihood
EXISTENCE (There is little chance of rain tonight); (Sowa 1994)
29 POSSIBILITY the state/condition of being possible; (I might go to Opera tonight); (Sowa 1994)
30 CERTAINTY the state/condition of being certain or without doubt; (He definitely left the house this morning);
31 THEME an entity that is changed/involved by the action/event denoted by the predicate;
(music lover; John opened the door.); (Sowa 1994)
32 RESULT the inanimate result of the action/event denoted by the predicate; includes EFFECT and PRODUCT.
(combustion gases; I finished the task completely.); (Sowa 1994)
33 STIMULUS stimulus of the action or event denoted by the predicate (We saw [the painting].
I sensed [the eagerness] in him. I can see [that you are feeling great].) (Baker, Fillmore, and Lowe 1998)
34 EXTENT the change of status on a scale (by a percentage or by a value) of some entity;
(The price of oil increased [ten percent]. Oil?s price increased by [ten percent]. ); (Blaheta and Charniak 2000)
35 PREDICATE expresses the property associated with the subject or the object through the verb;
(He feels [sleepy]. They elected him [treasurer]. ) (Blaheta and Charniak 2000)
Table 1: A list of semantic relations at various syntactic levels (including NP level), their definitions, some examples,
and references.
is considered problematic by linguists because they
involve an implicit relation that seems to allow for
a large variety of relational interpretations; for ex-
ample: ?John?s car?-POSSESSOR-POSSESSEE, ?Mary?s
brother?-KINSHIP, ?last year?s exhibition?-TEMPORAL,
?a picture of my nice?-DEPICTION-DEPICTED, and ?the
desert?s oasis?-PART-WHOLE/PLACE-AREA. A charac-
teristic of these constructions is that they are very pro-
ductive, as the construction can be given various inter-
pretations depending on the context. One such example
is ?Kate?s book? that can mean the book Kate owns, the
book Kate wrote, or the book Kate is very fond of.
Thus, the features that contribute to the semantic in-
terpretation of genitives are: the nouns? semantic classes,
the type of genitives, discourse and pragmatic informa-
tion.
Adjective Phrases are prepositional phrases attached to
nouns acting as adjectives (cf. (Semmelmeyer and
Bolander 1992)). Prepositions play an important role
both syntactically and semantically. Semantically speak-
ing, prepositional constructions can encode various se-
mantic relations, their interpretations being provided
most of the time by the underlying context. For instance,
the preposition ?with? can encode different semantic re-
lations: (1) It was the girl with blue eyes (MERONYMY),
(2) The baby with the red ribbon is cute (POSSESSION),
(3) The woman with triplets received a lot of attention
(KINSHIP).
The conclusion for us is that in addition to the nouns se-
mantic classes, the preposition and the context play im-
portant roles here.
In order to focus our research, we will concentrate for
now only on noun - noun or adjective - noun composi-
tional constructions at NP level, ie those whose mean-
ing can be derived from the meaning of the constituent
nouns (?door knob?, ?cup of wine?). We don?t consider
metaphorical names (eg, ?ladyfinger?), metonymies (eg,
?Vietnam veteran?), proper names (eg, ?John Doe?), and
NPs with coordinate structures in which neither noun is
the head (eg, ?player-coach?). However, we check if
the constructions are non-compositional (lexicalized) (the
meaning is a matter of convention; e.g., ?soap opera?,
?sea lion?), but only for statistical purposes. Fortunately,
some of these can be identified with the help of lexicons.
2.3 Corpus Analysis at NP level
In order to provide a unified approach for the detection of
semantic relations at different NP levels, we analyzed the
syntactic and semantic behavior of these constructions on
a large open-domain corpora of examples. Our intention
is to answer questions like: (1) What are the semantic re-
lations encoded by the NP-level constructions?, (2) What
is their distribution on a large corpus?, (3) Is there a com-
mon subset of semantic relations that can be fully para-
phrased by all types of NP constructions?, (4) How many
NPs are lexicalized?
The data
We have assembled a corpus from two sources: Wall
Street Journal articles from TREC-9, and eXtended
WordNet glosses (XWN) (http://xwn.hlt.utdallas.edu).
We used XWN 2.0 since all its glosses are syntacti-
cally parsed and their words semantically disambiguated
which saved us considerable amount of time. Table 2
shows for each syntactic category the number of ran-
domly selected sentences from each corpus, the num-
ber of instances found in these sentences, and finally the
number of instances that our group managed to annotate
by hand. The annotation of each example consisted of
specifying its feature vector and the most appropriate se-
mantic relation from those listed in Table 1.
Inter-annotator Agreement
The annotators, four PhD students in Computational Se-
mantics worked in groups of two, each group focusing
on one half of the corpora to annotate. Noun - noun
(adjective - noun, respectively) sequences of words were
extracted using the Lauer heuristic (Lauer 1995) which
looks for consecutive pairs of nouns that are neither
preceded nor succeeded by a noun after each sentence
was syntactically parsed with Charniak parser (Charniak
2001) (for XWN we used the gold parse trees). More-
over, they were provided with the sentence in which the
pairs occurred along with their corresponding WordNet
senses. Whenever the annotators found an example en-
coding a semantic relation other than those provided or
they didn?t know what interpretation to give, they had
to tag it as ?OTHERS?. Besides the type of relation, the
annotators were asked to provide information about the
order of the modifier and the head nouns in the syntac-
tic constructions if applicable. For instance, in ?owner
of car?-POSSESSION the possessor owner is followed by
the possessee car, while in ?car of John?-POSSESSION/R
the order is reversed. On average, 30% of the training
examples had the nouns in reverse order.
Most of the time, one instance was tagged with one
semantic relation, but there were also situations in which
an example could belong to more than one relation in the
same context. For example, the genitive ?city of USA?
was tagged as a PART-WHOLE/PLACE-AREA relation and
as a LOCATION relation. Overall, there were 608 such
cases in the training corpora. Moreover, the annotators
were asked to indicate if the instance was lexicalized or
not. Also, the judges tagged the NP nouns in the training
corpus with their corresponding WordNet senses.
The annotators? agreement was measured using the
Kappa statistics, one of the most frequently used mea-
sure of inter-annotator agreement for classification tasks:
 
	



, where ffSupport Vector Machines Applied to the Classification of Semantic Relations
in Nominalized Noun Phrases
Roxana Girju
Computer Science Department
Baylor University
Waco, Texas
girju@cs.baylor.edu
Ana-Maria Giuglea, Marian Olteanu,
Ovidiu Fortu, Orest Bolohan, and
Dan Moldovan
Department of Computing Science
University of Texas at Dallas
Dallas, Texas
moldovan@utdallas.edu
Abstract
The discovery of semantic relations in text
plays an important role in many NLP appli-
cations. This paper presents a method for the
automatic classification of semantic relations
in nominalized noun phrases. Nominalizations
represent a subclass of NP constructions in
which either the head or the modifier noun is
derived from a verb while the other noun is an
argument of this verb. Especially designed fea-
tures are extracted automatically and used in a
Support Vector Machine learning model. The
paper presents preliminary results for the se-
mantic classification of the most representative
NP patterns using four distinct learning mod-
els.
1 Introduction
1.1 Problem description
The automatic identification of semantic relations in text
has become increasingly important in Information Ex-
traction, Question Answering, Summarization, Text Un-
derstanding, and other NLP applications. This paper dis-
cusses the automatic labeling of semantic relations in
nominalized noun phrases (NPs) using a support vector
machines learning algorithm.
Based on the classification provided by the New Web-
ster?s Grammar Guide (Semmelmeyer and Bolander
1992) and our observations of noun phrase patterns on
large text collections, the most frequently occurring NP
level constructions are: (1) Compound Nominals consist-
ing of two consecutive nouns (eg pump drainage - an IN-
STRUMENT relation), (2) Adjective Noun constructions
where the adjectival modifier is derived from a noun (eg
parental refusal - AGENT), (3) Genitives (eg tone of con-
versation - a PROPERTY relation), (4) Adjective phrases
in which the modifier noun is expressed by a preposi-
tional phrase which functions as an adjective (eg amuse-
ment in the park - a LOCATION relation), and (5) Adjec-
tive clauses where the head noun is modified by a relative
clause (eg the man who was driving the car - an AGENT
relation between man and driving).
1.2 Previous work on the discovery of semantic
relations
The development of large semantically annotated cor-
pora, such as Penn Treebank2 and, more recently, Prop-
Bank (Kingsbury, et al 2002), as well as semantic
knowledge bases, such as FrameNet (Baker, Fillmore,
and Lowe 1998), have stimulated a high interest in the
automatic acquisition of semantic relations, and espe-
cially of semantic roles. In the last few years, many re-
searchers (Blaheta and Charniak 2000), (Gildea and Ju-
rafsky 2002), (Gildea and Palmer 2002), (Pradhan et
al. 2003) have focused on the automatic prediction of se-
mantic roles using statistical techniques. These statistical
techniques operate on the output of probabilistic parsers
and take advantage of the characteristic features of the
semantic roles that are then employed in a learning algo-
rithm.
While these systems focus on verb-argument semantic
relations, called semantic roles, in this paper we inves-
tigate predicate-argument semantic relations in nominal-
ized noun phrases and present a method for their auto-
matic detection in open-text.
1.3 Approach
We approach the problem top-down, namely identify and
study first the characteristics or feature vectors of each
noun phrase linguistic pattern and then develop models
for their semantic classification. The distribution of the
semantic relations is studied across different NP patterns
and the similarities and differences among resulting se-
mantic spaces are analyzed. A thorough understanding
of the syntactic and semantic characteristics of NPs pro-
vides valuable insights into defining the most representa-
tive feature vectors that ultimately drive the discriminat-
ing learning models.
An important characteristic of this work is that it re-
lies heavily on state-of-the-art natural language process-
ing and machine learning methods. Prior to the discovery
of semantic relations, the text is syntactically parsed with
Charniak?s parser (Charniak 2001) and words are seman-
tically disambiguated and mapped into their appropriate
WordNet senses. The word sense disambiguation is done
manually for training and automatically for testing with
a state-of-the-art WSD module, an improved version of
a system with which we have participated successfully
in Senseval 2 and which has an accuracy of 81% when
disambiguating nouns in open-domain. The discovery of
semantic relations is based on learning lexical, syntactic,
semantic and contextual constraints that effectively iden-
tify the most probable relation for each NP construction
considered.
2 Semantic Relations in Nominalized Noun
Phrases
In this paper we study the behavior of semantic relations
at the noun phrase level when one of the nouns is nom-
inalized. The following NP level constructions are con-
sidered: complex nominals, genitives, adjective phrases,
and adjective clauses.
Complex Nominals
Levi (Levi 1979) defines complex nominals (CNs) as ex-
pressions that have a head noun preceded by one or more
modifying nouns, or by adjectives derived from nouns
(usually called denominal adjectives). Each sequence of
nouns, or possibly adjectives and nouns, has a particular
meaning as a whole carrying an implicit semantic rela-
tion; for example, ?parental refusal? (AGENT).
The main tasks are the recognition, and the interpre-
tation of complex nominals. The recognition task deals
with the identification of CN constructions in text, while
the interpretation of CNs focuses on the detection and
classification of a comprehensive set of semantic rela-
tions between the noun constituents.
Genitives
In English there are two kinds of genitives; in one, the
modifier is morphologically linked to the possessive clitic
?s and precedes the head noun (s-genitive e.g. ?John?s
conclusion?), and in the second one the modifier is syn-
tactically marked by the preposition of and follows the
head noun (of-genitive, e.g. ?declaration of indepen-
dence?).
Adjective Phrases are prepositional phrases attached to
nouns and act as adjectives (cf. (Semmelmeyer and
Bolander 1992)). Prepositions play an important role
both syntactically and semantically ( (Dorr 1997). Prepo-
sitional constructions can encode various semantic re-
lations, their interpretations being provided most of the
time by the underlying context. For instance, the preposi-
tion ?with? can encode different semantic relations: (1) It
was the girl with blue eyes (MERONYMY), (2) The baby
with the red ribbon is cute (POSSESSION), (3) The woman
with triplets received a lot of attention (KINSHIP).
The conclusion for us is that in addition to the nouns se-
mantic classes, the preposition and the context play im-
portant roles here.
Adjective Clauses are subordinate clauses attached to
nouns (cf. (Semmelmeyer and Bolander 1992)). Often
they are introduced by a relative pronoun/adverb (ie that,
which, who, whom, whose, where) as in the following ex-
amples: (1) Here is the book which I am reading (book
is the THEME of reading) (2) The man who was driving
the car was a spy (man is the AGENT of driving). Adjec-
tive clauses are inherently verb-argument structures, thus
their interpretation consists of detecting the semantic role
between the head noun and the main verb in the relative
clause. This is addressed below.
3 Nominalizations and Mapping of NPs
into Grammatical Role Structures
3.1 Nominalizations
A further analysis of various examples of noun - noun
pairs encoded by the first three major types of NP-level
constructions shows the need for a different taxonomy
based on the syntactic and grammatical roles the con-
stituents have in relation to each other. The criterion in
this classification splits the noun - noun examples (re-
spectively, adjective - noun examples in complex nom-
inals) into nominalizations and non-nominalizations.
Nominalizations represent a particular subclass of NP
constructions that in general have ?a systematic corre-
spondence with a clause structure? (Quirk et al1985).
The head or modifier noun is derived from a verb while
the other noun (the modifier, or respectively, the head) is
interpreted as an argument of this verb. For example, the
noun phrase ?car owner? corresponds to ?he owns a car?.
The head noun owner is morphologically related to the
verb own. Otherwise said, the interpretation of this class
of NPs is reduced to the automatic detection and inter-
pretation of semantic roles mapped on the corresponding
verb-argument structure.
As in (Hull and Gomez 1996), in this paper we use
the term nominalization to refer only to those senses of
the nominalized nouns which are derived from verbs.
For example, the noun ?decoration? has three senses in
WordNet 2.0: an ornament (#1), a medal (#2), and the act
of decorating (#3). Only the last sense is a nominaliza-
tion. However, there are more complex situations when
the underlying verb has more than one sense that refers to
an action/event. This is the case of ?examination? which
has five senses of which four are action-related. In this
case, the selection of the correct sense is provided by the
context.
We are interested in answering the following ques-
tions: (1) What is the best set of features that can capture
the meaning of noun - noun nominalization pairs for each
NP-level construction? and (2) What is the semantic be-
havior of nominalization constructions across NP levels?
3.2 Taxonomy of nominalizations
Deverbal vs verbal noun.
(Quirk et al1985) generally classify nominalizations
based on the morphological formation of the nominal-
ized noun. They distinguish between deverbal nouns, i.e.
those derived from the underlying verb through word for-
mation; e.g., ?student examination?, and verbal nouns,
i.e. those derived from the verb by adding the gerund
suffix ?-ing?; e.g.: ?cleaning woman?. Most of the time,
verbal nouns are derived from verbs which don?t have a
deverbal correspondent.
Table 1 shows the mapping of the first three major syn-
tactic NP constructions to the grammatical role level. By
analyzing a large corpus, we have observed that Quirk?s
grammatical roles shown in Table 1 are not uniformly dis-
tributed over the types of NP-constructions. For example,
the ?
 	 
Learning Semantic Constraints for
the Automatic Discovery of Part-Whole Relations
Roxana Girju
Human Language Technology
Research Institute,
University of Texas at Dallas
and
Computer Science Department,
Baylor University
girju@ecs.baylor.edu
Adriana Badulescu and Dan Moldovan
Human Language Technology
Research Institute,
University of Texas at Dallas
adriana@hlt.utdallas.edu,
moldovan@utdallas.edu
Abstract
The discovery of semantic relations from text
becomes increasingly important for applica-
tions such as Question Answering, Informa-
tion Extraction, Text Summarization, Text Un-
derstanding, and others. The semantic rela-
tions are detected by checking selectional con-
straints. This paper presents a method and its
results for learning semantic constraints to de-
tect part-whole relations. Twenty constraints
were found. Their validity was tested on a
10,000 sentence corpus, and the targeted part-
whole relations were detected with an accuracy
of 83%.
1 Introduction
1.1 Problem description
An important semantic relation for several NLP applica-
tions is the part-whole relation, or meronymy. Consider
the text:
The car?s mail messenger is busy
at work in the mail car as the
train moves along. Through the
open side door of the car, moving
scenery can be seen. The worker
is alarmed when he hears an unusual
sound. He peeks through the door?s
keyhole leading to the tender and
locomotive cab and sees the two ban-
dits trying to break through the
express car door.
There are six part-whole relations in this text: 1) the
mail car is part of the train, 2) the side door is part of
the car, 3) the keyhole is part of the door, 4) the cab is
part of the locomotive, 5) the door is part of the car, and
6) the car is part of the express train (the last two in the
compound noun express car door).
Understanding part-whole relations allows Question
Answering systems to address questions such as ?What
are the components of X?, What is X made of? and others.
Question Answering, Information Extraction and Text
Summarization systems often need to identify relations
between entities as well as synthesize information gath-
ered from multiple documents. More and more knowl-
edge intensive techniques are used to augment statistical
methods when building advanced NLP applications.
This paper provides a method for deriving semantic
constraints necessary to discover part-whole relations.
1.2 Semantics of part-whole relation
There are different ways in which we refer to something
as being a part of something else, and this led many re-
searchers to claim that meronymy is a complex relation
that ?should be treated as a collection of relations, not as
a single relation? (Iris et al , 1988).
Based on linguistic and cognitive considerations about
the way parts contribute to the structure of the wholes,
Winston, Chaffin and Hermann (Winston et al ,
1987) determined in 1987 six types of part-whole rela-
tions: Component-Integral object (wheel - car), Member-
Collection (soldier - army), Portion-Mass (meter - kilo-
meter), Stuff-Object (alcohol - wine), Feature-Activity
(paying - shopping), and Place-Area (oasis - desert).
The part-whole relations in WordNet are classified into
three basic types: Member-of (e.g., UK IS-MEMBER-OF
NATO), Stuff-of (e.g., carbon IS-STUFF-OF coal), and
all other part-whole relations grouped under the general
name of Part-of (e.g., leg IS-PART-OF table). In this paper
we lump together all the part-whole relation types, but if
necessary, one can train the system separately on each of
the six meronymy types to increase the learning accuracy.
1.3 Previous work
Although part-whole relations were studied by philoso-
phers, logicians, psychologists and linguists, not much
work has been done to automatically identify the
                                                               Edmonton, May-June 2003
                                                                 Main Papers , pp. 1-8
                                                         Proceedings of HLT-NAACL 2003
meronymy relation in text. Hearst (Hearst, 1998) de-
veloped a method for the automatic acquisition of hyper-
nymy relations by identifying a set of frequently used
and unambiguous lexico-syntactic patterns. Then, she
tried applying the same method to meronymy, but with-
out much success, as the patterns detected also expressed
other semantic relations.
In 1999, Berland and Charniak (Charniak, 1999) ap-
plied statistical methods on a very large corpus to find
part-whole relations. Using Hearst?s method, they fo-
cused on a small set of lexico-syntactic patterns that fre-
quently refer to meronymy and a list of 6 seeds represent-
ing whole objects. Their system?s output was an ordered
list of possible parts according to some statistical metrics.
The accuracy obtained for the first 50 parts was 55%.
2 Lexico-syntactic patterns expressing
meronymy
2.1 Variety of meronymy expressions
Since there are many ways in which something can
be part of something else, there is a variety of lexico-
syntactic structures that can express the meronymy se-
mantic relation. Expressions that reflect semantic rela-
tions are either explicit or implicit. The explicit ones are
further broken down into unambiguous and ambiguous.
A. Explicit part-whole constructions
There are unambiguous lexical expressions that always
convey a part-whole relation. For example:
The substance consists of two ingre-
dients.
The cloud was made of dust.
Iceland is a member of NATO.
In these cases the simple detection of the patterns leads
to the discovery of part-whole relations.
On the other hand, there are many ambiguous expres-
sions that are explicit but convey part-whole relations
only in some contexts. These expressions can be detected
only with complex semantic constraints.
Examples are:
The horn is part of the car.
(whereas ??He is part of the game?? is not
meronymic).
B. Implicit part-whole constructions
In addition to the explicit patterns, there are other
patterns that express part-whole relations implicitly.
Examples are: girl?s mouth, eyes of the
baby, door knob, oxygen-rich water,
high heel shoes.
2.2 An algorithm for finding lexico-syntactic
patterns
In order to identify lexical forms that express part-whole
relations, the following algorithm was used:
Step 1. Pick pairs of WordNet concepts
 
,
 
among
which there is a part-whole relation.
We selected 100 pairs of part-whole concepts that were
evenly distributed over all nine WordNet noun hierar-
chies.
Step 2. Extract lexico-syntactic patterns that link the two
selected concepts of each pair by searching a collection
of texts.
For each pair of part-whole concepts determined
above, search a collection of documents and retain only
the sentences containing that pair. We chose two dis-
tinct text collections: SemCor 1.7 and LA Times from
TREC-9. From each collection 10,000 sentences were
selected randomly. We manually inspected these sen-
tences and picked only those in which the pairs referred
to meronymy.
The result of this step is a list of lexico-syntactic ex-
pressions that reflect meronymy. From syntactic point of
view, these patterns can be classified in two major cate-
gories:
 Phrase-level patterns, where the part and whole
concepts are included in the same phrase. For exam-
ple, in the pattern ? 
	
 ? the noun phrase that
contains the part (X) and the prepositional phrase
that contains the whole (Y) form a noun phrase (NP).
Throughout this paper, X represents the part, and Y
represents the whole.
 Sentence-level patterns, where the part-whole rela-
tion is intrasentential. A frequent example is the pat-
tern ?   verb  	 ?.
From the 20,000 SemCor and LA Times sentences,
535 part-whole occurrences were detected. Of these
493 (92.15%) were phrase-level patterns and only
42 sentence-level patterns. There were 54 distinct
meronymic lexico-syntactic patterns, of which 36 phrase-
level patterns and 18 sentence-level patterns. The most
frequent phrase-level patterns were:
? 
 of  ? occurring 173 of 493 times or 35%;
? 
 ?s  ? occurring 71 of 493 times or 14%;
The most frequent sentence-level pattern was
? 
 Verb  ? occurring 18 of 42 times (43%).
These observations are consistent with the results in
(Evens et al , 1980). Based on these statistics, we
decided to focus in this paper only on the three patterns
above. The problem, however, is that these are some of
the most ambiguous part-whole relation patterns. For ex-
ample, in addition to meronymic relations, the genitives
can express POSSESSION (Mary?s toy), KINSHIP (Mary?s
brother), and many other relations. The same is true
for ?   Verb   ? patterns (?Kate has green eyes?
is meronymic, while ?Kate has a cat? is POSSESSION).
As it can be seen, the genitives and the have-verb pat-
terns are ambiguous. Thus we need some semantic con-
straints to differentiate the part-whole relations from the
other possible meanings these patterns may have.
3 Learning Semantic Constraints
3.1 Approach
The learning procedure proposed here is supervised, for
the learning algorithm is provided with a set of in-
puts along with the corresponding set of correct outputs.
Based on a set of positive and negative meronymic train-
ing examples provided and annotated by the user, the al-
gorithm creates a decision tree and a set of rules that clas-
sify new data. The rules produce constraints on the noun
constituents of the lexical patterns.
For the discovery of the semantic constraints we used
C4.5 decision tree learning (Quinlan, 1993). The learned
function is represented by a decision tree, or a set of if-
then rules. The decision tree learning searches a complete
hypothesis space from simple to complex hypotheses un-
til it finds a hypothesis consistent with the data. Its bias
is a preference for the shorter tree that places high in-
formation gain attributes closer to the root. The error in
the training examples can be overcome by using different
training and a test corpora, or by cross-validation tech-
niques.
C4.5 receives in general two input files, the NAMES
file defining the names of the attributes, attribute values
and classes, and the DATA file containing the examples.
The output of C4.5 consists of two types of files, the
TREE file containing the decision tree and some statis-
tics, and the RULES file containing the rules extracted
from the decision tree and some statistics for training and
test data. This last file also contains a default rule that is
usually used to classify unseen instances when no other
rule applies.
3.2 Preprocessing Part-Whole Lexico-Syntactic
Patterns
Since our constraint learning procedure is based on the
semantic information provided by WordNet, we need to
preprocess the noun phrases (NPs) extracted and identify
the part and the whole. For each NP we keep only the
largest word sequence (from left to right) that is defined
in WordNet as a concept.
For example, from the noun phrase ?brown carving
knife? the procedure retains only ?carving knife?, as it
is the WordNet concept with the largest number of words
in the noun phrase. For each such concept, we manually
annotate it with its corresponding sense in WordNet, for
example carving knife   1 means sense number 1.
3.3 Building the Training Corpus and the Test
Corpus
In order to learn the constraints, we used the SemCor 1.7
and TREC 9 text collections. From the first two sets of
the SemCor collection, 19,000 sentences were selected.
Another 100,000 sentences were extracted from the LA
Times articles of TREC 9. A corpus ?A? was thus created
from the selected sentences of each text collection. Each
sentence in this corpus was then parsed using the syntac-
tic parser developed by Charniak (Charniak, 2000).
Focusing only on the sentences containing relations in-
dicated by the three patterns considered, we manually
annotated all the noun phrases in the 53,944 relation-
ships matched by these patterns with their correspond-
ing senses in WordNet (with the exception of those from
SemCor). 6,973 of these relationships were part-whole
relations, while 46,971 were not meronymic relations.
We used for training a corpus of 34,609 positive exam-
ples (6,973 pairs of NPs in a part-whole relation extracted
from the corpus ?A? and 27,636 extracted from WordNet
as selected pairs) and 46,971 negative examples (the non-
part-whole relations extracted from corpus ?A?).
3.4 Learning Algorithm
Input: positive and negative meronymic examples of
pairs of concepts.
Output: semantic constraints on concepts.
Step 1. Generalize the training examples
Initially, the training corpus consists of examples that
have the following format:
 part#sense; whole#sense; target  ,
where target can be either ?Yes? or ?No?, as the rela-
tion between the part and whole is meronymy or not.
For example,  oasis   1; desert   1; Yes  in-
dicates that between oasis and desert there is a
meronymic relation.
From this initial set of examples an intermediate cor-
pus was created by expanding each example using the
following format:
 part#sense, class part#sense;
whole#sense, class whole#sense;
target  ,
where class part and class whole correspond to
the WordNet semantic classes of the part, respec-
tively whole concepts. For instance, the initial
example  aria   1; opera   1; Yes  be-
comes

aria   1, entity   1; opera   1,
abstraction   6; Yes  , as the part concept
aria   1 belongs to the entity   1 hierarchy in
WordNet and the whole concept opera   1 is part of the
abstraction   6 hierarchy.
From this intermediate corpus a generalized set of
training examples was built, retaining only the semantic
classes and the target value. At this point, the generalized
training corpus contains three types of examples:
1. Positive examples
 
X hierarchy#sense; Y hierarchy#sense;
Yes 
2. Negative examples
 
X hierarchy#sense; Y hierarchy#sense; No 
3. Ambiguous examples
 
X hierarchy#sense; Y hierarchy#sense;
Yes/No 
The third situation occurs when the training cor-
pus contains both positive and negative examples for
the same hierarchy types. For example, both rela-
tions  apartment   1; woman   1; No  and
 hand   1; woman   1; Yes  are mapped into
the more general type  entity   1; entity   1;
Yes/No  . However, the first example is negative (a
POSSESSION relation), while the second one is a positive
example.
Step 2. Learning constraints for unambiguous examples
For the unambiguous examples in the generalized train-
ing corpus (those that are either positive or negative), con-
straints are determined using C4.5. In this context, the
features are the components of the relation (the part and,
respectively the whole) and the values of the features are
their corresponding WordNet semantic classes (the fur-
thest ancestor in WordNet of the corresponding concept).
With the first two types of examples, the unambiguous
ones, a new training corpus was created on which we ap-
plied C4.5 using a 10-fold cross validation. The output
is represented by 10 sets of rules generated from these
unambiguous examples.
The rules in each set were ranked according to their
frequency of occurrence and average accuracy obtained
for that particular set. In order to use the best rules, we
decided to keep only the ones that had a frequency above
a threshold (occur in at least 7 of the 10 sets of rules) and
an average accuracy greater than 50

.
Step 3. Specialize the ambiguous examples
A part of the generalized training corpus contains am-
biguous examples. These examples refer to the same se-
mantic classes in WordNet, but their target value is in
some cases ?Yes? and in others ?No?. Since C4.5 can-
not be applied in this situation, we recursively specialize
these examples to eliminate the ambiguity.
The specialization procedure is based on the IS-A in-
formation provided by WordNet. Initially, each semantic
class represented the root of one of the noun hierarchies
in WordNet. By specialization, a semantic class is re-
placed with its first hyponym, i.e. the concept immedi-
ately below in the hierarchy. For this task, we considered
again the intermediate training corpus of examples.
For instance, the examples  apartment   1,
entity   1; woman   1, entity   1; No 
and  hand   1, entity   1; woman   1,
entity   1; Yes  that caused the ambiguity

entity   ; entity   1; Yes/No  , were
replaced with  apartment   1, whole   2;
woman   1, causal agent   1; No  , re-
spectively  hand   1, part   7; woman   1,
causal agent   1; Yes  . These two intermediate
examples are thus generalized in the less ambiguous
examples  whole   2; causal agent   1; No 
and  part   7; causal agent   1; Yes  . This
way, we specialize the ambiguous examples with more
specific values for the attributes. The specialization
process for this particular example is shown in Figure 1.
causal_agent#1
apartment#1 hand#1 women#1
entity#1
whole#2 part#7
Figure 1: The specialization of the ambiguous training exam-
ples with the corresponding WordNet hyponyms as new seman-
tic classes
Although this specialization procedure eliminates a
part of the ambiguous examples, there is no guarantee
it will work for all the ambiguous examples of this type.
This is because the specialization splits the initial hier-
archy into smaller distinct subhierarchies, and thus, the
examples are distributed over this new set of subhier-
archies. For the examples described above, the proce-
dure eliminates the ambiguity through specialization of
the semantic classes into two new ones: whole - causal
agent, and respectively part - causal agent. However, if
the training corpus contained the examples  leg   2;
insect   1; Yes  and  world   7; insect   1;
No  , the procedure specializes them in the ambigu-
ous example  part   7; organism   1; Yes/No 
and the ambiguity still remains.
Steps 2 and 3 are repeated until there are no more
ambiguous examples. The general architecture of this
procedure is shown in Figure 2. Here is an example
file
DATA
Learn
C4.5
using
Format
Specialize
Examples for
Next Iteration
Examples
Training
FormatGeneralize
Examples
Examples
Ambiguous
Examples
Unambiguous
Constraints
Examples
Classes
NAMES file
Detect
Ambiguous
Examples
Examples
Semantic classes
Semantic
Features
New Semantic Classes
New Examples
Figure 2: Architecture of the constraint learning procedure.
of iterations produced by the program to specialize
ambiguous data:
entity   1 - entity   1
body of water   1 - location   1
location   1 - body of water   1
location   1 - object   1
region   1 - artifact   1
region   3 - artifact   1
Each indentation represents a new iteration.
3.5 The Constraints
Table 1 summarizes the constraints learned by the pro-
gram. The meaning of a constraint with the part Class-X,
the whole Class-Y and the value 1 is ?if Part is a Class-X
and Whole is a Class-Y then it is a part-whole relation?
and for the value 0 is ?if Part is a Class-X and Whole is a
Class-Y then it is not a part-whole relation?. For exam-
ple, ?if Part is an entity   and the Whole is a whole   2
then it is not a part-whole relation?. (whole   2 is the
WordNet concept meaning ?an assemblage of parts that
is regarded as a single entity?).
When forming larger, more complex rules, if the part
and the whole contain more then one value, one of these
values is negated (preceded by !). For example for the
part object   1 and the whole organism   1 the constraint
is ?if the Part is object   1 and not substance   1 and not
natural object   1 and the Whole is organism   1 and not
plant   2 and not animal   1 then NO part-whole rela-
tion?.
4 Results for discovering part-whole
relations
To validate the constraints for extracting part-whole re-
lations, a new test corpus ?B? was created from other
10,000 sentences of TREC-9 LA Times news articles.
This corpus was parsed and disambiguated using a Word
Sense Disambiguation system that has an accuracy of
81

when disambiguating nouns in open-domain (Mi-
halcea and Moldovan, 2001). The results provided by the
part-whole relation discovery procedure were validated
by a human annotator.
Let us define the precision and recall performance met-
rics in this context:
 
	

Discovery of Manner Relations and their Applicability to Question
Answering
Roxana Girju    , Manju Putcha   and Dan Moldovan  
Human Language Technology Research Institute
 
University of Texas at Dallas
and
Department of Computer Science

Baylor University
girju@ecs.baylor.edu, moldovan@utdallas.edu
Abstract
The discovery of semantic relations from
text becomes increasingly important for
applications such as Question Answer-
ing, Information Extraction, Summariza-
tion, Text Understanding and others. This
paper presents a method for the auto-
matic discovery of manner relations using
a Naive Bayes learning algorithm. The
method was tested on the UPenn Tree-
bank2 corpus, and the targeted manner re-
lations were detected with a precision of
64.44% and a recall of 68.67%.
1 Introduction
1.1 Problem description
An important semantic relation for several NLP ap-
plications is the manner relation. Consider the sen-
tence (from the Democratic response to the Presi-
dent Bush? 2003 State of the Union Address):
We want to work together to build our new
economy, creating jobs by investing in technology
so America can continue to lead the world
in growth and opportunity.
There are four manner relations in this text: (1)
together is a manner adverb that modifies the verb
work, (2) creating jobs is an adverbial phrase at-
tached through a manner relation to the verb work,
(3) by investing in technology is a prepositional
phrase that expresses manner and attaches to the
verb create, and (4) in growth and opportunity is a
manner prepositional phrase that modifies the verb
lead.
The discovery of manner relations in open text al-
lows Question Answering systems to identify these
relations and formulate answers to manner questions
that otherwise are not possible even with state-of-
the-art QA systems. For example, by identifying the
manner relations in the example above, the follow-
ing how questions may be answered:
Q: How do Democrats want America to lead the
world ? A: in growth and opportunity
Q: How do Democrats want to work? A: work to-
gether (with Republicans).
Q: How do Democrats want to build the economy ?
A: by creating jobs;
Q: How do Democrats want to create jobs? A: by
investing in technology
This paper provides a method for discovering
manner semantic relations in open text.
1.2 The semantics of manner relation
In WordNet, the manner relation is defined as a way
of acting or behaving. Similar definitions are pro-
vided by psychology researchers (Graesser et al,
2000).
There are different ways of expressing man-
ner and the difficulty arises that the same lexico-
syntactic patterns that express manner also express
other semantic relations in different contexts. A pos-
sible way to check whether or not a verb expression
conveys manner is to answer correctly the question
?In what manner/how  to verb  ?? For exam-
ple, for run quickly, we ask how to run? However,
this test holds only when there are no other answers
to questions like ?Where  verb  ??, or ?When 
verb  ?? that make sense. For example, jump over
the fence or jump always are not manner relations
although they may answer correctly a how question.
1.3 Previous work
Although manner relations were studied by philoso-
phers (Aristotle, 350BC), logicians, psychologists
and linguists (Quirk et al, 1985), (Fellbaum, 2002),
not much work has been done to automatically iden-
tify the manner relations in texts. Hearst (Hearst,
1998) developed a method for the automatic acqui-
sition of hypernymy relations by identifying a set of
frequently used and unambiguous lexico-syntactic
patterns. Then, she tried applying the same method
to other semantic relations, such as part-whole, but
without much success, as the patterns detected were
ambiguous.
2 Lexico-syntactic patterns expressing
manner
2.1 Manner as semantic role
The most frequently occurring form of manner is
as a semantic role (Quirk et al, 1985). In this
case, manner is encoded as a relationship between
a verb and one of its arguments which can be repre-
sented by various parts of speech, the most common
ones being adverb, adverbial phrase, prepositional
phrase, noun phrase, and clause.
Verb-adverb patterns
One of the most frequently used patterns expressing
manner is verb-adverb. In English, there are differ-
ent kinds of adverbs (Quirk et al, 1985): adverbs of
time, manner, degree, location, direction, frequency,
transition and hedges.
Based on the classification provided by Quirk et
al. (Quirk et al, 1985) and our statistics of English
texts, we present below the adverbial patterns in or-
der of their frequency of occurrence:
a) Adverbs of manner that end in ?-ly?
This manner adverbs are the most frequently used.
Their position is not fixed, as they can be placed
either before or after the verb they modify. These
adverbs can be modified by other adverbs forming
this way adverbial expressions. Examples: slowly,
heavily, angrily, etc.
b) Adverbs of manner that do not end in ?-ly?
These adverbs also called Quality description ad-
verbs provide a description of a particular quality.
Example: fast, good, well, etc.
c) Adverbial expressions
These are expressions that modify the underly-
ing verb and refer along with the verb to a man-
ner relation. Examples of such patterns are:  as
adv manner as  
	  ,  NP as adv manner
 ,  as adv manner S  .
Examples: several times as fast, as much as 60%
faster, louder than ever, all around, etc.
d) Compound adverbs of manner
These adverbs are usually formed with words linked
by hypens. Examples: radio-style, tax-free, flat-out,
first-hand, etc
e) Foreign adverbial expressions
There are expressions boroughed from other lan-
guages that are in a manner relationship with the
underlying verb. Examples: in flagrante, a la Gor-
bachev, en masse, etc.
2.2 Other forms of manner relations
In addition to the manner roles expressed as verb-
adverb pairs, manner relations are also expressed as
(1) complex nominals (fast car), (2) verbs of im-
plicit manner (for example whisper is a manner of
speaking), (3) verb-PP (I took your coat by mistake),
(4) verb-NP (He breathed a deep breath), (5) verb
clauses (I cook vegetables as Chinese do), and oth-
ers.
All these lexico-syntactic patterns are ambiguous.
Thus we need some syntactic and semantic con-
straints to differentiate the manner relations from the
other possible meanings these patterns may have.
In this paper we focus only on the discovery of
manner semantic roles expressed as verb- adverb
pairs. The method, however, is extendable to many
other manner forms and even to other semantic rela-
tions.
3 Approach
The learning procedure proposed here is supervised,
for the learning algorithm is provided with a set of
inputs along with the corresponding set of correct
outputs. In this paper we use the Naive Bayes Clas-
sifier approach to determine whether or not a verb-
adverb pair indicates a manner relation. This method
is similar with the basic algorithm for Document
Classification (Mitchell, 1997).
Nr. Feature
1 Specific adverb statistics
2 Parent phrase type
3 Present or not in the Adverb Dictionary
4 Distance between verb and adverb
5 Component before adverb
6 Component after the adverb
7 Adverbs ends or not with ?ly
Table 1: Summary of Manner Features.
This approach requires a decision on how to rep-
resent an arbitrary text in terms of attribute (or fea-
tures) values and how to estimate their probabilities
as required by the Naive Bayes Classifier.
4 Selecting features
Many researchers ((Blaheta-Charniak, 2000),
(Gildea-Jurafsky, 2000), (Gildea-Palmer, 2002))
showed that lexical and syntactic information is
very useful for predicate-argument recognition
tasks. Their systems are statistical-based and have
been trained to automatically label semantic roles
only from the output of syntactic parsers.
However, lexical and syntactic information alone
is not sufficient for the detection of the manner se-
mantic roles, semantic information is necessary as
well.
To represent the text for the discovery of manner
relations, seven features which contribute the most
to the classification were chosen. These features
capture the context of the adverb and help in decid-
ing the presence of the manner (MNR) component.
We have developed an Adverb Dictionary that is
a source for some of the features. The Adverb Dic-
tionary is created with adverbs from WordNet and
TreeBank. The adverbs that contain the pattern ?in
a ?? manner? in their gloss were extracted from
WordNet. The adverbs that are annotated in Tree-
Bank as MNR adverb-verb pairs are also included
in the Dictionary. A total of 2183 adverbs were in-
cluded in the Dictionary.
The features are explained with the help of the
following example:
(S1 (S (NP (DT The) (NN bank)) (VP (AUX is)
(ADVP (RB now))(VP (ADVP (RB aggressively))
(VBG marketing) (NP (JJ retail)(NNS services))
(PP (IN at) (NP (PRP$ its) (JJ domestic) (NNS
branches))))) (. .)))
(1) Specific adverb statistics
Feature 1 checks if a specific adverb is present in the
Dictionary or not. For example, aggressively is part
of the Dictionary, where as now is not. The posi-
tive frequency calculated from this feature is the to-
tal number of times that adverb was encountered in
the training corpus. In the case the adverb of a sen-
tence in the testing corpus is part of the Dictionary,
this feature helps in deciding what are its chances of
being a Positive/Negative Indicator of Manner. This
is a good feature as long as the training corpus is
very rich (i.e it covers all adverbs).
(2) Parent phrase type
The second feature is the phrase type to which the
adverb attaches. Here both now and aggressively at-
tach to ?VP?. Most of the MNR indicating adverbs
attach to verbs. This feature helps eliminate adverbs,
which modify nouns or adjectives.
(3) Whether or not Adverb is present in the
Dictionary
Feature 3, like feature 1 checks whether or not an
adverb is present in the Adverb Dictionary. The dif-
ference is that its statistics are not calculated on the
training corpus like in feature 1, but instead it takes
the probability of being a manner adverb in the Ad-
verb Dictionary.
The usefulness of feature 3 is realized when the
test corpus has an adverb which was not encountered
in the training corpus. The estimates from feature 1
fail to be of any use at such a point because it is
a missing value and both positive and negative fre-
quencies are the same. However, feature 3 assigns
the probabilities of that adverb being a manner ad-
verb in the Adverb Dictionary. So, we still have a
good estimate from this feature to decide if it is a
potential MNR indicator or not (which would have
been nullified, had we relied only on feature 1).
For example, let?s say we encounter the adverb
excitedly in the test corpus and it is present in the
Adverb Dictionary but not in the training corpus.
Feature 1 will not contribute to the decision while
feature 3 will help. We can use the lookup table for
feature 3 and it is evident that an adverb present in
the Dictionary has a higher probability of indicating
manner.
(4) Distance between verb and adverb
The fourth feature is the distance between verb and
adverb. This doesn?t take into consideration whether
the adverb precedes or succeeds the verb. Distance
refers to the number of English words that separate
them. For example, there are no words between
aggressively and marketing, thus the distance is 0.
Similarly, the distance between now and marketing
is 1. The rational of this feature is based on the ob-
servation that most frequently a MNR indicating ad-
verb appears immediately next to a VB.
(5) Component before the adverb
The fifth feature concerns the POS of the word pre-
ceding the adverb. This captures the context of the
adverb. This is based on the observation that an ad-
verb that succeeds an AUX is usually not a MNR
indicator. For example now is preceeded by ?AUX?
and aggressively is preceded by an ?ADVP?.
(6) Component after the adverb
The sixth feature concerns the POS of the word after
the RB. For example now is succeeded by an ?AUX?
and aggressively by an ?VBG?.
(7) Adverb ends in ?ly?
This feature is 1 when the adverb ends in ?ly? and 0
otherwise. The rational for this feature is that many
adverbs in manner roles end in ?ly?.
Estimating Probabilities
The next step is to calculate the probabilities re-
quired by the Naive Bayes Classifier.
a. Class prior probabilities. This is the ratio between
the number of adverbs of each class over the total
number of adverbs in the training examples. In our
case the classes are positive (or Manner) and nega-
tive (not Manner). This is defined as:
  	
 

where 
 is the total number of examples for which
the target value is  and 
 is the total number of
examples.
b. Class conditional probability. This is the proba-
bility that any of the seven features drawn from the
parsed text tagged positive or negative will belong
to the domain of the corresponding features. We use
the m-estimate to avoid the cases when  ffProceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 66?76,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Summarizing Contrastive Viewpoints in Opinionated Text
Michael J. Paul?
University of Illinois
Urbana, IL 61801, USA
mjpaul2@illinois.edu
ChengXiang Zhai
University of Illinois
Urbana, IL 61801, USA
czhai@cs.uiuc.edu
Roxana Girju
University of Illinois
Urbana, IL 61801, USA
girju@illinois.edu
Abstract
This paper presents a two-stage approach to
summarizing multiple contrastive viewpoints
in opinionated text. In the first stage, we
use an unsupervised probabilistic approach to
model and extract multiple viewpoints in text.
We experiment with a variety of lexical and
syntactic features, yielding significant perfor-
mance gains over bag-of-words feature sets.
In the second stage, we introduce Compara-
tive LexRank, a novel random walk formula-
tion to score sentences and pairs of sentences
from opposite viewpoints based on both their
representativeness of the collection as well as
their contrastiveness with each other. Exper-
imental results show that the proposed ap-
proach can generate informative summaries of
viewpoints in opinionated text.
1 Introduction
The amount of opinionated text available online has
been growing rapidly, increasing the need for sys-
tems that can summarize opinions expressed in such
text so that a user can easily digest them. In this pa-
per, we study how to summarize opinionated text in
a such a way that highlights contrast between multi-
ple viewpoints, which is a little-studied task.
Usually, online opinionated text is generated by
multiple people, and thus often contains multi-
ple viewpoints about an issue or topic. A view-
point/perspective refers to ?a mental position from
which things are viewed? (cf. WordNet). An opin-
ion is usually expressed in association with a partic-
ular viewpoint, even though the viewpoint is usually
?Now at Johns Hopkins University (mpaul@cs.jhu.edu).
not explicitly given; for example, a blogger that is
in favor of a policy would likely look at the positive
aspects of the policy (i.e., positive viewpoint), while
someone against the policy would likely empha-
size the negative aspects (i.e., negative viewpoint).
Moreover, in an opinionated text with diverse opin-
ions, the multiple viewpoints taken by opinion hold-
ers are often ?contrastive?, leading to opposite po-
larities. Indeed, such contrast in opinions may be a
main driving force behind many online discussions.
Futhermore, opinions regarding news events and
other short-term issues may quickly emerge and dis-
appear. Such opinions may reflect many differ-
ent types of viewpoints which cannot be modeled
by current systems. For this reason, we believe
that a viewpoint summarization system would ben-
efit from the ability to extract unlabeled viewpoints
without supervision. Even if such clustering has in-
accuracies, it could still be a useful starting point for
human editors to select representative excerpts.
Thus, given a set of opinionated documents about
a topic, we aim at automatically extracting and sum-
marizing the multiple contrastive viewpoints implic-
itly expressed in the opinionated text to facilitate
digestion and comparison of different viewpoints.
Specifically, we will generate two types of multi-
view summaries: macro multi-view summary and
micro multi-view summary. A macro multi-view
summary would contain multiple sets of sentences,
each representing a different viewpoint; these differ-
ent sets of sentences can be compared to understand
the difference of multiple viewpoints at the ?macro
level.? A micro multi-view summary would con-
tain a set of pairs of contrastive sentences (each pair
66
consists of two sentences representing two different
viewpoints), making it easy to understand the differ-
ence between two viewpoints at the ?micro level.?
Although opinion summarization has been exten-
sively studied (e.g., (Liu et al, 2005; Hu and Liu,
2004; Hu and Liu, 2006; Zhuang et al, 2006)), ex-
isting work has not attempted to generate our envi-
sioned contrastive macro and micro multi-view sum-
maries in an unsupervised way, which is the goal of
our work. For example, Hu and Liu (2006) rank sen-
tences based on their dominant sentiment according
to the polarity of adjectives occuring near a product
feature in a sentence. A contradiction occurs when
two sentences are highly unlikely to be simultane-
ously true (cf. (Marneffe et al, 2008)). Although
little work has been done on contradiction detection,
there are a few notable approaches (Harabagiu et al,
2006; Marneffe et al, 2008; Kim and Zhai, 2009).
The closest work to ours is perhaps that of Ler-
man and McDonald (2009) who present an approach
to contrastive summarization. They add an objective
to their summarization model such that the summary
model for one set of text is different from the model
for the other set. The idea is to highlight the key
differences between the sets, however this is a dif-
ferent type of contrast than the one we study here ?
our goal is instead to make the summaries similar to
each other, to contrast how the same information is
conveyed through different viewpoints.
In this paper, we propose a two-stage approach
to solving this novel summarization problem, which
will be explained in the following two sections.
2 Modeling Viewpoints
The first challenge to be solved in order to generate
a contrastive summary of multiple viewpoints is to
model and extract these viewpoints which are hidden
in text. In this paper we propose to solve this chal-
lenge by employing the Topic-Aspect Model (TAM)
(Paul and Girju, 2010), which is an extension of the
Latent Dirichlet Allocation (LDA) model (Blei et al,
2003) for jointly modeling topics and viewpoints in
text. While most existing work on such topic models
(including TAM) has taken a topic model as a gen-
erative model for word tokens in text, we propose to
take TAM as a generative model for more complex
linguistic features extracted from text. These are
more discriminative than single word tokens and can
improve the accuracy of extracting multiple view-
points as we will show in the experimental results?
section. Below we first give a brief introduction to
TAM and then present the proposed set of features.
2.1 Topic-Aspect Model (TAM)
LDA-style probabilistic topic models of document
content (Blei et al, 2003) have been shown to offer
state-of-the-art summarization quality. Such mod-
els also provide a framework for adding additional
structure to a summarization model (Haghighi and
Vanderwende, 2009). In our case, we want to add
more structure to a model to incorporate the notion
of viewpoint/perspective into our summaries.
When it comes to extracting viewpoints, recent re-
search suggests that it may be beneficial to model
both topics and perspectives, as sentiment may be
expressed differently depending on the issue in-
volved (Brody and Elhadad, 2010; Paul and Girju,
2010). For example, let?s consider a set of product
reviews for a home theater system. Content topics
in this data might include things like sound qual-
ity, usability, etc., while the viewpoints might be
the positive and negative sentiments. A word like
speakers, for instance depends on the sound topic
but not a viewpoint, while good would be an exam-
ple of a word that depends on a viewpoint but not
any particular topic. A word like loud would depend
on both (since it would be considered positive senti-
ment only in the context of the sound quality topic),
while a word like think depends on neither.
We make use of a recent model, the Topic-Aspect
Model (Paul and Girju, 2010), which can model
such behavior with or without supervision. Under
this model, a document has a mixture over topics as
well as a mixture over viewpoints. The two mix-
tures are drawn independently of each other, and
thus can be thought of as two separate clustering di-
mensions. A word is associated with variables de-
noting its topic and viewpoint assignments, as well
as two binary variables to denote if the word de-
pends on the topic and if the word depends on the
viewpoint. A word may depend on the topic, the
viewpoint, both, or neither, as in the above example.
The generative process for a document d under
this model can be briefly described as follows. For
each word in a document:
67
1. Sample a topic z from P (z|d) and a viewpoint v
from P (v|d).
2. Sample a ?level? ` ? {0, 1} from P (`|d). This
determines if the word will depend on the topic (topical
level) or not (background level).
3. Sample a ?route? r ? {0, 1} from P (r|`, z). This
determines if the word will depend on the viewpoint.
4. Sample a word w from P (w|z, v, r, `).
The probabilities are multinomial/binomial dis-
tributions with Dirichlet/Beta priors, and thus this
model falls under the standard LDA framework. The
number of topics and number of viewpoints are pa-
rameters that must be specified. Inference can be
done with Gibbs sampling (Paul and Girju, 2010).
TAM naturally gives us a very rich output to
use in a viewpoint summarization application. If
we are doing unsupervised viewpoint extraction,
we can use the output of the model to compute
P (v|sentence) which could be used to generate
summaries that contain only excerpts that strongly
highlight one viewpoint over another. Similarly,
we could use the learned topic mixtures to generate
topic-specific summaries. Futhermore, the variables
r and ` tell us if a word is dependent on the view-
point and topic, and we could use this information
to focus on sentences that contain informative con-
tent words. Note that without supervision, TAM?s
clustering is based only on co-occurrences and the
patterns it captures may or may not correspond with
the viewpoints we wish to extract. Nonetheless, we
show in this research that it can indeed find mean-
ingful viewpoints with reasonable accuracy on cer-
tain data sets. Although we do not explore this in
this paper, additional information about the view-
points could be added to TAM by defining priors on
the distributions to further improve the accuracy of
viewpoint discovery.
2.2 Features
Previous work with TAM used only bag of words
features, which may not be the best features for cap-
turing viewpoints. For example, ?Israel attacked
Palestine? and ?Palestine attacked Israel? are iden-
tical excerpts in an exchangable bag of words rep-
resentation, yet one is more likely to come from the
perspective of a Palestinian and the other from an Is-
raeli. In this subsection, we will propose a variety of
feature sets. We evaluate the utility of these features
to the task of modeling viewpoints by measuring the
accuracy of unsupervised clustering.
2.2.1 Words
We have experimented with simple bag of words
features as baseline approaches, both with and with-
out removing stop words, and found that the accu-
racy of clustering by viewpoint is better when re-
taining all words. This supports the observation
that common function words may have important
psychological properties (Chung and Pennebaker,
2007). Thus, we do not do any stop word removal
for any of our other feature sets. We find that we get
better results by stemming the words, so we apply
Porter?s stemmer to all of our features described.
2.2.2 Dependency Relations
It has been shown that using syntactic information
can improve the accuracy of sentiment models (Joshi
and Rose?, 2009). Thus, instead of representing doc-
uments as a bag of words, we will experiment with
using features returned by a dependency parser. For
this, we used the Stanford parser1, which returns de-
pendency tuples of the form rel(a, b) where rel is
some dependency relation and a and b are tokens of
a sentence. We can use these specific tuples as fea-
tures, referred here as the full-tuple representation.
One problem with this representation is that we
are using very specific information and it is harder
for learning algorithms to find patterns due to the
lack of redundancy. One solution is to generalize
these features and rewrite a tuple rel(a, b) as two
tuples: rel(a, ?) and rel(?, b) (Greene and Resnik,
2009; Joshi and Rose?, 2009). We will refer to this as
the split-tuple representation.
2.2.3 Negation
If a word wi appears in the head of a neg rela-
tion, then we would like this to be reflected in other
dependency tuples in which wi occurs. For a tuple
rel(wi, wj), if either wi or wj is negated, then we
simply rewrite it as ?rel(wi, wj).
An alternative would be to rewrite the individual
word wi as ?wi. However in our experiments this
representation produced worse accuracies, perhaps
because this produces less redundancy.
1http://nlp.stanford.edu/software/
68
2.2.4 Polarity
We also hypothesize that lexical polarity informa-
tion may improve our model. If we are using the
full-tuple representation, then a tuple becomes more
general by replacing the specific word with a + or
?. In the case that both words are polarity words,
we use two tuples, replacing only one word at a
time rather than replacing both words with their po-
larity signs. To determine the polarity of a word,
we simply use the Subjectivity Clues lexicon (Wil-
son et al, 2005) and as polarity values, positive (+),
negative (-), and neutral (*). Under our split-tuple
representation, this becomes more specific by re-
placing the ? with the polarity sign. For example,
the tuple amod(idea, good) would be represented
as amod(idea,+) and amod(?, good). We collapse
negated features to flip the polarity sign such that
?rel(a,+) becomes rel(a,?).
2.2.5 Generalized Relations
We also experimented with backing off the rela-
tions themselves. Since the Stanford dependencies
can be organized in a hierarchy2, we will represent
the relations at more generalized levels in the hi-
erarchy. For example, both a direct object and an
indirect object are a type of object. For a relation
rel, we define Rrel as the relation above rel in
the hierarchy ? for example, Rdobj = obj. We
make an exception for neg which has its own im-
portant properties that we wish to retain, so we let
Rneg = neg. Thus, when using these features, we
rewrite rel(a, b) as Rrel(a, b).
3 Multi-Viewpoint Summarization
As a computation problem, extractive multi-
viewpoint summarization would take as input a set
of candidate excerpts3 X = {x1, x2, ..., x|X|} with
k viewpoints and generate two types of multi-view
contrastive summaries: 1) A macro contrastive sum-
mary Smacro consists of k disjoint sets of excerpts,
X1, X2, ..., Xk ? X with each Xi containing repre-
sentative sentences of the i-th view (i.e., Smacro =
(X1, ..., Xk)). The number of excerpts in each Xi
can be empirically set based on application needs.
2The complete hierarchy can be found in the Stanford de-
pendencies manual (Marneffe and Manning, 2008).
3An ?excerpt? refers to the smallest unit of text that will
make up our summary such as a sentence.
2) A micro contrastive summary Smicro consists
of a set of excerpt pairs, each containing two ex-
cerpts from two different viewpoints, i.e., Smicro =
{(s1, t1), ..., (sn, tn)} where si ? X and ti ? X are
two comparable excerpts representing two different
viewpoints. n is the length of the summary, which
can be set empirically based on application needs.
Note that both macro and micro summaries can re-
veal contrast between different viewpoints, though
at different granularity levels.
To generate macro and micro summaries based
on the probabilistic assignment of excerpts to view-
points given by TAM, we propose a novel extension
to the LexRank algorithm (Erkan and Radev, 2004),
a graph-based method for scoring representative ex-
cerpts to be used in a summary. Our key idea is to
modify the definition of the jumping probability in
the random walk model so that it would favor ex-
cerpts that represent a viewpoint well and encour-
age jumping to an excerpt comparable with the cur-
rent one but from a different viewpoint. As a re-
sult, the stationary distribution of the random walk
model would capture representative contrastive ex-
cerpts and allow us to generate both macro and mi-
cro contrastive summaries within a unified frame-
work. We now describe this novel summarization
algorithm (called Comparative LexRank) in detail.
3.1 Comparative LexRank
LexRank is a PageRank-like algorithm (Page et al,
1998), where we define a random walk model on
top of a graph that has sentences to be summarized
as nodes and edges placed between two sentences
that are similar to each other. We can then score
all the sentences based on the expected probability
of a random walker visiting each sentence. We use
the short-hand P (xj |xi) to denote the probability of
being at node xj at a time t given that the walker
was at xi at time t ? 1. The jumping probability
from node xi to node xj is given by:
P (xj |xi) =
sim(xi, xj)
?
j??X sim(xi, xj?)
(1)
where sim is a content similarity function defined
on two sentence/excerpt nodes.
Our extension is mainly to modify this jumping
probability in two ways so as to favor visiting con-
trastive representative opinions from multiple view-
69
points. The first modification is to make it favor
jumping to a good representative excerpt x of any
viewpoint v (i.e., with high probability p(v|x) ac-
cording to the TAM model). The second modifica-
tion is to further favor jumping between two excerpts
that can potentially form a good contrastive pair for
use in generating a micro contrastive summary.
Specifically, under our model, the random walker
first decides whether to jump to a sentence of the
same viewpoint or to a sentence of a different view-
point. We define this decision as a binary variable
z ? {0, 1}. Intuitively, if we can force the ran-
dom walker to move back and forth between view-
points, then the final scores will favor sentences that
are similar across both viewpoints.
We define two different modified similarity func-
tions for the two possible values of z. The first one,
sim0 (corresponding to z = 0) scales the similarity
by the likelihood that the two x?s represent the same
viewpoint, and the second one, sim1 (for z = 1)
scales the similarity by the likelihood that the x?s
come from different viewpoints.
sim0(xi, xj) = sim(xi, xj)
k?
m=1
P (v = m|xi)P (v = m|xj)
sim1(xi, xj) = sim(xi, xj)?
?
m1,m2?[1,k],m1 6=m2
P (v = m1|xi)P (v = m2|xj)
where P (v|x) denotes the probability that the ex-
cerpt x belongs to the viewpoint v, and in general,
can be obtained through any multi-viewpoint model.
A special case of this is when the labels for view-
points are known, in which case P (v|x) = 1 for the
correct label and 0 for the others.
In our experiments, P (v|x) comes from the out-
put of TAM, and we define sim(xi, xj) as the cosine
between the vectors xi and xj , although again any
similarity function could be used. The conditional
transition probability from xi to xj given z is then:
P (xj |xi, z) =
simz(xi, xj)
?
j??X simz(xi, xj?)
(2)
Using ? to denote P (z = 0) and marginalizing
across z, we have the transition probability:
P (xj |xi) = ?P (xj |xi, z = 0)+ (1??)P (xj |xi, z = 1)
The stationary distribution of the random walk
gives us a scoring of the excerpts to be used in our
summary. It is also possible to score pairs of ex-
cerpts that contrast each other. We define the score
for a pair (xi, xj) as the probability of being at xi
and transitioning to xj or vice versa, where xi and
xj are of opposite viewpoints. Specifically:
P (xi)P (xj |xi, z = 1) + P (xj)P (xi|xj , z = 1) (3)
3.2 Summary Generation
The final summary should be a set of excerpts that
have a high relevance score according to our scoring
algorithm, but are not redundant among each other.
Many techniques could be used to accomplish this
(Carbonell and Goldstein, 1998; McDonald, 2007),
but we use a simple greedy approach: at each step
of the summary generation algorithm, we add the
excerpt with the highest relevance score as long as
the excerpt?s redundancy score ? the cosine similar-
ity between the candidate and the current summary
? is under some threshold ?. This is repeated until
the summary reaches a user-supplied length limit.
Macro contrastive summarization: A macro-level
summary consists of independent summaries for
each viewpoint, which we generate by first using the
random walk stationary distribution across all of the
data to rank the excerpts. We then separate the top-
ranked excerpts into two disjoint sets according to
their viewpoint based on whichever gives a greater
value of P (v|x), and finally remove redundancy and
produce the summary according to our method de-
scribed above. We refer to this as macro contrastive
summarization, because the summaries will contrast
each other in that they have related content, but the
excerpts in the summaries are not explicitly aligned
with each other.
Micro contrastive summarization: A candidate
excerpt for a micro-level summary will consist of
a pair (xi, xj) with the pairwise relevance score de-
fined in Equation 3. We can then rank these pairs and
remove redundancy. It is possible that both xi and xj
in a high-scoring pair may belong to the same view-
point; such a case would be filtered out since we are
mainly interested in including contrastive pairs in
our summary. We refer to this as micro contrastive
summarization, because the summaries will allow
us to see contrast at the level of individual excerpts
from different viewpoints.
70
4 Experiments and Evaluation
4.1 Experimental Setup
Evaluation of multi-view summarization is challeng-
ing as there is no existing data set we can use. We
leverage the resources on the Web and created two
data sets in the domain of political opinion.
Our first dataset is a set of 948 verbatim responses
to a Gallup R? phone survey about the 2010 U.S.
healthcare bill (Jones, 2010), conducted March 4-7,
2010. Responses in this set tend to be short and of-
ten incomplete or otherwise ill-formed and informal
sentences. Respondants indicate if they are ?for? or
?against? the bill, and there is a roughly even mix of
the two viewpoints (45% for and 48% against).
We also use the Bitterlemons corpus, a collection
of 594 editorials about the Israel-Palestine conflict.
This dataset is fully described in (Lin et al, 2006)
and has been used in other perspective modeling lit-
erature (Lin et al, 2008; Greene and Resnik, 2009).
The style of this data differs substantially from the
healthcare data in that documents in this set tend to
be long and verbose articles with well-formed sen-
tences. It again contains a fairly even mixture of two
different perspectives: 312 articles from Israeli au-
thors and 282 articles from Palestinian authors.
Moreover, for the healthcare data set, manually
extracted opinion polls are available on the Web,
which we further leverage to construct gold stan-
dard summaries to evaluate our method quantita-
tively. The data and test sets are available at
http://apfel.ai.uiuc.edu/resources.html.
4.2 Stage One: Modeling Viewpoints
The main research question we want to answer in
modeling viewpoints is whether richer feature sets
would lead to better accuracy than word features.
We used our various feature sets as input to TAM
and measured the accuracy of clustering documents
by viewpoint. This evaluation serves both to mea-
sure how accurately this type of clustering can be
done, as well as to measure which types of features
are important for modeling viewpoints.
We found that the clustering accuracy is improved
if we measure the accuracy of only the subset of
documents such that P (v|doc) is greater than some
threshold (we used 0.8). Thus, the accuraries pre-
sented in this section are measured using this confi-
dence threshold. We will use this approach for the
summarization task as well, as it ensures we are only
summarizing documents where we have high confi-
dence about their viewpoint membership.
There are several parameters to set for TAM.
Since our focus is on comparing linguistic features
with word features, we simply set these parame-
ters to some reasonable values: We used Dirich-
let pseudo-counts of 80.0 for P (` = 0), 20.0 for
P (` = 1), uniform pseudo-counts of 5.0 for P (x),
0.1 for the topic and aspect mixtures, and 0.01 for
the word distributions. We tell the model to use 2
viewpoints as well as 5 topics for the healthcare cor-
pus and 8 topics for the Bitterlemons corpus.
There is high variance in the accuracies depend-
ing on how the Gibbs samplers were initialized. We
thus repeated the experiments many times to obtain
relatively confident measures ? 200 times for the
healthcare set and 50 times for the Bitterlemons set,
with 2000 iterations each time. A natural way to se-
lect a model is to choose the model that gives the
highest likelihood to its input. To evaluate how well
this selection strategy would work, we measured the
correlation between accuracy and likelihood.
The results are shown in Table 1. We can make
several observations. (1) In all cases, the proposed
linguistic features yield higher accuracy than the
word features, supporting our hypothesis that for
viewpoint modeling, applying TAM to these features
improves performance over using simple word fea-
tures. Since virtually all existing work on topic mod-
els assumes word tokens as data to be modeled, our
results suggest that it would be interesting to explore
applying generative topic models to complex fea-
tures for other tasks as well. This may be because by
adding additional complex features to the observed
data, we artificially inflate the data likelihood to em-
phasize modeling co-occurrences of such features,
which effectively biases the model to capture a cer-
tain perspective of co-occurrences.
(2) The increase is substantially greater for the
Bitterlemons corpus, which may be due to the fact
that the parsing accuracy is likely better because the
language is formal. The split-tuple representation
is very significantly better for the healthcare corpus,
but it is not clear which is better for the Bitterlemons
corpus. It is also not clear how the generalized rela-
tions affect the performance.
71
Healthcare Corpus Bitterlemons Corpus
Feature Set Mean Med Max MaxLL Corr Mean Med Max MaxLL Corr
bag of words 61.12 +/- 0.76% 61.01 72.17 52.92 0.187 68.22 +/- 3.31% 69.26 88.27 84.94 0.39
- no stopwords 60.58 +/- 0.79% 60.50 72.18 62.58 0.154 61.29 +/- 3.05% 57.69 91.34 82.91 0.33
full-tuples 62.42 +/- 0.88% 62.47 74.04 63.37 0.201 80.89 +/- 3.45% 85.40 94.07 92.10 0.34
+ negation 63.67 +/- 0.81% 64.54 74.07 69.25 0.338 80.60 +/- 3.88% 88.07 95.61 91.32 0.66
+ neg. + polarity 63.16 +/- 0.94% 64.46 74.05 67.8 0.455 82.53 +/- 3.55% 86.64 94.44 91.16 0.31
gen. full-tuples 63.80 +/- 0.73% 64.35 73.29 71.70 0.254 76.62 +/- 4.09% 84.56 94.53 84.56 0.25
split-tuples 68.32 +/- 0.90% 70.74 77.80 76.57 0.646 77.14 +/- 3.64% 81.29 92.99 88.13 0.30
+ negation 68.00 +/- 0.91% 69.11 79.73 76.14 0.187 83.53 +/- 3.05% 87.71 95.00 95.00 0.12
+ neg. + polarity 65.11 +/- 1.05% 65.35 78.59 67.22 0.159 81.24 +/- 3.37% 83.44 95.03 88.55 0.08
gen. split-tuples 69.31 +/- 0.83% 70.69 77.90 73.90 0.653 76.69 +/- 4.36% 83.78 93.60 91.67 0.09
Table 1: The clustering accuracy with TAM using a variety of feature sets. These results were averaged over 200 randomly-initialized Gibbs
sampling procedures for the healthcare set, and 50 procedures for the Bitterlemons set. The 95% confidence interval using a standard t-test is also
given. Max refers to the maximum accuracy obtained over the 200 or 50 instances. MaxLL refers to the clustering accuracy using the model that
yielded the highest corpus log-likelihood as defined by TAM. Corr refers to the Pearson correlation coefficient between accuracy and log-likelihood.
(3) It appears that adding polarity helps the full-
tuple features (by making them more general) but
hurts the split-tuple features (by making them more
specific). Negation significantly improves the full-
tuple features in the Bitterlemons corpus, but it is
not clear if it helps in the other cases. It should be
noted that capturing negation and polarity is a very
complex and difficult task, and it is not expected that
our simple approaches will accurately capture these
properties. Nonetheless, it seems that these simple
features may help in certain cases.
4.3 Stage Two: Summarizing Viewpoints
For the second stage (i.e., the Comparative LexRank
algorithm), we mainly want to evaluate the quality
of the generated contrastive multi-viewpoint sum-
mary and study the effectiveness of our extension
to the standard LexRank. Below we present exten-
sive evaluation of our summarization method on the
healthcare data. We do not have an evaluation set
with which to compute quantitative metrics on the
Bitterlemons corpus, so we will instead perform a
simple qualitative evaluation in the last subsection.
4.3.1 Gold Standard Summaries
The responses to the Gallup healthcare poll are
described in an article4 which gives a table of the
main responses found in the data along with their
prominence in the data. In a way, this represents an
expert human-generated summary of our database,
and we will use this as a gold standard macro con-
trastive summary against which the representative-
4http://www.gallup.com/poll/126521/Favor-Oppose-
Obama-Healthcare-Plan.aspx
ness of a multi-viewpoint contrastive summary can
be evaluated. The reasons given in this table will
be used verbatim as our reference set, excluding the
other/no-reason/no-opinion reasons. A sample of
this table is shown in Table 2.
We also want to develop a reference set for micro
contrastive summaries, where we are mainly inter-
ested in evaluating contrastiveness. To do this, we
asked 3 annotators to identify contrastive pairs in
the ?main reasons? table described above. Each pair
must contain one reason from the ?for? side and one
reason from the ?against? side, though we do not re-
quire a one-to-one alignment; that is, multiple pairs
may contain the same reason. We take the set of
pairs that were identified as being contrastive by at
least 2 annotators to be our gold set of contrastive
pairs. Because these pairs come from the gold sum-
mary, they are still representative of the collection as
a whole, rather than fine-grained contrasts.
The macro reference set contains 9 ?for? reasons
and 15 ?against? reasons. The micro reference set
contains 13 annotator-identified pairs composed of 9
unique ?for? reasons and 8 unique ?against? reasons.
4.3.2 Baseline Approaches
Graph-based algorithms: The standard LexRank
algorithm can also be used to score pairs of sen-
tences according to Equation 3. We will thus com-
pare our new LexRank extension to the unmodified
form of this algorithm. When ? = 1, the random
walk model only transitions to sentences within the
same viewpoint, and thus in this case our modified
algorithm produces the same ranking as the unmod-
ified LexRank. This will be our first baseline.
72
For Against
People need health insurance/Too many uninsured 29% Will raise costs of insurance/Make it less affordable 20%
System is broken/Needs to be fixed 18% Does not address real problems 19%
Costs are out of control/Would help control costs 12% Need more information/clarity on how system would work 8%
Moral responsibility to provide/Obligation/Fair 12% Against big government/Too much government involvement 8%
Table 2: Some of the top reasons given along with their prominence in the healthcare data, as analyzed by Gallup. This is a sample of what will
serve as our gold set. The highlighted cells show an example of a contrastive pair identified by our annotators.
Model-based algorithms: We will also compare
against the approach of Lerman and McDonald
(2009) who introduce their contrastiveness objective
into a model-based summarization algorithm. The
basic form of this algorithm is to select a set of sen-
tences Sm to minimize the KL-divergence between
the models of the summary Sm and the entire collec-
tion Xm for a viewpoint m. The objective function
is: ?
?k
m=1KL(L(Sm)||L(Xm)) where L is an ar-
bitrary language model. We define L(A) simply as
the unigram distribution over words in the collection
A, a method also evaluated by Haghighi and Vander-
wende (2009). This is the fairest comparison to our
LexRank experiments, where sentences are also rep-
resented as unigrams. (We do not do any modeling
with TAM in our quantitative evaluation.)
Lerman and McDonald introduce an additional
term to maximize the KL-divergence between the
summary of one viewpoint and the collection of the
opposite viewpoint, so that each viewpoint?s sum-
mary is dissimilar to the other viewpoints. We bor-
row this idea but instead do the opposite so that the
viewpoints? summaries are more (rather than less)
similar to each other. This contrastive version of our
model-based baseline is formulated as:
?
k?
m1=1
KL(L(Sm1)||L(Xm1)) +
(
1
k?1
?
m2?[1,k],m1 6=m2
KL(L(Sm1)||L(Xm2))
)
Our summary generation algorithm is to iteratively
add excerpts to the summary in a greedy fash-
ion, selecting the excerpt with the highest score in
each iteration. Note that this approach only gen-
erates macro-level summaries, leaving us with the
LexRank baseline for micro-level summaries.
4.3.3 Metrics
We will evaluate our summaries using a variant of
the standard ROUGE evaluation metric (Lin, 2004).
Recall that we have two different evaluation sets
? one that contains all of the reasons for each view-
point, and one that consists only of aligned pairs of
excerpts. Since the same excerpt may appear in mul-
tiple pairs, there would be significant redundancy in
our reference summary if we were to include every
pair. Thus, we will restrict a contrastive reference
summary to exclude overlapping pairs, and we will
have many reference sets for all possible combina-
tions of pairs. There is only one reference set for the
representativeness criterion.
Our reference summaries have a unique property
in that the summaries have already been annotated
with the prominence of the different reasons in the
data. A good summary should capture the more
prominent statements, so we will include this in our
scoring function. We thus augment the basic ROUGE
n-gram recall score by weighting the n-gram counts
in the reference summary according to this percent-
age. This is a generalization of the standard ROUGE
formula where this percentage would be uniform.
For evaluating the macro-level summaries, we
will score the summaries for the two viewpoints sep-
arately, given a reference set Refi and a candidate
summary Ci for a viewpoint v = i. The final score
is a combination of the scores for both viewpoints,
i.e. Srep = 0.5S(Refi, Ci)+0.5S(Refj , Cj) where
S(Ref,C) is our ROUGE-based scoring metric. It
would also be interesting to measure how well a
viewpoint?s summary matches the gold summary
of the opposite viewpoint, which will give insights
into how well the Comparative LexRank algorithm
makes the two summaries similar to each other. We
will measure this as the inverse of the above metric,
i.e. Sopp = 0.5S(Refi, Cj) + 0.5S(Refj , Ci).
Finally, to score the micro-level comparative sum-
maries (recall that this gives explicitly-aligned pairs
of excerpts), we will concatenate each pair (xi, xj)
as a single excerpt, and use these as the excerpts in
our reference and candidate summaries. The scor-
ing function is then Sp = S(Refpairs, Cpairs). Note
that we have multiple reference summaries for the
73
? Srep-1 Sopp-1 Srep-2 Sopp-2 Sp-1 Sp-2
0.0 .425 .416 .083 .060 .309 .036
0.2 .410 .423 .082 .065 .285 .044
0.5 .419 .434 .085 .072 .386 .044
0.8 .410 .324 .095 .028 .367 .062
1.0 .354 .240 .070 .006 .322 .057
MB .362 .246 .089 .003
MC .347 .350 .054 .059
Table 3: Our evaluation scores for various values of ?. Smaller val-
ues of ? favor greater contrastiveness. Note that ? = 1 should be con-
sidered a baseline, because at this value the algorithm ignores the con-
trastiveness and it becomes a standard summarization problem. MB and
MC refer to our model-based baselines described in Subsection 4.3.2.
Bold scores are significant over all baselines according to a paired t-test.
micro-level evaluation due to overlapping pairs in
the evaluation set. In this case, the ROUGE score
is defined as the maximum score among all possible
reference summaries (Lin, 2004).
We measure both unigram (removing stop words,
denoted S-1) and bigram (retaining stop words, de-
noted S-2) recall, stemming words in all cases.
4.3.4 Evaluation Results
In order to evaluate our Comparative LexRank
algorithm by itself, in this subsection we will not
use the output of TAM as part of our summariza-
tion input, and will assign excerpts fixed values of
P (v|x) = 1 for the correct label and 0 otherwise.
We constructed our sentence vectors with unigrams
(removing stop words) and no IDF weighting.
We set the PageRank damping factor (Erkan and
Radev, 2004) to 0.01 and tried combinations of
the redundancy threshold ? ? {0.01, 0.05, 0.1, 0.2}
with different values of ?, the parameter which con-
trols the level of contrastiveness. For each value of
?, we optimized ? on the original data set according
to Srep?Sopp so that we can directly compare these
scores, and then we tuned ? separately for Sp. The
summary length is 6 excerpts. To obtain more ro-
bust results, we repeated the experiment 100 times
on random half-size subsets of our data. The scores
shown in Table 3 are averaged across these trials.
In general, increasing ? increases Srep, which
suggests that tuning ? behaves as expected, and
high- and mid-range ? values indeed produce sum-
maries where the summaries of the two viewpoints
are more similar to each other. Similarly, mid-range
? values produce substantially higher values of Sp-1,
the unigram ROUGE scores for the micro contrastive
summary, although there is not a large difference be-
tween the bigram scores. An example of our micro-
level output is shown in Table 4.
As for our model-based baseline, we show results
for both the basic algorithm (denoted MB) in addi-
tion to the contrastive modification (denoted MC).
We see that the contrastive modification behaves
as expected and produces much higher scores for
Sopp, however, this method does not outperform our
LexRank algorithm. It is interesting to note that in
almost all cases where a contrastive objective is in-
troduced, the scores for the opposite viewpoint Sopp
increase without decreasing the Srep scores, sug-
gesting that contrastiveness can be introduced into a
multi-view summarization problem without dimin-
ishing the overall quality of the summary. It is
admittedly difficult to make generalizations about
these methods from experiments with only one data
set, but we have at least some evidence that our al-
gorithm works as intended.
4.4 Unsupervised Summarization
So far we have focused on evaluating our viewpoint
clustering models and our multi-view summariza-
tion algorithms separately. We will finally show how
these two stages might work in tandem in unsuper-
vised summarization of the Bitterlemons corpus.
Without a gold set, it is difficult to perform an
extensive automatic evaluation as we did with the
healthcare data. Instead we will perform a sim-
ple qualitative evaluation to see if the algorithm ap-
pears to achieve its goal. Thus, we asked 8 people
to guess if each viewpoint?s summary was written
by Israeli or Palestinian authors. To diversify the
summaries, for each annotator we randomly split
each summary into two equal-sized subsets of the
sentence set. Thus each person was asked to label
four different summaries, which were presented in
a random order. If humans can correctly identify
the viewpoints, then this would suggest both that the
TAM accurately clustered documents by viewpoint
and the summarization algorithm is selecting sen-
tences that coherently represent the viewpoints.
We first ran TAM on our data using the same pro-
cedure and parameters as in Subsection 4.2 using the
full-tuple features. We repeated this 10 times and
used the model that gave the highest data likelihood
as our model for summarization input. We then gen-
74
For the Healthcare Bill Against the Healthcare Bill
the government already provides half of the healthcare dollars in the government is too much involvement.
united states [...] [they] might as well spend their dollars smarter
my kids are uninsured. a lot of people will be getting it that should be getting it on their own,
and my kids will be paying a lot of taxes.
so everybody would have it and afford it. we cannot afford it.
because of my family. i don?t know enough about it and i don?t know where exactly
it?s going to put my family.
because i have no health insurance and i need it. because i have health insurance.
cost of healthcare is so high. high costs.
Table 4: An example of our micro-level contrastive summarization output on the healthcare data, using ? = 0.05 and ? = 0.5.
erated macro contrastive summaries of our data for
the two viewpoints with 6 sentences per viewpoint.
We used unigram sentence vectors with IDF weight-
ing. We used ? = 0.5 and ? = 0.1, which gave the
highest score at this ? value on the healthcare data.
Only one of these sentences was clustered incor-
rectly by TAM. The human judges correctly labeled
78% of the summary sets, suggesting that our sys-
tem accurately selected some sentences that could
be recognized as belonging to the viewpoints, but
is not perfect. Unsupervised micro-level summaries
were less coherent. Many of the sentences are mis-
labeled, and the ones that are correctly labeled are
not representative of the collection.
This is not surprising, and indeed exposes the
challenge inherent in our problem definition: clus-
tering documents based on similarity and then high-
lighting sentences with high similarity but opposite
cluster membership are almost conflicting objectives
for an unsupervised learner. Such contrastive pairs
are perhaps the most difficult data points to model.
A good test of a viewpoint model may be whether it
can capture the nuanced properties of the viewpoints
needed to contrast them at the micro level.
5 Discussion
The properties of the text which we attempt to sum-
marize in our work are related to the concept of
framing from political science (Chong and Druck-
man, 2010), which is defined as ?an interpretation or
evaluation of an issue, event, or person that empha-
sizes certain of its features or consequences? focus-
ing on ?certain features and implications of the issue
? rather than others.? For example, someone in favor
of the healthcare bill might focus on the benefits and
someone against the bill might focus on the cost.
However, our approach is different in that our
contrastive objective encourages the summaries to
include each point as addressed by all viewpoints,
rather than each viewpoint selectively emphasizing
only certain points. In a sense, this makes our sum-
mary more like a live debate, where one side must
directly respond to a point raised by the other side.
For example, someone in favor of healthcare reform
might cite the high cost of the current system, but
someone against this might counter-argue that the
proposed system in the new bill has its own high
costs (as seen in the last row of Table 4). The idea is
to show how both sides address the same issues.
Thus, we can say that we are summarizing the
key arguments/issues/points from different opinions.
Futhermore, our models and algorithms are defined
very generally, and while we tested their viability in
the domain of political opinion, they may also be
useful for many other comparative tasks.
In conclusion, we have presented steps toward a
two-stage system that can automatically extract and
summarize viewpoints in opinionated text. First, we
have shown that accuracy of clustering documents
by viewpoint can be enhanced by using simple but
rich dependency features. This can be done within
the framework of existing probabilistic topic models
without altering the models simply by using a ?bag
of features? representation of documents.
Second, we have introduced Comparative
LexRank, an extension of the LexRank algorithm
that aims to generate contrastive summaries both at
the macro and micro level. The algorithm presented
is general enough that it can be applied to any
number of viewpoints, and can accomodate input
where the viewpoints are either given fixed labels,
or given probabilistic assignments. The tradeoff
between contrast and representation can flexibly be
tuned to an application?s needs.
75
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Samuel Brody and Noemie Elhadad. 2010. An unsuper-
vised aspect-sentiment model for online reviews. In
NAACL ?10.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In SIGIR ?98, pages
335?336.
Dennis Chong and James N. Druckman. 2010. Identi-
fying frames in political news. In Erik P. Bucy and
R. Lance Holbert, editors, Sourcebook for Political
Communication Research: Methods, Measures, and
Analytical Techniques. Routledge.
Cindy Chung and James W. Pennebaker. 2007. The psy-
chological function of function words. Social Commu-
nication: Frontiers of Social Psychology, pages 343?
359.
Gu?nes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text sum-
marization. J. Artif. Int. Res., 22(1):457?479.
Stephan Greene and Philip Resnik. 2009. More than
words: syntactic packaging and implicit sentiment. In
NAACL ?09, pages 503?511.
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
NAACL ?09, pages 362?370.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast and contradiction in text pro-
cessing.
Minqing Hu and Bing Liu. 2004. Mining opinion fea-
tures in customer reviews. In Proceedings of AAAI,
pages 755?760.
Minqing Hu and Bing Liu. 2006. Opinion extraction and
summarization on the Web. In Proceedings of the 21st
National Conference on Artificial Intelligence (AAAI-
2006), Nectar Paper Track, Boston, MA.
Jeffrey M. Jones. 2010. ?in u.s., 45% favor, 48% oppose
obama healthcare plan?, March.
Mahesh Joshi and Carolyn Penstein Rose?. 2009. Gen-
eralizing dependency features for opinion mining. In
ACL-IJCNLP ?09: Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 313?316.
Hyun Duk Kim and ChengXiang Zhai. 2009. Generating
comparative summaries of contradictory opinions in
text. In CIKM ?09: Proceeding of the 18th ACM con-
ference on Information and knowledge management,
pages 385?394, New York, NY, USA. ACM.
Kevin Lerman and Ryan McDonald. 2009. Contrastive
summarization: an experiment with consumer reviews.
In NAACL ?09, pages 113?116, Morristown, NJ, USA.
Association for Computational Linguistics.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on?: identifying perspectives at the document and
sentence levels. In CoNLL-X ?06: Proceedings of
the Tenth Conference on Computational Natural Lan-
guage Learning, pages 109?116.
Wei-Hao Lin, Eric Xing, and Alexander Hauptmann.
2008. A joint topic and perspective model for ideo-
logical discourse. In ECML PKDD ?08: Proceedings
of the European conference on Machine Learning and
Knowledge Discovery in Databases - Part II, pages
17?32, Berlin, Heidelberg. Springer-Verlag.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Stan Szpakowicz Marie-
Francine Moens, editor, Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pages 74?
81, Barcelona, Spain, July.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opinions
on the web. In WWW ?05: Proceedings of the 14th
international conference on World Wide Web, pages
342?351, New York, NY, USA. ACM Press.
Marie-Catherine De Marneffe and Christopher Manning.
2008. Stanford typed dependencies manual. Techni-
cal report, Stanford University.
Marie-Catherine De Marneffe, Anna Rafferty, and
Christopher Manning. 2008. Finding contradictions
in text. In Proceedings of the Association for Compu-
tational Linguistics Conference (ACL).
Ryan McDonald. 2007. A study of global infer-
ence algorithms in multi-document summarization. In
ECIR?07: Proceedings of the 29th European confer-
ence on IR research, pages 557?564, Berlin, Heidel-
berg. Springer-Verlag.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1998. The pagerank citation ranking:
Bringing order to the web. Technical report, Stanford
Digital Library Technologies Project.
Michael Paul and Roxana Girju. 2010. A two-
dimensional topic-aspect model for discovering multi-
faceted topics. In AAAI-2010: Twenty-Fourth Confer-
ence on Artificial Intelligence.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT ?05: Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 347?354.
Li Zhuang, Feng Jing, Xiao-yan Zhu, and Lei Zhang.
2006. Movie review mining and summarization. In
Proceedings of the ACM SIGIR Conference on Infor-
mation and Knowledge Management (CIKM).
76
Proceedings of the SIGDIAL 2013 Conference, pages 21?30,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Toward a Better Understanding of Causality between Verbal Events:
Extraction and Analysis of the Causal Power of Verb-Verb Associations
Mehwish Riaz and Roxana Girju
Department of Computer Science and Beckman Institute
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{mriaz2, girju}@illinois.edu
Abstract
The identification of causal relations be-
tween verbal events is important for
achieving natural language understanding.
However, the problem has proven notori-
ously difficult since it is not clear which
types of knowledge are necessary to solve
this challenging problem close to human
level performance. Instead of employing a
large set of features proved useful in other
NLP tasks, we split the problem in smaller
sub problems. Since verbs play a very im-
portant role in causal relations, in this pa-
per we harness, explore, and evaluate the
predictive power of causal associations of
verb-verb pairs. More specifically, we pro-
pose a set of knowledge-rich metrics to
learn the likelihood of causal relations be-
tween verbs. Employing these metrics, we
automatically generate a knowledge base
(KBc) which identifies three categories
of verb pairs: Strongly Causal, Ambigu-
ous, and Strongly Non-causal. The knowl-
edge base is evaluated empirically. The re-
sults show that our metrics perform signif-
icantly better than the state-of-the-art on
the task of detecting causal verbal events.
1 Introduction
The identification of semantic relations between
events is a mandatory component of natural lan-
guage understanding. In this paper, we focus
on the identification of causal relations between
events represented by verbs. Following Riaz and
Girju (2010), we define a verbal event evi as
?[subjectvi] vi [objectvi]?, where the subject and
object of the verb may or may not be explicitly
present in an instance. Consider the following ex-
amples:
1. Yoga builds stamina because you maintain your poses
for a certain period of time. (CAUSE (emaintain, ebuild))
2. The monster storm Katrina raged ashore along the
Gulf Coast Monday morning. There were early re-
ports of buildings collapsing along the coast. (CAUSE
(erage, ecollapse))
In example 1, the two bold events are causally
connected by an explicit and unambiguous dis-
course marker (because). However, in English,
not all discourse markers unambiguously iden-
tify causality (Prasad et al, 2008) - for exam-
ple, Bethard and Martin (2008) proposed a cor-
pus of 1000 causal and non-causal event pairs con-
joined by the marker and. Even more, causal re-
lations can be encoded by implicit contexts - i.e.,
those where no discourse marker is present (ex-
ample 2). Despite the recent achievements ob-
tained in discourse processing, it is still unclear
what types of knowledge can contribute most to-
wards detecting causality in both explicit and im-
plicit contexts (Sporleder and Lascarides, 2008).
The complexity of the task of detecting causality
between events stems from the fact that there are
many factors involved, such as contextual features
of an instance (e.g., lexical items, tenses of verbs,
arguments of verbs, etc.), semantic and pragmatic
features of events, background knowledge, world
knowledge, common sense, etc. Prior approaches
have employed contextual features of an instance
to identify causality between events or discourse
segments (Bethard and Martin, 2008; Pitler and
Nenkova, 2009; Pitler et al, 2009). Although
contextual features provide important knowledge
about sentence(s) in which events appear, humans
also make use of other information such as back-
ground knowledge to comprehend causality. For
instance, in example 2 we use knowledge about
the causal association between verbal entities rage
and collapse to label it with causality.
This research is motivated by the need to extract
and analyze other type of knowledge necessary for
the identification of causal relations between ver-
bal events. We start from the fact that verbs are the
21
main components of language to express events
and semantic relations between events. Thus, in
order to identify and extract causal relations be-
tween events (denoted by (evi , evj )), it is critical
for a model to employ knowledge about the ten-
dency of a verb pair (vi, vj) to encode causation.
For example, the pair (kill, arrest) has a high ten-
dency to encode a cause relation irrespective of the
context in which it is used, thereby a good indica-
tor of causality. The state-of-the-art resources on
verb semantics, such as WordNet, VerbNet, Prop-
Bank, FrameNet, etc. (Miller, 1990; Kipper et al,
2000; Kingsbury et al, 2002; Baker et al, 1998),
provide information about the semantic classes,
thematic roles and selectional restrictions of verbs.
Among these, WordNet is the only resource which
provides information about the cause relation be-
tween verbs, but it has very limited coverage.
For VERBOCEAN, a semi-automatically generated
resource, Chklovski and Pantel (2004) have used
explicit lexical patterns (e.g., ?verb * by verb?) as
means of mining enablement (cause-effect) rela-
tions between verbs. Such approaches help detect-
ing causality with high precision but suffer from
limited coverage due to the highly implicit na-
ture of language. Moreover, such resources do
not provide any information about the likelihood
of a causal relation in verb pairs - e.g., (kill, ar-
rest) has a high tendency to encode cause rela-
tion as compared with the pair (build, maintain).
The pair (build, maintain) seems ambiguous be-
cause it can encode both cause and non-cause re-
lations depending on the context, as shown by ex-
amples 1 and 3. Thus, causality detection models
should employ knowledge about which verb pairs
are strongly causal (non-causal) in nature and for
which pairs the context plays an important role to
signal causality.
3. Republicans had not cut the funds for maintaining the
levee and building up the ecological protections. (NON-
CAUSE)
We propose a fully automated procedure to learn
the likelihood of causal relations in verb pairs. In
this process, we create three categories of verb
pairs: Strongly Causal (Sc), Ambiguous (Ac) and
Strongly Non-causal (S?c). The result is a knowl-
edge base (KBc) of causal associations of verbs.
In KBc, the category Sc (S?c) contains the verb
pairs which have the greatest (least) likelihood to
encode a causal relation, respectively. However,
the category Ac contains ambiguous verb pairs
which have the likelihood to encode both causal
and non-causal relations. The information about
such causal associations provides a rich knowl-
edge source to causality detection models.
The main contributions of our research are as
follows:
? We propose a set of novel metrics (i.e., Explicit
Causal Association (ECA), Implicit Causal As-
sociation (ICA) and Boosted Causal Associa-
tion (BCA)) to identify the likelihood of verb
pairs to encode causality. Our metrics exploit
the information available from a large number
of unlabeled explicit and implicit instances of
verb pairs for this purpose.
? We introduce an automated procedure to build
a training corpus of causal and non-causal
event pairs. This prevents us from the trou-
ble of annotating a large number of event pairs
for cause and non-cause relations. Our metrics
make use of supervision from the training cor-
pus to identify causality in verb pairs. We also
provide a mechanism to determine causal verb
pairs which remain undiscovered due to the is-
sue of training data sparseness.
? We revisit recent approaches employing distri-
butional similarity methods to predict causal-
ity between events (Riaz and Girju, 2010;
Do et al, 2011). The state-of-the-art met-
ric Cause-Effect Association (CEA) (Do et
al., 2011) identifies causality mainly based on
probabilities of verb-verb, verb-argument, and
argument-argument pairs. In comparison with
CEA, our metrics perform significantly better
by improving the prior knowledge about the
causal associations from CEA?s components.
After a brief review of related work in next sec-
tion, we describe our approach for acquisition of
training corpus in section 3. The model for the ex-
traction of causal associations is presented in sec-
tion 4, followed by the evaluation and discussion
in section 5 and conclusion in section 6.
2 Related Work
Causality has long been studied from various
perspectives by philosophers, data-mining re-
searchers and computer scientists (Menzies, 2008;
Woodward, 2008; Suppes, 1970; Silverstein et al,
2000; Pearl, 2000).
In NLP, the problem of detecting causality be-
tween events is a very challenging but less re-
searched topic. Previously, researchers have stud-
22
ied this task by focusing on supervised classifi-
cation models for both verbal and nominal events
(Girju, 2003; Bethard and Martin, 2008). Bethard
and Martin (2008), for example, have focused
mainly on the contextual features available in test
instances of verbal event pairs to predict causality.
They have relied on a small scale dataset of 1000
instances (697 training and 303 test) for this task.
Unlike above models, recently some researchers
have employed unsupervised causality detection
metrics and minimal supervision for this task. For
example, Riaz and Girju (2010) have proposed an
unsupervised metric Effect-Control Dependency
(ECD) to determine causality between events in
news scenarios. Following their model, Do et al
(2011) introduced an improved metric CEA which
uses PMI and some components of ECD to pre-
dict the causal relation in verbal and nominal event
pairs in a text document. They also proposed a
minimally supervised method using explicit dis-
course markers. For example, they used ILP
framework to assign a non-causal relation to all the
event pairs appearing in two discourse segments
connected by a non-causal marker. They evalu-
ated their model on a set of 20 documents, a highly
skewed evaluation set with around 2-3% causal
instances and 58% human inter-annotator agree-
ment on cause-effect relations. On verbal events,
they reported 38.3% F-score with CEA and 1-2%
improvement using minimally supervised method.
As compared with above mentioned metrics, we
introduce knowledge rich association measures
which employ supervision from the automatically
generated training corpus to learn causality.
Several other NLP researchers have studied
related topics e.g., identifying events, building
of temporal chain of events sharing a common
protagonist (participant), predicting future events
and identifying hidden links in news articles to
build a coherent chain (Chambers and Jurafsky,
2008; Chambers and Jurafsky, 2009; Radinsky
and Horvitz, 2013; Shahaf and Guestrin, 2010).
Unlike these tasks, our focus is on identifying
causality between events.
3 Acquisition of Training Corpus
In this section, we propose a fully automated pro-
cedure to build a training corpus of event pairs
which encode cause and non-cause relations. This
training corpus is used in our model to identify the
likelihood of cause relations in verb pairs. As dis-
cussed earlier, previous researchers have worked
with a small scale dataset of annotated event pairs.
The current task requires us to use a large train-
ing corpus to learn the pervasive relation of causal-
ity and the manual generation of such corpus is a
laborious task. Therefore, we decided to depend
on the unambiguous discourse markers because
and but to automatically collect training instances
of cause and non-cause event pairs, respectively.
For example, the marker because in the instance
1 of section 1 encodes a cause relation between
the events ebuild and emaintain. Some researchers
have utilized unambiguous discourse markers to
acquire training instances of semantic relations be-
tween discourse segments (Marcu and Echihabi,
2001; Sporleder and Lascarides, 2008). However,
the process is not simple for the current problem
since it is not always clear how to create a causal
instance of an event pair. Consider the following
meta instance I:
I : <s>/m1 . . . v1 . . . v2 . . . vk . . . because . . . vk+1
. . . vk+2, . . ., vr, . . .m2/</s>.
It is composed of main verbs (v1, v2, . . .,
vr), discourse markers (m1, m2), and sentence
boundaries (<s>, </s>). Here, we assume that
the discourse markers or the sentence boundaries
whichever appear first in I represent the bound-
aries of discourse segments for the marker because
(appendix A contains a table of notations used in
this paper). In I , there are k and r ? k main verbs
appearing before and after because, respectively.
The problem here is to determine the event pair en-
coding causality out of k? (r? k) choices. Here,
we consider that the most dependent pair among
all choices in I is the best candidate to encode
causality.
In this work, we propose the following function
f(I) to pick the most dependent pair:
f(I) = argmax
(vi?mc ,vjmc )
CD(vi, vj)? PSI(vi, vj) (1)
Here, i (j) refers to all verbs that appear be-
fore (after) the causal marker (i.e., mc) because in
I . CD (equation 2) is a component of predicate-
predicate association of CEA (Do et al, 2011)
to determine causal dependency of a pair (vi, vj).
Do et al (2011) used the score CD to determine
causality in an unsupervised fashion but here we
employ this to build a training corpus of causal
event pairs.
CD(vi, vj) = PMI(vi, vj)?max(vi, vj)? IDF (vi, vj) (2)
23
The functions PMI, max and IDF depend on co-
occurrence probabilities and idf scores to deter-
mine causal dependency. Due to space limitations,
for details we refer the reader to Do et al (2011).
Next, we define a novel penalization factor PSI
for the verbs of a pair appearing at greater distance
from the causal marker because. For example, this
assumes the verbs in the pair (v2, vk+2) are less
likely to be in a cause relation as compared with
(vk, vk+1) in I . We came up with this idea because
our initial experiments revealed that the causal in-
stances obtained by penalizing CD with PSI pro-
vide better training for our model as compared to
using only CD for this purpose. The similar be-
havior of reduction in the likelihood of causality
with respect to increase in distance between two
events was observed by Riaz and Girju (2010).
PSI(vi, vj) = ? log
pos(vi) + pos(vj)
2.0? (C(vp) + C(vq))
(3)
Here, C(vp) (C(vq)) is the count of the main
verbs appearing before (after) because, respec-
tively. The distance of the verb is measured in
terms of its position (i.e., pos(vi)) with respect to
because. The position is 1 for the verb closest to
because and 2 for the verb next to the closest verb.
PSI has maximum value for (vk, vk+1) and it re-
duces for other pairs with verbs at greater distance
from because in instance I .
In order to extract non-causal event pairs, we
utilized instances with two discourse segments
conjoined by the marker but which represents
comparison (non-causal) relation. Any event pair
collected from the two discourse segments in non-
causal relation encodes non-causality. Therefore,
we depend on selecting the closest verb pair from
the instances of form I with marker but instead of
because.
In this paper, we present the results produced
using a training corpus of 240K instances (50%
for each class) from the English Gigaword Cor-
pus. In order to prepare this corpus, we identified
discourse markers (i.e., m1, m2), if available, be-
fore and after because/but in each instance I and
assumed that only those markers which have dis-
course usage in I define boundaries of discourse
segments of because/but. We used the list of 100
explicit discourse markers provided by Prasad et
al. (2008) and the supervised approach of Pitler
and Nenkova (2009) to detect markers and the dis-
course versus non-discourse usage of these mark-
ers. We use this training corpus to identify cau-
sation for both explicit and implicit instances of
event pairs. Using this training corpus, a model
tends to give higher causal weights to those in-
stances in which events are connected by the ex-
plicit causal marker because as compared to im-
plicit instances of causation. Thus, to provide fair
supervision to both explicit and implicit instances
of event pairs, we remove the cue words because
and but which were used to automatically label the
training instances.
4 Causal Associations of Verb Pairs
In this section, we explain our approach to learn
the likelihood of causal relations in verb pairs by
exploiting information available from both explicit
and implicit instances of these pairs. We extracted
around 12, 000 documents from the English Gi-
gaword corpus to collect instances of verb pairs
from single sentences (intra-sentential) and adja-
cent sentences (inter-sentential) of text. In this set,
we added instances from 3, 000 articles on news
stories ?Hurricane Katrina? and the ?Iraq war?.
These articles were collected and used to iden-
tify causal relations in news scenarios by Riaz and
Girju (2010). We used these collections because
natural disaster and war-related news articles are
rich in causal events and chains of such events.
In order to identify the causal associations with
high confidence, we decided to apply our model on
those verb pairs which have at least 30 instances
in the above mentioned documents. We acquired
10, 455 such verb pairs. The set of intra- and inter-
sentential instances of these verb pairs is referred
to as the development set for our model.
4.1 Explicit Causal Association (ECA)
In order to find the likelihood of a verb pair to en-
code causal relations, we define our novel metric
Explicit Causal Association (ECA) as follows:
ECA(vi, vj) =
1
| V P |
?
I(vi,vj)?V P
(CD(vi, vj)? CI) (4)
where V P is the set of intra- and inter-sentential
instances (denoted by I(vi, vj)) of the verb pair
(vi, vj), CD determines the causal dependency of
the verb pair in unsupervised fashion (equation 2),
and CI finds the tendency of instance I of (vi, vj)
to belong to the cause class as compared to the
non-cause class using training corpus of event
pairs. The goal of ECA is to combine the unsu-
pervised causal dependency (i.e., CD) with the su-
pervised score of instance I of belonging to cause
24
class than the non-cause one (i.e., CI ). Here, CD
represents the prior knowledge about the causal
association based on co-occurrence probabilities
and idf scores (equation 2). It can discover lots
of false positives because the co-occurrence prob-
abilities can fail to differentiate causality from any
other type of correlation. Therefore, we improve
this prior knowledge with the help of supervision
from the training corpus containing instances of
both cause and non-cause relations. The global
decision of the causal association is made by tak-
ing the average of scores on all the instances con-
taining that verb pair. Notice that CD can also be
moved out from the summation function in equa-
tion 4.
We define the function CI as follows:
CI =
n?
k=1
log( P (fk | c)P (fk | ?c)
) (5)
Here, the notations c and ? c represent cause
and non-cause class, respectively. The notation
fk represents the feature of an instance I . In this
work, we use some language features of events
and context of an instance I which are defined
later in this section. P(fk | c) and P(fk | ?c) are
the smoothed probabilities of feature fk given the
cause and non-cause training instances. The value
of CI is positive only when the instance I has more
tendency to encode a cause relation than a non-
cause one. To avoid negative values, we map CI
scores to the range [0, 1] using CI?CminCmax?Cmin whereCmin (Cmax) is the minimum (maximum) value of
CI obtained on our development set, respectively.
Also, we add a small value  to CI to avoid 0 value.
Similarly, to avoid negative scores of PMI in equa-
tion 2 we can map it to the range [0,1].
We present below the features for the calcula-
tion of CI . We use lexical, syntatic and semantic
features on verbs and verb phrases of both events
of a pair. These features include words, lemmas,
part-of-speech tags, all senses from WordNet for
the verbs and the lexical items of verb phrases.
These features were introduced by Bethard and
Martin (2008) (for an in-depth description of these
features see Bethard and Martin (2008)). Next, we
describe the set of features which are the contribu-
tions of this research.
1. Verbs Arguments: Words, lemma, part-of-
speech tags and all senses from WordNet for
subject and object of verbs of both events.
2. Verbs and Arguments Pairs: For this fea-
ture, we take the cross product of both
events of a pair (evi ,evj ) where evi =
[subjectvi] vi [objectvi] and evj = [subjectvj ]
vj [objectvj ]. Some examples of this fea-
ture are (subjectvi ,subjectvj ), (subjectvi ,vj),
(subjectvi ,objectvj ), etc. In this work, we use
unordered pairs as features (i.e., (vi,vj)) is
same as (vj ,vi)) because the temporal order of
events is unknown for the unlabeled develop-
ment set instances. In future, this feature can
be improved by adding temporal information.
The next three features are taken from the min-
imum relevant context (mincontext) of a verb
pair which we define as follows. mincontext of
a pair (vi, vj) in an intra-sentential instance is
<s>/m1 . . . vi . . . vj . . .m2/</s> ? i.e., words be-
tween the discourse markers (i.e., m1, m2) or sen-
tence boundaries (i.e., <s>, </s>) whichever ap-
pear first in the sentence. The mincontext for the
pair (vi, vj) in an inter-sentential is given below:
<s> / m1 . . . vi . . .m2 / </s>
<s> / m1 . . . vj . . .m2 / </s>
3. Context Words: Lemmas of all words from
mincontext. This feature captures words other
than two events.
4. Context Main Verbs: All main verbs and their
lemmas from mincontext. It collects informa-
tion about all verbs that appear with the causal
and non-causal event pair.
5. Context Main Verb Pairs: The pairs of main
verbs from mincontext. The lemmas are taken
from the feature ?Context Main Verbs? and
then the pairs on these lemmas are used as this
feature. For example, for lemmas of verbs (i.e.,
v1, v2, . . . , vk), pairs (i.e., (v1, v2), (v1, vk),
etc.) are used for this feature. This feature
is used to get information about the interest-
ing causal chains of verbs that may appear in
causal instances.
We propose next a novel metric ICA to avoid
the problem of training data sparsity.
4.2 Implicit Causal Association (ICA)
In order to determine the causal associations us-
ing ECA, we depend on explicit cause and non-
cause training instances for supervision. However,
it is possible that some strongly causal verb pairs
may frequently appear in implicit causal contexts.
Therefore, the causality of such pairs can remain
uncaptured by ECA which merely relies on ex-
plicit training instances. For example, a pair (fall,
25
break) seems strongly causal, but it does not ap-
pear often in our explicit training corpus due to
training data sparsity. Thus, in order to handle
this problem, we propose a new metric called ICA.
This metric makes use of functions for the identi-
fication of roles of events in a cause relation. After
briefly describing the roles of events in causal re-
lations below, we continue with the description of
ICA.
4.2.1 Roles of Events in Cause Relation
Each of the two events in a cause relation can be
assigned either cause or effect role. For example
for the following training instance, the verb ap-
pearing after because represents cause event and
the verb before because represents effect event.
1. Yoga builds stamina because you maintain your poses
for a certain period of time. (Role: rC )
2. Yoga builds stamina because you maintain your poses for
a certain period of time. (Role: rE)
The notation rC and rE represents the classes of
cause and effect role of events, respectively. We
use core features of events to determine the like-
lihood of their roles in causation. These features
include lemma, part-of-speech tag, all senses from
WordNet of both verbs and their arguments (i.e.,
subject and object). Next, we use these features to
handle training data sparseness.
4.2.2 Handling of Training Data Sparsity
To deal with the problem of training data sparsity,
we define the metric ICA as follows:
ICA(vi, vj) =
1
| V P |
?
I(vi,vj)?V P
(CD(vi, vj)? CI
?ERM(evi ,evj )) (6)
where CD and CI are defined earlier and ERM
determines the likelihood of roles of the events in
the cause relation. We remind the reader that CD
is the unsupervised causal dependency of verb pair
and CI is the likelihood of instance I of the verb
pair to belong to the cause class than the non-cause
one using full set of features from section 4.1.
Events Roles Matching (ERM(evi ,evj )) (equa-tions 7 and 8) is the negative log-likelihood of
events evi and evj appearing as cause or effect role
determined using the explicit causal instances of
the training corpus and the core features of events
defined in section 4.2.1.
ERM(evi ,evj ) = ?1.0?max(S(evi , rC) + S(evj , rE),
S(evi , rE) + S(evj , rC)) (7)
S(evi , rC) =
n?
k=1
log(P (fk | rC)) (8)
S(evj , rE) =
n?
k=1
log(P (fk | rE))
Here, S(evi , rC) is the score of evi being the
cause event and S(evj , rE) is the score of evj be-
ing the effect event. These scores are computed
using smoothed probabilities ? i.e., P(fk | rC) and
P(fk | rE). Similarly, S(evi , rE) and S(evj , rC)
are calculated and max is taken. The high value
of ERM represents low matching of an event pair
(verbs and their arguments) in the explicit causal
instances of the training corpus. The high value
of ERM of an event pair can have one of the fol-
lowing two interpretations: (A) it is a non-causal
event pair, or (B) it is a causal event pair but this
pair and the pairs which are semantically closer to
it hardly appear in explicit causal contexts. In the
metric ICA, CI? CD(vi, vj) is used as a guiding
score to interpret ERM as follows:
1. If CI? CD(vi, vj) has high score then the value
of ERM is not penalized by this guiding score
because ERM?s value can be interpreted using
(B) above.
2. If CI? CD(vi, vj) has low score then the value
of ERM is penalized by this guiding score be-
cause (evi , evj ) can be a non-causal pair ac-
cording to the interpretation (A) above.
ICA is a boosting factor to determine causal
verb pairs which remain undiscovered because of
training data sparseness. We also define a Boosted
Causal Association (BCA) metric by adding ICA
to original ECA metric as follows:
BCA(vi, vj) =
1
| V P |
?
I(vi,vj)?V P
(CD(vi, vj)? CI +
CD(vi, vj)? CI ? ERM(evi ,evj )) (9)
To build the knowledge base of causal asso-
ciations (KBc), we generate a ranked list of all
verb pairs based on the likelihood of causality en-
coded by these pairs. Here, we assume that verb
pairs are uniformally distributed across three cat-
egories - i.e., top one-third and bottom one-third
ranked verb pairs belong to Strongly Causal (Sc)
and Strongly Non-Causal (S?c) categories and rest
of the pairs are considered Ambiguous (Ac). Fol-
lowing our assumption, we evaluate this catego-
rization in next section, but in future researchers
can perform empirical study of how to automat-
ically cluster verb pairs into three or more cate-
gories with respect to causation.
26
5 Evaluation and Discussion
In this section, we present our evaluation of
knowledge base to identify causality between ver-
bal events. Specifically we performed experiments
to evaluate (1) the ranking of verb pairs based on
their likelihood of encoding causality, and (2) the
quality of the three categories of verb pairs inKBc
(i.e., Sc, Ac and S?c). For this purpose, we col-
lected two test sets. For each test set, we randomly
selected 50 verb pairs from the list of 10, 455 verb
pairs in KBc. For each verb pair, we selected
randomly 3 intra- and 3 inter-sentential instances
from the English Gigaword corpus and the ?Hur-
ricane Katrina? and ?Iraq war? articles. In order
to keep the development set different from the test
sets, we automatically traversed the development
set to determine if any test instance is available in
it. In case of finding any such test instance, we
removed it from the development set to perform
evaluation on unseen test instances. Two annota-
tors were asked to provide Cause or Non-Cause
labels for each instance. They were provided with
annotation guidelines from the manipulation the-
ory of causality (Woodward, 2008). Given these
guidelines have been successfully used by Riaz
and Girju (2010), we use them here as well. For
ease of annotation, we randomly selected inter-
sentential instances such that the length of each
sentence is at most 40 words.
The human inter-annotator agreement achieved
on Test-set1 (Test-set2) is 90% (88.3%) and the
agreement on the cause class is 70% (62.7%), re-
spectively. The kappa score on Test-set1 (Test-
set2) is 0.75 (0.69), respectively. The Test-set1
(Test-set2) contains 25% (22%) causal instances,
respectively.
We employed Spearman?s rank correlation co-
efficient (equation 10) to compare the ranked list
of verb pairs based on the scores of our metrics
and the rank given by the human annotators. The
score P ranges from +1 to ?1 where +1 and ?1
show strong and negative correlation, respectively.
P = n(
?
xiyi)? (
?
xi)(
?
yi)?
n(
?
x2i )? (
?
xi)2
?
n(
?
y2i )? (
?
yi)2
(10)
Here, n is the total number of verb pairs in the
test set, xi is the human annotation rank and yi is
the metric?s rank of verb pair i of the test set. The
values of xi and yi are determined as follows. For
each verb pair, Ch is calculated which is the num-
ber of cause labels given by both human annota-
Metric CEA ECA ICA BCA
Test-set1 -0.077 0.144 0.427 0.435
Test-set2 0.167 0.217 0.353 0.338
Table 1: The Spearman?s rank correlation coeffi-
cient for the metrics CEA, ECA, ICA and BCA.
Figure 1: The percentage of causal (%c) and non-
causal (%?c) test instances in Sc,Ac and S?c gen-
erated by the metrics CEA, ECA, ICA and BCA.
tors out of 6 instances of a verb pair. The pairs are
ranked in descending order according to the score
Ch s.t. the top scored pair(s) gets rank 50 and the
next to the top pair(s) gets rank 49 and so on. Sim-
ilarly, ranks are given to the verb pairs according
to the metric?s scores. This way of evaluation was
also used by Beamer and Girju (2009) for tempo-
rally ordered adjacent verb pairs. But here, we are
working with verb pairs appearing in any temporal
order in both intra- and inter-sentential instances.
We used ECA, ICA and BCA scores to gener-
ate the ranked list of all verb pairs. In this work,
we also used the state-of-the-art causality iden-
tifier CEA (Do et al, 2011) as baseline metric.
For each verb pair, we computed the likelihood of
causality by taking the average of CEA scores on
all instances of that pair in the development set.
The results with Spearman?s rank correlation
coefficient in Table 1 show that CEA is not very
capable of matching the human ranked list of pairs
as compared with our metrics (i.e., ECA, ICA and
BCA). Specifically, the difference is significant
for Test-set1 where the correlation coefficient with
CEA goes below 0. This behavior of CEA makes
sense because it is unsupervised and requires more
knowledge to perform well. As compared with
ECA, both ICA and BCA perform significantly
better to match human ranking. The Spearman?s
score gain by BCA on Test-set1 is of about 30
(52) points over ECA (CEA) and the gain by ICA
on Test-set2 is of about 13 (18) points over ECA
(CEA), respectively.
In order to explain the behavior of our metrics
27
more clearly, we performed an evaluation of three
categories of verb pairs as follows. We generated
three categories of verb pairs using our metrics and
CEA. We combined two test sets to show the per-
centage of total causal and non-causal instances of
verb pairs that lie in Sc, Ac and S?c using follow-
ing procedure. If a verb pair belongs to Sc and has
3 causal and 2 non-causal instances after human
agreement, then these 5 instances are considered
members of Sc. This step is performed for all verb
pairs in the test set. After this the percentage of
total causal and non-causal test instances are cal-
culated for each category (see Figure 1).
Figure 1 reveals that ICA, BCA and CEA are
successful in pulling more causal instances in Sc
as compared to ECA. But, CEA has a hard time
distinguishing cause from non-cause instances be-
cause it also brings the highest percentage of non-
causal instances in Sc. The reason is the depen-
dence of CEA on PMI scores of pairs of verbs and
arguments to make decision for causality where
PMI is not good enough to distinguish a simple
correlation from an asymmetric relation of causal-
ity. However, ICA and BCA work better by plac-
ing less non-causal instances in Sc as compared
with CEA. ICA and BCA also work better be-
cause by pulling more causal instances in Sc and
Ac, these metrics are keeping least percentage of
causal instances in S?c. Also, ICA and BCA
bring more causal instances in Sc as compared
with ECA by handling training data sparseness.
Another important line of research is the con-
struction of a classifier on top of the component
of knowledge base for the classes of cause and
non-cause relations. This allows us to evaluate our
model in terms of standard evaluation measures -
i.e., precision, recall and F-score. These measures
can also be used to compare our model with su-
pervised classifier depending merely on shallow
contextual features with no information from the
knowledge base. Due to space limitations, we plan
to present such classifiers and evaluation in the fu-
ture.
5.1 Analysis
In this work, we have focused on determining the
predictive power of knowledge of causal associ-
ations of verb pairs to identify causality between
events. Our results reveal that our best metrics
(i.e., ICA and BCA) bring desired behavior of
keeping least percentage of total causal instances
in category S?c. However, there is need to build a
classifier on top of knowledge base which can help
detection of non-causal instances for verb pairs lie
in Sc and Ac. Here, we state some brief details
of our test set which can help building such clas-
sifier in future. An important aspect to consider
is the highly skewed nature of real distribution of
test set. There are only 23.69% causal instances
in the test set and majority of these instances (i.e.,
56.7%) are intra-sentential instances. Therefore, a
classifier should have mechanism to decide why
inter-sentential instances of event pair are non-
causal most of the time. For example, some inter-
sentential events may not even be directly relevant
at first place because they appear in different sen-
tences. Another critical point to consider is the en-
coding of non-causal instances by strongly causal
verb pairs. For example, we asked one of the an-
notators to identify strongly causal verb pairs out
of 100 verb pairs of the test set. There are 22
such verb pairs determined by our annotator and
each of these pairs contain 43% causal instances
on the average. There are many factors (e.g., tem-
poral information, arguments of verbs) which can
make an instance of strongly causal verb pair non-
causal. For example, (call, respond) may encode
causality only if ecall temporally precedes erespond
as demonstrated by the following instances.
1. Deputies spotted the truck parked at the home of the sus-
pect?s father and called for assistance. The Border Patrol
agents and others responded. (CAUSE)
2. Prime Minister of Israel promptly responded to the
widespread unrest in the West Bank and Gaza, saying that
he would call a timeout to rethink Israel?s commitment to
peace talks. (NON-CAUSE)
In future, the above issues need to be addressed
to improve performance for the current task.
6 Conclusion
In this research, we have developed a knowledge
base (KBc1) of causal associations of verb pairs
to detect causality. This resource provides the
causal associations in terms of three categories of
verb pairs (i.e., Strongly Causal, Ambiguous and
Strongly Non-Causal). We have proposed a set of
knowledge rich metrics to learn these associations.
Our analysis of results reveals the biases of differ-
ent metrics and brings important insights into the
future research directions to address the challenge
of detecting causality between verbal events.
1We will make the resource available.
28
References
Collin F. Baker, Charles J. Fillmore and John B. Lowe.
1998. The Berkeley FrameNet project. In proceed-
ings of COLING-ACL. Montreal, Canada.
Brandon Beamer and Roxana Girju. 2009. Using a Bi-
gram Event Model to Predict Causal Potential. In
proceedings of Computational Linguistics and intel-
ligent Text Processing (CICLING), 2009.
Steven Bethard and James H. Martin. 2008. Learning
Semantic Links from a Corpus of Parallel Temporal
and Causal Relations. In proceedings of ACL-08:
HLT.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In pro-
ceedings of ACL-HLT 2008.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In proceedings of ACL 2009.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic
Verb Relations. In proceedings of Conference on
Empirical Methods in Natural Language Processing
(EMNLP-04). Barcelona, Spain.
Quang X. Do, Yee S. Chen and Dan Roth. 2011. Min-
imally Supervised Event Causality Identication. In
proceedings of EMNLP-2011.
Roxana Girju. 2003. Automatic detection of causal
relations for Question Answering. Association for
Computational Linguistics ACL, Workshop on Mul-
tilingual Summarization and Question Answering
Machine Learning and Beyond 2003.
Paul Kingsbury, Martha Palmer and Mitch Marcus.
2002. Adding semantic annotation to the Penn Tree-
Bank. In proceedings of HLT-2002. San Diego, Cal-
ifornia.
Karin Kipper, Hoa T. Dang, and Martha Palmer. 2000.
Class-based construction of a verb lexicon. In pro-
ceedings of AAAI-2000. Austin, TX.
Daniel Marcu and Abdessamad Echihabi. 2001. An
unsupervised approach to recognizing discourse re-
lations. In proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics
(ACL).
Peter Menzies. 2008. Counterfactual theories of cau-
sation. Online Encyclopedia of Philosophy, 2008.
George A. Miller. 1990. WordNet: An online lexi-
cal database. International Journal of Lexicography,
3(4).
Judea Pearl. 2000. Causality. Cambridge University
Press.
Emily Pitler, Annie Louis and Ani Nenkova. 2009.
Automatic Sense Prediction for Implicit Discourse
Relations in Text. In proceedings of ACL-IJCNLP,
2009.
Emily Pitler and Ani Nenkova. 2009. Using Syntax
to Disambiguate Explicit Discourse Connectives in
Text. In proceedings of ACL-IJCNLP, 2009.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, Bonnie Web-
ber. 2010. The penn discourse treebank 2.0. In
proceedings of LREC 2008.
Kira Radinsky and Eric Horvitz. 2013. Mining the
Web to Predict Future Events. In proceedings of
sixth ACM international conference on Web search
and data mining, WSDM ?13.
Mehwish Riaz and Roxana Girju. 2010. Another Look
at Causality: Discovering Scenario-Specific Contin-
gency Relationships with No Supervision. In pro-
ceedings of the IEEE 4th International Conference
on Semantic Computing (ICSC).
Dafna Shahaf and Carlos Guestrin. 2010. Connecting
the Dots Between News Articles. In proceedings of
Knowledge Discovery and Data Mining KDD 2010.
Craig Silverstein, Sergey Brin, Rajeev Motwani and
Jeff Ullman. 2000. Scalable Techniques for Min-
ing Causal Structures. Data Mining and Knowledge
Discovery, 2000, 4(2?3):163?192.
Caroline Sporleder and Alex Lascarides. 2008. Using
automatically labelled examples to classify rhetor-
ical relations: An assessment. Journal of Natural
Language Engineering Volume 14 Issue 3, July 2008
Pages 369?416.
Patrick Suppes. 1970. A Probabilistic Theory of
Causality. Amsterdam: North-Holland Publishing
Company, 1970.
James Woodward. 2008. Causation and Manipulation.
Online Encyclopedia of Philosophy, 2008.
29
Appendix A. Notations
This appendix presents the details of important notations used in this paper.
Notation Equation(s) Explanation
evi 6, 7, 8, 9 Verbal event represented by the verb vi
KBc ? Knowledge base of causal associations of verb pairs
Sc ? Strongly Causal category of verb pairs
Ac ? Ambiguous category of verb pairs
S?c ? Strongly Non-Causal category of verb pairs
mi ? Discourse marker
mc 1 Causal marker (e.g., because)
f(I) 1 Function to select the most dependent pair from two dis-
course segments conjoined with causal marker
CD(vi, vj) 1, 2, 4, 6, 9 Causal dependency of the verb pair (vi, vj)
PSI(vi, vj) 1, 3 Penalization factor for the verbs of the pair (vi, vj) with
respect to their distance from the causal marker
pos(vi) 3 Distance of verb in terms of its position with respect to
causal marker
C(vp) 3 Count of main verbs appearing before causal marker
C(vq) 3 Count of main verbs appearing after causal marker
ECA(vi, vj) 4 Explicit Causal Association of the verb pair (vi, vj)
V P 4, 6, 9 Set of intra- and inter-sentential instances of a verb pair
I(vi, vj) 4, 6, 9 Instance of the verb pair (vi, vj)
CI 4, 5, 6, 9 Tendency of the instance I to belong to cause class than
the non-cause one
c 5 Cause class
?c 5 Non-cause class
Cmin ? Minimum value of CI obtained on the development set
Cmax ? Maximum value of CI obtained on the development set
rC 7, 8 Class of cause role
rE 7, 8 Class of effect role
ICA(vi, vj) 6 Implicit Causal Association of the verb pair (vi, vj)
ERM(evi , evj ) 6, 7 Events Roles Matching (ERM) determines the negativelog-likelihood of events to belong to class of cause or
effect role
S(evi , rC) 8 Score of evi to belong to the class of cause role
S(evj , rE) 8 Score of evj to belong to the class of effect role
P (fk|.) 5, 8 Probability of feature fk given some class
BCA(vi, vj) 9 Boosted Causal Association of the verb pair (vi, vj)
Table 2: Details of notations.
30
