Fine-Grained Hidden Markov Modeling for Broadcast-
News Story Segmentation
Warren Greiff, Alex Morgan, Randall Fish, Marc Richards, Amlan Kundu,
MITRE Corporation
202 Burlington Road
Bedford, MA 01730-1420
(greiff, amorgan, fishr, marc, akundu)@mitre.org
ABSTRACT
We present the design and development of a Hidden Markov
Model for the division of news broadcasts into story segments.
Model topology, and the textual features used, are discussed,
together with the non-parametric estimation techniques that were
employed for obtaining estimates for both transition and
observation probabilities.  Visualization methods developed for
the analysis of system performance are also presented.
1. INTRODUCTION
Current technology makes the automated capture, storage,
indexing, and categorization of broadcast news feasible allowing
for the development of computational systems that provide for the
intelligent browsing and retrieval of news stories [Maybury,
Merlino & Morey ?97; Kubula, et al, ?00].  To be effective, such
systems must be able to partition the undifferentiated input signal
into the appropriate sequence of news-story segments.
In this paper we discuss an approach to segmentation based on the
use of a fine-grained Hidden Markov Model [Rabiner, `89] to
model the generation of the words produced during a news
program.  We present the model topology, and the textual features
used.  Critical to this approach is the application of non-parametric
estimation techniques, employed to obtain robust estimates for
both transition and observation probabilities. Visualization
methods developed for the analysis of system performance are
also presented.
Typically, approaches to news-story segmentation have been
based on extracting features of the input stream that are likely to
be different at boundaries between stories from what is observed
within the span of individual stories. In [Beeferman, Berger, &
Lafferty ?99], boundary decisions are based on how well
predictions made by a long-range exponential language model
compare to those made by a short range trigram model. [Ponte and
Croft, ?97] utilize Local Context Analysis [Xu, J. and Croft, ?96]
to enrich each sentence with related words, and then use dynamic
programming to find an optimal boundary sequence based on a
measure of word-occurrence similarity between pairs of enriched
sentences. In [Greiff, Hurwitz & Merlino, `99], a na?ve Bayes
classifier is used to make a boundary decision at each word of the
transcript.  In [Yamron, et al, ?98], a fully connected Hidden
Markov Model is based on automatically induced topic clusters,
with one node for each topic.  Observation probabilities for each
node are estimated using smoothed unigram statistics.
The approach reported in this paper goes further along the lines of
find-grained modeling in two respects: 1) differences in feature
patterns likely to be observed at different points in the
development of a news story are exploited, in contrast to
approaches that focus on boudary/no-boundary differences; and 2)
a more detailed modeling of the story-length distribution profile,
unique to each news source (for example, see the histogram of
story lengths for ABC World News Tonight shown in the top
graph of Figure 3, below).
2. GENERATIVE MODEL
We model the generation of news stories as a 251 state Hidden
Markov Model, with the topology shown in Figure 1. States
labeled, 1 to 250, correspond to each of the first 250 words of a
story.  One extra state, labeled 251, is included to model the
production of all words at the end of stories exceeding 250 words
in length.
Several other models were considered, but this model is
particularly suited to the features used, as it allows one to model
features that vary with depth into the story (Section 3.1), while
simultaneously, by delaying certain features.  It also allows one to
model features that occur in specific regions the boundaries
(Section 3.3).  This is possible because all states can feed into the
initial state, i.e. all stories end by going into the first word of a
new story.
1 2 3 250 251
Figure 1:  Current HMM Topology
For example, the original model involved a series of beginning
and then end states, with a single middle state that could be cycled
through (Figure 2).  This proved to be a problem because the ends
of long stories were being mixed with the ends of short stories
which led to problems with our spaced coherence feature (Section
3.1).  Another possibility involved splitting the model into two
main paths, one to model the shorter stories, and one to model the
longer as there is something of a bimodal distribution in story
lengths (Figure 4).  However, the fine-grained nature of our model
would suffer from splitting the data in this manner, and a choice
about at which length to fork the model would be somewhat
artificial.
3. FEATURES
Associated with the model is a set of features.  For each state, the
model assigns a probability distribution over all possible
combinations of values the features may take on.  The probability
assigned to value combinations is assumed to be independent of
the state/observation history, conditioned on the state. We further
assume that the value of any one feature is independent of all
others, once the current state is known. Features have been
explicitly designed with this assumption in mind.  Three
categories of features have been used, which we refer to as
coherence features, x-duration feature, and the trigger features.
3
W
sh
w
do
ap
of
st
tr
ra
fe
COHER-4  (Figures 3b, c & d) correspond to similar features; for
these, however, the buffer is separated by 50, 100, and 150 words,
respectively, from the current word.  Interestingly, the COHER-4
feature actually caused a reduction in performance, and was not
used in the final evaluation.
3.2. X-duration
This feature is based on indications given by the speech recognizer
that it was unable to transcribe a portion of the audio signal. The
existence of an untranscribable section prior to the word gives a
non-zero X-DURATION value based on the extent of the section.
Empirically this is an excellent predictor of boundaries in that an
untranscribable event has uniform likelihood of occurring
anywhere in a news story, except prior to the first word of a story,
where it is extremely likely to occur.
3.3. Triggers
Trigger features correspond to small regions at the beginning and
end of stories, and exploit the fact that some words are far more
likely to occur in these positions than in other parts of a news
segment.  One region, for example, is restricted to the first word of
the story.  In ABC?s World News Tonight, for example, the word
?finally? is far more likely to occur in the first word of a story than
would be expected by its general rate of occurrence in the training
data.  For a word, w, appearing in the input stream, the value of
the feature is an estimate of how likely it is for w to appear in the
region of interest.  The estimate used is given by:
( )Rw
Rw
fn
nRwp /1
1)(?
+
+
=? ?
where Rwn ? is the number of times w appeared in R in the training
data; wn  is the total number of occurrences of w; and Rf  is the
fraction of all tokens of w that occurred in the region.  This
estimate can be viewed as Bayesian estimate with a beta prior.
The beta prior is equivalent to a uniform prior and the observation
of one occurrence of the word in the region out of ( )Rf/1  total
occurrences.  This estimate was chosen so that: 1) the prior
probability would not be greatly affected for words observed only
a few times in the training data; 2) it would be pushed strongly
towards the empirical probability of the word appearing in the
region for words that were encountered in R; 3) it has a prior
probability, Rf , equal to the expectation for a randomly selected
word.  The regions used for the submission were restricted to the
one-word regions for: first word, second word, last word, and
1 2 500 501
Figure 3: Original Topology.1. Coherence
a
b
ce have used four coherence features.  The COHER-1 feature,
own schematically in Figure 2a, is based on a buffer of 50
ords immediately prior to the current word.  If the current word
es not appear in the buffer, the value of COHER-1 is 0.  If it does
pear in the buffer, the value is -log(sw/s), where sw is the number
 stories in which the word appears, and s is the total number of
ories, in the training data. Words that did not appear in the
aining data, are treated as having appeared once.  In this way,
re words get high feature values, and common words get low
ature values.  Three other features: COHER-2, COHER-3, and
next-to-last word.  Limited experimentation with multi-state
regions, was not fruitful.  For example, including the regions,
{3,4,?,10} and {-10,-9,?,-3}, where ?i is interpreted as i words
prior to the end of the story, did not improve segmentation
performance.
Since, as described, the current HMM topology does not model
end-of-story words (earlier versions of the topology did model
these states directly), trigger features for end-of-story regions are
delayed. That means that a trigger related to the last word in a
story would be delayed by a one word buffer.  In this way, it is
linked to the first word in the next story.  For example, the word
?Jennings? (the name of the main anchorperson) is strongly
d
Figure 2: Coherence Features
correlated with the last word in news stories in the ABC World
News Tonight corpus.  The estimated probability of it being the
last word of the story in which it appears is .235 (obtained by the
aforementioned method). The trained model associates a high
likelihood of seeing the value .235 at state = 1; the intuitive
interpretation being, "a word highly likely to appear at the last
word of a story, occurred 1-word ago".
4. PARAMETER ESTIMATION
The Hidden Markov Model requires the estimation of transition
and conditional observation probabilities.  There are 251 transition
probabilities to be estimated.  Much more of a problem are the
observation probabilities, there being 9 features in the model, for
each of which a probability distribution over as many as 100
values must be estimated, for each of 251 states.  With the goal of
developing methods for robust estimation in the context of story
segmentation, we have applied non-parametric kernel estimation
techniques, using the LOCFIT library [Loader, ?99] of the R open-
source statistical analysis package, which is based on the S-plus
system [Venables & Ripley,
`99; Chambers & Hastie, `92, Becker, Chambers & Wilks, `88].
For the transition probabilities, it is assumed that the underlying
probability distribution over story length is smooth, allowing the
empirical histogram, shown at the top of Figure 4, to be
transformed to the probability density estimate shown at the
bottom. From this probability distribution over story lengths, the
conditional transition probabilities can be estimated directly.
Conditional observation probabilities are also deduced from an
estimate of the joint probability distribution.  First, observation
values were binned.  Binning limits were set in an attempt to 1) be
large enough to obtain sufficient counts for the production of
robust probability estimates, and yet, 2) be constrained enough so
that important distinctions in the probabilities for different feature
values will be reflected in the model.  For each bin, the
observation counts are smoothed by performing a non-parametric
regression of the observation counts as a function of state.  The
smoothed observations counts corresponding to the regression are
then normalized so as to sum to the total observation count for the
bin.  The result is a conditional probability distribution over states
for a given binned feature value,  p(State=s|Feature=fv).  Once
this is done for all bin values, each conditional probability is
multiplied by the marginal probability, p(State=s), of being in a
given state, resulting in a joint distribution, p(fv,s), over the entire
space of (Feature,State) values.  From this joint distribution, the
necessary conditional probabilities, p(Feature=fv|State=s), can be
deduced directly.
Figure 5 shows the conditional probability estimates, p(fv | s), for
the feature value COHER-3=20, across all states, confirming the
intuition that, while the probability of seeing a value of 20 is small
for all states, the likelihood of seeing it is much higher in latter
parts of a story than it is in early-story states.
5. SEGMENTATION
Once parameters for the HMM have been determined,
segmentation is straightforward.  The Viterbi algorithm [Rabiner,
`89], is employed to determine the sequence of states most likely
to have produced the observation sequence associated with the
broadcast.  A boundary is then associated with each word
produced from State 1 for the maximum likelihood state sequence.
The version of the Viterbi algorithm we have implemented
provides for the specification of ?state-penalty? parameters, which
we have used for the ?boundary state?, state 1. In effect, the
probability for each path in consideration is multiplied by the
value of this parameter (which can be less than, equal to, or
greater than, 1) for each time the path passes through the boundary
state.  Variation of the parameter effectively controls the
?aggressiveness? of segmentation, allowing for tuning system
behavior in the context of the evaluation metric.
6. RESULTS
Preliminary test results of this approach are encouraging.  After
training on all but 15 of the ABC World News Tonight programs
from the TDT-2 corpus [Nist, ?00], a test on the remaining 15
produced a false-alarm (boundary predicted incorrectly)
probability of .11, with a corresponding miss (true boundary not
predicted) probability of .14, equal to the best performance
reported to date, for this news source.
A more intuitive appreciation for the quality of performance can
be garnered from the graphs in Figure 6, which contrast the
segmentation produced by the system (middle) with ground truth
(the top graph), for a typical member of the ABC test set. The x-
axis corresponds to time (in units of word tokens); i.e., the index
of the word produced by the speech recognizer, and the y-axis
Figure 4: Histograms of story lengths (up to 250 words)
-- raw and smoothed --
Figure 5: Likelihood of COHER-3=2 over all states
corresponds to the state of the HMM model. A path passing
through the point (301, 65), for example, corresponds to a path
through the network that produced the 65th word from state 301.
Returns to state=1 correspond to boundaries between stories. The
bottom graph shows the superposition of the two to help illustrate
the agreement between the path chosen by the system and the path
corresponding to perfect segmentation..
7. VISUALIZATION
The evolution of the segmentation algorithm was driven by
analysis of the behavior of the system, which was supported by
visualization routines developed using the graphing capability of
the R package.  Figure 7 gives an example of the kind of graphical
displays that were used for analysis of the segmentation of a
specific broadcast news program; in this case, analysis of the role
of the X-DURATION feature.  This graphical display allows for
the comparison of the maximum likelihood path produced by the
HMM to the path through the HMM that would be produced by a
perfect system ? one privy to ground-truth.
T
sh
gr
co
po
ha
fr
th
hi
lo
T
li
ge
sy
D
tr
V
st
X
t gative
p n that
r e true
p deling.
T system
p of the
e
Figure 6: Perfohe true state than from the predicted state.  Strongly ne
oints are a major component of the probability calculatio
esulted in the system preferring the path it chose over th
ath.  These points suggest potential deficiencies in the mo
heir identification directs the focus of analysis so that 
erformance can be improved by correcting weaknesses 
xisting model.
rmancehe
ies
og
ful
ut
ed
 is
he
ith
tal
ue
ce
odhe top graph corresponds to the bottom graph of Figure 6,
owing the states traversed by the two systems.  The second
aph shows the value of the X-DURATION feature
rresponding to each word of the broadcast. So, the plotting of a
int at (301, 3) corresponds to an X-DURATION value of 3
ving been observed at time, 301. One thing that can be seen
om this graph is that being at a story boundary (low-points on
e thicker-darker line of the top graph) is more frequent when
gher values of the X-DURATION cue are observed, than when
wer values are observed, as could be expected.
he third graph shows, on a log scale, how many times more
kely it is that the observed X-DURATION value would be
nerated from the true state than from the state predicted by the
stem.  Most points are close to 0, indicating that the X-
URATION value observed was as likely to have come from the
ue state as it is to have come from the state predicted by the
iterbi algorithm.  Of course, this is the case wherever the true
ate has been correctly predicted. Negative points indicate that the
-DURATION value observed is less likely to be produced from
The final graph shows the cumulative sum of the values from t
graph above it. (Note that the sum of the logs of the probabilit
is equivalent to the cumulative product of probabilities on a l
scale.)  The graphing of the cumulative sum can be very use
when the system is performing poorly due to a small b
consistent preference for the observations having been produc
by the state sequence chosen by the system.  This phenomenon
made evident by a steady downward trend in the graph of t
cumulative sum.  This is in contrast to an overall level trend w
occasional downward dips.  Note, that a similar graph for the to
probability (equal to the product of all the individual feature val
probabilities) will always have an overall downward trend, sin
the maximum likelihood path will always have a likeliho
Figure 7: Visualization for x-duration feature
greater than the likelihood of any other path.
Aside from supporting the detailed analysis of specific features,
the productions of these graphs for each of the features, together
with the corresponding graph for the total observation probability,
allowed us to quickly asses which of the features was most
problematic at any given stage of model development.
8. FURTHER WORK
It should be kept in mind that experimentation with this approach
has been based on relatively primitive features ? our focus, to this
point, having been on the development of the core segmentation
mechanism.  Features based on more sophisticated extraction
techniques, which have been reported in the literature ? for
example, the use of exponential models for determining trigger
cues used in [Beeferman, Berger, & Lafferty ?99] ? can easily be
incorporated into this general framework.  Integration of such
techniques can be expected to result in significant further
improvement in segmentation quality.
To date, the binning method described has given much better
results than two dimensional kernel density estimation techniques
which we also attempted to employ.  One of the main difficulties
with using traditional kernel density estimation techniques is that
they tend to inaccurately estimate the density at areas of
discontinuity, such as state=1 in our model and our trigger
features.  Preliminary work with boundary kernels [Scott, ?92] is
very promising.  It is certainly an area worthy of more in-depth
investigation.
Work done by another group [Liu, ?00] to segment documentaries
based on video cues alone has been moderately successful in the
past.  We engineered a neural network in an attempt to identify
video frames containing an anchorperson, a logo, and blank
frames, with a belief that these are all features that would contain
information about story boundaries.  Preliminary work was also
done to extract features directly from the audio signal, such as
trying to identify speaker change. Initial work with the audio and
video has been unable to aid in segmentation, but we feel this is
also an area worth continuing to pursue.
9. REFERENCES
1. [Becker, Chambers & Wilks, `88] Becker, Richard A.,
Chambers, John M., and Wilks, Allan R.  The New S
Language.  Wadsworth & Brooks/Cole, Pacific Grove, Cal.
2. [Beeferman, Berger, & Lafferty ?99] D. Beeferman, D., A.
Berger, A. and Lafferty, J.  Statistical models for text
segmentation.  Machine Learning, vol. 34, pp. 1-34, 1999.
3. [Chambers & Hastie, `88] Chambers, John M. and Hastie,
Trevor, J.  Statistical Models in S.  Wadsworth &
Brooks/Cole, Pacific Grove, Cal., 1988.
4. [Greiff, Hurwitz & Merlino, `99] Greiff, Warren, Hurwitz,
Laurie, and Merlino, Andrew.  MITRE TDT-3 segmentation
system.  TDT-3 Topic Detection and Tracking Conference,
Gathersburg, Md, February, 2000.
5. [Kubula, et al, ?00] Kubula, F., Colbath, S.,  Liu, D.,
Srivastava, A. and Makhoul, J.  Integrated technologies for
indexing spoken language, Communication of the ACM, vol.
43, no. 2, Feb., 2000.
6. [Liu, ?00] Liu, Tiecheng and Kender, John R.  A hidden
Markov model approach to the structure of documentaries.
Proceedings of the IEEE Workshop on Content-based
Access of Image and Video Libraries, 2000.
7. [Loader, `99] Loader, C.  Local Regression and Likelihood.
Springer, Murray Hill, N.J., 1999.
8. [Maybury, Merlino & Morey ?97] Maybury, M., Merlino, A.
Morey, D.  Broadcast news navigation using story
segments. Proceedings of the ACM International
Multimedia Conference, Seattle, WA, Nov., 1997.
9. [Nist, ?00] Topic Detection and Tracking (TDT-3) Evaluation
Project.  http://www.nist.gov/speech/tests/tdt/tdt99/.
10. [Ponte and Croft, ?97] Ponte, J.M. and Croft, W.B.  Text
segmentation by topic, Proceedings of the First European
Conference on Research and Advanced Technology for
Digital Libraries, pp. 120--129, 1997.
11. [Rabiner, `89] L. R. Rabiner, A tutorial on hidden Markov
models and selected applications in speech recognition.
Proceedings of the IEEE, vol. 37, no. 2, pp. 257-86,
February, 1989.
12. [Scott, ?92] David W. Scorr, Boundary kernels, Multivariate
Density Estimation: Theory and Practice, pp 146-149, 1992.
13. [Venables & Ripley, `99]  Venables, W. N. and Ripley, B. D.
Modern Applied Statistics with S-PLUS.  Springer, Murray
Hill, N.J., 1999.
14. [Xu, J. and Croft, ?96] Xu, J. and Croft, W.B., Query
expansion using local and global document analysis,
Proceedings of the Nineteenth Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pp. 4--11, 1996
15. [Yamron, et al, ?98] Yamron, J. P., Carp, I., Gillick, L., Lowe,
S. and van Mulbregt, P.  A Hidden Markov Model approach
to text segmentation and event tracking.  Proceedings
ICASSP-98, Seattle, WA. May, 1998.
A Probabilistic Rasch Analysis of Question Answering Evaluations 
 
Rense Lange 
Integrated Knowledge Systems  
rlange@iknowsys.org  
Warren R. Greiff 
The MITRE Corporation 
greiff@mitre.org  
Juan Moran 
University of Illinois, Urbana-Champaign 
jmoran@ncsa.uiuc.edu 
Lisa Ferro 
The MITRE Corporation 
lferro@mitre.org 
 
 
Abstract 
The field of Psychometrics routinely grapples 
with the question of what it means to measure 
the inherent ability of an organism to perform 
a given task, and for the last forty years, the 
field has increasingly relied on probabilistic 
methods such as the Rasch model for test con-
struction and the analysis of test results.  Be-
cause the underlying issues of measuring 
ability apply to human language technologies 
as well, such probabilistic methods can be ad-
vantageously applied to the evaluation of 
those technologies. To test this claim, Rasch 
measurement was applied to the results of 67 
systems participating in the Question Answer-
ing track of the 2002 Text REtrieval Confer-
ence (TREC) competition. Satisfactory model 
fit was obtained, and the paper illustrates the 
theoretical and practical strengths of Rasch 
scaling for evaluating systems as well as ques-
tions. Most important, simulations indicate 
that a test invariant metric can be defined by 
carrying forward 20 to 50 equating questions, 
thus placing the yearly results on a common 
scale. 
1 Introduction 
For a number of years, objective evaluation of state-of-
the-art computational systems on realistic language 
processing tasks has been a driving force in the advance 
of Human Language Technology (HLT). Often, such 
evaluations are based on the use of simple sum-scores 
(i.e., the number of correct answers) and derivatives 
thereof (e.g., percentages), or on ad-hoc ways to rank or 
order system responses according to their correctness. 
Unfortunately, research in other areas indicates that 
such approaches rarely yield a cumulative body of 
knowledge, thereby complicating theory formation and 
practical decision making alike. In fact, although it is 
often taken for granted that sums or percentages ade-
quately reflect systems? performance, this assumption 
does not agree with many models currently used in edu-
cational testing (cf., Hambleton and Swaminathan, 
1985; Stout, 2002). To address this situation, we present 
the use of Rasch (1960/1980) measurement to the HLT 
research community, in general, and to the Question 
Answering (QA) research community, in particular.  
Rasch measurement has evolved over the last forty 
years to rigorously quantify performance aspects in such 
diverse areas as educational testing, cognitive develop-
ment, moral judgment, eating disorders (see e.g., Bond 
and Fox, 2001), as well as olfactory screening for Alz-
heimer?s disease (Lange et al, 2002) and model glider 
competitions (Lange, 2003). In each case, the major 
contribution of Rasch measurement is to decompose 
performance into two additive sources: the difficulty of 
the task and the ability of the person or system perform-
ing this task. While Rasch measurement is new to the 
evaluation of the performance of HLT systems, we in-
tend to demonstrate that this approach applies here as 
well, and that it potentially provides significant advan-
tages over traditional evaluation approaches.  
Our principal theoretical argument in favor of 
Rasch modeling is that the decomposition of perform-
ance into task difficulty and system ability creates the 
potential for formulating detailed and testable hypothe-
ses in other areas of language technology.  For QA, the 
existence of a well-defined, precise, mathematical for-
mulation of question difficulty and system ability can 
provide the basis for the study of the dimensions inher-
ent in the answering task, the formal characterization of 
questions, and the methodical analysis of the strengths 
and weaknesses of competing algorithmic approaches. 
As Bond and Fox (2001, p. 3) explain: ?The goal is to 
create abstractions that transcend the raw data, just as in 
the physical sciences, so that inferences can be made 
about constructs rather than mere descriptions about raw 
data.? Researchers are then in a position to formulate 
 initial theories, validate the consequences of theories on 
real data, refine theories in light of empirical data, and 
follow up with revised experimentation in a dialectic 
process that forms the essence of scientific discovery. 
Rasch modeling offers a number of direct practical 
advantages as well.  Among these are: 
? Quantification of question difficulty and system 
ability on a single scale with a common metric. 
? Support for the creation of tailor-made questions 
and the compilation of questions that suit well-
defined evaluation objectives. 
? Equating (calibration) of distinct question corpora 
so that systems participating in distinct evaluation 
cycles can be directly compared. 
? Assessment of the degree to which independent 
evaluations assess the same system abilities. 
? Availability of rigorous statistical techniques for 
the following: 
- analysis of fit of the data produced from systems? 
performance to the Rasch modeling assumptions; 
- identification of individual systems whose per-
formance behavior does not conform to the per-
formance patterns of the population as a whole; 
- identification of individual test questions that ap-
pear to be testing facets distinct from those evalu-
ated by the test as a whole; 
- assessment of the reliability of the test ? that is, 
the degree to which we can expect estimates of 
systems? abilities to be replicated if these systems 
are given another test of equivalent questions; 
- identification of unmodeled sources of variation 
in the data through a variety of methods, includ-
ing bias tests and analysis of residual terms. 
The remainder of the paper is organized as follows.  
First, we present in section 2 the basic concepts of 
Rasch modeling.  We continue in section 3 with an ap-
plication of Rasch modeling to the data resulting from 
the QA track of the 2002 Text REtrieval Conference 
(TREC) competition.  We fit the model to the data, ana-
lyze the resulting fit, and demonstrate some of the bene-
fits that can be derived from this approach. In section 4 
we present simulation results on test equating. Finally, 
we conclude with a summary of our findings and pre-
sent ideas for continuing research into the application of 
Rasch models to technology development and scientific 
theory formation in the various fields of human lan-
guage processing. 
2 The Rasch Model for Binary Data 
For binary results, Rasch?s (1960/1980) measurement 
requires that the outcome of an encounter between com-
puter systems (1, ?, s, ?, ns) and questions (1, ?, q, ?, 
nq) should depend solely on the differences between 
these systems? abilities (Ss) and the questions? difficul-
ties (Qq). Together with mild and standard scaling as-
sumptions, the preceding implies that:  
11 ??+= )e(P sq SQsq              (1) 
In a QA context, Psq is the probability that a system with 
the ability Ss will answer a question with difficulty Qq 
correctly. For a rigorous derivation of Equation 1 and an 
overview of the assumptions involved, we refer the 
reader to work by Fisher (1995). Fisher also proves that 
sum-scores (and hence percentages of correct answers) 
are sufficient performance statistics if and only if the 
assumptions of the Rasch model are satisfied. 
10 8 6 4 2 0 2 4 6 8
0
0.25
0.5
0.75
1
Latent Dimension (Logits)
P
(c
or
re
ct
)
0.5
5? 2
 
Figure 1.  Three Sample Rasch Response 
Curves 
The binary Rasch model has several interesting 
properties. First, as is illustrated by the three solid lines 
in Figure 1, Equation 1 defines a set of non-intersecting 
logistic response-curves such that Psq = 0.5 whenever Ss 
= Qq. In the following, such points are also referred to 
as question?s locations. For instance, the locations of the 
three questions depicted in Figure 1 are -5, 0, and 2. 
Second, for each pair of systems i and j with Si > Sj, for 
any question q, system i has a better chance of respond-
ing correctly than system j, i.e., Piq > Pjq. Thus, the 
questions that are answered correctly by less capable 
systems always form a probabilistic subset of those an-
swered correctly by more capable systems. Third, restat-
ing Equation 1 as is shown below highlights that the 
question and system parameters are additive and ex-
pressed in a common metric: 
qsP
P
QS)ln(
sq
sq ?=?1  (2) 
Given the left-hand side of Equation 2, this metric?s 
units are called Logits. Note that this equation further 
implies that Ss and Qq are determined up to an additive 
constant only (i.e., their common origin is arbitrary).  
Finally, efficient maximum-likelihood procedures 
exist to estimate Ss and Qq independently, together with 
their respective standard errors SEs and SEq (see e.g., 
Wright and Stone, 1979). These procedures do not re-
quire any assumptions about the magnitudes or the dis-
tribution of the Ss in order to estimate the Qq, and vice-
 versa.  Accordingly, systems? abilities can be deter-
mined in a ?question free? fashion, as different sets of 
questions from the same pool will yield statistically 
equivalent Ss estimates. Analogously, questions? loca-
tions can be estimated in a ?system free? fashion, as 
similarly spaced Qq estimates should be obtained across 
different samples of systems. In this paper, the model 
parameters, together with their errors of estimate, will 
be computed via the Winsteps Rasch scaling software 
(Linacre, 2003).  
 
0.0 0.5 1.0 1.5 2.0
Question Outfit (Mean-Square)
-4
-2
0
2
Q
ue
st
io
n 
lo
ca
tio
ns
 Q
q 
(L
og
its
) -
 Q
ue
st
io
n 
D
iff
ic
ul
ty
2. Questions w ith Outfit > 2.0 are show n as X
1. The size of the dots is proportional to SEq
Note:
3. Symbols have been slightly jittered along the
E
as
y
H
ar
d
X-axis to reveal overlapping points
 
Figure 2.  Questions by Qq, Outfitq, and SEq
3 Analysis of TREC Evaluation Data 
We used the results from the Question Answering track 
of the 2002 TREC competition to test the feasibility of 
applying Rasch modeling to QA evaluation. Sixty-seven 
systems participated, and answered 500 questions by 
returning a single precise response extracted from a 3-
gigabyte corpus of texts. While the NIST judges as-
sessed each answer as correct, incorrect, non-exact, or 
unsupported, we created binary responses by treating 
each of these last three assessments as incorrect. Ten 
questions were excluded from all analyses, as these 
were not answered correctly by any system.1 The final 
                                                          
1 When all respondents answer some question q correctly (or 
data set thus consisted of 67 systems? responses to 490 
questions.  
 
0.0 0.5 1.0 1.5 2.0
System Outfit (Mean-Square)
-4
-2
0
2
S
ys
te
m
 lo
ca
tio
ns
 S
s 
(L
og
its
) -
 S
ys
te
m
 C
ap
ab
ili
ty Outfit = 2.68
Lo
w
 A
bi
lit
y
H
ig
h 
A
bi
lit
y
Note:
1. The size of the dots is proportional to SEs
2. The circled dot is displaced to fit the graph
 
Figure 3.  Systems by Ss, Outfits, and SEs
3.1 Question Difficulty and System Ability 
Maximum-likelihood estimates of the questions? diffi-
culty and the systems? abilities were computed via Win-
steps. Figure 2 displays the results associated with the 
questions, whereas Figure 3 addresses the systems. Each 
dot in these displays corresponds to one question or one 
system.  For questions, the Y-value gives the estimate of 
the questions? difficulty (i.e., Qq); for systems, the Y-
value reflects the estimate of systems? ability (Ss).  For 
questions, lower values correspond to easier questions, 
while higher values to difficult questions.  For systems, 
higher values correspond to greater ability. As is cus-
tomary, the mean difficulty of the questions is set at 
zero, thereby implicitly fixing the origin of the esti-
mated system abilities at -1.98. As was noted earlier, a 
                                                                                           
incorrectly), the parameter Qq cannot be estimated. Note that 
raw-score approaches implicitly ignore such questions as well 
since including them does not affect the order of systems? 
?number right.? Of course, by changing the denominator, 
percentages of right or wrong questions are affected.  
 constant value can be added to each Qq and Ss without 
thereby affecting the validity of the results.  The X-axes 
of Figures 2 and 3 refer to the quality of fit, as described 
in section 3.3 below. 
As an example, consider a question with difficulty 
level -2. This means that a system whose ability level is 
-2 has a probability of .5 (odds=1) of getting this ques-
tion correct. The odds of a system with ability of -1 get-
ting this same question correct would increase by a 
factor2 of 2.72, thus increasing the probability of a cor-
rect answer to Psq = 2.72/(1+2.72) = .73. For a system 
at ability level 0, the odds increase by another factor of 
2.72 to 7.39, giving a probability of .88. On the other 
hand, a system with an ability of -3, would have the 
even odds decrease by a factor of 2.72 to .369, yielding 
Psq = .27. In other words, increasing (decreasing) ques-
tions? difficulties or decreasing (increasing) systems? 
abilities by the same amounts affects the log-odds in the 
same fashion. The preceding thus illustrates that ques-
tion difficulty and system ability have additive proper-
ties on the log-odds, or, Logit, scale.3   
The smoothed densities in Figure 4 summarize the 
locations of the 490 questions (dotted distribution) and 
the 67 systems (solid).  It can be seen that question dif-
ficulties range approximately from -3 to +3, and that 
most questions fall in a region about -1.  Systems? abili-
ties mostly cover a lower range such that the questions? 
locations (MeanQ = 0 Logits) far exceed those of the 
systems (MeanS = -1.98 Logits). In other words, most 
questions are very hard for these systems. The vast ma-
jority of systems (those located near -1 or below) have 
only a small chance (below 15%) of answering a sig-
nificant portion of the questions (those located above 1), 
and an even smaller chance (below 5%) on a non-
negligible number of questions (those above 2). Of 
those systems, a large portion (those at -2 or below) will 
have even less of a chance on these questions.   
-5 -3 -1 1 3 5
Rasch Dimension (Logits)
0.0
0.1
0.2
0.3
0.4
0.5
D
en
si
ty
 (S
 a
nd
 Q
)
0.0
0.2
0.4
0.6
0.8
1.0
S
E
Q
 a
nd
 S
E
S
SES
SEQ
Question Density
System Density
 
Figure 4. Smoothed System and Question Location 
Densities 
                                                          
                                                          
2 The value of e, since we are working with natural logarithms.   
3 Note that a number of measures used in the physical sciences 
likewise achieve additivity by adopting a log scale. 
3.2 Standard Error of Estimate 
The two U-shaped curves in Figure 4 reflect the esti-
mates of error, SEq for questions (dotted curve) and SEs 
for systems (solid curve), as a function of their esti-
mated locations (X-axis). As is also reflected by the size 
of the dots in Figure 3, it can be seen that SEs is smaller 
for intermediate and high performing systems (i.e., Ss 
between -3 and 1 Logits) than for low performing sys-
tems (Ss < -3 Logits). This pattern suits the ?horse-race? 
nature of the TREC evaluation well since the top per-
forming systems are assessed with nearly optimal preci-
sion. While the most capable system shows somewhat 
greater SEs, calculation shows its performance is still 
significantly higher than that of the runner-up (z = 
10.64, p < .001).  
Figure 4 further shows that SEq increases dramati-
cally beyond -1 Logits (this is also reflected in the size 
of the dots in Figure 2). For instance, the standard error 
of estimate SEq exceeds 0.5 Logits for questions with Qq 
> 1 Logit. Thus, the locations of the hardest questions 
are known with very poor precision only. 
3.3 Question and System Fit 
According to the Rasch model, a system, A, with mid-
dling performance is expected to perform well on the 
easier questions and poorly on the harder questions.  
However, it is possible that some system, B, achieved 
the same score on the test by doing poorly on the easy 
questions and well on the harder questions. While the 
behavior of system A agrees with the model, system B 
does not. Accordingly, the fit of system B is said to be 
poor as this system contradicts the knowledge embodied 
in the data as a whole. Analogous comments can be 
made with respect to questions. Rasch modeling formal-
izes the preceding account in a statistical fashion. In 
particular, for each response to Question q by System s, 
Equation 1 allows the computation of a standardized 
residual zsq, which is the difference between an observed 
datum (i.e., 0 or 1) and the probability estimate Psq after 
division by its standard deviation. Since the zsq follow 
an approximately normal distribution, unexpected re-
sults (e.g., |zsq|>3) are easily identified. The overall fit 
for systems (across questions) and for questions (across 
systems) is quantified by their Outfit.4 For instance, for 
System s:  
)n/(zOutfit q
q
sqs 1
2 ??=  (3) 
Since the summed z2sq in Equation 3 define a ?2 statistic 
with expected value nq ? 1, the Outfit statistic ranges 
4 Additionally, systems? (or questions?) ?Infit? statistic is de-
fined by weighting the z2sq contributions inversely to their 
distance to the contributing questions (or systems). As such, 
Infit statistics are less sensitive to outlying observations. Since 
this paper focuses on overall model fit, Infit statistics are not 
reported.  
 from 0 to ?, with an expected value of 1. As a rule of 
thumb, for rather small samples such as the present, 
Outfit values in the range 0.6 to 1.6 are considered as 
being within the expected range of variation.  
Figure 2 shows 46 questions whose Outfit exceeds 
1.6 (those to the right of the rightmost dashed vertical 
line) and the Outfit values of 24 of these exceed 2.0 
(shown in the graph by Xs, plotted at the right with hori-
zontal jitter). These are questions on which low per-
forming systems performed surprisingly well, and/or 
high performing systems performed unexpectedly 
poorly. Thus, there is a clear indication that these ques-
tions have characteristics that differentiate them from 
typical questions. Such questions are worthy of individ-
ual attention by the system developers.  
Questions and systems with uncharacteristically 
small Outfit values are of interest as well. For instance, 
in the present context it seems plausible that some ques-
tions simply cannot be answered by systems lacking 
certain capabilities (e.g., pronominal anaphora resolu-
tion, acronym expansion, temporal phrase recognition), 
while such questions are easily answered by systems 
that possess such capabilities. We might find that these 
questions would be answered by the vast majority, if not 
all, of the high performing systems and very few if any 
of the low ability systems. The estimated fit would be 
much better (small Outfit) than expected by chance. 
Again, the identification and analysis of such overfitting 
questions and, similiarly, underfitting systems may 
greatly enhance our understanding of both. 
3.4 Example of System with large Outfit 
Note that Figure 3 above shows that the best performing 
system also exhibits the largest Outfit (2.68), and we 
investigated this system?s residuals in detail. Table 1 
indicates that this system failed (Datum = 0) on eight 
questions (q) where its modeled probability of success 
was very high (Psq > 0.98). Thus, the misfit results from 
this system?s failure to answer correctly questions that 
proved quite easy for most other systems. 
q Qq Datum Psq Residual z 
1411 -1.51 0 0.98 -0.98 -7.39 
1418 -1.96 0 0.99 -0.99 -9.26 
1465 -1.74 0 0.99 -0.99 -8.28 
1672 -1.59 0 0.98 -0.98 -7.67 
1671 -1.51 0 0.98 -0.98 -7.39 
1686 -2.11 0 0.99 -0.99 -9.97 
1697 -1.89 0 0.99 -0.99 -8.92 
1841 -1.66 0 0.98 -0.98 -7.97 
 
Table 1. Misfit Diagnosis of Best Performing Sys-
tem 
These are the eight questions listed in Table 1: 
1411 What Spanish explorer discovered the Mississippi 
River? 
1418 When was the Rosenberg trial? 
1465 What company makes Bentley cars? 
1642 What do you call a baby sloth? 
1671 Where is Big Ben? 
1686 Who defeated the Spanish armada? 
1697 Where is the Statue of Liberty? 
1841 What?s the final line in the Edgar Allen Poe poem 
?The Raven?? 
This situation should be highly relevant to the sys-
tem?s developers. Informally speaking, the best system 
studied here ?should have gotten these questions right,? 
and it might thus prove fruitful to determine exactly 
why the system failed. Even if no obvious mistakes can 
be identified, doing so could reveal strategies for system 
improvement by focusing on seemingly ?easy? issues 
first. Alternatively, it might turn out that precisely those 
aspects of the system that enable it do so well overall 
also cause it to falter on the easier questions.  Ascertain-
ing this might or might not be of help to the system?s 
designers, but it would certainly foster the development 
of a scientific theory of automatic question answering. 
3.5 Impact of Misfit 
The existence of misfitting entities raises the possibility 
that the estimated Rasch system abilities are distorted by 
the question and system misfit. We therefore recom-
puted systems? locations by iteratively removing the 
worst fitting questions until 372 questions with Outfitq< 
1.6 remained. In support of the robustness of the Rasch 
model, Figure 5 shows that the correlation between the 
two sets of estimates is nearly perfect (r = 0.99), indi-
cating that the original and the ?purified? questions pro-
duce essentially equivalent system evaluations. Thus, 
the observed misfit had negligible effect on the system 
parameter estimates. 
-4 -2 0 2
Using best 372 fitting items only (Logits)
-4
-2
0
2
U
si
ng
 a
ll 
ite
m
s 
(L
og
its
)
r = 0.99
 
Figure 5.  Effect of Removing Misfitting Questions 
on the Estimated System Abilities Ss
 4 Test Equating Simulation 
A major motivation for introducing Rasch models in 
educational assessment was that this allows for the cali-
bration, or equating, of different tests based on a limited 
set of common (i.e., repeated) questions. The purpose of 
equating is to achieve equivalent test scores across dif-
ferent test sets. Thus, equating opens the door to cali-
brating the difficulty of questions and the performance 
of systems across the test sets used in different years. 
Since appropriate data from different years are 
lacking, a simulation study was performed based on 
different subsets of the 490 available questions. We 
show how system abilities can be expressed in the same 
metric, even though systems are evaluated with a com-
pletely different set of questions. To rule out the possi-
bility that such a correspondence might come about by 
chance (e.g., equally difficult sets of questions might 
accidentally be produced), a worst-case scenario is used. 
The simulation also provides a powerful means to dem-
onstrate the superiority of the Rasch Logit metric com-
pared to raw scores as indices of system performance.  
To this end, we divide the available questions from 
TREC 2002 into two sets of equal size. The Easy set 
contains the easiest questions (lowest Qq) as identified 
in earlier sections.  For the simulation, this will be the 
test set for one year?s evaluation.  A second, Hard set, 
serves as the test set for a subsequent evaluation, and it 
contains the remaining 50% of the questions (highest 
Qq).  By design, the difference in questions? difficulties 
is far more extreme than is likely to be encountered in 
practice. The Rasch model is then fitted to the responses 
to the Easy set of questions. Next, based on questions? 
difficulties and their fit to the Rasch model, a subset of 
the Easy questions is selected for inclusion in the sec-
ond test in conjunction with the Hard question set. 
These questions are said to comprise the Equating set, 
as they serve to fix the overall locations of the questions 
in the Hard set.  
Normally, this second test would be administered to 
a new set of systems (some completely new, others im-
proved versions of systems evaluated previously). How-
ever, for the purposes of this simulation, we 
?administer? the second test to the same systems.  The 
Rasch model is then applied to the Hard and Equating 
questions combined, while fixing the locations of the 
Equating questions to those derived while scaling the 
Easy set. The Winsteps software achieves this by shift-
ing the locations in the Hard set to be consistent with 
the Equating set ? but without adjusting the spacing of 
the questions in the Hard or Easy sets. If the assump-
tions of the Rasch model hold, then the Easy and Hard 
question sets will now behave as if their levels had been 
estimated simultaneously. Since the same systems are 
used in the simulation, and the questions have been 
calibrated to be on the same scale, the estimated system 
abilities Ss as derived from the Easy and Hard question 
sets should be statistically identical. That is, these two 
estimates should show a high linear correlation and they 
should have very similar means and standard deviations 
(see e.g., Wright and Stone, 1979, p. 83-126).  
Common wisdom in the Rasch scaling community 
holds that relatively few questions are needed to achieve 
satisfactory equating results. For this reason, the simula-
tion study was performed three times, using Equating 
sets with 20, 30, and 50 questions, respectively (i.e., 
about 4, 6, and 10% of the total number of questions in 
the present study). 
4.1 Findings 
The simulation results are summarized in Table 2, 
whose rows reflect the sizes of the respective Equating 
sets (i.e., 20, 30, and 50). Each first sub-row reports the 
Rasch scaling results, while the second sub-row reports 
the raw-score (i.e., number correct) findings. The col-
umns report a number of basic statistics, including the 
mean (M) and standard deviations (SD) of the Logit and 
raw-score values in the Easy and Hard sets, and the 
correlation (rlinear) between systems? estimated abilities 
based on the Easy and Hard sets. 
 
Size of 
Equat-
ing Set Index Measy SDeasy Mhard SDhard rlinear
20 Rasch  -0.66 1.10 -0.65 1.23 0.90 
 # Correct 92.40 47.53 27.39 30.70 0.77 
       
30 Rasch  -0.68 1.10 -0.66 1.21 0.92 
 # Correct 92.88 47.92 29.82 31.80 0.80 
       
50 Rasch  -0.78 1.11 -0.77 1.18 0.94 
  # Correct 94.76 49.62 31.01 32.29 0.82 
 
Table 2.  Results of the Simulation Study 
The major findings are as follows. First, inspection 
of the rlinear columns indicates that Rasch equating con-
sistently produced higher correlations between systems? 
estimated abilities as estimated via the Easy and Hard 
question sets than did the raw scores for each set. Sec-
ond, for obvious reasons the raw-score estimates based 
on the Easy sets are considerably higher than those 
based on the Hard sets. However, Table 2 also shows 
that the standard deviations of the number correct esti-
mates obtained for the Easy sets exceed those of the 
Hard sets as well (sometimes by over 100%). In other 
words, when raw scores (or percentages) are used, the 
?spacing? of the systems is affected by the difficulty of 
the questions.  
 Third, the Rasch approach by contrast produces 
very similar means and standard deviations for the ca-
pability estimates based on the Easy and Hard question 
sets. This holds regardless of the size of the Equating 
sets. For instance, when 50 equating questions are used, 
the estimated abilities based on the Easy and Hard sets 
have nearly identical SD (i.e., 1.11 and 1.18 Logits, re-
spectively). The corresponding averages for this case 
are -0.78 and -0.77 Logits, i.e., a standardized difference 
(effect size) of less than 0.01 SD. Similarly small effects 
sizes are obtained for the other cases. Further, given the 
superior values of rlinear, it thus appears that Rasch 
equating provides an acceptable equating mechanism 
even when maximally different question sets are used. 
In fact, already for Equating sets of size 20 a correlation 
of  0.90 is produced. 
Fourth, as a check on the results, scatter plots of the 
various cases summarized in Table 2 were produced. 
The left panel of Figure 6 shows the Rasch capability 
estimates obtained for the Hard question set plotted 
against those for the Easy set, and it can be seen that 
these estimates are highly correlated (rlinear = 0.94). The 
corresponding raw scores are plotted in the right panel 
of Figure 6. In addition to showing a lower correlation 
(rlinear = 0.82), raw scores also clearly posses a non-
linear component, and in fact the quadratic trend is 
highly significant (tquad = 13.10, p < .001). Thus, in 
addition to being question-dependent, raw score and 
percentage based comparisons suffer from pronounced 
non-linearity. 
Despite the favorable results, we remind the reader 
that the above simulations represented a worse-case 
scenario. Indeed, more realistic simulations not reported 
here indicate that Rasch equating can further be im-
proved by omitting misfitting questions and by using 
less extreme question sets. 
5 Conclusions 
In this paper we have described the Rasch model for 
binary data and applied it to the 2002 TREC QA results. 
We addressed the estimation of question difficulty and 
system ability, the estimation of standard errors for 
these parameters, and how to assess the fit of individual 
questions and systems. Finally, we presented a simula-
tion which demonstrated the advantage of using Rasch 
modeling for calibration of question sets. 
Based on our findings, we recommend that test 
equating be introduced in formal evaluations of HLT. In 
particular, for the QA track of the TREC competition, 
we propose that NIST include a set of questions to be 
reused in the following year for calibration purposes.  
For instance, after evaluating the systems? performance 
in the 2004 competition, NIST would select a set of 
questions consistent with the criteria outlined above. 
Using twenty to fifty questions from a set of 500 will 
probably be sufficient, especially when misfitting ques-
tions are eliminated. When the results are released to the 
participants, they would be asked not to look at these 
equating questions, and not to use them to train their 
systems in the future. These equating questions would 
then be included in the 2005 question set so as to place 
the 2004 and 2005 results on the same Logit scale. The 
process would continue in each consecutive year.  
The approach outlined above serves several pur-
poses. For instance, the availability of equated tests 
would increase the confidence that the testing indeed 
measures progress, and not simply the unavoidable 
-4 -2 0 2 4
Estimated capablity based on "Easy" questions (Logits)
E
st
im
at
ed
 c
ap
ab
ili
ty
 b
as
ed
 o
n 
"H
ar
d"
 q
ue
st
io
ns
 (
Lo
gi
ts
)
rlinear = 0.94
Y 
= X
-4
-2
0
2
4
0 50 100 150
Raw  score on "Easy" questions
0
50
100
150
R
aw
 s
co
re
 o
n 
"H
ar
d"
 q
ue
st
io
ns
rlinear = 0.82
Y 
= X
Y = 24.32 - 0.55X + 0.0051 X2Y = 0.016 + 0.998 X
 
 
Figure 6. Systems? Performance on Easy vs. Hard Questions Based on Rasch Scaling (left) and Raw Scores (right)
 
 variations in difficulty across each year?s question set. 
Additionally, it would support the goal of making each 
competition increasingly more challenging by correctly 
identifying easy and  difficult questions. Further, cali-
brated questions could be combined into increasingly 
large corpora, and these corpora could then be used to 
provide researchers with immediate performance feed-
back in the same metric as the NIST evaluation scale. 
The availability of large corpora of equated questions 
might also provide the basis for the development of 
methods to predict question difficulty, thus stimulating 
important theoretical research in QA. 
The work presented here only begins to scratch the 
surface of adopting a probabilistic approach such as the 
Rasch model for the evaluation of human language 
technologies.  First, as was discussed above, questions 
displaying unexpectedly large or small Outfit values can 
be identified for further study. The questions themselves 
can be analyzed in terms of both content and linguistic 
expression. With the objective of beginning to form a 
theory of question difficulty, questions can be analyzed 
in concert with the occurrence of correct answers in the 
document corpus and the incorrect answers returned by 
systems.  Also, experimentation with more complex 
scaling models could be conducted to uncover informa-
tion other than questions? difficulty levels. For example, 
so-called 2-parameter IRT models (see e.g., Hambleton 
and Swaminathan, 1985) would allow for the estimation 
of a discrimination parameter together with the diffi-
culty parameter for each question. More direct informa-
tion concerning the diagnosis of systems? skill defects 
are described in Stout (2002).  
It is also possible to incorporate into the model 
other factors and variables affecting a system?s per-
formance. Rasch modeling can be extended to many 
other HLT evaluation contexts since Rasch measure-
ment procedures exist to deal with multi-level re-
sponses, counts, proportions, and rater effects. Of 
particular interest is application to technology areas that 
use metrics other than percent of items processed cor-
rectly. Measures such as average precision, R-precision 
and precision at fixed document cutoff, which are used 
in Information Retrieval (Voorhees and Harman, 1999), 
metrics such as BiLingual Evaluation Understudy 
(BLUE) (Papineni et al, 2002) used in Machine Trans-
lation, and F-measure (Van Rijsbergen, 1979) com-
monly used for evaluation of a variety of NLP tasks are 
just a few of the variety of metrics used for evaluation 
of language technologies that can benefit from Rasch 
scaling and related techniques. 
References 
Bond, T.G. and Fox, C.M. (2001). Applying the Rasch 
Model: Fundamental Measurement in the Human 
Sciences. New Jersey: Lawrence Erlbaum Associates. 
Fischer, G.H. (1995). Derivations of the Rasch model. 
In G.H. Fischer & I.W. Molenaar (Eds.), Rasch mod-
els: Foundations, recent developments, and applica-
tions. (pp. 15-38) New York: Springer. 
Hambleton, R.K. and Swaminathan, H. (1985). Item 
response theory: Principles and applications. Boston: 
Kluwer ? Nijhoff. 
Lange, R. (2003). Model Sailplane Competition: From 
Awarding Points to Measuring Performance Skills. 
RC Soaring Digest, August Issue. (This paper is also 
available as: http://www.iknowsys.org/Download/ -
Model_Sailplanes.pdf). 
Lange, R., Donathan, C.L., and Hughes, L.F. (2002). 
Assessing olfactory abilities with the University of 
Pensylvania smell identification test: A Rasch scaling 
approach. Journal of Alzheimer?s Disease, 4, 77-91. 
Linacre, J. M. (2003). WINSTEPS Rasch measurement 
computer program. Chicago, IL: Winsteps.com. 
Papineni, K., Roukos, S., Ward, T, Henderson, J. and  
Reeder F. (2002). Corpus-based Comprehensive and 
Diagnostic MT Evaluation: Initial Arabic, Chinese, 
French, and Spanish Results. Proceedings of the 2002 
Conference on Human Language Technology (pp. 
124-127). San Diego, CA, 2002. 
Stout W.F. (2002). Psychometrics from practice to the-
ory and back. Psychometrika, 67, 485-518. 
http://www.psychometricsociety.org/journal/online/A
RTICLEstout2002.pdf
Rasch, G. (1960/1980). Probabilistic models for some 
intelligence and attainment tests. Chicago, IL: MESA 
Press. 
Van Rijsbergen, C. J.  (1979).  Information Retrieval. 
Dept. of Computer Science, University of Glasgow. 
Voorhees, E. M. and Harman, D. K. (eds.). 1999. Pro-
ceedings of the Eighth Text REtrieval Conference 
(TREC-8), NIST Special Publication 500-246. 
Wright, B.D. and Stone, M.H. (1979). Best test design. 
Chicago, IL: MESA Press. 
Direct Maximization of Average Precision by Hill-Climbing, with a
Comparison to a Maximum Entropy Approach
William Morgan and Warren Greiff and John Henderson
The MITRE Corporation
202 Burlington Road MS K325
Bedford, MA 01730
{wmorgan, greiff, jhndrsn}@mitre.org
Abstract
We describe an algorithm for choosing term
weights to maximize average precision. The
algorithm performs successive exhaustive
searches through single directions in weight
space. It makes use of a novel technique for
considering all possible values of average pre-
cision that arise in searching for a maximum in
a given direction. We apply the algorithm and
compare this algorithm to a maximum entropy
approach.
1 Introduction
This paper presents an algorithm for searching term
weight space by directly hill-climbing on average pre-
cision. Given a query and a topic?that is, given a set
of terms, and a set of documents, some of which are
marked ?relevant??the algorithm chooses weights that
maximize the average precision of the document set when
sorted by the sum of the weighted terms. We show that
this algorithm, when used in the larger context of finding
?optimal? queries, performs similar to a maximum en-
tropy approach, which does not climb directly on average
precision.
This work is part of a larger research program on the
study of optimal queries. Optimal queries, for our pur-
poses, are queries that best distinguish relevant from non-
relevant documents for a corpus drawn from some larger
(theoretical) population of documents. Although both
performance on the training data and generalization abil-
ity are components of optimal queries, in this paper we
focus only on the former.
2 Motivation
Our initial approach to the study of optimal queries em-
ployed a conditional maximum entropy model. This
model exhibited some problematic behavior, which mo-
tivated the development of the weight search algorithm
described here.
The maximum entropy model is used as follows. It is
given a set of relevant and non-relevant documents and a
vector of terms (the query). For any document, the model
predicts the probability of relevance for that document
based on the Okapi term frequency (tf ) scores (Robertson
and Walker, 1994) for the query terms within it. Queries
are developed by starting with the best possible one-term
query and adding individual terms from a candidate set
chosen according to a mutual information criterion. As
each term is added, the model coefficients are set to max-
imize the probability of the empirical data (the document
set plus relevance judgments), as described in Section 4.
Treating the model coefficients as term weights yields
a weighted query. This query produces a retrieval status
value (RSV) for each document that is a monotonically
increasing function of the probability of relevance, in ac-
cord with the probability ranking principle (Robertson,
1977). We can then calculate the average precision of the
document set as ordered by these RSVs.
As each additional query term represents another de-
gree of freedom, one would expect model performance to
improve at each step. However, we noted that the addition
of a new term would occasionally result in a decrease in
average precision?despite the fact that the model could
have chosen a zero weight for the newly added term.
Figure 1 shows an example of this phenomenon for one
TREC topic.
This is the result of what might be called ?metric di-
vergence?. While we use average precision to evaluate
the queries, the maximum entropy model maximizes the
likelihood of the training data. These two metrics occa-
sionally disagree in their evaluation of particular weight
vectors. In particular, maximum entropy modeling may
favor increasing the estimation of documents lower in the
ranking at the expense of accuracy in the prediction of
highly ranked documents. This can increase training data
likelihood yet have a detrimental effect on average preci-
sion.
The metric divergence problem led us to consider an al-
ternative approach for setting term weights which would
hill-climb on average precision directly. In particular, we
were interested in evaluating the results produced by the
maximum entropy approach?how much was the maxi-
mization of likelihood affecting the ultimate performance
as measured by average precision? The algorithm de-
scribed in the following section was developed to this
end.
3 The Weight Search Algorithm
The general behavior of the weight search algorithm is
similar to the maximum entropy modeling described in
Section 2?given a document corpus and a term vector,
it seeks to maximize average precision by choosing a
weight vector that orders the documents optimally. Un-
like the maximum entropy approach, the weight search
algorithm hill-climbs directly on average precision.
The core of the algorithm is an exhaustive search of a
single direction in weight space. Although each direction
is continuous and unbounded, we show that the search
can be performed with a finite amount of computation.
This technique arises from a natural geometric interpreta-
tion of changes in document ordering and how they affect
average precision.
At the top level, the algorithm operates by cycling
through different directions in weight space, performing
an exhaustive search for a maximum in each direction,
until convergence is reached. Although a global maxi-
mum is found in each direction, the algorithm relies on a
greedy assumption of unimodality and, as with the max-
imum entropy model, is not guaranteed to find a global
maximum in the multi-dimensional space.
3.1 Framework
This section formalizes the notion of weight space and
what it means to search for maximum average precision
within it.
Queries in information retrieval can be treated as vec-
tors of terms t1, t2, ? ? ? , tN . Each term is, as the name
suggests, an individual word or phrase that might oc-
cur in the document corpus. Every term t i has a weight
?i determining its ?importance? relative to the other
terms of the query. These weights form a weight vec-
tor ? = ??1 ?2 ? ? ? ?N ?. Further, given a document
corpus ?, for each document dj ? ? we have a ?value
vector? ?j = ??j1 ?j2 ? ? ? ?jN ?, where each ?value?
?ji ? < gives some measure of term ti within document
dj?typically the frequency of occurrence or a function
thereof. In the case of the standard tf-idf formula, ? ji
is the term frequency and ?i the inverse document fre-
quency.
If the document corpus and set of terms is held fixed,
the average precision calculation can be considered a
function f : <N ? [0, 1] mapping ? to a single aver-
age precision value. Finding the weight vectors in this
5 10 15 20
0.
2
0.
4
0.
6
0.
8
1.
0
0.
2
0.
4
0.
6
0.
8
1.
0
number of terms
a
ve
ra
ge
 p
re
cis
io
n
Figure 1: Average precision by query size as generated
by the maximum entropy model for TREC topic 307.
context is then the familiar problem of finding maxima in
an N -dimensional landscape.
3.2 Powell?s algorithm
One general approach to this problem of searching a
multi-dimensional space is to decompose the problem
into a series of iterated searches along single directions
within the space. Perhaps the most basic technique, cred-
ited to Powell, is simply a round-robin-style iteration
along a set of unchanging direction vectors, until conver-
gence is reached (Press et al, 1992, pp. 412-420). This
is the approach used in this study.
Formally, the procedure is as follows. You are given
a set of direction vectors ?1, ?2, ? ? ? , ?N and a starting
point pi0. First move pi0 to the maximum along ?1 and
call this pi1, i.e. pi1 = pi0 + ?1?1 for some scalar ?1.
Next move pi1 to the maximum along ?2 and call this
pi2, and so on, until the final point piN . Finally, replace
pi0 with piN and repeat the entire process, starting again
with ?1. Do this until some convergence criterion is met.
This procedure has no guaranteed rate of convergence,
although more sophisticated versions of Powell?s algo-
rithm do. In practice this has not been a problem.
3.3 Exhaustively searching a single direction
Powell?s algorithm can make use of any one-dimensional
search technique. Rather than applying a completely gen-
eral hill-climbing search, however, in the case where doc-
ument scores are calculated by a linear equation on the
terms, i.e.
?j =
N
?
i=1
?i?ji = ? ? ?j
as they are in the tf-idf formula, we can exhaustively
search in a single direction of the weight space in an effi-
cient manner. This potentially yields better solutions and
potentially converges more quickly than a general hill-
climbing heuristic.
scale
do
cu
m
en
t s
co
re
a
a
b
b
e
c
c
?1
?2
f
f
d d
Figure 2: Sample plot of ? versus ? for a given direction.
The insight behind the algorithm is as follows. Given
a direction ? in weight space and a starting point pi, the
score of each document is a linear function of the scale ?
along ? from pi:
?j = ? ? ?j
= (pi + ??) ? ?j
= pi ? ?j + ? (? ? ?j) .
i.e. document di?s score, plotted against ?, is a line with
slope ? ? ?i and y-intercept pi ? ?j .
Consider the graph of lines for all documents, such as
the example in Figure 2. Each vertical slice of the graph,
at some point ? on the x axis, represents the order of the
documents when ? = ?; specifically, the order of the
documents is given by the order of the intersections of
the lines with the vertical line at x = ?.
Now consider the set of intersections of the document
lines. Given two documents dr and ds, their intersection,
if it exists, lies at point ?rs = (?xrs, ?yrs) where
?xrs =
pi ? (?s ? ?r)
? ? (?r ? ?s)
, and
?yrs = pi ? ?r + ?xrs (? ? ?r)
(Note that this is undefined if ? ? ?r = ? ? ?s, i.e., if the
document lines are parallel.)
Let ? be the set of all such document intersection
points for a given direction, document set and term vec-
tor. Note that more than two lines may intersect at the
same point, and that two intersections may share the same
x component while having different y components.
Now consider the set ?x, defined as the projection of
? onto the x axis, i.e. ?x = {? | ? ? ? ? s.t. ?x = ?}.
The points in ?x represent precisely those values of ?
where two or more documents are tied in score. There-
fore, the document ordering changes at and only at these
points of intersection; in other words, the points in ?x
partition the range of ? into at most M(M ?1)/2+1 re-
gions, where M is the total number of documents. Within
a given region, document ordering is invariant and hence
average precision is constant. As we can calculate the
boundaries of, and the document ordering and average
precision within, each region, we now have a way of find-
ing the maximum across the entire space by evaluating
a finite number of regions. Each of the O(M 2) regions
requires an O(M log M) sort, yielding a total computa-
tional bound of O(M 3 log M).
In fact, we can further reduce the computation by ex-
ploiting the fact that the change in document ordering be-
tween any two regions is known and is typically small.
The weight search algorithm functions in this manner. It
sorts the documents completely to determine the order-
ing in the left-most region. Then, it traverses the regions
from left to right and updates the document ordering in
each, which does not require a sort. Average precision
can be incrementally updated based on the document or-
dering changes. This reduces the computational bound to
O(M2 log M), the requirement for the initial sort of the
O(M2) intersection points.
4 Experiment Setup
In order to compare the results of the weight search al-
gorithm to those of the maximum entropy model, we em-
ployed the same experiment setup. We ran on 15 topics,
which were manually selected from the TREC 6, 7, and
8 collections (Voorhees and Harman, 2000), with the ob-
jective of creating a representative subset. The document
sets were divided into randomly selected training, valida-
tion and test ?splits?, comprising 25%, 25%, and 50%,
respectively, of the complete set.
For each query, a set of candidate terms was selected
based on mutual information between (binary) term oc-
currence and document relevance. From this set, terms
were chosen individually to be included in the query,
and coefficients for all terms were calculated using L-
BFGS, a quasi-Newton unconstrained optimization algo-
rithm (Zhu et al, 1994).
For experimenting with the weight search algorithm,
we investigated queries of length 1 through 20 for each
topic, so each topic involved 20 experiments. The first
term weight was fixed at 1.0. The single-term queries
did not require a weight search, as the weight of a single
term does not affect the average precision score. For the
remaining 19 experiments for each topic, the direction
vectors ? were chosen such that the algorithm searched
a single term weight at a time. For example, a query with
5 10 15 20
0.
2
0.
4
0.
6
0.
8
1.
0
0.
2
0.
4
0.
6
0.
8
1.
0
number of terms
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
301
302
307
330
332
347
352
375
383
384
388
391
407
425
439
Figure 3: Average precision versus query size for the
weight search algorithm. Each line represents a topic.
i terms used the i ? 1 directions
?i,1 = ?0 1 0 0 ? ? ? 0?,
?i,2 = ?0 0 1 0 ? ? ? 0?,...
?i,i?1 = ?0 0 0 0 ? ? ? 1?.
The two-term query for a topic started the search from the
point pi2,0 = ?1 0?, and each successive experiment for
that topic was initialized with the starting point pi0 equal
to the final point in the previous iteration, concatenated
with a 0. The ?value vectors? ?j used in all experiments
were Okapi tf scores.
5 Results
The average precision scores obtained by the maximum
entropy and weight search algorithm experiments are
listed in Table 1. The ?Best AP? and ?No. Terms?
columns describe the query size at which average preci-
sion was best and the score at that point. These columns
show that the maximum entropy approach performs just
as well as the average precision hill-climber, and in some
cases actually performs slightly better. This suggests that
the metric divergence as seen in Figure 1 did not prohibit
the maximum entropy approach from maximizing aver-
age precision in the course of maximizing likelihood.
The ?5 term AP? column compares the performance
of the algorithms on smaller queries. The weight search
algorithm shows a slight advantage over the maximum
entropy model on 10 of the 15 topics and equal perfor-
mance on the others, but definitive conclusions are diffi-
cult at this stage.
Figure 3 shows the average precision achieved by the
weight search algorithm, for all 20 query sizes and for
all 15 topics. Unlike the maximum entropy results,
the algorithm is guaranteed to yield monotonically non-
decreasing scores.
Topic 5 term AP Best AP No. Terms
WS ME WS ME WS ME
301 0.68 0.67 0.90 0.90 >20 >20
302 0.88 0.86 1.00 1.00 10 10
307 0.57 0.56 0.98 0.89 >20 >20
330 0.65 0.61 1.00 1.00 10 10
332 0.74 0.72 0.99 1.00 >20 18
347 0.78 0.78 1.00 1.00 17 14
352 0.55 0.55 0.94 0.93 >20 >20
375 0.92 0.92 1.00 1.00 9 9
383 0.89 0.89 1.00 1.00 9 9
384 0.77 0.73 1.00 1.00 8 8
388 0.82 0.80 1.00 1.00 7 7
391 0.64 0.63 0.99 0.98 >20 >20
407 0.83 0.83 1.00 1.00 9 9
425 0.75 0.73 1.00 1.00 12 12
439 0.53 0.51 1.00 1.00 17 16
Table 1: Average precision achieved for weight search
(WS) and maximum entropy (ME) algorithms.
6 Conclusions
We developed an algorithm for exhaustively searching a
continuous and unbounded direction in term weight space
in O(M2 log M) time. Initial results suggest that the
maximum entropy approach performs as well as this al-
gorithm, which hill-climbs directly on average precision,
allaying our concerns that the metric divergence exhib-
ited by the maximum entropy approach is a problem for
studying optimal queries.
References
William H. Press, Brian P. Flannery, Saul A. Teukolsky,
and William T. Vetterling. 1992. Numerical Recipes
in C: The Art of Scientic Computing. Cambridge Uni-
versity Press, second edition.
S. E. Robertson and S. Walker. 1994. Some simple effec-
tive approximations to the 2-Poisson model for proba-
bilistic weighted retrieval. In W. Bruce Croft and C. J.
van Rijsbergen, editors, Proc. 17th SIGIR Conference
on Information Retrieval.
S. E. Robertson. 1977. The probability ranking principle
in IR. Journal of Documentation, 33:294?304.
E. M. Voorhees and D. K. Harman. 2000. Overview of
the eighth Text REtrieval Conference (TREC-8). In
E. M. Voorhees and D. K. Harman, editors, The Eighth
Text REtreival Conference (TREC-8). NIST Special
Publication 500-246.
C. Zhu, R. Byrd, P. Lu, and J. Nocedal. 1994. LBFGS-B:
Fortran subroutines for large-scale bound constrained
optimization. Technical Report NAM-11, EECS De-
partment, Northwestern University.
Audio Hot Spotting and Retrieval Using Multiple Features  
 
 
Qian Hu 
MITRE Corporation 
qian@mitre.org 
Fred Goodman, Stanley Boykin, 
Randy Fish, Warren Greiff 
MITRE Corporation 
fgoodman@mitre.org, 
sboykin@mitre.org, 
fishr@mitre.org 
greiff@mitre.org 
 
Abstract 
 
This paper reports our on-going efforts to 
exploit multiple features derived from an 
audio stream using source material such as 
broadcast news, teleconferences, and 
meetings. These features are derived from 
algorithms including automatic speech 
recognition, automatic speech indexing, 
speaker identification, prosodic and audio 
feature extraction. We describe our research 
prototype ? the Audio Hot Spotting System ? 
that allows users to query and retrieve data 
from multimedia sources utilizing these 
multiple features.  The system aims to 
accurately find segments of user interest, i.e., 
audio hot spots within seconds of the actual 
event.  In addition to spoken keywords, the 
system also retrieves audio hot spots by 
speaker identity, word spoken by a specific 
speaker, a change of speech rate, and other 
non-lexical features, including applause and 
laughter.  Finally, we discuss our approach to 
semantic, morphological, phonetic query 
expansion to improve audio retrieval 
performance and to access cross-lingual data. 
 
1. Introduction 
 
Audio contains more information than is 
conveyed by the text transcript produced by 
an automatic speech recognizer [Johnson et 
al., 2000; Hakkani-Tur et al, 1999]. 
Information such as: a) who is speaking, b) 
the vocal effort used by each speaker, and c) 
prosodic features and certain non-speech 
background sounds, are lost in a simple 
speech transcript. In addition, due to the 
variability of acoustic channels and noise 
conditions, speaker variance, language 
models the recognizer is based on, and the 
limitations of automatic speech recognition 
(ASR), speech transcripts can be full of 
errors.  Deletion errors can prevent the users 
from finding what they are looking for from 
audio or video data, while insertion and 
substitution errors can be misleading and 
confusing.  Our approach is to automatically 
detect, index, and retrieve multiple features 
from the audio stream to compensate for the 
weakness of using speech transcribed text 
alone.  The multiple time-stamped features 
from the audio include an automatically 
generated index derived from ASR speech 
transcripts, automatic speaker identification, 
and automatically identified prosodic and 
audio cues. In this paper, we describe our 
indexing algorithm that automatically 
identifies potential search keywords that are 
information rich and provide a quick clue to 
the document content. We also describe how 
our Audio Hot Spotting prototype system 
uses multiple features to automatically locate 
regions of interest in an audio or video file 
that meet a user?s specified query criteria.  In 
the query, users may search for keywords or 
phrases, speakers, keywords and speakers 
together, non-verbal speech characteristics, 
or non-speech signals of interest. The system 
also uses multiple features to refine query 
results. Finally, we discuss our query 
expansion mechanism by using natural 
language processing techniques to improve 
retrieval performance and to access cross-
lingual data. 
 
 
2. Data 
 
We use a variety of multimedia data for the 
experiments in order to test Audio Hot Spotting 
algorithms and performance under different 
acoustic and noise environments.  These include 
broadcast news (e.g. HUB4), teleconferences, 
meetings, and MITRE corporate multimedia 
events.  In some cases, synthetic noise was added 
to clean source material to test algorithm 
robustness. 
 
3. Automatic Spoken Keyword 
Indexing 
 
As automatic speech recognition is imperfect, 
automatic speech transcripts contain errors.  Our 
indexing algorithm focuses on finding words that 
are information rich (i.e. content words) and 
machine recognizable.  Our approach is based on 
the principle that short duration and weakly 
stressed words are much more likely to be mis-
recognized, and are less likely to be important.  
To eliminate words that are information poor and 
prone to mis-recognition, our algorithm examines 
the speech recognizer output and creates an index 
list of content words. The index-generation 
algorithm takes the following factors into 
consideration: a) absolute word length by its 
utterance duration, b) the number of syllables, c) 
the recognizer?s own confidence score, and d) the 
part of speech (i.e. verb, noun) using a POS 
tagger with some heuristic rules. Experiments we 
have conducted using broadcast news data, with 
Gaussian white noise added to achieve a desired 
Signal-to-Noise Ratio (SNR), indicate that the 
index list produced typically covers about 10% of 
the total words in the ASR output, while more 
than 90% of the indexed words are actually 
spoken and correctly recognized given a Word 
Error Rate (WER [Fiscus, et al]) of 30%. The 
following table illustrates the performance of the 
automatic indexer as a function of Signal-to-
Noise Ratio during a short pilot study. 
 
 
 
 
 
 
 
SNR 
(dB) 
ASR 
WER 
(%) 
Index 
Coverage 
(%) 
 
IWER 
(%) 
Orig. 26.8 13.6 4.3 
24 32.0 12.3 3.3 
18 39.4 10.8 5.9 
12 54.7 8.0 12.2 
6 75.9 3.4 20.6 
3 87.9 1.4 41.7 
 
Table 1 Indexer SNR Performance 
where Index Coverage is the fraction of the words 
in the transcript chosen as index words and IWER 
is the index word error rate. 
 
As expected, increases in WER result in fewer 
words meeting the criteria for the index list.  
However, the indexer algorithm manages to find 
reliable words even in the presence of very noisy 
data.  At 12dB SNR, while the recognizer WER 
has jumped up to 54.7%, the Index Word Error 
Rate (IWER) has risen to 12.2%. Note that an 
index-word error indicates that an index word 
chosen from the ASR output transcript did not in 
fact occur in the original reference transcription. 
 
Whether this index list is valuable will 
depend on the application. If a user wants to get a 
feel for a 1-hour conversation in just a few 
seconds, automatically generated topic terms such 
as those described in [Kubala  et al, 2000] or an 
index list such as this could be quite valuable. 
 
 
4. Detecting and Using Multiple 
Features from the Audio 
 
Automatic speech recognition has been 
used extensively in spoken document retrieval 
[Garofolo et al, 2000; Rendals et al, 2000].  
However, high speech WER in the speech 
transcript, especially in less-trained domains such 
as spontaneous and non-broadcast quality data, 
greatly reduces the effectiveness of navigation 
and retrieval using the speech transcripts alone.  
Furthermore, the retrieval of a whole document or 
a story still requires the user to read the whole 
document or listen to the entire audio file in order 
to locate the segments where relevant information 
resides.  In our approach, we recognize that there 
is more information in the audio file than just the 
words and that other attributes such as speaker 
identification, prosodic features, and the type of 
background noise may also be helpful for the 
retrieval of information. In addition, we aim to 
retrieve the exact segments of interest rather than 
the whole audio or document so that the user can 
zero in on these specific segments rapidly. One of 
the challenges facing researchers is the need to 
identify "which" non-lexical features have 
information value.  Since these features have not 
been available to users in the past, they don't 
know enough to ask for them.  We have chosen to 
implement a variety of non-lexical cues with the 
intent of stimulating feedback from our user 
community. 
  
As an example of this, by extending a 
research speaker identification algorithm 
[Reynolds, 1995], we integrated speaker 
identification into the Audio Hot Spotting 
prototype to allow a user to retrieve three kinds of 
information.  First, if the user cannot find what 
he/she is looking for using keyword search but 
knows who spoke, the user can retrieve content 
defined by the beginning and ending timestamps 
associated with the specified speaker; assuming 
enough speech exists to build a model for that 
speaker.  Secondly, the system automatically 
generates speaker participation statistics 
indicating how many turns each speaker spoke 
and the total duration of each speaker?s audio.  
Finally, the system uses speaker identification to 
refine the query result by allowing the user to 
query keywords and speaker together.  For 
example, using the Audio Hot Spotting prototype, 
the user can find the audio segment in which 
President Bush spoke the word ?anthrax".   
 
In addition to speaker identification, we 
wanted to illustrate the information value of other 
non-lexical sounds in the audio track.  As a proof-
of-concept, we created detectors for crowd 
applause and laughter. The algorithms used both 
spectral information as well as the estimated 
probability density function (pdf) of the raw audio 
samples to determine when one of these situations 
was present. Laughter has a spectral envelope 
which is similar to a vowel, but since many 
people are voicing at the same time, the audio has 
no coherence. Applause, on the other hand, is 
spectrally speaking, much like noisy speech 
phones such as ?sh? or ?th.? However, we 
determined that the pdf of applause differed from 
those individual sounds in the number of high 
amplitude outlier samples present.  Applying this 
algorithm to the 2003 State of the Union address, 
we identified all instances of applause with only a 
2.6% false alarm rate (results were compared with 
hand-labeled data).  One can imagine a situation 
where a user would choose this non-lexical cue to 
identify statements that generated a positive 
response. 
 
Last year, we began to look at speech 
rate as a separate feature.  Speech rate estimation 
is important, both as an indicator of emotion and 
stress, as well as an aid to the speech recognizer 
itself (see for example [Mirghafori et al, 1996; 
Morgan, 1998; Zheng et al, 2000]).  Currently, 
recognizer word error rates are highly correlated 
to speech rate.  For the user, marking that a 
returned passage is from an abnormal speech rate 
segment and therefore more likely to contain 
errors allows him/her to save time by ignoring 
these passages or reading them with discretion if 
desired.  However, if passages of high stress are 
of interest, these are just the passages to be 
reviewed.  For the recognizer, awareness of 
speech rate allows modification of HMM state 
probabilities, and even permits different 
sequences of phones.  
 
One approach to determine the speech rate 
accurately is to examine the phone-level output of 
the speech recognizer. Even though the phone-
level error rate is quite high, the timing 
information is still valuable for rate estimation. 
By comparing the phone lengths of the recognizer 
output to phone lengths tabulated over many 
speakers, we have found that a rough estimate of 
speech rate is possible [Mirgafori et al 1996]. 
Initial experiments using MITRE Corporate event 
data have shown a rough correspondence between 
human perception of speed and the algorithm 
output. One outstanding issue is how to treat 
audio that includes both fast rate speech and 
significant silences between utterances. Is this 
truly fast speech?  
 
We are currently conducting research to 
detect other prosodic features by estimating vocal 
effort.  These features may indicate when a 
speaker is shouting suggesting elevated emotions 
or near a whisper. Queries based on such features 
can lead to the identification of very interesting 
audio hot spots for the end user.  Initial 
experiments are examining the spectral properties 
of detected glottal pulses obtained during voiced 
speech.  
 
5. Query Expansion and Retrieval 
Tradeoffs 
5.1  Effect of Passage Length 
TREC SDR found both a linear correlation 
between speech word error rate and retrieval rate 
[Garofolo et al, 2000] and that retrieval was 
fairly robust to WER.  However, the robustness 
was attributed to the fact that misrecognized 
words are likely to also be properly recognized in 
the same document if the document is long 
enough.  Since we limit our returned passages to 
roughly 10 seconds, we do not benefit from this 
full-document phenomenon.  The relationship 
between passage retrieval rate and passage length 
was studied by searching 500 hours of broadcast 
news from the TREC SDR corpus.  Using 679 
keywords, each with an error rate across the 
corpus of at least 30%, we found that passage 
retrieval rate was 71.7% when the passage was 
limited to only the query keyword.  It increased to 
76.2% when the passage length was increased to 
10sec and rose to 83.8% if the returned passage 
was allowed to be as long as 120sec. 
 
In our Audio Hot Spotting prototype, we 
experimented with semantic, morphological, and 
phonetic query expansion to achieve two 
purposes, 1) to improve the retrieval rate of 
related passages when exact word match fails, and 
2) to allow cross lingual query and retrieval. 
 
5.2  Keyword Query Expansion 
 
The Audio Hot Spotting prototype 
integrated the Oracle 9i Text engine to expand the 
query semantically, morphologically and 
phonetically.  For morphological expansion, we 
activated the stemming function.  For semantic 
expansion, we utilized expansion to include 
hyponyms, hypernyms, synonyms, and 
semantically related terms.  For example, when 
the user queried for "oppose", the exact match 
yielded no returns, but when semantic and 
morphological expansion options are selected, the 
query was expanded to include anti, anti-
government, against, opposed, opposition, and 
returned several passages containing these 
expanded terms.  
 
To address the noisy nature of speech 
transcripts, we used the phonetic expansion, i.e. 
"sound alike" feature from the Oracle database 
system.  This is helpful especially for proper 
names.  For example, if the proper name Nesbit is 
not in the speech recognizer vocabulary, the word 
will not be correctly transcribed.  In fact, it was 
transcribed as Nesbitt (with two 't's).  By phonetic 
expansion, Nesbit is retrieved.  We are aware of 
the limitations of Oracle?s phonetic expansion 
algorithms, which are simply based on spelling.  
This doesn?t work well when text is a mis-
transcription of the actual speech.  Hypothetically, 
a phoneme-based recognition engine may be a 
better candidate for phonetic query expansion.  
We are currently evaluating a phoneme-based 
audio retrieval system and comparing its 
performance with a word-based speech 
recognition system.  The comparison will help us 
to determine the strengths and weaknesses of each 
system so that we can leverage the strength of 
each system to improve audio retrieval 
performance. 
 
Obviously more is not always better.  
Some of the expanded queries are not exactly 
what the users are looking for, and the number of 
passages returned increases.  In our Audio Hot 
Spotting implementation we made query 
expansion an option allowing the user to choose 
to expand semantically and/or, morphologically, 
or phonetically.   
 
5.3   Cross-lingual Query Expansion 
 
In some applications it is helpful for a 
user to be able to query in a single language and 
retrieve passages of interest from documents in 
several languages. We treated translingual search 
as another form of query expansion.  We created a 
bilingual thesaurus by augmenting Oracle's 
default English thesaurus with Spanish dictionary 
terms.  With this type of query expansion 
enabled, the system retrieves passages that 
contain the keyword in either English or Spanish.  
A straightforward extension of this approach will 
allow other languages to be supported.  
 
6.   Future Directions 
 
As our research and prototype evolve, we 
plan to develop algorithms to detect more 
meaningful prosodic and audio features to allow 
the users to search for and retrieve them. We are 
also developing algorithms that can generate 
speaker identify in the absence of speaker training 
data.  For example, given an audio script, we 
expect the algorithms to automatically identify 
the number of different speakers present and the 
time speaker X changes to Y.  For semantic query 
expansion, we are considering using more 
comprehensive thesauri and local context analysis 
to locate relevant segments to compensate for 
high ASR word error rate.  We are also 
considering combining a word-based speech 
recognition system with a phoneme-based system 
to improve the retrieval performance especially 
for out of vocabulary words and multi-word 
queries. 
7.   Conclusion 
In this paper, we have shown that by 
automatically detecting multiple audio features 
and making use of these features in a relational 
database, our Audio Hot Spotting prototype 
allows a user to begin to apply the range of cues 
available in audio to the task of multi-media 
information retrieval.  Areas of interest can be 
specified using keywords, phrases, speaker 
identity, prosodic features, and information-
bearing background sounds, such as applause and 
laughter.  When matches are found, the system 
displays the recognized text and allows the user to 
play the audio or video in the vicinity of the 
identified "hot spot".  With the advance of 
component technologies such as automatic speech 
recognition, speaker identification, and prosodic 
and audio feature extraction, there will be a wider 
array of audio features for the multimedia 
information systems to query and retrieve, 
allowing the user to access the exact information 
desired rapidly. 
 
 
 
References 
 
1. John Garofolo, et al Nov., 2000. The TREC 
Spoken Document Retrieval Track: A Successful 
Story.  TREC 9. 
 
2. Julia Hirschberg, Steve Whittaker, Don Hindle, 
Fernando Pereira and Amit Singhal. April 1999. 
Finding Information In Audio: A New Paradigm 
For Audio Browsing/Retrieval, ESCA ETRW 
workshop Accessing information in spoken audio, 
Cambridge. 
 
3. Sue Johnson, Pierre Jourlin, Karen Sparck 
Jones, and Philip Woodland. Nov., 2000.  Spoken 
Document Retireval for TREC-9 at Cambridge 
University. TREC-9. 
 
4. John Fiscus, et al Speech Recognition Scoring 
Toolkit   (http://www.nist.gov/speech/tools/) 
 
5. D. Hakkani-Tur, G. Tur, A.Stolcke, E. Shriberg. 
Combining Words and Prosody for Information 
Extraction from Speech. Proc. EUROSPEECH'99, 
 
6.. N. Mirghafori, E. Fosler, and N. H. Morgan. 
Towards Robustness to Fast Speech in ASR, 
Proc. ICASSP, Atlanta, GA, May 1996.  
   
7. N. Morgan and E. Fosler-Lussier. Combining 
Multiple Estimators of peaking Rate,  Proc. 
ICASSP-98, pp. 729-732, Seattle, 1998  
 
8. M. D. Plumpe, T. F. Quatieri, and D. A. 
Reynolds. Modeling of the Glottal Flow 
Derivative Waveform with Application to 
Speaker Identification, IEEE Trans. On Speech 
and Audio Processing, September 1999. 
 
9. S. Rendals, and D. Abberley. The THISL SDR 
System at TREC-9.  TREC-9, Nov., 2000. 
 
10. K. N. Stevens and H. .M. Hanson. 
Classification of Glottal Vibration from Acoustic 
Measurements. In Vocal Fold Physiology: Voice 
Quality Control, Fujimura O., Hirano H. (eds.), 
Singular Publishing Group, San Diego, 1995. 
 
11. J. Zheng, H. Franco, F. Weng, A. Sankar and 
H. Bratt.. Word-level Rate-of-Speech Modeling 
Using Rate-Specific Phones and Pronunciations, 
Proc. ICASSP, vol 3, pp 1775-1778, 2000 
 
12. F. Kubala, S. Colbath, D. Liu, A. Srivastava, J. 
Makhoul. Integrated Technologies For Indexing 
Spoken Language, Communications of the ACM, 
February 2000. 
 
13. D. Reynolds. Speaker Identification And 
Verification Using Gaussian Mixture Speaker 
Models, Speech Communications, vol.17, pp.91, 
1995 
 
