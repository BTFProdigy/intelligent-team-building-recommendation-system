Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 962?971, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Entropy-based Pruning for Phrase-based Machine Translation
Wang Ling, Joa?o Grac?a, Isabel Trancoso, Alan Black
L2F Spoken Systems Lab, INESC-ID, Lisboa, Portugal
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA
{wang.ling,joao.graca,isabel.trancoso}@inesc-id.pt
awb@cs.cmu.edu
Abstract
Phrase-based machine translation models
have shown to yield better translations than
Word-based models, since phrase pairs en-
code the contextual information that is needed
for a more accurate translation. However,
many phrase pairs do not encode any rele-
vant context, which means that the transla-
tion event encoded in that phrase pair is led
by smaller translation events that are indepen-
dent from each other, and can be found on
smaller phrase pairs, with little or no loss in
translation accuracy. In this work, we pro-
pose a relative entropy model for translation
models, that measures how likely a phrase pair
encodes a translation event that is derivable
using smaller translation events with similar
probabilities. This model is then applied to
phrase table pruning. Tests show that con-
siderable amounts of phrase pairs can be ex-
cluded, without much impact on the transla-
tion quality. In fact, we show that better trans-
lations can be obtained using our pruned mod-
els, due to the compression of the search space
during decoding.
1 Introduction
Phrase-based Machine Translation Models (Koehn
et al 2003) model n-to-m translations of n source
words to m target words, which are encoded in
phrase pairs and stored in the translation model.
This approach has an advantage over Word-based
Translation Models (Brown et al 1993), since trans-
lating multiple source words allows the context for
each source word to be considered during trans-
lation. For instance, the translation of the En-
glish word ?in? by itself to Portuguese is not ob-
vious, since we do not have any context for the
word. This word can be translated in the con-
text of ?in (the box)? to ?dentro?, or in the con-
text of ?in (China)? as ?na?. In fact, the lexical
entry for ?in? has more than 10 good translations
in Portuguese. Consequently, the lexical translation
entry for Word-based models splits the probabilis-
tic mass between different translations, leaving the
choice based on context to the language model. On
the other hand, in Phrase-based Models, we would
have a phrase pair p(in the box, dentro da caixa)
and p(in china, na china), where the words ?in the
box? and ?in China? can be translated together to
?dentro da caixa? and ?na China?, which substan-
tially reduces the ambiguity. In this case, both the
translation and language models contribute to find
the best translation based on the local context, which
generally leads to better translations.
However, not all words add the same amount of
contextual information. Using the same example for
?in?, if we add the context ?(hid the key) in?, it is
still not possible to accurately identify the best trans-
lation for the word ?in?. The phrase extraction algo-
rithm (Ling et al 2010) does not discriminate which
phrases pairs encode contextual information, and ex-
tracts all phrase pairs with consistent alignments.
Hence, phrases that add no contextual information,
such as, p(hid the key in, escondeu a chave na)
and p(hid the key in, escondeu a chave dentro)
are extracted. This is undesirable because we are
populating translation models with redundant phrase
pairs, whose translations can be obtained using com-
962
binations of other phrases with the same probabil-
ities, namely p(hid the key, escondeu a chave),
p(in, dentro) and p(in, na). This is a problem
that is also found in language modeling, where
large amounts of redundant higher-order n-grams
can make the model needlessly large. For backoff
language models, multiple pruning strategies based
on relative entropy have been proposed (Seymore
and Rosenfeld, 1996) (Stolcke, 1998), where the ob-
jective is to prune n-grams in a way to minimize the
relative entropy between the model before and after
pruning.
While the concept of using relative entropy for
pruning is not new and frequently used in backoff
language models, there are no such models for ma-
chine translation. Thus, the main contribution of
our work is to propose a relative entropy pruning
model for translation models used in Phrase-based
Machine Translation. It is shown that our pruning
algorithm can eliminate phrase pairs with little or
no impact in the predictions made in our translation
model. In fact, by reducing the search space, less
search errors are made during decoding, which leads
to improvements in translation quality.
This paper is organized as follows. We describe
and contrast the state of the art pruning algorithms
in section 2. In section 3, we describe our relative-
entropy model for machine translation. Afterwards,
in section 4, we apply our model for pruning in
Phrase-based Machine Translation systems. We per-
form experiments with our pruning algorithm based
on phrase pair independence and analyse the results
in section 5. Finally, we conclude in section 6.
2 Phrase Table Pruning
Phrase table pruning algorithms are important in
translation, since they efficiently reduce the size of
the translation model, without having a large nega-
tive impact in the translation quality. This is espe-
cially relevant in environments where memory con-
straints are imposed, such as translation systems for
small devices like cellphones, and also when time
constraints for the translation are defined, such as
online Speech-to-Speech systems.
2.1 Significance Pruning
A relevant reference in phrase table pruning is the
work of (Johnson and Martin, 2007), where it is
shown that a significant portion of the phrase ta-
ble can be discarded without a considerable negative
impact on translation quality, or even positive one.
This work computes the probability, named p-value,
that the joint occurrence event of the source phrase
s and target phrase t occurring in same sentence pair
happens by chance, and are actually statistically in-
dependent. Phrase pairs that have a high p-value,
are more likely to be spurious and more prone to
be pruned. This work is followed in (Tomeh et al
2009), where phrase pairs are treated discriminately
based on their complexity. Significance-based prun-
ing has also been successfully applied in language
modeling in (Moore and Quirk, 2009).
Our work has a similar objective, but instead
of trying to predict the independence between the
source and target phrases in each phrase pair, we at-
tempt to predict the independence between a phrase
pair and other phrase pairs in the model.
2.2 Relevance Pruning
Another proposed approach (Matthias Eck and
Waibel, 2007) consists at collecting usage statistics
for phrase pairs. This algorithm decodes the train-
ing corpora and extracts the number of times each
phrase pair is used in the 1-best translation hypoth-
esis. Thus, phrase pairs that are rarely used during
decoding are excluded first during pruning.
This method considers the relationship between
phrase pairs in the model, since it tests whether
the decoder is more prone to use some phrase pairs
than others. However, it leads to some undesirable
pruning choices. Let us consider a source phrase
?the box in China? and 2 translation hypotheses,
where the first hypothesis uses the phrase transla-
tion p(the key in China, a chave na China) with
probability 70%, and the second hypothesis uses
two phrase translations p(the key, a chave) and
p(in China, na China) with probability 65%. This
approach will lean towards pruning the phrase pairs
in the second hypothesis, since the decoder will use
the first hypothesis. This is generally not desired,
since the 2 smaller phrase pairs can be used to trans-
late the same source sentence with a small probabil-
963
ity loss (5%), even if the longer phrase is pruned.
On the other hand, if the smaller phrases are pruned,
the longer phrase can not be used to translate smaller
chunks, such as ?the key in Portugal?. This matter is
aggravated due to the fact that the training corpora is
used to decode, so longer phrase pairs will be used
more frequently than when translating unseen sen-
tences, which will make the model more biased into
pruning shorter phrase pairs.
3 Relative Entropy Model For
Phrase-based Translation Models
In this section, we shall define our entropy model
for phrase pairs. We start by introducing some no-
tation to distinguish different types of phrase pairs
and show why some phrase pairs are more redun-
dant than others. Afterwards, we illustrate our no-
tion of relative entropy between phrase pairs. Then,
we describe our entropy model, its computation and
its application to phrase table pruning.
3.1 Atomic and Composite Phrase Pairs
We discriminate between 2 types of phrase pairs:
atomic phrase pairs and composite phrase pairs.
Atomic phrase pairs define the smallest transla-
tion units, such that given an atomic phrase pair that
translates from s to t, the same translation cannot
be obtained using any combination of other phrase
pairs. Removing these phrase pairs reduces the
range of translations that our model is capable of
translating and also the possible translations.
Composite phrase pairs define translations of a
given sequence of words that can also be obtained
using atomic or other smaller composite phrase
pairs. Each combination is called a derivation or
translation hypothesis. Removing these phrase pairs
does not change the amount of sentences that the
model can translate, since all translations encoded
in these phrases can still be translated using other
phrases, but these will lead to different translation
probabilities.
Considering table 1, we can see that atomic
phrases encode one elementary translation event,
while composite phrases encode joint events that are
encoded in atomic phrase pairs. If we look at the
source phrase ?in?, there is a multitude of possible
translations for this word in most target languages.
Taking Portuguese as the target language, the proba-
bility that ?in? is translated to ?em? is relatively low,
since it can also be translated to ?no?, ?na?, ?den-
tro?, ?dentro de? and many others.
However, if we add another word such as ?Por-
tugal? forming ?in Portugal?, it is more likely that
?in? is translated to ?em?. Thus, we define the
joint event of ?in? translating to ?em? (A1) and
?Portugal? to ?Portugal? (B1), denoted as A1 ? B1,
in the phrase pair p(in Portugal, em Portugal).
Without this phrase pair it is assumed that these
are independent events with probability given by
P (A1)P (B1)1, which would be 10%, leading to a
60% reduction. In this case, it would be more likely,
that in Portugal is translated to no Portugal or
na Portugal, which would be incorrect.
Some words, such as ?John?, forming ?John in?,
do not influence the translations for the word ?in?,
since it can still be translated to ?em?, ?no?, ?na?,
?dentro? or ?dentro de? depending on the word that
follows. By definition, if the presence of phrase
p(John, John) does not influence the translation of
p(in, em) and viceversa, we can say that probability
of the joint event P (A1?C1) is equal to the product
of the probabilities of the events P (A1)P (C1).
If we were given a choice of pruning either the
composite phrase pairs p(John in, John em) or
p(in Portugal, em Portugal), the obvious choice
would be the former, since the probability of the
event encoded in that phrase pair is composed by 2
independent events, in which case the decoder will
inherently consider the hypothesis that ?John in? is
translated to ?John em? with the same probability. In
another words, the model?s predictions even, with-
out this phrase pair will remain the same.
The example above shows an extreme case,
where the event encoded in the phrase pair
p(John in, John em) is decomposed into indepen-
dent events, and can be removed without chang-
ing the model?s prediction. However, finding and
pruning phrase pairs that are independent, based on
smaller events is impractical, since most translation
events are not strictly independent. However, many
phrase pairs can be replaced with derivations using
smaller phrases with a small loss in the model?s pre-
1For simplicity, we assume at this stage that no reordering
model is used
964
Phrase Pair Prob Event
Atomic Phrase Pairs
in? em 10% A1
in? na 20% A2
in? no 20% A3
in? dentro 5% A4
in? dentro de 5% A5
Portugal? Portugal 100% B1
John? John 100% C1
Composite Phrase Pairs
in Portugal? em Portugal 70% A1 ?B1
John in? John em 10% C1 ?A1
John in? John na 20% C1 ?A2
John in? John no 20% C1 ?A3
John in? John dentro 5% C1 ?A4
John in? John dentro de 5% C1 ?A5
Table 1: Phrase Translation Table with associated events
dictions.
Hence, we would like to define a metric for phrase
pairs that allows us evaluate how discarding each
phrase pair will affect the pruned model?s predic-
tions. By removing phrase pairs that can be derived
using smaller phrase pairs with similar probability,
it is possible to discard a significant portion of the
translation model, while minimizing the impact on
the model?s predictions.
3.2 Relative Entropy Model for Machine
Translation
For each phrase pair pa, we define the supporting
set SP (pa(s, t)) = S1, ..., Sk, where each element
Si = pi, ..., pj is a distinct derivation of pa(s, t) that
translates s to t, with probability P (Si) = P (pi) ?
...?P (pj). A phrase pair can have multiple elements
in its supporting set. For instance, the phrase pair
p(John in Portugal, John em Portugal), has 3
elements in the support set:
? S1 = {p(John, John), p(in, em), p(Portugal, Portugal)}
? S2 = {p(John, John), p(in Portugal, em Portugal)}
? S3 = {p(John in, John em), p(Portugal, Portugal)}
S1, S2 and S3 encode 3 different assumptions
about the event of translating ?John in Portugal?
to ?John em Portugal?. S1 assumes that the event
is composed by 3 independent events A1, B1 and
C1, S2 assumes that A1 and B1 are dependent, and
groups them into a single composite event A1 ?B1,
which is independent from C1, and S3 groups A1
and C1 independently from B1. As expected, the
event encoded in the phrase pair p itself isA1?B1?
C1, which assumes thatA1,B1 andC1 are all depen-
dent. We can see that if any of the events S1, S2 or
S3 has a ?similar probability? as the event coded in
the phrase pair, we can remove this phrase pair with
a minimal impact in the phrase prediction.
To formalize our notion of ?similar probabil-
ity?, we apply the relative entropy or the Kullback-
Leibler divergence, and define the divergence be-
tween a pruned translation model Pp(s, t) and the
unpruned model P (s, t) as:
D(Pp||P ) = ?
?
s,t
P (s, t)log
Pp(t|s)
P (t|s)
(1)
Where Pp(t|s)P (t|s) , measures the deviation from the
probability emission from the pruned model and the
original probability from the unpruned model, for
each source-target pair s, t. This is weighted by
the frequency that the pair s, t is observed, given by
P (s, t).
Our objective is to minimize D(Pp||P ), which
can be done locally by removing phrase pairs p(s, t)
with the lowest values for ?P (s, t)logPp(t|s)P (t|s) . Ide-
ally, we would want to minimize the relative entropy
for all possible source and target sentences, rather
than all phrases in our model. However, minimiz-
ing such an objective function would be intractable
due to reordering, since the probability assigned to a
phrase pair in a sentence pair by each model would
depend on the positioning of all other phrase pairs
used in the sentence. Because of these dependen-
cies, we would not be able to reduce this problem to
a local minimization problem. Thus, we assume that
all phrase pairs have the same probability regardless
of their context in a sentence.
Thus, our pruning algorithm takes a threshold ?
and prunes all phrase pairs that fail to meet the fol-
lowing criteria:
?P (s, t)log
Pp(t|s)
P (t|s)
> ? (2)
The main components of this function is the ratio
between the emission from the pruned model and
965
unpruned models given by Pp(t|s)P (t|s) , and the weight
given to each s, t pair given by P (s, t). In the re-
mainder of this section, we will focus on how to
model each of these components in equation 2.
3.3 Computing P (s, t)
The term P (s, t) can be seen as a weighting function
for each s, t pair. There is no obvious optimal dis-
tribution to model P (s, t). In this work, we apply 2
different distributions for P (s, t). First, an uniform
distribution, where all phrases are weighted equally.
Secondly, a multinomial function defined as:
P (s, t) =
N(s, t)
N
(3)
whereN is the number of sentence pairs in the paral-
lel data, and N(s, t) is the number of sentence pairs
where s was observed in the source sentence and t
was observed in the target sentence. Using this dis-
tribution, the model is more biased in pruning phrase
pairs with s, t pairs that do not occur frequently.
3.4 Computing Pp(t|s)P (t|s)
The computation of Pp(t|s)P (t|s) depends on how the de-
coder adapts when a phrase pair is pruned from the
model. In the case of back-off language models,
this can be solved by calculating the difference of
the logs between the n-gram estimate and the back-
off estimate. However, a translation decoder gen-
erally functions differently. In our work, we will
assume that the decoding will be performed using
a Viterbi decoder, such as MOSES (Koehn et al
2007), where the translation with the highest score
is chosen.
In the example above, where s=?John in Portu-
gal? and t=?John em Portugal?, the decoder would
choose the derivation with the highest probability
from s to t. Using the unpruned model, the possi-
ble derivations are either using phrase p(s, t) or one
element of its support set S1, S2 or S3. On the other
hand, on the pruned model where p(s, t) does not
exist, only S1, S2 and S3 can be used. Thus, given
a s, t pair one of three situations may occur. First, if
the probability of the phrase pair p(s, t) is lower than
the highest probability element in SP (p(s, t)), then
both the models will choose that element, in which
case, Pp(t|s)P (t|s) = 1. This can happen, if we define
features that penalize longer phrase pairs, such as
lexical weighting, or if we apply smoothing (Foster
et al 2006). Secondly, if the probability of p(s, t)
is equal to the most likely element in SP (p(s, t)),
regardless of whether the unpruned model choses to
use p(s, t) or that element, the probability emissions
of the pruned and unpruned model will be identi-
cal. Thus, for this case Pp(t|s)P (t|s) = 1. Finally, if the
probability of p(s, t) is higher than other possible
derivations, the unpruned model will choose to emit
the probability of p(s, t), while the pruned model
will emit the most likely element in SP (p(s, t)).
Hence, the probability loss between the 2 models,
will be the ratio between the probability of p(s, t)
and the probability of the most likely element in
SP (p(s, t)).
From the example above, we can generalize the
function for Pp(t|s)P (t|s) as:
?
p??argmax(SP (p(s,t))) P (p
?)
P (p(s, t))
(4)
Where P (p(s, t)) denotes the probability of
p(s, t) and
?
p??argmax(SP (p(s,t))) P (p
?) the most
likely sequence of phrasal translations that translates
s to t, with the probability equal to the product of all
phrase translation probabilities in that sequence.
Replacing in equation 2, our final condition that
must be satisfied for keeping a phrase pair is:
?P (s, t)log
?
p??argmax(SP (p(s,t))) P (p
?)
P (p(s, t))
> ? (5)
4 Application for Phrase-based Machine
Translation
We will now show how we apply our entropy prun-
ing model in the state-of-the-art phrase-based trans-
lation system MOSES and describe the problems
that need to be addressed during the implementation
of this model.
4.1 Translation Model
The translation model in Moses is composed by
a phrase translation model and a phrase reorder-
ing model. The first one models, for each phrase
pair p(s, t), the probability of translating the s to
t by combining multiple features ?i, weighted by
966
wTi , as PT (p) =
?n
i=1 ?i(p)
wTi . The reordering
model is similar, but models the local reordering be-
tween p, given the previous and next phrase accord-
ing to the target side, pP and pN , or more formally,
PR(p|pP , pN ) =
?m
i=1 ?i(p|pP , pP )
wRi
4.2 Building the Support Set
Essentially, implementing our model is equiva-
lent to calculating the components described in
equation 5. These are P (s, t), P (p(s|t)) and
argmax(SP (p(s, t))). Calculating the uniform dis-
tribution and multinomial distributions for P (s, t)
is simple, the uniform distribution just assumes the
same value for all s and t, and the multinomial dis-
tribution can be modeled by extracting counts from
the parallel corpora.
Calculating P (s|t) is also trivial, since it only en-
volves calculating PT (p(s, t)), which can be done
by retrieving the translation features of p and apply-
ing the weights for each feature.
The most challenging task is to calculate
argmax(SP (p(s, t))), which is similar to the de-
coding task in machine translation, where we need to
find the best translation t? for a sentence s, that is, t? =
argmaxtP (s|t)P (t). In practice, we are not search-
ing in the space of possible translations, but in the
space of possible derivations, which are sequences
of phrase translations p1(s1, t1), ..., pn(sn, tn) that
can be applied to s to generate an output t with the
score given by P (t)
?n
i=1 P (si, ti).
Our algorithm to determine SP (p(s, t)) can be
described as an adaptation to the decoding algorithm
in Moses, where we restrict the search space to the
subspace SP (p(s, t)), that is, our search space is
only composed by derivations that output t, with-
out using p itself. This can be done using the forced
decoding algorithm proposed in (Schwartz, 2008).
Secondly, the score of a given translation hypothesis
does not depend on the language model probability
P (t), since all derivations in this search space have
the same t, thus we discard this probability from
the score function. Finally, rather than using beam
search, we exhaustively search all the search space,
to reduce the hypothesis of incurring a search error
at this stage. This is possible, since phrase pairs are
generally smaller than text (less than 8 words), and
because we are constraining the search space to t,
which is an order of magnitude smaller than the reg-
ular search space with all possible translations.
4.3 Pruning Algorithm
The algorithm to generate a pruned translation
model is shown in 1. We iterate over all phrase pairs
p1(s1, t1), ..., pn(sn, tn), decode using our forced
decoding algorithm from si to ti, to obtain the best
path S. If no path is found then it means that the pi
is atomic. Then, we prune pi based on condition 5.
Algorithm 1 Independence Pruning
Require: pruning threshold ?,
unpruned model {p1(s1, t1), ..., pn(sn, tn)}
for pi(si, ti) ? {p1(s1, t1), ..., pn(sn, tn)} do
S := argmax(SP (pi)) \ pi
score :=?
if S 6= {} then
score := ?P (s, t)log
?
p?(s?,t?)?S P (s
?|t?)
P (s|t)
end if
if score ? ? then
prune(pi)
end if
end for
return pruned model
The main bottle neck in this algorithm is find-
ing argmax(SP (pi)). While this appears relatively
simple and similar to a document decoding task, the
size of our task is on a different order of magni-
tude, since we need to decode every phrase pair in
the translation model, which might not be tractable
for large models with millions of phrase pairs. We
address this problem in section 5.3.
Another problem with this algorithm is that the
decision to prune each phrase pair is made assuming
that all other phrase pairs will remain in the model.
Thus, there is a chance a phrase pair p1 is pruned
because of a derivation using p2 and p3 that leads to
the same translation. However, if p3 also happens to
be pruned, such a derivation will no longer be pos-
sible. One possible solution to address this problem
is to perform pruning iteratively, from the smallest
phrase pairs (number of words) and increase the size
at each iteration. However, we find this undesirable,
since the model will be biased into removing smaller
phrase pairs, which are generally more useful, since
they can be used in multiple derivation to replace
larger phrase pairs. In the example above, the model
967
would eliminate p3 and keep p1, yet the best deci-
sion could be to keep p3 and remove p1, if p3 is also
frequently used in derivations of other phrase pairs.
Thus, we leave the problem of finding the best set of
phrases to prune as future work.
5 Experiments
We tested the performance of our system under two
different environments. The first is the small scale
DIALOG translation task for IWSLT 2010 evalua-
tion (Paul et al 2010) using a small corpora for
the Chinese-English language pair (henceforth re-
ferred to as ?IWSLT?). The second one is a large
scale test using the complete EUROPARL (Koehn,
2005) corpora for the Portuguese-English language
pair, which we will denote by ?EUROPARL?.
5.1 Corpus
The IWSLT model was trained with 30K training
sentences. The development corpus and test corpus
were taken from the evaluation dataset in IWSLT
2006 (489 tuning and 500 test sentences with 7 ref-
erences). The EUROPARL model was trained using
the EUROPARL corpora with approximately 1.3M
sentence pairs, leaving out 1K sentences for tuning
and another 1K sentences for tests.
5.2 Setup
In the IWSLT experiment, word alignments were
generated using an HMM model (Vogel et al 1996),
with symmetric posterior constraints (V. Grac?a et
al., 2010), using the Geppetto toolkit2. This setup
was used in the official evaluation in (Ling et al
2010). For the EUROPARL experiment the word
alignments were generated using IBM model 4. In
both experiments, the translation model was built
using the phrase extraction algorithm (Paul et al
2010), with commonly used features in Moses (Ex:
probability, lexical weighting, lexicalized reordering
model). The optimization of the translation model
weights was done using MERT tuning (Och, 2003)
and the results were evaluated using BLEU-4.
5.3 Pruning Setup
Our pruning algorithm is applied after the translation
model weight optimization with MERT. We gener-
2http://code.google.com/p/geppetto/
ate multiple translation models by setting different
values for ?, so that translation models of different
sizes are generated at intervals of 5%. We also run
the significance pruning (Johnson and Martin, 2007)
algorithm in these conditions.
While the IWSLT translation model has only
88,424 phrase pairs, for the EUROPARL exper-
iment, the translation model was composed by
48,762,372 phrase pairs, which had to be decoded.
The average time to decode each phrase pair us-
ing the full translation model is 4 seconds per sen-
tence, since the table must be read from disk due to
its size. This would make translating 48M phrase
pairs unfeasible. To address this problem, we di-
vide the phrase pairs in the translation model into
blocks of K phrase pairs, that are processed sepa-
rately. For each block, we resort to the approach
used in MERT tuning, where the model is filtered to
only include the phrase pairs that are used for trans-
lating tuning sentences. We filter each block with
phrase pairs fromK to 2K with the source sentences
sK , ..., s2K . Furthermore, since we are force de-
coding using the target sentences, we also filter the
remaining translation models using the target sen-
tences tK , ..., t2K . We used blocks of 10,000 phrase
pairs and each filtered table was reduced to less than
1% of the translation table on average, reducing the
average decoding time to 0.03 seconds per sentence.
Furthermore, each block can be processed in parallel
allowing multiple processes to be used for the task,
depending on the resources that are available.
5.4 Results
Figure 1 shows the BLEU results for different sizes
of the translation model for the IWSLT experiment
using the uniform and multinomial distributions for
P (s, t). We observe that there is a range of values
from 65% to 95% where we actually observe im-
provements caused by our pruning algorithm, with
the peak at 85% for the uniform distribution, where
we improve from 15.68 to 15.82 (0.9% improve-
ment). Between 26% and 65%, the BLEU score is
lower than the baseline at 100%, with the minimum
at 26% with 15.54, where only atomic phrase pairs
remain and both the multinomial and uniform distri-
bution have the same performance, obviously. This
is a considerable reduction in phrase table size by
sacrificing 0.14 BLEU points. Regarding the com-
968
15.5	 ?
15.55	 ?
15.6	 ?
15.65	 ?
15.7	 ?
15.75	 ?
15.8	 ?
15.85	 ?
25
%	 ?
30
%	 ?
35
%	 ?
40
%	 ?
45
%	 ?
50
%	 ?
55
%	 ?
60
%	 ?
65
%	 ?
70
%	 ?
75
%	 ?
80
%	 ?
85
%	 ?
90
%	 ?
95
%	 ?
10
0%
	 ?
IWSLT	 ?Results	 ?
Uniform	 ?
Mul?nomial	 ?
Figure 1: Results for the IWSLT experiment. The x-
axis shows the percentage of the phrase table used. The
BLEU scores are shown in the y-axis. Two distributions
for P (s, t) were tested Uniform and Multinomial.
parison between the uniform and multinomial distri-
bution, we can see that both distributions yield sim-
ilar results, specially when a low number of phrase
pairs is pruned. In theory, the multinomial distri-
bution should yield better results, since the pruning
model will prefer to prune phrase pairs that are more
likely to be observed. However, longer phrase pairs,
which tend compete with other long phrase pairs on
which get pruned first. These phrase pairs gener-
ally occur only once or twice, so the multinomial
model will act similarly to the uniform model re-
garding longer phrase pairs. On the other hand, as
the model size reduces, we can see that using multi-
nomial distribution seems to start to improve over
the uniform distribution.
The comparison between our pruning model and
pruning based on significance is shown in table 2.
These models are hard to compare, since not all
phrase table sizes can be obtained using both met-
rics. For instance, the significance metric can ei-
ther keep or remove all phrase pairs that only appear
once, leaving a large gap of phrase table sizes that
cannot be attained. In the EUROPARL experiment
the sizes of the table suddenly drops from 60% to
8%. The same happens with our metric that cannot
distinguish atomic phrase pairs. In the EUROPARL
experiment, we cannot generate phrase tables with
sizes smaller than 15%. Thus, we only show re-
sults at points where both algorithms can produce
a phrase table.
Significant improvements are observed in the
Table size Significance Entropy (u) Entropy (m)
Pruning Pruning Pruning
IWSLT
57K (65%) 14.82 15.77 15.78
71K (80%) 15.14 15.76 15.77
80K (90%) 15.31 15.73 15.72
88K (100%) 15.68 15.68 15.68
EUROPARL
29M (60%) 28.64 28.82 28.91
34M (70%) 28.84 28.94 28.99
39M (80%) 28.86 28.99 28.99
44M (90%) 28.91 29.00 29.02
49M (100%) 29.18 29.18 29.18
Table 2: Comparison between Significance Pruning (Sig-
nificance Pruning) and Entropy-based pruning using the
uniform (Entropy (u) Pruning) and multinomial distribu-
tions (Entropy (m) Pruning).
IWSLT experiment, where significance pruning
does not perform as well. On the other hand, on the
EUROPARL experiment, our model only achieves
slightly higher results. We believe that this is re-
lated by the fact the EUROPARL corpora is gener-
ated from automatically aligning documents, which
means that there are misaligned sentence pairs.
Thus, many spurious phrase pairs are extracted. Sig-
nificance pruning performs well under these condi-
tions, since the measure is designed for this purpose.
In our metric, we do not have any means for detect-
ing spurious phrase pairs, in fact, spurious phrase
pairs are probably kept in the phrase table, since
each distinct spurious phrase pair is only extracted
once, and thus, they have very few derivations in
its support set. This suggests, that the significance
score can be integrated in our model to improve our
model, which we leave as future work.
John married Portugal
married 
in
in 
Portugal
married 
married 
in
John 
in 
Portugal
Portugal
a)
b)
Figure 2: Translation order in for different reordering
starting from left to right.
We believe that in language pairs such as Chinese-
969
English with large distance reorderings between
phrases are more prone to search errors and benefit
more from our pruning algorithm. To illustrate this,
let us consider the source sentence ?John married
in Portugal?, and translating either using the blocks
?John?, ?married? and ?in Portugal? or the blocks
?John?, ?married in?, ?Portugal?, the first hypoth-
esis would be much more viable, since the word
?Portugal? is more relevant as the context for the
word ?in?. Thus, the key choice for the decoder is
to decide whether to translate using ?married? with
or without ?in?, and it is only able to predict that
it is better to translate ?married? by itself until it
finds that ?in? is better translated with ?Portugal?.
Thus, a search error occurs if the hypothesis where
?married? is translated by itself is removed. In fig-
ure 2, we can see the order that blocks are consid-
ered for different reorderings, starting from left to
right. In a), we illustrate the case for a monotonous
translation. We observe that the correct decision be-
tween translating ?married in? or just ?married? is
found immediately, since the blocks ?Portugal? and
?in Portugal? are considered right afterwards. In this
case, it is unlikely that the hypothesis using ?mar-
ried? is removed. However, if we consider that due
to reordering, ?John? is translated after ?married?
and before ?Portugal?, which is shown in b). Then,
the correct decision can only be found after consid-
ering ?John?. In this case, ?John? does not have
many translations, so the likelihood of eliminating
the correct hypothesis. However, if there were many
translations for John, it is highly likely that the cor-
rect partial hypothesis is eliminated. Furthermore,
the more words exist between ?married? and ?Portu-
gal?, the more likely will the correct hypothesis not
exist when we reach ?Portugal?. By pruning the hy-
pothesis ?married in? a priori, we contribute in pre-
venting such search errors.
We observe that some categories of phrase pairs
that are systematically pruned, but these cannot
be generalized in rules, since there are many ex-
ceptions. The most obvious type of phrase pairs
are phrases with punctuations, such as ???.? to
?thanks .? and ?. ??? to ?thanks .?, since ?.?
is translated independently from most contextual
words. However, this rule should not be general-
ized, since in some cases ?.? is a relevant contextual
marker. For instance, the word ?please? is translated
to ??? in the sentence ?open the door, please.? and
translated to ????? in ?please my advisors?. An-
other example are sequences of numbers, which are
generally translated literally. For instance, ??(8)
?(3)?(8)? is translated to ?eight three eight? (Ex:
?room eight three eight?). Thus, phrase pairs for
number sequences can be removed, since those num-
bers can be translated one by one. However, for se-
quences such as ??(1)?(8)?, we need a phrase pair
to represent this specifically. This is because ??(1)?
can be translated to ?one?, but also to ?a?, ?an?, ?sin-
gle?. Other exceptions include ??(1)?(1)?, which
tends to be translated as ?eleven?, and which tends to
be translated to ?o?, rather than ?zero? in sequences
(?room eleven o five?).
6 Conclusions
We present a pruning algorithm for Machine Trans-
lation based on relative entropy, where we assess
whether the translation event encoded in a phrase
pair can be decomposed into combinations of events
encoded in other phrase pairs. We show that such
phrase pairs can be removed from the translation
model with little negative impact or even a positive
one in the overall translation quality. Tests show that
our method yields comparable or better results with
state of the art pruning algorithms.
As future work, we would like to combine our
approach with significance pruning, since both ap-
proaches are orthogonal and address different issues.
We also plan to improve the pruning step of our algo-
rithm to find the optimal set of phrase pairs to prune
given the pruning threshold.
The code used in this work will be made available.
7 Acknowledgements
This work was partially supported by FCT (INESC-
ID multiannual funding) through the PIDDAC Pro-
gram funds, and also through projects CMU-
PT/HuMach/0039/2008 and CMU-PT/0005/2007.
The PhD thesis of Wang Ling is supported by FCT
grant SFRH/BD/51157/2010. The authors also wish
to thank the anonymous reviewers for many helpful
comments.
970
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19:263?311, June.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?06, pages 53?61, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
J Howard Johnson and Joel Martin. 2007. Improv-
ing translation quality by discarding most of the
phrasetable. In In Proceedings of EMNLP-CoNLL?07,
pages 967?975.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 48?54, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-burch, Richard Zens, Rwth Aachen, Alexan-
dra Constantin, Marcello Federico, Nicola Bertoldi,
Chris Dyer, Brooke Cowan, Wade Shen, Christine
Moran, and Ondrej Bojar. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Summit,
pages 79?86, Phuket, Thailand. AAMT, AAMT.
Wang Ling, Tiago Lu??s, Joa?o Grac?a, Lu??sa Coheur, and
Isabel Trancoso. 2010. Towards a general and ex-
tensible phrase-extraction algorithm. In IWSLT ?10:
International Workshop on Spoken Language Transla-
tion, pages 313?320, Paris, France.
Stephen Vogal Matthias Eck and Alex Waibel. 2007. Es-
timating phrase pair relevance for translation model
pruning. MTSummit XI.
Robert C. Moore and Chris Quirk. 2009. Less is more:
significance-based n-gram selection for smaller, bet-
ter language models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 2 - Volume 2, EMNLP ?09,
pages 746?755, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics - Volume 1, ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Michael Paul, Marcello Federico, and Sebastian Stu?ker.
2010. Overview of the iwslt 2010 evaluation cam-
paign. In IWSLT ?10: International Workshop on Spo-
ken Language Translation, pages 3?27.
Lane Schwartz. 2008. Multi-source translation methods.
In Proceedings of AMTA, pages 279?288.
Kristie Seymore and Ronald Rosenfeld. 1996. Scalable
backoff language models. In In Proceedings of ICSLP,
pages 232?235.
Andreas Stolcke. 1998. Entropy-based pruning of back-
off language models. In In Proc. DARPA Broad-
cast News Transcription and Understanding Work-
shop, pages 270?274.
Nadi Tomeh, Nicola Cancedda, and Marc Dymetman.
2009. Complexity-based phrase-table filtering for sta-
tistical machine translation. MTSummit XII, Aug.
Joa?o V. Grac?a, Kuzman Ganchev, and Ben Taskar. 2010.
Learning Tractable Word Alignment Models with
Complex Constraints. Comput. Linguist., 36:481?504.
S. Vogel, H. Ney, and C. Tillmann. 1996. Hmm-
based word alignment in statistical translation. In
Proceedings of the 16th conference on Computational
linguistics-Volume 2, pages 836?841. Association for
Computational Linguistics.
971
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 73?84,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Paraphrasing 4 Microblog Normalization
Wang Ling Chris Dyer Alan W Black Isabel Trancoso
L2F Spoken Systems Lab, INESC-ID, Lisbon, Portugal
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA
Instituto Superior Te?cnico, Lisbon, Portugal
{lingwang,cdyer,awb}@cs.cmu.edu
isabel.trancoso@inesc-id.pt
Abstract
Compared to the edited genres that have
played a central role in NLP research, mi-
croblog texts use a more informal register with
nonstandard lexical items, abbreviations, and
free orthographic variation. When confronted
with such input, conventional text analysis
tools often perform poorly. Normalization
? replacing orthographically or lexically id-
iosyncratic forms with more standard variants
? can improve performance. We propose a
method for learning normalization rules from
machine translations of a parallel corpus of
microblog messages. To validate the utility of
our approach, we evaluate extrinsically, show-
ing that normalizing English tweets and then
translating improves translation quality (com-
pared to translating unnormalized text) using
three standard web translation services as well
as a phrase-based translation system trained
on parallel microblog data.
1 Introduction
Microblogs such as Twitter, Sina Weibo (a popular
Chinese microblog service) and Facebook have re-
ceived increasing attention in diverse research com-
munities (Han and Baldwin, 2011; Hawn, 2009, in-
ter alia). In contrast to traditional text domains that
use carefully controlled, standardized language, mi-
croblog content is often informal, with less adher-
ence to conventions regarding punctuation, spelling,
and style, and with a higher proportion of dialect
or pronouciation-derived orthography. While this
diversity itself is an important resource for study-
ing, e.g., sociolinguistic variation (Eisenstein et al,
2011; Eisenstein, 2013), it poses challenges to NLP
applications developed for more formal domains. If
retaining variation due to sociolinguistic or phono-
logical factors is not crucial, text normalization can
improve performance on downstream tasks (?2).
This paper introduces a data-driven approach to
learning normalization rules by conceiving of nor-
malization as a kind of paraphrasing and taking
inspiration from the bilingual pivot approach to
paraphrase detection (Bannard and Callison-Burch,
2005) and the observation that translation is an
inherently ?simplifying? process (Laviosa, 1998;
Volansky et al, 2013). Starting from a parallel cor-
pus of microblog messages consisting of English
paired with several other languages (Ling et al,
2013), we use standard web machine translation sys-
tems to re-translate the non-English segment, pro-
ducing ?English original,English MT? pairs (?3).
These are our normalization examples, with MT out-
put playing the role of normalized English. Sev-
eral techniques for identifying high-precision nor-
malization rules are proposed, and we introduce a
character-based normalization model to account for
predictable character-level processes, like repetition
and substitution (?4). We then describe our decod-
ing procedure (?5) and show that our normaliza-
tion model improve translation quality for English?
Chinese microblog translation (?6).1
2 Why Normalize?
Consider the English tweet shown in the first row of
Table 1 which contains several elements that NLP
1The datasets used in this paper are available from http:
//www.cs.cmu.edu/?lingwang/microtopia.
73
Table 1: Translations of an English microblog message
into Mandarin, using three web translation services.
orig. To DanielVeuleman yea iknw imma work on that
MT1 ?iknw DanielVeuleman?????
MT2 DanielVeuleman?iknw???????
MT3 ?DanielVeuleman??iknw imma??????
systems trained on edited domains may not handle
well. First, it contains several nonstandard abbre-
viations, such as, yea, iknw and imma (abbrevia-
tions of yes, I know and I am going to). Second,
there is no punctuation in the text although stan-
dard convention would dictate that it should be used.
To illustrate the effect this can have, consider now
the translations produced by Google Translate,2 Mi-
crosoft Bing,3 and Youdao,4 shown in rows 2?4.
Even with no knowledge of Chinese, it is not hard
to see that all engines have produced poor transla-
tions: the abbreviation iknw is left translated by all
engines, and imma is variously deleted, left untrans-
lated, or transliterated into the meaningless sequence
?? (pronounced y?? ma?).
While normalization to a form like To Daniel
Veuleman: Yes, I know. I am going to work on that.
does indeed lose some information (information im-
portant for an analysis of sociolinguistic or phono-
logical variation clearly goes missing), it expresses
the propositional content of the original in a form
that is more amenable to processing by traditional
tools. Translating the normalized form with Google
Translate produces ????Veuleman?????
???????????, which is a substantial
improvement over all translations in Table 1.
3 Obtaining Normalization Examples
We want to treat normalization as a supervised learn-
ing problem akin to machine translation, and to do
so, we need to obtain pairs of microblog posts and
their normalized forms. While it would be possible
to ask annotators to create such a corpus, it would
be quite expensive to obtain large numbers of ex-
amples. In this section, we propose a method for
creating normalization examples without any human
2http://translate.google.com/
3http://www.bing.com/translator
4http://fanyi.youdao.com/
Table 2: Translations of Chinese original post to English
using web-based service.
orig. To DanielVeuleman yea iknw imma work on that
orig. ?DanielVeuleman?????????
?????????
MT1 Right DanielVeuleman say, yes, I know, I?m
Xiangna efforts
MT2 DanielVeuleman said, Yes, I know, I?m that hard
MT3 Said to DanielVeuleman, yes, I know, I?m to
that effort
annotation, by leveraging existing tools and data re-
sources.
The English example sentence in Table 1 was se-
lected from the ?topia parallel corpus (Ling et
al., 2013), which consists of self-translated mes-
sages from Twitter and Sina Weibo (i.e., each mes-
sage contains a translation of itself). Row 2 of
Table 2 shows the Mandarin self-translation from
the corpus. The key observation is what happens
when we automatically translate the Mandarin ver-
sion back into English. Rows 3?5 shows automatic
translations from three standard web MT engines.
While not perfect, the translations contain several
correctly normalized subphrases. We will use such
re-translations as a source of (noisy) normalization
examples. Since such self-translations are relatively
numerous on microblogs, this technique can provide
a large amount of data.
Of course, to motivate this paper, we argued that
NLP tools ? like the very translation systems we
propose to use ? often fail on unnormalized input.
Is this a problem? We argue that it is not for the
following two reasons.
Normalization in translation. Work in transla-
tion studies has observed that translation tends to
be a generalizing process that ?smooths out? author-
and work-specific idiosyncrasies (Laviosa, 1998;
Volansky et al, 2013). Assuming this observa-
tion is robust, we expect that dialectal variant forms
found in microblogs to be normalized in translation.
Therefore, if the parallel segments in our microblog
parallel corpus did indeed originate through a trans-
lation process (rather than, e.g., being generated as
two independent utterances from a bilingual), we
may then state the following assumption about the
distribution of variant forms in a parallel segment
74
?e, f?: if e contains nonstandard lexical variants,
then f is likely to be a normalized translation using
with fewer nonstandard lexical variants (and vice-
versa).
Uncorrelated orthographic variants. Any writ-
ten language has the potential to make creative use
of orthography: alphabetic scripts can render ap-
proximations of pronunciation variants; logographic
scripts can use homophonic substitutions. However,
the kinds of innovations used in particular languages
will be language specific (depending on details of
the phonology, lexicon, and orthography of the lan-
guage). However, for language pairs that differ sub-
stantially in these dimensions, it may not always
be possible (or at least easy) to preserve particular
kinds of nonstandard orthographic forms in trans-
lation. Consider the (relatively common) pronoun-
verb compounds like iknw and imma from our mo-
tivating example: since Chinese uses a logographic
script without spaces, there is no obvious equivalent.
3.1 Variant?Normalized Parallel Corpus
For the two reasons outlined above, we argue that
we will be able to translate back into English us-
ing MT, even when the underlying English part of
the parallel corpus has a great deal of nonstandard
content. We leverage this fact to build the normal-
ization corpus, where the original English tweet is
treated as the variant form, and the automatic trans-
lation obtained from another language is considered
a potential normalization.5
Our process is as follows. The microblog cor-
pus of Ling et al (2013) contains sentence pairs ex-
tracted from Twitter and Sina Weibo, for multiple
language pairs. We use all corpora that include En-
glish as one of the languages in the pair. The respec-
tive non-English side is translated into English using
different translation engines. The different sets we
used and the engines we used to translate are shown
in Table 3. Thus, for each original English post o,
we obtain n paraphrases {pi}
n
i=1, from n different
translation engines.
5We additionally assume that the translation engines are
trained to output more standardized data, so there will be addi-
tional normalizing effect from the machine translation system.
Table 3: Corpora Used for Paraphrasing.
Lang. Pair Source Segs. MT Engines
ZH-EN Weibo 800K Google, Bing, Youdao
ZH-EN Twitter 113K Google, Bing, Youdao
AR-EN Twitter 114K Google, Bing
RU-EN Twitter 119K Google, Bing
KO-EN Twitter 78K Google, Bing
JA-EN Twitter 75K Google, Bing
3.2 Alignment and Filtering
Our parallel microblog corpus was crawled automat-
ically and contains many misaligned sentences. To
improve precision, we attempt to find the similar-
ity between the (unnormalized) original and each
of the normalizations using an alignment based on
the one used in METEOR (Denkowski and Lavie,
2011), which computes the best alignment between
the original tweet and each of the normalizations
but modified to permit domain-specific approximate
matches. To address lexical variants, we allow fuzzy
word matching, that is, we allow lexically similar,
such as yea and yes to be aligned (similarity is de-
termined by the Levenshtein distance). We also per-
form phrasal matchings, such as ikwn to i know. To
do so, we extend the alignment algorithm from word
to phrasal alignments. More precisely, given the
original post o and a candidate normalization n, we
wish to find the optimal segmentation producing a
good alignment. A segmentation s = ?s1, . . . , s|s|?
is a sequence of segments that aligns as a block to a
source word. For instance, for the sentence yea iknw
imma work on that, one possible segmentation could
be s1 =yea ikwn, s2 =imma and s3 =work on that.
Model. We define the score of an alignment a and
segmentation s in using a model that makes semi-
Markov independence assumptions, similar to the
work in (Bansal et al, 2011), u(a, s | o,n) =
|s|?
i=1
[
ue(si, ai | n)? ut(ai | ai?1)? u`(|si|)
]
In this model, the maximal scoring segmentation
and alignment can be found using a polynomial time
dynamic programming algorithm. Each segment
can be aligned to any word or segment in o. The
aligned segment for sk is defined as ak. For the
75
score of a segment correspondence ue(s, a | n), we
assume that this can be estimated using the lexical
similarity between segments, which we define to be
1? L(sk,ak)max{|sk|,|ak|} , where L(x, y) denotes the Leven-
shtein distance between strings x and y, normalized
by the highest possible distance between those seg-
ments.
For the alignment score ut, we assume that the
relative order of the two sequences will be mostly
monotonous. Thus, we approximate ut with the fol-
lowing density poss(ak) ? pose(ak?1) ? N (1, 1),
where the poss is the index of the first word in the
segment and pose the one of the last word.
After finding the Viterbi alignments, we compute
the similarity measure ? = |A||A|+|U | , used in (Resnik
and Smith, 2003), where |A| and |U | are the number
of words that were aligned and unaligned, respec-
tively. In this work, we extract the pair if ? > 0.2.
4 Normalization Model
From the normalization corpus, we learn a nor-
malization model that generalizes the normalization
process. That is, from the data we observe that To
DanielVeuleman yea iknw imma work on that is nor-
malized to To Daniel Veuleman: yes, I know. I
am going to work on that. However, this is not
useful, since the chances of the exact sentence To
DanielVeuleman yea iknw imma work on that occur-
ring in the data is low. We wish to learn a process to
convert the original tweet into the normalized form.
There are two mechanisms that we use in our
model. The first (?4.1) learns word?word and
phrase?phrase mappings. That is, we wish to find
that DanielVeuleman is normalized to Daniel Veule-
man, that iknw is normalized to I know and that
imma is normalized to I am going. These mappings
are more useful, since whenever iknw occurs in the
data, we have the option to normalize it to I know.
The second (?4.2) learns character sequence map-
pings. If we look at the normalization DanielVeule-
man to Daniel Veuleman, we can see that it is only
applicable when the exact word DanielVeuleman oc-
curs. However, we wish to learn that it is uncom-
mon for the letters l and v to occur in the same word
sequentially, so that be can add missing spaces in
words that contain the lv character sequence, such as
normalizing phenomenalvoter to phenomenal voter.
I wanna go 4 pizza 2day
I want go for pizza todayto
Figure 1: Variant?normalized alignment with the variant
form above and the normalized form below; solid lines
show potential normalizations, while dashed lines repre-
sent identical translations.
However, there are also cases where this is not true,
for instance, in the word velvet, we do not wish to
separate the letters l and v. Thus, we shall describe
the process we use to decide when to apply these
transformations.
4.1 From Sentences To Phrases
The process to find phrases from sentences has been
throughly studied in Machine Translation. This is
generally done in two steps, Word Alignments and
Phrase Extraction.
Alignment. The first step is to find the word-level
alignments between the original post and its nor-
malization. This is a well studied problem in MT,
referred as Word Alignment (Brown et al, 1993).
Many alignment models have been proposed, such
as, the HMM-based word alignment models (Vo-
gel et al, 1996) and the IBM models (Och and
Ney, 2003). Generally, a symmetrization step is per-
formed, where the bidirectional alignments are com-
bined heuristically. In our work, we use the fast
aligner proposed in (Dyer et al, 2013) to obtain the
word alignments. Figure 1 shows an example of an
word aligned pair of a tweet and its normalization.
Phrase Extraction. The phrasal extraction
step (Ling et al, 2010), uses the word aligned
sentences and extracts phrasal mappings between
the original tweet and its normalization, named
phrase pairs. For instance, in Figure 1, we would
like to extract the phrasal mapping from go 4 to go
for, so that we learn that the word 4 in the context of
go is normalized to the proposition for. To do this,
the most common approach is to use the template
proposed in (Och and Ney, 2004), which allows
phrase pairs to be extracted, if there is at least one
word alignment within the pair, and there are no
76
Table 4: Fragment of the phrase normalization model
built, for each original phrase o, we present the top-3 nor-
malized forms ranked by f(n | o).
Original (o) Normalization (n) f(n | o)
wanna want to 0.4679
wanna will 0.0274
wanna going to 0.0114
4 4 0.5641
4 for 0.01795
go 4 go for 1.0000
words inside the pair that are aligned to words not
in the pair. For instance, in the example above, the
phrase pair that normalizes wanna to want to would
be extracted, but the phrase pair normalizing wanna
to want to go would not, because the word go in the
normalization is aligned to a word not in the pair.
Phrasal Features. After extracting the phrase
pairs, a model is produced with features derived
from phrase pair occurrences during extraction. This
model is equivalent to phrasal translation model in
MT, but we shall refer to it as the normalization
model. For a phrase pair ?o,n?, where o is the origi-
nal phrase, and n is the normalized phrase, we com-
pute the normalization relative frequency f(n | o) =
C(n,o)
C(o) , where C(n, o) denotes the number of times
o was normalized to n and C(o) denotes the number
of times o was seen in the extracted phrase pairs. Ta-
ble 4 gives a fragment of the normalization model.
The columns represent the original phrase, its nor-
malization and the probability, respectively.
In Table 4, we observe that the abbreviation
wanna is normalized to want to with a relatively
high probability, but it can also be normalized to
other equivalent expressions, such as will and go-
ing to. The word 4 by itself has a low probability
to be normalized to the preposition for. This is ex-
pected, since this decision cannot be made without
context. However, we see that the phrase go 4 is
normalized to go for with a high probability, which
specifies that within the context of go, 4 is generally
used as a preposition.
4.2 From Phrases to Characters
While we can learn lexical variants that are in the
corpora using the phrase model, we can only address
word forms that have been observed in the corpora.
Table 5: Fragment of the character normalization model
where examples representative of the lexical variant gen-
eration process are encoded in the model.
Original (o) Normalization (n) f(n | o)
o o o o o 0.0223
o o o o 0.0439
s c 0.0331
z s 0.0741
s h c h 0.019
2 t o 0.014
4 f o r 0.0013
0 o 0.0657
i n g f o r i n g <space> f o r 0.4545
g f g <space> f 0.01028
This is quite limited, since we cannot expect all the
word forms to be present, such as all the possible
orthographic errors for the word cat, such as catt,
kat and caaaat. Thus, we will build a character-
based model that learns the process lexical variants
are generated at the subword level.
Our character-based model is similar to the
phrase-based model, except that, rather than learn-
ing word-based mappings from the original tweet
and the normalization sentences, we learn character-
based mappings from the original phrases to the nor-
malizations of those phrases. Thus, we extract the
phrase pairs in the phrasal normalization model, and
use them as a training corpora. To do this, for each
phrase pair, we add a start token, <start>, and a
end token, <end>, at the beginning and ending of
the phrase pair. Afterwards, we separate all charac-
ters by space and add a space token <space> where
spaces were originally. For instance, the phrase
pair normalizing DanielVeuleman to Daniel Veule-
man would be converted to <start> d a n i e l v e u
l e m a n <end> and <start> d a n i e l <space> v
e u l e m a n <end>.
Character-based Normalization Model - To
build the character-based model, we proceed using
the same approach as in the phrasal normalization
model. We first align characters using Word Align-
ment Models, and then we perform phrase extrac-
tion to retrieve the phrasal character segments, and
build the character-based model by collecting statis-
tics. Once again, we provide examples of entries in
the model in Table 5.
77
We observe that many of the normalizations dealt
with in the previous model by memorizing phrases
are captured with string transformations. For in-
stance, from phrase pairs such as tooo to too and
sooo to so, we learn that sequences of o?s can be
reduced to 2 or 1 o. Other examples include or-
thographic substitutions, such as 2 for to and 4
for for (as found in 2gether, 2morrow, 4ever and
4get). Moreover, orthographic errors can be gener-
ated from mistaking characters with similar phonetic
properties, such as, s to c, z to s and sh to ch, gener-
ating lexical variants such as reprecenting. Finally,
we learn that the number 0 that resembles the letter
o, can be used as a replacement, as in g00d. Finally,
we can see that the rule ingfor to ing for attempts to
find segmentation errors, such as goingfor, where a
space between going and for was omitted.6
5 Normalization Decoder
In section 4, we built two models to learn the process
of normalization, the phrase-based model and the
character-based model. In this section, we describe
the decoder we used to normalize the sentences.
The advantage of the phrase-based model is that it
can make decisions for normalization based on con-
text. That is, it contains phrasal units, such as, go
4, that determine, when the word 4 should be nor-
malized to the preposition for and when to leave it
as a number. However, it cannot address words that
are unseen in the corpora. For instance, if the word
form 4ever is not seen in the training corpora, it is
not be able to normalize it, even if it has seen the
word 4get normalized to forget. On the other hand,
the character-based model learns subword normal-
izations, for instance, if we see the word nnnnno
normalized to no, we can learn that repetitions of
the letter n are generally shorted to n, which al-
lows it to generate new word forms. This model
has strong generalization potential, but the weak-
ness of the character-based model is that it fails to
6Note that this captures the context in which such transfor-
mations are likely to occur: there are not many words that con-
tain the sequence ingfor, so the probability that these should be
normalized by inserting a space is high. On the other hand, we
cannot assume that if we observe the sequence gf, we can safely
separate these with a space. This is because, there are many
words that contain this sequence, such as the abbreviation of
gf (girlfriend), dogfight, and bigfoot.
consider the context of the normalization that the
phrase-based model uses to make normalization de-
cisions. Thus, our goal in this section is describe a
decoder that uses both models to improve the quality
of the normalizations.
5.1 Phrasal Decoder
We use Moses, an off-the-shelf phrase-based MT
system (Koehn et al, 2007), to ?translate? the orig-
inal tweet its normalized form using the phrasal
model (?4.1). Aside form the normalization prob-
ability, we also use the common features used in
MT. These are the reverse normalization probabil-
ity, the lexical and reverse lexical probabilities and
the phrase penalty. We also use the MSD reorder-
ing model proposed in (Koehn et al, 2005), which
adds reordering features.7 The final score of each
phrase pair is given as a sum of weighted log fea-
tures. The weights for these features are optimized
using MERT (Och, 2003). In our work, we sampled
150 tweets randomly from Twitter and normalized
them manually, and used these samples as devel-
opment data for MERT. As for the character-based
model features, we simply rank the training phrase
pairs by their relative frequency the f(n | o), and use
the top-1000 phrase pairs as development set. Fi-
nally, a language model is required during decoding
as a prior, since it defines the type of language that
is produced by the output. We wish to normalized
to formal language, which is generally better pro-
cessed by NLP tools. Thus, for the phrase model,
we use the English NIST dataset composed of 8M
sentences in English from the news domain to build
a 5-gram Kneser-Ney smoothed language model.
5.2 Character and Phrasal Decoder
We now turn to how to apply the character-based
(?4.2), together with the phrasal model. For this
model, we again use Moses, treating each charac-
ter as a ?word?. The simplest way to combine both
methods is first to decode the input o sentence with
the character-based decoder, normalizing each word
independently and then normalizing the resulting
output using the phrase-based decoder, which en-
ables the phrase model to score the outputs of the
character model in context.
7Reordering helps find lexical variants that are generated by
transposing characters, such as, mabye to maybe.
78
0 1 2 3 4 5 6
I
wanna
want to meeeeet
meet
met
DanielVeuleman
Daniel Veuleman
Figure 2: Example output lattice of the character-based decoder, for the sentence I wanna meeeeet DanielVeuleman.
Our process is as follows. Given the input sen-
tence o, with the words o1, . . . , om, where m is
the number of words in the input, we generate for
each word oi a list of n-best normalization candi-
dates z1oi , . . . , z
n
oi . We further filter the candidates
using two criteria. We start by filtering each can-
didate zjoi that occurs less frequently than the orig-
inal word oi. This is motivated by our observation
that lexical variants occur far less than the respec-
tive standard form. Second, we build a corpus of
English language Twitter consisting of 70M tweets,
extract the unigram counts, and perform Brown clus-
tering (Brown et al, 1992) with k = 3000 clusters.
Next, we calculate the cluster similarity between oi
and each surviving candidate, zjoi . We filter the can-
didate if the similarity is less than 0.8. The similar-
ity between two clusters represented as bit strings,
S[c(oi), c(z
j
oi)], calculated as:
S(x, y) =
2 ? |lpm{x, y)}|
|x|+ |y|
,
where lpm computes the longest common prefix of
the contexts and |x| is the length of the bit string.8
If a candidate contains more than one word (because
a space was inserted), we set its count as the mini-
mum count among its words. To find the cluster for
multiple word units, we concatenate the words to-
gether, and find the cluster with the resulting word if
it exists. This is motivated by the fact that it is com-
mon for missing spaces to exist in microblog cor-
pora, generating new word forms, such as wantto,
goingfor, and given a large enough corpora as the
one we used, these errors occur frequently enough to
be placed in the correct cluster. In fact, the variants
such as wanna and tmi, occur in the same clusters as
the words wantto and toomuchinformation.
Remaining candidates are combined into a word
lattice, enabling us to perform lattice-based decod-
8Brown clusters are organized such that more words with
more similar distributions share common prefixes.
ing with the phrasal model (Dyer et al, 2008). Fig-
ure 2, provides an example of such a lattice for the
variant sentence I wanna meeeet DanielVeuleman.
5.3 Learning Variants from Monolingual Data
Until now, we learned normalizations from pairs of
original tweets and their normalizations. We shall
now describe a process to leverage monolingual doc-
uments to learn new normalizations, since the mono-
lingual data is far easier to obtain than parallel data.
This process is similar to the work in (Han et al,
2012), where confusion sets of contextually simi-
lar words are built initially as potential normaliza-
tion candidates. We again use the k = 3000 Brown
clusters,9 and this time consider the contents of each
cluster as a set of possible normalization variants.
For instance, we find that the cluster that includes the
word never, also includes the variant forms neverrrr,
neva and nevahhh. However, the cluster also con-
tains non-variant forms, such as gladly and glady.
Thus, we want to find that neverrrr maps to never,
while glady maps to gladly in the same cluster. Our
work differs from previous work in that, rather than
defining features manually, we use our character-
based decoder to find the mappings between lexical
variants and their normalizations.
For every word type wi in cluster c(wi) =
{w1, . . . , wn}, we generate a set of possible candi-
dates for each word w1i , . . . , w
m
i . Then, we build
a directed acyclic graph (DAG), where every word.
We add an edge between wi and wj , if wi can be
decoded into wj using the character model from the
previous section, and also if wi occurs less than wj ;
the second condition guarantees that the graph will
be acyclic. Sample graphs are shown in Figure 3.
Afterwards, we find the number of paths between
all nodes in the graph (this can be computed effi-
ciently in O(|V | + |E|) time). Then, for each word
9The Brown clustering algorithm groups words together
based on contextual similarity.
79
neverr
neva neve
nevar
never
glady
gladly
cladly
Figure 3: Example DAGs, built from the cluster contain-
ing the words never and gladly.
wi, we find the wj to which it has the highest num-
ber of paths to and extract the normalization of wi
to wj . In case of a tie, we choose the word wj that
occurs more often in the monolingual corpora. This
is motivated by the fact that normalizations are tran-
sitive. Thus, even if neva cannot be decoded directly
to never, we can use nevar as an intermediate step to
find the correct normalization. This is performed for
all the clusters, and the resulting dictionary of lexi-
cal variants mapped to their standard forms is added
to the training data of the character-based model.
6 Experiments
We evaluate our normalization model intrinsically
by testing whether our normalizations more closely
resemble standardized data, and then extrinsically
by testing whether we can improve the translation
quality of in-house as well as online Machine Trans-
lation systems by normalizing the input.
6.1 Setup
We use the gold standard by Ling et al (2013), com-
posed by 2581 English-Mandarin microblog sen-
tence pairs. From this set, we randomly select 1290
pairs for development and 1291 pairs for testing.
The normalizer model is trained on the corpora
extracted and filtered in section 3, in total, there
were 1.3M normalization pairs used during training.
The test sentences are normalized using four differ-
ent setups. The first setup leaves the input sentence
unchanged, which we call No Norm. The second
uses the phrase-based model to normalize the input
sentence, which we will denote Norm+phrase. The
third uses the character-based model to output lat-
tices, and then decodes with the phrase based model,
which we will denote Norm+phrase+char. Finally,
we test the same model after adding the training data
extracted using monolingual documents, which we
will refer as Norm+phrase+char+mono.
To test the normalizations themselves, we used
Google Translate to translate the Mandarin side of
the 1291 test sentence pairs back to English and use
the original English tweet. While, this is by itself
does not guarantee that the normalizations are cor-
rect, since the normalizations could be syntactically
and semantically incorrect, it will allow us to check
whether the normalizations are closer to those pro-
duced by systems trained on news data. This exper-
iment will be called Norm.
As an application and extrinsic evaluation for our
normalizer, we test if we can obtain gains on the
MT task on microblog data by using our normalizer
prior to translation. We build two MT systems us-
ing Moses. Firstly, we build a out-of-domain model
using the full 2012 NIST Chinese-English dataset
(approximately 8M sentence pairs), which is dataset
from the news domain, and we will denote this sys-
tem as Inhouse+News. Secondly, we build a in-
domain model using the 800K sentence pairs from
?topia corpora (Ling et al, 2013). We also add
the NIST dataset to improve coverage. We call this
system Inhouse+News+Weibo. To train these sys-
tems, we use the Moses phrase-based MT system
with standard features (Koehn et al, 2003). For re-
ordering, we use the MSD reordering model (Axel-
rod et al, 2005). As the language model, we train
a 5-gram model with Kneser-ney smoothing using a
10M tweets from twitter. Finally, the weights were
tuned using MERT (Och, 2003). As for online sys-
tems, we consider the systems used to generate the
paraphrase corpora in section 3, which we will de-
note as Online A, Online B and Online C10
The normalization and MT results are evaluated
with BLEU-4 (Papineni et al, 2002) comparing the
produced translations or normalizations with the ap-
propriate reference.
6.2 Results
Results are shown in Table 6. In terms of the normal-
izations, we observe a much better match between
10The names of the systems are hidden to not violate the pri-
vacy issues in the terms and conditions of these online systems.
80
Table 6: Normalization and MT Results. Rows denote different normalizations, and columns different translation
systems, except the first column (Norm), which denotes the normalization experiment. Cells display the BLEU score
of that experiment.
Moses Moses
Condition Norm (News) (News+Weibo) Online A Online B Online C
baseline 19.90 15.10 24.37 20.09 17.89 18.79
norm+phrase 21.96 15.69 24.29 20.50 18.13 18.93
norm+phrase+char 22.39 15.87 24.40 20.61 18.22 19.08
norm+phrase+char+mono 22.91 15.94 24.46 20.78 18.37 19.21
the normalized text with the reference, than the orig-
inal tweets. In most cases, adding character-based
models improves the quality of the normalizations.
We observe that better normalizations tend to lead
to better translations. The relative improvements
are most significant, when moving from No Norm
to norm+phrase normalization. This is because,
we are normalizing words that are not seen in gen-
eral MT system?s training data, but occur frequently
in microblog data, such as wanna to want to, u to
you and im to i?m. The only exception is in the In-
house+News+Weibo system, where the normaliza-
tion deteriorates the results. This is to be expected,
since this system is trained on the same microblog
data used to learn the normalizations. However, we
can observe on norm+phrase+char that if we add
the character-based model, we can observe improve-
ments for this system as well as for all other ones.
This is because the model is actually learning nor-
malizations that are unseen in the data. Some ex-
amples of these normalization include, normalizing
lookin to looking, nutz to nuts and maimi to miami
but also separating peaceof to peace of. The fact
that these improvements are obtained for all sys-
tems is strong evidence that we are actually produc-
ing good normalizations, and not overfitting to one
of the systems that we used to generate our data.
The gains are much smaller from norm+phrase
to norm+phrase+char, since the improvements we
obtain come from normalizing less frequent words.
Finally, we can obtain another small improvement
by adding monolingual data to the character-based
model in norm+phrase+char+mono.
7 Related Work
Most of the work in microblog normalization is fo-
cused on finding the standard forms of lexical vari-
ants (Yang and Eisenstein, 2013; Han et al, 2013;
Han et al, 2012; Kaufmann, 2010; Han and Bald-
win, 2011; Gouws et al, 2011; Aw et al, 2006). A
lexical variant is a variation of a standard word in
a different lexical form. This ranges from minor or
major spelling errors, such as jst, juxt and jus that
are lexical variants of just, to abbreviations, such as
tmi and wanna, which stand for too much informa-
tion and want to, respectively. Jargon can also be
treated as variants, for instance cday is a slang word
for birthday, in some groups.
There are many rules that govern the process lex-
ical variants are generated. Some variants are gener-
ated from orthographic errors, caused by some mis-
take from the user when writing. For instance, the
variants representin, representting, or reprecenting
can be generated by a spurious letter swap, insertion
or substitution by the user. One way to normalize
these types of errors is to attempt to insert, remove
and swap words in a lexical variant until a word in
a dictionary of standard words is found (Kaufmann,
2010). Contextual features are another way to find
lexical variants, since variants generally occur in the
same context as their standard form. This includes
orthographic errors, abbreviations and slang. How-
ever, this is generally not enough to detect lexical
variants, as many words share similar contexts, such
as already, recently and normally. Consequently,
contextual features are generally used to generate a
confusion set of possible normalizations of a lexical
variant, and then more features are used to find the
correct normalization (Han et al, 2012). One simple
approach is to compute the Levenshtein distance to
find lexical similarities between words, which would
effectively capture the mappings between represent-
ting, reprecenting and representin to representing.
However, a pronunciation model (Tang et al, 2012)
81
would be needed to find the mapping between g8,
2day and 4ever to great, today and forever, respec-
tively. Moreover, visual character similarity features
would be required to find the mapping between g00d
and? to good and i.
Clearly, learning this process is a challenging
task, and addressing each different case individually
would require vast amounts of resources. Further-
more, once we change the language to normalize
to another language, the types of rules that generate
lexical variants would radically change and a new set
of features would have to be engineered. We believe
that to be successful in normalizing microblogs,
the process to learn new lexical variants should be
learned from data, making as few assumptions as
possible. We learn our models without using any
type of predefined features, such as phonetic fea-
tures or lexical features. In fact, we will not assume
that most words and characters map to themselves,
as it is assumed in methods using the Levenshtein
distance (Kaufmann, 2010; Han et al, 2012; Wang
and Ng, 2013). All these mappings are learned from
our data. Furthermore, in the work above, the dictio-
naries built using these methods assume that lexical
variants are mapped to standard forms in a word-to-
word mapping. Thus, variants such as wanna, gonna
and imma are not normalizable, since they are nor-
malized to multiple words want to, going to and I
am gonna. Moreover, there are segmentation errors
that occur from missing spaces, such as sortof and
goingfor, which also map to more than one word to
sort of and going for. These cases shall also be ad-
dressed in our work.
Wang and Ng (2013) argue that microblog nor-
malization is not simply to map lexical variants into
standard forms, but that other tasks, such as punctua-
tion correction and missing word recovery should be
performed. Consider the example tweet you free?,
while there are no lexical variants in this message,
the authors consider that it is the normalizer should
recover the missing article are and normalize this
tweet to are you free?. To do this, the authors train a
series of models to detect and correct specific errors.
While effective for narrow domains, training models
to address each specific type of normalization is not
scalable over all types of normalizations that need to
be performed within the language, and the fact that a
set of new models must be implemented for another
language limits the applicability of this work.
Another strong point of the work above is that
a decoder is presented, while the work on build-
ing dictionaries only normalize out of vocabu-
lary (OOV) words. The work on (Han et al, 2012)
trains a classifier to decide whether to normalize a
word or not, but is still preconditioned on the fact
that the word in question is OOV. Thus, lexical vari-
ants, such as, 4 and u, with the standard forms for
and you, are left untreated, since they occur in other
contexts, such as u in u s a. Inspired by the work
above, we also propose a decoder based on the exist-
ing off-the-self decoder Moses (Koehn et al, 2007).
Finally, the work in (Xu et al, 2013) obtains para-
phrases from Twitter, by finding tweets that contain
common entities, such as Obama, that occur during
the same period by matching temporal expressions.
The resulting paraphrase corpora can also be used to
train a normalizer.
8 Conclusion
We introduced a data-driven approach to microblog
normalization based on paraphrasing. We build a
corpora of tweets and their normalizations using par-
allel corpora from microblogs using MT techniques.
Then, we build two models that learn generalizations
of the normalization process, one the phrase level
and on the character level. Then, we build a de-
coder that combines both models during decoding.
Improvements on multiple MT systems support the
validity of our method.
In future work, we shall attempt to build normal-
izations for other languages. We shall also attempt
to learn an unsupervised normalization model with
only monolingual data, similar to the work for MT
in (Ravi and Knight, 2011).
Acknowledgements
The PhD thesis of Wang Ling is supported by FCT ?
Fundac?a?o para a Cie?ncia e a Tecnologia, under project
SFRH/BD/51157/2010. This work was supported by na-
tional funds through FCT ? Fundac?a?o para a Cie?ncia e a
Tecnologia, under project PEst-OE/EEI/LA0021/2013.
The authors also wish to express their gratitude to the
anonymous reviewers for their comments and insight.
82
References
[Aw et al2006] AiTi Aw, Min Zhang, Juan Xiao, and
Jian Su. 2006. A phrase-based statistical model for
SMS text normalization. In Proceedings of the ACL,
COLING-ACL ?06, pages 33?40, Stroudsburg, PA,
USA. Association for Computational Linguistics.
[Axelrod et al2005] Amittai Axelrod, Ra Birch Mayne,
Chris Callison-burch, Miles Osborne, and David Tal-
bot. 2005. Edinburgh system description for the 2005
iwslt speech translation evaluation. In In Proc. Inter-
national Workshop on Spoken Language Translation
(IWSLT.
[Bannard and Callison-Burch2005] Colin Bannard and
Chris Callison-Burch. 2005. Paraphrasing with bilin-
gual parallel corpora. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 597?604, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.
[Bansal et al2011] Mohit Bansal, Chris Quirk, and
Robert C. Moore. 2011. Gappy phrasal alignment by
agreement. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies - Volume 1, HLT ?11,
pages 1308?1317, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Brown et al1992] Peter F Brown, Peter V Desouza,
Robert L Mercer, Vincent J Della Pietra, and Jenifer C
Lai. 1992. Class-based n-gram models of natural lan-
guage. Computational linguistics, 18(4):467?479.
[Brown et al1993] Peter F. Brown, Vincent J. Della
Pietra, Stephen A. Della Pietra, and Robert L. Mer-
cer. 1993. The mathematics of statistical machine
translation: parameter estimation. Comput. Linguist.,
19:263?311, June.
[Denkowski and Lavie2011] Michael Denkowski and
Alon Lavie. 2011. Meteor 1.3: Automatic metric
for reliable optimization and evaluation of machine
translation systems. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
85?91, Edinburgh, Scotland, July. Association for
Computational Linguistics.
[Dyer et al2008] Chris Dyer, Smaranda Muresan, and
Philip Resnik. 2008. Generalizing word lattice trans-
lation. In Proceedings of HLT-ACL.
[Dyer et al2013] Chris Dyer, Victor Chahuneau, and
Noah A Smith. 2013. A simple, fast, and effective
reparameterization of ibm model 2. In Proceedings of
NAACL-HLT, pages 644?648.
[Eisenstein et al2011] Jacob Eisenstein, Noah A. Smith,
and Eric P. Xing. 2011. Discovering sociolinguis-
tic associations with structured sparsity. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies - Volume 1, HLT ?11, pages 1365?1374,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
[Eisenstein2013] Jacob Eisenstein. 2013. What to do
about bad language on the internet. In Proceedings
of NAACL-HLT, pages 359?369.
[Gouws et al2011] Stephan Gouws, Dirk Hovy, and Don-
ald Metzler. 2011. Unsupervised mining of lexical
variants from noisy text. In Proceedings of the First
Workshop on Unsupervised Learning in NLP, EMNLP
?11, pages 82?90, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Han and Baldwin2011] Bo Han and Timothy Baldwin.
2011. Lexical normalisation of short text messages:
makn sens a #twitter. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume
1, HLT ?11, pages 368?378, Stroudsburg, PA, USA.
Association for Computational Linguistics.
[Han et al2012] Bo Han, Paul Cook, and Timothy Bald-
win. 2012. Automatically constructing a normalisa-
tion dictionary for microblogs. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, EMNLP-CoNLL ?12, pages 421?
432, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
[Han et al2013] Bo Han, Paul Cook, and Timothy Bald-
win. 2013. Lexical normalization for social media
text. ACM Transactions on Intelligent Systems and
Technology (TIST), 4(1):5.
[Hawn2009] Carleen Hawn. 2009. Take two aspirin and
tweet me in the morning: how twitter, facebook, and
other social media are reshaping health care. Health
affairs, 28(2):361?368.
[Kaufmann2010] M. Kaufmann. 2010. Syntactic Nor-
malization of Twitter Messages. studies, 2.
[Koehn et al2003] Philipp Koehn, Franz Josef Och, and
Daniel Marcu. 2003. Statistical phrase-based trans-
lation. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy - Volume 1, NAACL ?03, pages 48?54, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
[Koehn et al2005] Philipp Koehn, Amittai Axelrod,
Alexandra Birch Mayne, Chris Callison-Burch, Miles
Osborne, David Talbot, and Michael White. 2005.
Edinburgh system description for the 2005 nist mt
evaluation. In Proceedings of Machine Translation
Evaluation Workshop 2005.
[Koehn et al2007] Philipp Koehn, Hieu Hoang, Alexan-
dra Birch, Chris Callison-burch, Richard Zens, Rwth
83
Aachen, Alexandra Constantin, Marcello Federico,
Nicola Bertoldi, Chris Dyer, Brooke Cowan, Wade
Shen, Christine Moran, and Ondrej Bojar. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
[Laviosa1998] Sara Laviosa. 1998. Core patterns of
lexical use in a comparable corpus of English lexical
prose. Meta, 43(4):557?570.
[Ling et al2010] Wang Ling, Tiago Lu??s, Joa?o Grac?a,
Lu??sa Coheur, and Isabel Trancoso. 2010. Towards a
general and extensible phrase-extraction algorithm. In
IWSLT ?10: International Workshop on Spoken Lan-
guage Translation, pages 313?320, Paris, France.
[Ling et al2013] Wang Ling, Guang Xiang, Chris Dyer,
Alan Black, and Isabel Trancoso. 2013. Microblogs
as parallel corpora. In Proceedings of the 51st An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?13. Association for Computational Lin-
guistics.
[Och and Ney2003] Franz Josef Och and Hermann Ney.
2003. A systematic comparison of various statis-
tical alignment models. Computational linguistics,
29(1):19?51.
[Och and Ney2004] Franz Josef Och and Hermann Ney.
2004. The alignment template approach to statistical
machine translation. Comput. Linguist., 30(4):417?
449, December.
[Och2003] Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics - Volume 1, ACL ?03,
pages 160?167, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Papineni et al2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
method for automatic evaluation of machine transla-
tion. In Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics, ACL ?02,
pages 311?318, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Ravi and Knight2011] Sujith Ravi and Kevin Knight.
2011. Deciphering foreign language. In ACL, pages
12?21.
[Resnik and Smith2003] Philip Resnik and Noah A
Smith. 2003. The web as a parallel corpus. Com-
putational Linguistics, 29(3):349?380.
[Tang et al2012] Hao Tang, Joseph Keshet, and Karen
Livescu. 2012. Discriminative pronunciation mod-
eling: A large-margin, feature-rich approach. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers-Volume
1, pages 194?203. Association for Computational Lin-
guistics.
[Vogel et al1996] S. Vogel, H. Ney, and C. Tillmann.
1996. Hmm-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational linguistics-Volume 2, pages 836?841. Asso-
ciation for Computational Linguistics.
[Volansky et al2013] Vered Volansky, Noam Ordan, and
Shuly Wintner. 2013. On the features of transla-
tionese. Literary and Linguistic Computing.
[Wang and Ng2013] Pidong Wang and Hwee Ng. 2013.
A beam-search decoder for normalization of social
media text with application to machine translation. In
Proceedings of NAACL-HLT 2013, NAACL ?13. As-
sociation for Computational Linguistics.
[Xu et al2013] Wei Xu, Alan Ritter, and Ralph Grish-
man. 2013. Gathering and generating paraphrases
from twitter with application to normalization. In Pro-
ceedings of the Sixth Workshop on Building and Us-
ing Comparable Corpora, pages 121?128, Sofia, Bul-
garia, August. Association for Computational Linguis-
tics.
[Yang and Eisenstein2013] Yi Yang and Jacob Eisenstein.
2013. A log-linear model for unsupervised text nor-
malization. In Proc. of EMNLP.
84
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 450?454,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Reordering Modeling using Weighted Alignment Matrices
Wang Ling, Tiago Lu??s, Joa?o Grac?a, Lu??sa Coheur and Isabel Trancoso
L2F Spoken Systems Lab
INESC-ID Lisboa
{wang.ling,tiago.luis,joao.graca}@inesc-id.pt
{luisa.coheur,isabel.trancoso}@inesc-id.pt
Abstract
In most statistical machine translation sys-
tems, the phrase/rule extraction algorithm uses
alignments in the 1-best form, which might
contain spurious alignment points. The usage
of weighted alignment matrices that encode all
possible alignments has been shown to gener-
ate better phrase tables for phrase-based sys-
tems. We propose two algorithms to generate
the well known MSD reordering model using
weighted alignment matrices. Experiments on
the IWSLT 2010 evaluation datasets for two
language pairs with different alignment algo-
rithms show that our methods produce more
accurate reordering models, as can be shown
by an increase over the regular MSD models
of 0.4 BLEU points in the BTEC French to
English test set, and of 1.5 BLEU points in the
DIALOG Chinese to English test set.
1 Introduction
The translation quality of statistical phrase-based
systems (Koehn et al, 2003) is heavily dependent
on the quality of the translation and reordering mod-
els generated during the phrase extraction algo-
rithm (Ling et al, 2010). The basic phrase extrac-
tion algorithm uses word alignment information to
constraint the possible phrases that can be extracted.
It has been shown that better alignment quality gen-
erally leads to better results (Ganchev et al, 2008).
However the relationship between the word align-
ment quality and the results is not straightforward,
and it was shown in (Vilar et al, 2006) that better
alignments in terms of F-measure do not always lead
to better translation quality.
The fact that spurious word alignments might oc-
cur leads to the use of alternative representations for
word alignments that allow multiple alignment hy-
potheses, rather than the 1-best alignment (Venu-
gopal et al, 2009; Mi et al, 2008; Christopher
Dyer et al, 2008). While using n-best alignments
yields improvements over using the 1-best align-
ment, these methods are computationally expen-
sive. More recently, the method described in (Liu
et al, 2009) produces improvements over the meth-
ods above, while reducing the computational cost
by using weighted alignment matrices to represent
the alignment distribution over each parallel sen-
tence. However, their results were limited by the
fact that they had no method for extracting a reorder-
ing model from these matrices, and used a simple
distance-based model.
In this paper, we propose two methods for gener-
ating the MSD (Mono Swap Discontinuous) reorder-
ing model from the weighted alignment matrices.
First, we test a simple approach by using the 1-best
alignment to generate the reordering model, while
using the alignment matrix to produce the translation
model. This reordering model is a simple adaptation
of the MSD model to read from alignment matrices.
Secondly, we develop two algorithms to infer the re-
ordering model from the weighted alignment matrix
probabilities. The first one uses the alignment infor-
mation within phrase pairs, while the second uses
contextual information of the phrase pairs.
This paper is organized as follows: Section 2 de-
scribes the MSD model; Section 3 presents our two
algorithms; in Section 4 we report the results from
the experiments conducted using these algorithms,
450
and comment on the results; we conclude in Sec-
tion 5.
2 MSD models
Moses (Koehn et al, 2007) allows many config-
urations for the reordering model to be used. In
this work, we will only refer to the default config-
uration (msd-bidirectional-fe), which uses the MSD
model, and calculates the reordering orientation for
the previous and the next word, for each phrase pair.
Other possible configurations are simpler than the
default one. For instance, the monotonicity model
only considers monotone and non-monotone orien-
tation types, whereas the MSD model also considers
the monotone orientation type, but distinguishes the
non-monotone orientation type between swap and
discontinuous. The approach presented in this work
can be adapted to the other configurations.
In the MSD model, during the phrase extraction,
given a source sentence S and a target sentence T ,
the alignment set A, where aji is an alignment from i
to j, the phrase pair with words in positions between
i and j in S, Sji , and n and m in T , T
m
n , can be
classified with one of three orientations with respect
to the previous word:
? The orientation is monotonous if only the pre-
vious word in the source is aligned with the pre-
vious word in the target, or, more formally, if
an?1i?1 ? A ? a
n?1
j+1 /? A.
? The orientation is swap, if only the next word
in the source is aligned with the previous word
in the target, or more formally, if an?1j+1 ? A ?
an?1i?1 /? A.
? The orientation is discontinuous if neither of
the above are true, which means, (an?1i?1 ?
A ? an?1j+1 ? A) ? (a
n?1
i?1 /? A ? a
n?1
j+1 /? A).
The orientations with respect to the next word are
given analogously. The reordering model is gener-
ated by grouping the phrase pairs that are equal, and
calculating the probabilities of the grouped phrase
pair being associated each orientation type and di-
rection, based on the orientations for each direction
that are extracted. Formally, the probability of the
phrase pair p having a monotonous orientation is
prev 
word(s)
source phrase
target phrase
prev 
word(t)
next 
word(s)
source phrase
target phrase
prev 
word(t)
a) b)
c)
source phrase
target phrase
prev 
word(t)
d)
next 
word(s)
source phrase
target phrase
prev 
word(t)
prev 
word(s)
Figure 1: Enumeration of possible reordering cases with
respect to the previous word. Case a) is classified as
monotonous, case b) is classified as swap and cases c)
and d) are classified as discontinuous.
given by:
P (p,mono) = C(mono)C(mono)+C(swap)+C(disc) (1)
Where C(o) is the number of times a phrase is ex-
tracted with the orientation o in that group of phrase
pairs. Moses also provides many options for this
stage, such as types of smoothing. We use the de-
fault smoothing configuration which adds the fixed
value of 0.5 to all C(o).
3 Weighted MSD Model
When using a weighted alignment matrix, rather
than working with alignments points, we use the
probability of each word in the source aligning with
each word in the target. Thus, the regular MSD
model cannot be directly applied here.
One obvious solution to solve this problem is to
produce a 1-best alignment set alng with the align-
ment matrix, and use the 1-best alignment to gen-
erate the reordering model, while using the align-
ment matrix to produce the translation model. How-
ever, this method would not be taking advantage of
the weighted alignment matrix. The following sub-
sections describe two algorithms that are proposed
to make use of the alignment probabilities.
3.1 Score-based
Each phrase pair that is extracted using the algorithm
described in (Liu et al, 2009) is given a score based
on its alignments. This score is higher if the align-
ment points in the phrase pair have high probabili-
ties, and if the alignment is consistent. Thus, if an
451
extracted phrase pair has better quality, its orienta-
tion should have more weight than phrase pairs with
worse quality. We implement this by changing the
C(o) function in equation 1 from being the number
of the phrase pairs with the orientation o, to the sum
of the scores of those phrases. We also need to nor-
malize the scores for each group, due to the fixed
smoothing that is applied, since if the sum of the
scores is much lower (e.g. 0.1) than the smoothing
factor (0.5), the latter will overshadow the weight
of the phrase pairs. The normalization is done by
setting the phrase pair with the highest value of the
sum of all MSD probabilities to 1, and readjusting
other phrase pairs accordingly. Thus, a group of 3
phrase pairs that have the MSD probability sums of
0.1, 0.05 and 0.1, are all set to 1, 0.5 and 1.
3.2 Context-based
We propose an alternative algorithm to calculate
the reordering orientations for each phrase pair.
Rather than classifying each phrase pair with either
monotonous (M ), swap (S) or discontinuous (D),
we calculate the probability for each orientation, and
use these as weighted counts when creating the re-
ordering model. Thus, for the previous word, given
a weighted alignment matrix W , the phrase pair be-
tween the indexes i and j in S, Sji , and n and m in
T , Tmn , the probability values for each orientation
are given by:
? Pc(M) = W
n?1
i?1 ? (1?W
n?1
j+1 )
? Pc(S) = W
n?1
j+1 ? (1?W
n?1
i?1 )
? Pc(D) = W
n?1
i?1 ?W
n?1
j+1
+ (1?Wn?1i?1 )? (1?W
n?1
j+1 )
These formulas derive from the adaptation of con-
ditions of each orientation presented in 2. In the
regular MSD model, the previous orientation for a
phrase pair is monotonous if the previous word in
the source phrase is aligned with the previous word
in the target phrase and not aligned with the next
word. Thus, the probability of a phrase pair to have a
monotonous orientation Pc(M) is given by the prob-
ability of the previous word in the source phrase
being aligned with the previous word in the target
phrase Wn?1i?1 , and the probability of the previous
word in the source to not be aligned with the next
word in the target (1 ? Wn?1j+1 ). Also, the sum of
the probabilities of all orientations (Pc(M), Pc(S),
Pc(D)) for a given phrase pair can be trivially shown
to be 1. The probabilities for the next word are
given analogously. Following equation 1, the func-
tion C(o) is changed to be the sum of all Pc(o), from
the grouped phrase pairs.
4 Experiments
4.1 Corpus
Our experiments were performed over two datasets,
the BTEC and the DIALOG parallel corpora from
the latest IWSLT evaluation 2010 (Paul et al, 2010).
BTEC is a multilingual speech corpus that contains
sentences related to tourism, such as the ones found
in phrasebooks. DIALOG is a collection of human-
mediated cross-lingual dialogs in travel situations.
The experiments performed with the BTEC cor-
pus used only the French-English subset, while the
ones perfomed with the DIALOG corpus used the
Chinese-English subset. The training corpora con-
tains about 19K sentences and 30K sentences, re-
spectively. The development corpus for the BTEC
task was the CSTAR03 test set composed by 506
sentences, and the test set was the IWSLT04 test set
composed by 500 sentences and 16 references. As
for the DIALOG task, the development set was the
IWSLT09 devset composed by 200 sentences, and
the test set was the CSTAR03 test set with 506 sen-
tences and 16 references.
4.2 Setup
We use weighted alignment matrices based on Hid-
den Markov Models (HMMs), which are produced
by the the PostCAT toolkit1, based on the poste-
rior regularization framework (V. Grac?a et al, 2010).
The extraction algorithm using weighted alignment
matrices employs the same method described in (Liu
et al, 2009), and the phrase pruning threshold was
set to 0.1. For the reordering model, we use the
distance-based reordering, and compare the results
with the MSD model using the 1-best alignment.
Then, we apply our two methods based on align-
ment matrices. Finally, we combine our two meth-
ods above by adapting the function C(o), to be the
1http://www.seas.upenn.edu/ strctlrn/CAT/CAT.html
452
sum of all Pc(o), weighted by the scores of the re-
spective phrase pairs. The optimization of the trans-
lation model weights was done using MERT, and
each experiment was run 5 times, and the final score
is calculated as the average of the 5 runs, in order to
stabilize the results. Finally, the results were eval-
uated using BLEU-4, METEOR, TER and TERp.
The BLEU-4 and METEOR scores were computed
using 16 references. The TER and TERp were com-
puted using a single reference.
4.3 Reordering model comparison
Tables 1 and 2 show the scores using the differ-
ent reordering models. Consistent improvements in
the BLEU scores may be observed when changing
from the MSD model to the models generated us-
ing alignment matrices. The results were consis-
tently better using our models in the DIALOG task,
since the English-Chinese language pair is more de-
pendent on the reordering model. This is evident
if we look at the difference in the scores between
the distance-based and the MSD models. Further-
more, in this task, we observe an improvement on all
scores from the MSD model to our weighted MSD
models, which suggests that the usage of alignment
matrices helps predict the reordering probabilities
more accurately.
We can also see that the context based reordering
model performs better than the score based model
in the BTEC task, which does not perform sig-
nificantly better than the regular MSD model in
this task. Furthermore, combining the score based
method with the context based method does not lead
to any improvements. We believe this is because the
alignment probabilities are much more accurate in
the English-French language pair, and phrase pair
scores remain consistent throughout the extraction,
making the score based approach and the regular
MSD model behave similarly. On the other hand,
in the DIALOG task, score based model has bet-
ter performance than the regular MSD model, and
the combination of both methods yields a significant
improvement over each method alone.
Table 3 shows a case where the context based
model is more accurate than the regular MSD model.
The alignment is obviously faulty, since the word
?two? is aligned with both ?deux?, although it
should only be aligned with the first occurrence.
BTEC BLEU METEOR TERp TER
Distance-based 61.84 65.38 27.60 22.40
MSD 62.02 65.93 27.40 22.80
score MSD 62.15 66.18 27.30 22.20
context MSD 62.42 66.29 27.00 22.00
combined MSD 62.42 66.14 27.10 22.20
Table 1: Results for the BTEC task.
DIALOG BLEU METEOR TERp TER
Distance-based 36.29 45.15 49.00 41.20
MSD 39.56 46.85 47.20 39.60
score MSD 40.2 47.16 46.52 38.80
context MSD 40.14 47.14 45.88 39.00
combined MSD 41.03 47.69 46.20 38.20
Table 2: Results for the DIALOG task.
Furthermore, the word ?twin? should be aligned
with ?a` deux lit?, but it is aligned with ?cham-
bres?. If we use the 1-best alignment to compute
the reordering type of the sentence pair ?Je voudrais
re?server deux? / ?I?d like to reserve two?, the re-
ordering type for the following orientation would
be monotonous, since the next word ?chambres?
is falsely aligned with ?twin?. However, it should
clearly be discontinuous, since the right alignment
for ?twin? is ?a` deux lit?. This problem is less seri-
ous when we use the weighted MSD model, since
the orientation probability mass would be divided
between monotonous and discontinuous since the
probability weighted matrix for the wrong alignment
is 0.5. On the BTEC task, some of the other scores
are lower than the MSD model, and we suspect that
this stems from the fact that our tuning process only
attempts to maximize the BLEU score.
5 Conclusions
In this paper we addressed the limitations of the
MSD reordering models extracted from the 1-best
alignments, and presented two algorithms to ex-
tract these models from weighted alignment matri-
ces. Experiments show that our models perform bet-
ter than the distance-based model and the regular
MSD model. The method based on scores showed a
good performance for the Chinese-English language
pair, but the performance for the English-French pair
was similar to the MSD model. On the other hand,
the method based on context improves the results on
453
Alignment Je vo
ud
ra
is
re?
se
rv
er
de
ux
ch
am
br
es
a` de
ux
lit
s
.
I 1
?d 0.7
like 0.7
to
reserve 1
two 1 0.5
twin 0.5 0.5
rooms 1
. 1
Table 3: Weighted alignment matrix for a training sen-
tence pair from BTEC, with spurious alignment proba-
bilities. Alignment points with 0 probabilities are left
empty.
both pairs. Finally, on the Chinese-English test, by
combining both methods we can achieve a BLEU
improvement of approximately 1.5%. The code used
in this work is currently integrated with the Geppetto
toolkit2 , and it will be made available in the next
version for public use.
6 Acknowledgements
This work was partially supported by FCT (INESC-
ID multiannual funding) through the PIDDAC Pro-
gram funds, and also through projects CMU-
PT/HuMach/0039/2008 and CMU-PT/0005/2007.
The PhD thesis of Tiago Lu??s is supported by
FCT grant SFRH/BD/62151/2009. The PhD the-
sis of Wang Ling is supported by FCT grant
SFRH/BD/51157/2010. The authors also wish to
thank the anonymous reviewers for many helpful
comments.
References
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing Word Lattice Translation. Tech-
nical Report LAMP-TR-149, University of Maryland,
College Park, February.
Kuzman Ganchev, Joa?o V. Grac?a, and Ben Taskar. 2008.
Better alignments = better translations? In Proceed-
ings of ACL-08: HLT, pages 986?993, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
2http://code.google.com/p/geppetto/
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 48?54, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-burch, Richard Zens, Rwth Aachen, Alexan-
dra Constantin, Marcello Federico, Nicola Bertoldi,
Chris Dyer, Brooke Cowan, Wade Shen, Christine
Moran, and Ondrej Bojar. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Wang Ling, Tiago Lu??s, Joao Grac?a, Lu??sa Coheur, and
Isabel Trancoso. 2010. Towards a general and ex-
tensible phrase-extraction algorithm. In IWSLT ?10:
International Workshop on Spoken Language Transla-
tion, pages 313?320, Paris, France.
Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 2 - Volume 2, EMNLP ?09, pages 1017?1026,
Morristown, NJ, USA. Association for Computational
Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT,
pages 192?199, Columbus, Ohio, June. Association
for Computational Linguistics.
Michael Paul, Marcello Federico, and Sebastian Stu?ker.
2010. Overview of the iwslt 2010 evaluation cam-
paign. In IWSLT ?10: International Workshop on Spo-
ken Language Translation, pages 3?27.
Joa?o V. Grac?a, Kuzman Ganchev, and Ben Taskar. 2010.
Learning Tractable Word Alignment Models with
Complex Constraints. Comput. Linguist., 36:481?504.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Wider pipelines: N-best
alignments and parses in MT training.
David Vilar, Maja Popovic, and Hermann Ney. 2006.
Aer: Do we need to ?improve? our alignments? In
International Workshop on Spoken Language Transla-
tion (IWSLT), pages 205?212.
454
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 176?186,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Microblogs as Parallel Corpora
Wang Ling123 Guang Xiang2 Chris Dyer2 Alan Black2 Isabel Trancoso 13
(1)L2F Spoken Systems Lab, INESC-ID, Lisbon, Portugal
(2)Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA
(3)Instituto Superior Te?cnico, Lisbon, Portugal
{lingwang,guangx,cdyer,awb}@cs.cmu.edu
isabel.trancoso@inesc-id.pt
Abstract
In the ever-expanding sea of microblog data, there
is a surprising amount of naturally occurring par-
allel text: some users create post multilingual mes-
sages targeting international audiences while oth-
ers ?retweet? translations. We present an efficient
method for detecting these messages and extract-
ing parallel segments from them. We have been
able to extract over 1M Chinese-English parallel
segments from Sina Weibo (the Chinese counter-
part of Twitter) using only their public APIs. As a
supplement to existing parallel training data, our
automatically extracted parallel data yields sub-
stantial translation quality improvements in trans-
lating microblog text and modest improvements
in translating edited news commentary. The re-
sources in described in this paper are available at
http://www.cs.cmu.edu/?lingwang/utopia.
1 Introduction
Microblogs such as Twitter and Facebook have
gained tremendous popularity in the past 10 years.
In addition to being an important form of commu-
nication for many people, they often contain ex-
tremely current, even breaking, information about
world events. However, the writing style of mi-
croblogs tends to be quite colloquial, with fre-
quent orthographic innovation (R U still with me
or what?) and nonstandard abbreviations (idk!
shm)?quite unlike the style found in more tra-
ditional, edited genres. This poses considerable
problems for traditional NLP tools, which were
developed with other domains in mind, which of-
ten make strong assumptions about orthographic
uniformity (i.e., there is just one way to spell you).
One approach to cope with this problem is to an-
notate in-domain data (Gimpel et al, 2011).
Machine translation suffers acutely from the
domain-mismatch problem caused by microblog
text. On one hand, standard models are probably
suboptimal since they (like many models) assume
orthographic uniformity in the input. However,
more acutely, the data used to develop these sys-
tems and train their models is drawn from formal
and carefully edited domains, such as parallel web
pages and translated legal documents. MT training
data seldom looks anything like microblog text.
This paper introduces a method for finding nat-
urally occurring parallel microblog text, which
helps address the domain-mismatch problem.
Our method is inspired by the perhaps surpris-
ing observation that a reasonable number of mi-
croblog users tweet ?in parallel? in two or more
languages. For instance, the American entertainer
Snoop Dogg regularly posts parallel messages on
Sina Weibo (Mainland China?s equivalent of Twit-
ter), for example, watup Kenny Mayne!! - Kenny
Mayne?????????, where an English
message and its Chinese translation are in the
same post, separated by a dash. Our method is able
to identify and extract such translations. Briefly,
this requires determining if a tweet contains more
than one language, if these multilingual utterances
contain translated material (or are due to some-
thing else, such as code switching), and what the
translated spans are.
The paper is organized as follows. Section 2
describes the related work in parallel data extrac-
tion. Section 3 presents our model to extract par-
allel data within the same document. Section 4
describes our extraction pipeline. Section 5 de-
scribes the data we gathered from both Sina Weibo
(Chinese-English) and Twitter (Chinese-English
and Arabic-English). We then present experiments
showing that our harvested data not only substan-
tially improves translations of microblog text with
176
existing (and arguably inappropriate) translation
models, but that it improves the translation of
more traditional MT genres, like newswire. We
conclude in Section 6.
2 Related Work
Automatic collection of parallel data is a well-
studied problem. Approaches to finding par-
allel web documents automatically have been
particularly important (Resnik and Smith, 2003;
Fukushima et al, 2006; Li and Liu, 2008; Uszko-
reit et al, 2010; Ture and Lin, 2012). These
broadly work by identifying promising candidates
using simple features, such as URL similarity or
?gist translations? and then identifying truly par-
allel segments with more expensive classifiers.
More specialized resources were developed using
manual procedures to leverage special features of
very large collections, such as Europarl (Koehn,
2005).
Mining parallel or comparable messages from
microblogs has mainly relied on Cross-Lingual In-
formation Retrieval techniques (CLIR). Jelh et al
(2012) attempt to find pairs of tweets in Twitter us-
ing Arabic tweets as search queries in a CLIR sys-
tem. Afterwards, the model described in (Xu et al,
2001) is applied to retrieve a set of ranked trans-
lation candidates for each Arabic tweet, which are
then used as parallel candidates.
The work on mining parenthetical transla-
tions (Lin et al, 2008), which attempts to find
translations within the same document, has some
similarities with our work, since parenthetical
translations are within the same document. How-
ever, parenthetical translations are generally used
to translate names or terms, which is more lim-
ited than our work which extracts whole sentence
translations.
Finally, crowd-sourcing techniques to obtain
translations have been previously studied and ap-
plied to build datasets for casual domains (Zbib
et al, 2012; Post et al, 2012). These approaches
require remunerated workers to translate the mes-
sages, and the amount of messages translated per
day is limited. We aim to propose a method that
acquires large amounts of parallel data for free.
The drawback is that there is a margin of error in
the parallel segment identification and alignment.
However, our system can be tuned for precision or
for recall.
3 Parallel Segment Retrieval
We will first abstract from the domain of Mi-
croblogs and focus on the task of retrieving par-
allel segments from single documents. Prior work
on finding parallel data attempts to reason about
the probability that pairs of documents (x, y) are
parallel. In contrast, we only consider one doc-
ument at a time, defined by x = x1, x2, . . . , xn,
and consisting of n tokens, and need to deter-
mine whether there is parallel data in x, and if
so, where are the parallel segments and their lan-
guages. For simplicity, we assume that there are
at most 2 continuous segments that are parallel.
As representation for the parallel seg-
ments within the document, we use the tuple
([p, q], l, [u, v], r, a). The word indexes [p, q] and
[u, v] are used to identify the left segment (from
p to q) and right segment (from u to v), which
are parallel. We shall refer [p, q] and [u, v] as the
spans of the left and right segments. To avoid
overlaps, we set the constraint p ? q < u ? v.
Then, we use l and r to identify the language of
the left and right segments, respectively. Finally, a
represents the word alignment between the words
in the left and the right segments.
The main problem we address is to find the
parallel data when the boundaries of the parallel
segments are not defined explicitly. If we knew
the indexes [p, q] and [u, v], we could simply run
a language detector for these segments to find l
and r. Then, we would use an word alignment
model (Brown et al, 1993; Vogel et al, 1996),
with source s = xp, . . . , xq, target t = xu, . . . , xv
and lexical table ?l,r to calculate the Viterbi align-
ment a. Finally, from the probability of the word
alignments, we can determine whether the seg-
ments are parallel.
Thus, our model will attempt to find the opti-
mal values for the segments [p, q][u, v], languages
l, r and word alignments a jointly. However, there
are two problems with this approach. Firstly, word
alignment models generally attribute higher prob-
abilities to smaller segments, since these are the
result of a smaller product chain of probabilities.
In fact, because our model can freely choose the
segments to align, choosing only one word as the
left segment that is well aligned to a word in the
right segment would be the best choice. This
is obviously not our goal, since we would not
obtain any useful sentence pairs. Secondly, in-
ference must be performed over the combination
of all latent variables, which is intractable using
177
a brute force algorithm. We shall describe our
model to solve the first problem in 3.1 and our
dynamic programming approach to make the in-
ference tractable in 3.2.
3.1 Model
We propose a simple (non-probabilistic) three-
factor model that models the spans of the parallel
segments, their languages, and word alignments
jointly. This model is defined as follows:
S([u, v], r, [p, q],l, a | x) =
S?S ([p, q], [u, v] | x)?
S?L(l, r | [p, q], [u, v], x)?
S?T (a | [p, q], l, [u, v], r, x)
Each of the components is weighted by the pa-
rameters ?, ? and ?. We set these values empiri-
cally ? = 0.3, ? = 0.3 and ? = 0.4, and leave the
optimization of these parameters as future work.
We discuss the components of this model in turn.
Span score SS . We define the score of hypothe-
sized pair of spans [p, q], [u, v] as:
SS([p, q], [u, v] | x) =
(q ? p+ 1) + (v ? u+ 1)?
0<p??q?<u??v??n(q? ? p? + 1) + (v? ? u? + 1)
?
?([p, q], [u, v], x)
The first factor is a distribution over all spans that
assigns higher probability to segmentations that
cover more words in the document. It is highest
for segmentations that cover all the words in the
document (this is desirable since there are many
sentence pairs that can be extracted but we want
to find the largest sentence pair in the document).
The function ? takes on values of 0 or 1 depend-
ing on whether certain constraints are violated,
these include: parenthetical constraints that en-
force that spans must not break text within par-
enthetical characters and language constraints that
ensure that we do break a sequence of Mandarin
characters, Arabic words or Latin words.
Language score SL. The language score
SL(l, r | [p, q], [u, v], x) indicates whether the lan-
guage labels l, r are appropriate to the document
contents:
SL(l, r | [p, q], [u, v], x) =?q
i=p L(l, xi) +
?v
i=u L(r, xi)
n
where L(l, x) is a language detection function that
yields 1 if the word xi is in language l, and 0 oth-
erwise. We build the function simply by consid-
ering all words that are composed of Latin char-
acters as English, Arabic characters as Arabic and
Han characters as Mandarin. This approach is not
perfect, but it is simple and works reasonably well
for our purposes.
Translation score ST . The translation score
ST (a | [p, q], l, [u, v], r) indicates whether [p, q]
is a reasonable translation of [u, v] with the align-
ment a. We rely on IBM Model 1 probabilities for
this score:
ST (a | [p, q], l, [u, v], r, x) =
1
(q ? p+ 1)v?u+2
v?
i=u
PM1(xi | xai).
The lexical tables PM1 for the various language
pairs are trained a priori using available parallel
corpora. While IBM Model 1 produces worse
alignments than other models, in our problem, we
need to efficiently consider all possible spans, lan-
guage pairs and word alignments, which makes
the problem intractable. We will show that dy-
namic programing can be used to make this prob-
lem tractable, using Model 1. Furthermore, IBM
Model 1 has shown good performance for sen-
tence alignment systems previously (Xu et al,
2005; Braune and Fraser, 2010).
3.2 Inference
Our goal is to find the spans, language pair and
alignments such that:
argmax
[p,q],l,[u,v],r,a
S([p, q], l, [u, v], r, a | x) (1)
A high score indicates that the predicted bispan is
likely to correspond to a valid parallel span, so we
set a constant threshold ? to determine whether a
document has parallel data, i.e., the value of z:
z? = max
[u,v],r,[p,q],l,a
S([u, v], r, [p, q], l, a | x) > ?
Naively maximizing Eq. 1 would require
O(|x|6) operations, which is too inefficient to be
practical on large datasets. To process millions
of documents, this process would need to be op-
timized.
The main bottleneck of the naive algorithm is
finding new Viterbi Model 1 word alignments ev-
ery time we change the spans. Thus, we propose
178
an iterative approach to compute the Viterbi word
alignments for IBM Model 1 using dynamic pro-
gramming.
Dynamic programming search. The insight we
use to improve the runtime is that the Viterbi
word alignment of a bispan can be reused to cal-
culate the Viterbi word alignments of larger bis-
pans. The algorithm operates on a 4-dimensional
chart of bispans. It starts with the minimal valid
span (i.e., [0, 0], [1, 1]) and progressively builds
larger spans from smaller ones. Let Ap,q,u,v rep-
resent the Viterbi alignment (under ST ) of the bis-
pan [p, q], [u, v]. The algorithm uses the follow-
ing recursions defined in terms of four operations
?{+v,+u,+p,+q} that manipulate a single dimension
of the bispan to construct larger spans:
? Ap,q,u,v+1 = ?+v(Ap,q,u,v) adds one token to
the end of the right span with index v + 1 and
find the viterbi alignment for that token. This
requires iterating over all the tokens in the left
span, [p, q] and possibly updating their align-
ments. See Fig. 1 for an illustration.
? Ap,q,u+1,v = ?+u(Ap,q,u,v) removes the first to-
ken of the right span with index u, so we only
need to remove the alignment from u, which can
be done in time O(1).
? Ap,q+1,u,v = ?+q(Ap,q,u,v) adds one token to
the end of the left span with index q + 1, we
need to check for each word in the right span, if
aligning to the word in index q+1 yields a better
translation probability. This update requires n?
q + 1 operations.
? Ap+1,q,u,v = ?+p(Ap,q,u,v) removes the first
token of the left span with index p. After re-
moving the token, we need to find new align-
ments for all tokens that were aligned to p.
Thus, the number of operations for this update
is K ? (q ? p + 1), where K is the number of
words that were aligned to p. In the best case, no
words are aligned to the token in p, and we can
simply remove it. In the worst case, if all target
words were aligned to p, this update will result
in the recalculation of all Viterbi Alignments.
The algorithm proceeds until all valid cells have
been computed. One important aspect is that the
update functions differ in complexity, so the se-
quence of updates we apply will impact the per-
formance of the system. Most spans are reach-
able using any of the four update functions. For
instance, the span A2,3,4,5 can be reached us-
ing ?+v(A2,3,4,4), ?+u(A2,3,3,5), ?+q(A2,2,4,5) or
?+p(A1,3,4,5). However, we want to use ?+u
a b - A B
a
b
-
A
B
a b - A B
p
qu v
p
qu v?+v
Figure 1: Illustration of the ?+v operator. The
light gray boxes show the parallel span and the
dark boxes show the span?s Viterbi alignment.
In this example, the parallel message contains a
?translation? of a b to A B.
whenever possible, since it only requires one op-
eration, although that is not always possible. For
instance, the state A2,2,2,4 cannot be reached us-
ing ?+u, since the state A2,2,1,4 is not valid, be-
cause the spans overlap. If this happens, incre-
mentally more expensive updates need to be used,
such as ?+v, then ?+q, which are in the same order
of complexity. Finally, we want to minimize the
use of ?+p, which is quadratic in the worst case.
Thus, we use the following recursive formulation
that guarantees the optimal outcome:
Ap,q,u,v =
?
????
????
?+u(Ap,q,u?1,v) if u > q + 1
?+v(Ap,q,u,v?1) else if v > q + 1
?+p(Ap?1,q,u,v) else if q = p+ 1
?+q(Ap,q?1,u,v) otherwise
This transition function applies the cheapest
possible update to reach state Ap,q,u,v.
Complexity analysis. We can see that ?+u
is only needed in the following the cases
[0, 1][2, 2], [1, 2][3, 3], ? ? ? , [n ? 2, n ? 1][n, n].
Since, this update is quadratic in the worst
case, the complexity of this operations is
O(n3). The update ?+q, is applied to the cases
[?, 1][2, 2], [?, 2][3, 3], ? ? ? , [?, n?1], [n, n], where
? denotes any number within the span constraints
but not present in previous updates. Since, the
update is linear and we need to iterate through
all tokens twice, this update takes O(n3) opera-
tions. The update ?+v is applied for the cases
[?, 1][2, ?], [?, 2][3, ?], ? ? ? , [?, n? 1], [n, ?]. Thus,
with three degrees of freedom and a linear update,
it runs in O(n4) time. Finally, update ?+u runs in
constant time, but is run for all remaining cases,
which constitute O(n4) space. By summing the
179
executions of all updates, we observe that the or-
der of magnitude of our exact inference process is
O(n4). Note that for exact inference, it is not pos-
sible to get a lower order of magnitude, since we
need to at least iterate through all possible span
values once, which takes O(n4) time.
4 Parallel Data Extraction
We will now describe our method to extract par-
allel data from Microblogs. The target domains
in this work are Twitter and Sina Weibo, and
the main language pair is Chinese-English. Fur-
thermore, we also run the system for the Arabic-
English language pair using the Twitter data.
For the Twitter domain, we use a previously
crawled dataset from the years 2008 to 2013,
where one million tweets are crawled every day.
In total, we processed 1.6 billion tweets.
Regarding Sina Weibo, we built a crawler that
continuously collects tweets from Weibo. We start
from one seed user and collect his posts, and then
we find the users he follows that we have not con-
sidered, and repeat. Due to the rate limiting es-
tablished by the Weibo API1, we are restricted in
terms of number of requests every hour, which
greatly limits the amount of messages we can col-
lect. Furthermore, each request can only fetch up
to 100 posts from a user, and subsequent pages of
100 posts require additional API calls. Thus, to
optimize the number of parallel posts we can col-
lect per request, we only crawl all messages from
users that have at least 10 parallel tweets in their
first 100 posts. The number of parallel messages
is estimated by running our alignment model, and
checking if ? > ?, where ? was set empirically
initially, and optimized after obtaining annotated
data, which will be detailed in 5.1. Using this
process, we crawled 65 million tweets from Sina
Weibo within 4 months.
In both cases, we first filter the collection of
tweets for messages containing at least one trigram
in each language of the target language pair, deter-
mined by their Unicode ranges. This means that
for the Chinese-English language pair, we only
keep tweets with more than 3 Mandarin charac-
ters and 3 latin words. Furthermore, based on the
work in (Jelh et al, 2012), if a tweet A is iden-
tified as a retweet, meaning that it references an-
other tweetB, we also consider the hypothesis that
these tweets may be mutual translations. Thus, if
A and B contain trigrams in different languages,
1http://open.weibo.com/wiki/API??/en
these are also considered for the extraction of par-
allel data. This is done by concatenating tweets A
and B, and adding the constraint that [p, q] must
be within A and [u, v] must be within B. Finally,
identical duplicate tweets are removed.
After filtering, we obtained 1124k ZH-EN
tweets from Sina Weibo, 868k ZH-EN and 136k
AR-EN tweets from Twitter. These language pairs
are not definite, since we simply check if there is
a trigram in each language.
Finally, we run our alignment model described
in section 3, and obtain the parallel segments and
their scores, which measure how likely those seg-
ments are parallel. In this process, lexical tables
for EN-ZH language pair used by Model 1 were
built using the FBIS dataset (LDC2003E14) for
both directions, a corpus of 300K sentence pairs
from the news domain. Likewise, for the EN-
AR language pair, we use a fraction of the NIST
dataset, by removing the data originated from UN,
which leads to approximately 1M sentence pairs.
5 Experiments
We evaluate our method in two ways. First, intrin-
sically, by observing how well our method identi-
fies tweets containing parallel data, the language
pair and what their spans are. Second, extrinsi-
cally, by looking at how well the data improves
a translation task. This methodology is similar to
that of Smith et al (2010).
5.1 Parallel Data Extraction
Data. Our method needs to determine if a given
tweet contains parallel data, and if so, what is
the language pair of the data, and what segments
are parallel. Thus, we had a native Mandarin
speaker, also fluent in English, to annotate 2000
tweets sampled from crawled Weibo tweets. One
important question of answer is what portion of
the Microblogs contains parallel data. Thus, we
also use the random sample Twitter and annotated
1200 samples, identifying whether each sample
contains parallel data, for the EN-ZH and AR-EN
filtered tweets.
Metrics. To test the accuracy of the score S, we
ordered all 2000 samples by score. Then, we cal-
culate the precision, recall and accuracy at increas-
ing intervals of 10% of the top samples. We count
as a true positive (tp) if we correctly identify a par-
allel tweet, and as a false positive (fp) spuriously
detect a parallel tweet. Finally, a true negative (tn)
occurs when we correctly detect a non-parallel
180
tweet, and a false negative (fn) if we miss a par-
allel tweet. Then, we set the precision as tptp+fp ,
recall as tptp+fn and accuracy as tp+tntp+fp+tn+fn . Forlanguage identification, we calculate the accuracy
based on the number of instances that were iden-
tified with the correct language pair. Finally, to
evaluate the segment alignment, we use the Word
Error Rate (WER) metric, without substitutions,
where we compare the left and right spans of our
system and the respective spans of the reference.
We count an insertion error (I) for each word in
our system?s spans that is not present in the refer-
ence span and a deletion error (D) for each word
in the reference span that is not present in our sys-
tem?s spans. Thus, we set WER = D+IN , where
N is the number of tokens in the tweet. To com-
pute this score for the whole test set, we compute
the average of the WER for each sample.
Results. The precision, recall and accuracy
curves are shown in Figure 2. The quality of the
parallel sentence detection did not vary signifi-
cantly with different setups, so we will only show
the results for the best setup, which is the baseline
model with span constraints.
0.2	 ?
0.3	 ?
0.4	 ?
0.5	 ?
0.6	 ?
0.7	 ?
0.8	 ?
0.9	 ?
1	 ?
10%	 ? 20%	 ? 30%	 ? 40%	 ? 50%	 ? 60%	 ? 70%	 ? 80%	 ? 90%	 ? 100%	 ?
Precision	 ?
Recall	 ?
Accuracy	 ?
Figure 2: Precision, recall and accuracy curves
for parallel data detection. The y-axis denotes the
scores for each metric, and the x-axis denotes the
percentage of the highest scoring sentence pairs
that are kept.
From the precision and recall curves, we ob-
serve that most of the parallel data can be found
at the top 30% of the filtered tweets, where 5 in 6
tweets are detected correctly as parallel, and only
1 in every 6 parallel sentences is lost. We will de-
note the score threshold at this point as ?, which is
a good threshold to estimate on whether the tweet
is parallel. However, this parameter can be tuned
for precision or recall. We also see that in total,
30% of the filtered tweets are parallel. If we gen-
eralize this ratio for the complete set with 1124k
tweets, we can expect approximately 337k paral-
lel sentences. Finally, since 65 million tweets were
extracted to generate the 337k tweets, we estimate
that approximately 1 parallel tweet can be found
for every 200 tweets we process using our tar-
geted approach. On the other hand, from the 1200
tweets from Twitter, we found that 27 had parallel
data in the ZH-EN pair, if we extrapolate for the
whole 868k filtered tweets, we expect that we can
find 19530. 19530 parallel sentences from 1.6 bil-
lion tweets crawled randomly, represents 0.001%
of the total corpora. For AR-EN, a similar re-
sult was obtained where we expect 12407 tweets
out of the 1.6 billion to be parallel. This shows
that targeted approaches can substantially reduce
the crawling effort required to find parallel tweets.
Still, considering that billions of tweets are posted
daily, this is a substantial source of parallel data.
The remainder of the tests will be performed on
the Weibo dataset, which contains more parallel
data. Tests on the Twitter data will be conducted
as future work, when we process Twitter data on a
larger scale to obtain more parallel sentences.
For the language identification task, we had an
accuracy of 99.9%, since distinguishing English
and Mandarin is trivial. The small percentage of
errors originated from other latin languages (Ex:
French) due to our naive language detector.
As for the segment alignment task. Our base-
line system with no constraints obtains a WER of
12.86%, and this can be improved to 11.66% by
adding constraints to possible spans. This shows
that, on average, approximately 1 in 9 words on
the parallel segments is incorrect. However, trans-
lation models are generally robust to such kinds of
errors and can learn good translations even in the
presence of imperfect sentence pairs.
Among the 578 tweets that are parallel, 496
were extracted within the same tweet and 82 were
extracted from retweets. Thus, we see that the ma-
jority of the parallel data comes from within the
same tweet.
Topic analysis. To give an intuition about the
contents of the parallel data we found, we looked
at the distribution over topics of the parallel
dataset inferred by LDA (Blei et al, 2003). Thus,
we grouped the Weibo filtered tweets by users,
and ran LDA over the predicted English segments,
with 12 topics. The 7 most interpretable topics are
shown in Table 1. We see that the data contains a
181
# Topic Most probable words in topic
1 (Dating) love time girl live mv back word night rt wanna
2 (Entertainment) news video follow pong image text great day today fans
3 (Music) cr day tour cn url amazon music full concert alive
4 (Religion) man god good love life heart would give make lord
5 (Nightlife) cn url beijing shanqi party adj club dj beijiner vt
6 (Chinese News) china chinese year people world beijing years passion country government
7 (Fashion) street fashion fall style photo men model vogue spring magazine
Table 1: Most probable words inferred using LDA in several topics from the parallel data extracted from
Weibo. Topic labels (in parentheses) were assigned manually for illustration purposes.
variety of topics, both formal (Chinese news, reli-
gion) and informal (entertainment, music).
Example sentence pairs. To gain some perspec-
tive on the type of sentence pairs we are extract-
ing, we will illustrate some sentence pairs we
crawled and aligned automatically. Table 2 con-
tains 5 English-Mandarin and 4 English-Arabic
sentence pairs that were extracted automatically.
These were chosen, since they contain some as-
pects that are characteristic of the text present in
Microblogs and Social Media. These are:
? Abbreviations - In most sentence pairs exam-
ples, we can witness the use of abbreviated
forms of English words, such as wanna, TMI,
4 and imma. These can be normalized as want
to, too much information, for and I am going
to, respectively. In sentence 5, we observe that
this phenomena also occurs in Mandarin. We
find that TMD is a popular way to write???
whose Pinyin rendering is ta? ma? de. The mean-
ing of this expression depends on the context it
is used, and can convey a similar connotation
as adding the intensifier the hell to an English
sentence.
? Jargon - Another common phenomena is the
appearance of words that are only used in sub-
communities. For instance, in sentence pair 4,
we the jargon word cday is used, which is a col-
loquial variant for birthday.
? Emoticons - In sentence 8, we observe the pres-
ence of the emoticon :), which is frequently
used in this media. We found that emoticons are
either translated as they are or simply removed,
in most cases.
? Syntax errors - In the domain of microblogs, it
is also common that users do not write strictly
syntactic sentences, for instance, in sentence
pair 7, the sentence onni this gift only 4 u, is
clearly not syntactically correct. Firstly, onni
is a named entity, yet it is not capitalized. Sec-
ondly, a comma should follow onni. Thirdly, the
verb is should be used after gift. Having exam-
ples of these sentences in the training set, with
common mistakes (intentional or not), might
become a key factor in training MT systems that
can be robust to such errors.
? Dialects - We can observe a much broader range
of dialects in our data, since there are no di-
alect standards in microblogs. For instance, in
sentence pair 6, we observe an arabic word (in
bold) used in the spoken Arabic dialect used in
some countries along the shores of the Persian
Gulf, which means means the next. In standard
Arabic, a significantly different form is used.
We can also see in sentence pair 9 that our
aligner does not alway make the correct choice
when determining spans. In this case, the segment
RT @MARYAMALKHAWAJA: was included in the
English segment spuriously, since it does not cor-
respond to anything in the Arabic counterpart.
5.2 Machine Translation Experiments
We report on machine translation experiments us-
ing our harvested data in two domains: edited
news and microblogs.
News translation. For the news test, we cre-
ated a new test set from a crawl of the Chinese-
English documents on the Project Syndicate web-
site2, which contains news commentary articles.
We chose to use this data set, rather than more
standard NIST test sets to ensure that we had re-
cent documents in the test set (the most recent
NIST test sets contain documents published in
2007, well before our microblog data was created).
We extracted 1386 parallel sentences for tuning
and another 1386 sentences for testing, from the
manually aligned segments. For this test set, we
used 8 million sentences from the full NIST par-
allel dataset as the language model training data.
We shall call this test set Syndicate.
2http://www.project-syndicate.org/
182
ENGLISH MANDARIN
1 i wanna live in a wes anderson world ??????Wes Anderson????
2 Chicken soup, corn never truly digests. TMI. ??????????????????.??
3 To DanielVeuleman yea iknw imma work on that ?DanielVeuleman?????????????????
4 msg 4 Warren G his cday is today 1 yr older. ????Warren G????????????????
5 Where the hell have you been all these years? ????TMD????
ENGLISH ARABIC
6 It?s gonna be a warm week! Qk ?


AJ
? @ ? ?J.?B@
7 onni this gift only 4 u ?? ?? 	? ?K
Y?? @ ? 	Y? ?

	
G?

@
8 sunset in aqaba :) (: ?J. ???@ ?

	
? ?? ??@ H. ?Q
	
?
9 RT @MARYAMALKHAWAJA: there is a call @Y 	? ??A 	J? ?Y? ?


	
? H@Q?A 	??? Z @Y 	K ?A 	J?for widespread protests in #bahrain tmrw
Table 2: Examples of English-Mandarin and English-Arabic sentence pairs. The English-Mandarin
sentences were extracted from Sina Weibo and the English-Arabic sentences were extracted from Twitter.
Some messages have been shorted to fit into the table. Some interesting aspects of these sentence pairs
are marked in bold.
Microblog translation. To carry out the mi-
croblog translation experiments, we need a high
quality parallel test set. Since we are not aware
of such a test set, we created one by manually se-
lecting parallel messages from Weibo. Our proce-
dure was as follows. We selected 2000 candidate
Weibo posts from users who have a high num-
ber of parallel tweets according to our automatic
method (at least 2 in every 5 tweets). To these, we
added another 2000 messages from our targeted
Weibo crawl, but these had no requirement on the
proportion of parallel tweets they had produced.
We identified 2374 parallel segments, of which we
used 1187 for development and 1187 for testing.
We refer to this test set as Weibo.3
Obviously, we removed the development and
test sets from our training data. Furthermore, to
ensure that our training data was not too similar to
the test set in the Weibo translation task, we fil-
tered the training data to remove near duplicates
by computing edit distance between each paral-
lel sentence in the heldout set and each training
instance. If either the source or the target sides
of the a training instance had an edit distance of
less than 10%, we removed it.4 As for the lan-
guage models, we collected a further 10M tweets
from Twitter for the English language model and
another 10M tweets from Weibo for the Chinese
language model.
3We acknowledge that self-translated messages are prob-
ably not a typically representative sample of all microblog
messages. However, we do not have the resources to produce
a carefully curated test set with a more broadly representative
distribution. Still, we believe these results are informative as
long as this is kept in mind.
4Approximately 150,000 training instances removed.
Syndicate Weibo
ZH-EN EN-ZH ZH-EN EN-ZH
FBIS 9.4 18.6 10.4 12.3
NIST 11.5 21.2 11.4 13.9
Weibo 8.75 15.9 15.7 17.2
FBIS+Weibo 11.7 19.2 16.5 17.8
NIST+Weibo 13.3 21.5 16.9 17.9
Table 3: BLEU scores for different datasets in dif-
ferent translation directions (left to right), broken
with different training corpora (top to bottom).
Baselines. We report results on these test sets us-
ing different training data. First, we use the FBIS
dataset which contains 300K high quality sentence
pairs, mostly in the broadcast news domain. Sec-
ond, we use the full 2012 NIST Chinese-English
dataset (approximately 8M sentence pairs, includ-
ing FBIS). Finally, we use our crawled data (re-
ferred as Weibo) by itself and also combined with
the two previous training sets.
Setup. We use the Moses phrase-based MT sys-
tem with standard features (Koehn et al, 2003).
For reordering, we use the MSD reordering
model (Axelrod et al, 2005). As the language
model, we use a 5-gram model with Kneser-
Ney smoothing. The weights were tuned using
MERT (Och, 2003). Results are presented with
BLEU-4 (Papineni et al, 2002).
Results. The BLEU scores for the different par-
allel corpora are shown in Table 3 and the top 10
out-of-vocabulary (OOV) words for each dataset
are shown in Table 4. We observe that for the
Syndicate test set, the NIST and FBIS datasets
183
Syndicate (test) Weibo (test)
FBIS NIST Weibo FBIS NIST Weibo
obama (83) barack (59) democracies (15) 2012 (24) showstudio (9) submissions (4)
barack (59) namo (6) imbalances (13) alanis (13) crue (9) ivillage (4)
princeton (40) mitt (6) mahmoud (12) crue (9) overexposed (8) scola (3)
ecb (8) guant (6) millennium (9) showstudio (9) tweetmeian (5) rbst (3)
bernanke (8) fairtrade (6) regimes (8) overexposed (8) tvd (5) curitiba (3)
romney (7) hollande (5) wolfowitz (7) itunes (8) iheartradio (5) zeman (2)
gaddafi (7) wikileaks (4) revolutions (7) havoc (8) xoxo (4) @yaptv (2)
merkel (7) wilders (3) qaddafi (7) sammy (6) snoop (4) witnessing (2)
fats (7) rant (3) geopolitical (7) obama (6) shinoda (4) whoohooo (2)
dialogue (7) esm (3) genome (7) lol (6) scrapbook (4) wbr (2)
Table 4: The most frequent out-of-vocabulary (OOV) words and their counts for the two English-source
test sets with three different training sets.
perform better than our extracted parallel data.
This is to be expected, since our dataset was ex-
tracted from an extremely different domain. How-
ever, by combining the Weibo parallel data with
this standard data, improvements in BLEU are ob-
tained. Error analysis indicates that one major fac-
tor is that names from current events, such as Rom-
ney and Wikileaks do not occur in the older NIST
and FBIS datasets, but they are represented in the
Weibo dataset. Furthermore, we also note that the
system built on the Weibo dataset does not per-
form substantially worse than the one trained on
the FBIS dataset, a further indication that harvest-
ing parallel microblog data yields a diverse collec-
tion of translated material.
For the Weibo test set, a significant improve-
ment over the news datasets can be achieved us-
ing our crawled parallel data. Once again newer
terms, such as iTunes, are one of the reasons older
datasets perform less well. However, in this case,
the top OOV words of the news domain datasets
are not the most accurate representation of cov-
erage problems in this domain. This is because
many frequent words in microblogs, e.g., nonstan-
dard abbreviations, like u and 4 are found in the
news domain as words, albeit with different mean-
ings. Thus, the OOV table gives an incomplete
picture of the translation problems when using
the news domain corpora to translate microblogs.
Also, some structural errors occur when training
with the news domain datasets, one such example
is shown in table 5, where the character ? is in-
correctly translated to said. This occurs because
this type of constructions is infrequent in news
datasets. Furthermore, we can see that compound
expressions, such as the translation from ???
? to party time are also learned.
Finally, we observe that combining the datasets
Source ?sam farrar??????
Reference to sam farrar , party time
FBIS farrar to sam said , in time
NIST to sam farrar said , the moment
WEIBO to sam farrar , party time
Table 5: Translation Examples using different
training sets.
yields another gain over individual datasets, both
in the Syndicate and in the Weibo test sets.
6 Conclusion
We presented a framework to crawl parallel data
from microblogs. We find parallel data from sin-
gle posts, with translations of the same sentence
in two languages. We show that a considerable
amount of parallel sentence pairs can be crawled
from microblogs and these can be used to improve
Machine Translation by updating our translation
tables with translations of newer terms. Further-
more, the in-domain data can substantially im-
prove the translation quality on microblog data.
The resources described in this paper and fur-
ther developments are available to the general pub-
lic at http://www.cs.cmu.edu/?lingwang/utopia.
Acknowledgements
The PhD thesis of Wang Ling is supported by FCT
grant SFRH/BD/51157/2010. The authors wish
to express their gratitude to thank William Cohen,
Noah Smith, Waleed Ammar, and the anonymous
reviewers for their insight and comments. We are
also extremely grateful to Brendan O?Connor for
providing the Twitter data and to Philipp Koehn
and Barry Haddow for providing the Project Syn-
dicate data.
184
References
[Axelrod et al2005] Amittai Axelrod, Ra Birch Mayne,
Chris Callison-burch, Miles Osborne, and David
Talbot. 2005. Edinburgh system description for the
2005 iwslt speech translation evaluation. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT.
[Blei et al2003] David M. Blei, Andrew Y. Ng, and
Michael I. Jordan. 2003. Latent dirichlet alocation.
J. Mach. Learn. Res., 3:993?1022, March.
[Braune and Fraser2010] Fabienne Braune and Alexan-
der Fraser. 2010. Improved unsupervised sentence
alignment for symmetrical and asymmetrical paral-
lel corpora. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 81?89, Stroudsburg, PA, USA.
Association for Computational Linguistics.
[Brown et al1993] Peter F. Brown, Vincent J. Della
Pietra, Stephen A. Della Pietra, and Robert L. Mer-
cer. 1993. The mathematics of statistical machine
translation: parameter estimation. Comput. Lin-
guist., 19:263?311, June.
[Fukushima et al2006] Ken?ichi Fukushima, Kenjiro
Taura, and Takashi Chikayama. 2006. A fast and
accurate method for detecting English-Japanese par-
allel texts. In Proceedings of the Workshop on Mul-
tilingual Language Resources and Interoperability,
pages 60?67, Sydney, Australia, July. Association
for Computational Linguistics.
[Gimpel et al2011] Kevin Gimpel, Nathan Schneider,
Brendan O?Connor, Dipanjan Das, Daniel Mills, Ja-
cob Eisenstein, Michael Heilman, Dani Yogatama,
Jeffrey Flanigan, and Noah A. Smith. 2011. Part-
of-speech tagging for twitter: annotation, features,
and experiments. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: short
papers - Volume 2, HLT ?11, pages 42?47, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
[Jelh et al2012] Laura Jelh, Felix Hiebel, and Stefan
Riezler. 2012. Twitter translation using translation-
based cross-lingual retrieval. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 410?421, Montre?al, Canada, June. Asso-
ciation for Computational Linguistics.
[Koehn et al2003] Philipp Koehn, Franz Josef Och,
and Daniel Marcu. 2003. Statistical phrase-based
translation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology - Volume 1, NAACL ?03, pages 48?54,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
[Koehn2005] Philipp Koehn. 2005. Europarl: A Par-
allel Corpus for Statistical Machine Translation. In
Proceedings of the tenth Machine Translation Sum-
mit, pages 79?86, Phuket, Thailand. AAMT, AAMT.
[Li and Liu2008] Bo Li and Juan Liu. 2008. Mining
Chinese-English parallel corpora from the web. In
Proceedings of the 3rd International Joint Confer-
ence on Natural Language Processing (IJCNLP).
[Lin et al2008] Dekang Lin, Shaojun Zhao, Benjamin
Van Durme, and Marius Pas?ca. 2008. Mining par-
enthetical translations from the web by word align-
ment. In Proceedings of ACL-08: HLT, pages 994?
1002, Columbus, Ohio, June. Association for Com-
putational Linguistics.
[Och2003] Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics - Volume 1, ACL ?03,
pages 160?167, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Papineni et al2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
method for automatic evaluation of machine trans-
lation. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 311?318, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
[Post et al2012] Matt Post, Chris Callison-Burch, and
Miles Osborne. 2012. Constructing parallel cor-
pora for six indian languages via crowdsourcing. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 401?409, Montre?al,
Canada, June. Association for Computational Lin-
guistics.
[Resnik and Smith2003] Philip Resnik and Noah A.
Smith. 2003. The web as a parallel corpus. Compu-
tational Linguistics, 29:349?380.
[Smith et al2010] Jason R. Smith, Chris Quirk, and
Kristina Toutanova. 2010. Extracting parallel sen-
tences from comparable corpora using document
level alignment. In Proceedings of the 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics.
[Ture and Lin2012] Ferhan Ture and Jimmy Lin. 2012.
Why not grab a free lunch? mining large corpora for
parallel sentences to improve translation modeling.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 626?630, Montre?al, Canada, June. Associa-
tion for Computational Linguistics.
[Uszkoreit et al2010] Jakob Uszkoreit, Jay Ponte,
Ashok C. Popat, and Moshe Dubiner. 2010. Large
scale parallel document mining for machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, pages 1101?
1109.
[Vogel et al1996] Stephan Vogel, Hermann Ney, and
Christoph Tillmann. 1996. Hmm-based word align-
ment in statistical translation. In Proceedings of the
16th conference on Computational linguistics - Vol-
ume 2, COLING ?96, pages 836?841, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
[Xu et al2001] Jinxi Xu, Ralph Weischedel, and Chanh
Nguyen. 2001. Evaluating a probabilistic model
185
for cross-lingual information retrieval. In Proceed-
ings of the 24th annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, SIGIR ?01, pages 105?110, New
York, NY, USA. ACM.
[Xu et al2005] Jia Xu, Richard Zens, and Hermann
Ney. 2005. Sentence segmentation using ibm word
alignment model 1. In Proceedings of EAMT 2005
(10th Annual Conference of the European Associa-
tion for Machine Translation, pages 280?287.
[Zbib et al2012] Rabih Zbib, Erika Malchiodi, Jacob
Devlin, David Stallard, Spyros Matsoukas, Richard
Schwarz, John Makhoul, Omar F. Zaidan, and Chris
Callison-Burch. 2012. Machine translation of Ara-
bic dialects. In Proceedings of the 2012 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies.
186
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 70?77,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The CMU Machine Translation Systems at WMT 2013:
Syntax, Synthetic Translation Options, and Pseudo-References
Waleed Ammar Victor Chahuneau Michael Denkowski Greg Hanneman
Wang Ling Austin Matthews Kenton Murray Nicola Segall Yulia Tsvetkov
Alon Lavie Chris Dyer?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
?Corresponding author: cdyer@cs.cmu.edu
Abstract
We describe the CMU systems submit-
ted to the 2013 WMT shared task in ma-
chine translation. We participated in three
language pairs, French?English, Russian?
English, and English?Russian. Our
particular innovations include: a label-
coarsening scheme for syntactic tree-to-
tree translation and the use of specialized
modules to create ?synthetic translation
options? that can both generalize beyond
what is directly observed in the parallel
training data and use rich source language
context to decide how a phrase should
translate in context.
1 Introduction
The MT research group at Carnegie Mellon Uni-
versity?s Language Technologies Institute par-
ticipated in three language pairs for the 2013
Workshop on Machine Translation shared trans-
lation task: French?English, Russian?English,
and English?Russian. Our French?English sys-
tem (?3) showcased our group?s syntactic sys-
tem with coarsened nonterminal types (Hanne-
man and Lavie, 2011). Our Russian?English and
English?Russian system demonstrate a new multi-
phase approach to translation that our group is us-
ing, in which synthetic translation options (?4)
to supplement the default translation rule inven-
tory that is extracted from word-aligned training
data. In the Russian-English system (?5), we used
a CRF-based transliterator (Ammar et al, 2012)
to propose transliteration candidates for out-of-
vocabulary words, and used a language model
to insert or remove common function words in
phrases according to an n-gram English language
model probability. In the English?Russian system
(?6), we used a conditional logit model to predict
the most likely inflectional morphology of Rus-
sian lemmas, conditioning on rich source syntac-
tic features (?6.1). In addition to being able to
generate inflected forms that were otherwise unob-
served in the parallel training data, the translations
options generated in this matter had features re-
flecting their appropriateness given much broader
source language context than usually would have
been incorporated in current statistical MT sys-
tems.
For our Russian?English system, we addition-
ally used a secondary ?pseudo-reference? transla-
tion when tuning the parameters of our Russian?
English system. This was created by automatically
translating the Spanish translation of the provided
development data into English. While the output
of an MT system is not always perfectly gram-
matical, previous work has shown that secondary
machine-generated references improve translation
quality when only a single human reference is
available when BLEU is used as an optimization
criterion (Madnani, 2010; Dyer et al, 2011).
2 Common System Components
The decoder infrastructure we used was cdec
(Dyer et al, 2010). Only the constrained data
resources provided for the shared task were used
for training both the translation and language
models. Word alignments were generated us-
ing the Model 2 variant described in Dyer et al
(2013). Language models used modified Kneser-
Ney smoothing estimated using KenLM (Heafield,
2011). Translation model parameters were dis-
criminatively set to optimize BLEU on a held-out
development set using an online passive aggres-
sive algorithm (Eidelman, 2012) or, in the case of
70
the French?English system, using the hypergraph
MERT algorithm and optimizing towards BLEU
(Kumar et al, 2009). The remainder of the paper
will focus on our primary innovations in the vari-
ous system pairs.
3 French-English Syntax System
Our submission for French?English is a tree-to-
tree translation system that demonstrates several
innovations from group?s research on SCFG-based
translation.
3.1 Data Selection
We divided the French?English training data into
two categories: clean data (Europarl, News Com-
mentary, UN Documents) totaling 14.8 million
sentence pairs, and web data (Common Crawl,
Giga-FrEn) totaling 25.2 million sentence pairs.
To reduce the volume of data used, we filtered
non-parallel and other unhelpful segments accord-
ing to the technique described by Denkowski et al
(2012). This procedure uses a lexical translation
model learned from just the clean data, as well as
source and target n-gram language models to com-
pute the following feature scores:
? French and English 4-gram log likelihood (nor-
malized by length);
? French?English and English?French lexical
translation log likelihood (normalized by
length); and,
? Fractions of aligned words under the French?
English and English?French models.
We pooled previous years? WMT news test sets
to form a reference data set. We computed the
same features. To filter the web data, we retained
only sentence for which each feature score was
no lower than two standard deviations below the
mean on the reference data. This reduced the web
data from 25.2 million to 16.6 million sentence
pairs. Parallel segments from all parts of the data
that were blank on either side, were longer than 99
tokens, contained a token of more than 30 charac-
ters, or had particularly unbalanced length ratios
were also removed. After filtering, 30.9 million
sentence pairs remained for rule extraction: 14.4
million from the clean data, and 16.5 million from
the web data.
3.2 Preprocessing and Grammar Extraction
Our French?English system uses parse trees in
both the source and target languages, so tokeniza-
tion in this language pair was carried out to match
the tokenizations expected by the parsers we used
(English data was tokenized with the Stanford to-
kenizer for English and an in-house tokenizer for
French that targets the tokenization used by the
Berkeley French parser). Both sides of the par-
allel training data were parsed using the Berkeley
latent variable parser.
Synchronous context-free grammar rules were
extracted from the corpus following the method of
Hanneman et al (2011). This decomposes each
tree pair into a collection of SCFG rules by ex-
haustively identifying aligned subtrees to serve as
rule left-hand sides and smaller aligned subtrees
to be abstracted as right-hand-side nonterminals.
Basic subtree alignment heuristics are similar to
those by Galley et al (2006), and composed rules
are allowed. The computational complexity is held
in check by a limit on the number of RHS elements
(nodes and terminals), rather than a GHKM-style
maximum composition depth or Hiero-style max-
imum rule span. Our rule extractor also allows
?virtual nodes,? or the insertion of new nodes in
the parse tree to subdivide regions of flat struc-
ture. Virtual nodes are similar to the A+B ex-
tended categories of SAMT (Zollmann and Venu-
gopal, 2006), but with the added constraint that
they may not conflict with the surrounding tree
structure.
Because the SCFG rules are labeled with non-
terminals composed from both the source and tar-
get trees, the nonterminal inventory is quite large,
leading to estimation difficulties. To deal with
this, we automatically coarsening the nonterminal
labels (Hanneman and Lavie, 2011). Labels are
agglomeratively clustered based on a histogram-
based similarity function that looks at what tar-
get labels correspond to a particular source label
and vice versa. The number of clusters used is de-
termined based on spikes in the distance between
successive clustering iterations, or by the number
of source, target, or joint labels remaining. Start-
ing from a default grammar of 877 French, 2580
English, and 131,331 joint labels, we collapsed
the label space for our WMT system down to 50
French, 54 English, and 1814 joint categories.1
1Selecting the stopping point still requires a measure of
intuition. The label set size of 1814 chosen here roughly cor-
responds to the number of joint labels that would exist in the
grammar if virtual nodes were not included. This equivalence
has worked well in practice in both internal and published ex-
periments on other data sets (Hanneman and Lavie, 2013).
71
Extracted rules each have 10 features associated
with them. For an SCFG rule with source left-
hand side `s, target left-hand side `t, source right-
hand side rs, and target right-hand side rt, they
are:
? phrasal translation log relative frequencies
log f(rs | rt) and log f(rt | rs);
? labeling relative frequency log f(`s, `t | rs, rt)
and generation relative frequency
log f(rs, rt | `s, `t);
? lexical translation log probabilities log plex(rs |
rt) and log plex(rt | rs), defined similarly to
Moses?s definition;
? a rarity score exp( 1c )?1exp(1)?1 for a rule with frequency
c (this score is monotonically decreasing in the
rule frequency); and,
? three binary indicator features that mark
whether a rule is fully lexicalized, fully abstract,
or a glue rule.
Grammar filtering. Even after collapsing la-
bels, the extracted SCFGs contain an enormous
number of rules ? 660 million rule types from just
under 4 billion extracted instances. To reduce the
size of the grammar, we employ a combination of
lossless filtering and lossy pruning. We first prune
all rules to select no more than the 60 most fre-
quent target-side alternatives for any source RHS,
then do further filtering to produce grammars for
each test sentence:
? Lexical rules are filtered to the sentence level.
Only phrase pairs whose source sides match the
test sentence are retained.
? Abstract rules (whose RHS are all nontermi-
nals) are globally pruned. Only the 4000 most
frequently observed rules are retained.
? Mixed rules (whose RHS are a mix of terminals
and nonterminals) must match the test sentence,
and there is an additional frequency cutoff.
After this filtering, the number of completely lex-
ical rules that match a given sentence is typically
low, up to a few thousand rules. Each fully ab-
stract rule can potentially apply to every sentence;
the strict pruning cutoff in use for these rules is
meant to focus the grammar to the most important
general syntactic divergences between French and
English. Most of the latitude in grammar pruning
comes from adjusting the frequency cutoff on the
mixed rules since this category of rule is by far the
most common type. We conducted experiments
with three different frequency cutoffs: 100, 200,
and 500, with each increase decreasing the gram-
mar size by 70?80 percent.
3.3 French?English Experiments
We tuned our system to the newstest2008 set of
2051 segments. Aside from the official new-
stest2013 test set (3000 segments), we also col-
lected test-set scores from last year?s newstest2012
set (3003 segments). Automatic metric scores
are computed according to BLEU (Papineni et al,
2002), METEOR (Denkowski and Lavie, 2011),
and TER (Snover et al, 2006), all computed ac-
cording to MultEval v. 0.5 (Clark et al, 2011).
Each system variant is run with two independent
MERT steps in order to control for optimizer in-
stability.
Table 1 presents the results, with the metric
scores averaged over both MERT runs. Quite in-
terestingly, we find only minor differences in both
tune and test scores despite the large differences in
filtered/pruned grammar size as the cutoff for par-
tially abstract rules increases. No system is fully
statistically separable (at p < 0.05) from the oth-
ers according to MultEval?s approximate random-
ization algorithm. The closest is the variant with
cutoff 200, which is generally judged to be slightly
worse than the other two. METEOR claims full
distinction on the 2013 test set, ranking the sys-
tem with the strictest grammar cutoff (500) best.
This is the version that we ultimately submitted to
the shared translation task.
4 Synthetic Translation Options
Before discussing our Russian?English and
English?Russian systems, we introduce the
concept of synthetic translation options, which
we use in these systems. We provide a brief
overview here; for more detail, we refer the reader
to Tsvetkov et al (2013).
In language pairs that are typologically similar,
words and phrases map relatively directly from
source to target languages, and the standard ap-
proach to learning phrase pairs by extraction from
parallel data can be very effective. However, in
language pairs in which individual source lan-
guage words have many different possible transla-
tions (e.g., when the target language word could
have many different inflections or could be sur-
rounded by different function words that have no
72
Dev (2008) Test (2012) Test (2013)
System BLEU METR TER BLEU METR TER BLEU METR TER
Cutoff 100 22.52 31.44 59.22 27.73 33.30 53.25 28.34 * 33.19 53.07
Cutoff 200 22.34 31.40 59.21 * 27.33 33.26 53.23 * 28.05 * 33.07 53.16
Cutoff 500 22.80 31.64 59.10 27.88 * 33.58 53.09 28.27 * 33.31 53.13
Table 1: French?English automatic metric scores for three grammar pruning cutoffs, averaged over two
MERT runs each. Scores that are statistically separable (p < 0.05) from both others in the same column
are marked with an asterisk (*).
direct correspondence in the source language), we
can expect the standard phrasal inventory to be
incomplete, except when very large quantities of
parallel data are available or for very frequent
words. There simply will not be enough exam-
ples from which to learn the ideal set of transla-
tion options. Therefore, since phrase based trans-
lation can only generate input/output word pairs
that were directly observed in the training corpus,
the decoder?s only hope for producing a good out-
put is to find a fluent, meaning-preserving transla-
tion using incomplete translation lexicons. Syn-
thetic translation option generation seeks to fill
these gaps using secondary generation processes
that produce possible phrase translation alterna-
tives that are not directly extractable from the
training data. By filling in gaps in the transla-
tion options used to construct the sentential trans-
lation search space, global discriminative transla-
tion models and language models can be more ef-
fective than they would otherwise be.
From a practical perspective, synthetic transla-
tion options are attractive relative to trying to build
more powerful models of translation since they
enable focus on more targeted translation prob-
lems (for example, transliteration, or generating
proper inflectional morphology for a single word
or phrase). Since they are translation options and
not complete translations, many of them may be
generated.
In the following system pairs, we use syn-
thetic translation options to augment hiero gram-
mar rules learned in the usual way. The synthetic
phrases we include augment draw from several
sources:
? transliterations of OOV Russian words (?5.3);
? English target sides with varied function words
(for example, given a phrase that translates into
cat we procedure variants like the cat, a cat and
of the cat); and,
? when translating into Russian, we generate
phrases by first predicting the most likely Rus-
sian lemma for a source word or phrase, and
then, conditioned on the English source context
(including syntactic and lexical features), we
predict the most likely inflection of the lemma
(?6.1).
5 Russian?English System
5.1 Data
We used the same parallel data for both the
Russian?English and English Russian systems.
Except for filtering to remove sentence pairs
whose log length ratios were statistical outliers,
we only filtered the Common Crawl corpus to re-
move sentence pairs with less than 50% concentra-
tion of Cyrillic characters on the Russian side. The
remaining data was tokenized and lower-cased.
For language models, we trained 4-gram Markov
models using the target side of the bitext and any
available monolingual data (including Gigaword
for English). Additionally, we trained 7-gram lan-
guage models using 600-class Brown clusters with
Witten-Bell smoothing.2
5.2 Baseline System
Our baseline Russian?English system is a hierar-
chical phrase-based translation model as imple-
mented in cdec (Chiang, 2007; Dyer et al, 2010).
SCFG translation rules that plausibly match each
sentence in the development and deftest sets were
extracted from the aligned parallel data using the
suffix array indexing technique of Lopez (2008).
A Russian morphological analyzer was used to
lemmatize the training, development, and test
data, and the ?noisier channel? translation ap-
proach of Dyer (2007) was used in the Russian?
English system to let unusually inflected surface
forms back off to per-lemma translations.
2http://www.ark.cs.cmu.edu/cdyer/ru-600/.
73
5.3 Synthetic Translations: Transliteration
Analysis revealed that about one third of the un-
seen Russian tokens in the development set con-
sisted of named entities which should be translit-
erated. We used individual Russian-English word
pairs in Wikipedia parallel headlines 3 to train a
linear-chained CRF tagger which labels each char-
acter in the Russian token with a sequence of zero
or more English characters (Ammar et al, 2012).
Since Russian names in the training set were in
nominative case, we used a simple rule-based mor-
phological generator to produce possible inflec-
tions and filtered out the ones not present in the
Russian monolingual corpus. At decoding, un-
seen Russian tokens are fed to the transliterator
which produces the most probable 20 translitera-
tions. We add a synthetic translation option for
each of the transliterations with four features: an
indicator feature for transliterations, the CRF un-
normalized score, the trigram character-LM log-
probability, and the divergence from the average
length-ratio between an English name and its Rus-
sian transliteration.
5.4 Synthetic Translations: Function Words
Slavic languages like Russian have a large number
of different inflected forms for each lemma, repre-
senting different cases, tenses, and aspects. Since
our training data is rather limited relative to the
number of inflected forms that are possible, we use
an English language model to generate a variety
of common function word contexts for each con-
tent word phrase. These are added to the phrase
table with a feature indicating that they were not
actually observed in the training data, but rather
hallucinated using SRILM?s disambig tool.
5.5 Summary
Table 5.5 summarizes our Russian-English trans-
lation results. In the submitted system, we addi-
tionally used MBR reranking to combine the 500-
best outputs of our system, with the 500-best out-
puts of a syntactic system constructed similarly to
the French?English system.
6 English?Russian System
The bilingual training data was identical to the
filtered data used in the previous section. Word
alignments was performed after lemmatizing the
3We contributed the data set to the shared task participants
at http://www.statmt.org/wmt13/wiki-titles.ru-en.tar.gz
Table 2: Russian-English summary.
Condition BLEU
Baseline 30.8
Function words 30.9
Transliterations 31.1
Russian side of the training corpus. An unpruned,
modified Kneser-Ney smoothed 4-gram language
model (Chen and Goodman, 1996) was estimated
from all available Russian text (410 million words)
using the KenLM toolkit (Heafield et al, 2013).
A standard hierarchical phrase-based system
was trained with rule shape indicator features, ob-
tained by replacing terminals in translation rules
by a generic symbol. MIRA training was per-
formed to learn feature weights.
Additionally, word clusters (Brown et al, 1992)
were obtained for the complete monolingual Rus-
sian data. Then, an unsmoothed 7-gram language
model was trained on these clusters and added as
a feature to the translation system. Indicator fea-
tures were also added for each cluster and bigram
cluster occurence. These changes resulted in an
improvement of more than a BLEU point on our
held-out development set.
6.1 Predicting Target Morphology
We train a classifier to predict the inflection of
each Russian word independently given the cor-
responding English sentence and its word align-
ment. To do this, we first process the Russian
side of the parallel training data using a statisti-
cal morphological tagger (Sharoff et al, 2008) to
obtain lemmas and inflection tags for each word
in context. Then, we obtain part-of-speech tags
and dependency parses of the English side of the
parallel data (Martins et al, 2010), as well as
Brown clusters (Brown et al, 1992). We extract
features capturing lexical and syntactical relation-
ships in the source sentence and train structured
linear logistic regression models to predict the tag
of each English word independently given its part-
of-speech.4 In practice, due to the large size of
the corpora and of the feature space dimension,
we were only able to use about 10% of the avail-
able bilingual data, sampled randomly from the
Common Crawl corpus. We also restricted the
4We restrict ourselves to verbs, nouns, adjectives, adverbs
and cardinals since these open-class words carry most inflec-
tion in Russian.
74
??? ???????? ???????? ???? ?? ?? ?????????
she had attempted to cross the road on her bike
PRP   VBD         VBN          TO    VB       DT     NN    IN  PRP$   NN
nsubj
aux
xcomp
aux
????????_V*+*mis/sfm/e
C50   C473        C28          C8    C275   C37   C43  C82 C94   C331
Figure 1: The classifier is trained to predict the verbal inflection mis-sfm-e based on the linear and
syntactic context of the words aligned to the Russian word; given the stem ???????? (pytat?sya), this
inflection paradigm produces the observed surface form ???????? (pytalas?).
set of possible inflections for each word to the set
of tags that were observed with its lemma in the
full monolingual training data. This was neces-
sary because of our choice to use a tagger, which
is not able to synthesize surface forms for a given
lemma-tag pair.
We then augment the standard hierarchical
phrase-base grammars extracted for the baseline
systems with new rules containing inflections not
necessarily observed in the parallel training data.
We start by training a non-gappy phrase transla-
tion model on the bilingual data where the Russian
has been lemmatized.5 Then, before translating an
English sentence, we extract translation phrases
corresponding to this specific sentence and re-
inflect each word in the target side of these phrases
using the classifier with features extracted from
the source sentence words and annotations. We
keep the original phrase-based translation features
and add the inflection score predicted by the clas-
sifier as well as indicator features for the part-of-
speech categories of the re-inflected words.
On a held-out development set, these synthetic
phrases produce a 0.3 BLEU point improvement.
Interestingly, the feature weight learned for using
these phrases is positive, indicating that useful in-
flections might be produced by this process.
7 Conclusion
The CMU systems draws on a large number of
different research directions. Techniques such as
MBR reranking and synthetic phrases allow dif-
ferent contributors to focus on different transla-
5We keep intact words belonging to non-predicted cate-
gories.
tion problems that are ultimately recombined into
a single system. Our performance, in particular,
on English?Russian machine translation was quite
satisfying, we attribute our biggest gains in this
language pair to the following:
? Our inflection model that predicted how an En-
glish word ought best be translated, given its
context. This enabled us to generate forms that
were not observed in the parallel data or would
have been rare independent of context with pre-
cision.
? Brown cluster language models seem to be quite
effective at modeling long-range morphological
agreement patterns quite reliably.
Acknowledgments
We sincerely thank the organizers of the work-
shop for their hard work, year after year, and the
reviewers for their careful reading of the submit-
ted draft of this paper. This research work was
supported in part by the U. S. Army Research
Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533,
by the National Science Foundation under grant
IIS-0915327, by a NPRP grant (NPRP 09-1140-
1-177) from the Qatar National Research Fund (a
member of the Qatar Foundation), and by com-
puting resources provided by the NSF-sponsored
XSEDE program under grant TG-CCR110017.
The statements made herein are solely the respon-
sibility of the authors.
75
References
Waleed Ammar, Chris Dyer, and Noah A. Smith. 2012.
Transliteration by sequence labeling with lattice en-
codings and reranking. In NEWS workshop at ACL.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computional Linguistics, 18(4):467?479.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 310?318, Santa Cruz, California, USA,
June. Association for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: Crontrolling for
optimizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Short Papers, pages 176?181, Portland,
Oregon, USA, June.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 85?91, Edinburgh, Scot-
land, UK, July.
Michael Denkowski, Greg Hanneman, and Alon Lavie.
2012. The cmu-avenue french-english translation
system. In Proceedings of the NAACL 2012 Work-
shop on Statistical Machine Translation.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proc. of ACL.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The CMU-ARK German-
English translation system. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM Model 2. In Proc. of NAACL.
Chris Dyer. 2007. The ?noiser channel?: Translation
from morphologically complex languages. In Pro-
ceedings of WMT.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the ACL, pages 961?968, Sydney, Australia,
July.
Greg Hanneman and Alon Lavie. 2011. Automatic
category label coarsening for syntax-based machine
translation. In Proceedings of SSST-5: Fifth Work-
shop on Syntax, Semantics, and Structure in Statis-
tical Translation, pages 98?106, Portland, Oregon,
USA, June.
Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsen-
ing the label set. In Proceedings of NAACL-HLT
2013, pages 288?297, Atlanta, Georgia, USA, June.
Greg Hanneman, Michelle Burroughs, and Alon Lavie.
2011. A general-purpose rule extractor for SCFG-
based machine translation. In Proceedings of SSST-
5: Fifth Workshop on Syntax, Semantics, and Struc-
ture in Statistical Translation, pages 135?144, Port-
land, Oregon, USA, June.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, Sofia, Bulgaria,
August.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, Scotland, UK, July.
Shankar Kumar, Wolfgang Macherey, Chris Dyer,
and Franz Och. 2009. Efficient minimum error
rate training and minimum Bayes-risk decoding for
translation hypergraphs and lattices. In Proc. of
ACL-IJCNLP.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proc. of COLING.
Nitin Madnani. 2010. The Circle of Meaning: From
Translation to Paraphrasing and Back. Ph.D. the-
sis, Department of Computer Science, University of
Maryland College Park.
Andre? F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M. Q. Aguiar, and Ma?rio A. T. Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July.
76
Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Designing and
evaluating a russian tagset. In Proc. of LREC.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the Seventh Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA,
August.
Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna
Batia. 2013. Generating English determiners in
phrase-based translation with synthetic translation
options. In Proceedings of the Eighth Workshop on
Statistical Machine Translation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York, New
York, USA, June.
77
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 426?436,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Crowdsourcing High-Quality Parallel Data Extraction from Twitter
?
Wang Ling
123
Lu?s Marujo
123
Chris Dyer
2
Alan Black
2
Isabel Trancoso
13
(1)L
2
F Spoken Systems Lab, INESC-ID, Lisbon, Portugal
(2)Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA
(3)Instituto Superior T?cnico, Lisbon, Portugal
{lingwang,lmarujo,cdyer,awb}@cs.cmu.edu
isabel.trancoso@inesc-id.pt
Abstract
High-quality parallel data is crucial for a
range of multilingual applications, from
tuning and evaluating machine translation
systems to cross-lingual annotation pro-
jection. Unfortunately, automatically ob-
tained parallel data (which is available
in relative abundance) tends to be quite
noisy. To obtain high-quality parallel data,
we introduce a crowdsourcing paradigm
in which workers with only basic bilin-
gual proficiency identify translations from
an automatically extracted corpus of par-
allel microblog messages. For less than
$350, we obtained over 5000 parallel seg-
ments in five language pairs. Evaluated
against expert annotations, the quality of
the crowdsourced corpus is significantly
better than existing automatic methods:
it obtains an performance comparable to
expert annotations when used in MERT
tuning of a microblog MT system; and
training a parallel sentence classifier with
it leads also to improved results. The
crowdsourced corpora will be made avail-
able in http://www.cs.cmu.edu/
~lingwang/microtopia/.
1 Introduction
High-quality parallel data is essential for tun-
ing and evaluating statistical MT systems, and
it plays a role in a wide range of multilingual
NLP applications, such as word sense disambigua-
tion (Gale et al., 1992; Ng et al., 2003; Specia
et al., 2005), paraphrasing (Bannard and Callison-
burch, 2005; Ganitkevitch et al., 2012), annota-
tion projection (Das and Petrov, 2011), and other
language-specific applications (Schwarck et al.,
?
A sample of the crowdsourced corpora and the inter-
faces used are available as supplementary material.
2010; Liu et al., 2011). While large amounts
of parallel data can be easily obtained by mining
the web (Resnik and Smith, 2003), comparable
corpora (Munteanu and Marcu, 2005), and even
social media sites (Ling et al., 2013), automati-
cally extracted parallel tends to be noisy, and, as a
result, ?evaluation-quality? parallel corpora have
generally been produced at considerable expense
by targeted translation efforts (Bojar et al., 2013,
inter alia). Unfortunately, in some domains such
as microblogs, the only corpora that are available
are automatically extracted and noisy.
While phrase-based translation models can ef-
fectively learn translation rules from noisy parallel
data (Goutte et al., 2012), having a subset of high-
quality parallel segments is nevertheless crucial.
Firstly, the automatic parallel data extraction sys-
tem?s parameters can be tuned by optimizing on
the gold standard data. Secondly, even though the
parallel data used to train MT systems can contain
a considerable amount of noise, it is conventional
to use human annotated parallel data to tune and
evaluate the system. Finally, other NLP applica-
tions may not be as noise-robust as MT.
We introduce a new crowdsourcing protocol for
obtaining high-quality parallel data from noisy,
automatically extracted parallel data (?3), focus-
ing on the challenging case of identifying par-
allel data in microblog messages (Ling et al.,
2013). In contrast to previous attempts to use
crowdsourcing to obtain parallel data, in which
workers performed translation (Ambati and Vo-
gel, 2010; Zaidan and Callison-Burch, 2011; Post
et al., 2012; Ambati et al., 2012), our approach
only requires that they identify whether a candi-
date message contains a translation, and if so, what
the spans of the translated segments are. This is
a much simpler task than translation, and one that
can often be completed by workers with only a ba-
sic proficiency in the source and target languages.
For evaluation (?4), we use our protocol to build
426
parallel datasets on a Chinese-English corpus orig-
inally extracted from Sina Weibo and for which we
have expert annotations. This lets us quantify the
effectiveness of our method under different task
variations. We also show that the crowdsourced
corpus performs as well as expert annotation (and
better than the automatically extracted corpus) for
tuning an MT system with MERT. We next apply
our method on a corpus of five language pairs (en-
ar, en-ja, en-ko, en-ru, en-zh) extracted from Twit-
ter (?5), for which we have no gold-standard data.
Using this data in a cross-validation setup, we train
and evaluate a maxent classifier for detecting par-
allel data (?6), and then we conclude (?7).
2 Related Work
Our work crosses crowdsourcing techniques and
automatic parallel data extraction from mi-
croblogs. In this section, we shall provide back-
ground information and analysis of the work per-
formed in these two fields.
2.1 Parallel Data Extraction from Microblogs
Many sources of parallel data exist on the
web. The most popular choice are parallel web
pages (Resnik and Smith, 2003), while other
work have looked at specific domains with large
amounts of data, such as Wikipedia (Smith et
al., 2010). Microblogs, such as Twitter and Sina
Weibo, represent a subdomain of the Web. Some
of its characteristics is the informal language used
and the short nature of the messages that are
posted. Due to its large size and growing pop-
ularity, work has been done on parallel data ex-
traction from this domain. Ling et al. (2013) at-
tempt to find naturally occurring parallel data from
Sina Weibo and Twitter. Some examples of what
is found are illustrated in Figure 1. The extrac-
tion process starts by finding the parallel segments
within the same message and the word alignments
between those segments that maximize a hand-
tuned model score.
Another method (Jehl et al., 2012) leverages
CLIR (Cross Lingual Information Retrieval) tech-
niques to find pairs of tweets that are translations.
The main challenge in this approach is the large
amount of pairs of tweets that must be considered,
which raises some scalability issues when process-
ing billions of tweets.
Our crowdsourcing method can be applied to
annotate data from any naturally occurring source.
In this paper, we will use the corpus developed
by Ling et al. (2013), since it is publicly available
and has parallel data for 6 languages from Twitter,
and for 10 languages from Sina Weibo.
2.2 Parallel Data using Crowdsourcing
Most of the work done in building parallel data
using crowdsourcing (Ambati and Vogel, 2010;
Zaidan and Callison-Burch, 2011; Post et al.,
2012; Ambati et al., 2012) relies on using crowd-
sourcing workers to translate. These methods
must address the fact that workers may produce
poor and sometimes incorrect translations. Thus,
in order to find good translations, subsequent
postediting and/or ranking is generally necessary.
In contrast, in our work, crowdsourcing is used
for data extraction rather than translation, a sub-
stantially simpler task than translation (in particu-
lar, translation of informal text) that requires less
expertise in the language pair (basic proficiency in
the two languages is generally sufficient to suc-
cessfully complete the task). Furthermore, assess-
ing whether a worker performed the task correctly
and combining the outputs of different workers is
simpler. The time spent per item is also reduced:
our annotation interface only requires the worker
to make a few clicks on the tweet to complete
each annotation, meaning that tasks are completed
faster and with less effort, allowing us to obtain
translations at lower cost. On the other hand,
the main drawback of our method is that it can
only obtain parallel data from translations that ex-
ist, which corresponds to the amount of posts that
have been translated and posted. This limits the
potential coverage of our method. Furthermore,
the resulting datasets may not be fully representa-
tive of the Twitter domain, since not all types of
content are translated and follow the same distri-
bution as the data in Twitter.
3 Proposed Crowdsourcing Protocol
As discussed above, automatically extracted par-
allel is often noisy. The sources of error range
from language detection errors, to errors determin-
ing if material is actually translation, and errors in
extracting the appropriate spans of the translated
material. Consider the fragment of the microblog
parallel corpus mined by Ling et al. (2013), which
is shown in Figure 1. In the Korean-English mes-
sage, the system may incorrectly added the un-
translated word Hahah in the English segment,
427
and missed the translated word Weather. At a high
level, the task faced by annotators will be to iden-
tify and resolve such errors.
3.1 Overview
We separate the tasks of identifying the parallel
posts, which we shall denote by identification,
and of locating the parallel segments, which we
will call location. The justification for this is that
the majority of the tweets are not parallel, as re-
ported by Ling et al. (2013), and the location of
the parallel data is only applicable if the tweet
actually contains parallel data. This is also de-
sirable because the identification task is simpler
than the location task. Firstly, identifying whether
a tweet contains translations requires much less
proficiency in the respective languages than locat-
ing the parallel segments, since it only requires
the worker to understand parts of the message.
This means we can have more potential workers
capable of performing this task. Secondly, the
first task is a binary decision, and each annota-
tion can be completed with only one action, which
means that the average required time for this task
is much lower than the second task and the pay-
ment required for each hit will naturally be lower
as well. Finally, combining worker results for a
binary decision is simpler than combining transla-
tions, since the space of possible answers is sev-
eral orders of magnitude lower.
As crowdsourcing platform, we use Amazon?s
Mechanical Turk. In this platform, the requesters
can submit tasks, where one can define the num-
ber of workers n that will complete each task and
what is the payment p for each task submission,
henceforth denoted as job. In our work, we had to
consider the following components:
? Interface - To submit a task, an interface
must be provided, which workers will be us-
ing to complete the job.
? Worker Quality Prediction - After submit-
ting a job, the requester can accept and pay
the agreed fee or reject the task. It is cru-
cial to have a method to automatically pre-
dict whether workers have performed the job
properly, and reject them otherwise.
? Result Combination - It is common for mul-
tiple workers to complete the same task with
different results. Thus, a method must be im-
plemented to combine multiple responses for
correctly predicting the desired response.
We structured each of our tasks as a series of q
questions, which include a small number of refer-
ences r, for which we know the answers. Thus,
the amount of answers we obtain for each dollar is
given by
q?r
np
, where n is the number of workers
per task and p is the payment for each task. In or-
der to maximize this quotient, we can either reduce
the number of reference question r, the number of
workers per task n, or the payment p. However,
reducing r will also limit our capability of esti-
mating the quality of the worker results, since we
will have less data to make such prediction. For
the same reason, reducing n will limit our abil-
ity to combine results properly. As for the pay-
ment p, while there is no direct effect on our task,
it has been noted that workers will perform the
task faster for higher payments (Post et al., 2012).
In our work, we will propose methods to predict
quality and combine results that will minimize the
requirements for n and r, while maximizing the
quality of the final results.
3.2 Parallel Post Identification
In the identification task, for each question, we
will show a post, and solicit the worker to detect if
it contains translations in a given language pair.
Interface The interface for this task is straight-
forward. We present to the worker each tweet in-
dividually, together with a checkbox to be checked
in case the tweet contains parallel data. The navi-
gation between tweets is done by adding next and
previous buttons, allowing the user to go back and
review previous answers. Finally, the worker can
only submit the HIT after traversing all 25 ques-
tions. Unlike the work in crowdsourcing transla-
tion (Zaidan and Callison-Burch, 2011), where au-
tomatic translation systems are discouraged, since
it produces poor output, we allow its usage as long
as this leads to correct annotations. In fact, we add
a button to automatically translate the tweet into
English from the non-English language.
Worker Quality Prediction We accept the job
if it answers enough reference questions correctly.
We consider two different approaches to select ref-
erences. A random sampler that selects tweets
randomly and a balanced sampler that selects
the same number of positive and negative sam-
ples. As notation, we will denote as acceptor
428
Figure 1: Parallel microblog posts in 5 language pairs. Shaded backgrounds mark the parallel segments
(annotated manually), non shaded parts do not have translations.
accept(rand, c, r) a setup where the worker?s job
is accepted if c out of r randomly sampled refer-
ences are correctly answered. Likewise, acceptor
accept(bal, c, r) denotes the same setup using bal-
anced reference questions.
Result Combination Given n jobs with answers
for a question that can be either positive or nega-
tive, we calculate the weighted ratio of positive an-
swers, given by
?
i=1..n
?
p
(i)w(i)
?
i=1..n
w(i)
, where ?
p
is one if
answer i is positive and 0 otherwise, and w(i) is
the weight of the worker. w(i) is defined as the
ratio of correct answers from job i in the reference
set. If the weighted ratio is higher than 0.5, we la-
bel the tweet as positive and otherwise as negative.
3.3 Parallel Data Location
In the location task, we also present one tweet per
question, where the worker will be asked to iden-
tify the parallel segments. The worker can also
define that there are no translations in the tweet.
Interface The interface for this task presents the
user with one tweet at a time, and allows the user
to break the tweet into segments, by clicking be-
tween characters. Each segment can then be clas-
sified as English, the non-English language (Ex:
Mandarin), or non-parallel, which is the default
option. To understand the concept of non-parallel
segments, notice that when we are locating par-
allel data in tweets, we are essentially breaking
the tweet into the structure ?N
left
P
left
N
middle
P
right
N
right
", where P
left
and P
right
are the par-
allel segments and N
left
, N
middle
and N
right
are
textual segments that are non-parallel. These may
not exist, for instance, the Arabic tweet in Fig-
ure 1 (line 1) does not contain any non-parallel text
and does not require any non-parallel segments
to delineate the parallel data. The Korean tweet
(line 2), on the other hand, has an N
middle
corre-
sponding to????????????????????????????* and an
N
right
corresponding to Hahah and requires two
non-parallel segments to locate the parallel data.
Thus, if the worker does not commit any errors,
each question can be answered with at most four
clicks, when all five segments exist, and two op-
tion choices for identifying the parallel segments.
In the easiest case, when only the parallel seg-
ments exist, only one click and two option choices
are needed. If there are no translations, the button
no translations can be clicked.
For instance, to annotate the Korean tweet in
Figure 1, the worker must click immediately be-
fore????, then before Weather and finally before
Hahah. Then on the drop-down box of the first
and and third segments, the worker must choose
Korean and English, respectively. The interface
after these operations is show in Figure 2.
Work Quality Prediction To score the worker?s
jobs, we use the scoring function devised in (Ling
et al., 2013), which measures the word overlap
between the reference parallel segments segments
and the predicted segments. However, setting the
score threshold to accept a job is a challenge, since
scores are bound to change for different language-
pairs and domains. Moreover, some tweets are
harder to annotate than others. Learning this
threshold automatically requires annotated data,
which we do not have for all language pairs and
domains. Thus, we propose a method to generate
thresholds specifically for each sample.
We consider a ?smart but lazy" pseudo worker,
who will complete the same jobs automatically
and generate scores that the real worker?s jobs
must beat to be accepted. We say he is ?smart",
429
Figure 2: Location Interface (After the annotation is performed)
since he knows the reference annotation, and
?lazy" because he will only define a new non-
parallel segment if it is significant, otherwise it
will just be left in the parallel segments. By sig-
nificant, we will define whether it is at least 20%
larger (in number of characters) than the parallel
segments. For instance, in the Korean example in
Figure 1, Hahah would be left in the English par-
allel segment, while ???????????????????????
??? ??* would not be in the Korean segment. We
will accept a job if the average of the scores in the
reference set is higher or equal than the pseudo
worker?s scores. This acceptor shall be denoted as
accept(lazy, a), where a is the number of refer-
ences used.
Another option is to use the automatic system?s
output as a baseline that workers must improve to
be accepted. We will also test this option and call
this acceptor accept(auto, a).
Result Combination Unlike the identification
task, where the result is binary and combining
multiple decisions is straightforward, the range of
results from this task is larger and combining them
is a challenge. Thus, we score each job based on
the WER on the reference set and use annotations
of the highest scoring job.
4 Experiments
To obtain results on the effectiveness of the meth-
ods described in Section 3, we will first perform
experiments using pre-annotated data. We use the
annotated dataset with tweets in Mandarin-English
from Sina Weibo created in (Ling et al., 2013).
It consists of approximately 4000 tweets crawled
from Sina Weibo that were annotated on whether
they contained parallel data and the location of the
parallel segments. In our experiment, we sample
1000 tweets from this dataset, where 602 tweets
were parallel and 398 were not.
1
We will not submit the same tasks using differ-
ent setups, since we would have to pay the cost of
the tasks multiple times. Furthermore, we know
the answers for all the questions in this controlled
experiment, the quality of a job can be evalu-
ated precisely by using all questions as references.
Thus, we will perform the task once, with a larger
number of workers and accepting and rejecting
jobs based on their real quality. Then, we will use
the resulting datasets and simulate the conditions
using different setups.
430
Acceptor avg(a) avg(r) d
accept(rand, 2, 2) 0.44 0.00 0.44
accept(rand, 3, 4) 0.44 0.00 0.44
accept(rand, 4, 4) 0.55 0.04 0.51
accept(bal, 2, 2) 0.69 0.09 0.60
accept(bal, 3, 4) 0.64 0.03 0.61
accept(bal, 4, 4) 0.76 0.15 0.61
Table 1: Agreement with the expert annotations
for different acceptors.
4.1 Identification Task
The 1000 tweets were distributed into 40 tasks
with 25 questions each (q = 25). Each task is
to be performed by 5 workers (n = 5) and upon
acceptance, a worker would be rewarded with 6
cents (p = 0.06). As we know the answers for
all the questions in this case, we will calculate the
Cohen?s Kappa between the responses of each job
and the expert annotator, and accept a job if it is
higher than 0.5. We decided to use Cohen?s kappa
to evaluate a job, rather than accuracy, since each
set of 25 questions does not contain the same num-
ber of positive and negative samples. For instance,
in a set of 20 negative samples, a worker would
achieve an accuracy of 80% if he simply answers
negatively to all questions, which is not an ade-
quate assessment of the job?s quality. On the other
hand, the Cohen?s Kappa balances the positive and
negative question in each task by using their prior
probabilities. In total, there were 566 jobs, where
200 where accepted and 366 were rejected.
Next, we pretended that we only have access to
4 references, which will be used for quality es-
timation and simulate the acceptances and rejec-
tions for each strategy. Table 1 shows the aver-
ages of the real Kappa values of accepted (col-
umn avg(a)) and rejected jobs (column avg(r))
using different acceptors. Our goal is to maximize
the number of acceptances with high Kappa val-
ues and minimize those that have low Kappa val-
ues. Thus, we define d as the difference between
avg(a) and avg(r). From the results, we observe
that using a balanced reference yields a much bet-
ter estimation of the jobs quality using our metric
d. Similar conclusions can be reached by compar-
ing accept(rand, 3, 4) with accept(bal, 3, 4) and
accept(rand, 4, 4) with accept(bal, 4, 4). Quality
predictors that use balanced reference sets achieve
1
We wished to annotate a sample where the number of
parallel posts is high, so that we would have enough samples
to perform the location task.
Acceptor prec recall F1 acc ?
Automatic 0.87 0.69 0.77 0.75 0.51
All jobs 0.75 0.84 0.8 0.74 0.44
accept(rand, 2, 2) 0.85 0.92 0.88 0.86 0.69
accept(rand, 3, 4) 0.84 0.93 0.88 0.85 0.68
accept(rand, 4, 4) 0.91 0.95 0.93 0.92 0.82
accept(bal, 2, 2) 0.94 0.94 0.94 0.92 0.84
accept(bal, 3, 4) 0.93 0.95 0.94 0.93 0.85
accept(bal, 4, 4) 0.94 0.93 0.93 0.92 0.84
Table 2: Parallel post prediction scores using dif-
ferent acceptors.
approximately the same results for d. However,
the setup accept(bal, 3, 4) has a lower Kappas for
both avg(a) and avg(r), which means that it is
less likely to reject good jobs at the cost of accept-
ing more bad jobs. This is desirable from an ethi-
cal perspective, since workers are not responsible
for errors in our quality prediction. Furthermore,
rejecting good jobs has a negative impact on the
progress of the task, since good workers may be
discouraged to perform more tasks.
Results on the identification task, obtained for
n = 3, are shown in Table 2. Naturally, us-
ing a balanced reference set yields better results,
since these have a higher d value. We can also
see the importance of quality prediction, since not
performing quality estimation (row All jobs) will
yield worse results than the automatic system.
Next, we will compare results using different
numbers of workers. We fix the quality predic-
tion methodology to accept(bal, 3, 4) and results
are shown in Table 3. We observe that in gen-
eral, using more workers will generate better re-
sults, but score gains from adding another worker
becomes lower as n increases. One problem for
n = 2 is the fact that there are many cases where
two workers with the same weight chose a posi-
tive and a negative answer, in which case, no de-
cision can be made, and we simply choose false
by default. This explains the high recall and low
precision values. However, this problem seems to
occur much less with higher values of n.
4.2 Location Task
For the location task, we used the predicted par-
allel posts the identification task with the setup
accept(bal, 3, 4) and n = 5. We preferred to use
this rather than using the expert annotations, since
it would not contain false positives, which does not
simulate a real situation. Then, we used 500 out of
431
# workers prec recall F1 acc ?
Automatic 0.87 0.69 0.77 0.75 0.51
1 0.86 0.85 0.85 0.82 0.64
2 0.85 0.95 0.90 0.87 0.72
3 0.93 0.95 0.94 0.93 0.85
4 0.94 0.96 0.95 0.94 0.87
5 0.96 0.96 0.96 0.95 0.90
Table 3: Identification scores for different n.
the 607 identified positive samples. This makes
20 tasks in total, with 25 questions (q = 25), and
each task would be run until 5 jobs are accepted
(n = 5). For this task, we set a payment of 30
cents (p = 0.3), since it is a more complex task.
Again, since we have the expert annotations for all
questions, we calculated the average WER on all
answers and rejected jobs scoring less than 0.6
2
.
This task is mainly focused on the quality pre-
diction of the workers, as the result combination
is done by finding the job with the highest score
in the reference set. This means, for an arbitrary
large n, all quality estimation methods will pro-
duce the same result, since we will find the best
job on the references eventually. However, bet-
ter quality estimation will allow us to find the best
jobs with lower n, which makes the task less ex-
pensive. Table 4 shows results using different se-
tups. In these results, we set aside 4 questions to
be used as references. We can see that for low n
(1 or 2), if we simply accept all jobs, the quality
of the results will be lower than the automatic sys-
tem. For n = 4, this approach can achieve a WER
score of 0.06. However, if we use the automatic
system as a baseline that jobs must surpass, we can
achieve this WER score with only two jobs, which
reduces the cost of this task by half. Yet, this is
strongly dependent on the automatic system, as a
worse system will be easier to match for the work-
ers. On the other hand, using the smart but lazy
pseudo worker, where we degrade the reference
annotations slightly, we can see that we can obtain
the 0.06 WER score using only the first worker. At
n = 2, we can see that the WER improves to 0.05,
which is lost for n = 3. This is because the pre-
diction of the quality of the job using the workers
is not always precise.
4.3 Machine Translation Results
Finally, we will perform an extrinsic test to see
how the improvements obtained by using crowd-
2
Determined empirically
Number of jobs 1 2 3 4 5
Automatic 0.16 0.16 0.16 0.16 0.16
All Jobs 0.23 0.21 0.07 0.06 0.06
accept(auto, 4) 0.09 0.06 0.06 0.06 0.06
accept(lazy, 4) 0.06 0.05 0.06 0.06 0.06
Table 4: Parallel data location scores for different
acceptors (rows) and different numbers of work-
ers. Each cell denotes the WER for that setup.
Auto (Pos) Crowd Expert Auto (All)
Size 483 479 483 908
EN-ZH 10.21 10.49 10.51 10.71
ZH-EN 7.59 7.87 7.82 8.02
Table 5: BLEU score comparison using different
corpora for MERT tuning. The Size row denotes
the number of sentences of each corpus, and the
EN-ZH and ZH-EN rows denote the BLEU scores
of the respective language pair and tuning dataset.
sourcing map to Machine Translations. We will
build an out of domain MT system using the FBIS
dataset (LDC2003E14), a corpus of 300K sen-
tence pairs from the news domain in the Chinese-
English pair using the Moses (Koehn et al., 2007)
pipeline. Due to the small size of our crowd-
sourced corpus, we will use it in the MERT tun-
ing (Och, 2003), and test its effects compared to
automatically extracted parallel data and the ex-
perts judgements. As the test set, we will use
1,500 sentence pairs from the Weibo gold standard
from Ling et al. (2013), that were not used in our
crowdsourcing experiment to prevent data over-
lap. For reordering, we use the MSD reordering
model (Axelrod et al., 2005) and as the language
model, we use a 5-gram model with Kneser-Ney
smoothing (Heafield, 2011). Finally, results are
presented with BLEU-4 (Papineni et al., 2002).
We build 3 tuning corpora, the automatically ex-
tracted corpus (denoted Auto), the crowdsourced
corpus (denoted Crowd) and the corpus annotated
by the expert (denoted Expert). This is done by
taking the 1000 tweets used in this experiment, se-
lect those that were identified as parallel accord-
ing to each criteria. For the automatic extraction,
the authors in (Ling et al., 2013) simply use all
tweets as parallel, which may influence the tun-
ing results. Thus, we test two versions of this cor-
pus, one where we take all samples as parallel (de-
noted Auto (All)), and one where we use the ex-
pert?s decision for the identification task only (de-
432
Pair Parallel Avg(en) cost(I) cost(L) total
en-ar 1512 8.3 $35.7 $43.2 $76.2
en-zh 1302 8.7 $35.7 $37.2 $70.2
en-ja 1155 7.9 $35.7 $33.0 $68.7
en-ko 1008 7.1 $35.7 $28.8 $64.5
en-ru 798 6.3 $35.7 $22.8 $58.5
all 5775 ? $178.5 $165.0 $343.5
Table 7: AMT costs for crowdsourced corpora
from Twitter.
noted Auto (Pos)). In the crowdsourcing case, we
use the accept(bal, 3, 4) setup, with n = 5, for the
identification task and the accept(lazy, 4) setup,
with n = 2, for the location task. From the re-
sulting parallel tweets, we also remove all tweets
that were used as reference in the accept(lazy, 4)
quality estimator, as this would give an unfair ad-
vantage to the crowdsourced corpora.
Results are shown in Table 5, where each cell
contains the average BLEU score in 5 MERT runs,
using a different tuning dataset. Surprisingly, us-
ing the whole set of automatically extracted cor-
pora actually achieves better results than using
carefully selected data that are parallel. We be-
lieve that is because many non-parallel segments
actually contain comparable information that can
be used to improve the weights during MERT tun-
ing. However, this does not mean that the qual-
ity of the automatically crawled corpus is better
than the crowdsourced and expert annotated cor-
pus. When using a similar number of parallel sen-
tences, we observe that using the crowdsourced
corpus yields better scores than the automatically
extracted corpora, comparable to experts annota-
tions. While results are not significantly better
than automatically extracted corpora, this suggests
that the crowdsourced corpora has a better overall
quality than automatically extracted corpora.
5 Five Language Twitter Parallel Corpus
Now that we have established the effectiveness of
our technique for extracting high-quality parallel
data in a scenario where we have gold standard
annotations, we apply it to creating parallel cor-
pora in five languages on Twitter, for which we
have no gold-standard parallel data: Arabic, Man-
darin, Japanese, Korean and Russian. Once again,
we use the extracted automatically Twitter cor-
pus from Ling et al. (2013) and deploy the task
in Mechanical Turk. We use the setup that ob-
tained the best results in Section 4. For the identi-
fication task, we used the accept(bal, 3, 4) setup,
with n = 5. The payment for each task was
0.06 dollars. Thus, for this task, each dollar spent
yields 70 annotated tweets. For the location task,
we used the accept(lazy, 4) setup, with n = 2
and each task was rewarded with 0.3 dollars. To
obtain the tweet sample, we filtered the corpora
in Ling et al. (2013) for tweets with alignment
scores higher than 0.1. Then, we uniformly ex-
tracted 2500 tweets for each language. To gener-
ate gold standard references, the authors manually
annotated 40 samples for each pair.
Table 7 contains information about the result-
ing corpora. The number of parallel sentences ex-
tracted from the 2500 tweets in each language pair
is shown in column Parallel and we can see that
this differs given the language pair. We can also
see in column Avg(en) that the average number of
English words is much smaller than what is seen
in more formal domains. Finally, Arabic parallel
data seems more predominant from our samples
followed by Mandarin, while Russian parallel data
seem scarcer.
6 Discriminative Parallel Data Detection
While the work in (Ling et al., 2013) used a linear
combination of three models, the alignment, lan-
guage and segment features, these weights were
determined manually. However, using the crowd-
sourced corpus (in Section 5), we will apply previ-
ously proposed methods that learn a classifier with
machine learning techniques as in related work
on finding parallel data (Resnik and Smith, 2003;
Munteanu and Marcu, 2005). In our work, we use
a max entropy classifier model, similar to that pre-
sented by Munteanu and Marcu (2005) to detect
parallel data in tweets. Our features are:
? Alignment feature - The baseline feature is
the alignment score from the work in (Ling et
al., 2013), and measures how well the paral-
lel segments align, which is derived from the
content-based matching methods for detect-
ing parallel data (Resnik and Smith, 2003).
? User features - An observation in (Ling et
al., 2013) is that a user that frequently posts
in parallel is likely to post more parallel mes-
sages. Based on this, we added the aver-
age alignment score from all messages of the
same user and the ratio of messages that are
predicted to be parallel as features.
433
Weibo (en-zh) Twitter (en-zh) Twitter (en-ar) Twitter (en-ru) Twitter (en-ko) Twitter (en-ja)
Alignment 0.781 0.599 0.721 0.692 0.635 0.570
+User 0.814 0.598 0.721 0.705 0.650 0.566
+Length 0.839 0.603 0.725 0.706 0.650 0.569
+Repetition 0.849 0.652 0.763 0.729 0.655 0.579
+Language 0.849 0.668 0.782 0.737 0.747 0.584
Table 6: Classification Results using a 10-fold cross validation over different datasets. Each cell contains
the F-measure using a given dataset and an incremental set of features.
? Repetition features - There are many words
that are not translated, such as hashtags, at
mentions, numbers and named entities. So, if
we see these repeated twice in the same post,
it can be used as a strong cue that this was
the result of a translation. Hence, we define
features for each of these cases, that trigger if
either of these occur in multiples of two times
in the same post. Named Entities were iden-
tified using a naive approach by considering
words with capital letters.
? Length feature - It is known that the length
differences between parallel sentences can
be modelled by a normal distribution (Gale
and Church, 1991). Hence, we used parallel
data in the respective language to determine
(??, ??
2
), which lets us calculate the likelihood
of two hypothesized segments being parallel.
Since we did not have annotated parallel data
for this domain, we used the top 2000 scoring
parallel sentences from the respective Twitter
dataset in (Ling et al., 2013).
? Language feature - It is common for non-
English words to be found in English seg-
ments, such as names of foreign celebri-
ties, numbers and hashtags. However, when
this happens to the majority of the words in
a segment that is supposed to be English,
it may indicated that there was an error in
the language detection. The same happens
with non-English segments. We used the
same naive approach to detect languages as
in (Ling et al., 2013), where we calculate the
ratio of number of words in the English seg-
ment and the total number of words from the
segment detected as English and the ratio of
the number of Foreign words and the total
number of words in the Foreign segment ,de-
tected by their unicode ranges. This was also
included in the work in (Ling et al., 2013).
Results using a 10 fold cross-validation are
shown in Table 6. In general, we can see that the
classifier performs worse in Twitter datasets com-
pared to the Weibo dataset. We believe that this is
because parallel sentences extracted from Twitter
are smaller, due to the 140 character limit, which
does not hold in Sina Weibo. Each parallel En-
glish segment from the Sina Weibo parallel data
contains 15.4 words on average. On other hand,
we see in Table 7 that this number is smaller in
the parallel data from Twitter. This means that the
aligner will have a much smaller range of words to
align when detecting parallel data, which makes it
more difficult to find parallel segments.
As for the features, we observe that by defin-
ing these simple features, we can get a signifi-
cant improvement over previous baselines. For
the User feature, we see that the improvements
in the Weibo dataset are much larger than in
the Twitter datasets. This is because the Twitter
dataset was crawled uniformly, whereas the Weibo
dataset was focused on users that post parallel
data frequently. Thus, in the Weibo dataset there
more posts that were posted by the same user,
which does not happen as frequently in the Twitter
dataset. As for the Length feature, we can see that
it yields a small but consistent improvement over
all datasets. Repetition based features also lead to
improvements across all datasets, and produces a
5% improvement in the English-Mandarin Twitter
dataset. Finally, language based features also add
another improvement over previous results.
7 Conclusions
We presented a crowdsourcing approach to extract
parallel data from tweets. As opposed to meth-
ods to crowdsource translations, our tasks do not
require workers to translate sentences, but to find
them in tweets. Our method is divided into two
tasks. First, we identify which tweets contain
translations, and we show that multiple worker?s
jobs can be combined to obtain results compara-
434
ble to those of expert annotators. Secondly, tweets
that are found to contain translations are given
to other workers to locate the parallel segments,
where we can also obtain high quality results.
Then, we use our method to extract high quality
parallel data from Twitter in 5 language pairs. Fi-
nally, we improve the automatic identification of
tweets with translations by using a max entropy
classifier trained on the crowdsourced data.
We are currently extracting more data and the
crowdsourced parallel data from Twitter will made
be available to the public.
References
[Ambati and Vogel2010] Vamshi Ambati and Stephan
Vogel. 2010. Can crowds build parallel corpora
for machine translation systems? In Proceedings
of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, Stroudsburg, PA, USA. Association for
Computational Linguistics.
[Ambati et al.2012] Vamshi Ambati, Stephan Vogel,
and Jaime Carbonell. 2012. Collaborative workflow
for crowdsourcing translation. In Proceedings of the
ACM 2012 Conference on Computer Supported Co-
operative Work, CSCW ?12, pages 1191?1194, New
York, NY, USA. ACM.
[Axelrod et al.2005] Amittai Axelrod, Ra Birch Mayne,
Chris Callison-burch, Miles Osborne, and David
Talbot. 2005. Edinburgh system description for the
2005 iwslt speech translation evaluation. In Pro-
ceedings International Workshop on Spoken Lan-
guage Translation (IWSLT.
[Bannard and Callison-burch2005] Colin Bannard and
Chris Callison-burch. 2005. Paraphrasing with
bilingual parallel corpora. In In ACL-2005, pages
597?604.
[Bojar et al.2013] Ond
?
rej Bojar, Christian Buck, Chris
Callison-Burch, Christian Federmann, Barry Had-
dow, Philipp Koehn, Christof Monz, Matt Post,
Radu Soricut, and Lucia Specia. 2013. Find-
ings of the 2013 Workshop on Statistical Machine
Translation. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 1?
44, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
[Das and Petrov2011] Dipanjan Das and Slav Petrov.
2011. Unsupervised part-of-speech tagging with
bilingual graph-based projections. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies - Volume 1, HLT ?11, pages 600?609,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
[Gale and Church1991] William A. Gale and Ken-
neth W. Church. 1991. A program for aligning
sentences in bilingual corpora. In Proceedings of
the 29th Annual Meeting on Association for Com-
putational Linguistics, ACL ?91, pages 177?184,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
[Gale et al.1992] William A. Gale, Kenneth W. Church,
and David Yarowsky. 1992. Using bilingual materi-
als to develop word sense disambiguation methods.
[Ganitkevitch et al.2012] Juri Ganitkevitch, Yuan Cao,
Jonathan Weese, Matt Post, and Chris Callison-
Burch. 2012. Joshua 4.0: Packing, PRO, and para-
phrases. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 283?291,
Montr?al, Canada, June. Association for Computa-
tional Linguistics.
[Goutte et al.2012] Cyril Goutte, Marine Carpuat, and
George Foster. 2012. The impact of sentence
alignment errors on phrase-based machine transla-
tion performance. In Proc. of AMTA.
[Heafield2011] Kenneth Heafield. 2011. KenLM:
faster and smaller language model queries. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 187?197, Edinburgh, Scot-
land, United Kingdom, July.
[Jehl et al.2012] Laura Jehl, Felix Hieber, and Stefan
Riezler. 2012. Twitter translation using translation-
based cross-lingual retrieval. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 410?421, Montr?al, Canada, June. Asso-
ciation for Computational Linguistics.
[Koehn et al.2007] Philipp Koehn, Hieu Hoang,
Alexandra Birch, Chris Callison-burch, Richard
Zens, Rwth Aachen, Alexandra Constantin, Mar-
cello Federico, Nicola Bertoldi, Chris Dyer, Brooke
Cowan, Wade Shen, Christine Moran, and Ondrej
Bojar. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume
Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic, June.
Association for Computational Linguistics.
[Ling et al.2013] Wang Ling, Guang Xiang, Chris Dyer,
Alan Black, and Isabel Trancoso. 2013. Microblogs
as parallel corpora. In Proceedings of the 51st An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?13. Association for Computational
Linguistics.
[Liu et al.2011] Feifan Liu, Fei Liu, and Yang Liu.
2011. Learning from chinese-english parallel data
for chinese tense prediction. In IJCNLP, pages
1116?1124.
[Munteanu and Marcu2005] Dragos Munteanu and
Daniel Marcu. 2005. Improving machine transla-
tion performance by exploiting comparable corpora.
Computational Linguistics, 31(4):477?504.
435
[Ng et al.2003] Hwee Tou Ng, Bin Wang, and Yee Seng
Chan. 2003. Exploiting parallel texts for word sense
disambiguation: An empirical study. In Proceedings
of ACL03, pages 455?462.
[Och2003] Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics - Volume 1, ACL ?03,
pages 160?167, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Papineni et al.2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
method for automatic evaluation of machine trans-
lation. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 311?318, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
[Post et al.2012] Matt Post, Chris Callison-Burch, and
Miles Osborne. 2012. Constructing parallel cor-
pora for six indian languages via crowdsourcing. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 401?409, Montr?al,
Canada, June. Association for Computational Lin-
guistics.
[Resnik and Smith2003] Philip Resnik and Noah A.
Smith. 2003. The web as a parallel corpus. Compu-
tational Linguistics, 29:349?380.
[Schwarck et al.2010] Florian Schwarck, Alexander
Fraser, and Hinrich Sch?tze. 2010. Bitext-based
resolution of german subject-object ambiguities. In
Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
?10, pages 737?740, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
[Smith et al.2010] Jason R. Smith, Chris Quirk, and
Kristina Toutanova. 2010. Extracting parallel sen-
tences from comparable corpora using document
level alignment. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 403?411, Stroudsburg, PA,
USA. Association for Computational Linguistics.
[Specia et al.2005] Lucia Specia, Maria Das Gra?as,
Volpe Nunes, and Mark Stevenson. 2005. Exploit-
ing parallel texts to produce a multilingual sense
tagged corpus for word sense disambiguation. In
Proceedings of RANLP-05, Borovets, pages 525?
531.
[Zaidan and Callison-Burch2011] Omar F. Zaidan and
Chris Callison-Burch. 2011. Crowdsourcing trans-
lation: professional quality from non-professionals.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies - Volume 1, HLT ?11, pages
1220?1229, Stroudsburg, PA, USA. Association for
Computational Linguistics.
436
