Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1392?1401, Dublin, Ireland, August 23-29 2014.
An Empirical Evaluation of Automatic Conversion from Constituency to
Dependency in Hungarian
Katalin Ilona Simk
?
o
1
, Veronika Vincze
1,2
, Zsolt Sz
?
ant
?
o
1
, Rich
?
ard Farkas
1
1
University of Szeged
Department of Informatics
2
MTA-SZTE Research Group on Artificial Intelligence
kata.simko@gmail.com
{vinczev,szantozs,rfarkas}@inf.u-szeged.hu
Abstract
In this paper, we investigate the differences between Hungarian sentence parses based on auto-
matically converted and manually annotated dependency trees. We also train constituency parsers
on the manually annotated constituency treebank and then convert their output to dependency
trees. We argue for the importance of training on gold standard corpora, and we also demon-
strate that although the results obtained by training on the constituency treebank and converting
the output to dependency format and those obtained by training on the automatically converted
dependency treebank are similar in terms of accuracy scores, the typical errors made by different
systems differ from each other.
1 Introduction
Nowadays, two popular approaches to data-driven syntactic parsing are based on constituency grammar
on the one hand and dependency grammar on the other hand. There exist constituency-based treebanks
for many languages and dependency treebanks for most of these languages are converted automatically
from constituent trees with the help of conversion rules, which is the case for e.g. the languages used in
the SPMRL-2013 Shared Task (Seddah et al., 2013) with the exception of Basque, where constituency
trees are converted from manually annotated dependency trees (Aduriz et al., 2003), and Hungarian,
where both treebanks are manually annotated (Csendes et al., 2005; Vincze et al., 2010). However, the
quality of automatic dependency conversion is hardly investigated.
Hungarian is one of those rare examples where there exist manual annotations for both constituency
and dependency syntax on the same bunch of texts, the Szeged (Dependency) Treebank (Csendes et al.,
2005; Vincze et al., 2010), which makes it possible to evaluate the quality of a rule-based automatic con-
version from constituency to dependency trees, to compare the two sets of manual annotations and also
the output of constituency and dependency parsers trained on converted and gold standard dependency
trees.
We investigate the effect of automatic conversions related to the two parsing paradigms as well. It is
well known that for English, the automatic conversion of a constituency parser?s output to dependency
format can achieve competitive unlabeled attachment scores (ULA) to a dependency parser?s output
trained on automatically converted trees
1
(cf. Petrov et al. (2010)). One of the possible explanations for
this is that English is a configurational language, hence constituency parsers have advantages over depen-
dency parsers here. We check whether this hypothesis holds for Hungarian too, which is the prototype
of free word order languages.
In this paper, we compare three pairs of dependency analyses in order to evaluate the usefulness
of converted trees. First, we examine the errors of the conversion itself by comparing the converted
dependency trees with the manually annotated gold standard ones. Second, we argue for the importance
of training parsers on gold standard trees by looking at the typical differences between the outputs of
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
However, it has been pointed out that errors in the conversion script may significantly influence the results of parsing, see
e.g. Petrov and McDonald (2012) and Pitler (2012)
1392
dependency parsers trained on converted (silver standard) trees, parsers trained on gold standard trees and
the manual annotation itself. Third, we demonstrate that similar to English, training on a constituency
treebank and converting the results to dependency format can achieve similar results in terms of ULA to
the dependency parser trained on the automatically converted treebank, but the typical errors they make
differ in both cases.
2 Parsing Hungarian on the Szeged Treebank
Hungarian is a morphologically rich language, where word order encodes information structure, which
makes its syntactic analysis very different from English?s as the arguments in a sentence cannot be
determined by their position but by their suffixes, cf.
?
E. Kiss (2002). Words? grammatical functions
are signified by case suffixes and verbs are marked for the number and person of their subject and the
definiteness of their object, thus these arguments may be often omitted from the sentence: L?atlak (see-
1SG2OBJ) ?I see you?. Due to word order reasons, words that form one syntactic phrase may not be
adjacent (long-distance dependencies), which is true for the possessive construction as well: the posses-
sor and the possessed may be situated in two distant positions: A fi?unak elvette a kalapj?at (the boy-DAT
take-PAST-3SGOBJ the hat-POSS3SG-ACC) ?He took the boy?s hat?. Verbless clauses are also com-
mon in Hungarian, as the copula in third person singular present tense indicative form is phonologically
empty, while it is present in all other moods and tenses: A kalap piros (the hat red) ?The hat is red?, but
A kalap piros volt (the hat red was) ?The hat was red?.
The Szeged Treebank (Csendes et al., 2005) is a manually annotated constituency treebank for Hun-
garian consisting of 82,000 sentences. Besides the phrase structure, grammatical roles of the verbs?
arguments and morphological information are also annotated. It incorporates texts from six different
domains: short business news, newspaper, law, literature, compositions and informatics, however, in this
paper, we just focus on the short business news domain.
The Szeged Dependency Treebank (Vincze et al., 2010) contains manual dependency syntax annota-
tions for the same texts. Certain linguistic phenomena ? such as discontinuous structures ? are annotated
in this treebank, but not in the constituency treebank. In the dependency treebank, the possessor is linked
to the possession while this connection is not annotated in the constituency treebank. The two types of
trees can be seen in Figure 1.
CP
PUNC
.
NP-ACC
N
kalapj?at
T
a
V
V0
V
elvette
NP-GEN
N
fi?unak
T
A
A fi?unak elvette a kalapj?at .
ROOT
DET
GEN
DET
OBJ
PUNCT
Figure 1: Discontinuous structure A fi?unak elvette a kalapj?at (the boy-DAT take-past3SGOBJ the hat-
POSS3SG-ACC) ?He took the boy?s hat? in constituency and dependency analysis.
Another difference between the two treebanks is the way they represent different types of complex
sentences, as can be seen in Figure 2. In the dependency treebank subordinations and coordinations are
1393
handled very similarly. The head of one of the clauses (the subordinated clause or the second clause in
the case of coordination) is linked to the head of the other clause (the matrix clause of the subordination
or the first clause of the coordination), only the type of relation between the two heads differs in the
two structures, in the dependency tree in Figure 2, the heads of the three clauses (?atj?ott ?came over?,
meg??g?erte ?promised? and elj?on ?come?) are linked to one another through their conjunctions with either
an ATT relation in the case of subordination or COORD for coordination. In the constituency treebank
these sentences are represented very differently: in the case of subordination, the subordinated clause is
within the matrix clause: CP
3
is within CP
2
in the constituency tree in Figure 2. Coordinated clauses
appear at the same level in the structure, in the same figure CP
1
and CP
2
are coordinated clauses.
CP
PUNC
.
CP
2
CP
3
elj?on velem
C0
hogy
PUNC
,
V
meg??g?erte
C0
?es
CP
1
?
Atj?ott hozz?am
?
Atj?ott hozz?am ?es meg??g?erte , hogy elj?on velem .
ROOT
OBL
CONJ
COORD
PUNCT
CONJ
ATT
OBL
PUNCT
Figure 2: Constituency and dependency analysis of coordination and subordination in the sentence
?
Atj?ott
hozz?am ?es meg??g?erte, hogy elj?on velem (through.come-PAST-3SG to.me and promise-PAST-3SG-OBJ
that away.come-3SG with.me) ?He came over and promised that he will come with me?.
The parallels of these two manually annotated treebanks make them suitable for testing our hypotheses
about automatic dependency conversion. The differences between them originate from the characteristics
of constituent and dependency syntax.
3 Converting Constituency Trees to Dependency Trees
In this section, we present our methods to convert constituency trees to dependency trees and we also
discuss the most typical sources of errors during conversion.
3.1 Conversion rules
In order to convert constituency trees to dependency trees, we used a rule based system. Sentences with
virtual dependency nodes were omitted, as they are not annotated in the constituent treebank and their
treatment in dependency trees is also problematic (Farkas et al., 2012; Seeker et al., 2012). As a result,
we worked with 7,372 sentences and 162,960 tokens.
First, we determined the head of each clause (CP) and the relations between CPs in complex sentences.
In most cases the head of the CP is a finite verb, if the CP contains no finite verb, the head is the either an
infinitive verb or a participle, if none of these are present in the CP, the head can be a nominal expression.
The relations between the CP heads make up the base of the dependency structure using ROOT relation
for the sentence?s main verb, COORD for coordination and ATT for subordination, as well as CONJ in
the case of conjunctions between the CPs.
1394
The arguments of verbs, infinitives and participles in the CP were linked to their governor and marked
for their grammatical role in the Szeged Treebank. We used this information to construct the appropriate
dependency relations between governors and their arguments. The main grammatical roles such as sub-
ject, object, dative have their own label in dependency syntax, while minor ones are assigned the oblique
(OBL) relation. The argument?s modifiers were then linked to the head or other modifiers based on the
phrase structure with relations according to their morphological code.
Long distance dependencies, like the connection between a genitive case possessor and the possessed
are not annotated in the constituency treebank. In these cases we used morphological information to link
these elements together in the dependency tree. Figure 3 shows an example of converting a constituency
tree to a dependency tree.
CP
PUNC
.
V
V0
V
volt
NEG
R
nem
NP
N
?uzletk?ot?es
NP
N
h?uspiacon
T
A
A h?uspiacon ?uzletk?ot?es nem volt .
ROOT
DET
OBL
SUBJ
NEG
PUNCT
Figure 3: Conversion of the sentence A h?uspiacon ?uzletk?ot?es nem volt (the meat.market-SUP transaction
not was) ?There were no transactions at the meat market.? from constituency to dependency trees.
3.2 Error Analysis
We automatically converted the constituency treebank into dependency trees following the prin-
ciples described above and detailed at our website (http://www.inf.u-szeged.hu/rgai/
SzegedTreebank). For evaluation, we applied the metrics labeled attachment score (LAS) and un-
labeled attachment score (ULA), without punctuation marks. The accuracy of the conversion was 96.51
(ULA) and 93.85 (LAS). The errors made during conversion were categorized manually in 200 sentences
selected randomly from the short business news subcorpus of the Szeged Dependency Treebank, and the
most typical ones are listed in Table 1, Column convError.
As it is shown, the most common source of error was when more than one modifier was within a
phrase as the example in Figure 4 shows. In each figure, the gold standard parse can be seen on the left
hand side while the erroneous one can be seen on the right hand side.
eur?opai , olcs?o utakat k??n?al?o l?egit?arsas?ag
ATT
PUNCT
ATT
OBJ
ATT
eur?opai , olcs?o utakat k??n?al?o l?egit?arsas?ag
ATT
PUNCT
ATT
OBJ
COORD
Figure 4: Multiple modifier error in eur?opai, olcs?o utakat k??n?al?o l?egit?arsas?ag (European cheap trips-
ACC offering airline) ?European airline offering cheap trips?.
1395
Error type convError goldTrain silverTrain BerkeleyConv convDep
# % # % # % # % # %
Coordination 26 13.00 39 13.22 59 14.82 55 16.37 64 19.57
Multiple modifiers 26 13.00 30 10.17 49 12.31 52 15.48 47 14.37
Determiner 7 3.50 28 9.49 25 6.28 31 9.23 31 9.48
Conj./adverb attached 33 16.50 23 7.80 45 11.31 39 11.61 42 12.84
Arg. of verbal element 10 5.00 27 9.15 34 8.54 59 17.56 44 13.46
Sub- vs. coordination 7 3.50 9 3.05 12 3.02 ? ? ? ?
Possessor 9 4.50 14 4.75 16 4.02 28 8.33 22 6.73
Wrong root 14 7.00 17 5.76 23 5.78 35 10.42 27 8.26
Consecutive nouns 4 2.00 11 3.73 14 3.52 13 3.87 15 4.59
Multiword NE 8 4.00 25 8.47 33 8.29 8 2.38 19 5.81
Wrong MOD label 25 12.50 26 8.81 34 8.54 ? ? ? ?
Wrong other label 17 8.50 33 11.19 30 7.54 ? ? ? ?
Other errors 14 7.00 13 4.41 24 6.03 16 4.76 16 4.89
Total 200 100 295 100 398 100 336 100 327 100
Table 1: Error Types. convError: errors made during converting constituency trees to dependency trees.
goldTrain: errors in the output got by training the Bohnet parser on the gold standard data. silverTrain:
errors in the output got by training the Bohnet parser on the silver standard data. BerkeleyConv: errors in
the output got by training the Berkeley parser on the gold standard constituency data and converting the
output into dependency format. convDep: errors in the output got by training the Bohnet parser without
dependency labels on the silver standard data.
Coordination errors occurred when multiple members of a coordination were wrongly connected. On
the other hand, the attachment of conjunctions and some adverbs was also problematic, for example in
Figure 5 the conjunction is ?also? is connected to the verb in the gold standard and to the noun in the
converted version.
a miniszt?erium is besz?all
DET
SUBJ
CONJ
a miniszt?erium is besz?all
DET
SUBJ
CONJ
Figure 5: Conjunction attachment error in a miniszt?erium is besz?all (the ministry also steps.in) ?the
ministry also steps in?.
Also, the constituency treebank did not mark all the grammatical relations (e.g. numerals and deter-
miners were simply parts of an NP but had no distinct labeling, like [NP az ?ot [ADJP fekete] kutya]
(the five black dog) ?the five black dogs?), but it was necessary to assign them a dependency label and
a parent node during conversion. However, in some cases it was not straightforward which modifier
modifies which parent node: for instance, in [NP nem [ADJP megfelel?o] m?odszerek] (not appropriate
methods) ?inappropriate methods?, the negation word nem is erroneously attached to the noun instead of
the adjective in the converted phrase. Determiner errors were those where the determiner was attached
to the wrong noun in a NP with a noun modifier. In CPs with multiple verbal elements (both a finite verb
and an infinitive or a participle in the CP) the arguments were sometimes linked to the wrong verb, as in
Figure 6.
1396
a saj?at pecseny?ej?ukkel voltak elfoglalva
DET
ATT
OBL
MODE
a saj?at pecseny?ej?ukkel voltak elfoglalva
DET
ATT
OBL
MODE
Figure 6: Verbal argument error in a saj?at pecseny?ej?ukkel voltak elfoglalva (the own roast-3PLPOSS-INS
were busy) ?they were busy with their own thing?.
Possessors are sometimes wrongly identified during conversion as long distance dependencies are not
marked in the constituency treebank (see Figure 7).
a gy?art?o sz?ar??t?o?uzem?eben hasznos??t
DET
SUBJ
OBL
a gy?art?o sz?ar??t?o?uzem?eben hasznos??t
DET
ATT
OBL
Figure 7: Possessor attachment error in a gy?art?o sz?ar??t?o?uzem?eben hasznos??t (the manufacturer
drying.plant-3SGPOSS-INE utilizes) ?the manufacturer utilizes it in its drying plant?.
In CPs with more verbal element, sometimes the wrong word is selected as the root, as in Figure 8.
a tenderre jelentkezett m?asik aj?anlattev?o ?erv?enytelen p?aly?azatot ny?ujtott be
ROOT
DET
OBL
ATT
ATT
SUBJ
ATT
OBJ
PREVERB
a tenderre jelentkezett m?asik aj?anlattev?o ?erv?enytelen p?aly?azatot ny?ujtott be
ROOT
DET
OBL
COORD
ATT
SUBJ
ATT
OBJ
PREVERB
Figure 8: Root error in a tenderre jelentkezett m?asik aj?anlattev?o ?erv?enytelen p?aly?azatot ny?ujtott be (the
tender-SUB applied other bidder invalid application-ACC submit-PAST-3SG) ?the other bidder applying
to the tender submitted an invalid application?.
In some cases, consecutive (but separate) noun phrases were taken as one unit as if one noun modified
the other, for example in Figure 9.
a tervezettn?el t?obb munkahelyet sz?untet meg
DET
OBL
ATT
OBJ
PREVERB
a tervezettn?el t?obb munkahelyet sz?untet meg
DET
OBL
ATT
OBJ
PREVERB
Figure 9: Consecutive noun error in a tervezettn?el t?obb munkahelyet sz?untet meg (the planned-ADE more
workplace-ACC terminates) ?it terminates more workplaces than planned?.
Multiword NEs also caused some problems in the conversion, as in Figure 10.
1397
Besz?all??t?oi Befektet?o Rt.
NE
NE
Besz?all??t?oi Befektet?o Rt.
ATT
NE
Figure 10: Multiword NE error in Besz?all??t?oi Befektet?o Rt. (a name of a company) .
In other cases, divergences between the gold standard and the converted trees are due to some erro-
neous annotations either in the constituency treebank or in the dependency treebank. A typical example
of this is the wrong MOD (modifier) label. In the treebank, locative and temporal modifiers were classi-
fied according to the tridirectionality typical of Hungarian adverbs and case suffixes: where, from where
and to where (or when, from what time and till what time) the action is taken place. Thus, there are
six dependency relations dedicated to these aspects and all the other adverbials are grouped under the
relation MOD. However, this distinction is rather semantic in nature and was sometimes erroneously
annotated in the constituency treebank, which was later corrected in the dependency one and thus now
resulted in conversion errors, as shown in Figure 11.
ny?ar v?ege fel?e kezdik
ATT
ATT
MODE
ny?ar v?ege fel?e kezdik
ATT
ATT
TO
Figure 11: MOD label error in ny?ar v?ege fel?e kezdik (summer end-3SGPOSS around begin) ?they begin
around the end of the summer?.
There were also some atypical errors that occurred too rarely to categorize them in a different class,
like cases when an article or determiner got erroneously attached to a verb and so on, so they were
lumped into the category of ?other errors? in Table 1.
4 Training on Gold Standard and Silver Standard Trees
We also experimented with training the Bohnet dependency parser (Bohnet, 2010) on the manually an-
notated (gold standard) and the converted (silver standard) treebank. The Bohnet parser (Bohnet, 2010)
is a state-of-the-art
2
graph-based parser, which employs online training with a perceptron. The parser
contains a feature function for the first order factor, one for the sibling factor, and one for the grandchil-
dren.
From the corpus, 5,892 sentences (130,211 tokens) were used in the training dataset and the remaining
1,480 sentences (32,749 tokens) in the test dataset. For evaluation, we again applied the metrics LAS
and ULA. Results are shown in Table 2, Rows goldTrain and silverTrain.
As the numbers show, better results can be achieved when the gold standard data are used as training
database than when the parser is trained on the silver standard data, the differences being 1.6% (ULA)
and 3.16% (LAS). Besides evaluation scores, we also compared the outputs of the two scenarios: we
used the same set of randomly selected sentences as when investigating conversion errors and carried out
a manual error analysis against the gold standard data in each case: see Table 1, Columns goldTrain and
silverTrain.
There are some common error types that seem to cause problems for both ways of parsing. For
instance, coordination and multiple modifiers are among the most frequent sources of errors in both
cases as for the error rates are concerned. However, with regard to the absolute numbers, we can see
that both error types are reduced when the gold standard dataset is used for training. On the other hand,
finding the parent node of a conjunction or an adverb seems to improve significantly when the parser is
trained on gold standard data. This is probably due to the fact that they are not marked in the constituency
treebank and thus training data for these grammatical phenomena are very noisy in the silver standard
treebank. All in all, we argue that there are some grammatical phenomena ? e.g. the attachment of
2
For a comparative evaluation with other dependency parsers on the same treebank see Farkas et al. (2012). According to
their results, the Bohnet parser achieved the best scores on the treebank hence we also used this parser in our experiments.
1398
Setting LAS ULA
Conversion 93.85 96.51
goldTrain 93.48 95.17
silverTrain 90.32 93.57
BerkeleyConv ? 92.78
convDep ? 93.23
Table 2: Results of the experiments. Conversion: converting constituency trees to dependency trees.
goldTrain: training the Bohnet parser on the gold standard data. silverTrain: training the Bohnet parser
on the silver standard data. BerkeleyConv: training the Berkeley parser on the gold standard constituency
data and converting the output into dependency format. convDep: training the Bohnet parser without
dependency labels on the silver standard data.
conjunctions or adverbs ? that require manual checking even if automatic conversion from constituency
to dependency is applied.
5 Pre- or Post Conversion?
It is well known that for English, converting a constituency parser?s output to dependency format (post
conversion) can achieve competitive ULA scores to a dependency parser?s output trained on automati-
cally converted trees (pre conversion) (Petrov et al., 2010; Farkas and Bohnet, 2012). One of the pos-
sible reasons for this may be that English is a configurational language, hence constituency parsers are
expected to perform better here. In this paper, we investigate whether this is true for Hungarian, which
is the prototype of morphologically rich languages with free word order.
We employed the product-of-grammars procedure (Petrov, 2010) of the Berkeleyparser (Petrov et al.,
2006), where grammars are trained on the same dataset but with different initialization setups, which
leads to different grammars. We trained 8 grammars and used tree-level inference. The output of the
parser was then automatically converted to dependency format, based on the rules described in Section
3 (BerkeleyConv). Second, we used the silver standard dependency treebank for training the Bohnet
parser (convDep). Since our constituency parser did not produce grammatical functions for the nodes,
we trained the Bohnet parser on unlabeled dependency trees in order to ensure a fair comparison here
(that is the difference between the columns BerkeleyConv and convDep in Table 1).
As the numbers show, competitive results can be obtained with both methods, yielding an ULA score
of 92.78 and 93.23, respectively. This means that the same holds for Hungarian as for English and the
surprisingly good results of post conversion are not related to the configurational level of the language.
Manually analysing the errors on the same set of sentences as before, there are again some error cate-
gories that occur frequently in both cases such as coordination, the attachment of conjunctions, modifiers
and determiners. On the other hand, training on constituency trees seems to have some specific sources
of errors. First, the possessor in possessive constructions is less frequently attached to its possessed,
which may be due to the fact that the genitive possessor is not linked to the possessed in the constituency
treebank and thus the parser is not able to learn this relationship. Second, arguments of verbal elements
(i.e. verbs, participles and infinitives) are also somewhat more difficult to find when there are at least two
verbal elements within the clause, which is especially true for adverbial participles and infinitives. In
Figure 6, the differences between the two trees are shown. The noun pecseny?ej?ukkel (roast-3PLPOSS-
INS) ?with their thing? is linked to the adverbial participle in the correct analysis, but it connects to the
main verb in the other. Third, identifying the root node of the sentence may also be problematic for this
setting. As Farkas and Bohnet (2012) reported that preconversion can achieve better results for finding
the root node in English, this seems to be a language-specific issue and it represents an interesting differ-
ence between English and Hungarian. Nevertheless, training on constituency trees has a beneficial effect
on finding multiword named entities. Hence, it can be concluded that although the evaluation scores are
similar, the errors the two systems make differ from each other.
1399
6 Discussion and Conclusions
Here, we compared dependency analyses of Hungarian obtained in different ways. It was revealed that
although the accuracy scores are similar to each other, each system makes different types of errors. On
the other hand, there are some specific linguistic phenomena that seem to be difficult for dependency
parsing generally as they were among the most frequent sources of errors in each case (e.g. coordination,
multiple modifiers and the attachment of conjunctions and adverbs).
Converting constituency trees into dependency trees enabled us to experiment with a silver standard
dependency corpus as well. Our results empirically showed that better results can be achieved on the
gold standard corpus, hence manual annotation of dependency trees is desirable. However, when there
is no access to manually annotated dependency data, converting the output of a constituency parser into
dependency format or training the dependency parser on converted data may also be viable: similar to
English, both solutions result in competitive scores but the errors the systems make differ from each
other.
In the future, we would like to investigate how the advantages of constituency and dependency repre-
sentations may be further exploited in parsing Hungarian and we also plan to carry out some uptraining
experiments with both types of parsers.
Acknowledgements
This work was supported in part by the European Union and the European Social Fund through the
project FuturICT.hu (grant no.: T
?
AMOP-4.2.2.C-11/1/KONV-2012-0013).
References
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arriola, Aitziber Atutxa, A. Diaz de Ilarraza, Aitzpea Garmendia,
and Maite Oronoz. 2003. Construction of a Basque dependency treebank. In Proceedings of the 2nd Workshop
on Treebanks and Linguistic Theories (TLT), pages 201?204, V?axj?o, Sweden.
Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd
International Conference on Computational Linguistics (Coling 2010), pages 89?97.
D?ora Csendes, J?anos Csirik, Tibor Gyim?othy, and Andr?as Kocsor. 2005. The Szeged TreeBank. In V?aclav
Matousek, Pavel Mautner, and Tom?as Pavelka, editors, Proceedings of the 8th International Conference on
Text, Speech and Dialogue, TSD 2005, Lecture Notes in Computer Science, pages 123?132, Berlin / Heidelberg,
September. Springer.
Katalin
?
E. Kiss. 2002. The Syntax of Hungarian. Cambridge University Press, Cambridge.
Rich?ard Farkas and Bernd Bohnet. 2012. Stacking of dependency and phrase structure parsers. In Proceedings of
COLING 2012, pages 849?866, Mumbai, India, December. The COLING 2012 Organizing Committee.
Rich?ard Farkas, Veronika Vincze, and Helmut Schmid. 2012. Dependency Parsing of Hungarian: Baseline Re-
sults and Challenges. In Proceedings of the 13th Conference of the European Chapter of the Association for
Computational Linguistics, pages 55?65, Avignon, France, April. Association for Computational Linguistics.
Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 shared task on parsing the web. Notes of the First
Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable
tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational Linguistics, pages 433?440.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and Hiyan Alshawi. 2010. Uptraining for accurate determin-
istic question parsing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing, pages 705?713, Cambridge, MA, October. Association for Computational Linguistics.
Slav Petrov. 2010. Products of random latent variable grammars. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages
19?27, Los Angeles, California, June. Association for Computational Linguistics.
1400
Emily Pitler. 2012. Conjunction representation and ease of domain adaptation. Notes of the First Workshop on
Syntactic Analysis of Non-Canonical Language (SANCL).
Djam?e Seddah, Reut Tsarfaty, Sandra K?ubler, Marie Candito, Jinho D. Choi, Rich?ard Farkas, Jennifer Foster,
Iakes Goenaga, Koldo Gojenola Galletebeitia, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann,
Wolfgang Maier, Yuval Marton, Joakim Nivre, Adam Przepi?orkowski, Ryan Roth, Wolfgang Seeker, Yannick
Versley, Veronika Vincze, Marcin Woli?nski, and Alina Wr?oblewska. 2013. Overview of the SPMRL 2013
shared task: A cross-framework evaluation of parsing morphologically rich languages. In Proceedings of the
Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 146?182, Seattle, Washing-
ton, USA, October. Association for Computational Linguistics.
Wolfgang Seeker, Rich?ard Farkas, Bernd Bohnet, Helmut Schmid, and Jonas Kuhn. 2012. Data-driven depen-
dency parsing with empty heads. In Proceedings of COLING 2012: Posters, pages 1081?1090, Mumbai, India,
December. The COLING 2012 Organizing Committee.
Veronika Vincze, D?ora Szauter, Attila Alm?asi, Gy?orgy M?ora, Zolt?an Alexin, and J?anos Csirik. 2010. Hungarian
Dependency Treebank. In Proceedings of LREC 2010, Valletta, Malta, May. ELRA.
1401
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 135?144,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Special Techniques for Constituent Parsing of Morphologically Rich
Languages
Zsolt Sz
?
ant
?
o, Rich
?
ard Farkas
University of Szeged
Department of Informatics
{szanto,rfarkas}@inf.u-szeged.hu
Abstract
We introduce three techniques for improv-
ing constituent parsing for morphologi-
cally rich languages. We propose a novel
approach to automatically find an optimal
preterminal set by clustering morphologi-
cal feature values and we conduct exper-
iments with enhanced lexical models and
feature engineering for rerankers. These
techniques are specially designed for mor-
phologically rich languages (but they are
language-agnostic). We report empirical
results on the treebanks of five morpho-
logically rich languages and show a con-
siderable improvement in accuracy and in
parsing speed as well.
1 Introduction
From the viewpoint of syntactic parsing, the
languages of the world are usually categorized
according to their level of morphological rich-
ness (which is negatively correlated with config-
urationality). At one end, there is English, a
strongly configurational language while there is
Hungarian at the other end of the spectrum with
rich morphology and free word order (Fraser et al.,
2013). A large part of the methodology for syn-
tactic parsing has been developed for English but
many other languages of the world are fundamen-
tally different from English. In particular, mor-
phologically rich languages ? the other end of the
configurational spectrum ? convey most sentence-
level syntactic information by morphology (i.e. at
the word level), not by configuration. Because of
these differences the parsing of morphologically
rich languages requires techniques that differ from
or extend the methodology developed for English
(Tsarfaty et al., 2013). In this study, we present
three techniques to improve constituent parsing
and these special techniques are dedicated to han-
dle the challenges of morphologically rich lan-
guages.
Constituency parsers have advanced consider-
ably in the last two decades (Charniak, 2000;
Charniak and Johnson, 2005; Petrov et al., 2006;
Huang, 2008) boosted by the availability of the
Penn Treebank (Marcus et al., 1993). While
there is a progress on parsing English (especially
the Penn Treebank), the treebanks of morphologi-
cally rich languages have been attracted much less
attention. For example, a big constituent treebank
has been available for Hungarian for almost 10
years (Csendes et al., 2005) and to the best of
our knowledge our work is the first one report-
ing results on this treebank. One reason for the
moderate level of interest in constituent parsing of
morphologically rich languages is the widely held
belief that dependency structures are better suited
for representing syntactic analyses for morpho-
logically rich languages than constituent represen-
tations because they allow non-projective struc-
tures (i.e. discontinuous constituents). From a
theoretical point of view, Tsarfaty et al. (2010)
point out, however, this is not the same as prov-
ing that dependency parsers function better than
constituency parsers for parsing morphologically
rich languages. For a detailed discussion, please
see Fraser et al. (2013).
From an empirical point of view, the organiz-
ers of the recent shared task on ?Statistical Pars-
ing of Morphologically Rich Languages? (Seddah
et al., 2013) provided datasets only for languages
having treebanks in both dependency and con-
stituency format and their cross-framework evalu-
ation ? employing the unlabeled TedEval (Tsarfaty
et al., 2012) as evaluation procedure ? revealed
that at 4 out of 9 morphologically rich languages,
the results of constituent parsers were higher than
the scores achieved by the best dependency pars-
ing system. Based on these theoretical issues and
empirical results, we support the conclusion of
135
Fraser et al. (2013) that ?... there is no clear
evidence for preferring dependency parsing over
constituency parsing in analyzing languages with
rich morphology and instead argue that research
in both frameworks is important.?
In this study, we propose answers to the two
main challenges of constituent parsing of mor-
phologically rich languages, which are finding the
optimal preterminal set and handling the huge
number of wordforms. The size of the pretermi-
nal set in the standard context free grammar envi-
ronment is crucial. If we use only the main POS
tags as preterminals, we lose a lot of information
encoded in the morphological description of the
tokens. On the other hand, using the full mor-
phological description as preterminal yields a set
of over a thousand preterminals, which results in
data sparsity and performance problems as well.
The chief contribution of this work is to propose a
novel automatic procedure to find the optimal set
of preterminals by merging morphological fea-
ture values. The main novelties of our approach
over previous work are that it is very fast ? it
operates inside a probabilistic context free gram-
mar (PCFG) instead of using a parser as a black
box with re-training for every evaluation of a fea-
ture combination ? and it can investigate particular
morphological feature values instead of removing
a feature with all of its values.
Another challenge is that because of the inflec-
tional nature of morphologically rich languages
the number of wordforms is much higher com-
pared with English. Hence the number of
unknown and very rare tokens ? i.e. the tokens
that do not appear in the training dataset ? is
higher here, which hurts the performance of PCFG
parsers. Following Goldberg and Elhadad (2013),
we enhance the lexical model by exploiting an
external lexicon. We investigate the applicabilities
of fully supervised taggers instead of unsupervised
ones for gathering external lexicons.
Lastly, we introduce novel feature templates
for an n-best reranker operating on the top of a
PCFG parser. These feature templates are exploit-
ing atomic morphological features and achieve
improvements over the standard feature set engi-
neered for English.
We conducted experiments by the above men-
tioned three techniques on Basque, French, Ger-
man, Hebrew and Hungarian, five morphologi-
cally rich languages. The BerkeleyParser (Petrov
et al., 2006) enriched with these three techniques
achieved state-of-the-art results on each language.
2 Related Work
Constituent parsing of English is a well researched
area. The field has been dominated by data-driven,
i.e. treebank-based statistical approaches in the
last two decades (Charniak, 2000; Charniak and
Johnson, 2005; Petrov et al., 2006). We extend
here BerkeleyParser (Petrov et al., 2006), which
is a PCFG parser using latent annotations at non-
terminals. Its basic idea is to iteratively split each
non-terminal into subsymbols thus capturing the
different subusage of them instead of manually
designed annotations.
The constituent parsing of morphologically
rich languages is a much less investigated field.
There exist constituent treebanks for several lan-
guages along with a very limited number of
parsing reports on them. For instance, Petrov
(2009) trained BerkeleyParser on Arabic, Bulgar-
ian, French, German and Italian and he reported
good accuracies, but there has been previous work
on Hebrew (Goldberg and Elhadad, 2013), Korean
(Choi et al., 1994) and Spanish (Le Roux et al.,
2012) etc. The recently organized ?Statistical Pars-
ing of Morphologically Rich Languages? (Seddah
et al., 2013) addressed the dependency and con-
stituency parsing of nine morphologically rich lan-
guages and provides useful benchmark datasets
for these languages.
Our chief contribution in this paper is a pro-
cedure to merge preterminal labels. The related
work for this line of research includes the studies
on manual refinement of preterminal sets such as
Marton et al. (2010) and Le Roux et al. (2012).
The most closely related approach to our proposal
is Dehdari et al. (2011), who defines metaheuris-
tics to incrementally insert or remove morphologi-
cal features. Their approach uses parser ? training
and parsing ? as a black box evaluation of a preter-
minal set. In contrast, our proposal operates as a
submodule of the BerkeleyParser, hence does not
require the re-training of the parser for every pos-
sible preterminal set candidate, thus it is way more
faster.
The most successful supervised constituent
parsers contain a second feature-rich discrimina-
tive parsing step (Charniak and Johnson, 2005;
Huang, 2008; Chen and Kit, 2012) as well. At
the first stage they apply a PCFG to extract pos-
136
Basque French German Hebrew Hungarian
#sent. in training 7577 14759 40472 5000 8146
#sent. in dev 948 1235 5000 500 1051
#sent. in test 946 2541 5000 716 1009
avg. token/sent. 12.92 30.13 17.51 25.33 21.76
#non-terminal labels 3000 770 994 1196 890
#main POS labels 16 33 54 46 16
unknown token ratio (dev) 18.35% 3.22% 6.34% 19.94% 19.94%
Table 1: Basic statistics of the treebanks used.
sible parses. The n-best list parsers keep just
the 50-100 best parses according to the PCFG
(Charniak and Johnson, 2005). These methods
employ a large feature set (usually a few mil-
lion features) (Collins, 2000; Charniak and John-
son, 2005). These feature sets are engineered for
English. In this study, we introduce feature tem-
plates for exploiting morphological information
and investigate their added value over the standard
feature sets.
3 Experimental Setup
We conducted experiments on the treebanks of
the 2013 shared task on ?Statistical Parsing of
Morphologically Rich Languages? (Seddah et al.,
2013). We used the train/dev/test splits of the
shared task?s Basque (Aduriz et al., 2003), French
(Abeill?e et al., 2003), Hebrew (Sima?an et al.,
2001), German (Brants et al., 2002) and Hun-
garian (Csendes et al., 2005) treebanks. Table 1
shows the basic statistics of these treebanks, for
a more detailed description about their annotation
schemata, domain, preprocessing etc. please see
Seddah et al. (2013).
As evaluation metrics we employ the PARSE-
VAL score (Abney et al., 1991) along with the
exact match accuracy (i.e. the ratio of perfect
parse trees). We use the evalb implementation of
the shared task
1
.
4 Enhanced Lexical Models
Before introducing our proposal and experiments
with preterminal set optimisation, we have to offer
a solution for the out-of-vocabulary (OOV) prob-
lem, which ? because of the inflectional nature ?
is a crucial problem in morphologically rich lan-
1
Available at http://pauillac.inria.fr/
?
seddah/evalb_spmrl2013.tar.gz. An important
change in this version compared to the original evalb is the
penalization of unparsed sentences.
guages. We follow here Goldberg and Elhadad
(2013) and enhance a lexicon model trained on the
training set of the treebank with frequency infor-
mation about the possible morphological analyses
of tokens. We estimate the tagging probability
P (t|w) of the tag t given the word w by
P (t|w) =
{
P
tb
(t|w), if c(w) ? K
c(w)P
tb
(t|w)+P
ex
(t|w)
1+c(w)
, otherwise
where c(w) is the count of w in the training set,
K is predefined constant, P
tb
(t|w) is the proba-
bility estimate from the treebank (the relative fre-
quency with smoothing) and P
ex
(t|w) is the prob-
ability estimate from an external lexicon. We
calculate the emission probabilities P (w|t) from
the tagging probabilities P (t|w) by applying the
Bayesian rule.
The key question here is how to construct the
external lexicon. For a baseline, Goldberg and
Elhadad (2013) suggest using the uniform dis-
tribution over all possible morphological analy-
ses coming from a morphological analyser (?uni-
form?).
Goldberg and Elhadad (2013) also report con-
siderable improvements over the ?uniform? base-
line by relative frequencies counted on a large
corpus which was automatically annotated in the
unsupervised POS tagging paradigm (Goldberg
et al., 2008). Here we show that even a super-
vised morphological tagger without a morpho-
logical analyzer can achieve the same level of
improvement. We employ MarMot
2
(Mueller
et al., 2013) for predicting full morphological
analysis (i.e. POS tags and morphological fea-
tures jointly). MarMot is a Conditional Random
Field tagger which incrementally creates forward-
backward lattices of increasing order to prune the
2
https://code.google.com/p/cistern/
137
sizable space of possible morphological analy-
ses. We used MarMoT with the default param-
eters. This purely data-driven tagger achieves a
tagging accuracy of 97.6 evaluated at full mor-
phological analyses on the development set of the
Hungarian treebank, which is competitive with the
state-of-the-art Hungarian taggers which employ
language-specific rules (e.g. magyarlanc (Zsibrita
et al., 2013)). The chief advantage of using Mar-
Mot instead of an unsupervised tagger is that the
former does not require any morphological lex-
icon/analyser (which can lists the possible tags
for a given word). This morphological lexi-
con/analyser is language-dependent, usually hand-
crafted and it has to be compatible with the tree-
bank in question. In contrast, a supervised mor-
phological tagger can build a reasonable tagging
model on the training part of the treebanks ? espe-
cially for morphologically rich languages, where
the tag ambiguity is generally low ? thus each of
these problems is avoided.
Table 2 shows the results of various P
ex
(t|w)
estimates on the Hungarian development set. The
first row ?BerkeleyParser? is our absolute base-
line, i.e. the original implementation of Berke-
leyParser
3
defining signatures for OOVs. For
the ?uniform? results, we used the morphologi-
cal analyser module of magyarlanc (Zsibrita et al.,
2013). The last two rows show the results achieved
by training MarMot on the treebank?s training
dataset, having tagged the development set plus
a huge unlabeled corpus (10M sentences from the
Hungarian National Corpus (V?aradi, 2002)) with it
then having counted relative tag frequencies. We
report scores on only using the frequencies from
the development set (?dev?) and from the concate-
nation of the development set and the huge corpus
(?huge?).
After a few preliminary experiments, we set
K = 7 and use this value thereafter.
Table 2 shows that even ?dev? yields a consid-
erable improvement over the baseline parser and
?uniform?. These results are also in line with
the findings of Goldberg and Elhadad (2013), i.e.
?uniform? has some added value and using relative
frequencies gathered from automatically tagged
corpora contributes more. Although we can see
another nice improvement by exploiting unlabeled
corpora (?huge?), we will use the ?dev? setting in
3
http://code.google.com/p/
berkeleyparser/
PARSEVAL EX
BerkeleyParser 87.22 12.75
uniform 87.31 14.78
dev 88.29 15.22
huge 89.27 16.97
Table 2: The results achieved by using various
external lexical models on the Hungarian devel-
opment set.
the experiments of the next sections as we did not
have access to huge, in-domain unlabeled corpora
for each language used in this study.
5 Morphological Feature Values as
Preterminals
Finding the optimal set of morphological features
incorporating into the perterminal labels is cru-
cial for any PCFG parsers. Removing morpho-
logical features might reduce data sparsity prob-
lems while it might lead to loss of information for
the syntactic parser. In this section, we propose
a novel method for automatically finding the opti-
mal set of preterminals then we present empirical
results with this method and compare it to various
baselines.
Merge Procedure for Morphological Feature
Values: There have been studies published on
the automatic reduction of the set of pretermi-
nals for constituent parsing. For instance, Dehdari
et al. (2011) proposed a system which iteratively
removes morphological features as a unit then
evaluates the preterminal sets by running the train-
ing and parsing steps of a black-box constituent
parser. Our motivation here is two-fold. First,
morphological features should not be handled as
a unit because different values of a feature might
behave differently. Take for instance the degree
feature in Hungarian adjectives. Here the val-
ues positive and superlative behave similarly (can
be merged) while distinguishing comparative and
positive+superlative is useful for syntactic pars-
ing because comparative adjectives often have an
argument (e.g. x is more beautiful than y) while
positive and superlative adjectives are not syntac-
tic governors thus have no arguments. Second,
keeping a morphological feature can be useful for
particular POS tags and useless at other particular
POS tags (e.g. the number of possessed in Hun-
garian for nouns and pronouns).
138
Algorithm 1 The preterminal set merger algorithm.
1. training the standard BerkeleyParser using only main POS tags as preterminals
2. merging each subsymbols at the preterminal level
3. for each POS tag - morphological feature pair
(a) split the POS tag for the values of the morphological feature
4
(b) recalculating the rule probabilities where there are preterminals in the right-hand side by uni-
formly distribute the probability mass among subsymbols
(c) set the lexical probabilities according to the relative frequencies of morphological values
counted on gold standard morphological tags of the treebank
(d) running 10 iterations of the Expectation-Maximization procedure on the whole treebank ini-
tialized with (b)-(c)
(e) constructing a fully connected graph whose nodes are the morphological values of the feature
in question
(f) for every edge of the graph, calculate the loss in likehood for the merging the two subsymbols
(the same way as for BerkeleyParser?s merge procedure)
4. removing edges from the entire set of graphs (controlled by the parameter th)
5. merge the morphological values of the graphs? connected components
Based on these observations we propose a pro-
cedure which starts from the full morphological
description of a treebank then iteratively merges
particular morphological feature values and it han-
dles the same feature at the different POS tags sep-
arately. The result of this procedure is a clustering
of the possible values of each morphological fea-
ture. The removal of a morphological feature is a
special case of our approach because if the values
of the feature in question form one single cluster
it does not have any discriminative function any-
more. Hence our proposal can be regarded as a
generalisation of the previous approaches.
This general approach requires much more eval-
uation of intermediate candidate preterminal sets,
which is not feasible within the external black-box
parser evaluation scenario (training and parsing
an average sized treebank by the BerkeleyParser
takes more than 1 hour). Our idea here is that re-
training a parser for the evaluation of each preter-
minal set candidates is not necessary. They key
objective here is to select among preterminal sets
based on their usefulness for the syntactic parser.
This is the motivation of the merge procedure of
the BerkeleyParser. After randomly splitting non-
terminals, BerkeleyParser calculates for each split
the loss in likelihood incurred when merging the
subsymbols back. If this loss is small, the new
annotation does not carry enough useful informa-
tion and can be removed (Petrov et al., 2006). Our
task is the same at the preterminal level. Hence at
the preterminal level, ? instead of using the auto-
matic subsymbol splits of the BerkeleyParser ? we
call this merging procedure over the morpholog-
ical feature values. Algorithm 1 shows our pro-
posal for the preterminal merging procedure.
Baseline Preterminal Set Constructions: The
two basic approaches for preterminal set con-
struction are the use of only the main POS tag
set (?mainPOS?) and the use of the full morpho-
logical description as preterminals (?full?). For
Hungarian, we also had access to a linguistically
motivated, hand-crafted preterminal set (?man-
ual?) which was designed for a morphological tag-
ger (Zsibrita et al., 2013). This manual code set
keeps different morphological features at differ-
ent POS tags and merges morphological values
instead of fully removing features hence it inspired
our automatic merge procedure introduced in the
previous section.
Our last baseline is the repetition of the experi-
ments of Dehdari et al. (2011). For this, we started
from the full morphological feature set and com-
pletely removed features (from all POS) one-by-
one then re-trained our parser. We observed the
greatest drop in PARSEVAL score at removing the
139
Basque French German Hebrew Hungarian
mainPOS 68.8/3.9 16 78.4/13.9 33 82.3/38.7 54 88.3/12.0 46 82.6/7.3 16
full 81.8/18.4 2976 78.9/15.0 676 82.3/40.3 686 88.9/15.2 257 88.3/15.2 680
preterminal merger 81.6/16.9 2791 79.7/15.6 480 82.3/39.3 111 89.0/14.6 181 88.5/15.4 642
Table 3: PARSEVAL / exact match scores on the development sets. The third small numbers in cells
show the size of the preterminal sets.
?Num? feature and the least severe one at remov-
ing ?Form?. ?Num? denotes number for verbs and
nominal elements (nouns, adjectives and numer-
als), and since subject-verb agreement is deter-
mined by the number and person features of the
predicate (the verb) and the subject (the noun),
deleting the feature ?Num? results in a serious
decline in performance. On the other hand, ?Form?
denotes whether a conjunction is single or com-
pound (which is a lexical feature) or whether a
number is spelt with letters, Arabic or Roman
numbers (which is an orthographic feature). It is
interesting to see that their deletion hardly harms
the PARSEVAL scores, moreover, it can even
improve the exact match scores, which is probably
due to the fact that the distinction between differ-
ent orthographic versions of the same number (e.g.
6 and VI) just confused the parser. On the other
hand, members of a compound conjunction are not
attached to each other in any way in the parse tree,
and behave similar to single compounds, so this
distinction might also be problematic for parsing.
Results with Various Preterminal Sets: Table
4 summarizes the results achieved by our four
baseline methods along with the scores of two
preterminal sets output by our merger approach at
two different merging threshold th value.
#pt PARSEVAL EX
mainPOS 16 82.36 5.52
manual 72 85.38 9.23
full 680 88.29 15.22
full - Num 479 87.43 14.49
full - Form 635 88.24 15.73
merged (th = 0.5) 378 88.36 15.92
merged (th = 0.1) 642 88.52 15.44
Table 4: The results achieved by using various
preterminal sets on the Hungarian development
set.
The difference between mainPOS and full is
surprisingly high, which indicates that the mor-
phological information carried in preterminals is
extremely important for the constituent parser and
the BerkeleyParser can handle preterminal sets of
the size of several hundreds. For Hungarian, we
found that the full removal of any feature cannot
increase the results. This finding is contradictory
with Dehdari et al. (2011) in Arabic, where remov-
ing ?Case? yielded a gain of 1.0 in PARSEVAL.
We note that baselines for Arabic and Hungar-
ian are also totally different, Dehdari et al. (2011)
reports basically no difference between mainPOS
and full in Arabic.
We report results of our proposed procedure
with two different merging thresholds. The th =
0.1 case merges only a few morphological feature
values and it can slightly outperform the ?full? set-
ting (statistically significant
5
in exact match.). On
the other hand, the th = 0.5 setting is competitive
with the ?full? setting in terms of parsing accuracy
but it uses only the third of the preterminals used
by ?full?. Although it is not statistically better than
?full? in accuracy, it almost halves the running time
of parsing
6
.
Table 3 summarizes the results achieved by
the most important baselines and our approach
along with the size of the particular preterminal
sets applied. The ?full? results outperform ?main-
POS? at each language with a striking difference at
Basque and Hungarian. These results show that ?
contradictory to the general belief ? the detailed
morphological description is definitely useful in
constituent parsing as well. The last row of the
table contains the result achieved by our merger
approach. Here we run experiments with several
merging threshold th values and show the highest
scores for each language.
Our merging proposal could find a better preter-
minal set than full on French and Hungarian, it
found a competitive tag set in terms of accuracies
5
According to two sample t-test with p<0.001.
6
Parsing the 1051 sentences of the Hungarian develop-
ment set takes 15 and 9 minutes with full and th = 0.5
respectively (on an Intel Xeon E7 2GHz).
140
which are much smaller than full on German and
Hebrew and it could not find any useful merge at
Basque. The output of the merger procedure con-
sists of one sixth of preterminals compared with
full. Manually investigating the clusters, we can
see that it basically merged every morphological
feature except case at nouns and adjectives (but
merged case at personal pronouns). This finding
is in line with the experimental results of Fraser et
al. (2013).
6 Morphology-based Features in n-best
Reranking
n-best rerankers (Collins, 2000; Charniak and
Johnson, 2005) are used as second stage after a
PCFG parser and they usually achieve consider-
able improvement over the first stage parser. They
extract a large feature set to describe the n best
output of a PCFG parser and they select the best
parse from this set (i.e. rerank the parses). Here,
we define feature templates exploiting morpho-
logical information and investigate their added
value for the standard feature sets (engineered for
English). We reimplemented the feature templates
from Charniak and Johnson (2005) and Versley
and Rehbein (2009) excluding the features based
on external corpora and use them as our baseline
feature set.
We used n = 50 in our experiment and fol-
lowed a 5-fold-cross-parsing (a.k.a. jackknifing)
approach for generating unseen parse candidates
for the training sentences (Charniak and Johnson,
2005). The reranker is trained for the maximum
entropy objective function of Charniak and John-
son (2005), i.e. the sum of posterior probabilities
of the oracles. We used a slightly modified version
of the Mallet toolkit for reranking (McCallum,
2002) and L2 regularizer with its default value for
coefficient.
The feature templates of the baseline feature set
frequently incorporate preterminals as atomic fea-
ture. As a first step, we investigated which preter-
minal set is the most useful for the baseline fea-
ture set. We took the 50 best output from the
parser using the merged preterminal set and used
its preterminals (?merged?) or only the main POS
tag (?mainPOS?) as atomic building blocks for the
reranker?s feature extractor. Table 5 shows that
mainPOS outperformed full. This is probably due
to data sparsity problems.
Based on this observation, we decided to use
mainPOS as preterminal in the atomic building
block of the baseline features and designed new
feature templates capturing the information in the
morphological analysis. We experimented with
the following templates:
For each preterminal of the candidate parse and
for each morphological feature value inside the
preterminal we add the pair of wordform and mor-
phological feature value as a new feature. In a sim-
ilar way, we define a reranker feature from every
morphological feature value of the head word of
the constituent. For each head-daughter attach-
ment in the candidate parse we add each pair of the
morphological feature values from the head words
of the attachment?s participants. Similarly we take
each combination of head word?s morphological
features values from sister constituents.
The first two templates enable the reranker to
incorporate information into its learnt model from
the rich morphology of the language at the lexi-
cal and constituent levels, while the last two tem-
plates might capture (dis)agreement at the mor-
phological level. The motivation for using these
features is that because of the free(er) word order
of morphologically rich languages, morphological
(dis)agreement can be a good indicator of attach-
ment.
Table 5 shows the added value of these fea-
ture templates over mainPOS (?extended?), which
is again statistically significant in exact match.
Exploiting the morphological agreement in syn-
tactic parsing has been investigated in previous
studies, e.g. the Bohnet parser (Bohnet, 2010)
employs morphological feature value pairs simi-
lar to our feature templates and Seeker and Kuhn
(2013) introduces an integer linear programming
framework including constraints for morpholog-
ical agreement. However, these works focus on
dependency parsing and to the best of our knowl-
edge, this is the first study on experimenting with
atomic morphological features and their agree-
ment in a constituency parsing.
PARSEVAL EX
reranker (merged morph) 89.05 18.45
reranker (mainPOS) 89.33 18.64
reranker (extended) 89.47 20.35
Table 5: The results achieved by using various
feature template sets for 50-best reranking on the
Hungarian development set.
141
Basque French German Hebrew Hungarian
BerkeleyParser 79.21 / 19.03 79.53 / 18.46 74.77 / 26.56 87.87 / 14.53 88.22 / 26.96
+ Lexical model 82.02 / 25.69 78.91 / 17.87 75.64 / 28.36 88.53 / 13.69 89.09 / 26.76
+ Preterminal merger 83.19 / 24.74 79.53 / 18.58 77.12 / 30.02 88.07 / 13.83 89.15 / 28.05
+ reranker 83.81 / 25.66 80.31 / 18.91 77.78 / 29.80 88.38 / 15.12 89.57 / 30.23
+ reranker + morph feat 84.03 / 26.28 80.41 / 20.07 77.74 / 29.23 88.55 / 15.24 89.91 / 30.55
Table 6: PARSEVAL / exact match scores on the test sets.
7 Results of the Full System
After our investigations focusing on building
blocks of our system independently from each
other on the development set, we parsed the test
sets of the treebanks adding steps one-by-one.
Table 6 summarizes our final results. We start
from the BerkeleyParser using the full morpholog-
ical descriptions as preterminal set, then we enrich
the lexical model with tagging frequencies gath-
ered from the automatic parsing of the test sets
(?+ lexical model?). In the third step we replace
the full preterminal set by the output of our preter-
minal merger procedure (?+ preterminal merger?).
We tuned the merging threshold of our method
on the development set for each language. The
last two rows contain the results achieved by the
50-best reranker with the standard feature set (?+
reranker?) and with the feature set extended by
morphological features (?+ morph features?).
The enhanced lexical model contributes a lot
at Basque and considerable improvements are
present at German and Hungarian as well while
it harmed the results in French. The advance of
the preterminal merger approach over the full set-
ting is clear at French and Hungarian, similarly to
the development set. It is interesting that an ratio-
nalized preterminal set could compensate the loss
suffered by a inadequate lexical model at French.
Although the reranking step could further
improve the results at each languages we have
to note that the gain (0.5 in average) is much
smaller here than the gains reported on English
(over 1.5). This might be because of the high
number of wordforms at morphologically rich lan-
guages i.e. most of feature templates are incor-
porate the words itself and the huge dictionary
can indicate data sparsity problems again. Our
morphology-based reranking features yielded a
moderate improvement at four languages, but we
believe there a lots of space for improvement here.
8 Conclusions
In this study we introduced three techniques for
better constituent parsing of morphologically rich
languages. We believe that research in con-
stituency parsing is important next to dependency
parsing. In general, we report state-of-the-art
results with constituent parsers with our entirely
language-agnostic techniques.
Our chief contribution here is the pretermi-
nal merger procedure. This is a more general
approach than previous proposals and still much
faster thank to operating on probabilities from a
PCFG instead of employing a full train+parse step
for evaluating every preterminal set candidate. We
found that the inclusion of the rich morphological
description into the preterminal level is crucial for
parsing morphologically rich languages. Our pro-
posed preterminal merger approach could outper-
form the full setting at 2 out of 5 languages, i.e. we
have reported gains in parsing accuracies by merg-
ing morphological feature values. At the other lan-
guages, the results with the full preterminal set and
our approach are competitive in terms of parsing
accuracies while our approach could achieve these
scores with a smaller preterminal set, which leads
to considerable parsing time advantages.
We also experimented with exploiting external
corpora in the lexical model. Here we showed
that automatic tagging of an off-the-shelf super-
vised morphological tagger (MarMot) can con-
tribute to the results. Our last experiment was car-
ried out with the feature set of an n-best reranker.
We showed that incorporating feature templates
built on morphological information improves the
results.
Acknowledgments
This work was supported in part by the Euro-
pean Union and the European Social Fund through
project FuturICT.hu (grant no.: T
?
AMOP-4.2.2.C-
11/1/KONV-2012-0013).
142
References
Anne Abeill?e, Lionel Cl?ement, and Franc?ois Toussenel.
2003. Building a treebank for french. In Anne
Abeill?e, editor, Treebanks. Kluwer, Dordrecht.
S. Abney, S. Flickenger, C. Gdaniec, C. Grishman,
P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-
vans, M. Liberman, M. Marcus, S. Roukos, B. San-
torini, and T. Strzalkowski. 1991. Procedure for
quantitatively comparing the syntactic coverage of
english grammars. In E. Black, editor, Proceedings
of the workshop on Speech and Natural Language,
pages 306?311.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. D??az de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency tree-
bank. In TLT-03, pages 201?204.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 89?97.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Erhard Hinrichs and Kiril Simov, edi-
tors, Proceedings of the First Workshop on Tree-
banks and Linguistic Theories (TLT 2002), pages
24?41.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ?05, pages 173?180.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics conference, pages 132?139.
Xiao Chen and Chunyu Kit. 2012. Higher-order con-
stituent parsing and parser combination. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 1?5.
Key-Sun Choi, Young S Han, Young G Han, and Oh W
Kwon. 1994. Kaist tree bank project for korean:
Present and future development. In Proceedings
of the International Workshop on Sharable Natural
Language Resources, pages 7?14. Citeseer.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
Seventeenth International Conference on Machine
Learning, ICML ?00, pages 175?182.
D?ora Csendes, J?anos Csirik, Tibor Gyim?othy, and
Andr?as Kocsor. 2005. The Szeged Treebank. In
TSD, pages 123?131.
Jon Dehdari, Lamia Tounsi, and Josef van Gen-
abith. 2011. Morphological features for parsing
morphologically-rich languages: A case of arabic.
In Proceedings of the Second Workshop on Statis-
tical Parsing of Morphologically Rich Languages,
pages 12?21, Dublin, Ireland, October. Association
for Computational Linguistics.
Alexander Fraser, Helmut Schmid, Rich?ard Farkas,
Renjing Wang, and Hinrich Sch?utze. 2013. Knowl-
edge sources for constituent parsing of german, a
morphologically rich and less-configurational lan-
guage. Computational Linguistics, 39(1):57?85.
Yoav Goldberg and Michael Elhadad. 2013. Word
segmentation, unknown-word resolution, and mor-
phological agreement in a hebrew parsing system.
Computational Linguistics, 39(1):121?160.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. EM can find pretty good HMM POS-taggers
(when given a good start). In Proceedings of ACL-
08: HLT, pages 746?754.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586?594.
Joseph Le Roux, Benoit Sagot, and Djam?e Seddah.
2012. Statistical parsing of spanish and data driven
lemmatization. In Proceedings of the ACL 2012
Joint Workshop on Statistical Parsing and Seman-
tic Processing of Morphologically Rich Languages,
pages 55?61.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: the penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Yuval Marton, Nizar Habash, and Owen Rambow.
2010. Improving arabic dependency parsing with
lexical and inflectional morphological features. In
Proceedings of the NAACL HLT 2010 First Work-
shop on Statistical Parsing of Morphologically-Rich
Languages, pages 13?21.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Thomas Mueller, Helmut Schmid, and Hinrich
Sch?utze. 2013. Efficient higher-order CRFs for
morphological tagging. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 322?332.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433?440.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley, Berkeley, CA, USA.
143
Djam?e Seddah, Reut Tsarfaty, Sandra K?ubler, Marie
Candito, Jinho D. Choi, Rich?ard Farkas, Jennifer
Foster, Iakes Goenaga, Koldo Gojenola Gallete-
beitia, Yoav Goldberg, Spence Green, Nizar Habash,
Marco Kuhlmann, Wolfgang Maier, Yuval Mar-
ton, Joakim Nivre, Adam Przepi?orkowski, Ryan
Roth, Wolfgang Seeker, Yannick Versley, Veronika
Vincze, Marcin Woli?nski, and Alina Wr?oblewska.
2013. Overview of the SPMRL 2013 shared
task: A cross-framework evaluation of parsing
morphologically rich languages. In Proceedings
of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 146?182.
Wolfgang Seeker and Jonas Kuhn. 2013. Morphologi-
cal and syntactic case in statistical dependency pars-
ing. Computational Linguistics, 39(1):23?55.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,
and Noa Nativ. 2001. Building a Tree-Bank for
Modern Hebrew Text. In Traitement Automatique
des Langues.
Reut Tsarfaty, Djam?e Seddah, Yoav Goldberg, Sandra
Kuebler, Yannick Versley, Marie Candito, Jennifer
Foster, Ines Rehbein, and Lamia Tounsi. 2010. Sta-
tistical parsing of morphologically rich languages
(spmrl) what, how and whither. In Proceedings of
the NAACL HLT 2010 First Workshop on Statistical
Parsing of Morphologically-Rich Languages, pages
1?12.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Cross-framework evaluation for statistical
parsing. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 44?54.
Reut Tsarfaty, Djam?e Seddah, Sandra K?ubler, and
Joakim Nivre. 2013. Parsing morphologically rich
languages: Introduction to the special issue. Com-
putational Linguistics, 39(1):15?22.
Tam?as V?aradi. 2002. The hungarian national cor-
pus. In In Proceedings of the Second International
Conference on Language Resources and Evaluation,
pages 385?389.
Yannick Versley and Ines Rehbein. 2009. Scalable dis-
criminative parsing for german. In Proceedings of
the 11th International Conference on Parsing Tech-
nologies (IWPT?09), pages 134?137.
J?anos Zsibrita, Veronika Vincze, and Rich?ard Farkas.
2013. magyarlanc: A toolkit for morphological and
dependency parsing of hungarian. In Proceedings of
RANLP.
144
First Joint Workshop on Statistical Parsing of Morphologically Rich Languages
and Syntactic Analysis of Non-Canonical Languages, pages 97?102 Dublin, Ireland, August 23-29 2014.
Introducing the IMS-Wroc?aw-Szeged-CIS Entry at the SPMRL 2014
Shared Task: Reranking and Morphosyntax Meet Unlabeled Data?
Anders Bjo?rkelund? and O?zlem C?etinog?lu? and Agnieszka Falen?ska,?
Richa?rd Farkas? and Thomas Mu?ller? and Wolfgang Seeker? and Zsolt Sza?nto??
?Institute for Natural Language Processing University of Stuttgart, Germany
Institute of Computer Science, University of Wroc?aw, Poland
?Department of Informatics University of Szeged, Hungary
?Center for Information and Language Processing University of Munich, Germany
{anders,ozlem,muellets,seeker}@ims.uni-stuttgart.de
agnieszka.falenska@cs.uni.wroc.pl
{rfarkas,szantozs}@inf.u-szeged.hu
Abstract
We summarize our approach taken in the SPMRL 2014 Shared Task on parsing morphologically
rich languages. Our approach builds upon our contribution from last year, with a number of
modifications and extensions. Though this paper summarizes our contribution, a more detailed
description and evaluation will be presented in the accompanying volume containing notes from
the SPMRL 2014 Shared Task.
1 Introduction
This paper summarizes the approach of IMS-Wroc?aw-Szeged-CIS taken for the SPMRL 2014 Shared
Task on parsing morphologically rich languages (Seddah et al., 2014). Since this paper is a rough sum-
mary that is written before submission of test runs we refer the reader to the full description paper which
will be published after the shared task (Bjo?rkelund et al., 2014).1
The SPMRL 2014 Shared Task is a direct extension of the SPMRL 2013 Shared Task (Seddah et al.,
2013) which targeted parsing morphologically rich languages. The task involves parsing both depen-
dency and phrase-structure representations of 9 languages: Arabic, Basque, French, German, Hebrew,
Hungarian, Korean, Polish, and Swedish. The only difference between the two tasks is that large amounts
of unlabeled data are additionally available to participants for the 2014 task.
Our contribution builds upon our system from last year (Bjo?rkelund et al., 2013), with additional
features and components that try to exploit the unlabeled data. Given the limited window of time to
participate in this year?s shared task, we only contribute to the setting with predicted preprocessing,
using the largest available training data set for each language.2 We also do not participate in the Arabic
track since the shared task organizers did not provide any unlabeled data at a reasonable time.
2 Review of Last Year?s System
Our current system is based on the system we participated with in the SPMRL 2013 Shared Task. We
summarize the architecture of this system as three different components.
?Authors in alphabetical order
1Due to logistical constraints this paper had to be written before the deadlines for the actual shared task and do thus not contain
a full description of the system, nor the experimental evaluation of the same.
2In other words, no gold preprocessing or smaller training sets.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
97
2.1 Preprocessing
As the initial step of preprocessing we converted the Shared Task data from the CoNLL06 format to
CoNLL09, which required a decision on using coarse or fine grained POS tags. After a set of preliminary
experiments we picked fine POS tags where possible, except Basque and Korean.
We used MarMoT3 (Mu?ller et al., 2013) to predict POS tags and morphological features jointly. We in-
tegrated the output from external morphological analyzers as features to MarMoT. We also experimented
with the integration of predicted tags provided by the organizers and observed that these stacked models
help improve Basque, Polish, and Swedish preprocessing. The stacked models provided additional infor-
mation to our tagger since the provided predictions were coming from models trained on larger training
sets than the shared task training sets.
2.2 Dependency Parsing
The dependency parsing architecture of our SPMRL 2013 Shared Task contribution is summarized in
Figure 1. The first step combines the n-best trees of two parsers, namely the mate parser4 (Bohnet, 2010)
and a variant of the EasyFirst parser (Goldberg and Elhadad, 2010), which we call best-first parser. We
merged the 50-best analyses from these parsers into one n-best list of 50 to 100 trees. We then added
parsing scores to the n-best trees from the two parsers, and additionally from the turboparser5 (Martins
et al., 2010).
mate parser
best-first
parser
turboparser
merged list
of 50-100 best
trees/sentence
merged list
scored by
all parsers
ranker
ptb trees
Parsing Ranking
IN OUT
scores
scores
scores
features
Figure 1: Architecture of the dependency ranking system from (Bjo?rkelund et al., 2013).
The scored trees are fed into the ranking system. The ranker utilizes the parsing scores and fea-
tures coming from both constituency and dependency parses. We specified a default feature set and
experimented with additional features for each language for optimal results. We achieved over 1% LAS
improvement on all languages except a 0.3% improvement on Hungarian.
2.3 Constituency Parsing
The constituency parsing architecture advances in three steps. For all setups we removed the morphologi-
cal annotation of POS tags and the function labels of non-terminals and apply the Berkeley Parser (Petrov
et al., 2006) as our baseline. As the first setup, we replaced words with a frequency < 20 with their pre-
dicted part-of-speech and morphology tags and improved the PARSEVAL scores across languages. The
second setup employed a product grammar (Petrov, 2010), where we combined 8 different grammars
trained on the same data but with different initialization setups. As a result, the scores substantially
improved on all languages.
Finally, we conducted ranking experiments on the 50-best outputs of the product grammars. We used
a slightly modified version of the Mallet toolkit (McCallum, 2002), where the reranker is trained for the
3https://code.google.com/p/cistern/
4https://code.google.com/p/mate-tools
5http://www.ark.cs.cmu.edu/TurboParser/
98
maximum entropy objective function of Charniak and Johnson (2005) and uses the standard feature set
from Charniak and Johnson (2005) and Collins (2000). Hebrew and Polish scores remained almost the
same, whereas Basque, French, and Hungarian highly benefited from reranking.
3 Planned Additions to Last Year?s System
This year we extend our systems for both the constituency and dependency tracks to add additional
information and try to profit from unlabeled data.
3.1 Preprocessing
We use the mate-tools? lemmatizer and MarMoT to preprocess all labeled and unlabeled data. From the
SPMRL 2013 Shared Task, we learned that getting as good preprocessing as possible is an important
part of the overall improvements. Preprocessing consists of predicting lemmas, part-of-speech, and
morphological features. Preprocessing for the training data is done via 5-fold jackknifing to produce
realistic input features for the parsers. This year we do not do stacking on top of provided morphological
analyses since the annotations on the labeled and unlabeled data were inconsistent for some languages.6
3.2 Dependency Parsing
We pursue two different ways of integrating additional information into our system from the SPMRL
2013 Shared Task (Bjo?rkelund et al., 2013): supertags and co-training.
Supertags (Bangalore and Joshi, 1999) are tags that encode more syntactic information than standard
part-of-speech tags. Supertags have been used in deep grammar formalisms like CCG or HPSG to prune
the search space for the parser. The idea has been applied to dependency parsing by Foth et al. (2006)
and recently to statistical dependency parsing (Ouchi et al., 2014; Ambati et al., 2014), where supertags
are used as features rather than to prune the search space. Since the supertag set is dynamically derived
from the gold-standard syntactic structures, we can encode different kinds of information into a supertag,
in particular also morphological information. Supertags are predicted before parsing using MarMoT and
are then used as features in the mate parser and the turboparser.
We will use a variant of co-training (Blum and Mitchell, 1998) by applying two different parsers to
select additional training material from unlabeled data. We use the mate parser and the turboparser to
parse the unlabeled data provided by the organizers. We then select sentences where both parsers agree
on the structure as additional training examples following Sagae and Tsujii (2007). We then train two
more models: one on the labeled training data and the unlabeled data selected by the two parsers, and
one only on the unlabeled data. These two models are then integrated into our parsing system from 2013
as additional scorers to score the n-best list. Their scores are used as features in the ranker.
Before we parse the unlabeled data to obtain the training sentences, we filter it in order to arrive
at a cleaner corpus. Most importantly, we only keep sentences up to length 50, and which contain at
maximum two unknown words (compared to the labeled training data).
3.3 Constituency Parsing
We experiment with two approaches for improving constituency parsing:
Preterminal labelsets play an important role in constituency parsing of morphologically rich lan-
guages (Dehdari et al., 2011). Instead of removing the morphological annotation of POS tags, we use a
preterminal set which carries more linguistic information while still keeping it compact. We follow the
merge procedure for morphological feature values of Sza?nto? and Farkas (2014). This procedure outputs a
clustering of full morphological descriptions and we use the cluster IDs as preterminal labels for training
the Berkeley Parser.
Reranking at the constituency parsing side is enriched by novel features. We define feature tem-
plates exploiting co-occurrence statistics from the unlabeled datasets; automatic dependency parses of
the sentence in question (Farkas and Bohnet, 2012); Brown clusters (Brown et al., 1992); and atomic
morphological feature values (Sza?nto? and Farkas, 2014).
6The organizers later resolved this issue by patching the data, although time constraints prevented us from using the patched
data.
99
4 Conclusion
This paper describes our plans for the SPMRL 2014 Shared Task, most of which are yet to be imple-
mented. For the actual system description and our results, we refer the interested reader to (Bjo?rkelund
et al., 2014) and (Seddah et al., 2014).
Acknowledgements
Agnieszka Falen?ska is funded through the Project International computer science and applied mathemat-
ics for business study programme at the University of Wroc?aw co-financed with European Union funds
within the European Social Fund No. POKL.04.01.01-00-005/13. Richa?rd Farkas and Zsolt Sza?nto? are
funded by the European Union and the European Social Fund through the project FuturICT.hu (grant no.:
TA?MOP-4.2.2.C-11/1/KONV-2012-0013). Thomas Mu?ller is supported by a Google Europe Fellowship
in Natural Language Processing. The remaining authors are funded by the Deutsche Forschungsgemein-
schaft (DFG) via the SFB 732, projects D2 and D8 (PI: Jonas Kuhn).
We also express our gratitude to the treebank providers for each language: Arabic (Maamouri et al.,
2004; Habash and Roth, 2009; Habash et al., 2009; Green and Manning, 2010), Basque (Aduriz et al.,
2003), French (Abeille? et al., 2003), Hebrew (Sima?an et al., 2001; Tsarfaty, 2010; Goldberg, 2011;
Tsarfaty, 2013), German (Brants et al., 2002; Seeker and Kuhn, 2012), Hungarian (Csendes et al., 2005;
Vincze et al., 2010), Korean (Choi et al., 1994; Choi, 2013), Polish (S?widzin?ski and Wolin?ski, 2010),
and Swedish (Nivre et al., 2006).
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel. 2003. Building a treebank for french. In Anne Abeille?,
editor, Treebanks. Kluwer, Dordrecht.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. D??az de Ilarraza, A. Garmendia, and M. Oronoz. 2003.
Construction of a Basque dependency treebank. In TLT-03, pages 201?204.
Bharat Ram Ambati, Tejaswini Deoskar, and Mark Steedman. 2014. Improving dependency parsers using combi-
natory categorial grammar. In Proceedings of the 14th Conference of the European Chapter of the Association
for Computational Linguistics, volume 2: Short Papers, pages 159?163, Gothenburg, Sweden, April. Associa-
tion for Computational Linguistics.
Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging: An approach to almost parsing. Computational
Linguistics, 25(2):237?265.
Anders Bjo?rkelund, O?zlem C?etinog?lu, Richa?rd Farkas, Thomas Mu?ller, and Wolfgang Seeker. 2013. (re)ranking
meets morphosyntax: State-of-the-art results from the SPMRL 2013 shared task. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 135?145, Seattle, Washington,
USA, October. Association for Computational Linguistics.
Anders Bjo?rkelund, O?zlem C?etinog?lu, Agnieszka Falen?ska, Richa?rd Farkas, Thomas Mu?ller, Wolfgang Seeker,
and Zsolt Sza?nto?. 2014. The IMS-Wroc?aw-Szeged-CIS entry at the SPMRL 2014 Shared Task: Reranking and
Morphosyntax meet Unlabeled Data. In Notes of the SPMRL 2014 Shared Task on Parsing Morphologically-
Rich Languages, Dublin, Ireland, August.
Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of
the Eleventh Annual Conference on Computational Learning Theory, COLT? 98, pages 92?100, New York, NY,
USA. ACM.
Bernd Bohnet. 2010. Top Accuracy and Fast Dependency Parsing is not a Contradiction. In Proceedings of
the 23rd International Conference on Computational Linguistics (Coling 2010), pages 89?97, Beijing, China,
August. Coling 2010 Organizing Committee.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER treebank.
In Erhard Hinrichs and Kiril Simov, editors, Proceedings of the First Workshop on Treebanks and Linguistic
Theories (TLT 2002), pages 24?41, Sozopol, Bulgaria.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer. 1992. Class-based
n-gram models of natural language. Computational Linguistics, 18(4):467?479.
100
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.
In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ?05, pages
173?180.
Key-Sun Choi, Young S Han, Young G Han, and Oh W Kwon. 1994. Kaist tree bank project for korean: Present
and future development. In Proceedings of the International Workshop on Sharable Natural Language Re-
sources, pages 7?14. Citeseer.
Jinho D. Choi. 2013. Preparing korean data for the shared task on parsing morphologically rich languages. CoRR,
abs/1309.1649.
Michael Collins. 2000. Discriminative Reranking for Natural Language Parsing. In Proceedings of the Seven-
teenth International Conference on Machine Learning, ICML ?00, pages 175?182.
Do?ra Csendes, Jano?s Csirik, Tibor Gyimo?thy, and Andra?s Kocsor. 2005. The Szeged treebank. In Va?clav Ma-
tous?ek, Pavel Mautner, and Toma?s? Pavelka, editors, Text, Speech and Dialogue: Proceedings of TSD 2005.
Springer.
Jon Dehdari, Lamia Tounsi, and Josef van Genabith. 2011. Morphological features for parsing morphologically-
rich languages: A case of arabic. In Proceedings of the Second Workshop on Statistical Parsing of Morphologi-
cally Rich Languages, pages 12?21, Dublin, Ireland, October. Association for Computational Linguistics.
Richa?rd Farkas and Bernd Bohnet. 2012. Stacking of dependency and phrase structure parsers. In Proceedings of
COLING 2012, pages 849?866, Mumbai, India, December. The COLING 2012 Organizing Committee.
Kilian A. Foth, Tomas By, and Wolfgang Menzel. 2006. Guiding a constraint dependency parser with supertags.
In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages 289?296, Sydney, Australia, July. Association for Com-
putational Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. An Efficient Algorithm for Easy-First Non-Directional Dependency
Parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages 742?750, Los Angeles, California, June. Association for
Computational Linguistics.
Yoav Goldberg. 2011. Automatic syntactic processing of Modern Hebrew. Ph.D. thesis, Ben Gurion University of
the Negev.
Spence Green and Christopher D. Manning. 2010. Better arabic parsing: Baselines, evaluations, and analysis. In
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 394?402,
Beijing, China, August. Coling 2010 Organizing Committee.
Nizar Habash and Ryan Roth. 2009. Catib: The columbia arabic treebank. In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 221?224, Suntec, Singapore, August. Association for Computational
Linguistics.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009. Syntactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on Arabic Language Resources and Tools, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and Wigdan Mekki. 2004. The Penn Arabic Treebank: Building
a Large-Scale Annotated Arabic Corpus. In NEMLAR Conference on Arabic Language Resources and Tools.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar, and Mario Figueiredo. 2010. Turbo Parsers: Dependency
Parsing by Approximate Variational Inference. In Proceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 34?44, Cambridge, MA, October. Association for Computational
Linguistics.
Andrew Kachites McCallum. 2002. ?mallet: A machine learning for language toolkit?.
http://mallet.cs.umass.edu.
Thomas Mu?ller, Helmut Schmid, and Hinrich Schu?tze. 2013. Efficient Higher-Order CRFs for Morphological
Tagging. In In Proceedings of EMNLP.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Talbanken05: A Swedish treebank with phrase structure and
dependency annotation. In Proceedings of LREC, pages 1392?1395, Genoa, Italy.
101
Hiroki Ouchi, Kevin Duh, and Yuji Matsumoto. 2014. Improving dependency parsers with supertags. In Proceed-
ings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, volume
2: Short Papers, pages 154?158, Gothenburg, Sweden, April. Association for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable
tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational Linguistics, pages 433?440. Association for Computa-
tional Linguistics.
Slav Petrov. 2010. Products of Random Latent Variable Grammars. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages
19?27, Los Angeles, California, June. Association for Computational Linguistics.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency parsing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 1044?1050,
Prague, Czech Republic, June. Association for Computational Linguistics.
Djame? Seddah, Reut Tsarfaty, Sandra Ku?bler, Marie Candito, Jinho D. Choi, Richa?rd Farkas, Jennifer Foster, Iakes
Goenaga, Koldo Gojenola Galletebeitia, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolf-
gang Maier, Joakim Nivre, Adam Przepio?rkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika
Vincze, Marcin Wolin?ski, Alina Wro?blewska, and Eric Villemonte de la Clergerie. 2013. Overview of the
SPMRL 2013 shared task: A cross-framework evaluation of parsing morphologically rich languages. In Pro-
ceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 146?182,
Seattle, Washington, USA, October. Association for Computational Linguistics.
Djame? Seddah, Reut Tsarfaty, Sandra Ku?bler, Marie Candito, Jinho Choi, Matthieu Constant, Richa?rd Farkas,
Iakes Goenaga, Koldo Gojenola, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolfgang
Maier, Joakim Nivre, Adam Przepiorkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika Vincze,
Marcin Wolin?ski, Alina Wro?blewska, and Eric Villemonte de la Cle?rgerie. 2014. Overview of the SPMRL 2014
shared task on parsing morphologically rich languages. In Notes of the SPMRL 2014 Shared Task on Parsing
Morphologically-Rich Languages, Dublin, Ireland.
Wolfgang Seeker and Jonas Kuhn. 2012. Making Ellipses Explicit in Dependency Conversion for a German
Treebank. In Proceedings of the 8th International Conference on Language Resources and Evaluation, pages
3132?3139, Istanbul, Turkey. European Language Resources Association (ELRA).
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman, and Noa Nativ. 2001. Building a Tree-Bank for Modern
Hebrew Text. In Traitement Automatique des Langues.
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards a bank of constituent parse trees for Polish. In Text,
Speech and Dialogue: 13th International Conference (TSD), Lecture Notes in Artificial Intelligence, pages
197?204, Brno, Czech Republic. Springer.
Zsolt Sza?nto? and Richa?rd Farkas. 2014. Special techniques for constituent parsing of morphologically rich lan-
guages. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational
Linguistics, pages 135?144, Gothenburg, Sweden, April. Association for Computational Linguistics.
Reut Tsarfaty. 2010. Relational-Realizational Parsing. Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A Unified Morpho-Syntactic Scheme of Stanford Dependencies. Proceedings of ACL.
Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgy Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010. Hungarian
dependency treebank. In LREC.
102
