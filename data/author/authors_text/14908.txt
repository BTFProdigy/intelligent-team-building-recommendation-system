Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 288?298,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Local Histograms of Character N -grams for Authorship Attribution
Hugo Jair Escalante
Graduate Program in Systems Eng.
Universidad Auto?noma de Nuevo Leo?n,
San Nicola?s de los Garza, NL, 66450, Me?xico
hugo.jair@gmail.com
Thamar Solorio
Dept. of Computer and Information Sciences
University of Alabama at Birmingham,
Birmingham, AL, 35294, USA
solorio@cis.uab.edu
Manuel Montes-y-Go?mez
Computer Science Department, INAOE,
Tonantzintla, Puebla, 72840, Me?xico
Department of Computer and Information Sciences,
University of Alabama at Birmingham,
Birmingham, AL, 35294, USA
mmontesg@cis.uab.edu
Abstract
This paper proposes the use of local his-
tograms (LH) over character n-grams for au-
thorship attribution (AA). LHs are enriched
histogram representations that preserve se-
quential information in documents; they have
been successfully used for text categorization
and document visualization using word his-
tograms. In this work we explore the suitabil-
ity of LHs over n-grams at the character-level
for AA. We show that LHs are particularly
helpful for AA, because they provide useful
information for uncovering, to some extent,
the writing style of authors. We report experi-
mental results in AA data sets that confirm that
LHs over character n-grams are more help-
ful for AA than the usual global histograms,
yielding results far superior to state of the art
approaches. We found that LHs are even more
advantageous in challenging conditions, such
as having imbalanced and small training sets.
Our results motivate further research on the
use of LHs for modeling the writing style of
authors for related tasks, such as authorship
verification and plagiarism detection.
1 Introduction
Authorship attribution (AA) is the task of deciding
whom, from a set of candidates, is the author of a
given document (Houvardas and Stamatatos, 2006;
Luyckx and Daelemans, 2010; Stamatatos, 2009b).
There is a broad field of application for AA meth-
ods, including spam filtering (de Vel et al, 2001),
fraud detection, computer forensics (Lambers and
Veenman, 2009), cyber bullying (Pillay and Solorio,
2010) and plagiarism detection (Stamatatos, 2009a).
Therefore, the development of automated AA tech-
niques has received much attention recently (Sta-
matatos, 2009b). The AA problem can be natu-
rally posed as one of single-label multiclass clas-
sification, with as many classes as candidate au-
thors. However, unlike usual text categorization
tasks, where the core problem is modeling the the-
matic content of documents (Sebastiani, 2002), the
goal in AA is modeling authors? writing style (Sta-
matatos, 2009b). Hence, document representations
that reveal information about writing style are re-
quired to achieve good accuracy in AA.
Word and character based representations have
been used in AA with some success so far (Houvar-
das and Stamatatos, 2006; Luyckx and Daelemans,
2010; Plakias and Stamatatos, 2008b). Such rep-
resentations can capture style information through
word or character usage, but they lack sequential in-
formation, which can reveal further stylistic infor-
mation. In this paper, we study the use of richer
document representations for the AA task. In partic-
ular, we consider local histograms over n-grams at
the character-level obtained via the locally-weighted
bag of words (LOWBOW) framework (Lebanon et
al., 2007).
Under LOWBOW, a document is represented by a
set of local histograms, computed across the whole
document but smoothed by kernels centered on dif-
ferent document locations. In this way, document
288
representations preserve both word/character usage
and sequential information (i.e., information about
the positions in which words or characters occur),
which can be more helpful for modeling the writ-
ing style of authors. We report experimental re-
sults in an AA data set used in previous studies un-
der several conditions (Houvardas and Stamatatos,
2006; Plakias and Stamatatos, 2008b; Plakias and
Stamatatos, 2008a). Results confirm that local his-
tograms of character n-grams are more helpful for
AA than the usual global histograms of words or
character n-grams (Luyckx and Daelemans, 2010);
our results are superior to those reported in re-
lated works. We also show that local histograms
over character n-grams are more helpful than lo-
cal histograms over words, as originally proposed
by (Lebanon et al, 2007). Further, we performed
experiments with imbalanced and small training
sets (i.e., under a realistic AA setting) using the
aforementioned representations. We found that the
LOWBOW-based representation resulted even more
advantageous in these challenging conditions. The
contributions of this work are as follows:
? We show that the LOWBOW framework can be
helpful for AA, giving evidence that sequential in-
formation encoded in local histograms is useful for
modeling the writing style of authors.
? We propose the use of local histograms over
character-level n-grams for AA. We show that
character-level representations, which have proved
to be very effective for AA (Luyckx and Daelemans,
2010), can be further improved by adopting a local
histogram formulation. Also, we empirically show
that local histograms at the character-level are more
helpful than local histograms at the word-level for
AA.
? We study several kernels for a support vector ma-
chine AA classifier under the local histograms for-
mulation. Our study confirms that the diffusion ker-
nel (Lafferty and Lebanon, 2005) is the most ef-
fective among those we tried, although competitive
performance can be obtained with simpler kernels.
? We report experimental results that are superior to
state of the art approaches (Plakias and Stamatatos,
2008b; Plakias and Stamatatos, 2008a), with im-
provements ranging from 2%?6% in balanced data
sets and from 14%? 30% in imbalanced data sets.
2 Related Work
AA can be faced as a multiclass classifica-
tion task with as many classes as candidate au-
thors. Standard classification methods have been
applied to this problem, including support vec-
tor machine (SVM) classifiers (Houvardas and Sta-
matatos, 2006) and variants thereon (Plakias and
Stamatatos, 2008b; Plakias and Stamatatos, 2008a),
neural networks (Tearle et al, 2008), Bayesian clas-
sifiers (Coyotl-Morales et al, 2006), decision tree
methods (Koppel et al, 2009) and similarity based
techniques (Keselj et al, 2003; Lambers and Veen-
man, 2009; Stamatatos, 2009b; Koppel et al, 2009).
In this work, we chose an SVM classifier as it has
reported acceptable performance in AA and because
it will allow us to directly compare results with pre-
vious work that has used this same classifier.
A broad diversity of features has been used to rep-
resent documents in AA (Stamatatos, 2009b). How-
ever, as in text categorization (Sebastiani, 2002),
word-based and character-based features are among
the most widely used features (Stamatatos, 2009b;
Luyckx and Daelemans, 2010). With respect to
word-based features, word histograms (i.e., the bag-
of-words paradigm) are the most frequently used
representations in AA (Zhao and Zobel, 2005;
Argamon and Levitan, 2005; Stamatatos, 2009b).
Some researchers have gone a step further and
have attempted to capture sequential information
by using n-grams at the word-level (Peng et al,
2004) or by discovering maximal frequent word se-
quences (Coyotl-Morales et al, 2006). Unfortu-
nately, because of computational limitations, the lat-
ter methods cannot discover enough sequential in-
formation from documents (e.g., word n-grams are
often restricted to n ? {1, 2, 3}, while full se-
quential information would be obtained with n ?
{1 . . . D} where D is the maximum number of
words in a document).
With respect to character-based features, n-grams
at the character level have been widely used in AA
as well (Plakias and Stamatatos, 2008b; Peng et
al., 2003; Luyckx and Daelemans, 2010). Peng et
al. (2003) propose the use of language models at the
n-gram character-level for AA, whereas Keselj et
al. (2003) build author profiles based on a selection
of frequent n-grams for each author. Stamatatos and
co-workers have studied the impact of feature se-
lection, with character n-grams, in AA (Houvardas
and Stamatatos, 2006; Stamatatos, 2006a), ensem-
ble learning with character n-grams (Stamatatos,
2006b) and novel classification techniques based
289
on characters at the n-gram level (Plakias and Sta-
matatos, 2008a).
Acceptable performance in AA has been reported
with character n-gram representations. However,
as with word-based features, character n-grams are
unable to incorporate sequential information from
documents in their original form (in terms of the
positions in which the terms appear across a doc-
ument). We believe that sequential clues can be
helpful for AA because different authors are ex-
pected to use different character n-grams or words
in different parts of the document. Accordingly,
in this work we adopt the popular character-based
and word-based representations, but we enrich them
in a way that they incorporate sequential informa-
tion via the LOWBOW framework. Hence, the pro-
posed features preserve sequential information be-
sides capturing character and word usage informa-
tion. Our hypothesis is that the combination of se-
quential and frequency information can be particu-
larly helpful for AA.
The LOWBOW framework has been mainly used
for document visualization (Lebanon et al, 2007;
Mao et al, 2007), where researchers have used in-
formation derived from local histograms for dis-
playing a 2D representation of document?s con-
tent. More recently, Chasanis et al (2009) used
the LOWBOW framework for segmenting movies
into chapters and scenes. LOWBOW representa-
tions have also been applied to discourse segmen-
tation (AMIDA, 2007) and have been suggested for
text summarization (Das and Martins, 2007). How-
ever, to the best of our knowledge the use of the
LOWBOW framework for AA has not been studied
elsewhere. Actually, the only two references using
this framework for text categorization are (Lebanon
et al, 2007; AMIDA, 2007). The latter can be due to
the fact that local histograms provide little gain over
usual global histograms for thematic classification
tasks. In this paper we show that LOWBOW rep-
resentations provide important improvements over
global histograms for AA; in particular, local his-
tograms at the character-level achieve the highest
performance in our experiments.
3 Background
This section describes preliminary information on
document representations and pattern classification
with SVMs.
3.1 Bag of words representations
In the bag of words (BOW) representation, docu-
ments are represented by histograms over the vo-
cabulary1 that was used to generate a collection of
documents; that is, a document i is represented as:
di = [xi,1, . . . , xi,|V |] (1)
where V is the vocabulary and |V | is the number of
elements in V , di,j = xi,j is a weight that denotes
the contribution of term j to the representation of
document i; usually xi,j is related to the occurrence
(binary weighting) or the weighted frequency of oc-
currence (e.g., the tf-idf weighting scheme) of the
term j in document i.
3.2 Locally-weighted bag-of-words
representation
Instead of using the BOW framework directly, we
adopted the LOWBOW framework for document
representation (Lebanon et al, 2007). The underly-
ing idea in LOWBOW is to compute several local
histograms per document, where these histograms
are smoothed by a kernel function, see Figure 1.
The parameters of the kernel specify the position of
the kernel in the document (i.e., where the local his-
togram is centered) and its scale (i.e., to what extent
it is smoothed). In this way the sequential informa-
tion in the document is preserved together with term
usage statistics.
Let Wi = {wi,1, . . . , wi,Ni}, denote the terms
(in order of appearance) in document i where Ni
is the number of terms that appear in document i
and wi,j ? V is the term appearing at position
j; let vi = {vi,1, . . . , vi,Ni} be the set of indexes
in the vocabulary V of the terms appearing in Wi,
such that vi,j is the index in V of the term wi,j ;
let t = [t1, . . . , tNi ] be a set of (equally spaced)
scalars that determine intervals, with 0 ? tj ? 1 and?Ni
j=1 tj = 1, such that each tj can be associated to
a position in Wi. Given a kernel smoothing function
Ks?,? : [0, 1] ? R with location parameter ? and
scale parameter ?, where ?kj=1 Ks?,?(tj) = 1 and
1In the following we will refer to arbitrary vocabularies,
which can be formed with terms from either words or character
n-grams.
290
Figure 1: Diagram of the process for obtaining local
histograms. Terms (wi) appearing in different posi-
tions (1, . . . , N ) of the document are weighted according
to the locations (?1, . . . , ?k) of the smoothing function
K?,?(x). Then, the term position weighting is combined
with term frequency weighting for obtaining local his-
tograms over the terms in the vocabulary (1, . . . , |V |).
? ? [0, 1]. The LOWBOW framework computes a
local histogram for each position ?j ? {?1, . . . , ?k}
as follows:
dlji,{vi,1,...,vi,Ni} = di,{vi,1,...,vi,Ni} ?K
s
?j ,?(t) (2)
where dli,vj :vj 6?vi = const, a small constant value,
and di,j is defined as above. Hence, a set dl{1,...,k}i
of k local histograms are computed for each doc-
ument i. Each histogram dlji carries information
about the distribution of terms at a certain position
?j of the document, where ? determines how the
nearby terms to ?j influence the local histogram
j. Thus, sequential information of the document is
considered throughout these local histograms. Note
that when ? is small, most of the sequential informa-
tion is preserved, as local histograms are calculated
at very local scales; whereas when ? ? 1, local his-
tograms resemble the traditional BOW representa-
tion.
Under LOWBOW documents can be represented
in two forms (Lebanon et al, 2007): as a single his-
togram dLi = const ?
?k
j=1 dlji (hereafter LOW-
BOW histograms) or by the set of local histograms
itself dl{1,...,k}i . We performed experiments with
both forms of representation and considered words
and n-grams at the character-level as terms (c.f. Sec-
tion 5). Regarding the smoothing function, we con-
sidered the re-normalized Gaussian pdf restricted to
[0, 1]:
Ks?,?(x) =
?
?
?
N (x;?,?)
?( 1??? )??(??? ) if x ? [0, 1]
0 otherwise
(3)
where ?(x) is the cumulative distribution function
for a Gaussian with mean 0 and standard deviation 1,
evaluated at x, see (Lebanon et al, 2007) for further
details.
3.3 Support vector machines
Support vector machines (SVMs) are pattern classi-
fication methods that aim to find an optimal sepa-
rating hyperplane between examples from two dif-
ferent classes (Shawe-Taylor and Cristianini, 2004).
Let {xi, yi}N be pairs of training patterns-outputs,
where xi ? Rd and y ? {?1, 1}, with d the di-
mensionality of the problem. SVMs aim at learn-
ing a mapping from training instances to outputs.
This is done by considering a linear function of the
form: f(x) = Wx + b, where parameters W and b
are learned from training data. The particular linear
function considered by SVMs is as follows:
f(x) =
?
i
?iyiK(xi, x)? b (4)
that is, a linear function over (a subset of) training
examples, where ?i is the weight associated with
training example i (those for which ?i > 0 are the so
called support vectors) and yi is the label associated
with training example i, K(xi, xj) is a kernel2 func-
tion that aims at mapping the input vectors, (xi, xj),
into the so called feature space, and b is a bias
term. Intuitively, K(xi, xj) evaluates how similar
instances xi and xj are, thus the particular choice of
kernel is problem dependent. The parameters in ex-
pression (4), namely ?{1,...,N} and b, are learned by
using exact optimization techniques (Shawe-Taylor
and Cristianini, 2004).
2One should not confuse the kernel smoothing function,
Ks?,?(x), defined in Equation (3) with the Mercer kernel in
Equation (4), as the former acts as a smoothing function and
the latter acts as a similarity function.
291
4 Authorship Attribution with LOWBOW
Representations
For AA we represent the training documents of
each author using the framework described in Sec-
tion 3.2, thus each document of each candidate au-
thor is either a LOWBOW histogram or a bag of lo-
cal histograms (BOLH). Recall that LOWBOW his-
tograms are an un-weighted sum of local histograms
and hence can be considered a summary of term us-
age and sequential information; whereas the BOLH
can be seen as term occurrence frequencies across
different locations of the document.
For both types of representations we consider an
SVM classifier under the one-vs-all formulation for
facing the AA problem. We consider SVM as base
classifier because this method has proved to be very
effective in a large number of applications, including
AA (Houvardas and Stamatatos, 2006; Plakias and
Stamatatos, 2008b; Plakias and Stamatatos, 2008a);
further, since SVMs are kernel-based methods, they
allow us to use local histograms for AA by consid-
ering kernels that work over sets of histograms.
We build a multiclass SVM classifier by con-
sidering the pairs of patterns-outputs associated to
documents-authors. Where each pattern can be ei-
ther a LOWBOW histogram or the set of local his-
tograms associated with the corresponding docu-
ment, and the output associated to each pattern is
a categorical random variable (outputs) that asso-
ciates the representation of each document to its cor-
responding author y1,...,N ? {1, . . . , C}, with C
the number of candidate authors. For building the
multiclass classifier we adopted the one-vs-all for-
mulation, where C binary classifiers are built and
where each classifier fi discriminates among exam-
ples from class i (positive examples) and the rest
j : j ? {1, . . . , C}, j 6= i; despite being one of the
simplest formulations, this approach has shown to
obtain comparable and even superior performance to
that obtained by more complex formulations (Rifkin
and Klautau, 2004).
For AA using LOWBOW histograms, we con-
sider a linear kernel since it has been success-
fully applied to a wide variety of problems (Shawe-
Taylor and Cristianini, 2004), including AA (Hou-
vardas and Stamatatos, 2006; Plakias and Sta-
matatos, 2008b). However, standard kernels can-
not work for input spaces where each instance is de-
scribed by a set of vectors. Therefore, usual kernels
are not applicable for AA using BOLH. Instead, we
rely on particular kernels defined for sets of vectors
rather than for a single vector. Specifically, we con-
sider kernels of the form (Rubner et al, 2001; Grau-
man, 2006):
K(P,Q) = exp (? D(P,Q)
2
?
) (5)
where D(P,Q) is the sum of the distances between
the elements of the bag of local histograms asso-
ciated to author P and the elements of the bag of
histograms associated with author Q; ? is the scale
parameter of K. Let P = {p1, . . . , pk} and Q =
{q1, . . . , qk} be the elements of the bags of local
histograms for instances P and Q, respectively, Ta-
ble 1 presents the distance measures we consider for
AA using local histograms.
Kernel Distance
Diffusion D(P,Q) = ?kl=1 arccos
(??pl ?
?ql?
)
EMD D(P,Q) = EMD(P,Q)
Eucidean D(P,Q) =
??k
l=1(pl ? ql).2
?2 D(P,Q) =
??k
l=1
(pl?ql)2
(pl+ql)
Table 1: Distance functions used to calculate the kernel
defined in Equation (5).
Diffusion, Euclidean, and ?2 kernels compare lo-
cal histograms one to one, which means that the lo-
cal histograms calculated at the same locations are
compared to each other. We believe that for AA
this is advantageous as it is expected that an author
uses similar terms at similar locations of the docu-
ment. The Earth mover?s distance (EMD), on the
other hand, is an estimate of the optimal cost in tak-
ing local histograms from Q to local histograms in
P (Rubner et al, 2001); that is, this measure com-
putes the optimal matching distance between local
histograms from different authors that are not neces-
sarily computed at similar locations.
5 Experiments and Results
For our experiments we considered the data set used
in (Plakias and Stamatatos, 2008b; Plakias and Sta-
matatos, 2008a). This corpus is a subset of the
RCV1 collection (Lewis et al, 2004) and comprises
292
documents authored by 10 authors. All of the docu-
ments belong to the same topic. Since this data set
has predefined training and testing partitions, our re-
sults are comparable to those obtained by other re-
searchers. There are 50 documents per author for
training and 50 documents per author for testing.
We performed experiments with LOWBOW3 rep-
resentations at word and character-level. For the ex-
periments with words, we took the top 2,500 most
common words used across the training documents
and obtained LOWBOW representations. We used
this setting in agreement with previous work on
AA (Houvardas and Stamatatos, 2006). For our
character n-gram experiments, we obtained LOW-
BOW representations for character 3-grams (only
n-grams of size n = 3 were used) considering
the 2, 500 most common n-grams. Again, this set-
ting was adopted in agreement with previous work
on AA with character n-grams (Houvardas and
Stamatatos, 2006; Plakias and Stamatatos, 2008b;
Plakias and Stamatatos, 2008a; Luyckx and Daele-
mans, 2010). All our experiments use the SVM im-
plementation provided by Canu et al (2005).
5.1 Experimental settings
In order to compare our methods to related works
we adopted the following experimental setting. We
perform experiments using all of the training doc-
uments per author, that is, a balanced corpus (we
call this setting BC). Next we evaluate the perfor-
mance of classifiers over reduced training sets. We
tried balanced reduced data sets with: 1, 3, 5 and
10 documents per author (we call this configura-
tion RBC). Also, we experimented with reduced-
imbalanced data sets using the same imbalance rates
reported in (Plakias and Stamatatos, 2008b; Plakias
and Stamatatos, 2008a): we tried settings 2 ? 10,
5? 10, and 10? 20, where, for example, setting 2-
10 means that we use at least 2 and at most 10 doc-
uments per author (we call this setting IRBC). BC
setting represents the AA problem under ideal con-
ditions, whereas settings RBC and IRBC aim at em-
ulating a more realistic scenario, where limited sam-
ple documents are available and the whole data set is
highly imbalanced (Plakias and Stamatatos, 2008b).
3We used LOWBOW code of G. Lebanon and Y. Mao avail-
able from http://www.cc.gatech.edu/?ymao8/lowbow.htm
5.2 Experimental results in balanced data
We first compare the performance of the LOWBOW
histogram representation to that of the traditional
BOW representation. Table 2 shows the accuracy
(i.e., percentage of documents in the test set that
were associated to its correct author) for the BOW
and LOWBOW histogram representations when us-
ing words and character n-grams information. For
LOWBOW histograms, we report results with three
different configurations for ?. As in (Lebanon et al,
2007), we consider uniformly distributed locations
and we varied the number of locations that were in-
cluded in each setting. We denote with k the number
of local histograms. In preliminary experiments we
tried several other values for k, although we found
that representative results can be obtained with the
values we considered here.
Method Parameters Words Characters
BOW - 78.2% 75.0%
LOWBOW k = 2;? = 0.2 75.8% 72.0%
LOWBOW k = 5;? = 0.2 77.4% 75.2%
LOWBOW k = 20;? = 0.2 77.4% 75.0%
Table 2: Authorship attribution accuracy for the BOW
representation and LOWBOW histograms. Column 2
shows the parameters we used for the LOWBOW his-
tograms; columns 3 and 4 show results using words and
character n-grams, respectively.
From Table 2 we can see that the BOW repre-
sentation is very effective, outperforming most of
the LOWBOW histogram configurations. Despite a
small difference in performance, BOW is advanta-
geous over LOWBOW histograms because it is sim-
pler to compute and it does not rely on parameter
selection. Recall that the LOWBOW histogram rep-
resentations are obtained by the combination of sev-
eral local histograms calculated at different locations
of the document, hence, it seems that the raw sum of
local histograms results in a loss of useful informa-
tion for representing documents. The worse perfor-
mance was obtained when k = 2 local histograms
are considered (see row 3 in Table 2). This re-
sult is somewhat expected since the larger the num-
ber of local histograms, the more LOWBOW his-
tograms approach the BOW formulation (Lebanon
et al, 2007).
We now describe the AA performance obtained
when using the BOLH formulation; these results
293
are shown in Table 3. Most of the results from
this table are superior to those reported in Table 2,
showing that bags of local histograms are a better
way to exploit the LOWBOW framework for AA.
As expected, different kernels yield different results.
However, the diffusion kernel outperformed most of
the results obtained with other kernels; confirming
the results obtained by other researchers (Lebanon
et al, 2007; Lafferty and Lebanon, 2005).
Kernel Euc. Diffusion EMD ?2
Words
Setting-1 78.6% 81.0% 75.0% 75.4%
Setting-2 77.6% 82.0% 76.8% 77.2%
Setting-3 79.2% 80.8% 77.0% 79.0%
Characters
Setting-1 83.4% 82.8% 84.4% 83.8%
Setting-2 83.4% 84.2% 82.2% 84.6%
Setting-3 83.6% 86.4% 81.0% 85.2%
Table 3: Authorship attribution accuracy when using bags
of local histograms and different kernels for word-based
and character-based representations. The BC data set is
used. Settings 1, 2 and 3 correspond to k = 2, 5 and 20,
respectively.
On average, the worse kernel was that based on
the earth mover?s distance (EMD), suggesting that
the comparison of local histograms at different loca-
tions is not a fruitful approach (recall that this is the
only kernel that compares local histograms at differ-
ent locations). This result evidences that authors use
similar word/character distributions at similar loca-
tions when writing different documents.
The best performance across settings and kernels
was obtained with the diffusion kernel (in bold, col-
umn 3, row 9) (86.4%); that result is 8% higher
than that obtained with the BOW representation and
9% better than the best configuration of LOWBOW
histograms, see Table 2. Furthermore, that result
is more than 5% higher than the best reported re-
sult in related work (80.8% as reported in (Plakias
and Stamatatos, 2008b)). Therefore, the consid-
ered local histogram representations over character
n-grams have proved to be very effective for AA.
One should note that, in general, better per-
formance was obtained when using character-level
rather than word-level information. This confirms
the results already reported by other researchers
that have used character-level and word-level infor-
mation for AA (Houvardas and Stamatatos, 2006;
Plakias and Stamatatos, 2008b; Plakias and Sta-
matatos, 2008a; Peng et al, 2003). We believe this
can be attributed to the fact that character n-grams
provide a representation for the document at a finer
granularity, which can be better exploited with local
histogram representations. Note that by considering
3-grams, words of length up to three are incorpo-
rated, and usually these words are function words
(e.g., the, it, as, etc.), which are known to be in-
dicative of writing style. Also, n-gram information
is more dense in documents than word-level infor-
mation. Hence, the local histograms are less sparse
when using character-level information, which re-
sults in better AA performance.
True author
AC AS BL DL JM JG MM MD RS TN
88 2 0 0 0 0 0 0 0 0
10 98 0 0 0 0 0 0 0 0
0 0 68 0 40 0 0 0 0 0
0 0 0 80 0 0 0 0 0 4
0 0 12 2 42 0 0 2 0 0
0 0 0 0 0 100 0 0 0 2
2 0 2 0 0 0 100 0 0 0
0 0 18 0 18 0 0 98 0 0
0 0 0 2 0 0 0 0 100 4
0 0 0 16 0 0 0 0 0 90
Table 4: Confusion matrix (in terms of percentages) for
the best result in the BC corpus (i.e., last row, column 3
in Table 3). Columns show the true author for test docu-
ments and rows show the authors predicted by the SVM.
Table 4 shows the confusion matrix for the setting
that reached the best results (i.e., column 3, last row
in Table 3). From this table we can see that 8 out
of the 10 authors were recognized with an accuracy
higher or equal to 80%. For these authors sequential
information seems to be particularly helpful. How-
ever, low recognition performance was obtained for
authors BL (B. K. Lim) and JM (J. MacArtney).
The SVM with BOW representation of character n-
grams achieved recognition rates of 40% and 50%
for BL and JM respectively. Thus, we can state that
sequential information was indeed helpful for mod-
eling BL writing style (improvement of 28%), al-
though it is an author that resulted very difficult to
model. On the other hand, local histograms were not
very useful for identifying documents written by JM
(made it worse by ?8%). The largest improvement
(38%) of local histograms over the BOW formula-
tion was obtained for author TN (T. Nissen). This
294
result gives evidence that TN uses a similar distri-
bution of words in similar locations across the doc-
uments he writes. These results are interesting, al-
though we would like to perform a careful analysis
of results in order to determine for what type of au-
thors it would be beneficial to use local histograms,
and what type of authors are better modeled with a
standard BOW approach.
5.3 Experimental results in imbalanced data
In this section we report results with RBC and
IRBC data sets, which aim to evaluate the perfor-
mance of our methods in a realistic setting. For
these experiments we compare the performance of
the BOW, LOWBOW histogram and BOLH repre-
sentations; for the latter, we considered the best set-
ting as reported in Table 3 (i.e., an SVM with dif-
fusion kernel and k = 20). Tables 5 and 6 show
the AA performances when using word and charac-
ter information, respectively.
We first analyze the results in the RBC data set
(recall that for this data set we consider 1, 3, 5, 10,
and 50, randomly selected documents per author).
From Tables 5 and 6 we can see that BOW and
LOWBOW histogram representations obtained sim-
ilar performance to each other across the different
training set sizes, which agree with results in Table 2
for the BC data sets. The best performance across
the different configurations of the RBC data set was
obtained with the BOLH formulation (row 6 in Ta-
bles 5 and 6). The improvements of local histograms
over the BOW formulation vary across different set-
tings and when using information at word-level and
character-level. When using words (columns 2-6
in Table 5) the differences in performance are of
15.6%, 6.2%, 6.8%, 2.9%, 3.8% when using 1, 3, 5,
10 and 50 documents per author, respectively. Thus,
it is evident that local histograms are more beneficial
when less documents are considered. Here, the lack
of information is compensated by the availability of
several histograms per author.
When using character n-grams (columns 2-6 in
Table 6) the corresponding differences in perfor-
mance are of 5.4%, 6.4%, 6.4%, 6% and 11.4%,
when using 1, 3, 5, 10, and 50 documents per au-
thor, respectively. In this case, the larger improve-
ment was obtained when 50 documents per author
are available; nevertheless, one should note that re-
sults using character-level information are, in gen-
eral, significantly better than those obtained with
word-level information; hence, improvements are
expected to be smaller.
When we compare the results of the BOLH for-
mulation with the best reported results elsewhere
(c.f. last row 6 in Tables 5 and 6) (Plakias and Sta-
matatos, 2008b), we found that the improvements
range from 14% to 30.2% when using character n-
grams and from 1.2% to 26% when using words.
The differences in performance are larger when less
information is used (e.g., when 5 documents are
used for training) and we believe the differences
would be even larger if results for 1 and 3 documents
were available. These are very positive results; for
example, we can obtain almost 71% of accuracy, us-
ing local histograms of character n-grams when a
single document is available per author (recall that
we have used all of the test samples for evaluating
the performance of our methods).
We now analyze the performance of the different
methods when using the IRBC data set (columns 7-
9 in Tables 5 and 6). The same pattern as before can
be observed in experimental results for these data
sets as well: BOW and LOWBOW histograms ob-
tained comparable performance to each other and
the BOLH formulation performed the best. The
BOLH formulation outperforms state of the art ap-
proaches by a considerable margin that ranges from
10% to 27%. Again, better results were obtained
when using character n-grams for the local his-
tograms. With respect to RBC data sets, the BOLH
at the character-level resulted very robust to the re-
duction of training set size and the highly imbal-
anced data.
Summarizing, the results obtained in RBC and
IRBC data sets show that the use of local histograms
is advantageous under challenging conditions. An
SVM under the BOLH representation is less sen-
sitive to the number of training examples available
and to the imbalance of data than an SVM using
the BOW representation. Our hypothesis for this
behavior is that local histograms can be thought of
as expanding training instances, because for each
training instance in the BOW formulation we have
k?training instances under BOLH. The benefits of
such expansion become more notorious as the num-
ber of available documents per author decreases.
295
WORDS
Data set Balanced Imbalanced
Setting 1-doc 3-docs 5-docs 10-docs 50-docs 2-10 5-10 10-20
BOW 36.8% 57.1% 62.4% 69.9% 78.2% 62.3% 67.2% 71.2%
LOWBOW 37.9% 55.6% 60.5% 69.3% 77.4% 61.1% 67.4% 71.5%
Diffusion kernel 52.4% 63.3% 69.2% 72.8% 82.0% 66.6% 70.7% 74.1%
Reference - - 53.4% 67.8% 80.8% 49.2% 59.8% 63.0%
Table 5: AA accuracy in RBC (columns 2-6) and IRBC (columns 7-9) data sets when using words as terms. We report
results for the BOW, LOWBOW histogram and BOLH representations. For reference (last row), we also include the
best result reported in (Plakias and Stamatatos, 2008b), when available, for each configuration.
CHARACTER N-GRAMS
Data set Balanced Imbalanced
Setting 1-doc 3-docs 5-docs 10-docs 50-docs 2-10 5-10 10-20
BOW 65.3% 71.9% 74.2% 76.2% 75.0% 70.1% 73.4% 73.1%
LOWBOW 61.9% 71.6% 74.5% 73.8% 75.0% 70.8% 72.8% 72.1%
Diffusion kernel 70.7% 78.3% 80.6% 82.2% 86.4% 77.8% 80.5% 82.2%
Reference - - 50.4% 67.8% 76.6% 49.2% 59.8% 63.0%
Table 6: AA accuracy in the RBC and IRBC data sets when using character n-grams as terms.
6 Conclusions
We have described the use of local histograms (LH)
over character n-grams for AA. LHs are enriched
histogram representations that preserve sequential
information in documents (in terms of the positions
of terms in documents); we explored the suitabil-
ity of LHs over n-grams at the character-level for
AA. We showed evidence supporting our hypothe-
sis that LHs are very helpful for AA; we believe that
this is due to the fact that LOWBOW representations
can uncover, to some extent, the writing preferences
of authors. Our experimental results showed that
LHs outperform traditional bag-of-words formula-
tions and state of the art techniques in balanced,
imbalanced, and reduced data sets. The improve-
ments were larger in reduced and imbalanced data
sets, which is a very positive result as in real AA
applications one often faces highly imbalanced and
small sample issues. Our results are promising and
motivate further research on the use and extension
of the LOWBOW framework for related tasks (e.g.
authorship verification and plagiarism detection).
As future work we would like to explore the use
of LOWBOW representations for profile-based AA
and related tasks. Also, we would like to develop
model selection strategies for learning what combi-
nation of hyperparameters works better for modeling
each author.
Acknowledgments
We thank E. Stamatatos for making his data set
available. Also, we are grateful for the thought-
ful comments of L. A. Barro?n and those of the
anonymous reviewers. This work was partially sup-
ported by CONACYT under project grants 61335,
and CB-2009-134186, and by UAB faculty develop-
ment grant 3110841.
References
AMIDA. 2007. Augmented multi-party interaction
with distance access. Available from http://www.
amidaproject.org/, AMIDA Report.
S. Argamon and S. Levitan. 2005. Measuring the useful-
ness of function words for authorship attribution. In
Proceedings of the Joint Conference of the Association
for Computers and the Humanities and the Association
for Literary and Linguistic Computing, Victoria, BC,
Canada.
S. Canu, Y. Grandvalet, V. Guigue, and A. Rakotoma-
monjy. 2005. SVM and kernel methods Matlab tool-
box. Perception Systmes et Information, INSA de
Rouen, Rouen, France.
V. Chasanis, A. Kalogeratos, and A. Likas. 2009. Movie
segmentation into scenes and chapters using locally
weighted bag of visual words. In Proceedings of the
ACM International Conference on Image and Video
Retrieval, pages 35:1?35:7, Santorini, Fira, Greece.
ACM Press.
R. M. Coyotl-Morales, L. Villasen?or-Pineda, M. Montes-
y-Go?mez, and P. Rosso. 2006. Authorship attribu-
tion using word sequences. In Proceedings of 11th
296
Iberoamerican Congress on Pattern Recognition, vol-
ume 4225 of LNCS, pages 844?852, Cancun, Mexico.
Springer.
D. Das and A. Martins. 2007. A survey on au-
tomatic text summarization. Available from:
http://www.cs.cmu.edu/?nasmith/LS2/
das-martins.07.pdf, Literature Survey for the
Language and Statistics II course at Carnegie Mellon
University.
O. de Vel, A. Anderson, M. Corney, and G. Mohay. 2001.
Multitopic email authorship attribution forensics. In
Proceedings of the ACM Conference on Computer Se-
curity - Workshop on Data Mining for Security Appli-
cations, Philadelphia, PA, USA.
K. Grauman. 2006. Matching Sets of Features for Ef-
ficient Retrieval and Recognition. Ph.D. thesis, Mas-
sachusetts Institute of Technology.
J. Houvardas and E. Stamatatos. 2006. N-gram fea-
ture selection for author identification. In Proceedings
of the 12th International Conference on Artificial In-
telligence: Methodology, Systems, and Applications,
volume 4183 of LNCS, pages 77?86, Varna, Bulgaria.
Springer.
V. Keselj, F. Peng, N. Cercone, and C. Thomas. 2003. N-
gram-based author profiles for authorship attribution.
In Proceedings of the Pacific Association for Compu-
tational Linguistics, pages 255?264, Halifax, Canada.
M. Koppel, J. Schler, and S. Argamon. 2009. Computa-
tional methods in authorship attribution. Journal of the
American Society for Information Science and Tech-
nology, 60:9?26.
J. Lafferty and G. Lebanon. 2005. Diffusion kernels
on statistical manifolds. Journal of Machine Learning
Research, 6:129?163.
M. Lambers and C. J. Veenman. 2009. Forensic author-
ship attribution using compression distances to pro-
totypes. In Computational Forensics, Lecture Notes
in Computer Science, Volume 5718. ISBN 978-3-642-
03520-3. Springer Berlin Heidelberg, 2009, p. 13, vol-
ume 5718 of LNCS, pages 13?24. Springer.
G. Lebanon, Y. Mao, and J. Dillon. 2007. The locally
weighted bag of words framework for document rep-
resentation. Journal of Machine Learning Research,
8:2405?2441.
D. Lewis, T. Yang, and F. Rose. 2004. RCV1: A new
benchmark collection for text categorization research.
Journal of Machine Learning Research, 5:361?397.
K. Luyckx and W. Daelemans. 2010. The effect of au-
thor set size and data size in authorship attribution.
Literary and Linguistic Computing, pages 1?21, Au-
gust.
Y. Mao, J. Dillon, and G. Lebanon. 2007. Sequential
document visualization. IEEE Transactions on Visu-
alization and Computer Graphics, 13(6):1208?1215.
F. Peng, D. Shuurmans, V. Keselj, and S. Wang. 2003.
Language independent authorship attribution using
character level language models. In Proceedings of the
10th conference of the European chapter of the Associ-
ation for Computational Linguistics, volume 1, pages
267?274, Budapest, Hungary.
F. Peng, D. Shuurmans, and S. Wang. 2004. Augmenting
naive Bayes classifiers with statistical language mod-
els. Information Retrieval Journal, 7(1):317?345.
S. R. Pillay and T. Solorio. 2010. Authorship attribution
of web forum posts. In Proceedings of the eCrime Re-
searchers Summit (eCrime), 2010, pages 1?7, Dallas,
TX, USA. IEEE.
S. Plakias and E. Stamatatos. 2008a. Author identifi-
cation using a tensor space representation. In Pro-
ceedings of the 18th European Conference on Artifi-
cial Intelligence, volume 178, pages 833?834, Patras,
Greece. IOS Press.
S. Plakias and E. Stamatatos. 2008b. Tensor space mod-
els for authorship attribution. In Proceedings of the 5th
Hellenic Conference on Artificial Intelligence: Theo-
ries, Models and Applications, volume 5138 of LNCS,
pages 239?249, Syros, Greece. Springer.
R. Rifkin and A. Klautau. 2004. In defense of one-vs-all
classification. Journal of Machine Learning Research,
5:101?141.
Y. Rubner, C. Tomasi, J. Leonidas, and J. Guibas. 2001.
The earth mover?s distance as a metric for image re-
trieval. International Journal of Computer Vision,
40(2):99?121.
F. Sebastiani. 2002. Machine learning in automated text
categorization. ACM Computing Surveys, 34(1):1?47.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
E. Stamatatos. 2006a. Authorship attribution based on
feature set subspacing ensembles. International Jour-
nal on Artificial Intelligence Tools, 15(5):823?838.
E. Stamatatos. 2006b. Ensemble-based author identifi-
cation using character n-grams. In Proceedings of the
3rd International Workshop on Text-based Information
Retrieval, pages 41?46, Riva del Garda, Italy.
E. Stamatatos. 2009a. Intrinsic plagiarism detec-
tion using character n-gram profiles. In Proceed-
ings of the 3rd International Workshop on Uncovering
Plagiarism, Authorship, and Social Software Misuse,
PAN?09, pages 38?46, Donostia-San Sebastian, Spain.
E. Stamatatos. 2009b. A survey of modern authorship
attribution methods. Journal of the American Society
for Information Science and Technology, 60(3):538?
556.
M. Tearle, K. Taylor, and H. Demuth. 2008. An
algorithm for automated authorship attribution using
neural networks. Literary and Linguist Computing,
23(4):425?442.
297
Y. Zhao and J. Zobel. 2005. Effective and scalable au-
thorship attribution using function words. In Proceed-
ings of 2nd Asian Information Retrieval Symposium,
volume 3689 of LNCS, pages 174?189, Jeju Island,
Korea. Springer.
298
Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 46?54,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Sexual predator detection in chats with chained classifiers
Hugo Jair Escalante
LabTL, INAOE
Luis Enrique Erro No. 1,
72840, Puebla, Mexico
hugojair@inaoep.mx
Esau? Villatoro-Tello?
Universidad Auto?noma Metropolitana
Unidad Cuajimalpa
Mexico City, Mexico
villatoroe@inaoep.mx
Antonio Jua?rez
LabTL, INAOE
Luis Enrique Erro No. 1,
72840, Puebla, Mexico
antjug@inaoep.mx
Luis Villasen?or
LabTL, INAOE
72840, Puebla, Mexico
villasen@inaoep.mx
Manuel Montes-y-Go?mez
LabTL, INAOE
72840, Puebla, Mexico
mmontesg@inaoep.mx
Abstract
This paper describes a novel approach for sex-
ual predator detection in chat conversations
based on sequences of classifiers. The pro-
posed approach divides documents into three
parts, which, we hypothesize, correspond to
the different stages that a predator employs
when approaching a child. Local classifiers
are trained for each part of the documents and
their outputs are combined by a chain strat-
egy: predictions of a local classifier are used
as extra inputs for the next local classifier.
Additionally, we propose a ring-based strat-
egy, in which the chaining process is iterated
several times, with the goal of further improv-
ing the performance of our method. We re-
port experimental results on the corpus used
in the first international competition on sex-
ual predator identification (PAN?12). Experi-
mental results show that the proposed method
outperforms a standard (global) classification
technique for the different settings we con-
sider; besides the proposed method compares
favorably with most methods evaluated in the
PAN?12 competition.
1 Introduction
Advances in communications? technologies have
made possible to any person in the world to com-
municate with any other in different ways (e.g.,
text, voice, and video) regardless of their geograph-
ical locations, as long as they have access to in-
ternet. This undoubtedly represents an important
and highly needed benefit to society. Unfortunately,
this benefit also has brought some collateral issues
?Esau? Villatoro is also external member of LabTL at
INAOE.
that affect the security of internet users, as nowa-
days we are vulnerable to many threats, including:
cyber-bullying, spam, fraud, and sexual harassment,
among others.
A particularly important concern has to do with
the protection of children that have access to inter-
net (Wolak et al, 2006). Children are vulnerable
to attacks from paedophiles, which ?groom? them.
That is, adults who meet underage victims online,
engage in sexually explicit text or video chat with
them, and eventually convince the children to meet
them in person. In fact, one out of every seven
children receives an unwanted sexual solicitation
online (Wolak et al, 2006). Hence, the detection
of cyber-sexual-offenders is a critical security issue
that challenges the field of information technologies.
This paper introduces an effective approach for
sexual predator detection (also called sexual preda-
tor identification) in chat conversations based on
chains of classifiers. The proposed approach di-
vides documents into three parts, with the hypoth-
esis that different parts correspond to the differ-
ent stages that predators adopt when approaching
a child (Michalopoulos and Mavridis, 2011). Lo-
cal classifiers are trained for each part of the doc-
uments and their outputs are combined by a chain-
ing strategy. In the chain-based approach the pre-
dictions of a local classifier are used as extra inputs
for the next local classifier. This strategy is inspired
from chain-based classifiers developed for the task
of multi-label classification (Read et al, 2011). A
ring-based approach is proposed, in which the gener-
ation of chains of classifiers is iterated several times.
We report experimental results in the corpus used in
the first international competition on sexual preda-
tor identification (PAN-2012) (Inches and Crestani,
46
2012). Experimental results show that chain-based
classifiers outperform standard classification meth-
ods for the different settings we considered. Further-
more, the proposed method compares favorably with
alternative methods developed for the same task.
2 Sexual predator detection
We focus on the detection of sexual predators in chat
rooms, among the many cyber-menaces targeting
children. This is indeed a critical problem because
most sexually-abused children have agreed volun-
tarily to met with their abuser (Wolak et al, 2006).
Therefore, anticipatively detecting when a person at-
tempts to approach a children, with malicious inten-
tions, could reduce the number of abused children.
Traditionally, a term that is used to describe mali-
cious actions with a potential aim of sexual exploita-
tion or emotional connection with a child is referred
as ?Child Grooming? or ?Grooming Attack? (Ku-
cukyilmaz et al, 2008). Defined in (Harms, 2007)
as: ?a communication process by which a perpetra-
tor applies affinity seeking strategies, while simulta-
neously engaging in sexual desensitization and in-
formation acquisition about targeted victims in or-
der to develop relationships that result in need ful-
fillment? (e.g. physical sexual molestation).
The usual approach1 to catch sexual predators is
through police officers or volunteers, whom behave
as fake children in chat rooms and provoke sexual
offenders to approach them. Unfortunately, online
sexual predators always outnumber the law enforce-
ment officers and volunteers. Therefore, tools that
can automatically detect sexual predators in chat
conversations (or at least serve as support tool for
officers) are highly needed.
A few attempts to automate processes related to
the sexual predator detection task have been pro-
posed already (Pendar, 2007; Michalopoulos and
Mavridis, 2011; RahmanMiah et al, 2011; Inches
and Crestani, 2012; Villatoro-Tello et al, 2012;
Bogdanova et al, 2013). The problem of detect-
ing conversations that potentially include a sex-
ual predator approaching a victim has been ap-
proached, for example, by (RahmanMiah et al,
2011; Villatoro-Tello et al, 2012; Bogdanova et al,
1Adopted for example by the Perverted Justice organization,
http://www.perverted-justice.com/
2013). RahmanMiah et al discriminated among
child-exploitation, adult-adult and general-chatting
conversations using a text categorization approach
and psychometric information (RahmanMiah et al,
2011). Recently, Bogdanova et al approached
the same problem, the authors concluded that stan-
dard text-mining features are useful to distinguish
general-chatting from child-exploitation conversa-
tions, but not for discriminating between child-
exploitation and adult-adult conversations (Bog-
danova et al, 2013). In the latter problem, fea-
tures that model behavior and emotion resulted par-
ticularly helpful. N. Pendar approached the prob-
lem of distinguishing predators from victims within
chat conversations previously confirmed as contain-
ing a grooming attack (Pendar, 2007). The author
collapsed all of the interventions from each partici-
pant into a document and approached the problem as
a standard text categorization task with two classes
(victim vs. predator).
A more fine grained approximation to the problem
was studied by (Michalopoulos and Mavridis, 2011).
The authors developed a probabilistic method that
classifies chat interventions into one of three classes:
1) Gaining Access: indicate predators intention to
gain access to the victim; 2) Deceptive Relationship:
indicate the deceptive relationship that the preda-
tor tries to establish with the minor, and are pre-
liminary to a sexual exploitation attack; and 3) Sex-
ual Affair: clearly indicate predator?s intention for
a sexual affair with the victim. These categories
correspond to the different stages that a sexual of-
fender adopt when approaching a child. As (Pen-
dar, 2007), (Michalopoulos and Mavridis, 2011) ap-
proached this problem as one of text categorization
(equating interventions to short-documents). They
removed stop words and applied a spelling correc-
tion strategy, their best results were obtained with a
Na??ve Bayes classifier, reaching performance close
to 96%. Thus giving evidence that the three cate-
gories can be recognized reasonably well. Which
in turn gives evidence that modeling the three stages
could be beneficial for recognizing sexual predators;
for example, when it is not known whether a con-
versation contains or not a grooming attack. This
is the underlying hypothesis behind the proposed
method. We aim to use local classifiers, special-
ized in the different stages a predator approaches a
47
child. Then, we combine the outputs of local classi-
fiers with the goal of improving the performance on
sexual predator detection in conversations including
both: grooming attacks and well-intentioned conver-
sations.
Because of the relevance of the problem, and of
the interest of several research groups from NLP,
it was organized in 2012 the first competition of
sexual predator identification (Inches and Crestani,
2012). The problem approached in the competition
was that of identifying sexual predators from con-
versations containing both: grooming attacks and
well-intentioned conversations. The organizers pro-
vided a large corpus divided into development and
evaluation data. Development (training) data were
provided to participants for building their sexual-
predator detection system. In a second stage, eval-
uation (testing) data were provided to participants,
whom had to apply their system to that data and sub-
mit their results. Organizers evaluated participants
using their predictions on evaluation data (labels for
the evaluation data were not provided to participants
during the competition).
Several research groups participated in that com-
petition, see (Inches and Crestani, 2012). Some
participants developed tailored features for detect-
ing sexual predators (see e.g., (Eriksson and Karl-
gren, 2012)), whereas other researchers focused on
the development of effective classifiers (Parapar et
al., 2012). The winning approach implemented a
two stage formulation (Villatoro-Tello et al, 2012):
in a first step suspicious conversations where iden-
tified using a two class classifier. Suspicious con-
versations are those that potentially include a sexual
predator (i.e., a similar approach to (RahmanMiah
et al, 2011)). In a second stage, sexual predators
were distinguished from victims in the suspicious
conversations identified in the first stage (a similar
approach to that of (Pendar, 2007)). For both stages
a standard classifier and a bag-of-words representa-
tion was used.
The methods proposed in this paper were eval-
uated in the corpus used in the first interna-
tional competition on sexual predator detection,
PAN?12 (Inches and Crestani, 2012). As explained
in the following sections, the proposed method uses
standard representation and classification methods,
therefore, the proposed methods can be improved if
we use tailored features or learning techniques for
sexual predator detection.
3 Chain-based classifiers for SPD
Chain-based classifiers were first proposed to deal
with multi-label classification (Read et al, 2011).
The goal was to incorporate dependencies among
different labels, which are disregarded by most
multi-label classification methods. The underlying
idea was to increase the input space of classifiers
with the outputs provided by classifiers trained for
other labels. The authors showed important im-
provements over traditional methods.
In this paper, we use chain-based classifiers to in-
corporate dependencies among local classifiers asso-
ciated to different segments of a chat conversation.
The goal is building an effective predator-detection
model made of a set of local models specialized at
classifying certain segments of the conversation. In-
tuitively, we would like to have a local model asso-
ciated to each of the stages in which a sexual preda-
tor approaches a child: gaining access, deceptive
relationship and sexual affair (Michalopoulos and
Mavridis, 2011). We associate a segment of the con-
versation to each of the three stages. The raw ap-
proach proposed in this work consists of dividing
the conversation into three segments of equal length.
The first, second and third segments of each conver-
sation are associated to the first, second and third
stages, respectively. Although, this approach is too
simple, our goal was to determine whether having
local classifiers combined via a chaining strategy
could improve the performance on sexual predator
detection.
We hypothesize that as the vocabulary used in dif-
ferent segments of the conversation is different, spe-
cialized models can result in better performance for
classifying these local segments. Since local classi-
fiers can only capture local information, it is desir-
able to somehow connect these classifiers in order to
make predictions taking into account the whole con-
versation. One way to make local classifiers depen-
dent is thought the chain-based methodology, where
the outputs of one local classifier are feed as inputs
for the next local classifier; the final prediction for
the whole conversation can be obtained in several
ways as described below.
48
The proposed approach is described in Figure 1.
Since our goal is to detect sexual predators from
chat conversations directly, we model each user
(well-intentioned user, victim or sexual predator)
by their set of interventions. Thus, we generate a
single conversation for each user using their inter-
ventions, keeping the order in which such interven-
tions happened. The approached problem is to clas-
sify these conversations into sexual-predator or any-
other-type-of-user. In the following we call sim-
ply conversations to the generated per-user conver-
sations.
Chat conversations are divided into three
(equally-spaced) parts. Next, one local-classifier
is trained for each part of the document according
to a predefined order2, where two out of the three
classifiers (second and third) are not independent.
Let p1, p2, and p3 denote the segments of text
that will be used for generating the first, second
and third classifiers. The triplet {p1, p2, p3} can
be any of the six permutations of 3 segments, this
tripled determines the order in which classifiers
will be built. Once that a particular order has been
defined, a first local-classifier, f1, is trained using
the part p1 from all of the training documents
(p1 ? {first, second, third}). Next, a second
local-classifier, f2, is trained by using the part p2
from all of the training documents. f2 is built
by using both attributes extracted from part p2 of
conversations and the outputs of the first classifier
over the training documents. Thus, classifier f2
depends on classifier f1, through the outputs of the
latter model. A third local-classifier, f3, is trained
using attributes extracted from part p3 from all
conversations, the input space for training f3 is
augmented with the predictions of classifiers f2 and
f1 over the training documents. Hence, the third
classifier depends on the outputs of the first and
second classifiers.
Once trained, the chain of local-classifiers can be
used to make predictions for the whole conversation
in different ways. When a test conversation needs to
be classified it is also split into 3 parts. Part p1 is
feeded to classifier f1, which generates a prediction
for f1. Next, part p2 from the test document, to-
2We hypothesize that building a chain of classifiers using
different orders results in different performances, we evaluate
this aspect in Section 4.
Figure 1: General diagram of the chain-based approach.
gether with the prediction for p1 as generated by f1
are feeded to classifier f2. Likewise, the outputs of
f2 and f1, together with part p3 from the document
are used as inputs for classifier f3. Clearly, since we
have predictions for the test document at the three
stages of the chain (from f1,2,3) we can make a pre-
diction at any stage. The prediction from classifier
f3 is called chain-prediction as it is the outcome of
the dependent local-classifiers.
Additionally to local and chain-prediction, we
propose a ring-like structure for chain-based classi-
fiers in which the outputs of the third local-classifier
are used again as inputs for another local model,
where the order can be different to that used in the
previous iteration. This process is iterated for a num-
ber of times, where we can make predictions at every
link (local-classifier) of the ring. In addition, after
a number of iterations we can make predictions by
combining the outputs (like in an ensemble) gener-
ated by all of the classifiers considered in the ring
up to that iteration. The underlying idea is to ex-
plore the performance of the chain as more local-
models, that can use short and long term dependen-
cies with other classifiers, are incorporated. Our hy-
pothesis is that after incorporating a certain number
of local-dependent-models, the predictions for the
whole conversations will be steady and will improve
the performance of the straight chain approach.
Algorithm 1 describes the proposed ring-based
classifier. E denotes the set of extra inputs that have
to be added to individual classifiers, which are the
cumulative outputs of individual classifiers. P is a
set of predefined permutations from which different
orders can be took from, where Pi is the ith per-
mutation. We denote with atts (pi, E) to the pro-
49
cess of extracting attributes from documents? part
pi and merging them with attributes stored in E .
atts generates the representation that a classifier
can use. train [f(X)] denotes the process of train-
ing classifier f using inputs X . Mc stores the mod-
els trained through the ring process.
Algorithm 1 Ring-based classifier.
Require: g : # iterations; P : set of permutations;
E = {}
i = 0; c = 1;
while i ? g do
i++;
{p1, p2, p3} ? Pi;
for j = 1? 3 do
X ? atts [pj , E ]
f?j ? train [fj(X)];
Mc ? f?j ;
E ? E ? f?j (pj , E);
c++;
end for
end while
return Mc : trained classifiers (ring-based approach);
When a test conversation needs to be labeled, the
set of classifiers in M are applied to it using the
same order in the parts that was used when generat-
ing the models. Each time a model is applied to the
test instance, the prediction of such model is used
to increase the input space that is to be used for the
next model. We call the prediction given by the last
model Mg, ring-prediction. One should note that,
as before, we can have predictions for the test con-
versation from every model Mi. Besides, we can
accumulate the predictions for the whole set of mod-
els M1,...,g. Another alternative is to combine the
predictions of the three individual classifiers in each
iteration of the ring (every execution of the for-loop
in Algorithm 1); this can be done, e.g., by weight
averaging. In the next section we report the perfor-
mance obtained by all these configurations.
4 Experiments and results
For the evaluation of the proposed approach we
considered the data set used in the first interna-
tional competition on sexual predator identification3
(PAN-2012) (Inches and Crestani, 2012). Table 1
3http://pan.webis.de/
presents some features from the considered data set.
The data set contains both chat conversations includ-
ing sexual predators approaching minors and (au-
thentic) conversations between users (which can or
cannot be related to a sexual topic). The data set pro-
vided by the organizers contained too much noisy in-
formation that could harm the performance of classi-
fication methods (e.g., conversations with only one
participant, conversations of a few characters long,
etc.). Therefore, we applied a preprocessing that
aimed to both remove noisy conversations and re-
ducing the data set for scalability purposes. The fil-
tering preprocessing consisted of eliminating: con-
versations with only one participant, conversations
with less than 6 interventions per each participant,
conversations that had long sequences of unrecog-
nized characters (images, apparently). The char-
acteristics of the data set after filtering are shown
within parentheses in Table 1. It can be seen that
the size of the data set was reduced considerably,
although a few sexual predators were removed, we
believe the information available from them was in-
sufficient to recognize them.
Table 1: Features of the data set considered for experi-
mentation (Inches and Crestani, 2012). We show the fea-
tures of the raw data and in parentheses the corresponding
features after applying the proposed preprocessing.
Feature Development Evaluation
# Convers. 66, 928 (6, 588) 155, 129 (15, 330)
# Users 97, 690 (11, 038) 218, 702 (25, 120)
# Sexual Pr. 148(136) 254 (222)
Conversations were represented using their bag-
of-words. We evaluated the performance of dif-
ferent representations and found that better results
were obtained with a Boolean weighting scheme.
No stop-word removal nor stemming was applied,
in fact, punctuation marks were conserved. We pro-
ceeded this way because we think in chat conver-
sations every character conveys useful information
to characterize users, victims and sexual predators.
This is because of the highly unstructured and in-
formal language used in chat conversations, as dis-
cussed in related works (Kucukyilmaz et al, 2008;
RahmanMiah et al, 2011; Rosa and Ellen, 2009).
For indexing conversations we used the TMG
toolbox (Zeimpekis and Gallopoulos, 2006). The re-
50
sultant vocabulary was of 56, 964 terms. For build-
ing classifiers we used a neural network as imple-
mented in the CLOP toolbox (Saffari and Guyon,
2006). Our choice is based on results from a prelim-
inary study.
4.1 Performance of local classifiers
We first evaluate the performance of global and local
classifiers separately. A global classifier is that gen-
erated using the content of the whole conversation, it
resembles the formulation from (Pendar, 2007). Lo-
cal classifiers were generated for each of the seg-
ments. Table 2 shows the performance of the global
and local models. We report the average (of 5 runs)
of precision, recall and F1 measure for the positive
class (sexual predators).
Table 2: Performance of global (row 2) and local classi-
fiers (rows 3-6).
Setting Precision Recall F1 Measure
Global 95.14% 49.91% 65.42%
Segment 1 96.16% 59.20% 73.23%
Segment 2 96.25% 48.82% 64.72%
Segment 3 93.43% 51.87% 66.68%
It can be seen from Table 2 that the performance
of the global model and that obtained for segments
2 and 3 are comparable to each other in terms of
the three measures we considered. Interestingly,
the best performance was obtained when the only
the first segment of the conversation was used for
classification. The difference is considerable, about
11.93% of relative improvement. This is a first con-
tribution of our work: using the first segment of a
conversation can improve the performance obtained
by a global classifier. Since the first segment of con-
versations (barely) corresponds to the gaining ac-
cess stage, the result provides evidence that sexual
predators can be detected by the way they start ap-
proaching to their victims. That is, the way a well-
intentioned person starts a conversation is somewhat
different to that of sexual predators approaching a
child. Also, it is likely that this makes a difference
because for segments 2 and 3, conversations con-
taining grooming attacks and well-intentioned con-
versations can be very similar (well-intentioned con-
versations can deal sexual thematic as well).
4.2 Chain-based classifiers
In this section we report the performance obtained
by different settings of chain based classifiers. We
first report the performance of the chain-prediction
strategy, see Section 3. Figure 2 shows the precision,
recall and F1 measure, obtained by the chain-based
classifier for the different permutations of the 3 seg-
ments (i.e., all possible orders for the segments). For
each order, we report the initial performance (that
obtained with the segment in the first order) and the
chain-prediction, that is the prediction provided by
the last classifier in the chain.
Figure 2: F1 measure by the initial and chain-based clas-
sifier for different orders.
From Figure 2 it can be observed that the chain-
prediction outperformed the initial classifier for
most of the orders in terms of F1 measure. For or-
ders starting with segment 1 (1-2-3 and 1-3-2) chain-
based classifiers worsen the initial performance.
This is due to the high performance of local clas-
sifier for segment 1 (see Table 2), which cannot be
improved with successive local classifiers. However,
the best performance overall was obtained by the
chain-based classifier with the order 2-3-1. The rela-
tive improvement of this configuration for the chain-
based method over the global classifier (the one us-
ing the whole conversations) was of 18.52%. One
should note that the second-best performance was
obtained with the order 3-2-1. Hence, putting the
most effective classifier (that for segment 1) at the
end seems to have a positive influence in the chain-
based classifier. We have shown evidence that chain-
based classifiers outperform both the global classi-
fier and any of the local methods. Also, the order of
classifiers is crucial for obtaining acceptable results
with the chain technique: using the best classifier
in the last position yields better performance; and,
putting the best classifier at the beginning would
lead the chain to worsen initial performance.
51
4.3 Ring-based classifiers
In this section we report experimental results on sex-
ual predator detection obtained with the ring-based
strategy. Recall a ring-based classifier can be seen as
a chain that is replicated several times with different
orders, so we can have predictions for each of the
local classifiers at each node of the ring/chain. Be-
sides, we can obtain periodical/cumulative predic-
tions from the chain and predictions derived from
combining predictions from a subset of local classi-
fiers in the chain. We explore the performance of all
of these strategies in the rest of this section.
We implement ring-based classifiers by succes-
sively applying chain-based classifiers with differ-
ent orders. We consider the following alternatives
for detecting predators with ring-based classifiers:
? Local. We make predictions with local classifiers
each time a local classifier is added to the ring (no
dependencies are considered). We report the av-
erage performance (segments avg.) and the maxi-
mum performance (segments max.) obtained by lo-
cal classifiers in each of the orders tried.
? Chain-prediction. We make predictions with
chain-based classifiers each time a local classifier
is added to the ring. We report the average perfor-
mance (chain-prediction avg.) and the maximum
performance (chain-prediction max.) obtained by
chain-based classifiers per each of the orders tried.
? Ensemble of chain-based classifiers. We combine
the outputs of the three chain-based classifiers built
for each order; this method is referred to as LC-
Ensemble.
? Cumulative ensemble. We combine the outputs
(via averaging) of all the chain-based classifiers that
have been built each time an order is added to the
ring; we call this method Cumulative-Ensemble.
Besides reporting results for these approaches we
also report the performance obtained by the global
classifier (Whole conversations), see Table 2.
We iterated the ring-based classifier for a fixed
number of orders. We tried 24 orders, repeating the
following process two times: we tried the permu-
tations of the 3 segments in lexicographical order,
followed by the same permutations on inverted lex-
icographical order. So a total of 24 different orders
were evaluated. Figure 3 shows the results obtained
by the different settings we consider for a typical run
of our approach.
Several findings can be drawn from Figure 3.
With exception of the average of local classifiers
(segments avg.), all of the methods outperformed
consistently the global classifier (whole conversa-
tions). Thus confirming the competitive perfor-
mance of local classifiers and that of chain-based
variants. The best local classifier from each or-
der (segments max.) achieved competitive perfor-
mance, although it was outperformed by the average
of chain-based classifiers (chain-prediction avg.).
Since local classifiers are independent, no tendency
on their performance can be observed as more orders
are tried. On the contrary, the performance chain-
based methods (as evidenced by the avg. and max
of chain-predictions) improves for the first 8-9 or-
ders and then remains steady. In fact, the best (per-
order) chain-prediction (chain-prediction max.) ob-
tained performance comparable to that obtained by
ensemble methods. One should note, however, that
in the chain-prediction max. formulation we report
the best performance from each order tried, which
might correspond to different segments in the differ-
ent orders. Therefore, it is not clear how the select
the specific order to use and the specific segment of
the chain that will be used for making predictions,
when putting in practice the method for a sexual-
predator detection system. Notwithstanding, stable
average predictions can be obtained when more than
6-8 orders are used (chain-prediction avg.), still the
performance of this approach is lower than that of
ensembles.
Clearly, the best performance was obtained
with the ensemble methods: chain-ensemble and
cumulative-ensemble. Both approaches obtained
similar performance, although the chain-ensemble
slightly outperformed cumulative-ensemble. The
chain-ensemble considers dependencies within each
order and not across orders, thus its performance af-
ter trying the 6 permutations of 3 segments did not
vary significantly. This is advantageous as only 6
orders have to be evaluated to obtain competitive
performance. Unfortunately, as with single chain-
classifiers it may be unclear how to select the par-
ticular order to use to implement a sexual-predator
detection system.
On the other hand, the cumulative-ensemble ob-
52
Figure 3: Performance of the different variants of ring-based classifiers for sexual predator detection.
tained stable performance after ? 12 orders were
considered. Recall this method incorporates depen-
dencies among the different orders tried. Although
it requires the evaluation of more orders than the
chain-ensemble to converge, this method is advanta-
geous for a real application: after a certain number
of orders it achieves steady performance, and since
it averages the outputs of all of the chain-classifiers
evaluated up to a certain iteration, its performance
does not rely on selecting a particular configuration.
In consequence, we claim the cumulative-ensemble
offers the best tradeoff between performance, stabil-
ity and model selection.
4.4 Comparison with related works
Table 3 shows a comparison of the configuration
cumulative-ensemble against the top-ranked partic-
ipants in the PAN?12 competition. We show the
performance of the top-5 participants as described
in (Inches and Crestani, 2012), additionally we re-
port the average performance obtained by the meth-
ods of the 16 participating teams. We report, F1
and F0.5 measures, and the rank for each participant.
We report F0.5 measure because that was the leading
evaluation measure for the PAN?12 competition.
From Table 3 it can be observed that the proposed
method is indeed very competitive. The results ob-
tained by our method outperformed significantly the
average performance (row 7) obtained by all of the
participants in all of the considered measures. In
terms of F1 measure our method would be ranked in
the fourth position, while in terms of the F0.5 mea-
sure our method would be ranked third.
Table 3: Comparison of the proposed method with related
works evaluated in the PAN?12 competition (Inches and
Crestani, 2012).
Participant F1 F0.5 Rk.
(Villatoro-Tello et al, 2012) 87.34 93.46 1
(Inches and Crestani, 2012) 83.18 91.68 2
(Parapar et al, 2012) 78.16 86.91 3
(Morris and Hirst, 2012) 74.58 86.52 4
(Eriksson and Karlgren, 2012) 87.48 86.38 5
(Inches and Crestani, 2012) 49.10 51.06 -
Our method 78.98 89.14 -
5 Conclusions
We introduced a novel approach to sexual-predator
detection in which documents are divided into 3
segments, which, we hypothesize, could correspond
to the different stages in that a sexual predator ap-
proaches a child. Local classifiers are built for each
of the segments, and the predictions of local classi-
fiers are combined through a strategy inspired from
chain-based classifiers. We report results on the
corpus used in the PAN?12 competition, the pro-
posed method outperforms a global approach. Re-
sults are competitive with related works evaluated in
PAN?12. Future work includes applying the chain-
based classifiers under the two-stage approach from
Villatoro et al (Villatoro-Tello et al, 2012).
Acknowledgments
This project was supported by CONACYT under
project grant 134186. The authors thank INAOE,
UAM-C and SNI for their support.
53
References
D. Bogdanova, P. Rosso, and T. Solorio. 2013. Explor-
ing high-level features for detecting cyberpedophilia.
In Special issue on on Computational Approaches
to Subjectivity, Sentiment and Social Media Analysis
(WASSA 2012), Computer Speech and Language (ac-
cepted).
G. Eriksson and J. Karlgren. 2012. Features for mod-
elling characteristics of conversations. In P. Forner,
J. Karlgren, and C. Womser-Hacker, editors, Working
notes of the CLEF 2012 Evaluation Labs and Work-
shop, Rome, Italy. CLEF.
C. Harms. 2007. Grooming: An operational definition
and coding scheme. Sex Offender Law Report, 8(1):1?
6.
G. Inches and F. Crestani. 2012. Overview of the inter-
national sexual predator identification competition at
PAN-2012. In P. Forner, J. Karlgren, and C. Womser-
Hacker, editors, Working notes of the CLEF 2012
Evaluation Labs and Workshop, Rome, Italy. CLEF.
T. Kucukyilmaz, B. Cambazoglu, C. Aykanat, and F. Can.
2008. Chat mining: predicting user and message at-
tributes in computer-mediated communication. In In-
formation Processing and Management, 44(4):1448?
1466.
D. Michalopoulos and I. Mavridis. 2011. Utilizing doc-
ument classification for grooming attack recognition.
In Proceedings of the IEEE Symposium on Computers
and Communications, pages 864?869.
C. Morris and G. Hirst. 2012. Identifying sexual preda-
tors by svm classification with lexical and behavioral
features. In P. Forner, J. Karlgren, and C. Womser-
Hacker, editors, Working notes of the CLEF 2012
Evaluation Labs and Workshop, Rome, Italy. CLEF.
J. Parapar, D. E. Losada, and A. Barreiro. 2012. A
learning-based approach for the identification of sex-
ual predators in chat logs. In P. Forner, J. Karlgren,
and C. Womser-Hacker, editors, Working notes of the
CLEF 2012 Evaluation Labs and Workshop, Rome,
Italy. CLEF.
N. Pendar. 2007. Toward spotting the pedophile telling
victim from predator in text chats. In Proceedings of
the IEEE International Conference on Semantic Com-
puting, pages 235?241, Irvine California USA.
M. W. RahmanMiah, J. Yearwood, and S. Kulkarni.
2011. Detection of child exploiting chats from a mixed
chat dataset as text classification task. In Proceed-
ings of the Australian Language Technology Associ-
ation Workshop, pages 157?165.
J. Read, B. Pfahringer, G. Holmes, and E. Frank. 2011.
Classifier chains for multi-label classification. Ma-
chine Learning Journal, 85(3):333?359.
K. D. Rosa and J. Ellen. 2009. Text classification
methodologies applied to micro-text in military chat.
In Proceedings of the eight IEEE International Con-
ference on Machine Learning and Applications, pages
710?714.
A. Saffari and I Guyon. 2006. Quick start guide for
CLOP. Technical report, Graz-UT and CLOPINET,
May.
E. Villatoro-Tello, A. Jua?rez-Gonza?lez, H. J. Escalante,
M. Montes-Y-Go?mez, and L. Villasen?or-Pineda. 2012.
A two-step approach for effective detection of mis-
behaving users in chats. In P. Forner, J. Karlgren,
and C. Womser-Hacker, editors, Working notes of the
CLEF 2012 Evaluation Labs and Workshop, Rome,
Italy. CLEF.
J. Wolak, K. Mitchell, and D. Finkelhor. 2006. On-
line victimization of youth: Five years later. Bulleting
07-06-025, National Center for Missing and Exploited
Children, Alexandia, Alexandria, VA.
D. Zeimpekis and E. Gallopoulos, 2006. Grouping Mul-
tidimensional Data: Recent Advances in Clustering,
chapter TMG: A MATLAB toolbox for generating
term-document matrices from text collections, pages
187?210. Springer.
54
