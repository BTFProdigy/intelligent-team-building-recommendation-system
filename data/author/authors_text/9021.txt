Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Building a Large-Scale Annotated Chinese Corpus

Nianwen Xue
IRCS, University of Pennsylvania
Suite 400A, 3401 Walnut Street
Philadelphia, PA 19104, USA
xueniwen@linc.cis.upenn.edu
Fu-Dong Chiou and Martha Palmer
CIS, University of Pennsylvania
200 S 33rd Street
Philadelphia, PA 19104, USA
{chioufd,mpalmer}@linc.cis.upenn.edu

Abstract
In this paper we address issues related to
building a large-scale Chinese corpus. We
try to answer four questions: (i) how to
speed up annotation, (ii) how to maintain
high annotation quality, (iii) for what
purposes is the corpus applicable, and
finally (iv) what future work we anticipate.
Introduction
The Penn Chinese Treebank (CTB) is an
ongoing project, with its objective being to
create a segmented Chinese corpus annotated
with POS tags and syntactic brackets. The first
installment of the project (CTB-I) consists of
Xinhua newswire between the years 1994 and
1998, totaling 100,000 words, fully segmented,
POS-tagged and syntactically bracketed and it
has been released to the public via the Penn
Linguistic Data Consortium (LDC). The
preliminary results of this phase of the project
have been reported in Xia et al (2000). Currently
the second installment of the project, the
400,000-word CTB-II is being developed and is
expected to be completed early in the year 2003.
CTB-II will follow the standards set up in the
segmentation (Xia 2000b), POS tagging (Xia
2000a) and bracketing guidelines (Xue and Xia
2000) and it will use articles from Peoples'
Daily, Hong Kong newswire and material
translated into Chinese from other languages in
addition to the Xinhua newswire used in CTB-I
in an effort to diversify the sources.

The availability of CTB-I changed our approach
to CTB-II considerably. Due to the existence of
CTB-I, we were able to train new automatic
Chinese language processing (CLP) tools, which
crucially use annotated corpora as training
material. These tools are then used for
preprocessing in the development of the CTB-II.
We also developed tools to control the quality of
the corpus. In this paper, we will address three
issues in the development of the Chinese
Treebank: annotation speed, annotation accuracy
and usability of the corpus. Specifically, we
attempt to answer four questions: (i) how do we
speed up the annotation process, (ii) how do we
maintain high quality, i.e. annotation accuracy
and inter-annotator consistency during the
annotation process, and (iii) for what purposes is
the corpus applicable, and (iv) what are our
future plans? Although we will touch upon
linguistic problems that are specific to Chinese,
we believe these issues are general enough for
the development of any single language corpus.
1 Annotation Speed
There are three main factors that affect the
annotation speed : annotators? background,
guideline design and more importantly, the
availability of preprocessing tools. We will
discuss how each of these three factors affects
annotation speed.
1.1 Annotator Background
Even with the best sets of guidelines, it is
important that annotators have received
considerable training in linguistics, particularly
in syntax. In both the segmentation/POS tagging
phase and the syntactic bracketing phase,
understanding the structure of the sentences is
essential for correct annotation with reasonable
speed. For example,
 
/de is assigned two part-
of-speech tags, depending on where it occurs in
the sentence. It is tagged as DEC when it marks
the end of the preceding modifying clause and
DEG when it follows a nominal phrase. This
distinction is useful in that it marks different
relations : between the nominal phrase and the
noun head, and between the clause and the noun
head respectively.

1.a.   /NN  /DEG  /NN
        leader         DE          responsibility
        ?leader?s responsibility?
  b. 	 /NT 
 /VV  /DEC  	 /NN  /NN
      recently hold       DE        demonstration
      ?recently held demonstration?

During the bracketing phase, the modifying
clause is further divided into relative clauses and
complement (appositive) clauses. The structures
of these two types of clauses are different, as
illustrated in 2:

2.a. (NP (CP (WHNP-1 *OP*)
                      (CP  (IP  (NP-SBJ (-NONE- *T*-1))
                        (VP (NP-TMP  /NT)     recently
                         (VP  /hold)))               hold
                                /DEC))
         (NP  /NN CombiningClassifiersforChineseWordSegmentation

NianwenXue

InstituteforResearchinCognitiveScience
UniversityofPennsylvania
Suite400A,3401Walnut
Philadelphia,PA19014
xueniwen@linc.cis.upenn.edu
SusanP.Converse

Dept.ofComputerandInformationScience
UniversityofPennsylvania
200South33rdStreet,
Philadelphia,PA19104-6389
spc@linc.cis.upenn.edu



Abstract

 In this paper we report results of a
supervised machine-learning approach to
Chinese word segmentation. First, a maximum
entropy tagger is trainedonmanually annotated
data to automatically labels the characters with
tagsthatindicatethepositionofcharacterwithin
a word. An error-driven transformation-based
tagger is then trained to clean up the tagging
inconsistencies of the first tagger. The tagged
output is then converted into segmented text.
The preliminary results show that this approach
is competitive compared with other supervised
machine-learning segmenters reported in
previousstudies.

1 Introduction

It is generally agreed among researchers that
word segmentation is a necessary first step in
Chinese language processing. Most of the
previous work in this area views a good
dictionaryasthecornerstoneofthistask.Several
word segmentation algorithms have been
developedusingadictionaryasanessentialtool.
Most notably, variants of the maximum
matching algorithm have been applied to word
segmentation with considerable success.  The
results that have been reported are generally in
the upper 90 percentile range.  However, the
successofsuchalgorithmsispremisedonalarge,
exhaustive dictionary. The accuracy of word
segmentation degrades sharply as new words
appear. Since Chinese word formation is a
highlyproductiveprocess,newwordsarebound
to appear in substantial numbers in realistic
scenarios(WuandJiang1998,Xue2001),andit
is virtually impossible to list all the words in a
dictionary.Inrecentyears,asannotatedChinese
corpora have become available, various
machine-learning approaches have been applied
to Chinese word segmentation, with different
levels of success. Compared with dictionary-
based approaches, machine-learning approaches
have the advantage of not needing a dictionary
and thus are more suitable for use on naturally
occurring Chinese text. In this paper we report
results of a supervised machine-learning
approach towards Chinese word segmentation
that combines two fairly standard machine
learningmodels.We show that this approach is
very promising comparedwith dictionary-based
approaches as well as other machine-learning
approaches that have been reported in the
literature.

2 Combining Classifiers for
Chinesewordsegmentation

Thetwomachine-learningmodelsweuseinthis
work are the maximum entropy model
(Ratnaparkhi 1996) and the error-driven
transformation-based learning model (Brill
1994).Weusetheformerasthemainworkhorse
and the latter to correct some of the errors
producedbytheformer.

2.1Reformulatingwordsegmentation
asataggingproblem

Before we apply the machine-learning
algorithms we first convert the manually
segmented words in the corpus into a tagged
sequence of Chinese characters. To do this, we
tageachcharacterwithoneofthefourtags,LL,
RR, MM and LR, depending on its position
withinaword.ItistaggedLLifitoccursonthe
leftboundaryofaword,and formsawordwith
thecharacter(s)onitsright. It is taggedRRif it
occurs on the right boundary of a word, and
formsawordwith thecharacter(s)on its left. It
istaggedMMifitoccursinthemiddleofaword.
It is taggedLR if it forms awordby itself.We
call such tags position-of-character (POC) tags
todifferentiatethemfromthemorefamiliarpart-
of-speech(POS)tags.Forexample,themanually
segmentedstringin(1)awillbetaggedasshown
in(1)b:

(1)a.          
	        

   Annotating the Propositions in the Penn Chinese Treebank
Nianwen Xue
Dept. of Computer and Info. Science
University of Pennsylvania
Philadelphia, PA 19104, USA
xueniwen@linc.cis.upenn.edu
Martha Palmer
Dept. of Computer and Info. Science
University of Pennsylvania
Philadelphia, PA 19104, USA
mpalmer@linc.cis.upenn.edu
Abstract
In this paper, we describe an approach to
annotate the propositions in the Penn Chi-
nese Treebank. We describe how diathe-
sis alternation patterns can be used to
make coarse sense distinctions for Chi-
nese verbs as a necessary step in anno-
tating the predicate-structure of Chinese
verbs. We then discuss the representation
scheme we use to label the semantic argu-
ments and adjuncts of the predicates. We
discuss several complications for this type
of annotation and describe our solutions.
We then discuss how a lexical database
with predicate-argument structure infor-
mation can be used to ensure consistent
annotation. Finally, we discuss possible
applications for this resource.
1 Introduction
Linguistically interpreted corpora are instrumental
in supervised machine learning paradigms of natu-
ral language processing. The information encoded
in the corpora to a large extent determines what can
be learned by supervised machine learning systems.
Therefore, it is crucial to encode the desired level of
information for its automatic acquisition. The cre-
ation of the Penn English Treebank (Marcus et al,
1993), a syntactically interpreted corpus, played a
crucial role in the advances in natural language pars-
ing technology (Collins, 1997; Collins, 2000; Char-
niak, 2000) for English. The creation of the Penn
Chinese Treebank (Xia et al, 2000) is also begin-
ning to help advance technologies in Chinese syn-
tactic analysis (Chiang, 2000; Bikel and Chiang,
2000). Since the treebanks are generally syntac-
tically oriented (cf. Sinica Treebank (Chen et al,
to appear)), the information encoded there is ?shal-
low?. Important information useful for natural lan-
guage applications is missing. Most notably, signifi-
cant regularities in the predicate-argument structure
of lexical items are not captured. Recent effort in
semantic annotation, the creation of the Penn Propo-
sition Bank (Kingsbury and Palmer, 2002) on top
of the Penn English Treebank is beginning to ad-
dress this issue for English. In this new layer of
annotation, the regularities of the predicates, mostly
verbs, are captured in the predicate-argument struc-
ture. For example, in the sentences ?The Congress
passed the bill? and ?The bill passed?, it is intu-
itively clear that ?the bill? plays the same role in the
two occurrences of the verb ?pass?. Similar regular-
ities also exist in Chinese. For example, in ? /this
/CL /bill /pass /AS? and ? /Congress
/pass /AS /this /CL /bill?, ? /bill?
also plays the same role for the verb ? /pass? even
though it occurs in different syntactic positions (sub-
ject and object respectively).
Capturing such lexical regularities requires a
?deeper? level of annotation than generally provided
in a typical syntactically oriented treebank. It also
requires making sense distinctions at the appropriate
granularity. For example, the regularities demon-
strated for ?pass? does not exist in other senses of
this verb. For example, in ?He passed the exam?
and ?He passed?, the object ?the exam? of the tran-
sitive use of ?pass? does not play the same role as
the subject ?he? of the intransitive use. In fact, the
subject plays the same role in both sentences.
However, how deep the annotation can go is con-
strained by two important factors: how consistently
human annotators can implement this type of anno-
tation (the consistency issue) and whether the an-
notated information is learnable by machine (the
learnability issue). Making fine-grained sense dis-
tinctions, in particular, has been known to be dif-
ficult for human annotators as well as machine-
learning systems (Palmer et al, submitted). It seems
generally true that structural information is more
learnable than non-structural information, as evi-
denced by the higher parsing accuracy and relatively
poor fine-grained WSD accuracy. With this in mind,
we will propose a level of semantic annotation that
still can be captured in structural terms and add this
level of annotation to the Penn Chinese Treebank.
The rest of the paper is organized as follows. In Sec-
tion 2, we will discuss the annotation model in de-
tail and describe our representation scheme. We will
discuss some complications in Section 3 and some
implementation issues in Section 4. Possible appli-
cations of this resource are discussed in Section 5.
We will conclude in Section 6.
2 Annotation Model
In this section we describe a model that annotates
the predicate-argument structure of Chinese pred-
icates. This model captures the lexical regulari-
ties by assuming that different instances of a pred-
icate, usually a verb, have the same predicate argu-
ment structure if they have the same sense. Defin-
ing sense has been one of the most thorny issues
in natural language research (Ide and Vronis, 1998),
and the term ?sense? has been used to mean differ-
ent things, ranging from part-of-speech and homo-
phones, which are easier to define, to slippery fine-
grained semantic distinctions that are hard to make
consistently. Determining the ?right? level of sense
distinction for natural language applications is ul-
timately an empirical issue, with the best level of
sense distinction being the level with the least granu-
larity and yet sufficient for a natural language appli-
cation in question. Without gearing towards one par-
ticular application, our strategy is to use the struc-
tural regularities demonstrated in Section 1 to define
sense. Finer sense distinctions without clear struc-
tural indications are avoided. All instances of a pred-
icate that realize the same set of semantic roles are
assumed to have one sense, with the understanding
that not all of the semantic roles for this verb sense
have to be realized in a given verb instance, and that
the same semantic role may be realized in different
syntactic positions. All the possible syntactic real-
izations of the same set of semantic roles for a verb
sense are then alternations of one another. This
state of affairs has been characterized as diathe-
sis alternation and used to establish cross-predicate
generalizations and classifications (Levin, 1993). It
has been hypothesized and demonstrated that verbs
sharing the same disthesis alternation patterns also
have similar meaning postulates. It is equally plausi-
ble to assume then that verb instances having differ-
ent diathesis alternation patterns also have different
semantic properties and thus different senses.
Using diathesis alternation patterns as a diagnos-
tic test, we can identify the different senses for a
verb. Alternating syntactic frames for a particular
verb sense realizing the same set of semantic roles
(we call this roleset) form a frameset and share sim-
ilar semantic properties. It is easy to see that each
frameset, a set of syntactic frames for a verb, corre-
sponds with one roleset and vice versa. From now
on, we use the term frameset instead of sense for
clarity. Each frameset consists of one or more syn-
tactic frames and each syntactic frame realizes one
or more semantic roles. One frame differs from an-
other in the number and type of arguments its pred-
icate actually takes, and one frameset differs from
another in the total number and type of arguments
its predicate CAN take. This is illustrated graphi-
cally in Figure 1.
Annotating the predicate-argument structure in-
volves mapping the frameset identification informa-
tion for a predicate to an actual predicate instance in
the corpus and assign the semantic roles to its argu-
ments based on the syntactic frame of that predicate
instance. It is hoped that since framesets are defined
through diathesis alternation of syntactic frames, the
distinctions made are still structural in nature and
thus are machine-learnable and can be consistently
annotated by human annotators.
So far our discussion has focused on semantic ar-
Verb
FS                    FS               FS                 ......          FS
FR FR FR......FR j
        i
.....
Argk Arg ArgArg0 1 2
   0        1   2
0 1 2
FS = Frameset       FR = Syntactic Frames       Arg = Arguments
Figure 1: Annotation model
guments, which play a central role in determining
the syntactic frames and framesets. There are other
elements in a proposition: semantic adjuncts. Com-
pared with semantic arguments, semantic adjuncts
do not play a role in defining the syntactic frames
or framesets because they occur in a wide variety of
predicates and as a result are not as discriminative as
semantic arguments. On the other hand, since they
can co-occur with a wide variety of predicates, they
are more generalizable and classifiable than seman-
tic arguments. In the next section, we will describe a
representation scheme that captures this dichotomy.
2.1 Representing arguments and adjuncts
Since the number and type of semantic arguments
for a predicate are unique and thus define the seman-
tic roles for a predicate, we label the arguments for
a predicate with a contiguous sequence of integers,
in the form of argN, where
 
is the integer between
0 and 5. Generally, a predicate has fewer than 6 ar-
guments. Since semantic adjuncts are not subcate-
gorized for by the predicate, we use one label argM
for all semantic adjuncts. ArgN identifies the argu-
ments while argM identifies all adjuncts. An argN
uniquely identifies an argument of a predicate even
if it occupies different syntactic positions in different
predicate instances. Missing arguments of a predi-
cate instance can be inferred by noting the missing
argument labels.
Additionally, we also use secondary tags to gen-
eralize and classify the semantic arguments and ad-
juncts when possible. For example, an adjunct re-
ceiving a  tag if it is a temporal adjunct. The
secondary tags are reserved for semantic adjuncts,
predicates that serve as arguments, as well as certain
arguments for phrasal verbs. The 18 secondary tags
and their descriptions are presented in Table 1.
11 functional tags for semantic adjuncts
ADV adverbial, default tag
BNF beneficiary
CND condition
DIR direction
DGR degree
FRQ frequency
LOC locative
MNR manner
PRP purpose or reason
TMP temporal
TPC topic
1 functional tag for predicate as argument
PRD predicate
6 functional tags for arguments to phrasal verbs
AS , , ,
AT ,
INTO , ,
ONTO
TO ,
TOWARDS ,
Table 1: List of functional tags
3 Complications
In this section we discuss several complications in
annotating the predicate-argument structure as de-
scribed in Section 2. Specifically, we discuss the
phenomenon of ?split arguments? and the annota-
tion of nominalized verbs (or deverbal nouns).
3.1 Split Arguments
What can be characterized as ?split arguments? are
cases where a constituent that occurs as one argu-
ment in one sentence can also be realized as mul-
tiple arguments (generally two) for the same pred-
icate in another sentence, without causing changes
in the meaning of the sentences. This phenomenon
surfaces in several different constructions. One such
construction involves ?possessor raising?, where the
possessor (in a broad sense) raises to a higher posi-
tion. Examples 1a and 1b illustrate this. In 1a, the
possessor originates from the subject position and
raises to the topic1 position, while in 1b, the pos-
sessor originates from the object position and raises
1In Chinese, it is possible to have a topic in addition to the
subject. The topic is higher than the subject and plays an im-
portant role in the sentence (Li and Thompson, 1976).
to the subject position. The exact syntactic analysis
is not important here, and what is important is that
one argument in one sentence becomes two in an-
other. The challenge is then to capture this regularity
when annotating the predicate-argument structure of
the verb.
1. Possessor Raising
a. Subject to Topic
(IP (NP-PN-TPC /China)
(NP-TMP /last year)
(NP-SBJ /import-export
/total volume)
(VP /exceed
(QP-OBJ /325 Billion
(CLP /US. Dollar))))
/exceed
arg0-psr: /China
arg0-pse: /import-export /total volume
arg1: /325 Billion /US. Dollar
(IP (NP-TMP /last year)
(NP-SBJ (DNP (NP-PN /China)
/DE)
(NP /import-export
/volume))
(VP /exceed
(QP-OBJ /325 Billion
(CLP /US. Dollar))))
/exceed
arg0: /China /DE /import-export
/volume
arg1: /325 Billion /US. Dollar
b. Object to Subject
(IP (NP-SBJ (NP-PN /China)
(NP /economy
/expansion))
(VP (ADVP /also)
(ADVP /will)
(VP /slow down
(NP-OBJ /speed)))
/slow down
arg1-psr: /China /economy /expansion
arg1-pse: /speed
(IP (NP-SBJ (DNP (NP (NP-PN /China)
(NP /economy
/expansion))
)
(NP /speed))
(VP (ADVP /also)
(ADVP /will)
(VP /slow down))
/slow down
arg1: /China /economy /expansion
/DE /speed
Another case of ?split arguments? involves the co-
ordinated noun phrases. In 2a, for example, the co-
ordinated structure as a whole is an argument to the
verb ? /sign?. In contrast, in 2b, one piece of the
argument, ? /China? is realized as a noun phrase
introduced by a preposition. There is no apparent
difference in meaning for the two sentences.
2. Coordination vs. Prepositional phrase
a. (IP (NP-PN-SBJ /Burma
/and
/China)
(VP (ADVP /already)
(VP /sign
/ASP
(NP-OBJ /border
/trade
/agreement))))
/sign
arg0: /Burma /and /China
arg1: /border /trade /agreement
b. (IP (NP-PN-SBJ /Burma)
(VP (ADVP /already)
(PP /with
(NP-PN /China))
(VP /sign
/ASP
(NP-OBJ /border
/trade
/agreement))))
/sign
arg0-crd: /Burma
arg0-crd: /China
arg1: /border /trade /agreement
There are two ways to capture this type of regu-
larity. One way is to treat each piece as a separate
argument. The problem is that for coordinated noun
phrases, there can be arbitrarily many coordinated
constituents. So we adopt the alternative approach
of representing the entire constituent as one argu-
ment. When the pieces are separate constituents,
they will receive the same argument label, with dif-
ferent secondary tags indicating they are parts of a
larger constituent. For example, in 1, when pos-
sessor raising occurs, the possessor and possessee
receive the same argument label with different sec-
ondary tags psr and pse. In 2b, both ? /China?
and ? /Burma? receive the label arg0, and the sec-
ondary label crd indicates each one is a part of the
coordinated constituent.
3.2 Nominalizations
Another complication involves nominalizations (or
deverbal nouns) and their co-occurrence with light
and not-so-light verbs. A nominalized verb, while
serving as an argument to another predicate (gen-
erally a verb), also has its own predicate-argument
structure. For example, in 3, the predicate-argument
structure for ? /doubt? should be ? ( ,
)?, where all the arguments of ? /doubt?
are embedded in the NP headed by ? /doubt?.
The complication arises when the nominalized noun
is a complement to another verb, as in 4, where
the subject ? /reader? is an argument to both
the verb ? /produce? and the nominalized verb
? /doubt?. More interestingly, the other argument
? /this /CL /news? is realized as an adjunct to
the verb (introduced by a preposition) even though
it bears no apparent thematic relationship to it.
It might be tempting to treat the verb
? /develop? as a ?light verb? that does not
have its own predicate-argument structure, but this
is questionable because ? /doubt? can also take a
noun that is not a nominalized verb: ? /I /towards
/she /develop /LE /feeling?. In addition,
there is no apparent difference in meaning for
? /develop? between this sentence and 4, so there
is little basis to say these are two different senses of
this verb. So we annotate the predicate-argument
structure of both the verb ? ( , )? and the
nominalized verb ? ( , )?.
3. (IP (NP-SBJ (NP /reader)
(DNP (PP /towards
(NP (DP /this
(CLP /CL))
(NP /news)))
)
(NP /doubt))
(VP /deepen
/LE))
/deepen
arg1: /reader /towards /this /CL
/news
4. (IP (NP-SBJ /reader)
(VP (PP-DIR /towards
(NP (DP /this
(CLP /CL))
(NP /news)))
(ADVP /too)
(VP /will
(VP /develop
(NP-OBJ /doubt)))))
/develop
arg0: /reader
arg1: /doubt
4 Implementation
To implement the annotation model presented in
Section 2, we create a lexical database. Each entry is
a predicate listed with its framesets. The set of pos-
sible semantic roles for each frameset are also listed
with a mnemonic explanation. This explanation is
not part of the formal annotation. It is there to help
human annotators understand the different semantic
roles of this frameset. An annotated example is also
provided to help the human annotator.
As illustrated in Example 5, the verb ? /pass?
has three framesets, and each frameset corresponds
with a different meaning. The different meanings
can be diagnosed with diathesis alternations. For
example, when ? /pass? means ?pass through?,
it allows dropped object. That is, the object does
not have to be syntactically realized. When it means
?pass by vote?, it also has an intransitive use. How-
ever, in this case, the verb demonstrates ?subject of
the intransitive / object of the transitive? alternation.
That is, the subject in the intransitive use refers to
the same entity as the object in the transitive use.
When the verb means ?pass an exam, test, inspec-
tion?, there is also the transitive/intransitive alterna-
tion. Only in this case, the object of the transitive
counterpart is now part of the subject in the intran-
sitive use. This is the argument-split problem dis-
cussed in the last section. The three framesets, rep-
resenting three senses, are illustrated in 5.
5. Verb: /pass
Frameset.01: , /pass through
Roles: arg0(?passer?), arg1(?place?)
Example:
(IP (NP-SBJ /train)
(VP (ADVP /now)
(VP /pass
(NP-OBJ /tunnel))))
.01/pass
arg0: /train
arg1: /tunnel
argM-ADV: /now
(IP (NP-SBJ /train)
(VP (ADVP /now)
(VP /pass)))
.01/pass
arg0: /train
argM-ADV: /now
Frameset.02: , ( , )/pass
(an exam, etc.)
(IP (NP-SBJ (DNP (NP /he)
/DE)
(NP /drug inspection))
(VP (ADVP /not)
(VP /pass)))
.02/pass
arg1: /he /DE /drug inspection
(IP (NP-SBJ (NP /he)
(VP (ADVP /not)
(VP /pass)))
(NP-OBJ /drug inspection))
.02/pass
arg1-psr: /he
arg1-pse: /drug inspection
Frameset.03: /pass (a bill, a law, etc.)
(IP (NP-PN-SBJ /the U.S.
/Congress)
(VP (NP-TMP /recently)
(VP /pass
/ASP
(NP-OBJ /interstate
/banking law))))
.03/pass
arg0: /the U.S.
arg1: /interstate /banking law
(IP (NP-SBJ (ADJP /interstate)
(NP /banking law))
(VP (NP-TMP /recently)
(VP /pass
/ASP)))
.03/pass
arg1: /interstate /banking law
The human annotator can use the information
specified in this entry to annotate all instances of
? /pass? in a corpus. When annotating a predicate
instance, the annotator first determines the syntactic
frame of the predicate instance, and then determine
which frameset this frame instantiates. The frame-
set identification is then attached to this predicate
instance. This can be broadly construed as ?sense-
tagging?, except that this type of sense tagging is
coarser, and the ?senses? are based on structural dis-
tinctions rather than just semantic nuances. A dis-
tinction is made only when the semantic distinc-
tions also coincide with some structural distinctions.
The expectation is that this type of sense tagging is
much amenable to automatic machine-learning ap-
proaches. The annotation does not stop here. The
annotator will go on identifying the arguments and
adjuncts for this predicate instance. For the argu-
ments, the annotator will determine which semantic
role each argument realizes, based on the set of pos-
sible roles for this frameset, and attach the appropri-
ate semantic role label (argN) to it. For adjuncts, the
annotator will determine the type of adjunct this is
and attach a secondary tag to argM.
5 Applications
A resource annotated with predicate-argument struc-
ture can be used for a variety of natural language
applications. For example, this level of abstraction
is useful for Information Extraction. The argument
role labels can be easily mapped to an Information
Extraction template, where each role is mapped to a
piece of information that an IE system is interested
in. Such mapping will not be as straightforward if
it is between surface syntactic entities such as the
subject and IE templates.
This level of abstraction can also provide a plat-
form where lexical transfer can take place. It opens
up the possibility of linking a frameset of a predi-
cate in one language with that of another, rather than
using bilingual (or multilingual) dictionaries where
one word is translated into one or more words in a
different language. This type of lexical transfer has
several advantages. One is that the transfer is made
more precise, in the sense that there will be more
cases where one-to-one mapping is possible. Even
in cases where one-to-one mapping is still not possi-
ble, the identification of the framesets of a predicate
will narrow down the possible lexical choices. For
example, sign.02 in the English Proposition Bank
(Kingsbury and Palmer, 2002) will be linked to ?
.01/enter into an agreement?. This type of linking
rules out ? ? as a possible translation for sign.02,
even though it is a translation for other framesets of
the word sign.
The transfer will also be more precise in another
sense, that is, the predicate-argument structure of a
word instance will be preserved during the trans-
fer process. Knowing the arguments of a predicate
instance can further constrain the lexical choices
and rule out translation candidates whose predicate-
argument structures are incompatible. For example,
if the realized arguments of ?sign.01? of the En-
glish Proposition Bank in a given sentence are the
signer, the document, and the signature, among the
translation candidates ? , ? (? .01/enter into
an agreement? is ruled out as a possibility for this
frameset), only ? ? is possible, because ? ? can
only take two arguments, namely, the signer and the
document.
6. /he /at /this /CL /document /LC /sign
/LE /self /DE /name
?He signed his name on this document.?
One might argue that the syntactic subcategoriza-
tion frame obtained from the syntactic parse tree
can also constrain the lexical choices. For exam-
ple, knowing that ?sign? has a subject, an object
and a prepositional phrase should be enough to rule
out ? ? as a possible translation. This argument
breaks down when there are lexical divergences.
The ?document? argument of ? ? can only be re-
alized as a prepositional phrase in Chinese while
in English it can only be realized the direct object
of ?sign?. If the syntactic subcategorization frame
is used to constrain the lexical choices for ?sign?,
? ? will be incorrectly ruled out as a possible
translation. There will be no such problem if the
more abstract predicate-argument structure is used
for this purpose. Even when the document is re-
alized as a prepositional phrase, it is still the same
argument. Of course, ? /sign? is also a possi-
ble translation. So compared with the surface syn-
tactic frames, the predicate-argument structure con-
strains the lexical choices without incorrectly ruling
out legitimate translation candidates. This is under-
standable because the predicate-structure abstracts
away from the syntactic idiosyncracies of the differ-
ent languages and thus are more transferable across
languages.
7. /he /at /this /CL /document /LC /sign
/he /sign /this /CL /document
?He signed this document.?
Annotating the predicate-argument structure as
described in previous sections will not reduce the
lexical choices to one-to-one mappings in call cases.
For example, ? ? can be translated into ?standard-
ize? or ?unite?, even though there is only one frame-
set for both finer senses of this verb. It is conceiv-
able that one might want to posit two framesets, each
for one finer sense of this verb. This is essentially
a trade-off: either one can conduct deep analysis
of the source language, resolve all sense ambigui-
ties on the source side and have a more straightfor-
ward mapping, or one takes the one-to-many map-
pings and select the correct translation on the tar-
get language side. Hopefully, the annotation of the
predicate-argument provides just the right level of
abstraction and the resource described here, with
each predicate annotated with its arguments and ad-
juncts in context, enables the automatic acquisition
of the predicate-argument structure.
6 Summary
In this paper, we described an approach to annotate
the propositions in the Penn Chinese Treebank. We
described how diathesis alternation patterns can be
used to make coarse sense distinctions for Chinese
verbs as a necessary step in annotating the predicate-
structure of predicates. We also described the repre-
sentation scheme we use to label the semantic argu-
ments and adjuncts of the predicates. We discussed
several complications for this type of annotation and
described our solutions. We then discussed how a
lexical database with predicate-argument structure
information can be used to ensure consistent annota-
tion. Finally, we discussed possible applications for
this resource.
7 Acknowledgement
This work is supported by MDA904-02-C-0412.
References
Daniel M. Bikel and David Chiang. 2000. Two statisti-
cal parsing models applied to the chinese treebank. In
Proceedings of the 2nd Chinese Language Processing
Workshop, Hong Kong, China.
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proc. of NAACL-2000.
Keh-Jiann Chen, Chu-Ren Huang, Feng-Yi Chen, Chi-
Ching Luo, Ming-Chung Chang, and Chao-Jan Chen.
to appear. Sinica Treebank: Design Criteria, rep-
resentational issues and immplementation. In Anne
Abeille, editor, Building and Using Syntactically An-
notated Corpora. Kluwer.
David Chiang. 2000. Statisitical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of the 38th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 456-463,
Hong Kong.
Mike Collins. 1997. Three Generative, Lexicalised Mod-
els for Statistical Parsing. In Proc. of ACL-1997.
Mike Collins. 2000. Discriminative Reranking for Natu-
ral Language Parsing. In Proc. of ICML-2000.
N. Ide and J. Vronis. 1998. Word sense disambigua-
tion: The state of the art. Computational Linguistics,
24(1):1?40.
Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evalu-
ation (LREC2002), Las Palmas, Spain.
Beth Levin. 1993. English Verbs and Alternations: A
Preliminary Investigation. Chicago: The Unversity of
Chicago Press.
Charles Li and Sandra Thompson. 1976. Subject and
Topic: A new typology of language. In Charles Li,
editor, Subject and Topic. Academic Press.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a Large Annotated Corpus of English:
the Penn Treebank. Computational Linguistics.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. submitted. Making fine-grained and coarse-
grained sense distinctions, both manually and auto-
matically. Journal of Natural Language Engineering.
Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen
Okurowski, John Kovarik, Fu-Dong Chiou, Shizhe
Huang, Tony Kroch, and Mitch Marcus. 2000. Devel-
oping Guidelines and Ensuring Consistency for Chi-
nese Text Annotation. In Proc. of the 2nd Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2000), Athens, Greece.
Chinese Word Segmentation as LMR Tagging
Nianwen Xue
Inst. for Research in Cognitive Science
University of Pennsylvania
Philadelphia, PA 19104, USA
xueniwen@linc.cis.upenn.edu
Libin Shen
Dept. of Computer and Info. Science
University of Pennsylvania
Philadelphia, PA 19104, USA
libin@linc.cis.upenn.edu
Abstract
In this paper we present Chinese word
segmentation algorithms based on the so-
called LMR tagging. Our LMR taggers
are implemented with the Maximum En-
tropy Markov Model and we then use
Transformation-Based Learning to com-
bine the results of the two LMR taggers
that scan the input in opposite directions.
Our system achieves F-scores of   
and  	
 on the Academia Sinica corpus
and the Hong Kong City University corpus
respectively.
1 Segmentation as Tagging
Unlike English text in which sentences are se-
quences of words delimited by white spaces, in Chi-
nese text, sentences are represented as strings of
Chinese characters or hanzi without similar natural
delimiters. Therefore, the first step in a Chinese lan-
guage processing task is to identify the sequence of
words in a sentence and mark boundaries in appro-
priate places. This may sound simple enough but in
reality identifying words in Chinese is a non-trivial
problem that has drawn a large body of research in
the Chinese language processing community (Fan
and Tsai, 1988; Gan et al, 1996; Sproat et al, 1996;
Wu, 2003; Xue, 2003).
The key to accurate automatic word identification
in Chinese lies in the successful resolution of ambi-
guities and a proper way to handle out-of-vocabulary
words. The ambiguities in Chinese word segmenta-
tion is due to the fact that a hanzi can occur in differ-
ent word-internal positions (Xue, 2003). Given the
proper context, generally provided by the sentence
in which it occurs, the position of a hanzi can be de-
termined. In this paper, we model the Chinese word
segmentation as a hanzi tagging problem and use a
machine-learning algorithm to determine the appro-
priate position for a hanzi. There are several reasons
why we may expect this approach to work. First,
Chinese words generally have fewer than four char-
acters. As a result, the number of positions is small.
Second, although each hanzi can in principle occur
in all possible positions, not all hanzi behave this
way. A substantial number of hanzi are distributed
in a constrained manner. For example, , the plu-
ral marker, almost always occurs in the word-final
position. Finally, although Chinese words cannot be
exhaustively listed and new words are bound to oc-
cur in naturally occurring text, the same is not true
for hanzi. The number of hanzi stays fairly constant
and we do not generally expect to see new hanzi.
We represent the positions of a hanzi with four
different tags (Table 1): LM for a hanzi that oc-
curs on the left periphery of a word, followed by
other hanzi, MM for a hanzi that occurs in the mid-
dle of a word, MR for a hanzi that occurs on the
right periphery of word, preceded by other hanzi,
and LR for hanzi that is a word by itself. We call
this LMR tagging. With this approach, word seg-
mentation is a process where each hanzi is assigned
an LMR tag and sequences of hanzi are then con-
verted into sequences of words based on the LMR
tags. The use of four tags is linguistically intuitive
in that LM tags morphemes that are prefixes or stems
in the absence of prefixes, MR tags morphemes that
are suffixes or stems in the absence of suffixes, MM
tags stems with affixes and LR tags stems without
affixes. Representing the distributions of hanzi with
LMR tags also makes it easy to use machine learn-
ing algorithms which has been successfully applied
to other tagging problems, such as POS-tagging and
IOB tagging used in text chunking.
Right Boundary (R) Not Right Boundary (M)
Left Boundary (L) LR LM
Not Left Boundary (M) MR MM
Table 1: LMR Tagging
2 Tagging Algorithms
Our algorithm consists of two parts. We first imple-
ment two Maximum Entropy taggers, one of which
scans the input from left to right and the other scans
the input from right to left. Then we implement a
Transformation Based Algorithm to combine the re-
sults of the two taggers.
2.1 The Maximum Entropy Tagger
The Maximum Entropy Markov Model (MEMM)
has been successfully used in some tagging prob-
lems. MEMM models are capable of utilizing a
large set of features that generative models cannot
use. On the other hand, MEMM approaches scan
the input incrementally as generative models do.
The Maximum Entropy Markov Model used in
POS-tagging is described in detail in (Ratnaparkhi,
1996) and the LMR tagger here uses the same prob-
ability model. The probability model is defined over

, where

is the set of possible contexts or
?histories? and  is the set of possible tags. The
model?s joint probability of a history  and a tag  is
defined as

ffProposition Bank II:  Delving Deeper 
 
 
Olga Babko-Malaya, Martha Palmer, Nianwen Xue, Aravind Joshi1, Seth Kulick 
University of Pennsylvania 
{malayao/mpalmer/xueniwen/joshi/skulick}@linc.cis.upenn.edu 
                                                          
1 Associated with Penn Discourse Treebank (PDTB). Other members of the project are Eleni Miltsakaki, Rashmi Prasad, 
(Univ. of PA) and Bonnie Webber (Univ. of Edinburgh) 
 
 
Abstract 
The PropBank project is creating a corpus of 
text annotated with information about basic 
semantic propositions. PropBank I (Kingsbury 
& Palmer, 2002) added a layer of predicate-
argument information, or semantic roles, to 
the syntactic structures of the English Penn 
Treebank.   This paper presents an overview 
of the second phase of PropBank Annotation, 
PropBank II, which is being applied to Eng-
lish and Chinese, and includes (Neodavid-
sonian) eventuality variables, nominal 
references, sense tagging, and connections to 
the Penn Discourse Treebank (PDTB), a pro-
ject for annotating discourse connectives and 
their arguments. 
1 Introduction 
An important question is the degree to which current 
statistical NLP systems can be made more domain-
independent without prohibitive costs, either in terms of 
engineering or annotation.  The Proposition Bank is 
designed as a broad-coverage resource to facilitate the 
development of more general systems.  It focuses on the 
argument structure of verbs, and provides a complete 
corpus annotated with semantic roles, including partici-
pants traditionally viewed as arguments and ad-
juncts.  Correctly identifying the semantic roles of the 
sentence constituents is a crucial part of interpreting 
text, and in addition to forming a component of the in-
formation extraction problem, can serve as an interme-
diate step in machine translation or automatic 
summarization. 
 
The Proposition Bank project takes a practical approach 
to semantic representation, adding a layer of predicate-
argument information, or semantic roles, to the syntactic 
structures of the Penn Treebank.  The resulting resource 
can be thought of as shallow, in that it does not repre-
sent co-reference, quantification, and many other 
higher-order phenomena, but also broad, in that it cov-
ers every verb in the corpus and allows representative 
statistics to be calculated. The semantic annotation pro-
vided by PropBank is only a first approximation at cap-
turing the full richness of semantic representation. 
Additional annotation of nominalizations and other 
noun predicates has already begun at NYU. This paper 
presents an overview of the second phase of PropBank 
Annotation, PropBank II, which is being applied to Eng-
lish and Chinese and includes (Neodavidsonian) eventu-
ality variables, nominal references, sense tagging, and 
discourse connectives.   
2 PropBank I 
PropBank (Kingsbury & Palmer, 2002) is an annotation 
of the Wall Street Journal portion of the Penn Treebank 
II (Marcus, 1994) with `predicate-argument' structures, 
using sense tags for highly polysemous words and se-
mantic role labels for each argument. An important goal 
is to provide consistent semantic role labels across dif-
ferent syntactic realizations of the same verb, as in the 
window in [ARG0 John] broke [ARG1 the window] and 
[ARG1 The window] broke. PropBank can provide fre-
quency counts for (statistical) analysis or generation 
components in a machine translation system, but pro-
vides only a shallow semantic analysis in that the anno-
tation is close to the syntactic structure and each verb is 
its own predicate. 
 
In PropBank, semantic roles are defined on a verb-by-
verb basis.  An individual verb's semantic arguments are 
simply numbered, beginning with 0.  Polysemous verbs 
have several Framesets, corresponding to a relatively 
coarse notion of word senses, with a separate set of 
numbered roles, a roleset, defined for each Frameset.  
For instance, leave has both a DEPART Frameset ([ARG0 
John] left [ARG1 the room]) and a GIVE Frameset, ([ARG0 
I] left [ARG1 my pearls] [ARG2 to my daughter-in-law] 
[ARGM-LOC in my will].)   While most Framesets have 
three or four numbered roles, as many as six can appear, 
in particular for certain verbs of motion. Verbs can take 
any of a set of general, adjunct-like arguments 
(ARGMs), such as LOC (location), TMP (time), DIS 
(discourse connectives), PRP (purpose) or DIR (direc-
tion).  Negations (NEG) and modals (MOD) are also 
marked. 
 
The same annotation philosophy has been extended to 
the Penn Chinese Proposition Bank (Xue and Palmer, 
2003). The Chinese PropBank annotation is performed 
on a smaller (250k words) and yet growing corpus an-
notated with syntactic structures (Xue et al2004). The 
same syntactic alternations that form the basis for the 
English PropBank annotation also exist in robust quanti-
ties in Chinese, even though it may not be the case that 
the same exact verbs (meaning verbs that are close 
translations of one another) have the exact same range 
of syntactic realization for Chinese and English.  For 
example, in (1), "xin-nian/New Year  zhao-dai-
hui/reception" plays the same role in (a) and (b), which 
is the event or activity held,  even though it occurs in 
different syntactic positions. Assigning the same argu-
ment label, Arg1, to both instances, captures this regu-
larity. It is worth noting that the predicate ?ju-
xing/hold" does not have passive morphology in (1a), 
despite of what its English translation suggests. Like the 
English PropBank, the adjunct-like elements receive 
more general labels like TMP or LOC, as also illustrated 
in (1). The tag set for Chinese and English PropBanks 
are to a large extent similar and more details can be 
found in (Xue and Palmer, 2003). 
  
(1) a. [ARG1 xin-nian/New Year zhao-dai-
hui/reception] [ARGM-TMP jin-tian/today] [ARGM-
LOC zai/at diao-yu-tai/Diaoyutai guo-bin-guan/state 
guest house ju-xing/hold]  
"The New Year reception was held in Diaoyutai State 
Guest House today." 
 
          b. [ARG0 tang-jia-xuan/Tang Jiaxuan] [ARGM-
TMP jin-tian/today] [ARGM-LOC zai/at diao-yu-
tai/Diaoyutai guo-bin-guan/state guest house] ju-
xing/hold [arg1 xin-nian/New Year zhao-dai-
hui/reception] 
"Tang Jiaxuan was holding the New Year Reception in 
Diaoyutai State Guest House today." 
 
For polysemous verbs that take different sets of seman-
tic roles, we also distinguish different Framesets. (2) 
and (3) illustrate the different Framesets of "tong-
guo/pass", which correspond loosely with major senses 
of the verb.  The Frameset in (2) roughly means "pass 
by voting" while the Frameset illustrated by (3) means 
"pass through". The different Framesets are generally 
reflected in the different alternation patterns, which can 
serve as a cue for statistical systems performing Frame-
set disambiguation. (2) is similar to the causa-
tive/inchoative alternation (Levin, 1993). In contrast, (3) 
shows object drop. 
 
(2) a. [ARG0 guo-hui/Congress] zui-jin/recently tong-
guo/pass le/ASP [ARG1 zhou-ji/interstate yin-hang-
fa/banking law] 
    "The U.S. Congress recently passed the inter-state 
banking law." 
      b. [ARG1 zhou-ji/interstate yin-hang-fa/banking 
law] zui-jin/recently tong-guo/pass le/ASP 
       "The inter-state banking law passed recently." 
 
(3) a. [ARG0 huo-che/train] zheng-zai/now tong-
guo/pass [ARG1 sui-dao/tunnel] 
       "The train is passing through the tunne." 
        b. [ARG0 huo-che/train]  zheng-zai/now  tong-
guo/pass. 
       "The train is passing." 
 
There are also some notable differences between Chi-
nese PropBank and English PropBank. In general, the 
verbs in the Chinese PropBank are less polysemous, 
with the vast majority of the verbs having just one 
Frameset. On the other hand, the Chinese PropBank has 
more verbs (including static verbs which are generally 
translated into adjectives in English) normalized by the 
corpus size.  
3 Adding Event Variables to PropBank 
Event variables provide a rich analytical tool for analyz-
ing verb meaning. Positing that there is an event vari-
able allows for a straightforward representation of the 
logical form of adverbial modifiers, the capturing of 
pronominal reference to events, and the representation 
of nouns that refer to events. For example, event vari-
ables make it possible to have direct reference to an 
event with a noun phrase, as in (4a) destruction, and to 
refer back to an event with a pronoun (as illustrated in 
(4b) That): 
 
(4) a. The destruction of Pompeii happened in the 1st 
century. 
       b. Brutus stabbed Caesar. That was a pivotal event 
in history. 
 
PropBank I annotations can be translated straightfor-
wardly into logical representations with event variables, 
as illustrated in (5), with relations being defined as 
predicates of events, and Args and ArgMs representing 
relations between event variables and corresponding 
phrases.   
 
(5) a. Mr. Bush met him privately, in the White House,          
on Thursday. 
 
     b. PropBank annotation 
  Rel:  met                      
       Arg0: Mr. Bush  
       ArgM-MNR: privately 
       ArgM-LOC: in the White House 
       ArgM-TMP: on Thursday 
 
     c. Logical representation with an event variable  
 ?e meeting(e) & Arg0(e, Mr. Bush) & Arg1(e, he) 
& MNR(e, privately) & LOC(e, ?in the White 
House?) & TIME(e, ?on Thursday?) 
 
As the representation in (5c) shows, we adopt Neo-
davidsonian analysis of events, which follows Parsons 
(1990) in treating arguments on a par with modifiers in 
the event structure. An alternative analysis is the origi-
nal Davidsonian analysis of events (Davidson 1967), 
where the arguments of the verb are analyzed as its 
logical arguments. 
  
Our choice of a Neodavidsonian representation is moti-
vated by its predictions with respect to obligatoriness of 
arguments. Under the Davidsonian approach, arguments 
are logical arguments of the verb and thus must be im-
plied by the meaning of the sentence, either explicitly or 
implicitly (i.e. existentially quantified). On the other 
hand, it has been a crucial assumption in PropBank that 
not all roles must necessarily be present in each sen-
tence. For example, the Frameset for the verb serve, 
shown in (6a) has three roles: Arg0, Arg1, and Arg2. 
Actual usages of the verb, on the other hand, do not 
require the presence of all three roles. For example, the 
sentence in (6b), as its PropBank annotation in (6c) 
shows, does not include Arg1. 
 
(6)  a.  serve.01 "act, work":            
Arg0:worker 
Arg1:job, project 
Arg2:employer 
 
     b.  Each new trading roadblock is likely to be beaten     
by institutions seeking better ways *trace* to serve 
their high-volume clients.  
 
c. Arg0:  *trace* -> institutions 
     REL:    serve 
     Arg2:   their high-volume clients 
 
As the representations in (7) illustrate, only the Neo-
davidsonian representation gives the correct interpreta-
tion of this sentence. 
 
(7) Davidsonian representation: 
 ?e ?z serve(e, institutions, z, their high-volume 
clients) 
 
Neodavidsonian representation: 
     ?e serve(e)&Arg0(e, institutions)&Arg2(e, their 
high-volume clients) 
 
Assuming a Neodavidsonian representation, we can 
analyze all Args and certain types of modifiers as predi-
cates of events.  The types of ArgMs that can be ana-
lyzed as predicates of event variables are shown below: 
 
? MNR:   to manage businesses profitably 
? TMP:    to run the company for 23 years 
? LOC:    to use the notes on the test 
? DIR:     to jump up 
? CAU:    because of ? 
? PRP:      in order to ? 
 
Whereas for the most part, translating these adverbials 
into modifiers of event variables does not require man-
ual annotation, certain constructions need human revi-
sion. For example, in the sentence in (8a) the temporal 
ArgM ?for the past five years? does not modify the event 
variable e introduced by the verb manage, as our auto-
matic translation would predict. The revised analysis of 
this sentence, given in (8b), follows Krifka 1989, who 
proposed that negated sentences refer to maximal events 
? events that have everything that happened during their 
running time as a part. Annotation of this sentence 
would thus require us to introduce an additional event 
variable, the maximal event e?, which has a duration 
?for the past five years? and has no event of unions 
managing wage increases as part. 
 
(8)  a. For the past five years, unions have not managed 
to win wage increases. 
       b. ?e? TMP(e?, ?for the past five years?) &  
??e(e<e? & managing(e) & Arg0(e, unions) & 
Arg1(e, ?win wage increases?)) 
 
Further annotation involves linking empty categories in 
PropBank to event variables in cases of control, as illus-
trated in (9), where event variables can be viewed as the 
appropriate antecedents for PRO, marked as ?*? below: 
    
(9) The car collided with a lorry, * killing both drivers. 
 
And, finally, we will consider tagging variables accord-
ing to the aspectual class of the eventuality they denote, 
such as states or events. Events, such as John built a 
house, involve some kind of change and usually imply 
that some condition, which obtains when the event be-
gins, is terminated by the event. States, on the other 
hand, do not involve any change and hold for varying 
amounts of time. It does not make sense to ask how long 
a state took (as opposed to events), and whether the 
state is culminated or finished.  
 
This distinction between states and events plays an im-
portant role for the temporal analysis of discourse, as 
the following examples (from Kamp and Reyle 1993) 
illustrate: 
 
(10) a. A man entered the White Hart. Bill served him a 
beer. 
       b. I arrived at the Olivers? cottage on Friday night. 
It was not a propitious beginning to my visit. She 
was ill and he in a foul mood. 
 
If a non-initial sentence denotes an event, then it is typi-
cally understood as following the event described by the 
preceding sentence. For example, in (10a), the event of 
Bill serving a beer is understood as taking place after 
the event of ?a man entering the White Hart? was com-
pleted.  On the other hand, states are interpreted as tem-
porally overlapping with the time of the preceding 
sentence, as illustrated in (10b). The sentences she was 
ill and he was in a foul mood seem to describe a state of 
affairs obtaining at the time of the speaker?s arrival.  
 
As this example illustrates, there are different types of 
temporal relations between eventualities (as we will call 
both events and states) and adverbials that modify them, 
such as temporal overlap and temporal containment. 
Furthermore, these relations crucially depend on the 
aspectual properties of the sentence. Translation of PB 
annotations to logical representations with eventuality 
variables and tagging these variables according to their 
aspectual type would thus make it possible to provide an 
analysis of temporal relations. This analysis should also 
be compatible with a higher level of annotation of tem-
poral structure (e.g. Ferro et al 2001).   
4 Annotation of Nominal Coreference 
Our approach to coreference annotation is based on the 
recognition of the different types of relationships that 
might be called "coreference".  The most straightfor-
ward case is that of two semantically definite NPs that 
refer to identical entities, as in (11).  Anaphoric rela-
tions (very broadly defined) are those in which one NP 
(or possessive adjective) has no referential value of its 
own but depends on an antecedent for its interpretation. 
In some cases this can be relatively simple, as in (12), in 
which the pronoun He takes John Smith as its antece-
dent.  However, in some cases, as in (13), the antecedent 
may not even be a referring expression, or can, as in 
(14), refer to an entity that may or may not exist, with 
the non-existent a car being the antecedent of it.  The 
anaphor does not have to be an NP, as in (15), in which 
the possessive their, which takes many companies as its 
antecedent, is an adjective. 
 
(11) John Smith of Company X arrived yesterday.  Mr. 
Smith said that..." 
(12) John Smith of Company X arrived yesterday.  He 
said that..." 
(13) No team spoke about its system. 
(14) I want to buy a car.  I need it to go to work. 
(15) Many companies raised their payouts by more than 
10%. 
 
Another level of complexity is raised by NPs that are 
not anaphors, in that they have their own reference (per-
haps abstract or nonexistent), but are not in an identity 
relationship with an antecedent, but rather describe a 
property of that antecedent.  Typical cases of this are 
predicate nominals, as in (16), or appositives, as in (17), 
and other cases as in (18). 
 
(16) Larry is a university lecturer. 
(17) Larry, the chair of his department, became presi-
dent. 
(18) The stock price fell from $4.02 to $3.85 
 
As has been discussed (e.g., van Deemter & Kibble, 
2001), such cases have fundamentally different proper-
ties than either the identity relationships of (11) or the 
anaphoric relationships of (12)-(15).   
 
Annotation of nominal co-reference is being done in 
two passes. The first pass involves annotation of true 
co-reference between semantically definite NPs`. The 
issue here is to consider what the semantically definite 
nouns are.  Initially, they are defined as proper nouns 
(named entities), either as NPs (America) or prenominal 
adjectives (American politicians).   
 
(19) The last time the S&P 500 yield dropped below 3% 
was in the summer of 1987... There have been only 
seven other times when the yield on the S&P 500 
dropped....   
 
It is reasonable to expand this to definite descriptions, 
so that in (19), the S&P 500 yield and the yield on the 
S&P 500 are marked as coreferring.  However, some 
definite NPs can refer to clauses, not NPs, such as The 
pattern in (20), and we will not do such cases of clausal 
antecedents on the first pass. 
 
(20) The index fell 40% in 1975 and jumped 80% in 
1976.  The pattern is an unusual one. 
 
Anaphoric relations are being done on a "need-to-
annotate" basis.   For each anaphoric NP or possessive 
adjective, the annotator needs to determine its antece-
dent.  As discussed, this is a different type of relation 
than identity, and this distinction will be noted in the 
annotation. The issue here is what we consider an ana-
phoric element to be.  We consider all cases of pro-
nouns, possessives, reflexives, and NPs with that/those 
to be potential cases of anaphors (again, broadly de-
fined). However, as with definite NPs, we only mark 
those that have an NP antecedent, and not clausal ante-
cedents.  For example, in (21), it refers to the current 
3.3% reading, and so would be marked as being in an 
antecedent-anaphor relation. In (22), it refers to having 
the dividend increases, which is not an NP, and so 
would not be marked as being in an anaphor relation in 
the first pass. Similar considerations apply to potential 
anaphors like those NP, that NP, etc. 
 
 (21) ...the current 3.3% reading isn't as troublesome as 
it might have been. 
(22) Having the dividend increases is a supportive ele-
ment in the market outlook, but I don't think it's a 
main consideration". 
 
Note that placing the burden on the anaphors to deter-
mine what gets marked as being in an anaphor-
antecedent leaves it open as to what the antecedent 
might be, other than the requirement just mentioned of it 
being an NP.  Not only might it be non-referring NPs as 
in  (13) or (14), it could even be a generic, as in (23), in 
which books is the antecedent for they. 
 
(23) I like books.  They make me smile. 
 
The second pass will tackle the more difficult issues: 
 
1. Descriptive NPs, as in (16)-(18).  While the informa-
tion provided by these cases would be extremely valu-
able for information extraction and other systems, there 
are some uncertain issues here, mostly focusing on how 
such descriptors describe the antecedent at different 
moments in time and/or space.  The crucial question is 
therefore what to take the descriptor to be.   
 
(24) Henry Higgins might become the president of 
Dreamy Detergents. 
 
For example, in (18), it can't be just $4.02 and $3.85, 
since this does not include information about *when* 
the stock price had such values. The same issue arises 
for (17).  As van Deemter & Kibble point out, such 
cases can interact with issues of modality in uncertain 
ways, as illustrated in (24).  Just saying that in (24) the 
president of Dreamy Detergents is in the same type of 
relationship with Henry Higgins as a university lecturer 
is with Larry in (16) would be very misleading. 
 
2. Clausal antecedents - Here we will handle cases of it 
and other anaphor elements and definite NPs referring 
to non-NPs as antecedents, as in (21).  This will most 
likely be done by referring to the eventuality variable 
associated with the antecedent. 
5 Linking to the Penn Discourse Treebank 
(PDTB)  
The Penn Discourse Treebank (PDTB) is currently be-
ing built by the PDTB team at the University of Penn-
sylvania, providing the next appropriate level of  
annotation: the annotation of the predicate argument 
structure of connectives (Miltsakaki et al2004a/b). The 
PDTB project is based on the idea that discourse con-
nectives can be thought of as predicates with their asso-
ciated argument structure. This perspective of discourse 
is based on a series of papers extending lexicalized tree-
adjoining grammar (LTAG) to discourse (DLTAG), 
beginning with Webber and Joshi (1998).2  This level of 
annotation is quite complex for a variety of reasons, 
such as the lack of available literature describing dis-
course connectives and frequent occurrences of empty 
(lexically null) connectives between two sentences that 
cannot be ignored. Also, unlike the predicates at the 
sentence level, some of the discourse connectives, espe-
cially discourse adverbials, take their arguments ana-
phorically and not structurally, requiring an intimate 
association with event variable representation.  
 
The long-range goal of the PDTB project is to develop a 
large scale and reliably annotated corpus that will en-
code coherence relations associated with discourse con-
nectives, including their argument structure and 
anaphoric links, thus exposing a clearly defined level of 
discourse structure and supporting the extraction of a 
range of inferences associated with discourse connec-
tives. This annotation will reference the Penn Treebank 
(PTB) annotations as well as PropBank. 
 
In PDTB, a variety of connectives are considered, such 
as subordinate and coordinate conjunctions, adverbial 
connectives and implicit connectives amounting to a 
total of approximately 20,000 annotations; 10,000 im-
                                                          
2 The PDTB annotations are deliberately kept independ-
ent of DLTAG framework for two reasons: (1) to make the 
annotated corpus widely useful to researchers working in 
different frameworks and (2) to make the annotation task 
easier, thereby increasing interannotator reliability. 
plicit connectives and 10,000 annotations of the 250 
explicit connectives identified in the corpus (for details 
see (Miltsakaki et al2004a and Miltsakaki et al2004b).  
Current annotations in PDTB are performed by four 
annotators. Individual annotation proceeds one connec-
tive at a time. This way, the annotators quickly gain 
experience with that connective and develop a better 
understanding of its predicate-argument characteristics. 
For the annotation of implicit connectives, the annota-
tors are required to provide an explicit connective that 
best expressed the inferred relation. 
 
The PDTB is expected to be released by November 
2005. The final version of the corpus  will also contain 
characterizations of the semantic roles associated with 
the arguments of each type of connective as well as 
links to PropBank. 
6.   Annotation of Word Senses 
The critical question with respect to sense tagging in-
volves the choice of senses.  In other words, which 
sense inventory, and which level of granularity with 
respect to that sense inventory?  The PropBank Frames 
Files for the verbs include coarse-grained sense distinc-
tions based primarily on usages of a verb that have dif-
ferent numbers of predicate-arguments. These are 
termed Framesets ? referring to the set of roles for each 
one and the corresponding set of syntactic frames.  We 
are currently sense-tagging the annotated predicates for 
lemmas with multiple Framesets, which can be done 
quickly and accurately with an inter-annotator agree-
ment of over 90%.  The distinctions made by the 
Framesets are very coarse, and each one would map to 
several standard dictionary entries for the lemma in 
question.  More fine-grained sense distinctions could be 
useful for Automatic Content Extraction, yet it remains 
to be determined exactly which distinctions are neces-
sary and what methodology should be followed to pro-
vide additional word sense annotation. 
 
Palmer et al(2004b) present an hierarchical approach to 
verb senses, where different levels of sense distinctions, 
from PropBank Framesets to WordNet senses, form a 
continuum of granularity. At the intermediate level of 
sense hierarchy we are considering manual groupings of 
the SENSEVAL-2 verb senses (Palmer, et.al., 2004a), 
developed in a separate project. Given a large disagree-
ment rate between annotators (average inter-annotator 
agreement rate for Senseval-2 verbs was only 71%), 
verbs were grouped by two or more people into sets of 
closely related senses, with grouping differences being 
reconciled, and the sense groups were used for coarse-
grained scoring of the systems. These groupings of 
WordNet senses were shown to reconcile a substantial 
portion of the manual and automatic tagging disagree-
ments, showing that many of these disagreements are 
fairly subtle.  Using the groups as a more coarse-grained 
set of sense distinctions improved ITA and system 
scores by almost 10%, to 82% and 69%, respectively 
(Palmer, et. al. 2004a). 
 
We have been investigating whether or not the groups 
can provide an intermediate level of hierarchy in be-
tween the PropBank Framesets and the WN 1.7 senses.  
Based on our existing WN 1.7 tags and Frameset tags of 
the Senseval2 verbs in the Penn Treebank, 95% of the 
verb instances map directly from sense groups to 
Framesets, with each Frameset typically corresponding 
to two or more sense groups. Using the PropBank 
coarse-grained senses as a starting place, and WordNet 
sense tagging for over 1000 verbs produced automati-
cally through mapping VerbNet to PropBank (Kipper, 
et. al., 2004), we have the makings of a large scale tag-
ging experiment on the Penn Treebank.  This will en-
able investigations into the applicability of clearly 
defined criteria for sense distinctions at varying levels 
of granularity, and produce a large, 1M word corpus of 
sense-tagged text for training WSD systems 
 
The hierarchical approach to verb senses, as utilized by 
most standard dictionaries as well as Hector (Atkins, 
?93), and as applied to SENSEVAL-2, presents obvious 
advantages for the problem of Word Sense Disambigua-
tion. The human annotation task is simplified, since 
there are fewer choices at each level and clearer distinc-
tions between them.  The automated systems can com-
bine training data from closely related senses to 
overcome the sparse data problem, and both humans 
and systems can back off to a more coarse-grained 
choice when fine-grained choices prove too difficult.  
 
Conclusion 
This paper has presented an overview of the second 
phase of PropBank Annotation, PropBank II, which is 
being applied to English and Chinese. It  includes (Neo-
davidsonian) eventuality variables, nominal references, 
an hierarchical approach to sense tagging, and connec-
tions to the Penn Discourse Treebank (PDTB), a project 
for annotating discourse connectives and their argu-
ments. 
References 
 
Atkins, S. (1993) Tools for computer-aided corpus lexi-
cography: The Hector Project.  Actu Linguistica Hunguricu, 
41:5-72. 
 
Carlson, L., Marcu, D. and Okurowski, M. E. (2002). 
Building a Discourse-Tagged Corpus in the Framework of 
Rhetorical Structure Theory. In Current Directions in Dis-
course and Dialogue, Jan van Kuppevelt and Ronnie Smith 
eds., Kluwer Academic Publishers. To appear. 
 
Davidson, D. 1967.  The Logical Form of Action Sen-
tences. In The Logic of Decision and Action, ed. Nicholas 
Rescher.  81--95. Pittsburgh: University of Pittsburgh 
Press.  Republished in Donald Davidson, Essays on Actions 
and Events,Oxford University Press, Oxford, 1980. 
 
Edmonds, P. and Cotton, S. 2001. SENSEVAL-2: Over-
view. In Proceedings of SENSEVAL-2: Second International 
Workshop on Evaluating Word Sense Disambiguation Sys-
tems, ACL-SIGLEX, Toulouse, France. 
 
Ferro L, I. Mani, B. Sundheim and G.Wilson 2001 TIDES 
Temporal Annotation Guidelines, MITRE Technical Report, 
MTR 01W0000041.  
 
Kamp, H. and U.Reyle. 1993. From Discourse to Logic, 
Kluwer, Dordrecht. 
 
Kingsbury, P. and Palmer, M, (2002), From TreeBank to 
PropBank, Third International Conference on Language  Re-
sources and Evaluation, LREC-02, Las Palmas, Canary Is-
lands, Spain, May 28- June 3. 
 
Kilgarriff, A. and Palmer, M.. 2000. Introduction to the 
special issue on Senseval, Computers and the Humanities, 
34(1-2):1-13. 
 
Kipper K., B. Snyder, and M. Palmer. (to appear, 2004) 
"Extending a verb-lexicon using a semantically annotated 
corpus". Proceedings of the 4th International Conference on 
Language Resources and Evaluation (LREC-04). Lisbon, 
Portugal, 2004. 
 
Krifka, M. 1989. Nominalreferenz und Zeitkonstitution. 
M?nchen, Wilhelm Fink Verlag 
 
Levin, B. 1993. English Verb Classes and Alternations: a 
Preliminary Investigation. Chicago: The University of Chi-
cago Press. 
 
Mann, W. and S. Thompson. 1986. ?Relational Proposi-
tions in Discourse?, Discourse Processes 9, 57-90. 
Marcu, D. 2000. The Theory and Practice of Discourse 
Parsing and Summarization. The MIT Press. 
Miltsakaki, E., R. Prasad, A. Joshi and B. Webber. 2004a. 
The Penn Discourse Treebank. In Proceedings of the 4th In-
ternational Conference on Language Resources and Evalua-
tion (LREC 2004), Lisbon. 
 
Miltsakaki, E., R. Prasad, A. Joshi and B. Webber. 2004b. 
Annotation of Discourse Connectives and Their Arguments, in 
Proceedings of the HLT-EACL Workshop on Frontiers in 
Corpus Annotation, Boston, Massachussetts. 
      Palmer, M., Dang, H. T, and Fellbaum, C., 2004a.  Making 
fine-grained and coarse-grained sense distinctions, both manu-
ally and automatically, under revision for Natural Language 
Engineering. 
Palmer, M., Babko-Malaya, O., Dang, H. T., 2004b. Dif-
ferent Sense Granularities for Different Applications, to ap-
pear in the Scalable Natural Language Understanding 
Workshop, held in conjunction with HLT/NAACL-04, May, 
2004. 
 
Parsons, T. 1990.  Events in the Semantics of Eng-
lish.  Cambridge, MA: MIT Press. 
  
van Deemter, K. and R. Kibble. 2000. ?On Coreferring: 
Coreference in MUC and Related Annotation Schemes?, 
Computational Linguistics 26:629-637. 
 
Webber B. and A. Joshi. 1998. Anchoring a lexicalized 
tree-adjoining grammar for discourse. In ACL/COLING 
Workshop on Discourse Relations and Discourse Markers, 
Montreal, Canada, pp. 41-48. 
 
 
Xue, N. and Palmer, M. 2003. Annotating the Propositions 
in the Penn Chinese Treebank. In the Proceedings of the Sec-
ond SIGHAN Workshop on Chinese Language Processing.  
Sapporo, Japan. 
  
Xue, Nianwen, Xia, Fei, Chiou, Fu-dong and Palmer, 
Martha. 2004. The Penn Chinese Treebank: phrase structure 
annotation of a large corpus. Natural Language Engineering, 
10(4):1-30, June 2004.  
 
 
 
	

	Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 61?67,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Parallel Proposition Bank II for Chinese and English?
Martha Palmer, Nianwen Xue, Olga Babko-Malaya, Jinying Chen, Benjamin Snyder
Department of Computer and Information Science
University of Pennsylvania
{mpalmer/xueniwen/malayao/Jinying/bsnyder3}@linc.cis.upenn.edu
Abstract
The Proposition Bank (PropBank) project
is aimed at creating a corpus of text an-
notated with information about seman-
tic propositions. The second phase of
the project, PropBank II adds additional
levels of semantic annotation which in-
clude eventuality variables, co-reference,
coarse-grained sense tags, and discourse
connectives. This paper presents the re-
sults of the parallel PropBank II project,
which adds these richer layers of semantic
annotation to the first 100K of the Chinese
Treebank and its English translation. Our
preliminary analysis supports the hypoth-
esis that this additional annotation recon-
ciles many of the surface differences be-
tween the two languages.
1 Introduction
There is a pressing need for a consensus on a task-
oriented level of semantic representation that can en-
able the development of powerful new semantic ana-
lyzers in the same way that the Penn Treebank (Mar-
cus et al, 1993) enabled the development of sta-
tistical syntactic parsers (Collins, 1999; Charniak,
2001). We believe that shallow semantics expressed
as a dependency structure, i.e., predicate-argument
structure, for verbs, participial modifiers, and nom-
inalizations provides a feasible level of annotation
that would be of great benefit. This annotation, cou-
pled with word senses, minimal co-reference links,
?This work is funded by the NSF via Grant EIA02-05448 .
event identifiers, and discourse and temporal rela-
tions, could provide the foundation for a major ad-
vance in our ability to automatically extract salient
relationships from text. This will in turn facilitate
breakthroughs in message understanding, machine
translation, fact retrieval, and information retrieval.
The Proposition Bank project is a major step towards
providing this type of annotation. It takes a prac-
tical approach to semantic representation, adding a
layer of predicate argument information, or seman-
tic roles, to the syntactic structures of the Penn Tree-
bank (Palmer et al, 2005). The Frame Files that
provide guidance to the annotators constitute a rich
English lexicon with explicit ties between syntac-
tic realizations and coarse-grained senses, Frame-
sets. PropBank Framesets are distinguished primar-
ily by syntactic criteria such as differences in sub-
categorization frames, and can be seen as the top-
level of an hierarchy of sense distinctions. Group-
ings of fine-grained WordNet senses, such as those
developed for Senseval2 (Palmer et al, to appear)
provide an intermediate level, where groups are dis-
tinguished by either syntactic or semantic criteria.
WordNet senses constitute the bottom level. The
PropBank Frameset distinctions, which can be made
consistently by humans and systems (over 90% ac-
curacy for both), are surprisingly compatible with
the groupings; 95% of the groups map directly onto
a single PropBank frameset sense (Palmer et al,
2004).
The semantic annotation provided by PropBank
is only a first approximation at capturing the full
richness of semantic representation. Additional an-
notation of nominalizations and other noun pred-
61
icates has already begun at NYU. This paper de-
scribes the results of PropBank II, a project to pro-
vide richer semantic annotation to structures that
have already been propbanked, specifically, eventu-
ality ID.s, coreference, coarse-grained sense tags,
and discourse connectives. Of special interest to the
machine translation community is our finding, pre-
sented in this paper, that PropBank II annotation rec-
onciles many of the surface differences of the two
languages.
2 PropBank I
PropBank (Palmer et al, 2005) is an annotation of
the Wall Street Journal portion of the Penn Treebank
II (Marcus et al, 1994) with ?predicate-argument?
structures, using sense tags for highly polysemous
words and semantic role labels for each argument.
An important goal is to provide consistent seman-
tic role labels across different syntactic realizations
of the same verb, as in the window in [ARG0 John]
broke [ARG1 the window] and [ARG1 The window]
broke. PropBank can provide frequency counts for
(statistical) analysis or generation components in
a machine translation system, but provides only a
shallow semantic analysis in that the annotation is
close to the syntactic structure and each verb is its
own predicate.
In PropBank, semantic roles are defined on a
verb-by-verb basis. An individual verb?s seman-
tic arguments are simply numbered, beginning with
0. Polysemous verbs have several framesets, cor-
responding to a relatively coarse notion of word
senses, with a separate set of numbered roles, a role-
set, defined for each Frameset. For instance, leave
has both a DEPART Frameset ([ARG0 John] left
[ARG1 the room]) and a GIVE Frameset, ([ARG0
I] left [ARG1 my pearls] [ARG2 to my daughter-in-
law] [ARGM-LOC in my will].) While most Frame-
sets have three or four numbered roles, as many
as six can appear, in particular for certain verbs of
motion. Verbs can take any of a set of general,
adjunct-like arguments (ARGMs), such as LOC (lo-
cation), TMP (time), DIS (discourse connectives),
PRP (purpose) or DIR (direction). Negations (NEG)
and modals (MOD) are also marked.
There are several other annotation projects,
FrameNet (Baker et al, 1998), Salsa (Ellsworth et
al., 2004), and the Prague Tectogrammatics (Haji-
cova and Kucerova, 2002), that share similar goals.
Berkeley.s FrameNet project, (Baker et al, 1998;
Fillmore and Atkins, 1998; Johnson et al, 2002)
is committed to producing rich semantic frames on
which the annotation is based, but it is less con-
cerned with annotating complete texts, concentrat-
ing instead on annotating a set of examples for each
predicator (including verbs, nouns and adjectives),
and attempting to describe the network of relations
among the semantic frames. For instance, the buyer
of a buy event and the seller of a sell event would
both be Arg0.s (Agents) in PropBank, while in
FrameNet one is the BUYER and the other is the
SELLER. The Salsa project (Ellsworth et al, 2004)
in Germany is producing a German lexicon based
on the FrameNet semantic frames and annotating a
large German newswire corpus. PropBank style an-
notation is being used for verbs which do not yet
have FrameNet frames defined.
The PropBank annotation philosophy has been
extended to the Penn Chinese Proposition Bank
(Xue and Palmer, 2003). The Chinese PropBank an-
notation is performed on a smaller (250k words) and
yet growing corpus annotated with syntactic struc-
tures (Xue et al, To appear). The same syntac-
tic alternations that form the basis for the English
PropBank annotation also exist in robust quantities
in Chinese, even though it may not be the case that
the same exact verbs (meaning verbs that are close
translations of one another) have the exact same
range of syntactic realization for Chinese and En-
glish. For example, in (1), ?#c/New Year???/
reception? plays the same role in (a) and (b), which
is the event or activity held, even though it occurs in
different syntactic positions. Assigning the same ar-
gument label, Arg1, to both instances, captures this
regularity. It is worth noting that the predicate /?
1/hold? does not have passive morphology in (1a),
despite what its English translation suggests. Like
the English PropBank, the adjunct-like elements re-
ceive more general labels like TMP or LOC, as also
illustrated in (1). The functional tags for Chinese
and English PropBanks are to a large extent similar
and more details can be found in (Xue and Palmer,
2003).
(1) a. [ARG1 #c/New Year ???/reception] [ARGM-
TMP 8 U/today] [ARGM-LOC 3/at M ~
62
/DiaoyutaiIU,/state guest house ?1/hold]
?The New Year reception was held in Diao-yutai
State Guest House today.?
b. [ARG0 /[^/Tang Jiaxuan] [ARGM-TMP 8
U/today] [ARGM-LOC 3/at M~/Diaoyutai I
U,/state guest house] ?1/ hold [arg1 #c/New
Year???/reception]
?Tang Jiaxuan was holding the New Year reception in
Diaoyutai State Guest House today.?
3 A Parallel PropBank II
As discussed above, PropBank II adds richer se-
mantic annotation to the PropBank I predicate ar-
gument structures, notably eventuality variables,
co-references, coarse-grained sense tags (Babko-
Malaya et al, 2004; Babko-Malaya and Palmer,
2005), and discourse connectives (Xue, To appear)
To create our parallel PropBank II, we began with
the first 100K words of the Chinese Treebank which
had already been propbanked, and which we had
had translated into English. The English transla-
tion was first treebanked and then propbanked, and
we are now in the process of adding the PropBank
II annotation to both the English and the Chinese
propbanks. We will discuss our progress on each of
the three individual components of PropBank II in
turn, bringing out translation issues along the way
that have been highlighted by the additional anno-
tation. In general we find that this level of abstrac-
tion facilitates the alignment of the source and tar-
get language descriptions: event ID.s and event
coreferences simplify the mappings between verbal
and nominal events; English coarse-grained sense
tags correspond to unique Chinese lemmas; and dis-
course connectives correspond well.
3.1 Eventuality variables
Positing eventuality1 variables provides a straight-
forward way to represent the semantics of adver-
bial modifiers of events and capture nominal and
pronominal references to events. Given that the ar-
guments and adjuncts for the verbs are already an-
notated in Propbank I, adding eventuality variables
is for the most part straightforward. The example
in (2) illustrates a Propbank I annotation, which is
identified with a unique event id in Propbank II.
1The term ?eventuality? is used here to refer to events and
states.
(2) a. Mr. Bush met him privately in the White House on
Thursday.
b. Propbank I: Rel: met, Arg0: Mr. Bush, Arg1: him,
ArgM-MNR: privately, ArgM-LOC: in the White
House, ArgM-TMP: on Thursday.
c. Propbank II: ?e meeting(e) & Arg0(e,Mr. Bush) &
Arg1(e, him) & MNR (e, privately) & LOC(e, in the
White House) & TMP (e, on Thursday).
Annotation of event variables starts by auto-
matically associating all Propbank I annotations
with potential event ids. Since not all annotations
actually denote eventualities, we manually filter
out selected classes of verbs. We further attempt
to identify all nouns and nominals which describe
eventualities as well as all sentential arguments of
the verbs which refer to events. And, finally, part
of the PropBank II annotation involves tagging of
event coreference for pronouns as well as empty
categories. All these tasks are discussed in more
detail below.
Identifying event modifiers. The actual annota-
tion starts from the presumption that all verbs are
events or states and nouns are not. All the verbs in
the corpus are automatically assigned a unique event
identifier and the manual part of the task becomes (i)
identification of verbs or verb senses that do not de-
note eventualities, (ii) identification of nouns that do
denote events. For example, in (3), begin is an as-
pectual verb that does not introduce an event vari-
able, but rather modifies the verb -take., as is
supported by the fact that it is translated as an ad-
verb ??/initially? in the corresponding Chinese sen-
tence.
(3) ?:/key u?/develop /DE ??/medicine ?/and )
?/biology E?/technology, #/new E?/technology,
#/new ?/material, O ? ?/computer 9/and A
^/application, 1/photo >/electric ?Nz/integration
/etc. ?/industry ?/already ?/initially ?/take 5
/shape.
/Key developments in industries such as medicine,
biotechnology, new materials, computer and its applica-
tions, protoelectric integration, etc. have begun to take
shape.0
Nominalizations as events Although most nouns
do not introduce eventualities, some do and these
nouns are generally nominalizations2 . This is true
2The problem of identifying nouns which denote events is
addressed as part of the sense-tagging tagging. Detailed discus-
sion can be found in (Babko-Malaya and Palmer, 2005).
63
for both English and Chinese, as is illustrated in (4).
Both /u?/develop0and /\/deepening0are
nominalized verbs that denote events. Having a par-
allel propbank annotated with event variables allows
us to see how events are lined up in the two lan-
guages and how their lexical realizations can vary.
The nominalized verbs in Chinese can be translated
into verbs or their nominalizations, as is shown in
the alternative translations of the Chinese original
in (4). What makes this particular example even
more interesting is the fact that the adjective mod-
ifier of the events, /??/continued0, can ac-
tually be realized as an aspectual verb in English.
The semantic representations of the Propbank II an-
notation, however, are preserved: both the aspec-
tual verb /continue0in English and the adjective
/??/continued0in Chinese are modifiers of the
events denoted by /u?/development0and /
\/deepening0.
(4) ? X/with ? I/China ? L/economy /DE ?
?/continuedu?/development ?/and?/to	/outside
m?/open/DE??/continued\/deepen ,
/As China.s economy continues to develop and
its practice of opening to the outside continues to
deepen,0
/With the continued development of China.s economy
and the continued deepening of its practice of opening to
the outside,0
Event Coreference Another aspect of the event
variable annotation involves identifying pronominal
expressions that corefer with events. These pronom-
inal expressions may be overt, as in the Chinese ex-
ample in (5), while others correspond to null pro-
nouns, marked as pro3. in the Treebank annotations,
as in (6):
(5) ?/additionally, ? ?/export ??/commodity (
/structure UY/continue ` z/optimize, c/last
year ? ?/industry ? ? ?/finished product ?
?/export /quota ?/account for I/entire country
? ?/export o /quantity /DE ' ?/proportion
?/reach z??l??:8/85.6 percent, ?/this ?
?/clearly L?/indicate ?I/China ??/industry 
?/product/DE?E/produce Y?/level'/compared
with L /past k/have 
/LE ?/very ?/big J
p/improvement.
/Moreover, the structure of export com-modities
continues to optimize, and last year.s export volume
of manufactured products ac-counts for 85.6 percent of
3The small *pro* and big *PRO* distinction made in the
Chinese Treebank is exploratory in nature. The idea is that it is
easier to erase this distinction if it turns out to be implausible or
infeasible than to add it if it turns out to be important.
the whole countries.export, *pro* clearly indicating
that China.s industrial product manufacturing level has
improved.0
(6) ?
/these ?J/achievement ?/among k/have ?
z n? l/138 ?/item Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 84?91,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Annotating Discourse Connectives in the Chinese Treebank ?
Nianwen Xue
Department of Computer and Information Science
University of Pennsylvania
xueniwen@linc.cis.upenn.edu
Abstract
In this paper we examine the issues that
arise from the annotation of the discourse
connectives for the Chinese Discourse
Treebank Project. This project is based on
the same principles as the PDTB, a project
that annotates the English discourse con-
nectives in the Penn Treebank. The pa-
per begins by outlining range of discourse
connectives under consideration in this
project and examines the distribution of
the explicit discourse connectives. We
then examine the types of syntactic units
that can be arguments to the discourse
connectives. We show that one of the
most challenging issues in this type of dis-
course annotation is determining the tex-
tual spans of the arguments and this is
partly due to the hierarchical nature of dis-
course relations. Finally, we discuss sense
discrimination of the discourse connec-
tives, which involves separating discourse
connective from non-discourse connective
senses and teasing apart the different dis-
course connective senses, and discourse
connective variation, the use of differ-
ent connectives to represent the same dis-
course relation.
?I thank Aravind Johi and Martha Palmer for their com-
ments. All errors are my own, of course.
1 Introduction
The goal of the Chinese Discourse Treebank
(CDTB) Project is to add a layer of discourse anno-
tation to the Penn Chinese Treebank (Xue et al, To
appear), the bulk of which has also been annotated
with predicate-argument structures. This project
is focused on discourse connectives, which include
explicit connectives such as subordinate and coor-
dinate conjunctions, discourse adverbials, as well
as implicit discourse connectives that are inferable
from neighboring sentences. Like the Penn English
Discourse Treebank (Miltsakaki et al, 2004a; Milt-
sakaki et al, 2004b), the CDTB project adopts the
general idea presented in (Webber and Joshi, 1998;
Webber et al, 1999; Webber et al, 2003) where
discourse connectives are considered to be predi-
cates that take abstract objects such as propositions,
events and situations as their arguments. This ap-
proach departs from the previous approaches to dis-
course analysis such as the Rhetorical Structure The-
ory (Mann and Thompson, 1988; Carlson et al,
2003) in that it does not start from a predefined in-
ventory of abstract discourse relations. Instead, all
discourse relations are lexically grounded and an-
chored by a discourse connective. The discourse
relations so defined can be structural or anaphoric.
Structural discourse relations, generally anchored by
subordinate and coordinate conjunctions, hold lo-
cally between two adjacent units of discourse (such
as clauses). In contrast, anaphoric discourse rela-
tions are generally anchored by discourse adverbials
and only one argument can be identified structurally
in the local context while the other can only be de-
84
rived anaphorically in the previous discourse. An
advantage of this approach to discourse analysis is
that discourse relations can be built up incrementally
in a bottom-up manner and this advantage is magni-
fied in large-scale annotation projects where inter-
annotator agreement is crucial and has been verified
in the construction of the Penn English Discourse
Treebank (Miltsakaki et al, 2004a). This approach
closely parallels the annotation of the the verbs in
the English and Chinese Propbanks (Palmer et al,
2005; Xue and Palmer, 2003), where verbs are the
anchors of predicate-argument structures. The dif-
ference is that the extents of the arguments to dis-
course connectives are far less certain, while the ar-
ity of the predcates is fixed for the discourse connec-
tives.
This paper outlines the issues that arise from the
annotation of Chinese discourse connectives, with
an initial focus on explicit discourse connectives.
Section 2 gives an overview of the different kinds
of discourse connectives that we plan to annotate
for the CDTB Project. Section 3 surveys the dis-
tribution of the discourse connectives and Section
4 describes the kinds of discourse units that can be
arguments to the discourse connectives. Section 5
specifies the scope of the arguments of discourse re-
lations and describes what should be included in or
excluded from the text span of the arguments. Sec-
tions 6 and 7 describes the need for a mechanism
to address sense disambiguation and discourse con-
nective variation, drawing evidence from examples
of explicit discourse connectives. Finally, Section 8
concludes this paper.
2 Overview of Chinese Discourse
Connectives
With our theoretical disposition, a discourse connec-
tive is viewed as a predicate taking two abstract ob-
jects such as propositions, events, or situations as
its arguments. A discourse connective can be ei-
ther explicit or implicit. An explicit discourse con-
nective is realized in the form of one lexical item
or several lexical items while an implicit discourse
connective must be inferred between adjacent dis-
course units. Typical explicit discourse connectives
are subordinate and coordinate conjunctions as well
as discourse adverbials. While the arguments for
subordinate and coordinate conjunctions are gener-
ally local, the first argument for a discourse adver-
bial may need to be identified long-distance in the
previous discourse.
2.1 Subordinate conjunctions
There are two types of subordinate conjunctions in
Chinese, single and paired. With single subordi-
nate conjunctions, the subordinate conjunction in-
troduces the subordinate clause, as in (1). By con-
vention, the subordinate clause is labeled ARG1 and
the main clause is labeled ARG2. The subordinate
conjunction is NOT included as part of the argu-
ment. The subordinate clause generally precedes the
main clause in Chinese, but occasionally it can also
follow the main clause. The assignment of the argu-
ment labels to the discourse units is independent of
their syntactic distributions. The subordinate clause
is always labeled ARG1 whether it precedes or fol-
lows the main clause.
Simple subordinate conjunctions: Simple sub-
ordinate conjunctions are very much like English
where the subordinate clause is introduced by a sub-
ordinate conjunction:
(1) w
report
@?
believe
?
,
[conn XJ
if
] [arg1 ?L
economic
?
and
7K
financial
?
policy
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 707?714,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Automatic inference of the temporal location of situations in Chinese text
Nianwen Xue
Center for Computational Language and Education Research
University of Colorado at Boulder
Colorado, U.S.A.
Nianwen.Xue@colorado.edu
Abstract
Chinese is a language that does not have mor-
phological tense markers that provide explicit
grammaticalization of the temporal location of
situations (events or states). However, in many
NLP applications such as Machine Transla-
tion, Information Extraction and Question An-
swering, it is desirable to make the temporal
location of the situations explicit. We describe
a machine learning framework where differ-
ent sources of information can be combined to
predict the temporal location of situations in
Chinese text. Our experiments show that this
approach significantly outperforms the most
frequent tense baseline. More importantly,
the high training accuracy shows promise that
this challenging problem is solvable to a level
where it can be used in practical NLP applica-
tions with more training data, better modeling
techniques and more informative and general-
izable features.
1 Introduction
In a language like English, tense is an explicit (and
maybe imperfect) grammaticalization of the tempo-
ral location of situations, and such temporal location
is either directly or indirectly defined in relation to
the moment of speech. Chinese does not have gram-
maticalized tense in the sense that Chinese verbs are
not morphologically marked for tense. This is not
to say, however, that Chinese speakers do not at-
tempt to convey the temporal location of situations
when they speak or write, or that they cannot inter-
pret the temporal location when they read Chinese
text, or even that they have a different way of repre-
senting the temporal location of situations. In fact,
there is evidence that the temporal location is rep-
resented in Chinese in exactly the same way as it is
represented in English and most world languages: in
relation to the moment of speech. One piece of evi-
dence to support this claim is that Chinese temporal
expressions like 8U (?today?), ?U (?tomorrow?)
and ?U (?yesterday?) all assume a temporal deixis
that is the moment of speech in relation to which
all temporal locations are defined. Such temporal
expressions, where they are present, give us a clear
indication of the temporal location of the situations
they are associated with. However, not all Chinese
sentences have such temporal expressions associated
with them. In fact, they occur only infrequently in
Chinese text. It is thus theoretically interesting to
ask, in the absence of grammatical tense and explicit
temporal expressions, how do readers of a particular
piece of text interpret the temporal location of situa-
tions?
There are a few linguistic devices in Chinese that
provide obvious clues to the temporal location of
situations, and one such linguistic device is aspect
markers. Although Chinese does not have grammat-
ical tense, it does have grammaticalized aspect in the
form of aspect markers. These aspect markers often
give some indication of the temporal location of an
event. For example, Chinese has the perfective as-
pect marker
and L, and they are often associated
with the past. Progressive aspect marker X, on the
other hand, is often associated with the present. In
addition to aspect, certain adverbs also provide clues
to the temporal location of the situations they are as-
707
sociated with. For example, ? or ?? (?already?),
often indicates that the situation they are associated
with has already occurred and is thus in the past. 3,
another adverbial modifier, often indicates that the
situation it modifies is in the present. However, such
linguistic associations are imperfect, and they can
only be viewed as tendencies rather than rules that
one can use to deterministically infer the temporal
location of a situation. For example, while ? in-
deed indicates that the situation described in (1) is
in the past, when it modifies a stative verb as it does
in (1b), the situation is still in the present.
(1) a. ?
he
[?]
already
?
finish
T
this
?8
project
"
.
?He already finished the project.?
b. ?I
China
[?]
already
Pk
has
)
produce
?.?
world-class
^?
software

DE
?:
foundation
"
.
?China already has the foundation to pro-
duce world-class software.?
More importantly, only a small proportion of verb
instances in any given text have such explicit tempo-
ral indicators and therefore they cannot be the whole
story in the temporal interpretation of Chinese text.
It is thus theoretically interesting to go beyond the
obvious and investigate what additional information
is relevant in determining the temporal location of a
situation in Chinese.
Being able to infer the temporal location of a situ-
ation has many practical applications as well. For
example, this information would be highly valu-
able to Machine Translation. To translate a lan-
guage like Chinese into a language like English in
which tense is grammatically marked with inflec-
tional morphemes, an MT system will have to in-
fer the necessary temporal information to determine
the correct tense for verbs. Statistical MT systems,
the currently dominant research paradigm, typically
do not address this issue directly. As a result, when
evaluated for tense, current MT systems often per-
form miserably. For example, when a simple sen-
tence like ??/he ? U/tomorrow ??/return ?
?/Shanghai? is given to Google?s state-of-the-art
Machine Translation system1, it produces the out-
put ?He returned to Shanghai tomorrow?, instead of
the correct ?he will return to Shanghai tomorrow?.
The past tense on the verb ?returned? contradicts
the temporal expression ?tomorrow?. Determining
the temporal location is also important for an Infor-
mation Extraction task that extracts events so that
the extracted events are put in a temporal context.
Similarly, for Question Answering tasks, it is also
important to know whether a situation has already
happened or it is going to happen, for example.
In this paper, we are interested in investigating
the kind of information that is relevant in inferring
the temporal location of situations in Chinese text.
We approach this problem by manually annotating
each verb in a Chinese document with a ?tense? tag
that indicates the temporal location of the verb2. We
then formulate the tense determination problem as
a classification task where standard machine learn-
ing techniques can be applied. Figuring out what
linguistic information contributes to the determina-
tion of the temporal location of a situation becomes
a feature engineering problem of selecting features
that help with the automatic classification. In Sec-
tion 2, we present a linguistic annotation framework
that annotates the temporal location of situations in
Chinese text. In Section 3 we describe our setup
for an automatic tense classification experiment and
present our experimental results. In Section 4 we
focus in on the features we have used in our exper-
iment and attempt to provide a quantitative as well
as intuitive explanation of the contribution of the in-
dividual features and speculate on what additional
features could be useful. In Section 5 we discuss
related work and Section 6 concludes the paper and
discusses future work.
2 Annotation framework
It is impossible to define the temporal loca-
tion without a reference point, a temporal deixis.
As we have shown in Section 1, there is con-
vincing evidence from the temporal adverbials
like ? U(?yesterday?), 8 U(?today?) and ? U
1http://www.google.com/language tools
2For simplicity, we use the term ?tense? exchangeably with
the temporal location of an event or situation, even though tense
usually means grammatical tense while temporal location is a
more abstract semantic notion.
708
??tomorrow?) that Chinese, like most if not all lan-
guages of the world, use the moment of speech as
this reference point. In written text, which is the pri-
mary source of data that we are dealing with, the
temporal deixis is the document creation time. All
situations are temporally related to this document
creation time except in direct quotations, where the
temporal location is relative to the moment of speech
of the speaker who is quoted.
In addition to the moment of speech or document
creation time in the case of written text, Reference
Time and Situation Time are generally accepted as
important to determining the temporal location since
Reichenbach (1947) first proposed them. Situation
Time is the time that a situation actually occurs
while Reference time is the temporal perspective
from which the speaker invites his audience to con-
sider the situation. Reference Time does not nec-
essarily overlap with Situation Time, as in the case
of present perfective tense, where the situation hap-
pened in the past but the reader is invited to look at
it from the present moment and focus on the state of
completion of the situation. Reference Time is in our
judgment too subtle to be annotated consistently and
thus in our annotation scheme we only consider the
relation between Situation Time and the document
creation time when defining the temporal location
of situations. Another key decision we made when
formulating our annotation scheme is to define an
abstract ?tense? that do not necessarily model the ac-
tual tense system in any particular language that has
grammatical tense. In a given language, the gram-
matical tense reflected in the morphological system
may not have a one-to-one mapping between the
grammatical tense and the temporal location of a sit-
uation. For example, in an English sentence like ?He
will call me after he gets here?, while his ?getting
here? happens at a time in the future, it is assigned
the present tense because it is in a clause introduced
by ?after?. It makes more sense to ask the annota-
tor, who is necessarily a native speaker of Chinese,
to make a judgment of the temporal location of the
situation defined in terms of the relation between the
Situation Time and the moment of speech rather than
by such language-specific idiosyncracies of another
language.
Temporal locations that can be defined in terms of
the relation between Situation Time and the moment
of speech are considered to be absolute tense. In
some cases, the temporal location of a situation can-
not be directly defined in relation to the moment of
speech. For example in (2), the temporal location of
k? (?intend?) cannot be determined independently
of that of ??(?reveal?). The temporal location of
k? is simultaneous with ??. If the temporal
location of ?? is in the past, then the temporal
location of k? is also in the past. If the temporal
location of ?? is in the future, then the temporal
location of k? is also in the future. In this spe-
cific case, the situation denoted by the matrix verb
?? is in the past. Therefore the situation denoted
by k? is also located in the past.
(2) ?
he
?
also
??
reveal
?d
Russia
k?
intend
3
in
8 
next
?c
ten years
S
within
,
,
?
to
?K
Iran
J?
provide
??
weapons
.
.
?He also revealed that Russia intended to pro-
vide weapons to Iran within the next ten years.?
Therefore in our Chinese ?tense? annotation task,
we annotate both absolute and relative tenses. We
define three absolute tenses based on whether the sit-
uation time is anterior to (in the past), simultaneous
with (in the present), or posterior to (in the future)
document creation time. In addition to the absolute
tenses, we also define one relative tense, future-in-
past, which happens when a future situation is em-
bedded in a past context. We do not assign a tense
tag to modal verbs or verb particles. The set of tense
tags are described in more detail below:
2.1 Present tense
A situation is assigned the present tense if it is true at
an interval of time that includes the present moment.
The present tense is compatible with states and ac-
tivities. When non-stative situations are temporally
located in the present, they either have an imperfec-
tive aspect or have a habitual or frequentive reading
which makes them look like states, e.g.,
(3) ?
he
~~
often
?\
attend
r	
outdoors
??
activities
"
.
?He often attends outdoors activities.?
709
2.2 Past tense
Situations that happen before the moment of speech
(or the document creation time) are temporally lo-
cated in the past as in (4):
(4) ??
Chinese
<

personnel
9
and
{?
Chinese nationals
S
safely
?l
withdraw from
?Labeling Chinese Predicates with
Semantic Roles
Nianwen Xue?
University of Colorado at Boulder
In this article we report work on Chinese semantic role labeling, taking advantage of two recently
completed corpora, the Chinese PropBank, a semantically annotated corpus of Chinese verbs, and
the Chinese Nombank, a companion corpus that annotates the predicate?argument structure of
nominalized predicates. Because the semantic role labels are assigned to the constituents in a
parse tree, we first report experiments in which semantic role labels are automatically assigned
to hand-crafted parses in the Chinese Treebank. This gives us a measure of the extent to which se-
mantic role labels can be bootstrapped from the syntactic annotation provided in the treebank. We
then report experiments using automatic parses with decreasing levels of human annotation in
the input to the syntactic parser: parses that use gold-standard segmentation and POS-tagging,
parses that use only gold-standard segmentation, and fully automatic parses. These experiments
gauge how successful semantic role labeling for Chinese can be in more realistic situations. Our
results show that when hand-crafted parses are used, semantic role labeling accuracy for Chinese
is comparable to what has been reported for the state-of-the-art English semantic role labeling
systems trained and tested on the English PropBank, even though the Chinese PropBank is
significantly smaller in size. When an automatic parser is used, however, the accuracy of our
system is significantly lower than the English state of the art. This indicates that an improvement
in Chinese parsing is critical to high-performance semantic role labeling for Chinese.
1. Introduction
Semantic role labeling (SRL) is the task of identifying arguments for a predicate and
assigning semantically meaningful labels to them. A semantic role represents a semantic
relation between a predicate and one of its arguments. Typical semantic roles include
agent, patient, source, goal, and so forth, that are core to a predicate, as well as location,
time, manner, cause, and so on, that are peripheral. Such semantic information is im-
portant in answering who, what, when, where, and why questions therefore is crucial to
natural language processing (NLP) tasks such as question-answering (Narayanan and
Harabagiu 2004), information extraction (Surdeanu et al 2005), summarization (Melli
et al 2005), and machine translation (Boas 2002). Any NLP task that requires some form
of semantic interpretation could potentially benefit from a high performance semantic
role labeling system.
For an automatic system, a semantic role labeling task involves locating the linguis-
tic units, typically words or phrases, in natural language text that serve as arguments
? 1777 Exposition Drive, Boulder, CO 80309. E-mail: Nianwen.Xue@colorado.edu.
Submission received: 15 July 2006; revised submission received: 4 May 2007; accepted for publication:
19 June 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 2
to a predicate and assigning semantic role labels to them based on the context in which
they occur. Since the seminal work of Gildea and Jurafsky (2002), statistical andmachine
learning approaches have been the predominant research paradigm in semantic role
labeling, like most of the subfields in natural language processing and computational
linguistics. A prerequisite for statistical and machine learning approaches to semantic
role labeling is the availability of a significant amount of semantically interpreted
corpora from which automatic systems can learn. The recent activities in semantic role
labeling (Carreras and Ma`rquez 2004b, 2005; Litkowski 2004) have in large part been
driven by the availability of semantically annotated corpora such as the FrameNet
(Baker, Fillmore, and Lowe 1998), Proposition Bank (Palmer, Gildea, and Kingsbury
2005), andNombank (Meyers et al 2004) projects for English; the tectogrammatical layer
annotation of the Prague Dependency Treebank (Sgall, Panevova?, and Hajic?ova? 2004)
for Czech; and the Salsa Project for German (Burchardt et al 2006). These semantically
annotated corpora not only provide the training and test material for the development
of machine learning systems, but also effectively define semantic role labeling as a task.
PropBank and FrameNet have been the two most widely used corpora in de-
veloping automatic semantic role labeling systems. Although both corpora provide
predicate?argument structure annotation, they use very different semantic role labels,
especially for the core arguments of each predicate. In FrameNet, the semantic roles
of a predicate (called a Lexical Unit (LU)) are organized by semantic frames, which
are conceptual structures that describe a particular situation or event along with their
participants, which are called frame elements (FEs). All LUs in the same semantic frame
share one set of semantic roles. For example, the verbs buy and sell both belong to the
semantic frame Commercial_transaction, which involves a Buyer and Seller exchanging
Money and Goods. In addition to these four core FEs, there are also three Non-Core FEs:
Means, the manner in which the transaction takes place; Rate, the price of payment per
unit of Goods; and Unit, the unit of measure for the Goods. Semantic role labeling based
on FrameNet annotation attempts to identify the syntactic constituents in a sentence
and assign FEs to them (1a). Notice that for any given sentence, not all FEs have to be
realized and they do not have to be realized in the same syntactic position.
(1) a. FrameNet
[Buyer We] always [LU bought] [Goods a few dark-red carnations] [Seller
from her]
During the later part of the nineteenth century, [Seller the landowners] [LU
sold] [Goods the land] [Buyer to developers] in very small lots.
b. PropBank
[Arg0 We] always [Rel bought] [Arg1 a few dark-red carnations] [Arg2 from
her]
During the later part of the nineteenth century, [Arg0 the landowners] [Rel
sold] [Arg1 the land] [Arg2 to developers] in very small lots.
Like FrameNet, PropBank also assigns semantic role labels to syntactic constituents
(rather than to the heads in a dependency structure) in a sentence. Unlike FrameNet,
there is no reference ontology like the semantic frame that provides a general set of
semantic roles. Instead, for the core arguments, the PropBank uses a set of predicate-
specific semantic role labels represented by an integer prefixed by Arg: Arg0 through
Arg5. Predicates vary on the number of core arguments they take, but generally the
total number of core arguments does not exceed six. These core arguments are defined
in frame files, with one frame file for each predicate. Within a frame file, the core
226
Xue Semantic Role Labeling of Chinese Predicates
arguments are organized by framesets, which are themajor senses of a predicate. A new
frameset is postulated only when it takes a different set of core arguments from existing
framesets. In addition to the core arguments, there is also a finite set of roles reserved for
adjunct-like arguments. Each adjunct-like argument is represented as ArgM, indicating
that it is a modifier argument, followed by a secondary tag indicating the type of
modifier. Secondary tags are for semantic information such as location, manner, and
time that are not specific to a particular verb or even a particular class of verbs and
they are defined based on a general set of guidelines. There is thus a dichotomy in
the representation of the semantic roles for the core and peripheral arguments in the
PropBank annotation.
The predicate-specific nature of the PropBank semantic roles is clear when com-
pared with the FrameNet FE. In (1b), for example, the seller is always labeled Seller and
the buyer is always labeled Buyer in the FrameNet annotation whether the predicate
is buy or sell. In contrast, in the PropBank annotation, the buyer is Arg0 when the
predicate is buy and Arg2 when the predicate is sell. Conversely, the seller is Arg0 when
the predicate is sell and Arg2 when the predicate is buy. While FrameNet annotates
the semantic roles of both verbal and nominal predicates, the annotation of PropBank
is limited to verbs, with the nominal predicates annotated in a separate but related
project, the Nombank Project (Meyers et al 2004). The NomBank Project adopted the
same predicate-specific approach in representing the core arguments of a predicate as
PropBank, with special treatment for noun-specific phenomena such as support verbs.
There is considerable work on English semantic role labeling with both anno-
tation conventions. Gildea and Jurafsky (2002) did their seminal work using data
from FrameNet. The Senseval-3 international competition on semantic role labeling
(Litkowski 2004) also used the FrameNet annotation. There is an even larger body of
work using PropBank because it has a larger amount of annotated data on a well-
established data set, the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993).
Using the standard training and test sets in the Penn Treebank, there has been a
rapid improvement in performance due to the use of more advanced machine-learning
techniques and more informative linguistic features. The performance using automatic
parses on Section 23 of the Penn Treebank has approached 0.81 F-score (Pradhan, Ward
et al 2005). There have also been two consecutive CoNLL competitions (Carreras and
Ma`rquez 2004b, 2005) on semantic role labeling using the PropBank data.
Research on Chinese semantic role labeling is still in its infancy. Work on Chinese
semantic role labeling has been scant and sporadic, mostly due to the lack of a pub-
licly available semantically annotated corpus of significant size. Although most of the
machine-learning techniques used in English semantic role labeling are readily trans-
ferable to Chinese, such technological transfer is only possible with similarly annotated
data. To our knowledge, there are only two such data sets, which all used a small
corpus that the authors created on their own. Sun and Jurafsky (2004) did preliminary
work on Chinese semantic role labeling on 10 selected verbs using Support Vector
Machines and reported promising early results.1 Noting that Chinese syntactic parsing
is an especially challenging task, Kwong and T?sou (2005) reformulated semantic role
labeling as a task of detecting and classifying the heads of arguments to avoid the
hard problem of getting the correct text spans for the arguments. In this article, we
1 They restated their results in Chen, Sun, and Jurafsky (2005) due to an error in retraining the Collins
parser (Collins 1999) on Chinese, which led to inflated Chinese syntactic parsing and therefore
semantic role labeling results.
227
Computational Linguistics Volume 34, Number 2
report work on the semantic role labeling of Chinese predicates for both verbs and their
nominalizations, exploiting two recently completed corpora, the Chinese PropBank
(Xue and Palmer 2003), a corpus that annotates the predicate?argument structure of
verbs, and the Chinese NomBank (Xue 2006a), a companion corpus that annotates the
predicate-argument structure of nominalized predicates in Chinese. Both corpora are
built on top of the Chinese Treebank (Xue et al 2005), in the sense that the semantic role
labels are assigned to constituents in the parse tree.
The Chinese PropBank and Nombank adopted the English PropBank predicate-
specific approach in representing the semantic roles of the core arguments. In the
absence of a Chinese linguistic ontology like the semantic frames developed for the
English FrameNet Project, using the PropBank-style of semantic roles allows faster de-
velopment. The predicate-specific approach of the PropBank annotation builds a solid
foundation for making high-level generalizations in a bottom-up manner, if broader
generalizations are needed. The Chinese PropBank focuses on the context-sensitive
component of the semantic role annotation, using frame files to guide its annotation.
The semantic roles defined in the frame files are for expected arguments, that is, all
possible arguments for each frameset of a predicate. In a particular sentence, an ex-
pected argument may not always be realized, and when it is, it may not always be
realized in the same syntactic position as a result of syntactic alternations (Levin 1993)
or other syntactic processes. In addition, different framesets of a verb take different sets
of arguments that demonstrate different syntactic patterns. Thus, predicate?argument
structure analysis at the PropBank annotation level represents a crucial leap towards
proper representation of semantic structure from the syntactic structure. Should the
need for more general semantic roles arise, these predicate-specific semantic roles can be
mapped (Yi, Loper, and Palmer 2007) to FrameNet-style or even VerbNet-style (Kipper
et al 2006) labels.
Using the semantic annotation of the Chinese PropBank and NomBank as training
and test material, we were able for the first time to develop a Chinese semantic role
labeling system that is trained and tested on semantically annotated Chinese corpora
of significant sizes. Using parses produced with different levels of automation (a fully
automatic parser, a parser with correct segmentation, a parser with both correct seg-
mentation and POS-tagging, and treebank gold-standard parses), we were able to quan-
tify the impact of different Chinese language processing components on the semantic
analysis of Chinese predicates. Using a Maximum Entropy?based (McCallum 2002)
machine learning system, our experimental results show that just by using the features
reported in the English semantic role labeling literature, our baseline system achieved
a very high accuracy on Chinese verbs when the gold-standard treebank parses are
used. This suggests that these features port very well between English and Chinese.
There is a gradual degradation in semantic role labeling performance with a decreasing
level of human annotation (from gold-standard treebank parses to fully automatic
parses). We were able to achieve a modest improvement with additional features tai-
lored to the Chinese language, bringing the overall accuracy to F1-scores of 0.92 and
0.67, respectively, when using treebank and fully automatic parses. We were able to
achieve a larger improvement on the semantic role labeling of nominalized predicates
by using noun-specific features (F-scores of 0.70 and 0.57, respectively, for treebank
and fully automatic parses), but our results still show that the semantic role labeling
of nominalized predicates is a much more challenging task than that of verbs. This is
partly due to the smaller training set for nominalized predicates, with the number of
nominalized predicates being less than one third of the number of verb instances in
the same corpus. More importantly, the arguments of nominalized predicates have a
228
Xue Semantic Role Labeling of Chinese Predicates
muchmore uneven distribution: Arguments of a nominalized predicate can occur either
inside the NP headed by the predicate or outside when a support verb is present (see
Section 2 for examples). This makes it particularly challenging to determine whether a
constituent in the parse tree is an argument or not. This observation is supported by
the large margin in performance between the semantic role labeling results we achieved
when the constituents are known and unknown.
This article is structured as follows. In Section 2, we discuss the semantic annotation
of the Chinese PropBank and NomBank in greater detail. In Section 3, we describe the
general architecture of our system, focusing on shared components for both verbs and
their nominalizations. In Sections 4 and 5 we present our experiments on verbs and
nouns, respectively. Section 6 discusses related work and Section 7 concludes this article
and discusses future work.
2. The Chinese PropBank and NomBank
The Chinese PropBank and the Chinese NomBank adopt the descriptive framework of
the English Proposition Bank in which semantic arguments and adjuncts are treated dif-
ferently. The semantic arguments of a predicate are labeled with a contiguous sequence
of integers, in the form of ArgN, whereN is an integer between 0 and 5. These labels can
only be interpreted in the context of a specific predicate. In other words, these argument
labels are not meaningful without knowing what the predicate is. In fact, as we will
show later in this section, these numbered labels are meaningful only within a particular
sense of a predicate. In general, like English, a Chinese predicate takes fewer than 6
arguments. The assignment of numbered argument labels is illustrated in Example (2),
where the predicate is the verb  (?investigate?). Its subject  (?the police?) is
labeled Arg0 and its object  (?accident?)  (?cause?) Arg1. The semantic role
labels added to the parse tree are in bold.
(2) IP
Arg0 VP
NP-SBJ ArgM-TMP ArgM-MNR VP

police
ADVP-TMP ADVP-MNR Rel Arg1

now

thoroughly
VV NP-OBJ

investigate
NN NN

accident

cause
?The police are thoroughly investigating the cause of the accident.?
The semantic adjuncts, on the other hand, are annotated as such with the label
ArgM followed by a secondary tag that represents the semantic classification of the
adjunct. Unlike the numbered argument labels for semantic arguments, the secondary
229
Computational Linguistics Volume 34, Number 2
tags represent information that is not predicate-specific. For instance, the adverbial
modifiers (?right now?) and (?thorough?) in Example (2) are labeled ArgM-
TMP and ArgM-MNR respectively, where the secondary tag TMP indicates a temporal
modifier and MNR indicates manner. The secondary tags are not predicate-specific in
the sense that they are not required by this particular predicate and they are not selective
with regard to the predicate they can occur with. There is a limited set of such secondary
tags that are used in the Chinese PropBank and the Chinese NomBank and the complete
list of such secondary tags is presented in Table 1.
The same approach is taken to annotate the nominalized predicates in the Chinese
NomBank. This is illustrated in Example (3), a close paraphrase of Example (2), where
the nominalized predicate (?investigation?) takes the same arguments as its verbal
counterpart.  (?the police?) is again Arg0 and  (?toward?)  (?accident?)
 (?cause?), despite its categorial change from a noun phrase to a prepositional
phrase, remains Arg1. There are also two semantic adjuncts: ArgM-TMP (?now?)
and ArgM-MNR (?thorough?). It is worth noting, however, that in this particular
case, the nominalized predicate needs a support verb  (?conduct?) to satisfy the
grammatical constraint that there be a verb in the sentence, and it is explicitly marked
as Sup. In addition, despite the categorial change of from adverb to adjective, the
semantic role label remains unchanged. In this sense, the semantic annotation abstracts
away from the underlying syntactic annotation.
(3) IP
Arg0 VP
NP-SBJArgM-TMP Arg1 VP

police
ADVP-TMP PP Sup NP-OBJ

now
P NP VV ArgM-MNR Rel

toward
NN NN 
conduct
ADJP NN

accident

cause

thorough

investigation
?The police are conducting a thorough investigation of the cause of the accident.?
Table 1
The complete list of functional tags defined in the Chinese Propbank and NomBank.
ADV adverbial FRQ frequency
BNF beneficiary LOC locative
CND condition MNR manner
DIR direction PRP purpose or reason
DIS discourse marker TMP temporal
DGR degree TPC topic
EXT extent
230
Xue Semantic Role Labeling of Chinese Predicates
Not all occurrences of a nominalized predicate need to be accompanied by a support
verb. In fact, it is often the case that all arguments of a nominalized predicate occur
in a noun phrase headed by the nominalized predicate. For example, in Example (4),
both Arg0 (?police?) and Arg1 (?toward?) (?accident?) (?cause?) are
syntactic modifiers of the nominalized predicate (?investigation?).
(4) IP
NP-SBJ VP
Arg0 DNP NP ADVP VP
NP Arg1 DEG Rel 
already

end

police
PP 
DE
NN
P NP 
investigation

toward
NN NN

accident

cause
?The police investigation of the cause of the accident has ended.?
The PropBank-style annotation is designed to account for syntactic variations, that
is, the different ways in which the same predicate?argument structure is realized. In
Examples (2) and (3), we have already seen where essentially the same predicate is real-
ized as a verb or a noun, and its arguments are realized as different syntactic categories
in different syntactic positions. Syntactic variations occur even without the categorial
change of the predicate. Levin (1993) demonstrates extensively how the argument
structure of English verbs can be realized differently through diathesis alternations.
Similar alternations can also be observed in Chinese, and Example (5) shows this:
(5) a. [Arg1
China

the U.S.

contact

DE

door
] [rel
open
]
ASP

.
?The door of contact between China and the U.S. has opened.?
b. [ArgM-TMP
70s

beginning
]
,
[Arg0
China

the U.S.

two

country

leader
] [ArgM-ADV
decisively
] [rel 
open
]
ASP
[Arg1
China

the U.S.

contact

DE

door
]
.
?In the beginning of the 1970s, the leaders of China and the U.S. decisively
opened the door of contact between China and the U.S.?
Note that even though (?China?) (?the U.S.?) (?contact?)  (?DE?)
 (?door?) occurs in different syntactic positions in (5a) and (5b), it is labeled Arg1 in
both cases. The semantic role label an argument receives is independent of its syntactic
231
Computational Linguistics Volume 34, Number 2
realizations. The semantic roles or expected arguments can be realized syntactically in
different ways. It should also be pointed out that the line drawn between arguments
and adjuncts here is not based on the obligatory/optional dichotomy. In some cases,
some constituents are clearly arguments but they are also clearly optional. For example,
in the unaccusative (or pseudo-passive) construction, the agent is clearly optional syn-
tactically and it is equally clear that it is an argument. In Example (5a), for example, the
?door-opener? is optional but is clearly an argument.
The Chinese PropBank also adds a coarse-grained sense tag to the predicate. The
senses of a predicate are motivated by the argument structure of this predicate and
are thus an integral part of the predicate?argument structure annotation. Sense disam-
biguation is performed only when different senses of a predicate require different sets of
arguments. For example, the ?evolve? sense of the verb ?? expects five arguments:
The cause of the evolution, which is often not realized, the entity evolving, the starting
point of the evolution, the end point of the evolution, and the range of the evolution.
When it means ?recruit,? two arguments are expected: the recruiter and the entity
recruited. Because each of these senses can be realized in different subcategorization
frames related through syntactic alternations, in the PropBank annotation convention,
these senses are formally called framesets, meaning sets of subcategorization frames
that realize a particular sense. The examples in (6) illustrate two of the framesets
of ??.
(6) Frameset 1: ?evolve?
Semantic roles:
Arg0: cause of evolution
Arg1: entity evolving
Arg2: evolving from
Arg3: evolving to
Arg4: range of evolution
a. [Arg1
Russia

domestic

for

industrial product

demand
] [Arg3
in
	
mid- and upper scale

direction
] [Rel
develop
]
.
?Russia?s domestic demand for industrial products is evolving in the direction
of mid- and upper scale products.?
Frameset 2: ?recruit?
Arg0: recruiter
Arg1: entity recruited
b. [Arg0
CEC
] [ArgM-TMP
presently
] [ArgM-LOC
in

the world

160

CL

country

and


region
] [Rel
recruited
]
ASP
[Arg1
more than eight thousand

CL

subscriber
]
.
?CEC presently have recruited over eight thousand subscribers from 160 coun-
tries and regions in the world.?
The Chinese NomBank uses the same framesets as defined for verbs because its
annotation is guided by the same frame files. However, typically only a subset of the
framesets for verbs have corresponding nominalized forms. For example, Frameset 1 in
232
Xue Semantic Role Labeling of Chinese Predicates
Example (6) has a corresponding nominalized form as illustrated in Example (7), but
Frameset 2 does not.
(7) 
Taiwan Straits

two

side

from now on

can

together

plan
[Arg1
cross-Strait
	
relations
]
DE
[Rel
development
]
.
?The two sides of the Taiwan Straits can plan the development of the cross-Strait
relations hereafter.?
3. System Overview
Assuming the availability of a parse tree (either hand-crafted parses in a treebank or
parses generated by an automatic parser) as input, to assign the semantic role labels
described in Section 2 automatically involves first of all identifying which constituents
in the parse tree are semantic arguments to the predicate in question and then assigning
appropriate semantic role labels to them. The predominant approach to the semantic
role labeling task is to formulate it as a classification problem (Pradhan, Ward et al
2004; Xue and Palmer 2004) that can be solved with machine-learning techniques. One
can imagine a classification task in which each constituent in the parse tree is labeled
either with one of the numbered argument labels (Arg0 through Arg5), or with one
of the semantic adjunct labels ArgM-TMP, ArgM-MNR, and so on, or with the NULL
label, indicating the constituent is neither an argument nor adjunct to the predicate.
This simple formulation of the classification problem is rarely practiced in the semantic
role labeling literature for the simple reason that the majority of the constituents in a
parse tree are generally not related to the predicate in question. For machine-learning
approaches, this means that the negative samples, constituents that are labeled NULL,
would far outweigh the positive samples, constituents that are actual semantic argu-
ments or adjuncts. Such an imbalance would lead to poor performance for machine-
learning systems, so in practice, most semantic role labeling systems work in stages,
whichminimally consist of an argument detection stage and an argument classification
stage. Argument detection is generally formulated as a binary classification task that
separates constituents that are arguments or adjuncts to a predicate from those that are
not related to the predicate in question. By lumping together argument and adjunct
labels, the positive and negative sample imbalance is alleviated somewhat. In addition,
it has been shown that argument detection and argument classification need different
sets of features (Xue and Palmer 2004). A system cannot take advantage of this if both
are done in one fell swoop. With a powerful machine-learning algorithm, argument
detection can be done with high accuracy (Hacioglu et al 2004; Pradhan, Ward et al
2004), provided that the appropriate features are used.
The positive and negative sample imbalance can only be partially addressed by
having a separate argument detection stage. Even with a binary classification task, the
number of negative samples is still overwhelmingly larger than the positive samples.
In addition, it does not take advantage of the fact that the arguments and adjuncts of a
predicate, verbal or nominalized, are related to the predicate itself in linguistically well-
understood structural configurations. The overwhelmingmajority of the arguments and
adjuncts are populated along the spine of the parse tree that the predicate projects. A
substantial number of the constituents can be eliminated from further consideration
as negative samples with a high degree of certainty. (See Sections 4.1 and 5.1 for an
233
Computational Linguistics Volume 34, Number 2
evaluation of how effectively the pruning algorithm works for verbs and nouns.) This
was proved to be a successful strategy by Xue and Palmer (2004) for the semantic role
labeling of English verbs; they use a heuristic algorithm to first prune out irrelevant con-
stituents before the remaining candidates are fed into an argument detection algorithm.
This strategy has also been effectively adopted by others (Cohn and Blunsom 2005;
Punyakanok, Roth, and Yih 2005) and is used here. As wewill show in later sections, the
pruning algorithm needs to be slightly different for verbal and nominal predicates and
they do not work equally well for all experimental conditions. Generally, the efficacy of
the pruning algorithm correlates with the quality of syntactic parses that the semantic
role labeling system takes as input. That is, it works much better with treebank parses
than with automatic parses. It also works more effectively for verbs than for nouns, the
arguments of which have a more diverse distribution.
Argument classification, which classifies the constituents into a category that corre-
sponds to one of the semantic role labels, is a natural multi-category classification prob-
lem. It has been generally shown in the literature (Pradhan et al 2003) that it is a good
idea to bias the argument detection stage toward high recall so that reasonably good
candidates can be passed along to the argument classification stage, and this means the
tag set for argument classification also includes the NULL label. Many classification
techniques?SVM (Pradhan, Ward et al 2004), perceptrons (Carreras and Ma`rquez
2004a), MaximumEntropy (Xue and Palmer 2004), and so forth?have been successfully
used to solve the semantic role labeling problem. In the work we report here, for both
argument detection and classification tasks, we used aMaximumEntropy classifier with
a tunable Gaussian prior in the Mallet Toolkit (McCallum 2002). TheMaximum Entropy
classifier does multi-category classification and thus can be straightforwardly applied
to the problem here. The classifier can be tuned to minimize overfitting by adjusting the
Gaussian prior.
In summary, in our system, the semantic role labeling is done in three stages, as
illustrated in Figure 1: pruning, argument detection, and argument classification.
4. Semantic Role Labeling of Verbs
In this section we report our experiments on the semantic role labeling of Chinese verbs,
using the Chinese PropBank as training and test material. There are two variables in
our experimental settings. The first variable is the level of human annotation in the
syntactic parses that serve as input to our semantic role labeling system.We used parses
that are fully automatic, parses that assume correct segmentation, parses that assume
correct segmentation as well as POS-tagging, and hand-crafted treebank parses. The
second experimental variable is whether it is known which constituents in the parse
Figure 1
System architecture.
234
Xue Semantic Role Labeling of Chinese Predicates
tree are arguments. If the constituents are known, the semantic role labeling reduces to
a pure classification problem where each class is one of the semantic role labels. The
semantic role labeling system only needs to determine what the correct semantic role
should be. If it is unknown which constituents are arguments or adjuncts and which
ones are irrelevant to the predicate in question, the system then needs to first figure
out which constituents, out of all the constituents in the parse tree, are arguments to
the predicate and then decide what the correct semantic role labels should be. We did
not experiment with all combinations of these two variables and the known constituent
experiment is only done for treebank parses.
In this section, we start by describing our pruning procedure for verbs in Section
4.1. We then present the features for our experiments on verbs in Section 4.2. In Section
4.3 we briefly describe the parsers we used for our experiments and we discuss our
experimental results in Section 4.4.
4.1 Pruning for Verbs
Section 3 demonstrated the need for and the feasibility of using a heuristic algorithm
to address the imbalance of positive and negative samples and in this section we show
how this algorithm is implemented for verbal predicates. The algorithm starts from the
predicate that anchors the annotation, and first collects all the syntactic complements
of this predicate, which are represented as sisters to the predicate. It then iteratively
moves one level up to the parent of the current node till it reaches the root of the tree.
At each level, the system tries to determine whether or not that level is a coordination
structure. The system only considers a constituent to be a potential candidate if it is a
modifier or a complement to the current node. In the case of a coordination structure,
the conjunct that the predicate does not occur in and all its children are eliminated as
possible arguments to the predicate in question. Punctuation marks at all levels are
ignored. It is worth pointing out that the functional tags and traces, which would have
been useful for this purpose, are not used to determine the candidates to allow for a fair
comparison between experiments on hand-crafted parses and parses generated by an
automatic parser. Typically, current parsers do not predict functional tags and traces.2 To
use Example (8) as a walk-through example, assuming the predicate we are interested
in is  (?investigate?), the algorithm starts from this predicate and adds the NP 
 (?accident?)  (?cause?) to the candidate list because it is a complement to the
predicate. Then it goes one level up to the VP and adds its two sisters, the ADVPs, 
 (?right now?) and (?carefully?) to the candidate list because they are modifiers.
Then the algorithm goes another level up to another VP, and determines that the two
VPs at this level are conjoined by the punctuation mark, and no candidate is added
at this level because it is a coordination structure. The algorithm then goes up to the
highest VP level, and adds its sister, the NP (?police?) to the list of candidates. The
algorithm terminates at the highest IP3 level. The candidates collected by this algorithm
are in circles. The nodes traversed are linked by dotted lines.
It is perhaps not surprising that the pruning algorithm works better with treebank
parses than with automatic parses. When the treebank parses are used, our pruning
2 There are some ongoing efforts to develop parsers that produce functional tags and traces (Gabbard,
Kulick, and Marcus 2006).
3 IP in the Chinese Treebank roughly corresponds to S in the Penn Treebank.
235
Computational Linguistics Volume 34, Number 2
algorithm can recall over 99% (8,052 out of 8,121 arguments in the test data) of the
arguments while pruning out over 93% (258,959 out of 276,734) nodes in the parse trees.
When automatic parses (Maxent segmentation + Bikel parser) are used as input to our
semantic role labeling system, out of 87% of the arguments that have a corresponding
constituent in the parse tree, our pruning algorithm can recall 74% of the arguments
while pruning out over 92% (247,530 out of 267,381) of the nodes in the parses. Our
experiments show that even when the automatic parses are used, the results are far
better when the pruning algorithm is used than when it is not used. If the pruning
algorithm is not used, the recall improves somewhat, but the precision plummets. The
less-than-expected drop in recall when the pruning algorithm is used is perhaps due to
the fact that the arguments that are pruned out also happen to be the hardest for the
semantic role labeling system to get right.
(8) IP
NP VP

police
VP PU VP
ADVP VP  ADVP ADVP VP

already
VV NP 
now

thoroughly
VV NP

arrive

scene

investigate
NN NN

accident

cause
?The police have arrived at the scene and are thoroughly investigating the cause
of the accident.?
4.2 Features
One characteristic of feature-based semantic role modeling is that the feature space is
generally large. This is in contrast with a low-level NLP task such as POS tagging, which
generally has a small feature space. A wide range of features have been shown to be
useful in previous work on semantic role labeling (Gildea and Jurafsky 2002; Pradhan,
Ward et al 2004; Xue and Palmer 2004) and we suspect that many more will be tested
before the field will settle down to a core set of features. In their preliminary work on
Chinese semantic role labeling, Sun and Jurafsky (2004) successfully ported a number
of the features used in Gildea and Jurafsky (2002) to Chinese. In our experiments we
adapted more features that have been described in recent work on English semantic
role labeling to Chinese. We used a combination of features from Gildea and Jurafsky
(G&J) (2002), Pradhan, Ward et al (P et al) (2004), and Xue and Palmer (X&P) (2004),
and these are used as baseline features. In addition, we proposed a set of new features
that used verb class information induced from the frame files of the Chinese PropBank,
as well as features that were designed to exploit the grammatical constructions that
236
Xue Semantic Role Labeling of Chinese Predicates
are unique to Chinese, specifically the BA (Bender 2000) and BEI (Huang 1999) con-
structions. We briefly discuss these features and where necessary explain at an intuitive
level why they are useful for semantic role labeling. It has been well-established in
the semantic role labeling literature that features are not equally effective for argument
detection and argument classification (Xue and Palmer 2004). Our experimental results
on Chinese semantic role labeling generally support this observation. The features we
used for the semantic role labeling of verbs are listed below. Features that are marked as
?C? are used only in the argument classification task. Features that are marked as ?D?
are for argument detection only. Features that are used in both argument detection and
argument classification stages are marked as ?C,D?.
I. Baseline Features:
C Position: The position is defined in relation to the predicate verb and the
values are before and after. (G&J)
C Subcat frame: The rule that expands the parent of the verb, for example,
VP?VV+NP. (G&J)
C Phrase type: The syntactic category of the constituent in focus, for example,
NP, PP. (G&J)
C First and last word of the constituent in focus. (P et al)
C Phrase type of the sibling to the left. (P et al)
C Subcat frame+: The subcat frame that consists of the NPs that surround the
predicate verb. This feature is defined by the position of the constituent in
focus in relation to this syntactic frame. (X&P)
C,D Predicate: The verb itself. (G&J)
C,D Path: The path between the constituent in focus and the predicate. (G&J)
C,D Head word and its part of speech: The head word and its part of speech are
often good indicators of the semantic role of a constituent. (G&J)
C,D Combination features: Predicate head word combination, predicate phrase
type combination. (X&P)
II. New features
C,D Path to BA and BEI: BA and BEI are function words that impact the order
of the arguments. BA words are a closed set and in the Chinese Treebank
they have the POS tag BA. Similarly, BEI words are also a closed set and
they are POS-tagged SB (for short BEI) and LB (for long BEI).
C,D Verb class: Verb class itself, verb class + head word combination, verb class
+ phrase type combination.
The position feature is useful because constituents receiving a particular semantic
role label may occur in some typical positions. For example, the majority of the adjuncts,
ARGMs, occur before the verb in Chinese. The path feature, defined as the route from the
constituent in focus to the predicate, represents amore ?fine-grained? position.Whereas
the values for the simple position feature are just BEFORE or AFTER, the values for
the path feature can represent syntactic notions like subject or object. For example, a
237
Computational Linguistics Volume 34, Number 2
subject may be represented as ?NP?IP?VP?VV? and an object may be represented as
?VV?VP?NP.? Intuitively, path features are more informative than simple position
features but they are also sparse because they are more specific. The path feature
proves to be particularly effective for the argument detection task, which is perhaps not
unexpected. As we have shown in Section 4.1, the arguments and adjuncts of a predicate
tend to be populated along the spine of a parse tree anchored by the predicate, and this
information is captured very nicely by the path from the predicate to the constituent in
question.
The head word and its part of speech are clearly informative for semantic role
labeling. For example, a noun phrase headed by  (?today?) is very likely to be a
temporal element; so is a prepositional phrase with the head word (?at?). However,
for prepositional phrases, the preposition is not always the most informative element.
Sometimes the head word of its NP complement is more predictive of the semantic
category. For example, in the prepositional phrase  (?at?) 	 (?Beijing?), the NP
head 	 (?Beijing?) is more telling of the fact that it indicates a location. So for
prepositional phrases we use both the preposition and the head noun as features in our
system. As has been discussed by Sun and Jurafsky, the head word feature also tends
to be sparse, especially given the smaller size of the Chinese Treebank. The chance of
seeing a word in the test data that also occurs in the training data is small. The POS
tag serves as one form of backoff: Constituents headed by words that have the same
part-of-speech are likely to receive the same semantic role labels as well.
The subcat feature, as implemented in previous work (Gildea and Jurafsky 2002), is
defined as the rule that expands the VP dominating the verbal predicate. By definition, it
does not vary with the constituents in a parse tree. In other words, all constituents in the
parse tree share the same subcat feature. Tomake up for the weakness of this feature, we
implemented another feature called subcat+, which is the syntactic frame feature in Xue
and Palmer (2004). The subcat+ feature heuristically identifies the key NP arguments
for a given predicate, and the feature value of a given constituent is determined by its
position in relation to these NPs and the predicate. In this way this feature varies with
the constituent being classified and it also partially addresses the issue that the semantic
role of one constituent is not independent of other arguments for this predicate.
As we pointed out in Section 2, the argument labels in the PropBank annotation
are verb-specific. Given a head word or phrase type, the system will be more certain
of the semantic role label when it also knows what the predicate is. The same head
word or phrase type may be associated with different semantic role labels for different
predicates. The head word + predicate and the phase type + predicate features are
designed to capture this linguistic intuition. The other type of combination features are
verb class + head word and verb class + phrase type. We will discuss the use of verb
classes as features in detail in Section 4.2.1.
The first-word-in-the-constituent and the phrase label of the left sibling features are
from Pradhan, Ward et al (2004) and the interested reader is referred to their work for
an explanation of why these are useful features. Because Chinese is a language with
mixed headedness, namely, some phrases are left-headed and some phrases are right-
headed, the first word and last word are a more robust but sloppier way of finding the
head when the head-finding heuristics fail.
Some of the linguistic phenomena that impact the syntactic realization of argument
structures in Chinese are the BA and BEI constructions. In the Chinese Treebank, BA
and BEI represent closed sets of light verbs that take clausal complements. The subject
of the clausal complement of BA tends to be Arg1 instead of Arg0 in a canonical clause
structure. The BEI construction is the Chinese passive construction in which the subject
238
Xue Semantic Role Labeling of Chinese Predicates
of the clause headed by BEI is typically Arg1. In order to capture this information, we
added as features the path from the BEI and BA words to the constituent in focus.
BA and BEI are not predicates themselves, so these features are only invoked for the
predicate in the complement clause of the BA and BEI.
4.2.1 Using Verb Classes to Improve Semantic Role Labeling.With the current experimental
setup, as is also the case in most of the work on semantic role labeling, training data and
test data are not divided by verb instances but by the number of articles. As a result,
it is expected that the verb instances are not evenly divided. It is entirely possible that
some verbs can only be found in the training data and others can only be found in the
test data. By our count, there are 4,526 verb types in the training data and 1,038 verb
types in the test data. One hundred seventy-six verb types that occur in the test data are
absent from the training data. Because the semantic role labels are defined with regard
to the individual verbs, this can be a real problem because the model learned in the
training process does not optimally fit with the test data if different verbs are involved.
Fortunately, many verbs have similar argument structures and therefore are annotated
with similar semantic role labels in the Chinese PropBank. For example, verbs like 

 (?enlarge?),
 (?make more drastic?),
 (?accelerate?),
 (?strengthen?),

 (?deepen?),
 (?accelerate?),
 (?give more weight?),
 (?make higher?) all
take two arguments, a theme that undergoes a change of state and an external force
or agent that brings about the change of state. These verbs are uniformly annotated
and they all have two numbered arguments with Arg0 denoting the cause and Arg1
denoting the theme. It would make sense to group these verbs together into a class and
use this information in the features as has been done for English using VerbNet (Yi,
Loper, and Palmer 2007). Having a membership in a particular class says something
about the predicate?argument structure of a verb. When a verb is absent in the training
data, which is a familiar sparse data problem, the class information may tell the system
how to label the semantic roles of this verb based on its semantic class.
Although to our knowledge no such classification exists for Chinese verbs based on
the predicate?argument structure, a rough classification can be automatically derived
from the frame files, which are created to guide the PropBank annotation. We classified
the verbs along three dimensions: the number of arguments, the number of framesets,
and selected syntactic alternations.
Number of arguments Verbs in the Chinese PropBank can have one to five argu-
ments, with the majority of them having one, two, or three arguments. Verbs with
zero arguments are auxiliary verbs4 like (?will?), (?be able to?),	 (?should?),

 (?dare?),  (?may?),  (?be willing to?),  (?can?),  (?can?),  (?must?), 	
 (?should?), and some other light verbs. Verbs that have five arguments are change of
state verbs like  (?lengthen?),  (?shorten?),  (?lower?),  (?increase?),
 (?enlarge?), 	 (?make smaller?). These verbs generally take as arguments a
theme that undergoes the change of state, the original state, the new state, the range
of the change, and the cause or agent that brings about the change.
Number of framesets A frameset roughly corresponds to a major sense. This infor-
mation is used because it is common that the different framesets of a verb can have
different numbers of arguments. For example, verbs like
 (?balance?) can be used
either as a non-stative verb, in which case it means ?balance,? or a stative verb, in
4 One could say that the argument of the auxiliary verb is the entire proposition, but in this phase of the
Chinese PropBank, auxiliary verbs are not annotated.
239
Computational Linguistics Volume 34, Number 2
which case it means ?balanced.? When it is used as a non-stative verb, it takes two
arguments, the thing or situation that is balanced and the balancer, the entity that
maintains the balance. When it is used as a stative verb, obviously it only takes a single
argument.
Syntactic alternations We also represent certain types of syntactic alternations. One
salient type of syntactic alternation is the well-known ?subject of intransitive / object
of transitive? alternation described in detail in Levin (1993). Chinese verbs that demon-
strate this alternation pattern include (?publish?). For example, (?this?) (CL)
 (?book?) plays the same semantic role even though it is the subject in ?/this/CL
/book /publish /AS? and the object in ?/this /CL /publishing
/house/publish/ASP/this/CL/book.?
Thus each verb will belong to a class with a symbol representing each of the
three dimensions. For example, a given verb may belong to the class ?C1C2a,? which
means that this verb has two framesets, with the first frameset having one argu-
ment and the second having two arguments. The ?a? in the second frameset repre-
sents a type of syntactic alternation. Forty classes were semi-automatically derived in
this manner.
Such a classification scheme will undoubtedly prove to be linguistically unsophis-
ticated. Verbs that have the same number of arguments may have different types of
arguments, and the current classification system does not pick up these distinctions.
However, our experiments show that even such a simple classification can be used to
provide features that improve the semantic role labeling performance.
4.3 Using Automatic Parses
Previous work (Sun and Jurafsky 2004) on Chinese semantic role labeling uses a
parser that assumes correct (hand-crafted) segmentation. As word segmentation is
a very challenging problem that has attracted a large body of research by itself, it is
still unclear how well semantic role tagging in Chinese can be performed in realistic
situations. In our experiments, we implemented a Maximum Entropy?based parser
similar to Luo (2003). The parser performs Chinese word segmentation, POS tagging,
and parsing in one integrated system. The parser is trained on the Xinhua news and
Broadcast news portion of the Chinese Treebank, which has 498K words. Tested on the
held-out test data, the parser achieved an unlabeled precision and recall of 0.889 and
0.868, respectively, for the combined word segmentation and parsing accuracy. When
the word segmentation is singled out for evaluation, the parser achieved an F-score of
0.969. It is important to point out that these results cannot be directly compared with
most of the results reported in the literature, where correct segmentation is assumed.
In addition, in order to account for the differences in segmentation, each character has
to be treated as a leaf of the parse tree. This is in contrast with word-based parsers
where words are terminals. For comparison purposes, we also used the Bikel parser
(Bikel 2004). Because the Bikel parser assumes segmented sentences as input, we
extracted the segmentation from the output of our parser and fed it into the Bikel
parser. We also experimented with using gold-standard segmentation and POS from
the Chinese Treebank as input to the Bikel parser to measure the effect of segmentation
and POS tagging on the performance on the semantic role labeling. Because semantic
role labeling is performed on the output of a syntactic parser, only constituents in the
parse tree are candidates. If there is no constituent in the parse tree that shares the same
text span with an argument in the manual annotation, the system cannot possibly get
240
Xue Semantic Role Labeling of Chinese Predicates
Table 2
Semantic role labeling results for verbal predicates.
parse constituents feature set precision recall F1 measure
treebank known baseline n/a n/a .931 (acc)
treebank known all n/a n/a .941 (acc)
treebank unknown baseline .920 .900 .910
treebank unknown all .930 .910 .920
maxent parser unknown baseline .689 .597 .639
maxent parser unknown all .694 .602 .645
Bikel parser (auto seg) unknown baseline .745 .596 .662
Bikel parser (auto seg) unknown all .748 .603 .668
Bikel parser (gold seg) unknown all .768 .625 .689
Bikel parser (gold pos) unknown all .795 .656 .719
a correct annotation. In other words, the best the system can do is to correctly label all
arguments that have a constituent with the same text span as in the parse tree.
4.4 Results and Discussion
4.4.1 Data. In all our experiments we use the Chinese Proposition Bank Version 1.0.5 This
version of the Chinese PropBank (Xue and Palmer 2003) consists of standoff annotation
on the first 760 articles (chtb_001.fid to chtb_931.fid) of the Chinese Treebank. This
chunk of the data has 250K words and 10,364 sentences. The total number of verb
types in this chunk of the data is 4,854.6 Following the convention of the English
semantic role labeling experiments, we divide the training and test data by the number
of articles, not by the verb instances. For all our experiments on semantic role labeling
of verbs, 72 files (chtb_001.fid to chtb_040.fid and chtb_900.fid to chtb_931.fid)
are held out as test data,7 40 files (chtb_041.fid to chtb_080.fid) are used as devel-
opment set, and the remaining 648 files (chtb_081.fidto chtb_899.fid) are used as
training data. The training, development, and test sets have 30,280; 1,971; and 3,454
propositions, respectively. Our parser is trained on the training and development set
plus 275K words of broadcast news that have been recently annotated as part of the
Chinese Treebank Project.8 That is, in addition to the training data for the semantic
role labeling experiments, it also uses a portion of the treebank which has not yet been
propbanked.
4.4.2 Results. The results of the semantic role labeling for both hand-crafted and au-
tomatic parses are presented in Table 2. These results represent an improvement over
what has been reported in Xue and Palmer (2005) due to the improved parsing results
and new features. To be used in real-world natural language applications, a semantic
5 This data is publicly available through the Linguistic Data Consortium.
6 These include the so-called stative verbs, which roughly correspond to adjectives in English.
7 This chunk of data is chosen as test data because it is double annotated and adjudicated.
8 We did not use the Sinorama portion of the Chinese Treebank because it is a very different genre and
adding it to the training data hurts parser performance (Bikel 2004).
241
Computational Linguistics Volume 34, Number 2
role tagger has to use automatically produced constituent boundaries either from a
parser or by some other means, but experiments with hand-crafted parses will help us
evaluate how much of a challenge it is to map a syntactic representation to a semantic
representation, which may very well vary from language to language. When hand-
crafted parses in the Chinese Treebank are used as input, our system achieved an
F-score of 0.92 for combined argument detection and classification. This accuracy is
achieved when the new features are added. Without the new features, the accuracy
drops about one percentage point. When the arguments are known, the accuracy is
at 94.1% when the new features are used, up one percentage point from the baseline.
This accuracy is fairly high considering the fact that the state-of-the-art for semantic
role labeling systems trained on the English PropBank (Palmer, Gildea, and Kingsbury
2005) is about 93% percent (Pradhan, Ward et al 2004; Xue and Palmer 2004) when
the arguments are known and the English PropBank is a much larger corpus that has
one million words. The high baseline accuracy also suggests that the features used
for the English semantic role labeling port very well to Chinese. In addition, there are
several facilitating factors for Chinese semantic role labeling when hand-crafted parses
are provided as input. First of all, Chinese verbs appear to be less polysemous, at least
the ones that occur in the Chinese Treebank. Of the 4,854 verbs in this version of the
Chinese Proposition bank, only 62 verbs have three or more framesets. In contrast, 294
verbs out of the 3,300 verbs in the Penn English PropBank have three or more framesets.
When a verb is less polysemous, the arguments of the verb tend to be realized in a more
uniform manner in syntax. As a result, the argument labels are easier to predict from
their structure. Chinese seems to compensate for this fact by using a larger number of
verbs. This becomes obvious when we consider the fact that the 4,854 verbs are from
just 250K words and the 3,300 verbs in the English PropBank are from one million
words. A related fact is that adjectives in Chinese are traditionally counted as verbs
and they generally have only one argument with a much simpler syntactic realization.
For example, 
 (?inexpensive?) and  (?thin?) are considered stative verbs in the
Chinese Treebank.
We also believe that a more subtle explanation for the higher semantic role labeling
accuracy given the annotation of the Chinese Treebank is the fact that the Chinese
Treebank has richer structure (see Xue et al [2005] for a comparison of the Penn English
Treebank and the Chinese Treebank). By using less flat and more hierarchical structures,
the Chinese Treebank resolves some of the attachment ambiguities that impair semantic
role labeling. For example, the complement and adjunct in a VP in the Chinese Treebank
are attached in different syntactic configurations with regard to the verb. Because com-
plements are generally numbered arguments and adjuncts are generally ARGMs, the
semantic role labeler can take advantage of this information when it tries to determine
when a constituent is a numbered argument or an adjunct.
This apparent advantage in Chinese semantic role labeling is diminished when an
automatic parser is used. First of all, the hierarchical structures in the hand-crafted
parses that aid semantic role labeling are hard to recover with an automatic parser.
Resolving the many attachment ambiguities caused by the hierarchical structures in
language is one of the most difficult problems in the parsing literature. Parsing Chinese
in a realistic scenario is especially difficult given that it has to build structures from
characters rather than words, and Chinese also has few morphological clues to help in
making parsing decisions. Our results show that the semantic role labeling accuracy
improves by 2.1% in F-score when the correct segmentation is used as input to the Bikel
parser. When the correct POS tags are used, the semantic role labeling accuracy im-
proves another 3%. At present, improvement in Chinese parsing is also hindered by the
242
Xue Semantic Role Labeling of Chinese Predicates
smaller training set. Although the Chinese Treebank 5.1 has a decent size of 500Kwords,
it consists of data from very different sources. Due to their very different styles, training
on one portion of the data does not help or may even hurt the parsing accuracy on the
other portion (see Bikel [2004] for a discussion of this issue). The situation improves
somewhat with the addition of the 275K words from broadcast news,9 which leads to
an improvement in parsing accuracy. We believe further improvement in semantic role
labeling accuracy will be to a large extent contingent on the parsing accuracy, which
requires more training materials that are similar in style.
5. Semantic Role Labeling of Nominalized Predicates
In this section, we describe our experiments on nominalized predicates in Chinese,
using the Chinese NomBank as training and test data. In Section 5.1 we show that
the pruning algorithm for nominalized predicates needs to account for two disjoint
cases. When a support verb is present, the pruning algorithm needs to go outside
the NP headed by the predicate to search for potential arguments. When there is no
support verb, the arguments can generally be found inside the NP headed by the
predicate. In Section 5.2, we describe the features used in the semantic role labeling of
nominalized predicates. There are three groups of features: features used in the semantic
role labeling of verbs minus a few features that do not carry over to nouns, features
used for verbs that need to be substantially adapted, and new features we designed
specifically for nominalized predicates. The experiments we conducted on nominalized
predicates largely parallel those of verbs, for an easier comparison. Again there are
two experimental variables, the level of human annotation in the input to the semantic
role labeling system and whether the constituents for the arguments are known. The
experimental results are presented in Section 5.3.
5.1 Pruning for Nominalizations
Like verbal predicates, the arguments and adjuncts of a nominalized predicate are
related to the predicate itself in linguistically well-understood structural configurations.
As we pointed out in Section 2, most of the arguments for nominalized predicates are
inside the NP headed by the predicate unless the NP is the object of a support verb, in
which case its arguments can occur outside the NP. Typically the subject of the support
verb is also an argument of the nominalized predicate, as illustrated in Example (3).
The majority of the constituents are not related to the predicate in question, especially
because the sentences in the treebank tend to be very long. There are two distinct cases
that need to be handled differently, depending on the presence or absence of a support
verb for the nominalized predicate. When the nominalized predicate does not occur
with a support verb, generally all of its arguments are realized within the NP of which
it is the head. The pruning algorithm starts from the predicate, collects its sisters, and
adds them to the candidate list. It then iteratively goes up one level and collects the
sisters of that constituent until it reaches the top-level NP of which it is the head. An
exception is made when the constituent is DNP, in which case the candidate added is
the first daughter of the DNP, not the DNP itself. This is illustrated in Example (9),
9 This new data set will soon be available via the LDC in another Chinese Treebank release.
243
Computational Linguistics Volume 34, Number 2
where the algorithm starts from the nominalized predicate (?investigation?), and,
because it does not have any sisters, it does not add anything to the candidate list at
this level. It then goes up to its parent NP, and collects its sisters NP ( ?police?)
and DNP ( ?toward? ?accident? ?cause? ?DE?). In the case of DNP, the
candidate added is actually its first daughter, the PP.
(9) IP
NP VP
NP DNP NP ADVP VP

police
PP DEG NN 
already

end
P NP 
DE

investigation

toward
NN NN

accident

cause
?The police investigation of the cause of the accident has ended.?
When a nominalized predicate occurs with a support verb, the NP headed by the
nominalized predicate is generally the object of the support verb. Arguments can often
be found both inside and outside this object NP. The pruning algorithm starts from
the nominalized predicate and collects its sisters. It then iteratively goes one level up
until it reaches the top-level IP node. At each level, the sisters of the current node are
added to the list of candidates. Note that the algorithm does not stop at the top NP
level, so that arguments outside the NP can also be captured. In practice, it is generally
not known to the algorithm whether the governing verb, the verb that takes the NP
headed by the nominalized predicate as object, is a support verb or not. Support verbs
are often light verbs and they are only a subset of all governing verbs. The system simply
assumes that all verbs taking the NP headed by a nominalized predicate as its object are
support verbs, adds constituents outside the NP as candidates, and lets the machine-
learning algorithm figure out whether they are arguments or not. This pruning process
is illustrated in Example (10), where the algorithm starts from the nominalized predicate
 (?investigation?). It first collects its sister ADJP ( ?thorough?), and then it will
go one level up to the NP, and adds the support verb ( ?conduct?) to the candidate
list. It will go another level up to the VP and adds its sisters ADVP ( ?now?) and
PP ( ?toward?  ?accident?  ?cause?) to the candidate list. It then goes one
more level up and decides this is a coordination structure; no candidate is added at this
level. At the next VP level it adds (?police?) to the list of candidates. The algorithm
terminates at the IP node.
244
Xue Semantic Role Labeling of Chinese Predicates
(10) IP
NP VP

police
VP PU VP
ADVP VP  ADVP PP VP

already
VV NP 
now
P NP VV NP

arrive

scene

toward
NN NN 
conduct
ADJP NN

accident

cause

thorough

investigation
?The police has arrived at the scene and is thoroughly investigating the cause of
the accident.?
Overall, pruning works less effectively for nouns than for verbs. When treebank
parses are used, our pruning algorithm can recall over 94% of the arguments while
pruning out 93% (87,724 out of 93,916) of the nodes. When automatic parses (maxent
segmentation + Bikel parser) are used, our pruning algorithm can recall 73% of the
arguments out of the 88% of arguments that have a constituent in the parse tree, while
pruning out 93% (85,160 out of 91,356) of the nodes. However, although there is a small
drop in recall (from 0.569 to 0.529) compared with when the pruning algorithm is not
used, there is a huge gain in precision (from 0.146 to 0.623), a similar trend to that which
we have observed for the semantic role labeling of verbs.
5.2 Features
The features we use for the semantic role labeling of nominalized predicates fall into
three groups. The baseline features we used are the same features we used for the
semantic role labeling of verbs. The second group of features are adapted from features
used in the semantic role labeling of verbs. In particular, the path feature is redefined
in the semantic role labeling of nominalized predicates. A significant number of NPs in
the Chinese Treebank are flat and they consist of a sequence of nouns. When there are
nouns on both sides of a predicate, which is a noun itself, the path from the predicate
to the preceding or following noun has the same value. However, the preceding and
following nouns do not have the same probability of being an argument. We therefore
need to clearly mark the position of the predicate (e.g, P=NN?NP?NN is not the same
as NN?NP?NN=P). Such a problem does not exist for the semantic role labeling of
verbs because their arguments are rarely a verb as well. The third group of features
are new features we added specifically for the semantic role labeling of nominalized
predicates. Like the features for the semantic role labeling of verbal predicates, the
features for argument detection only are marked as ?D? and the features for argument
245
Computational Linguistics Volume 34, Number 2
classification only are marked as ?C.? The features for both argument detection and
argument classification are marked as ?C,D.? The complete list of features is listed here.
I. Baseline Features:
C Position: The position is defined in relation to the predicate and the values
are before and after. Because most of the arguments for nominalized
predicates in Chinese are before the predicates, this feature is not as
discriminative as when it is used for verbal predicates where arguments
can be both before or after the predicate. (G&J)
C Phrase type: The syntactic category of the constituent being classified. (G&J)
C First and last word of the constituent being classified. (P et al)
C,D Predicate: The nominalized predicate itself. (G&J)
C,D Predicate combination features: Predicate + head word combination,
predicate + phrase type combination. (X&P)
C,D Predicate class: The verb class the predicate belongs to; same predicate class
as those used for verbs.
C,D Predicate class combination features. Predicate class + head combination,
predicate class + phrase type combination.
C,D Head word and its part of speech: The head word and its part of speech. (G&J)
C,D Path: The path between the constituent being classified and the predicate.
(G&J)
II. Adapted features:
C,D Path: The path between the constituent being classified and the predicate,
with the predicate clearly identified.
III. New Features:
D Topic NP: A binary feature indicating whether the constituent is a topic if
the predicate is the subject.
D Inside NP headed by the predicate: A binary feature indicating whether the
constituent in focus is inside the NP headed by the predicate.
D Position of the constituent in relation to the support verb: The value can be
before or after the support verb, or is the support verb itself.
C,D Sisterhood with predicate: A binary feature that indicates whether the
constituent is a sister to the predicate.
C,D Path + governing verb. The path feature combined with the governing verb.
Several features that we used for the semantic role labeling of verbal predicates were
dropped from our experiments with nominalized predicates. Specifically, the subcat
feature and subcat+ features were not used because it is not clear how these features
can be defined for a nominalized predicate. A couple of new features were added
to the feature set for semantic role labeling of nominalized predicates. As we have
demonstrated in Section 5.1, a support verb to a large extent determines whether or
246
Xue Semantic Role Labeling of Chinese Predicates
not the arguments of a nominalized predicate can occur outside the NP of which it
is the head. Therefore it is effective information for discriminating arguments from
non-arguments. It is also indicative of the specific semantic role of an argument in the
argument classification task. To capture this observation, we used a combined feature of
path+ governing verb that was only invoked when there was an intervening governing
verb between the constituent being classified and the predicate. The governing verb
is used as an approximation of the support verb for this feature because the system
does not have prior knowledge of whether a verb is a support verb or not absent some
external resource that provides a list of possible support verbs. The governing verb, on
the other hand, can be approximated by looking at the syntactic configuration between
the nominalized predicate and the verb. This feature is used for both argument detection
and argument classification. Another feature we specifically used for the semantic role
labeling of nominalized predicates is the sisterhood feature. When looking at the data,
we found a substantial number of NPs headed by a nominalized predicate have a flat
structure with their sisters as their arguments. The sisterhood feature is designed to
capture this observation and it is also used for both argument detection and argument
classification. The other three new features were used for argument detection only.
When a nominalized predicate is in the subject position, the NP in the topic position
tends to be its argument. A binary feature is invoked when the constituent in focus is
an NP that is the left sister of the subject NP headed by the predicate. Whether an NP is
a subject is also determined heuristically: An NP is considered to be subject if its parent
is an IP and its right sister is a VP. Another binary feature used for argument detection
is whether the constituent in focus is inside the NP headed by the predicate. Finally, the
position of the constituent in relation to the support verb is also used as a feature for
argument detection. The value for this feature can be before or after the support verb,
or it can be the support verb itself.
5.3 Experiments
5.3.1 Data. Our system is trained and tested on a pre-release version of the Chinese
NomBank. This version of the Chinese NomBank consists of standoff annotation on
the first 760 articles (chtb_001.fid to chtb_931.fid) of the Chinese Treebank. This
is the same chunk of treebank data as used in our experiments on verbs. It has 1,227
nominalized predicate types and 10,497 nominalized predicate instances, in comparison
with the 4,854 verb predicate types and 37,183 verb predicate instances in the same
chunk of data. By instance, the NomBank is between a quarter and one third of the
size of the Chinese PropBank. Similarly to our experiments on verbs, we divide the
training, development, and test data by the number of articles, not by the predicate
instances. For all our experiments, we used the same data split as that of the verbs: 648
files (chtb_081.fid to chtb_899.fid) are used as training data, 40 files (chtb_041.fid
to chtb_080.fid) are used as development data, and the other 72 files (chtb_001.fid to
chtb_040.fid and chtb_900.fid to chtb_931.fid) are held out as test data. The same
parsers are used for the semantic role labeling experiments for verbs and nouns.
5.3.2 Results and Discussion. Parallel to our experiments on verbs, we also present
experiments using hand-crafted and automatic parses. The experimental results are
presented in Table 3, which represents an improvement from what has been reported
in Xue (2006b). The baseline results are obtained using the subset of features used in the
semantic role labeling of verbs, minus the subcat and subcat+ features. We also report
improved results by using additional new features and adapting the path feature. The
247
Computational Linguistics Volume 34, Number 2
Table 3
Semantic role labeling results for nominalized predicates.
parse constituents feature set precision recall F1 measure
treebank known baseline n/a n/a .843 (acc)
treebank known all n/a n/a .849 (acc)
treebank unknown baseline .722 .608 .660
treebank unknown all .734 .661 .696
maxent parser unknown baseline .60 .471 .526
maxent parser unknown all .60 .502 .547
Bikel parser (auto seg) unknown baseline .611 .492 .545
Bikel parser (auto seg) unknown all .623 .529 .573
Bikel parser (gold seg) unknown all .629 .531 .576
Bikel parser (gold pos) unknown all .657 .560 .604
use of adapted and new features leads to significant improvement in all experiment
settings except when the constituents are already known and treebank parses are used.
This is not surprising given that more new features were added to the argument detec-
tion task than the argument classification task.
Compared with the 94.1% for verbal predicates on the same data, the 84.3% the
system achieved for nominalized predicates on treebank parses when the constituents
are given is considerably lower, suggesting that the semantic role labeling for nominal-
ized predicates is a much more challenging task. The difference between the semantic
role labeling accuracy for verbal and nominalized predicates is even greater when the
constituents are not given and the system has to identify the arguments to be classified.
Our system achieves an F-score of 0.696 when treebank parses are used, and this is
in contrast with the F-score of 0.92 for verbal predicates under similar experimental
conditions.
For our experiments using automatic parses, we used the same parsers for nomi-
nalized and verbal predicates. The first parser is the character-basedMaximum Entropy
parser that we developed in-house; and it does word segmentation, POS-tagging, and
syntactic parsing in one integrated system. The second parser is the Bikel parser that
takes three different kinds of input. In its fully automatic mode, it uses the segmentation
extracted from the output of our Maxent parser. We also experimented with using
correct segmentation and correct segmentation plus correct POS-tagging as input to
the Bikel parser to measure the degradation in performance with decreasing levels of
human annotation. Our results show that the Bikel parser outperforms our Maxent
parser 0.028 (F-score) in semantic role labeling accuracy when using fully automatic
parses. When the Bikel parser is used, the system achieves an F-score of 0.573, in com-
parison with the 0.547 achieved by the Maxent parser. There is a gradual degradation in
performancewith less human annotation, consistent with our experiments on verbs. It is
somewhat surprising that the segmentation does not affect the semantic role labeling for
nominalized predicates as it does for verbs. Using correct POS tags as input to the Bikel
parser, however, leads to a significant improvement of 0.028 in F-score over using correct
segmentation only, from 0.576 to 0.604. Overall, there is a smaller gap between when
treebank parses are used and when automatic parses are used. There are two possible
explanations. One is that the NP structures are more local and less prone to parsing
errors, so there is less of a difference between treebank and automatic parses. This is
248
Xue Semantic Role Labeling of Chinese Predicates
consistent with the fact that 88% of the arguments for nominalized predicates were
recovered by the parser, in contrast with the 87% of the arguments for verbal predicates.
Another possible explanation is that argument detection is challenging even with gold-
standard treebank parses, which makes the gap between treebank and automatic parses
smaller.
5.3.3 Error Analysis. The much lower accuracy in the semantic role labeling of nomi-
nalized predicates warrants a closer examination. One thing we looked at is the fact
that arguments of nominalized predicates can occur either inside or outside the NP
headed by the predicate. Of the 1,124 predicate instances in the test data, 331 of them
have arguments that occur outside the NP headed by the predicate. The remaining 793
instances have all their arguments inside the NP. We found a significant difference in
the semantic role labeling accuracy for the two types of predicates in the experiment
setting where the input to the semantic role labeling system is treebank parses and the
constituents are unknown. For the predicates that have arguments outside the NP, the
system achieved a precision and recall of 0.868 and 0.633, respectively. For the predicates
that have all their arguments inside the NP, the precision and recall are 0.661 and 0.695,
respectively. We believe the large difference in precision is the result of the system
erroneously identifying arguments outside the NP when the predicate heads an NP
that is the object of a verb, even if the verb is not a support verb. With a small data set,
there is insufficient training data for the system to tell whether or not a verb is a support
verb that licenses arguments outside the NP headed by the predicate. We also examined
cases where the predicate heads an NP that is the head of a relative clause. Because the
NP headed by the predicate is semantically associated with a trace inside the relative
clause, its arguments can generally be found inside the relative clause. Out of the 1,124
predicate instances, 138 are the heads of relative clauses. The precision and recall for
these predicates are 0.668 and 0.5, respectively, in comparison with the 0.749 and 0.696
for predicates that are not the head of a relative clause. The much lower recall suggests
the arguments for the head of a relative clause are much harder to identify.
6. Related Work
Computational approaches to semantic interpretation have a long tradition, but the
line of research that this work follows is relatively young. Gildea and Jurafsky (2002)
provided the seminal work on the semantic role labeling, using the FrameNet corpus
as training and test material. Since then, there has been rapid improvement in the se-
mantic role labeling accuracy of English verbs, fueled by the development of PropBank
(Palmer, Gildea, and Kingsbury 2005), which annotates the verbs in the one-million-
word Penn Treebank with semantic role labels. A wide range of statistical and machine
learning techniques have been applied to the semantic role labeling of verbs, using
PropBank as training and test material. The machine-learning techniques used include
Support VectorMachines (Pradhan,Ward et al 2004; Tsai et al 2005), MaximumEntropy
(Xue and Palmer 2004; Haghighi, Toutanova, and Manning 2005; Liu et al 2005; Yi
and Palmer 2005), Conditional Random Fields (Cohn and Blunsom 2005), and many
others. Because semantic role labeling is a complex task based on a wide range of
lower level natural language techniques, many different preprocessing, integration, and
combination techniques have been explored. The relative merits of using a full syntactic
parser that provides hierarchical structures (Xue and Palmer 2004) vs. a shallow chunker
(Pradhan, Hacioglu et al 2005; Hacioglu et al 2004) has been studied extensively.
Noting that parsing errors are difficult or even impossible to recover at the semantic
249
Computational Linguistics Volume 34, Number 2
role labeling stage, Yi and Palmer (2005) experimented with integrating semantic role
labeling with aMaximum Entropy-based parser, effectively treating semantic role labels
as function tags on the constituents in a parse tree. Koomen et al (2005), Pradhan, Ward
et al (2005), Ma`rquez et al (2005), and Tsai et al (2005) pursued alternative approaches
to make their semantic role labeling systems more robust by combining the output of
multiple systems. Punyakanok, Roth, and Yi (2005), in particular, achieved the best per-
formance (F1 = 0.794) on the WSJ test set in the 2005 CoNLL shared task by combining
multiple semantic role labeling systems using an integer linear programming technique
(Punyakanok et al 2004). Pradhan, Hacioglu et al (2005) reported the best result (F1 =
0.684) on the Brown test set using the WSJ data as the training set by combining the
output of different semantic role labeling classifiers using a chunking procedure. They
also reported the state-of-the-art result (F1 = 0.81) on the standard PropBank test set,
using the same techniques. Most of the early systems consider each argument on its
own when assigning the semantic role labels, allowing the theoretical possibility that
more than one core argument may share the same semantic role label, violating the
linguistic constraint that the same semantic role label cannot be assigned to more than
one core argument. Toutanova, Haghighi, and Manning (2005) address this by using a
joint-learning strategy to rule out such conflicting argument labels.
The semantic role labeling performance on the FrameNet data set has also
improved significantly from Gildea and Jurafsky?s (2002) early results, thanks mostly
to the Senseval-3 semantic role labeling competition (Litkowski 2004). Participants of
Senseval-3 have used a variety of machine learning algorithms to tackle the semantic
role labeling problem: Maximum Entropy (Baldewein et al 2004; Ngai et al 2004;
Kwon, Fleischman, and Hovy 2004); Boosting, SNOW, and Decision Lists (Ngai et al
2004); SVM (Bejan et al 2004; Moldovan et al 2004; Ngai et al 2004); Memory-based
learning (Baldewein et al 2004), as well as Generative models (Thompson, Patwardhan,
and Arnold 2004). Bejan et al (2004) achieved the best result using an SVM classifier
combined with improved linguistic features. They achieved an F1 measure of 0.763 in
their internal evaluation, and 0.831 using the more lenient official Senseval-3 scorer.
Compared with the large body of work on the semantic role labeling on verbs, the
argument structure analysis of nominal predicates has so far received less attention.
Jiang and Ng (2006) reports a semantic role labeling system on nominal predicates,
also using the maximum entropy approach. Their system achieves F1 scores of 0.727
and 0.691, respectively, on gold-standard and automatic parses, indicating semantic role
labeling of nominal predicates is a much more difficult problem than that of verbs for
English as well. Outside the narrow domain of semantic role labeling, there has been a
steady accumulation of work on semantic analysis of nouns and a gradual expansion
of the domain in which the semantic analysis is performed. Lapata (2002) developed a
probabilistic model for the interpretation of nominalizations, focusing on the semantic
relation between the noun head and its prenominal modifier in a nominalized com-
pound (i.e., whether the prenominal modifier is an underlying subject or direct object of
the verb fromwhich the nominalized head is derived). Theirmodel achieved a very high
accuracy of 0.861 when evaluated on data extracted from the British National Corpus.
Girju et al (2004) andMoldovan and Badulescu (2005) extended the domain of linguistic
analysis to that of noun phrases. In particular, they focused on the study of four nominal
constructions: complex nominals in which a head noun is modified by other nouns
or adjectives derived from nouns, genitives, adjective phrases, and adjective clauses.
In general, previous work on nominals, perhaps with the exception of Nakov and
Hearst (2006) all attempt to specify a finite set of semantic relations between the nouns
and their modifiers in the spirit of Levi (1979). The PropBank/NomBank approach to
250
Xue Semantic Role Labeling of Chinese Predicates
semantic role labeling adopted here represents a departure from this tradition in that
the semantic relations in PropBank and NomBank are predicate-specific. There is no
serious attempt to induce cross-predicate semantic relations. In addition, the semantic
relations represented by the PropBank/NomBank semantic roles are not pairwise rela-
tions between the predicate and one of its arguments as they are in previous work. A
third difference is that the arguments of a nominalized predicate can be found outside
the noun phrase headed by the predicate (Meyers, Reeves, and Macleod 2004; Meyers
et al 2004), making argument identification a much more challenging task. Whereas the
English NomBank annotates both relational nouns and nominalizations, the Chinese
NomBank only deals with nominalization, making it a more coherent task.
Work on Chinese semantic role labeling is still in its infancy. Lacking a Chinese cor-
pus annotated with semantic roles, the few prior works generally relied on annotating
a small corpus for their experiments. Sun and Jurasfky (2004) did preliminary work on
the semantic role labeling of Chinese verbs by annotating 10 selected verbs that have a
frequency ranging from 41 to 230, using the Chinese PropBank annotation guidelines.
Pradhan, Sun et al (2004) extended that work to Chinese nominalizations, and reported
preliminary work for analyzing the predicate?argument structure of 630 propositions
for 22 nominalizations taken from the Chinese Treebank. Noting the difficulty of
Chinese parsing, Kwong and T?sou (2005) approached the semantic role labeling task
as one of identifying and labeling the head word of the arguments. They annotated
the semantic roles for 41 verbs in 980 sentences in a primary school textbook corpus
and the same verbs in 2,122 sentences in a news corpus. Perhaps not surprisingly, they
reported F1 scores of 0.529 and 0.444, respectively, for the textbook and news corpora
when training and test data are from the same corpus, and 0.463 and 0.398, respectively,
when the training and test data are from different corpora. As far as we know, the
work reported here is the first to use sizable Chinese semantically annotated corpora.
The approach adopted in the present work emphasizes the integration of linguistically
informed heuristics and machine-learning approaches, and the exploration of the un-
derlining linguistic insights behind the features used in machine-learning systems. We
believe semantic role labeling provides an ideal stage where linguistic observations can
be formalized as features and fed into a general machine-learning framework for testing
and verification and natural language technologies can be advanced in the process.
7. Conclusions and Future Work
Wehave presented the first experimental results on Chinese semantic role labeling using
the Chinese PropBank and the Chinese NomBank. We have shown that given gold-
standard parses, Chinese semantic role labeling can be performed with considerable
accuracy on Chinese verbs. In fact, even though the Chinese PropBank is a significantly
smaller corpus than the English PropBank, we achieved results that are comparable
with the state-of-the-art English semantic role labeling systems.We suggest three factors
that are particularly conducive to the semantic role labeling of Chinese verbs when
the hand-crafted treebank parses are used as input. One is that Chinese verbs tend
to be less polysemous compared with English, which contributes to a more uniform
mapping between the predicate?argument structure and its syntactic realization. An-
other facilitating factor is that stative verbs, which generally translate into adjectives in
English, account for a large proportion of all the verbs in the Chinese PropBank and they
tend to have simple argument structures. Finally, we suggest that the richer structure
in the Chinese Treebank makes certain aspects of the semantic role labeling simpler.
251
Computational Linguistics Volume 34, Number 2
One such example is that the clear structural distinction between syntactic arguments
and adjuncts makes it easier for the semantic role labeling system to differentiate core
arguments and adjuncts for Chinese verbs. These all translate into lower confusability
along the lines of Erk and Pado? (2005) in the mapping from the syntactic structure to
the semantic role labels.
When the semantic role labeling takes raw text as input, it cannot take advantage
of the rich syntactic structure of the treebank unless it can be reproduced with high
accuracy by an automatic parser. Even though our experiments using fully automatic
parses yield promising initial results, the accuracy is significantly lower than the English
state of the art. Our parsing accuracy is hampered by a significantly smaller training set
that is only half the size of the Penn Treebank. We also suggest that there are a few
inherent linguistic properties of the Chinese language that make syntactic parsing a
particularly challenging task. The first has to do with the fact that Chinese text does
not come with word boundaries and our parser has to build structures from characters
rather than words. The second has to do with the fact that Chinese has very little
inflectional morphology that the parser can exploit when deciding the part-of-speech
tags of the words. Both word segmentation and POS-tagging difficulties will lead to
parsing errors when larger phrase structures are built.
Our experimental results also show a substantial gap between system performance
on verbs and nominalized predicates. This difference can be partially attributed to the
smaller corpus size of the Chinese Nombank, with fewer instances of nominalized
predicates than verbs in the underlying Chinese Treebank, but we believe the main
reason is that the semantic role labeling is more challenging for nominalized predicates
than for verbs. This again can be explained in terms of confusability in the mapping
from syntactic structure to the predicate?argument structure. In general, the NPs in the
Chinese Treebank have flatter structures compared with verbs. For example, there is no
clear structural distinction between arguments and adjuncts for nominalized predicates
that are analogous to the argument/adjunct distinction for verbs. Another reason for
the lower accuracy for nominalized predicates is the more diverse distribution of their
arguments. Arguments can be found either inside or outside the NP headed by the
predicate, or even in relative clauses that modify the NP headed by the predicate.
There are many directions we can go from here for future work. There are many
proven techniques that can be implemented for Chinese, the most important of which
is to make Chinese parsers more robust. One thing we plan to experiment with is the
combination of multiple parsers and multiple semantic role labeling systems. We also
believe that we have not settled on an ?optimal? set of features for Chinese semantic role
labeling and more language-specific customization is necessary. We believe that joint-
learning is also a promising avenue to pursue, especially for verbs where generallymore
core arguments are realized.
Acknowledgments
We would like to thank Martha Palmer for
her comments on this manuscript and early
versions of the paper, and more importantly
for her steadfast support for this line of
research. We also would like to thank Scott
Cotton for providing a PropBank library that
greatly simplified our implementation.
Thanks also to the anonymous reviewers for
their invaluable comments. This work is
supported by the NSF ITR via grant
130-1303-4-541984-XXXX-2000-1070.
References
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley
FrameNet project. In Proceedings of
COLING/ACL, pages 86?90, Montreal,
Canada.
252
Xue Semantic Role Labeling of Chinese Predicates
Baldewein, Ulrike, Katrin Erk, Sebastian
Pado?, and Detlef Prescher. 2004. Semantic
role labelling with similarity-based
generalization using em-based clustering.
In Rada Mihalcea and Phil Edmonds,
editors, Proceedings of Senseval-3,
pages 64?68, Barcelona, Spain.
Bejan, Cosmin Adrian, Alessandro Moschitti,
Paul Mora?rescu, Gabriel Nicolae, and
Sanda Harabagiu. 2004. Semantic parsing
based on framenet. In Rada Mihalcea and
Phil Edmonds, editors, Proceedings of
Senseval-3, pages 73?76, Barcelona,
Spain.
Bender, Emily. 2000. The syntax of Mandarin
-ba. Journal of East Asian Linguistics,
9(2):105?145.
Bikel, Daniel M. 2004. On the Parameter Space
of Generative Lexicalized Statistical Parsing
Models. Ph.D. thesis, University of
Pennsylvania.
Boas, Hans C. 2002. Bilingual FrameNet
dictionaries for machine translation. In
Proceedings of LREC 2002, pages 1364?1371,
Las Palmas, Spain.
Burchardt, A., K. Erk, A. Frank, A. Kowalski,
S. Pado, and M. Pinkal. 2006. The SALSA
Corpus: a German corpus resource for
lexical semantics. In Proceedings of LREC
2006, pages 969?974, Genoa, Italy.
Carreras, Xavier and Llu??s Ma`rquez. 2004a.
Hierarchical recognition of propositional
arguments with perceptrons. In Proceedings
of the Eighth Conference on Natural Language
Learning, pages 106?109, Boston, MA.
Carreras, Xavier and Llu??s Ma`rquez. 2004b.
Introduction to the CoNLL-2004 shared
task: Semantic role labeling. In Proceedings
of the Eighth Conference on Natural
Language Learning, pages 89?97,
Boston, MA.
Carreras, Xavier and Llu??s Ma`rquez. 2005.
Introduction to the CoNLL-2005 shared
task: Semantic role labeling. In Proceedings
of the Nineth Conference on Natural Language
Learning, pages 152?164, Ann Arbor, MI.
Chen, Ying, Hongling Sun, and Dan Jurafsky.
2005. A corrigendum to Sun and Jurafsky
(2004) ?Shallow Semantic Parsing of
Chinese.? Technical Report
TR-CSLR-2005-01, University of Colorado
at Boulder CSLR Tech Report.
Cohn, Trevor and Philip Blunsom. 2005.
Semantic role labeling with tree conditional
random fields. In Proceedings of CoNLL2005,
pages 169?172, Ann Arbor, MI.
Collins, Michael. 1999. Head-driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania.
Erk, K. and S Pado?. 2005. Analyzing
models for semantic role assignment
using confusability. In Proceedings of
HLT-EMNLP, pages 891?898, Vancouver,
British Columbia, Canada.
Gabbard, Ryan, Seth Kulick, and Mitchell
Marcus. 2006. Fully parsing the Penn
treebank. In Proceedings of HLT-NAACL
2006, pages 184?191, New York, NY.
Gildea, D. and D. Jurafsky. 2002. Automatic
labeling for semantic roles. Computational
Linguistics, 28(3):245?288.
Girju, Roxana, Ana-Maria Giuglea, Marian
Olteanu, Ovidiu Fortu, Orest Bolohan,
and Dan Moldovan. 2004. Support
vector machines applied to the
classification of semantic relations.
In Proceedings of the HLT/NAACL
Workshop on Computational Lexical
Semantics, pages 68?75, Boston, MA.
Hacioglu, Kadri, Sameer Pradhan, Wayne
Ward, James H. Martin, and Daniel
Jurafsky. 2004. Semantic role labeling
by tagging syntactic chunks. In
Proceedings of CoNLL-2004, pages 110?113,
Ann Arbor, MI.
Haghighi, Aria, Kristina Toutanova, and
Christopher Manning. 2005. A joint model
for semantic role labeling. In Proceedings of
CoNLL, pages 173?176, Ann Arbor, MI.
Huang, James C. T. 1999. Chinese passives in
comparative perspective. Tsing Hua Journal
of Chinese Studies, 29:423?509.
Jiang, Zheng Ping and Hwee Tou Ng. 2006.
Semantic role labeling of NomBank:
A maximum entropy approach. In
Proceedings of the EMNLP, pages 138?145,
Sydney, Australia.
Kipper, K., A. Korhonen, N. Bryant, and
M. Palmer. 2006. Extending verbNet with
novel verb classes. In Proceedings of LREC,
Genoa, Italy.
Koomen, Peter, Vasin Punyakanok,
Dan Roth, and Wen tau Yih. 2005.
Generalized inference with multiple
semantic role labeling systems. In
Proceedings of the Nineth Conference on
Natural Language Learning, pages 181?184,
Ann Arbor, MI.
Kwon, Namhee, Michael Fleischman, and
Eduard Hovy. 2004. Senseval automatic
labeling of semantic roles using Maximum
Entropy models. In Rada Mihalcea and
Phil Edmonds, editors, Proceedings of
Senseval-3, pages 129?132, Barcelona,
Spain.
Kwong, Oi Yee and Benjamin K. T?sou. 2005.
Data homogeneity and semantic role
tagging in Chinese. In Proceedings of the
253
Computational Linguistics Volume 34, Number 2
ACL-SIGLEX Workshop on Deep Lexical
Acquisition, pages 1?9, Ann Arbor, MI.
Lapata, Maria. 2002. The disambiguation of
nominalizations. Computational Linguistics,
28(3):357?388.
Levi, Judith. 1979. The Syntax and Semantic of
Complex Nominals. New York: Academic
Press.
Levin, Beth. 1993. English Verbs and
Alternations: A Preliminary Investigation.
Chicago: The Unversity of Chicago Press.
Litkowski, Ken. 2004. Senseval-3 task:
Automatic labeling of semantic roles. In
Rada Mihalcea and Phil Edmonds, editors,
Proceedings of Senseval-3, pages 9?12,
Barcelona, Spain.
Liu, T., W. Che, S. Li, Y. Hu, and H. Liu.
2005. Semantic role labeling with a
maximum entropy classifier. In
Proceedings of CoNLL-2005, pages 189?192,
Ann Arbor, MI.
Luo, Xiaoqiang. 2003. A maximum entropy
Chinese character-based parser. In
Proceedings of EMNLP, Sapporo, Japan.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313?330.
Ma`rquez, Llu??s, Mihai Surdeanu, Pere
Comas, and Jordi Turmo. 2005. A robust
combination strategy for semantic role
labeling. In Proceedings of HLT/EMNLP
2005, pages 644?651, Vancouver, Canada.
McCallum, Andrew Kachites. 2002. Mallet: A
machine learning for language toolkit.
Available at http://mallet.cs.umass.edu.
Melli, G., Y. Wang, Y. Liu, M. M. Kashani,
Z. Shi, B. Gu, A. Sarkar, and F. Popowich.
2005. Description of SQUASH, the SFU
question and summary handler for the
DUC-2005 summarization task. In
Document Understanding Conference 2005,
Vancouver, Canada.
Meyers, A., R. Reeves, C. Macleod,
R. Szekely, V. Zielinska, B. Young, and
R. Grishman. 2004. The NomBank Project:
An interim report. In Proceedings of the
NAACL/HLT Workshop on Frontiers in
Corpus Annotation, pages 24?31,
Boston, MA.
Meyers, A., R. Reeves, and Catherine
Macleod. 2004. NP-External arguments: A
study of argument sharing in English. In
The ACL 2004 Workshop on Multiword
Expressions: Integrating Processing,
pages 96?103, Barcelona, Spain.
Moldovan, Dan and Adriana Badulescu.
2005. A semantic scattering model for the
automatic interpretation of genitives. In
Proceedings of HLT-EMNLP, pages 891?898,
Vancouver, Canada.
Moldovan, Dan, Roxana Girju, Marian
Olteanu, and Ovidiu Fortu. 2004. SVM
classification of FrameNet semantic roles.
In Proceedings of Senseval-3, pages 167?170,
Barcelona, Spain.
Nakov, Preslav and Marti Hearst. 2006.
Using verbs to characterize noun-noun
relations. In Proceedings of AIMSA,
pages 233?244, Varna, Bulgaria.
Narayanan, Srini and Sanda Harabagiu.
2004. Question answering based on
semantic structures. In Proceedings
of the 20th International Conference on
Computational Linguistics, pages 693?701,
Geneva, Switzerland.
Ngai, Grace, Dekai Wu, Marine Carpuat,
Chi-Shing Wang, and Chi-Yung Wang.
2004. Semantic role labeling with
boosting, svms, maximum entropy,
snow, and decision lists. In Rada
Mihalcea and Phil Edmonds, editors,
Proceedings of Senseval-3, pages 183?186,
Barcelona, Spain.
Palmer, Martha, Daniel Gildea, and Paul
Kingsbury. 2005. The Proposition Bank:
An annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?106.
Pradhan, Sameer, Kadri Hacioglu,
Wayne Ward and James H. Martin, and
Daniel Jurafsky. 2005. Semantic role
chunking combining complementary
syntactic views. In Proceedings of CoNLL
2005, pages 217?220, Ann Arbor, MI.
Pradhan, Sameer, Kadri Hacioglu,
Wayne Ward, James H. Martin, and
Daniel Jurafsky. 2003. Semantic role
parsing: Adding semantic structure
to unstructured text. In Proceedings
of the International Conference on Data
Mining (ICDM-2003), pages 629?632,
Melbourne, FL.
Pradhan, Sameer, Honglin Sun, Wayne
Ward, James H. Martin, and Daniel
Jurafsky. 2004. Parsing arguments of
nominalizations in English and Chinese.
In Proceedings of NAACL-HLT 2004,
pages 141?144, Boston, MA.
Pradhan, Sameer, Wayne Ward, Kadri
Hacioglu, James H. Martin, and Dan
Jurafsky. 2005. Semantic role labeling using
different syntactic views. In Proceedings of
ACL 2005, pages 581?588, Ann Arbor, MI.
Pradhan, Sameer, Wayne Ward, Kadri
Hacioglu, James H. Martin, and Daniel
Jurafsky. 2004. Shallow semantic parsing
using support vector machines. In
254
Xue Semantic Role Labeling of Chinese Predicates
Proceedings of NAACL-HLT 2004,
pages 233?240, Boston, MA.
Punyakanok, Vasin, Dan Roth, and W. Yih.
2005. The necessity of syntactic parsing for
semantic role labeling. In Proceedings of
IJCAI-2005, pages 1124?1129,
Edinburgh, UK.
Punyakanok, Vasin, Dan Roth, Wen Tau. Yih,
and Dav Zimak. 2004. Semantic role
labeling via integer programming
inference. In Proceedings of COLING-2004,
pages 1346?1352, Geneva, Switzerland.
Sgall, Petr, Jarmila Panevova?, and Eva
Hajic?ova?. 2004. Deep syntactic annotation:
Tectogrammatical representation and
beyond. In A. Meyers, editor, Proceedings of
the HLT-NAACL 2004 Workshop: Frontiers in
Corpus Annotation, pages 32?38, Boston,
MA.
Sun, Honglin and Daniel Jurafsky. 2004.
Shallow semantic parsing of Chinese. In
Proceedings of NAACL 2004, pages 249?256,
Boston, MA.
Surdeanu, Mihai, Sanda Harabagiu, John
Williams, and Paul Aarseth. 2005. Using
predicate-argument structures for
information extraction. In Proceedings of
ACL 2003, Ann Arbor, MI.
Thompson, Cynthia, Siddharth Patwardhan,
and Carolin Arnold. 2004. Generative
models for semantic role labeling. In Rada
Mihalcea and Phil Edmonds, editors,
Proceedings of Senseval-3, pages 235?238,
Barcelona, Spain.
Toutanova, Kristina, Aria Haghighi, and
Christopher Manning. 2005. Joint learning
improves semantic role labeling. In
Proceedings of ACL-2005, pages 589?596,
Ann Arbor, MI.
Tsai, Tzong-Han, Chia-Wei Wu, Yu-Chun
Lin, and Wen lian Hsu. 2005. Exploiting
full parsing information to label semantic
roles using an ensemble of ME and SVM
via integer linear programming. In
Proceedings of CoNLL-2005, pages 233?236,
Ann Arbor, MI.
Xue, Nianwen. 2006a. Annotating the
predicate-argument structure of
Chinese nominalizations. In Proceedings
of the Fifth International Conference on
Language Resources and Evaluation,
pages 1382?1387, Genoa, Italy.
Xue, Nianwen. 2006b. Semantic role labeling
of nominalized predicates in Chinese.
In Proceedings of HLT-NAACL 2006,
pages 431?438, New York, NY.
Xue, Nianwen and Martha Palmer. 2003.
Annotating the propositions in the Penn
Chinese Treebank. In The Proceedings of the
2nd SIGHANWorkshop on Chinese Language
Processing, Sapporo, Japan.
Xue, Nianwen and Martha Palmer. 2004.
Calibrating features for semantic
role labeling. In Proceedings of 2004
Conference on Empirical Methods in Natural
Language Processing, pages 88?94,
Barcelona, Spain.
Xue, Nianwen and Martha Palmer. 2005.
Automatic semantic role labeling for
Chinese verbs. In Proceedings of the
Nineteenth International Joint Conference on
Artificial Intelligence, pages 1160?1165,
Edinburgh, Scotland.
Xue, Nianwen, Fei Xia, Fu dong Chiou, and
Martha Palmer. 2005. The Penn Chinese
TreeBank: Phrase structure annotation of a
large corpus. Natural Language Engineering,
11(2):207?238.
Yi, Szuting, Edward Loper, and Martha
Palmer. 2007. Can semantic roles
generalize across genres? In Proceedings
of NAACL-2007, pages 548?555,
Rochester, NY.
Yi, Szuting and Martha Palmer. 2005. The
integration of syntactic parsing and
semantic role labeling. In Proceedings
of CoNLL-2005, pages 237?240,
Ann Arbor, MI.
255

Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 431?438,
New York, June 2006. c?2006 Association for Computational Linguistics
Semantic Role Labeling of Nominalized Predicates in Chinese
Nianwen Xue
Center for Research in Spoken Language
University of Colorado
Boulder, CO, 80309
Nianwen.Xue@colorado.edu
Abstract
Recent work on semantic role labeling
(SRL) has focused almost exclusively on
the analysis of the predicate-argument
structure of verbs, largely due to the lack
of human-annotated resources for other
types of predicates that can serve as train-
ing and test data for the semantic role
labeling systems. However, it is well-
known that verbs are not the only type
of predicates that can take arguments.
Most notably, nouns that are nominalized
forms of verbs and relational nouns gen-
erally are also considered to have their
own predicate-argument structure. In this
paper we report results of SRL experi-
ments on nominalized predicates in Chi-
nese, using a newly completed corpus,
the Chinese Nombank. We also dis-
cuss the impact of using publicly avail-
able manually annotated verb data to im-
prove the SRL accuracy of nouns, exploit-
ing a widely-held assumption that verbs
and their nominalizations share the same
predicate-argument structure. Finally, we
discuss the results of applying reranking
techniques to improve SRL accuracy for
nominalized predicates, which showed in-
significant improvement.
1 Introduction
Detecting and classifying the arguments of predi-
cates has been an active area of research in recent
years, driven by the availability of large-scale se-
mantically annotated corpora such as the FrameNet
(Baker et al, 1998) and the Propbank (Palmer et
al., 2005). It is generally formulated as a seman-
tic role labeling (SRL) task, where each argument
of the predicate is assigned a label that represents
the semantic role it plays with regard to its pred-
icate (Gildea and Jurafsky, 2002; Hacioglu et al,
2003; Pradhan et al, 2004b; Xue and Palmer, 2004;
Toutanova et al, 2005; Koomen et al, 2005). It
has been the shared task for the CoNLL competition
for two consecutive years (Carreras and Ma`rquez,
2004b; Carreras and Ma`rquez, 2005). This line of
research has also expanded from English to other
languages (Sun and Jurafsky, 2004; Xue and Palmer,
2005). So far, however, most of the research efforts
have focused on analyzing the predicate-argument
structure of verbs, largely due to absence of an-
notated data for other predicate types. In this pa-
per, we report SRL experiments performed on nom-
inalized predicates in Chinese, taking advantage of
a newly completed corpus, the Chinese Nombank
(Xue, 2006), which we describe in greater detail in
Section 2. The rest of the paper is organized as fol-
lows. Section 3 describes the architecture of our sys-
tem as well as the features we used in our experi-
ments. In Section 4 we describe the experimental
setups and report our experimental results. We first
present experiments that use hand-crafted parses as
input, providing a measurement of how well the
Nombank annotation can be bootstrapped from the
syntactic structure in the treebank. We then describe
a more realistic experimental setup in which an au-
tomatic parser is first used to parse unsegmented raw
431
text and its output is then fed into our SRL system.
We also discuss whether verb data can be used to im-
prove the SRL accuracy of nominalized predicates.
Finally we describe a preliminary experiment that
uses reranking techniques to improve the SRL ac-
curacy on hand-crafted parses. Section 5 attempts to
put our results in perspective in the context of related
work. Section 6 concludes our paper.
2 The Chinese Nombank
The Chinese Nombank extends the general anno-
tation framework of the English Proposition Bank
(Palmer et al, 2005) and the English Nombank
(Meyers et al, 2004) to the annotation of nomi-
nalized predicates in Chinese. Like the English
Nombank project, the Chinese Nombank adds a
layer of semantic annotation to the Chinese Tree-
Bank (CTB), a syntactically annotated corpus of 500
thousand words. The Chinese Nombank annotates
two types of elements that are associated with the
nominalized predicate: argument-like elements that
are expected of this predicate, and adjunct-like el-
ements that modify this predicate. Arguments are
assigned numbered labels (prefixed by ARG, e.g.,
ARG0...ARGn) while adjuncts receive a functional
tag (e.g., TMP for temporal, LOC for locative, MNR
for manner) prefixed by ARGM. A predicate gen-
erally has no more than six numbered arguments
and the complete list of functional tags for adjuncts
and their descriptions can be found in the annotation
guidelines of this project.
The Chinese Nombank also adds a coarse-grained
sense tag to the predicate. The senses of a predicate,
formally called framesets, are motivated by the ar-
gument structure of this predicate and are thus an
integral part of the predicate-argument structure an-
notation. Sense disambiguation is performed only
when different senses of a predicate require different
sets of arguments. These senses are the same senses
defined for the corresponding verbs in the Chinese
Proposition Bank, but typically only a subset of the
verb senses are realized in their nominalized forms.
The example in 1 illustrates the Chinese Nombank
annotations, which are the labels in bold in the parse
tree. Take u?(?development?) as an example, f1
is the frameset identifier. Of the four expected argu-
ments for this frameset, ARG0 the cause or agent,
ARG1 the theme, ARG2 the initial state and ARG3
the end state or goal, only ARG1 is realized and it
is ?W'X(?cross-Strait relations?). The predi-
cate also has a modifier labeled ARGM-TMP, 8
 (?hereafter?).
Typically the arguments and adjuncts of a nomi-
nalized predicate are realized inside the noun phrase
headed by the nominalized predicate, as is the case
for u?(?development?) in Example 1. A main
exception is when the noun phrase headed by the
nominalized predicate is an object of a support verb,
in which case the arguments of this predicate can
occur outside the noun phrase. This is illustrated
by 5y(?planning?) in Example 1, where the noun
phrase of which it is the head is the object of a sup-
port verb ?1(?conduct?), which has little mean-
ing of its own. Both arguments of this predicate,
? b ? W(?the two sides of the Taiwan Strait?)
and 8 ? W ' Xu ?(?the development of
the cross-Strait relations?), are realized outside the
noun phrase. There are also a few other general ten-
dencies about the arguments of nominalized predi-
cates that are worth pointing out. The distribution of
their arguments is much less predictable than verbs
whose arguments typically occupy prominent syn-
tactic positions like the subject and object. There
also tend to be fewer arguments that are actually
realized for nominalized predicates. Nominalized
predicates also tend to take fewer types of adjuncts
(ARGMs) than their verbal counterpart and they
also tend to be less polysemous, having only a subset
of the senses of their verb counterpart.
The goal of the semantic role labeling task de-
scribed in this paper is to identify the arguments
and adjuncts of nominalized predicates and assign
appropriate semantic role labels to them. For the
purposes of our experiments, the sense information
of the predicates are ignored and left for future re-
search.
3 System description
The predominant approach to the semantic role la-
beling task is to formulate it as a classification prob-
lem that can be solved with machine-learning tech-
niques. Argument detection is generally formulated
as a binary classification task that separates con-
stituents that are arguments or adjuncts to a pred-
432
IP
NP-SBJ VP
ARG0/REL2 VV VP
?b?W
the two sides
of the Straits
?
can
PP-DIR VP
ARG1/REL2 VV NN
P NP SUP/REL2 REL2
?
regarding
NP DNP NP ?1
conduct
f1
ARGM-TMP/REL1 NP DEG NN 5y
plan
8 
hereafter
NP  REL1
ARG1/REL1 f1
?W'X
Cross-Strait
relations
u?
development
The two sides of the Taiwan Straits can plan the development of the cross-Strait relations hereafter.
Table 1: A nominalized predicate annotated with semantic roles
icate from those that are not related to the pred-
icate in question. Argument classification, which
classifies the constituents into a category that cor-
responds to one of the argument or adjunct la-
bels is a natural multi-category classification prob-
lem. Many classification techniques, SVM (Pradhan
et al, 2004b), perceptrons (Carreras and Ma`rquez,
2004a), Maximum Entropy (Xue and Palmer, 2004),
etc. have been successfully used to solve SRL prob-
lems. For our purposes here, we use a Maximum En-
tropy classifier with a tunable Gaussian prior in the
Mallet Toolkit1. The Maximum Entropy classifier
does multi-category classification and thus can be
1http://mallet.cs.umass.edu
straightforwardly applied to the problem here. The
classifier can be tuned to minimize overfitting by ad-
justing the Gaussian prior.
3.1 A three-stage architecture
Like verbal predicates, the arguments and adjuncts
of a nominalized predicate are related to the pred-
icate itself in linguistically well-understood struc-
tural configurations. As we pointed out in Section
2, most of the arguments for nominalized predicates
are inside the NP headed by the predicate unless the
NP is the object of a support verb, in which case its
arguments can occur outside the NP. Typically the
subject of the support verb is also an argument of the
nominalized predicate, as illustrated in Example 1.
433
The majority of the constituents are not related to the
predicate in question, especially since the sentences
in the treebank tend to be very long. This is clearly
a lingustic observation that can be exploited for the
purpose of argument detection. There are two com-
mon approaches to argument detection in the SRL
literature. One is to apply a binary classifier directly
to all the constituents in the parse tree to separate
the arguments from non-arguments, and let the ma-
chine learning algorithm do the work. This can be
done with high accuracy when the machine-learning
algorithm is powerful and is provided with appro-
priate features (Hacioglu et al, 2003; Pradhan et
al., 2004b). The alternative approach is to combine
heuristic and machine-learning approaches (Xue and
Palmer, 2004). Some negative samples are first fil-
tered out with heuristics that exploit the syntactic
structures represented in a parse tree before a binary
classifier is applied to further separate the positive
samples from the negative samples. It turns out the
heuristics that are first proposed in Xue and Palmer
(2004) to prune out non-arguments for verbal pred-
icates can be easily adapted to detect arguments for
the nominalized predicates as well, so in our exper-
iments we adopt the latter approach. The algorithm
starts from the predicate that anchors the annotation,
and first collects all the sisters of this predicate. It
then iteratively moves one level up to the parent of
the current node to collect its sisters till it reaches the
appropriate top-level node. At each level, the sys-
tem has a procedure to determine whether that level
is a coordination structure or a modification struc-
ture. The system only considers a constituent to be
a potential candidate if it is an adjunct to the current
node. Punctuation marks at all levels are skipped.
After this initial procedure, a binary classifier is ap-
plied to distinguish the positive samples from the
negative samples. A lower threshold is used for pos-
itive samples than negative samples to maximize the
recall so that we can pass along as many positive
samples as possible to the next stage, which is the
multi-category classification.
3.2 Features
SRL differs from low-level NLP tasks such as POS
tagging in that it has a fairly large feature space and
as a result linguistic knowledge is crucial in design-
ing effective features for this task. A wide range of
features have been shown to be useful in previous
work on semantic role labeling for verbal predicates
(Gildea and Jurafsky, 2002; Pradhan et al, 2004b;
Xue and Palmer, 2004) and our experiments show
most of them are also effective for SRL of nominal-
ized predicates. The features for our multicategory
classifier are listed below:
? Predicate: The nominalized predicate itself.
? Position: The position is defined in relation to
the predicate and the values are before and af-
ter. Since most of the arguments for nominal-
ized predicates in Chinese are before the predi-
cates, this feature is not as effective as when it
is used for verbal predicates.
? path: The path between the constituent being
classified and the predicate.
? path + dominating verb. The path feature com-
bined with the dominating verb. This feature is
only invoked when there is an intervening dom-
inating verb between the constituent being clas-
sified and the predicate. It is used to capture
the observation that only a closed set of verbs
can be support verbs for nominalized predicates
and they are good indicators of whether or not
the constituent is an argument of this predicate
and the semantic role of the argument.
? Head word and its part of speech: The head
word and its part-of-speech have proved to be
a good indicator of the semantic role label of
a constituent for verbal predicates in previous
work. It proves to be a good feature for nominal
predicates as well.
? Phrase type: The syntactic category of the con-
stituent being classified.
? First and last word of the constituent being
classified
? sisterhood with predicate: A binary feature that
indicates whether the constituent being classi-
fied is a sister to the nominalized predicate.
? Combination features: predicate-head word
combination, predicate-phrase type combina-
tion.
434
? class features. Features that replace the pred-
icate with its class. The class features are in-
duced from frame files through a procedure first
introduced in (Xue and Palmer, 2005).
Not all the features used for multicategory clas-
sification are equally effective for binary classifica-
tion, which only determines whether or not a con-
stituent is an argument or adjunct to the nominal-
ized predicate. Therefore, the features for the binary
classifier are a subset of the features used for multi-
category classification. These are path, path plus
dominating verb, head word and its part-of-speech
and sisterhood.
4 Experiments
4.1 Data
Our system is trained and tested on a pre-release
version of the Chinese Nombank. This version of
the Chinese Nombank consists of standoff annota-
tion on the first 760 articles (chtb_001.fid to
chtb_931.fid) of the Penn Chinese Treebank2.
This chunk of data has 250K words and 10,364 sen-
tences. It has 1,227 nominalized predicate types and
10,497 nominalized predicate instances. In com-
parison, there are 4,854 verb predicate types and
37,183 verb predicate instances in the same chunk
of data. By instance, the size of the Nombank is be-
tween a quarter and one third of the Chinese Propo-
sition Bank. Following the convention of the se-
mantic role labeling experiments in previous work,
we divide the training and test data by the num-
ber of articles, not by the predicate instances. This
pretty much guarantees that there will be unseen
predicates in the test data. For all our experiments,
688 files are used as training data and the other
72 files (chtb_001.fid to chtb_040.fid and
chtb_900.fid to chtb_931.fid) are held out
as test data. The test data is selected from the
double-annotated files in the Chinese Treebank and
the complete list of double-annotated files can be
found in the documentation for the Chinese Tree-
bank 5.1. Our parser is trained and tested with the
same data partition as our semantic role labeling sys-
tem.
2The most current version (CTB5.1) of the Penn Chinese
Treebank has 507K words, 825K Chinese characters, 18,716
sentences and 890 articles.
4.2 Semantic role tagging with hand-crafted
parses
In this section we present experimental results us-
ing Gold Standard parses in the Chinese Treebank
as input. To be used in real-world natural language
applications, a semantic role tagger has to use au-
tomatically produced constituent boundaries either
from a parser or by some other means, but experi-
ments with Gold Standard input will help us evaluate
how much of a challenge it is to map a syntactic rep-
resentation to a semantic representation, which may
very well vary from language to language. There
are two experimental setups. In the first experiment,
we assume that the constituents that are arguments
or adjuncts are known. We only need to assign the
correct argument or adjunct labels. In the second
experiment, we assume that all the constituents in a
parse tree are possible arguments. The system first
filters out consituents that are highly unlikely to be
an argument for the predicate, using the heuristics
described in Section 3. A binary classifier is then
applied to the remaining constituents to do further
separation. Finally the multicategory classifier is
applied to the candidates that the binary classifier
passes along. The results of these two experiments
are presented in Table 2.
experiments all corep (%) r(%) f(%) f(%)
constituents known n/a n/a 86.6 86.9
constituents unknown 69.7 73.7 71.6 72.0
Table 2: Results for hand-crafted parses
Compared with the 93.9% reported by Xue and
Palmer (2005) for verbal predicates on the same
data, the 86.9% the system achieved when the con-
situents are given is considerably lower, suggest-
ing that SRL for nominalized predicates is a much
more challenging task. The difference between the
SRL accuracy for verbal and nominalized predicates
is even greater when the constituents are not given
and the system has to identify the arguments to be
classified. Xue and Palmer reported an f-score of
91.4% for verbal predicates under similar experi-
mental conditions, in contrast with the 71.6% our
system achieved for nominalized predicates. Care-
ful error analysis shows that one important cause for
435
this degradation in performance is the fact that there
is insufficient training data for the system to reliably
separate support verbs from other verbs and deter-
mine whether the constituents outside the NP headed
by the nominalized predicate are related to the pred-
icate or not.
4.3 Using automatic parses
We also conducted an experiment that assumes a
more realistic scenario in which the input is raw un-
segmented text. We use a fully automatic parser
that integrates segmentation, POS tagging and pars-
ing. Our parser is similar to (Luo, 2003) and is
trained and tested on the same data partition as the
semantic role labeling system. Tested on the held-
out test data, the labeled precision and recall are
83.06% and 80.15% respectively for all sentences.
The results are comparable with those reported in
Luo (Luo, 2003), but they cannot be directly com-
pared with most of the results reported in the litera-
ture, where correct segmentation is assumed. In ad-
dition, in order to account for the differences in seg-
mentation, each character has to be treated as a leaf
of the parse tree. This is in contrast with word-based
parsers where words are terminals. Since semantic
role tagging is performed on the output of the parser,
only constituents in the parse tree are candidates. If
there is no constituent in the parse tree that shares
the same text span with an argument in the manual
annotation, the system cannot possibly get a correct
annotation. In other words, the best the system can
do is to correctly label all arguments that have a con-
stituent with the same text span in the parse tree.
all core
p (%) r(%) f(%) f(%)
49.7 53.1 51.3 48.3
Table 3: Results for automatic parses
The results show a similar performance degrada-
tion compared with the results reported for verbs on
the same data in previous work, which is not unex-
pected. Xue and Palmer (2005) reported an f-score
of 61.3% when a parser is used to preprocess the
data.
4.4 Using verb data to improve noun SRL
accuracy
Since verbs and their nominalized counterparts are
generally considered to share the same argument
structure and in fact the Chinese Nombank is an-
notated based on the same set of lexical guide-
lines (called frame files) as the Chinese PropBank,
it seems reasonable to expect that adding the verb
data to the training set will improve the SRL accu-
racy of the nominal predicates, especially when the
training set is relatively small. Given that verbs and
their nominalized counterpart share the same mor-
phological form in Chinese, adding the verb data to
the training set is particularly straightforward. In
our experiments, we extracted verb instances from
the CPB that have nominalized forms in the portion
of the Chinese Treebank on which our SRL exper-
iments are performed and added them to the train-
ing set. Our experiments show, however, that sim-
ply adding the verb data to the training set and in-
discriminately extracting the same features from the
verb and noun instances will hurt the overall perfor-
mance instead of improving it. This result is hardly
surprising upon closer examination: the values of
certain features are vastly different for verbal and
nominal predicates. Most notably, the path from the
predicate to the constituent being classified, an im-
portant feature for semantic role labeling systems,
differ greatly from nominal and verbal predicates.
When they are thrown in the same training data mix,
they effectively create noise and neutralize the dis-
criminative effect of this feature. Other features,
such as the head words and their POS tags, are the
same and adding these features does indeed improve
the SRL accuracy of nominal predicates, although
the improvement is not statistically significant.
4.5 Reranking
In a recent paper on the SRL on verbal predicates
for English, (Toutanova et al, 2005) pointed out that
one potential flaw in a SRL system where each ar-
gument is considered on its own is that it does not
take advantage of the fact that the arguments (not the
adjuncts) of a predicate are subject to the hard con-
straint that they do not have the same label3. They
3For certain symmetrical predicates, arguments can have the
same label, although these cases are rare.
436
show that by performing joint learning of all the ar-
guments in the same proposition (for the same predi-
cate), the SRL accuracy is improved. To test the effi-
cacy of joint-learning for nominalized predicates in
Chinese, we conducted a similar experiment, using
a perceptron reranker described in Shen and Joshi
(2004). Arguments and adjuncts of the same predi-
cate instance (proposition) are chained together with
their joint probability being the product of the indi-
vidual arguments and the top K propositions are se-
lected as the reranking candidates. When the argu-
ments are given and the input is hand-crafted gold-
standard parses in the treebank, selecting the top 10
propositions yields an oracle score of 97%. This ini-
tial promise does not pan out, however. Performing
reranking on the top 10 propositions did not lead
to significant improvement, using the five feature
classes described in (Haghighi et al, 2005). These
are features that are hard to implement for individual
arguments: core argument label sequence, flattened
core argument label sequence, core argument labels
and phrase type sequence, repeated core argument
labels with phrase types, repeated core argument la-
bels with phrase types and adjacency information.
We speculate that the lack of improvement is due
to the fact that the constraint that core (numbered)
arguments should not have the same semantic role
label for Chinese nominalized predicates is not as
rigid as it is for English verbs. However further error
analysis is needed to substantiate this speculation.
5 Related Work
Compared with large body of work on the SRL
of verbal predicates, there has been relatively lit-
tle work done in analyzing the predicate-argument
structure of nominalized predicates. There are even
less work done for the nominalized predicates for
Chinese. (Hull and Comez, 1996) implemented a
rule-based system for identifying the arguments for
nominal predicates and (Lapata, 2002) has a system
that interprets the relation between the head of noun
compound and its head, but no meaningful compar-
ison can be made between our work and theirs. Per-
haps the closest work to that of ours is that of (Prad-
han et al, 2004a), where they reported preliminary
work for analyzing the predicate-argument structure
of Chinese nominalizations, using a small data set of
630 proposition for 22 nominalizations taken from
the Chinese Treebank. Since different data sets are
used, the results cannot be meaningfully compared.
The results reported here for nominalized pred-
icates are consistent with what Xue and Palmer
(2005) reported for the SRL of Chinese verbs with
regard to the role of the parser in their semantic
role labeling system: there is a substantial perfor-
mance drop when the automatic parser is used. At
present, improvement in Chinese parsing is hindered
by insufficient training material. Although the Chi-
nese Treebank has a decent size of 500K words, it
is evenly divided into two portions of very differ-
ent sources, Xinhua newswire from mainland China
and Sinorama magazines from Taiwan. Due to their
very different styles, training on one portion of the
data does not help or even hurt the parsing accuracy
of the other portion. The lack of sufficient train-
ing material is compounded by inherent properties
of the Chinese language that makes Chinese pars-
ing particularly difficult. Chinese segmentation is
a much more difficult problem than tokenization of
English text and Chinese words do not have mor-
phological clues that can help parsing decisions. We
believe further improvement in SRL accuracy will
be to a large extent contingent on the parsing accu-
racy, which requires more training material.
6 Conclusion and future work
We reported first results on the semantic role label-
ing of nominalized predicates in Chinese, using a
sizable annotated corpus, the Chinese Nombank, as
training and test data. Compared with that of ver-
bal predicates, SRL of nominalized predicates gen-
erally presents a more challenging problem, for all
experimental conditions. While the smaller train-
ing set compared with that of verbal predicates may
provide partial explanation for the degradation in
performance, we believe another important reason
is that the arguments for nominalized predicates do
not occupy prominent syntactic positions such as the
subject and object, as arguments of verbal predicates
often do. As a result, the syntactic structure repre-
sented in the parse tree does not provide as much of a
clue for their detection and classification. However,
this makes SRL of nominalized predicates a more
pressing issue to solve, as they represent a substan-
437
tial proportion of the predicates in the corpus. Our
results also show that the k-best propositions pro-
duced by the local classifier have a very high ora-
cle score, which perhaps indicates a promising path
that deserves further exploration, based on careful
analysis of the errors. We intend to continue to ex-
periment with new features and parameters for the
reranking algorithm.
7 Acknowledgement
I would like to thank Martha Palmer for her unwa-
vering support for this line of research. This work
is funded by the NSF ITR via grant 130-1303-4-
541984-XXXX-2000-1070.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Pro-
ceedings of COLING/ACL, pages 86?90, Montreal,
Canada.
Xavier Carreras and Llu??s Ma`rquez. 2004a. Hierarchi-
cal Recognition of Propositional Arguments with Per-
ceptrons. In Proceedings of the Eighth Conference on
Natural Language Learning, Boston, Massachusetts.
Xavier Carreras and Llu??s Ma`rquez. 2004b. Introduction
to the CoNLL-2004 Shared Task: Semantic Role La-
beling. In Proceedings of the Eighth Conference on
Natural Language Learning, Boston, Massachusetts.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 Shared Task: Semantic Role La-
beling. In Proceedings of the Nineth Conference on
Natural Language Learning, Ann Arbor, Michigan.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing for semantic roles. Computational Linguistics,
28(3):245?288.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James H.
Martin, and Daniel Jurafsky. 2003. Shallow Seman-
tic Parsing Using Support Vector Machines. Technical
Report CSLR-2003-1, Center for Spoken Language
Research at the University of Colorado.
Aria Haghighi, Kristina Toutanova, and Christopher
Manning. 2005. A Joint Model for Semantic Role
Labeling. In Proceedings of the Nineth Conference on
Natural Language Learning, Ann Arbor, Michigan.
Richard D. Hull and Fernando Comez. 1996. Semantic
interpretation of nominalizations. In The AAAI Con-
ference, pages 1062?1068, Oregon.
Peter Koomen, Vasin Punyakanok, Dan Roth, and Wen
tau Yih. 2005. Generalized Inference with Multiple
Semantic Role Labeling Systems. In Proceedings of
the Nineth Conference on Natural Language Learning,
Ann Arbor, Michigan.
Maria Lapata. 2002. The disambiguation of nominaliza-
tions. Computational Linguistics, 28(3):357?388.
Xiaoqiang Luo. 2003. A Maximum Entropy Chinese
Character-Based Parser. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2003), Sapporo, Japan.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank Project: An Interim Report. In Proceedings of
the NAACL/HLT Workshop on Frontiers in Corpus An-
notation, Boston, Massachusetts.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: An Annotated Corpus of Se-
mantic Roles. Computational Linguistics, 31(1).
Sameer Pradhan, Honglin Sun, Wayne Ward, James H.
Martin, and Daniel Jurafsky. 2004a. Parsing Argu-
ments of Nominalizations in English and Chinese. In
Proceedings of NAACL-HLT 2004, Boston, Mass.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H.
Martin, and Daniel Jurafsky. 2004b. Shallow Seman-
tic Parsing Using Support Vector Machines. In Pro-
ceedings of NAACL-HLT 2004, Boston, Mass.
Libin Shen and Aravind K. Joshi. 2004. Flexible Margin
Selection for Reranking with Full Pairwise Samples.
In Proceedings of IJCNLP-2004, pages 446?455.
Honglin Sun and Daniel Jurafsky. 2004. Shallow Se-
mantic Parsing of Chinese. In Proceedings of NAACL
2004, Boston, USA.
Kristina Toutanova, Aria Haghighi, and Christopher
Manning. 2005. Joint Learning Improves Semantic
Role Labeling. In Proceedings of ACL-2005.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for Semantic Role Labeling. In Proceedings
of 2004 Conference on Empirical Methods in Natural
Language Processing, Barcelona, Spain.
Nianwen Xue and Martha Palmer. 2005. Automatic Se-
mantic Role Labeling for Chinese verbs. In Proceed-
ings of the Nineteenth International Joint Conference
on Artificial Intelligence, Edinburgh, Scotland.
Nianwen Xue. 2006. Annotating the predicate-argument
structure of Chinese nominalizations. In Proceedings
of the fifth international conference on Language Re-
sources and Evaluation, Genoa, Italy.
438
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 921?928,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Aligning Features with Sense Distinction Dimensions  
1 Nianwen Xue,  2 Jinying Chen,  3 Martha Palmer 
1CSLR and 3Department of Linguistics 
University of Colorado 
Boulder, CO, 80309 
{Nianwen.Xue,Martha.Palmer}@colorado.edu 
2 Department of Computer and Information Science 
University of Pennsylvania 
Philadelphia, PA, 19104 
jinying@cis.upenn.edu 
 
Abstract 
In this paper we present word sense 
disambiguation (WSD) experiments on 
ten highly polysemous verbs in Chinese, 
where significant performance 
improvements are achieved using rich 
linguistic features. Our system performs 
significantly better, and in some cases 
substantially better, than the baseline on 
all ten verbs. Our results also 
demonstrate that features extracted from 
the output of an automatic Chinese 
semantic role labeling system in general 
benefited the WSD system, even though 
the amount of improvement was not 
consistent across the verbs. For a few 
verbs, semantic role information actually 
hurt WSD performance. The 
inconsistency of feature performance is a 
general characteristic of the WSD task, as 
has been observed by others. We argue 
that this result can be explained by the 
fact that word senses are partitioned 
along different dimensions for different 
verbs and the features therefore need to 
be tailored to particular verbs in order to 
achieve adequate accuracy on verb sense 
disambiguation. 
1 Introduction 
Word sense disambiguation, the determination of 
the correct sense of a polysemous word from a 
number of possible senses based on the context 
in which it occurs, is a continuing obstacle to 
high performance natural language processing 
applications. There are several well-documented 
factors that make accurate WSD particularly 
challenging. The first has to do with how senses 
are defined. The English data used for the 
SENSEVAL exercises, arguably the most widely 
used data to train and test WSD systems, are 
annotated based on very fine-grained distinctions 
defined in WordNet (Fellbaum, 1998), with 
human inter-annotator agreement at a little over 
seventy percent and the top-ranked systems? 
performances falling between 60%~70%  
(Palmer, et al, 2001; Mihalcea et al, 2004). The 
second source of difficulty for accurate WSD 
comes from how senses are distributed. It is 
often the case that a polysemous word has a 
dominant sense or several dominant senses that 
occur with high frequency and not enough 
instances can be found for its low frequency 
senses in the currently publicly available data. 
There are on-going efforts to address these 
issues. For example, the sense annotation 
component of the OntoNotes project (Hovy, et 
al., 2006) attempts to create a large-scale coarse-
grained sense-annotated corpus with senses 
defined based on explicit linguistic criteria. 
These problems will be alleviated when 
resources like this are available to the general 
NLP community. There have already been 
experiments that show such coarse-grained 
senses lead to substantial improvement in system 
performance (Palmer et al 2006).  
The goal of our experiments is to explore the 
implications of a related and yet separate 
problem, specifically the extent to which the 
linguistic criteria used to define senses are 
related to what features need to be used in 
machine-learning systems. There are already 
published results that show WSD for different 
syntactic categories may need different types of 
features. For example, Yarowsky and Florian 
(2002), in their experiments on SENSEVAL2 
English data, showed that sense distinctions of 
verbs relied more on linguistically motivated 
features than other parts-of-speech. In this paper, 
921
we will go one step further and show that even 
for words of the same syntactic category senses 
are often defined along different dimensions 
based on different criteria. One direct implication 
of this observation for supervised machine-
learning approaches to WSD is that the features 
have to be customized for different word 
categories, or even for different words of the 
same category. This supports previous arguments 
for word-specific feature design and parametric 
modeling for WSD tasks (Chen and Palmer, 
2005; Hoste et al 2002). We report experiments 
on ten highly polysemous Chinese verbs and 
show that features are not uniformly useful for 
all words.  
The rest of the paper is organized as follows. 
In Section 2, we describe our WSD system, 
focusing on the features we used. We also briefly 
compare the features we use for Chinese with 
those used in a similar English WSD system. In 
Section 3, we present our experimental results 
and show that although rich linguistic features 
and features derived from a Chinese Semantic 
Role Labeling improve the WSD accuracy, the 
improvement is not uniform across all verbs. We 
show that this lack of consistency is due to the 
different dimensions along which the features are 
defined. In Section 4, we discuss related work. 
Finally Section 5 concludes this paper and 
describes future directions.  
2 WSD System for Chinese Verbs 
Our WSD system uses a smoothed maximum 
entropy (MaxEnt) model with a Gaussian prior 
(McCallum, 2002) for learning Chinese verb 
senses. The primary reason is that the MaxEnt 
model provides a natural way for combining 
different features without the assumption of 
feature independence. Furthermore, smoothing 
the MaxEnt model with a Gaussian prior is better 
than other smoothing methods at alleviating the 
overfitting problem caused by low frequency 
features (Chen et al, 1999). This model has been 
applied successfully for English WSD (Dang, 
2004; Chen and Palmer, 2005). 
The features used by our Chinese WSD 
system include: 
Collocation Features 
- Previous and next word  (relative to the target 
verb), w-1 and w1 and their parts-of-speech p-1 
and p1 
 
Syntactic Features 
- Whether the target verb takes a direct object 
(i.e., in a transitive use)  
- Whether the verb takes a sentential 
complement 
- Whether the verb, if it consists of a single 
character, occurs at the last position of a 
compound verb 
 
Semantic Features 
- The semantic role information about the verbs 
- The semantic categories for the verb?s NP 
arguments from a general Chinese noun 
Taxonomy 
  
All of these features require some level of 
preprocessing of the Chinese raw text, which 
comes without word boundaries. To extract the 
collocation features the raw text needs to be 
segmented and POS-tagged; to extract the 
syntactic and semantic features, the Chinese text 
needs to be parsed. We use an integrated parser 
that does segmentation, POS-tagging and parsing 
in one step. Since part of the sense-tagged data 
comes from the Chinese Treebank that the parser 
is trained on, we divide the Chinese Treebank 
into nine equal-sized portions and parse each 
portion with a parsing model trained on the other 
eight portions so that the parser has not seen any 
of the data it parses. The data that is not from the 
Chinese Treebank is parsed with a parsing model 
trained on the entire Chinese Treebank. The 
parser produces a segmented, POS-tagged and 
parsed version of the same text to facilitate the 
extraction of the different types of features. The 
extraction of the semantic role labels as features 
requires the use of a semantic role tagger, which 
we describe in greater detail in Section 2.2. 
In addition to using the semantic role labeling 
information, we also extract another type of 
semantic features from the verb?s NP arguments. 
These features are top-level semantic categories 
from a three-level general taxonomy for Chinese 
nouns, which was created semi-automatically 
based on two Chinese semantic dictionaries 
(Chen and Palmer, 2004). 
2.1 A Comparison with  Our English WSD 
System 
Similar to our English WSD system, which 
achieved the best published results on 
SENSEVAL2 English verbs for both fine-
grained and coarse-grained senses (Chen and 
Palmer, 2005), our Chinese WSD system uses 
the same smoothed MaxEnt machine learning 
model and linguistically motivated features for 
Chinese verb sense disambiguation. However, 
the features used in the two systems differ 
922
somewhat  due to the different properties of  the 
two languages . 
For example, our English system uses the 
inflected form and the part-of-speech tag of the 
target verb as feature. For Chinese we no longer 
use such features since Chinese words, unlike 
English ones, do not contain morphology that 
marks tense. 
The collocation features used by our English 
system include bi-grams and tri-grams of the 
words that occur within two positions before or 
after the target verb and their part-of-speech tags. 
In contrast, our Chinese system extracts 
collocation features from a narrower, three-word 
window, with one word immediately before and 
after the target verb. This decision was made 
based on two observations about the Chinese 
language. First, certain single-character Chinese 
verbs, such as the verbs ??|chu?, ??|kai? and 
??|cheng? in our experiments, often form a 
compound with  a verb to its immediate left. That 
verb is often a good indicator of the sense of this 
verb. An example is given in (1):   
 
(1) ??    ?       ??   ? 
       Liaoning  already     show      completion         
      
    ???                    ??          ?? ? 
     multidimensional   development        trend  
 
 ?Liaoning Province has shown the trend of 
multidimensional development.? 
 
Being the last  word of a verb compound is a 
strong indicator for Sense 8 of the verb ??
|chu1? (used after a verb to indicate direction or 
aspect), as in ???|cheng2xian4?|chu1?. 
Second, unlike English common nouns that 
often require determiners such as the, a or an, 
Chinese common nouns can stand alone. 
Therefore, the direct object of a verb often 
occurs right after the verb in Chinese, as shown 
in (2). 
 
?2???         ??         ??      ??           ?  
    mobilize     people      tighten   waistband  collect 
 
    ?   ?  ?? (direct object)? 
    funds   build highway 
 
?Mobilize people to tighten their waistbands (i.e., 
save money) in order to collect funds to build 
highways.? 
  
Based on these observations, we use words 
surrounding  the target verb and their part-of-
speech tags  as collocation features. A further 
investigation on the different sizes of the context 
window (3,5,7,9,11) showed that  increasing the 
window size decreased our system?s accuracy.   
2.2 Features Based on Automatic Semantic 
Role Tagging 
In a recent paper on the WSD of English verbs, 
Dang and Palmer (2005) showed that semantic 
role information significantly improves the WSD 
accuracy of English verbs for both fine-grained 
and coarse-grained senses. However, this result 
assumes the human annotation of the Penn 
English Propbank (Palmer et al 2005). It seems 
worthwhile to investigate whether the semantic 
role information produced by a fully automatic 
Semantic Role tagger can improve the WSD 
accuracy on verbs, and test the hypothesis that 
the senses of a verb  have a high correlation to 
the arguments it takes. To that end, we assigned 
semantic role labels to the arguments  of the 
target verb with a fully automatic semantic role 
tagger (Xue and Palmer, 2005) trained on the 
Chinese Propbank (CPB) (Xue and Palmer, 
2003), a corpus annotated with semantic role 
labels that are similiar in style to the Penn 
English Propbank. In this annotation, core 
arguments such as agent or theme are labeled 
with numbered arguments such as Arg0 and Arg1, 
up to Arg5 while adjunct-like elements are 
assigned functional tags such as TMP (for 
temporal), MNR, prefixed by ArgM. The 
Semantic Role tagger takes as input syntactic 
parses produced by the parser described above as 
input and produces a list of arguments for each 
of  the sense-tagged target verbs and assigns 
argument labels to them. Features are extracted 
from both the core arguments and adjuncts of the 
target verb. In addition to providing the sematnic 
role labels (e.g., Arg0 and Arg1) of the extracted 
core arguments, the Semantic Role tagger also 
provides Hownet (Dong and Dong, 1991) 
semantic categories associated with these 
arguments. (3) shows the arguments for the 
target verb ???  identified by the Semantic Role 
tagger: 
 
(3)  [ArgM-MNR ??    ?    ?     ?      ?] ,    
                  through  three   year   hard   work,   
  
 [arg0 ?        ?]    [rel ?]    ?             
            whole   county   dig         finish   
 
  [Arg1 ?      ??]      ?       ? ? 
        deep    well         three  classifier  
923
 
?The whole county finished digging three deep 
wells through 3 years of hard work.? 
 
Based on the output of the Semantic Role tagger 
and the Chinese noun taxonomy (as described  in 
Section 2.1), the following features are extracted: 
 
SRL+lex               SRL+HowNet     SRL+Taxonomy 
ARG1-??  ARG1-??         ARG1_location  
ARG0-? ARG0-??         ARG0_location 
ARGM|MNR-?? ARGM|MNR-?? ARGM|MNR 
 
In this example, semantic role related features 
include: (1) the head word of the core arguments 
(ARG1-?? and ARG0-?) and the adjunct 
(ARGM|MNR-??); (2) the HowNet semantic 
category for the head word (ARG1-??, ARG0-
??, ARGM|MNR-??); (3) the semantic role 
label of the adjunct (ARGM|MNR); and (4) the 
top level semantic category from the taxonomy 
of Chinese nouns for the head word of the NP 
arguments (ARG1_location and ARG0_location).       
3 Experimental Results 
The data we used for our experiments are 
developed as part of the OntoNotes project 
(Hovy et al, 2006) and they come from a variety 
of sources. Part of the data is from the Chinese 
Treebank (Xue et al 2005), which has a 
combination of Xinhua news and Sinorama 
News Magazine. Since some verbs have an 
insufficient number of instances for any 
meaningful experiments, we also annotated 
portions of the People?s Daily corpus, developed 
by Peking University. We chose not to use the 
Chinese WSD dataset used in Senseval 3 1 
because we are mainly interested in investigating 
how the features used in WSD are related to the 
criteria used to define the senses of Chinese 
verbs. The Chinese Senseval dataset includes 
both nouns and verbs. In addition, the criteria 
used to define their senses are not made explicit 
and therefore are not clear to us.  
Table 1 summarizes the corpus statistics and 
the experimental results for the 10 highly 
polysemous Chinese verbs used in our 
experiments. The results were obtained by using 
5-fold cross validation. The top five verbs are 
verbs that were identified as difficult verbs in 
Dang et als (2002) experiments. The first three 
columns show the verbs (and their pinyin), the 
number of instances and the number of senses for 
                                                 
1 http://www.senseval.org/senseval3 
each verb in the data. The fourth column shows 
the sense entropy for each verb in its test data, as 
calculated in Equation 1. 
)(log)(
1
i
n
i
i sensePsenseP?
=
?            (1) 
Where n is the number of senses of a verb in our 
data; )( isenseP is the probability of the ith sense 
of the verb, which is estimated based on the 
frequency count of the verb?s senses in the data. 
Sense entropy generally reflects the frequency 
distribution of senses in the corpus. A verb with 
an evenly distributed sense distribution tends to 
have a high entropy value. However, a verb can 
also have a high sense entropy simply because it 
is highly polysemous (say, has 20 or more senses) 
even though the sense distribution may be 
skewed, with one or two dominant senses. To 
separate the effects of the number of senses, we 
also use a normalized sense entropy metric (the 
sixth column in Table 1), as calculated in 
Equation 2.  
)1(log1
)(log)(
1
1
n
P
n
sensePsenseP
n
i
i
n
i
i
?
?
=
=
?
?
        (2) 
Here a large sense number n corresponds to a 
high value for the normalization factor 
)1(log1
1 n
P
n
n
i
?
=
? . Therefore, normalized sense 
entropy can indicate sense frequency distribution 
more precisely than sense entropy.  
Table 1 (Columns 7 to 10) also shows the 
experimental results. As we can see, on average, 
our system achieved about 19% improvement 
(absolute gain) in accuracy compared to the most 
frequent sense baseline. Its performance is 
consistently better than the baseline for all 10 
verbs. 
3.1 Corpus Statistics and Disambiguation 
Accuracy 
The data in Table 1 shows that verbs with a high 
normalized sense entropy have the low frequency 
baselines. Furthermore, this relation is stronger 
than that between un-normalized sense entropy 
and the baseline. However, sense entropy is a 
better predictor for system performance than 
normalized sense entropy. The reason is intuitive: 
unlike the baseline, the automatic WSD system, 
trained on the training data, does not only rely on 
sense frequency information to predict senses.  
924
  
# of 
instance 
# of 
sense 
sense 
entropy
norm. 
sense 
entropy baseline all feat all-SRL 
?|chu 271 11 1.12 0.47 74.54 79.70 78.59 
??|huifu 113 3 0.93 0.84 50.44 69.91 72.57 
?|jian 167 7 1.01 0.52 72.46 82.63 82.03 
?|xiang 231 6 1.00 0.56 65.80 76.19 77.49 
?|yao 254 9 1.56 0.71 33.46 46.46 49.21 
?|cheng 161 8 1.38 0.67 43.48 73.29 72.67 
?|da 313 21 2.29 0.75 20.77 45.05 32.59 
?|kai 382 18 2.31 0.80 19.37 50.00 39.27 
??|tongguo 384 4 0.97 0.70 55.73 81.51 79.17 
??|fazhan 1141 7 0.88 0.45 74.76 79.58 77.56 
average   9.4     51.08 70.18 67.13 
total 3417             
The number of senses has a direct impact on how 
many training instances exist for each verb sense. 
As a consequence, it is more difficult for the 
system to make good generalizations from the 
limited training data that is available for highly 
polysemous verbs. Therefore, sense entropy, 
which is based on both sense frequency 
distribution and polysemy is more appropriate 
for predicting system accuracy. A related 
observation is that the system gain (compared 
with the baseline) is bigger for verbs with a high 
normalized sense entropy, such as ???|huifu?, 
??|da?, ??|kai?, and ???|tongguo?, than for 
other verbs; and the system gain is very small for 
verbs with low normalized sense entropy and a 
relatively large number of senses, such as ??
|chu? and ???|fazhan?, since they already have 
high baselines. 
3.2 The Effect of Semantic Role Features 
When Semantic Role information is used in 
features, the system?s performance on average 
improves 3.05%, from 67.13% to 70.18% 
compared with when the features derived from 
the Semantic Role information is not used.  If we 
look at the system?s performance on individual 
verbs, the results show that adding Semantic 
Role information as features improves the 
accuracy of 7 of the 10 verbs. For the remaining 
3 verbs, adding semantic role information 
actually hurts the system?s performance. We 
believe this apparent inconsistency can be 
explained by looking at how senses are defined 
for the different verbs.  The two verbs that 
present the most challenge to the system, are 
??|da? and  ??|yao? While Semantic Role 
features substantially improve the accuracy of 
??|da?, they actually hurt the accuracy of ??
|yao?. For ??|yao?, its three most frequent 
senses account for 86% of its total instances (232 
out of 270) and they are the ?intend to?, ?must, 
should? and ?need? senses: 
 
(4) Three most frequent senses of ??|yao? 
(a)  ??        ??       ?       ??? ?? ? 
    two sides  indicate  intend  further   cooperation                  
 
?The two sides indicated that they intended to step up 
their cooperation.? 
 
(b) ?     ?  ? ? ??  ?            ?? ? 
      road  very  slippery,     everybody  should  careful     
 
?The road is slippery. Everybody should be careful.? 
 
(c) ??  ?                   ?       ?       ? 
 Suzhou Steel Works  every   year    need   depend 
      
???            ??         ?? ? 
 
 the Great Canal      transport    raw material 
 
?Suzhou Steel Works needs to depend on the Great 
Canal to transport raw material.?  
 
Two of the senses, ?must? and ?need?, are 
used as auxiliary verbs. As such, they do not take 
arguments in the same way non-auxiliary verbs 
do. For example, they do not take noun phrases 
as arguments. As a result, the Semantic Role 
tagger, which assigns argument labels to head 
words of noun phrases or clauses, cannot 
produce a meaningful argument for an auxiliary 
verb. For the ?intend to? sense, even if it is not 
Table 1 Corpus Statistics and Experimental Results for the 10 Chinese Verbs 
925
an auxiliary verb, it still does not take a noun 
phrase as an object. Instead, its object is a verb 
phrase or a clause, depending on the analysis.  
The correct head word of its argument should be 
the lower verb, which apparently is not a useful 
discriminative feature either. 
In contrast, the senses of ??|da? are generally 
defined based on their arguments. The three most 
frequent senses of ? ? |da? are ?call by 
telephone?, ?play? and ?fight? and they account 
for 40% of the ??|da? instances. Some examples 
are provided in (5) 
 
(5) Top three senses of ??|da? 
 
(a) ?      ??     ??                          ?     
       you   have     queue in long line     call    
        
?? ??            ?        ?? ?           ? 
       public   phone  DE     experience    ma    
 
?Do you have the experience of queuing in a line 
and waiting to make a call with a public phone?? 
 
(b) ??      ??     ?? ??           
a few     on duty    personnel      sit      
 
? ?            ?  ??   ? 
one      circle     play    poker  
 
?A few of the personnel on duty were sitting in a 
circle and playing poker.? 
 
(c) ?? ?   ??  ??  ?? 
mobilize    whole society  power   fight 
 
       ??   ??  ? ? 
       helping the poor crucial battle 
 
??mobilize the power of the whole society and 
fight the crucial battle of helping the poor.? 
 
The senses of ??|da? are to a large extent 
determined by its PATIENT (or Arg1) argument, 
which is generally realized in the object position. 
The Arg1 argument usually forms highly 
coherent lexical classes. For example, the Arg1 
of the ?call? sense can be ? ? ?
|dianghua/phone, ???|shouji/cellphone?, 
etc. its Arg1 argument can be ? ? ?
|langqiu/basketball?, ? ?? |qiaopai/bridge?, 
???|youxi/game?, etc for the "play" sense,  
Finally , for its sense ?fight?, the Arg1 argument 
can be ???|gongjian/crucial ?|zhan/battle?, 
??? |xiangzhang/street warfare?, ?????
youjizhan/guerilla warfare?, etc.. It?s not 
surprising that recognizing the arguments of 
??|da? is crucial in determining its sense.  
The accuracy for both verbs is still very low, 
but for very different reasons. In the case of 
??|yao4?, the challenge is identifying 
discriminative features that may not be found in 
the narrow local context. These could for 
instance include discourse features. In the case of 
??|da?, one important reason why the accuracy 
is still low is because ??|da? is highly 
polysemous and has over forty senses. Given its 
large number of senses, the majority of its senses 
do not have enough instances to train a 
reasonable model. We believe that more data will 
improve its WSD accuracy.  
There are other dimensions along which verb 
senses are defined in addition to whether or not a 
verb is an auxiliary verb and what type of 
auxiliary verb it is, and what types of arguments 
it takes. One sense of ??|chu? is a verb particle 
that indicates the direction or aspect of the main 
verb that generally immediately precedes it. In 
this case the most important feature for 
identifying this sense is the collocation feature.  
 
Our experimental results seem to lend support 
to a WSD approach where features are tailored to 
each target word, or at least each class of words, 
based on a careful analysis of the dimensions 
along which senses are defined. Automatic 
feature selection (Blum and Langley, 1997) 
could also prove useful in providing this type of 
tailoring. An issue that immediately arises is the 
feasibility of this approach. At least for Chinese, 
the task is not too daunting, as the number of 
highly polysemous verbs is small. Our estimation 
based on a 250K-word chunk of the Chinese 
Treebank and a large electronic dictionary in our 
possession shows only 6% or 384 verb types 
having four or more definitions in the dictionary.  
Even for these verbs, the majority of them are 
not difficult to disambiguate, based on work by 
Dang et al (2002). Only a small number of these 
verbs truly need customized features. 
4 Related work 
There is a large body of literature on WSD and 
here we only discuss a few that are most relevant 
to our work. Dang and Palmer (2005) also use 
predicate-argument information as features in 
their work on English verbs, but their argument 
labels are not produced by an automatic SRL 
system. Rather, their semantic role labels are 
directly extracted from a human annotated 
926
corpus, the English Proposition Bank (Palmer et 
al, 2005), citing the inadequate accuracy of 
automatic semantic role labeling systems. In 
contrast, we used a fully antomated SRL system 
trained on the Chinese Propbank. Nevertheless, 
their results show, as ours do, that the use of 
semantic role labels as features improves the 
WSD accuracy of verbs.  
There are relatively few attempts to use 
linguistically motivated features for Chinese 
word sense disambiguation. Niu et al(2004) 
applied a Naive Bayesian model to Chinese 
WSD and experimented with different window 
sizes for extracting local and topical features and 
different types of local features (e.g., bigram 
templates, local words with position or parts-of-
speech information). One basic finding of their 
experiments is that simply increasing the window 
size for extracting local features or enriching the 
set of local features does not improve 
disambiguation performance. This is consistent 
with our usage of a small size window for 
extracting bigram collocation features. Li et al 
(2005) used sense-tagged true bigram 
collocations 2  as features. These features were 
obtained from a collocation extraction system 
that used lexical co-occurrence statistics to 
extract candidate collocations and then selected 
true collocations by using syntactic dependencies 
(Xu et al, 2003). In their experiments on 
Chinese nouns and verbs extracted from the 
People?s Daily News and the SENSEVAL3 data 
set,  the Naive Bayesian classifier using true 
collocation features generally performed better 
than that using simple bigram collocation 
features (i.e., bigram co-occurence features). It is 
worth noting that the true collocations overlap to 
a large degree with rich syntactic information 
used here such as the subject and direct object of 
a target verb. Therefore, their experiments show 
evidence that rich linguistic information benefits 
WSD on Chinese, consistent with our results. 
Our work is more closely related to the work 
of Dang et al(2002), who conducted 
experiments on 28 verbs and achieved an 
accuracy of 94.2%. However the high accuarcy is 
largely due to  the fact that their verbs are 
randomly chosen from the Chinese Treebank and 
some of them are not even polysemous (having a 
single sense). Extracting features from the gold 
                                                 
2 In their definition, a collocation is a recurrent and 
conventional fixed expression of words that holds 
syntactic and semantic relations. 
 
standard parses also contributed to the high 
accuracy, although not by much. For 5 of their 28 
verbs, their initial experimental results did not 
break the most frequent sense baseline. They 
annotated additional data on those five verbs and 
their system trained on this new data did 
outperfom the baseline. However, they 
concluded that the contribution of linguistic 
motivated features, such as features extracted 
from a syntactic parse, is insignificant, a finding 
they attributed to unique properties of Chinese 
given that the same syntactic features 
significantly improves the WSD accuracy. Our 
experimental results show that this conclusion is 
premature, without a detailed analysis of the 
senses for the individual verbs.  
5 Conclusion and future work 
We presented experiments with ten highly 
polysemous Chinese verbs and showed that a 
previous conclusion that rich linguistic features 
are not useful for the WSD of Chinese verbs is 
premature. We demonstrated that rich linguistic 
features, specifically features based on syntactic 
and semantic role information, are useful for the 
WSD of Chinese verbs. We believe that the 
WSD systems can benefit even more from rich 
linguistic features as the performance of other 
NLP tools such as parsers and Semantic Role 
Taggers improves. Our experimental results also 
lend support to the position that feature design 
for WSD should be linked tightly to the study of 
the criteria that sense distinctions are based on. 
This position calls for the customization of 
features for individual verbs based on 
understanding of the dimensions along which 
sense distinctions are made and a closer marriage 
between machine learning and linguistics. We 
believe this represents a rich area of exploration 
and we intend to experiment with more verbs 
with further customization of features, including 
experimenting with automatic feature selection. 
Acknowledgement 
This work was supported by National Science 
Foundation Grant NSF-0415923, Word Sense 
Disambiguation, the DTO-AQUAINT NBCHC-
040036 grant under the University of Illinois 
subcontract to University of Pennsylvania 2003-
07911-01 and the GALE program of the Defense 
Advanced Research Projects Agency, Contract 
No. HR0011-06-C-0022. Any opinions, findings, 
and conclusions or recommendations expressed 
in this material are those of the authors and do 
927
not necessarily reflect the views of the National 
Science Foundation, the DTO, or DARPA. 
References 
Avrim L. Blum and Pat Langley. 1997. Selection of 
relevant features and examples in machine learning. 
Artificial Intelligence, 97:245-271, 1997. 
Jinying Chen and Martha Palmer. 2004. Chinese Verb 
Sense Discrimination Using an EM Clustering 
Model with Rich Linguistic Features, In Proc. of 
the 42nd Annual meeting of the Assoication for 
Computational Linguistics, ACL-04. July 21-24, 
Barcelona, Spain 
Jinying Chen and Martha Palmer. 2005. Towards 
Robust High Performance Word Sense 
Disambiguation of English Verbs Using Rich 
Linguistic Features. In Proc. of the 2nd 
International Joint Conference on Natural 
Language Processing. Jeju Island, Korea, in press.    
Stanley. F. Chen and Ronald Rosenfeld. 1999. A 
Gaussian Prior for Smoothing Maximum Entropy 
Modals. Technical Report CMU-CS-99-108, CMU. 
Hoa T. Dang, Ching-yi Chia, Martha Palmer and Fu-
Dong Chiou. 2002. Simple Features for Chinese 
Word Sense Disambiguation. In Proceedings of 
COLING-2002, the Nineteenth Int. Conference on 
Computational Linguistics, Taipei, Aug.24?Sept.1. 
Hoa T. Dang. 2004. Investigations into the role of 
lexical semantics in word sense disambiguation.  
PhD Thesis. University of Pennsylvania.  
Hoa Dang and Martha Palmer. 2005. The role of 
semantic roles in  disambiguating verb senses. In 
Proceedings of ACL-05, Ann Arbor, Michigan. 
Zhendong Dong and Qiang Dong, HowNet. 1991. 
http://www.keenage.com. 
Christiane Fellbaum, ed. 1998. WordNet: An 
Electronic Lexical Database. Cambridge, MA: 
MIT Press. 
Veronique Hoste, Iris Hendrickx, Walter Daelemans, 
and Antal van den Bosch. 2002. Parameter 
optimization for machine-learning of word sense 
disambiguation. NLE, Special Issue on Word Sense 
Disambiguation Systems, 8(4):311?325.  
Eduard Hovy, Mtchchell Marcus, Martha Palmer, 
Lance Ramshaw and Ralph Weischedel. 2006. 
OntoNotes: the 90% solution. In Proceedings of the 
HLT-NAACL 2006, New York City. 
Wanyin Li, Qin Lu and Wenjie Li. 2005. Integrating 
Collocation Features in Chinese Word Sense 
Disambiguation. In Proceedings of the Fourth 
Sighan Workshop on Chinese Language Processing.  
pp: 87-94. Jeju, Korea.  
Andrew K. McCallum: MALLET: A Machine 
Learning for Language Toolkit.  http://www.cs. 
umass.edu/~mccallum/mallet (2002). 
Rada Mihalcea, Timothy Chklovski and Adam 
Kilgarriff. 2004. The Senseval-3 English lexical 
sample task. In Proceedings of Senseval-3: The 
Third International Workshop on the Evaluation of 
Systems for the Semantic Analysis of Text. 
Barcelona, Spain. July. 
Zheng-Yu Niu, Dong-Hong Ji and Chew Lim Tan, 
Optimizing Feature Set for Chinese Word Sense 
Disambiguation. 2004. In Proceedings of the 3rd 
International Workshop on the Evaluation of 
Systems for the Semantic Analysis of Text 
(SENSEVAL-3). Barcelona, Spain. 
Martha Palmer, Christiane Fellbaum, Scott Cotton, 
Lauren Delfs, and Hoa Trang Dang. 2001. English 
tasks: All-words and verb lexical sample. 
Proceedings of Senseval-2: Second International 
Workshop on Evaluating Word Sense 
Disambiguation Systems, Toulouse, France, 21-24. 
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 
2005. The Proposition Bank: An Annotated Corpus 
of Semantic Roles, Computational Linguistics, 
31(1): 71?106. 
Martha Palmer, Christiane Fellbaum and Hoa Trang 
Dang. (to appear, 2006). Making fine-grained and 
coarse-grained sense distinctions, both manually 
and automatically. Natural Language Engineering. 
Ruifeng Xu , Qin Lu, and Yin Li. 2003. An automatic 
Chinese Collocation Extraction Algorithm Based 
On Lexical Statistics. In Proceedings of the 
NLPKE Workshop. Beijing, China. 
Nianwen Xue, Fei Xia, Fu-Dong Chiou and Martha 
Palmer. 2005. The Penn Chinese Treebank: Phrase 
Structure Annotation of a Large Corpus. Natural 
Language Engineering, 11(2):207-238. 
Nianwen Xue and Martha Palmer. 2003. Annotating 
Propositions in the Penn Chinese Treebank, In 
Proceedings of the 2nd SIGHAN Workshop on 
Chinese Language Processing, in conjunction with 
ACL'03. Sapporo, Japan. 
Nianwen Xue and Martha Palmer. 2005. Automatic 
Semantic Role Labeling for Chinese Verbs. In 
Proceedings of the 19th International Joint 
Conference on Artificial Intelligence. Edinburgh, 
Scotland. 
David Yarowsky and Radu Florian. 2002. Evaluating 
sense disambiguation across diverse parameter 
spaces. Journal of Natural Language Engineering,  
8(4): 293?310. 
 
928
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 1?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The CoNLL-2009 Shared Task:
Syntactic and Semantic Dependencies in Multiple Languages
Jan Hajic?? Massimiliano Ciaramita? Richard Johansson? Daisuke Kawahara?
Maria Anto`nia Mart???? Llu??s Ma`rquez?? Adam Meyers?? Joakim Nivre?? Sebastian Pado???
Jan ?Ste?pa?nek? Pavel Stran?a?k? Mihai Surdeanu?? Nianwen Xue?? Yi Zhang??
?: Charles University in Prague, {hajic,stepanek,stranak}@ufal.mff.cuni.cz
?: Google Inc., massi@google.com
?: University of Trento, johansson@disi.unitn.it
?: National Institute of Information and Communications Technology, dk@nict.go.jp
??: University of Barcelona, amarti@ub.edu
??: Technical University of Catalonia, Barcelona, lluism@lsi.upc.edu
??: New York University, meyers@cs.nyu.edu
??: Uppsala University and Va?xjo? University, joakim.nivre@lingfil.uu.se
??: Stuttgart University, pado@ims.uni-stuttgart.de
??: Stanford University, mihais@stanford.edu
??: Brandeis University, xuen@brandeis.edu
??: Saarland University, yzhang@coli.uni-sb.de
Abstract
For the 11th straight year, the Conference
on Computational Natural Language Learn-
ing has been accompanied by a shared task
whose purpose is to promote natural language
processing applications and evaluate them in
a standard setting. In 2009, the shared task
was dedicated to the joint parsing of syntac-
tic and semantic dependencies in multiple lan-
guages. This shared task combines the shared
tasks of the previous five years under a unique
dependency-based formalism similar to the
2008 task. In this paper, we define the shared
task, describe how the data sets were created
and show their quantitative properties, report
the results and summarize the approaches of
the participating systems.
1 Introduction
Every year since 1999, the Conference on Com-
putational Natural Language Learning (CoNLL)
launches a competitive, open ?Shared Task?. A
common (?shared?) task is defined and datasets are
provided for its participants. In 2004 and 2005, the
shared tasks were dedicated to semantic role label-
ing (SRL) in a monolingual setting (English). In
2006 and 2007 the shared tasks were devoted to
the parsing of syntactic dependencies, using corpora
from up to 13 languages. In 2008, the shared task
(Surdeanu et al, 2008) used a unified dependency-
based formalism, which modeled both syntactic de-
pendencies and semantic roles for English. The
CoNLL-2009 Shared Task has built on the 2008 re-
sults by providing data for six more languages (Cata-
lan, Chinese, Czech, German, Japanese and Span-
ish) in addition to the original English1. It has thus
naturally extended the path taken by the five most
recent CoNLL shared tasks.
As in 2008, the CoNLL-2009 shared task com-
bined dependency parsing and the task of identify-
ing and labeling semantic arguments of verbs (and
other parts of speech whenever available). Partici-
pants had to choose from two tasks:
? Joint task (syntactic dependency parsing and
semantic role labeling), or
? SRL-only task (syntactic dependency parses
have been provided by the organizers, using
state-of-the art parsers for the individual lan-
guages).
1There are some format changes and deviations from the
2008 task data specification; see Sect. 2.3
1
In contrast to the previous year, the evaluation data
indicated which words were to be dealt with (for the
SRL task). In other words, (predicate) disambigua-
tion was still part of the task, whereas the identi-
fication of argument-bearing words was not. This
decision was made to compensate for the significant
differences between languages and between the an-
notation schemes used.
The ?closed? and ?open? challenges have been
kept from last year as well; participants could have
chosen one or both. In the closed challenge, systems
had to be trained strictly with information contained
in the given training corpus; in the open challenge,
systems could have been developed making use of
any kind of external tools and resources.
This paper is organized as follows. Section 2 de-
fines the task, including the format of the data, the
evaluation metrics, and the two challenges. A sub-
stantial portion of the paper (Section 3) is devoted
to the description of the conversion and develop-
ment of the data sets in the additional languages.
Section 4 shows the main results of the submitted
systems in the Joint and SRL-only tasks. Section 5
summarizes the approaches implemented by partic-
ipants. Section 6 concludes the paper. In all sec-
tions, we will mention some of the differences be-
tween last year?s and this year?s tasks while keeping
the text self-contained whenever possible; for details
and observations on the English data, please refer to
the overview paper of the CoNLL-2008 Shared Task
(Surdeanu et al, 2008) and to the references men-
tioned in the sections describing the other languages.
2 Task Definition
In this section we provide the definition of the shared
task; after introducing the two challenges and the
two tasks the participants were to choose, we con-
tinue with the format of the shared task data, fol-
lowed by a description of the evaluation metrics
used.
For three of the languages (Czech, English and
German), out-of-domain data (OOD) have also been
prepared for the final evaluation, following the same
guidelines and formats.
2.1 Closed and Open Challenges
Similarly to the CoNLL-2005 and CoNLL-2008
shared tasks, this shared task evaluation is separated
into two challenges:
Closed Challenge The aim of this challenge was to
compare performance of the participating systems in
a fair environment. Systems had to be built strictly
with information contained in the given training cor-
pus, and tuned with the development section. In
addition, the lexical frame files (such as the Prop-
Bank and NomBank for English, the valency dictio-
nary PDT-Vallex for Czech etc.) were provided and
may have been used. These restrictions mean that
outside parsers (not trained by the participants? sys-
tems) could not be used. However, we did provide
the output of a single, state-of-the-art dependency
parser for each language so that participants could
build a SRL-only system (using the provided parses
as inputs) within the closed challenge (as opposed to
the 2008 shared task).
Open Challenge Systems could have been devel-
oped making use of any kind of external tools and
resources. The only condition was that such tools or
resources must not have been developed with the an-
notations of the test set, both for the input and output
annotations of the data. In this challenge, we were
interested in learning methods which make use of
any tools or resources that might improve the per-
formance. The comparison of different systems in
this setting may not be fair, and thus ranking of sys-
tems is not necessarily important.
2.2 Joint and SRL-only tasks
In 2008, systems participating in the open challenge
could have used state-of-the-art parsers for the syn-
tactic dependency part of the task. This year, we
have provided the output of these parsers for all the
languages in an uniform way, thus allowing an or-
thogonal combination of the two tasks and the two
challenges. For the SRL-only task, participants in
the closed challenge simply had to use the provided
parses only.
Despite the provisions for the SRL-only task, we
are more interested in the approaches and results of
the Joint task. Therefore, primary system ranking is
provided for the Joint task while additional measures
2
are computed for various combinations of parsers
and SRL methods across the tasks and challenges.
2.3 Data Format
The data format used in this shared task has been
based on the CoNLL-2008 shared task, with some
differences. The data follows these general rules:
? The files contain sentences separated by a blank
line.
? A sentence consists of one or more tokens and
the information for each token is represented on
a separate line.
? A token consists of at least 14 fields. The fields
are separated by one or more whitespace char-
acters (spaces or tabs). Whitespace characters
are not allowed within fields.
The data is thus a large table with whitespace-
separated fields (columns). The fields provided in
the data are described in Table 1. They are identical
for all languages, but they may differ in contents;
for example, some fields might not be filled for all
the languages provided (such as the FEAT or PFEAT
fields).
For the SRL-only task, participants have been
provided will all the data but the PRED and
APREDs, which they were supposed to fill in with
their correct values. However, they did not have
to determine which tokens are predicates (or more
precisely, which are the argument-bearing tokens),
since they were marked by ?Y? in the FILLPRED
field.
For the Joint task, participants could not (in ad-
dition to the PRED and APREDs) see the gold-
standard nor the predicted syntactic dependencies
(HEAD, PHEAD) and their labels (DEPREL, PDE-
PREL). These syntactic dependencies were also to
be filled by participants? systems.
In both tasks, participants have been free to
use any other data (columns) provided, except the
LEMMA, POS and FEAT columns (to get more ?re-
alistic? results using only their automatically pre-
dicted variants PLEMMA, PPOS and PFEAT).
Besides the corpus proper, predicate dictionaries
have been provided to participants in order to be able
to properly match the predicates to the tokens in the
corpus; their contents could have been used e.g. as
features for the PRED/APREDs predictions (or even
for the syntactic dependencies, i.e., for filling in the
PHEAD and PDEPREL fields).
The system of filling-in the APREDs follows
the 2008 pattern; for each argument-bearing token
(predicate), a new APREDn column is created in the
order in which the predicate token is encountered
within the sentence (i.e., based on its ID seen as a
numerical value). Then, for each token in the sen-
tence, the value in the intersection of the APREDn
column and the token row is either left unfilled
(if the token is not an argument), or a predicate-
argument label(s) is(are) filled in.
The differences between the English-only 2008
task and this year?s multilingual task can be briefly
summarized as follows:
? only ?split?2 lemmas and forms have been pro-
vided in the English datasets (for the other lan-
guages, original tokenization from the respec-
tive treebanks has been used);
? rich morphological features have been added
wherever available;
? syntactic dependencies by state-of-the-art
parsers have been provided (for the SRL-only
task);
? multiple semantic labels for a single token have
been allowed (and properly evaluated) in the
APREDs columns;
? predicates have been pre-identified and marked
in both the training and test data;
? some of the fields (e.g. the APREDx) and val-
ues (ARG0? A0 etc.) have been renamed.
2.4 Evaluation Measures
It was required that participants submit results in all
seven languages in the chosen task and in any of (or
both) the challenges. Submission of out-of-domain
data files has been optional.
The main evaluation measure, according to which
systems are primarily compared, is the Joint task,
2Splitting of forms and lemmas in English has been intro-
duced in the 2008 shared task to match the tokenization con-
vention for the arguments in NomBank.
3
Field # Name Description
1 ID Token counter, starting at 1 for each new sentence
2 FORM Form or punctuation symbol (the token; ?split? for English)
3 LEMMA Gold-standard lemma of FORM
4 PLEMMA Automatically predicted lemma of FORM
5 POS Gold-standard POS (major POS only)
6 PPOS Automatically predicted major POS by a language-specific tagger
7 FEAT Gold-standard morphological features (if applicable)
8 PFEAT Automatically predicted morphological features (if applicable)
9 HEAD Gold-standard syntactic head of the current token (ID or 0 if root)
10 PHEAD Automatically predicted syntactic head
11 DEPREL Gold-standard syntactic dependency relation (to HEAD)
12 PDEPREL Automatically predicted dependency relation to PHEAD
13 FILLPRED Contains ?Y? for argument-bearing tokens
14 PRED (sense) identifier of a semantic ?predicate? coming from a current token
15... APREDn Columns with argument labels for each semantic predicate (in the ID order)
Table 1: Description of the fields (columns) in the data provided. The values of columns 9, 11 and 14 and above are
not provided in the evaluation data; for the Joint task, columns 9?12 are also empty in the evaluation data.
closed challenge, Macro F1 score. However, scores
can also be computed for a number of other condi-
tions:
? Task: Joint or SRL-only
? Challenge: open or closed
? Domain: in-domain data (IDD, separated from
training corpus) or out-of-domain data (OOD)
Joint task participants are also evaluated separately
on the syntactic dependency task (labeled attach-
ment score, LAS). Finally, systems competing in
both tasks are compared on semantic role labeling
alone, to assess the impact of the the joint pars-
ing/SRL task compared to an SRL-only task on pre-
parsed data.
Finally, as an explanatory measure, precision and
recall of the semantic labeling task have been com-
puted and tabulated.
We have decided to omit several evaluation fig-
ures that were reported in previous years, such as the
percentage of completely correct sentences (?Exact
Match?), unlabeled scores, etc. With seven lan-
guages, two tasks (plus two challenges, and the
IDD/OOD distinction), there are enough results to
get lost even as it is.
2.4.1 Syntactic Dependency Measures
The LAS score is defined similarly as in the pre-
vious shared tasks, as the percentage of tokens for
which a system has predicted the correct HEAD and
DEPREL columns. The unlabeled attachment score
(UAS), i.e., the percentage of tokens with correct
HEAD regardless if the DEPREL is correct, has not
been officially computed this year. No precision and
recall measures are applicable, since all systems are
supposed to output a single dependency with a single
label (see also below the footnote to the description
of the combined score).
2.4.2 Semantic Labeling Measures
The semantic propositions are evaluated by con-
verting them to semantic dependencies, i.e., we cre-
ate n semantic dependencies from every predicate
to its n arguments. These dependencies are labeled
with the labels of the corresponding arguments. Ad-
ditionally, we create a semantic dependency from
each predicate to a virtual ROOT node. The latter
dependencies are labeled with the predicate senses.
This approach guarantees that the semantic depen-
dency structure conceptually forms a single-rooted,
connected (but not necessarily acyclic) graph. More
importantly, this scoring strategy implies that if a
system assigns the incorrect predicate sense, it still
receives some points for the arguments correctly as-
signed. For example, for the correct proposition:
verb.01: A0, A1, AM-TMP
the system that generates the following output for
the same argument tokens:
4
verb.02: A0, A1, AM-LOC
receives a labeled precision score of 2/4 because two
out of four semantic dependencies are incorrect: the
dependency to ROOT is labeled 02 instead of 01
and the dependency to the AM-TMP is incorrectly la-
beled AM-LOC. Using this strategy we compute pre-
cision, recall, and F1 scores for semantic dependen-
cies (labeled only).
For some languages (Czech, Japanese) there may
be more than one label in a given argument position;
for example, this happens in Czech in special cases
of reciprocity when the same token serves as two or
more arguments to the same predicate. The scorer
takes this into account and considers such cases to
be (as if) multiple predicate-argument relations for
the computation of the evaluation measures.
For example, for the correct proposition:
v1f1: ACT|EFF, ADDR
the system that generates the following output for
the same argument tokens:
v1f1: ACT, ADDR|PAT
receives a labeled precision score of 3/4 because
the PAT is incorrect and labeled recall 3/4 be-
cause the EFF is missing (should the ACT|EFF and
ADDR|PAT be taken as atomic values, the scores
would then be zero).
2.4.3 Combined Syntactic and Semantic Score
We combine the syntactic and semantic measures
into one global measure using macro averaging. We
compute macro precision and recall scores by aver-
aging the labeled precision and recall for semantic
dependencies with the LAS for syntactic dependen-
cies:3
LMP = Wsem ? LPsem + (1?Wsem) ? LAS (1)
LMR = Wsem ? LRsem + (1 ?Wsem) ? LAS (2)
where LMP is the labeled macro precision and
LPsem is the labeled precision for semantic depen-
dencies. Similarly, LMR is the labeled macro re-
call and LRsem is the labeled recall for semantic
dependencies. Wsem is the weight assigned to the
3We can do this because the LAS for syntactic dependen-
cies is a special case of precision and recall, where the predicted
number of dependencies is equal to the number of gold depen-
dencies.
semantic task.4 The macro labeled F1 score, which
was used for the ranking of the participating sys-
tems, is computed as the harmonic mean of LMP
and LMR.
3 Data
The unification of the data formats for the various
languages appeared to be a challenge in itself. We
will briefly describe the processes of the conversion
of the existing treebanks in the seven languages of
the CoNLL-2009 shared task. In many instances,
the original treebanks had to be not only converted
format-wise, but also merged with other resources in
order to generate useful training and testing data that
fit the task description.
3.1 The Input Corpora
The data used as the input for the transformations
aimed at arriving at the data contents and format de-
scribed in Sect. 2.3 are described in (Taule? et al,
2008), (Xue and Palmer, 2009), (Hajic? et al, 2006),
(Surdeanu et al, 2008), (Burchardt et al, 2006) and
(Kawahara et al, 2002).
In the subsequent sections, the procedures for the
data conversion for the individual languages are de-
scribed. The data has been collected by the main
organization site and checked for format errors, and
repackaged for distribution.
There were three packages of the data distributed
to the participants: Trial, Training plus Develop-
ment, and Evaluation. The Trial data were rather
small, just to give the feeling of the format and
languages involved. A visual representation of the
Trial data was also created to make understanding
of the data easier. Any data in the same format
can be transformed and displayed in the Tree Editor
TrEd5 (Pajas and ?Ste?pa?nek, 2008) with the CoNLL
2009 Shared Task extension that can be installed
from within the editor. A sample visualization of an
English sentence after its conversion to the shared
task format (Sect. 2.3) is in Fig. 1.
Due to licensing requirements, every package of
the data had to be split into two portions. One
portion (Catalan, German, Japanese, and Spanish
data) was published on the task?s webpage for down-
4We assign equal weight to the two tasks, i.e., Wsem = 0.5.
5http://ufal.mff.cuni.cz/?pajas/tred
5
$QG
'(3 &&
VRPHWLPHV
703 5%
D
102' '7
UHSXWDEOH
102' --
FKDULW\
6%- 11
ZLWK
102' ,1
D
102' '7
KRXVHKROG
102' 11
QDPH QDPH
302' 11
JHWV JHW
5227 9%=
XVHG XVH
9& 9%1
DQG
&225' &&
GRHV
&21- 9%=
Q
W
$'9 5%
HYHQ
$'9 5%
NQRZ NQRZ
9& 9%
LW
2%- 353

3  
$0703$0703$0703
 

$$$$

 
 

$
 

 

$



$01(*

$0$'9
 


$

Figure 1: Visualisation of the English sentence ?And sometimes a reputable charity with a houshold name gets used
and doesn?t even know it.? (Penn Treebank, wsj 0559) showing jointly the labeled syntactic and semantic depen-
dencies. The basic tree shape comes from the syntactic dependencies; syntactic labels and POS tags are on the 2nd
line at each node. Semantic dependencies which do not follow the syntactic ones use dotted lines. Predicate senses
in parentheses (use:01, ...) follow the word label. SRLs (A0, AM-TMP, ...) are on the last line. Please note that
multiple semantic dependencies (e.g., there are four for charity: A0? know, A1? gets, A1? used, A1? name)
and self-dependencies (name) appear in this sentence.
load, the other portion (Czech, English, and Chinese
data) was invoiced and distributed by the Linguistic
Data Consortium under a special agreement free of
charge.
Distribution of the Evaluation package was a bit
more complicated, because there were two types of
the packages - one for the Joint task and one for the
SRL-only task. Every participant had to subscribe
to one of the two tasks; subsequently, they obtained
the appropriate data (again, from the webpage and
LDC).
Prior to release, each data file was checked to
eliminate errors. The following test were carried
out:
? For every sentence, number of PREDs rows
matches the number of APREDs columns.
? The first line of each file is never empty, while
the last line always is.
? The first character on a non-empty line is al-
ways a digit, the last one is never a whitespace.
? The number of empty lines (i.e. the number
of sentences) equals the number of lines begin-
ning with ?1?.
? The data contain no spaces nor double tabs.
Some statistics on the data can be seen in Ta-
bles 2, 3 and 4. Whereas the training sizes of the
data have not been that different as they were e.g.
for the 2007 shared task on multilingual dependency
parsing (Nivre et al, 2007)6, substantial differences
existed in the distribution of the predicates and ar-
guments, the input features, the out-of-vocabulary
rates, and other statistical characteristics of the data.
Data sizes have been relatively uniform in all the
datasets, with Japanese having the smallest dataset
6http://nextens.uvt.nl/depparse-wiki/
DataOverview
6
containing data for SRL annotation training. To
compensate at least for the dependency parsing part,
an additional, large Japanese corpus with syntactic
dependency annotation has been provided.
The average sentence length, the vocabulary sizes
for FORM and LEMMA fields and the OOV rates
characterize quite naturally the properties of the re-
spective languages (in the domain of the training and
evaluation data). It is no surprise that the FORM
OOV rate is the highest for Czech, a highly inflec-
tional language, and that the LEMMA OOV rate is
the highest for German (as a consequence of keeping
compounds as a single lemma). The other statistics
also reflect (to a large extent) the annotation speci-
fication and conventions used for the original tree-
banks and/or the result of the conversion process to
the unified CoNLL-2009 Shared Task format.
Starting with the POS and FEAT fields, it can be
seen that Catalan, Czech and Spanish use only the
12 major part-of-speech categories as values of the
POS field (with richly populated FEAT field); En-
glish and Chinese are the opposite extreme, disre-
garding the use of the FEAT field completely and
coding everything as a POS value. While for Chi-
nese this is quite understandable, English follows the
PTB tradition in this respect. German and Japanese
use relatively rich set of values in both the POS and
FEAT fields.
For the dependency relations (DEPREL), all
the languages use a similarly-sized set except for
Japanese, which only encodes the distinction be-
tween a root and a dependent node (and some in-
frequent special ones).
Evaluation data are over 10% of the size of the
training data for Catalan, Chinese, Czech, Japanese
and Spanish and roughly 5% for English and Ger-
man.
Table 3 shows the distribution of the five most fre-
quent dependency relations (determined as part of
the subtask of syntactic parsing). With the exception
of Japanese, which essentially does not label depen-
dency relations at this level, all the other languages
show little difference in this distribution. For exam-
ple, the unconditioned probability of ?subjects? is
almost the same for all the six other languages (be-
tween 6 and 8 percent). The probability mass cov-
ered by the first five most frequent DEPRELs is also
almost the same (again, except for Japanese), sug-
gesting that the labeling task might have similar dif-
ficulty7. The most skewed one is for Czech (after
Japanese).
Table 4 shows similar statistics for the argument
labels (PRED/APREDs); it also adds the average
number of arguments per ?predicate? token, since
this is part of the SRL task8. It is apparent from the
comparison of the ?Total? rows in this table and Ta-
ble 3 that the first five argument labels cover more
that their syntactic counterparts. For example, the
arguments A0-A4 account for all but 3% of all ar-
guments labels, whereas Spanish and Catalan have
much more rich set of argument labels, with a high
entropy of the most-frequent-label distribution.
3.2 Catalan and Spanish
The Catalan and Spanish datasets (Taule? et al, 2008)
were generated from the AnCora corpora9 through
an automatic conversion process from a constituent-
based formalism to dependencies (Civit et al, 2006).
AnCora corpora contain about half million words
for Catalan and Spanish annotated with syntactic
and semantic information. Text sources for the Cata-
lan corpus are EFE news agency (?75Kw), ACN
Catalan news agency (?225Kw), and ?El Perio?dico?
newspaper (?200Kw). The Spanish corpus comes
from the Lexesp Spanish balanced corpus (?75Kw),
the EFE Spanish news agency (?225Kw), and the
Spanish version of ?El Perio?dico? (?200Kw). The
subset from ?El Perio?dico? corresponds to the same
news in Catalan and Spanish, spanning from January
to December 2000.
Linguistic annotation is the same in both lan-
guages and includes: PoS tags with morphologi-
cal features (gender, number, person, etc.), lemma-
tization, syntactic dependencies (syntactic func-
tions), semantic dependencies (arguments and the-
matic roles), named entities and predicate semantic
classes (Lexical Semantic Structure, LSS). Tag sets
are shared by the two languages.
If we take into account the complete PoS tags,
7Yes, this is overgeneralization since this distribution does
not condition on the features, dependencies etc. But as a rough
measure, it often correlates well with the results.
8A number below 1 means there are some argument-bearing
words (often nouns) which have no arguments in the particular
sentence in which they appear.
9http://clic.ub.edu/ancora
7
Characteristic Catalan Chinese Czech English German Japanese Spanish
Training data size (sentences) 13200 22277 38727 39279 36020 4393a 14329
Training data size (tokens) 390302 609060 652544 958167 648677 112555a 427442
Avg. sentence length (tokens) 29.6 27.3 16.8 24.4 18.0 25.6 29.8
Tokens with argumentsb (%) 9.6 16.9 63.5 18.7 2.7 22.8 10.3
DEPREL types 50 41 49 69 46 5 49
POS types 12 41 12 48 56 40 12
FEAT types 237 1 1811 1 267 302 264
FORM vocabulary size 33890 40878 86332 39782 72084 36043 40964
LEMMA vocabulary size 24143 40878 37580 28376 51993 30402 26926
Evaluation data size (sent.) 1862 2556 4213 2399 2000 500 1725
Evaluation data size (tokens) 53355 73153 70348 57676 31622 13615 50630
Evaluation FORM OOVc 5.40 3.92 7.98/8.62d 1.58/3.76d 7.93/7.57d 6.07 5.63
Evaluation LEMMA OOVc 4.14 3.92 3.03/4.29d 1.08/2.30d 5.83/7.36d 5.21 3.69
Table 2: Elementary data statistics for the CoNLL-2009 Shared Task languages. The data themselves, the original
treebanks they were derived from and the conversion process are described in more detail in sections 3.2-3.7. All
evaluation data statistics are derived from the in-domain evaluation data.
aThere were additional 33257 sentences (839947 tokens) available for syntactic dependency parsing of Japanese; the type and
vocabulary statistics are computed using this larger dataset.
bPercentage of tokens with FILLPRED=?Y?.
cPercentage of FORM/LEMMA tokens not found in the respective vocabularies derived solely from the training data.
dOOV percentage for in-domain/out-of-domain data.
DEPREL Catalan Chinese Czech English German Japanese Spanish
sn 0.16 COMP 0.21 Atr 0.26 NMOD 0.27 NK 0.31 D 0.93 sn 0.16
spec 0.15 NMOD 0.14 AuxP 0.10 P 0.11 PUNC 0.14 ROOT 0.04 spec 0.15
Labels f 0.11 ADV 0.10 Adv 0.10 PMOD 0.10 MO 0.12 P 0.03 f 0.12
sp 0.09 UNK 0.09 Obj 0.07 SBJ 0.07 SB 0.07 A 0.00 sp 0.08
suj 0.07 SBJ 0.08 Sb 0.06 OBJ 0.06 ROOT 0.06 I 0.00 suj 0.08
Total 0.58 0.62 0.59 0.61 0.70 1.00 0.59
Table 3: Unigram probability for the five most frequent DEPREL labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five dependency labels shown.
APRED Catalan Chinese Czech English German Japanese Spanish
arg1-pat 0.22 A1 0.30 RSTR 0.30 A1 0.37 A0 0.40 GA 0.33 arg1-pat 0.20
arg0-agt 0.18 A0 0.27 PAT 0.18 A0 0.25 A1 0.39 WO 0.15 arg0-agt 0.19
Labels arg1-tem 0.15 ADV 0.20 ACT 0.17 A2 0.12 A2 0.12 NO 0.15 arg1-tem 0.15
argM-tmp 0.08 TMP 0.07 APP 0.06 AM-TMP 0.06 A3 0.06 NI 0.09 arg2-atr 0.08
arg2-atr 0.08 DIS 0.04 LOC 0.04 AM-MNR 0.03 A4 0.01 DE 0.06 argM-tmp 0.08
Total 0.71 0.91 0.75 0.83 0.97 0.78 0.70
Avg. 2.25 2.26 0.88 2.20 1.97 1.71 2.26
Table 4: Unigram probability for the five most frequent APRED labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five argument labels shown. The ?Avg.? line
shows the average number of arguments per predicate or other argument-bearing token (i.e. for those marked by
FILLPRED=?Y?).
8
AnCora has 280 different labels. Considering only
the main syntactic categories, the tag set is reduced
to 47 tags. The syntactic tag set consists of 50 dif-
ferent syntactic functions. Regarding semantic ar-
guments, we distinguish Arg0, Arg1, Arg2, Arg3,
Arg4, ArgM, and ArgL. The first five tags are num-
bered from less to more obliqueness with respect
to the verb, ArgM corresponds to adjuncts. The
list of thematic roles consists of 20 different labels:
AGT (Agent), AGI (Induced Agent), CAU (Cause),
EXP (Experiencer), SCR (Source), PAT (Patient),
TEM (Theme), ATR (Attribute), BEN (Beneficiary),
EXT (Extension), INS (Instrument), LOC (Loca-
tive), TMP (Time), MNR (Manner), ORI (Origin),
DES (Goal), FIN (Purpose), EIN (Initial State), EFI
(Final State), and ADV (Adverbial). Each argument
position can map onto specific thematic roles. By
way of example, Arg1 can be PAT, TEM or EXT. For
Named Entities, we distinguish six types: Organiza-
tion, Person, Location, Date, Number, and Others.
An incremental process guided the annotation of
AnCora, since semantics depends on morphosyntax,
and syntax relies on morphology. This procedure
made it possible to check, correct, and complete
the previous annotations, thus guaranteeing the final
quality of the corpora and minimizing the error rate.
The annotation process was carried out sequentially
from lower to upper layers of linguistic description.
All resulting layers are independent of each other,
thus making easier the data management. The ini-
tial annotation was performed manually for syntax,
semiautomatically in the case of arguments and the-
matic roles, and fully automatically for PoS (Mart??
et al, 2007; Ma`rquez et al, 2007).
The Catalan and Spanish AnCora corpora were
straightforwardly translated into the CoNLL-2009
shared task formatting (information about named
entities was skipped in this process). The resulting
Catalan corpus (including training, development and
test partitions) contains 16,786 sentences with an av-
erage length of 29.59 lexical tokens per sentence.
Long sentences abound in this corpus. For instance,
10.73% of the sentences are longer than 50 tokens,
and 4.42% are longer than 60. The corpus con-
tains 47,537 annotated predicates (2.83 predicates
per sentence, on average) with 107,171 arguments
(2.25 arguments per predicate, on average). From
the latter, 73.89% correspond to core arguments and
26.11% to adjuncts. Numbers for the Spanish cor-
pus are comparable in all aspects: 17,709 sentences
with 29.84 lexical tokens on average (11.58% of the
sentences longer than 50 tokens, 4.07% longer than
60); 54,075 predicates (3.05 per sentence, on aver-
age) and 122,478 arguments (2.26 per predicate, on
average); 73.34% core arguments and 26.66% ad-
juncts.
The following are important features of the Cata-
lan and Spanish corpora in the CoNLL-2009 shared
task setting: (1) all dependency trees are projective;
(2) no word can be the argument of more than one
predicate in a sentence; (3) semantic dependencies
completely match syntactic dependency structures
(i.e., no new edges are introduced by the semantic
structure); (4) only verbal predicates are annotated
(with exceptional cases referring to words that can
be adjectives and past participles); (5) the corpus is
segmented so multi-words, named entities, temporal
expressions, compounds, etc. are grouped together;
and (6) segmentation also accounts for elliptical pro-
nouns (there are marked as empty lexical tokens ?_?
with a pronoun POS tag).
Finally, the predicted columns (PLEMMA,
PPOS, and PFEAT) have been generated with the
FreeLing Open source suite of Language Analyz-
ers10. Accuracy in PLEMMA and PPOS columns
is above 95% for the two languages. PHEAD
and PDEPREL columns have been generated using
MaltParser11. Parsing accuracy (LAS) is above 86%
for the the two languages.
3.3 Chinese
The Chinese Corpus for the 2009 CoNLL Shared
Task was generated by merging the Chinese Tree-
bank (Xue et al, 2005) and the Chinese Proposition
Bank (Xue and Palmer, 2009) and then converting
the constituent structure to a dependency formalism
as specified in the CoNLL Shared Task. The Chi-
nese data used in the shared task is based on Chinese
Treebank 6.0 and the Chinese Proposition Bank 2.0,
both of which are publicly available via the Linguis-
tic Data Consortium.
The Chinese Treebank Project originated at Penn
and was later moved to University of Colorado at
10http://www.lsi.upc.es/?nlp/freeling
11http://w3.msi.vxu.se/?jha/maltparser
9
Boulder. Now it is the process of being to moved
to Brandeis University. The data sources of the Chi-
nese Treebank range from Xinhua newswire (main-
land China), Hong Kong news, and Sinorama Maga-
zine (Taiwan). More recently under DARPA GALE
funding it has been expanded to include broadcast
news, broadcast conversation, news groups and web
log data. It currently has over one million words
and is fully segmented, POS-tagged and annotated
with phrase structure. The version of the Chinese
Treebank used in this shared task, CTB 6.0, includes
newswire, magazine articles, and transcribed broad-
cast news 12. The training set has 609,060 tokens,
the development set has 49,620 tokens, and the test
set has 73,153 tokens.
The Chinese Proposition Bank adds a layer of se-
mantic annotation to the syntactic parses in the Chi-
nese Treebank. This layer of semantic annotation
mainly deals with the predicate-argument structure
of Chinese verbs and their nominalizations. Each
major sense (called frameset) of a predicate takes a
number of core arguments annotated with numeri-
cal labels Arg0 through Arg5 which are defined in
a predicate-specific manner. The Chinese Proposi-
tion Bank also annotates adjunctive arguments such
as locative, temporal and manner modifiers of the
predicate. The version of the Chinese Propbank used
in this CoNLL Shared Task is CPB 2.0, but nominal
predicates are excluded because the annotation is in-
complete.
Since the Chinese Treebank is annotated with
constituent structures, the conversion and merging
procedure converts the constituent structures to de-
pendencies by identifying the head for each con-
stituent in a parse tree and making its sisters its de-
pendents. The Chinese Propbank pointers are then
shifted from the entire constituent to the head of that
constituent. The conversion procedure identifies the
head by first exploiting the structural information
in the syntactic parse and detecting six broad cate-
gories of syntactic relations that hold between the
head and its dependents (predication, modification,
complementation, coordination, auxiliary, and flat)
and then designating the head based on these rela-
tions. In particular, the first conjunct of a coordina-
12A small number of files were taken out of the CoNLL
shared task data due to conversion problems and time con-
straints to fix them.
tion structure is designated as the head and the heads
of the other conjuncts are the conjunctions preced-
ing them. The conjunctions all ?modify? the first
conjunct.
3.4 Czech
For the training, development and evaluation data,
Prague Dependency Treebank 2.0 was used (Hajic?
et al, 2006). For the out-of-domain evaluation data,
part of the Czech side of the Prague Czech-English
Dependency Treebank (version 2, under construc-
tion) was used13, see also ( ?Cmejrek et al, 2004). For
the OOD data, no manual annotation of LEMMA,
POS, and FEAT existed, so the predicted values
were used. The same conversion procedure has been
applied to both sources.
The FORM column was created from the form
element of the morphological layer, not from the
?token? from the word-form layer. Therefore, most
typos, errors in word segmentation and tokenization
are corrected and numerals are normalized.
The LEMMA column was created from the
lemma element of the morphological layer. Only
the initial string of the element was used, so there is
no distinction between homonyms. However, some
components of the detailed lemma explanation were
incorporated into the FEAT column (see below).
The POS column was created form the morpho-
logical tag element, its first character more pre-
cisely.
The FEAT column was created from the remain-
ing characters of the tag element. In addition, the
special feature ?Sem? corresponds to a semantic fea-
ture of the lemma.
For the HEAD and DEPREL columns, the PDT
analytical layer was used. The DEPREL was taken
from the analytic function (the afun node at-
tribtue). There are 27 possible values for afun el-
ement: Pred, Pnom, AuxV, Sb, Obj, Atr, Adv,
Atv, AtvV, Coord, Apos, ExD, and a number
of auxiliary and ?double-function? labels. The first
nine of these are the ?most interesting? from the
point of view of the shared task, since they relate to
semantics more closely than the rest (at least from
the linguistic point of view). The HEAD is a pointer
to its parent, which means the PDT?s ord attribute
13http://ufal.mff.cuni.cz/pedt
10
(within-sentence ID / word position number) of the
parent. If a node is a member of a coordination
or apposition (is_member element), its DEPREL
obtains the _M suffix. The parenthesis annotation
(is_parenthesis_root element) was ignored.
The PRED and APREDs columns were created
from the tectogrammatical layer of PDT 2.0 and the
valency lexicon PDT-Vallex according to the follow-
ing rules:
? Every line corresponding to an analytical node
referenced by a lexical reference (a/lex.rf)
from the tectogrammatical layer has a PRED
value filled. If the referring non-generated
tectogrammatical node (is_generated not
equal to 1) has a valency frame assigned
(val_frame.rf), the value of PRED is the
identifier of the frame. Otherwise, it is set to
the same value as the LEMMA column.
? For every tectogrammatical node, a corre-
sponding analytical node is searched for:
1. If the tectogrammatical node is not
generated and has a lexical reference
(a/lex.rf), the referenced node is
taken.
2. Otherwise, if the tectogrammatical node
has a coreference (coref_text.rf or
coref_gram.rf) or complement refer-
ence (compl.rf) to a node that has an
analytical node assigned (by 1. or 2.), the
assigned node is taken.
APRED columns are filled with respect to the
following correspondence: for a tectogrammatical
node P and its effective child C with functor F, the
column for P?s corresponding analytical node at the
row for C?s corresponding analytical node is filled
with F. Some nodes can thus have several functors
in one APRED column, separated by a vertical bar
(see Sect. 2.4.2).
PLEMMA, PPOS and PFEAT were gener-
ated by the (cross-trained) morphological tagger
MORCE (Spoustova? et al, 2009), which gives full
combined accuracy (PLEMMA+PPOS+PFEAT)
slightly under 96%.
PHEAD and PDEPREL were generated by
the (cross-trained) MST parser for Czech (Chu?
Liu/Edmonds algorithm, (McDonald et al, 2005)),
which has typical dependency accuracy around
85%.
The valency lexicon, converted from (Hajic? et al,
2003), has four columns:
1. lemma (can occur several times in the lexicon,
with different frames)
2. frame identifier (as found in the PRED column)
3. list of space-separated actants and obligatory
members of the frame
4. example(s)
The source of the out-of-domain data uses an
extended valency lexicon (because of out-of-
vocabulary entries). For simplicity, the extended
lexicon was not provided; instead, such words were
not marked as predicates in the OOD data (their
FILLPRED was set to ?_?) and thus not evaluated.
3.5 English
The English corpus is almost identical to the cor-
pus used in the closed challenge in the CoNLL-2008
shared task evaluation (Surdeanu et al, 2008). This
corpus was generated through a process that merges
several input corpora and converts them from the
constituent-based formalism to dependencies. The
following corpora were used as input to the merging
procedure:
? Penn Treebank 3 ? The Penn Treebank 3 cor-
pus (Marcus et al, 1994) consists of hand-
coded parses of the Wall Street Journal (test,
development and training) and a small subset
of the Brown corpus (W. N. Francis and H.
Kucera, 1964) (test only).
? BBN Pronoun Coreference and Entity Type
Corpus ? BBN?s NE annotation of the Wall
Street Journal corpus (Weischedel and Brun-
stein, 2005) takes the form of SGML inline
markup of text, tokenized to be completely
compatible with the Penn Treebank annotation.
For the CoNLL-2008 shared task evaluation,
this corpus was extended by the task organizers
to cover the subset of the Brown corpus used as
a secondary testing dataset. From this corpus
we only used NE boundaries to derive NAME
11
dependencies between NE tokens, e.g., we cre-
ate a NAME dependency from Mary to Smith
given the NE mention Mary Smith.
? Proposition Bank I (PropBank) ? The Prop-
Bank annotation (Palmer et al, 2005) classifies
the arguments of all the main verbs in the Penn
Treebank corpus, other than be. Arguments are
numbered (Arg0, Arg1, . . .) based on lexical
entries or frame files. Different sets of argu-
ments are assumed for different rolesets. De-
pendent constituents that fall into categories in-
dependent of the lexical entries are classified as
various types of adjuncts (ArgM-TMP, -ADV,
etc.).
? NomBank ? NomBank annotation (Meyers et
al., 2004) uses essentially the same framework
as PropBank to annotate arguments of nouns.
Differences between PropBank and NomBank
stem from differences between noun and verb
argument structure; differences in treatment of
nouns and verbs in the Penn Treebank; and dif-
ferences in the sophistication of previous re-
search about noun and verb argument structure.
Only the subset of nouns that take arguments
are annotated in NomBank and only a subset of
the non-argument siblings of nouns are marked
as ArgM.
The complete merging process and the conversion
from the constituent representation to dependencies
is detailed in (Surdeanu et al, 2008).
The main difference between the 2008 and 2009
version of the corpora is the generation of word lem-
mas. In the 2008 version the only lemmas pro-
vided were predicted using the built-in lemmatizer
in WordNet (Fellbaum, 1998) based on the most fre-
quent sense for the form and the predicted part-of-
speech tag. These lemmas are listed in the 2009
corpus under the PLEMMA column. The LEMMA
column in the 2009 version of the corpus contains
lemmas generated using the same algorithm but us-
ing the correct Treebank part-of-speech tags. Addi-
tionally, the PHEAD and PDEPREL columns were
generated using MaltParser14, similarly to the open
challenge corpus in the CoNLL 2008 shared task.
14http://w3.msi.vxu.se/?nivre/research/
MaltParser.html
3.6 German
The German in-domain dataset is based on the an-
notated verb instances of the SALSA corpus (Bur-
chardt et al, 2006), a total of around 40k sen-
tences15. SALSA provides manual semantic role
annotation on top of the syntactically annotated
TIGER newspaper corpus, one of the standard Ger-
man treebanks. The original SALSA corpus uses se-
mantic roles in the FrameNet paradigm. We con-
structed mappings between FrameNet frame ele-
ments and PropBank argument positions at the level
of frame-predicate pairs semi-automatically. For the
frame elements of each frame-predicate pair, we first
identified the semantically defined PropBank Arg-
0 and Arg-1 positions. To do so, we annotated a
small number of very abstract frame elements with
these labels (Agent, Actor, Communicator as Arg-
0, and Theme, Effect, Message as Arg-1) and per-
colated these labels through the FrameNet hierar-
chy, adding further manual labels where necessary.
Then, we used frequency and grammatical realiza-
tion information to map the remaining roles onto
higher-numbered Arg roles. We considerably sim-
plified the annotations provided by SALSA, which
use a rather complex annotation scheme. In partic-
ular, we removed annotation for multi-word expres-
sions (which may be non-contiguous), annotations
involving multiple frames for the same predicate
(metaphors, underspecification), and inter-sentence
roles.
The out-of-domain dataset was taken from a study
on the multi-lingual projection of FrameNet annota-
tion (Pado and Lapata, 2005). It is sampled from
the EUROPARL corpus and was chosen to maxi-
mize the lexical coverage, i.e., it contains of a large
number of infrequent predicates. Both syntactic and
semantic structure were annotated manually, in the
TIGER and SALSA format, respectively. Since it
uses a simplified annotation schemes, we did not
have to discard any annotation.
For both datasets, we converted the syntactic
TIGER (Brants et al, 2002) representations into de-
pendencies with a similar set of head-finding rules
used for the preparation of the CoNLL-X shared task
German dataset. Minor modifications (for the con-
15Note, however, that typically not all predicates in each sen-
tence are annotated (cf. Table 2).
12
version of person names and coordinations) were
made to achieve better consistency with datasets
of other languages. Since the TIGER annotation
allows non-contiguous constituents, the resulting
dependencies can be non-projective. Secondary
edges were discarded in the conversion. As for the
automatically constructed features, we used Tree-
Tagger (Schmid, 1994) to produce the PLEMMA
and PPOS columns, and the Morphisto morphol-
ogy (Zielinski and Simon, 2008) for PFEAT.
3.7 Japanese
For Japanese, we used the Kyoto University Text
Corpus (Kawahara et al, 2002), which consists of
approximately 40k sentences taken from Mainichi
Newspapers. Out of them, approximately 5k sen-
tences are annotated with syntactic and semantic de-
pendencies, and are used the training, development
and test data of this year?s shared task. The remain-
ing sentences, which are annotated with only syntac-
tic dependencies, are provided for the training cor-
pus of syntactic dependency parsers.
This corpus adopts a dependency structure repre-
sentation, and thus the conversion to the CoNLL-
2009 format was relatively straightforward. How-
ever, since the original dependencies are annotated
on the basis of phrases (Japanese bunsetsu), we
needed to automatically convert the original annota-
tions to word-based ones using several criteria. We
used the following basic criteria: the words except
the last word in a phrase depend on the next (right)
word, and the last word in a phrase basically depends
on the head word of the governing phrase.
Semantic dependencies are annotated for both
verbal predicates and nominal predicates. The se-
mantic roles (APRED columns) consist of 41 sur-
face cases, many of which are case-marking post-
positions such as ga (nominative), wo (accusative)
and ni (dative). Semantic frame discrimination is not
annotated, and so the PRED column is the same as
the LEMMA column. The original corpus contains
coreference annotations and inter-sentential seman-
tic dependencies, such as inter-sentential zero pro-
nouns and bridging references, but we did not use
these annotations, which are not the target of this
year?s shared task.
To produce the PLEMMA, PPOS and PFEAT
columns, we used the morphological analyzer JU-
MAN 16 and the dependency and case structure an-
alyzer KNP 17. To produce the PHEAD and PDE-
PREL columns, we used the MSTParser 18.
4 Submissions and Results
Participants uploaded the results through the shared
task website, and the official evaluation was per-
formed centrally. Feedback was provided if any for-
mal problems were encountered (for a list of checks,
see the previous section). One submission had to
be rejected because only English results were pro-
vided. After the evaluation period had passed, the
results were anonymized and published on the web.
A total of 20 systems participated in the closed
challenge; 13 of them in the Joint task and seven in
the SRL-only task. Two systems participated in the
open challenge (Joint task). Moreover, 17 systems
provided output in the out-of-domain part of the task
(11 in the OOD Joint task and six in the OOD SRL-
only task).
The main results for the core task - the Joint task
(dependency syntax and semantic relations) in the
context of the closed challenge - are summarized and
ranked in Table 5.
The largest number of systems can be compared
in the SRL results table (Table 6), where all the sys-
tems have been evaluated solely on the SRL perfor-
mance regardless whether they participated in the
Joint or SRL-only task. However, since the results
might have been influenced by the supplied parser,
separate ranking is provided for both types of the
systems.
Additional breakdown of the results (open chal-
lenge, precision and recall tables for the semantic
labeling task, etc.) are available from the CoNLL-
2009 Shared Task website19.
5 Approaches
Table 7 summarizes the properties of the systems
that participated in the closed the open challenges.
16http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
17http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
18http://sourceforge.net/projects/
mstparser
19http://ufal.mff.cuni.cz/conll2009-st
13
Rank System Average Catalan Chinese Czech English German Japanese Spanish
1 Che 82.64 81.84 76.38 83.27 87.00 82.44 85.65 81.90
2 Chen 82.52 83.01 76.23 80.87 87.69 81.22 85.28 83.31
3 Merlo 82.14 82.66 76.15 83.21 86.03 79.59 84.91 82.43
4 Bohnet 80.85 80.44 75.91 79.57 85.14 81.60 82.51 80.75
5 Asahara 78.43 75.91 73.43 81.43 86.40 69.84 84.86 77.12
6 Brown 77.27 77.40 72.12 75.66 83.98 77.86 76.65 77.21
7 Zhang 76.49 75.00 73.42 76.93 82.88 73.76 78.17 75.25
8 Dai 73.98 72.09 72.72 67.14 81.89 75.00 80.89 68.14
9 Lu Li 73.97 71.32 65.53 75.85 81.92 70.93 80.49 71.72
10 Llu??s 71.49 56.64 66.18 75.95 81.69 72.31 81.76 65.91
11 Vallejo 70.81 73.75 67.16 60.50 78.19 67.51 77.75 70.78
12 Ren 67.81 59.42 75.90 60.18 77.83 65.77 77.63 57.96
13 Zeman 51.07 49.61 43.50 57.95 50.27 49.57 57.69 48.90
Table 5: Official results of the Joint task, closed challenge. Teams are denoted by the last name (first name added
only where needed) of the author who registered for the evaluation data. Results are sorted in descending order of the
language-averaged macro F1 score on the closed challenge Joint task. Bold numbers denote the best result for a given
language.
Rank Rank in task System Average Catalan Chinese Czech English German Japanese Spanish
1 1 (SRLonly) Zhao 80.47 80.32 77.72 85.19 85.44 75.99 78.15 80.46
2 2 (SRLonly) Nugues 80.31 80.01 78.60 85.41 85.63 79.71 76.30 76.52
3 1 (Joint) Chen 79.96 80.10 76.77 82.04 86.15 76.19 78.17 80.29
4 2 (Joint) Che 79.94 77.10 77.15 86.51 85.51 78.61 78.26 76.47
5 3 (Joint) Merlo 78.42 77.44 76.05 86.02 83.24 71.78 77.23 77.19
6 3 (SRLonly) Meza-Ruiz 77.46 78.00 77.73 75.75 83.34 73.52 76.00 77.91
7 4 (Joint) Bohnet 76.00 74.53 75.29 79.02 80.39 75.72 72.76 74.31
8 5 (Joint) Asahara 75.65 72.35 74.17 84.69 84.26 63.66 77.93 72.50
9 6 (Joint) Brown 72.85 72.18 72.43 78.02 80.43 73.40 61.57 71.95
10 7 (Joint) Dai 70.78 66.34 71.57 75.50 78.93 67.43 71.02 64.64
11 8 (Joint) Zhang 70.31 67.34 73.20 78.28 77.85 62.95 64.71 67.81
12 9 (Joint) Lu Li 69.72 66.95 67.06 79.08 77.17 61.98 69.58 66.23
13 4 (SRLonly) Baoli Li 69.26 74.06 70.37 57.46 69.63 67.76 72.03 73.54
14 10 (Joint) Vallejo 68.95 70.14 66.71 71.49 75.97 61.01 68.82 68.48
15 5 (SRLonly) Moreau 66.49 65.60 67.37 71.74 72.14 66.50 57.75 64.33
16 11 (Joint) Llu??s 63.06 46.79 59.72 76.90 75.86 62.66 71.60 47.88
17 6 (SRLonly) Ta?ckstro?m 61.27 57.11 63.41 71.05 67.64 53.42 54.74 61.51
18 7 (SRLonly) Lin 57.18 61.70 70.33 60.43 65.66 59.51 23.78 58.87
19 12 (Joint) Ren 56.69 41.00 72.58 62.82 67.56 54.31 58.73 39.80
20 13 (Joint) Zeman 32.14 24.19 34.71 58.13 36.05 16.44 30.13 25.36
Table 6: Official results of the semantic labeling, closed challenge, all systems. Teams are denoted by the last name
(first name added only where needed) of the author who registered for the evaluation data. Results are sorted in
descending order of the semantic labeled F1 score (closed challenge). Bold numbers denote the best result for a given
language. Separate ranking is provided for SRL-only systems.
The second column of the table highlights the over-
all architectures. We used + to indicate that the
components are sequentially connected. The lack of
a + sign indicates that the corresponding tasks are
performed jointly.
It is perhaps not surprising that most of the obser-
vations from the 2008 shared task still hold; namely,
the best systems overall do not use joint learning or
optimization (the best such system was placed third
in the Joint task, and there were only four systems
where the learning methodology can be considered
?joint?).
Therefore, most of the observations and conclu-
sions from 2008 shared task hold as well for the
current results. For details, we will leave it to the
reader to interpret the architectures and methods
14
O
v
er
a
ll
D
D
D
PA
PA
PA
Jo
in
t
M
L
Sy
st
em
a
A
rc
h.
b
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
c
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
Le
a
rn
in
g/
O
pt
.
M
et
ho
ds
Zh
ao
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
/g
lo
ba
l
se
ar
ch
(S
R
L-
o
n
ly
)
M
E
N
u
gu
es
(P
C+
A
I+
A
C)
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
be
am
se
ar
ch
+
re
ra
n
ki
n
g
(S
R
L-
o
n
ly
)
L2
-
re
gu
la
riz
ed
lin
.
re
gr
es
sio
n
Ch
en
P
+
PC
+
A
I+
A
C
gr
ap
h
pa
rt
ia
lly
M
ST
C
L
/E
cl
as
s
n
o
gr
ee
dy
(?)
n
o
M
E
Ch
e
D
+
PC
+
A
IC
gr
ap
h
n
o
M
ST
H
O
E
cl
as
s
n
o
IL
P
n
o
SV
M
,
M
E
M
er
lo
D
PA
IC
+
D
ge
n
er
at
iv
e,
tr
an
s
n
o
be
am
se
ar
ch
tr
an
s
n
o
be
am
se
ar
ch
sy
n
ch
ro
n
iz
ed
de
riv
at
io
n
IS
B
N
M
ez
a-
R
u
iz
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
M
ar
ko
v
LN
n
o
Cu
tti
n
g
Pl
an
e
(S
R
L-
o
n
ly
)
M
IR
A
B
o
hn
et
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
C
+
re
ar
ra
n
ge
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
IR
A
)
A
sa
ha
ra
D
+
PI
C
+
A
IC
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
n
-
be
st
re
la
x
.
n
o
pe
rc
ep
tr
o
n
D
ai
D
+
PC
+
A
C
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
pr
o
b
ite
ra
tiv
e
M
E
Zh
an
g
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
E
cl
as
s
n
o
cl
as
sifi
ca
tio
n
n
o
M
IR
A
,
M
E
Lu
Li
D
+
(P
C
||
A
IC
)
gr
ap
h
fo
r
ea
ch
la
n
g.
M
ST
C
L
/E
,
M
ST
E
cl
as
s
n
o
gr
ee
dy
n
o
M
E
B
ao
li
Li
PC
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
,
kN
N
,
M
E
Va
lle
jod
[D
+
P+
A
]C
+
D
I
cl
as
s
n
o
re
ra
n
ki
n
g
cl
as
s
n
o
re
ra
n
ki
n
g
u
n
ifi
ed
la
be
ls
M
B
L
M
o
re
au
D
+
PI
+
Cl
u
st
er
in
g
+
A
I+
A
C
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
CR
F
(S
R
L-
o
n
ly
)
CR
F
Ll
u
??s
D
+
D
A
IC
+
PC
gr
ap
h
n
o
M
ST
E
gr
ap
h
n
o
M
ST
E
ye
s,
M
ST
E
Av
g.
Pe
rc
ep
tr
o
n
Ta?
ck
st
ro?
m
D
+
PI
+
A
I
+
A
C
+
Co
n
st
ra
in
tS
at
isf
ac
tio
n
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
R
en
D
+
PC
+
A
IC
tr
an
s
n
o
gr
ee
dy
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
al
t),
M
E
Ze
m
an
D
I+
D
C+
PC
+
A
I+
A
C
tr
an
s
n
o
gr
ee
dy
w
ith
he
u
ris
tic
s
cl
as
s
n
o
gr
ee
dy
n
o
co
o
cc
u
rr
en
ce
Ta
bl
e
7:
Su
m
m
ar
y
o
fs
ys
te
m
ar
ch
ite
ct
u
re
s
fo
r
th
e
Co
N
LL
-
20
09
sh
ar
ed
ta
sk
;
al
ls
ys
te
m
s
ar
e
in
cl
u
de
d.
SR
L-
o
n
ly
sy
st
em
s
do
n
o
t
ha
v
e
th
e
D
co
lu
m
n
s
an
d
th
e
Jo
in
t
Le
ar
in
g/
O
pt
.
co
lu
m
n
s
fil
le
d
in
.
Th
e
sy
st
em
s
ar
e
so
rt
ed
by
th
e
se
m
an
tic
la
be
le
d
F 1
sc
o
re
av
er
ag
ed
o
v
er
al
lt
he
la
n
gu
ag
es
(sa
m
e
as
in
Ta
bl
e
6).
O
n
ly
th
e
sy
st
em
s
th
at
ha
v
e
a
co
rr
es
po
n
di
n
g
pa
pe
r
in
th
e
pr
o
ce
ed
in
gs
ar
e
in
cl
u
de
d.
A
cr
o
n
ym
s
u
se
d:
D
-
sy
n
ta
ct
ic
de
pe
n
de
n
ci
es
,
P
-
pr
ed
ic
at
e,
A
-
ar
gu
m
en
t,
I-
id
en
tifi
ca
tio
n
,
C
-
cl
as
sifi
ca
tio
n
.
O
v
er
a
ll
a
rc
h.
st
an
ds
fo
r
th
e
co
m
pl
et
e
sy
st
em
ar
ch
ite
ct
u
re
;D
A
rc
h.
st
an
ds
fo
r
th
e
ar
ch
ite
ct
u
re
o
ft
he
sy
n
ta
ct
ic
pa
rs
er
;D
C
o
m
b.
in
di
ca
te
s
if
th
e
fin
al
pa
rs
er
o
u
tp
u
tw
as
ge
n
er
at
ed
u
sin
g
pa
rs
er
co
m
bi
n
at
io
n
;D
In
fe
re
n
ce
st
an
ds
fo
r
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
sy
n
ta
ct
ic
pa
rs
in
g;
PA
A
rc
h.
st
an
ds
th
e
ty
pe
o
fa
rc
hi
te
ct
u
re
u
se
d
fo
r
PA
IC
;P
A
C
o
m
b.
in
di
ca
te
s
if
th
e
PA
o
u
tp
u
t
w
as
ge
n
er
at
ed
th
ro
u
gh
sy
st
em
co
m
bi
n
at
io
n
;P
A
In
fe
re
n
ce
st
an
ds
fo
r
th
e
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
PA
IC
;J
o
in
tL
ea
rn
in
g/
O
pt
.
in
di
ca
te
s
if
so
m
e
fo
rm
o
fjo
in
tl
ea
rn
in
g
o
r
o
pt
im
iz
at
io
n
w
as
im
pl
em
en
te
d
fo
r
th
e
sy
n
ta
ct
ic
+
se
m
an
tic
gl
o
ba
lt
as
k;
M
L
M
et
ho
ds
lis
ts
th
e
M
L
m
et
ho
ds
u
se
d
th
ro
u
gh
o
u
tt
he
co
m
pl
et
e
sy
st
em
.
a
A
u
th
o
rs
o
ft
w
o
sy
st
em
s:
?
B
ro
w
n
?
an
d
?
Li
n
?
di
dn
?
ts
u
bm
it
a
pa
pe
r,
so
th
ei
r
sy
st
em
s?
ar
ch
ite
ct
u
re
s
ar
e
u
n
kn
ow
n
.
b T
he
sy
m
bo
l+
in
di
ca
te
s
se
qu
en
tia
lp
ro
ce
ss
in
g
(ot
he
rw
ise
,
pa
ra
lle
l/jo
in
t).
Th
e
||
m
ea
n
s
th
at
se
v
er
al
di
ffe
re
n
ta
rc
hi
te
ct
u
re
s
sp
an
n
in
g
m
u
lti
pl
e
su
bt
as
ks
ra
n
in
pa
ra
lle
l.
c
M
ST
C
L
/E
as
u
se
d
by
M
cD
o
n
al
d
(20
05
),
M
ST
C
by
Ca
rr
er
as
(20
07
),M
ST
E
by
Ei
sn
er
(20
00
),
M
ST
H
O
E
=
M
ST
E
w
ith
hi
gh
er
-
o
rd
er
fe
at
u
re
s
(si
bl
in
gs
+
al
lg
ra
n
dc
hi
ld
re
n
).
d T
he
sy
st
em
u
n
ifi
es
th
e
sy
n
ta
ct
ic
an
d
se
m
an
tic
la
be
ls
in
to
o
n
e
la
be
l,
an
d
tr
ai
n
s
cl
as
sifi
er
s
o
v
er
th
em
.
It
is
th
u
s
di
ffi
cu
lt
to
sp
lit
th
e
sy
st
em
ch
ar
ac
te
ris
tic
in
to
a
?
D
?
/?
PA
?
pa
rt
.
15
when comparing Table 7 with the Tables 5 and 6).
6 Conclusion
This year?s task has been demanding in several re-
spects, but certainly the most difficulty came from
the fact that participants had to tackle all seven lan-
guages. It is encouraging that despite this added af-
fort the number of participating systems has been
almost the same as last year (20 vs. 22 in 2008).
There are several positive outcomes from this
year?s enterprise:
? we have prepared a unified format and data for
several very different lanaguages, as a basis
for possible extensions towards other languages
and unified treatment of syntactic depenndecies
and semantic role labeling across natural lan-
guages;
? 20 participants have produced SRL results for
all seven languages, using several different
methods, giving hope for a combined system
with even substantially better performance;
? initial results have been provided for three lan-
guages on out-of-domain data (being in fact
quite close to the in-domain results).
Only four systems tried to apply what can be de-
scribed as joint learning for the syntactic and seman-
tic parts of the task. (Morante et al, 2009) use a true
joint learning formulation that phrases syntactico-
semantic parsing as a series of classification where
the class labels are concatenations of syntactic and
semantic edge labels. They predict (a), the set of
syntactico-semantic edge labels for each pair of to-
kens; (b), the set of incoming syntactico-semantic
edge labels for each individual token; and (c), the
existence of an edge between each pair of tokens.
Subsequently, they combine the (possibly conflict-
ing) output of the three classifiers by a ranking ap-
proach to determine the most likely structure that
meets all well-formedness constraints. (Llu??s et al,
2009) present a joint approach based on an exten-
sion of Eisner?s parser to accommodate also seman-
tic dependency labels. This architecture is similar
to the one presented by the same authors in the past
edition, with the extension to a second-order syn-
tactic parsing and a particular setting for Catalan
and Spanish. (Gesmundo et al, 2009) use an in-
cremental parsing model with synchronous syntac-
tic and semantic derivations and a joint probability
model for syntactic and semantic dependency struc-
tures. The system uses a single input queue but two
separate stacks and synchronizes syntactic and se-
mantic derivations at every word. The synchronous
derivations are modeled with an Incremental Sig-
moid Belief Network that has latent variables for
both syntactic and semantic states and connections
from syntax to semantics and vice versa. (Dai et
al., 2009) designed an iterative system to exploit
the inter-connections between the different subtasks
of the CoNLL shared task. The idea is to decom-
pose the joint learning problem into four subtasks
? syntactic dependency identification, syntactic de-
pendency labeling, semantic dependency identifica-
tion and semantic dependency labeling. The initial
step is to use a pipeline approach to use the input of
one subtask as input to the next, in the order speci-
fied. The iterative steps then use additional features
that are not available in the initial step to improve the
accuracy of the overall system. For example, in the
iterative steps, semantic information becomes avail-
able as features to syntactic parsing, so on and so
forth.
Despite these results, it is still not clear whether
joint learning has a significant advantage over other
approaches (and if yes, then for what languages). It
is thus necessary to carefully plan the next shared
tasks; it might be advantageous to bring up a sim-
ilar task in the future once again, and/or couple it
with selected application(s). There, (we hope) the
benefits of the dependency representation combined
with semantic roles the way we have formulated it
in 2008 and 2009 will really show up.
Acknowledgments
We would like to thank the Linguistic Data Consor-
tium, mainly to Denise DiPersio, Tony Casteletto
and Christopher Cieri for their help and handling
of invoicing and distribution of the data for which
LDC has a license. For all of the trial, training and
evaluation data they had to act a very short notice.
All the data has been at the participants? disposal
(again) free of charge. We are grateful to all of them
for LDC?s continuing support of the CoNLL Shared
16
Tasks.
We would also like to thank organizers of the pre-
vious four shared tasks: Sabine Buchholz, Xavier
Carreras, Ryan McDonald, Amit Dubey, Johan Hall,
Yuval Krymolowski, Sandra Ku?bler, Erwin Marsi,
Jens Nilsson, Sebastian Riedel and Deniz Yuret.
This shared task would not have been possible with-
out their previous effort.
We also acknowledge the support of the M?SMT
of the Czech Republic, projects MSM0021620838
and LC536; the Grant Agency of the Academy of
sciences of the Czech Republic 1ET201120505 (for
Jan Hajic?, Jan ?Ste?pa?nek and Pavel Stran?a?k).
Llu??s Ma`rquez and M. Anto`nia Mart?? partici-
pation was supported by the Spanish Ministry of
Education and Science, through the OpenMT and
TextMess research projects (TIN2006-15307-C03-
02, TIN2006-15265-C06-06).
The following individuals directly contributed to
the Chinese Treebank (in alphabetic order): Meiyu
Chang, Fu-Dong Chiou, Shizhe Huang, Zixin Jiang,
Tony Kroch, Martha Palmer, Mitch Marcus, Fei
Xia, Nianwen Xue. The contributors to the Chi-
nese Proposition Bank include (in alphabetic order):
Meiyu Chang, Gang Chen, Helen Chen, Zixin Jiang,
Martha Palmer, Zhiyi Song, Nianwen Xue, Ping Yu,
Hua Zhong. The Chinese Treebank and the Chinese
Proposition Bank were funded by DOD, NSF and
DARPA.
Adam Meyers? work on the shared task has been
supported by the NSF Grant IIS-0534700 ?Structure
Alignment-based MT.?
We thank the Mainichi Newspapers for the per-
mission of distributing the sentences of the Kyoto
University Text Corpus for this shared task.
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
EMNLP-CoNLL 2007, pages 957?961, June. Prague,
Czech Republic.
Montserrat Civit, M. Anto`nia Mart??, and Nu?ria Buf??.
2006. Cat3LB and Cast3LB: from constituents to
dependencies. In Proceedings of the 5th Interna-
tional Conference on Natural Language Processing,
FinTAL, pages 141?153, Turku, Finland. Springer Ver-
lag, LNAI 4139.
Qifeng Dai, Enhong Chen, and Liu Shi. 2009. An it-
erative approach for joint dependency parsing and se-
mantic role labeling. In Proceedings of the 13th Con-
ference on Computational Natural Language Learning
(CoNLL-2009), June 4-5, Boulder, Colorado, USA.
June 4-5.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Tehcnologies, pages 29?62. Kluwer Academic
Publishers.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press, Cambridge.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of the 13th Conference on
Computational Natural Language Learning (CoNLL-
2009), June 4-5, Boulder, Colorado, USA. June 4-5.
Jan Hajic?, Jarmila Panevova?, Zden?ka Ures?ova?, Alevtina
Be?mova?, Veronika Kola?r?ova?- ?Rezn??c?kova??, and Petr
Pajas. 2003. PDT-VALLEX: Creating a Large-
coverage Valency Lexicon for Treebank Annotation.
In J. Nivre and E. Hinrichs, editors, Proceedings of The
Second Workshop on Treebanks and Linguistic Theo-
ries, pages 57?68, Vaxjo, Sweden. Vaxjo University
Press.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan ?Ste?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k ?Zabokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Xavier Llu??s, Stefan Bott, and Llu??s Ma`rquez. 2009.
A second-order joint eisner model for syntactic and
semantic dependency parsing. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA. June 4-5.
17
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313?330.
Llu??s Ma`rquez, Luis Villarejo, M. Anto`nia Mart??, and
Mariona Taule?. 2007. SemEval-2007 Task 09: Mul-
tilevel semantic annotation of catalan and spanish.
In Proceedings of the 4th International Workshop on
Semantic Evaluations (SemEval-2007), pages 42?47,
Prague, Czech Republic.
M. Anto`nia Mart??, Mariona Taule?, Llu??s Ma`rquez, and
Manu Bertran. 2007. Anotacio?n semiautoma?tica
con papeles tema?ticos de los corpus CESS-ECE.
Procesamiento del Lenguaje Natural, SEPLN Journal,
38:67?76.
Ryan McDonald, Fernando Pereira, Jan Hajic?, and Kiril
Ribarov. 2005. Non-projective dependency parsing
using spanning tree algortihms. In Proceedings of
NAACL-HLT?05, Vancouver, Canada, pages 523?530.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank Project: An Interim Report. In NAACL/HLT
2004 Workshop Frontiers in Corpus Annotation,
Boston.
Roser Morante, Vincent Van Asch, and Antal van den
Bosch. 2009. A simple generative pipeline approach
to dependency parsing and semantic role labeling. In
Proceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL-2009), Boulder,
Colorado, USA. June 4-5.
Joakim Nivre, Johann Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The conll 2007 shared task on depen-
dency parsing. In Proceedings of the EMNLP-CoNLL
2007 Conference, pages 915?932, Prague, Czech Re-
public.
Sebastian Pado and Mirella Lapata. 2005. Cross-lingual
projection of role-semantic information. In Proceed-
ings of the Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing (HLT/EMNLP-2005), pages 859?
866, Vancouver, BC.
Petr Pajas and Jan ?Ste?pa?nek. 2008. Recent advances in
a feature-rich framework for treebank annotation. In
The 22nd International Conference on Computational
Linguistics - Proceedings of the Conference (COL-
ING?08), pages 673?680, Manchester.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing.
Drahom??ra ?Johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised train-
ing for the averaged perceptron POS tagger. In Pro-
ceedings of the European ACL Cenference EACL?09,
Athens, Greece.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008), pages 159?177.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Martin ?Cmejrek, Jan Cur???n, Jan Hajic?, Jir??? Havelka,
and Vladislav Kubon?. 2004. Prague Czech-English
Dependency Treebank: Syntactically Anntoated Re-
sources for Machine Translation. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation (LREC-2004), pages 1597?
1600, Lisbon, Portugal.
W. N. Francis and H. Kucera. 1964. Brown Corpus Man-
ual of Information to accompany A Standard Corpus
of Present-Day Edited American English, for use with
Digital Computers. Revised 1971, Revised and Am-
plified 1979, available at www.clarinet/brown.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Technical report,
Lin- guistic Data Consortium.
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Nianwen Xue, Fei Xia, Fu Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Andrea Zielinski and Christian Simon. 2008. Morphisto:
An open-source morphological analyzer for german.
In Proceedings of the Conference on Finite State Meth-
ods in Natural Language Processing.
18
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 146?154,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Automatic Recognition of Logical Relations for English, Chinese and
Japanese in the GLARF Framework
Adam Meyers?, Michiko Kosaka?, Nianwen Xue?, Heng Ji?, Ang Sun?, Shasha Liao? and Wei Xu?
? New York University, ?Monmouth University, ?Brandeis University, ? City University of New York
Abstract
We present GLARF, a framework for repre-
senting three linguistic levels and systems for
generating this representation. We focus on a
logical level, like LFG?s F-structure, but com-
patible with Penn Treebanks. While less fine-
grained than typical semantic role labeling ap-
proaches, our logical structure has several ad-
vantages: (1) it includes all words in all sen-
tences, regardless of part of speech or seman-
tic domain; and (2) it is easier to produce ac-
curately. Our systems achieve 90% for En-
glish/Japanese News and 74.5% for Chinese
News ? these F-scores are nearly the same as
those achieved for treebank-based parsing.
1 Introduction
For decades, computational linguists have paired a
surface syntactic analysis with an analysis represent-
ing something ?deeper?. The work of Harris (1968),
Chomsky (1957) and many others showed that one
could use these deeper analyses to regularize differ-
ences between ways of expressing the same idea.
For statistical methods, these regularizations, in ef-
fect, reduce the number of significant differences be-
tween observable patterns in data and raise the fre-
quency of each difference. Patterns are thus easier
to learn from training data and easier to recognize in
test data, thus somewhat compensating for the spare-
ness of data. In addition, deeper analyses are often
considered semantic in nature because conceptually,
two expressions that share the same regularized form
also share some aspects of meaning. The specific de-
tails of this ?deep? analysis have varied quite a bit,
perhaps more than surface syntax.
In the 1970s and 1980s, Lexical Function Gram-
mar?s (LFG) way of dividing C-structure (surface)
and F-structure (deep) led to parsers such as (Hobbs
and Grishman, 1976) which produced these two lev-
els, typically in two stages. However, enthusiasm
for these two-stage parsers was eclipsed by the ad-
vent of one stage parsers with much higher accu-
racy (about 90% vs about 60%), the now-popular
treebank-based parsers including (Charniak, 2001;
Collins, 1999) and many others. Currently, many
different ?deeper? levels are being manually anno-
tated and automatically transduced, typically using
surface parsing and other processors as input. One
of the most popular, semantic role labels (annota-
tion and transducers based on the annotation) char-
acterize relations anchored by select predicate types
like verbs (Palmer et al, 2005), nouns (Meyers et
al., 2004a), discourse connectives (Miltsakaki et al,
2004) or those predicates that are part of particular
semantic frames (Baker et al, 1998). The CONLL
tasks for 2008 and 2009 (Surdeanu et al, 2008;
Hajic? et al, 2009) has focused on unifying many of
these individual efforts to produce a logical structure
for multiple parts of speech and multiple languages.
Like the CONLL shared task, we link surface lev-
els to logical levels for multiple languages. How-
ever, there are several differences: (1) The logical
structures produced automatically by our system can
be expected to be more accurate than the compara-
ble CONLL systems because our task involves pre-
dicting semantic roles with less fine-grained distinc-
tions. Our English and Japanese results were higher
than the CONLL 2009 SRL systems. Our English F-
scores range from 76.3% (spoken) to 89.9% (News):
146
the best CONLL 2009 English scores were 73.31%
(Brown) and 85.63% (WSJ). Our Japanese system
scored 90.6%: the best CONLL 2009 Japanese score
was 78.35%. Our Chinese system 74.5%, 4 points
lower than the best CONLL 2009 system (78.6%),
probably due to our system?s failings, rather than the
complexity of the task; (2) Each of the languages
in our system uses the same linguistic framework,
using the same types of relations, same analyses of
comparable constructions, etc. In one case, this re-
quired a conversion from a different framework to
our own. In contrast, the 2009 CONLL task puts
several different frameworks into one compatible in-
put format. (3) The logical structures produced by
our system typically connect all the words in the sen-
tence. While this is true for some of the CONLL
2009 languages, e.g., Czech, it is not true about
all the languages. In particular, the CONLL 2009
English and Chinese logical structures only include
noun and verb predicates.
In this paper, we will describe the GLARF frame-
work (Grammatical and Logical Representation
Framework) and a system for producing GLARF
output (Meyers et al, 2001; Meyers, 2008). GLARF
provides a logical structure for English, Chinese and
Japanese with an F-score that is within a few per-
centage points of the best parsing results for that
language. Like LFG?s (LFG) F-structure, our log-
ical structure is less fine-grained than many of the
popular semantic role labeling schemes, but also has
two main advantages over these schemes: it is more
reliable and it is more comprehensive in the sense
that it covers all parts of speech and the resulting
logical structure is a connected graph. Our approach
has proved adequate for three genetically unrelated
natural languages: English, Chinese and Japanese.
It is thus a good candidate for additional languages
with accurate parsers.
2 The GLARF framework
Our system creates a multi-tiered representation in
the GLARF framework, combining the theory un-
derlying the Penn Treebank for English (Marcus et
al., 1994) and Chinese (Xue et al, 2005) (Chom-
skian linguistics of the 1970s and 1980s) with: (2)
Relational Grammar?s graph-based way of repre-
senting ?levels? as sequences of relations; (2) Fea-
ture structures in the style of Head-Driven Phrase
Structure Grammar; and (3) The Z. Harris style goal
of attempting to regularize multiple ways of saying
the same thing into a single representation. Our
approach differs from LFG F-structure in several
ways: we have more than two levels; we have a
different set of relational labels; and finally, our ap-
proach is designed to be compatible with the Penn
Treebank framework and therefore, Penn-Treebank-
based parsers. In addition, the expansion of our the-
ory is governed more by available resources than by
the underlying theory. As our main goal is to use
our system to regularize data, we freely incorporate
any analysis that fits this goal. Over time, we have
found ways of incorporating Named Entities, Prop-
Bank, NomBank and the Penn Discourse Treebank.
Our agenda also includes incorporating the results of
other research efforts (Pustejovsky et al, 2005).
For each sentence, we generate a feature structure
(FS) representing our most complete analysis. We
distill a subset of this information into a dependency
structure governed by theoretical assumptions, e.g.,
about identifying functors of phrases. Each GLARF
dependency is between a functor and an argument,
where the functor is the head of a phrase, conjunc-
tion, complementizer, or other function word. We
have built applications that use each of these two
representations, e.g., the dependency representation
is used in (Shinyama, 2007) and the FS represen-
tation is used in (K. Parton and K. R. McKeown
and R. Coyne and M. Diab and R. Grishman and
D. Hakkani-Tu?r and M. Harper and H. Ji and W. Y.
Ma and A. Meyers and S. Stolbach and A. Sun and
G. Tu?r and W. Xu and S. Yarman, 2009).
In the dependency representation, each sentence
is a set of 23 tuples, each 23-tuple characterizing up
to three relations between two words: (1) a SUR-
FACE relation, the relation between a functor and an
argument in the parse of a sentence; (2) a LOGIC1
relation which regularizes for lexical and syntac-
tic phenomena like passive, relative clauses, deleted
subjects; and (3) a LOGIC2 relation corresponding
to relations in PropBank, NomBank, and the Penn
Discourse Treebank (PDTB). While the full output
has all this information, we will limit this paper to
a discussion of the LOGIC1 relations. Figure 1 is
a 5 tuple subset of the 23 tuple GLARF analysis of
the sentence Who was eaten by Grendel? (The full
147
L1 Surf L2 Func Arg
NIL SENT NIL Who was
PRD PRD NIL was eaten
COMP COMP ARG0 eaten by
OBJ NIL ARG1 eaten Who
NIL OBJ NIL by Grendel
SBJ NIL NIL eaten Grendel
Figure 1: 5-tuples: Who was eaten by Grendel
Who
eaten
was
by
PRD
S?OBJ
L?OBJ
ARG1
COMP
ARG0
S?SENT
L?SBJ
Grendel
Figure 2: Graph of Who was eaten by Grendel
23 tuples include unique ids and fine-grained lin-
guistic features). The fields listed are: logic1 label
(L1), surface label (Surf), logic2 label (L2), func-
tor (Func) and argument (Arg). NIL indicates that
there is no relation of that type. Figure 2 repre-
sents this as a graph. For edges with two labels,
the ARG0 or ARG1 label indicates a LOGIC2 re-
lation. Edges with an L- prefix are LOGIC1 la-
bels (the edges are curved); edges with S-prefixes
are SURFACE relations (the edges are dashed); and
other (thick) edges bear unprefixed labels represent-
ing combined SURFACE/LOGIC1 relations. Delet-
ing the dashed edges yields a LOGIC1 representa-
tion; deleting the curved edges yields a SURFACE
representation; and a LOGIC2 consists of the edges
labeled ARGO and ARG1 relations, plus the sur-
face subtrees rooted where the LOGIC2 edges ter-
minate. Taken together, a sentence?s SURFACE re-
lations form a tree; the LOGIC1 relations form a
directed acyclic graph; and the LOGIC2 relations
form directed graphs with some cycles and, due to
PDTB relations, may connect sentences to previous
ones, e.g., adverbs like however, take the previous
sentence as one of their arguments.
LOGIC1 relations (based on Relational Gram-
mar) regularize across grammatical and lexical al-
ternations. For example, subcategorized verbal ar-
guments include: SBJect, OBJect and IND-OBJ (in-
direct Object), COMPlement, PRT (Particle), PRD
(predicative complement). Other verbal modifiers
include AUXilliary, PARENthetical, ADVerbial. In
contrast, FrameNet and PropBank make finer dis-
tinctions. Both PP arguments of consulted in John
consulted with Mary about the project bear COMP
relations with the verb in GLARF, but would have
distinct labels in both PropBank and FrameNet.
Thus Semantic Role Labeling (SRL) should be more
difficult than recognizing LOGIC1 relations.
Beginning with Penn Treebank II, Penn Treebank
annotation includes Function tags, hyphenated addi-
tions to phrasal categories which indicate their func-
tion. There are several types of function tags:
? Argument Tags such as SBJ, OBJ, IO (IND-
OBJ), CLR (COMP) and PRD?These are lim-
ited to verbal relations and not all are used in
all treebanks. For example, OBJ and IO are
used in the Chinese, but not the English tree-
bank. These labels can often be directly trans-
lated into GLARF LOGIC1 relations.
? Adjunct Tags such as ADV, TMP, DIR, LOC,
MNR, PRP?These tags often translate into a
single LOGIC1 tag (ADV). However, some of
these also correspond to LOGIC1 arguments.
In particular, some DIR and MNR tags are re-
alized as LOGIC1 COMP relations (based on
dictionary entries). The fine grained seman-
tic distinctions are maintained in other features
that are part of the GLARF description.
In addition, GLARF treats Penn?s PRN phrasal
category as a relation rather than a phrasal category.
For example, given a sentence like, Banana ketchup,
the agency claims, is very nutritious, the phrase
the agency claims is analyzed as an S(entence) in
GLARF bearing a (surface) PAREN relation to the
main clause. Furthermore, the whole sentence is a
COMP of the verb claims. Since PAREN is a SUR-
FACE relation, not a LOGIC1 relation, there is no
LOGIC1 cycle as shown by the set of 5-tuples in
Figure 3? a cycle only exists if you include both
SURFACE and LOGIC1 relations in a single graph.
Another important feature of the GLARF frame-
work is transparency, a term originating from N.
148
L1 Surf L2 Func Arg
NIL SBJ ARG1 is ketchup
PRD PRD ARG2 is nutritious
SBJ NIL NIL nutritious Ketchup
ADV ADV NIL nutritious very
N-POS N-POS NIL ketchup Banana
NIL PAREN NIL is claims
SBJ SBJ ARG0 claims agency
Q-POS Q-POS NIL agency the
COMP NIL ARG1 claims is
Figure 3: 5-tuples: Banana Ketchup, the agency claims,
is very nutritious
L1 Surf L2 Func Arg
SBJ SBJ ARG0 ate and
OBJ OBJ ARG1 ate box
CONJ CONJ NIL and John
CONJ CONJ NIL and Mary
COMP COMP NIL box of
Q-POS Q-POS NIL box the
OBJ OBJ NIL of cookies
Figure 4: 5-tuples: John and Mary ate the box of cookies
Sager?s unpublished work. A relation between two
words is transparent if: the functor fails to character-
ize the selectional properties of the phrase (or sub-
graph in a Dependency Analysis), but its argument
does. For example, relations between conjunctions
(e.g., and, or, but) and their conjuncts are transparent
CONJ relations. Thus although and links together
John and Mary, it is these dependents that deter-
mine that the resulting phrase is noun-like (an NP
in phrase structure terminology) and sentient (and
thus can occur as the subject of verbs like ate). An-
other common example of transparent relations are
the relations connecting certain nouns and the prepo-
sitional objects under them, e.g., the box of cookies
is edible, because cookies are edible even though
boxes are not. These features are marked in the
NOMLEX-PLUS dictionary (Meyers et al, 2004b).
In Figure 4, we represent transparent relations, by
prefixing the LOGIC1 label with asterisks.
The above description most accurately describes
English GLARF. However, Chinese GLARF has
most of the same properties, the main exception be-
ing that PDTB arguments are not currently marked.
For Japanese, we have only a preliminary represen-
tation of LOGIC2 relations and they are not derived
from PropBank/NomBank/PDTB.
2.1 Scoring the LOGIC1 Structure
For purposes of scoring, we chose to focus on
LOGIC1 relations, our proposed high-performance
level of semantics. We scored with respect to: the
LOGIC1 relational label, the identity of the functor
and the argument, and whether the relation is trans-
parent or not. If the system output differs in any of
these respects, the relation is marked wrong. The
following sections will briefly describe each system
and present an evaluation of its results.
The answer keys for each language were created
by native speakers editing system output, as repre-
sented similarly to the examples in this paper, al-
though part of speech is included for added clar-
ity. In addition, as we attempted to evaluate logi-
cal relation (or dependency) accuracy independent
of sentence splitting. We obtained sentence divi-
sions from data providers and treebank annotation
for all the Japanese and most of the English data, but
used automatic sentence divisions for the English
BLOG data. For the Chinese, we omitted several
sentences from our evaluation set due to incorrect
sentence splits. The English and Japanese answer
keys were annotated by single native speakers ex-
pert in GLARF. The Chinese data was annotated by
several native speakers and may have been subject
to some interannotator agreement difficulties, which
we intend to resolve in future work. Currently, cor-
recting system output is the best way to create an-
swer keys due to certain ambiguities in the frame-
work, some of which we hope to incorporate into fu-
ture scoring procedures. For example, consider the
interpretation of the phrase five acres of land in Eng-
land with respect to PP attachment. The difference
in meaning between attaching the PP in England
to acres or to land is too subtle for these authors?
we have difficulty imagining situations where one
statement would be accurate and the other would
not. This ambiguity is completely predictable be-
cause acres is a transparent noun and similar ambi-
guities hold for all such cases where a transparent
noun takes a complement and is followed by a PP
attachment. We believe that a more complex scor-
ing program could account for most of these cases.
149
Similar complexities arise for coordination and sev-
eral other phenomena.
3 English GLARF
We generate English GLARF output by applying a
procedure that combines:
1. The output of the 2005 version of the Charniak
parser described in (Charniak, 2001), which
label precision and recall scores in the 85%
range. The updated version of the parser seems
to perform closer to 90% on News data and per-
form lower on other genres. That performance
would reflect reports on other versions of the
Charniak parser for which statistics are avail-
able (Foster and van Genabith, 2008).
2. Named entity (NE) tags from the JET NE sys-
tem (Ji and Grishman, 2006), which achieves
F-scores ranging 86%-91% on newswire for
both English and Chinese (depending on
Epoch). The JET system identifies seven
classes of NEs: Person, GPE, Location, Orga-
nization, Facility, Weapon and Vehicle.
3. Machine Readable dictionaries: COMLEX
(Macleod et al, 1998), NOMBANK dictio-
naries (from http://nlp.cs.nyu.edu/
meyers/nombank/) and others.
4. A sequence of hand-written rules (citations
omitted) such that: (1) the first set of rules con-
vert the Penn Treebank into a Feature Structure
representation; and (2) each rule N after the
first rule is applied to an entire Feature Struc-
ture that is the output of rule N ? 1.
For this paper, we evaluated the English output for
several different genres, all of which approximately
track parsing results for that genre. For written
genres, we chose between 40 and 50 sentences.
For speech transcripts, we chose 100 sentences?we
chose this larger number because a lot of so-called
sentences contained text with empty logical de-
scriptions, e.g., single word utterances contain no
relations between pairs of words. Each text comes
from a different genre. For NEWS text, we used 50
sentences from the aligned Japanese-English data
created as part of the JENAAD corpus (Utiyama
Genre Prec Rec F
NEWS 731815 = 89.7%
715
812 = 90.0% 89.9%
BLOG 704844 = 83.4%
704
899 = 78.3% 80.8%
LETT 392434 = 90.3%
392
449 = 87.3% 88.8%
TELE 472604 = 78.1%
472
610 = 77.4% 77.8%
NARR 732959 = 76.3%
732
964 = 75.9% 76.1%
Table 1: English Aggregate Scores
Corpus Prec Rec F Sents
NEWS 90.5% 90.8% 90.6% 50
BLOG 84.1% 79.6% 81.7% 46
LETT 93.9% 89.2% 91.4% 46
TELE 81.4% 83.2% 84.9% 103
NARR 77.1% 78.1% 79.5% 100
Table 2: English Score per Sentence
and Isahara, 2003); the web text (BLOGs) was
taken from some corpora provided by the Linguistic
Data Consortium through the GALE (http:
//projects.ldc.upenn.edu/gale/) pro-
gram; the LETTer genre (a letter from Good Will)
was taken from the ICIC Corpus of Fundraising
Texts (Indiana Center for Intercultural Communi-
cation); Finally, we chose two spoken language
transcripts: a TELEphone conversation from
the Switchboard Corpus (http://www.ldc.
upenn.edu/Catalog/readme_files/
switchboard.readme.html) and one NAR-
Rative from the Charlotte Narrative and Conversa-
tion Collection (http://newsouthvoices.
uncc.edu/cncc.php). In both cases, we
assumed perfect sentence splitting (based on Penn
Treebank annotation). The ICIC, Switchboard
and Charlotte texts that we used are part of the
Open American National Corpus (OANC), in
particular, the SIGANN shared subcorpus of the
OANC (http://nlp.cs.nyu.edu/wiki/
corpuswg/ULA-OANC-1) (Meyers et al, 2007).
Comparable work for English includes: (1) (Gab-
bard et al, 2006), a system which reproduces the
function tags of the Penn Treebank with 89% accu-
racy and empty categories (and their antecedents)
with varying accuracies ranging from 82.2% to
96.3%, excluding null complementizers, as these are
theory-internal and have no value for filling gaps.
(2) Current systems that generate LFG F-structure
150
such as (Wagner et al, 2007) which achieve an F
score of 91.1 on the F-structure PRED relations,
which are similar to our LOGIC1 relations.
4 Chinese GLARF
The Chinese GLARF program takes a Chinese
Treebank-style syntactic parse and the output of a
Chinese PropBanker (Xue, 2008) as input, and at-
tempts to determine the relations between the head
and its dependents within each constituent. It does
this by first exploiting the structural information
and detecting six broad categories of syntactic rela-
tions that hold between the head and its dependents.
These are predication, modification, complementa-
tion, coordination, auxiliary, and flat. Predication
holds at the clause level between the subject and the
predicate, where the predicate is considered to be
the head and the subject is considered to the depen-
dent. Modification can also hold mainly within NPs
and VPs, where the dependents are modifiers of the
NP head or adjuncts to the head verb. Coordination
holds almost for all phrasal categories where each
non-punctuation child within this constituent is ei-
ther conjunction or a conjunct. The head in a co-
ordination structure is underspecified and can be ei-
ther a conjunct or a conjunction depending on the
grammatical framework. Complementation holds
between a head and its complement, with the com-
plement usually being a core argument of the head.
For example, inside a PP, the preposition is the head
and the phrase or clause it takes is the dependent. An
auxiliary structure is one where the auxiliary takes
a VP as its complement. This structure is identi-
fied so that the auxiliary and the verb it modifies can
form a verb group in the GLARF framework. Flat
structures are structures where a constituent has no
meaningful internal structure, which is possible in a
small number of cases. After these six broad cate-
gories of relations are identified, more fine-grained
relation can be detected with additional information.
Figure 5 is a sample 4-tuple for a Chinese translation
of the sentence in figure 3.
For the results reported in Table 3, we used the
Harper and Huang parser described in (Harper and
Huang, Forthcoming) which can achieve F-scores
as high as 85.2%, in combination with informa-
tion about named entities from the output of the
Figure 5: Agency claims, Banana Ketchup is very have
nutrition DE.
JET Named Entity tagger for Chinese (86%-91% F-
measure as per section 3). We used the NE tags to
adjust the parts of speech and the phrasal boundaries
of named entities (we do the same with English).
As shown in Table 3, we tried two versions of the
Harper and Huang parser, one which adds function
tags to the output and one that does not. The Chinese
GLARF system scores significantly (13.9% F-score)
higher given function tagged input, than parser out-
put without function tags. Our current score is about
10 points lower than the parser score. Our initial er-
ror analysis suggests that the most common forms
of errors involve: (1) the processing of long NPs;
(2) segmentation and POS errors; (3) conjunction
scope; and (4) modifier attachment.
5 Japanese GLARF
For Japanese, we process text with the KNP parser
(Kurohashi and Nagao, 1998) and convert the output
into the GLARF framework. The KNP/Kyoto Cor-
pus framework is a Japanese-specific Dependency
framework, very different from the Penn Treebank
framework used for the other systems. Process-
ing in Japanese proceeds as follows: (1) we pro-
cess the Japanese with the Juman segmenter (Kuro-
151
Type Prec Rec F
No Function Tags Version
Aggr 8431374 = 61.4%
843
1352 = 62.4% 61.8%
Aver 62.3% 63.5% 63.6%
Function Tags Version
Aggr 10311415 = 72.9%
1031
1352 = 76.3% 74.5%
Aver 73.0% 75.3% 74.9%
Table 3: 53 Chinese Newswire Sentences: Aggregate and
Average Sentence Scores
hashi et al, 1994) and KNP parser 2.0 (Kurohashi
and Nagao, 1998), which has reported accuracy of
91.32% F score for dependency accuracy, as re-
ported in (Noro et al, 2005). As is standard in
Japanese linguistics, the KNP/Kyoto Corpus (K)
framework uses a dependency analysis that has some
features of a phrase structure analysis. In partic-
ular, the dependency relations are between bun-
setsu, small constituents which include a head word
and some number of modifiers which are typically
function words (particles, auxiliaries, etc.), but can
also be prenominal noun modifiers. Bunsetsu can
also include multiple words in the case of names.
The K framework differentiates types of dependen-
cies into: the normal head-argument variety, coor-
dination (or parallel) and apposition. We convert
the head-argument variety of dependency straight-
forwardly into a phrase consisting of the head and
all the arguments. In a similar way, appositive re-
lations could be represented using an APPOSITIVE
relation (as is currently done with English). In the
case of bunsetsu, the task is to choose a head and
label the other constituents?This is very similar to
our task of labeling and subdividing the flat noun
phrases of the English Penn Treebank. Conjunction
is a little different because the K analysis assumes
that the final conjunct is the functor, rather than a
conjunction. We automatically changed this analy-
sis to be the same as it is for English and Chinese.
When there was no actual conjunction, we created a
theory-internal NULL conjunction. The final stages
include: (1) processing conjunction and apposition,
including recognizing cases that the parser does not
recognize; (2) correcting parts of speech; (3) label-
ing all relations between arguments and heads; (4)
recognizing and labeling special constituent types
Figure 6: It is the state?s duty to protect lives and assets.
Type Prec Rec F
Aggr 764843 = 91.0%
764
840 = 90.6% 90.8%
Aver 90.7% 90.6% 90.6%
Table 4: 40 Japanese Sentences from JENAA Corpus:
Aggregate and Average Sentence Scores
such as Named Entities, double quote constituents
and number phrases (twenty one); (5) handling com-
mon idioms; and (6) processing light verb and cop-
ula constructions.
Figure 6 is a sample 4-tuple for a Japanese
sentence meaning It is the state?s duty to protect
lives and assets. Conjunction is handled as dis-
cussed above, using an invisible NULL conjunction
and transparent (asterisked) logical CONJ relations.
Copulas in all three languages take surface subjects,
which are the LOGIC1 subjects of the PRD argu-
ment of the copula. We have left out glosses for the
particles, which act solely as case markers and help
us identify the grammatical relation.
We scored Japanese GLARF on forty sentences of
the Japanese side of the JENAA data (25 of which
are parallel with the English sentences scored). Like
the English, the F score is very close to the parsing
scores achieved by the parser.
152
6 Concluding Remarks and Future Work
In this paper, we have described three systems
for generating GLARF representations automati-
cally from text, each system combines the out-
put of a parser and possibly some other processor
(segmenter, Named Entity Recognizer, PropBanker,
etc.) and creates a logical representation of the sen-
tence. Dictionaries, word lists, and various other
resources are used, in conjunction with hand writ-
ten rules. In each case, the results are very close to
parsing accuracy. These logical structures are in the
same annotation framework, using the same labeling
scheme and the same analysis for key types of con-
structions. There are several advantages to our ap-
proach over other characterizations of logical struc-
ture: (1) our representation is among the most accu-
rate and reliable; (2) our representation connects all
the words in the sentence; and (3) having the same
representation for multiple languages facilitates run-
ning the same procedures in multiple languages and
creating multilingual applications.
The English system was developed for the News
genre, specifically the Penn Treebank Wall Street
Journal Corpus. We are therefore considering
adding rules to better handle constructions that ap-
pear in other genres, but not news. The experi-
ments describe here should go a long way towards
achieving this goal. We are also considering ex-
periments with parsers tailored to particular genres
and/or parsers that add function tags (Harper et al,
2005). In addition, our current GLARF system uses
internal Propbank/NomBank rules, which have good
precision, but low recall. We expect that we achieve
better results if we incorporate the output of state
of the art SRL systems, although we would have to
conduct experiments as to whether or not we can im-
prove such results with additional rules.
We developed the English system over the course
of eight years or so. In contrast, the Chinese and
Japanese systems are newer and considerably less
time was spent developing them. Thus they cur-
rently do not represent as many regularizations. One
obstacle is that we do not currently use subcate-
gorization dictionaries for either language, while
we have several for English. In particular, these
would be helpful in predicting and filling relative
clause and others gaps. We are considering auto-
matically acquiring simple dictionaries by recording
frequently occurring argument types of verbs over
a larger corpus, e.g., along the lines of (Kawahara
and Kurohashi, 2002). In addition, existing Japanese
dictionaries such as the IPAL (monolingual) dictio-
nary (technology Promotion Agency, 1987) or previ-
ously acquired case information reported in (Kawa-
hara and Kurohashi, 2002).
Finally, we are investigating several avenues for
using this system output for Machine Translation
(MT) including: (1) aiding word alignment for other
MT system (Wang et al, 2007); and (2) aiding the
creation various MT models involving analyzed text,
e.g., (Gildea, 2004; Shen et al, 2008).
Acknowledgments
This work was supported by NSF Grant IIS-
0534700 Structure Alignment-based MT.
References
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The
Berkeley FrameNet Project. In Coling-ACL98, pages
86?90.
E. Charniak. 2001. Immediate-head parsing for language
models. In ACL 2001, pages 116?123.
N. Chomsky. 1957. Syntactic Structures. Mouton, The
Hague.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
J. Foster and J. van Genabith. 2008. Parser Evaluation
and the BNC: 4 Parsers and 3 Evaluation Metrics. In
LREC 2008, Marrakech, Morocco.
R. Gabbard, M. Marcus, and S. Kulick. 2006. Fully pars-
ing the penn treebank. In NAACL/HLT, pages 184?
191.
D. Gildea. 2004. Dependencies vs. Constituents for
Tree-Based Alignment. In EMNLP, Barcelona.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,
M. A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?,
J. ?Ste?pa?nek, P. Stran?a?k, M. Surdeanu, N. Xue, and
Y. Zhang. 2009. The CoNLL-2009 shared task:
Syntactic and semantic dependencies in multiple lan-
guages. In CoNLL-2009, Boulder, Colorado, USA.
M. Harper and Z. Huang. Forthcoming. Chinese Statis-
tical Parsing. In J. Olive, editor, Global Autonomous
Language Exploitation. Publisher to be Announced.
M. Harper, B. Dorr, J. Hale, B. Roark, I. Shafran,
M. Lease, Y. Liu, M. Snover, L. Yung, A. Krasnyan-
skaya, and R. Stewart. 2005. Parsing and Spoken
153
Structural Event. Technical Report, The John-Hopkins
University, 2005 Summer Research Workshop.
Z. Harris. 1968. Mathematical Structures of Language.
Wiley-Interscience, New York.
J. R. Hobbs and R. Grishman. 1976. The Automatic
Transformational Analysis of English Sentences: An
Implementation. International Journal of Computer
Mathematics, 5:267?283.
H. Ji and R. Grishman. 2006. Analysis and Repair of
Name Tagger Errors. In COLING/ACL 2006, Sydney,
Australia.
K. Parton and K. R. McKeown and R. Coyne and M. Diab
and R. Grishman and D. Hakkani-Tu?r and M. Harper
and H. Ji and W. Y. Ma and A. Meyers and S. Stol-
bach and A. Sun and G. Tu?r and W. Xu and S. Yarman.
2009. Who, What, When, Where, Why? Comparing
Multiple Approaches to the Cross-Lingual 5W Task.
In ACL 2009.
D. Kawahara and S. Kurohashi. 2002. Fertilization
of Case Frame Dictionary for Robust Japanese Case
Analysis. In Proc. of COLING 2002.
S. Kurohashi and M. Nagao. 1998. Building a Japanese
parsed corpus while improving the parsing system. In
Proceedings of The 1st International Conference on
Language Resources & Evaluation, pages 719?724.
S. Kurohashi, T. Nakamura, Y. Matsumoto, and M. Na-
gao. 1994. Improvements of Japanese Morpholog-
ical Analyzer JUMAN. In Proc. of International
Workshop on Sharable Natural Language Resources
(SNLR), pages 22?28.
C. Macleod, R. Grishman, and A. Meyers. 1998. COM-
LEX Syntax. Computers and the Humanities, 31:459?
481.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313?330.
A. Meyers, M. Kosaka, S. Sekine, R. Grishman, and
S. Zhao. 2001. Parsing and GLARFing. In Proceed-
ings of RANLP-2001, Tzigov Chark, Bulgaria.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004a. The Nom-
Bank Project: An Interim Report. In NAACL/HLT
2004 Workshop Frontiers in Corpus Annotation,
Boston.
A. Meyers, R. Reeves, Catherine Macleod, Rachel Szeke-
ley, Veronkia Zielinska, and Brian Young. 2004b. The
Cross-Breeding of Dictionaries. In Proceedings of
LREC-2004, Lisbon, Portugal.
A. Meyers, N. Ide, L. Denoyer, and Y. Shinyama. 2007.
The shared corpora working group report. In Pro-
ceedings of The Linguistic Annotation Workshop, ACL
2007, pages 184?190, Prague, Czech Republic.
A. Meyers. 2008. Using treebank, dictionaries and
glarf to improve nombank annotation. In Proceedings
of The Linguistic Annotation Workshop, LREC 2008,
Marrakesh, Morocco.
E. Miltsakaki, A. Joshi, R. Prasad, and B. Webber. 2004.
Annotating discourse connectives and their arguments.
In A. Meyers, editor, NAACL/HLT 2004 Workshop:
Frontiers in Corpus Annotation, pages 9?16, Boston,
Massachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
T. Noro, C. Koike, T. Hashimoto, T. Tokunaga, and
Hozumi Tanaka. 2005. Evaluation of a Japanese CFG
Derived from a Syntactically Annotated corpus with
Respect to Dependency Measures. In 2005 Workshop
on Treebanks and Linguistic theories, pages 115?126.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
J. Pustejovsky, A. Meyers, M. Palmer, and M. Poe-
sio. 2005. Merging PropBank, NomBank, TimeBank,
Penn Discourse Treebank and Coreference. In ACL
2005 Workshop: Frontiers in Corpus Annotation II:
Pie in the Sky.
L. Shen, J. Xu, and R. Weischedel. 2008. A New String-
to-Dependency Machine Translation Algorithm with a
Target Dependency Language Model. In ACL 2008.
Y. Shinyama. 2007. Being Lazy and Preemptive at
Learning toward Information Extraction. Ph.D. the-
sis, NYU.
M. Surdeanu, R. Johansson, A. Meyers, Ll. Ma?rquez,
and J. Nivre. 2008. The CoNLL-2008 Shared Task
on Joint Parsing of Syntactic and Semantic Dependen-
cies. In Proceedings of the CoNLL-2008 Shared Task,
Manchester, GB.
Information technology Promotion Agency. 1987. IPA
Lexicon of the Japanese Language for Computers
IPAL (Basic Verbs). (in Japanese).
M. Utiyama and H. Isahara. 2003. Reliable Measures
for Aligning Japanese-English News Articles and Sen-
tences. In ACL-2003, pages 72?79.
J. Wagner, D. Seddah, J. Foster, and J. van Genabith.
2007. C-Structures and F-Structures for the British
National Corpus. In Proceedings of the Twelfth In-
ternational Lexical Functional Grammar Conference,
Stanford. CSLI Publications.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese syn-
tactic reordering for statistical machine translation. In
EMNLP-CoNLL 2007, pages 737?745.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
Penn Chinese TreeBank: Phrase Structure Annotation
of a Large Corpus. Natural Language Engineering,
11:207?238.
N. Xue. 2008. Labeling Chinese Predicates with Seman-
tic roles. Computational Linguistics, 34:225?255.
154
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 116?120,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Transducing Logical Relations from Automatic and Manual GLARF
Adam Meyers?, Michiko Kosaka?, Heng Ji?, Nianwen Xue?,
Mary Harper?, Ang Sun?, Wei Xu? and Shasha Liao?
? New York Univ., ?Monmouth Univ., ?Brandeis Univ,, ?City Univ. of New York, ?Johns
Hopkins Human Lang. Tech. Ctr. of Excellence & U. of Maryland, College Park
Abstract
GLARF relations are generated from tree-
bank and parses for English, Chinese and
Japanese. Our evaluation of system out-
put for these input types requires consid-
eration of multiple correct answers.1
1 Introduction
Systems, such as treebank-based parsers (Char-
niak, 2001; Collins, 1999) and semantic role la-
belers (Gildea and Jurafsky, 2002; Xue, 2008), are
trained and tested on hand-annotated data. Evalu-
ation is based on differences between system out-
put and test data. Other systems use these pro-
grams to perform tasks unrelated to the original
annotation. For example, participating systems in
CONLL (Surdeanu et al, 2008; Hajic? et al, 2009),
ACE and GALE tasks merged the results of sev-
eral processors (parsers, named entity recognizers,
etc.) not initially designed for the task at hand.
This paper discusses differences between hand-
annotated data and automatically generated data
with respect to our GLARFers, systems for gen-
erating Grammatical and Logical Representation
Framework (GLARF) for English, Chinese and
Japanese sentences. The paper describes GLARF
(Meyers et al, 2001; Meyers et al, 2009) and
GLARFers and compares GLARF produced from
treebank and parses.
2 GLARF
Figure 1 includes simplified GLARF analyses for
English, Chinese and Japanese sentences. For
each sentence, a GLARFer constructs both a Fea-
ture Structure (FS) representing a constituency
analysis and a set of 31-tuples, each representing
1Support includes: NSF IIS-0534700 & IIS-0534325
Structure Alignment-based MT; DARPA HR0011-06-C-
0023 & HR0011-06-C-0023; CUNY REP & GRTI Program.
This work does not necessarily reflect views of sponsors.
up to three dependency relations between pairs of
words. Due to space limitations, we will focus on
the 6 fields of the 31-tuple represented in Figure 1.
These include: (1) a functor (func); (2) the de-
pending argument (Arg); (3) a surface (Surf) la-
bel based on the position in the parse tree with no
regularizations; (4) a logic1 label (L
?
1) for a re-
lation that reflects grammar-based regularizations
of the surface level. This marks relations for fill-
ing gaps in relative clauses or missing infinitival
subjects, represents passives as paraphrases as ac-
tives, etc. While the general framework supports
many regularizations, the relations actually repre-
sented depends on the implemented grammar, e.g.,
our current grammar of English regularizes across
passives and relative clauses, but our grammars
of Japanese and Chinese do not currently.; (5) a
logic2 label (L2) for Chinese and English, which
represents PropBank, NomBank and Penn Dis-
course Treebank relations; and (6) Asterisks (*)
indicate transparent relations, relations where the
functor inherits semantic properties of certain spe-
cial arguments (*CONJ, *OBJ, *PRD, *COMP).
Figure 1 contains several transparent relations.
The interpretation of the *CONJ relations in the
Japanese example, include not only that the nouns
[zaisan] (assets) and [seimei] (lives) are con-
joined, but also that these two nouns, together
form the object of the Japanese verb [mamoru]
(protect). Thus, for example, semantic selection
patterns should treat these nouns as possible ob-
jects for this verb. Transparent relations may serve
to neutralize some of the problematic cases of at-
tachment ambiguity. For example, in the English
sentence A number of phrases with modifiers are
not ambiguous, there is a transparent *COMP re-
lation between numbers and of and a transpar-
ent *OBJ relation between of and phrases. Thus,
high attachment of the PP with modifiers, would
have the same interpretation as low attachment
since phrases is the underlying head of number of
116
Figure 1: GLARF 5-tuples for 3 languages
phrases. In this same example, the adverb not can
be attached to either the copula are or the pred-
icative adjective, with no discernible difference in
meaning?this factor is indicated by the transparent
designation of the relations where the copula is a
functor. Transparent features also provide us with
a simple way of handling certain function words,
such as the Chinese word De which inherits the
function of its underlying head, connecting a vari-
ety of such modifiers to head nouns (an adjective
in the Chinese example.). For conjunction cases,
the number of underlying relations would multi-
ply, e.g., Mary and John bought and sold stock
would (underlyingly) have four subject relations
derived by pairing each of the underlying subject
nouns Mary and John with each of the underlying
main predicate verbs bought and sold.
3 Automatic vs. Manual Annotation
Apart from accuracy, there are several other ways
that automatic and manual annotation differs. For
Penn-treebank (PTB) parsing, for example, most
parsers (not all) leave out function tags and empty
categories. Consistency is an important goal for
manual annotation for many reasons including: (1)
in the absence of a clear correct answer, consis-
tency helps clarify measures of annotation quality
(inter-annotator agreement scores); and (2) consis-
tent annotation is better training data for machine
learning. Thus, annotation specifications use de-
faults to ensure the consistent handling of spurious
ambiguity. For example, given a sentence like I
bought three acres of land in California, the PP in
California can be attached to either acres or land
with no difference in meaning. While annotation
guidelines may direct a human annotator to prefer,
for example, high attachment, systems output may
have other preferences, e.g., the probability that
land is modified by a PP (headed by in) versus the
probability that acres can be so modified.
Even if the manual annotation for a particular
corpus is consistent when it comes to other factors
such as tokenization or part of speech, developers
of parsers sometimes change these guidelines to
suit their needs. For example, users of the Char-
niak parser (Charniak, 2001) should add the AUX
category to the PTB parts of speech and adjust
their systems to account for the conversion of the
word ain?t into the tokens IS and n?t. Similarly, to-
kenization decisions with respect to hyphens vary
among different versions of the Penn Treebank, as
well as different parsers based on these treebanks.
Thus if a system uses multiple parsers, such differ-
ences must be accounted for. Differences that are
not important for a particular application should
be ignored (e.g., by merging alternative analyses).
For example, in the case of spurious attachment
ambiguity, a system may need to either accept both
as right answers or derive a common representa-
tion for both. Of course, many of the particular
problems that result from spurious ambiguity can
be accounted for in hind sight. Nevertheless, it
is precisely this lack of a controlled environment
which adds elements of spurious ambiguity. Us-
ing new processors or training on new treebanks
can bring new instances of spurious ambiguity.
4 Experiments and Evaluation
We ran GLARFers on both manually created tree-
banks and automatically produced parses for En-
glish, Chinese and Japanese. For each corpus, we
created one or more answer keys by correcting
117
system output. For this paper, we evaluate solely
on the logic1 relations (the second column in fig-
ure 1.) Figure 2 lists our results for all three lan-
guages, based on treebank and parser input.
As in (Meyers et al, 2009), we generated 4-
tuples consisting of the following for each depen-
dency: (A) the logic1 label (SBJ, OBJ, etc.), (B)
its transparency (True or False), (C) The functor (a
single word or a named entity); and (D) the argu-
ment (a single word or a named entity). In the case
of conjunction where there was no lexical con-
junction word, we used either punctuation (com-
mas or semi-colons) or the placeholder *NULL*.
We then corrected these results by hand to produce
the answer key?an answer was correct if all four
members of the tuple were correct and incorrect
otherwise. Table 2 provides the Precision, Recall
and F-scores for our output. The F-T columns
indicates a modified F-score derived by ignoring
the +/-Transparent distinction (resulting changes
in precision, recall and F-score are the same).
For English and Japanese, an expert native
speaking linguist corrected the output. For Chi-
nese, several native speaking computational lin-
guists shared the task. By checking compatibil-
ity of the answer keys with outputs derived from
different sources (parser, treebank), we could de-
tect errors and inconsistencies. We processed the
following corpora. English: 86 sentence article
(wsj 2300) from the Wall Street Journal PTB test
corpus (WSJ); 46 sentence letter from Good Will
(LET), the first 100 sentences of a switchboard
telephone transcript (TEL) and the first 100 sen-
tences of a narrative from the Charlotte Narra-
tive and Conversation (NAR). These samples are
taken from the PTB WSJ Corpus and the SIGANN
shared subcorpus of the OANC. The filenames are:
110CYL067, NapierDianne and sw2014. Chi-
nese: a 20 sentence sample of text from the
Penn Chinese Treebank (CTB) (Xue et al, 2005).
Japanese: 20 sentences from the Kyoto Corpus
(KYO) (Kurohashi and Nagao, 1998)
5 Running the GLARFer Programs
We use Charniak, UMD and KNP parsers (Char-
niak, 2001; Huang and Harper, 2009; Kurohashi
and Nagao, 1998), JET Named Entity tagger (Gr-
ishman et al, 2005; Ji and Grishman, 2006)
and other resources in conjunction with language-
specific GLARFers that incorporate hand-written
rules to convert output of these processors into
a final representation, including logic1 struc-
ture, the focus of this paper. English GLAR-
Fer rules use Comlex (Macleod et al, 1998a)
and the various NomBank lexicons (http://
nlp.cs.nyu.edu/meyers/nombank/) for
lexical lookup. The GLARF rules implemented
vary by language as follows. English: cor-
recting/standardizing phrase boundaries and part
of speech (POS); recognizing multiword expres-
sions; marking subconstituents; labeling rela-
tions; incorporating NEs; regularizing infiniti-
val, passives, relatives, VP deletion, predica-
tive and numerous other constructions. Chi-
nese: correcting/standardizing phrase boundaries
and POS, marking subconstituents, labeling rela-
tions; regularizing copula constructions; incorpo-
rating NEs; recognizing dates and number expres-
sions. Japanese: converting to PTB format; cor-
recting/standardizing phrase boundaries and POS;
labeling relations; processing NEs, double quote
constructions, number phrases, common idioms,
light verbs and copula constructions.
6 Discussion
Naturally, the treebank-based system out-
performed parse-based system. The Charniak
parser for English was trained on the Wall Street
Journal corpus and can achieve about 90% accu-
racy on similar corpora, but lower accuracy on
other genres. Differences between treebank and
parser results for English were higher for LET and
NAR genres than for the TEL because the system
is not currently designed to handle TEL-specific
features like disfluencies. All processors were
trained on or initially designed for news corpora.
Thus corpora out of this domain usually produce
lower results. LET was easier as it consisted
mainly of short simple sentences. In (Meyers et
al., 2009), we evaluated our results on 40 Japanese
sentences from the JENAAD corpus (Utiyama
and Isahara, 2003) and achieved a higher F-score
(90.6%) relative to the Kyoto corpus, as JENAAD
tends to have fewer long complex sentences.
By using our answer key for multiple inputs, we
discovered errors and consequently improved the
quality of the answer keys. However, at times we
were also compelled to fork the answer keys?given
multiple correct answers, we needed to allow dif-
ferent answer keys corresponding to different in-
puts. For English, these items represent approxi-
mately 2% of the answer keys (there were a total
118
Treebank Parser
ID % Prec % Rec F F-T % Prec % Rec F F-T
WSJ 12381491 = 83.0
1238
1471 = 84.2 83.6 87.1
1164
1452 = 80.2
1164
1475 = 78.9 79.5 81.8
LET 419451 = 92.9
419
454 = 92.3 92.6 93.3
390
434 = 89.9
390
454 = 85.9 87.8 87.8
TEL 478627 = 76.2
478
589 = 81.2 78.6 82.2
439
587 = 74.8
439
589 = 74.5 74.7 77.4
NAR 8171013 = 80.7
817
973 =84.0 82.3 84.1
724
957 = 75.7
724
969 = 74.7 75.2 76.1
CTB 351400 = 87.8
351
394 = 89.1 88.4 88.7
352
403 = 87.3
352
438 = 80.4 83.7 83.7
KYO 525575 = 91.3
525
577 = 91.0 91.1 91.1
493
581 = 84.9
493
572 = 86.2 85.5 87.8
Figure 2: Logic1 Scores
Figure 3: Examples of Answer Key Divergences
of 74 4-tuples out of a total of 3487). Figure 3 lists
examples of answer key divergences that we have
found: (1) alternative tokenizations; (2) spurious
differences in attachment and conjunction scope;
and (3) ambiguities specific to our framework.
Examples 1 and 2 reflect different treatments of
hyphenation and contractions in treebank specifi-
cations over time. Parsers trained on different tree-
banks will either keep hyphenated words together
or separate more words at hyphens. The Treebank
treatment of can?t regularizes so that (can need
not be differentiated from ca), whereas the parser
treatment makes maintaining character offsets eas-
ier. In example 3, the Japanese parser recognizes
a single word whereas the treebank divides it into
a prefix plus stem. Example 4 is a case of differ-
ences in character encoding (zero).
Example 5 is a common case of spurious attach-
ment ambiguity for English, where a transparent
noun takes an of PP complement?nouns such as
form, variety and thousands bear the feature trans-
parent in the NOMLEX-PLUS dictionary (a Nom-
Bank dictionary based on NOMLEX (Macleod et
al., 1998b)). The relative clause attaches either
to the noun thousands or people and, therefore,
the subject gap of the relative is filled by either
thousands or people. This ambiguity is spurious
since there is no meaningful distinction between
these two attachments. Example 6 is a case of
attachment ambiguity due to a support construc-
tion (Meyers et al, 2004). The recipient of the
gift will be Goodwill regardless of whether the
PP is attached to give or gift. Thus there is not
much sense in marking one attachment more cor-
rect than the other. Example 7 is a case of conjunc-
tion ambiguity?the context does not make it clear
whether or not the pearls are part of a necklace or
just the beads are. The distinction is of little con-
sequence to the understanding of the narrative.
Example 8 is a case in which our grammar han-
dles a case ambiguously: the prenominal adjective
can be analyzed either as a simple noun plus ad-
jective phrase meaning various businesses or as a
noun plus relative clause meaning businesses that
are varied. Example 9 is a common case in Chi-
nese where the verb/noun distinction, while un-
clear, is not crucial to the meaning of the phrase ?
under either interpretation, 5 billion was exported.
7 Concluding Remarks
We have discussed challenges of automatic an-
notation when transducers of other annotation
schemata are used as input. Models underly-
ing different transducers approximate the origi-
nal annotation in different ways, as do transduc-
ers trained on different corpora. We have found
it necessary to allow for multiple correct answers,
due to such differences, as well as, genuine and
spurious ambiguities. In the future, we intend to
investigate automatic ways of identifying and han-
dling spurious ambiguities which are predictable,
including examples like 5,6 and 7 in figure 3 in-
volving transparent functors.
119
References
E. Charniak. 2001. Immediate-head parsing for lan-
guage models. In ACL 2001, pages 116?123.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
D. Gildea and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics,
28:245?288.
R. Grishman, D. Westbrook, and A. Meyers. 2005.
Nyu?s english ace 2005 system description. In ACE
2005 Evaluation Workshop.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,
M. A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre,
S. Pado?, J. ?Ste?pa?nek, P. Stran?a?k, M. Surdeanu,
N. Xue, and Y. Zhang. 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In CoNLL-2009, Boulder, Col-
orado, USA.
Z. Huang and M. Harper. 2009. Self-training PCFG
Grammars with Latent Annotations across Lan-
guages. In EMNLP 2009.
H. Ji and R. Grishman. 2006. Analysis and Repair of
Name Tagger Errors. In COLING/ACL 2006, Syd-
ney, Australia.
S. Kurohashi and M. Nagao. 1998. Building a
Japanese parsed corpus while improving the pars-
ing system. In Proceedings of The 1st International
Conference on Language Resources & Evaluation,
pages 719?724.
C. Macleod, R. Grishman, and A. Meyers. 1998a.
COMLEX Syntax. Computers and the Humanities,
31:459?481.
C. Macleod, R. Grishman, A. Meyers, L. Barrett, and
R. Reeves. 1998b. Nomlex: A lexicon of nominal-
izations. In Proceedings of Euralex98.
A. Meyers, M. Kosaka, S. Sekine, R. Grishman, and
S. Zhao. 2001. Parsing and GLARFing. In Pro-
ceedings of RANLP-2001, Tzigov Chark, Bulgaria.
A. Meyers, R. Reeves, and Catherine Macleod. 2004.
NP-External Arguments: A Study of Argument
Sharing in English. In The ACL 2004 Workshop
on Multiword Expressions: Integrating Processing,
Barcelona, Spain.
A. Meyers, M. Kosaka, N. Xue, H. Ji, A. Sun, S. Liao,
and W. Xu. 2009. Automatic Recognition of Log-
ical Relations for English, Chinese and Japanese in
the GLARF Framework. In SEW-2009 at NAACL-
HLT-2009.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma?rquez,
and J. Nivre. 2008. The CoNLL-2008 Shared Task
on Joint Parsing of Syntactic and Semantic Depen-
dencies. In Proceedings of the CoNLL-2008 Shared
Task, Manchester, GB.
M. Utiyama and H. Isahara. 2003. Reliable Mea-
sures for Aligning Japanese-English News Articles
and Sentences. In ACL-2003, pages 72?79.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
Penn Chinese Treebank: Phrase Structure Annota-
tion of a Large Corpus. Natural Language Engi-
neering.
N. Xue. 2008. Labeling Chinese Predicates with Se-
mantic roles. Computational Linguistics, 34:225?
255.
120
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 121?124,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Using Parallel Propbanks to enhance Word-alignments
Jinho D. Choi
Dept. of Computer Science
Univ. of Colorado at Boulder
choijd@colorado.edu
Martha Palmer
Dept. of Linguistics
Univ. of Colorado at Boulder
mpalmer@colorado.edu
Nianwen Xue
Dept. of Computer Science
Brandeis University
xuen@brandeis.edu
Abstract
This short paper describes the use of the
linguistic annotation available in paral-
lel PropBanks (Chinese and English) for
the enhancement of automatically derived
word alignments. Specifically, we sug-
gest ways to refine and expand word
alignments for verb-predicates by using
predicate-argument structures. Evalua-
tions demonstrate improved alignment ac-
curacies that vary by corpus type.
1 Introduction
Since verbs tend to be the roots of dependency re-
lations in a sentence (Palmer et al, 2005), when it
comes down to translations, finding correct map-
pings between verbs in a source and a target lan-
guage is very important. Many machine transla-
tion systems (Fraser and Marcu, 2007) use word-
alignment tools such as GIZA++ (Och and Ney,
2003) to retrieve word mappings between a source
and a target language. Although GIZA++ gives
well-structured alignments, it has limitations in
several ways. First, it is hard to verify if align-
ments generated by GIZA++ are correct. Second,
GIZA++ may not find alignments for low-frequent
words. Third, GIZA++ does not account for any
semantic information.
In this paper, we suggest a couple of ways to
enhance word-alignments for predicating expres-
sions such as verbs1. We restricted the source
and the target language to Chinese and English,
respectively. The goal is to use the linguistic
annotation available in parallel PropBanks (Xue
and Palmer, 2009) to refine and expand automatic
word-alignments. First, we check if the alignment
for each Chinese predicate, generated by GIZA++,
is also a predicate in English (Section 3). If it is,
we verify if the alignment is correct by matching
1Throughout the paper, all predicates refer to verbs.
their arguments (Section 4.1). If it is not, we find
an English predicate that has the maximum argu-
ment matching with the Chinese predicate (Sec-
tion 4.2). Finally, we evaluate the potential of the
enhanced word-alignments for providing a signif-
icant improvement over the GIZA++ baseline.
2 Parallel Corpus
We used the ?English Chinese Translation Tree-
bank? (ECTB), a parallel English-Chinese cor-
pus. In addition to the treebank syntactic struc-
ture, the corpus has also been annotated with
semantic role labels in the standard PropBank
style of Arg0, Arg1, etc., based on verb specific
frame file definitions (Xue and Palmer, 2009).
The corpus is divided into two parts: the Xin-
hua Chinese newswire with literal English trans-
lations (4,363 parallel sentences) and the Sino-
rama Chinese news magazine with non-literal En-
glish translations (12,600 parallel sentences). We
experimented with the two parts separately to
see how literal and non-literal translations affect
word-alignments.
3 Predicate Matching
For preprocessing, we ran GIZA++ on ECTB to
get word-alignments between Chinese and En-
glish. Then, for each Chinese predicate, we
checked if it is aligned to an English predicate by
using the gold-standard parallel Propbanks. Ta-
ble 1 shows how many Chinese predicates were
aligned to what kind of English words.
Only (45.3%-Xinhua, 19.1%-Sinorama) of Chi-
nese predicates were aligned to words that are
predicates in English. It is true that not all Chi-
nese verbs are supposed to be translated to verbs
in English, but that does not account for the num-
bers in Table 1. We therefore assume that there
are opportunities to enhance word-alignments for
Chinese and English predicates.
121
Alignment Xinhua Sinorama
Ch.pred? En.pred 5,842 7,643
Ch.pred? En.be 386 1,229
Ch.pred? En.else 2,489 8,726
Ch.pred? En.none 4,178 22,488
Total 12,895 40,086
Table 1: Results of predicate matching (Ch: Chi-
nese, En: English, pred: predicates, be: be-verbs,
else: non-verbs, none: no word). The numbers in-
dicate the amount of verb-tokens, not verb-types.
4 Argument Matching
For Chinese predicates aligned to English predi-
cates, we can verify the alignments by ?Top-down
argument matching?: given Chinese and English
predicates that are aligned, check if their argu-
ments are also aligned (arguments are found from
parallel Propbanks). The intuition is that if the
predicates are correctly aligned across the lan-
guages, their arguments should be aligned as well.
For Chinese predicates not aligned to any En-
glish words, we can find their potential English
alignments by ?Bottom-up argument matching?:
given a set of arguments for a such Chinese predi-
cate, find some English predicate whose set of ar-
guments has the most words aligned to words in
the Chinese arguments. If the words in the argu-
ments are mostly aligned (above a certain thresh-
old) across the languages, we suspect that the
predicates should be aligned as well.
4.1 Top-down Argument Matching (T-D)
Given a Chinese predicate pc aligned to an English
predicate pe, let Sc and Se be a set of arguments
for pc and pe, respectively. For each cai ? Sc, we
match it with some eaj ? Se that has the most
words aligned to words in cai. If such eaj ex-
ists, we count the number of aligned words, say
|cai ? eaj |; otherwise, the count is 0. Once the
matchings are done, we average the proportions
of the counts and if the average is above a certain
threshold, we consider the alignment is correct.
Let us look at the example in Table 2. Af-
ter the preprocessing, a Chinese predicate ????
is aligned to an English predicate ?set up? by
GIZA++. ???? has two arguments, Ch.Arg0 and
Ch.Arg1, retrieved from the Chinese Propbank.
For each Chinese argument, we search for some
argument of ?set? (from the English Propbank) that
? Chinese Sentence ?
: ?????????????????????
- Predicate: ??.01? set up
- Ch.Arg0: ????? those municipalities
- Ch.Arg1: ??????????
? fourteen border economic cooperation zones
? English Sentence ?
: At the same time it also sanctioned those municipalities
to set up fourteen border economic cooperation zones
- Predicate: set.03 (set up)
- En.Arg0: those municipalities
- En.Arg1: fourteen border economic cooperation zones
Table 2: Parallel sentences labelled with their se-
mantic roles
has the most words aligned. For instance, words
in Ch.Arg0, ??? ???, are aligned to ?those
municipalities? by GIZA++ so Ch.Arg0 finds
En.Arg0 as the one maximizes word-interscetions
(similar for Ch.Arg1 and En.Arg1). In this case,
the argument matchings for all pairs of arguments
are 100%, so we consider the alignment is correct.
Table 3 shows the average argument matching
scores for all pairs of Chinese and English predi-
cates. For each pair of predicates, ?macro-average?
measures the proportion of word-intersections for
each pair of Chinese and English arguments (with
the most words aligned) and averages the pro-
portions whereas ?micro-average? counts word-
intersections for all pairs of arguments (each pair
with the most words aligned) and divides it by the
total number of words in Chinese arguments.
? Sc = a set of Chinese arguments, cai ? Sc
? Se = a set of English arguments, eaj ? Se
? Macro average argument matching score
= 1|Sc|
?
?cai
(argmax(|cai ? eaj |)|cai| )
? Micro average argument matching score
=
?
?cai argmax(|cai ? eaj |)?
?cai |cai|
Xinhua Sinorama
Macro Avg. 80.55% 53.56%
Micro Avg. 83.91% 52.62%
Table 3: Average argument matching scores for
top-down argument matching
122
It is not surprising that Xinhua?s scores are
higher because the English sentences in Xinhua
are more literally translated than ones in Sinorama
so that it is easier to find correct alignments in Xin-
hua.
4.2 Bottom-Up Argument Matching (B-U)
A large portion of Chinese predicates are aligned
to no English words. For such Chinese predicate,
say pc, we check to see if there exists an English
predicate within the parallel sentence, say pe, that
is not aligned to any Chinese word and gives the
maximum micro-average score (Section 4.1) com-
pare to all other predicates in the English sen-
tence. If the micro-average score is above a certain
threshold, we align pc to pe.
The thresholds we used are 0.7 and 0.8. Thresh-
olds below 0.7 assumes too many alignments that
are incorrect and ones above 0.8 assumes too few
alignments to be useful. Table 4 shows the average
argument matching scores for alignments found by
bottom-up argument matching.
Xinhua Sinorama
Thresh. 0.7 0.8 0.7 0.8
Macro 80.74 83.99 77.70 82.86
Micro 82.63 86.46 79.45 85.07
Table 4: Average argument matching scores in
percentile for bottom-up argument matching
5 Evaluations
Evaluations are done by a Chinese-English bilin-
gual. We used a different English-Chinese paral-
lel corpus for evaluations. There are 100 paral-
lel sentences, 365 Chinese verb-tokens, and 273
Chinese verb-types in the corpus. We tested
word-alignments, refined and expanded by our ap-
proaches, on verb-types rather than verb-tokens
to avoid over-emphasizing multiple appearances
of a single type. Furthermore, we tested word-
alignments from Xinhua and Sinorama separately
to see how literal and non-literal translations affect
the outcomes.
5.1 Refining word-alignment
We used three kinds of measurements for compar-
isons: term coverage, term expansion, and align-
ment accuracy. ?Term coverage? shows how many
source terms (Chinese verb-types) are covered by
word-alignments found in each corpus. Out of
273 Chinese verb-types in the test corpus, (79-
Xinhua, 129-Sinorama) were covered by word-
alignments generated by GIZA++. ?Term expan-
sion? shows how many target terms (English verb-
types) are suggested for each of the covered source
terms. There are on average (1.77-Xinhua, 2.29-
Sinorama) English verb-types suggested for each
covered Chinese verb-type. ?Alignment accuracy?
shows how many of the suggested target terms are
correct. Among the suggested English verb-types,
(83.35%-Xinhua, 57.76%-Sinorama) were correct
on average.
The goal is to improve the alignment accu-
racy with minimum reduction of the term cov-
erage and expansion. To accomplish the goal,
we set a threshold for the T-D?s macro-average
score: for Chinese predicates aligned to English
predicates, we kept only alignments whose macro-
average scores meet or exceed a certain threshold.
The thresholds we chose are 0.4 and 0.5; lower
thresholds did not have much effect and higher
thresholds threw out too many alignments. Table 5
shows the results of three measurements with re-
spect to the thresholds (Note that all these align-
ments were generated by GIZA++).
Xinhua Sinorama
TH TC ATE AAA TC ATE AAA
0.0 79 1.77 83.35 129 2.29 57.76
0.4 76 1.72 83.54 93 1.8 65.88
0.5 76 1.68 83.71 62 1.58 78.09
Table 5: Results for alignment refinement (TH:
threshold, TC: term coverage, ATE: average term
expansion, AAA: average alignment accuracy in
percentage). The highest score for each measure-
ment is marked as bold.
As you can see, thresholds did not have much
effect on alignments found in Xinhua. This is
understandable because the translations in Xin-
hua are so literal that it was relatively easy for
GIZA++ to find correct alignments; in other
words, the alignments generated by GIZA++ were
already very accurate. However, for alignments
found in Sinorama, the average alignment accu-
racy increases radically as the threshold increases.
This implies that it is possible to refine word-
alignments found in a corpus containing many
non-literal translations by using T-D.
Notice that the term coverage for Sinorama de-
creases as the threshold increases. Considering
123
how much improvement it made for the average
alignment accuracy, we suspect that it filtered out
mostly ones that were incorrect alignments.
5.2 Expanding word-alignment
We used B-U to expand word-alignments for Chi-
nese predicates aligned to no English words. We
decided not to expand alignments for Chinese
predicates aligned to non-verb English words be-
cause GIZA++ generated alignments are more ac-
curate than ones found by B-U in general.
There are (22-Xinhua, 20-Sinorama) additional
verb-types covered by the expanded-alignments.
Note that these alignments are already filtered by
the micro-average score (Section 4.2). To refine
the alignments even more, we set a threshold on
the macro-average score as well. The thresholds
we used for the macro-average score are 0.6 and
0.7. Table 6 shows the results of the expanded-
alignments found in Xinhua and Sinorama.
Mac - 0.7 Mac - 0.8
TC ATE AAA TC ATE AAA
Mic Xinhua
0.0 22 4.27 50.38 20 3.35 57.50
0.6 21 3.9 54.76 18 3.39 63.89
0.7 19 3.47 55.26 17 3.12 61.76
Mic Sinorama
0.0 37 3.59 18.01 29 3.14 14.95
0.6 31 3.06 15.11 27 2.93 14.46
0.7 21 2.81 11.99 25 2.6 11.82
Table 6: Results for expanded-alignments found in
Xinhua and Sinorama (Mac: threshold on macro-
average score, Mic: threshold on micro-average
score)
The average alignment accuracy for Xinhua is
encouraging; it shows that B-U can expand word-
alignments for a corpus with literal translations.
The average alignment accuracy for Sinorama is
surprisingly low; it shows that B-U cannot func-
tion effectively given non-literal translations.
6 Summary and Future Works
We have demonstrated the potential for using par-
allel Propbanks to improve statistical verb transla-
tions from Chinese to English. Our B-U approach
shows promise for expanding the term-coverage
of GIZA++ alignments that are based on literal
translations. In contrast, our T-D is most effec-
tive with non-literal translations for verifying the
alignment accuracy, which has been proven diffi-
cult for GIZA++.
This is still a preliminary work but in the fu-
ture, we will try to enhance word-alignments
by using automatically labelled Propbanks, Nom-
banks (Meyers et al, 2004), Named-entity tag-
ging, and test the enhancement on bigger corpora.
Furthermore, we will also evaluate the integration
of our enhanced alignments with statistical ma-
chine translation systems.
Acknowledgments
Special thanks to Daniel Gildea, Ding Liu
(University of Rochester) who provided word-
alignments, Wei Wang (Information Sciences In-
stitute at University of Southern California) who
provided the test-corpus, and Hua Zhong (Uni-
versity of Colorado at Boulder) who performed
the evaluations. We gratefully acknowledge
the support of the National Science Foundation
Grants IIS-0325646, Domain Independent Seman-
tic Parsing, CISE-CRI-0551615, Towards a Com-
prehensive Linguistic Annotation, and a grant
from the Defense Advanced Research Projects
Agency (DARPA/IPTO) under the GALE pro-
gram, DARPA/CMO Contract No. HR0011-06-
C-0022, subcontract from BBN, Inc. Any contents
expressed in this material are those of the authors
and do not necessarily reflect the views of the Na-
tional Science Foundation.
References
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Computational Linguistics, 33(3):293?303.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The nombank project: An interim report. In HLT-
NAACL 2004 Workshop: Frontiers in Corpus Anno-
tation, pages 24?31.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Nianwen Xue and Martha Palmer. 2009. Adding se-
mantic roles to the chinese treebank. Natural Lan-
guage Engineering, 15(1):143?172.
124
Coling 2010: Poster Volume, pages 1363?1372,
Beijing, August 2010
Applying Syntactic, Semantic and Discourse Constraints in Chinese
Temporal Annotation
Nianwen Xue
Brandeis University
xuen@brandeis.edu
Yuping Zhou
Brandeis University
yzhou@brandeis.edu
Abstract
We describe a Chinese temporal annota-
tion experiment that produced a sizable
data set for the TempEval-2 evaluation
campaign. We show that while we have
achieved high inter-annotator agreement
for simpler tasks such as identification of
events and time expressions, temporal re-
lation annotation proves to be much more
challenging. We show that in order to im-
prove the inter-annotator agreement it is
important to strategically select the anno-
tation targets, and the selection of annota-
tion targets should be subject to syntactic,
semantic and discourse constraints.
1 Introduction
Event-based temporal inference is a fundamen-
tal natural language technology that attempts to
determine the temporal location of an event as
well as the temporal ordering between events. It
supports a wide range of natural language appli-
cations such as Information Extraction, Question
Answering and Text Summarization. For some
genres of text (such as news), a temporal order-
ing of events can be the most informative summa-
rization of a document (Mani and Wilson, 2000;
Filatova and Hovy, 2001). Temporal inference
is especially important for multi-document sum-
marization where events extracted from multiple
documents need to be put in a chronological or-
der (Lin and Hovy, 2001; Barzilay et al, 2002)
to make logical sense. Event-based temporal in-
ference is also necessary for Question Answer-
ing (Harabagiu and Bejan, 2005; Harabagiu and
Bejan, 2006). For example, to answer ?When
was Beijing Olympics held??, events extracted
from natural language text have to be associated
with a temporal location, whereas to answer ?how
many terrorists have been caught since 9/11??,
temporal ordering of multiple events is the pre-
requisite. Event-based temporal inference has
also been studied extensively in the context of
Information Extraction, which typically involves
extracting unstructured information from natural
language sources and putting them into a struc-
tured database for querying or other forms of in-
formation access. For event extraction, this means
extracting the event participants as well as its tem-
poral location. Generally, an event has to occur in
a specific time and space, and the temporal loca-
tion of an event provides the necessary context for
accurately understanding that event.
Being able to infer the temporal location of an
event in Chinese text has many additional applica-
tions. Besides Information Extraction, Question
Answering and Text Summarization, knowing the
temporal location of an event is also highly valu-
able to Machine Translation. To translate a lan-
guage like Chinese into a language like English
in which tense is grammatically marked with in-
flectional morphemes, an MT system will have
to infer the necessary temporal information to
determine the correct tense for verbs. Statisti-
cal MT systems, the currently dominant research
paradigm, typically do not address this issue di-
rectly or even indirectly.
As machine learning approaches are gaining
dominance in computational linguistics and pro-
ducing state-of-the-art results in many areas, they
have in turn fueled the demand for large quan-
tities of human-annotated data of various types
1363
that machine learning algorithms can be trained
on and evaluated against. In the temporal in-
ference domain, this has led to the creation of
TimeBank (Pustejovsky et al, 2003), which is an-
notated based on the TimeML language (Puste-
jovsky et al, 2005). TimeML is becoming an ISO
standard for annotating events and time expres-
sions (ISO/TC 37/SC 4/WG 2, 2007). A version
of the TimeBank has been provided as a shared
public resource for TempEval-2007, the first tem-
poral evaluation campaign aimed at automatically
identifying temporal relations between events and
time expressions as well the temporal ordering be-
tween events.
In this paper, we report work for a Chinese tem-
poral annotation project as part of the 2010 multi-
lingual temporal evaluation campaign (TempEval-
2)1. Besides Chinese, TempEval-2 also includes
English, French, Italian, Korean and Spanish.
Our temporal annotation project is set up within
the confines of BAT2, a database-driven multilin-
gual temporal annotation tool that is also used
to support other TempEval-2 languages. The
TempEval-2 evaluation framework takes a divide-
and-conquer approach to temporal annotation.
With the eventual goal being the annotation of
temporal relations between events and between
events and time expressions, the TempEval-2 an-
notation consists of a series of event and temporal
annotation subtasks. The idea is that each of these
subtasks will be easier to annotate than the larger
task as a whole and is less demanding on the an-
notators. The hope is that this will lead to more
consistent annotation that will be easier to learn
for automatic systems as well.
The rest of the paper will be organized as fol-
lows. In Section 2, we briefly describe the seven
layers of annotation. In Section 3, we describe our
annotation procedure. In Section 4, we address a
major issue that arises from our annotation effort,
which is the question of how to select annotation
targets. Our experience, some positive and some
negative, shows that temporal annotation can be
carried out much more smoothly and with higher
quality when the right annotation targets are pre-
sented to the annotators. This is especially true
1http://www.timeml.org/tempeval2/
2http://www.timeml.org/site/bat
during the annotation of temporal relations be-
tween events and between events and time expres-
sions, which are more complex than simpler anno-
tation tasks such as identifying the events and time
expressions. Section 5 concludes our paper.
2 Layers of annotation
2.1 Events and time expressions
The ultimate goal for a temporal annotation
project is to determine the temporal relationship
between events, and between events and time ex-
pressions. In order to achieve that objective,
events and time expressions must be first iden-
tified. Specifically, this means marking up text
spans in a document that can be used to represent
the events and time expressions. Events in partic-
ular are abstract objects and a full description of
an event would include its participants and tempo-
ral and spatial location. The TempEval annotation
framework simplifies this by just marking a verb
or a noun that best represents an event. The verb
or noun can be considered as an ?event anchor?
that represents the most important aspect of the
event. This is illustrated in (1), where the verbs
?? (?attend?), ?? (?hold?) and the noun ?
? (?ceremony?) are marked as event anchors.
(1) ???
State Council
???
Vice Premier
???
Zou Jiahua
??
attend
?
ASP
??
today
??
hold
?
DE
??
commissioning
??
ribbon-cutting
??
ceremony
?
.
?Vice Premier Zou Jiahua of the State Coun-
cil attended today?s commissioning ribbon-
cutting ceremony?.
Once the text spans of event anchors are anno-
tated, these events are then annotated with a set of
attributes. The TempEval annotation framework
allows variations across languages in the number
of attributes one can define as well as the values
for these attributes. For example, in the English
annotation, one of the event attributes is grammat-
ical tense which can be read off the morphological
inflections of a verb. Chinese verbs, on the other
hand, are not inflected for tense. Instead, in the
1364
Chinese annotation, we have a more fully devel-
oped aspect attribute that has eight possible val-
ues: Actual, Experiential, Complementive, Delim-
itative, Progressive, Durative, Inceptive, and Con-
tinuative, largely based on the theoretical work of
Xiao and McEnery (2004).
The most important attribute for both English
and Chinese, however, is the Class attribute. The
values for this attribute include Reporting, As-
pectual, Perception, I-Action, I-State, State, and
Occurrence. The different values of the Class
attribute effectively constitute a classification of
events, and they are defined in the TimeML spec-
ification language (Pustejovsky et al, 2005).
The other building block in the TempEval anno-
tation framework is time expressions. Like events,
time expressions are marked with both text spans
and a set of attributes. The annotation of time
expressions is relatively straightforward, and we
follow the TimeML standards in our annotation
study. In TimeML, time expressions are formally
called TIMEX3s, and they have two obligatory at-
tributes: Type and Value. The value of Type is one
of time, date, duration or set. The Value attribute
is essentially a normalized time value based on
the TIDES standard for annotating time expres-
sions (Ferro et al, 2004). The normalization al-
lows easy comparison of time expression. For ex-
ample, there are three time expressions in (2), ?
????(?1992?),????? (?1996?) and?
? (?this year?). Note that even though ???
?? ? ????? (? 1992 to 1996?) forms
one duration, it is annotated as two time expres-
sions. All three time expressions in the sentence
are dates, and their normalized values are 1992,
1996, and 1997 respectively. To determine the
normalized value for ?? (?this year?), we need
to know the document creation time, and fortu-
nately this information is available in the meta-
data for the Chinese Treebank documents.
(2) ?????
1992
?
to
?????
1996
??
Shanghai
?? ?? ??
GDP
??
per year on average
??
grow
???????
14.2%
?
,
??
this year
?
DE
??
growth
??
speed
?
also
?
will
??
reach
?????
13%
??
above
?
?From 1992 to 1996, Shanghai?s GDP on av-
erage grows at14.2% per year. This year the
(GDP) growth will also reach above 13%.?
2.2 Temporal relations
Once the events and time expressions are in place,
we are in a position to annotate various temporal
relations that are defined over them. (Since events
and time expressions are entities that temporal re-
lation is defined upon, we will subsume them un-
der the cover term ?temporal entity? when conve-
nient.) The ultimate goal of temporal annotation
is to identify all temporal relations in text. This
goal cannot be achieved by manually annotating
temporal relation of all temporal entities for three
reasons. First, it is infeasible, given the number of
temporal entities in a typical document. Second,
it is unnecessary due to the transitive property of
certain types of temporal relation. For example, if
e1, e2 and e3 are all events, and if e1 is before e2,
and e2 is before e3, there is no need to also an-
notate the relation between e1 and e3. Third, the
result of annotating all temporal entity pairs does
not reflect the natural temporal relations that exist
in text. Verhagen et al (2009) found that a major
contributor to high inter-annotator disagreement
was hard-to-classify cases that annotators were in-
structed not to avoid. If a temporal relation is not
made clear in text, then it should not be present in
annotation.
Since it is infeasible, unnecessary and even
detrimental to manually annotate all possible rela-
tions between temporal entities, the question then
becomes one of selecting which temporal rela-
tions to annotate. The TempEval-2 evaluation
starts by annotating the following temporal rela-
tions, which it considers to be a priority:
1. between an event and a time expression
2. between an event and the document creation
time
3. between a subordinating event and its corre-
sponding subordinated event
1365
4. between a main event and its immediately
preceding main event
The TempEval-2 annotation uses six values for
all temporal relations, and they are Before, Before-
or-Overlap, Overlap, Overlap-or-After, After and
Vague. The Vague value is only used as the last
resort when the annotator really cannot determine
the temporal relationship between a pair of tem-
poral entities. In the meantime, the TempEval-2
also allows variations from language to language
regarding specific annotation strategies for each
subtask. For Chinese temporal annotation, most
of the decisions we have to make revolve around
one central question, and that is which temporal
entity pair to annotate.
2.2.1 Relation between events and time
expressions
The annotation of the relationship between
events and time expressions involves i) determin-
ing which event is related to which time expres-
sion, and ii) what is the nature of this relation-
ship. In (3), for example, there are three events
and three time expressions that enter into the tem-
poral relation annotation. If the annotator is re-
quired to annotate all possible event/time combi-
nations, there will be nine possible pairs. There
are at least three possible strategies to go about
selecting event/time pairs to annotate. The first
strategy is to annotate all possible pairs. This
seems to add unnecessary burden to the annota-
tor because if we know that e1 overlaps t1, we
can infer the temporal relationship between e1 and
t3 by virtue of the fact that t1 occurs before t3.
The second strategy is to allow the annotator to
freely choose which event/time pair to annotate
based on whether there is a clear temporal rela-
tion between them. This eliminates the possibility
that the annotator is forced to annotate hard-to-
classify and inconsequential relations, but leaving
this decision to the annotator entirely might lead
to low inter-annotator agreement where annota-
tors choose to annotate different event/time pairs.
(3) ?? ?? ?? ??
International Monetary Fund
[t1???
21st
]
?
at
??
here
[e1??
publish
]?
one
?
CL
??
preliminary
??
assessment
??
report
?
,
??
again
[e2??
lower
]?
AS
?
its
?
regarding
[t2?
this
] [t3?
next
]?
two
?
year
??
global
??
economic
??
growth
??
speed
?
DE
[e3
??
forecast
]?
.
?The International Monetary Fund on 21
published a preliminary assessment report,
again lowering its forecast of the global eco-
nomic growth for this year and next year.?
In our annotation, we adopt a third strategy. In-
stead of simply asking which event bears a tem-
poral relation to which temporal expression in the
same sentence, we ask annotators to judge which
event(s) a given temporal expression is intended
to modify. In essence, this amounts to asking the
annotator to first make a syntactic decision about
which events fall within the scope of a time ex-
pression. In (3), all three events e1, e2 and e3
fall within the scope of t1, and none of them are
in the scope of t2 and t3. This approach reduces
the number of fuzzy temporal relations that an-
notators might disagree on due to preference for
thoroughness vs. accuracy.
2.2.2 Temporal relation between
subordinating event and subordinated
event
The two tasks in the TempEval framework that
deal with event pairs are to annotate temporal re-
lation between the subordinating event and the
subordinated event, as well as the relation in
main event pairs. The division of labor between
them is quite clear: the former deals with intra-
sentential temporal relations whereas the latter
handles inter-sentential relations. It is not imme-
diately clear, however, how each of the two types
of relations should be defined.
Unlike in the event/time annotation where syn-
tactic notions are invoked in selecting event/time
pairs to annotate, our definitions of subordinat-
ing and subordinated events are primarily based
on semantic criteria. The subordinating event is
roughly the predicate while the subordinated event
is one of its arguments, provided that both the
1366
predicate and the argument are anchors of events.
For example, in (4), there are two subordinating
and subordinated event pairs. e2 is a subordinated
event of e1, and e4 is a subordinated event of e3.
(4) ??
Guangdong
[e1??]
hold
[e2???]
symposium
[e3
??]
introduce
[e4??]
tax reform
?
and
??
processing
??
trade
??
accounting
??
regulation
?Guangdong held a symposium introducing
the tax reform and the accounting regulations
on processing trade.?
An alternative to using the notion of predicate-
argument structure in determining the subordinat-
ing/subordinated events is to resort to syntactic re-
lations such as the verb and its object. The net re-
sult would be the same for Example (4). However,
the same argument that motivates the annotation
of the predicate-argument structures in the Prop-
bank (Palmer et al, 2005) and the Chinese Prop-
bank (Xue and Palmer, 2009) also applies to tem-
poral annotation. That is, the predicate-argument
structure and temporal relations tend to hold con-
stant in spite of the syntactic alternations and vari-
ations. For example, the temporal relation be-
tween the noun??? (?symposium?) event and
the verb?? (?hold?) event remains the same in
(5) in spite of the change in the syntactic relation
between them. If only event pairs in a verb-object
relation are annotated, the temporal relation be-
tween e2 and e1 in (5) would be lost.
(5) [e2???]
symposium
?
PREP
??
Guangdong
[e1??]
hold
?The symposium was held in Guangdong.?
2.2.3 Temporal relations between main
events
The purpose of annotating the temporal relation
between main events is to capture the temporal or-
dering of events scattered in different sentences
that constitute the main chain of events covered
in the article. Annotation of the temporal relation
between main events is further divided into two
steps. In the first step, main events are first iden-
tified among all events in a sentence, and then the
temporal relation between the main events in adja-
cent pairs of sentences is annotated. As a first ap-
proximation, we define ?main event? as follows:
a main event is the event expressed by the main
verb of the top-most level clause of a sentence.
The underlying assumption is that good writing
would place words representing important events
in prominent positions of a sentence and the first
choice of a prominent position in a sentence is
probably the main verb. An additional stipulation
is that in case of a co-ordinated construction in-
volving two or more main verbs at the top-most
level, the event represented by the first is the main
event of the sentence. This is to ensure that each
sentence has only one main event. As we shall
see in Section 3, this seemingly simple turns out
to be surprisingly difficult, as reflected in the low
inter-annotator agreement.
2.2.4 Temporal relation between events and
the document creation time
In this layer, all the events identified in a doc-
ument are annotated according to their temporal
relation to the document creation time. This task
is particularly challenging and intellectually inter-
esting for Chinese. As an isolating language (Li
and Thompson, 1981), Chinese has a small word
to morpheme ratio. That is, the majority of its
words consist of single morphemes. As a result, it
lacks the inflectional morphology that grammat-
ically marks tense. Tense directly encodes the
temporal location of an event in natural language
text and the lack of observable grammatical tense
makes it that much harder to determine the tem-
poral location of an event in Chinese text. This is
not to say, however, that Chinese speakers do not
attempt to convey the temporal location of events
when they speak or write, or that they cannot inter-
pret the temporal location when they read Chinese
text, or even that they have a different way of rep-
resenting the temporal location of events. In fact,
there is evidence that the temporal location is rep-
resented in Chinese in exactly the same way as it is
represented in English and most world languages:
in relation to the moment of speech. One piece of
evidence to support this claim is that Chinese tem-
poral expressions like ?? (?today?),?? (?to-
morrow?) and ?? (?yesterday?) all assume a
1367
temporal deixis that is the moment of speech in re-
lation to which all temporal locations are defined.
Annotating the temporal relation between events
and document creation time would then directly
capture the temporal location of events.
3 Annotation procedure and annotation
consistency
The data set consists of 60 files taken from the
Chinese Treebank (Xue et al, 2005). The source
of these files is Xinhua newswire. It goes through
a two-phase double blind and adjudication pro-
cess. The first phase involves three annotators,
with each file annotated by two annotators; the
second phase involves two judges, with each dou-
ble annotated document assigned to a single judge
for disagreement resolution. The inter-annotator
agreement between the two annotators (A and B)
as the agreement between each annotator and the
judge (J) are presented in Table 1. The agree-
ment is measured in terms of F1-score3, which is
a weighted average between precision and recall.
The F1-score is calculated as follows:
F = 2 ? precision ? recallprecision + recall (1)
The agreement statistics in Table 1 clearly show
that event and time expression annotations are
easier but temporal relations are harder as re-
flected in the lower inter-annotator agreement
scores. This is somewhat expected because rela-
tions involve two temporal entities while we are
only dealing with one temporal entity with event
and time expression annotations. The figures also
show the seemingly simple task of main event an-
notation (which only involves picking one event
per sentence as the main event) has a surprisingly
low inter-annotator agreement score. One reason
might be that in a less grammaticalized language
like Chinese, it is not always clear which verb is
the main verb when the syntactic tree information
is not displayed in the annotation interface. An-
other reason is that annotators sometimes disre-
3For a subset of the tasks, the total number of annotated
instances for the two annotators is the same. This subset
includes identification of main events, the temporal relation
between the main events in two adjacent sentences, and the
temporal relation between an event and the document cre-
ation time.
Layer f(A, B) f(A, J) f(B, J)
event-extent 0.90 0.93 0.94
timex-extent 0.86 0.88 0.93
main-events 0.74 0.90 0.82
tlinks-main-events 0.65 0.70 0.75
tlinks-dct-events 0.77 0.86 0.90
tlinks-e-t 0.75 0.88 0.83
tlinks-sub-e 0.53 0.74 0.70
Table 1: Inter-annotator agreement for the sub-
tasks: event-extent, the textual extent of an event
anchor; timex-extent, the textual span of a time
expression; tlinks-main-event, the temporal rela-
tion between the main events; tlinks-dct-events,
the temporal link between an event and the doc-
ument creation time; tlinks-e-t, the temporal re-
lation between an event and a time expression;
tlinks-sub-e, the temporal relation between a sub-
ordinating event and a subordinated event.
gard the syntax-based rule when it runs too much
afoul to their intuition, a point that we will come
back to and discuss in greater detail in Section 4.
It is worth noting that the annotation of the tem-
poral relation between an event and a time ex-
pression, and between a subordinating event and
a subordinated event involves two decisions. The
annotator needs to first decide which pairs of tem-
poral entities to annotate, and then decide what
temporal relation should be assigned to each tem-
poral entity pair. To take a closer look at which
of these two decisions creates more of a prob-
lem for the annotator, we computed the agreement
figures for these two steps respectively. In Table
2, Column 3 presents the figure for just identify-
ing which pair to annotate, and Column 4 is the
agreement for just assigning the temporal relation,
assuming the same pair of temporal entities are
found by both annotators.
Layer all identification f relation
tlinks-e-t 0.75 0.86 0.89
tlinks-sub-e 0.53 0.60 0.87
Table 2: Detailed agreement for event-time and
subordinating-subordinated events
From Table 2, it is clear that for both tasks,
1368
there is lower agreement between the annotators
in deciding which pair to annotate. Once the two
annotators agree on which pair to annotate, deter-
mining the temporal relation is relatively easier, as
reflected in higher agreement.
4 Detailed discussion
As described in Section 2, when annotating the
temporal relation between an event and a time ex-
pression, the annotators are instructed to annotate
an event-time pair if the event is falling within the
syntactic scope of the time expression. When an-
notating the relation between subordinating and
subordinated events, the annotators are instructed
to select event pairs based on the semantic notion
of predicate-argument structure. This assumes
a certain level of linguistic sophistication on the
part of the annotators. From the lower agreement
score in identifying event-time pairs (Table 2), it
is clear that our annotators, who are not trained
linguists, lack in this type of specialized knowl-
edge. They are better at making the more in-
tuitive judgment regarding the temporal relation
between two temporal entities. One solution is
obviously to find better trained linguists to per-
form these tasks, but it may not always be fea-
sible. Since our data is taking from the Chinese
Treebank and has already been annotated with
syntactic structures and predicate-argument struc-
tures (from the Chinese Propbank annotation (Xue
and Palmer, 2009)), an alternative is to extract the
event-time or event-event pairs using the syntactic
and predicate-argument structures as constraints.4
The annotation of main events and their rela-
tions presents a different challenge. Our first ap-
proximation is to select main events based on syn-
tactic considerations. A main event is equated
with the matrix verb in a sentence. In many
cases this turns out to be unintuitive. Two of the
recurring counter-intuitive cases involve directly
quoted speech and coordination structures.
Directly quoted speech In Chinese newswire
text, it is often the case that the source of informa-
tion is explicitly cited in the form of direct quota-
tions. (6) is such an example:
4See a similar approach in Bethard et al (2007).
(6) ??
Song-Jian
?
say
?
,
?
?
??
nowadays
?
,
??
China
?
already
?
can
??
produce
??
tens-of-thousands
?
CL
??
digital
??
telephone
?????
PBX
??
?Song Jian said, ?nowadays, China is capa-
ble of producing tens of thousands of digital
telephone PBX.? ?
While the event represented by the underlined
verb ? (?say?) may very well be important in
some natural language processing applications
(for example, sometimes the source of the target
information is crucial), it is not normally part of
the intended information being covered by a news
article. And it does not make much sense to anno-
tate its temporal relation to adjacent main events
that are on a par with what was said, not the saying
event itself. The point would be even clearer when
such a case is contrasted with a case in which a
similar semantic relation is formulated in a differ-
ent syntactic structure, as shown in (7):
(7) ?
according to
??
official
??
authority
??
source
??
divulge
?
,
??
this-year
??
China
??
government
??
determine
?
DE
??
economic
???
growth rate
?
be
????
8%
?
?According to some official sources in posi-
tion of authority, the economic growth rate
determined by the Chinese government is
8%.?
Because of the presence of the preposition
? (?according to?), the underlined reporting verb
?? (?divulge?), similar to? (?say?) in (6) with
respect to its semantic relation to the following
material, would not be annotated as representing
the main event of the sentence. The difference
in the annotation of the main event between (7)
and (6) seems to be an undesirable artifact of the
purely syntax-based annotation rule for identify-
ing main events.
1369
Co-ordination structure Co-ordination by no
means is a rare occurrence in the data, and of-
ten times, all events within a co-ordination struc-
ture, taken together, represent the main event of
the sentence. For example, in (8), both events
represented by the underlined verbs seem to be
equally significant and should be included in the
same chain of events. Given the prevalence of co-
ordination between verbs, the stipulation that only
the first one counts significantly undermines the
coverage of the task and goes against the annota-
tor?s intuitions.
(8) ??
This year
??
September
?
,
?
many
?
CL
??
foreign
??
oil
??
company
?
with
?
Kazakstan
??
national
??
oil
??
company
??
sign
?
LE
???
a series of
???
?century
???
contract?
?
,
??
these
??
contract
?
will
?
in
??
future
??
40
?
years
?
within
??
generate
?????
700-billion
??
dollar
?
DE
??
enormous
??
profit
?
?In September of this year, many foreign oil
companies signed a series of ?century con-
tract? with Kazakstan National Oil Company.
These contracts will generate an enormous
profit of 700-billion dollars.?
The issue in the annotation of the temporal re-
lation between main events seem to be more in the
selection of main event pairs than in the determi-
nation of the nature of their relationship. Our cur-
rent rule states that any two main events in consec-
utive sentences form a pair for annotation. This
task suffers a low level of inter-annotator agree-
ment partly because many main events identified
by syntactic criteria are not actually main events
in our intended sense. Often times, two consecu-
tive main events come from different levels of the
discourse structure or different chains of events,
which puts annotators in a hard-to-classify situa-
tion.
To achieve high inter-annotator consistency
when annotating the temporal relation between
events from different sentences, we believe the se-
lection of event pairs has to be informed by the
discourse structure of the document. This only
makes sense given that the annotation of tempo-
ral relation between events and time expressions
within one sentence is informed by the syntactic
structure, and the temporal relation between sub-
ordination and subordinating events benefits from
an understanding of the predicate-argument struc-
ture.
The specific type of discourse structure we have
in mind is the kind represented in the Penn Dis-
course Treebank (Miltsakaki et al, 2004). The
Penn Discourse Treebank-style of annotation can
inform temporal relation annotation in at least two
ways. First, the Penn Discourse Treebank anno-
tates the discourse relation between two adjacent
sentences. The discourse relation holds between
two abstract objects such as events or proposi-
tions. If a discourse relation holds between two
events, the temporal relation between those two
events might also be what we are interested in for
temporal annotation. The implicit assumption is
that the discourse structure of a document repre-
sents the important temporal relations within that
document as well. (9) is an example taken from
the Penn Discourse Treebank. The discourse re-
lation, characterized by the discourse connective
?in particular?, holds between the events anchored
by?dropped? and ?fell?. The temporal relation be-
tween these events also happens to be what we
would be interested in if we are to annotate the
main events between two adjacent sentences. No-
tice that in (9), material that is irrelevant to the
discourse relation is taken out of the two argu-
ments of this discourse relation, which are marked
in italics and bold face respectively.
(9) Meanwhile, the average yield on taxable
funds dropped nearly a tenth of a percent-
age point, the largest drop since midsum-
mer. implicit = in particular The average
seven-day compound yield, which assumes
that dividends are reinvested and that current
rates continue for a year, fell to 8.47%, its
lowest since late last year, from 8.55% the
week before, according to Donoghue?s.
The Penn Discourse Treebank also marks attri-
butions when annotating discourse relations. In
1370
(10), for example, ?he says? will be marked as a
case of attribution and the ?say? verb would be
marked as the main event of the sentence if syn-
tactic criteria are followed. Having attributions
identified would directly help with the temporal
annotation of examples like (6), where the main
event is embedded in direct quoted speech.
(10) When Mr. Green won a $240,000 verdict in
a land condemnation case against the State
in June 1983, [he says] Judge O?Kicki
unexpectedly awarded him an additional
$100,000.
As of now, the data we use for our temporal
annotation experiment have not yet been anno-
tated with discourse structures. In order to make
our temporal annotation sensitive to the discourse
structure, we either have to annotate the discourse
structure in a separate pass, or to incorporate the
key elements of the discourse structure when de-
veloping guidelines for temporal annotation.
5 Conclusion
We described a Chinese temporal annotation ex-
periment that produced a sizable data set for
the TempEval-2 annotation campaign. We show
that while we have achieved high inter-annotator
agreement for simpler tasks such as identifica-
tion of events and time expressions, temporal rela-
tion annotation proves to be much more challeng-
ing. We show that in order to improve annotation
consistency it is important to strategically select
the annotation targets, and this selection process
should be subject to syntactic, semantic and dis-
course constraints.
Acknowledgements
This work is supported by the National Sci-
ence Foundation via Grant No. 0855184 entitled
?Building a community resource for temporal in-
ference in Chinese?. All views expressed in this
paper are those of the authors and do not neces-
sarily represent the view of the National Science
Foundation.
References
Regina Barzilay, Noemie Elhadad, and Kathleen McK-
eown. 2002. Inferring strategies for sentence order-
ing in multidocument news summarization. Journal
of Artificial Intelligence Research, 17:35?55.
Steven Bethard, James H. Martin, and Sara Klin-
genstein. 2007. Finding Temporal Structure in
Text: Machine Learning of Syntactic Temporal Re-
lations. International Journal of Semantic Comput-
ing, 11(4).
Lisa Ferro, Laurie Gerber, Inderjeet Mani, Beth Sund-
heim, and George Wilson. 2004. TIDES 2003 Stan-
dard for the Annotation of Temporal Expressions.
Elena Filatova and Eduard Hovy. 2001. Assigning
Time-Stamps to Event Clauses. In Proceedings of
the Workshop on Temporal and Spatial Information
Processing, Toulouse.
Sanda Harabagiu and Cosmin Adrian Bejan. 2005.
Question Answering Based on Temporal Inference.
In Proceedings of the AAAI-2005 Workshop on In-
ference for Textual Question Answering, Pittsburgh,
Pennsylvania.
Sanda Harabagiu and Cosmin Adrian Bejan. 2006. An
Answer Bank for Temporal Inference. In Proceed-
ings of LREC 2006, Genoa, Italy.
ISO/TC 37/SC 4/WG 2. 2007. Language Resource
Management ? Semantic Annotation Framework
(SemAF) ? Part 1: Time and events.
Charles Li and Sandra Thompson. 1981. Mandarin
Chinese: A Functional Reference Grammar. Berke-
ley, Los Angeles, London: University of California
Press.
Chin-Yew Lin and Eduard Hovy. 2001. Neats: A mul-
tidocument summarizer. In Proceedings of the Doc-
ument Understanding Workshop.
Inderjeet Mani and George Wilson. 2000. Robust
temporal processing of news. In Proceedings of the
ACL?2000, Hong Kong, China.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. The Penn Discourse Tree-
Bank. In Proceedings of the Language Resources
and Evaluation Conference, Lisbon, Portugal.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
James Pustejovsky, Patrick Hanks, Roser Sauri,
Andrew See, David Day, Lisa Ferro, Robert
Gaizauskas, Marcia Lazo, Andrea Setzer, and Beth
1371
Sundheim. 2003. The TimeBank Corpus. Corpus
Linguistics, pages 647?656.
James Pustejovsky, Bob Ingria, Roser Sauri, Jose Cas-
tano, Jessica Littman, Rob Gaizauskas, Andrea Set-
zer, G. Katz, and I. Mani. 2005. The specification
language TimeML. In I. Mani, J. Pustejovsky, and
R. Gaizauskas, editors, The Language of Time: a
Reader. Oxford University Press.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James
Pustejovsky. 2009. The TempEval Challenge:
Identifying Temporal Relation in Text. Language
Resources and Evaluation, 43(1):161?179.
Richard Xiao and Tony McEnery. 2004. Aspect in
Mandarin Chinese: A Corpus-based Study. Ams-
terdam: John Benjamins.
Nianwen Xue and Martha Palmer. 2009. Adding se-
mantic roles to the Chinese Treebank. Natural Lan-
guage Engineering, 15(1):143?172.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural
Language Engineering, 11(2):207?238.
1372
Coling 2010: Poster Volume, pages 1382?1390,
Beijing, August 2010
Chasing the ghost: recovering empty categories in the Chinese Treebank
Yaqin Yang
Computer Science Department
Brandeis University
yaqin@cs.brandeis.edu
Nianwen Xue
Computer Science Department
Brandeis University
xuen@cs.brandeis.edu
Abstract
Empty categories represent an impor-
tant source of information in syntactic
parses annotated in the generative linguis-
tic tradition, but empty category recovery
has only started to receive serious atten-
tion until very recently, after substantial
progress in statistical parsing. This paper
describes a unified framework in recover-
ing empty categories in the Chinese Tree-
bank. Our results show that given skele-
tal gold standard parses, the empty cate-
gories can be detected with very high ac-
curacy. We report very promising results
for empty category recovery for automatic
parses as well.
1 Introduction
The use of empty categories to represent the syn-
tactic structure of a sentence is the hallmark of the
generative linguistics and they represent an im-
portant source of information in treebanks anno-
tated in this linguistic tradition. The use of empty
categories in the annotation of treebanks started
with the Penn Treebank (Marcus et al, 1993), and
this practice is continued in the Chinese Treebank
(CTB) (Xue et al, 2005) and the Arabic Tree-
bank, the Penn series of treebanks. Empty cat-
egories come in a few different varieties, serv-
ing different purposes. One use of empty cate-
gories is to mark the extraction site of an dislo-
cated phrase, thus effectively reconstructing the
canonical structure of a sentence, allowing easy
extraction of its predicate-argument structure. For
example, in Figure 1, the empty category *T*-
1 is coindexed with the dislocated topic NP ?
? (?Ningbo?), indicating that the canonical po-
sition of this NP is next to the verb ? (?come?).
The empty category effectively localizes the syn-
tactic dependency between the verb and this NP,
making it easier to detect and extract this relation.
Marking the extraction site of a dislocated item
is not the only use of empty categories. For lan-
guages like Chinese, empty categories are also
used to represent dropped pronouns. Chinese is
a pro-drop language (Huang, 1989) and subject
pronouns are routinely dropped. Recovering these
elliptical elements is important to many natural
language applications. When translated into an-
other language, for example, these dropped pro-
nouns may have to be made explicit and replaced
with overt pronouns or noun phrases if the target
language does not allow dropped pronouns.
Although empty categories have been an inte-
gral part of the syntactic representation of a sen-
tence ever since the Penn Treebank was first con-
structed, it is only recently that they are starting
to receive the attention they deserve. Works on
automatic detection of empty categories started
to emerge (Johnson, 2002; Dienes and Dubey,
2003; Campbell, 2004; Gabbard et al, 2006) af-
ter substantial progress has been made in statis-
tical syntactic parsing. This progress has been
achieved after over a decade of intensive research
on syntactic parsing that has essentially left the
empty categories behind (Collins, 1999; Char-
niak, 2000). Empty categories were and still are
routinely pruned out in parser evaluations (Black
et al, 1991). They have been excluded from the
parser development and evaluation cycle not so
much because their importance was not under-
stood, but because researchers haven?t figured out
1382
IP
NP-PN-TPC-1 NP-SBJ VP
NR PN VC VP
??
ningbo
Ningbo
?
wo
I
?
shi
be
QP-ADV VP
OD CLP VV NP-OBJ
??
disan
third
M ?
lai
come
-NONE-
?
ci
*T*-1
?Ningbo, this is the third time I came here.?
Figure 1: A CTB tree with empty categories
a way to incorporate the empty category detection
in the parsing process. In fact, the detection of
empty categories relies heavily on the other com-
ponents of the syntactic representation, and as a
result, empty category recovery is often formu-
lated as postprocessing problem after the skeletal
structure of a syntactic parse has been determined.
As work on English has demonstrated, empty cat-
egory detection can be performed with high accu-
racy given high-quality skeletal syntactic parses as
input.
Because Chinese allows dropped pronouns and
thus has more varieties of empty categories than
languages like English, it can be argued that there
is added importance in Chinese empty category
detection. However, to our knowledge, there has
been little work in this area, and the work we
report here represents the first effort in Chinese
empty category detection. Our results are promis-
ing, but they also show that Chinese empty cat-
egory detection is a very challenging problem
mostly because Chinese syntactic parsing is dif-
ficult and still lags significantly behind the state
of the art in English parsing. We show that given
skeletal gold-standard parses (with empty cate-
gories pruned out), the empty detection can be
performed with a fairly high accuracy of almost
89%. The performance drops significantly, to
63%, when the output of an automatic parser is
used.
The rest of the paper is organized as follows.
In Section 2, we formulate the empty category de-
tection as a binary classification problem where
each word is labeled as either having a empty cat-
egory before it or not. This makes it possible to
use any standard machine learning technique to
solve this problem. The key is to find the appro-
priate set of features. Section 3 describes the fea-
tures we use in our experiments. We present our
experimental results in Section 4. There are two
experimental conditions, one with gold standard
treebank parses (stripped of empty categories) as
input and the other with automatic parses. Section
5 describes related work and Section 6 conclude
our paper.
2 Formulating the empty category
detection as a tagging problem
In the CTB, empty categories are marked in a
parse tree which represents the hierarchical struc-
ture of a sentence, as illustrated in Figure 1.
There are eight types of empty categories anno-
tated in the CTB, and they are listed in Table 1.
Among them, *pro* and *PRO* are used to rep-
resent nominal empty categories, *T* and *NP*
are used to represent traces of dislocated items,
*OP* is used to represent empty relative pronouns
in relative clauses, and *RNR* is used to repre-
sent pseudo attachment. The reader is referred to
the CTB bracketing manual (Xue and Xia, 2000)
for detailed descriptions and examples. As can
be seen from Table 1, the distribution of these
empty categories is very uneven, and many of
these empty categories do not occur very often.
1383
EC Type count Description
*pro* 2024 small pro
*PRO* 2856 big pro
*T* 4486 trace for extraction
*RNR* 217 right node raising
*OP* 879 operator
* 132 trace for raising
Table 1: Empty categories in CTB.
As a first step of learning an empty category
model, we treat all the empty categories as a uni-
fied type, and for each word in the sentence, we
only try to decide if there is an empty category
before it. This amounts to an empty category de-
tection task, and the objective is to first locate the
empty categories without attempting to determine
the specific empty category type. Instead of pre-
dicting the locations of the empty categories in a
parse tree and having a separate classifier for each
syntactic construction where an empty category is
likely to occur, we adopt a linear view of the parse
tree and treat empty categories, along with overt
word tokens, as leaves in the tree. This allows us
to identify the location of the empty categories in
relation to overt word tokens in the same sentence,
as illustrated in Example (1):
(1) ?? ? ? ?? ? ? *T*?
In this representation, the position of the empty
category can be defined either in relation to the
previous or the next word, or both. To make
this even more amenable to machine learning ap-
proaches, we further reformulate the problem as a
tagging problem so that each overt word is labeled
either with EC, indicating there is an empty cate-
gory before this word, or NEC, indicating there is
no empty category. This reformulated representa-
tion is illustrated in Example (2):
(2) ? ?/NEC ?/NEC ?/NEC ? ?/NEC
?/NEC?/NEC?/EC
In (2), the EC label attached to the final period
indicates that there is an empty category before
this punctuation mark. There is a small price to
pay with this representation: when there is more
than one empty category before a word, it is indis-
tinguishable from cases where there is only one
empty category. What we have gained is a sim-
ple unified representation for all empty categories
that lend itself naturally to machine learning ap-
proaches. Another advantage is that for natural
language applications that do not need the full
parse trees but only need the empty categories,
this representation provides an easy-to-use repre-
sentation for those applications. Since this linear-
lized representation is still aligned with its parse
tree, we still have easy access to the full hierar-
chical structure of this tree from which useful fea-
tures can be extracted.
3 Features
Having modeled empty category detection as a
machine learning task, feature selection is crucial
to successfully finding a solution to this problem.
The machine learning algorithm scans the words
in a sentence from left to right one by one and
determine if there is an empty category before it.
When the sentence is paired with its parse tree,
the feature space is all the surrounding words of
the target word as well as the syntactic parse for
the sentence. The machine learning algorithm also
has access to the empty category labels (EC or
NEC) of all the words before the current word.
Figure 2 illustrates the feature space for the last
word (a period) in the sentence.
NP VP
NR PN VC
VP
QP
OD
CLP
M
VP
VV PU
NP
IP
!" # $% & ' !
NEC NEC NEC NEC NEC NEC EC
Ningbo I be third time come .
(
"Ningbo, this is the third time I came here."
Figure 2: Feature space of empty category detec-
tion
For purposes of presentation, we divide our
features into lexical and syntactic features. The
1384
lexical features are different combinations of the
words and their parts of speech (POS), while syn-
tactic features are the structural information gath-
ered from the nonterminal phrasal labels and their
syntactic relations.
3.1 Lexical features
The lexical features are collected from a narrow
window of five words and their POS tags. If the
target word is a verb, the lexical features also in-
clude transitivity information of this verb, which
is gathered from the CTB. A transitivity lexicon is
induced from the CTB by checking whether a verb
has a right NP or IP sibling. Each time a verb is
used as a transitive verb (having a right NP or IP
sibling), its transitive count is incremented by one.
Conversely, each time a verb is used as an intran-
sitive verb (not having a right NP or IP sibling), its
intransitive use is incremented by one. The result-
ing transitivity lexicon after running through the
entire Chinese Treebank consists of a list of verbs
with frequencies of their transitive and intransitive
uses. A verb is considered to be transitive if its in-
transitive count in this lexicon is zero or if its tran-
sitive use is more than three times as frequent as
its intransitive use. Similarly, a verb is considered
to be intransitive if its transitive count is zero or
if its intransitive use is at least three times as fre-
quent as its transitive use. The full list of lexical
features is presented in Table 2.
3.2 Syntactic features
Syntactic features are gathered from the CTB
parses stripped of function tags and empty cate-
gories when the gold standard trees are used as
input. The automatic parses used as input to our
system are produced by the Berkeley parser. Like
most parsers, the Berkeley parser does not repro-
duce the function tags and empty categories in the
original trees in the CTB. Syntactic features cap-
ture the syntactic context of the target word, and
as we shall show in Section 4, the syntactic fea-
tures are crucial to the success of empty category
detection. The list of syntactic features we use in
our system include:
1. 1st-IP-child: True if the current word is the
first word in the lowest IP dominating this
word.
Feature Names Description
word(0) Current word
word(-1) Previous word
pos(0) POS of current word
pos(-1,0) POS of previous and cur-
rent word
pos(0, 1) POS of current and next
word
pos(0, 1, 2) POS of current & next
word, & word 2 after
pos(-2, -1) POS of previous word &
word 2 before
word(-1), pos(0) Previous word & POS of
current word
pos(-1),word(0) POS of previous word&
current word
trans(0) current word is transitive
or intransitive verb
prep(0) true if POS of current
word is a preposition
Table 2: Feature set.
2. 1st-word-in-subjectless-IP: True if the cur-
rent word starts an IP with no subject. Sub-
ject is detected heuristically by looking at left
sisters of a VP node. Figure 3 illustrates this
feature for the first word in a sentence where
the subject is a dropped pronoun.
3. 1st-word-in-subjectless-IP+POS: POS of
the current word if it starts an IP with no sub-
ject.
4. 1st-VP-child-after-PU: True if the current
word is the first terminal child of a VP fol-
lowing a punctuation mark.
5. NT-in-IP: True if POS of current word is NT,
and it heads an NP that does not have a sub-
ject NP as its right sister.
6. verb-in-NP/VP: True if the current word is a
verb in an NP/VP.
7. parent-label: Phrasal label of the parent of
the current node, with the current node al-
ways corresponding to a terminal node in the
parse tree.
8. has-no-object: True If the previous word is
a transitive verb and this verb does not take
an object.
1385
!
"#
$%
&'
("#
)*+,-%.*/
 0
("#
.12
3 4
("#
*)/.*2%
56
("#
*77898)*,:;.)%-
<=
("#
*>>/?;.
@ABCD
("#
EFG
H
("#
I
LCP
PP
AD AD
VV
CD MLCNT
P QP
VPADVPADVP
VP
IP
By the end of  last year, (Shanghai) has approved 216 ...
Figure 3: First word in a subject-less IP
Empty categories generally occur in clausal or
phrasal boundaries, and most of the features are
designed to capture such information. For exam-
ple, the five feature types, 1st-IP-child, 1st-word-
in-subjectless-IP, 1st-word-in-subjectless-IP, 1st-
VP-child-after-PU and NT-in-IP all represent the
left edge of a clause (IP) with some level of gran-
ularity. parent label and verb-in-NP/VP represent
phrases within which empty categories typically
occur do not occur. The has-no-object feature is
intended to capture transitive uses of a verb when
the object is missing.
4 Experiments
Given that our approach is independent of specific
machine learning techniques, many standard ma-
chine learning algorithms can be applied to this
task. For our experiment we built a Maximum En-
tropy classifier with the Mallet toolkit1.
4.1 Data
In our experiments, we use a subset of the CTB
6.0. This subset is further divided into train-
ing (files chtb 0081 thorough chtb 0900), devel-
opment (files chtb 0041 through chtb 0080) and
test sets (files chtb 0001 through chtb 0040, files
chtb 0901 through chtb 0931). The reason for not
using the entire Chinese Treebank is that the data
in the CTB is from a variety of different sources
and the automatic parsing accuracy is very uneven
across these different sources.
1http://mallet.cs.umass.edu
4.2 Experimental conditions
Two different kinds of data sets were used in the
evaluation of our method: 1) gold standard parse
trees from the CTB; and 2) automatic parses pro-
duced by the Berkeley parser2 .
4.2.1 Gold standard parses
There are two experimental conditions. In our
first experiment, we use the gold standard parse
trees from the CTB as input to our classifier. The
version of the parse tree that we use as input to
our classifier is stripped of the empty category
information. What our system effectively does
is to restore the empty categories given a skele-
tal syntactic parse. The purpose of this experi-
ment is to establish a topline and see how accu-
rately the empty categories can be restored given
a?correct?parse.
4.2.2 Automatic parses
To be used in realistic scenarios, the parse trees
need to be produced automatically from raw text
using an automatic parser. In our experiments we
use the Berkeley Parser as a representative of the
state-of-the-art automatic parsers. The input to the
Berkeley parser is words that have already been
segmented in the CTB. Obviously, to achieve fully
automatic parsing, the raw text should be auto-
matically segmented as well. The Berkeley parser
comes with a fully trained model, and to make
sure that none of our test and development data is
included in the training data in the original model,
we retrained the parser with our training set and
used the resulting model to parse the documents
in the development and test sets.
When training our empty category model using
automatic parses, it is important that the quality
of the parses match between the training and test
sets. So the automatic parses in the training set
are acquired by first training the parser with 4/5
of the data and using the resulting model to parse
the remaining 1/5 of the data that has been held
out. Measured by the ParsEval metric (Black et
al., 1991), the parser accuracy stands at 80.3% (F-
score), with a precision of 81.8% and a recall of
78.8% (recall).
2http://code.google.com/p/berkeleyparser
1386
4.3 Evaluation metrics
We use precision, recall and F-measure as our
evaluation metrics for empty category detection.
Precision is defined as the number of correctly
identified Empty Categories (ECs) divided by the
total number of ECs that our system produced.
Recall is defined as the number of correctly iden-
tified ECs divided by the total number of EC la-
bels in the CTB gold standard data. F-measure
is defined as the geometric mean of precision and
recall.
R = # of correctly detected EC
# of EC tagged in corpus (1)
P = # of correctly detected EC
# of EC reported by the system (2)
F = 21/R + 1/P (3)
4.4 Overall EC detection performance
We report our best result for the gold standard
trees and the automatic parses produced by the
Berkeley parser in Table 3. These results are
achieved by using all lexical and syntactic features
presented in Section 3.
Data Prec.(%) Rec.(%) F(%)
Gold 95.9 (75.3) 83.0 (70.5) 89.0 (72.8)
Auto 80.3 (57.9) 52.1 (50.2) 63.2 (53.8)
Table 3: Best results on the gold tree.
As shown in Table 3, our feature set works
well for the gold standard trees. Not surprisingly,
the accuracy when using the automatic parses is
lower, with the performance gap between using
the gold standard trees and the Berkeley parser
at 25.8% (F-score). When the automatic parser
is used, although the precision is 80.3%, the re-
call is only 52.1%. As there is no similar work in
Chinese empty category detection using the same
data set, for comparison purposes we established
a baseline using a rule-based approach. The rule-
based algorithm captures two most frequent loca-
tions of empty categories: the subject and the ob-
ject positions. Our algorithm labels the first word
within a VP with EC if the VP does not have a
subject NP. Similarly, it assigns the EC label to the
word immediately following a transitive verb if it
does not have an NP or IP object. Since the miss-
ing subjects and objects account for most of the
empty categories in Chinese, this baseline covers
most of the empty categories. The baseline results
are also presented in Table 3 (in brackets). The
baseline results using the gold standard trees are
75.3% (precision), 70.5% (recall), and 72.8% (F-
score). Using the automatic parses, the results are
57.9% (precision), 50.2% (recall), and 53.8% (F-
score) respectively. It is clear from our results that
our machine learning model beats the rule-based
baseline by a comfortable margin in both exper-
imental conditions. Table 4 breaks down our re-
sults by empty category types. Notice that we did
not attempt to predict the specific empty category
type. This only shows the percentage of empty
categories our model is able to recover (recall) for
each type. As our model does not predict the spe-
cific empty category type, only whether there is an
empty category before a particular word, we can-
not compute the precision for each empty category
type. Nevertheless, this breakdown gives us a
sense of which empty category is easier to recover.
For both experimental conditions, the empty cate-
gory that can be recovered with the highest accu-
racy is *PRO*, an empty category often used in
subject/object control constructions. *pro* seems
to be the category that is most affected by parsing
accuracy. It has the widest gap between the two
experimental conditions, at more than 50%.
EC Type Total Correct Recall(%)
*pro* 290 274/125 94.5/43.1
*PRO* 299 298/196 99.7/65.6
*T* 578 466/338 80.6/58.5
*RNR* 32 22/20 68.8/62.5
*OP* 134 53/20 40.0/14.9
* 19 9/5 47.4/26.3
Table 4: Results of different types of empty cate-
gories.
4.5 Comparison of feature types
To investigate the relative importance of lexical
and syntactic features, we experimented with us-
ing just the lexical or syntactic features under
both experimental conditions. The results are pre-
1387
sented in Table 5. Our results show that when
using only the lexical features, the drop in accu-
racy is small when automatic parses are used in
place of gold standard trees. However, when us-
ing only the syntactic features, the drop in accu-
racy is much more dramatic. In both experimental
conditions, however, syntactic features are more
effective than the lexical features, indicating the
crucial importance of high-quality parses to suc-
cessful empty category detection. This makes in-
tuitive sense, given that all empty categories oc-
cupy clausal and phrasal boundaries that can only
defined in syntactic terms.
Data Prec.(%) Rec.(%) F(%)
Lexical 79.7/77.3 47.6/39.9 59.6/52.7
Syntactic 95.9/78.0 70.0/44.5 81.0/56.7
Table 5: Comparison of lexical and syntactic fea-
tures.
4.6 Comparison of individual features
Given the importance of syntactic features, we
conducted an experiment trying to evaluate the
impact of each individual syntactic feature on the
overall empty category detection performance. In
this experiment, we kept the lexical feature set
constant, and switched off the syntactic features
one at a time. The performance of the different
syntactic features is shown in Table 6. The re-
sults here assume that automatic parses are used.
The first row is the result of using all features
(both syntactic and lexical) while the last row is
the result of using only the lexical features. It
can be seen that syntactic features contribute more
than 10% to the overall accuracy. The results also
show that features (e.g., 1st-IP-child) that capture
clause boundary information tend to be more dis-
criminative and they occupy the first few rows of
a table that sorted based on feature performance.
5 Related work
The problem of empty category detection has been
studied both in the context of reference resolution
and syntactic parsing. In the reference resolution
literature, empty category detection manifests it-
self in the form of zero anaphora (or zero pronoun)
Feature Name Prec.(%) Rec.(%) F(%)
all 80.3 52.1 63.2
1st-IP-child 79.8 49.2 60.8
1st-VP-child-
after-PU
79.7 50.5 61.8
NT-in-IP 79.4 50.8 61.9
1st-word-in-
subjectless-
IP+Pos
79.5 51.1 62.2
has-no-object 80.0 51.1 62.4
1st-word-in-
subjectless-IP
79.4 51.5 62.5
verb-in-NP/VP 79.9 52.0 63.0
parent-label 79.4 52.4 63.1
only lexical 77.3 39.9 52.7
Table 6: Performance for individual syntactic fea-
tures with automatic parses.
detection and resolution. Zero anaphora resolu-
tion has been studied as a computational prob-
lem for many different languages. For example,
(Ferra?ndez and Peral, 2000) describes an algo-
rithm for detecting and resolving zero pronouns
in Spanish texts. (Seki et al, 2002) and (Lida et
al., 2007) reported work on zero pronoun detec-
tion and resolution in Japanese.
Zero anaphora detection and resolution for
Chinese has been studied as well. Converse
(2006) studied Chinese pronominal anaphora res-
olution, including zero anaphora resolution, al-
though there is no attempt to automatically de-
tect the zero anaphors in text. Her work only
deals with anaphora resolution, assuming the zero
anaphors have already been detected. Chinese
zero anaphora identification and resolution have
been studied in a machine learning framework-
ing in (Zhao and Ng, 2007) and (Peng and Araki,
2007).
The present work studies empty category re-
covery as part of the effort to fully parse natural
language text and as such our work is not lim-
ited to just recovering zero anaphors. We are
also interested in other types of empty categories
such as traces. Our work is thus more closely re-
lated to the work of (Johnson, 2002), (Dienes and
Dubey, 2003), (Campbell, 2004) and (Gabbard et
1388
al., 2006).
Johnson (2002) describes a pattern-matching
algorithm for recovering empty nodes from phrase
structure trees. The idea was to extract minimal
connected tree fragments that contain an empty
node and its antecedent(s), and to match the ex-
tracted fragments against an input tree. He eval-
uated his approach both on Penn Treebank gold
standard trees stripped of the empty categories and
on the output of the Charniak parser (Charniak,
2000).
(Dienes and Dubey, 2003) describes an empty
detection method that is similar to ours in that it
treats empty detection as a tagging problem. The
difference is that the tagging is done without ac-
cess to any syntactic information so that the iden-
tified empty categories along with word tokens in
the sentence can then be fed into a parser. The suc-
cess of this approach depends on strong local cues
such as infinitive markers and participles, which
are non-existent in Chinese. Not surprisingly, our
model yields low accuracy if only lexical features
are used.
Cambell (2004) proposes an algorithm that uses
linguistic principles in empty category recovery.
He argues that a rule-based approach might per-
form well for this problem because the locations
of the empty categories, at least in English, are in-
serted by annotators who follow explicit linguistic
principles.
Yuqing(2007) extends (Cahill et al, 2004) ?s
approach for recovering English non-local depen-
dencies and applies it to Chinese. This paper pro-
poses a method based on the Lexical-Functional
Grammar f-structures, which differs from our ap-
proach. Based on parser output trees including
610 files from the CTB, the authors of this pa-
per claimed they have achieved 64.71% f-score for
trace insertion and 54.71% for antecedent recov-
ery.
(Gabbard et al, 2006) describes a more recent
effort to fully parse the Penn Treebank, recovering
both the function tags and the empty categories.
Their approach is similar to ours in that they treat
empty category recovery as a post-processing pro-
cess and use a machine learning algorithm that
has access to the skeletal information in the parse
tree. Their approach is different from ours in that
they have different classifiers for different types of
empty categories.
Although generally higher accuracies are re-
ported in works on English empty category re-
covery, cross-linguistic comparison is difficult be-
cause both the types of empty categories and
the linguistic cues that are accessible to machine
learning algorithms are different. For example,
there are no empty complementizers annotated in
the CTB while English does not allow dropped
pronouns.
6 Conclusion and future work
We describe a unified framework to recover empty
categories for Chinese given skeletal parse trees as
input. In this framework, empty detection is for-
mulated as a tagging problem where each word
in the sentence receives a tag indicating whether
there is an empty category before it. This ad-
vantage of this approach is that it is amenable to
learning-based approaches and can be addressed
with a variety of machine learning algorithms.
Our results based on a Maximum Entropy model
show that given skeletal gold standard parses,
empty categories can be recovered with very high
accuracy (close to 90%). We also report promis-
ing results (over 63%). when automatic parses
produced by an off-the-shelf parser is used as in-
put.
Detecting empty categories is only the first step
towards fully reproducing the syntactic represen-
tation in the CTB, and the obvious next step is to
also classify these empty categories into different
types and wherever applicable, link the empty cat-
egories to their antecedent. This is the line of re-
search we intend to pursue in our future work.
Acknowledgment
This work is supported by the National Sci-
ence Foundation via Grant No. 0910532 enti-
tled ?Richer Representations for Machine Trans-
lation?. All views expressed in this paper are
those of the authors and do not necessarily repre-
sent the view of the National Science Foundation.
1389
References
Black, E., S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A proce-
dure for quantitively comparing the syntactic cov-
erage of English grammars. In Proceedings of the
DARPA Speech and Natural Language Workshop,
pages 306?311.
Cahill, Aoife, Michael Burke, Ruth O?Donovan,
Josef van Genabith, and Andy Way. 2004. Long-
Distance Dependency Resolution in Automatically
Acquired Wide-Coverage PCFG-Based LFG Ap-
proximations. In In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics.
Campbell, Richard. 2004. Using linguistic principles
to recover empty categories. In Proceedings of the
42nd Annual Meeting on Association For Computa-
tional Linguistics.
Charniak, E. 2000. A Maximum-Entropy-Inspired
Parser. In Proceedings of NAACL-2000, pages 132?
139, Seattle, Washington.
Collins, Michael. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Converse, Susan. 2006. Pronominal anaphora resolu-
tion for Chinese. Ph.D. thesis.
Dienes, Pe?ter and Amit Dubey. 2003. Deep syntac-
tic processing by combining shallow methods. In
Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics, volume 1.
Ferra?ndez, Antonio and Jesu?s Peral. 2000. A compu-
tational approach to zero-pronouns in Spanish. In
Proceedings of the 38th Annual Meeting on Associ-
ation For Computational Linguistics.
Gabbard, Ryan, Seth Kulick, and Mitchell Marcus.
2006. Fully parsing the penn treebank. In Proceed-
ings of HLT-NAACL 2006, pages 184?191, New
York City.
Guo, Yuqing, Haifeng Wang, and Josef van Genabith.
2007. Recovering Non-Local Dependencies for
Chinese. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning.
Huang, James C.-T. 1989. Pro drop in Chinese, a
generalized control approach. In O, Jaeggli and
K. Safir, editors, The Null Subject Parameter. D.
Reidel Dordrecht.
Johnson, Mark. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguis-
tics.
Lida, Ryu, Kentaro Inui, and Yuji Matsumoto. 2007.
Zero-anaphora resolution by learning rich syntactic
pattern features. ACM Transactions on Asian Lan-
guage Information Processing, pages 1?22.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313?330.
Peng, Jing and Kenji Araki. 2007. Zero-anaphora res-
olution in chinese using maximum entropy. IEICE
- Trans. Inf. Syst., E90-D(7):1092?1102.
Seki, Kazuhiro, Atsushi Fujii, and Tetsuya Ishikawa.
2002. A probabilistic method for analyzing
Japanese anaphora integrating zero pronoun detec-
tion and resolution. In Proceedings of the 19th in-
ternational Conference on Computational Linguis-
tics, volume 1.
Xue, Nianwen and Fei Xia. 2000. The Bracket-
ing Guidelines for Penn Chinese Treebank Project.
Technical Report IRCS 00-08, University of Penn-
sylvania.
Xue, Nianwen, Fei Xia, Fu dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural
Language Engineering, 11(2):207?238.
Zhao, Shanheng and Hwee Tou Ng. 2007. Identifi-
cation and Resolution of Chinese Zero Pronouns:
A Machine Learning Approach. In Proceedings of
EMNLP-CoNLL Joint Conference, Prague, Czech
Republic.
1390
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1511?1520, Dublin, Ireland, August 23-29 2014.
Building a Hierarchically Aligned Chinese-English Parallel Treebank
Dun Deng and Nianwen Xue
Computer Science Department, Brandeis University
415 South Street, Waltham MA, USA
ddeng@brandeis.edu, xuen@brandeis.edu
Abstract
We construct a hierarchically aligned Chinese-English parallel treebank by manually doing word
alignments and phrase alignments simultaneously on parallel phrase-based parse trees. The main
innovation of our approach is that we leave words without a translation counterpart (which are
mostly language-particular function words) unaligned on the word level, and locate and align the
appropriate phrases which encapsulate them. In doing so, we harmonize word-level and phrase-
level alignments. We show that this type of annotation can be performedwith high inter-annotator
consistency and have both linguistic and engineering potentials.
1 Introduction
The value of human annotated syntactic structures for Statistical Machine Translation has been clearly
demonstrated in string-to-tree (Galley et al., 2004; Galley et al., 2006; Huang et al., 2006), tree-to-string
(Liu et al., 2006; Liu and Gildea, 2008), and tree-to-tree (Eisner, 2003; Liu et al., 2009; Chiang, 2010)
models. One recurring issue which hampers the utility of syntactic structures is the incompatibility be-
tween word alignments and syntactic structures (Denero and Klein, 2007; Fossum et al., 2008; Pauls et
al., 2010). The incompatibility arises because word alignments and syntactic structures are established
independently of each other. In the case of tree-to-tree models, there is also the issue of incompatible par-
allel tree structures resulting from divergent syntactic annotation standards that have been independently
conceived based on monolingual corpora (Chiang, 2010). In this paper, we report an effort in building
a Hierarchically Aligned Chinese-English Parallel Treebank (HACEPT) where we manually do word-
level and phrase-level alignments simultaneously on parallel phrase-based parse trees. In this process,
we attempt to establish an annotation standard that harmonizes word-level and phrase-level alignments.
We also analyze a common incompatibility issue between Chinese-English parallel parse trees exposed
in the annotation process, with the goal of solving the issue by semi-automatically revising the trees.
In the rest of this paper, we describe how we construct the HACEPT and discuss issues arising in the
construction process. In Section 2, we discuss the problems of word alignment done without considering
its interaction with syntactic structures. In Section 3, we describe our annotation procedure where we
perform word-level and phrase-level alignments simultaneously in a coordinated manner, and show how
our approach is free of the problems discussed in Section 2. In Section 4, we report a common incom-
patibility issue between parse trees and propose a solution. We also compare the issue with translation
divergence (Dorr, 1994) and show that they are different in nature and occurrence frequency. In Section
5, we present the results of two experiments we have done on our annotation to show the intuitiveness of
our approach and the linguistic and engineering potentials of our corpus. We then describe related work
in Section 6 and conclude our paper in Section 7.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1511
2 Incompatibilities between word alignments and syntactic structures
All the existing word alignment practice we know of treats word alignment as a stand-alone task with-
out systematically considering its interaction with the syntactic structure of a sentence. The inevitable
consequence of the practice is that both redundancies and incompatibilities between word alignments and
syntactic structures will arise in many places. In this section, we illustrate the issues through language-
particular function words, where the problems are most frequently found. Due to language-particular
idiosyncrasy and lack of lexical content, these function words usually do not have a translation coun-
terpart, which presents a great challenge to alignment annotation. There are two logical possibilities of
dealing with these words, both of which are represented in existing annotation practice. The first is to
leave them unaligned or link them to a fictitious NULL word (Ahrenberg, 2007; Brown et al., 1990), and
the second, which also seems to be the more common pratice, is to attach these function words to a word
that has a translation counterpart, and then align the function word and its host with the counterpart of
the host (Melamed, 1998; Li et al., 2009). For ease of discussion, below we will refer to the latter pratice
as the "glue-to-a-host" strategy (GTAHS). Both approaches are less than desirable: the former leaves the
function words unaccounted for, and the latter leads to incompatibility issues we discuss in detail below.
First note that, by attaching language-particular function words to a host, the GTAHS creates redun-
dancies between word alignments and syntactic structures since many of these function words have al-
ready been associated with a host within a constituent in the parse tree (e.g., the English determiner the
is placed inside the projection of its host, namely an NP). A more serious issue is that the GTAHS cre-
ates spurious ambiguities. Lexical ambiguity is inevitable in translation. For instance, the English noun
bank has more than one lexical meaning and each of the meanings corresponds to a different Chinese
word. That fact aside, the GTAHS creates spurious ambiguities, which, in our view, would be harmful
to Machine Translation (MT) if extracted as translation rules. Consider the following example, where
the Chinese noun?? is aligned to six English strings (aligned elements are underlined):
(1) a. eat apples <>???
b. eat an apple <>???
c. eat the apple <>???
d. fond of apples <>????
e. talk about apples <>????
f. provide them with apples <>?????
The English apple and the Chinese?? match in meaning and are both unambiguous. In cases where
the English noun is used with a determiner as in (1b) and (1c), since Chinese has no determiners and
the bare noun?? can be the appropriate translation for either an apple or the apple given a context,
the GTAHS attaches the determiner to apple and the whole string is aligned with??. In other similar
cases where an English element such as a preposition is absent in Chinese as in (1d), (1e) and (1f), the
GTAHS glues the preposition to apple and the whole PP is aligned with ??. With the GTAHS, the
unambiguous Chinese ?? ends up being aligned with more than one English string. This kind of
spurious ambiguity is very common given the GTAHS.
The second issue is that, by attaching function words to a host, the GTAHS effectively creates rudi-
mentary syntactic structures, which are often incompatible with the syntactic structures annotated based
on existing treebanking annotation standards. For example, all the aligned multi-word strings underlined
in (2) do not correspond to a constituent in a Penn TreeBank (Marcus et al., 1993) or Chinese TreeBank
(Xue et al., 2005) parse tree:
(2) a. If I were him <>???????
b. He is visiting Beijing <>??????
1512
c. the beginning of the new year <>????
d. to quickly and efficiently solve the problem <>?????????
Given the incompatibilities between existing word alignments and syntactic structures, in the next sec-
tion we describe an approach where we perform word-level and phrase-level alignments simultaneously
on parallel phrase-based parse trees, attempting to construct a hierarchically aligned corpus where word
alignments are harmonized with syntactic structures.
3 Annotation specification and procedure
The data we annotate is the Chinese-English portion of the Parallel Aligned Treebank (PAT) described
in (Li et al., 2012). Our data consists of two batches, one of which is weblogs and the other of which is
postings from online discussion forums. The English sentences in the data set are annotated based on the
original Penn TreeBank (PTB) annotation stylebook (Bies et al., 1995) as well as its extensions (Warner
et al., 2004), while the Chinese sentences in the data set are annotated based on the Chinese TreeBank
(CTB) annotation guidelines (Xue and Xia, 2000) and its extensions (Zhang and Xue, 2012). The PAT
only has word alignments, which are done under the GTAHS, and no phrase alignments.
The main departure of our approach is that we loosen the requirement that every word in a sentence
pair needs to be word-aligned. On the word level, we only align words that have an equivalent in terms
of lexical meaning and grammatical function. For words that do not have a translation counterpart, we
leave them unaligned and locate the appropriate phrases in which they appear to be aligned. This way, we
eliminate both the redundancies and spurious ambiguities discussed in Section 2. Since phrase alignment
is done between syntactic nodes on parallel parse trees, we also eliminate the incompatibilities between
word alignments and syntactic structures. See the discussion of the concrete example in Figure 1 below
to see the points made here.
Next we discuss our annotation procedure in detail. Our annotators are presented with sentence pairs
that come with parallel parse trees. The task of the annotator is to decide, first on the word level and then
on the phrase level, if a word or phrase needs to be aligned at all, and if so, to which word or phrase it
should be aligned. The decisions about word alignment and phrase alignment are not independent, and
must obey well-formedness constraints as outlined in (Tinsley et al., 2007):
a. A non-terminal node can only be aligned once.
b. if Node n
c
is aligned to Node n
e
, then the descendants of n
c
can only be aligned to descendants of
n
e
.
c. if Node n
c
is aligned to Node n
e
, then the ancestors of n
c
can only be aligned to ancestors of n
e
.
This means that once a word alignment is in place, it puts constraints on phrase alignments. A pair of
non-terminal nodes (n
c
, n
e
) cannot be aligned if a word that is a descendant of n
c
is aligned to a word
that is not a descendant of n
e
on the word level.
Let us use the concrete example in Figure 1 to illustrate the annotation process, which is guided by a set
of detailed annotation guidelines. On the word level, only those words that are connected with a dashed
line are aligned since they have equivalents. Note that the Chinese words? (a function word used to
prepose the object to the left of the verb),?? (an adverb meaning "this way"),? (a modal meaning
"can") and the English discourse connective so that, the auxiliary verb is and the preposition from are all
left unaligned on the word level. Aligning these function words will generate artificial ambiguous cases
and create incompatibilities between word alignments and parse trees that have already been illustrated
and discussed in Section 2. For instance, if? is to be word-aligned, it would be glued to the noun?
? and the whole string??? will be aligned to the English gravity. Note that both?? and gravity
are unambiguous and form a one-to-one correspondence. With the word alignment between ? ??
and gravity, we make the unambiguous gravity correspond to both ?? and ? ?? (and possibly
1513
.IP.
.PU
.?
.
.VP
c2
.
.VP
c9
.
.VP
c3
.
.VP
c10
.
.IP
.VP
c4
.
.NP
.NN
.??
.
.VV
.??
.
.NP
.NN
.??
.VV
.??
.
.VV
.?
.
.ADVP
.AD
.??
.
.PU
.,
.VP.
.VP
c1
.
.IP.
.VP.
.NP.
.NP
.NN
.??
.
.ADJP
.JJ
.??
.
.VV
.??
.
.NP
.NN
.??
.
.BA
.?
.ADVP
.AD
.?
.NP
.PN
.??
.PU
.,
.ADVP
.AD
.??
.S.
..
..
.
.VP
e2
.
.VP.
.SBAR.
.S
e3
.
.VP
e9
.
.VP
e10
.
.PP
e11
.
.S
.VP
e4
.
.ADVP
.RB
.downhill
.
.VBG
.running
.
.IN
.from
.
.VBN
.prohibited
.
.VB
.is
.
.NP
.NN
.sludge
.
.IN
.that
.IN
.so
.
.VP
e1
.
.NP
.NN
.gravity
.
.VV
.outlaw
.
.MD
.will
.
.NP
.PRP
.we
.ADVP
.RB
.Eventually
Figure 1: A hierarchically aligned sentence pair
1514
more strings), thus creating a spurious ambiguity. Also note that the string ? ?? does not form a
constituent in the Chinese parse tree, so the word alignment is incompatible with the syntactic structure
of the sentence. By leaving? unaligned, we avoid both the spurious ambiguity and the incompatibility.
With word alignments in place, next the annotator needs to perform phrase alignments. Note that word
alignments place restrictions on phrase alignments. For instance, e
9
and e
10
will be ruled out as possible
alignments for c
10
, because??, a descendant of c
10
, is aligned to sludge, which is not a descendant of
either e
9
or e
10
. By contrast, e
3
is a possible alignment for c
10
because the alignment does not violate the
well-formedness constraints. The annotator then needs to decide whether this possible phrase alignment
can be actually made. This is a challenging task since, for a given phrase, there usually are more than one
candidate from which a single alignment needs to be picked. For instance, for e
3
, there are in total three
possible phrase alignments, namely c
10
, c
3
and c
9
, all of which obey the well-formedness constraints.
Since a non-terminal node is not allowed to be aligned to multiple non-terminal nodes on the other side,
the annotator needs to choose one among all the candidates. This highlights the point that the alignment
of non-terminal nodes cannot be deterministically inferred from the alignment of terminal nodes. This is
especially true given our approach where some terminal nodes are left unaligned on the word level. For
instance, the reason why c
9
is a possible alignment for e
3
is because the word?? is left unaligned. If
?? were aligned with so that, c
9
could not be aligned with e
3
since so that is not a descendant of e
3
and aligning the two nodes will violate Constraint b.
While Constraints b and c can be enforced automatically given the word alignments, the decisions
regarding the alignment of non-terminal nodes which satisfy Constraint a are based on linguistic consid-
erations. One key consideration is to determine which non-terminal nodes encapsulate the grammatical
relations signaled by the unaligned words so that the alignment of the non-terminal nodes will effectively
capture the unaligned words in their syntactic context. When identifying non-terminal nodes to align,
we follow two seemingly conflicting general principles:
? Phrase alignment should not sever key dependencies involving the grammatical relation signaled
by an unaligned word.
? Phrase alignment should be minimal, in the sense that the phrase alignment should contain only the
elements involved in the grammatical relation, and nothing more.
The first principle ensures that the grammatical relation is properly encapsulated in the aligned non-
terminal nodes. For example in Figure 1, if we attach the English preposition from to running and aligning
them to??, we would fail to capture the fact that from signals a relation between prohibit and running
downhill. Aligning VP
c3
with S
e3
captures this relation.
The first principle in and of itself is insufficient to produce desired alignment. Taken to the extreme, it
can be trivially satisfied by aligning the two root nodes of the sentence pair. We also need the alignment
to be minimal, in the sense that aligned non-terminal nodes should contain only the elements involved
in the grammatical relation, and nothing more. These two requirements used in conjunction ensure that
a unique phrase alignment can be found for each unaligned word. The phrase alignments (VP
c1
, VP
e1
),
(VP
c2
, VP
e2
), (VP
c3
, S
e3
), as illustrated in Figure 1, all satisfy these two principles.
In addition to making phrase alignments, the annotator needs to assign labels to phrase alignments. We
have four labels that are designed along two dimensions: the presence/absence of word order difference
and the presence/absence of unaligned function words. The name and definition of each of the four labels
are listed below, and an example for each label is given in Figure 2:
a REO, reordering that does not involve unaligned function words (Figure 2a)
b UFW, unaligned function words (Figure 2b)
c REU, reordering that also involves unaligned function words (Figure 2c)
d STD, structural divergence due to cross-linguistic differences (Figure 2d)
1515
.VP
1
.
.VP
.VV
.??
.
.PP.
.NP.
.NP
.NN
.??
.
.DP
.DT
.?
.P
.?
.VP
1
.
.PP.
.NP.
.NN
.service
.
.DT
.the
.
.P
.for
.
.VB
.charge
(a) REO
.VP
.VSB.
.VV
.??
.
.VV
.??
.VP.
.S
.VP.
.VP
.VB
.fight
.
.TO
.to
.
.ADVP
.RB
.out
.VB
.go
(b) UFW
.NP.
.NP
.NN
.???
.
.DNP.
.DEG
.?
.NP.
.NN
.??
.NN
.??
.NP.
.PP.
.NP.
.NN
.economy
.
.NN
.world
.DT
.the
.
.IN
.of
.
.NP.
.NN
.powerhouse
.
.DT
.the
(c) REU
.IP
1
.VP.
.VP
.VV
.??
.
.ADVP
.AD
.??
.S
1
.
.VP.
.PP.
.NP
.PRP
.me
.
.P
.over
.
.VBG
.flowing
.
.NP.
.NN
.sweat
.
.DT
.the
(d) STD
Figure 2: Phrase alignment types
Figure 2a is an example where there is a reordering of the immediate children of the aligned VP nodes.
This is a very typical word order difference between Chinese and English. In Chinese, the PP modifier
is before the verb while in English the PP modifier is after the verb. The phrase alignment illustrated
by Figure 2b has an unaligned function word, namely the English infinitive marker to, which has no
counterpart in Chinese. There are both reordering (difference in the relative order of powerhouse and
economy) and unaligned function words (Chinese? and English of ) in the phrase alignment in Figure
2c. Figure 2d provides an example where the aligned phrases have structural divergence caused by cross-
linguistic differences between Chinese and English, which we will discuss in some detail in Section 4.
4 A common incompatibility issue between parse trees
During the annotation process, we encountered some incompatibility issues between parse trees. For
a comprehensive and detailed discussion of the issues, see (Deng and Xue, 2014). Here we report the
most common issue, which is caused by differences between treebank annotation guidelines. As already
mentioned, the English parse trees we use are annotated based on the original PTB annotation stylebook
(Bies et al., 1995) as well as its extensions (Warner et al., 2004), while the Chinese parse trees are
annotated based on the CTB annotation guidelines (Xue and Xia, 2000) and its extensions (Zhang and
Xue, 2012). Since PTB and CTB are independently annotated, there are some differences in how certain
structures are annotated. The main issue is that certain structures are so flat as to make some nodes that
should be aligned impossible to be aligned. In general, our alignment task favors deeper structures over
shallower ones so that the annotator can have more choices. This is an issue for both Chinese and English
parse trees. To get a concrete idea of the issue, take a look at Figure 3.
As shown by Figure 3, VP
c1
and the English string probably decrease rapidly with distance, and VP
e1
and the Chinese string????????, cannot be aligned although they match in meaning and
should be aligned. They cannot be aligned because there is no node for either of the two strings in the
respective parse tree. Note that the incompatibility between the two trees here is due to a difference in
annotation style but not a deep cross-linguistic difference. Both PTB and CTB simplified the annotation
task by making the tree structures flatter to increase annotation speed, but the simplification does not
always come from the same places. The consequence of these annotation decisions is that relevant struc-
tures are sometimes incompatible, which has negatively affected their utility for MT purposes (Chiang,
1516
.IP.
.VP
c1
.
.VP
.VV
.??
.
.ADVP
.AD
.??
.ADVP
.AD
.?
.PP.
.NP
.NN
.??
.
.P
.?
.ADVP
.AD
.??
.
.NP.
.NP
.NN
.??
.DP.
.CLP
.M
.?
.
.DT
.?
.S.
.VP
e1
.
.PP.
.NP
.NN
.distance
.
.IN
.with
.
.ADVP
.RB
.rapidly
.VBP
.decrease
.
.ADVP
.RB
.probably
.NP.
.NNS
.benefits
.
.DT
.The
Figure 3: Unalignable nodes due to differences in tree representation
2010).
To solve this incompatibility issue, we need to create more structures through binarization, which can
be done automatically. Still take Figure 3 for instance, on the English side, if we create a new VP by
combining VP
e1
and its sister ADVP, the resulting VP can be aligned with VP
c1
. On the Chinese side,
if we do binarization to create a VP that dominates the string????????, VP
e1
would have
an alignment. Since changing tree structures has the potential risk of causing inconsistency with parse
trees in the original treebanks and had better be done systematically after all the annotation is finished,
we have not done binarization as of the writing of this paper. For the time being, we assign the label
UA (short for Unalignable Node) to nodes which should be aligned but cannot be aligned so that we can
gather some statistics on the extent of the problem. We will come back to revisit the nodes carrying UA
such as VP
c1
and VP
e1
by proposing systematic changes to the original treebanks.
The UA case discussed above should not be confused with another case of incompatibility, namely
structural divergence between parallel sentences in translation (Dorr, 1994). As shown above, UA is
basically an artificial issue that is caused by difference in parsing guideline design and fixable through
automatic binarization. Structural divergence arises mainly due to genuine cross-linguistic differences.
We provide an example of structural divergence (STD) in Figure 2d. As shown in the figure, the two
aligned phrases (VP and S) are structurally quite different: the English string is a clause with the NP the
sweat as the subject and the VP flowing over me as the predicate (the example is taken out of the sentence
I felt the sweat flowing over me to save space). The Chinese string is a simple verb phrase where the
adverb?? (literally whole-body) modifies the verb?? (literally emerge-sweat). In terms of meaning
correspondence,?? expresses the meaning of the English PP over me and the verb matches in meaning
with the sweat flowing. We have run an experiment on STD and found that the STD cases are pretty rare
(on average 5 instances in a file with 500 sentence pairs), indicating that the structural difference between
Chinese and English is not so fundamental as to make a big impact on alignment annotation.
1517
5 Annotation experiments
We did two experiments on our annotation. The first is about inter-annotator agreement (IAA), which
is a way of both evaluating the annotation quality and judging the intuitiveness of the annotation task.
An unintuitive annotation task would force the annotator to make subjective choices, which would result
in low IAA. Since the annotation task involves parse trees, ideally we need annotators who are trained
in syntax, but that would put a constraint on the pool of qualified annotators and make it difficult for
the annotation to scale up. In our annotation experiments, we use four annotators who are fluent in both
English and Chinese but have no prior linguistic training, led by a syntactician who performs the final
adjudication.
As of this writing, we have completed the single annotation of 8,932 sentence pairs, 2,500 of which
are double annotated. The IAA statistics presented in Table 1 are based on the double-annotated 2,500
sentence pairs, which are divided into 5 chunks of 500 sentence pairs each. The statistics are for phrase
alignment only, and the micro-average for the 5 chunks is 0.87 (F1), indicating we are able to get good
quality annotation for this task. In addition, the agreement statistics for the 5 chunks are very stable,
even though they are performed by different pairs of annotators, indicating we are getting consistent
annotation from different annotators.
Table 2 shows the result of the second experiment, namely the distribution of the different types of
phrase alignment. It shows that alignments that contain unaligned function words outnumber those that
do not, and that alignments that do not involve reordering outnumber those that do. It also shows that an
overwhelming number of alignments that involve reordering also have unaligned function words. This
means that the function words are potentially useful "triggers" for reordering, which is an important issue
that MT systems are trying to address.
Chunk No. precision recall F1-measure
1 0.91 0.86 0.89
2 0.92 0.80 0.86
3 0.89 0.89 0.89
4 0.88 0.88 0.88
5 0.89 0.89 .086
micro-average 0.90 0.85 0.87
Table 1: Statistics of IAA
Annotator +UFW -UFW total
+REO
1 6,473 379 6,852
2 6,670 379 7,049
-REO
1 7,328 6,872 14,200
2 7,797 7,334 15,131
total
1 13,801 7,251 21,052
2 14,467 7,713 22,180
Table 2: Statistics of phrase alignment by types
6 Related work
Parallel treebanks are not something new. However, most of the existing parallel treebanks (Li et al.,
2012; Megyesi et al., 2010) do not have phrase alignments. Some (Sulger et al., 2013; Kapanadze, 2012)
do have phrase alignments, but neither discussion about the interaction between word-level and phrase-
level alignments nor report of IAA is provided. There have been a few recent attempts at automatically
aligning subtrees (comparable to our phrases) in the context ofMT research, and the automatic alignments
are evaluated against a small manually aligned data set. For example, (Tinsley et al., 2007) evaluated
an unsupervised algorithm on 810 parsed English-French pairs annotated with subtree alignment. (Xiao
and Zhu, 2013) also developed unsupervised subtree alignment methods (EM and Variational Bayes)
and evaluated their automatic alignment model on 637 sentences from the Chinese TreeBank (and use
the other 99 for tuning). (Sun et al., 2010b; Sun et al., 2010a) also report work on aligning subtrees
and evaluate their impact on MT. However, we are not aware of any attempt to systematically harmonize
word alignment with the alignment of phrases, or subtrees, or to systematically study the incompatibilities
between parallel parse trees.
1518
7 Conclusion
In this paper we report our effort on the construction of a Chinese-English parallel treebank with both
word-level and phrase-level alignments. When constructing the treebank, we systematically consider
the interaction between word alignments and phrase alignments, and try to harmonize the two kinds of
alignments by removing redundancies and incompatibilities between them. We show that this type of
annotation can be performed with high inter-annotator consistency. Given our intention for the treebank
to be a resource for MT, the next step is to synchronize the parallel parse trees, and of course, to perform
automatic hierarchical alignment experiments and MT experiments.
Acknowledgements
This work is supported by the IBM subcontract No. 4913014934 under DARPA Prime Contract No.
0011-12-C-0015 entitled "Broad Operational Language Translation". We would like to thank Libin Shen
and Salim Roukos for their inspiration and discussion during early stages of the project, Abe Ittycheriah
and Niyu Ge for their help with setting up the data, Loretta Bandera for developing and maintaining the
annotation tool, and three anonymous reviewers for their helpful comments. We are grateful for the hard
work of our four annotators: Hui Gao, Shiman Guo, Tse-ming Wang and Lingya Zhou. Any opinions,
findings, conclusions or recommendations expressed in this material are those of the authors and do not
necessarily reflect those of the sponsor or any of the people mentioned above.
References
Lars Ahrenberg. 2007. LinES: An English-Swedish parallel Treebank. In Proceedings of Nodalida 2007, pages
270--273, Tartu, Estonia.
Ann Bies, Mark Ferguson, Karen Katz, Robert MacIntyre, Victoria Tredinnick, Grace Kim, Mary Ann
Marcinkiewicz, and Britta Schasberger. 1995. Bracketing guidelines for Treebank II style Penn Treebank
project. Technical report, University of Pennsylvania.
Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, John D. Lafferty,
Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine translation. Computational
Linguistics, 16(2):79--85.
David Chiang. 2010. Learning to translate with source and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics, pages 1443--1452.
John Denero and Dan Klein. 2007. Tailoring word alignments to syntactic machine translation. In Proceedings
of the 45th Annual Meeting of the Association for Computational Linguistics, pages 17--24.
Dun Deng and Nianwen Xue. 2014. Aligning Chinese-English parallel parse trees: is it feasible? In Proceedings
of the 8th Linguistic Annotation Worshop (the LAW VIII).
Bonnie J. Dorr. 1994. Machine translation divergences: a formal description and proposed solution. Computa-
tional Linguistics, 20(4):597--633.
Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computational Linguistics, pages 205--208.
Victoria Fossum, Kevin Knight, and Steven Abney. 2008. Using syntax to improve word alignment precision for
syntax-based machine translation. In Proceedings of the Third Workshop on Statistical Machine Translation,
pages 44--52.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What's in a translation rule? In HLT-
NAACL, pages 273--280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer.
2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of the 21st
International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for
Computational Linguistics, pages 961--968.
1519
LiangHuang, KevinKnight, andAravind Joshi. 2006. Statistical syntax-directed translationwith extended domain
of locality. In Proceedings of AMTA, pages 66--73.
Oleg Kapanadze. 2012. Building parallel Treebanks for the lesser-resourced languages. Technical report, Univer-
sit?t des Saarlandes.
Xuansong Li, Niyu Ge, and Stephanie Strassel. 2009. Tagging guidelines for Chinese-English word alignment.
Technical report, Linguistic Data Consortium.
Xuansong Li, Stephanie Strassel, Stephen Grimes, Safa Ismael, Mohamed Maamouri, Ann Bies, and Nianwen
Xue. 2012. Parallel Aligned Treebanks at LDC: New Challenges Interfacing Existing Infrastructures. In
Proceedings of LREC-2012, Istanbul, Turkey.
Ding Liu and Daniel Gildea. 2008. Improved tree-to-string transducer for machine translation. In Proceedings of
the Third Workshop on Statistical Machine Translation, pages 62--69.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation.
In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting
of the Association for Computational Linguistics, pages 609--616.
Yang Liu, Yajuan L?, and Qun Liu. 2009. Improving tree-to-tree translation with packed forests. In Proceedings
of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages 558--566.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a Large Annotated Corpus
of English: The Penn Treebank. Computational Linguistics, 19(2):313--330.
Beata Megyesi, Bengt Dahlqvist, Eva A. Csato, and Joakim Nivre. 2010. The English-Swedish-Turkish Parallel
Treebank. In Proceedings of LREC-2010, Valletta, Malta.
I. Dan Melamed. 1998. Annotation style guide for the Blinker project. Technical report, University of Pennsyl-
vania.
Adam Pauls, Dan Klein, David Chiang, and Kevin Knight. 2010. Unsupervised syntactic alignment with inversion
transduction grammars. InHuman Language Technologies: The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguistics, pages 118--126.
Sebastian Sulger, Miriam Butt, Tracy Holloway King, and Paul Meurer et.al. 2013. ParGramBank: The ParGram
Parallel Treebank. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,
pages 550--560.
Jun Sun, Min Zhang, and Chew Lim Tan. 2010a. Discriminative induction of sub-tree alignment using limited
labeled data. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 1047--
1055.
Jun Sun, Min Zhang, and Chew Lim Tan. 2010b. Exploring syntactic structural features for sub-tree alignment
using bilingual tree kernels. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics, pages 306--315.
John Tinsley, Ventsislav Zhechev, Mary Hearne, and Andy Way. 2007. Robust language pair-independent subtree
alignment. In Proceedings of Machine Translation Summit XI.
Colin Warner, Ann Bies, Christine Brisson, and Justin Mott. 2004. Addendum to the Penn Treebank II style
bracketing guidelines: BioMedical Treebank annotation. Technical report, University of Pennsylvania.
Tong Xiao and Jingbo Zhu. 2013. Unsupervised sub-tree alignment for tree-to-tree translation. Journal of Artifi-
cial Intelligence Research, 48:733--782.
Nianwen Xue and Fei Xia. 2000. The bracketing guidelines for Penn Chinese Treebank project. Technical report,
University of Pennsylvania.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha Palmer. 2005. The Penn Chinese Treebank: Phrase Structure
Annotation of a Large Corpus. Natural Language Engineering, 11(2):207--238.
Xiuhong Zhang and Nianwen Xue. 2012. Extending and scaling up the chinese treebank annotation. In Proceed-
ings of the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing.
1520
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1902?1911,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Automatic Inference of the Tense of Chinese Events Using Implicit
Linguistic Information
Yuchen Zhang
Brandeis University
415 South Street
Waltham, MA
yuchenz@brandeis.edu
Nianwen Xue
Brandeis University
415 South Street
Waltham, MA
xuen@brandeis.edu
Abstract
We address the problem of automatically
inferring the tense of events in Chinese
text. We use a new corpus annotated with
Chinese semantic tense information and
other implicit Chinese linguistic informa-
tion using a ?distant annotation? method.
We propose three improvements over a rel-
atively strong baseline method ? a statisti-
cal learning method with extensive feature
engineering. First, we add two sources
of implicit linguistic information as fea-
tures ? eventuality type and modality of
an event, which are also inferred automat-
ically. Second, we perform joint learning
on semantic tense, eventuality type, and
modality of an event. Third, we train arti-
ficial neural network models for this prob-
lem and compare its performance with
feature-based approaches. Experimental
results show considerable improvements
on Chinese tense inference. Our best per-
formance reaches 68.6% in accuracy, out-
performing a strong baseline method.
1 Introduction
As a language with no grammatical tense, Chinese
does not encode the temporal location of an event
directly in a verb, while in English, the grammati-
cal tense of a verb is a strong indicator of the tem-
poral location of an event. In this paper we ad-
dress the problem of inferring the semantic tense,
or the temporal location of an event (e.g., present,
past, future) in Chinese text. The semantic tense is
defined relative to the utterance time or document
creation time, and it does not always agree with
the grammatical tense in languages like English
where there is grammatical tense. Inferring se-
mantic tense potentially benefits natural language
processing tasks such as Machine Translation and
Information Extraction (Xue, 2008; Reichart and
Rappoport, 2010; Ye et al., 2006; Ye, 2007; Liu et
al., 2011), but previous work has shown that auto-
matic inference of the semantic tense of events in
Chinese is a very challenging task (Xue, 2008; Ye
et al., 2006; Liu et al., 2011).
There are at least two reasons why this is a dif-
ficult problem. First, since Chinese does not have
grammatical tense which could serve as an impor-
tant clue when annotating the semantic tense of
an event, generating consistent annotation for Chi-
nese semantic tense has proved to be a challenge.
Xue and Zhang (2014) use a ?distant annotation?
method to address this problem. They take advan-
tage of an English-Chinese parallel corpus with
manual word alignments (Li et al., 2012) , and per-
form annotation on the English side, which pro-
vides more explicit information such as grammati-
cal tense that helps annotators decide the appropri-
ate semantic tense. The annotations are then pro-
jected to the Chinese side via the word alignments.
They show consistent annotation agreements on
semantic tense. Second, the lack of grammatical
tense also makes automatic inference of Chinese
semantic tense challenging since the grammatical
tense would be an important source of information
for predicting the semantic tense. Previous work
has shown that it is very difficult to achieve high
accuracy using standard machine learning tech-
niques such as Maximum Entropy and Conditional
Random Field classifiers combined with extensive
feature engineering.
We address these challenges in two ways. First
of all, we take advantage of the newly annotated
corpus described in (Xue and Zhang, 2014) in
which semantic tense is annotated together with
eventuality type and modality using the distant an-
notation method. This makes it possible to use
these two additional sources of information to help
predict tense. Eventuality type and modality are
intricately tied to tense. For example, Smith and
1902
Erbaugh (2005) show that states by default hold in
the present but (episodic) events occur by default
in the past. This means knowing the eventuality
type of an event would help determine the tense.
Eventuality type and modality are also annotated
on the English side and then projected onto the
Chinese side via manual word alignments, taking
advantage of the rich morphosyntactic clues in En-
glish. High inter-annotator agreement scores are
also reported on eventuality type and modality.
We experimented with two ways of using even-
tuality type and modality information. In the first
approach, we first train statistical machine learn-
ing models to predict eventuality type and modal-
ity and then use these two sources of information
as features to predict semantic tense. In the sec-
ond approach we trained joint learning models be-
tween semantic tense and eventuality type, and be-
tween semantic tense and modality. We show both
approaches improve the tense inference accuracy
over a baseline where these two sources of infor-
mation are not used. Second, in our statistical
machine learning experiments on tense inference
using feature engineering, we find that the design
of feature templates has great influence on the re-
sults. So in order to explore more possible feature
combinations and mitigate the feature engineering
work, we apply artificial neural network models to
this problem. This shows improvements on tense
inference accuracy as well in some of the experi-
ment settings.
The rest of the paper is organized as follows.
Section 2 discusses related work in automatic
tense inference. Section 3 briefly introduces the
distant annotation method. In section 4, we de-
scribe our experiments and analyze the experimen-
tal results. We conclude this paper in section 7.
2 Related Work
Inferring the semantic tense of events in Chinese
text is not a new topic. There have been several
attempts at it, yet high accuracy in this task has
proved to be elusive. Using a corpus with tense
annotated directly in Chinese text, Xue (2008) per-
formed extensive feature engineering in a machine
learning framework to address this problem. They
used both local lexical features and structured fea-
tures extracted from manually annotated syntactic
parsing trees. In our baseline method, we adopt
most of their features as the baseline, only on a
new corpus in which semantic tenses are not an-
notated directly on Chinese events but projected
from annotations from the English side of a par-
allel Chinese-English corpus. In our experiments,
we also use structural features extracted from au-
tomatic parse trees, so our experimental settings
are more realistic.
Ye et al. (2006) took a similar approach in
which they predict tense with feature engineering
in a statistical learning framework. They also used
a Chinese-English parallel corpus and projected
tense for English events onto Chinese events via
human alignments. The main difference between
their data and ours is that they used the gram-
matical tense of the English events, while we use
human-annotated semantic tense which we believe
are more ?transferrable? across languages as it
is free of the language-specific idiosyncrasies of
grammatical tense. In addition, they also used hu-
man annotated linguistic information as ?latent?
features in their work, which are similar to our
implicit linguistic features. However, the ?latent?
features that they used in their system are human-
annotated, while the eventuality type and modality
features in our system are predicted automatically.
Another difference is that they ignored events that
are not verbs. For example, they excluded ver-
bal expressions in Chinese that are translated into
nominal phrases in English. In contrast, we kept
all events in our data, and they can be realized as
verbs, nouns, as well as words in other parts of
speech. We performed separate experiments on
events realized as verbal expressions and events
not in verbal expressions to investigate their im-
pact on semantic tense inference.
Liu et al. (2011) introduced more global fea-
tures in a machine learning framework, and on
top of that proposed an iterative learning algorithm
which better handles noisy data, but they also ig-
nored events that are not realized as verbal ex-
pressions, or events that are verbal expressions but
have more than one verb in them. They mainly
focused on events that are one-verb expressions.
In a similar work on inferring tense in English
text, Reichart and Rappoport (2010) aimed at in-
ferring fine-grained semantic tenses for events in
English. They introduced a fine-grained sense tax-
onomy for tense in a more general Tense Sense
Disambiguation (TSD) task to annotate and dis-
ambiguate semantic tenses. The underlying senses
include ?things that are always true?, ?general and
repeated actions and habits?, ?plans, expectations
1903
and hopes?, etc., which encode a combination of
tense, eventuality type and modality. In the corpus
that we use, the same information is organized in
a more structured manner along three dimensions
? semantic tense, eventuality type, and modality.
3 Distant Annotation
Figure 1 shows the distant annotation procedure
from (Xue and Zhang, 2014). Starting with a
word-aligned parallel English-Chinese corpus, all
sentences are part-of-speech (POS) tagged first
and then all verb instances in the English text as
well as expressions aligned with verb instances
on the Chinese side are targeted for annotation.
As we will show in Section 4, these expressions
include verbs as well as nouns, prepositions and
word sequences ?headed? by a verb. We consider
those expressions as events. Annotators work only
on the English side and tag every event with a
pre-defined semantic tense label. These labels are
then projected from the English side to the Chi-
nese side via word alignments. The resulting cor-
pus contains events annotated with semantic tense
labels in both languages. Categories for seman-
tic tense are ?Past?, ?Present?, ?Future?, ?Relative
Past?, ?Relative Present?, ?Relative Future?, and
?None?.
Events annotated with relative tenses are also
linked to another event that serves as the tempo-
ral reference for the event in question. In some
cases the relative tense can be resolved to an ab-
solute tense. For example, if an event is anno-
tated with a ?relative past? tense to a reference
event that is annotated with a present tense, then
the semantic tense of that event can resolve to an
absolute ?past? tense. In other cases, they can
not be resolved. For example, if an event is la-
beled with a ?relative future? tense and the refer-
ence event has a past tense, then its tense cannot
be resolved to an absolute tense, which is defined
with regard to the utterance time or document cre-
ation time. In our work, where possible, we re-
solve these links and keep only absolute tense la-
bels. For events with relative tenses that can not
be resolved (i.e. events which are ?Relative Fu-
ture? to ?Past? events, or events which are ?Rela-
tive Past? to ?Future? events), we use ?None? as
the default label.
Eventuality type and modality are labeled in the
same way as auxiliary annotation that can help
with the inference of tense. Labels for eventual-
ity type include ?Episodic?, ?Habitual?, ?State?,
?Progressive?, ?Completed?, and ?None?. Labels
for modality are ?Actual?, ?Intended?, ?Hypothet-
ical?, ?Modalized?, and ?None?. Readers are ref-
ered to (Xue and Zhang, 2014) for detailed expla-
nations of each label.
Figure 1: Distant annotation procedure.
As we mentioned in Section 2, in this corpus
not only verbs but also their counterparts on the
opposite language are considered as events, yield-
ing events that may not be verbs. For example,
in the following sentence pair (1), the Chinese
verb (VV) ???? is aligned with an English noun
(NN) ?use?. In the sentence pair (2), the English
verb (VBG) ?opening? is aligned with an Chinese
noun (NN) ????.
(1) Statistics show that , in the past five years ,
Guangxi?s foreign trade and its use of foreign
investments has expanded rapidly.
?????????????????
???(li4yong4)?????????
(2) Beihai has already become a bright star aris-
ing from China?s policy of opening up to the
outside world.
???????????(kai1fang4) ?
????????
In this corpus, events could be either one verb,
or a verb compound, or a verb sequence ?headed?
by a verb, or even nouns and words of other parts
of speech.
4 Experiments
4.1 Experimental Setting
Xue and Zhang (2014) annotated semantic tense,
eventuality type and modality on top of the Par-
allel Aligned Treebank (Li et al., 2012), a corpus
1904
of word-aligned Chinese-English sentences tree-
banked based on the Penn TreeBank (Marcus et
al., 1993) and the Chinese TreeBank (Xue et al.,
2005) standards. Human annotation of tense is
performed on the newswire and webblog sections
of this corpus. They report that the average pair-
wise agreement among three annotators consis-
tently stays above 80% and the average Kappa
score consistently exceeds 70%, indicating reli-
able annotation.
Apart from using the entire corpus, we also con-
ducted experiments on three different subsets of
the corpus. An examination of the data indicates
that newswire data is grammatically more for-
mal and complete than webblog data, so we also
conducted separate experiments on newswire data
only. Considering that the diversity of the parts
of speech of the events may affect the inference
accuracy and that most of our features extracted
from the parse trees assume that our events being
verbs, we also conducted experiments exclusively
on ?v events?. ?v events? consist of two parts.
One part is events that are realized as a single word
and the word is a verb; the other one is events
which have multiple words but there is only one
verb among them. In the latter case, we stripped
off words tagged with other parts of speech and
only keep the verbs as events. This makes it more
effective to use features from previous work that
are designed for single verbs. One such feature is
the aspect marker. Distinctions between newswire
and webblog data and between v events and other
events are further explored in Section 5.1 and Sec-
tion 5.2. Table 1 presents the statistics for each
subset of the experimental data.
dataset # of v events # of all events
nw 6,686 8,268
all 17,153 20,885
Table 1: Statistics of four subsets of the annotated
corpus (Chinese side). ?nw? denotes the newswire
data. ?v events? denotes events that consist of or
can be reduced to only a single verb.
For each subset, randomly selected 80% were
used as the training set, while 10% were used as
the development set and 10% were used as the test
set.
4.2 Baseline
Based on previous approaches on Chinese tense
inference, we used a Maximum Entropy model
with extensive feature engineering as our baseline
method. We use the implementation of the Maxi-
mum Entropy algorithm in Mallet
1
for our exper-
iments. The corpus is parsed using the Berkeley
Parser for the purpose of extracting structure fea-
tures. Since the Parallel Alignment TreeBank is
a subset of the Chinese TreeBank (CTB) 8.0, we
automatically parsed the CTB 8.0 by doing a 10-
fold cross validation. The bracketing F-score is
80.5%. Feature extractions are performed on the
automatic parse trees. Adopted features include
previous word and its POS tag, next word and
its POS tag, aspect marker following the event,
?following the event, the governing verb of the
event, the character string of the main verb in the
previous clause that is coordinated with the clause
the event is in, whether the event is in quote, and
left modifiers of the event including head of adver-
bial phrases, temporal noun phrases, prepositional
phrases, localizer phrases, as well as subordinat-
ing clauses. Readers are referred to (Xue, 2008)
for details of these features. Since in this corpus
an event can span over more than one verb, we
also use the character string and the POS string of
the entire event instead of one word and one POS
tag as features.
? The character string of an event ? it could
be one or more words. In our corpus, only
69.7% events consist of single word (e.g. ??
??, ?live?), the other 30.3% of the events are
expressed with two or more words (e.g. ??
?+??, ?have caused?).
? The POS string of an event ? it could be
verbs, nouns, or POS sequences of other
word sequences. Table 2 shows the top ten
POS tag or POS tag sequences with example
word or word sequences.
Other features that we used in the baseline sys-
tem are as follows.
? DEC ? if the word immediately following an
event has the POS tag ?DEC?, use its charac-
ter string as a feature. In most cases, ?DEC?
is the POS tag for ??? when it used as a com-
plementizer marking the boundary in a rela-
tive clause. This feature implies that an event
1
http://mallet.cs.umass.edu/
1905
POS freq examples
VV 48.2% ??(live)
NN 5.8% ??(opening)
VC 5.2% ?(is)
VV+AS 5.2% ??+?(have caused)
VV+DEC 3.2% ??+?(isolated)
AD+VV 3.0% ??+??(is suggesting)
VA 3.0% ?(is big)
AD 2.0% ??(seemed)
VE 1.9% ?(there is)
P 1.8% ??(according to)
Table 2: Frequencies and examples of the ten most
frequent POS tag or POS tag sequences for events
in our corpus.
is inside a relative clause modifying a noun
phrase and it is more often stative than even-
tive.
? Determiners ? we find the subject of an event
from its parse tree and extract the determiner
of the subject, if there is one, as a fea-
ture. This feature indicates different types
of agents, and different types of agents of-
ten signal different types of events. For ex-
ample, individual agents tend to perform one-
time episodic actions which are by default lo-
cated in the past or described by a state in
the present, while multiple agents tend to in-
volved in habitual actions that spans over a
long period of time.
Baseline results are reported in Table 5 and Ta-
ble 6, in MaxEnt b rows.
4.3 Eventuality Type and Modality as
Features
Xue and Zhang (2014) reports that gold eventual-
ity type and modality labels significantly help the
inference of tense in Chinese, improving the ac-
curacy by more than 20%. However, it is unreal-
istic to expect to have human annotated eventual-
ity type and modality labels in a random new data
set if we want to use these two sources of implicit
linguistic information in any Chinese text. So we
trained statistical learning models to automatically
extract these two labels. We trained Maximum En-
tropy models and ran a 10-fold cross validation on
the entire corpus in order to get automatic labels
for every event. Feature used for labeling modal-
ity are as follows. Table 3 shows the average ac-
curacies for automatic modality labeling.
? The character string of an event.
? The POS string of an event.
? The character string of an event?s governing
verb and its POS tag.
? Whether the event is in a conditional clause.
If an event is in a subtree with the func-
tional tag ?CND?, return ?True?; otherwise,
return ?False?. This feature indicates that the
event?s modality label is ?Hypothetical?.
? Whether the event is in a purpose or reason
clause. If an event is in a subtree with the
functional tag ?PRP?, return ?True?; other-
wise, return ?False? as a feature. This feature
indicates the event?s modality label is ?In-
tended?.
? Whether the event string is the start of a sen-
tence. If an event is the start of a sentence, re-
turn ?True?; otherwise, return ?False?. Sen-
tences that start with an event is often impera-
tive, and the event generally has ?modalized?
modality label.
dataset v events all events
nw 81.1% 81.2%
all 75.4% 76.4%
Table 3: Average modality labeling accuracy, us-
ing a 10-fold cross validation.
Statistics show that the five labels for modality
have a skewed distribution in this corpus. Among
all events, 67.3% of them fall in the ?Actual? cat-
egory, while the events of all the other categories
are around or less than 10%. Similar distributions
are found in all four subsets of the data. Still, com-
pared with always choosing the most frequent la-
bel (around 67% accuracy), we still get a big im-
provement from our statistical model, even though
only a very simple set of features are used.
Features used for labeling eventuality type are
as follows. Table 4 shows the average accuracies
for automatic eventuality type labeling.
? The character string of an event.
? The POS string of an event.
1906
? Adverbs on the left that modifies the event.
? Aspect marker following the event
? Whether the event is Inside a relative clause.
If an event is in a CP subtree with the word
??? and POS tag ?DEC? as its last node,
return ?True?; otherwise, return ?False?.
Events in relative clauses modifying a noun
phrase and tend to be more often stative than
eventive.
dataset v events all events
nw 68.7% 67.7%
all 65.3% 65.1%
Table 4: Average automatic eventuality type label-
ing accuracy using a 10-fold cross validation.
The six labels of eventuality type are also dis-
tributed unevenly. The first group of columns in
Figure 3 shows the distribution of all events. Over
65% of events are either ?Episodic? or ?State?,
while the other types of events are less than 15%.
There are two categories that are even less than
5%. However, even though we only use some sim-
ple features, our model still beats the most fre-
quent label baseline (around 35% accuracy) by a
big margin, as shown in Table 4.
Tense inference accuracies using automatic
eventuality type and/or modality features are re-
ported in Table 5 and Table 6, in MaxEnt e, Max-
Ent m, and MaxEnt em rows.
4.4 Joint Learning
Apart from using eventuality type and modality la-
bels as features, we also conducted joint learning
experiments on them. Joint learning are applied
on 1) tense and eventuality type, and 2) tense and
modality. Features used are the union of the two
sets of features in inferring each single label. Max-
Ent jle and MaxEnt jlm rows in Table 5 and Table
6 present the experimental results on joint learn-
ing.
4.5 Artificial Neural Network
For each of the experiments using the maximum
entropy algorithm, we conducted a neural network
experiment using the same setting in order to ex-
plore more possible feature combinations and mit-
igate the feature engineering work. We convert
the features in each of our tense inference meth-
ods into feature vectors. If a feature is not a word,
we use a one-hot representation for that feature (a
vector with all 0s except for a 1 at the place of the
feature?s index in our feature lexicon). If a feature
is a word, we convert it into a word embedding. To
get a dictionary of word embeddings, we use the
word2vec tool
2
(Mikolov et al., 2013) and train it
on the Chinese Gigaword corpus (LDC2003T09).
For each word embedding, a 300-dimensional vec-
tor is used. Artificial neural networks are built us-
ing the theano package
3
(Bergstra et al., 2010).
We use 5000 hidden units for all networks and set
the learning rate ? = 0.01. Experimental results
are presented in the ANN rows of Tables 5 and 6.
5 Results Analysis
A comparison of the baseline accuracy for the four
different subsets of the data shows that (1) tense
inference is slightly better on v events than on all
events, but the difference is not substantial; and
(2) tense inference on newswire data performs bet-
ter than on all data by around 8% on v events and
around 5% on all events, verifying our assump-
tion that automatic tense inference is easier on
newswire data than webblog data. Although our
experiments are performed on different data sets
from that of previous work, our baseline method
still shows strong results compared with previous
work (Xue, 2008; Ye et al., 2006; Liu et al., 2011).
Adding automatic eventuality type and modal-
ity labels as features for semantic tense inference
leads to improvements over the baseline on all four
data subsets. In fact they provide considerable
improvements (around 2% increase) on newswire
v events dataset. MaxEnt e rows report results
when only automatic eventuality type is added
as a feature, and MaxEnt m rows report results
when only automatic modality is added as a fea-
ture. They both outperform (or, in several datasets,
match) the baseline results on all datasets. Max-
Ent em rows report results when both automatic
linguistic labels are added as features, and they
show further improvements over when only one
source of information is used. Analysis of the
results shows again that tense inference accuracy
is higher than webblog data under this experi-
ment condition. The results also show that after
adding eventuality type and modality as features,
2
http://code.google.com/p/word2vec/
3
http://deeplearning.net/software/theano/
1907
method all data nw data
MaxEnt b 58.9% 66.8%
MaxEnt e 59.5% 67.9%
MaxEnt m 59.5% 67.1%
MaxEnt em 59.6% 68.6%
MaxEnt jle 59.6% 63.5%
MaxEnt jlm 60.5% 66.9%
MaxEnt ge 74.6% 77.4%
MaxEnt gm 66.6% 70.0%
MaxEnt gem 76.2% 76.9%
ANN b 63.4% 67.2%
ANN e 62.6% 66.1%
ANN m 63.4% 59.8%
ANN em 59.7% 68.3%
ANN jle 62.7% 64.5%
ANN jlm 62.0% 65.6%
Table 5: Accuracy of tense inference on v events.
Best performances for each group of methods are
in bold.
the improvements on v events (0.7% and 1.8%)
are much bigger than that on all events (0.2% and
0.4%), regardless of the data genre (newswire or
weblog).
In order to test the potential for these two new
features, we also conducted experiments using
gold eventuality type and/or modality labels as
features for the Maximum Entropy models (Table
5 and Table 6, MaxEnt ge, MaxEnt gm, and Max-
Ent gem rows.). They outperform our best Max-
Ent results by around 10% on newswire data and
around 15% on all data, indicating strong poten-
tials for more accurately classified automatic even-
tuality type and modality labels.
Results also show that joint learning with
modality proves to be working better than the
baseline (Table 5 and Table 6, MaxEnt jle, Max-
Ent jlm). In fact, on the datasets with all events,
joint learning with modality produces the highest
accuracy among all approaches. However, joint
learning with eventuality is even worse than the
baseline. One possible explanation is that the
lower eventuality type classification accuracy af-
fects the tense inference accuracy. We also believe
there is still room for improvement with features
tuned for the joint learning model. Simply adding
the features may not be the best strategy.
On the entire dataset, regardless of v events or
other events, results of the neural network models
show improvements over the maximum entropy
method all data nw data
MaxEnt b 59.7% 65.1%
MaxEnt e 59.9% 65.1%
MaxEnt m 59.9% 65.4%
MaxEnt em 59.9% 65.5%
MaxEnt jle 59.7% 62.7%
MaxEnt jlm 60.4% 65.6%
MaxEnt ge 75.3% 76.1%
MaxEnt gm 67.1% 69.0%
MaxEnt gem 76.2% 75.9%
ANN b 63.0% 64.0%
ANN e 63.2% 66.9%
ANN m 60.1% 64.7%
ANN em 57.8% 66.1%
ANN jle 61.4% 63.0%
ANN jlm 62.9% 63.5%
Table 6: Accuracy of tense inference on all events.
Best performances for each group of methods are
in bold.
models under most experimental conditions. A
clear trend is that artificial neural networks help
more on all data than on newswire data only, in-
dicating greater potentials of the neural network
models to select and combine features with care-
fully trained parameters, given noisier but larger
training sets.
Experimental results also show significant dif-
ferences in accuracy between newswire data and
webblog data, and smaller but still recognizable
difference between v events and all events. There-
fore, we specifically look into distinctions be-
tween these data sets.
5.1 Newswire Data vs. Webblog Data
Considering the big gap in accuracy between
newswire and webblog data in our baseline results,
we delve deeper into the data and found several
major distinctions between these two domains that
might have contributed to the rather significant dif-
ference in performance on tense inference. First,
we look into the word frequency distribution of the
two datasets. Here by ?word? we mean the char-
acter string of an event. We find that both datasets
have a small portion of words with high frequen-
cies, but the webblog dataset contains much more
low-frequent words than the newswire dataset. In
Figure 2, the x-axis shows possible frequencies of
words and the y-axis shows the number of words at
a particular frequency. It can be seen that the num-
1908
ber of words that appear only once in the webblog
dataset is about three times as large as that in the
newswire dataset. The entire newswire dataset has
a vocabulary of only 2671 entries, while the web-
blog dataset has a vocabulary size of 6117. This
greatly reduces the coverage of features extracted
from the training dataset on the events in the test
dataset.
Second, webblog data contains more events
that are ?inherently? ambiguous on temporal lo-
cation. Among four possible labels for tense
in this corpus, ?None? is for events whose tem-
poral locations are not clear even to human an-
notators. Statistics show that in webblog data
about 13.4% of the events are tagged as ?None?,
while in newswire data only around 6.7% are
?None?. Another piece of evidence showing web-
blog data is harder to process is the different inter-
annotator agreement scores for tense annotation
on newswire and webblog data reported by (Xue
and Zhang, 2014). Newswire data has a 89.0%
agreement score and a 84.9% kappa score, while
webblog data only has a 81.0% agreement score
and a 72.7% kappa. Third, automatic parse trees
for newswire data is also more accurate than that
for webblog data. The bracketing F-score of au-
tomatically parsed newswire data is 83.0% while
it is only 80.4% for weblog data. Moreover, sen-
tences in newswire data are more grammatically
complete. Analysis shows that webblog data has
more dropped constituents in sentences. There
are around 40.5% sentences in newswire data that
have nominal empty categories, while in webblog
data the number is 48.1%. Dropped constituents
affect the structures of parse trees and some of
the features, which can affect tense inference ac-
curacy.
0	 ?
500	 ?
1000	 ?
1500	 ?
2000	 ?
2500	 ?
3000	 ?
3500	 ?
4000	 ?
4500	 ?
5000	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ? 6	 ? 7	 ? 8	 ? 9	 ? 10	 ?
wo
rd	 ?
cou
nt 
word	 ?frequency	 ?
 nw	 ?dataset	 ?
 wb	 ?dataset	 ?
Figure 2: Word frequency distribution in newswire
and webblog datasets.
5.2 V events vs. All Events
In our definition, v events are (1) events that are
single verbs (example 1, 3, 7, 9 in Table 2), and
(2) events that are multi-word sequences but only
one word among them is a verb and any non-verb
words are stripped off (verbs in example 4, 5, 6
in Table 2). Conversely, events that do not fall into
this definition include (1) events that have no verbs
in their surface form (example 2, 8, 10 in Table 2),
and (2) events that have more than one verb in their
surface form (e.g. ??+??(shi3+cheng2wei2)?,
VV+VV, ?make it become?). So from the point
of view of a statistical learning algorithm, ev-
ery v event has one and only one verb. This
makes sure that all features that we used are ap-
plicable to v events. For other events, however,
some features may be not applicable. For ex-
ample, for an event which has a nominal expres-
sion, aspect marker, DER, and DEC features are
all ?None? because these features are only appli-
cable to verbs. Another major distinction between
v events and ?other events? is that the distributions
of eventuality type labels on them are very differ-
ent, presented in the second and third groups of
columns in Figure 3. There is a rather high per-
centage of ?State? among ?other events? and very
low percentage of ?Completed? and ?None?. The
highly uneven distribution of eventuality type la-
bels make it less effective as a feature for tense
inference.
0.0%	 ?
5.0%	 ?
10.0%	 ?
15.0%	 ?
20.0%	 ?
25.0%	 ?
30.0%	 ?
35.0%	 ?
40.0%	 ?
45.0%	 ?
50.0%	 ?
all	 ?events	 ? v_events	 ? other	 ?events	 ?
Episodic	 ?
Habitual	 ?
State	 ?
Progressive	 ?
Completed	 ?
None	 ?
Figure 3: Statistics of eventuality types on differ-
ent events.
We also find that, on newswire datasets, max-
imum entropy models and neural network mod-
els do not show much difference in performance.
To understand this result better, we plot learn-
ing curves of the artificial neural network model,
trained and tested on newswire v events dataset.
In Figure 4, the black line represents the error rate
on training set, and the grey line represents the er-
1909
ror rate on test set. As the size of training data
grows, the error rate on the training set gets larger
because with more training examples the training
set becomes noisier and it gets harder to model all
samples with the same number of features; and the
error rate on the test set gets smaller because a big-
ger training set reduces the data sparsity and trains
the parameters better. Both lines end at a rather
high error rate (around 30%, i.e. only around 70%
in accuracy) which means the current network is
general enough to cover most cases in the test set,
but it is under-fitting the training data. The cur-
rent model is not specific enough to better cap-
ture the fine distinctions between the tense cat-
egories. The black line being not very smooth
is also understandable, given that there are only
around 6000 training examples in the newswire
v events dataset.
0.0%	 ?
5.0%	 ?
10.0%	 ?
15.0%	 ?
20.0%	 ?
25.0%	 ?
30.0%	 ?
35.0%	 ?
40.0%	 ?
10%
	 ?
20%
	 ?
30%
	 ?
40%
	 ?
50%
	 ?
60%
	 ?
70%
	 ?
80%
	 ?
90%
	 ?
100
%	 ?
Err
or
	 ?ra
te
 
Percentage	 ?of	 ?training	 ?data 
 train	 ?
 test	 ?
Figure 4: Learning curves of the artificial neural
network model, trained and tested on newswire
v events dataset.
6 Error Analysis
In order to get a better understanding of the use
of eventuality type and modality, we look into the
error rates for each error type in greater detail.
In Table 7, ?Pa? stands for ?Past?, ?Pre? is short
for ?Present?, ?Fu? is for ?Future?, and ?No? is
?None?. For each error type, the left-hand side
is the gold-standard tense, and the right-hand side
is the wrongly assigned label. Statistics are col-
lected on the newswire v events data test set. Ta-
ble 7 compares the different error types between
the baseline method and the MaxEnt em method,
the best approach for this dataset. We can see that
(1) ?Present? and ?Past? is the most frequently
confused tense pair, and (2) eventuality type and
modality information help disambiguate ?Present?
and ?Past? events greatly, and reduce the errors
due to mis-classifying ?Past? as ?Future?, or ?Fu-
ture? as ?Present?, or ?None? as ?Present?.
error type MaxEnt b MaxEnt em
Pre? Pa 11.7% 11.2%
Pa? Pre 9.8% 9.2%
Pa? Fu 2.5% 2.3%
No? Pa 2.0% 2.0%
Fu? Pre 1.9% 1.6%
Pre? Fu 1.4% 1.4%
Fu? Pa 1.4% 1.4%
No? Pre 1.6% 1.2%
No? Fu 0.5% 0.5%
Pa? No 0.3% 0.3%
Pre? No 0.2% 0.2%
Fu? No 0.0% 0.0%
Table 7: Tense inference error rates for different
error types on newswire v events test set.
A closer examination of the sentences in which
events are assigned the wrong tense reveals that
?Pre ? Pa? error is prone to occur on events
in relative clauses. The Chinese verb implies a
past episodic event, while the event is actually a
present state or habitual event. As a good example,
the ???(sheng1chan3)? event in Sentence (3) is
wrongly labeled as ?Past? by MaxEnt b but cor-
rectly classified as ?Present? by MaxEnt em with
eventuality type ?Habitual? and modality tag ?Ac-
tual? (the underlined part in the Chinese sentence
is the relative clause). It is also found that most
?Pa ? Pre? errors occur on events that are more
stative. It is reasonable since classifiers tend to
assign ?Present? to states and ?Past? to episodic
events. MaxEnt em managed to correct some with
?episodic? as their correct eventuality type.
(3) ??????(sheng1chan3)???? ?
?????????????????
??????????????????
??
At present , the Pu Kang Company
, which produces the vaccine in this zone ,
has already formed a production scale of 5
million doses per year , which has great sig-
nificance in effectively controlling the hepati-
tis A epidemic .
We are also surprised to see that over 2% ?Past?
events are classified as ?Future? events, ranking
1910
third among all error types. This mistake seems
very unlikely, but it is still possible when per-
forming tense inference on a language with no
grammatical tense at all. Take the following sen-
tence pair (4) as an example. In the Chinese sen-
tence, MaxEnt b classifies ???(tao3lun4)? as
?Future? because there is no grammatical indica-
tor in the Chinese sentence implying that the ?dis-
cussion? has already happened and it is reason-
able to assume the ?discussion? is in the near fu-
ture. However, with eventuality type ?Episodic?
and modality label ?Actual?, MaxEnt em classi-
fies it as ?Past? correctly, because episodic events
tend to occur in the past and future events tend to
get ?Intended? or ?Hypothetical? modality labels.
(4) ??????????????????
?????????(tao3lun4) ????
????????????
He also said, the French government ?even
directed its representative not to vote Yes
when the Security Council discussed the res-
olution on sanctions on Cuba?.
7 Conclusion and Future Work
In this paper, we address the problem of automatic
inference of Chinese semantic tense. We took ad-
vantage of a new corpus annotated with rich lin-
guistic information, and experimented with three
approaches. In the first approach, we use two
sources of implicit linguistic information, even-
tuality type and modality, automatically derived,
as features in tense inference. We then conducted
joint learning on tense and each of these two infor-
mation types. Finally, we experimented with using
artificial neural networks to train models for tense
prediction. All three approaches outperformed a
strong baseline, a maximum entropy model with
extensive engineering. Our future work will in-
clude exploring ways to improve automatic even-
tuality type and modality labeling accuracy to fur-
ther improve tense inference accuracy.
Acknowledgments
We would like to thank the three anonymous re-
viewers for their suggestions and comments. This
work is supported by the National Science Foun-
dation via Grant No. 0910532 entitled ?Richer
Representations for Machine Translation?. All
views expressed in this paper are those of the au-
thors and do not necessarily represent the view of
the National Science Foundation.
References
James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and
GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), June. Oral Presentation.
Xuansong Li, Stephanie Strassel, Stephen Grimes, Safa
Ismael, Mohamed Maamouri, Ann Bies, and Nian-
wen Xue. 2012. Parallel Aligned Treebanks at
LDC: New Challenges Interfacing Existing Infras-
tructures. In Proceedings of LREC-2012, Istanbul,
Turkey.
Feifan Liu, Fei Liu, and Yang Liu. 2011. Learning
from chinese-english parallel data for chinese tense
prediction. In Proceedings of the 5th International
Conference on Natural Language Processing, pages
1116?1124, November.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313?330.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at ICLR.
Roi Reichart and Ari Rappoport. 2010. Tense sense
disambiguation: A new syntactic polysemy task.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
325?334, Cambridge, MA, October. Association for
Computational Linguistics.
Carlota S. Smith and Mary Erbaugh. 2005. Tempo-
ral interpretation in Mandarin Chinese. Linguistics,
43(4):713?756.
Nianwen Xue and Yuchen Zhang. 2014. Buy one get
one free: Distant annotation of chinese tense, event
type, and modality. In Proceedings of LREC-2014,
Reykjavik, Iceland.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural
Language Engineering, 11(2):207?238.
Nianwen Xue. 2008. Automatic Inference of the Tem-
poral Location of Situations in Chinese Text. In
EMNLP-2008, Honolulu, Hawaii.
Yang Ye, Victoria Li Fossum, and Steven Abney. 2006.
Latent features in automatic tense translation be-
tween Chinese and English. In The Proceedings
of the 5th SIGHAN Workshop on Chinese Language
Processing, Sydney, Australia.
Yang Ye. 2007. Automatic Tense and Aspect Trans-
lation between Chinese and English. Ph.D. thesis,
University of Michigan.
1911
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 645?654,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Discovering Implicit Discourse Relations Through Brown Cluster Pair
Representation and Coreference Patterns
Attapol T. Rutherford
Department of Computer Science
Brandeis University
Waltham, MA 02453, USA
tet@brandeis.edu
Nianwen Xue
Department of Computer Science
Brandeis University
Waltham, MA 02453, USA
xuen@brandeis.edu
Abstract
Sentences form coherent relations in a
discourse without discourse connectives
more frequently than with connectives.
Senses of these implicit discourse rela-
tions that hold between a sentence pair,
however, are challenging to infer. Here,
we employ Brown cluster pairs to rep-
resent discourse relation and incorporate
coreference patterns to identify senses of
implicit discourse relations in naturally
occurring text. Our system improves the
baseline performance by as much as 25%.
Feature analyses suggest that Brown clus-
ter pairs and coreference patterns can re-
veal many key linguistic characteristics of
each type of discourse relation.
1 Introduction
Sentences must be pieced together logically in a
discourse to form coherent text. Many discourse
relations in the text are signaled explicitly through
a closed set of discourse connectives. Simply
disambiguating the meaning of discourse connec-
tives can determine whether adjacent clauses are
temporally or causally related (Pitler et al., 2008;
Wellner et al., 2009). Discourse relations and their
senses, however, can also be inferred by the reader
even without discourse connectives. These im-
plicit discourse relations in fact outnumber explicit
discourse relations in naturally occurring text. In-
ferring types or senses of implicit discourse re-
lations remains a key challenge in automatic dis-
course analysis.
A discourse parser requires many subcompo-
nents which form a long pipeline. The implicit
discourse relation discovery has been shown to be
the main performance bottleneck of an end-to-end
parser (Lin et al., 2010). It is also central to many
applications such as automatic summarization and
question-answering systems.
Existing systems, which make heavy use of
word pairs, suffer from data sparsity problem as
a word pair in the training data may not appear
in the test data. A better representation of two
adjacent sentences beyond word pairs could have
a significant impact on predicting the sense of
the discourse relation that holds between them.
Data-driven theory-independent word classifica-
tion such as Brown clustering should be able
to provide a more compact word representation
(Brown et al., 1992). Brown clustering algorithm
induces a hierarchy of words in a large unanno-
tated corpus based on word co-occurrences within
the window. The induced hierarchy might give
rise to features that we would otherwise miss. In
this paper, we propose to use the cartesian product
of Brown cluster assignment of the sentence pair
as an alternative abstract word representation for
building an implicit discourse relation classifier.
Through word-level semantic commonalities
revealed by Brown clusters and entity-level rela-
tions revealed by coreference resolution, we might
be able to paint a more complete picture of the
discourse relation in question. Coreference resolu-
tion unveils the patterns of entity realization within
the discourse, which might provide clues for the
types of the discourse relations. The information
about certain entities or mentions in one sentence
should be carried over to the next sentence to form
a coherent relation. It is possible that coreference
chains and semantically-related predicates in the
local context might show some patterns that char-
acterize types of discourse relations. We hypoth-
esize that coreferential rates and coreference pat-
terns created by Brown clusters should help char-
acterize different types of discourse relations.
Here, we introduce two novel sets of features
for implicit discourse relation classification. Fur-
ther, we investigate the effects of using Brown
clusters as an alternative word representation and
analyze the impactful features that arise from
645
Number of instances
Implicit Explicit
COMPARISON 2503 (15.11%) 5589 (33.73%)
CONTINGENCY 4255 (25.68%) 3741 (22.58%)
EXPANSION 8861 (53.48%) 72 (0.43%)
TEMPORAL 950 (5.73%) 3684 (33.73%)
Total 16569 (100%) 13086 (100%)
Table 1: The distribution of senses of implicit dis-
course relations is imbalanced.
Brown cluster pairs. We also study coreferential
patterns in different types of discourse relations in
addition to using them to boost the performance
of our classifier. These two sets of features along
with previously used features outperform the base-
line systems by approximately 5% absolute across
all categories and reveal many important charac-
teristics of implicit discourse relations.
2 Sense annotation in Penn Discourse
Treebank
The Penn Discourse Treebank (PDTB) is the
largest corpus richly annotated with explicit
and implicit discourse relations and their senses
(Prasad et al., 2008). PDTB is drawn from
Wall Street Journal articles with overlapping an-
notations with the Penn Treebank (Marcus et al.,
1993). Each discourse relation contains the infor-
mation about the extent of the arguments, which
can be a sentence, a constituent, or an incontigu-
ous span of text. Each discourse relation is also
annotated with the sense of the relation that holds
between the two arguments. In the case of implicit
discourse relations, where the discourse connec-
tives are absent, the most appropriate connective
is annotated.
The senses are organized hierarchically. Our fo-
cus is on the top level senses because they are the
four fundamental discourse relations that various
discourse analytic theories seem to converge on
(Mann and Thompson, 1988). The top level senses
are COMPARISON, CONTINGENCY, EXPANSION,
and TEMPORAL.
The explicit and implicit discourse relations al-
most orthogonally differ in their distributions of
senses (Table 1). This difference has a few im-
plications for studying implicit discourse relations
and uses of discourse connectives (Patterson and
Kehler, 2013). For example, TEMPORAL relations
constitute only 5% of the implicit relations but
33% of the explicit relations because they might
not be as natural to create without discourse con-
nectives. On the other hand, EXPANSION rela-
tions might be more cleanly achieved without ones
as indicated by its dominance in the implicit dis-
course relations. This imbalance in class distri-
bution requires greater care in building statistical
classifiers (Wang et al., 2012).
3 Experiment setup
We followed the setup of the previous studies
for a fair comparison with the two baseline sys-
tems by Pitler et al. (2009) and Park and Cardie
(2012). The task is formulated as four sepa-
rate one-against-all binary classification problems:
one for each top level sense of implicit discourse
relations. In addition, we add one more classifica-
tion task with which to test the system. We merge
ENTREL with EXPANSION relations to follow the
setup used by the two baseline systems. An argu-
ment pair is annotated with ENTREL in PDTB if
an entity-based coherence and no other type of re-
lation can be identified between the two arguments
in the pair. In this study, we assume that the gold
standard argument pairs are provided for each re-
lation. Most argument pairs for implicit discourse
relations are a pair of adjacent sentences or adja-
cent clauses separated by a semicolon and should
be easily extracted.
The PDTB corpus is split into a training set, de-
velopment set, and test set the same way as in the
baseline systems. Sections 2 to 20 are used to train
classifiers. Sections 0?1 are used for developing
feature sets and tuning models. Section 21?22 are
used for testing the systems.
The statistical models in the following exper-
iments are from MALLET implementation (Mc-
Callum, 2002) and libSVM (Chang and Lin,
2011). For all five binary classification tasks, we
try Balanced Winnow (Littlestone, 1988), Maxi-
mum Entropy, Naive Bayes, and Support Vector
Machine. The parameters and the hyperparame-
ters of each classifier are set to their default values.
The code for our model along with the data ma-
trices is available at github.com/attapol/
brown_coref_implicit.
4 Features
Unlike the baseline systems, all of the features
in the experiments use the output from automatic
natural language processing tools. We use the
Stanford CoreNLP suite to lemmatize and part-
of-speech tag each word (Toutanova et al., 2003;
646
Toutanova and Manning, 2000), obtain the phrase
structure and dependency parses for each sentence
(De Marneffe et al., 2006; Klein and Manning,
2003), identify all named entities (Finkel et al.,
2005), and resolve coreference (Raghunathan et
al., 2010; Lee et al., 2011; Lee et al., 2013).
4.1 Features used in previous work
The baseline features consist of the following:
First, last, and first 3 words, numerical ex-
pressions, time expressions, average verb phrase
length, modality, General Inquirer tags, polarity,
Levin verb classes, and production rules. These
features are described in greater detail by Pitler et
al. (2009).
4.2 Brown cluster pair features
To generate Brown cluster assignment pair fea-
tures, we replace each word with its hard Brown
cluster assignment. We used the Brown word
clusters provided by MetaOptimize (Turian et
al., 2010). 3,200 clusters were induced from
RCV1 corpus, which contains about 63 million to-
kens from Reuters English newswire. Then we
take the Cartesian product of the Brown clus-
ter assignments of the words in Arg1 and the
ones of the words in Arg2. For example, sup-
pose Arg1 has two words w
1,1
, w
1,2
, Arg2 has
three words w
2,1
, w
2,2
, w
2,3
, and then B(.) maps
a word to its Brown cluster assignment. A
word w
ij
is replaced by its corresponding Brown
cluster assignment b
ij
= B(w
ij
). The result-
ing word pair features are (b
1,1
, b
2,1
), (b
1,1
, b
2,2
),
(b
1,1
, b
2,3
), (b
1,2
, b
2,1
), (b
1,2
, b
2,2
), and (b
1,2
, b
2,3
).
Therefore, this feature set can generate
O(3200
2
) binary features. The feature set size is
orders of magnitude smaller than using the actual
words, which can generate O(V
2
) distinct binary
features where V is the size of the vocabulary.
4.3 Coreference-based features
We want to take advantage of the semantics of
the sentence pairs even more by considering how
coreferential entities play out in the sentence pairs.
We consider various inter-sentential coreference
patterns to include as features and also to better
describe each type of discourse relation with re-
spect to its place in the coreference chain.
For compactness in explaining the following
features, we define similar words to be the words
assigned to the same Brown cluster.
Number of coreferential pairs: We count the
number of inter-sentential coreferential pairs.
We expect that EXPANSION relations should be
more likely to have coreferential pairs because the
detail or information about an entity mentioned
in Arg1 should be expanded in Arg2. Therefore,
entity sharing might be difficult to avoid.
Similar nouns and verbs: A binary feature
indicating whether similar or coreferential nouns
are the arguments of the similar predicates. Predi-
cates and arguments are identified by dependency
parses. We notice that sometimes the author uses
synonyms while trying to expand on the previous
predicates or entities. The words that indicate the
common topics might be paraphrased, so exact
string matching cannot detect whether the two ar-
guments still focus on the same topic. This might
be useful for identifying CONTINGENCY relations
as they usually discuss two causally-related events
that involve two seemingly unrelated agents
and/or predicates.
Similar subject or main predicates: A binary
feature indicating whether the main verbs of the
two arguments have the same subjects or not
and another binary feature indicating whether the
main verbs are similar or not. For our purposes,
the two subjects are said to be the same if they
are coreferential or assigned to the same Brown
cluster. We notice that COMPARISON relations
usually have different subjects for the same main
verbs and that TEMPORAL relations usually have
the same subjects but different main verbs.
4.4 Feature selection and training sample
reweighting
The nature of the task and the dataset poses at
least two problems in creating a classifier. First,
the classification task requires a large number of
features, some of which are too rare and incon-
ducive to parameter estimation. Second, the la-
bel distribution is highly imbalanced (Table 1) and
this might degrade the performance of the classi-
fiers (Japkowicz, 2000). Recently, Park and Cardie
(2012) and Wang et al. (2012) addressed these
problems directly by optimally select a subset of
features and training samples. Unlike previous
work, we do not discard any of data in the training
set to balance the label distribution. Instead, we
reweight the training samples in each class during
parameter estimation such that the performance on
the development set is maximized. In addition, the
647
Current Park and Cardie (2012) Pitler et al. (2009)
P R F
1
F
1
F
1
COMPARISON vs others 27.34 72.41 39.70 31.32 21.96
CONTINGENCY vs others 44.52 69.96 54.42 49.82 47.13
EXPANSION vs others 59.59 85.50 70.23 - -
EXP+ENTREL vs others 69.26 95.92 80.44 79.22 76.42
TEMPORAL vs others 18.52 63.64 28.69 26.57 16.76
Table 2: Our classifier outperform the previous systems across all four tasks without the use of gold-
standard parses and coreference resolution.
COMPARISON
Feature set F
1
% change
All features 39.70 -
All excluding Brown cluster pairs 35.71 -10.05%
All excluding Production rules 37.27 -6.80%
All excluding First, last, and First 3 39.18 -1.40%
All excluding Polarity 39.39 -0.79%
CONTINGENCY
Feature set F
1
% change
All 54.42 -
All excluding Brown cluster pairs 51.50 -5.37%
All excluding First, last, and First 3 53.56 -1.58%
All excluding Polarity 53.82 -1.10%
All excluding Coreference 53.92 -0.92%
EXPANSION
Feature set F
1
% change
All 70.23 -
All excluding Brown cluster pairs 67.48 -3.92%
All excluding First, last, and First 3 69.43 -1.14%
All excluding Inquirer tags 69.73 -0.71%
All excluding Polarity 69.92 -0.44%
TEMPORAL
Feature set F
1
% change
All 28.69 -
All excluding Brown cluster pairs 24.53 -14.50%
All excluding Production rules 26.51 -7.60%
All excluding First, last, and First 3 26.56 -7.42%
All excluding Polarity 27.42 -4.43%
Table 3: Ablation study: The four most impact-
ful feature classes and their relative percentage
changes are shown. Brown cluster pair features
are the most impactful across all relation types.
number of occurrences for each feature must be
greater than a cut-off, which is also tuned on the
development set to yield the highest performance
on the development set.
5 Results
Our experiments show that the Brown cluster and
coreference features along with the features from
the baseline systems improve the performance for
all discourse relations (Table 2). Consistent with
the results from previous work, the Naive Bayes
classifier outperforms MaxEnt, Balanced Winnow,
and Support Vector Machine across all tasks re-
gardless of feature pruning criteria and training
sample reweighting. A possible explanation is that
the small dataset size in comparison with the large
number of features might favor a generative model
like Naive Bayes (Jordan and Ng, 2002). So we
only report the performance from the Naive Bayes
classifiers.
It is noteworthy that the baseline systems use
the gold standard parses provided by the Penn
Treebank, but ours does not because we would
like to see how our system performs realistically in
conjunction with other pre-processing tasks such
as lemmatization, parsing, and coreference reso-
lution. Nevertheless, our system still manages to
outperform the baseline systems in all relations by
a sizable margin.
Our preliminary results on implicit sense classi-
fication suggest that the Brown cluster word rep-
resentation and coreference patterns might be in-
dicative of the senses of the discourse relations,
but we would like to know the extent of the im-
pact of these novel feature sets when used in con-
junction with other features. To this aim, we con-
duct an ablation study, where we exclude one of
the feature sets at a time and then test the result-
ing classifier on the test set. We then rank each
feature set by the relative percentage change in
F
1
score when excluded from the classifier. The
data split and experimental setup are identical to
the ones described in the previous section but only
with Naive Bayes classifiers.
The ablation study results imply that Brown
cluster features are the most impactful feature set
across all four types of implicit discourse rela-
tions. When ablated, Brown cluster features de-
grade the performance by the largest percentage
compared to the other feature sets regardless of the
relation types(Table 3). TEMPORAL relations ben-
648
efit the most from Brown cluster features. With-
out them, the F
1
score drops by 4.12 absolute or
14.50% relative to the system that uses all of the
features.
6 Feature analysis
6.1 Brown cluster features
This feature set is inspired by the word pair fea-
tures, which are known for its effectiveness in pre-
dicting senses of discourse relations between the
two arguments. Marcu et al (2002), for instance,
artificially generated the implicit discourse rela-
tions and used word pair features to perform the
classification tasks. Those word pair features work
well in this case because their artificially gener-
ated dataset is an order of magnitude larger than
PDTB. Ideally, we would want to use the word
pair features instead of word cluster features if
we have enough data to fit the parameters. Con-
sequently, other less sparse handcrafted features
prove to be more effective than word pair features
for the PDTB data (Pitler et al., 2009). We remedy
the sparsity problem by clustering the words that
are distributionally similar together and greatly re-
duce the number of features.
Since the ablation study is not fine-grained
enough to spotlight the effectiveness of the indi-
vidual features, we quantify the predictiveness of
each feature by its mutual information. Under
Naive Bayes conditional independence assump-
tion, the mutual information between the features
and the labels can be efficiently computed in a
pairwise fashion. The mutual information be-
tween a binary feature X
i
and class label Y is de-
fined as:
I(X
i
, Y ) =
?
y
?
x=0,1
p?(x, y) log
p?(x, y)
p?(x)p?(y)
p?(?) is the probability distribution function whose
parameters are maximum likelihood estimates
from the training set. We compute mutual infor-
mation for all four one-vs-all classification tasks.
The computation is done as part of the training
pipeline in MALLET to ensure consistency in pa-
rameter estimation and smoothing techniques. We
then rank the cluster pair features by mutual in-
formation. The results are compactly summa-
rized in bipartite graphs shown in Figure 1, where
each edge represents a cluster pair. Since mu-
tual information itself does not indicate whether
a feature is favored by one or the other label, we
also verify the direction of the effects of each of
the features included in the following analysis by
comparing the class conditional parameters in the
Naive Bayes model.
The most dominant features for COMPARISON
classification are the pairs whose members are
from the same Brown clusters. We can distinctly
see this pattern from the bipartite graph because
the nodes on each side are sorted alphabetically.
The graph shows many parallel short edges, which
suggest that many informative pairs consist of the
same clusters. Some of the clusters that participate
in such pair consist of named-entities from vari-
ous categories such as airlines (King, Bell, Virgin,
Continental, ...), and companies (Thomson, Volk-
swagen, Telstra, Siemens). Some of the pairs form
a broad category such as political agents (citizens,
pilots, nationals, taxpayers) and industries (power,
insurance, mining). These parallel patterns in the
graph demonstrate that implicit COMPARISON re-
lations might be mainly characterized by juxtapos-
ing and explicitly contrasting two different entities
in two adjacent sentences.
Without the use of a named-entity recogni-
tion system, these Brown cluster pair features ef-
fectively act as features that detect whether the
two arguments in the relation contain named-
entities or nouns from the same categories or not.
These more subtle named-entity-related features
are cleanly discovered through replacing words
with their data-driven Brown clusters without the
need for additional layers of pre-processing.
If the words in one cluster semantically relates
to the words in another cluster, the two clusters
are more likely to become informative features
for CONTINGENCY classification. For instance,
technical terms in stock and trading (weighted,
Nikkei, composite, diffusion) pair up with eco-
nomic terms (Trading, Interest, Demand, Produc-
tion). The cluster with analysts and pundits pairs
up with the one that predominantly contains quan-
tifiers (actual, exact, ultimate, aggregate). In ad-
dition to this pattern, we observed the same par-
allel pair pattern we found in COMPARISON clas-
sification. These results suggest that in establish-
ing a CONTINGENCY relation implicitly the au-
thor might shape the sentences such that they have
semantically related words if they do not mention
named-entities of the same category.
Through Brown cluster pairs, we obtain features
that detect a shift between generality and speci-
649
Arg 1
COMPARISON
Arg 2
? ?
American American
Bank
Big,Human,Civil,Greater,...
Centre,Bay,Park,Hospital,... Board,Corps
Congress Centre,Bay,Park,Hospital,...
East
Congress
Exchange
East
Fed,CWB Fed,CWB
Israelis,Moslems,Jews,terrorists,...
GM,Ford,Barrick,Anglo,...
Japan
Israelis,Moslems,Jews,terrorists,...
King,Bell,Virgin,Continental,... Japan
March
King,Bell,Virgin,Continental,...
Miert,Lumpur,der,Metall,...
March
Olivetti,Eurotunnel,Elf,Lagardere,... Miert,Lumpur,der,Metall,...
Power,Insurance,Mining,Engineering,... Olivetti,Eurotunnel,Elf,Lagardere,...
Soviet,Homeland,Patriotic
Power,Insurance,Mining,Engineering,...
Standard,Hurricane,Time,Long,...
Soviet,Homeland,Patriotic
Thomson,Volkswagen,Telstra,Siemens,... Standard,Hurricane,Time,Long,...
advertising,ad Thomson,Volkswagen,Telstra,Siemens,...
agency
actual,exact,ultimate,aggregate,...
analysts,pundits advertising,ad
auto,semiconductor,automotive,automobile,...
agency
average average
cash bank
chemicals,entertainment,machinery,packaging,... cars,vehicles,tyres,vans,...
citizens,pilots,nationals,taxpayers,...
cash
closed
chemicals,entertainment,machinery,packaging,...
common citizens,pilots,nationals,taxpayers,...
computer,mainframe
closed
sales
common
computer,mainframe
Arg 1
CONTINGENCY
Arg 2
:
&,und
Bank
?
Christopher,Simitis,Perry,Waigel,...
10
Electric,Motor,Life,Chemical,...
;
Friday
Bank
Holdings,Industries,Investments,Foods,...
Bill,Mrs.
If,Unless,Whether,Maybe,... Christopher,Simitis,Perry,Waigel,...
King,Bell,Virgin,Continental,...
Dow,shuttle,DAX,Ifo,...
Major,Howard,Arthuis,Chang,...
Electric,Motor,Life,Chemical,...
March
Friday
Power,Insurance,Mining,Engineering,... GM,Ford,Barrick,Anglo,...
Royal,Port,Cape,Santa,... Holdings,Industries,Investments,Foods,...
Senate,senate
I
age,identity,integrity,identification,... If,Unless,Whether,Maybe,...
also
King,Bell,Virgin,Continental,...
am,?m
Major,Howard,Arthuis,Chang,...
analysts,pundits
March
average
Power,Insurance,Mining,Engineering,...
back
Royal,Port,Cape,Santa,...
her
Senate,senate
his
To,Would
index
Trading,Interest,Demand,Production,...
market
actual,exact,ultimate,aggregate,...
no age,identity,integrity,identification,...
now
ago
our
all
they
weighted,Nikkei,composite,diffusion,...
world
Arg 1
EXPANSION
Arg 2
American
?
Analysts,Economists,Diplomats,Forecasters,...
American
But,Saying Boeing,BT,Airbus,Netscape,...
Democrats December
Dow,shuttle,DAX,Ifo,...
Exchange
Dutroux,Lopez,Morris,Hamanaka,... GM,Ford,Barrick,Anglo,...
Electric,Motor,Life,Chemical,...
I
For,Like
Lynch,Fleming,Reagan,Brandford,...
Net,Operating,Primary,Minority,...
No
Olivetti,Eurotunnel,Elf,Lagardere,... Olivetti,Eurotunnel,Elf,Lagardere,...
Plc,Oy,NV,AB,... Plc,Oy,NV,AB,...
S&P,Burns,Tietmeyer,Rifkind,... Republicans,Bonds,Rangers,Greens,...
Soviet,Homeland,Patriotic Senate,senate
Telecommunications,Broadcasting,Futures,Rail,...
Soviet,Homeland,Patriotic
Texas,Queensland,Ohio,Illinois,...
That
U.S.
Trading,Interest,Demand,Production,...
VW,Conrail,Bre-X,Texaco,...
U.S.
advertising,ad
VW,Conrail,Bre-X,Texaco,...
am,?m
We,Things
businesses all
dollar,greenback,ecu analyst,meteorologist
five-year,three-year,two-year,four-year,...
because
get
been
if,whenever,wherever
business
investors
cents,pence,p.m.,a.m.,...
no compared,coupled,compares
occupation,Index,Kurdistan,Statements,...
could
plan
stocks
under
Arg 1
TEMPORAL
Arg 2
?ve ?
*,@,**,?,...
?re
14,13,16
?ve
20
*,@,**,?,...
30
0.5,44,0.2,0.3,...
50,1.50,0.50,0.05,...
10
: 10-year,collective,dual,30-year,...
American
17,19,21
British
1991,1989,1949,1979,...
Friday
200,300,150,120,...
Investors,Banks,Companies,Farmers,...
26,28,29
Prices,Results,Sales,Thousands,... 27,22,23
Sept,Nov.,Oct.,Oct,...
3,A1,C1,C3,...
Trading,Interest,Demand,Production,...
30
Treasury,mortgage-backed
50,1.50,0.50,0.05,...
York,York-based
age,identity,integrity,identification,...
bond,floating-rate
books,words,budgets,clothes,...
consumer
convertible,bonus,Brady,subordinated,...
increase
interest
loss
months
no-fly,year-ago,corresponding,buffer,...
people
quarter
results
rose
there
Figure 1: The bipartite graphs show the top 40 non-stopword Brown cluster pair features for all four
classification tasks. Each node on the left and on the right represents word cluster from Arg1 and Arg2
respectively. We only show the clusters that appear fewer than six times in the top 3,000 pairs to exclude
stopwords. Although the four tasks are interrelated, some of the highest mutual information features vary
substantially across tasks.
ficity within the scope of the relation. For exam-
ple, a cluster with industrial categories (Electric,
Motor, Life, Chemical, Automotive) couples with
specific brands or companies (GM, Ford, Barrick,
Anglo). Or such a pair might simply reflects a shift
in plurality e.g. businesses - business and Analysts
-analyst. EXPANSION relations capture relations
in which one argument provides a specification of
the previous and relations in which one argument
provides a generalization of the other. Thus, these
shift detection features could help distinguish EX-
PANSION relations.
We found a few common coreference patterns
of names in written English to be useful. First and
last name are used in the first sentence to refer to a
person who just enters the discourse. That person
is referred to just by his/her title and last name in
the following sentence. This pattern is found to be
650
All coreference Subject coreference
0.0
0.1
0.2
0.3
0.4
0.5
Com
pari
son
Con
tinge
ncy
Exp
ansi
on
Tem
pora
l
Com
pari
son
Con
tinge
ncy
Exp
ansi
on
Tem
pora
l
Co
ref
er
en
tia
l ra
te
Figure 2: The coreferential rate for TEMPORAL
relations is significantly higher than the other three
relations (p < 0.05, corrected for multiple com-
parison).
informative for EXPANSION relations. For exam-
ple, the edges (not shown in the graph due to lack
of space) from the first name clusters to the title
(Mr, Mother, Judge, Dr) cluster.
Time expressions constitutes the majority of the
nodes in the bipartite graph for TEMPORAL rela-
tions. More strikingly, the specific dates (e.g. clus-
ters that have positive integers smaller than 31)
are more frequently found in Arg2 than Arg1 in
implicit TEMPORAL relations. It is possible that
TEMPORAL relations are more naturally expressed
without a discourse connective if a time point is
clearly specified in Arg2 but not in Arg1.
TEMPORAL relations might also be implicitly
inferred through detecting a shift in quantities. We
notice that clusters whose words indicate changes
e.g. increase, rose, loss pair with number clusters.
Sentences in which such pairs participate might be
part of a narrative or a report where one expects a
change over time. These changes conveyed by the
sentences constitute a natural sequence of events
that are temporally related but might not need ex-
plicit temporal expressions.
6.2 Coreference features
Coreference features are very effective given that
they constitute a very small set compared to the
other feature sets. In particular, excluding them
from the model reduces F
1
scores for TEMPORAL
and CONTINGENCY relations by approximately
1% relative to the system that uses all of the
features. We found that the sentence pairs in these
two types of relations have distinctive coreference
patterns.
We count the number of pairs of arguments that
are linked by a coreference chain for each type of
relation. The coreference chains used in this study
are detected automatically from the training set
through Stanford CoreNLP suite (Raghunathan et
al., 2010; Lee et al., 2011; Lee et al., 2013). TEM-
PORAL relations have a significantly higher coref-
erential rate than the other three relations (p <
0.05, pair-wise t-test corrected for multiple com-
parisons). The differences between COMPARI-
SON, CONTINGENCY, and EXPANSION, however,
are not statistically significant (Figure 2).
The choice to use or not to use a discourse
connective is strongly motivated by linguistic fea-
tures at the discourse levels (Patterson and Kehler,
2013). Additionally, it is very uncommon to
have temporally-related sentences without using
explicit discourse connectives. The difference in
coreference patterns might be one of the factors
that influence the choice of using a discourse con-
nective to signal a TEMPORAL relation. If sen-
tences are coreferentially linked, then it might be
more natural to drop a discourse connective be-
cause the temporal ordering can be easily inferred
without it. For example,
(1) Her story is partly one of personal down-
fall. [previously] She was an unstinting
teacher who won laurels and inspired stu-
dents... (WSJ0044)
The coreference chain between the two
temporally-related sentences in (1) can easily
be detected. Inserting previously as suggested
by the annotation from the PDTB corpus does
not add to the temporal coherence of the sen-
tences and may be deemed unnecessary. But the
presence of coreferential link alone might bias
the inference toward TEMPORAL relation while
CONTINGENCY might also be inferred.
Additionally, we count the number of pairs of
arguments whose grammatical subjects are linked
by a coreference chain to reveal the syntactic-
coreferential patterns in different relation types.
Although this specific pattern seems rare, more
than eight percent of all relations have coreferen-
tial grammatical subjects. We observe the same
statistically significant differences between TEM-
PORAL relations and the other three types of re-
lations. More interestingly, the subject coreferen-
tial rate for CONTINGENCY relations is the lowest
among the three categories (p < 0.05, pair-wise
t-test corrected for multiple comparisons).
651
It is possible that coreferential subject patterns
suggest temporal coherence between the two sen-
tences without using an explicit discourse connec-
tive. CONTINGENCY relations, which can only in-
dicate causal relationships when realized implic-
itly, impose the temporal ordering of events in the
arguments; i.e. if Arg1 is causally related to Arg2,
then the event described in Arg1 must temporally
precede the one in Arg2. Therefore, CONTIN-
GENCY and TEMPORAL can be highly confusable.
To understand why this pattern might help distin-
guish these two types of relations, consider these
examples:
(2) He also asserted that exact questions weren?t
replicated. [Then] When referred to the ques-
tions that match, he said it was coincidental.
(WSJ0045)
(3) He also asserted that exact questions weren?t
replicated. When referred to the questions
that match, she said it was coincidental.
When we switch out the coreferential subject
for an arbitrary uncoreferential pronoun as we do
in (3), we are more inclined to classify the relation
as CONTINGENCY.
7 Related work
Word-pair features are known to work very well
in predicting senses of discourse relations in an
artificially generated corpus (Marcu and Echi-
habi, 2002). But when used with a realistic cor-
pus, model parameter estimation suffers from data
sparsity problem due to the small dataset size. Bi-
ran and McKeown (2013) attempts to solve this
problem by aggregating word pairs and estimating
weights from an unannotated corpus but only with
limited success.
Recent efforts have focused on introducing
meaning abstraction and semantic representation
between the words in the sentence pair. Pitler et al.
(2009) uses external lexicons to replace the one-
hot word representation with semantic information
such as word polarity and various verb classifica-
tion based on specific theories (Stone et al., 1968;
Levin, 1993). Park and Cardie (2012) selects an
optimal subset of these features and establishes the
strongest baseline to best of our knowledge.
Brown word clusters are hierarchical clusters
induced by frequency of co-occurrences with other
words (Brown et al., 1992). The strength of this
word class induction method is that the words that
are classified to the same clusters usually make
an interpretable lexical class by the virtue of their
distributional properties. This word representation
has been used successfully to augment the perfor-
mance of many NLP systems (Ritter et al., 2011;
Turian et al., 2010).
Louis et al. (2010) uses multiple aspects of
coreference as features to classify implicit dis-
course relations without much success while sug-
gesting many aspects that are worth exploring. In a
corpus study by Louis and Nenkova (2010), coref-
erential rates alone cannot explain all of the rela-
tions, and more complex coreference patterns have
to be considered.
8 Conclusions
We present statistical classifiers for identifying
senses of implicit discourse relations and intro-
duce novel feature sets that exploit distributional
similarity and coreference information. Our clas-
sifiers outperform the classifiers from previous
work in all types of implicit discourse relations.
Altogether these results present a stronger base-
line for the future research endeavors in implicit
discourse relations.
In addition to enhancing the performance of the
classifier, Brown word cluster pair features dis-
close some of the new aspects of implicit dis-
course relations. The feature analysis confirms
our hypothesis that cluster pair features work well
because they encapsulate relevant word classes
which constitute more complex informative fea-
tures such as named-entity pairs of the same cat-
egories, semantically-related pairs, and pairs that
indicate specificity-generality shift. At the dis-
course level, Brown clustering is superior to a
one-hot word representation for identifying inter-
sentential patterns and the interactions between
words.
Coreference chains that traverse through the
discourse in the text shed the light on differ-
ent types of relations. The preliminary analy-
sis shows that TEMPORAL relations have much
higher inter-argument coreferential rates than the
other three senses of relations. Focusing on only
subject-coreferential rates, we observe that CON-
TINGENCY relations show the lowest coreferential
rate. The coreference patterns differ substantially
and meaningfully across discourse relations and
deserve further exploration.
652
References
Or Biran and Kathleen McKeown. 2013. Aggregated
word pair features for implicit discourse relation dis-
ambiguation. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics, pages 69?73. The Association for Compu-
tational Linguistics.
Peter F Brown, Peter V deSouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n -gram models of natural language.
Computational Linguistics, 18(4):467?479, Decem-
ber.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1?27:27. Software available at http://
www.csie.ntu.edu.tw/
?
cjlin/libsvm.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generat-
ing typed dependency parses from phrase structure
parses. In Proceedings of LREC, volume 6, pages
449?454.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363?370. Association for Computational Lin-
guistics.
Nathalie Japkowicz. 2000. Learning from imbalanced
data sets: a comparison of various strategies. In
AAAI workshop on learning from imbalanced data
sets, volume 68.
Michael Jordan and Andrew Ng. 2002. On discrimi-
native vs. generative classifiers: A comparison of lo-
gistic regression and naive bayes. Advances in neu-
ral information processing systems, 14:841.
Dan Klein and Christopher D Manning. 2003. Accu-
rate unlexicalized parsing. In the 41st Annual Meet-
ing, pages 423?430, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford?s multi-pass sieve coref-
erence resolution system at the conll-2011 shared
task. In Proceedings of the Fifteenth Conference on
Computational Natural Language Learning: Shared
Task, pages 28?34. Association for Computational
Linguistics.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics.
Beth Levin. 1993. English verb classes and alter-
nations: A preliminary investigation, volume 348.
University of Chicago press Chicago.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan.
2010. A PDTB-Styled End-to-End Discourse
Parser. arXiv.org, November.
Nick Littlestone. 1988. Learning quickly when irrele-
vant attributes abound: A new linear-threshold algo-
rithm. Machine learning, 2(4):285?318.
Annie Louis and Ani Nenkova. 2010. Creating lo-
cal coherence: An empirical assessment. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 313?316.
Association for Computational Linguistics.
Annie Louis, Aravind Joshi, Rashmi Prasad, and Ani
Nenkova. 2010. Using entity features to classify
implicit discourse relations. In Proceedings of the
11th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, pages 59?62. Associa-
tion for Computational Linguistics.
William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 368?375. Association for Computational Lin-
guistics.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313?330.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
Joonsuk Park and Claire Cardie. 2012. Improving im-
plicit discourse relation recognition through feature
set optimization. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 108?112. Association for Com-
putational Linguistics.
Gary Patterson and Andrew Kehler. 2013. Predicting
the presence of discourse connectives. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing. Association for Compu-
tational Linguistics.
Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani
Nenkova, Alan Lee, and Aravind K Joshi. 2008.
Easily identifiable discourse relations. Technical
Reports (CIS), page 884.
653
Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume
2, pages 683?691. Association for Computational
Linguistics.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K Joshi, and Bon-
nie L Webber. 2008. The penn discourse treebank
2.0. In LREC. Citeseer.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?10,
pages 492?501, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1524?1534. Association for Computational Linguis-
tics.
Philip Stone, Dexter C Dunphy, Marshall S Smith, and
DM Ogilvie. 1968. The general inquirer: A com-
puter approach to content analysis. Journal of Re-
gional Science, 8(1).
Kristina Toutanova and Christopher D Manning. 2000.
Enriching the knowledge sources used in a maxi-
mum entropy part-of-speech tagger. In Proceedings
of the 2000 Joint SIGDAT conference on Empirical
methods in natural language processing and very
large corpora: held in conjunction with the 38th An-
nual Meeting of the Association for Computational
Linguistics-Volume 13, pages 63?70. Association for
Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173?180. Association for Compu-
tational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Xun Wang, Sujian Li, Jiwei Li, and Wenjie Li. 2012.
Implicit discourse relation recognition by selecting
typical training examples. In Proceedings of COL-
ING 2012, pages 2757?2772, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Ben Wellner, James Pustejovsky, Catherine Havasi,
Anna Rumshisky, and Roser Sauri. 2009. Clas-
sification of discourse coherence relations: An ex-
ploratory study using multiple knowledge sources.
In Proceedings of the 7th SIGdial Workshop on Dis-
course and Dialogue, pages 117?125. Association
for Computational Linguistics.
654
Proceedings of NAACL HLT 2009: Tutorials, pages 11?12,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
OntoNotes: The 90% Solution
Sameer S. Pradhan and Nianwen Xue
OntoNotes is a five year multi-site collaboration between BBN Technologies, Information Sciences In-
stitute of University of Southern California, University of Colorado, University of Pennsylvania and Bran-
deis University. The goal of the OntoNotes project is to provide linguistic data annotated with a skeletal
representation of the literal meaning of sentences including syntactic parse, predicate-argument structure,
coreference, and word senses linked to an ontology, allowing a new generation of language understanding
technologies to be developed with new functional capabilities.
In its third year of existence, the OntoNotes project has generated a large amount of high quality data
covering various layers of linguistic annotation. This is probably the first time that data of such quality has
been available in large quantities covering multiple genres (newswire, broadcast news, broadcast conver-
sation and weblogs) and languages (English, Chinese and Arabic). The guiding principle has been to find
a ?sweet spot? in the space of inter-tagger agreement, productivity, and depth of representation. The most
effective use of this resource for research requires simultaneous access to multiple layers of annotation. This
has been made possible by representing the corpus with a relational database to accommodate the dense
connectedness of the data and ensure consistency across layers. In order to facilitate ease of understanding
and manipulability, the database has also been supplemented with a object-oriented Python API.
The tutorial consists of two parts. In the first part we will familiarize the user with this new resource,
describe the various layers of annotations in some detail and discuss the linguistic principles and some-
times practical considerations behind the important design decisions that shapes the corpus. We will also
describe the salient differences between the three languages at each layer of annotation and how linguistic
peculiarities of different languages were handled in the data.
In the second part, we will describe the data formats of each of the layers and talk about various design
decisions that went into the creation of the architecture of the database and the individual tables comprising
it, along with issues that came up during the representation process and compromises that were made
without sacrificing some primary objectives one of which being the independent existence of each layer
that is necessary to allow multi-site collaboration. We will explain how the database schema attempts to
interconnect all the layers. Then we will go into the details of the Python API that allows easy access to each
of the layers and show that by making the objects closely resemble database tables, the API allows for their
flexible integration. This will be followed by a hands-on working session.
1 Tutorial Outline
1. Annotation Layers
? Overview of OntoNotes
? Design principles
? Depth of annotation
? Consistency (ITA)
? Linguistics principles
? Potential applications
? Question Answering
? Machine Translation
? Layers of Annotation in English, Chinese and Arabic
? Treebank
? PropBank
? Word Sense
111
? Name
? Coreference
? Ontology
? Comparison with existing multi-layer annotation corpora
2. Data Access API
? Data
? File format
? Metadata specification
? Database schema representing each layer of annotation
? ER diagram
? Inter-connection between the annotation layers (database tables)
? Python Access API
? Introduction to the Python modules
? Correspondence between MySQL tables and Python classes
? Introduction to some frequently used module functionalities
? Extending the API to add a new layer of annotation
? Hands on Session
? Creating a sample OntoNotes database from MySQL dump file
? Loading it into memory
? Creating Python objects representing various annotation layers
? Performing cross-layer queries using a combination of API and database
? We will provide some sample queries
? Users can use their own experience to generate novel queries
? Manipulating the data as in Python world and MySQL world
? Writing the modified versions back to the database
2 Target Audience
This tutorial is designed for people interested in using one or more layers of OntoNotes in their research to
further language understanding through improved shallow semantic analysis. Detailed knowledge of any of
the layers is not necessary. Some familiarity with Python would be preferable.
Sameer Pradhan is a Research Scientist at BBN Technologies. His research interests include computational
semantics, question answering, application of machine learning to language understanding and annotation
science. He have been leading the data integration and coreference annotation effort in the DARPA funded
GALE OntoNotes project at BBN. In the past he was the technical lead on the AQUAINT project at BBN.
He serves on the ACL SIGANN committee and has been one of the organizers of the Linguistics Annotation
Workshops (LAW II and III) He has been on the programme committees of Workshop on UIMA for NLP,
and Conference on Global Interoperability of Language Resources (ICGL) He has also served on the guest
Editorial Board of Computational Linguistics: Special issue on Semantic Role Labeling. He got his PhD in
Computer Science at the University of Colorado at Boulder.
Nianwen Xue is an Assistant Professor of Language & Linguistics and Computer Science at Brandeis Univer-
sity. His research interests include formal representation of linguistic structures and its impact on natural
language processing, aspects of syntax, computational linguistics, corpus linguistics and Chinese language
processing. He is currently leading the effort to expand the Chinese Treebank, Proposition Bank and word
sense annotation, funded by DARPA as part of the GALE OntoNotes project. He serves on the ACL SIGANN
committee. He is one of the organizers of the Linguistics Annotation Workshops (LAW II and III) and is also
on the organizing committee of the CoNLL Shared Task on Syntactic and Semantic Dependencies in Multiple
Languages. He has also served on the guest Editorial Board of Computational Linguistics: Special issue on
Semantic Role Labeling. He got his PhD in linguistics from University of Delaware.
212
Proceedings of NAACL-HLT 2013, pages 1051?1060,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Dependency-based empty category detection via phrase structure trees
Nianwen Xue
Brandeis University
Waltham, MA, USA
xuen@brandeis.edu
Yaqin Yang
Brandeis University
Waltham, MA, USA
yaqin@brandeis.edu
Abstract
We describe a novel approach to detecting
empty categories (EC) as represented in de-
pendency trees as well as a new metric for
measuring EC detection accuracy. The new
metric takes into account not only the position
and type of an EC, but also the head it is a
dependent of in a dependency tree. We also
introduce a variety of new features that are
more suited for this approach. Tested on a sub-
set of the Chinese Treebank, our system im-
proved significantly over the best previously
reported results even when evaluated with this
more stringent metric.
1 Introduction
In modern theoretical linguistics, empty categories
(ECs) are an important piece of machinery in repre-
senting the syntactic structure of a sentence and they
are used to represent phonologically null elements
such as dropped pronouns and traces of dislocated
elements. They have also found their way into large-
scale treebanks which have played an important role
in advancing the state of the art in syntactic parsing.
In phrase-structure treebanks, ECs have been used to
indicate long-distance dependencies, discontinuous
constituents, and certain dropped elements (Marcus
et al, 1993; Xue et al, 2005). Together with la-
beled brackets and function tags, they make up the
full syntactic representation of a sentence.
The use of ECs captures some cross-linguistic
commonalities and differences. For example, while
both the Penn English TreeBank (PTB) (Marcus et
al., 1993) and the Chinese TreeBank (CTB) (Xue
et al, 2005) use traces to represent the extraction
site of a dislocated element, dropped pronouns (rep-
resented as *pro*s) are much more widespread in
the CTB. This is because Chinese is a pro-drop lan-
guage (Huang, 1984) that allows the subject to be
dropped in more contexts than English does. While
detecting and resolving traces is important to the in-
terpretation of the syntactic structure of a sentence in
both English and Chinese, the prevalence of dropped
nouns in Chinese text gives EC detection added sig-
nificance and urgency. They are not only an impor-
tant component of the syntactic parse of a sentence,
but are also essential to a wide range of NLP appli-
cations. For example, any meaningful tracking of
entities and events in natural language text would
have to include those represented by dropped pro-
nouns. If Chinese is translated into a different lan-
guage, it is also necessary to render these dropped
pronouns explicit if the target language does not al-
low pro-drop. In fact, Chung and Gildea (2010) re-
ported preliminary work that has shown a positive
impact of automatic EC detection on statistical ma-
chine translation.
Some ECs can be resolved to an overt element in
the same text while others only have a generic ref-
erence that cannot be linked to any specific entity.
Still others have a plausible antecedent in the text,
but are not annotated due to annotation limitations.
A common practice is to resolve ECs in two separate
stages (Johnson, 2002; Dienes and Dubey, 2003b;
Dienes and Dubey, 2003a; Campbell, 2004; Gab-
bard et al, 2006; Schmid, 2006; Cai et al, 2011).
The first stage is EC detection, where empty cate-
gories are first located and typed. The second stage
1051
is EC resolution, where empty categories are linked
to an overt element if possible.
In this paper we describe a novel approach to de-
tecting empty categories in Chinese, using the CTB
as training and test data. More concretely, EC de-
tection involves (i) identifying the position of the
EC, relative to some overt word tokens in the same
sentence, and (ii) determining the type of EC, e.g.,
whether it is a dropped pronoun or a trace. We fo-
cus on EC detection here because most of the ECs
in the Chinese Treebank are either not resolved to
an overt element or linked to another EC. For ex-
ample, dropped pronouns (*pro*) are not resolved,
and traces (*T*) in relative clauses are linked to an
empty relative pronoun (*OP*).
In previous work, ECs are either represented lin-
early, where ECs are indexed to the following word
(Yang and Xue, 2010) or attached to nodes in a
phrase structure tree (Johnson, 2002; Dienes and
Dubey, 2003b; Gabbard et al, 2006). In a linear
representation where ECs are indexed to the follow-
ing word, it is difficult to represent consecutive ECs
because that will mean more than one EC will be
indexed to the same word (making the classification
task more complicated). While in English consecu-
tive ECs are relatively rare, in Chinese this is very
common. For example, it is often the case that an
empty relative pronoun (*OP*) is followed imme-
diately by a trace (*T*). Another issue with the lin-
ear representation of ECs is that it leaves unspecified
where the EC should be attached, and crucial depen-
dencies between ECs and other elements in the syn-
tactic structure are not represented, thus limiting the
utility of this task.
In a phrase structure representation, ECs are at-
tached to a hierarchical structure and the problem
of multiple ECs indexed to the same word token can
be avoided because linearly consecutive ECs may be
attached to different non-terminal nodes in a phrase
structure tree. In a phrase structure framework, ECs
are evaluated based on their linear position as well
as on their contribution to the overall accuracy of
the syntactic parse (Cai et al, 2011).
In the present work, we propose to look at EC
detection in a dependency structure representation,
where we define EC detection as (i) determining its
linear position relative to the following word token,
(ii) determining its head it is a dependent of, and (iii)
determining the type of EC. Framing EC detection
this way also requires a new evaluation metric. An
EC is considered to be correctly detected if its linear
position, its head, and its type are all correctly de-
termined. We report experimental results that show
even using this more stringent measure, our EC de-
tection system achieved performance that improved
significantly over the state-of-the-art results.
The rest of the paper is organized as follows. In
Section 2, we will describe how to represent ECs
in a dependency structure in detail and present our
approach to EC detection. In Section 3, we describe
how linguistic information is encoded as features.
In Section 4, we discuss our experimental setup and
present our results. In Section 5, we describe related
work. Section 6 concludes the paper.
2 Approach
In order to detect ECs anchored in a dependency
tree, we first convert the phrase structure trees in the
CTB into dependency trees. After the conversion,
each word token in a dependency tree, including the
ECs, will have one and only one head (or parent).
We then train a classifier to predict the position and
type of ECs in the dependency tree. Let W be a se-
quence of word tokens in a sentence, and T is syn-
tactic parse tree for W , our task is to predict whether
there is a tuple (h, t, e), such that h and t are word to-
kens in W , e is an EC, h is the head of e, and t imme-
diately follows e. When EC detection is formulated
as a classification task, each classification instance
is thus a tuple (h, t). The input to our classifier is
T , which can either be a phrase structure tree or a
dependency tree. We choose to use a phrase struc-
ture tree because phrase structure parsers trained on
the Chinese Treebank are readily available, and we
also hypothesize that phrase structure trees have a
richer hierarchical structure that can be exploited as
features for EC detection.
2.1 Empty categories in the Chinese Treebank
According to the CTB bracketing guidelines (Xue
and Xia, 2000), there are seven different types of
ECs in the CTB. Below is a brief description of the
empty categories:
1. *pro*: small pro, used to represent dropped
pronouns.
1052
2. *PRO*: big PRO, used to represent shared el-
ements in control structures or elements that
have generic references.
3. *OP*: null operator, used to represent empty
relative pronouns.
4. *T*: trace left by movement such as topical-
ization and relativization.
5. *RNR*: right node raising.
6. *: trace left by passivization and raising.
7. *?*: missing elements of unknown category.
An example parse tree with ECs is shown in
Figure 1. In the example, there are two ECs, an
empty relative pronoun (*OP*) and a trace (*T*), a
common syntactic pattern for relative clauses in the
CTB.


Shanghai

Pudong

recently

issue
*OP*

 involve
NN

DEC

document
NR NR
AD
VV
VV
NN
DEC
NP
ADVP
NP
NP
NP
WHNP
VP
IP
CP
CP
NP
VP
VP
IP
"Shanghai Pudong recently enacted 71 regulatory documents involving
the enconomic field."
ASP

AS
T*
NN
QP
CD
M


CLP
ADJP
JJ
	
regulatory
 
economic
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 631?635,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Chinese sentence segmentation as comma classification
Nianwen Xue and Yaqin Yang
Brandeis University, Computer Science Department
Waltham, MA, 02453
{xuen,yaqin}@brandeis.edu
Abstract
We describe a method for disambiguating Chi-
nese commas that is central to Chinese sen-
tence segmentation. Chinese sentence seg-
mentation is viewed as the detection of loosely
coordinated clauses separated by commas.
Trained and tested on data derived from the
Chinese Treebank, our model achieves a clas-
sification accuracy of close to 90% overall,
which translates to an F1 score of 70% for
detecting commas that signal sentence bound-
aries.
1 Introduction
Sentence segmentation, or the detection of sentence
boundaries, is very much a solved problem for En-
glish. Sentence boundaries can be determined by
looking for periods, exclamation marks and ques-
tion marks. Although the symbol (dot) that is used to
represent period is ambiguous because it is also used
as the decimal point or in abbreviations, its resolu-
tion only requires local context. It can be resolved
fairly easily with rules in the form of regular expres-
sions or in a machine-learning framework (Reynar
and Ratnaparkhi, 1997).
Chinese also uses periods (albeit with a different
symbol), question marks, and exclamation marks to
indicate sentence boundaries. Where these punctua-
tion marks exist, sentence boundaries can be unam-
biguously detected. The difference is that the Chi-
nese comma also functions similarly as the English
period in some context and signals the boundary of a
sentence. As a result, if the commas are not disam-
biguated, Chinese would have these ?run-on? sen-
tences that can only be plausibly translated into mul-
tiple English sentences. An example is given in (1),
where one Chinese sentence is plausibly translated
into three English sentences.
(1) ?
this
?
period
??
time
??
AS
?
AS
??
pay attention to
?
this
?
CL
nano
Nano
3
3
?
,
[1]?
even
??
in person
?
visit
?
AS
?
a few
?
AS
??
computer
??
market
,
,
[2]???
comparatively
??
speaking
,
,
[3]??
Zhuoyue
?
?s
??
price
?
relatively
?
low
?
DE
,
,
[4]??
and
?
can
??
guarantee
?
be
??
genuine
?[5]
,
???
therefore
?
place
?
[AS]
?
order
?
.
?I have been paying attention to this Nano 3 re-
cently, [1] and I even visited a few computer
stores in person. [2] Comparatively speaking,
[3] Zhuoyue?s prices are relatively low, [4]
and they can also guarantee that their products
are genuine. [5] Therefore I placed the order.?
In this paper, we formulate Chinese sentence seg-
mentation as a comma disambiguation problem. The
problem is basically one of separating commas that
mark sentence boundaries (such as [2] and [5] in (1))
from those that do not (such as [1], [3] and [4]).
Sentences that can be split on commas are gener-
ally loosely coordinated structures that are syntacti-
cally and semantically complete on their own, and
they do not have a close syntactic relation with one
another. We believe that a sentence boundary detec-
tion task that disambiguates commas, if successfully
631
solved, simplifies downstream tasks such as parsing
and Machine Translation.
The rest of the paper is organized as follows. In
Section 2, we describe our procedure for deriving
training and test data from the Chinese Treebank
(Xue et al, 2005). In Section 3, we present our
learning procedure. In Section 4 we report our re-
sults. Section 5 discusses related work. Section 6
concludes our paper.
2 Obtaining data
To our knowledge, there is no data in the public
domain with commas explicitly annotated based on
whether they mark sentence boundaries. One could
imagine using parallel data where a Chinese sen-
tence is word-aligned with multiple English sen-
tences, but such data is generally noisy and com-
mas are not disambiguated based on a uniform stan-
dard. We instead pursued a different path and de-
rived our training and test data from the Chinese
Treebank (CTB). The CTB does not disambiguate
commas explicitly, and just like the Penn English
Treebank (Marcus et al, 1993), the sentence bound-
aries in the CTB are identified by periods, exclama-
tion and question marks. However, there are clear
syntactic patterns that can be used to disambiguate
the two types of commas. Commas that mark sen-
tence boundaries delimit loosely coordinated top-
level IPs, as illustrated in Figure 1, and commas that
don?t cover all other cases. One such example is
Figure 2, where a PP is separated from the rest of
the sentence with a comma. We devised a heuristic
algorithm to detect loosely coordinated structures in
the Chinese Treebank, and labeled each comma with
either EOS (end of a sentence) or Non-EOS (not the
end of a sentence).
3 Learning
After the commas are labeled, we have basically
turned comma disambiguation into a binary classi-
fication problem. The syntactic structures are an
obvious source of information for this classification
task, so we parsed the entire CTB 6.0 in a round-
robin fashion. We divided CTB 6.0 into 10 portions,
and parsed each portion with a model trained on
other portions, using the Berkeley parser (Petrov and
Klein, 2007). The labels for the commas are derived
????
?
?? ??
?
??
?
??
??
??
?
?? ????
?
IP PU IP PU IP PU
IP
NP VP
??
NP
VP
VV
NP
NP
VP
VV
IP
NP VP
VV NP
*pro*
ADVP
VP
??
??? ??
ADVP
VP
VV
Figure 1: Sentence-boundary denoting comma
IP
PP PU NP NP VP PU
?
P NP DNP NP
NP
DEG
VV
??
?
? ?? ? ?? ?
?? ?? ? ??? ?? ??
??
??
?
Figure 2: Non-sentence boundary denoting comma
from the gold-standard parses using the heuristics
described in Section 2, as they obviously should be.
We first established a baseline by applying the same
heuristic algorithm to the automatic parses. This will
give us a sense of how accurately commas can be
disambiguated given imperfect parses. The research
question we?re trying to address here basically is:
can we improve on the baseline accuracy with a ma-
chine learning model?
We conducted our experiments with a Maximum
Entropy classifier trained with the Mallet package
(McCallum, 2002). The following are the features
we used to train our classifier. All features are de-
scribed relative to the comma being classified and
the context is the sentence that the comma is in. The
actual feature values for the first comma in Figure 1
are given as examples:
1. Part-of-speech tag of the previous word, and
the string representation of the previous word
if it has a frequency of greater than 20 in the
training corpus, e.g., f1=VV, f2=??.
2. Part-of-speech of the following word and the
632
string representation of the following word if it
has a frequency of greater than 20 in the train-
ing corpus, e.g., f3=JJ, f4=??
3. The string representation of the following word
if it occurs more than 12,000 times in sentence-
initial positions in a large corpus external to our
training and test data.1
4. The phrase label of the left sibling and the
phrase label of their right sibling in the syntac-
tic parse tree, as well as their conjunction, e.g,
f6=IP, f7=IP, f8=IP+IP
5. The conjunction of the ancestors, the phrase la-
bel of the left sibling, and the phrase label of
the right sibling. The ancestor is defined as the
path from the parent of the comma to the root
node of the parse tree, e.g., f9=IP+IP+IP.
6. Whether there is a subordinating conjunction
(e.g., ?if?, ?because?) to the left of the comma.
The search starts at the comma and stops at the
previous punctuation mark or the beginning of
the sentence, e.g., f10=noCS.
7. Whether the parent of the comma is a coordi-
nating IP construction. A coordinating IP con-
struction is an IP that dominates a list of coor-
dinated IPs, e.g., f11=CoordIP.
8. Whether the comma is a top-level child, defined
as the child of the root node of the syntactic
tree, e.g., f12=top.
9. Whether the parent of the comma is a
top-level coordinating IP construction, e.g.,
f13=top+coordIP.
10. The punctuation mark template for this sen-
tence, e.g., f14=,+,+?
11. whether the length difference between the left
and right segments of the comma is smaller
than 7. The left (right) segment spans from the
previous (next) punctuation mark or the begin-
ning (end) of the sentence to the comma, e.g.,
f15=>7
4 Results and discussion
Our comma disambiguation models are trained and
evaluated on a subset of the Chinese TreeBank
(CTB) 6.0, released by the LDC. The unused por-
tion of CTB 6.0 consists of broadcast news data that
1This feature is not instantiated here because the following
word in this example does not occur with sufficient accuracy.
contains disfluencies, different from the rest of the
CTB 6.0. We used the training/test data split rec-
ommended in the Chinese Treebank documentation.
The CTB file IDs used in our experiments are listed
in Table 1. The automatic parses in each test set
are produced by retraining the Berkeley parser on
its corresponding training set, plus the unused por-
tion of the CTB 6.0. Measured by the ParsEval met-
ric (Black et al, 1991), the parsing accuracy on the
CTB test set stands at 83.63% (F-score), with a pre-
cision of 85.66% and a recall of 81.69%.
Data Train Test
CTB
41-325, 400-454, 500-554 1-40
590-596, 600-885, 900 901-931
1001-1078, 1100-1151
Table 1: Data set division.
There are 1,510 commas in the test set, and our
heuristic baseline algorithm is able to correctly label
1,321 or 87.5% of the commas. Among these, 250
or 16.6% of them are EOS commas that mark sen-
tence boundaries and 1,260 of them are Non-EOS
commas. The results of our experiments are pre-
sented in Table 2. The baseline precision and recall
for the EOS commas are 59.1% and 79.6% respec-
tively with an F1 score of 67.8% . For Non-EOS
commas, the baseline precision and recall are 95.7%
and 89.0% respectively, amounting to an F1 score of
70.1%. The learned maximum classifier achieved a
modest improvement over the baseline. The over-
all accuracy of the learned model is 89.2%, just shy
of 90%. The precision and recall for EOS commas
are 64.7% and 76.4% respectively and the combined
F1 score is 70.1%. For Non-EOS commas, the pre-
cision and recall are 95.1% and 91.7% respectively,
with the F1 score being 93.4%. Other than a list
of most frequent words that start a sentence, all the
features are extracted from the sentence the comma
occurs in. Given that the heuristic algorithm and the
learned model use essentially the same source of in-
formation, we attribute the improvement to the use
of lexical features that the heuristic algorithm cannot
easily take advantage of.
Table 3 shows the contribution of individual fea-
ture groups. The numbers reflect the accuracy when
each feature group is taken out of the model. While
all the features have made a contribution to the over-
633
Baseline Learning
(%) p r f1 p r f1
Overall 87.5 89.2
EOS 59.1 79.6 67.8 64.7 76.4 70.1
Non-
EOS
95.7 89.0 92.2 95.1 91.7 93.4
Table 2: Accuracy for the baseline heuristic algorithm
and the learned model
all accuracy on the development set, some of the
features (3 and 8) actually hurt the overall perfor-
mance slightly on the test set. What?s interesting is
while the heuristic algorithm that is based entirely
on syntactic structure produced a strong baseline,
when formulated as features they are not at all effec-
tive. In particular, feature groups 7, 8, 9 are explicit
reformulations of the heuristic algorithm, but they
all contributed very little to or even slightly hurt the
overall performance. The more effective features are
the lexical features (1, 2, 10, 11) probably because
they are more robust. What this suggests is that we
can get reasonable sentence segmentation accuracy
without having to parse the sentence (or rather, the
multi-sentence group) first. The sentence segmenta-
tion can thus come before parsing in the processing
pipeline even in a language like Chinese where sen-
tences are not unambiguously marked.
overall f1 (EOS) f1 (non-EOS)
all 89.2 70.1 93.4
- (1,2) 87.5 67.7 92.3
-10 87.8 67.5 92.5
-11 88.6 68.6 93.1
-4 89.0 69.6 93.3
-5 89.1 69.5 93.3
-6 89.1 69.9 93.4
-7 89.1 70.1 93.4
-9 89.1 69.7 93.3
-8 89.2 70.5 93.4
- 3 89.4 70.5 93.5
Table 3: Feature effectiveness
5 Related work
There has been a fair amount of research on punctua-
tion prediction or generation in the context of spoken
language processing (Lu and Ng, 2010; Guo et al,
2010). The task presented here is different in that the
punctuation marks are already present in the text and
we are only concerned with punctuation marks that
are semantically ambiguous. Our specific focus is
on the Chinese comma, which sometimes signals a
sentence boundary and sometimes doesn?t. The Chi-
nese comma has also been studied in the context of
syntactic parsing for long sentences (Jin et al, 2004;
Li et al, 2005), where the study of comma is seen as
part of a ?divide-and-conquer? strategy to syntactic
parsing. Long sentences are split into shorter sen-
tence segments on commas before they are parsed,
and the syntactic parses for the shorter sentence seg-
ments are then assembled into the syntactic parse for
the original sentence. We study comma disambigua-
tion in its own right aimed at helping a wide range of
NLP applications that include parsing and Machine
Translation.
6 Conclusion
The main goal of this short paper is to bring to
the attention of the field a problem that has largely
been taken for granted. We show that while sen-
tence boundary detection in Chinese is a relatively
easy task if formulated based on purely orthographic
grounds, the problem becomes much more challeng-
ing if we delve deeper and consider the semantic and
possibly the discourse basis on which sentences are
segmented. Seen in this light, the central problem
to Chinese sentence segmentation is comma disam-
biguation. We trained a statistical model using data
derived from the Chinese Treebank and reported
promising preliminary results. Much remains to be
done regarding how sentences in Chinese should be
segmented and how this problem should be modeled
in a statistical learning framework.
Acknowledgments
This work is supported by the National Science
Foundation via Grant No. 0910532 entitled ?Richer
Representations for Machine Translation?. All
views expressed in this paper are those of the au-
thors and do not necessarily represent the view of
the National Science Foundation.
634
References
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A proce-
dure for quantitively comparing the syntactic coverage
of English grammars. In Proceedings of the DARPA
Speech and Natural Language Workshop, pages 306?
311.
Yuqing Guo, Haifeng Wang, and Josef Van Genabith.
2010. A Linguistically Inspired Statistical Model for
Chinese Punctuation Generation. ACM Transactions
on Asian Language Processing, 9(2).
Meixun Jin, Mi-Young Kim, Dong-Il Kim, and Jong-
Hyeok Lee. 2004. Segmentation of Chinese Long
Sentences Using Commas. In Proceedings of the
SIGHANN Workshop on Chinese Language Process-
ing.
Xing Li, Chengqing Zong, and Rile Hu. 2005. A Hier-
archical Parsing Approach with Punctuation Process-
ing for Long Sentence Sentences. In Proceedings of
the Second International Joint Conference on Natural
Language Processing: Companion Volume including
Posters/Demos and Tutorial Abstracts.
We Lu and Hwee Tou Ng. 2010. Better Punctuation
Prediction with Dynamic Conditional Random Fields.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, MIT, Mas-
sachusetts.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a Large Annotated Corpus of English:
the Penn Treebank. Computational Linguistics.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Slav Petrov and Dan Klein. 2007. Improved Inferencing
for Unlexicalized Parsing. In Proc of HLT-NAACL.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
Maximum Entropy Approach to Identifying Sentence
Boundaries. In Proceedings of the Fifth Conference on
Applied Natural Language Processing (ANLP), Wash-
ington, D.C.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2):207?238.
635
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 69?77,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
PDTB-style Discourse Annotation of Chinese Text
Yuping Zhou
Computer Science Department
Brandeis University
Waltham, MA 02452
yzhou@brandeis.edu
Nianwen Xue
Computer Science Department
Brandeis University
Waltham, MA 02452
xuen@brandeis.edu
Abstract
We describe a discourse annotation scheme
for Chinese and report on the preliminary re-
sults. Our scheme, inspired by the Penn Dis-
course TreeBank (PDTB), adopts the lexically
grounded approach; at the same time, it makes
adaptations based on the linguistic and statisti-
cal characteristics of Chinese text. Annotation
results show that these adaptations work well
in practice. Our scheme, taken together with
other PDTB-style schemes (e.g. for English,
Turkish, Hindi, and Czech), affords a broader
perspective on how the generalized lexically
grounded approach can flesh itself out in the
context of cross-linguistic annotation of dis-
course relations.
1 Introduction
In the realm of discourse annotation, the Penn Dis-
course TreeBank (PDTB) (Prasad et al, 2008) sep-
arates itself by adopting a lexically grounded ap-
proach: Discourse relations are lexically anchored
by discourse connectives (e.g., because, but, there-
fore), which are viewed as predicates that take ab-
stract objects such as propositions, events and states
as their arguments. In the absence of explicit dis-
course connectives, the PDTB asks the annotator to
fill in a discourse connective that best describes the
discourse relation between these two sentences, in-
stead of selecting from an inventory of predefined
discourse relations. By keeping the discourse an-
notation lexically grounded even in the case of im-
plicit discourse relations, the PDTB appeals to the
annotator?s judgment at an intuitive level. This is in
contrast with an approach in which the set of dis-
course relations are pre-determined by linguistic ex-
perts and the role of the annotator is just to select
from those choices (Mann and Thompson, 1988;
Carlson et al, 2003). This lexically grounded ap-
proach led to consistent and reliable discourse anno-
tation, a feat that is generally hard to achieve for dis-
course annotation. The PDTB team reported inter-
annotator agreement in the lower 90% for explicit
discourse relations (Miltsakaki et al, 2004).
In this paper we describe a discourse annota-
tion scheme for Chinese that adopts this lexically
grounded approach while making adaptations when
warranted by the linguistic and statistical properties
of Chinese text. This scheme is shown to be practi-
cal and effective in the annotation experiment.
The rest of the paper is organized as follows: In
Section 2, we review the key aspects of the PDTB
annotation scheme under discussion in this paper. In
Section 3, we first show that some key features of
Chinese make adaptations necessary in Section 3.1,
and then in Section 3.2, we present our systematic
adaptations that follow from the differences outlined
in Section 3.1. In Section 4, we present the prelim-
inary annotation results we have so far. And finally
in Section 5, we conclude the paper.
2 The PDTB annotation scheme
As mentioned in the introduction, discourse relation
is viewed as a predication with two arguments in the
framework of the PDTB. To characterize the pred-
ication, the PDTB annotates its argument structure
and sense. Two types of discourse relation are dis-
tinguished in the annotation: explicit and implicit.
69
Although their annotation is carried out separately, it
conforms to the same paradigm of a discourse con-
nective with two arguments. In what follows, we
highlight the key points that will be under discussion
in the following sections. To get a more compre-
hensive and detailed picture of the PDTB scheme,
see the PDTB 2.0 annotation manual (Prasad et al,
2007).
2.1 Annotation of explicit discourse relations
Explicit discourse relations are those anchored by
explicit discourse connectives in text. Explicit con-
nectives are drawn from three grammatical classes:
? Subordinating conjunctions: e.g., because,
when, since, although;
? Coordinating conjunctions: e.g., and, or, nor;
? Discourse adverbials: e.g., however, other-
wise, then, as a result, for example.
Not all uses of these lexical items are considered to
function as a discourse connective. For example,
coordinating conjunctions appearing in VP coordi-
nations, such as ?and? in (1), are not annotated as
discourse connectives.
(1) More common chrysotile fibers are curly and
are more easily rejected by the body, Dr. Moss-
man explained.
The text spans of the two arguments of a discourse
connective are marked up. The two arguments, Arg1
and Arg2, are defined based on the physical location
of the connective: Arg2 is the argument expressed
by the clause syntactically bound to the connective,
and Arg1 is the other argument. There are no restric-
tions on how many clauses can be included in the
text span for an argument other than the Minimality
Principle: Only as many clauses and/or sentences
should be included in an argument selection as are
minimally required and sufficient for the interpreta-
tion of the relation.
2.2 Annotation of implicit discourse relations
In the case of implicit discourse relations, annotators
are asked to insert a discourse connective that best
conveys the implicit relation; when no such connec-
tive expression is appropriate, the implicit relation
is further distinguished as the following three sub-
types:
? AltLex: when insertion of a connective leads
to redundancy due to the presence of an alter-
natively lexicalized expression, as in (2).
? EntRel: when the only relation between the
two arguments is that they describe different as-
pects of the same entity, as in (3).
? NoRel: when neither a lexicalized discourse re-
lation nor entity-based coherence is present. It
is to be noted that at least some of the ?NoRel?
cases are due to the adjacency constraint (see
below for more detail).
(2) And she further stunned her listeners by re-
vealing her secret garden design method: [Arg1
Commissioning a friend to spend five or six
thousand dollars . . . on books that I ultimately
cut up.] [Arg2 AltLex After that, the layout had
been easy.
(3) [Arg1 Hale Milgrim, 41 years old, senior vice
president, marketing at Elecktra Entertainment
Inc., was named president of Capitol Records
Inc., a unit of this entertainment concern].
[Arg2 EntRel Mr. Milgrim succeeds David
Berman, who resigned last month].
There are restrictions on what kinds of implicit
relations are subjected to annotation, presented be-
low. These restrictions do not have counterparts in
explicit relation annotation.
? Implicit relations between adjacent clauses in
the same sentence not separated by a semi-
colon are not annotated, even though the rela-
tion may very well be definable. A case in point
is presented in (4) below, involving an intra-
sentential comma-separated relation between a
main clause and a free adjunct.
? Implicit relations between adjacent sentences
across a paragraph boundary are not annotated.
? The adjacency constraint: At least some part
of the spans selected for Arg1 and Arg2 must
belong to the pair of adjacent sentences initially
identified for annotation.
(4) [MC The market for export financing was liber-
alized in the mid-1980s], [FA forcing the bank
to face competition].
70
2.3 Annotation of senses
Discourse connectives, whether originally present in
the data in the case of explicit relations, or filled in
by annotators in the case of implicit relations, along
with text spans marked as ?AltLex?, are annotated
with respect to their senses. There are three levels in
the sense hierarchy:
? Class: There are four major semantic classes:
TEMPORAL, CONTINGENCY, COMPARISON,
and EXPANSION;
? Type: A second level of types is further de-
fined for each semantic class. For example,
under the class CONTINGENCY, there are two
types: ?Cause? (relating two situations in a di-
rect cause-effect relation) and ?Condition? (re-
lating a hypothetical situation with its (possi-
ble) consequences);1
? Subtype: A third level of subtypes is defined
for some, but not all, types. For instance, under
the type ?CONTINGENCY:Cause?, there are two
subtypes: ?reason? (for cases like because and
since) and ?result? (for cases like so and as a
result).
It is worth noting that a type of implicit relation,
namely those labeled as ?EntRel?, is not part of the
sense hierarchy since it has no explicit counterpart.
3 Adapted scheme for Chinese
3.1 Key characteristics of Chinese text
Despite similarities in discourse features between
Chinese and English (Xue, 2005), there are differ-
ences that have a significant impact on how dis-
course relations could be best annotated. These dif-
ferences can be illustrated with (5):
(5) ??
according to reports
?
,
[AO1 ??
Dongguan
??
Customs
?
in total
??
accept
??
company
??
contract
??
record
?????
8400 plus
? ]
CLASS
?[AO2
,
?
compare
??
pilot
?
before
?
slight
?
EXIST
?? ]
increase
?
,
[AO3??
company
1There is another dimension to this level, i.e. literal or prag-
matic use. If this dimension is taken into account, there could be
said to be four types: ?Cause?, ?Pragmatic Cause?, ?Condition?,
and ?Pragmatic Condition?. For details, see Prasad et al (2007).
??
respond/response
?? ]
well/good
?
,
[AO4??
generally
??
acknowledge
?? ]
accept/acceptance
?
.
?According to reports, [AO1 Dongguan District
Customs accepted more than 8400 records of com-
pany contracts], [AO2 a slight increase from before
the pilot]. [AO3 Companies responded well], [AO4
generally acknowledging acceptance].?
This sentence reports on how a pilot program
worked in Dongguan City. Because all that is said
is about the pilot program, it is perfectly natural to
include it all in a single sentence in Chinese. Intu-
itively though, there are two different aspects of how
the pilot program worked: the number of records
and the response from the affected companies. To
report the same facts in English, it is more natural
to break them down into two sentences or two semi-
colon-separated clauses, but in Chinese, not only are
they merely separated by comma, but also there is no
connective relating them.
This difference in writing style necessitates re-
thinking of the annotation scheme. If we apply the
PDTB scheme to the English translation, regardless
of whether the two pieces of facts are expressed in
two sentences or two semi-colon-separated clauses,
at least one discourse relation will be annotated, re-
lating these two text units. In contrast, if we apply
the same scheme to the Chinese sentence, no dis-
course relation will be picked out because this is
just one comma-separated sentence with no explicit
discourse connectives in it. In other words, the dis-
course relation within the Chinese sentence, which
would be captured in its English counterpart follow-
ing the PDTB procedure, would be lost when anno-
tating Chinese. Such loss is not a sporadic occur-
rence but rather a very prevalent one since it is asso-
ciated with the customary writing style of Chinese.
To ensure a reasonable level of coverage, we need to
consider comma-delimited intra-sentential implicit
relations when annotating Chinese text.
There are some complications associated with this
move. One of them is that it introduces into dis-
course annotation considerable ambiguity associ-
ated with the comma. For example, the first in-
stance of comma in (5), immediately following ??
?? (?according to reports?), clearly does not indi-
cate a discourse relation, so it needs to be spelt out in
71
the guidelines how to exclude such cases of comma
as discourse relation indicators. We think, however,
that disambiguating the commas in Chinese text is
valuable in its own right and is a necessary step in
annotating discourse relations.
Another complication is that some comma-
separated chunks are ambiguous as to whether they
should be considered potential arguments in a dis-
course relation. The chunks marked AO2 and AO4
in (5) are examples of such cases. They, judging
from their English translation, may seem clear cases
of free adjuncts in PDTB terms (Prasad et al, 2007),
but there is no justification for treating them as such
in Chinese. The lack of justification comes from at
least three features of Chinese:
? Certain words, for instance, ???? (?re-
spond/response?), ???? (?well/good?) and
???? (?accept/acceptance?), are ambiguous
with respect to their POS, and when they com-
bine, the resulting sentence may have more
than one syntactic analysis. For example, AO3
may be literally translated as ?Companies re-
sponded well? or ?Companies? response was
good?.
? There are no inflectional clues to differenti-
ate free adjuncts and main clauses. For ex-
ample, one can be reasonably certain that ??
?? (?acknowledge?) functions as a verb in (5),
however, there is no indication whether it is
in the form corresponding to ?acknowledging?
or ?acknowledged? in English. Or putting it
differently, whether one wants to express in
Chinese the meaning corresponding to the -ing
form or the tensed form in English, the same
form ???? could apply.
? Both subject and object can be dropped in Chi-
nese, and they often are when they are infer-
able from the context. For example, in the two-
sentence sequence below, the subject of (7) is
dropped since it is clearly the same as the sub-
ject of the previous sentence in (6) .
(6) [S1
recent
?
five
?
years
?
since
?
,
?
Shanghai
??
through
??
actively
??
from
?
other
?
province
?
city
?
procure
??
export
??
supply
??
,
?
organize
??
China
??
East
??
Export
??
Commodity
??
Fair
???
etc.
?
event,
???
strengthen
??
port
??
to
?
whole country
??
DE
?
connection
??
capability
??
.
?]
?[S1 In the past five years, Shanghai strength-
ened the connection of its port to other areas
of the country through actively procuring ex-
port supplies from other provinces and cities,
and through organizing events such as the East
China Export Commodities Fair.]?
(7) [S2??
At the same time
?
,
??
develop
??
transnational
??
operation
?
,
??
vigorously
??
open up
???
diversified
???]
market
?[S2 At the same time, (it) developed transna-
tional operations (and) vigorously opened up
diversified markets.]?
Since the subject can be omitted from the en-
tire sentence, absence or presence of subject in
a clause is not an indication whether the clause
is a main clause or a free adjunct, or whether it
is part of a VP coordination without a connec-
tive. So if we take into account both the lack of
differentiating inflectional clues and the possi-
bility of omitting the subject, AO4 in (5) may
be literally translated as ?generally acknowl-
edging acceptance?, or ?(and) generally ac-
knowledged acceptance?, or ?(companies) gen-
erally acknowledged acceptance?, or ?(compa-
nies) generally acknowledged (they) accepted
(it)?.
Since in Chinese, there is no reliable indicator dis-
tinguishing between main clauses and free adjuncts,
or distinguishing between coordination on the clause
level without the subject and coordination on the VP
level, we will not rely on these distinctions in anno-
tation, as the PDTB team does in their annotation.
These basic decisions directly based on linguistic
characteristics of Chinese lead to more systematic
adaptations to the annotation scheme, to which we
will turn in the next subsection.
3.2 Systematic adaptations
The main consequence of the basic decisions de-
scribed in Section 3.1 is that we have a whole lot
72
more tokens of implicit relation than explicit rela-
tion to deal with. According to a rough count on
20 randomly selected files from Chinese Treebank
(Xue et al, 2005), 82% are tokens of implicit rela-
tion, compared to 54.5% in the PDTB 2.0. Given
the overwhelming number of implicit relations, we
re-examine where it could make an impact in the an-
notation scheme. There are three such areas.
3.2.1 Procedural division between explicit and
implicit discourse relation
In the PDTB, explicit and implicit relations are
annotated separately. This is probably partly be-
cause explicit connectives are quite abundant in En-
glish, and partly because the project evolved in
stages, expanding from the more canonical case of
explicit relation to implicit relation for greater cov-
erage. When annotating Chinese text, maintaining
this procedural division makes much less sense: the
landscape of discourse relation (or at least the key
elements of it) has already been mapped out by the
PDTB work and to set up a separate task to cover
18% of the data does not seem like a worthwhile
bother without additional benefits for doing so.
So the question now is how to annotate explicit
and implicit relations in one fell swoop? In Chi-
nese text, the use of a discourse connective is al-
most always accompanied by a punctuation or two
(usually period and/or comma), preceding or flank-
ing it. So a sensible solution is to rely on punctu-
ations as the denominator between explicit and im-
plicit relations;and in the case of explicit relation,
the connective will be marked up as an attribute of
the discourse relation. This unified approach simpli-
fies the annotation procedure while preserving the
explicit/implicit distinction in the process.
One might question, at this point, whether such
an approach can still call itself ?lexically grounded?.
Certainly not if one interprets the term literally ; but
in a broader sense, our approach can be seen as an
instantiation of a generalized version of it, much the
same way that the PDTB is an, albeit different, in-
stantiation of it for English. The thrust of the lexi-
cally grounded approach is that discourse annotation
should be a data-driven, bottom-up process, rather
than a top-down one, trying to fit data into a pre-
scriptive system. Once the insight that a discourse
connective functions like a predicate with two ar-
guments is generalized to cover all discourse rela-
tions, there is no fundamental difference between
explicit and implicit discourse relations: both work
like a predicate whether or not there is a lexicaliza-
tion of it. As to what role this distinction plays in
the annotation procedure, it is an engineering issue,
depending on a slew of factors, among which are
cross-linguistic variations. In the case of Chinese,
we think it is more economical to treat explicit and
implicit relations alike in the annotation process.
To treat explicit and implicit relations alike actu-
ally goes beyond annotating them in one pass; it also
involves how they are annotated, which we discuss
next.
3.2.2 Annotation of implicit discourse relations
In the PDTB, treatment of implicit discourse rela-
tions is modeled after that of explicit relations, and at
the same time, some restrictions are put on implicit,
but not explicit, relations. This is quite understand-
able: implicit discourse relations tend to be vague
and elusive, so making use of explicit relations as a
prototype helps pin them down, and restrictions are
put in place to strike a balance between high relia-
bility and good coverage. When implicit relations
constitute a vast majority of the data as is the case
with Chinese, both aspects need to be re-examined
to strike a new balance.
In the PDTB, annotators are asked to insert a
discourse connective that best conveys the implicit
discourse relation between two adjacent discourse
units; when no such connective expression is ap-
propriate, the implicit discourse relation is further
distinguished as ?AltLex?, ?EntRel?, and ?NoRel?.
The inserted connectives and those marked as ?Al-
tLex?, along with explicit discourse connectives, are
further annotated with respect to their senses.
When a connective needs to be inserted in a ma-
jority of cases, the difficulty of the task really stands
out. In many cases, it seems, there is a good rea-
son for not having a connective present and because
of it, the wording rejects insertion of a connective
even if it expresses the underlying discourse relation
exactly (or sometimes, maybe the wording itself is
the reason for not having a connective). So to try
to insert a connective expression may very well be
too hard a task for annotators, with little to show for
their effort in the end.
73
Furthermore, the inter-annotator agreement for
providing an explicit connective in place of an im-
plicit one is computed based on the type of explicit
connectives (e.g. cause-effect relations, temporal re-
lations, contrastive relations, etc.), rather than based
on their identity (Miltsakaki et al, 2004). This sug-
gests that a reasonable degree of agreement for such
a task may only be reached with a coarse classifica-
tion scheme.
Given the above two considerations, our solution
is to annotate implicit discourse relations with their
senses directly, bypassing the step of inserting a con-
nective expression. It has been pointed out that to
train annotators to reason about pre-defined abstract
relations with high reliability might be too hard a
task (Prasad et al, 2007). This difficulty can be
overcome by associating each semantic type with
one or two prototypical explicit connectives and ask-
ing annotators to consider each to see if it expresses
the implicit discourse relation. This way, annotators
have a concrete aid to reason about abstract relations
without having to choose one connective from a set
expressing roughly the same relation or having to
worry about whether insertion of the connective is
somehow awkward.
It should be noted that annotating implicit rela-
tions directly with their senses means that sense an-
notation is no longer restricted to those that can be
lexically expressed, but also includes those that can-
not, notably those labeled ?EntRel/NoRel? in the
PDTB.2 In other words, we annotate senses of dis-
course relations, not just connectives and their lex-
ical alternatives (in the case of AltLex). This ex-
pansion is consistent with the generalized view of
the lexically grounded approach discussed in Sec-
tion 3.2.1.
With respect to restrictions on implicit relation,
we will adopt them as they prove to be necessary
in the annotation process, with one exception. The
exception is the restriction that implicit relations be-
tween adjacent clauses in the same sentence not sep-
arated by a semi-colon are not annotated. This re-
striction seems to apply mainly to a main clause and
any free adjunct attached to it in English; in Chinese,
however, the distinction between a main clause and a
2Thus ?EntRel? and ?NoRel? are treated as relation senses,
rather than relation types, in our scheme.
free adjunct is not as clear-cut for reasons explained
in Section 3.1. So this restriction is not applicable
for Chinese annotation.
3.2.3 Definition of Arg1 and Arg2
The third area that an overwhelming number of
implicit relation in the data affects is how Arg1 and
Arg2 are defined. As mentioned in the introduc-
tion, discourse relations are viewed as a predication
with two arguments. These two arguments are de-
fined based on the physical location of the connec-
tive in the PDTB: Arg2 is the argument expressed by
the clause syntactically bound to the connective and
Arg1 is the other argument. In the case of implicit
relations, the label is assigned according to the text
order.
In an annotation task where implicit relations con-
stitute an overwhelming majority, the distinction of
Arg1 and Arg2 is meaningless in most cases. In addi-
tion, the phenomenon of parallel connectives is pre-
dominant in Chinese. Parallel connectives are pairs
of connectives that take the same arguments, exam-
ples of which in English are ?if..then?, ?either..or?,
and ?on the one hand..on the other hand?. In Chi-
nese, most connectives are part of a pair; though
some can be dropped from their pair, it is considered
?proper? or formal to use both. (8) below presents
two such examples, for which parallel connectives
are not possible in English.
(8) a. ??
London
??
stock market
?
because
??
coincide
???
Bank Holiday
?
,
?
therefore
??
NEG
???
open market
?London Stock Market did not open because it
was Bank Holiday.?
b. ??
Although
??
they
?
NEG
?
leave
?
land
?
,
?
NEG
?
leave
?
home village
?
,
?
but
??
strict
?
PART
?
speak
?
already
??
no longer
?
be
??
tradition
??
sense
?
PREP
?
DE
???
peasant
?Although they do not leave land or their home
village, strictly speaking, they are no longer
peasants in the traditional sense.?
In the PDTB, parallel connectives are annotated dis-
continuously; but given the prevalence of such phe-
nomenon in Chinese, such practice would generate
74
a considerably high percentage of essentially repeti-
tive annotation among explicit relations.
So the situation with Chinese is that distinguish-
ing Arg1 and Arg2 the PDTB way is meaningless
in most cases, and in the remaining cases, it of-
ten results in duplication. Rather than abandoning
the distinction altogether, we think it makes more
sense to define Arg1 and Arg2 semantically. It will
not create too much additional work beyond distinc-
tion of different senses of discourse relation in the
PDTB. For example, in the semantic type CONTIN-
GENCY:Cause, we can define ?reason? as Arg1 and
?result? as Arg2. In this scheme, no matter which
one of? (?because?) and? (?therefore?) appears
without the other, or if they appear as a pair in a
sentence, or if the relation is implicit, the Arg1 and
Arg2 labels will be consistently assigned to the same
clauses.
This approach is consistent with the move from
annotating senses of connectives to annotating
senses of discourse relations, pointed out in Section
3.2.2. For example, in the PDTB?s sense hierarchy,
?reason? and ?result? are subtypes under type CON-
TINGENCY:Cause: ?reason? applies to connectives
like ?because? and ?since? while ?result? applies
to connectives like ?so? and ?as a result?. When
we move to annotating senses of discourse relations,
since both types of connectives express the same un-
derlying discourse relation, there will not be further
division under CONTINGENCY:Cause, and the ?rea-
son?/?result? distinction is an intrinsic property of
the semantic type. We think this level of generality
makes sense semantically.
4 Annotation experiment
To test our adapted annotation scheme, we have con-
ducted annotation experiments on a modest, yet sig-
nificant, amount of data and computed agreement
statistics.
4.1 Set-up
The agreement statistics come from annotation con-
ducted by two annotators in training so far. The data
set consists of 98 files taken from the Chinese Tree-
bank (Xue et al, 2005). The source of these files is
Xinhua newswire. The annotation is carried out on
the PDTB annotation tool3.
4.2 Inter-annotator agreement
To evaluate our proposed scheme, we measure
agreement on each adaption proposed in Section
3, as well as agreement on argument span deter-
mination. Whenever applicable, we also present
(roughly) comparable statistics of the PDTB (Milt-
sakaki et al, 2004). The results are summarized in
Table 1.
Chinese PDTB
tkn no. F(p/r) (%) (%)
rel-ident 3951*
95.4
N/A
(96.0/94.7)
rel-type 3951 95.1 N/A
imp-sns-type 2967 87.4 72
arg-order 3059 99.8 N/A
argument span
exp-span-xm 1580 84.2 90.2
exp-span-pm 1580 99.6 94.5
imp-span-xm 5934 76.9 85.1
overall-bnd- 14039*
87.7
N/A
(87.5/87.9)
Table 1: Inter-annotator agreement in various aspects
of Chinese discourse annotation: rel-ident, discourse
relation identification; rel-type, relation type classifica-
tion; imp-sns-type, classification of sense type of im-
plicit relations; arg-order, order determination of Arg1
and Arg2. For agreement on argument spans, the
naming convention is <type-of-relation>-<element-as-
independent-token>-<matching-method>. exp: explicit
relations; imp: implicit relations; span: argument span;
xm: exact match; pm: partial match; bnd: boundary. *:
number of tokens agreed on by both annotators.
The first adaption we proposed is to annotate ex-
plicit and implicit discourse relations in one pass.
This introduces two steps, at which agreement can
each be measured: First, the annotator needs to
make the judgment, at each instance of the punctu-
ations, whether there is a discourse relation (a step
we call ?relation identification?); second, once a dis-
course relation is identified, the annotator needs to
classify the type as one of ?Explicit?, ?Implicit?, or
?AltLex? (a step we call ?relation type classifica-
tion?). The agreement at these two steps is 95.4%
3http://www.seas.upenn.edu/?pdtb/tools.shtml#annotator
75
and 95.1% respectively.
The second adaption is to bypass the step of in-
serting a connective when annotating an implicit dis-
course relation and classify the sense directly. The
third adaptation is to define Arg1 and Arg2 semanti-
cally for each sense. To help annotators think about
relation sense abstractly and determine the order of
the arguments, we put a helper item alongside each
sense label, like ?Causation: ??arg1??arg2?
(?Causation: because arg1 therefore arg2?). This
approach works well, as evidenced by 87.4%4 and
99.8% agreement for the two processes respectively.
To evaluate agreement on determining argument
span, we adopt four measures. In the first three,
explicit and implicit relations are calculated sepa-
rately (although they are actually annotated in the
same process) to make our results comparable to
the published PDTB results. Each argument span is
treated as an independent token and either exact or
partial match (i.e. if two spans share one boundary)
counts as 1. The fourth measure is less stringent than
exact match and more stringent than partial match:
It groups explicit and implicit relation together and
treats each boundary as an independent token. Typ-
ically, an argument span has two boundaries, but it
can have four (or more) boundaries when an argu-
ment span is interrupted by a connective and/or an
AltLex item.
Evidently, determining argument span is the most
challenging aspect of discourse annotation. How-
ever, it should be pointed out that agreement was on
an overall upward trend, which became especially
prominent after we instituted a restriction on im-
plicit relations across a paragraph boundary towards
the end of the training period. It restricts full anno-
4Two more points should be made about this number. First,
it may be partially attributed to our differently structured sense
hierarchy. It is a flat structure containing the following 12 val-
ues: ALTERNATIVE, CAUSATION, CONDITIONAL, CONJUNC-
TION, CONTRAST, EXPANSION, PROGRESSION, PURPOSE,
RESTATEMENT, TEMPORAL, EntRel, and NoRel. Aside from in-
cluding EntRel and NoRel (the reason and significance of which
have been discussed in Section 3.2.2), the revision was by and
large not motivated by Chinese-specific features, so we do not
address it in detail in this paper. Second, in making the compar-
ison with the PDTB result, the 12-value structure is collapsed
into 5 values: TEMPORAL, CONTINGENCY, COMPARISON, EX-
PANSION, and EntRel/NoRel, which must be different from the
5 values in Miltsakaki et al (2004), judging from the descrip-
tions.
tation to only three specific situations so that most
loose and/or hard-to-delimit relations across para-
graph boundaries are excluded. This restriction ap-
pears to be quite effective, as shown in Table 2.
num Overall Arg Span
of boundary span-em
rel.?s F(p/r) (%) (%)
last 5 wks 1103 90.0 (90.0/89.9) 80.8
last 3 wks 677 91.0 (91.0/91.0) 82.5
last 2 wks 499 91.8 (91.8/91.8) 84.2
Table 2: Inter-annotator agreement on argument span
during the last 5 weeks of training.
5 Conclusions
We have presented a discourse annotation scheme
for Chinese that adopts the lexically ground ap-
proach of the PDTB while making systematic adap-
tations motivated by characteristics of Chinese text.
These adaptations not only work well in practice, as
evidenced by the results from our annotation exper-
iment, but also embody a more generalized view of
the lexically ground approach to discourse annota-
tion: Discourse relations are predication involving
two arguments; the predicate can be either covert
(i.e. Implicit) or overt, lexicalized as discourse con-
nectives (i.e. Explicit) or their more polymorphous
counterparts (i.e. AltLex). Consistent with this
generalized view is a more semantically motivated
sense annotation scheme: Senses of discourse rela-
tions (as opposed to just connectives) are annotated;
and the two arguments of the discourse relation are
semantically defined, allowing the sense structure
to be more general and less connective-dependent.
These framework-level generalizations can be ap-
plied to discourse annotation of other languages.
Acknowledgments
This work is supported by the IIS Division of the Na-
tional Science Foundation via Grant No. 0910532
entitled ?Richer Representations for Machine Trans-
lation?and by the CNS Division via Grant No.
0855184 entitled ?Building a community resource
for temporal inference in Chinese?. All views ex-
pressed in this paper are those of the authors and do
76
not necessarily represent the view of the National
Science Foundation.
References
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2003. Building a Discourse-Tagged Corpus in the
Framework of Rhetorical Structure Theory. In Current
Directions in Discourse and Dialogue. Kluwer Aca-
demic Publishers.
William Mann and Sandra Thompson. 1988. Rhetori-
cal structure theory. Toward a functional theory of text
organization. Text, 8(3):243?281.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. Annotating discourse con-
nectives and their arguments. In Proceedings of the
HLT/NAACL Workshop on Frontiers in Corpus Anno-
tation, pages 9?16, Boston, MA, May.
Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh, Alan
Lee, Aravind Joshi, Livio Robaldo, and Bonnie Web-
ber, 2007. The Penn Discourse Treebank 2.0 Annota-
tion Manual. The PDTB Research Group, December.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse Treebank 2.0.
In Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC 2008).
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Nianwen Xue. 2005. Annotating the Discourse Con-
nectives in the Chinese Treebank. In Proceedings of
the ACL Workshop on Frontiers in Corpus Annotation,
Ann Arbor, Michigan.
77
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 786?794,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Chinese Comma Disambiguation for Discourse Analysis
Yaqin Yang
Brandeis University
415 South Street
Waltham, MA 02453, USA
yaqin@brandeis.edu
Nianwen Xue
Brandeis University
415 South Street
Waltham, MA 02453, USA
xuen@brandeis.edu
Abstract
The Chinese comma signals the boundary of
discourse units and also anchors discourse
relations between adjacent text spans. In
this work, we propose a discourse structure-
oriented classification of the comma that can
be automatically extracted from the Chinese
Treebank based on syntactic patterns. We
then experimented with two supervised learn-
ing methods that automatically disambiguate
the Chinese comma based on this classifica-
tion. The first method integrates comma clas-
sification into parsing, and the second method
adopts a ?post-processing? approach that ex-
tracts features from automatic parses to train
a classifier. The experimental results show
that the second approach compares favorably
against the first approach.
1 Introduction
The Chinese comma, which looks graphically very
similar to its English counterpart, is functionally
quite different. It has attracted a significant amount
of research that studied the problem from the view-
point of natural language processing. For exam-
ple, Jin et al( 2004) and Li et al( 2005) view
the disambiguation of the Chinese comma as a way
of breaking up long Chinese sentences into shorter
ones to facilitate parsing. The idea is to split a
long sentence into multiple comma-separated seg-
ments, parse them individually, and reconstruct the
syntactic parse for the original sentence. Although
both studies show a positive impact of this approach,
comma disambiguation is viewed merely as a con-
venient tool to help achieve a more important goal.
Xue and Yang ( 2011) point out that the very rea-
son for the existence of these long Chinese sentences
is because the Chinese comma is ambiguous and in
some context, it identifies the boundary of a sentence
just as a period, a question mark, or an exclamation
mark does. The disambiguation of comma is viewed
as a necessary step to detect sentence boundaries in
Chinese and it can benefit a whole range of down-
stream NLP applications such as syntactic parsing
and Machine Translation. In Machine Translation,
for example, it is very typical for ?one? Chinese
sentence to be translated into multiple English sen-
tences, with each comma-separated segment corre-
sponding to one English sentence. In the present
work, we expand this view and propose to look at
the Chinese comma in the context of discourse anal-
ysis. The Chinese comma is viewed as a delimiter
of elementary discourse units (EDUs), in the sense
of the Rhetorical Structure Theory (Carlson et al,
2002; Mann et al, 1988). It is also considered to
be the anchor of discourse relations, in the sense of
the Penn Discourse Treebank (PDT) (Prasad et al,
2008). Disambiguating the comma is thus necessary
for the purpose of discourse segmentation, the iden-
tification of EDUs, a first step in building up the dis-
course structure of a Chinese text.
Developing a supervised or semi-supervised
model of discourse segmentation would require
ground truth annotated based on a well-established
representation scheme, but as of right now no such
annotation exists for Chinese to the best of our
knowledge. However, syntactically annotated tree-
banks often contain important clues that can be used
to infer discourse-level information. We present
786
a method of automatically deriving a preliminary
form of discourse structure anchored by the Chinese
comma from the Penn Chinese Treebank (CTB)
(Xue et al, 2005), and using this information to
train and test supervised models. This discourse
information is formalized as a classification of the
Chinese comma, with each class representing the
boundary of an elementary discourse unit as well
as the anchor of a coarse-grained discourse rela-
tion between the two discourse units that it delimits.
We then develop two comma classification methods.
In the first method, we replace the part-of-speech
(POS) tag of each comma in the CTB with a de-
rived discourse category and retrain a state-of-the-
art Chinese parser on the relabeled data. We then
evaluate how accurately the commas are classified
in the parsing process. In the second method, we
parse these sentences and extract lexical and syn-
tactic information as features to predict these new
discourse categories. The second approach gives us
more control over what features to extract and our
results show that it compares favorably against the
first approach.
The rest of the paper is organized as follows. In
Section 2, we present our approach to automati-
cally extract discourse information from a syntac-
tically annotated treebank and present our classifi-
cation scheme. In Section 3, we describe our su-
pervised learning methods and the features we ex-
tracted. Section 4 presents our experiment setup and
experimental results. Related work is reviewed in
Section 5. We conclude in Section 6.
2 Chinese comma classification
There are many ways to conceptualize the discourse
structure of a text (Mann et al, 1988; Prasad et
al., 2008), but there is more of a consensus among
researchers about the fundamental building blocks
of the discourse structure. For the Rhetorical Dis-
course Theory, the building blocks are Elementary
Discourse Units (EDUs). For the PDT, the build-
ing blocks are abstract objects such as propositions,
facts. Although they are phrased in different ways,
syntactically these discourse units are generally re-
alized as clauses or built on top of clauses. So the
first step in building the discourse structure of a text
is to identify these discourse units.
In Chinese, these elementary discourse units are
generally delimited by the comma, but not all com-
mas mark the boundaries of a discourse unit. In (1),
for example, Comma [1] marks the boundary of a
discourse unit while Comma [2] does not. This is
reflected in its English translation: while the first
comma corresponds to an English comma, the sec-
ond comma is not translated at all, as it marks the
boundary between a subject and its predicate, where
no comma is needed in English. Disambiguating
these two types of commas is thus an important first
step in identifying elementary discourse units and
building up the discourse structure of a text.
(1) ??
Wang Xiang
?
although
?
age
?
over
??
50
?[1]
,
?
but
?
his
??
abundant
?
DE
??
energy
?
and
??
quick
?
DE
??
thinking
?[2]
,
?
give
?
people
?
one
?
CL
???
challenger
?
DE
??
impression
?
.
?Although Wang Xiang is over 50 years old, his
abundant energy and quick thinking leave peo-
ple the impression of a challenger.?
Although to the best of our knowledge, no such
discourse segmented data for Chinese exists in the
public domain, this information can be extracted
from the syntactic annotation of the CTB. In the
syntactic annotation of the sentence, illustrated in
(a), it is clear that while the first comma in the sen-
tence marks the boundary of a clause, the second
one marks the demarcation between the subject NP
and the predicate VP and thus is not an indicator of
a discourse boundary.
(a)
IP
IP-CND
, 1
ADVP NP , 2 VP
In addition to a binary distinction of whether a
comma marks the boundary of a discourse unit,
the CTB annotation also allows the extraction of a
more elaborate classification of commas based on
coordination and subordination relations of comma-
separated clauses. This classification of the Chinese
787
comma can be viewed as a first approximation of the
discourse relations anchored by the comma that can
be refined later via a manual annotation process.
Based on the syntactic annotation in the CTB, we
classify the Chinese comma into seven hierarchi-
cally organized categories, as illustrated in Figure
1. The first distinction is made between commas
that indicate a discourse boundary (RELATION)
and those that do not (OTHER). Commas that in-
dicate discourse boundaries are further divided into
commas that separate coordinated discourse units
(COORD) vs commas that separate discourse units
in a subordination relation (SUBORD). Based on
the levels of embedding and the syntactic category
of the coordinated structures, we define three dif-
ferent types of coordination (SB, IP COORD and
VP COORD). We also define three types of subordi-
nation relations (ADJ, COMP, Sent SBJ), based on
the syntactic structure. As we will show below, each
of the six relations has a clear syntactic pattern that
can be exploited for their automatic detection.
ALL
OTHER
RELATION
SB COORD_IP COORD_VP ADJ COMP Sent_SBJ
COORD SUBORD
Figure 1: Comma classification
Sentence Boundary (SB): Following (Xue and
Yang, 2011), we consider the loosely coordinated
IPs that are the immediate children of the root IP to
be independent sentences, and the commas separat-
ing them to be delimiters of sentence boundary. This
is illustrated in (2), where a Chinese sentence can be
split into two independent shorter sentences at the
comma. We view this comma to be a marker of the
sentence boundary and it serves the same function as
the unambiguous sentence boundary delimitors (pe-
riods, question marks, exclamation marks) in Chi-
nese. The syntactic pattern that is used to infer this
relation is illustrated in (b).
(2) ???
Guangdong province
??
establish
?
ASP
??
natural
??
science
??
foundation
?[3]
,
??
every year
??
investment
?
at
??
one hundred millioin
?
yuan
??
above
?
.
?Natural Science Foundation is established in
Guangdong Province. More than one hundred
million yuan is invested every year.?
(b) IP-Root
IP
Clause
, IP
Clause
IP Coordination (IP COORD): Coordinated IPs
that are not the immediate children of the root IP are
also considered to be discourse units and the com-
mas linking them are labeled IP COORD. Different
from the sentence boundary cases, these coordinated
IPs are often embedded in a larger structure. An ex-
ample is given in (3) and its typical syntactic pattern
is illustrated in (c).
(3) ?
According to
???
Lu Renfa
??
presentation
?[4]
,
??
the whole country
??
revenue
??
goal
?
already
??
exceeding quota
??
complete
?[5]
,
??
overall
??
situation
??
fairly
??
good .
?According to Lu Renfa, the national revenue
goal is met and exceeded, and the overall situa-
tion is fairly good.?
(c) IP
PP
Modifier
, IP
IP
Conjunct
, IP
Conjunct
VP Coordination (VP COORD): Coordinated
VPs, when separated by the comma, are not seman-
tically different from coordinated IPs. The only dif-
ference is that in the latter case, the coordinated VPs
788
share a subject, while coordinated IPs tend to have
different subjects. Maintaining this distinction allow
us to model subject (dis)continuity, which helps re-
cover a subject when it is dropped, a prevalent phe-
nomenon in Chinese. As shown in (4), the VPs in the
text spans separated by Comma [6] have the same
subject, thus the subject in the second VP is dropped.
The syntactic pattern that allows us to extract this
structure is given in (d).
(4) ??
China
??
Bank
?
is
??
four major
??
state-owned
??
commercial
??
bank
??
one of these
?[6]
,
?
also
?
is
??
China
?
DE
??
major
??
foreign exchange
??
bank
?
.
?Bank of China is one of the four major state-
owned commercial banks, and it is also China?s
major foreign exchange bank.?
(d) IP
NP
Subject
VP
VP
Conjunct
, VP
Conjunct
Adjunction (ADJ): Adjunction is one of three
types of subordination relations we define. It holds
between a subordinate clause and its main clause.
The subordinate clause is normally introduced by a
subordinating conjunction and it typically provides
the cause, purpose, manner, or condition for the
main clause. In the PDT terms, these subordinate
conjunctions are discourse connectives that anchor
a discourse relation between the subordinate clause
and the main clause. In Chinese, with few excep-
tions, the subordinate clause comes before the main
clause. (5) is an example of this relation.
(5) ?
if
??
project
??
happen
??
insurance
??
liability
??
scope
?
inside
?
DE
??
natural
??
disaster
?[7]
,
??
China Insurance
??
property
??
insurance
??
company
?
will
?
according to
??
provision
??
excecute
??
compensation
?
.
?If natural disasters within the scope of the in-
surance liability happen in the project, PICC
Property Insurance Company will provide
compensations according to the provisions.?
(e) IP
CP/IP-CND
Subordinate Clause
,
Main Clause
(e) shows how (5) is represented in the syntac-
tic structure in the CTB. Extracting this relation re-
quires more than just the syntactic configuration be-
tween these two clauses. We also take advantage
of the functional (dash) tags provided in the tree-
bank. The functional tags are attached to the sub-
ordinate clause and they include CND (conditional),
PRP (purpose or reason), MNR (manner), or ADV
(other types of subordinate clauses that are adjuncts
to the main clause).
Complementation (COMP): When a comma
separates a verb governor and its complement
clause, this verb and its subject generally describe
the attribution of the complement clause. Attribu-
tion is an important notion in discourse analysis in
both the RST framework and in the PDT. An exam-
ple of this is given in (6), and the syntactic pattern
used to extract this relation is illustrated in (f).
(6) ?
The
??
company
??
present
?[8]
,
?
at
??
future
?
DE
??
five year
?
within
??
they
?
will
??
additionally
??
invest
???
ninety million
??
U.S. dollars
?[9]
,
??
estimate
???
annual output
?
will
?
reach
??
three hundred million
??
U.S. dollars
?
.
?According to the the company?s presentation,
they will invest an additional ninety million
789
U.S. dollars in the next five years, and the esti-
mated annual output will reach $ 300 million.?
(f) IP
....
VP
VV , IP
......
Sentential Subject (SBJ): This category is for
commas that separate a sentential subject from its
predicate VP. An example is given in (7) and the
syntactic pattern used to extract this relation is il-
lustrated in (g).
(7) ??
export
??
rapid
??
grow
?[10]
,
??
become
??
promote
??
economy
??
growth
?
DE
??
important
??
force
?
.
?The rapid growth of export becomes an impor-
tant force in promoting economic growth.?
(g) IP
IP-SBJ
Sentential Subject
, VP
......
Others (OTHER): The remaining cases of
comma receive the OTHER label, indicating they do
not mark the boundary of a discourse segment.
Our proposed comma classification scheme
serves the dual purpose of identifying elementary
discourse units and at the same time detecting
coarse-grained discourse relations anchored by the
comma. The discourse relations identified in this
manner by no means constitute the full discourse
analysis of a text, they are, however, a good first
approximation. The advantage of our approach is
that we do not require manual discourse annotations,
and all the information we need is automatically ex-
tracted from the syntactic annotation of the CTB
and attached to instances of the comma in the cor-
pus. This makes it possible for us to train supervised
models to automatically classify the commas in any
Chinese text.
3 Two comma classification methods
Given the gold standard parses, based on the syntac-
tic patterns described in Section 2, we can map the
POS tag of each comma instance in the CTB to one
of the seven classes described in Section 2. Using
this relabeled data as training data, we experimented
with two automatic comma disambiguation meth-
ods. In the first method, we simply retrained the
Berkeley parser (Petrov and Klein, 2007) on the re-
labeled data and computed how accurately the com-
mas are labeled in a held-out test set. In the second
method, we trained a Maximum Entropy classifier
with the Mallet (McCallum et al, 2002) machine
learning package to classify the commas. The fea-
tures are extracted from the CTB data automatically
parsed with the Berkeley parser. We implemented
features described in (Xue and Yang, 2011), and
also experimented with a set of new features as fol-
lows. In general, these new features are extracted
from the two text spans surrounding the comma.
Given a comma, we define the preceding text span as
i span and the following text span as j span. We also
collected a number of subject-predicate pairs from a
large corpus that doesn?t overlap with the CTB. We
refer to this corpus as the auxiliary corpus.
Subject and Predicate features: We explored
various combinations of the subject (sbj), predicate
(pred) and object (obj) of the two spans. The sub-
ject of i span is represented as sbji, etc.
1. The existence of sbji, sbjj , both, or neither.
2. The lemma of predi, the lemma of predj , the
conjunction of sbji and predj , the conjunction
of predi and sbjj
3. whether the conjunction of sbji and predj oc-
curs more than 2 times in the auxiliary corpus
when j does not have a subject.
4. whether the conjunction of obji and predj oc-
curs more than 2 times in the auxiliary corpus
when j does not have a subject
5. Whether the conjunction of predi and sbjj oc-
curs more than 2 times in the auxiliary corpus
when i does not have a subject.
Mutual Information features: Mutual informa-
tion is intended to capture the association strength
between the subject of a previous span and the predi-
cate of the current span. We use Mutual Information
790
(Church and Hanks, 1989) as shown in Equation
(1) and the frequency count computed based on the
auxiliary corpus to measure such constraints.
MI = log2
# co-occur of S and P * corpus size
# S occur * # P occur
(1)
1. The conjunction of sbji and predj when j does
not have a subject if their MIvalue is greater
than -8.0, an empirically established threshold.
2. Whether obji and predj has an MI value
greater than 5.0 if j does not have a subject.
3. Whether the MI value of sbji and predj is
greater than 0.0, and they occur 2 times in the
auxiliary corpus when j doesn?t have a subject.
4. Whether the MI value of obji and predj is
greater than 0.0 and they occur 2 times in the
auxiliary corpus when j doesn?t have a subject.
5. Whether the MI value of predi and sbjj is
greater than 0.0 and they occur more than 2
times in the auxiliary corpus when i does not
have a subject.
Span features: We used span features to cap-
ture syntactic information, e.g. the comma separated
spans are constituents in Tree (b) but not in Tree (d).
1. Whether i forms a single constituent, whether
j forms a single constituent.
2. The conjunction and hierarchical relation of all
constituent labels in i/j, if i/j does not form
a single constituent. The conjunction of all
constituent labels in both spans, if neither span
form a single constituent.
Lexical features:
1. The first word in i if it is an adverb, the first
word in j if it is an adverb.
2. The first word in i span if it is a coordinating
conjunction, the first word in j if it is a coordi-
nating conjunction.
4 Experiments
4.1 Datasets
We use the CTB 6.0 in our experiments and divide
it into training, development and test sets using the
data split recommended in the CTB 6.0 documenta-
tion, as shown in Table 1. There are 5436 commas
in the test set, including 1327 commas that are sen-
tence boundaries (SB), 539 commas that connect co-
ordinated IPs (IP COORD), 1173 commas that join
coordinated VPs (VP COORD), 379 commas that
delimits a subordinate clause and its main clause
(ADJ), 314 commas that anchor complementation
relations (COMP), and 1625 commas that belong to
the OTHER category.
4.2 Results
As mentioned in Section 3, we experimented with
two comma classification methods. In the first
method, we replace the part-of-speech (POS) tags of
the commas with the seven classes defined in Sec-
tion 2. We then retrain the Berkeley parser (Petrov
and Klein, 2007) using the training set as presented
in Table 1, parse the test set, and evaluate the comma
classification accuracy.
In the second method, we use the relabeled com-
mas as the gold-standard data to train a supervised
classifier to automatically classify the commas. As
shown in the previous section, syntactic structures
are an important source of information for our clas-
sifier. For feature extraction purposes, the entire
CTB6.0 is automatically parsed in a round-robin
fashion. We divided CTB 6.0 into 10 portions,
and parsed each portion with a model trained on
other portions, using the Berkeley parser (Petrov and
Klein, 2007). Measured by the ParsEval metric
(Black et al, 1991), the parsing accuracy on the
CTB test set stands at 83.29% (F-score), with a pre-
cision of 85.18% and a recall of 81.49%.
The results are presented in Table 2, which shows
the overall accuracy of the two methods as well as
the results for each individual category. As should
be clear from Table 2, the results for the two meth-
ods are very comparable, with the second method
performing modestly better than the first method.
4.2.1 Subject continuity
One of the goals for this classification scheme is
to model subject continuity, which answers the ques-
tion of how accurately we can predict whether two
comma-separated text spans have the same subject
or different subjects. When the two spans share
the same subject, the comma belongs to the cate-
gory VP COORD. When they have different sub-
jects, they belong to the categories IP COORD or
791
Data Train Dev Test
CTB-6.0
81-325, 400-454, 500-554 41-80 (1-40,901-931 newswire)
590-596, 600-885, 900 1120-1129 (1018, 1020, 1036, 1044
1001-1017, 1019, 1021-1035 2140-2159 1060-1061,
1037-1043, 1045-1059,1062-1071 2280-2294 1072, 1118-1119, 1132
1073-1078, 1100-1117, 1130-1131 2550-2569 1141-1142, 1148 magazine)
1133-1140, 1143-1147, 1149-1151 2775-2799 (2165-2180, 2295-2310
2000-2139, 2160-2164, 2181-2279 3080-3109 2570-2602, 2800-2819
2311-2549, 2603-2774, 2820-3079 3110-3145 broadcast news)
Table 1: CTB 6.0 data set division.
SB. When this question is meaningless, e.g., when
one of the span does not even have a subject, the
comma belongs to other categories. To evaluate the
performance of our model on this problem, we re-
computed the results by putting IP COORD and SB
in one category, putting VP COORD in another cat-
egory and the rest of the labels in a third category.
The results are presented in Table 3.
4.2.2 The effect of genre
CTB 6.0 consists of data from three different gen-
res, including newswire, magazine and broadcast
news. Data genres may have very different char-
acteristics. To evaluate how our model works on
different genres, we train a model using training
and development sets, and test the model on differ-
ent genres as described in Table 1. The results on
these three genres are presented in Table 4, and they
shows a significant fluctuation across genres. Our
model works the best on newswire, but not as good
on broadcast news and magazine articles.
4.2.3 Comparison with prior work
(Xue and Yang, 2011) presented results on a
binary classification of whether or not a comma
marks a sentence boundary, while the present work
addresses a multi-category classification problem
aimed at identifying discourse segments and prelim-
inary discourse relations anchored by the comma.
However, since we also have a SB category, com-
parison is possible. For comparison purposes, we
retrained our model on their data sets, and computed
the results of SB vs other categories. The results are
shown in Table 5. Our results are very comparable
with (Xue and Yang, 2011) despite that we are per-
forming a multicategory classification.
4.3 Error analysis
Even though our feature-based approach can the-
oretically ?correct? parsing errors, meaning that a
comma can in theory be classified correctly even if a
sentence is incorrectly parsed, when examining the
system output, errors in automatic parses often lead
to errors in comma classification. A common pars-
ing error is the confusion between Structures (h) and
(i). If the subject of the text span after a comma is
dropped as shown in (h), the parser often produces
a VP coordination structure as shown in (i) and vice
versa. This kind of parsing errors would lead to er-
rors in our syntactic features and thus directly affect
the accuracy of our model.
(h) IP
IP
NP VP
, IP
VP
(i) IP
NP VP
VP , VP
5 Related Work
There is a large body of work on discourse analysis
in the field of Natural Language Processing. Most of
the work, however, are on English. An unsupervised
approach was proposed to recognize discourse rela-
tions in (Marcu and Echihabi, 2002), which extracts
discourse relations that hold between arbitrary spans
of text making use of cue phrases. Like the present
work, a lot of research on discourse analysis is car-
ried out at the sentence level. (Soricut and Marcu,
2003; Sporleder and Lapata, 2005; Polanyi et al,
2004). (Soricut and Marcu, 2003) and (Polanyi et
al., 2004) implement models to perform discourse
parsing, while (Sporleder and Lapata, 2005) intro-
duces discourse chunking as an alternative to full-
792
Class Metric Method 1 Method 2
all acc. (%) 71.5 72.9
SB
Prec. (%) 65.6 66.2
Rec. (%) 71.7 73.1
F. (%) 68.5 69.5
IP COORD
Prec. (%) 53.3 56.0
Rec. (%) 50.5 48.6
F. (%) 52.0 52.0
VP Coord
Prec. (%) 65.6 68.3
Rec. (%) 76.3 78.2
F. (%) 70.5 72.9
ADJ
Prec. (%) 66.9 66.8
Rec. (%) 29.3 37.7
F. (%) 40.8 48.2
Comp
Prec. (%) 88.3 91.2
Rec. (%) 93.9 92.4
F. (%) 91.0 91.8
SentSBJ
Prec. (%) 25.0 31.8
Rec. (%) 6 10
F. (%) 9.7 15.6
Other
Prec. (%) 86.9 85.6
Rec. (%) 83.4 84.1
F. (%) 85.1 84.8
Table 2: Overall accuracy of the two methods as well as
the results for each individual category.
scale discourse parsing.
The emergence of linguistic corpora annotated
with discourse structure such as the RST Discourse
Treebank (Carlson et al, 2002) and PDT (Miltsakaki
et al, 2004; Prasad et al, 2008) have changed the
landscape of discourse analysis. More robust, data-
driven models are starting to emerge.
Compared with English, much less work has
been done in Chinese discourse analysis, presum-
ably due to the lack of discourse resources in Chi-
nese. (Huang and Chen, 2011) constructs a small
corpus following the PDT annotation scheme and
Prec. (%) Rec. (%) F. (%)
VP COORD 68.3 78.2 72.9
IP COORD+SB 76.0 78.7 77.3
Other 89.0 80.2 84.4
Table 3: Subject continuity results based on Maximum
Entropy model
Genre NW BN MZ
Accuracy. (%) 79.1 73.6 67.7
Table 4: Results on different genres based on Maximum
Entropy model
Xue and Yang our model
(%) p r f1 p r f1
Overall 89.2 88.7
EOS 64.7 76.4 70.1 63.0 77.9 69.7
NEOS 95.1 91.7 93.4 95.3 90.8 93.0
Table 5: Comparison of (Xue and Yang, 2011) and the
present work based on Maximum Entropy model
trains a statistical classifier to recognize discourse
relations. Their work, however, is only concerned
with discourse relations between adjacent sentences,
thus side-stepping the hard problem of disambiguat-
ing the Chinese comma and analyzing intra-sentence
discourse relations. To the best of our knowledge,
our work is the first in attempting to disambiguating
the Chinese comma as the first step in performing
Chinese discourse analysis.
6 Conclusions and future work
We proposed a approach to disambiguate the Chi-
nese comma as a first step toward discourse analy-
sis. Training and testing data are automatically de-
rived from a syntactically annotated corpus. We pre-
sented two automatic comma disambiguation meth-
ods that perform comparably. In the first method,
comma disambiguation is integrated into the parsing
process while in the second method we train a super-
vised classifier to classify the Chinese comma, us-
ing features extracted from automatic parses. Much
needs to be done in the area, but we believe our work
provides insight into the intricacy and complexity of
discourse analysis in Chinese.
Acknowledgment
This work is supported by the IIS Division of Na-
tional Science Foundation via Grant No. 0910532
entitled ?Richer Representations for Machine
Translation?. All views expressed in this paper are
those of the authors and do not necessarily represent
the view of the National Science Foundation.
793
References
L Carlson, D Marcu, M E Okurowski. 2002. RST Dis-
course Treebank. Linguistic Data Consortium 2002.
Caroline Sporleder, Mirella Lapata. 2005. Discourse
chunking and its application to sentence compression.
In Proceedings of HLT/EMNLP 2005.
Livia Polanyi, Chris Culy, Martin Van Den Berg, Gian
Lorenzo Thione and David Ahn. 2004. Sentential
structure and discourse parsing. In Proceeedings of
the ACL 2004 Workshop on Discourse Annotation
2004.
Hen-Hsen Huang and Hsin-Hsi Chen. 2011. Chinese
Discourse Relation Recognition. In Proceedings of
the 5th International Joint Conference on Natural Lan-
guage Processing 2011,pages 1442-1446.
Daniel Marcu and Abdessamad Echihabi. 2002. An Un-
supervised Approach to Recognizing Discourse Rela-
tions. In Proceedings of the ACL, July 6-12, 2002,
Philadelphia, PA, USA.
Radu Soricut and Daniel Marcu. 2003. Sentence Level
Discourse Parsing using Syntactic and Lexical Infor-
mation. In Proceedings of the ACL 2003.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi andBon-
nie Webber. 2004. The Penn Discourse Treebank. In
Proceedings of LREC 2004.
Nianwen Xue and Yaqin Yang. 2011. Chinese sentence
segmentation as comma classification. In Proceedings
of ACL 2011.
Nianwen Xue, Fei Xia, Fu-Dong Chiou and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2):207-238.
Slav Petrov and Dan Klein. 2007. Improved Inferenc-
ing for Unlexicalized Parsing. In Proceedings of HLT-
NAACL 2007.
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos, B.
Santorini, and T. Strzalkowski. 1991. A procedure
for quantitively comparing the syntactic coverage of
English grammars. In Proceedings of the DARPA
Speech and Natural Language Workshop, pages 306-
311.
Mann, William C. and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a functional the-
ory of text organization. Text 8 (3): 243-281.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse Treebank 2.0..
In Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC 2008).
Meixun Jin, Mi-Young Kim, Dong-Il Kim, and Jong-
Hyeok Lee. 2004. Segmentation of Chinese Long
Sentences Using Commas. In Proceedings of the
SIGHANN Workshop on Chinese Language Process-
ing.
Xing Li, Chengqing Zong, and Rile Hu. 2005. A Hier-
archical Parsing Approach with Punctuation Process-
ing for Long Sentence Sentences. In Proceedings of
the Second International Joint Conference on Natural
Language Processing: Companion Volume including
Posters/Demos and Tutorial Abstracts.
Andrew Kachites McCallum. 2002. MALLET:
A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Church, K., and Hanks, P. 1989. Word Association
Norms, Mutual Information and Lexicography. As-
sociation for Computational Linguistics, Vancouver ,
Canada
794
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 623?627,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Lattice-based Framework for Joint Chinese Word Segmentation, 
POS Tagging and Parsing 
Zhiguo Wang1, Chengqing Zong1 and Nianwen Xue2 
1National Laboratory of Pattern Recognition, 
Institute of Automation, Chinese Academy of Sciences, Beijing, China, 100190 
2Computer Science Department, Brandeis University, Waltham, MA 02452 
{zgwang, cqzong}@nlpr.ia.ac.cn   xuen@brandeis.edu 
 
Abstract 
For the cascaded task of Chinese word seg-
mentation, POS tagging and parsing, the pipe-
line approach suffers from error propagation 
while the joint learning approach suffers from 
inefficient decoding due to the large combined 
search space. In this paper, we present a novel 
lattice-based framework in which a Chinese 
sentence is first segmented into a word lattice, 
and then a lattice-based POS tagger and a lat-
tice-based parser are used to process the lattice 
from two different viewpoints: sequential POS 
tagging and hierarchical tree building. A strat-
egy is designed to exploit the complementary 
strengths of the tagger and parser, and encour-
age them to predict agreed structures. Experi-
mental results on Chinese Treebank show that 
our lattice-based framework significantly im-
proves the accuracy of the three sub-tasks. 
1 Introduction 
Previous work on syntactic parsing generally 
assumes a processing pipeline where an input 
sentence is first tokenized, POS-tagged and then 
parsed (Collins, 1999; Charniak, 2000; Petrov 
and Klein, 2007). This approach works well for 
languages like English where automatic tokeni-
zation and POS tagging can be performed with 
high accuracy without the guidance of the high-
level syntactic structure. Such an approach, how-
ever, is not optimal for languages like Chinese 
where there are no natural delimiters for word 
boundaries, and word segmentation (or tokeniza-
tion) is a non-trivial research problem by itself. 
Errors in word segmentation would propagate to 
later processing stages such as POS tagging and 
syntactic parsing. More importantly, Chinese is a 
language that lacks the morphological clues that 
help determine the POS tag of a word. For ex-
ample, ??  (?investigate/investigation?) can 
either be a verb (?investigate?) or a noun (?inves-
tigation?), and there is no morphological varia-
tion between its verbal form and nominal form. 
This contributes to the relatively low accuracy 
(95% or below) in Chinese POS tagging when 
evaluated as a stand-alone task (Sun and Uszko-
reit, 2012), and the noun/verb ambiguity is a ma-
jor source of error.  
More recently, joint inference approaches 
have been proposed to address the shortcomings 
of the pipeline approach. Qian and Liu (2012) 
proposed a joint inference approach where syn-
tactic parsing can provide feedback to word 
segmentation and POS tagging and showed that 
the joint inference approach leads to improve-
ments in all three sub-tasks. However, a major 
challenge for joint inference approach is that the 
large combined search space makes efficient de-
coding and parameter estimation very hard.  
In this paper, we present a novel lattice-based 
framework for Chinese. An input Chinese sen-
tence is first segmented into a word lattice, 
which is a compact representation of a small set 
of high-quality word segmentations. Then, a lat-
tice-based POS tagger and a lattice-based parser 
are used to process the word lattice from two 
different viewpoints. We next employ the dual 
decomposition method to exploit the comple-
mentary strengths of the tagger and parser, and 
encourage them to predict agreed structures. Ex-
perimental results show that our lattice-based 
framework significantly improves the accuracies 
of the three sub-tasks  
2 The Lattice-based Framework 
Figure 1 gives the organization of the framework. 
There are four types of linguistic structures: a 
Chinese sentence, the word lattice, tagged word 
sequence and parse tree of the Chinese sentence. 
An example for each structure is provided in 
Figure 2. We can see that the terminals and pre-
terminals of a parse tree constitute a tagged word 
sequence. Therefore, we define a comparator 
between a tagged word sequence and a parse tree: 
if they contain the same word sequence and POS 
tags, they are equal, otherwise unequal. 
623
Figure 1 also shows the workflow of the 
framework. First, the Chinese sentence is seg-
mented into a word lattice using the word seg-
mentation system. Then the word lattice is fed 
into the lattice-based POS tagger to produce a 
tagged word sequence   and into the lattice-
based parser to separately produce a parse tree  . 
We then compare   with   to see whether they 
are equal. If they are equal, we output   as the 
final result. Otherwise, the guidance generator 
generates some guidance orders based on the 
difference between   and  , and guides the tag-
ger and the parser to process the lattice again. 
This procedure may iterate many times until the 
tagger and parser predict equal structures. 
 
 
The motivation to design such a framework is 
as follows. First, state-of-the-art word segmenta-
tion systems can now perform with high accura-
cy. We can easily get an F1 score greater than 
96%, and an oracle (upper bound) F1 score 
greater than 99%  for the word lattice (Jiang et 
al., 2008). Therefore, a word lattice provides us a 
good enough search space to allow sufficient 
interaction among word segmentation, POS tag-
ging and parsing systems. Second, both the lat-
tice-based POS tagger and the lattice-based pars-
er can select word segmentation from the word 
lattice and predict POS tags, but they do so from 
two different perspectives. The lattice-based POS 
tagger looks at a path in a word lattice as a se-
quence and performs sequence labeling based on 
linear local context, while the lattice-based pars-
er builds the parse trees in a hierarchical manner. 
They have different strengths with regard to 
word segmentation and POS tagging. We hypo-
thesize that exploring the complementary 
strengths of the tagger and parser would improve 
each of the sub-tasks. 
We build a character-based model (Xue, 2003) 
for the word segmentation system, and treat 
segmentation as a sequence labeling task, where 
each Chinese character is labeled with a tag. We 
use the tag set provided in Wang et al (2011) 
and use the same feature templates. We use the 
Maximum Entropy (ME) model to estimate the 
feature weights. To get a word lattice, we first 
generate N-best word segmentation results, and 
then compact the N-best lists into a word lattice 
by collapsing all the identical words into one 
edge. We also assign a probability to each edge, 
which is calculated by multiplying the tagging 
probabilities of each character in the word. 
    The goal of the lattice-based POS tagger is to 
predict a tagged word sequence   for an input 
word lattice  :   = argmax ?    ( ) ?  ( ) 
where     ( ) represents the set of all possible 
tagged word sequences derived from the word 
lattice  .  ( ) is used to map   onto a global fea-
ture vector, and   is the corresponding weight 
vector. We use the same non-local feature tem-
plates used in Jiang et al (2008) and a similar 
decoding algorithm. We use the perceptron algo-
rithm (Collins, 2002) for parameter estimation. 
Goldberg and Elhadad (2011) proposed a lat-
tice-based parser for Heberw based on the 
PCFG-LA model (Matsuzaki et al, 2005). We 
adopted their approach, but found the un-
weighted word lattice their parser takes as input 
to be ineffective for our Chinese experiments. 
Instead, we use a weighted lattice as input and 
weigh each edge in the lattice with the word 
probability. In our model, each syntactic catego-
ry   is split into multiple subcategories  [ ] by 
labeling a latent annotation  . Then, a parse tree 
????????????? 
Brown?s group will leave Shanghai to Guangzhou tonight. 
(a) Chinese Sentence 
 
 (b) Word Lattice 
?? ?????????
NR NRVVNRPNTP PU
??
NN
Brown .GuangzhougoShanghaileavetonightingroup  
(c) Tagged Word Sequence 
Brown
.
Guangzhou
go
Shanghai
leavetonight
ingroup
?? ?
?? ?
??
?
NR P
NT
NP
PP
VV
NR
NP
VP
PUNP
IP
VP
??
NN
NP NP
?
?
VV
NR
NP
VP
VP
 
(d) Parse Tree 
Figure 2: Linguistic structure examples. 
Chinese Sentence
Word Segmentation
Word Lattice
Lattice-based Parser Lattice-based POS Tagger
Guidance Generator
Parse Tree Tagged Word 
Sequence
The Final Parse Tree
No
Yes
Equal?
 
Figure 1: The lattice-based framework. 
624
  is refined into  [ ], where X is the latent an-
notation vector for all non-terminals in  . The 
probability of  [ ] is calculated as:  ( [ ]) =   ( [ ] ?  [ ] [ ]) ?  ( [ ] ?  )?  ( ) 
where the three terms are products of all syntac-
tic rule probabilities, lexical rule probabilities 
and word probabilities in  [ ] respectively. 
3 Combined Optimization Between The 
Lattice-based POS Tagger and The 
Lattice-based Parser  
We first define some variables to make it easier 
to compare a tagged word sequence   with a 
parse tree  . We define   as the set of all POS 
tags. For  , we define  ( ,  , )=1 if   contains a 
POS tag  ?   spanning from the i-th character 
to the j-th character, otherwise  ( ,  , ) = 0. We 
also define  ( ,  , #) = 1 if   contains the word 
spanning from the i-th character to the j-th cha-
racter, otherwise  ( ,  , #) = 0. Similarly, for  , 
we define  ( ,  , )=1 if   contains a POS tag  ?   spanning from the i-th character to the j-th 
character, otherwise  ( ,  ,  ) = 0. We also define  ( ,  , #)  = 1 if   contains the word spanning 
from the i-th character to the j-th character, oth-
erwise  ( ,  , #) = 0. Therefore,   and   are equal, 
only if  ( ,  ,  ) =  ( ,  ,  )  for all  ? [0,  ] ,  ? [ + 1,  ] and  ?  ? #, otherwise unequal. 
Our framework expects the tagger and the 
parser to predict equal structures and we formu-
late it as a constraint optimization problem:    ,   = argmax ,    ( ) +   ( ) 
Such that for all  ? [0, ] ,  ? [ + 1, ]  and  ?  ? #:  ( ,  ,  ) =  ( ,  , ) 
 
where   ( ) =  ?  ( )  is a scoring function 
from the viewpoint of the lattice-based POS tag-
ger, and   ( ) = log  ( ) is a scoring function 
from the viewpoint of the lattice-based parser.  
The dual decomposition (a special case of La-
grangian relaxation) method introduced in Ko-
modakis et al (2007) is suitable for this problem. 
Using this method, we solve the primal con-
straint optimization problem by optimizing the 
dual problem. First, we introduce a vector of La-
grange multipliers  ( ,  ,  )  for each equality 
constraint. Then, the Lagrangian is formulated as:  ( ,  ,  ) =   ( ) +   ( ) +   ( ,  , )( ( ,  ,  )?  ( ,  , )) , ,  
By grouping the terms that depend on   and  , 
we rewrite the Lagrangian as  ( , , ) =    ( ) +   ( ,  , ) ( ,  , ) , ,   +   ( )?  ( ,  , ) ( ,  , ) , ,   
Then, the dual objective is  ( ) = max ,  ( , , ) = max    ( ) +   ( ,  , ) ( ,  , ) , ,  + max    ( )?  ( ,  , ) ( ,  , ) , ,   
The dual problem is to find min  ( ). 
    We use the subgradient method (Boyd et al, 
2003) to minimize the dual. Following Rush et al 
(2010), we define the subgradient of   ( ) as:  ( ,  , ) =  ( ,  , )?  ( ,  ,  )  for all ( ,  , ) 
Then, adjust  ( ,  ,  ) as follows:   ( ,  , ) =  ( ,  , )?  ( ( ,  ,  )?  ( ,  , )) 
where  >0 is a step size. 
 
Algorithm 1 presents the subgradient method 
to solve the dual problem. The algorithm initia-
lizes the Lagrange multiplier values with 0 (line 
1) and then iterates many times. In each iteration, 
the algorithm finds the best   ( )  and   ( )  by 
running the lattice-based POS tagger (line 3) and 
the lattice-based parser (line 4). If   ( ) and    ( ) 
share the same tagged word sequence (line 5), 
then the algorithm returns the solution (line 6). 
Otherwise, the algorithm adjusts the Lagrange 
multiplier values based on the differences be-
tween    ( ) and   ( ) (line 8). A crucial point is 
that the argmax problems in line 3 and line 4 can 
be solved efficiently using the original decoding 
algorithms, because the Lagrange multiplier can 
be regarded as adjustments for lexical rule prob-
abilities and word probabilities.  
4 Experiments 
We conduct experiments on the Chinese Tree-
bank Version 5.0 and use the standard data split 
Algorithm 1: Combined Optimization 
1: Set  ( )( ,  , )=0, for all  ( ,  , ) 
2: For k=1 to K 
3:     ( ) ? argmax    ( ) + ?   (   )( ,  , ) ( ,  , )  , ,    
4:     ( ) ? argmax    ( )? ?   (   )( ,  ,  ) ( ,  ,  )  , ,   
5:   If  ( )( ,  ,  ) =  ( )( ,  ,  ) for all ( ,  ,  )  
6:      Return (  ( ),   ( )) 
7:   Else  
8:       ( )( ,  ,  ) =  (   )( ,  ,  ) ?  ( ( )( ,  ,  )?  ( )( ,  , ))  
 
625
(Petrov and Klein, 2007). The traditional evalua-
tion metrics for POS tagging and parsing are not 
suitable for the joint task. Following with Qian 
and Liu (2012), we redefine precision and recall 
by computing the span of a constituent based on 
character offsets rather than word offsets.  
4.1 Performance of the Basic Sub-systems 
We train the word segmentation system with 100 
iterations of the Maximum Entropy model using 
the OpenNLP toolkit. Table 1 shows the perfor-
mance. It shows that our word segmentation sys-
tem is comparable with the state-of-the-art sys-
tems and the upper bound F1 score of the word 
lattice exceeds 99.6%. This indicates that our 
word segmentation system can provide a good 
search space for the lattice-based POS tagger and 
the lattice-based parser. 
 
To train the lattice-based POS tagger, we gen-
erate the word lattice for each sentence in the 
training set using cross validation approach. We 
divide the entire training set into 18 folds on av-
erage (each fold contains 1,000 sentences). For 
each fold, we segment each sentence in the fold 
into a word lattice by compacting 20-best seg-
mentation list produced with a model trained on 
the other 17 folds. Then, we train the lattice-
based POS tagger with 20 iterations of the aver-
age perceptron algorithm. Table 2 presents the 
joint word segmentation and POS tagging per-
formance and shows that our lattice-based POS 
tagger obtains results that are comparable with 
state-of-the-art systems. 
 
We implement the lattice-based parser by 
modifying the Berkeley Parser, and train it with 
5 iterations of the split-merge-smooth strategy 
(Petrov et al, 2006). Table 3 shows the perfor-
mance, where the ?Pipeline Parser? represents 
the system taking one-best segmentation result 
from our word segmentation system as input and 
?Lattice-based Parser? represents the system tak-
ing the compacted word lattice as input. We find 
the lattice-based parser gets better performance 
than the pipeline system among all three sub-
tasks. 
 
4.2 Performance of the Framework 
For the lattice-based framework, we set the max-
imum iteration in Algorithm 1 as K = 20. The 
step size   is tuned on the development set and 
empirically set to be 0.8. Table 4 shows the pars-
ing performance on the test set. It shows that the 
lattice-based framework achieves improvement 
over the lattice-based parser alone among all 
three sub-tasks: 0.16 points for word segmenta-
tion, 1.19 points for POS tagging and 1.65 points 
for parsing. It also outperforms the lattice-based 
POS tagger by 0.65 points on POS tagging accu-
racy. Our lattice-based framework also improves 
over the best joint inference parsing system 
(Qian and Liu, 2012) by 0.57 points. 
 
5 Conclusion  
In this paper, we present a novel lattice-based 
framework for the cascaded task of Chinese 
word segmentation, POS tagging and parsing. 
We first segment a Chinese sentence into a word 
lattice, then process the lattice using a lattice-
based POS tagger and a lattice-based parser. We 
also design a strategy to exploit the complemen-
tary strengths of the tagger and the parser and 
encourage them to predict agreed structures. Ex-
perimental results show that the lattice-based 
framework significantly improves the accuracies 
of the three tasks. The parsing accuracy of the 
framework also outperforms the best joint pars-
ing system reported in the literature. 
  P R F 
(Qian and Liu, 
2012) 
 
Seg. 97.56 98.36 97.96 
POS 93.43 94.2 93.81 
Parse 83.03 82.66 82.85 
Lattice-based  
Framework 
Seg. 97.82 97.9 97.86 
POS 94.36 94.44 94.40 
Parse 83.34 83.5 83.42 
 Table 4: Lattice-based framework evaluation. 
  P R F 
Pipeline Parser 
 
Seg. 96.97 98.06 97.52 
POS 92.01 93.04 92.52 
Parse 80.86 81.47 81.17 
 
Lattice-based 
 Parser 
Seg. 97.73 97.66 97.70 
POS 93.24 93.18 93.21 
Parse 81.83 81.71 81.77 
 Table 3: Parsing evaluation. 
 P R F (Kruengkrai et al, 2009) 93.28 94.07 93.67 
(Zhang and Clark, 2010) - - 93.67 
(Qian and Liu, 2012) 93.1 93.96 93.53 
(Sun, 2011) - - 94.02 
Lattice-based POS tagger 93.64 93.87 93.75 
Table 2: POS tagging evaluation. 
  P R F 
(Kruengkrai et al, 2009) 97.46 98.29 97.87 
(Zhang and Clark, 2010) - - 97.78 
(Qian and Liu, 2012) 97.45 98.24 97.85 
(Sun, 2011) - - 98.17 
Our Word Seg. System 96.97 98.06 97.52 
Word Lattice Upper Bound 99.55 99.75 99.65 
Table 1: Word segmentation evaluation. 
626
Acknowledgments 
The research work has been funded by the Hi-
Tech Research and Development Program ("863" 
Program) of China under Grant No. 
2011AA01A207, 2012AA011101, and 
2012AA011102 and also supported by the Key 
Project of Knowledge Innovation Program of 
Chinese Academy of Sciences under Grant 
No.KGZD-EW-501. This work is also supported 
in part by the DAPRA via contract HR0011-11-
C-0145 entitled "Linguistic Resources for Multi-
lingual Processing". 
References  
S. Boyd, L. Xiao and A. Mutapcic. 2003. Subgradient 
methods. Lecture notes of EE392o, Stanford Uni-
versity. 
E. Charniak. 2000. A maximum?entropy?inspired 
parser. In NAACL ?00, page 132?139. 
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis, 
University of Pennsylvania. 
Michael Collins. 2002. Discriminative training me-
thods for hidden markov models: Theory and expe-
riments with perceptron algorithms. In Proc. of 
EMNLP2002, pages 1-8. 
Yoav Goldberg and Michael Elhadad. 2011. Joint 
Hebrew segmentation and parsing using a PCFG-
LA lattice parser. In Proc. of ACL2011. 
Wenbin Jiang, Haitao Mi and Qun Liu. 2008. Word 
lattice reranking for Chinese word segmentation 
and part-of-speech tagging. In Proc. of Coling 2008, 
pages 385-392. 
Komodakis, N., Paragios, N., and Tziritas, G. 2007. 
MRF optimization via dual decomposition: Mes-
sage-passing revisited. In ICCV 2007. 
C. Kruengkrai, K. Uchimoto, J. Kazama, Y. Wang, K. 
Torisawa and H. Isahara. 2009. An error-driven 
word-character hybrid model for joint Chinese 
word segmentation and POS tagging. In Proc. of 
ACL2009, pages 513-521. 
Takuya Matsuzaki, Yusuke Miyao and Jun'ichi Tsujii. 
2005. Probabilistic CFG with latent annotations. In 
Proc. of ACL2005, pages 75-82. 
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proc. of ACL2006, 
pages 433-440. 
Slav Petrov and Dan Klein. 2007. Improved inference 
for unlexicalized parsing. In Proc. of NAACL2007, 
pages 404-411. 
Xian Qian and Yang Liu. 2012. Joint Chinese Word 
segmentation, POS Tagging Parsing. In Proc. of 
EMNLP 2012, pages 501-511. 
Alexander M. Rush, David Sontag, Michael Collins 
and Tommi Jaakkola. 2010. On dual decomposi-
tion and linear programming relaxations for natural 
language processing. In Proc. of EMNLP2010, 
pages 1-11. 
Weiwei Sun. 2011. A stacked sub-word model for 
joint Chinese word segmentation and part-of-
speech tagging. In Proc. of ACL2011, pages 1385-
1394. 
Weiwei Sun and Hans Uszkoreit. Capturing paradig-
matic and syntagmatic lexical relations: Towards 
accurate Chinese part-of-speech tagging. In Proc. 
of ACL2012. 
Yiou Wang, Jun'ichi Kazama, Yoshimasa Tsuruoka, 
Wenliang Chen, Yujie Zhang and Kentaro Torisa-
wa. 2011. Improving Chinese word segmentation 
and POS tagging with semi-supervised methods us-
ing large auto-analyzed data. In Proc. of 
IJCNLP2011, pages 309-317. 
Nianwen Xue. 2003. Chinese word segmentation as 
character tagging. Computational Linguistics and 
Chinese Language Processing, 8 (1). pages 29-48. 
Yue Zhang and Stephen Clark. 2010. A fast decoder 
for joint word segmentation and POS-tagging using 
a single discriminative model. In Proc. of 
EMNLP2010, pages 843-852. 
627
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 733?742,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Joint POS Tagging and Transition-based Constituent Parsing in Chinese
with Non-local Features
Zhiguo Wang
Brandeis University
Waltham, MA, USA
zgwang@brandeis.edu
Nianwen Xue
Brandeis University
Waltham, MA, USA
xuen@brandeis.edu
Abstract
We propose three improvements to ad-
dress the drawbacks of state-of-the-art
transition-based constituent parsers. First,
to resolve the error propagation problem
of the traditional pipeline approach, we
incorporate POS tagging into the syntac-
tic parsing process. Second, to allevi-
ate the negative influence of size differ-
ences among competing action sequences,
we align parser states during beam-search
decoding. Third, to enhance the pow-
er of parsing models, we enlarge the fea-
ture set with non-local features and semi-
supervised word cluster features. Exper-
imental results show that these modifica-
tions improve parsing performance signif-
icantly. Evaluated on the Chinese Tree-
Bank (CTB), our final performance reach-
es 86.3% (F1) when trained on CTB 5.1,
and 87.1% when trained on CTB 6.0, and
these results outperform all state-of-the-art
parsers.
1 Introduction
Constituent parsing is one of the most fundamen-
tal tasks in Natural Language Processing (NLP). It
seeks to uncover the underlying recursive phrase
structure of sentences. Most of the state-of-the-
art parsers are based on the PCFG paradigm and
chart-based decoding algorithms (Collins, 1999;
Charniak, 2000; Petrov et al, 2006). Chart-based
parsers perform exhaustive search with dynam-
ic programming, which contributes to their high
accuracy, but they also suffer from higher run-
time complexity and can only exploit simple local
structural information.
Transition-based constituent parsing (Sagae and
Lavie, 2005; Wang et al, 2006; Zhang and Clark,
2009) is an attractive alternative. It utilizes a se-
ries of deterministic shift-reduce decisions to con-
struct syntactic trees. Therefore, it runs in linear
time and can take advantage of arbitrarily complex
structural features from already constructed sub-
trees. The downside is that they only search a tiny
fraction of the whole space and are therefore com-
monly considered to be less accurate than chart-
based parsers. Recent studies (Zhu et al, 2013;
Zhang et al, 2013) show, however, that this ap-
proach can also achieve the state-of-the-art perfor-
mance with improved training procedures and the
use of additional source of information as features.
However, there is still room for improvemen-
t for these state-of-the-art transition-based con-
stituent parsers. First, POS tagging is typically
performed separately as a preliminary step, and
POS tagging errors will propagate to the parsing
process. This problem is especially severe for lan-
guages where the POS tagging accuracy is rela-
tively low, and this is the case for Chinese where
there are fewer contextual clues that can be used
to inform the tagging process and some of the
tagging decisions are actually influenced by the
syntactic structure of the sentence. This creates
a chicken and egg problem that needs to be ad-
dressed when designing a parsing model. Second,
due to the existence of unary rules in constituen-
t trees, competing candidate parses often have d-
ifferent number of actions, and this increases the
disambiguation difficulty for the parsing model.
Third, transition-based parsers have the freedom
to define arbitrarily complex structural features,
but this freedom has not fully been taken advan-
tage of and most of the present approaches only
use simple structural features.
In this paper, we address these drawbacks to
improve the transition-based constituent parsing
for Chinese. First, we integrate POS tagging in-
to the parsing process and jointly optimize these
two processes simultaneously. Because non-local
syntactic information is now available to POS tag
733
determination, the accuracy of POS tagging im-
proves, and this will in turn improve parsing ac-
curacy. Second, we propose a novel state align-
ment strategy to align candidate parses with dif-
ferent action sizes during beam-search decoding.
With this strategy, parser states and their unary
extensions are put into the same beam, therefore
the parsing model could decide whether or not
to use unary actions within local decision beam-
s. Third, we take into account two groups of
complex structural features that have not been
previously used in transition-based parsing: non-
local features (Charniak and Johnson, 2005) and
semi-supervised word cluster features (Koo et al,
2008). With the help of the non-local features,
our transition-based parsing system outperform-
s all previous single systems in Chinese. After
integrating semi-supervised word cluster features,
the parsing accuracy is further improved to 86.3%
when trained on CTB 5.1 and 87.1% when trained
on CTB 6.0, and this is the best reported perfor-
mance for Chinese.
The remainder of this paper is organized as fol-
lows: Section 2 introduces the standard transition-
based constituent parsing approach. Section 3
describes our three improvements to standard
transition-based constituent parsing. We discuss
and analyze the experimental results in Section 4.
Section 5 discusses related work. Finally, we con-
clude this paper in Section 6.
2 Transition-based Constituent Parsing
This section describes the transition-based con-
stituent parsing model, which is the basis of Sec-
tion 3 and the baseline model in Section 4.
2.1 Transition-based Constituent Parsing
Model
A transition-based constituent parsing model is a
quadruple C = (S, T, s
0
, S
t
), where S is a set of
parser states (sometimes called configurations), T
is a finite set of actions, s
0
is an initialization func-
tion to map each input sentence into a unique ini-
tial state, and S
t
? S is a set of terminal states.
Each action t ? T is a transition function to tran-
sit a state into a new state. A parser state s ? S is
defined as a tuple s = (?, ?), where ? is a stack
which is maintained to hold partial subtrees that
are already constructed, and ? is a queue which is
used for storing word-POS pairs that remain un-
processed. In particular, the initial state has an
B
0,3
c
2,3
w
2
A
0,2
b
1,2
w
1
a
0,1
w
0
sh,sh,rr-A,sh,rl-B
(a)
B
0,3
F
2,3
c
2,3
w
2
E
0,2
A
0,2
D
1,2
b
1,2
w
1
C
0,1
a
0,1
w
0
sh,ru-C,sh,ru-D,rr-A,
ru-E,sh,ru-F,rl-B
(b)
Figure 1: Two constituent trees for an example
sentence w
0
w
1
w
2
with POS tags abc. The cor-
responding action sequences are given below, the
spans of each nodes are annotated and the head n-
odes are written with Bold font type.
empty stack ? and a queue ? containing the entire
input sentence (word-POS pairs), and the terminal
states have an empty queue ? and a stack ? con-
taining only one complete parse tree. The task of
transition-based constituent parsing is to scan the
input POS-tagged sentence from left to right and
perform a sequence of actions to transform the ini-
tial state into a terminal state.
In order to construct lexicalized constituen-
t parse trees, we define the following actions for
the action set T according to (Sagae and Lavie,
2005; Wang et al, 2006; Zhang and Clark, 2009):
? SHIFT (sh): remove the first word-POS pair
from ?, and push it onto the top of ?;
? REDUCE-UNARY-X (ru-x): pop the top
subtree from ?, construct a new unary node
labeled with X for the subtree, then push the
new subtree back onto ?. The head of the
new subtree is inherited from its child;
? REDUCE-BINARY-{L/R}-X (rl/rr-x): pop
the top two subtrees from ?, combine them
into a new tree with a node labeled with X,
then push the new subtree back onto ?. The
left (L) and right (R) versions of the action
indicate whether the head of the new subtree
is inherited from its left or right child.
With these actions, our parser can process trees
with unary and binary branches easily. For exam-
ple, in Figure 1, for the input sentence w
0
w
1
w
2
and its POS tags abc, our parser can construct t-
wo parse trees using action sequences given below
these trees. However, parse trees in Treebanks of-
ten contain an arbitrary number of branches. To
734
Type Feature Templates
unigrams
p
0
tc, p
0
wc, p
1
tc, p
1
wc, p
2
tc
p
2
wc, p
3
tc, p
3
wc, q
0
wt, q
1
wt
q
2
wt, q
3
wt, p
0l
wc, p
0r
wc
p
0u
wc, p
1l
wc, p
1r
wc, p
1u
wc
bigrams
p
0
wp
1
w, p
0
wp
1
c, p
0
cp
1
w, p
0
cp
1
c
p
0
wq
0
w, p
0
wq
0
t, p
0
cq
0
w, p
0
cq
0
t
q
0
wq
1
w, q
0
wq
1
t, q
0
tq
1
w, q
0
tq
1
t
p
1
wq
0
w, p
1
wq
0
t, p
1
cq
0
w, p
1
cq
0
t
trigrams
p
0
cp
1
cp
2
c, p
0
wp
1
cp
2
c, p
0
cp
1
wq
0
t
p
0
cp
1
cp
2
w, p
0
cp
1
cq
0
t, p
0
wp
1
cq
0
t
p
0
cp
1
wq
0
t, p
0
cp
1
cq
0
w
Table 1: Baseline features, where p
i
represents the
i
th
subtree in the stack ? and q
i
denotes the i
th
item in the queue ?. w refers to the head lexicon,
t refers to the head POS, and c refers to the con-
stituent label. p
il
and p
ir
refer to the left and right
child for a binary subtree p
i
, and p
iu
refers to the
child of a unary subtree p
i
.
process such trees, we employ binarization and
debinarization processes described in Zhang and
Clark (2009) to transform multi-branch trees into
binary-branch trees and restore the generated bi-
nary trees back to their original forms.
2.2 Modeling, Training and Decoding
To determine which action t ? T should the parser
perform at a state s ? S, we use a linear model to
score each possible ?s, t? combination:
score(s, t) = ~w ? ?(s, t) =
?
i
w
i
f
i
(s, t) (1)
where ?(s, t) is the feature function used for map-
ping a state-action pair into a feature vector, and
~w is the weight vector. The score of a parser state
s is the sum of the scores for all state-action pairs
in the transition path from the initial state to the
current state. Table 1 lists the feature templates
used in our baseline parser, which is adopted from
Zhang and Clark (2009). To train the weight vec-
tor ~w, we employ the averaged perceptron algo-
rithm with early update (Collins and Roark, 2004).
We employ the beam search decoding algorith-
m (Zhang and Clark, 2009) to balance the trade-
off between accuracy and efficiency. Algorithm
1 gives details of the process. In the algorithm,
we maintain a beam (sometimes called agenda)
to keep k best states at each step. The first beam
0
Algorithm 1 Beam-search Constituent Parsing
Input: A POS-tagged sentence, beam size k.
Output: A constituent parse tree.
1: beam
0
? {s
0
} . initialization
2: i? 0 . step index
3: loop
4: P ? {} . a priority queue
5: while beam
i
is not empty do
6: s? POP(beam
i
)
7: for all possible t ? T do
8: s
new
? apply t to s
9: score s
new
with E.q (1)
10: insert s
new
into P
11: beam
i+1
? k best states of P
12: s
best
? best state in beam
i+1
13: if s
best
? S
t
then
14: return s
best
15: i? i+ 1
is initialized with the initial state s
0
(line 1). At
step i, each of the k states in beam
i
is extended
by applying all possible actions (line 5-10). For
all newly generated states, only the k best states
are preserved for beam
i+1
(line 11). The decod-
ing process repeats until the highest scored state in
beam
i+1
reaches a terminal state (line 12-14).
3 Joint POS Tagging and Parsing with
Non-local Features
To address the drawbacks of the standard
transition-based constituent parsing model (de-
scribed in Section 1), we propose a model to joint-
ly solve POS tagging and constituent parsing with
non-local features.
3.1 Joint POS Tagging and Parsing
POS tagging is often taken as a preliminary step
for transition-based constituent parsing, therefore
the accuracy of POS tagging would greatly affec-
t parsing performance. In our experiment (de-
scribed in Section 4.2), parsing accuracy would
decrease by 8.5% in F
1
in Chinese parsing when
using automatically generated POS tags instead of
gold-standard ones. To tackle this issue, we inte-
grate POS tagging into the transition-based con-
stituent parsing process and jointly optimize these
two processes simultaneously. Inspired from Ha-
tori et al (2011), we modify the sh action by as-
signing a POS tag for the word when it is shifted:
? SHIFT-X (sh-x): remove the first word from
735
?, assign POS tag X to the word and push it
onto the top of ?.
With such an action, POS tagging becomes a nat-
ural part of transition-based parsing. However,
some feature templates in Table 1 become unavail-
able, because POS tags for the look-ahead words
are not specified yet under the joint framework.
For example, for the template q
0
wt , the POS tag
of the first word q
0
in the queue ? is required, but
it is not specified yet at the present state.
To overcome the lack of look-ahead POS tags,
we borrow the concept of delayed features origi-
nally developed for dependency parsing (Hatori et
al., 2011). Features that require look-ahead POS
tags are defined as delayed features. In these fea-
tures, look-ahead POS tags are taken as variables.
During parsing, delayed features are extracted and
passed from one state to the next state. When a
sh-x action is performed, the look-ahead POS
tag of some delayed features is specified, there-
fore these delayed features can be transformed in-
to normal features (by replacing variable with the
newly specified POS tag). The remaining delayed
features will be transformed similarly when their
look-ahead POS tags are specified during the fol-
lowing parsing steps.
3.2 State Alignment
Assuming an input sentence contains n words, in
order to reach a terminal state, the initial state re-
quires n sh-x actions to consume all words in ?,
and n ? 1 rl/rr-x actions to construct a com-
plete parse tree by consuming all the subtrees in
?. However, ru-x is a very special action. It on-
ly constructs a new unary node for the subtree on
top of ?, but does not consume any items in ? or
?. As a result, the number of ru-x actions varies
among terminal states for the same sentence. For
example, the parse tree in Figure 1a contains no
ru-x action, while the parse tree for the same in-
put sentence in Figure 1b contains four ru-x ac-
tions. This makes the lengths of complete action
sequences very different, and the parsing model
has to disambiguate among terminal states with
varying action sizes. Zhu et al (2013) proposed a
padding method to align terminal states containing
different number of actions. The idea is to append
some IDLE actions to terminal states with shorter
action sequence, and make sure all terminal states
contain the same number of actions (including I-
DLE actions).
Algorithm 2 Beam-search with State Alignment
Input: A word-segmented sentence, beam size k.
Output: A constituent parse tree.
1: beam
0
? {s
0
} . initialization
2: for i? 0 to 2n? 1 do . n is sentence length
3: P
0
? {}, P
1
? {} . two priority queues
4: while beam
i
is not empty do
5: s? POP(beam
i
)
6: for t ? {sh-x,rl-x,rr-x} do
7: s
new
? apply t to s
8: score s
new
with E.q (1)
9: insert s
new
into P
0
10: for all state s in P
0
do
11: for all possible t ? {ru-x} do
12: s
new
? apply t to s
13: score s
new
with E.q (1)
14: insert s
new
into P
1
15: insert all states of P
1
into P
0
16: beam
i+1
? k best states of P
0
17: return the best state in beam
2n?1
We propose a novel method to align states dur-
ing the parsing process instead of just aligning ter-
minal states like Zhu et al (2013). We classify all
the actions into two groups according to whether
they consume items in ? or ?. sh-x, rl-x, and
rr-x belong to consuming actions, and ru-x be-
longs to non-consuming action. Algorithm 2 gives
the details of our method. It is based on the beam
search decoding algorithm described in Algorith-
m 1. Different from Algorithm 1, Algorithm 2 is
guaranteed to perform 2n? 1 parsing steps for an
input sentence containing n words (line 2), and
divides each parsing step into two parsing phas-
es. In the first phase (line 4-9), each of the k s-
tates in beam
i
is extended by consuming action-
s. In the second phase (line 10-14), each of the
newly generated states is further extended by non-
consuming actions. Then, all these states extend-
ed by both consuming and non-consuming action-
s are considered together (line 15), and only the
k highest-scored states are preserved for beam
i+1
(line 16). After these 2n ? 1 parsing steps, the
highest scored state in beam
2n?1
is returned as
the final result (line 17). Figure 2 shows the states
aligning process for the two trees in Figure 1. We
find that our new method aligns states with their
ru-x extensions in the same beam, therefore the
parsing model could make decisions on whether
using ru-x actions or not within local decision
736
s0
a
0,1
b
1,2
A
0,2
c
2,3
B
0,3
T
0
C
0,1
b
1,2
D
1,2
A
0,2
E
0,2
c
2,3
F
2,3
B
0,3
T
1
beam
0
beam
1
beam
2
beam
3
beam
4
beam
5
Figure 2: State alignment for the two trees in Fig-
ure 1, where s
0
is the initial state, T
0
and T
1
are
terminal states corresponding to the two trees in
Figure 1. For clarity, we represent each state as a
rectangle with the label of top subtree in the stack
?. We also denote sh-x with?, ru-x with ? or
?, rl-x with?, and rr-x with?.
beams.
3.3 Feature Extension
One advantage of transition-based constituen-
t parsing is that it is capable of incorporating ar-
bitrarily complex structural features from the al-
ready constructed subtrees in ? and unprocessed
words in ?. However, all the feature templates
given in Table 1 are just some simple structural
features. To further improve the performance of
our transition-based constituent parser, we con-
sider two group of complex structural features:
non-local features (Charniak and Johnson, 2005;
Collins and Koo, 2005) and semi-supervised word
cluster features (Koo et al, 2008).
Table 2 lists all the non-local features we want
to use. These features have been proved very help-
ful for constituent parsing (Charniak and Johnson,
2005; Collins and Koo, 2005). But almost all pre-
vious work considered non-local features only in
parse reranking frameworks. Instead, we attempt
to extract non-local features from newly construct-
ed subtrees during the decoding process as they
become incrementally available and score newly
generated parser states with them. One difficul-
ty is that the subtrees built by our baseline pars-
er are binary trees (only the complete parse tree
is debinarized into its original multi-branch form),
but most of the non-local features need to be ex-
tracted from their original multi-branch forms. To
resolve this conflict, we integrate the debinariza-
tion process into the parsing process, i.e., when a
(Collins and Koo, 2005) (Charniak and Johnson, 2005)
Rules CoPar HeadTree
Bigrams CoLenPar
Grandparent Rules RightBranch
Grandparent Bigrams Heavy
Lexical Bigrams Neighbours
Two-level Rules NGramTree
Two-level Bigrams Heads
Trigrams Wproj
Head-Modifiers Word
Table 2: Non-local features for constituent pars-
ing.
new subtree is constructed during parsing, we de-
binarize it immediately if it is not rooted with an
intermediate node
1
. The other subtrees for sub-
sequent parsing steps will be built based on these
debinarized subtrees. After the modification, our
parser can extract non-local features incrementally
during the parsing process.
Semi-supervised word cluster features have
been successfully applied to many NLP tasks
(Miller et al, 2004; Koo et al, 2008; Zhu et
al., 2013). Here, we adopt such features for our
transition-based constituent parser. Given a large-
scale unlabeled corpus (word segmentation should
be performed), we employ the Brown cluster al-
gorithm (Liang, 2005) to cluster all words into a
binary tree. Within this binary tree, words ap-
pear as leaves, left branches are labeled with 0 and
right branches are labeled with 1. Each word can
be uniquely identified by its path from the root,
and represented as a bit-string. By using various
length of prefixes of the bit-string, we can produce
word clusters of different granularities (Miller et
al., 2004). Inspired from Koo et al (2008), we
employ two types of word clusters: (1) taking 4
bit-string prefixes of word clusters as replacements
of POS tags, and (2) taking 8 bit-string prefixes as
replacements of words. Using these two types of
clusters, we construct semi-supervised word clus-
ter features by mimicking the template structure of
the original baseline features in Table 1.
4 Experiment
4.1 Experimental Setting
We conducted experiments on the Penn Chinese
Treebank (CTB) version 5.1 (Xue et al, 2005):
Articles 001-270 and 400-1151 were used as the
training set, Articles 301-325 were used as the
development set, and Articles 271-300 were used
1
Intermediate nodes are produced by binarization process.
737
as the test set. Standard corpus preparation step-
s were performed before our experiments: emp-
ty nodes and functional tags were removed, and
the unary chains were collapsed to single unary
rules as Harper and Huang (2011). To build word
clusters, we used the unlabeled Chinese Gigaword
(LDC2003T09) and conducted Chinese word seg-
mentation using a CRF-based segmenter.
We used EVALB
2
tool to evaluate parsing per-
formance. The metrics include labeled precision
(LP ), labeled recall (LR), bracketing F
1
and POS
tagging accuracy. We set the beam size k to 16,
which brings a good balance between efficiency
and accuracy. We tuned the optimal number of
iterations of perceptron training algorithm on the
development set.
4.2 Pipeline Approach vs Joint POS Tagging
and Parsing
In this subsection, we conducted some experi-
ments to illustrate the drawbacks of the pipeline
approach and the advantages of our joint approach.
We built three parsing systems: Pipeline-Gold
system is our baseline parser (described in Sec-
tion 2) taking gold-standard POS tags as input;
Pipeline system is our baseline parser taking as
input POS tags automatically assigned by Stan-
ford POS Tagger
3
; and JointParsing system is
our joint POS tagging and transition-based pars-
ing system described in subsection 3.1. We trained
these three systems on the training set and evalu-
ated them on the development set. The second,
third and forth rows in Table 3 show the parsing
performances. We can see that the parsing F
1
de-
creased by about 8.5 percentage points in F
1
score
when using automatically assigned POS tags in-
stead of gold-standard ones, and this shows that
the pipeline approach is greatly affected by the
quality of its preliminary POS tagging step. Af-
ter integrating the POS tagging step into the pars-
ing process, our JointParsing system improved the
POS tagging accuracy to 94.8% and parsing F
1
to 85.8%, which are significantly better than the
Pipeline system. Therefore, the joint parsing ap-
proach is much more effective for transition-based
constituent parsing.
4.3 State Alignment Evaluation
We built two new systems to verify the effective-
ness of our state alignment strategy proposed in
2
http://nlp.cs.nyu.edu/evalb/
3
http://nlp.stanford.edu/downloads/tagger.shtml
System LP LR F
1
POS
Pipeline-Gold 92.2 92.5 92.4 100
Pipeline 83.9 83.8 83.8 93.0
JointParsing 85.1 86.6 85.8 94.8
Padding 85.4 86.4 85.9 94.8
StateAlign 86.9 85.9 86.4 95.2
Nonlocal 88.0 86.5 87.2 95.3
Cluster 89.0 88.3 88.7 96.3
Nonlocal&Cluster 89.4 88.7 89.1 96.2
Table 3: Parsing performance on Chinese devel-
opment set.
Subsection 3.2. The first system Padding extend-
s our JointParsing system by aligning terminal s-
tates with the padding strategy proposed in Zhu et
al. (2013), and the second system StateAlign ex-
tends the JointParsing system with our state align-
ment strategy. The fifth and sixth rows of Table 3
give the performances of these two systems. Com-
pared with the JointParsing system which does not
employ any alignment strategy, the Padding sys-
tem only achieved a slight improvement on pars-
ing F
1
score, but no improvement on POS tag-
ging accuracy. In contrast, our StateAlign system
achieved an improvement of 0.6% on parsing F
1
s-
core and 0.4% on POS tagging accuracy. All these
results show us that our state alignment strategy is
more helpful for beam-search decoding.
4.4 Feature Extension Evaluation
In this subsection, we examined the usefulness
of the new non-local features and the semi-
supervised word cluster features described in Sub-
section 3.3. We built three new parsing system-
s based on the StateAlign system: Nonlocal sys-
tem extends the feature set of StateAlign system
with non-local features, Cluster system extends
the feature set with semi-supervised word cluster
features, and Nonlocal&Cluster system extend the
feature set with both groups of features. Parsing
performances of the three systems are shown in
the last three rows of Table 3. Compared with the
StateAlign system which takes only the baseline
features, the non-local features improved parsing
F
1
by 0.8%, while the semi-supervised word clus-
ter features result in an improvement of 2.3% in
parsing F
1
and an 1.1% improvement on POS tag-
ging accuracy. When integrating both groups of
features, the final parsing F
1
reaches 89.1%. Al-
738
Type System LP LR F
1
POS
Our Systems
Pipeline 80.0 80.3 80.1 94.0
JointParsing 82.4 83.0 82.7 95.1
Padding 82.7 83.6 83.2 95.1
StateAlign 84.2 82.9 83.6 95.5
Nonlocal 85.6 84.2 84.9 95.9
Cluster 85.2 84.5 84.9 95.8
Nonlocal&Cluster 86.6 85.9 86.3 96.0
Single Systems
Petrov and Klein (2007) 81.9 84.8 83.3 -
Zhu et al (2013) 82.1 84.3 83.2 -
Reranking Systems
Charniak and Johnson (2005)
?
80.8 83.8 82.3 -
Wang and Zong (2011) - - 85.7 -
Semi-supervised Systems Zhu et al (2013) 84.4 86.8 85.6 -
Table 4: Parsing performance on Chinese test set.
?
Huang (2009) adapted the parse reranker to CTB5.
l these results show that both the non-local fea-
tures and the semi-supervised features are helpful
for our transition-based constituent parser.
4.5 Final Results on Test Set
In this subsection, we present the performances of
our systems on the CTB test set. The correspond-
ing results are listed in the top rows of Table 4.
We can see that all these systems maintain a simi-
lar relative relationship as they do on the develop-
ment set, which shows the stability of our systems.
To further illustrate the effectiveness of our
systems, we compare them with some state-of-
the-art systems. We group parsing systems into
three categories: single systems, reranking sys-
tems and semi-supervised systems. Our Pipeline,
JointParsing, Padding, StateAlign and Nonlocal
systems belong to the category of single system-
s, because they don?t utilize any extra process-
ing steps or resources. Our Cluster and Nonlo-
cal&Cluster systems belong to semi-supervised
systems, because both of them have employed
semi-supervised word cluster features. The pars-
ing performances of state-of-the-art systems are
shown in the bottom rows of Table 4. We can see
that the final F
1
of our Nonlocal system reached
84.9%, and it outperforms state-of-the-art single
systems by more than 1.6%. As far as we know,
this is the best result on the CTB test set acquired
by single systems. Our Nonlocal&Cluster sys-
tem further improved the parsing F
1
to 86.3%,
and it outperforms all reranking systems and semi-
supervised systems. To our knowledge, this is the
System F
1
Huang and Harper (2009) 85.2
Nonlocal&Cluster 87.1
Table 5: Parsing performance based on CTB 6.
best reported performance in Chinese parsing.
All previous experiments were conducted on
CTB 5. To check whether more labeled data can
further improve our parsing system, we evaluat-
ed our Nonlocal&Cluster system on the Chinese
TreeBank version 6.0 (CTB6), which is a super
set of CTB5 and contains more annotated data.
We used the same development set and test set
as CTB5, and took all the remaining data as the
new training set. Table 5 shows the parsing per-
formances on CTB6. Our Nonlocal&Cluster sys-
tem improved the final F
1
to 87.1%, which is 1.9%
better than the state-of-the-art performance on CT-
B6 (Huang and Harper, 2009). Compared with it-
s performance on CTB5 (in Table 4), our Nonlo-
cal&Cluster system also got 0.8% improvemen-
t. All these results show that our approach can
become more powerful when given more labeled
training data.
4.6 Error Analysis
To better understand the linguistic behavior of
our systems, we employed the berkeley-parser-
analyser tool
4
(Kummerfeld et al, 2013) to cat-
egorize the errors. Table 6 presents the average
4
http://code.google.com/p/berkeley-parser-analyser/
739
System
NP
Int.
Unary
1-Word
Span
Coord
Mod.
Attach
Verb
Args
Diff
Label
Clause
Attach
Noun
Edge
Worst 1.75 0.74 0.44 0.49 0.39 0.37 0.29 0.15 0.14
Pipeline
JointParsing
Padding
StateAlign
Nonlocal
Cluster
Nonlocal&Cluster
Best 1.33 0.42 0.28 0.29 0.19 0.21 0.17 0.07 0.09
Table 6: Parse errors on Chinese test set. The shaded area of each bar indicates average number of that
error type per sentence, and the completely full bar indicates the number in the Worst row.
System VV?NN NN?VV DEC?DEG JJ?NN NR?NN DEG?DEC NN?NR NN?JJ
Worst 0.26 0.18 0.15 0.09 0.08 0.07 0.06 0.05
Pipeline
JointParsing
Padding
StateAlign
Nonlocal
Cluster
Nonlocal&Cluster
Best 0.14 0.10 0.03 0.07 0.05 0.03 0.03 0.02
Table 7: POS tagging error patterns on Chinese test set. For each error pattern, the left hand side tag is
the gold-standard tag, and the right hand side is the wrongly assigned tag.
number of errors for each error type by our pars-
ing systems. We can see that almost all the Worst
numbers are produced by the Pipeline system. The
JointParsing system reduced errors of all types
produced by the Pipeline system except for the
coordination error type (Coord). The StateAlign
system corrected a lot of the NP-internal errors
(NP Int.). The Nonlocal system and the Cluster
system produced similar numbers of errors for al-
l error types. The Nonlocal&Cluster system pro-
duced the Best numbers for all the error types. NP-
internal errors are still the most frequent error type
in our parsing systems.
Table 7 presents the statistics of frequent POS
tagging error patterns. We can see that JointPars-
ing system disambiguates {VV, NN} and {DEC,
DEG} better than Pipeline system, but cannot deal
with the NN?JJ pattern very well. StateAlign
system got better results in most of the patterns,
but cannot disambiguate {NR, NN} well. Non-
local&Cluster system got the best results in dis-
ambiguating the most ambiguous POS tag pairs of
{VV, NN}, {DEC, DEG}, {JJ, NN} and {NN, N-
R}.
5 Related Work
Joint POS tagging with parsing is not a new idea.
In PCFG-based parsing (Collins, 1999; Charniak,
2000; Petrov et al, 2006), POS tagging is consid-
ered as a natural step of parsing by employing lex-
ical rules. For transition-based parsing, Hatori et
al. (2011) proposed to integrate POS tagging with
dependency parsing. Our joint approach can be
seen as an adaption of Hatori et al (2011)?s ap-
proach for constituent parsing. Zhang et al (2013)
proposed a transition-based constituent parser to
process an input sentence from the character level.
However, manual annotation of the word-internal
structures need to be added to the original Tree-
bank in order to train such a parser.
Non-local features have been successfully used
for constituent parsing (Charniak and Johnson,
2005; Collins and Koo, 2005; Huang, 2008).
However, almost all of the previous work use non-
local features at the parse reranking stage. The
reason is that the single-stage chart-based parser
cannot use non-local structural features. In con-
trast, the transition-based parser can use arbitrari-
ly complex structural features. Therefore, we can
concisely utilize non-local features in a single-
740
stage parsing system.
6 Conclusion
In this paper, we proposed three improvements to
transition-based constituent parsing for Chinese.
First, we incorporated POS tagging into transition-
based constituent parsing to resolve the error prop-
agation problem of the pipeline approach. Second,
we proposed a state alignment strategy to align
competing decision sequences that have different
number of actions. Finally, we enhanced our pars-
ing model by enlarging the feature set with non-
local features and semi-supervised word cluster
features. Experimental results show that all these
methods improved the parsing performance sub-
stantially, and the final performance of our parsing
system outperformed all state-of-the-art systems.
Acknowledgments
We thank three anonymous reviewers for their
cogent comments. This work is funded by the
DAPRA via contract HR0011-11-C-0145 entitled
/Linguistic Resources for Multilingual Process-
ing0. All opinions expressed here are those of the
authors and do not necessarily reflect the views of
DARPA.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative r-
eranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173?180. Association for Computational Lin-
guistics.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics conference, pages 132?139. Asso-
ciation for Computational Linguistics.
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25?70.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main Volume,
pages 111?118, Barcelona, Spain, July.
Michael Collins. 1999. HEAD-DRIVEN STATISTI-
CAL MODELS FOR NATURAL LANGUAGE PARS-
ING. Ph.D. thesis, University of Pennsylvania.
Mary Harper and Zhongqiang Huang. 2011. Chinese
statistical parsing. Handbook of Natural Language
Processing and Machine Translation.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2011. Incremental joint pos tagging
and dependency parsing in chinese. In Proceed-
ings of 5th International Joint Conference on Nat-
ural Language Processing, pages 1216?1224, Chi-
ang Mai, Thailand, November. Asian Federation of
Natural Language Processing.
Zhongqiang Huang and Mary Harper. 2009. Self-
training pcfg grammars with latent annotations
across languages. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 2-Volume 2, pages 832?841.
Association for Computational Linguistics.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL, pages 586?
594.
Ling-Ya Huang. 2009. Improve chinese parsing with
max-ent reranking parser. Master Project Report,
Brown University.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08: HLT, pages 595?603,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Jonathan K. Kummerfeld, Daniel Tse, James R. Cur-
ran, and Dan Klein. 2013. An empirical examina-
tion of challenges in chinese parsing. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 98?103, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Ph.D. thesis, Massachusetts Institute
of Technology.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In HLT-NAACL, volume 4, pages
337?342. Citeseer.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL, pages
404?411.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computation-
al Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, pages 433?
440. Association for Computational Linguistics.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the Ninth International Workshop on Parsing
Technology, pages 125?132. Association for Com-
putational Linguistics.
741
Zhiguo Wang and Chengqing Zong. 2011. Parse r-
eranking based on higher-order lexical dependen-
cies. In IJCNLP, pages 1251?1259.
Mengqiu Wang, Kenji Sagae, and Teruko Mitamura.
2006. A fast, accurate deterministic parser for chi-
nese. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computation-
al Linguistics, pages 425?432. Association for Com-
putational Linguistics.
Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural lan-
guage engineering, 11(2):207?238.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the chinese treebank using a global dis-
criminative model. In Proceedings of the 11th Inter-
national Conference on Parsing Technologies, pages
162?171. Association for Computational Linguistic-
s.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2013. Chinese parsing exploiting characters.
In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 125?134, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
434?443, Sofia, Bulgaria, August. Association for
Computational Linguistics.
742
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 199?205,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Effective Document-Level Features for Chinese Patent Word
Segmentation
Si Li
Chinese Language Processing Group
Brandeis University
Waltham, MA 02453, USA
lisi@brandeis.edu
Nianwen Xue
Chinese Language Processing Group
Brandeis University
Waltham, MA 02453, USA
xuen@brandeis.edu
Abstract
A patent is a property right for an inven-
tion granted by the government to the in-
ventor. Patents often have a high con-
centration of scientific and technical terms
that are rare in everyday language. How-
ever, some scientific and technical terms
usually appear with high frequency only
in one specific patent. In this paper, we
propose a pragmatic approach to Chinese
word segmentation on patents where we
train a sequence labeling model based on
a group of novel document-level features.
Experiments show that the accuracy of our
model reached 96.3% (F
1
score) on the de-
velopment set and 95.0% on a held-out test
set.
1 Introduction
It is well known that Chinese text does not come
with natural word delimiters, and the first step
for many Chinese language processing tasks is
word segmentation, the automatic determination
of word boundaries in Chinese text. Tremendous
progress was made in this area in the last decade
or so due to the availability of large-scale human
segmented corpora coupled with better statistical
modeling techniques. On the data side, there exist
a few large-scale human annotated corpora based
on established word segmentation standards, and
these include the Chinese TreeBank (Xue et al,
2005), the Sinica Balanced Corpus (Chen et al,
1996), the PKU Peoples? Daily Corpus (Duan et
al., 2003), and the LIVAC balanced corpus (T?sou
et al, 1997). Another driver for the improvemen-
t in Chinese word segmentation accuracy comes
from the evolution of statistical modeling tech-
niques. Dictionaries used to play a central role
in early heuristics-based word segmentation tech-
niques (Chen and Liu, 1996; Sproat et al, 1996).
Modern word segmentation systems have moved
away from dictionary-based approaches in favor
of character tagging approaches. This allows the
word segmentation problem to be modeled as a
sequence labeling problem, and lends itself to dis-
criminative sequence modeling techniques (Xue,
2003; Peng et al, 2004). With these better model-
ing techniques, state-of-the-art systems routinely
report accuracy in the high 90%, and a few recen-
t systems report accuracies of over 98% in F
1
s-
core (Sun, 2011; Zeng et al, 2013b).
Chinese word segmentation is not a solved
problem however and significant challenges re-
main. Advanced word segmentation systems per-
form very well in domains such as newswire
where everyday language is used and there is a
large amount of human annotated training data.
There is often a rapid degradation in performance
when systems trained on one domain (let us call it
the source domain) are used to segment data in a
different domain (let us call it the target domain).
This problem is especially severe when the target
domain is distant from the source domain. This is
the problem we are facing when we perform word
segmentation on Chinese patent data. The word
segmentation accuracy on Chinese patents is very
poor if the word segmentation model is trained on
the Chinese TreeBank data, which consists of data
sources from a variety of genres but no patents.
To address this issue, we annotated a corpus of
142 patents which contain about 440K words ac-
cording to the Chinese TreeBank standards. We
trained a character-tagging based CRF model for
word segmentation, and based on the writing style
of patents, we propose a group of document-level
features as well as a novel character part-of-speech
feature (C_POS). Our results show these new fea-
tures are effective and we are able to achieve an
accuracy of 96.3% (F
1
score) on the development
set and 95% (F
1
score) on the test set.
199
2 Method
We adopt the character-based sequence labeling
approach, first proposed in (Xue, 2003), as our
modeling technique for its simplicity and effec-
tiveness. This approach treats each sentence as a
sequence of characters and assigns to each charac-
ter a label that indicates its position in the word. In
this paper, we use the BMES tag set to indicate the
character positions. The tag set has four labels that
represent for possible positions a character can oc-
cupy within a word: B for beginning, M for mid-
dle, E for ending, and S for a single character as a
word. After each character in a sentence is tagged
with a BMES label, a sequence of words can be
derived from this labeled character sequence.
We train a Conditional Random Field (CRF)
(Lafferty et al, 2001) model for this sequence
labeling. When extracting features to train a
CRF model from a sequence of n characters
C
1
C
2
...C
i?1
C
i
C
i+1
...C
n
, we extract features for
each character C
i
from a fixed window. We start
with a set of core features extracted from the anno-
tated corpus that have been shown to be effective
in previous works and propose some new features
for patent word segmentation. We describe each
group of features in detail below.
2.1 Character features (CF)
When predicting the position of a character with-
in a word, features based on its surrounding char-
acters and their types have shown to be the most
effective features for this task (Xue, 2003). There
are some variations of these features depending on
the window size in terms of the number of char-
acters to examine, and here we adopt the feature
templates used in (Ng and Low, 2004).
Character N-gram features The N-gram fea-
tures are various combinations of the surrounding
characters of the candidate character C
i
. The 10
features we used are listed below:
? Character unigrams: C
k
(i? 3 < k < i+ 3)
? Character bigrams: C
k
C
k+1
(i ? 3 < k <
i+ 2) and C
k?1
C
k+1
(k = i)
Character type N-gram features We classify
the characters in Chinese text into 4 types: Chi-
nese characters or hanzi, English letters, numbers
and others. T
i
is the character type of C
i
. The
character type has been used in the previous work-
s in various forms (Ng and Low, 2004; Jiang et al,
2009), and the 4 features we use are as follows:
? Character type unigrams: T
k
(k = i)
? Character type bigrams: T
k
T
k+1
(i?2 < k <
i+ 1) and T
k?1
T
k+1
(k = i)
Starting with this baseline, we extract some new
features to improve Chinese patent word segmen-
tation accuracy.
2.2 POS of single-character words (C_POS)
Chinese words are composed of Chinese hanzi,
and an overwhelming majority of these Chinese
characters can be single-character words them-
selves in some context. In fact, most of the multi-
character words are compounds that are 2-4 char-
acters in length. The formation of these compound
words is not random and abide by word formation
rules that are similar to the formation of phras-
es (Xue, 2000; Packard, 2000). In fact, the Chi-
nese TreeBank word segmentation guidelines (X-
ia, 2000) specify how words are segmented based
on the part-of-speech (POS) of their componen-
t characters. We hypothesize that the POS tags
of the single-character words would be useful in-
formation to help predict how they form the com-
pound words, and these POS tags are more fine-
grained information than the character type infor-
mation described in the previous section, but are
more robust and more generalizable than the char-
acters themselves.
Since we do not have POS-tagged patent da-
ta, we extract this information from the Chinese
TreeBank (CTB) 7.0, a 1.2-million-word out-of-
domain dataset. We extract the POS tags for al-
l the single-character words in the CTB. Some of
the single-character words will have more than one
POS tag. In this case, we select the POS tag with
the highest frequency as the C_POS tag for this
character. The result of this extraction process is
a list of single-character Chinese words, each of
which is assigned a single POS tag.
When extracting features for the target character
C
i
, if C
i
is in this list, the POS tag of C
i
is used as
a feature for this target character.
2.3 Document-level features
A patent is a property right for an invention grant-
ed by the government to the inventor, and many of
the patents have a high concentration of scientif-
ic and technical terms. From a machine learning
perspective, these terms are hard to detect and seg-
ment because they are often "new words" that are
not seen in everyday language. These technical
200
Algorithm 1 Longest n-gram sequence extraction.
Input:
Sentences {s
i
} in patent P
i
;
Output:
Longest n-gram sequence list for P
i
;
1: For each sentence s
i
in P
i
do:
n-gram sequence extraction
(2?n?length(s
i
));
2: Count the frequency of each n-gram sequence;
3: Delete the sequence if its frequency<2;
4: Delete sequence i if it is contained in a longer
sequence j;
5: All the remaining sequences form a longest n-
gram sequence list for P
i
;
6: return Longest n-gram sequences list.
terminologies also tend to be very sparse, either
because they are related to the latest invention that
has not made into everyday language, or because
our limited patent dataset cannot possibly cover all
possible technical topics. However, these techni-
cal terms are also topical and they tend to have
high relative frequency within a patent document
even though they are sparse in the entire patent da-
ta set. We attempt to exploit this distribution prop-
erty with some document-level features which are
extracted based on each patent document.
Longest n-gram features (LNG) We propose a
longest n-gram (LNG) feature as a document-level
feature. Each patent document is treated as an in-
dependent unit and the candidate longest n-gram
sequence lists for each patent are obtained as de-
scribed in Algorithm 1.
For a given patent, the LNG feature value for the
target character C
i
?s LNG is set to 'S' if the bigram
(C
i
,C
i+1
) are the first two characters of an n-gram
sequence in this patent?s longest n-gram sequence
list. If (C
i?1
, C
i
) are the last two characters of an
n-gram sequence in this patent?s longest n-gram
sequence list, the target character C
i
?s LNG is set
to 'F'. It is set to 'O' otherwise. If C
i
can be labeled
as both 'S' and 'F' at the same time, label 'T' will be
given as the final label. For example, if '?' is the
target character C
i
in patent A and the sequence
'??Z6?' is in patent A?s longest n-gram se-
quence list. If the character next to '?' is '?', the
value of the LNG feature is set to 'S'. If the next
character is not '?', the value of the LNG feature
is set to 'O'.
Algorithm 2 Pseudo KL divergence.
Input:
Sentences {s
i
} in patent P
i
;
Output:
Pseudo KL divergence values between differ-
ent characters in P
i
;
1: For each sentence s
i
in P
i
do:
trigram sequences extraction;
2: Count the frequency of each trigram;
3: Delete the trigram if its frequency<2;
4: For C
i
in trigram C
i
C
i+1
C
i+2
do :
PKL(C
i
, C
i+1
) = p(C
i
1
)log
p(C
i
1
)
p(C
i+1
2
)
(1)
PKL(C
i
, C
i+2
) = p(C
i
1
)log
p(C
i
1
)
p(C
i+2
3
)
(2)
The superscripts {1,2,3} indicate the character
position in trigram sequences;
5: return PKL(C
i
, C
i+1
) and PKL(C
i
, C
i+2
)
for the first character C
i
in each trigram.
Pseudo Kullback-Leibler divergence (PKL)
The second document-level feature we propose
is the Pseudo Kullback-Leibler divergence fea-
ture which is calculated following the form of
the Kullback-Leibler divergence. The relative
position information is very important for Chi-
nese word segmentation as a sequence labeling
task. Characters XY may constitute a meaningful
word, but characters Y X may not be. Therefore,
if we want to determine whether character X and
character Y can form a word, the relative position
of these two characters should be considered. We
adopt a pseudo KL divergence with the relative po-
sition information as a measure of the association
strength between two adjacent characters X and
Y . The pseudo KL divergence is an asymmetric
measure. The PKL value between character X
and character Y is described in Algorithm 2.
The PKL values are real numbers and are s-
parse. A common solution to sparsity reduction
is binning. We rank the PKL values between t-
wo adjacent characters in each patent from low to
high, and then divide all values into five bins. Each
bin is assigned a unique ID and all PKL values in
the same bin are replaced by this ID. This ID is
then used as the PKL feature value for the target
character C
i
.
201
Pointwise Mutual information (PMI) Point-
wise Mutual information has been widely used
in previous work on Chinese word segmentation
(Sun and Xu, 2011; Zhang et al, 2013b) and it is a
measure of the mutual dependence of two strings
and reflects the tendency of two strings appearing
in one word. In previous work, PMI statistics are
gathered on the entire data set, and here we gather
PMI statistics for each patent in an attempt to cap-
ture character strings with high PMI in a particu-
lar patent. The procedure for calculating PMI is
the same as that for computing pseudo KL diver-
gence, but the functions (1) and (2) are replaced
with the following functions:
PMI(C
i
, C
i+1
) = log
p(C
i
1
, C
i+1
2
)
p(C
i
1
)p(C
i+1
2
)
(3)
PMI(C
i
, C
i+2
) = log
p(C
i
1
, C
i+2
3
)
p(C
i
1
)p(C
i+2
3
)
(4)
For the target character C
i
, we obtain the values
for PMI(C
i
, C
i+1
) and PMI(C
i
, C
i+2
). In each
patent document, we rank these values from high
to low and divided them into five bins. Then the
PMI feature values are represented by the bin IDs.
3 Experiments
3.1 Data preparation
We annotated 142 Chinese patents following the
CTB word segmentation guidelines (Xia, 2000).
Since the original guidelines are mainly designed
to cover non-technical everyday language, many
scientific and technical terms found in patents are
not covered in the guidelines. We had to extend
the CTB word segmentation guidelines to han-
dle these new words. Deciding on how to seg-
ment these scientific and technical terms is a big
challenge since these patents cover many differ-
ent technical fields and without proper technical
background, even a native speaker has difficulty
in segmenting them properly. For difficult scien-
tific and technical terms, we consult BaiduBaike
("Baidu Encyclopedia")
1
, which we use as a scien-
tific and technical terminology dictionary during
our annotation. There are still many words that
do not appear in BaiduBaiKe, and these include
chemical names and formulas. These chemical
names and formulas (e.g., /??????Z
/1-bromo-3-chloropropane0) are usually very
1
http://baike.baidu.com/
Table 1: Training, development and test data on
Patent data
Data set # of words # of patent
Training 345336 113
Devel. 46196 14
Test 48351 15
long, and unlike everyday words, they often have
numbers and punctuation marks in them. We de-
cided not to try segmenting the internal structures
of such chemical terms and treat them as single
words, because without a technical background in
chemistry, it is very hard to segment their internal
structures consistently.
The annotated patent dataset covers many topics
and they include chemistry, mechanics, medicine,
etc. If we consider the words in our annotated
dataset but not in CTB 7.0 data as new words (or
out-of-vocabulary, OOV), the new words account
for 18.3% of the patent corpus by token and 68.1%
by type. This shows that there is a large number of
words in the patent corpus that are not in the ev-
eryday language vocabulary. Table 1 presents the
data split used in our experiments.
3.2 Main results
We use CRF++ (Kudo, 2013) to train our sequence
labeling model. Precision, recall, F
1
score and
R
OOV
are used to evaluate our word segmentation
methods, whereR
OOV
for our purposes means the
recall of new words which do not appear in CTB
7.0 but in patent data.
Table 2 shows the segmentation results on the
development and test sets with different feature
templates and different training sets. The CTB
training set includes the entire CTB 7.0, which has
1.2 million words. The model with the CF fea-
ture template is considered to be the baseline sys-
tem. We conducted 4 groups of experiments based
on the different datasets: (1) patent training set +
patent development set; (2) patent training set +
patent test set; (3) CTB training set + patent de-
velopment set; (4) CTB training set + patent test
set.
The results in Table 2 show that the model-
s trained on the patent data outperform the mod-
els trained on the CTB data by a big margin on
both the development and test set, even if the CTB
training set is much bigger. That proves the im-
portance of having a training set in the same do-
202
Table 2: Segmentation performance with different feature sets on different datasets.
Train set Test set Features P R F
1
R
OOV
Patent train Patent dev.
CF 95.34 95.28 95.32 90.02
CF+C_POS 95.58 95.40 95.49 90.40
CF+C_POS+LNG 96.32 96.00 96.15 91.22
CF+C_POS+PKL 95.62 95.41 95.51 90.40
CF+C_POS+PMI 95.65 95.40 95.53 89.94
CF+C_POS+PMI+PKL 95.72 95.53 95.62 90.37
CF+C_POS+LNG+PMI 96.42 96.09 96.26 91.66
CF+C_POS+LNG+PMI+PKL 96.48 96.12 96.30 91.69
Patent train Patent test
CF 93.98 94.49 94.23 85.19
CF+C_POS+LNG+PKL+PMI 94.89 95.10 95.00 87.89
CTB train Patent dev. CF+C_POS+LNG+PKL+PMI 89.04 90.75 89.89 72.80
CTB train Patent test CF+C_POS+LNG+PKL+PMI 87.88 89.03 88.45 70.89
main. The results also show that adding the new
features we proposed leads to consistent improve-
ment across all experimental conditions, and that
the LNG features are the most effective and bring
about the largest improvement in accuracy.
4 Related work
Most of the previous work on Chinese word seg-
mentation focused on newswire, and one wide-
ly adopted technique is character-based represen-
tation combined with sequential learning models
(Xue, 2003; Low et al, 2005; Zhao et al, 2006;
Sun and Xu, 2011; Zeng et al, 2013b; Zhang
et al, 2013b; Wang and Kan, 2013). More re-
cently, word-based models using perceptron learn-
ing techniques (Zhang and Clark, 2007) also pro-
duce very competitive results. There are also some
recent successful attempts to combine character-
based and word-based techniques (Sun, 2010;
Zeng et al, 2013a).
As Chinese word segmentation has reached a
very high accuracy in the newswire domain, the
attention of the field has started to shift to other
domains where there are few annotated resources
and the problem is more challenging, such as work
on the word segmentation of literature data (Li-
u and Zhang, 2012) and informal language gen-
res (Wang and Kan, 2013; Zhang et al, 2013a).
Patents are distinctly different from the above gen-
res as they contain scientific and technical terms
that require some special training to understand.
There has been very little work in this area, and
the only work that is devoted to Chinese word
segmentation is (Guo et al, 2012), which reports
work on Chinese patent word segmentation with
a fairly small test set without any annotated train-
ing data in the target domain. They reported an
accuracy of 86.42% (F
1
score), but the results are
incomparable with ours as their evaluation data is
not available to us. We differ from their work in
that we manually segmented a significant amount
of data, and trained a model with document-level
features designed to capture the characteristics of
patent data.
5 Conclusion
In this paper, we presented an accurate character-
based word segmentation model for Chinese
patents. Our contributions are two-fold. Our first
contribution is that we have annotated a signifi-
cant amount of Chinese patent data and we plan
to release this data once the copyright issues have
been cleared. Our second contribution is that we
designed document-level features to capture the
distributional characteristics of the scientific and
technical terms in patents. Experimental results
showed that the document-level features we pro-
posed are effective for patent word segmentation.
Acknowledgments
This paper is supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) vi-
a contract NO. D11PC20154. All views ex-
pressed in this paper are those of the authors and
do not necessarily represent the view of IARPA,
DoI/NBC, or the U.S. Government.
203
References
Keh-Jiann Chen and Shing-Huan Liu. 1996. Word
Identification for Mandarin Chinese Sentences. In
Proceedings of COLING?92, pages 101?107.
Keh-Jiann Chen, Chu-Ren Huang, Li-Ping Chang, and
Hui-Li Hsu. 1996. Sinica Corpus: Design Method-
ology for Balanced Corpora. In Proceedings of the
11 th Pacific Asia Conference on Language, Infor-
mation and Computation, pages 167?176.
Huiming Duan, Xiaojing Bai, Baobao Chang, and Shi-
wen Yu. 2003. Chinese word segmentation at
Peking University. In Proceedings of the second
SIGHAN workshop on Chinese language process-
ing, pages 152?155.
Zhen Guo, Yujie Zhang, Chen Su, and Jinan Xu. 2012.
Exploration of N-gram Features for the Domain
Adaptation of Chinese Word Segmentation. In Pro-
ceedings of Natural Language Processing and Chi-
nese Computing Natural Language Processing and
Chinese Computing, pages 121?131.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic Adaptation of Annotation Standards: Chi-
nese Word Segmentation and POS Tagging - A Case
Study. In Proceedings of ACL?09, pages 522?530.
Taku Kudo. 2013. CRF++: Yet Another CRF toolkit.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML?01, pages
282?289.
Yang Liu and Yue Zhang. 2012. Unsupervised Do-
main Adaptation for Joint Segmentation and POS-
Tagging. In Proceedings of COLING?12, pages
745?754.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.
A Maximum Entropy Approach to Chinese Word
Segmentation. In Proceedings of the 4th SIGHAN
Workshop on Chinese Language Processing, pages
970?979.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese Part-
of-Speech Tagging: One-at-a-Time or All-at-Once?
Word-Based or Character-Based? In Proceedings of
EMNLP?04, pages 277?284.
Jerome Packard. 2000. The Morphology of Chinese: a
cognitive and linguistic approach. Cambridge Uni-
versity Press.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese Segmentation and New Word Detec-
tion using Conditional Random Fields. In Proceed-
ings of COLING?04.
Richard Sproat, Chilin Shih, William Gale, and Nan-
cy Chang. 1996. A Stochastic Finite-State Word-
Segmentation Algorithm for Chinese. Computation-
al Linguistics, 22(3):377?404.
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
Word Segmentation Using Unlabeled Data. In Pro-
ceedings of EMNLP?11, pages 970?979.
Weiwei Sun. 2010. Word-based and character-based
word segmentation models: Comparison and com-
bination. In Proceedings of ACL?10, pages 1211?
1219.
Weiwei Sun. 2011. A Stacked Sub-Word Model
for Joint Chinese Word Segmentation and Part-of-
Speech Tagging. In Proceedings of ACL?11, pages
1385?1394.
Benjamin K. T?sou, Hing-Lung Lin, Godfrey Liu,
Terence Chan, Jerome Hu, Ching hai Chew, and
John K.P. Tse. 1997. A Synchronous Chinese Lan-
guage Corpus from Different Speech Communities:
Construction and Application. International Jour-
nal of Computational Linguistics and Chinese Lan-
guage Processing, 2(1):91?104.
Aobo Wang and Min-Yen Kan. 2013. Mining Infor-
mal Language from Chinese Microtext: Joint Word
Recognition and Segmentation. In Proceedings of
ACL?13, pages 731?741.
Fei Xia. 2000. The segmentation guidelines for the
Penn Chinese Treebank (3.0).
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural
Language Engineering, 11(2):207?238.
Nianwen Xue. 2000. Defining and identifying words
in Chinese. Ph.D. thesis, University of Delaware.
Nianwen Xue. 2003. Chinese Word Segmentation as
Character Tagging. International Journal of Com-
putational Linguistics and Chinese Language Pro-
cessing, 8(1):29?48.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and
Isabel Trancoso. 2013a. Co-regularizing character-
based and word-based models for semi-supervised
Chinese word segmentation. In Proceedings of A-
CL?13, pages 171?176.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and
Isabel Trancoso. 2013b. Graph-based Semi-
Supervised Model for Joint Chinese Word Segmen-
tation and Part-of-Speech Tagging. In Proceedings
of ACL?13, pages 770?779.
Yue Zhang and Stephen Clark. 2007. Chinese Seg-
mentation Using a Word-based Perceptron Algorith-
m. In Proceedings of ACL?07, pages 840?847.
Longkai Zhang, Li Li, Zhengyan He, Houfeng Wang,
and Ni Sun. 2013a. Improving Chinese Word Seg-
mentation on Micro-blog Using Rich Punctuations.
In Proceedings of ACL?13, pages 177?182.
204
Longkai Zhang, Houfeng Wang, Xu Sun, and Mairgup
Mansur. 2013b. Exploring Representations from
Unlabeled Data with Co-training for Chinese Word
Segmentation. In Proceedings of EMNLP?13, pages
311?321.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An
improved Chinese word segmentation system with
conditional random field. In Proceedings of the 5th
SIGHAN Workshop on Chinese Language Process-
ing, pages 162?165.
205
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 82?90,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
PropBank Annotation of Multilingual Light Verb Constructions 
 
 
Jena D. Hwang1, Archna Bhatia3, Clare Bonial1, Aous Mansouri1,  
Ashwini Vaidya1, Nianwen Xue2, and Martha Palmer1 
1Department of Linguistics, University of Colorado at Boulder, Boulder CO 80309 
2Department of Computer Science, Brandeis University, Waltham MA 02453 
3Department of Linguistics, University of Illinois at Urbana-Champaign, Urbana IL 61801 
{hwangd,claire.bonial,aous.mansouri,ashwini.vaidya,martha.palmer} 
@colorado.edu, bhatia@illinois.edu, xuen@brandeis.edu 
 
  
 
Abstract 
In this paper, we have addressed the task 
of PropBank annotation of light verb 
constructions, which like multi-word 
expressions pose special problems. To 
arrive at a solution, we have evaluated 3 
different possible methods of annotation. 
The final method involves three passes: 
(1) manual identification of a light verb 
construction, (2) annotation based on the 
light verb construction?s Frame File, and 
(3) a deterministic merging of the first 
two passes. We also discuss how in 
various languages the light verb 
constructions are identified and can be 
distinguished from the non-light verb 
word groupings.  
1 Introduction  
One of the aims in natural language processing, 
specifically the task of semantic role labeling 
(SRL), is to correctly identify and extract the 
different semantic relationships between words 
in a given text. In such tasks, verbs are 
considered important, as they are responsible for 
assigning and controlling the semantic roles of 
the arguments and adjuncts around it. Thus, the 
goal of the SRL task is to identify the arguments 
of the predicate and label them according to their 
semantic relationship to the predicate (Gildea 
and Jurafsky, 2002; Pradhan et al, 2003).  
To this end, PropBank (Palmer et. al., 2005) 
has developed semantic role labels and labeled 
large corpora for training and testing of 
supervised systems. PropBank identifies and 
labels the semantic arguments of the verb on a 
verb-by-verb basis, creating a separate Frame 
File that includes verb specific semantic roles to 
account for each subcategorization frame of the 
verb. It has been shown that training supervised 
systems with PropBank?s semantic roles for 
shallow semantic analysis yield good results (see 
CoNLL 2005 and 2008).  
However, semantic role labeling tasks are 
often complicated by multiword expressions 
(MWEs) such as idiomatic expressions (e.g., 
?Stop pulling my leg!?), verb particle 
constructions (e.g., ?You must get over your 
shyness.?), light verb constructions (e.g., ?take a 
walk?, ?give a lecture?), and other complex 
predicates (e.g., V+V predicates such as Hindi?s 
???? ??? nikal gayaa, lit. ?exit went?, means 
?left? or ?departed?). MWEs that involve verbs 
are especially challenging because the 
subcategorization frame of the predicate is no 
longer solely dependent on the verb alone. 
Rather, in many of these cases the argument 
structure is assigned by the union of two 
predicating elements. Thus, it is important that 
the manual annotation of semantic roles, which 
will be used by automatic SRL systems, define 
and label these MWEs in a consistent and 
effective manner. 
In this paper we focus on the PropBank 
annotation of light verb constructions (LVCs). 
We have developed a multilingual schema for 
annotating LVCs that takes into consideration the 
similarities and differences shared by the 
construction as it appears in English, Arabic, 
Chinese, and Hindi. We also discuss in some 
detail the practical challenges involved in the 
crosslinguistic analysis of LVCs, which we hope 
will bring us a step closer to a unified 
crosslinguistic analysis.    
Since NomBank, as a companion to 
PropBank, provides corresponding semantic role 
82
labels for noun predicates (Meyers et al, 2004), 
we would like to take advantage of NomBank?s 
existing nominalization Frame Files and 
annotations as much as possible.  A question that 
we must therefore address is, ?Are 
nominalization argument structures exactly the 
same whether or not they occur within an LVC?? 
as will be discussed in section 6.1. 
2 Identifying Light Verb Constructions 
Linguistically LVCs are considered a type of a 
complex predicate. Many studies from differing 
angles and frameworks have characterized 
complex predicates as a fusion of two or more 
predicative elements. For example, Rosen (1997) 
treats complex structures as complementation 
structures, where the argument structure of 
elements in a complex predicate are fused 
together.  Goldberg (1993) takes a constructional 
approach to complex predicates and arrives at an 
analysis that is comparable to viewing complex 
predicates as a single lexical item. Similarly, 
Mohanan (1997) assumes different levels of 
linguistic representation for complex predicates 
in which the elements, such as the noun and the 
light verb, functionally combine to give a single 
clausal nucleus. Alsina (1997) and Butt (1997) 
suggest that complex predicates may be formed 
by syntactically independent elements whose 
argument structures are brought together by a 
predicate composition mechanism.  
While there is no clear-cut definition of LVCs, 
let alne the whole range of complex predicates, 
for the purposes of this study, we have adapted 
our approach largely from Butt?s (2004) criteria 
for defining LVCs. LVCs are characterized by a 
light verb and a predicating complement 
(henceforth, true predicate) that ?combine to 
predicate as a single element.? (Ibid.) In LVC, 
the verb is considered semantically bleached in 
such a way that the verb does not hold its full 
predicating power. Thus, the light verb plus its 
true predicate can often be paraphrased by a 
verbal form of the true predicate without loss of 
the core meaning of the expression. For example, 
the light verb ?gave? and the predicate ?lecture? 
in ?gave a lecture?, together form a single 
predicating unit such that it can be paraphrased 
by ?lectured?. 
True predicates in LVCs can be a noun (the 
object of the verb or the object of the preposition 
in a prepositional phrase), an adjective, or a verb. 
One light verb plus true predicate combination 
found commonly across all our PropBank 
languages (i.e., English, Arabic, Chinese, and 
Hindi) is the noun as the object of the verb as in 
?Sara took [a stroll] along the beach?. In Hindi, 
true predicates can be adjectives or verbs, in 
addition to the nouns. 
??? ? ??? [?????]  ???         (Adjective) 
to-me  you [nice]  seem 
lit. ?You seem nice to me? 
'You (are) liked to me (=I like you).' 
?????  ?? ???  [??] ????   (Verb) 
I-ERG everything  [do] took 
lit. ?I took do everything? 
'I have done everything.' 
As for Arabic, the LVCs come in verb+noun 
pairings. However, they surface in two syntactic 
forms. It can either be the object of the verb just 
like in English: 
 
 ???? ????]?????? ] ????? ??  
gave.he Georges [lecture] PREP Lebanon 
lit.'Georges gave a lecture about Lebanon' 
?Georges lectured about Lebanon? 
or the complement can be the object of a 
preposition: 
 
 ??????]????? ]????? ????? 
conduct.I [PREP-visit] our.saint Ilias 
lit. ?I will conduct with visit Saint Ilias?s? 
?I will visit Saint Ilias?s? 
3 Standard PropBank  
Annotation Procedure 
The PropBank annotation process can be broken 
down into two major steps: creation of the Frame 
Files for verbs occurring in the data and 
annotation of the data using the Frame Files. 
During the creation of the Frame Files, the 
usages of the verbs in the data are examined by 
linguists (henceforth, ?framers?). Based on these 
observations, the framers create a Frame File for 
each verb containing one or more framesets, 
which correspond to coarse-grained senses of the 
predicate lemma. Each frameset specifies the 
PropBank labels (i.e., ARG0, ARG1,?ARG5) 
corresponding to the argument structure of the 
verb. Additionally, illustrative examples are 
included for each frameset, which will later be 
referenced by the annotators. These examples 
also include the use of the ARGM labels. 
Thus, the framesets are based on the 
examination of the data, the framers? linguistic 
knowledge and native-speaker intuition. At 
83
times, we also make use of the syntactic and 
semantic behavior of the verb as described by 
certain lexical resources. These resources include 
VerbNet (Kipper et. al., 2006) and FrameNet 
(Baker et. al., 1998) for English, a number of 
monolingual and bilingual dictionaries for 
Arabic, and Hindi WordNet and DS Parses 
(Palmer et. al., 2009) for Hindi. Additionally, if 
available, we consult existing framesets of words 
with similar meanings across different languages. 
The data awaiting annotation are passed onto 
the annotators for a double-blind annotation 
process using the previously created framesets. 
The double annotated data is then adjudicated by 
a third annotator, during which time the 
differences of the two annotations are resolved to 
produce the Gold Standard. 
Two major guiding considerations during the 
framing and annotating process are data 
consistency and annotator productivity. During 
the frameset creation process, verbs that share 
similar semantic and syntactic characteristics are 
framed similarly. During the annotation process, 
the data is organized by verbs so that each verb is 
tackled all at once. In doing so, we firstly ensure 
that the framesets of similar verbs, and in turn, 
the annotation of the verbs, will both be 
consistent across the data. Secondly, by tackling 
annotation on verb-by-verb basis, the annotators 
are able to concentrate on a single verb at a time, 
making the process easier and faster for the 
annotators. 
4 Annotating LVC 
A similar process must be followed when 
annotating light verb constructions The first step 
is to create consistent Frame Files for light verbs. 
Then in order to make the annotation process 
produce consistent data at a reasonable speed, we 
have decided to carry out the light verb 
annotation in three passes (Table 1):  (1) annotate 
the light verb, (2) annotate the true predicate, and 
(3) merge the two annotations into one. 
The first pass involves the identification of the 
light verb. The most important parts of this step 
are to identify a verb as having bleached 
meaning, thereafter assign a generic light verb 
frameset and identify the true predicating 
expression of the sentence, which would be 
marked with ARG-PRX (i.e., ARGument-
PRedicating eXpression). For English, for 
example, annotators were instructed to use Butt?s 
(2004) criteria as described in Section 2. These 
criteria required that annotators be able to 
recognize whether or not the complement of a 
potential light verb was itself a predicating 
element. To make this occasionally difficult 
judgment, annotators used a simple heuristic test 
of whether or not the complement was headed by 
an element that has a verbal counterpart.  If so, 
the light verb frameset was selected. 
The second pass involves the annotation of the 
sentence with the true predicate as the relation. 
During this pass, the true predicate is annotated 
with an appropriate frameset. In the third pass, 
the arguments and the modifiers of the two 
previous passes are reconciled and merged into a 
single annotation. In order to reduce the number 
of hand annotation, it is preferable for this last 
pass, the Pass 3, to be done automatically. 
Since the nature of the light verb is different 
from that of other verbs as described in Section 
2, the advantage of doing the annotation of the 
light verb and the true predicate on separate 
passes is that in the light verb pass the annotators 
will be able to quickly dispose of the verb as a 
light verb and in the second pass, they will be 
allowed to solely focus on the annotation of the 
light verb?s true predicate. 
The descriptions of how the arguments and 
modifiers of the light verbs and their true 
predicates are annotated are mentioned in Table 
1, but notably, none of the examples in it 
currently include the annotation of arguments 
 Pass 1: Pass 2: Pass 3: 
 Light Verb Annotation True Predicate Annotation Merge of Pass1&2 Annotation 
Relation Light verb True predicate Light verb + true predicate 
Arguments 
and 
Modifiers 
- Predicating expression is 
annotated with ARG-PRX 
- Arguments and modifiers of 
the light verb are annotated 
- Arguments and modifiers of 
the true predicate are annotated 
- Arguments and modifiers 
found in the two passes are 
merged, preferably 
automatically. 
Frameset Light verb frameset True predicate?s frameset LVC?s frameset 
 
Example 
?John took a brisk walk through the park.? 
REL: took 
ARG-PRX: a brisk walk 
ARG-MNR: brisk  
REL: walk 
REL: took walk 
ARG-MNR: brisk 
Table 1. Preliminary Annotation Scheme 
84
and modifiers.  This is intentional, as coming to 
an agreement concerning the details of what 
exactly each of the three passes looks like while 
meeting the needs of the four PropBank 
languages is quite challenging. Thus, for the rest 
of the paper we will discuss the strengths and 
weaknesses of the two trial methods of 
annotation we have considered and discarded in 
Section 5, as well as the final annotation scheme 
we chose in Section 6. 
5 Trials 
5.1 Method 1 
As our first attempt, the annotation of argument 
and adjuncts was articulated in the following 
manner (Table 2). 
Pass 1: Pass 2: 
Light verb True predicate 
- Predicating expression 
is labeled ARG-PRX 
- Annotate the Subject 
argument of the light 
verb as the Arg0. 
- Annotate the rest of the 
arguments and modifiers 
of the light verb with 
ARGM labels. 
- Annotate arguments 
and modifiers of the 
true predicate within 
its domain of locality. 
Generic light verb Frame 
File 
True predicate?s 
Frame File 
?-RKQ WRRN D EULVN ZDON WKURXJK WKH SDUN? 
ARG0: John 
REL: took 
ARG-PRX: a brisk walk 
ARG-DIR: through the park 
ARG-MNR: brisk  
REL: walk 
Table 2. Method 1 for annotation for Passes 1 and 2. 
Revised information is in italics. 
In Pass 1, in addition to annotating the 
predicating expression of the light verb with 
ARG-PRX, the subject argument was marked 
with an ARG0. The choice of ARG0, which 
corresponds to a proto-typical agent, was guided 
by the observation that English LVCs tend to 
lend a component of agentivity to the subject 
even in cases where the true predicate would not 
necessarily assign an agent as its subject. The 
rest of the arguments and modifiers were labeled 
with corresponding ARGM (i.e., modifier) 
labels. The assumption here is that the arguments 
of the light verb will also be the arguments of the 
true predicate.   
In Pass 2, then, the annotation of the 
arguments of the true predicate was restricted to 
its domain of locality (i.e., the span of the ARG-
PRX as marked in Pass1). That is, in the example 
?John took a brisk walk through the park?, the 
labeled spans for the true predicate would be 
limited to the NP ?a brisk walk? and neither 
?John? nor through the park? would be annotated 
as the arguments of the true predicate ?walk?. 
Frame Files: This method would require three 
Frame Files: a generic light verb Frame File, a 
true predicate Frame File, and an LVC Frame 
File. The Frame File for the light verb would not 
be specific to the form of the light verb (e.g., 
same frame for take and make). Rather, it would 
indicate a skeletal argument structure in order to 
reduce the amount of Frame Files made, 
including only Arg0 as its argument1.  
5.2 Weakness of Method 1 
This method has one glaring problem: the 
assumption that the semantic roles of the 
arguments as assigned by the light verb 
uniformly coincide with those assigned by the 
true predicate does not always hold. Consider the 
following English sentence2. 
whether Wu Shu-Chen would make another 
[appearance] in court was subject to observation 
In this example, ?Wu Shu-Chen? is the agent 
argument (Arg0) of the light verb ?make? and is 
the theme or patient argument (Arg1) of a typical  
?appearance? event. Also consider the following 
example from Hindi.  
It is possible that in a light verb construction, 
the light verb actually modifies the standard 
underlying semantics of a nominalization like 
appearance.  In any event, we cannot assume that 
the expected argument labels for the light verb 
and for the standard interpretation of the 
nominalization will always coincide. Thus, we 
could say that Pass 2?s true predicate annotation 
is only partial and is not representative of the 
complete argument structure. In particular, we 
are left with a very difficult merging problem, 
because the argument labels of the two separate 
passes conflict as seen in the above examples. 
5.3 Method 2 
In order to remedy the problem of conflicting 
argument labels, we revised Method 1?s Pass 2 
annotation scheme. This is shown in Table 3. 
Pass 1 remains unchanged from Method 1. 
In this method, both the light verb and the true 
predicate of the sentence receive complete sets of 
                                                          
1 This is why the rest of the argument/modifiers would be 
annotated using ARGM modifier labels. 
2  The light verb is in boldface, the true predicate is in bold 
and square brackets, and the argument/adjunct under 
consideration is underlined. 
85
argument and modifier labels. In Pass 2, the 
limitation of annotating within the domain of 
locality is removed. That is, the arguments and 
modifiers inside and outside the true predicate?s 
domain of control are annotated with respect to 
their semantic relationship to the true predicate 
(e.g., in the English example of Section 5.2, ?Wu 
Shu-Chen? would be considered ARG1 of 
?appearance?).  
Frame Files: This method would also require 
three Frame Files. The major difference is that 
with this method the Frame File for the true 
predicate includes arguments that are sisters to 
the light verb.  
5.4 Weaknesses of Method 2 
If in Method 1 we have committed the error of 
semantic unfaithfulness due to omission, in 
Method 2 we are faced with the problem of 
including too much. In the following sentence, 
consider the role of the underlined adjunct: 
A New York audience ? gave it a big round 
of applause when the music started to play. 
By the annotation in Method 2, the underlined 
temporal adjunct ?when the music started to 
play? is labeled as both the argument of ?give? 
and of ?applause?. The question here is does the 
argument apply to both the giving and the 
applauding event? In other words, does the 
adjunct play an equal role in both passes?  
 Since it could be easily said that the temporal 
phrase applies to both the applauding and the 
giving of the applause events, this example may 
not be particularly compelling. However, what if 
a syntactic complement of the light verb is a 
semantic argument of the true predicate and the 
true predicate only? This is seen more frequently 
in the cases where the light verb is less bleached 
than in the case of ?give? above. Consider the 
following Arabic example. 
 
 ????? ??]???????? ] ????? ????????? ??????? ??????  
took.we PREP DEF-consideration PREP 
prepertations.our possibility sustain.their losses 
?We took into [consideration] during our prepa-
rations the possibility of them sustaining losses? 
 
Here, even though the constituent ?of them 
sustaining losses? is the syntactic complement of 
the verb ?to take;? semantically, it modifies only 
the nominal object of the PP ?consideration.?  
There are similar phenomena in Chinese light 
verb constructions. Syntactic modifiers of the 
light verb are semantic arguments of the true 
predicate, which is usually a nominalization that 
serves as its complement.  
 
?? ?  ?    ? ? ??    [??]    ?? ? 
we now regarding this CL issue [conduct] discussion. 
lit.?We are conducting a discussion on this issue.? 
 ?We are discussing this issue.? 
 
The prepositional phrase ????? ?regarding 
this issue? is a sister to the light verb but 
semantically it is an argument of the nominalized 
predicate ?? ?discussion?. 
The logical next question would be: does the 
annotation of the arguments, adjuncts and 
modifiers have to be all or nothing? It could 
conceivably be possible to assign a selected set 
of arguments at the light verb or true predicate 
level. For example, in the Chinese sentence, the 
modifier ?regarding this CL issue?, though a 
syntactic adjunct to the light verb, could be left 
out from the semantic annotation in Pass 1 and 
included only in the Pass 2. 
However, the objection to this treatment 
comes from a more practical need. As mentioned 
above, in order to keep the manual annotation to 
a minimum, it would be necessary to keep Pass 3 
completely deterministic. As is, with the 
unmodified Method 2, there would be the need to 
choose between Pass 1 or Pass 2 annotation to 
when doing the automatic Pass 3. If we modify 
Method 2 by annotating only a selected set of 
syntactic arguments for the light verb or the true 
predicate, then this issue is exacerbated. In such 
a case there we would have to develop with strict 
rules for which arguments of which pass should 
be included in Pass 3. Pass 3 would no longer be 
automatic, and should be done manually.  
Pass 2: 
True predicate 
- Annotate the Subject argument of the light verb 
with the appropriate role of the true predicate 
- Annotate arguments and modifiers of the true 
predicate without limitation as to the domain of 
locality. 
True predicate?s Frame File 
?+H PDGH DQRWKHU DSSHDUDQFH DW WKH SDUW\? 
ARG1: He 
ARG-ADV: another 
REL: appearance 
ARG-DIR: at court 
Table 3. Method 2 for annotation for Pass 2. Pass 
1 as presented in Table 2 remains unchanged. 
Revised information for Pass 2 is in italics 
 
86
6 Final Annotation Scheme 
6.1 Semantic Fidelity 
Many of the objections so far to Methods 1 and 2 
have centered on the issue of semantic fidelity 
during the annotation of each of the two passes. 
The debate of whether both passes should be 
annotated and to what extent has practical 
implications for the third Pass, as described 
above. However, more importantly it comes 
down to whether or not the semantics of the final 
light verb plus true predicate combination is 
indeed distinct from the semantics of its parts 
(i.e. light verb and true predicate, separately). 
This may be a fascinating linguistic question, but 
it is not something our annotators can be 
debating for each and every instance.   
Instead, we argue that the semantic argument 
structure of the light verb plus true predicate 
combination can in practice be different from 
that of the expressions taken independently as 
has been proposed by various studies (Butt, 
2004; Rosen, 1997; Grimshaw & Mester, 1988). 
Thus, we resolve the cases in which the 
differences in argument roles as assigned by the 
light verb and the nominalization (Section 5.2) 
by handling the argument structure of the 
standard nominalization separately from that of 
the nominalization participating in the LVC. In 
the example ?Chen made another appearance in 
court?, we annotate ?Chen? as the Agent (ARG0) 
of the full predicate ?[make] [appearance]?, 
which is different from the argument structure of 
the standard nominalization which would label 
?Chen? to be the Patient argument (ARG1). 
6.2 Method 3: Final Method 
Our final method of light verb annotation reflects 
the notion that the noun, verb, or adjective as a 
true predicate within an LVC can have a 
different argument structure from that of the 
word alone. Table 4 shows the final annotation 
scheme for light verb construction.  
During Pass 1, the LVCs and their predicating 
expressions are identified in the data. Instances 
identified as LVCs in Pass 1 are then manually 
annotated during Pass 2, annotating the 
arguments and adjuncts of the light verb and the 
true predicate with roles that reflect their 
semantic relationships to the light verb plus true 
predicate. In practice, Pass 1 becomes a way of 
simply manually identifying the light verb 
usages. It is in Pass 2 that we make the final 
choice of argument labels for all of the 
arguments. Thus in Pass 3, the light verb and the 
true predicate lemmas from Pass 1 and 2 are 
joined into a single unit (e.g., in the example 
found in Table 4, the light verb ?took? would be 
joined with the true predicate ?walk? into 
?took+walk?) 3. In this final method, Pass 3 can 
be achieved completely deterministically. 
The major difference in this annotation 
scheme from that of Methods 1 and 2 is that 
instead of annotating in terms of the semantics of 
the bare noun, adjective or verb, the argument 
structure is determined for the entire predicate or 
the full event: semantics of the light verb plus the 
true predicate. This means that for the sentences 
where the argument roles of the verb and the 
nominalization disagree like ?Chen? in ?Chen 
                                                          
3 The order of Pass 2 and Pass 3 as presented in Table 4 is 
arguably a product of how the annotation tools for 
PropBank are set up for Arabic, Chinese, and English. That 
is, the order of the Pass 2 and Pass 3 could potentially be 
flipped provided that the tools and procedures of annotation 
support it, as is the case for Hindi PropBank. After the LVC 
and ARG-PRX are identified in Pass 1, the light verb and 
the true predicate can be deterministically joined into a 
single relation in Pass 2, leaving the manual annotation of 
LVC for Pass 3.  The advantage of this alternative ordering 
is that because the annotation of LVC is done around light 
verb plus the true predicate as a single relation, rather than 
the true predicate alone as in Table 4, the argument 
annotation may in actuality be more intuitive for annotators 
even with less training. 
 Pass 1: Pass 2:  Pass 3: 
 Light Verb Identification LVC Annotation Deterministic relation merge 
Relation Light verb True predicate Light verb + true predicate 
Arguments 
& Modifiers 
- Predicating expression is 
annotated with ARG-PRX 
- Arguments and modifiers of 
the LVCs are annotated 
- Arguments and modifiers 
are taken from Pass 2 
Frame File <no Frame File needed> LVC?s Frame File LVC?s Frame File 
 
Example 
?John took a brisk walk through the park.? 
REL: took 
ARG-PRX: a brisk walk 
ARG0: John 
ARG-MNR: brisk  
REL: walk 
ARGM-DIR: through the park 
ARG0: John 
ARG-MNR: brisk  
REL: [took][walk] 
ARGM-DIR: through the park 
Table 4. Final Annotation Scheme 
87
made another4 appearance in court?, we label the 
argument with the role that is consistent with the 
entire predicate (i.e. Agent, ARG0).  
Frame Files: The final advantage to this 
method is that only one Frame File is needed. 
Since Pass 1 is an identification round, no Frame 
File is required. A single Frame File for LVC 
that includes the argument structure with respect 
to the light verb plus true predicate combination 
will suffice for Pass 2 and Pass 3. 
7 Distinguishing LVCs from MWEs 
As we have discussed in Section 2, we adapted 
our approach from Butt?s (2004) definition of 
LVCs. That is, an LVC is characterized by a 
semantically bleached light verb and a true 
predicate. These elements combine as a single 
predicating unit, in such a way that the light verb 
plus its true predicate can be paraphrased by a 
verbal form of the true predicate without loss of 
the core meaning of the expression (e.g. 
?lectured? for ?gave a lecture?). Also, as 
discussed in Section 6.1, our approach advocates 
the notion that the semantic argument structure 
of the light verb plus true predicate is different 
from that of the expressions taken independently 
(as also proposed by Butt, 2004; Rosen, 1997; 
Grimshaw & Mester, 1988 among others). 
While these definitions are appropriate for the 
PropBank annotation task as we have presented 
it, there are still cases that merit closer attention. 
Even English with a rather limited set of verbs 
that are commonly cited as LVCs, includes a 
problematic mixture of what could arguably be 
termed either LVCs or idiomatic expressions: 
?make exception?, ?take charge?. This difficulty 
in part is the effect of frequency and 
entrenchment of particular constructions.  The 
light verbs themselves do not diminish in form 
over time in a manner similar to auxiliaries (Butt, 
2004), although the complements of common 
LVCs can change over time such that it is no 
longer clear that the complement is a predicating 
element.   
In the case of English, the expressions ?take 
charge? may be more commonly found today as a 
LVC than independently in its verbal form.  As 
we discovered with our annotators, native 
English speakers are uncomfortable using the 
verb ?charge? (i.e. to burden with a 
                                                          
4 The adjective ?another? is annotated as the modifier of the 
full predicate ?[make][appearance]? as it can be interpreted 
to mean that the make appearance event happened a 
previous appearance has been made. 
responsibility) as an independent matrix verb. A 
similar phenomenon can be seen in Arabic, 
where the predicate ??? ???? lit. ?release name? 
exemplifies a prototypical LVC that means ?to 
name?. However, in our data we see cases in 
which the complement is missing, while the 
semantics of the LVC remains intact: 
 ???? ???? ?? ??????? ??????  
CONJ REL be released.he PREP-him/it  
DEF-sector DEF-public 
lit ?Or what is released to it ?the public sector?? 
?Or what is called/named ?the public sector.?? 
This raises the question of: when does a 
construction that may have once been an LVC 
become more properly defined as an idiomatic 
expression due to such entrenchment?  Idiomatic 
expressions can potentially be distinguished from 
LVCs through judgments of how fixed or 
syntactically variable a construction is, and on 
the basis of how semantically transparent or 
decomposable the construction is (Nunberg et. 
al., 1994). However, sometimes the dividing line 
is hard to draw.  
A similar problem arises in determining 
whether a construction is a case of an LVC or 
simply a usage with a distinct sense of the verb. 
Take, for example, the following Arabic 
sentence. 
 ?????? ????? 
   take.he DEF-food 
lit. ?(he) took food? 
?he ate? 
Here, the Arabic word ???? ?food? is the noun 
derivation of the root shared by the verb ???? ?to 
eat?, in such a way that the sentence could be 
rephrased as ???? ?(he) ate?. This example falls 
neatly into the LVC category. However, further 
examples suggest that the example is a case of a 
distinct sense of ?to take orally? where the 
restrictions on the object are that the theme must 
be something that can be taken by mouth: 
?????? ????? 
take.he DEF-medicine 
?he took medicine? 
?????? ????? 
take.he DEF-soup 
?he took soup? 
Finally, determining the appropriate criteria to 
distinguish between a truly semantically 
bleached verb and verbs that seem to be 
participating in complex predication but 
contribute more to the semantics of the 
construction is a challenge for all languages. For 
example, in English data, there are potential 
LVCs with verbs that are not often thought of as 
light verbs, such as ?produce an alteration? and 
88
?issue a complaint?.  Although most English 
speakers would agree that the verbs in these 
constructions do not contribute to the semantics 
of the construction (e.g. ?issue a complaint? can 
be paraphrased to ?to complain?), there are 
similar constructions such as ?register a 
complaint,? wherein the verb cannot be 
considered light. For the purposes of annotation, 
where it is necessary for annotators to understand 
clear criteria for distinguishing light verbs, such 
cases are highly problematic because there is no 
deterministic way to measure the extent to which 
the verbal element contributes to the semantics 
of the construction.  In turn, there is not a good 
way to distinguish some of these borderline 
verbs from their normal, heavy usages.  
Such problems can be resolved by establishing 
language-specific semantic or syntactic tests that 
can be used for taking care of the borderline 
cases of LVCs. However, there is one other 
plausible manner we have identified that could 
help in detecting such atypical LVCs. This can 
be done by focusing on the argument structures 
of predicating complements rather than focusing 
on the verbs themselves.  Grimshaw & Mester 
(1988) suggest that the formation of LVCs 
involves argument transfer from the predicating 
complement to the verb, which is semantically 
bleached and thematically incomplete and 
assigns no thematic roles itself.  Similarly, 
Stevenson et al (2004) suggest that the 
acceptability of a potential LVC depends on the 
semantic properties of the complement.  Thus, 
atypical LVCs, such as the English construction 
?issue a complaint,? can potentially be detected 
during the annotation of eventive nouns, planned 
for all PropBank languages.  
This process will make our treatment of LVCs 
more comprehensive. Used with our language-
specific semantic and syntactic criteria relating to 
both the verb and the predicating complement, it 
will help us to more effectively capture as many 
types of LVCs as possible, including those of the 
V+ADJ and V+V varieties. 
8 Usefulness of our Approach 
Two basic approaches have previously been 
taken to handle all types of MWEs, including 
LVCs in natural language processing 
applications. The first is to treat MWEs quite 
simply as fixed expressions or long strings of 
words with spaces in between; the second is to 
treat MWEs as purely compositional (Sag et al, 
2002). The words-with-spaces approach is 
adequate for handling fixed idiomatic 
expressions, but issues of lexical proliferation 
and flexibility quickly arise when this approach 
is applied to light verbs, which are syntactically 
flexible and can number in the tens of thousands 
for a given language (Stevenson et al, 2004; Sag 
et al, 2002).  Nonetheless, large-scale lexical 
resources such as FrameNet (Baker et al, 1998) 
and WordNet (Fellbaum, 1999) continue to 
expand with entries that are MWEs.   
The purely compositional approach is also 
problematic for light verbs because it is 
notoriously difficult to predict which light verbs 
can grammatically combine with other 
predicating elements; thus, this approach leads to 
problems of overgeneration (Sag et al, 2002).  In 
order to overcome this problem, Stevenson et al 
(2004) attempted to determine which 
nominalizations could form a valid complement 
to the English light verbs take, give and make, 
using Levin?s (1993) verb classes to group 
similar nominalizations.  This approach was 
rather successful for take and give, but 
inconclusive for the verb make.  
Our approach can help to develop a resource 
that is useful whether one takes a words-with-
spaces approach or a compositional approach. 
Specifically, for those implementing a words-
with-spaces approach, the resulting PropBank 
annotation can serve as a lexical resource listing 
for LVCs. For those interested in implementing a 
compositional approach the PropBank annotation 
can serve to assist in predicting likely 
combinations. Moreover, information in the 
PropBank Frame Files can be used to generalize 
across classes of nouns that can occur with a 
given light verb with the help of lexical resources 
such as WordNet (Fellbaum, 1998), FrameNet 
(Baker et. al., 1998), and VerbNet (Kipper-
Schuler, 2005) (in a manner similar to the 
approach of Stevenson et al (2004)). 
Acknowledgements 
We also gratefully acknowledge the support of the 
National Science Foundation Grant CISE-CRI 
0709167, Collaborative: A Multi-Representational 
and Multi-Layered Treebank for Hindi/Urdu, and a 
grant from the Defense Advanced Research Projects 
Agency (DARPA/IPTO) under the GALE program, 
DARPA/CMO Contract No HR0011-06-C-0022, 
subcontract from BBN, Inc.  
Any opinions, findings, and conclusions or 
recommendations expressed in this material are those 
of the authors and do not necessarily reflect the views 
of the National Science Foundation. 
89
Reference 
Alsina, A. 1997. Causatives in Bantu and Romance. 
In A. Alsina, J. Bresnan, and P. Sells eds. Complex 
Predicates. Stanford, California: CSLI 
Publications, p. 203-246. 
Baker, Collin F., Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet Project. In 
Proceedings of the 17th International Conference 
on Computational Linguistics (COLING/ACL-98), 
pages 86?90, Montreal. ACL. 
Butt, M. 2004.  The Light Verb Jungle. In G. Aygen, 
C. Bowern & C. Quinn eds.  Papers from the 
GSAS/Dudley House Workshop on Light Verbs. 
Cambridge, Harvard Working Papers in 
Linguistics, p. 1-50.   
Butt, M. 1997. Complex Predicates in Urdu. In A. 
Alsina, J. Bresnan, and P. Sells eds. Complex 
Predicates. Stanford, California: CSLI 
Publications, p. 107-149. 
Fellbaum, Christine, ed.: 1998, WordNet: An 
Electronic Lexical Database, Cambridge, MA: 
MIT Press.  
Grimshaw, J., and A. Mester. 1988. Light verbs and 
?-marking. Linguistic Inquiry 19(2):205?232. 
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Computational 
Linguistics 28:3, 245-288. 
Goldberg, Adele E. 2003.  ?Words by Default: 
Inheritance and the Persian Complex Predicate 
Construction.? In E. Francis and L. Michaelis 
(eds). Mismatch: Form-Function Incongruity and 
the Architecture of Grammar. CSLI Publications.  
84-112. 
Kipper-Schuler, Karin. 2005. VerbNet: A broad 
coverage, comprehensive verb lexicon. Ph.D. 
thesis, University of Pennsylvania. 
Levin, B. 1993. English Verb Classes and 
Alternations: A Preliminary Investigation. 
Chicago: Chicago Univ. Press.  
Meyers, A., R. Reeves, C. Macleod, R. Szekely, V. 
Zielinska, B. Young, and R. Grishman. 2004. The 
NomBank Project: An interim report. In 
Proceedings of the HLT-NAACL 2004 Workshop: 
Frontiers in Corpus Annotation, pages 24- 31, 
Boston, MA. pages 430?437, Barcelona, Spain. 
Mohanan, T. 1997. Multidimensionality of 
Representation: NV Complex Predicates in Hindi. 
In A. Alsina, J. Bresnan, and P. Sells eds. Complex 
Predicates. Stanford, California: CSLI 
Publications, p. 431-471. 
Martha Palmer, Rajesh Bhatt, Bhuvana Narasimhan, 
Owen Rambow, Dipti Misra Sharma, Fei Xia, 
Hindi Syntax: Annotating Dependency, Lexical 
Predicate-Argument Structure, and Phrase 
Structure, In the Proceedings of the 7th 
International Conference on Natural Language 
Processing, ICON-2009, Hyderabad, India, Dec 
14-17, 2009 
Palmer, Martha, Daniel Gildea, and Paul Kingsbury. 
2005. The Proposition Bank: An Annotated Corpus 
of Semantic Roles. Computational Linguistics, 
31(1):71?106. 
Pradhan, Sameer, Kadri Hacioglu, Valerie Krugler, 
Wayne Ward, James H. Martin, Daniel Jurafsky. 
2004. Shallow Semantic Parsing using Support 
Vector Machines. University of Colorado 
Technical Report: TR-CSLR 2003-03. 
Rosen, C. 1997. Auxiliation and Serialization: On 
Discerning the Difference. In A. Alsina, J. 
Bresnan, and P. Sells eds. Complex Predicates. 
Stanford, California: CSLI Publications, p. 175-
202. 
Sag, I., Baldwin, T. Bond, F., Copestake, A., 
Flickinger, D. 2002.  Multiword expressions: A 
pain in the neck for NLP.  In Proceedings of teh 
Third International Conference on Intelligent Text 
processing and Computatinal Linguistics 
(CICLING 2002), p. 1-15, Mexico City, Mexico. 
ACL. 
Stevenson, S., Fazly, A., and North, R. (2004). 
Statistical measures of the semi-productivity of 
light verb constructions. In Proceedings of the 
ACL-04 Workshop on Multiword Expressions: 
Integrating Processing, p. 1?8. 
 
90
Proceedings of the Fifth Law Workshop (LAW V), pages 161?169,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Discourse-constrained Temporal Annotation
Yuping Zhou
Brandeis University
Waltham, MA 02452
yzhou@brandeis.edu
Nianwen Xue
Brandeis University
Waltham, MA 02452
xuen@brandeis.edu
Abstract
We describe an experiment on a temporal or-
dering task in this paper. We show that by se-
lecting event pairs based on discourse struc-
ture and by modifying the pre-existent tem-
poral classification scheme to fit the data bet-
ter, we significantly improve inter-annotator
agreement, as well as broaden the coverage of
the task. We also present analysis of the cur-
rent temporal classification scheme and pro-
pose ways to improve it in future work.
1 Introduction
Event-based temporal inference is a fundamental
natural language technology aimed at determining
the temporal anchoring and relative temporal or-
dering between events in text. It supports a wide
range of natural language applications such as In-
formation Extraction (Ji, 2010), Question Answer-
ing (Harabagiu and Bejan, 2005; Harabagiu and
Bejan, 2006) and Text Summarization (Lin and
Hovy, 2001; Barzilay et al, 2002). Creating con-
sistently annotated domain-independent data suffi-
cient to train automatic systems has been the bot-
tleneck. While low-level temporal annotation tasks
such as identifying events and time expressions are
relatively straightforward and can be done with high
consistency, high-level tasks necessary to eventually
arrange events in a document in a temporal order
have proved to be much more challenging.
Among these high-level tasks, the task of annotat-
ing the temporal relation between main events stands
out as probably the most challenging. This task was
the only task in the TempEval campaigns (Verha-
gen et al, 2009; Verhagen et al, 2010) to deal with
inter-sentential temporal relations, and also the only
one to directly tackle event ordering. The idea is
that events covered in an article are scattered in dif-
ferent sentences, with some, presumably important
ones, expressed as predicates in prominent positions
of a sentence (i.e. the ?main event? of the sentence).
By relating main events from different sentences of
an article temporally, one could get something of a
chain of important events from the article.
This task, in both previously reported attempts,
one for English (Verhagen et al, 2009) and the other
for Chinese (Xue and Zhou, 2010), has the lowest
inter-annotator agreement (at 65%) among all tasks
focusing on annotating temporal relations. Verha-
gen et al (2009) attribute the difficulty, shared by all
tasks annotating temporal relations, mainly to two
factors: rampant temporal vagueness in natural lan-
guage and the fact that annotators are not allowed to
skip hard-to-classify cases.
Xue and Zhou (2010) take a closer look at this
task specifically. They report that part of the diffi-
culty comes from ?wrong? main events (in the sense
that they are not main events in the intended sense)
being selected in the preparation step. This step is a
separate task upstream of the temporal relation task.
The ?wrong? main events produced in this step be-
come part of event pairs whose temporal relation it
makes no sense to annotate, and often is hard-to-
classify. The reason ?wrong? main events get se-
lected is because the selection is based on syntactic
criteria. In fact, these syntactic criteria produce re-
sults so counter-intuitive that this seemingly simple
161
preparation task only achieves 74% inter-annotator
agreement.
Another part of the difficulty comes from me-
chanical pairing of main events for temporal relation
annotation. Simply pairing up main events from ad-
jacent sentences oversimplifies the structure within
an article and is prone to produce hard-to-classify
cases for temporal relation annotation. Both causes
point to the need for a deeper level of text analysis to
inform temporal annotation. For this, Xue and Zhou
(2010) suggest introduction of discourse structure as
annotated in the Penn Discourse Treebank (PDTB)
into temporal relation annotation.
So the previous two reports, taken together, seem
to suggest that the reason this task is especially chal-
lenging is because the difficulty associated with tem-
poral vagueness in natural language, which is shared
by all tasks dealing with temporal relation, is com-
pounded by the problem of having to annotate far-
fetched pairs that should not be annotated, which is
unique for the only task dealing with inter-sentential
temporal relations. These two problems are the foci
of our experiment done on Chinese data.
The paper is organized as follows: In Section 2,
we describe the annotation scheme; in Section 3, we
describe the annotation procedure; in Section 4 we
report and discuss the experiment results. And fi-
nally we conclude the paper.
2 Annotation Scheme
As stated in the introduction, there are two prob-
lems to be addressed in our experiment. The first
problem is that ?wrong? main events get identified
and main events that do not bear any relation are
paired up for temporal annotation. To address this
problem, we follow the suggestion by Xue and Zhou
(2010), namely using a PDTB-style discourse struc-
ture to pick out and pair up main events. We be-
lieve that adopting a discourse-constrained approach
to temporal annotation will not only improve anno-
tation consistency but also increase the Informative-
ness Value of the annotated data, under the assump-
tion that temporal relations that accord with the dis-
course structure are more valuable in conveying the
overall information of a document. Since there is no
Chinese data annotated with PDTB-style discourse
structure available, we have to develop our own. The
scheme for this step is described in Section 2.1.
The second problem is that there is too much tem-
poral vagueness in natural language with respect to
the temporal classification scheme. Since we can-
not change the way natural language works, we try
to model the classification scheme after the data it is
supposed to classify. The scheme for the temporal
annotation is covered in Sections 2.2 and 2.3.
2.1 Discourse-constrained selection of main
events and their pairs
2.1.1 Discourse annotation scheme
The PDTB adopts a lexically grounded approach
to discourse relation annotation (Prasad et al, 2008).
Based on discourse connectives like ?since?, ?and?,
and ?however?, discourse relation is treated as a
predicate taking two abstract objects (AO?s) (such
as events, states, and propositions) as arguments.
For example, in the sentence below, ?since? is the
lexical anchor of the relation between Arg1 and
Arg2 (example from Prasad et al (2007)).
(1) Since [Arg2 McDonald?s menu prices rose
this year], [Arg1 the actual decline may have
been more].
This notion is generalized to cover discourse rela-
tions that do not have a lexical anchor, i.e. im-
plicit discourse relations. For example, in the two-
sentence sequence below, although no discourse
connective is present, a discourse relation similar to
the one in (1) is present between Arg1 and Arg2 (ex-
ample from Prasad et al (2007)).
(2) [Arg1 Some have raised their cash positions to
record levels]. [Arg2 High cash positions help
buffer a fund when the market falls].
Based on this insight, we have fashioned a scheme
tailored to linguistic characteristics of Chinese text.
The linguistic characteristics of Chinese text rele-
vant to discussion here can be illustrated with the
following sentence.
(3) ??
according to reports
?
,
[AO1 ??
Dongguan
??
Customs
?
in total
??e1
accept
??
company
??
contract
??
record
?????
8400 plus
? ]
CL
?[AO2
,
?
compare
??
pilot
?
before
162
?
slight
?e2
EXIST
?? ]
increase
?
,
[AO3??
company
??e3
respond/response
?? ]
well/good
?
,
[AO4??
generally
??e4
acknowledge
?? ]
accept/acceptance
?
?According to reports, [AO1 Dongguan District
Customs acceptede1 more than 8400 records
of company contracts], [AO2 (showinge2) a
slight increase from before the pilot]. [AO3
Companies respondede3 well], [AO4 generally
acknowledginge4 acceptance].?
One feature is that it is customary to have complex
ideas packed into one sentence in Chinese. The sen-
tence above reports on how a pilot program worked
in Dongguan City. Because all that is said is about
the pilot program, it is perfectly natural to include
it all in a single sentence in Chinese. Intuitively
though, there are two different aspects of how the
pilot program worked: the number of records and
the response from the affected companies. To report
the same facts in English, it is probably more natural
to break them down into two sentences, but in Chi-
nese, not only are they merely separated by comma,
but also there is no connective relating them.
Another feature is that grammatical relation be-
tween comma-separated chunks within a sentence
is not always clear. In the above sentence, for in-
stance, although the grammatical relations between
AO1 and AO2, and between AO3 and AO4 are clear
in the English translation (i.e. the first in each pair is
the main clause and the second an adjunct), it is not
at all clear in the original. This is the result of sev-
eral characteristics of Chinese, for example, there is
no inflectional clues on the verb to indicate its gram-
matical function in the sentence.
Based on these features of Chinese text1, we have
decided to use punctuation as the main potential
indicator for discourse relations: the annotator is
asked to judge, at every instance of comma, pe-
riod, colon and semi-colon, if it is an indicator for
discourse relation; if both chunks separated by the
punctuation are projections of a predicate, then there
is a discourse relation between them. Applying this
scheme to the sentence in (3), we have four abstract
objects as marked up in the example.
1A more detailed justification for this scheme is presented in
Zhou and Xue (2011).
To determine the exact text span of each argu-
ment of a relation, we adopt the Minimality Princi-
ple formulated in Prasad et al (2007): only as many
clauses and/or sentences should be included in an ar-
gument selection as are minimally required and suf-
ficient for the interpretation of the relation. Apply-
ing this principle to the sentence in (3), we can de-
limit the three sets of discourse relations as follows:
AO1?AO2, (AO1,AO2)?(AO3,AO4), and AO3?AO4.
2.1.2 Selection and pairing-up of main events
Selection of main events is done on the level of the
simplex abstract object, with one main event per sim-
plex AO. The main event corresponds to the predi-
cate heading the simplex AO. In (3), there are four
simplex AO?s, AO1-4 ( which further form two com-
plex AO?s, (AO1,AO2) and (AO3,AO4)). The an-
chors for the four main events are the underlined
verbs labeled as ?e1-4?.
Pairing up the main events is done on the level
of discourse relation. In the case of a relation
only involving simplex AO?s, the main events of
the two AO?s pair up; in the case of a relation
involving complex AO?s, the discourse relation is
distributed among the simplex AO?s to form main
event pairs. For example, with the discourse relation
(AO1,AO2)?(AO3,AO4), four pairs of main events
are formed: e1?e3, e1?e4, e2?e3, and e2?e4. This
gets tedious fast as the number of simplex AO?s in
a complex AO increases; in this experiment, the an-
notator relies on her discretion in such cases. This
problem should be addressed in a more elegant way
in the future.
It is worth noting that in addition to picking out
right main events and event pairs for temporal anno-
tation, this scheme also broadens the coverage of the
task. In the old scheme based on syntactic criteria,
there is a stipulation: one main event per sentence.
Because the new discourse-constrained scheme is
tailored to the characteristics of Chinese text, it is
able to expose more main events (in the intended
sense) to temporal annotation.
2.2 Classification scheme for temporal relation
annotation
By modifying the six-value scheme used in Tem-
pEval (containing before, overlap, after, before-or-
overlap, overlap-or-after and vague), our classifica-
163
tion scheme has seven values in it: before, overlap,
after, not-before, not-after, groupie, and irrelevant.
2.2.1 The values ?not-before? and ?not-after?
The values ?not-before? and ?not-after? are
equivalent to ?overlap-or-after? and ?before-or-
overlap? in the TempEval scheme. The reason we
made this seemingly vacuous change is because we
found that the old values were used for two different
purposes by annotators. In addition to their intended
use, i.e. to capture indeterminacy between the two
simplex values, they were also used to label a spe-
cific case of ?overlap?. An example of such misuse
of the value ?before-or-overlap? is presented below:
(4) ????
1996
?
year
?
,
[e1 ??]
generate
?
ASP
??
first
?
CL
??
local
??
Chinese
??
judge
?
,
?
until
??
at present
?
,
?
already
?
EXIST
?
close
??
20
?
CL
??
local
??
Chinese
[e2
??]
hold the post
??
judicial
??
official
?
.
?The first local ethnic Chinese judge [e1 assumed]
the office in 1996; up until now, there have been
close to 20 ethnic Chinese locals [e2 holding] the
posts of judicial officials.?
The reason for such use is probably because it repre-
sents two alternative ways of looking at the temporal
relation between the two events : either e1 is before
the later bulk of e2 or e1 overlaps the beginning tip
of e2. To avoid such mis-uses, we made the above
change.
2.2.2 The value ?groupie?
This value is set up for two events whose tempo-
ral relation to each other is unclear, but are known to
happen within the same temporal range. For exam-
ple, the temporal relation between the events repre-
sented by the underlined verbs should be classified
as ?groupie?.
(5) ?
today
?
yesterday
?
two
?
day
?
,
??
Hong Kong
??
SAR
????
CPPCC
??
member
?
also
[e1 ??]
inspect
?
ASP
??
Ningbo
???
development district
?
,
??
Ningbo
???
Xitianxin
??
Textile
????
Ltd.
?
,
[e2 ??]
tour
?
ASP
???
Tianyi Pavilion
?
,
??
Chiang
??
ancestral home
?
.
?Yesterday and today, CPPCC members from Hong
Kong SAR also [e1 visited] Ningbo Development
District and Ningbo Xitianxin Textile Ltd., and [e2
toured] Tianyi Pavilion and the ancestral home of
Chiang Kai-shek.?
In this example, the common range shared by the
two events is expressed in the form of a time ex-
pression, ?????? (?yesterday and today?), but
it does not have to be the case. It can be in the form
of another event (e.g., ????????? (?during
the process of project construction?)), or another en-
tity with a time stamp (e.g., ?????? (?in the
Eighth Five-year Plan period?)).
It should be noted that the linguistic phenomenon
captured by this value can occur in a situation where
the internal temporal relation between two events
can be classified with another value. So ideally, this
value should be set up as a feature parallel to the
existent classification scheme. But due to technical
restrictions imposed on our experiment, we grouped
it with all the others and instructed the annotators
to use it only when none of the five more specific
values applies.
2.2.3 The value ?irrelevant?
We substituted this value for the old one ?vague?
because it is too vague. Anything that cannot fit into
the classification scheme would be labeled ?vague?,
but in fact, some cases are temporally relevant and
probably should be characterized in the classifica-
tion scheme. Case in point are those we now label
?groupie?.
This change reflects our guiding principle for de-
signing the classification scheme. If the relation be-
tween two events is temporally relevant, we should
try to characterize it in some way; if too many rela-
tions are temporally relevant but too vague to fit into
the classification scheme (comfortably), then the ad-
equacy of the scheme is questionable.
2.3 An additional specification: which event?
In addition to the classification scheme, it is also
necessary to specify which event should be con-
sidered for temporal annotation. This question has
164
never been clearly addressed, probably because it
seems self-evident: the event in question is the one
expressed by the event anchor (usually a verb). This
intuitive answer actually accounts for some too-
vague-to-classify cases. In some cases, the event
that is easily annotated (and should be the one being
annotated in our opinion) is not the event expressed
by the verb, as is the case in (6).
(6) ?
PREP
??
absorb
??
foreign business
??
invest
??
aspect
?
,
??
China
?
now
?
already
??
become
??
world
?
POSTP
??
utilize
??
foreign fund
??
most
?
DE
???
developing
???
country.
?With regard to attracting foreign business invest-
ments, China has now become the developing coun-
try that utilizes the most foreign funds in the world.?
This sentence is taken from an article summarizing
China?s economic progress during the ?Eighth Five-
Year Plan? period (from 1991 to 1995). The an-
chor for the main event of the sentence is clearly
???? (?become?), but should the event it repre-
sents, the process of China becoming the develop-
ing country that utilizes the most foreign funds, be
considered for the temporal relation annotation? It
is both counter-intuitive and impractical.
Intuitively, the sentence is a statement of the cur-
rent state with regard to attracting foreign business
investments, not of the process leading up to that
state. If we were to consider the process of ?be-
coming? in relation to other events temporally, we
would have to ask, when are the starting and ending
points of this process? How does one decide when it
is not made clear in the article? One could conceiv-
ably go as far back as to when China did not use one
cent of foreign funds. Should it be restricted to the
?Eighth Five-Year Plan? period since it is the target
period of the whole article? But why use the five-
year period, when there are more specific, syntac-
tically explicit aspectual/temporal modifiers in the
sentence, i.e. ???? (?now already?), to restrict it?
To make use of these in-sentence aspectual/temporal
modifiers, we have to go with our intuition that the
event is the current state of China with regard to uti-
lizing foreign investments, i.e. the temporal location
of the event is at present.
So the event that should be considered for tem-
poral annotation is not the one represented by the
event anchor itself, but rather the one described by
the whole clause/sentence headed by the event an-
chor. This allows all sorts of temporal clues in the
same clause/sentence to help decide the temporal lo-
cation of the event, hence makes the annotation task
easier in many cases.
3 Annotation procedure
The annotation process consists of two separate
stages, with a different annotation procedure in place
for each. The first stage involves only one annotator,
and it deals with picking out pairs of event anchors
based on the discourse relation as described in Sec-
tion 2.1. The output of this stage defines the targets
for the next stage of annotation: temporal relation
annotation. Temporal relation annotation is a two-
phase process, including double-blind annotation by
two annotators and then adjudication by a judge.
With this procedure in place, the results we re-
port in Section 4 are all from the second stage. Two
annotators go through ten weeks of training, which
includes annotating 10 files each week, submitting
them to adjudication, and then attending a training
session at the end of each week. In the training ses-
sion, the judge discusses with the annotators her ad-
judication notes from the previous week, as well as
specific questions the annotators raise.
The data set consists of 100 files taken from the
Chinese Treebank (Xue et al, 2005). The source of
these files is Xinhua newswire. The annotation is
carried out within the confines of the Brandeis An-
notation Tool (BAT)2 (Verhagen, 2010).
4 Evaluation and discussion
Table 1 reports the inter-annotator agreement of tem-
poral annotation, both between the two annotators
(A and B) and between each annotator and the judge
(J), over a training period of ten weeks. Each week,
10 files are assigned, averaging about 315 event
pairs for annotation.
Table 1 shows that annotators have taken up the
temporal annotation scheme fairly quickly, reaching
75% agreement within three weeks. After several
2http://timeml.org/site/bat-versions/bat-redesign
165
Week No. of tokens f(A, B) f(A, J) f(B, J)
1 310 0.4806
2 352 0.6278
3 308 0.7532
4 243 0.7737
5 286 0.8007 0.8601 0.8566
6 299 0.7659 0.8662 0.8896
7 296 0.7973 0.8784 0.8784
8 323 0.7988 0.8978 0.8793
9 358 0.8212 0.9106 0.8966
10 378 0.8439 0.9365 0.8995
Table 1: Inter-annotator agreement over 10 weeks of
training.
weeks of consolidation and fine-tuning, the agree-
ment slowly reaches the lower 80% towards the end
of the 10-week training period. This level of agree-
ment is a substantial improvement over the previ-
ously reported results, at 65%, for both English and
Chinese data (Verhagen et al, 2009; Xue and Zhou,
2010). This indicates that the general direction of
our experiment is on the right track.
Table 2 below is the confusion matrix based on
the annotation data from the final 4 weeks:
a b o na nb g i
a 148 3 19 0 1 0 1
b 0 344 29 1 0 0 7
o 14 10 1354 3 3 2 82
na 0 0 3 3 0 0 0
nb 0 0 1 0 1 0 0
g 2 1 9 0 0 13 1
i 3 7 67 0 0 1 572
Table 2: Confusion matrix on annotation from Weeks
7-10: a=after; b=before; o=overlap; na=not-after;
nb=not-before; g=groupie; i=irrelevant.
The matrix is fairly clean except when the value
?overlap? is concerned. This value really stands out
in more than one way. It is the most nebulous one in
the whole scheme, prone to be confused with all six
other values. In particular, it is most likely to be con-
fused with the value ?irrelevant?. It is also the most
used value among all seven values, covering roughly
half of the tokens. We will discuss this value in more
detail in Section 4.2 below.
The value ?groupie? may also seem troublesome
if we look at mis-classification as a percentage of its
total occurrences, however, it may not be as bad as it
seems. As pointed out in Section 2.2.2, despite the
fact that the linguistic phenomenon this value cap-
tures can, and does, co-occur with temporal relations
represented by other values, we had to set it up as an
opposing value to the rest due to technical restric-
tions. If/when this value is set up as a stand-alone
feature to capture the linguistic phenomenon fully,
the percentage of mis-classification should drop sig-
nificantly because the number of total occurrences
will increase dramatically.
The overall distribution of values shown in Table
2 is very skewed. At one end of the distribution
spectrum is the value ?overlap?, covering half of
the data; at the other end are the values ?not-before?
and ?not-after?, covering less than 0.3% of the token
combined. It raises the question if such a classifica-
tion scheme is well-designed to produce data useful
for machine learning.
To shed light on what is behind the numbers and
to uncover trends that numbers do not show, we also
take a closer look at the annotation data. Three is-
sues stand out.
4.1 Event anchor
In our current scheme, effort is made to pick out the
predicate from a clause as the event anchor for tem-
poral annotation. Our experiment suggests maybe
this step should be skipped since it, in practice, un-
dermines a specification of the scheme. The specifi-
cation is that the event to be considered for temporal
annotation is the one being described by the whole
clause, but the practice of displaying a mere word to
the annotator in effect instructs the annotator to con-
centrate on the word itself, rather than the clause.
Despite repeated reminder during training sessions,
the suggestive power of the display still sometimes
gets the upper hand. (7) presents such an example
concerning e1 and e2.
(7) ?
PREP
?
this
??
period
?
,
??
West Africa
??
peacekeeping
??
force
?
once
[e1 ??]
dispatch
???
fighter jet
??
bomb
??
rebel
??
position
?
,
[e2 ??]
bomb-dead
??
rebel
?
about
???
50 plus
166
??
CL
?During this period, West African Peacekeeping
Force [e1 dispatched] fighter jets and bombed rebel
positions, [e2 killing] about 50 rebel troops.?
One annotator classified the relation as ?before?, ob-
viously thinking of the event of dispatching fighter
jets as e1; had he considered the event of dispatch-
ing fighter jets and bombing the rebel positions, the
event being described by the clause, the value would
have easily been ?overlap?.
Since displaying the single-word event anchor
sometimes leads annotators astray, this step proba-
bly should be skipped. Doing so also simplifies the
annotation process.
4.2 The value ?overlap?
As pointed out above, the value ?overlap? is quite
a troubling character in the classification scheme: it
is both the most-used and probably the least well-
defined. Annotation data show that when it is con-
fused with ?after?, ?before?, ?not-after?, and ?not-
before?, it usually involves a perceptually punc-
tual event (?pp-event? henceforth) and a perceptu-
ally lasting event (?pl-event? henceforth), and the is-
sue is whether the pp-event coincides with one of the
temporal edges of the pl-event. If it does, then the
value is ?overlap?; otherwise, it is ?after?/?before?.
And on top of it is the factor of how sure one is
of the issue: if one is sure, either way, the value
is ?overlap?/?after?/?before?; otherwise, it is ?not-
after?/?not-before?. Below is an example on which
the two annotators disagree as to whether the rela-
tion between e1 and e2 should be classified as ?be-
fore? or ?overlap?.
(8) ??
in addition
?
,
??
Brazil
??
woman
???
national team
?
PREP
??
S. America
???
soccer match
?
POSTP
?
,
[e1 ??]
sweep
??
thousand-troop
?
like
?
roll
?
mat
?
,
[e2 ??]
ascend
?
ASP
??
champion
??
throne
?
.
?In addition, in the South America Cup, Brazil-
ian Women?s national team totally [e1 annihilated]
all their opponents and [e2 ascended] the throne of
champion.?
In this example, e2 is the pp-event and e1 is the pl-
event. Depending on when one thinks e2 happened,
either as soon as the last match ended or at the later
medal ceremony, (and if the former, whether there is
temporal overlap between e1 and e2), it is classified
as either ?before? or ?overlap; and if one is unsure,
it can be classified as ?not-after?.
Such cases again raise the same question as the
drastically uneven distribution of values shown in
Table 2: Does the current classification scheme slice
the temporal pie the right way? Let us make a poster
child out of ?overlap?: it seems to both impose too
stringent a condition and not make enough distinc-
tion. It imposes too stringent a condition on those
cases like (8) to which whether there is temporal
overlap seems beside the point. At the same time,
it does not make enough distinction for cases like
(4), in which an event does share one edge of an-
other event temporally: once such cases are classi-
fied as ?overlap?, the specific information regard-
ing the edge is lost. Such information could be very
useful in temporal inference. Since it is infeasible
to annotate the temporal relation between all events
in an article, temporal inference is needed to expand
the scope of temporal annotation. For example, if it
is known from annotation that e1 is before e2 and
e2 is before e3, then it can be inferred e1 is before
e3. In the case of ?overlap?, whenever it is one of
the premises, no inference can be made, but if the
?edge? information is supplied, some inferences are
possible.
To make finer-grained distinctions in the classifi-
cation scheme runs counter to the conventional wis-
dom that a coarser-grained scheme would do a bet-
ter job handling vagueness. But our experiment has
proven the conventional wisdom wrong: our seven-
value system achieved much higher agreement than
the old six-value system. So the key is not fewer, but
better, distinctions, ?better? in the sense that they
characterize the data in a more intuitive and insight-
ful way. Temporal relation in natural language is
?too? vague only when we judge it against a sys-
tem of temporal logic, in fact, we think the right
word to describe temporal relation in natural lan-
guage is ?flexible?: it is as precise as the situation
calls for. To characterize the flexibility better, for
starters, ?overlap? needs to be restructured for rea-
sons put forth above, and ?not-before? and ?not-
167
after? should be discarded since they obviously do
not carry weight.
4.3 Objective vs. subjective temporal reference
A major contributor to uncertainty and disagreement
in annotation is subjective temporal reference. Sub-
jective temporal reference is made based on the au-
thor?s perspective of the temporal axis, for example,
???? (?today?), ???? (?at present), and ????
(?past?). In this group, references with a fixed span
do not constitute a problem once the point of utter-
ance is determined (e.g. literal use of ?today?, ?this
month?); it is those with an elastic temporal span that
cause disagreement. For example, ?at present? can
have a span of a second, or several minutes, or a cou-
ple of hours, or even years depending on the context.
When an event modified with this type of tempo-
ral expression is paired with another event modified
with direct reference to a point/span on the tempo-
ral axis (i.e. with an objective reference), annotation
becomes tricky. The event pair e1-e2 in the two-
sentence sequence below is such an example.
(9) ??
past
?
,
?
PREP
??
Yangtze River
?
POSTP
?
build
??
bridge
?
be
?
CL
????
national affair
?
,
??
nowadays
??
almost
[e1
??]
become
????
common scene.
??????
1992-year,
??
Jiangsu
???
Yangzhong County
??
farmer
[e2 ??]
raise funds
??
build-finish
?
ASP
??
Yangzhong
??
Yangtze
??
Bridge
?
,
?
and
??
Hubei
?
DE
??
Chibi
??
Yangtze
??
Bridge
?
total
??
invest
???
300 million plus
?
Yuan
?
,
??
all
?
depend
??
private
??
raise funds
??
build-finish
?
.
?In the past, building a bridge on Yangtze River was
a national affair, nowadays it almost [e1becomes]
a common scene. In 1992, farmers in Yangzhong
County, Jiangsu Province [e2raised] funds and com-
pleted Yangzhong Yangtze Bridge, while Chibi
Yangtze Bridge in Hubei Province cost more than
300 million Yuan, all from private fund-raising.?
This is taken from a piece written in 1997. In the
context, it is clear that the contrast is between the
situation before the opening-up of China and the sit-
uation about 20 years later. So it is reasonable to as-
sume that the year 1992 falls inside the span of what
the author considered nowadays; at the same time, it
seems also reasonable to assume a narrow interpre-
tation of ???? (?nowadays?) that does not include
the year 1992 in the span. These two interpretations
would result in ?overlap? and ?after? respectively,
and actually did so in our experiment.
There are also extreme cases in which objective
and subjective temporal references come in direct
conflict. For example,
(10) ?
while
??
reporter
[e1 ??]
ask about
?
China
?
Russia
??
relationship
?
DE
??
status
?
and
??
cooperation
??
prospect
?
when
?
,
???
Jiang Zemin
??
President
[e2 ?]
say
?...
, ...
?When a reporter [e1 asked] about the status of
China-Russia relationship and the prospects for co-
operation, President Jiang Zemin [e2 said], ...?
The relation between e1 and e2 is before based
on objective reference, but overlap according to
the subjective reference, indicated by ??..??
(?when?). This problem should be factored in when
a new classification scheme is designed.
5 Conclusions
In this paper, we have described an experiment that
focuses on two aspects of the task of annotating
temporal relation of main events: annotation tar-
get selection and a better-fitting temporal classifica-
tion scheme. Experiment results show that selecting
main event pairs based on discourse structure and
modeling the classification scheme after the data im-
proves inter-annotator agreement dramatically. Re-
sults also show weakness of the current temporal
classification scheme. For that, we propose a re-
structuring along the lines of what this experiment
has proven working: making more intuitive and in-
sightful distinctions that characterize the data bet-
ter. This direction can be taken to improve other
high-level temporal annotation tasks that have been
plagued by the same ?vagueness? problem.
Acknowledgments
This work is supported by the National Science
Foundation via Grant No. 0855184 entitled ?Build-
ing a community resource for temporal inference
168
in Chinese?. All views expressed in this paper are
those of the authors and do not necessarily represent
the view of the National Science Foundation.
References
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2002. Inferring strategies for sentence ordering
in multidocument news summarization. Journal of Ar-
tificial Intelligence Research, 17:35?55.
Sanda Harabagiu and Cosmin Adrian Bejan. 2005.
Question Answering Based on Temporal Inference. In
Proceedings of the AAAI-2005 Workshop on Inference
for Textual Question Answering, Pittsburgh, Pennsyl-
vania.
Sanda Harabagiu and Cosmin Adrian Bejan. 2006. An
Answer Bank for Temporal Inference. In Proceedings
of LREC 2006, Genoa, Italy.
Heng Ji. 2010. Challenges from information extrac-
tion to information fusion. In Proceedings of COLING
2010, pages 507?515, Beijing, China, August.
Chin-Yew Lin and Eduard Hovy. 2001. Neats: A mul-
tidocument summarizer. In Proceedings of the Docu-
ment Understanding Workshop.
Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh, Alan
Lee, Aravind Joshi, Livio Robaldo, and Bonnie Web-
ber, 2007. The Penn Discourse Treebank 2.0 Annota-
tion Manual. The PDTB Research Group, December.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse Treebank 2.0.
In Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC 2008).
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James Puste-
jovsky. 2009. The TempEval Challenge: Identifying
Temporal Relation in Text. Language Resources and
Evaluation, 43(1):161?179.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 57?62, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Marc Verhagen. 2010. The Brandeis Annotation Tool.
In Language Resources and Evaluation Conference,
LREC 2010, pages 3638?3643, Malta.
Nianwen Xue and Yuping Zhou. 2010. Applying Syn-
tactic, Semantic and Discourse Constraints to Chinese
Temporal Annotation. In Proceedings of COLING
2010, pages 1363?1372, Beijing, China, August.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Yuping Zhou and Nianwen Xue. 2011. A PDTB-inspired
Discourse Annotation Scheme for Chinese. Submitted
to EMNLP 2011.
169
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 88?97,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
Improving MT Word Alignment Using Aligned Multi-Stage Parses
Adam Meyers?, Michiko Kosaka?, Shasha Liao? and Nianwen Xue?
? New York University, ?Monmouth University, ?Brandeis University
Abstract
We use hand-coded rules and graph-aligned
logical dependencies to reorder English text
towards Chinese word order. We obtain a
1.5% higher F-score for Giza++ compared to
running with unprocessed text. We describe
this research and its implications for SMT.
1 Introduction
Some statistical machine translation (SMT) systems
use pattern-based rules acquired from linguistically
processed bitexts. They acquire these rules through
the alignment of a parsed structure in one language
with a raw string in the other language (Yamada and
Knight, 2001; Shen et al, 2008) or the alignment
of source/target language parse trees (Zhang et al,
2008; Cowan, 2008). This paper shows that ma-
chine translation (MT) can also benefit by aligning a
?deeper? level of analysis than parsed text, which in-
cludes semantic role labeling, regularization of pas-
sives and wh constructions, etc. We create GLARF
representations (Meyers et al, 2009) for English and
Chinese sentences, in the form of directed acyclic
graphs. We describe two graph-based techniques
for reordering English sentences to be closer to that
of corresponding Chinese sentences. One technique
is based on manually created rules and the other is
based on an automatic alignment of GLARF repre-
sentations of Chinese/English sentences. After re-
ordering, we align words of the reordered English
with the words of the Chinese, using the Giza++
word aligner(Och and Ney, 2003). For both tech-
niques, the resulting alignment has a higher F-score
than Giza++ on raw text (a 0.7% to 1.5% absolute
improvement). In principle, our reordered text can
be used to improve any Chinese/English SMT sys-
tem for which Giza++ (or other word aligners) are
part of the processing pipeline.
These experiments are a first step in using
GLARF-style analyses for MT, potentially improv-
ing systems that already perform well with aligned
text lacking large gaps in surface alignment. We hy-
pothesize that SMT systems are most likely to ben-
efit from deep analysis for structures where source
and target language word order differs the most. We
propose using deep analysis to reorder such struc-
tures in one language to more closely reflect the
word order of the other language. The text would be
reordered at two stages in an SMT system: (1) prior
to acquiring a translation model; and (2) either prior
to translation (if source text is reordered) or after
translation (if target text is reordered). Our system
moves large constituents (e.g., noun post-modifiers)
to bring English word order closer to that of parallel
Chinese sentences. This improves word alignment
and is likely to improve SMT.
For this work we use two English/Chinese bitext
corpora developed by the Linguistic Data Consor-
tium (LDC): the Tides FBIS corpus and the GALE
Y1 Q4 Chinese/English Word-Alignment corpus.
We used 2300 aligned sentences from FBIS for de-
velopment purposes. We divided the GALE corpus
into into a 3407 sentence development subcorpus
(DEV) and a 1505 sentence test subcorpus (TEST).
We used the LDC?s manual alignments of the FBIS
corpus to score these data.
88
2 Related Work in SMT
Four papers stand out as closely related to the
present study. (Collins et al, 2005; Wang et al,
2007) describe experiments which use manually cre-
ated parse-tree-based rules to reorder one side of
a bitext: German/English in (Collins et al, 2005)
and English/Chinese in (Wang et al, 2007). Both
achieve BLEU score improvements for SMT: 25.2%
to 26.8% for (Collins et al, 2005) and 28.52 to 30.86
for (Wang et al, 2007). (Wang et al, 2007) uses
rules very similar to our own as they use the same
language pair, although they reorder the Chinese,
whereas we reorder the English. The most signifi-
cant differences between our research and (Collins
et al, 2005; Wang et al, 2007) are: (1) our manual
rules benefit from a level of representation ?deeper?
than a surface parse; and (2) In addition to the hand-
coded rules, we also use automatic alignment-based
rules. (Wu and Fung, 2009) uses PropBank role la-
bels (Palmer et al, 2005) as the basis of a second
pass filter over an SMT system to improve the BLEU
score from 42.99 to 43.51. The main similarity to
the current study is the use of a level of represen-
tation that is ?deeper? than a surface parse. How-
ever, our application of linguistic structure is more
like that of (Wang et al, 2007) and our ?deep? level
connects all predicates and arguments in the sen-
tence, regardless of part of speech, rather than just
connecting verbs to their arguments. (Bryl and van
Genabith, 2010) describes an open source LFG F-
structure alignment tool with an algorithm similar to
our previous work. They evaluate their alignment
output on 20 manually-aligned German and English
F-structures. They leave the impact of their work on
MT to future research.
In addition to these papers, there has also been
some work on rule-based reordering preprocessors
to word alignment based on shallower linguistic in-
formation. For example (Crego and Marin?o, 2006)
reorders based on patterns of POS tags. We hypoth-
esize that this is similar to the above approaches in
that patterns of POS tags are likely to simulate pars-
ing or chunking.
3 Preparing the Data
The two stage parsers of previous decades (Hobbs
and Grishman, 1976) generated a syntactic repre-
sentation analogous to the (more accurate) output
of current treebank-based parsers (Charniak, 2001)
and an additional second stage output that regular-
ized constructions (passive, active, relative clauses)
to representations similar to active clauses with no
gaps, e.g., The book was read by Mary was given a
representation similar to that of Mary read the book.
Treating the active clause as canonical provides a
way to reduce variation in language and thus, mak-
ing it easier to acquire and apply statistical informa-
tion from corpora?there is more evidence for partic-
ular statistical patterns when applications learn pat-
terns and patterns more readily match data.
Two-stage parsers were influenced by linguistic
theories (Harris, 1968; Chomsky, 1957; Bresnan and
Kaplan, 1982) which distinguish a ?surface? and a
?deep? level. The deep level neutralizes differences
between ways to express the same meaning?a pas-
sive like The cheese was eaten by rats was analyzed
in terms of the active form Rats ate the cheese. Cur-
rently ?semantic parsing? refers to a similar repre-
sentation, e.g., (Wagner et al, 2007) or our own
GLARF (Meyers et al, 2009). However, the term is
also used for semantic role labelers (Gildea and Ju-
rafsky, 2002; Xue, 2008), systems which typically
label semantic relations between verbs and their ar-
guments and rarely cover arguments of other parts
of speech. Second stage semantic parsers like our
own, connect all the tokens in the sentence. Aligned
text processed in this way can (for example) repre-
sent differences in English/Chinese noun modifier
order, including relative clauses. In contrast, few
role labelers handle noun modifiers and none han-
dle relative clauses. Below, we describe the GLARF
framework and our system for generating GLARF
representations of English and Chinese sentences.
For each language, we combine several types of
information which may include: named entity (NE)
tagging, date/number regularization, recognition of
multi-word expressions (the preposition with respect
to, the noun hand me down and the verb ad lib),
role labels for predicates of all parts of speech, regu-
larizing passives and other constructions, error cor-
rection, among other processes into a single typed
feature structure (TFS) representation. This TFS
is converted into a set of 25-tuples representing
dependency-style relations between pairs of words
in the sentence. Three types of dependencies are
89
n1
know
SBJ OBJ
n3
of
n5 OBJ
SBJ OBJ
N?POS
COMP
n4
the
Q?POS
n2? n3?
n6?
n1?
I rules
tennis
n6
n2
 
 
  
Figure 1: Word-Aligned Logic1 Dependencies
represented: surface dependencies (close to the level
of the parser), logic1 dependencies (reflecting var-
ious regularizations) and logic2 dependencies (re-
flecting the output of a PropBanker, NomBanker
and Penn Discourse Treebank transducer).(Palmer
et al, 2005; Xue and Palmer, 2003; Meyers et al,
2004; Miltsakaki et al, 2004) The surface depen-
dency graph is a tree; The logic1 dependency graph
is an directed acyclic graph; and The logic2 depen-
dency graph is a directed graph with cycles, cover-
ing only a subset of the tokens in the sentence. For
these experiments, we focus on the logic1 relations,
but will sometimes use the surface relations as well.
Figure 1 is a simple dependency-based logic1 repre-
sentation of I know the rules of tennis and its Chi-
nese translation. The edge labels name the relations
between heads and dependents, e.g., I is the SBJ of
know and the dashed lines indicate word level corre-
spondences. Each node is labeled with both a word
and a unique node identifier (n1, n1?, etc.)
The English system achieves F-scores for logic1
dependencies on parsed news text in the 80?90%
range and the Chinese system achieves F-scores in
the 74?84% range, depending on the complexity of
the text. The English system has been created over
the course of about 9 years, and consequently is
more extensive than the Chinese system, which has
been created over the past 3 years. The systems are
described in more detail in (Meyers et al, 2009).
The GLARF representations are created in a se-
ries of steps involving several processors. The En-
glish pipeline includes: (1) dividing text into sen-
tences; (2) running the JET NE tagger (Ji and Gr-
ishman, 2006); (3) running scripts that clean up data
(to prevent parser crashes); (4) running a parser (cur-
rently Charniak?s 2005 parser based on (Charniak,
2001)); (5) running filters that: (a) correct com-
mon parsing errors; (b) merge NE information with
the parse, resolving conflicts in constituent bound-
aries by hand-coded rules; (c) regularize numbers,
dates, times and holidays; (d) identify heads and
label relations between constituents; (e) regularize
text grammatically (filling empty subjects, resolv-
ing relative clause and Wh gaps, etc.); (f) mark con-
junction scope; (g) identify transparent constituents
(e.g., recognizing, that A variety of different peo-
ple has the semantic features of people (human), not
those of variety, the syntactic head of the phrase.);
among other aspects. The Chinese pipeline is simi-
lar, except that it includes the LDC word segmenter
and a PropBanker (Xue, 2008). Also, the regulariza-
tion routines are not as completely developed, e.g.,
relative clause gaps and passives are not handled
yet. The Chinese system currently uses the Berke-
ley parser (Petrov and Klein, 2007). Each of these
pipelines derives typed feature structure representa-
tions, which are then converted into the 25 tuple rep-
resentation of 3 types of dependencies between pairs
of tokens: surface, logic1 and logic2.
To insure that the logic1 graphs are acyclic, we as-
sume that certain edges are surface only and that the
resulting directed acyclic graphs can have multiple
roots. It turns out that the multiple rooted cases are
mostly limited to a few constructions, the most com-
mon being parenthetical clauses and relative clauses.
A parenthetical clause takes the main clause as an
argument. For example, in The word ?potato?, he
claimed, is spelled with a final ?e?., the verb claimed,
takes the entire main clause as an argument, we as-
sume that he claimed is a dependent on the main
verb (is) spelled labeled PARENTHETICAL in our
surface dependency structure, but that the main verb
(is) spelled is a dependent of the verb claimed in
our logic1 structure, labeled COMPLEMENT. Thus
the logic1 surface dependency structure have dis-
tinct roots. In a relative clause, such as the book that
I read?, we assume that the clause that I read is a de-
pendent on the noun book in our surface dependency
structure with the label RELATIVE, but book is a de-
pendent on the verb read in our logic1 dependency
structure, with the label OBJ. This, means that our
logic1 dependency graphs for sentences containing
relative clauses are multi-rooted. One of the roots is
the same as the root of the surface tree and the other
root is the root of the relative clause graph (a rela-
90
tive pronoun or a main verb). Furthermore, there is
a surface path connecting the relative clause root to
the rest of the graph. Noncyclic graph traversal is
possible, provide that: (1) we use the surface path to
enter the graph representing the relative clause ? oth-
erwise, the traversal would skip the relative clause;
and (2) we halt the traversal if we reach this path a
second time ? this avoids traversing down an end-
less path. The parenthetical and relative clause are
representative of the handful of cases in which naive
representations would introduce loops. All cases of
which we are aware have the essential properties of
one of these two cases: (1) either introducing a dif-
ferent single root of the clause; or (2) introducing an
additional root that can be bridged by a surface path.
4 Manual Reordering Rules
We derived manual rules for making the English
Word Order more like the Chinese by manually in-
specting the data. We inspected the first 100-200
sentences of the DEV corpus by first transliterating
the Chinese into English ? replaced each Chinese
word with the aligned English counterpart. Several
patterns emerged which were easy to formalize into
rules in the GLARF framework. These patterns were
verified and sometimes generalized through discus-
sions with native Chinese speakers and linguists.
Our rules, similar to those of (Wang et al, 2007) are
as follows (results are discussed in section 6): (1)
Front a post-nominal PP headed by a preposition in
the list {of, in, with, about)}. (2) Front post-nominal
relative clause that begins with that or does not have
any relative pronoun, such that the main predicate is
not a copula plus adjective construction. (3) Front
post-nominal relative clause that begins with that or
has no relative pronoun if the main predicate is a
copula+adjective construction which is not negated
by a word from the set {no neither nor never not
n?t}. (4) Front post-nominal reduced relative in the
form of a passive or adjectival phrase. (5) Move ad-
verbials more than and less than after numbers that
they modify. (6) Move PPs that post-modify adjec-
tives to the position before the adjective. (7) Move
subordinate conjunctions before and after to the end
of the clause that they introduce. (8) Move an ini-
tial one-word-long title (Mr., Ms., Dr., President) to
the end of the name. (9) Move temporal adverbials
(adverb, PP, subordinate clause that is semantically
temporal) to pre-verb position.
5 Automatic Node Alignment and its
Application for Word Alignment
In this experiment, we automatically derive re-
orderings of the English sentences from an align-
ment between nodes in logic1 dependency graphs
for the English (source) and Chinese (target) sen-
tences. Source/Target designations are for conve-
nience, since the direction of MT is irrelevant.
We define an alignment as a partial function from
the nodes in the source graph and the nodes in the
target graph. We, furthermore, assume that this map-
ping is 1 to 1 for most node pairs, but can be n to 1
(or 1 to n). Furthermore, we allow some nodes, in
effect, to represent multiple tokens. These are iden-
tified as part of the GLARF analysis of a particular
sentence string and reflect language-specific rules.
Thus, for our purposes, a mapping between a source
and target node, each representing a multi-word ex-
pression is 1 to 1, rather than N to N.
We identify the following types of multi-word ex-
pressions for this purpose: (a) idiomatic expressions
from our monolingual lexicons, (b) dates, (c) times
(d) numbers and (e) ACE (Grishman, 2000) NEs.
Dates, holidays and times are regularized using ISO-
TimeML, e.g., January 3, 1977 becomes 1977-03-01
and numbers are converted to Arabic numbers.
5.1 ALIGN-ALG1
This work uses a modified version of ALIGN-
ALG1, a graph alignment algorithm we previously
used to align 1990s-style two-stage parser output for
MT experiments. ALIGN-ALG1 is an O(n2) algo-
rithm, n is the maximum number of nodes in the
source and target graphs (Meyers et al, 1996; Mey-
ers et al, 1998). Given Source Tree T and Target
Tree T ?, an alignment(T, T ?) is a partial function
from nodes N in T to nodes N ? in T ?. An exhaus-
tive search of possible alignments would consider all
non-intersecting combinations of the T ?T ? pairs of
source/target nodes ? There are at most T ! such pair-
ings where T >= T ?.1 However, ALIGN-ALG1 as-
sumes that some of these pairings are unlikely, and
1This ignores N to 1 matches, which we allow, although rel-
atively rarely.
91
favors pairings that assume the structure of the trees
correspond more closely. In particular, it is assumed
that ancestor nodes are more likely to match if most
of their descendant nodes match as well.
ALIGN-ALG1 finds the highest scoring align-
ment, where the score of an alignment is the sum
of the scores of the node pairs in the partial func-
tion. The score for each node pair (n, n?) partially
depends on the scores of a mapping from the chil-
dren of n to the children of n?. While the process
of calculating the scores is recursive, it can be made
efficient using dynamic programming.
ALIGN-ALG1 assumes that we align r and r?,
the roots of T and T ?. Calculating the scores for r
and r?, entails calculating the scores of pairs of their
children, and by extension all mappings from N to
N ? that obey the dominance preserving constraint:
Given nodes n1 and n2 in N and nodes n?1 and n?2
in N ?, where all 4 nodes are part of the alignment,
it cannot be the case that: n1 dominates n2, but
n?1 does not dominate n?2. Here, dominates means
is an ancestor in the dependency graph. ALIGN-
ALG1 scores each pair of nodes using the formula:
Score(n, n?) = Lex(n, n?) + ChildV al(n, n?),
where Lex(n, n?) is a score based on matching the
words labeling nodes n and n?, e.g., the score is 1 if
the pair is found in a bilingual dictionary and 0 oth-
erwise. Given n has children c0, . . . , ci and n? has
children c?0, . . . , c?j , to calculate ChildVal: (1) Cre-
ate Child-Matrix, a (i+ 1)? (j + 1) matrix (2) Fill
every position (1 <= x <= i, 1 <= x? <= j)
with Score(x, x?) (3) Fill every position (i+1, 1 <=
x? <= j) with Score(n, x?) minus a penalty (e.g.,
- .1) for collapsing an edge. This treats n? and x?
as a single unit, matched to n.2 (4) Fill every po-
sition (1 <= x <= i, j+1) with Score(x, n?) mi-
nus a penalty for collapsing an edge. Thus n + x is
paired with n?. (5) Set (i+1,j+1) to ??. Collapsing
both source and target edges is not permitted. (6) For
all sets of positions in the matrix such that no node
or column is repeated, select the set with the high-
est aggregate score. The aggregate score is the nu-
meric value of ChildV al(n, n?). If (n,n?) is part of
the alignment that is ultimately chosen, this choice
of node pairs is also part of the alignment. There
2The slight penalty represents that collapsing edges compli-
cate the analysis and is thus disfavored (Occam?s Razor).
are at most max(i + 1, j + 1)! possible pairings.
Rather than calculating them all, a greedy heuristic
can reduce the calculation time with minimal effect
on accuracy: the highest scoring cell in the matrix is
chosen first, conflicting cells are eliminated, the next
highest scoring cell is chosen, etc.
Consider the example in Figure 1, assum-
ing the dashed lines connect lexical matches
(the function LEX returns 1 for these node
pairs). Where n1 and n1? are the roots,
Score(n1, n1?) = 1 + ChildV al(n1, n1?). Cal-
culating ChildV al(n1, n1?) requires a recursive
descent down the pairs of nodes, until the bot-
tom most pair is scored. Score(n6, n6?) = 1.
Score(n5, n6?) = 0 + .9 (derived by collaps-
ing an edge and subtracting a penalty of .1).
Score(n3, n3?) = 1 + .9 = 1.9. Score(n2, n2?) =
1. ChildV al(n1, n1?) = 1 + 1.9 = 2.9. Thus
Score(n1, n1?) = 3.9. The alignment includes:
(n1, n1?), (n2, n2?), (n3, n3?), (n5, n6?), (n6, n6?).
The collapsing of edges helps recognize cases
where multiple predicates form substructures, e.g.,
take a walk, is angry, etc. in one tree can map to sin-
gle verbs in the other tree, allowing outgoing edges
from walk or angry to map to outgoing edges of the
corresponding verb, e.g., the agent and goal of John
walked to the store could map to the agent and goal
of John took a walk to the store.
In practice, ALIGN-ALG1 falls short because:
(1) Our translation dictionary does not have suffi-
cient coverage for the algorithm to perform well; (2)
The assumption that the roots of both graphs should
be aligned is often false. Parallel text often reflects
a dynamic, rather than a literal translation. In one
pair of aligned sentences in the FBIS corpus, the
English phrase the above mentioned requests cor-
responds to: meaning these re-
quests of Chen Shui-bian ? Chen Shui-bian has no
counterpart in the English. Parts of translations can
be omitted due to: (a) the discretion of the trans-
lators, (b) the expected world knowledge of partic-
ular language communities, (c) the cultural impor-
tance of particular information, etc.; (3) Violations
of the dominance-preserving constraint exist. The
most common type that we have observed consists
of sequences of transparent nouns and of (e.g., se-
ries of) in English corresponding to quantifiers in
92
Chinese ( ). Thus the head of the English con-
struction corresponds to the dependent of the Chi-
nese construction and vice versa.
5.2 Lexical Resources
Our primary bilingual Chinese/English dictionary
(LEX1) had insufficient coverage for ALIGN-ALG1
to be effective. LEX1 is a merger between:
The LDC 2002 Chinese-English Dictionary and
HowNet. In addition, we manually added additional
translations of units of measure from English. We
also used NEDICT, a name translation dictionary (Ji
et al, 2009) and AUTODICT, English/Chinese word
to word pairs with high similarity scores taken from
MT phase tables created as part of the (Zhang et al,
2007) system. The NEDICT was used both for pre-
cise matches and partial matches (since, NEs can
often be synonymous with substrings of NEs). In
addition, we used some WordNet (Fellbaum, 1998)
synonyms of English to expand the coverage of all
the dictionaries, allowing English words to match
Chinese word translations of their synonyms. We
allowed additional matches of function words that
served similar functions in the two languages includ-
ing: copulas, pronouns and determiners.
Finally, we use a mutual information (MI) based
approach to find further lexical information. We run
our alignment program over the corpus two times,
the first time, we acquire statistical information
useful for generating a MI-based score. This score
is used as a lexical score on the second pass for
items that do not match any of the dictionaries. On
the first pass, we tally the frequency of each pair
of source/target words s and t, such that neither
s, nor t are matched lexically to any other item
in the sentence. We, furthermore, keep track of
the number of times each word appears in the
corpus and the number of times each word appeared
unaligned in the corpus. We tally MI as follows:
pair?frequency2
1+(source?word?frequency?target?word?frequency)
One is added to the denominator as a variation on
add-one smoothing (Laplace, 1816), intended to
penalize low frequency scores. We calculate this
score in two ways: (a) using the global frequencies
of the source and target words; and (b) using the
frequency these words were unaligned. The larger
of the two scores is the one that is actually use.
Different lexicons are given different weights.
Matches between words in the hand-coded transla-
tion dictionary and NEDICT are given a score of
1.0. Matches in other dictionaries are allotted lower
scores to represent that these are based on automati-
cally acquired information, which we assume is less
reliable than manually coded information.3
5.3 ALIGN-ALG2
With ALIGN-ALG2, we partially address two lim-
itations of ALIGN-ALG1: (1) the assumption that
the roots of source and target graph are aligned;
and (2) the dominance-preserving constraint. Ba-
sically, we assume that structural similarity is fa-
vored, but not necessarily at the global level. Thus
it is likely that many subparts of corresponding trees
correspond closely, but not necessarily the highest
nodes in the trees.
We use ALIGN-ALG1 to align every possible pair
of S source nodes and T target nodes. Then we look
for P , the highest scoring node pair of all SXT
pairs. P and all the pairs of descendants that are
used to derive this score (the highest scoring pairs
of children, grand children, etc.) become the initial
output. Then we find all unmatched source and tar-
get children, and look up the highest scoring pair of
these nodes, and we repeat the process, adding the
resulting node pairs to the output. We continue to
repeat this process until either all the nodes are in-
cluded in the output or there is no remaining pair
with a score above a threshold score (we leave au-
tomatic methods of tuning this score to future work
and preliminarily have set this parameter to .3). This
means that: 1) some parts of the graphs are left un-
aligned (the alignment is a partial mapping); 2) the
alignment is more resilient to misalignment caused
by differences in graph structure, regardless of the
reason; and 3) the alignment may be between pair
of unconnected graphs, each containing subsets of
nodes and edges in the source and target graphs.
While more complex than ALIGN-ALG1, ALIGN-
ALG2 performs relatively quickly. After one itera-
tion using ALIGN-ALG1, scores are looked up, not
recalculated.
3Current informal weights of .2 to .6 may be replaced with
automatically tuned weights (hill-climbing, etc.) in future work.
93
5.4 Treating Multiple Tokens as One
In some cases, parsing and segmentation of text
can be corrected through minor modifications to our
alignment routine. Similarly, we use bilingual lex-
ical information to determine that certain other ad-
jacent tokens should be treated as single words for
purposes of alignment.
Given a language for which segmentation is a
common source of processing error (Chinese), if a
token is unaligned, we check to see whether subdi-
viding the token into two sub-tokens would allow
one or both of these sub-tokens to be alignable with
unaligned tokens in the other language. We iter-
ate through the string one token at a time, trying
all partitions. Given a source token ABC, consist-
ing of segments A, B and C, we test the two pairs of
subsequences {A, BC} and {AB, C}, to see which
of the two partitions (if any) could be aligned with
unaligned target tokens and we compare the scores
of both, selecting the highest score. Unless no par-
tition yields further source/target matches, we then
choose the highest scoring partition and add the re-
sulting node pairings to our alignment. In a similar
way, if there are a pair of aligned names consisting
of source tokens sj . . . sk and target tokens tj . . . tk,
we look for adjacent unaligned source nodes (a se-
quence of nodes ending in sj?1 or beginning with
sk+1) and/or adjacent target language nodes, such
that adding these nodes to the name sequence would
produce at least as high a lexical score. The lexi-
con can also be used to match two adjacent items to
the same word. We use a similar routine that checks
our lexicons for words that are adjacent to matching
words. This is particularly meaningful for the entries
automatically acquired by means of MI, as our cur-
rent method for acquiring MI would not distinguish
between 1 to 1 and N to 1 cases. Thus MI scores
for adjacent items typically does mean that an N to
1 match is appropriate. For example, the Chinese
word had high MI with every word
in the sequence (except and): ambassador extraor-
dinary and plenipotentiary (example is from FBIS).
This routine was able to cause our procedure to treat
this English sequence as a single token.
5.5 Using Node Alignment for Reordering
Given a node alignment, we can attempt to reorder
the source language so that words associated with
aligned nodes reflect the order of the words label-
ing the corresponding target nodes. Specifically,
we reorder our surface phrase structure-based repre-
sentation of the source language (English) and then
print out all the words yielded from the resulting
reordered tree. Reordering takes place in a bottom
up fashion as follows: for each phrase P with chil-
dren c0 . . . cn, reorder the structure beneath the child
nodes first. Then build the new-constituent right
to left, one child at a time from cn . . . c0. Start-
ing with an empty sequence, each item is put in
its proper place among the constituents in the se-
quence so far. At each step, place some ci after some
cj in ci+1 . . . cn, such that cj align precedes ci
and cj is after every ck in ci+1 . . . cn such that
ci align precedes ck. If cj does not exist, ci is
placed at the beginning of the sequence so far.
Definition of X align precedes Y , where X and
Y are nodes sharing the same parent: (1) Let pairsX
be the set of source/target pairs in the alignment such
that some (leaf node) descendant of X is the source
node in the pair; (2) Let pairsY be the set of pairs
in the alignment such that some descendant of Y is
the source node in the pair; (3) let Xtmax be the last
target member of a pair in pairsX , where the or-
der is determined by the word order of the target
words labeling the nodes; (4) let Ytmin be the first
target member of a pair in pairsY , where the order
is determined the same way; (5) let Xsmin be the
first source member of a pair in pairsx, according
to the source sentence word order; (6) let Ysmax be
the last source word in a pair in pairsY ordered the
same way. (7) X align precedes Y if: Xtmax pre-
cedes Ytmin and there is no source/target pair Q,R
in the alignment such that: (A) R precedes, Ytmin;
(B) Xtmax precedes R; (C) Q either precedes Xsmin
or follows Ysmax; (D) If Q precedes Ysmax, then R
does not precede Ytmin.
Essentially, the align precedes operator pro-
vides a conservative way to order the source sub-
trees S1 and S2 by their aligned target sub-tree coun-
terparts T1 and T2. The idea is that if T1 and T2
are ordered in an opposite manner to S1 and S2,
the source subtrees should trade places. However,
94
System DEV TEST
BASELINE 53.1% 49.9%
MANUAL 54.0% 50.6%
(p < .01) (not significant)
ALIGN 53.5% 51.1%
(p < .05) (p < .01)
ALIGN+MI 53.8% 51.4%
(p < .01) (p < .01)
Table 1: F Scores for Reordering Rules
a source/target pair Bs, Bt can block this reorder-
ing if doing so would upset the order of the moved
constituents relative to Bs and Bt e.g., if before the
move, Bs precedes S2 and Bt precedes T2, but af-
ter the move S2 would precede Bs. This reordering
proceeds from right to left, halting after placing c0.
6 Results
The results summarized in table 1, provide F-scores
(the harmonic mean of precision and recall) of the
word alignment resulting from running GIZA++
with and without our reordering rules, using the
LDC?s manually created word alignments for our
DEV and TEST corpora.4 Giza++ is run with En-
glish as source and Chinese as target. Our baseline
is the result of running Giza++ on the raw text. The
statistical significance of differences from the base-
line are provided in parentheses, next to each non-
baseline score(rounded to 2 significant digits). We
divided both corpora into 20 parts and ran all ver-
sions of the program on each section. We compared
the system output for each section against the base-
line and used the sign test to calculate statistical sig-
nificance. All system output except one5 achieved
at least p < .05 and most systems achieved signifi-
cance well below p < .01.
Informally, we observe that the rules reordering
common noun modifiers produce most of the total
4We used F-scores, which (Fraser and Marcu, 2007) show to
correlate well with improvements in BLEU. We weighted pre-
cision and recall evenly since we do not currently have BLEU
scores for MT that use these alignments and therefore cannot
tune the weights. Our results also showed improvements in
alignment error rate (AER) (Och and Ney, 2000), which incor-
porate the ?possible? and ?sure? portions of the manual align-
ment into F-score, but do not seem to correlate well with BLEU.
5When run on the test corpus, the manual system outper-
formed the baseline system on only 13 out of 20 sections.
improvement. However, space limitations prevent a
detailed exploration of these differences. The results
show that for both DEV and TEST corpora, both re-
ordering approaches improve F-scores of GIZA++
over the baseline. The manual rules (MANUAL)
seem to suffer somewhat from overtraining on the
DEV corpus, as they were designed based on DEV
corpus examples, whereas the alignment based ap-
proaches (ALIGN and subsequent entries in the ta-
ble) seem resilient to these effects. The use of Mu-
tual Information (ALIGN+MI) seems to further im-
prove the F-score.
The two approaches worked for many of the same
phenomena, e.g., they fronted many of the same
noun post-modifiers. The advantage of the hand-
coded rules seems to be that they cover reordering
of words which we cannot align. For example, a
rule that fronts post-nominal of phrases operates re-
gardless of dictionary coverage. Thus the rule-based
version fronted the of phrase in the NP the govern-
ment of the Guangxi Zhuangzu Autonomous Region
in our DEV corpus, due to the absolute application
of the rule. However, the alignment-based version
did not front the PP because the name was not found
in NEDICT. On the other hand, exceptions to this
rule were better handled by the alignment-based sys-
tem. For example, if series of aligns with the quan-
tifier , the PP would be incorrectly fronted
by the manual, but not the alignment-based system.
Also, the alignment-based method can handle cases
not covered by our rules with minimal labor. Thus,
the automatic system, but not the manual-rule sys-
tem fronted the locative PP in Guangxi to the po-
sition between been and quite in the sentence: for-
eign businessmen have been quite actively investing
in Guangxi. This is closer to the Chinese, but may
have been difficult to predict with an automatic rule
for several reasons, e.g., it is not clear if all post-
verbal locative phrases should front.
We further analyzed the DEV ALIGN+MI run to
determine both how often nodes were combined to-
gether by our algorithm to produce N to 1 align-
ments and the number of reorderings undertaken. It
turns out that out of the 59,032 pairs of nodes were
aligned for 3076 sentence pairs:6 55,391 alignments
6When sentences were misparsed in one language or the
other they were not reordered by the program.
95
were 1 to 1 (93.8% of the total) , 3443 alignments
were 2 to 1 (5.8% of the total) and 203 alignments
were N to 1, where N is greater than 2 (0.3% of the
total). The reordering program moved 1597 single
tokens; 2140 blocks 2 or 3 tokens long; 1203 blocks
of 4 or 5 tokens; 610 blocks of 6 or 7 tokens, 419
blocks of 8, 9 or 10 tokens, and 383 blocks of more
than 10 tokens.
7 Concluding Remarks
We have demonstrated that deep level linguistic
analysis can be used to improve word alignment re-
sults. It is natural to consider whether or not these
reorderings are likely to improve MT results. Both
the manual and alignment-based systems moved
post-nominal English modifiers to pre-nominal po-
sition, to reflect Chinese word order ? other move-
ments were much less frequent. In principle, these
selective reorderings may help SMT systems iden-
tify phrases of English that correspond to phrases of
Chinese, thus improving the quality of the phrase ta-
bles, especially when large chunks are moved. We
would also expect that the precision of our system to
be more important than the recall, since our system
would not yield an improvement if it produced too
much noise. Further experiments with current MT
systems are needed to assess whether this is actually
the case. We are considering such tests for future re-
search, using the Moses SMT system (Koehn et al,
2007).
Our representation had several possible advan-
tages over pure parse-based methods. We used se-
mantic features such as temporal, locative and trans-
parent (whether a low-content words inherits its se-
mantics) to help guide our alignment. The regu-
larized structure, also, helped identify long-distance
dependency relationships. We are also consider-
ing several improvements for our alignment-based
rules: (1) using additional dictionary resources such
as CATVAR (Habash and Dorr, 2003), so that cross-
part-of speech alignments can be more readily rec-
ognized; (2) finding more optimal orderings for
unaligned source language words. For example,
the alignment-based method reordered a bright star
arising from China?s policy to a bright arising from
China ?s policy star, separating bright from star,
even though bright star function as a unit; (3) incor-
porating and using multi-word bilingual dictionary
entries.; (4) automatic methods for tuning parame-
ters of our system that are currently hand-coded; (5)
training MI on a much larger corpus; (6) investigat-
ing possible ways to merge the manual-rules with
the alignment-based approach; and (7) performing
similar experiments with English/Japanese bitexts.
We would expect both parse-based approaches
and our system to handle mismatches that cover
large distances better than more shallow approaches
to reordering, e.g., (Crego and Marin?o, 2006) in the
same way that a full-parse handles constituent struc-
ture more completely than a chunker. In addition,
we would expect our approach to work best in lan-
guages where there are large differences in word or-
der, as these are exactly the cases that all predicate-
argument structure is designed to handle well (they
reduce apparent variation in structure). Towards this
end we are currently working on a Japanese/English
system. Obviously, the cost of developing GLARF
(or similar) systems are high, require linguistic ex-
pertise and may not be possible for resource-poor
languages. Nevertheless, we maintain that such sys-
tems are useful for many purposes and are there-
fore worth the cost. The GLARF system for En-
glish is available for download at http://nlp.
cs.nyu.edu/meyers/GLARF.html.
Acknowledgments
This work was supported by NSF Grant IIS-
0534700 Structure Alignment-based MT.
References
J. Bresnan and R. M. Kaplan. 1982. Syntactic Represen-
tation: Lexical-Functional Grammar: A Formal The-
ory for Grammatical Representation. In J. Bresnan,
editor, The Mental Representation of Grammatical Re-
lations. The MIT Press, Cambridge.
A. Bryl and J. van Genabith. 2010. f-align: An Open-
Source Alignment Tool for LFG f-Structures. In Pro-
ceedings of AMTA 2010.
E. Charniak. 2001. Immediate-head parsing for language
models. In ACL 2001, pages 116?123.
N. Chomsky. 1957. Syntactic Structures. Mouton, The
Hague.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause
Restructuring for Statistical Machine Translation. In
ACL 2005.
96
B. A. Cowan. 2008. A Tree-to-Tree Model for Statistical
Machine Translation. Ph.D. thesis, MIT.
J. M. Crego and J. B. Marin?o. 2006. Integration of POS-
tag-based source reordering into SMT decoding by an
extended search graph. In AMTA?06.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. The MIT Press, Cambridge.
A. Fraser and D. Marcu. 2007. Measuring Word
Alignment Quality for Statistical Machine Translation.
Computational Linguistics, 33:293?303.
D. Gildea and D. Jurafsky. 2002. Automatic Labeling of
Semantic Roles. Computational Linguistics, 28:245?
288.
R. Grishman. 2000. Entity Annotation Guidelines.
ftp://jaguar.ncsl.nist.gov/ace/phase1/edt phase1 v2.2.pdf.
N. Habash and B. Dorr. 2003. CatVar: A Database of
Categorial Variations for English. In Proceedings of
the MT Summit, pages 471?474, New Orleans.
Z. Harris. 1968. Mathematical Structures of Language.
Wiley-Interscience, New York.
J. R. Hobbs and R. Grishman. 1976. The Automatic
Transformational Analysis of English Sentences: An
Implementation. International Journal of Computer
Mathematics, 5:267?283.
H. Ji and R. Grishman. 2006. Analysis and Repair of
Name Tagger Errors. In COLING/ACL 2006, Sydney,
Australia.
H. Ji, R. Grishman, D. Freitag, M. Blume, J. Wang,
S. Khadivi, R. Zens, and H. Ney. 2009. Name Transla-
tion for Distillation. In Global Autonomous Language
Exploitation. Springer.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In ACL 2007 Demon-
stration Session, Prague.
P. Laplace. 1816. Essai philosophique sur les probabil-
its. Courcier Imprimeur, Paris.
Adam Meyers, Roman Yangarber, and Ralph Grishman.
1996. Alignment of Shared Forests for Bilingual Cor-
pora. In Proceedings of Coling 1996: The 16th In-
ternational Conference on Computational Linguistics,
pages 460?465.
Adam Meyers, Roman Yangarber, Ralph Grishman,
Catherine Macleod, and Antonio Moreno-Sandoval.
1998. Deriving Transfer Rules from Dominance-
Preserving Alignments. In Proceedings of Coling-
ACL98: The 17th International Conference on Com-
putational Linguistics and the 36th Meeting of the As-
sociation for Computational Linguistics.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. Annotating
Noun Argument Structure for NomBank. In Proceed-
ings of LREC-2004, Lisbon, Portugal.
A. Meyers, M. Kosaka, N. Xue, H. Ji, A. Sun, S. Liao,
and W. Xu. 2009. Automatic Recognition of Logi-
cal Relations for English, Chinese and Japanese in the
GLARF Framework. In SEW-2009 at NAACL-HLT-
2009.
E. Miltsakaki, A. Joshi, R. Prasad, and B. Webber. 2004.
Annotating discourse connectives and their arguments.
In A. Meyers, editor, NAACL/HLT 2004 Workshop:
Frontiers in Corpus Annotation, pages 9?16, Boston,
Massachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
F. J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In ACL 2000.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
S. Petrov and D. Klein. 2007. Improved Inference for
Unlexicalized Parsing. In HLT-NAACL 2007.
L. Shen, J. Xu, and R. Weischedel. 2008. A New String-
to-Dependency Machine Translation Algorithm with a
Target Dependency Language Model. In ACL 2008.
J. Wagner, D. Seddah, J. Foster, and J. van Genabith.
2007. C-Structures and F-Structures for the British
National Corpus. In Proceedings of the Twelfth In-
ternational Lexical Functional Grammar Conference,
Stanford. CSLI Publications.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese syn-
tactic reordering for statistical machine translation. In
EMNLP-CoNLL 2007, pages 737?745.
D. Wu and P. Fung. 2009. Semantic roles for smt: A
hybrid two-pass model. In HLT-NAACL-2009, pages
13?16, Boulder, Colorado, June. Association for Com-
putational Linguistics.
N. Xue and M. Palmer. 2003. Annotating the Proposi-
tions in the Penn Chinese Treebank. In The Proceed-
ings of the 2nd SIGHAN Workshop on Chinese Lan-
guage Processing, Sapporo.
N. Xue. 2008. Labeling Chinese Predicates with Seman-
tic roles. Computational Linguistics, 34:225?255.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In ACL, pages 523?530.
Y. Zhang, R. Zens, and H. Ney. 2007. Chunk-Level
Reordering of Source Language Sentences with Auto-
matically Learned Rules for Statistical Machine Trans-
lation. In Proc. of NAACL/HLT 2007.
M. Zhang, H. Jiang, A. Aw, H. Li, C. L. Tan, and S. Li.
2008. A Tree Sequence Alignment-based Tree-to-Tree
Translation Model. In ACL 2008.
97
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 1?27,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
CoNLL-2011 Shared Task:
Modeling Unrestricted Coreference in OntoNotes
Sameer Pradhan
BBN Technologies,
Cambridge, MA 02138
pradhan@bbn.com
Lance Ramshaw
BBN Technologies,
Cambridge, MA 02138
lramshaw@bbn.com
Mitchell Marcus
University of Pennsylvania,
Philadelphia, 19104
mitch@linc.cis.upenn.edu
Martha Palmer
University of Colorado,
Boulder, CO 80309
martha.palmer@colorado.edu
Ralph Weischedel
BBN Technologies,
Cambridge, MA 02138
weischedel@bbn.com
Nianwen Xue
Brandeis University,
Waltham, MA 02453
xuen@cs.brandeis.edu
Abstract
The CoNLL-2011 shared task involved pre-
dicting coreference using OntoNotes data. Re-
sources in this field have tended to be lim-
ited to noun phrase coreference, often on a
restricted set of entities, such as ACE enti-
ties. OntoNotes provides a large-scale corpus
of general anaphoric coreference not restricted
to noun phrases or to a specified set of en-
tity types. OntoNotes also provides additional
layers of integrated annotation, capturing ad-
ditional shallow semantic structure. This pa-
per briefly describes the OntoNotes annota-
tion (coreference and other layers) and then
describes the parameters of the shared task
including the format, pre-processing informa-
tion, and evaluation criteria, and presents and
discusses the results achieved by the partic-
ipating systems. Having a standard test set
and evaluation parameters, all based on a new
resource that provides multiple integrated an-
notation layers (parses, semantic roles, word
senses, named entities and coreference) that
could support joint models, should help to en-
ergize ongoing research in the task of entity
and event coreference.
1 Introduction
The importance of coreference resolution for the
entity/event detection task, namely identifying all
mentions of entities and events in text and clustering
them into equivalence classes, has been well recog-
nized in the natural language processing community.
Automatic identification of coreferring entities and
events in text has been an uphill battle for several
decades, partly because it can require world knowl-
edge which is not well-defined and partly owing to
the lack of substantial annotated data. Early work
on corpus-based coreference resolution dates back
to the mid-90s by McCarthy and Lenhert (1995)
where they experimented with using decision trees
and hand-written rules. A systematic study was
then conducted using decision trees by Soon et al
(2001). Significant improvements have been made
in the field of language processing in general, and
improved learning techniques have been developed
to push the state of the art in coreference resolu-
tion forward (Morton, 2000; Harabagiu et al, 2001;
McCallum and Wellner, 2004; Culotta et al, 2007;
Denis and Baldridge, 2007; Rahman and Ng, 2009;
Haghighi and Klein, 2010). Various different knowl-
edge sources from shallow semantics to encyclo-
pedic knowledge are being exploited (Ponzetto and
Strube, 2005; Ponzetto and Strube, 2006; Versley,
2007; Ng, 2007). Researchers continued finding
novel ways of exploiting ontologies such as Word-
Net. Given that WordNet is a static ontology and
as such has limitation on coverage, more recently,
there have been successful attempts to utilize in-
formation from much larger, collaboratively built
resources such as Wikipedia (Ponzetto and Strube,
2006). In spite of all the progress, current techniques
still rely primarily on surface level features such as
string match, proximity, and edit distance; syntac-
tic features such as apposition; and shallow seman-
tic features such as number, gender, named entities,
semantic class, Hobbs? distance, etc. A better idea
of the progress in the field can be obtained by read-
ing recent survey articles (Ng, 2010) and tutorials
(Ponzetto and Poesio, 2009) dedicated to this sub-
ject.
Corpora to support supervised learning of this
task date back to the Message Understanding Con-
ferences (MUC). These corpora were tagged with
coreferring entities identified by noun phrases in the
text. The de facto standard datasets for current coref-
erence studies are the MUC (Hirschman and Chin-
1
chor, 1997; Chinchor, 2001; Chinchor and Sund-
heim, 2003) and the ACE1 (G. Doddington et al,
2000) corpora. The MUC corpora cover all noun
phrases in text, but represent small training and test
sets. The ACE corpora, on the other hand, have much
more annotation, but are restricted to a small subset
of entities. They are also less consistent, in terms of
inter-annotator agreement (ITA) (Hirschman et al,
1998). This lessens the reliability of statistical ev-
idence in the form of lexical coverage and seman-
tic relatedness that could be derived from the data
and used by a classifier to generate better predic-
tive models. The importance of a well-defined tag-
ging scheme and consistent ITA has been well rec-
ognized and studied in the past (Poesio, 2004; Poe-
sio and Artstein, 2005; Passonneau, 2004). There
is a growing consensus that in order for these to be
most useful for language understanding applications
such as question answering or distillation ? both of
which seek to take information access technology
to the next level ? we need more consistent anno-
tation of larger amounts of broad coverage data for
training better automatic techniques for entity and
event identification. Identification and encoding of
richer knowledge ? possibly linked to knowledge
sources ? and development of learning algorithms
that would effectively incorporate them is a neces-
sary next step towards improving the current state
of the art. The computational learning community,
in general, is also witnessing a move towards eval-
uations based on joint inference, with the two pre-
vious CoNLL tasks (Surdeanu et al, 2008; Hajic? et
al., 2009) devoted to joint learning of syntactic and
semantic dependencies. A principle ingredient for
joint learning is the presence of multiple layers of
semantic information.
One fundamental question still remains, and that
is ? what would it take to improve the state of the art
in coreference resolution that has not been attempted
so far? Many different algorithms have been tried in
the past 15 years, but one thing that is still lacking
is a corpus comprehensively tagged on a large scale
with consistent, multiple layers of semantic infor-
mation. One of the many goals of the OntoNotes
project2 (Hovy et al, 2006; Weischedel et al, 2011)
is to explore whether it can fill this void and help
push the progress further ? not only in coreference,
but with the various layers of semantics that it tries
to capture. As one of its layers, it has created a
corpus for general anaphoric coreference that cov-
1http://projects.ldc.upenn.edu/ace/data/
2http://www.bbn.com/nlp/ontonotes
ers entities and events not limited to noun phrases
or a limited set of entity types. A small portion of
this corpus from the newswire and broadcast news
genres (?120k) was recently used for a SEMEVAL
task (Recasens et al, 2010). As mentioned earlier,
the coreference layer in OntoNotes constitutes just
one part of a multi-layered, integrated annotation of
shallow semantic structure in text with high inter-
annotator agreement, which also provides a unique
opportunity for performing joint inference over a
substantial body of data.
The remainder of this paper is organized as
follows. Section 2 presents an overview of the
OntoNotes corpus. Section 3 describes the coref-
erence annotation in OntoNotes. Section 4 then de-
scribes the shared task, including the data provided
and the evaluation criteria. Sections 5 and 6 then de-
scribe the participating system results and analyze
the approaches, and Section 7 concludes.
2 The OntoNotes Corpus
The OntoNotes project has created a corpus of large-
scale, accurate, and integrated annotation of multi-
ple levels of the shallow semantic structure in text.
The idea is that this rich, integrated annotation cov-
ering many layers will allow for richer, cross-layer
models enabling significantly better automatic se-
mantic analysis. In addition to coreference, this
data is also tagged with syntactic trees, high cov-
erage verb and some noun propositions, partial verb
and noun word senses, and 18 named entity types.
However, such multi-layer annotations, with com-
plex, cross-layer dependencies, demands a robust,
efficient, scalable mechanism for storing them while
providing efficient, convenient, integrated access to
the the underlying structure. To this effect, it uses a
relational database representation that captures both
the inter- and intra-layer dependencies and also pro-
vides an object-oriented API for efficient, multi-
tiered access to this data (Pradhan et al, 2007a).
This should facilitate the creation of cross-layer fea-
tures in integrated predictive models that will make
use of these annotations.
Although OntoNotes is a multi-lingual resource
with all layers of annotation covering three lan-
guages: English, Chinese and Arabic, for the scope
of this paper, we will just look at the English por-
tion. Over the years of the development of this cor-
pus, there were various priorities that came into play,
and therefore not all the data in the English portion is
annotated with all the different layers of annotation.
There is a core portion, however, which is roughly
2
1.3M words which has been annotated with all the
layers. It comprises ?450k words from newswire,
?150k from magazine articles, ?200k from broad-
cast news, ?200k from broadcast conversations and
?200k web data.
OntoNotes comprises the following layers of an-
notation:
? Syntax ? A syntactic layer representing a re-
vised Penn Treebank (Marcus et al, 1993;
Babko-Malaya et al, 2006).
? Propositions ? The proposition structure of
verbs in the form of a revised PropBank(Palmer
et al, 2005; Babko-Malaya et al, 2006).
? Word Sense ? Coarse grained word senses
are tagged for the most frequent polysemous
verbs and nouns, in order to maximize cov-
erage. The word sense granularity is tailored
to achieve 90% inter-annotator agreement as
demonstrated by Palmer et al (2007). These
senses are defined in the sense inventory files
and each individual sense has been connected
to multiple WordNet senses. This provides a
direct access to the WordNet semantic struc-
ture for users to make use of. There is also a
mapping from the word senses to the PropBank
frames and to VerbNet (Kipper et al, 2000) and
FrameNet (Fillmore et al, 2003).
? Named Entities ? The corpus was tagged with
a set of 18 proper named entity types that
were well-defined and well-tested for inter-
annotator agreement by Weischedel and Burn-
stein (2005).
? Coreference ? This layer captures general
anaphoric coreference that covers entities and
events not limited to noun phrases or a limited
set of entity types (Pradhan et al, 2007b). We
will take a look at this in detail in the next sec-
tion.
3 Coreference in OntoNotes
General anaphoric coreference that spans a rich set
of entities and events ? not restricted to a few types,
as has been characteristic of most coreference data
available until now ? has been tagged with a high
degree of consistency. Attributive coreference is
tagged separately from the more common identity
coreference.
Two different types of coreference are distin-
guished in the OntoNotes data: Identical (IDENT),
and Appositive (APPOS). Appositives are treated
separately because they function as attributions, as
described further below. The IDENT type is used
for anaphoric coreference, meaning links between
pronominal, nominal, and named mentions of spe-
cific referents. It does not include mentions of
generic, underspecified, or abstract entities.
Coreference is annotated for all specific entities
and events. There is no limit on the semantic types
of NP entities that can be considered for coreference,
and in particular, coreference is not limited to ACE
types.
The mentions over which IDENT coreference ap-
plies are typically pronominal, named, or definite
nominal. The annotation process begins by auto-
matically extracting all of the NP mentions from the
Penn Treebank, though the annotators can also add
additional mentions when appropriate. In the fol-
lowing two examples (and later ones), the phrases
notated in bold form the links of an IDENT chain.
(1) She had a good suggestion and it was unani-
mously accepted by all.
(2) Elco Industries Inc. said it expects net income
in the year ending June 30, 1990, to fall below a
recent analyst?s estimate of $ 1.65 a share. The
Rockford, Ill. maker of fasteners also said it
expects to post sales in the current fiscal year
that are ?slightly above? fiscal 1989 sales of $
155 million.
3.1 Verbs
Verbs are added as single-word spans if they can
be coreferenced with a noun phrase or with an-
other verb. The intent is to annotate the VP, but we
mark the single-word head for convenience. This in-
cludes morphologically related nominalizations (3)
and noun phrases that refer to the same event, even
if they are lexically distinct from the verb (4). In the
following two examples, only the chains related to
the growth event are shown.
(3) Sales of passenger cars grew 22%. The strong
growth followed year-to-year increases.
(4) Japan?s domestic sales of cars, trucks and buses
in October rose 18% from a year earlier to
500,004 units, a record for the month, the Japan
Automobile Dealers? Association said. The
strong growth followed year-to-year increases
of 21% in August and 12% in September.
3
3.2 Pronouns
All pronouns and demonstratives are linked to any-
thing that they refer to, and pronouns in quoted
speech are also marked. Expletive or pleonastic pro-
nouns (it, there) are not considered for tagging, and
generic you is not marked. In the following exam-
ple, the pronoun you and it would not be marked. (In
this and following examples, an asterisk (*) before a
boldface phrase identifies entity/event mentions that
would not be tagged as coreferent.)
(5) Senate majority leader Bill Frist likes to tell a
story from his days as a pioneering heart sur-
geon back in Tennessee. A lot of times, Frist re-
calls, *you?d have a critical patient lying there
waiting for a new heart, and *you?d want to
cut, but *you couldn?t start unless *you knew
that the replacement heart would make *it to
the operating room.
3.3 Generic mentions
Generic nominal mentions can be linked with refer-
ring pronouns and other definite mentions, but are
not linked to other generic nominal mentions. This
would allow linking of the bracketed mentions in (6)
and (7), but not (8).
(6) Officials said they are tired of making the same
statements.
(7) Meetings are most productive when they are
held in the morning. Those meetings, however,
generally have the worst attendance.
(8) Allergan Inc. said it received approval to
sell the PhacoFlex intraocular lens, the first
foldable silicone lens available for *cataract
surgery. The lens? foldability enables it to be
inserted in smaller incisions than are now pos-
sible for *cataract surgery.
Bare plurals, as in (6) and (7), are always consid-
ered generic. In example (9) below, there are two
generic instances of parents. These are marked as
distinct IDENT chains (with separate chains distin-
guished by subscripts X, Y and Z), each containing
a generic and the related referring pronouns.
(9) ParentsX should be involved with theirX chil-
dren?s education at home, not in school. TheyX
should see to it that theirX kids don?t play tru-
ant; theyX should make certain that the children
spend enough time doing homework; theyX
should scrutinize the report card. ParentsY are
too likely to blame schools for the educational
limitations of theirY children. If parentsZ are
dissatisfied with a school, theyZ should have
the option of switching to another.
In (10) below, the verb ?halve? cannot be linked
to ?a reduction of 50%?, since ?a reduction? is in-
definite.
(10) Argentina said it will ask creditor banks to
*halve its foreign debt of $64 billion ? the
third-highest in the developing world . Ar-
gentina aspires to reach *a reduction of 50%
in the value of its external debt.
3.4 Pre-modifiers
Proper pre-modifiers can be coreferenced, but
proper nouns that are in a morphologically adjecti-
val form are treated as adjectives, and not corefer-
enced. For example, adjectival forms of GPEs such
as Chinese in ?the Chinese leader?, would not be
linked. Thus we could coreference United States in
?the United States policy? with another referent, but
not American ?the American policy.? GPEs and Na-
tionality acronyms (e.g. U.S.S.R. or U.S.). are also
considered adjectival. Pre-modifier acronyms can be
coreferenced unless they refer to a nationality. Thus
in the examples below, FBI can be coreferenced to
other mentions, but U.S. cannot.
(11) FBI spokesman
(12) *U.S. spokesman
Dates and monetary amounts can be considered
part of a coreference chain even when they occur as
pre-modifiers.
(13) The current account deficit on France?s balance
of payments narrowed to 1.48 billion French
francs ($236.8 million) in August from a re-
vised 2.1 billion francs in July, the Finance
Ministry said. Previously, the July figure was
estimated at a deficit of 613 million francs.
(14) The company?s $150 offer was unexpected.
The firm balked at the price.
3.5 Copular verbs
Attributes signaled by copular structures are not
marked; these are attributes of the referent they mod-
ify, and their relationship to that referent will be
captured through word sense and propositional ar-
gument tagging.
4
(15) JohnX is a linguist. PeopleY are nervous
around JohnX, because heX always corrects
theirY grammar.
Copular (or ?linking?) verbs are those verbs that
function as a copula and are followed by a sub-
ject complement. Some common copular verbs are:
be, appear, feel, look, seem, remain, stay, become,
end up, get. Subject complements following such
verbs are considered attributes, and not linked. Since
Called is copular, neither IDENT nor APPOS corefer-
ence is marked in the following case.
(16) Called Otto?s Original Oat Bran Beer, the brew
costs about $12.75 a case.
3.6 Small clauses
Like copulas, small clause constructions are not
marked. The following example is treated as if the
copula were present (?John considers Fred to be an
idiot?):
(17) John considers *Fred *an idiot.
3.7 Temporal expressions
Temporal expressions such as the following are
linked:
(18) John spent three years in jail. In that time...
Deictic expressions such as now, then, today, to-
morrow, yesterday, etc. can be linked, as well as
other temporal expressions that are relative to the
time of the writing of the article, and which may
therefore require knowledge of the time of the writ-
ing to resolve the coreference. Annotators were al-
lowed to use knowledge from outside the text in re-
solving these cases. In the following example, the
end of this period and that time can be coreferenced,
as can this period and from three years to seven
years.
(19) The limit could range from three years to
seven yearsX, depending on the composition
of the management team and the nature of its
strategic plan. At (the end of (this period)X)Y,
the poison pill would be eliminated automati-
cally, unless a new poison pill were approved
by the then-current shareholders, who would
have an opportunity to evaluate the corpora-
tion?s strategy and management team at that
timeY.
In multi-date temporal expressions, embedded
dates are not separately connected to to other men-
tions of that date. For example in Nov. 2, 1999, Nov.
would not be linked to another instance of November
later in the text.
3.8 Appositives
Because they logically represent attributions, appos-
itives are tagged separately from Identity corefer-
ence. They consist of a head, or referent (a noun
phrase that points to a specific object/concept in the
world), and one or more attributes of that referent.
An appositive construction contains a noun phrase
that modifies an immediately-adjacent noun phrase
(separated only by a comma, colon, dash, or paren-
thesis). It often serves to rename or further define
the first mention. Marking appositive constructions
allows us to capture the attributed property even
though there is no explicit copula.
(20) Johnhead, a linguistattribute
The head of each appositive construction is distin-
guished from the attribute according to the following
heuristic specificity scale, in a decreasing order from
top to bottom:
Type Example
Proper noun John
Pronoun He
Definite NP the man
Indefinite specific NP a man I know
Non-specific NP man
This leads to the following cases:
(21) Johnhead, a linguistattribute
(22) A famous linguistattribute, hehead studied at ...
(23) a principal of the firmattribute, J. Smithhead
In cases where the two members of the appositive
are equivalent in specificity, the left-most member of
the appositive is marked as the head/referent. Defi-
nite NPs include NPs with a definite marker (the) as
well as NPs with a possessive adjective (his). Thus
the first element is the head in all of the following
cases:
(24) The chairman, the man who never gives up
(25) The sheriff, his friend
(26) His friend, the sheriff
In the specificity scale, specific names of diseases
and technologies are classified as proper names,
whether they are capitalized or not.
(27) A dangerous bacteria, bacillium, is found
5
Type Description
Annotator Error An annotator error. This is a catch-all category for cases of errors that do not fit in the other
categories.
Genuine Ambiguity This is just genuinely ambiguous. Often the case with pronouns that have no clear an-
tecedent (especially this & that)
Generics One person thought this was a generic mention, and the other person didn?t
Guidelines The guidelines need to be clear about this example
Callisto Layout Something to do with the usage/design of Callisto
Referents Each annotator thought this was referring to two completely different things
Possessives One person did not mark this possessive
Verb One person did not mark this verb
Pre Modifiers One person did not mark this Pre Modifier
Appositive One person did not mark this appositive
Extent Both people marked the same entity, but one person?s mention was longer
Copula Disagreement arose because this mention is part of a copular structure
a) Either each annotator marked a different half of the copula
b) Or one annotator unnecessarily marked both
Figure 1: Description of various disagreement types
Figure 1: The distribution of disagreements across the various types in Table 2
Sheet1
Page 1
Copulae 2%Appositives 3%Pre Modifiers 3%Verbs 3%Possessives 4%Refer nts 7%Callisto Layout 8%Guidelines 8%Generics 11%Genuine Ambiguity 25%Annotator Error 26%
Copulae
Appositives
Pre Modifiers
Verbs
Possessives
Referents
Callisto Layout
Guidelines
Generics
Genuine Ambiguity
Annotator Error
0% 5% 10% 15% 20% 25% 30%
Figure 2: The distribution of disagreements across the various types in Table 1
When the entity to which an appositive refers is
also mentioned elsewhere, only the single span con-
taining the entire appositive construction is included
in the larger IDENT chain. None of the nested NP
spans are linked. In the example below, the en-
tire span can be linked to later mentions to Richard
Godown. The sub-spans are not included separately
in the IDENT chain.
(28) Richard Godown, president of the Indus-
trial Biotechnology Association
Ages are tagged as attributes (as if they were el-
lipses of, for example, a 42-year-old):
(29) Mr.Smithhead, 42attribute,
3.9 Special Issues
In addition to the ones above, there are some special
cases such as:
? No coreference is marked between an organi-
zation and its members.
Genre ANN1-ANN2 ANN1-ADJ ANN2-ADJ
Newswire 80.9 85.2 88.3
Broadcast News 78.6 83.5 89.4
Broadcast Conversation 86.7 91.6 93.7
Magazine 78.4 83.2 88.8
Web 85.9 92.2 91.2
Table 1: Inter Annotator and Adjudicator agreement for
the Coreference Layer in OntoNotes measured in terms
of the MUC score.
? GPEs are linked to references to their govern-
ments, even when the references are nested
NPs, or the modifier and head of a single NP.
3.10 Annotator Agreement and Analysis
Table 1 shows the inter-annotator and annotator-
adjudicator agreement on all the genres of
OntoNotes. We also analyzed about 15K dis-
agreements in various parts of the data, and grouped
them into one of the categories shown in Figure 1.
Figure 2 shows the distribution of these different
types that were found in that sample. It can be
6
seen that genuine ambiguity and annotator error
are the biggest contributors ? the latter of which is
usually captured during adjudication, thus showing
the increased agreement between the adjudicated
version and the individual annotator version.
4 CoNLL-2011 Coreference Task
This section describes the CoNLL-2011 Corefer-
ence task, including its closed and open track ver-
sions, and characterizes the data used for the task
and how it was prepared.
4.1 Why a Coreference Task?
Despite close to a two-decade history of evaluations
on coreference tasks, variation in the evaluation cri-
teria and in the training data used have made it dif-
ficult for researchers to be clear about the state of
the art or to determine which particular areas require
further attention. There are many different parame-
ters involved in defining a coreference task. Looking
at various numbers reported in literature can greatly
affect the perceived difficulty of the task. It can seem
to be a very hard problem (Soon et al, 2001) or one
that is somewhat easier (Culotta et al, 2007). Given
the space constraints, we refer the reader to Stoy-
anov et al (2009) for a detailed treatment of the
issue.
Limitations in the size and scope of the available
datasets have also constrained research progress.
The MUC and ACE corpora are the two that have
been used most for reporting comparative results,
but they differ in the types of entities and corefer-
ence annotated. The ACE corpus is also one that
evolved over a period of almost five years, with dif-
ferent incarnations of the task definition and dif-
ferent corpus cross-sections on which performance
numbers have been reported, making it hard to un-
tangle and interpret the results.
The availability of the OntoNotes data offered an
opportunity to define a coreference task based on a
larger, more broad-coverage corpus. We have tried
to design the task so that it not only can support the
current evaluation, but also can provide an ongoing
resource for comparing different coreference algo-
rithms and approaches.
4.2 Task Description
The CoNLL-2011 shared task was based on the En-
glish portion of the OntoNotes 4.0 data. The task
was to automatically identify mentions of entities
and events in text and to link the coreferring men-
tions together to form entity/event chains. The target
coreference decisions could be made using automat-
ically predicted information on the other structural
layers including the parses, semantic roles, word
senses, and named entities.
As is customary for CoNLL tasks, there were two
tracks, closed and open. For the closed track, sys-
tems were limited to using the distributed resources,
in order to allow a fair comparison of algorithm per-
formance, while the open track allowed for almost
unrestricted use of external resources in addition to
the provided data.
4.2.1 Closed Track
In the closed track, systems were limited to the pro-
vided data, plus the use of two pre-specified external
resources: i) WordNet and ii) a pre-computed num-
ber and gender table by Bergsma and Lin (2006).
For the training and test data, in addition to the
underlying text, predicted versions of all the supple-
mentary layers of annotation were provided, where
those predictions were derived using off-the-shelf
tools (parsers, semantic role labelers, named entity
taggers, etc.) as described in Section 4.4.2. For the
training data, however, in addition to predicted val-
ues for the other layers, we also provided manual
gold-standard annotations for all the layers. Partici-
pants were allowed to use either the gold-standard or
predicted annotation for training their systems. They
were also free to use the gold-standard data to train
their own models for the various layers of annota-
tion, if they judged that those would either provide
more accurate predictions or alternative predictions
for use as multiple views, or wished to use a lattice
of predictions.
More so than previous CoNLL tasks, corefer-
ence predictions depend on world knowledge, and
many state-of-the-art systems use information from
external resources such as WordNet, which can
add a layer that helps the system to recognize se-
mantic connections between the various lexical-
ized mentions in the text. Therefore, the use of
WordNet was allowed, even for the closed track.
Since word senses in OntoNotes are predominantly3
coarse-grained groupings of WordNet senses, sys-
tems could also map from the predicted or gold-
standard word senses provided to the sets of under-
lying WordNet senses. Another significant piece of
knowledge that is particularly useful for coreference
but that is not available in the layers of OntoNotes is
that of number and gender. There are many different
3There are a few instances of novel senses introduced in
OntoNotes which were not present in WordNet, and so lack a
mapping back to the WordNet senses
7
ways of predicting these values, with differing accu-
racies, so in order to ensure that participants in the
closed track were working from the same data, thus
allowing clearer algorithmic comparisons, we spec-
ified a particular table of number and gender predic-
tions generated by Bergsma and Lin (2006), for use
during both training and testing.
Following the recent CoNLL tradition, partici-
pants were allowed to use both the training and the
development data for training the final model.
4.2.2 Open Track
In addition to resources available in the closed track,
the open track, systems were allowed to use external
resources such as Wikipedia, gazetteers etc. This
track is mainly to get an idea of a performance ceil-
ing on the task at the cost of not getting a compar-
ison across all systems. Another advantage of the
open track is that it might reduce the barriers to par-
ticipation by allowing participants to field existing
research systems that already depend on external re-
sources ? especially if there were hard dependen-
cies on these resources. They can participate in the
task with minimal or no modification to their exist-
ing system.
4.3 Coreference Task Data
Since there are no previously reported numbers on
the full version of OntoNotes, we had to create
a train/development/test partition. The only por-
tion of OntoNotes that has a previously determined,
widely used, standard split is the WSJ portion of the
newswire data. For that subcorpus, we maintained
the same partition. For all the other portions we cre-
ated stratified training, development and test parti-
tions over all the sources in OntoNotes using the pro-
cedure shown in Algorithm 1. The list of training,
development and test document IDs can be found on
the task webpage.4
4.4 Data Preparation
This section gives details of the different annota-
tion layers including the automatic models that were
used to predict them, and describes the formats in
which the data were provided to the participants.
4.4.1 Manual Annotation Gold Layers
We will take a look at the manually annotated, or
gold layers of information that were made available
for the training data.
4http://conll.bbn.com/download/conll-train.id
http://conll.bbn.com/download/conll-dev.id
http://conll.bbn.com/download/conll-test.id
Algorithm 1 Procedure used to create OntoNotes
training, development and test partitions.
Procedure: GENERATE PARTITIONS(ONTONOTES) returns TRAIN,
DEV, TEST
1: TRAIN? ?
2: DEV? ?
3: TEST? ?
4: for all SOURCE ? ONTONOTES do
5: if SOURCE = WALL STREET JOURNAL then
6: TRAIN? TRAIN ? SECTIONS 02 ? 21
7: DEV? DEV ? SECTIONS 00, 01, 22, 24
8: TEST? TEST ? SECTION 23
9: else
10: if Number of files in SOURCE ? 10 then
11: TRAIN? TRAIN ? FILE IDS ending in 1 ? 8
12: DEV? DEV ? FILE IDS ending in 0
13: TEST? TEST ? FILE IDS ending in 9
14: else
15: DEV? DEV ? FILE IDS ending in 0
16: TEST? TEST ? FILE ID ending in the highest number
17: TRAIN? TRAIN ? Remaining FILE IDS for the
SOURCE
18: end if
19: end if
20: end for
21: return TRAIN, DEV, TEST
Coreference The manual coreference annotation
is stored as chains of linked mentions connecting
multiple mentions of the same entity. Coreference is
the only document-level phenomenon in OntoNotes,
and the complexity of annotation increases non-
linearly with the length of a document. Unfortu-
nately, some of the documents ? especially ones in
the broadcast conversation, weblogs, and telephone
conversation genre ? are very long which prohib-
ited us from efficiently annotating them in entirety.
These had to be split into smaller parts. We con-
ducted a few passes to join some adjacent parts, but
since some documents had as many as 17 parts, there
are still multi-part documents in the corpus. Since
the coreference chains are coherent only within each
of these document parts, for this task, each such part
is treated as a separate document. Another thing
to note is that there were some cases of sub-token
annotation in the corpus owing to the fact that to-
kens were not split at hyphens. Cases such as pro-
WalMart had the sub-span WalMart linked with another
instance of the same. The recent Treebank revision
which split tokens at most hyphens, made a majority
of these sub-token annotations go away. There were
still some residual sub-token annotations. Since
subtoken annotations cannot be represented in the
CoNLL format, and they were a very small quantity
? much less than even half a percent ? we decided to
ignore them.
For various reasons, not all the documents in
OntoNotes have been annotated with all the differ-
8
Corpora Words Documents
Total Train Dev Test Total Train Dev Test
MUC-6 25K 12K 13K 60 30 30
MUC-7 40K 19K 21K 67 30 37
ACE (2000-2004) 1M 775K 235K - - -
OntoNotes5 1.3M 1M 136K 142K 2,083(2,999) 1,674(2,374) 202(303) 207(322)
Table 2: Number of documents in the OntoNotes data, and some comparison with the MUC and ACE data sets. The
numbers in parenthesis for the OntoNotes corpus indicate the total number of parts that correspond to the documents.
Each part was considered a separate document for evaluation purposes.
Syntactic category Train Development Test
Count % Count % Count %
NP 60,345 59.71 8,463 59.31 8,629 53.09
PRP 25,472 25.21 3,535 24.78 5,012 30.84
PRP$ 8,889 8.80 1,208 8.47 1,466 9.02
NNP 2,643 2.62 468 3.28 475 2.92
NML 900 0.89 151 1.06 118 0.73
Vx 1,915 1.89 317 2.22 314 1.93
Other 893 0.88 126 0.88 239 1.47
Overall 101,057 100.00 14,268 100.00 16,253 100.00
Table 3: Distribution of mentions in the data by their syn-
tactic category.
Train Development Test
Entities/Chains 26,612 3,752 3,926
Links 74,652 10,539 12,365
Mentions 101,264 14,291 16,291
Table 4: Number of entities, links and mentions in the
OntoNotes 4.0 data.
ent layers of annotation, with full coverage.6 There
is a core portion, however, which is roughly 1.3M
words which has been annotated with all the layers.
This is the portion that we used for the shared task.
The number of documents in the corpus for this
task, for each of the different genres, are shown in
Table 2. Tables 3 and 4 shows the distribution of
mentions by the syntactic categories, and the counts
of entities, links and mentions in the corpus respec-
tively. All of this data has been Treebanked and
PropBanked either as part of the OntoNotes effort
or some preceding effort.
For comparison purposes, Table 2 also lists the
number of documents in the MUC-6, MUC-7, and
ACE (2000-2004) corpora. The MUC-6 data was
taken from the Wall Street Journal, whereas the
MUC-7 data was from the New York Times. The
ACE data spanned many different genres similar to
6Given the nature of word sense annotation, and changes in
project priorities, we could not annotate all the low frequency
verbs and nouns in the corpus. Furthermore, PropBank annota-
tion currently only covers verb predicates.
the ones in OntoNotes.
Parse Trees This represents the syntactic layer
that is a revised version of the Penn Treebank. For
purposes of this task, traces were removed from the
syntactic trees, since the CoNLL-style data format,
being indexed by tokens, does not provide any good
means of conveying that information. Function tags
were also removed, since the parsers that we used
for the predicted syntax layer did not provide them.
One thing that needs to be dealt with in conversa-
tional data is the presence of disfluencies (restarts,
etc.). In the original OntoNotes parses, these are
marked using a special EDITED7 phrase tag ? as was
the case for the Switchboard Treebank. Given the
frequency of disfluencies and the performance with
which one can identify them automatically,8 a prob-
able processing pipeline would filter them out be-
fore parsing. Since we did not have a readily avail-
able tagger for tagging disfluencies, we decided to
remove them using oracle information available in
the Treebank.
Propositions The propositions in OntoNotes con-
stitute PropBank semantic roles. Most of the verb
predicates in the corpus have been annotated with
their arguments. Recent enhancements to the Prop-
Bank to make it synchronize better with the Tree-
bank (Babko-Malaya et al, 2006) have enhanced
the information in the proposition by the addition of
two types of LINKs that represent pragmatic corefer-
ence (LINK-PCR) and selectional preferences (LINK-
SLC). More details can be found in the addendum to
the PropBank guidelines9 in the OntoNotes 4.0 re-
7There is another phrase type ? EMBED in the telephone con-
versation genre which is similar to the EDITED phrase type, and
sometimes identifies insertions, but sometimes contains logical
continuation of phrases, so we decided not to remove that from
the data.
8A study by Charniak and Johnson (2001) shows that one
can identify and remove edits from transcribed conversational
speech with an F-score of about 78, with roughly 95 Precision
and 67 recall.
9doc/propbank/english-propbank.pdf
9
lease. Since the community is not used to this rep-
resentation which relies heavily on the trace struc-
ture in the Treebank which we are excluding, we de-
cided to unfold the LINKs back to their original rep-
resentation as in the Release 1.0 of the Proposition
Bank. This functionality is part of the OntoNotes
DB Tool.10
Word Sense Gold word sense annotation was
supplied using sense numbers as specified in
the OntoNotes list of senses for each lemma.11
The sense inventories that were provided in the
OntoNotes 4.0 release were not all mapped to the lat-
est version 3.0 of WordNet, so we provided a revised
version of the sense inventories, containing mapping
to WordNet 3.0, on the task page for the participants.
Named Entities Named Entities in OntoNotes
data are specified using a catalog of 18 Name types.
Other Layers Discourse plays a vital role in
coreference resolution. In the case of broadcast con-
versation, or telephone conversation data, it partially
manifests in the form of speakers of a given utter-
ance, whereas in weblogs or newsgroups it does so
as the writer, or commenter of a particular article
or thread. This information provides an important
clue for correctly linking anaphoric pronouns with
the right antecedents. This information could be au-
tomatically deduced, but since it would add addi-
tional complexity to the already complex task, we
decided to provide oracle information of this meta-
data both during training and testing. In other words,
speaker and author identification was not treated
as an annotation layer that needed to be predicted.
This information was provided in the form of an-
other column in the .conll table. There were some
cases of interruptions and interjections that ideally
would associate parts of a sentence to two different
speakers, but since the frequency of this was quite
small, we decided to make an assumption of one
speaker/writer per sentence.
4.4.2 Predicted Annotation Layers
The predicted annotation layers were derived using
automatic models trained using cross-validation on
other portions of OntoNotes data. As mentioned ear-
lier, there are some portions of the OntoNotes corpus
that have not been annotated for coreference but that
have been annotated for other layers. For training
10http://cemantix.org/ontonotes.html
11It should be noted that word sense annotation in OntoNotes
is note complete, so only some of the verbs and nouns have
word sense tags specified.
Senses Lemmas
1 1,506
2 1,046
> 2 1,016
Table 6: Word sense polysemy over verb and noun lem-
mas in OntoNotes
models for each of the layers, where feasible, we
used all the data that we could for that layer from
the training portion of the entire OntoNotes release.
Parse Trees Predicted parse trees were produced
using the Charniak parser (Charniak and Johnson,
2005).12 Some additional tag types used in the
OntoNotes trees were added to the parser?s tagset,
including the NML tag that has recently been added
to capture internal NP structure, and the rules used to
determine head words were appropriately extended.
The parser was then re-trained on the training por-
tion of the release 4.0 data using 10-fold cross-
validation. Table 5 shows the performance of the
re-trained Charniak parser on the CoNLL-2011 test
set. We did not get a chance to re-train the re-ranker,
and since the stock re-ranker crashes when run on n-
best parses containing NMLs, because it has not seen
that tag in training, we could not make use of it.
Word Sense We trained a word sense tagger us-
ing a SVM classifier and contextual word and part
of speech features on all the training portion of the
OntoNotes data. The OntoNotes 4.0 corpus com-
prises a total of 14,662 sense definitions across 4877
verb and noun lemmas13. The distribution of senses
per lemma is as shown in Table 6. Table 7 shows
the performance of this classifier over both the verbs
and nouns in the CoNLL-2011 test set. Again this
performance is not directly comparable to any re-
ported in the literature before, and it seems lower
then performances reported on previous versions
of OntoNotes because this is over all the genres
of OntoNotes, and aggregated over both verbs and
nouns in the CoNLL-2011 test set.
Propositions To predict propositional structure,
ASSERT14 (Pradhan et al, 2005) was used, re-
trained also on all the training portion of the release
12http://bllip.cs.brown.edu/download/reranking-
parserAug06.tar.gz
13The number of lemmas in Table 6 do not add up to this
number because not all of them have examples in the training
data, where the total number of instantiated senses amounts to
7933.
14http://cemantix.org/assert.html
10
All Sentences Sentence len < 40
N POS R P F N R P F
Broadcast Conversation (BC) 2,194 95.93 84.30 84.46 84.38 2124 85.83 85.97 85.90
Broadcast News (BN) 1,344 96.50 84.19 84.28 84.24 1278 85.93 86.04 85.98
Magazine (MZ) 780 95.14 87.11 87.46 87.28 736 87.71 88.04 87.87
Newswire (NW) 2,273 96.95 87.05 87.45 87.25 2082 88.95 89.27 89.11
Telephone Conversation (TC) 1,366 93.52 79.73 80.83 80.28 1359 79.88 80.98 80.43
Weblogs and Newsgroups (WB) 1,658 94.67 83.32 83.20 83.26 1566 85.14 85.07 85.11
Overall 9,615 96.03 85.25 85.43 85.34 9145 86.86 87.02 86.94
Table 5: Parser performance on the CoNLL-2011 test set
Frameset Total Total % Perfect Argument ID + Class
Accuracy Sentences Propositions Propositions P R F
Broadcast Conversation (BC) 0.92 2,037 5,021 52.18 82.55 64.84 72.63
Broadcast News (BN) 0.91 1,252 3,310 53.66 81.64 64.46 72.04
Magazine (MZ) 0.89 780 2,373 47.16 79.98 61.66 69.64
Newswire (NW) 0.93 1,898 4,758 39.72 80.53 62.68 70.49
Weblogs and Newsgroups (WB) 0.92 929 2,174 39.19 81.01 60.65 69.37
Overall 0.91 6,896 17,636 46.82 81.28 63.17 71.09
Table 8: Performance on the propositions and framesets in the CoNLL-2011 test set.
Accuracy
Broadcast Conversation (BC) 0.70
Broadcast News (BN) 0.68
Magazine (MZ) 0.60
Newswire (NW) 0.62
Weblogs and Newsgroups (WB) 0.63
Overall 0.65
Table 7: Word sense performance over both verbs and
nouns in the CoNLL-2011 test set
4.0 data. Given time constraints, we had to per-
form two modifications: i) Instead of a single model
that predicts all arguments including NULL argu-
ments, we had to use the two-stage mode where the
NULL arguments are first filtered out and the remain-
ing NON-NULL arguments are classified into one of
the argument types, and ii) The argument identifi-
cation module used an ensemble of ten classifiers
? each trained on a tenth of the training data and
performed an unweighted voting among them. This
should still give a close to state of the art perfor-
mance given that the argument identification perfor-
mance tends to start to be asymptotic around 10k
training instances. At first glance, the performance
on the newswire genre is much lower than what has
been reported for WSJ Section 23. This could be
attributed to two factors: i) the fact that we had to
compromise on the training method, but more im-
portantly because ii) the newswire in OntoNotes not
only contains WSJ data, but also Xinhua news. One
could try to verify using just the WSJ portion of the
data, but it would be hard as it is not only a sub-
set of the documents that the performance has been
reported on previously, but also the annotation has
been significantly revised; it includes propositions
for be verbs missing from the original PropBank,
and the training data is a subset of the original data
as well. Table 8 shows the detailed performance
numbers.
In addition to automatically predicting the argu-
ments, we also trained a classifier to tag PropBank
frameset IDs in the data using the same word sense
module as mentioned earlier. OntoNotes 4.0 con-
tains a total of 7337 framesets across 5433 verb
lemmas.15 An overwhelming number of them are
monosemous, but the more frequent verbs tend to be
polysemous. Table 9 gives the distribution of num-
ber of framesets per lemma in the PropBank layer of
the OntoNotes 4.0 data.
During automatic processing of the data, we
tagged all the tokens that were tagged with a part
of speech VBx. This means that there would be cases
where the wrong token would be tagged with propo-
sitions. The CoNLL-2005 scorer was used to gener-
ate the scores.
Named Entities BBN?s IdentiFinderTMsystem
was used to predict the named entities. Given the
15The number of lemmas in Table 9 do not add up to this
number because not all of them have examples in the training
data, where the total number of instantiated senses amounts to
4229.
11
Framesets Lemmas
1 2,722
2 321
> 2 181
Table 9: Frameset polysemy across lemmas
Overall BC BN MZ NW TC WB
F F F F F F F
ALL Named Entities 71.8 64.8 72.2 61.5 84.3 39.5 55.2
Cardinal 68.7 51.8 71.1 66.1 82.8 34.0 68.7
Date 76.1 63.7 77.9 66.7 83.7 60.5 56.0
Event 27.6 00.0 34.8 30.8 47.6 - 13.3
Facility 41.9 55.0 16.7 23.1 66.7 00.0 22.9
GPE 87.9 87.5 90.3 73.7 92.9 65.9 88.7
Language 41.2 - 50.0 50.0 00.0 20.0 75.0
Law 63.0 00.0 85.7 00.0 67.9 00.0 50.0
Location 58.4 59.1 59.6 53.3 68.0 00.0 23.5
Money 74.6 16.7 66.7 73.2 79.4 30.8 61.5
NORP 00.0 00.0 00.0 00.0 00.0 00.0 00.0
Ordinal 73.4 73.8 73.4 78.1 78.4 88.9 37.0
Organization 71.0 57.8 67.1 52.9 86.9 21.2 32.1
Percent 71.2 88.9 76.9 69.6 92.1 01.2 71.6
Person 79.6 78.9 87.7 66.7 91.6 65.1 64.8
Product 46.9 00.0 43.8 00.0 81.8 00.0 00.0
Quantity 47.5 25.3 58.3 61.1 71.9 00.0 22.2
Time 58.6 56.9 64.1 42.9 80.0 23.8 51.7
Work of Art 41.9 26.9 37.1 16.0 77.9 00.0 05.6
Table 10: Named Entity performance on the CoNLL-
2011 test set
time constraints, we could not re-train it on the
OntoNotes data and so an existing, pre-trained
model was used, therefore the results are not a
good indicator of the model?s best performance.
The pre-trained model had also used a somewhat
different catalog of name types, which did not
include the OntoNotes NORP type (for nationalities,
organizations, religions, and political parties),
so that category was never predicted. Table 10
shows the overall performance of the tagger on the
CoNLL-2011 test set, as well as the performance
broken down by individual name types. IdentiFinder
performance has been reported to be in the low 90?s
on WSJ test set.
Other Layers As noted above, systems were al-
lowed to make use of gender and number predic-
tions for NPs using the table from Bergsma and Lin
(Bergsma and Lin, 2006).
4.4.3 Data Format
In order to organize the multiple, rich layers of anno-
tation, the OntoNotes project has created a database
representation for the raw annotation layers along
with a Python API to manipulate them (Pradhan et
al., 2007a). In the OntoNotes distribution the data is
organized as one file per layer, per document. The
API requires a certain hierarchical structure with
documents at the leaves inside a hierarchy of lan-
guage, genre, source and section. It comes with var-
ious ways of cleanly querying and manipulating the
data and allows convenient access to the sense in-
ventory and propbank frame files instead of having
to interpret the raw .xml versions. However, main-
taining format consistency with earlier CoNLL tasks
was deemed convenient for sites that already had
tools configured to deal with that format. Therefore,
in order to distribute the data so that one could make
the best of both worlds, we created a new file type
called .conll which logically served as another layer
in addition to the .parse, .prop, .name and .coref
layers. Each .conll file contained a merged repre-
sentation of all the OntoNotes layers in the CoNLL-
style tabular format with one line per token, and with
multiple columns for each token specifying the input
annotation layers relevant to that token, with the fi-
nal column specifying the target coreference layer.
Because OntoNotes is not authorized to distribute
the underlying text, and many of the layers contain
inline annotation, we had to provide a skeletal form
(.skel of the .conll file which was essentially the
.conll file, but with the word column replaced with
a dummy string. We provided an assembly script
that participants could use to create a .conll file tak-
ing as input the .skel file and the top-level directory
of the OntoNotes distribution that they had sepa-
rately downloaded from the LDC16 Once the .conll
file is created, it can be used to create the individual
layers such as .parse, .name, .coref etc. using an-
other set of scripts. Since the propositions and word
sense layers are inherently standoff annotation, they
were provided as is, and did not require that extra
merging step. One thing thing that made this data
creation process a bit tricky was the fact that we had
dissected some of the trees for the conversation data
to remove the EDITED phrases. Table 11 describes
the data provided in each of the column of the .conll
format. Figure 3 shows a sample from a .conll file.
4.5 Evaluation
This section describes the evaluation criteria used.
Unlike for propositions, word sense and named en-
tities, where it is simply a matter of counting the
correct answers, or for parsing, where there are sev-
eral established metrics, evaluating the accuracy of
coreference continues to be contentious. Various al-
16OntoNotes is deeply grateful to the Linguistic Data Con-
sortium for making the source data freely available to the task
participants.
12
Column Type Description
1 Document ID This is a variation on the document filename
2 Part number Some files are divided into multiple parts numbered as 000, 001, 002, ... etc.
3 Word number This is the word index in the sentence
4 Word The word itself
5 Part of Speech Part of Speech of the word
6 Parse bit This is the bracketed structure broken before the first open parenthesis in the parse, and the
word/part-of-speech leaf replaced with a *. The full parse can be created by substituting
the asterix with the ([pos] [word]) string (or leaf) and concatenating the items in the
rows of that column.
7 Predicate lemma The predicate lemma is mentioned for the rows for which we have semantic role informa-
tion. All other rows are marked with a -
8 Predicate Frameset ID This is the PropBank frameset ID of the predicate in Column 7.
9 Word sense This is the word sense of the word in Column 3.
10 Speaker/Author This is the speaker or author name where available. Mostly in Broadcast Conversation and
Web Log data.
11 Named Entities These columns identifies the spans representing various named entities.
12:N Predicate Arguments There is one column each of predicate argument structure information for the predicate
mentioned in Column 7.
N Coreference Coreference chain information encoded in a parenthesis structure.
Table 11: Format of the .conll file used on the shared task
#begin document (nw/wsj/07/wsj_0771); part 000
...
...
nw/wsj/07/wsj_0771 0 0 ?? ?? (TOP(S(S* - - - - * * (ARG1* * * -
nw/wsj/07/wsj_0771 0 1 Vandenberg NNP (NP* - - - - (PERSON) (ARG1* * * * (8|(0)
nw/wsj/07/wsj_0771 0 2 and CC * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 3 Rayburn NNP *) - - - - (PERSON) *) * * *(23)|8)
nw/wsj/07/wsj_0771 0 4 are VBP (VP* be 01 1 - * (V*) * * * -
nw/wsj/07/wsj_0771 0 5 heroes NNS (NP(NP*) - - - - * (ARG2* * * * -
nw/wsj/07/wsj_0771 0 6 of IN (PP* - - - - * * * * * -
nw/wsj/07/wsj_0771 0 7 mine NN (NP*)))) - - 5 - * *) * * * (15)
nw/wsj/07/wsj_0771 0 8 , , * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 9 ?? ?? *) - - - - * * *) * * -
nw/wsj/07/wsj_0771 0 10 Mr. NNP (NP* - - - - * * (ARG0* (ARG0* * (15
nw/wsj/07/wsj_0771 0 11 Boren NNP *) - - - - (PERSON) * *) *) * 15)
nw/wsj/07/wsj_0771 0 12 says VBZ (VP* say 01 1 - * * (V*) * * -
nw/wsj/07/wsj_0771 0 13 , , * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 14 referring VBG (S(VP* refer 01 2 - * * (ARGM-ADV* (V*) * -
nw/wsj/07/wsj_0771 0 15 as RB (ADVP* - - - - * * * (ARGM-DIS* * -
nw/wsj/07/wsj_0771 0 16 well RB *) - - - - * * * *) * -
nw/wsj/07/wsj_0771 0 17 to IN (PP* - - - - * * * (ARG1* * -
nw/wsj/07/wsj_0771 0 18 Sam NNP (NP(NP* - - - - (PERSON* * * * * (23
nw/wsj/07/wsj_0771 0 19 Rayburn NNP *) - - - - *) * * * * -
nw/wsj/07/wsj_0771 0 20 , , * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 21 the DT (NP(NP* - - - - * * * * (ARG0* -
nw/wsj/07/wsj_0771 0 22 Democratic JJ * - - - - (NORP) * * * * -
nw/wsj/07/wsj_0771 0 23 House NNP * - - - - (ORG) * * * * -
nw/wsj/07/wsj_0771 0 24 speaker NN *) - - - - * * * * *) -
nw/wsj/07/wsj_0771 0 25 who WP (SBAR(WHNP*) - - - - * * * * (R-ARG0*) -
nw/wsj/07/wsj_0771 0 26 cooperated VBD (S(VP* cooperate 01 1 - * * * * (V*) -
nw/wsj/07/wsj_0771 0 27 with IN (PP* - - - - * * * * (ARG1* -
nw/wsj/07/wsj_0771 0 28 President NNP (NP* - - - - * * * * * -
nw/wsj/07/wsj_0771 0 29 Eisenhower NNP *))))))))))) - - - - (PERSON) * *) *) *) 23)
nw/wsj/07/wsj_0771 0 30 . . *)) - - - - * * * * * -
nw/wsj/07/wsj_0771 0 0 ?? ?? (TOP(S* - - - - * * * -
nw/wsj/07/wsj_0771 0 1 They PRP (NP*) - - - - * (ARG0*) * (8)
nw/wsj/07/wsj_0771 0 2 allowed VBD (VP* allow 01 1 - * (V*) * -
nw/wsj/07/wsj_0771 0 3 this DT (S(NP* - - - - * (ARG1* (ARG1* (6
nw/wsj/07/wsj_0771 0 4 country NN *) - - 3 - * * *) 6)
nw/wsj/07/wsj_0771 0 5 to TO (VP* - - - - * * * -
nw/wsj/07/wsj_0771 0 6 be VB (VP* be 01 1 - * * (V*) (16)
nw/wsj/07/wsj_0771 0 7 credible JJ (ADJP*))))) - - - - * *) (ARG2*) -
nw/wsj/07/wsj_0771 0 8 . . *)) - - - - * * * -
#end document
Figure 3: Sample portion of the .conll file.
13
ternative metrics have been proposed, as mentioned
below, which weight different features of a proposed
coreference pattern differently. The choice is not
clear in part because the value of a particular set of
coreference predictions is integrally tied to the con-
suming application.
A further issue in defining a coreference metric
concerns the granularity of the mentions, and how
closely the predicted mentions are required to match
those in the gold standard for a coreference predic-
tion to be counted as correct.
Our evaluation criterion was in part driven by the
OntoNotes data structures. OntoNotes coreference
distinguishes between identity coreference and ap-
positive coreference, treating the latter separately
because it is already captured explicitly by other lay-
ers of the OntoNotes annotation. Thus we evaluated
systems only on the identity coreference task, which
links all categories of entities and events together
into equivalent classes.
The situation with mentions for OntoNotes is also
different than it was for MUC or ACE. OntoNotes
data does not explicitly identify the minimum ex-
tents of an entity mention, but it does include hand-
tagged syntactic parses. Thus for the official evalua-
tion, we decided to use the exact spans of mentions
for determining correctness. The NP boundaries
for the test data were pre-extracted from the hand-
tagged Treebank for annotation, and events trig-
gered by verb phrases were tagged using the verbs
themselves. This choice means that scores for the
CoNLL-2011 coreference task are likely to be lower
than for coref evaluations based on MUC, where the
mention spans are specified in the input,17 or those
based on ACE data, where an approximate match is
often allowed based on the specified head of the NP
mention.
4.5.1 Metrics
As noted above, the choice of an evaluation met-
ric for coreference has been a tricky issue and there
does not appear to be any silver bullet approach that
addresses all the concerns. Three metrics have been
proposed for evaluating coreference performance
over an unrestricted set of entity types: i) The link
based MUC metric (Vilain et al, 1995), ii) The men-
tion based B-CUBED metric (Bagga and Baldwin,
1998) and iii) The entity based CEAF (Constrained
Entity Aligned F-measure) metric (Luo, 2005). Very
recently BLANC (BiLateral Assessment of Noun-
Phrase Coreference) measure (Recasens and Hovy,
17as is the case in this evaluation with Gold Mentions
2011) has been proposed as well. Each of the met-
ric tries to address the shortcomings or biases of the
earlier metrics. Given a set of key entities K, and
a set of response entities R, with each entity com-
prising one or more mentions, each metric generates
its variation of a precision and recall measure. The
MUC measure if the oldest and most widely used. It
focuses on the links (or, pairs of mentions) in the
data.18 The number of common links between en-
tities in K and R divided by the number of links
in K represents the recall, whereas, precision is the
number of common links between entities in K and
R divided by the number of links in R. This met-
ric prefers systems that have more mentions per en-
tity ? a system that creates a single entity of all
the mentions will get a 100% recall without signifi-
cant degradation in its precision. And, it ignores re-
call for singleton entities, or entities with only one
mention. The B-CUBED metric tries to addresses
MUCS?s shortcomings, by focusing on the mentions
and computes recall and precision scores for each
mention. If K is the key entity containing mention M,
and R is the response entity containing mention M,
then recall for the mention M is computed as |K?R||K|
and precision for the same is is computed as |K?R||R| .
Overall recall and precision are the average of the
individual mention scores. CEAF aligns every re-
sponse entity with at most one key entity by finding
the best one-to-one mapping between the entities us-
ing an entity similarity metric. This is a maximum
bipartite matching problem and can be solved by
the Kuhn-Munkres algorithm. This is thus a entity
based measure. Depending on the similarity, there
are two variations ? entity based CEAF ? CEAFe and
a mention based CEAF ? CEAFe. Recall is the total
similarity divided by the number of mentions in K,
and precision is the total similarity divided by the
number of mentions in R. Finally, BLANC uses a
variation on the Rand index (Rand, 1971) suitable
for evaluating coreference. There are a few other
measures ? one being the ACE value, but since this
is specific to a restricted set of entities (ACE types),
we did not consider it.
4.5.2 Official Evaluation Metric
In order to determine the best performing system
in the shared task, we needed to associate a single
number with each system. This could have been
one of the metrics above, or some combination of
more than one of them. The choice was not sim-
ple, and while we consulted various researchers in
18The MUC corpora did not tag single mention entities.
14
the field, hoping for a strong consensus, their con-
clusion seemed to be that each metric had its pros
and cons. We settled on the MELA metric by Denis
and Baldridge (2009), which takes a weighted av-
erage of three metrics: MUC, B-CUBED, and CEAF.
The rationale for the combination is that each of the
three metrics represents a different important dimen-
sion, the MUC measure being based on links, the
B-CUBED based on mentions, and the CEAF based
on entities. For a given task, a weighted average
of the three might be optimal, but since we don?t
have an end task in mind, we decided to use the un-
weighted mean of the three metrics as the score on
which the winning system was judged. We decided
to use CEAFe instead of CEAFm.
4.5.3 Scoring Metrics Implementation
We used the same core scorer implementation19 that
was used for the SEMEVAL-2010 task, and which
implemented all the different metrics. There were a
couple of modifications done to this scorer after it
was used for the SEMEVAL-2010 task.
1. Only exact matches were considered cor-
rect. Previously, for SEMEVAL-2010 non-exact
matches were judged partially correct with a
0.5 score if the heads were the same and the
mention extent did not exceed the gold men-
tion.
2. The modifications suggested by Cai and Strube
(2010) were incorporated in the scorer.
Since there are differences in the version used for
CoNLL and the one available on the download site,
and it is possible that the latter would be revised in
the future, we have archived the version of the scorer
on the CoNLL-2011 task webpage.20
5 Systems and Results
About 65 different groups demonstrated interest in
the shared task by registering on the task webpage.
Of these, 23 groups submitted system outputs on the
test set during the evaluation week. 18 groups sub-
mitted only closed track results, 3 groups only open
track results, and 2 groups submitted both closed and
open track results. 2 participants in the closed track,
did not write system papers, so we don?t use their re-
sults in the discussion. Their results will be reported
on the task webpage.
19http://www.lsi.upc.edu/ esapena/downloads/index.php?id=3
20http://conll.bbn.com/download/scorer.v4.tar.gz
The official results for the 18 systems that submit-
ted closed track outputs are shown in Table 12, with
those for the 5 systems that submitted open track
results in Table 13. The official ranking score, the
arithmetic mean of the F-scores of MUC, B-CUBED
and CEAFe, is shown in the rightmost column. For
convenience, systems will be referred to here using
the first portion of the full name, which is unique
within each table.
For completeness, the tables include the raw pre-
cision and recall scores from which the F-scores
were derived. The tables also include two additional
scores (BLANC and CEAFm) that did not factor into
the official ranking score. Useful further analysis
may be possible based on these results beyond the
preliminary results presented here.
As discussed previously in the task description,
we will consider three different test input conditions:
i) Predicted only (Official), ii) Predicted plus gold
mention boundaries, and iii) Predicted plus gold
mentions
5.1 Predicted only (Official)
For the official test, beyond the raw source text,
coreference systems were provided only with the
predictions from automatic engines as to the other
annotation layers (parses, semantic roles, word
senses, and named entities).
In this evaluation it is important to note that the
mention detection score cannot be considered in iso-
lation of the coreference task as has usually been the
case. This is mainly owing to the fact that there are
no singleton entities in the OntoNotes data. Most
systems removed singletons from the response as a
post-processing step, so not only will they not get
credit for the singleton entities that they correctly re-
moved from the data, but they will be penalized for
the ones that they accidentally linked with another
mention. What this number does indicate is the ceil-
ing on recall that a system would have got in absence
of being penalized for making mistakes in corefer-
ence resolution. A close look at the Table 12 indi-
cates a possible outlier in case of the sapena system.
The recall for this system is very high, and precision
way lower than any other system. Further investi-
gations uncovered that the reason for this aberrant
behavior was that fact that this system opted to keep
singletons in the response. By design, the scorer re-
moves singletons that might be still present in the
system, but it does so after the mention detection
accuracy is computed.
The official scores top out in the high 50?s. While
this is lower than the figures cited in previous coref-
15
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
75
.07
66
.81
70
.7
0
61
.76
57
.53
59
.57
68
.40
68
.23
68
.31
56
.37
56
.37
56
.3
7
43
.41
47
.75
45
.4
8
70
.63
76
.21
73
.02
57
.7
9
sap
en
a
92
.39
28
.19
43
.20
56
.32
63
.16
59
.55
62
.75
72
.08
67
.09
53
.51
53
.51
53
.51
44
.75
38
.38
41
.32
69
.50
73
.07
71
.10
55
.99
ch
an
g
68
.08
61
.96
64
.88
57
.15
57
.15
57
.15
67
.14
70
.53
68
.7
9
54
.40
54
.40
54
.40
41
.94
41
.94
41
.94
71
.19
77
.09
73
.7
1
55
.96
nu
gu
es
69
.87
68
.08
68
.96
60
.20
57
.10
58
.61
66
.74
64
.23
65
.46
51
.45
51
.45
51
.45
38
.09
41
.06
39
.52
71
.99
70
.31
71
.11
54
.53
san
tos
67
.80
63
.25
65
.45
59
.21
54
.30
56
.65
68
.79
62
.81
65
.66
49
.54
49
.54
49
.54
35
.86
40
.21
37
.91
73
.37
66
.91
69
.46
53
.41
son
g
57
.81
80
.41
67
.26
53
.73
67
.79
59
.9
5
60
.65
66
.05
63
.23
46
.29
46
.29
46
.29
43
.37
30
.71
35
.96
69
.49
59
.71
61
.47
53
.05
sto
ya
no
v
70
.84
64
.98
67
.78
63
.61
54
.04
58
.43
72
.58
53
.27
61
.44
46
.08
46
.08
46
.08
32
.00
40
.82
35
.88
73
.21
58
.93
60
.88
51
.92
sob
ha
67
.82
62
.09
64
.83
51
.08
49
.88
50
.48
62
.63
65
.43
64
.00
49
.48
49
.48
49
.48
40
.65
41
.82
41
.23
61
.40
68
.35
63
.88
51
.90
ko
bd
an
i
62
.06
60
.04
61
.03
55
.64
51
.50
53
.49
69
.66
62
.43
65
.85
42
.70
42
.70
42
.70
32
.33
35
.40
33
.79
61
.86
63
.51
62
.61
51
.04
zh
ou
61
.08
63
.59
62
.31
45
.65
52
.79
48
.96
57
.14
72
.91
64
.07
47
.53
47
.53
47
.53
43
.19
36
.79
39
.74
61
.10
73
.94
64
.72
50
.92
ch
art
on
65
.90
62
.77
64
.30
55
.09
50
.05
52
.45
66
.26
58
.44
62
.10
46
.82
46
.82
46
.82
34
.33
39
.05
36
.54
69
.94
62
.23
64
.80
50
.36
ya
ng
71
.92
57
.53
63
.93
59
.91
46
.43
52
.31
71
.64
55
.14
62
.32
46
.55
46
.55
46
.55
30
.28
42
.39
35
.33
71
.11
61
.75
64
.63
49
.99
ha
o
64
.50
64
.11
64
.30
57
.89
51
.42
54
.47
67
.83
55
.43
61
.01
45
.07
45
.07
45
.07
30
.08
35
.76
32
.67
72
.61
62
.37
65
.35
49
.38
xin
xin
65
.49
58
.71
61
.92
48
.54
44
.85
46
.62
61
.59
62
.28
61
.93
44
.75
44
.75
44
.75
35
.19
38
.62
36
.83
63
.04
65
.83
64
.27
48
.46
zh
an
g
55
.35
68
.25
61
.13
42
.03
55
.62
47
.88
52
.57
73
.05
61
.14
44
.46
44
.46
44
.46
42
.00
30
.28
35
.19
62
.84
69
.22
65
.21
48
.07
ku
mm
erf
eld
69
.77
56
.97
62
.72
46
.39
39
.56
42
.70
63
.60
57
.30
60
.29
45
.35
45
.35
45
.35
35
.05
42
.26
38
.32
58
.74
61
.58
59
.91
47
.10
zh
ek
ov
a
67
.49
37
.60
48
.29
28
.87
20
.66
24
.08
67
.14
56
.67
61
.46
40
.43
40
.43
40
.43
31
.57
41
.21
35
.75
52
.77
57
.05
53
.77
40
.43
irw
in
17
.06
61
.09
26
.67
12
.45
50
.60
19
.98
35
.07
89
.90
50
.46
31
.68
31
.68
31
.68
45
.84
17
.38
25
.21
51
.48
56
.83
51
.12
31
.88
Ta
ble
12
:P
erf
orm
an
ce
of
sys
tem
sin
the
of
fic
ia
l,
cl
os
ed
tra
ck
usi
ng
all
pre
dic
ted
inf
orm
ati
on
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
74
.31
67
.87
70
.9
4
62
.83
59
.34
61
.0
3
68
.85
69
.01
68
.9
3
56
.70
56
.70
56
.7
0
43
.29
46
.80
44
.9
8
71
.90
76
.55
73
.96
58
.3
1
cai
67
.15
67
.64
67
.40
56
.73
58
.90
57
.80
64
.60
71
.03
67
.66
53
.37
53
.37
53
.37
42
.71
40
.68
41
.67
69
.77
73
.96
71
.62
55
.71
ury
up
ina
70
.60
66
.31
68
.39
59
.70
55
.70
57
.63
66
.29
64
.12
65
.18
51
.42
51
.42
51
.42
38
.34
42
.17
40
.16
69
.23
68
.54
68
.88
54
.32
kle
nn
er
64
.41
60
.28
62
.28
49
.04
50
.71
49
.86
61
.70
68
.61
64
.97
50
.03
50
.03
50
.03
41
.28
39
.70
40
.48
66
.05
73
.90
69
.05
51
.77
irw
in
24
.60
62
.27
35
.27
18
.56
51
.01
27
.21
38
.97
85
.57
53
.55
33
.86
33
.86
33
.86
43
.33
19
.36
26
.76
51
.62
52
.91
51
.76
35
.84
Ta
ble
13
:P
erf
orm
an
ce
of
sys
tem
sin
the
of
fic
ia
l,
op
en
tra
ck
usi
ng
all
pre
dic
ted
inf
orm
ati
on
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
79
.52
71
.25
75
.1
6
65
.87
62
.05
63
.9
0
69
.52
70
.55
70
.0
3
59
.26
59
.26
59
.2
6
46
.29
50
.48
48
.3
0
72
.00
78
.55
74
.7
7
60
.7
4
nu
gu
es
74
.18
70
.74
72
.42
64
.33
60
.05
62
.12
68
.26
65
.17
66
.68
53
.84
53
.84
53
.84
39
.86
44
.23
41
.93
72
.53
71
.04
71
.75
56
.91
ch
an
g
63
.37
73
.18
67
.92
55
.00
65
.50
59
.79
62
.16
76
.65
68
.65
54
.95
54
.95
54
.95
46
.77
37
.17
41
.42
70
.97
79
.30
74
.29
56
.62
san
tos
65
.82
69
.90
67
.80
57
.76
61
.39
59
.52
64
.49
70
.27
67
.26
51
.87
51
.87
51
.87
41
.42
38
.16
39
.72
72
.72
71
.97
72
.34
55
.50
ko
bd
an
i
67
.11
65
.09
66
.08
62
.63
56
.80
59
.57
73
.20
62
.22
67
.27
44
.49
44
.49
44
.49
32
.87
37
.25
34
.92
64
.07
64
.13
64
.10
53
.92
sto
ya
no
v
76
.90
64
.73
70
.29
69
.81
55
.01
61
.54
77
.07
52
.54
62
.48
48
.08
48
.08
48
.08
30
.97
44
.84
36
.64
76
.57
60
.33
62
.96
53
.55
zh
an
g
59
.62
71
.19
64
.89
46
.06
58
.75
51
.64
53
.89
73
.41
62
.16
46
.62
46
.62
46
.62
43
.49
32
.11
36
.95
64
.11
70
.47
66
.54
50
.25
son
g
58
.43
77
.64
66
.68
46
.66
68
.40
55
.48
54
.40
70
.19
61
.29
43
.62
43
.62
43
.62
43
.77
25
.88
32
.53
66
.29
58
.76
60
.22
49
.77
zh
ek
ov
a
69
.19
57
.27
62
.67
33
.48
37
.15
35
.22
55
.47
68
.23
61
.20
41
.31
41
.31
41
.31
38
.29
34
.65
36
.38
53
.45
63
.33
54
.79
44
.27
Ta
ble
14
:P
erf
orm
an
ce
of
sys
tem
sin
the
sup
ple
me
nta
ry
cl
os
ed
tra
ck
usi
ng
pre
dic
ted
inf
orm
ati
on
plu
sg
ol
d
bo
un
da
ri
es
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
78
.71
72
.33
75
.39
66
.93
63
.91
65
.39
70
.09
71
.49
70
.78
59
.78
59
.78
59
.78
46
.34
49
.62
47
.92
73
.38
79
.00
75
.83
61
.36
Ta
ble
15
:P
erf
orm
an
ce
of
sys
tem
sin
the
sup
ple
me
nta
ry
op
en
tra
ck
usi
ng
pre
dic
ted
inf
orm
ati
on
plu
sg
ol
d
bo
un
da
ri
es
16
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
ch
an
g
10
0
10
0
10
0
80
.46
84
.75
82
.55
72
.84
74
.57
73
.70
69
.71
69
.71
69
.71
70
.45
60
.75
65
.24
78
.01
76
.57
77
.26
73
.83
Ta
ble
16
:P
erf
orm
an
ce
of
sys
tem
sin
the
su
pp
le
m
en
ta
ry
,c
lo
se
d
tra
ck
usi
ng
pre
dic
ted
inf
orm
ati
on
plu
sg
ol
d
m
en
ti
on
s
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
83
.37
10
0
90
.93
74
.79
89
.68
81
.56
67
.46
86
.88
75
.95
70
.73
70
.73
70
.73
77
.75
51
.05
61
.64
76
.65
85
.85
80
.35
73
.05
Ta
ble
17
:P
erf
orm
an
ce
of
sys
tem
sin
the
su
pp
le
m
en
ta
ry
,o
pe
n
tra
ck
usi
ng
pre
dic
ted
inf
orm
ati
on
plu
sg
ol
d
m
en
ti
on
s
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
76
.79
68
.34
72
.3
2
63
.29
58
.96
61
.05
68
.84
68
.72
68
.78
57
.28
57
.28
57
.2
8
44
.19
48
.75
46
.3
6
70
.93
76
.58
73
.36
58
.7
3
sap
en
a
95
.27
29
.07
44
.55
56
.99
63
.91
60
.25
62
.89
72
.31
67
.27
53
.90
53
.90
53
.90
45
.22
38
.70
41
.71
69
.71
73
.32
71
.32
56
.41
ch
an
g
69
.88
63
.61
66
.60
58
.48
58
.48
58
.48
67
.42
70
.91
69
.1
2
55
.21
55
.21
55
.21
42
.66
42
.66
42
.66
71
.42
77
.36
73
.9
6
56
.75
nu
gu
es
72
.96
71
.08
72
.01
62
.68
59
.46
61
.03
67
.24
64
.89
66
.04
52
.82
52
.82
52
.82
39
.25
42
.50
40
.81
72
.57
70
.86
71
.68
55
.96
san
tos
70
.39
65
.67
67
.95
61
.28
56
.20
58
.63
69
.25
63
.16
66
.07
50
.47
50
.47
50
.47
36
.51
41
.15
38
.69
73
.92
67
.32
69
.93
54
.46
son
g
59
.24
82
.39
68
.92
54
.92
69
.29
61
.27
60
.89
66
.27
63
.46
46
.97
46
.97
46
.97
44
.49
31
.15
36
.65
69
.73
59
.87
61
.61
53
.79
sto
ya
no
v
74
.43
68
.28
71
.22
67
.18
57
.08
61
.7
2
74
.06
53
.45
62
.09
47
.40
47
.40
47
.40
32
.78
42
.52
37
.02
74
.10
59
.34
61
.31
53
.61
sob
ha
71
.06
65
.06
67
.93
53
.91
52
.64
53
.27
63
.17
66
.14
64
.62
50
.80
50
.80
50
.80
41
.77
43
.03
42
.39
61
.91
69
.15
64
.49
53
.43
ko
bd
an
i
65
.98
63
.83
64
.89
59
.22
54
.81
56
.93
70
.49
63
.12
66
.60
44
.17
44
.14
44
.15
33
.19
36
.50
34
.77
62
.52
64
.25
63
.32
52
.77
zh
ou
64
.11
66
.74
65
.40
48
.00
55
.51
51
.48
57
.18
73
.71
64
.40
48
.40
48
.40
48
.40
44
.18
37
.35
40
.48
61
.54
74
.86
65
.30
52
.12
ch
art
on
71
.01
67
.64
69
.28
59
.24
53
.82
56
.40
67
.10
59
.02
62
.80
48
.91
48
.91
48
.91
35
.96
41
.39
38
.48
70
.65
62
.71
65
.34
52
.56
ya
ng
73
.73
58
.97
65
.53
61
.23
47
.45
53
.47
71
.88
55
.13
62
.40
47
.05
47
.05
47
.05
30
.54
43
.16
35
.77
71
.39
61
.92
64
.83
50
.55
ha
o
66
.79
66
.38
66
.59
59
.55
52
.89
56
.02
68
.27
55
.46
61
.20
45
.95
45
.95
45
.95
30
.76
36
.81
33
.51
73
.22
62
.73
65
.78
50
.24
xin
xin
69
.05
61
.91
65
.28
50
.99
47
.11
48
.97
61
.59
62
.70
62
.14
45
.64
45
.64
45
.64
35
.86
39
.57
37
.62
63
.42
66
.29
64
.68
49
.58
zh
an
g
57
.41
70
.78
63
.40
43
.48
57
.53
49
.53
52
.44
73
.60
61
.24
44
.97
44
.97
44
.97
42
.71
30
.44
35
.55
63
.12
69
.63
65
.53
48
.77
ku
mm
erf
eld
71
.05
58
.01
63
.87
47
.42
40
.44
43
.65
63
.73
57
.39
60
.39
45
.76
45
.76
45
.76
35
.30
42
.72
38
.66
58
.89
61
.77
60
.07
47
.57
zh
ek
ov
a
72
.65
40
.48
51
.99
31
.73
22
.70
26
.46
66
.92
56
.68
61
.37
41
.04
41
.04
41
.04
31
.93
42
.17
36
.34
53
.09
57
.86
54
.22
41
.39
irw
in
17
.58
62
.96
27
.49
12
.69
51
.59
20
.37
34
.88
89
.98
50
.27
31
.71
31
.71
31
.71
46
.13
17
.33
25
.20
51
.51
56
.93
51
.14
31
.95
Ta
ble
18
:H
ea
d
w
or
d
ba
se
d
pe
rfo
rm
an
ce
of
sys
tem
sin
the
of
fic
ia
l,
cl
os
ed
tra
ck
usi
ng
all
pre
dic
ted
inf
orm
ati
on
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
76
.01
69
.43
72
.5
7
64
.40
60
.83
62
.5
7
69
.34
69
.57
69
.4
5
57
.68
57
.68
57
.6
8
44
.15
47
.85
45
.9
2
72
.23
76
.94
74
.3
2
59
.3
1
cai
69
.32
69
.82
69
.57
58
.39
60
.63
59
.49
64
.88
71
.53
68
.04
54
.36
54
.36
54
.36
43
.74
41
.58
42
.64
70
.13
74
.39
72
.01
56
.72
ury
up
ina
72
.10
67
.72
69
.84
60
.74
56
.68
58
.64
66
.43
64
.25
65
.32
52
.00
52
.00
52
.00
38
.87
42
.85
40
.76
69
.43
68
.73
69
.07
54
.91
kle
nn
er
71
.73
67
.14
69
.36
55
.17
57
.04
56
.09
62
.67
70
.69
66
.44
53
.25
53
.25
53
.25
44
.27
42
.39
43
.31
67
.45
75
.92
70
.68
55
.28
irw
in
25
.24
63
.87
36
.18
18
.90
51
.94
27
.71
38
.79
85
.64
53
.40
33
.89
33
.89
33
.89
43
.59
19
.31
26
.76
51
.66
52
.98
51
.80
35
.96
Ta
ble
19
:H
ea
d
w
or
d
ba
se
d
pe
rfo
rm
an
ce
of
sys
tem
sin
the
of
fic
ia
l,
op
en
tra
ck
usi
ng
all
pre
dic
ted
inf
orm
ati
on
17
erence evaluations, that is as expected, given that the
task here includes predicting the underlying men-
tions and mention boundaries, the insistence on ex-
act match, and given that the relatively easier appos-
itive coreference cases are not included in this mea-
sure. The top-performing system (lee) had a score
of 57.79 which is about 1.8 points higher than that
of the second (sapena) and third (chang) ranking
systems, which scored 55.99 and 55.96 respectively.
Another 1.5 points separates them from the fourth
best score of 54.53 (nugues). Thus the performance
differences between the better-scoring systems were
not large, with only about three points separating the
top four systems.
This becomes even clearer if we merge in the re-
sults of systems that participated only in the open
track but that made relatively limited use of outside
resources.21 Comparing that way, the cai system
scores in the same ball park as the second rank sys-
tems (sapena and chang). The uryupina system sim-
ilarly scores very close to nugues?s 54.53
Given that our choice of the official metric was
somewhat arbitrary, if is also useful to look at
the individual metrics, including the mention-based
CEAFm and BLANC metrics that were not part of
the official metric. The lee system which scored
the best using the official metric does slightly worse
than song on the MUC metric, and also does slightly
worse than chang on the B-CUBED and BLANC met-
rics. However, it does much better than every other
group on the entity-based CEAFe, and this is the pri-
mary reason for its 1.8 point advantage in the offi-
cial score. If the CEAFe measure does indicate the
accuracy of entities in the response, this suggests
that the lee system is doing better on getting coher-
ent entities than any other system. This could be
partly due to the fact that that system is primarily
a precision-based system that would tend to create
purer entities. The CEAFe measure also seems to pe-
nalize other systems more harshly than do the other
measures.
We cannot compare these results to the ones ob-
tained in the SEMEVAL-2010 coreference task using
a small portion of OntoNotes data because it was
only using nominal entities, and had heuristically
added singleton mentions to the OntoNotes data22
21The cai system specifically mentions that, and the only re-
source that the uryupina system used outside of the closed track
setting was the Stanford named entity tagger.
22The documentation that comes with the SEMEVAL data
package from LDC (LDC2011T01) states: ?Only nominal
mentions and identical (IDENT) types were taken from the
OntoNotes coreference annotation, thus excluding coreference
5.2 Predicted plus gold mention boundaries
We also explored performance when the systems
were provided with the gold mention boundaries,
that is, with the exact spans (expressed in terms of
token offsets) for all of the NP constituents in the
human-annotated parse trees for the test data. Sys-
tems could use this additional data to ensure that the
output mention spans in their entity chains would not
clash with those in the answer set. Since this was
a secondary evaluation, it was an optional element,
and not all participants ran their systems on this task
variation. The results for those systems that did par-
ticipate in this optional task are shown in Tables 14
(closed track) and 15 (open track).
Most of the better scoring systems did supply
these results. While all systems did slightly better
here in terms of raw scores, the performance was
not much different from the official task, indicating
that mention boundary errors resulting from prob-
lems in parsing do not contribute significantly to the
final output.23
One side benefit of performing this supplemental
evaluation was that it revealed a subtle bug in the
automatic scoring routine that we were using that
could double-count duplicate correct mentions in a
given entity chain. These can occur, for example, if
the system considers a unit-production NP-PRP com-
bination as two mentions that identify the exact same
token in the text, and reports them as separate men-
tions. Most systems had a filter in their processing
that selected only one of these duplicate mentions,
but the kobdani system considered both as potential
mentions, and its developers tuned their algorithm
using that flawed version of the scorer.
When we fixed the scorer and re-evaluated all of
the systems, the kobdani system was the only one
whose score was affected significantly, dropping by
about 8 points, which lowered that system?s rank
from second to ninth. It is not clear how much of
this was owing to the fact that the system?s param-
relations with verbs and appositives. Since OntoNotes is only
annotated with multi-mention entities, singleton referential ele-
ments were identified heuristically: all NPs and possessive de-
terminers were annotated as singletons excluding those func-
tioning as appositives or as pre-modifiers but for NPs in the
possessive case. In coordinated NPs, single constituents as well
as the entire NPs were considered to be mentions. There is no
reliable heuristic to automatically detect English expletive pro-
nouns, thus they were (although inaccurately) also annotated as
singletons.?
23It would be interesting to measure the overlap between the
entity clusters for these two cases, to see whether there was
any substantial difference in the mention chains, besides the ex-
pected differences in boundaries for individual mentions.
18
eters had been tuned using the scorer with the bug,
which double-credited duplicate mentions. To find
out for sure, one would have to re-tune the system
using the modified scorer.
One difficulty with this supplementary evaluation
using gold mention boundaries is that those bound-
aries alone provide only very partial information.
For the roughly 10% of mentions that the automatic
parser did not correctly identify, while the systems
knew the correct boundaries, they had no hierarchi-
cal parser or semantic role label information, and
they also had to further approximate the already
heuristic head word identification. This incomplete
data complicated the systems? task and also compli-
cates interpretation of the results.
5.3 Predicted plus gold mentions
The final supplementary condition that we explored
was if the systems were supplied with the manually-
annotated spans for exactly those mentions that did
participate in the gold standard coreference chains.
This supplies significantly more information than
the previous case, where exact spans were supplied
for all NPs, since the gold mentions list here will also
include verb headwords that are linked to event NPs,
but will not include singleton mentions, which do
not end up as part of any chain. The latter constraint
makes this test seem somewhat artificial, since it di-
rectly reveals part of what the systems are designed
to determine, but it still has some value in quanti-
fying the impact that mention detection has on the
overall task and what the results are if the mention
detection is perfect.
Since this was a logical extension of the task and
since the data was available to the participants for
the development set, a few of the sites did run ex-
periments of this type. Therefore we decided to pro-
vide the gold mentions data to a few sites who had
reported these scores, so that we could compute the
performance on the test set. The results of these ex-
periments are shown in Tables 16 and 17. The results
show that performance does go up significantly, in-
dicating that it is markedly easier for the systems
to generate better entities given gold mentions. Al-
though, ideally, one would expect a perfect mention
detection score, it is the case that one of the two sys-
tems ? lee ? did not get a 100% Recall. This could
possibly be owing to unlinked singletons that were
removed in post-processing.
The lee system developers also ran a further ex-
periment where both gold mentions for the elements
of the coreference chains and also gold annota-
tions for all the other layers were available to the
system. Surprisingly, the improvement in corefer-
ence performance from having gold annotation of
the other layers was almost negligible. This sug-
gests that either: i) the automatic models are pre-
dicting those layers well enough that switching to
gold doesn?t make much difference; ii) information
from the other layers does not provide much lever-
age for coreference resolution; or iii) current coref-
erence models are not capable of utilizing the infor-
mation from these other layers effectively. Given
the performance numbers on the individual layers
cited earlier, (i) seems unlikely, and we hope that
further research in how best to leverage these lay-
ers will result in models that can benefit from them
more definitively.
5.4 Head word based scoring
In order to check how stringent the official, exact
match scoring is, we also performed a relaxed scor-
ing. Unlike ACE and MUC, the OntoNotes data does
not have manually annotated minimum spans that
a mention must contain to be considered correct.
However, OntoNotes does have manual syntactic
analysis in the form of the Treebank. Therefore, we
decided to approximate the minimum spans by using
the head words of the mentions using the gold stan-
dard syntax tree. If the response mention contained
the head word and did not exceed the true mention
boundary, then it was considered correct ? both from
the point of view of mention detection, and corefer-
ence resolution. The scores using this relaxed strat-
egy for the open and closed track submissions using
predicted data are shown in Tables 18 and 19. It
can be observed that the relaxed, head word based,
scoring does not improve performance very much.
The only exception was the klenner system whose
performance increased from 51.77 to 55.28. Over-
all, the ranking remained quite stable, though it did
change for some adjacent systems which had very
close exact match scores.
5.5 Genre variation
In order to check how the systems did on various
genres, we scored their performance per genre as
well. Tables 20 and 21 summarize genre based per-
formance for the closed and open track participants
respectively. System performance does not seem
to vary as much across the different genres as is
normally the case with language processing tasks,
which could suggest that coreference is relatively
genre insensitive, or it is possible that scores are
two low for the difference to be apparent. Compar-
isons are difficult, however, because the spoken gen-
19
MD MUC BCUB Cm Ce BLANC O MD MUC BCUB Cm Ce BLANC O
F F F F F F F F F F F F F F
lee GENRE zhou GENRE
BC 72.2 60.0 66.2 53.9 43.7 71.7 56.7 BC 64.1 49.5 62.1 45.3 38.8 61.8 50.1
BN 72.0 59.0 68.7 57.6 48.7 68.8 58.8 BN 60.8 45.9 64.4 49.5 41.2 66.8 50.5
MZ 70.1 58.0 72.2 61.6 50.9 75.0 60.4 MZ 58.8 44.4 66.9 50.1 41.8 64.6 51.0
NW 65.4 54.3 69.4 56.5 45.5 70.4 56.4 NW 57.7 44.8 65.7 48.7 40.3 63.1 50.2
TC 75.9 66.8 69.5 59.3 41.3 81.6 59.2 TC 69.2 58.1 60.8 43.1 35.7 62.6 51.5
WB 73.0 63.9 65.7 54.2 42.7 73.4 57.5 WB 67.4 55.4 62.8 47.9 39.2 69.1 52.5
sapena charton
BC 48.7 58.8 64.6 50.8 39.4 70.4 54.3 BC 65.8 53.1 59.1 44.6 35.2 64.4 49.1
BN 47.1 60.0 69.1 57.4 45.0 74.3 58.0 BN 65.5 52.0 64.0 50.0 39.6 65.9 51.9
MZ 35.3 59.2 72.3 60.4 48.2 75.0 59.9 MZ 61.7 46.3 64.6 49.7 39.9 64.1 50.3
NW 35.2 57.9 69.7 55.3 41.9 73.8 56.5 NW 57.6 44.6 64.5 48.2 37.7 67.0 48.9
TC 60.4 64.3 63.3 48.3 35.1 68.8 54.2 TC 73.1 66.8 56.2 42.8 29.9 58.1 51.0
WB 46.3 60.1 62.5 49.1 37.4 67.4 53.3 WB 67.6 57.6 59.3 45.1 33.3 66.6 50.0
chang yang
BC 65.5 56.4 67.1 51.5 39.8 71.6 54.4 BC 65.7 53.8 62.3 46.8 35.0 67.5 50.3
BN 66.6 57.4 69.1 56.0 45.6 70.5 57.4 BN 66.0 53.1 63.8 49.1 40.0 63.1 52.3
MZ 61.6 52.7 71.3 57.6 46.4 72.9 56.8 MZ 58.8 43.9 59.7 42.6 32.8 55.5 45.5
NW 61.0 53.3 69.1 54.1 42.1 71.9 54.8 NW 57.2 44.7 62.9 45.3 35.0 62.7 47.6
TC 72.2 68.5 71.4 59.6 37.7 81.7 59.2 TC 74.2 66.8 66.3 55.3 36.0 76.1 56.4
WB 66.4 59.7 66.7 52.7 39.4 74.7 55.3 WB 67.6 57.6 57.0 42.6 32.1 60.1 48.9
nugues hao
BC 71.4 59.2 62.4 48.2 37.2 68.4 52.9 BC 68.9 58.7 58.9 44.8 31.7 64.9 49.8
BN 70.0 58.5 67.4 54.5 43.1 73.1 56.3 BN 62.0 51.1 63.0 46.2 35.5 64.1 49.9
MZ 65.4 53.6 68.6 54.2 42.2 70.1 54.8 MZ 60.3 46.7 61.5 46.3 34.3 61.9 47.5
NW 61.8 51.9 67.0 51.3 39.2 69.4 52.7 NW 57.2 47.7 63.3 45.5 32.9 66.0 48.0
TC 77.2 69.2 63.9 53.0 37.9 72.2 57.0 TC 67.9 60.4 58.8 44.7 30.3 68.3 49.8
WB 72.9 64.2 63.4 51.1 38.5 74.3 55.4 WB 71.4 61.8 55.7 42.6 30.0 64.4 49.2
santos xinxin
BC 66.6 57.2 64.8 48.5 37.2 68.6 53.0 BC 64.8 47.8 60.2 43.9 35.5 65.1 47.9
BN 66.9 57.3 66.9 52.3 41.0 71.8 55.1 BN 61.5 44.7 63.2 47.0 38.9 65.8 48.9
MZ 62.7 51.0 65.9 48.9 37.8 64.5 51.6 MZ 54.6 35.5 64.5 45.7 37.7 61.0 45.9
NW 58.4 49.5 66.2 48.1 37.4 66.9 51.0 NW 54.3 39.5 64.0 45.0 37.5 61.1 47.0
TC 74.2 66.9 65.9 52.5 35.5 72.5 56.1 TC 74.2 62.0 57.9 45.4 33.4 66.5 51.1
WB 70.4 63.2 63.4 49.5 38.2 70.3 55.0 WB 66.9 52.6 58.5 42.2 35.9 63.4 49.0
song zhang
BC 68.9 61.4 61.0 44.1 34.3 59.5 52.2 BC 65.8 50.6 61.1 45.3 35.5 67.3 49.1
BN 66.2 58.4 64.8 49.0 38.2 65.2 53.8 BN 56.3 43.9 61.0 45.8 35.8 66.8 46.9
MZ 63.7 53.4 65.5 49.9 39.0 63.4 52.6 MZ 57.1 35.1 62.2 44.4 36.1 59.4 44.5
NW 62.4 53.6 64.3 48.0 37.2 62.7 51.7 NW 49.9 37.8 61.8 43.2 35.2 59.8 44.9
TC 76.9 74.4 62.0 43.3 33.2 58.1 56.5 TC 75.4 65.9 60.2 46.0 32.1 67.1 52.7
WB 70.0 63.0 60.1 43.3 31.8 60.8 51.6 WB 69.2 55.4 57.4 42.5 34.6 64.7 49.1
stoyanov kummerfield
BC 69.5 59.1 57.6 43.5 34.0 58.7 50.2 BC 66.4 41.5 55.6 41.7 36.2 57.9 44.4
BN 69.2 59.1 65.4 50.4 40.0 65.5 54.8 BN 68.3 48.2 63.4 51.7 44.7 61.6 52.1
MZ 66.7 55.1 65.5 51.0 39.9 63.7 53.5 MZ 58.0 39.9 65.8 51.0 43.4 64.1 49.7
NW 61.8 52.0 63.3 46.2 36.1 62.0 50.5 NW 55.2 41.3 64.7 46.8 37.0 63.5 47.6
TC 72.6 66.6 57.6 42.3 31.0 57.6 51.7 TC 61.8 34.5 51.5 34.7 30.0 54.1 38.7
WB 71.5 63.9 58.3 44.8 33.1 61.1 51.8 WB 68.2 48.1 56.0 44.4 38.6 59.6 47.6
sobha zhekova
BC 68.3 51.7 61.4 47.8 40.4 62.9 51.2 BC 50.5 23.8 60.6 39.4 35.1 53.4 39.8
BN 66.5 51.9 66.5 53.7 45.5 66.3 54.6 BN 51.2 26.0 62.4 42.5 37.5 54.3 42.0
MZ 68.8 54.9 70.3 58.9 49.3 69.8 58.1 MZ 44.0 22.6 63.4 43.3 37.3 56.0 41.1
NW 55.1 43.1 65.8 48.6 39.0 64.9 49.3 NW 39.7 19.4 62.8 41.0 35.8 53.7 39.3
TC 71.5 55.1 57.5 44.2 36.7 60.5 49.7 TC 59.4 31.6 58.2 37.7 33.6 54.1 41.1
WB 70.5 55.7 59.2 46.6 39.8 62.6 51.6 WB 54.1 27.8 58.7 38.5 34.7 53.0 40.4
kobdani irwin
BC 63.2 56.3 65.8 40.6 32.4 61.9 51.5 BC 23.5 16.1 46.0 29.4 23.6 49.8 28.6
BN 63.5 55.7 68.5 46.9 37.5 64.6 53.9 BN 24.9 20.0 49.7 34.2 27.1 52.9 32.3
MZ 57.5 52.2 69.8 45.7 36.4 61.7 52.8 MZ 23.2 17.9 55.9 36.2 28.5 53.0 34.1
NW 52.2 41.7 64.4 43.2 33.7 62.6 46.6 NW 27.5 21.6 56.4 33.9 27.3 52.6 35.1
TC 67.7 60.2 65.3 36.6 28.5 57.6 51.3 TC 28.0 19.3 38.2 24.5 18.7 49.0 25.4
WB 68.7 62.8 62.4 42.5 32.9 64.0 52.7 WB 33.6 24.8 47.6 29.7 23.0 50.2 31.8
Table 20: Detailed look at the performance per genre for the official, closed track using automatic performance. MD
represents MENTION DETECTION; BCUB represents B-CUBED; Cm represents CEAFm; Ce represents CEAFe and O
represents the OFFICIAL score.
20
res were treated here with perfect speech recognition
accuracy and perfect speaker turn information. Un-
der more realistic application conditions, the spread
in performance between genres might be greater.
MD MUC BCUB Cm Ce BLANC O
F F F F F F F
lee GENRE
BC 72.7 61.7 67.0 54.5 43.6 72.7 57.4
BN 72.0 60.6 69.4 57.9 48.1 70.3 59.3
MZ 69.9 58.4 72.1 61.2 50.1 75.2 60.2
NW 65.3 55.8 70.0 56.7 44.9 71.7 56.9
TC 76.6 68.4 70.4 59.6 40.8 82.1 59.9
WB 73.8 65.5 66.2 54.5 42.1 74.2 57.9
cai
BC 69.7 59.1 66.0 50.5 39.9 69.2 55.0
BN 68.6 57.6 67.8 55.4 45.5 68.2 56.9
MZ 64.0 51.1 69.5 55.9 45.6 71.2 55.4
NW 60.3 49.9 67.8 52.7 41.2 69.1 53.0
TC 75.6 70.5 72.2 59.6 38.0 80.3 60.2
WB 71.7 63.9 65.0 51.8 39.8 72.8 56.2
uryupina
BC 70.2 58.3 62.7 48.7 38.0 68.7 53.0
BN 69.0 57.6 66.8 53.6 43.1 69.2 55.8
MZ 65.7 52.4 68.3 54.3 43.6 68.8 54.8
NW 62.6 52.1 68.3 53.2 41.2 71.3 53.9
TC 75.7 67.1 61.0 50.7 34.6 67.1 54.2
WB 72.0 61.7 60.9 48.8 38.3 67.6 53.6
klenner
BC 63.2 50.3 63.4 48.2 38.9 66.8 50.8
BN 63.1 48.6 65.0 51.0 42.6 66.0 52.1
MZ 59.1 43.7 67.1 52.9 45.3 65.0 52.0
NW 55.3 41.3 65.0 48.0 39.6 64.5 48.7
TC 73.9 64.9 67.9 56.4 39.0 78.0 57.3
WB 66.8 58.1 64.0 50.1 39.6 72.7 53.9
irwin
BC 36.6 27.6 50.9 32.0 25.5 50.2 34.7
BN 30.8 24.6 51.9 36.4 28.6 54.8 35.0
MZ 26.1 20.0 57.3 37.6 29.4 54.3 35.6
NW 32.3 24.7 58.4 34.7 27.9 51.1 37.0
TC 46.4 34.3 44.6 29.4 21.9 51.7 33.6
WB 41.7 32.9 50.5 32.9 25.1 53.2 36.2
Table 21: Detailed look at the performance per genre for
the official, open track using predicted information. MD
represents MENTION DETECTION; BCUB represents B-
CUBED; Cm represents CEAFm; Ce represents CEAFe and
O represents the OFFICIAL score.
6 Approaches
Tables 22 and 23 summarize the approaches of the
participating systems along with some of the impor-
tant dimensions.
Most of the systems broke the problem into two
phases, first identifying the potential mentions in the
text and then linking the mentions to form corefer-
ence chains. Most participants also used rule-based
approaches for mention detection, though two did
use trained models. While trained morels seem able
to better balance precision and recall, and thus to
achieve a higher F-score on the mention task itself,
their recall tends to be quite a bit lower than that
achievable by rule-based systems designed to fa-
vor recall. This impacts coreference scores because
the full coreference system has no way to recover
if the mention detection stage misses a potentially
anaphoric mention.
Only one of the participating systems cai at-
tempted to do joint mention detection and corefer-
ence resolution. While it did not happen to be among
the top-performing systems, the difference in perfor-
mance could be due to the richer features used by
other systems rather than to the use of a joint model.
Most systems represented the markable mentions
internally in terms of the parse tree NP constituent
span, but some systems used shared attribute mod-
els, where the attributes of the merged entity are
determined collectively by heuristically merging the
attribute types and values of the different constituent
mentions.
Various types of trained models were used for pre-
dicting coreference. It is interesting to note that
some of the systems, including the best-performing
one, used a completely rule-based approach even for
this component.
Most participants appear not to have focused
much on eventive coreference, those coreference
chains that build off verbs in the data. This usu-
ally meant that mentions that should have linked to
the eventive verb were instead linked in with some
other entity. Participants may have chosen not to fo-
cus on events because they pose unique challenges
while making up only a small portion of the data.
Roughly 91% of mentions in the data are NPs and
pronouns.
In the systems that used trained models, many
systems used the approach described in Soon et al
(2001) for selecting the positive and negative train-
ing examples, while others used some of the al-
ternative approaches that have been introduced in
the research literature more recently. Many of the
trained systems also were able to improve their per-
formance by using feature selection, though things
varied some depending on the example selection
strategy and the classifier used. Almost half of the
trained systems used the feature selection strategy
from Soon et al (2001) and found it beneficial. It is
not clear whether the other systems did not explore
this path, or whether it just did not prove as useful in
their case.
7 Conclusions
In this paper we described the anaphoric coreference
information and other layers of annotation in the
21
Ta
sk
Sy
nta
x
Le
arn
ing
Fra
me
wo
rk
Ma
rka
ble
Ide
nti
fic
ati
on
Ma
rka
ble
Ve
rb
Fe
atu
re
Se
lec
tio
n
#F
eat
ure
s
Tra
ini
ng
lee
C+
O
P
Ru
le-
ba
sed
Ru
les
to
ex
clu
de
Co
pu
lar
co
nst
ruc
tio
n,
Ap
po
sit
ive
s,P
leo
na
sti
ci
t,e
tc.
Fe
atu
re
de
pe
nd
en
t
wi
th
sha
red
att
rib
ute
s
?
?
?
sap
en
a
C
P
De
cis
ion
Tre
e+
Re
lax
ati
on
La
be
lin
g
NP
(m
ax
im
al
spa
n)
+P
RP
+N
E
+C
ap
ita
liz
ed
no
un
he
uri
sti
c
Fu
llp
hra
se
?
?
Tra
in
+D
ev
ch
an
g
C
P
Le
arn
ing
Ba
sed
Jav
a
NP
,N
E,
PR
P,
PR
P$
Fu
llp
hra
se
?
?
Tra
in
+D
ev
cai
O
P
Co
mp
ute
hy
pe
red
ge
we
igh
tso
n
30
%
of
tra
ini
ng
da
ta
NP
,P
RP
,P
RP
$,
Ba
se
ph
ras
ec
hu
nk
s,
Ple
on
ast
ic
it
filt
er
Fu
llp
hra
se
?
?
?
nu
gu
es
C
D
Lo
gis
tic
Re
gre
ssi
on
(LI
BL
IN
EA
R)
NP
,P
RP
$a
nd
seq
ue
nc
eo
fN
NP
(s)
in
po
st
pro
ces
sin
gu
sin
gA
LI
AS
an
dS
TR
IN
GM
AT
CH
He
ad
wo
rd
?
Fo
rw
ard
+B
ack
wa
rd
sta
rtin
gf
rom
So
on
fea
tur
es
et
24
Tra
in
+D
ev
ury
up
ina
O
P
De
cis
ion
Tre
e.
Di
ffe
ren
t
cla
ssi
fie
rs
for
Pro
no
mi
na
la
nd
no
n-P
ron
om
ina
lm
en
tio
ns
NP
,N
E,
PR
P,
PR
P$
,a
nd
rul
es
to
ex
clu
de
som
es
pe
cifi
cc
ase
s
Fu
llp
hra
se
?
Mu
lti-
Ob
jec
tiv
e
Op
tim
iza
tio
no
nt
hre
e
spl
its
.N
SG
A-
II
46
Tra
in
+D
ev
san
tos
C
P
ET
L
(E
ntr
op
yg
uid
ed
Tra
nsf
orm
ati
on
al
Le
arn
ing
)
co
mm
itte
ea
nd
Ra
nd
om
Fo
res
t
(W
EK
A)
Al
lN
Pa
nd
all
pro
no
un
sa
nd
PE
R,
OR
G,
GP
E
in
NP
Fu
llp
hra
se
?
Inh
ere
nt
to
the
cla
ssi
fie
rs
Tra
in
+D
ev
son
g
C
P
Ma
xE
nt
(O
pe
nN
LP
)
Me
nti
on
de
tec
tio
nc
las
sifi
er
Fu
llp
hra
se
?
Sa
me
fea
tur
es
et,
bu
t
pe
rc
las
sifi
er
40
Tra
in
sto
ya
no
v
C
P
Av
era
ge
dp
erc
ep
tro
n
NE
an
dp
oss
ess
ive
sin
ad
dit
ion
to
AC
E
ba
sed
sys
tem
Fu
llp
hra
se
?
?
76
?
sob
ha
C
P
CR
Ff
or
no
n-p
ron
om
ina
la
nd
sal
ien
ce
fac
tor
for
pro
no
mi
na
l
res
olu
tio
n
Ma
ch
ine
lea
rne
dp
leo
na
sti
ci
t,p
lus
NP
,P
RP
,
PR
P$
an
dN
E
Mi
nim
al
(C
hu
nk
/N
E)
an
dM
ax
im
um
spa
n
?
?
Tra
in
kle
nn
er
O
D
Ru
le-
ba
sed
.S
ali
en
ce
me
asu
re
usi
ng
de
pe
nd
en
cie
sg
en
era
ted
fro
m
tra
ini
ng
da
ta
NP
,N
E,
PR
P,
PR
P$
Sh
are
d
att
rib
ute
d/t
ran
sit
ivi
ty
by
usi
ng
av
irtu
al
pro
tot
yp
e
?
?
?
ko
bd
an
i
C
P
De
cis
ion
Tre
e
NP
(no
me
nti
on
of
PR
P$
)
Sta
rtw
ord
,E
nd
wo
rd
an
dH
ead
of
NP
?
Inf
orm
ati
on
ga
in
rat
io
Tra
in
zh
ou
C
P
SV
M
tre
ek
ern
el
usi
ng
BC
po
rtio
n
of
the
da
ta
Ru
le-
ba
sed
;F
ive
rul
es:
PR
P$
,P
RP
,N
E,
sm
all
est
NP
sub
sum
ing
NE
an
dD
ET
+N
P
Fu
llp
hra
se
?
?
17
Tra
in
+D
ev
ch
art
on
C
P
Mu
lti-
lay
er
pe
rce
ptr
on
Ru
les
ba
sed
on
PO
S,
NE
an
dfi
lte
ro
ut
ple
on
ast
ic
it
usi
ng
rul
e-b
ase
dfi
lte
r
Fu
llp
hra
se
?
?
22
Tra
in
ya
ng
C
P
Ma
xE
nt
(M
AL
LE
T)
NP
,P
RP
,P
RP
$,
pre
-m
od
ifie
rs
an
dv
erb
s
Fu
llp
hra
se
?
?
40
Tra
in
+D
ev
ha
o
C
P
Ma
xE
nt
NP
,P
RP
,P
RP
$,
VB
D
ful
lp
hra
se
?
?
Tra
in
+D
ev
xin
xin
C
P
IL
P/I
nfo
rm
ati
on
ga
in
NP
,P
RP
,P
RP
$
Fu
llp
hra
se
?
Inf
orm
ati
on
ga
in
rat
io
65
?
zh
an
g
C
P
SV
M
IO
B
cla
ssi
fic
ati
on
Fu
llp
hra
se
?
?
?
ku
mm
erfi
eld
C
P
Un
sup
erv
ise
dg
en
era
tiv
em
od
el
NP
,P
RP
,P
RP
$w
ith
ma
xim
al
spa
n
Fu
llp
hra
se
?
?
?
zh
ek
ov
a
C
P
TI
M
BL
me
mo
ry
ba
sed
lea
rne
r
NP
,P
rop
er
no
un
s,P
RP
,P
RP
$,
plu
sv
erb
wi
th
pre
dic
ate
lem
ma
He
ad
wo
rd
?
?
Tra
in
+D
ev
irw
in
C+
O
P
Cl
ass
ific
ati
on
-ba
sed
ran
ke
r
NP
,P
RP
,P
RP
$
Sh
are
da
ttri
bu
tes
?
?
?
Ta
ble
22
:P
art
ici
pa
tin
gs
yst
em
pro
file
s?
Pa
rt
I.I
nt
he
Ta
sk
co
lum
n,
C/
O
rep
res
en
ts
wh
eth
er
the
sys
tem
pa
rtic
ipa
ted
in
the
cl
os
ed
,o
pe
n
or
bo
th
tra
ck
s.
In
the
Sy
nta
xc
olu
mn
,a
Pr
ep
res
en
ts
tha
tth
es
yst
em
su
sed
ap
hra
se
str
uc
tur
eg
ram
ma
rr
ep
res
en
tat
ion
of
syn
tax
,w
he
rea
sa
D
rep
res
en
ts
tha
tth
ey
use
da
de
pe
nd
en
cy
rep
res
en
tat
ion
.
22
Po
sit
ive
Tra
ini
ng
Ex
am
ple
s
Ne
ga
tiv
eT
rai
nin
gE
xa
mp
les
De
co
din
g
Pa
rse
Co
nfi
gu
rat
ion
lee
?
?
Mu
lti-
pa
ss
Sie
ve
s
sap
en
a
Al
lm
en
tio
np
air
sa
nd
lon
ge
ro
fn
est
ed
me
nti
on
sw
ith
co
mm
on
he
ad
ke
pt
Me
nti
on
pa
irs
wi
th
les
sth
an
thr
esh
old
(5)
nu
mb
er
of
dif
fer
en
ta
ttri
bu
te
val
ue
sa
re
co
nsi
de
red
(22
%
ou
to
f9
9%
ori
gin
al
are
dis
car
de
d)
Ite
rat
ive
1-b
est
ch
an
g
Cl
ose
sta
nte
ced
en
t
Al
lp
rec
ed
ing
me
nti
on
sin
au
nio
no
fo
fg
ol
d
an
dp
re
di
ct
ed
me
nti
on
s.
Me
nti
on
sw
he
re
the
firs
tis
pro
no
un
an
do
the
rn
ot
are
no
t
co
nsi
de
red
Be
stl
ink
an
dA
lll
ink
ss
tra
teg
y;
wi
th
an
d
wi
tho
ut
co
nst
rai
nts
?B
est
lin
kw
ith
ou
t
co
nst
rai
nts
wa
ss
ele
cte
df
or
the
offi
cia
lru
n
cai
We
igh
tsa
re
tra
ine
do
np
art
of
the
tra
ini
ng
da
ta
Re
cu
rsi
ve
2-w
ay
Sp
ect
ral
clu
ste
rin
g
(A
ga
rw
al,
20
05
)
nu
gu
es
Cl
ose
stA
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Cl
ose
st-
firs
tc
lus
ter
ing
for
pro
no
un
sa
nd
Be
st-
firs
tc
lus
ter
ing
for
no
n-p
ron
ou
ns
1-b
est
ury
up
ina
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
me
nti
on
pa
irm
od
el
wi
tho
ut
ran
kin
ga
sin
So
on
20
01
san
tos
Ex
ten
de
dv
ers
ion
of
So
on
(20
01
)w
he
re
in
ad
dit
ion
to
the
irs
tra
teg
y,
po
sit
ive
an
dn
ega
tiv
e
ex
am
ple
sf
rom
me
nti
on
sin
the
sen
ten
ce
of
the
clo
ses
tp
rec
ed
ing
an
tec
ed
en
ta
re
co
nsi
de
red
Lim
ite
dn
um
be
ro
fp
rec
ed
ing
me
nti
on
s6
0
for
au
tom
ati
ca
nd
40
giv
en
go
ld
bo
un
da
rie
s;
Ag
gre
ssi
ve
-m
erg
ec
lus
ter
ing
(M
cca
rth
ya
nd
Le
nh
ert
,1
99
5)
son
g
Pre
-cl
ust
er
pa
irm
od
els
sep
ara
te
for
eac
hp
air
NP
-N
P,
NP
-PR
Pa
nd
PR
P-P
RP
Pre
-cl
ust
ers
,w
ith
sin
gle
ton
pro
no
un
pre
-cl
ust
ers
,a
nd
use
clo
ses
t-fi
rst
clu
ste
rin
g.
Di
ffe
ren
tli
nk
mo
de
lsb
ase
do
nt
he
typ
eo
f
lin
kin
gm
en
tio
ns
?N
P-P
RP
,P
RP
-PR
Pa
nd
NP
-N
P
sto
ya
no
v
Sm
art
Pa
irG
en
era
tio
n(
Sm
art
PG
)w
he
re
the
typ
eo
fa
nte
ced
en
tis
de
ter
mi
ne
db
yt
he
typ
eo
f
an
ap
ho
ru
sin
ga
set
of
rul
es
Sin
gle
-lin
kc
lus
ter
ing
by
co
mp
uti
ng
tra
nsi
tiv
ec
los
ure
be
tw
een
pa
irw
ise
po
sit
ive
s.
sob
ha
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Pro
no
mi
na
l:a
llp
rec
ed
ing
NP
sin
the
sen
ten
ce
an
dp
rec
ed
ing
4s
en
ten
ces
kle
nn
er
?
?
Inc
rem
en
tal
en
tity
cre
ati
on
ko
bd
an
i
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Be
st-
firs
tc
lus
ter
ing
.T
hre
sho
ld
of
10
0w
ord
s
use
df
or
lon
gd
oc
um
en
ts
1-b
est
zh
ou
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
?
ch
art
on
Fro
m
the
en
do
fth
ed
oc
um
en
t,u
nti
la
n
an
tec
ed
en
tis
fou
nd
,o
r1
0m
en
tio
ns
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t
ML
Pw
ith
sco
re
of
0.5
use
df
or
lin
kin
ga
nd
10
me
nti
on
s
ya
ng
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Ma
xim
um
23
sen
ten
ces
to
the
lef
t;
Co
nst
rai
ne
dc
lus
ter
ing
ha
o
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Be
am
sea
rch
(L
uo
,2
00
4)
Pa
ck
ed
for
est
xin
xin
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Be
st-
firs
tc
lus
ter
ing
fol
low
ed
by
IL
P
op
tim
iza
tio
n
zh
an
g
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Wi
nd
ow
of
10
0m
ark
ab
les
ku
mm
erfi
eld
?
?
Pre
-a
nd
po
st-
res
olu
tio
nfi
lte
rs
Gi
ve
n+
Be
rke
ley
pa
rse
rp
ars
es;
pa
rse
s
wi
tho
ut
NM
Ls
im
pro
ve
dp
erf
orm
an
ce
sli
gh
tly
;re
-tr
ain
ed
Be
rke
ley
pa
rse
r
zh
ek
ov
a
Ex
am
ple
sin
the
pa
stt
hre
es
en
ten
ces
Fro
m
las
tp
oss
ibl
em
en
tio
ni
nd
oc
um
en
t
irw
in
Cl
ust
er
qu
ery
wi
th
NU
LL
clu
ste
rfo
rd
isc
ou
rse
new
me
nti
on
s
Cl
ust
er-
ran
kin
ga
pp
roa
ch
(ra
hm
an
,2
00
9)
Ta
ble
23
:P
art
ici
pa
tin
gs
yst
em
pro
file
s?
Pa
rtI
I.T
his
foc
use
so
nt
he
wa
yp
osi
tiv
ea
nd
ne
ga
tiv
ee
xa
mp
les
we
re
ge
ne
rat
ed
an
dt
he
de
co
din
gs
tra
teg
yu
sed
.
23
OntoNotes corpus, and presented the results from an
evaluation on learning such unrestricted entities and
events in text. The following represent our conclu-
sions on reviewing the results:
? Perhaps the most surprising finding was that the
best-performing system (lee) was completely
rule-based, rather than trained. This suggests
that their rule-based approach was able to do
a more effective job of combining the multiple
sources of evidence than the trained systems.
The features for coreference prediction are cer-
tainly more complex than for many other lan-
guage processing tasks, which makes it more
challenging to generate effective feature com-
binations. The rule-based approach used by
the best-performing system seemed to benefit
from a heuristic that captured the most con-
fident links before considering less confident
ones, and also made use of the information in
the guidelines in a slightly more refined man-
ner than other systems. They also included ap-
positives and copular constructions in their cal-
culations. Although OntoNotes does not count
those as instances of IDENT coreference, using
that information may have helped their system
discover additional useful links.
? It is interesting to note that the developers of
the lee system also did the experiment of run-
ning their system using gold standard informa-
tion on the individual layers, rather than auto-
matic model predictions. The somewhat sur-
prising result was that using perfect informa-
tion for the other layers did not end up improv-
ing coreference performance much, if at all. It
is not clear whether this means that: i) Auto-
matic predictors for the individual layers are
accurate enough already; ii) Information cap-
tured by those supplementary layers actually
does not provide much leverage for resolving
coreference; or iii) researchers have yet have
found an effective way of capturing and utiliz-
ing the extra information provided by these lay-
ers.
? It does seem that collecting information about
an entity by merging information across the
various attributes of the mentions that comprise
it can be useful, though not all systems that at-
tempted this achieved a benefit.
? System performance did not seem to vary as
much across the different genres as is nor-
mally the case with language processing tasks,
which could suggest that coreference is rela-
tively genre insensitive, or it is possible that
scores are two low for the difference to be ap-
parent. Comparisons are difficult, however, be-
cause the spoken genres were treated here with
perfect speech recognition accuracy and perfect
speaker turn information. Under more realis-
tic application conditions, the spread in perfor-
mance between genres might be greater.
? It is noteworthy that systems did not seem to
attempt the kind of joint inference that could
make use of the full potential of various layers
available in OntoNotes, but this could well have
been owing to the limited time available for the
shared task.
? We had expected to see more attention paid to
event coreference, which is a novel feature in
this data, but again, given the time constraints
and given that events represent only a small
portion of the total, it is not surprising that most
systems chose not to focus on it.
? Scoring coreference seems to remain a signif-
icant challenge. There does not seem to be an
objective way to establish one metric in prefer-
ence to another in the absence of a specific ap-
plication. On the other hand, the system rank-
ings do not seem terribly sensitive to the par-
ticular metric chosen. It is interesting that both
versions of the CEAF metric ? which tries to
capture the goodness of the entities in the out-
put ? seem much lower than the other metric,
though it is not clear whether that means that
our systems are doing a poor job of creating
coherent entities or whether that metric is just
especially harsh.
Finally, it is interesting to note that the problem of
coreference does not seem to be following the same
kind of learning curve that we are used to with other
problems of this sort. While performance has im-
proved somewhat, it is not clear how far we will be
able to go given the strategies at hand, or whether
new techniques will be needed to capture additional
information from the texts or from world knowl-
edge. We hope that this corpus and task will provide
a useful resource for continued experimentation to
help resolve this issue.
Acknowledgments
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
24
(DARPA/IPTO) under the GALE program,
DARPA/CMO Contract No. HR0011-06-C-0022.
We would like to thank all the participants. Without
their hard work, patience and perseverance this eval-
uation would not have been a success. We would
also like to thank the Linguistic Data Consortium
for making the OntoNotes 4.0 corpus freely and
timely available to the participants. Emili Sapena,
who graciously allowed the use of his scorer
implementation, and made available enhancements
and immediately fixed issues that were uncovered
during the evaluation. Finally, we offer our special
thanks to Llu??s Ma`rquez and Joakim Nivre for their
wonderful support and guidance without which this
task would not have been successful.
References
Olga Babko-Malaya, Ann Bies, Ann Taylor, Szuting Yi,
Martha Palmer, Mitch Marcus, Seth Kulick, and Li-
bin Shen. 2006. Issues in synchronizing the English
treebank and propbank. In Workshop on Frontiers in
Linguistically Annotated Corpora 2006, July.
Amit Bagga and Breck Baldwin. 1998. Algorithms for
Scoring Coreference Chains. In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563?566.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 33?40, Sydney,
Australia, July.
Jie Cai and Michael Strube. 2010. Evaluation metrics
for end-to-end coreference resolution systems. In Pro-
ceedings of the 11th Annual Meeting of the Special In-
terest Group on Discourse and Dialogue, SIGDIAL
?10, pages 28?36.
Eugene Charniak and Mark Johnson. 2001. Edit detec-
tion and parsing for transcribed speech. In Proceed-
ings of the Second Meeting of North American Chapter
of the Association of Computational Linguistics, June.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL), Ann
Arbor, MI, June.
Nancy Chinchor and Beth Sundheim. 2003. Message
understanding conference (MUC) 6. In LDC2003T13.
Nancy Chinchor. 2001. Message understanding confer-
ence (MUC) 7. In LDC2001T02.
Aron Culotta, Michael Wick, Robert Hall, and Andrew
McCallum. 2007. First-order probabilistic models for
coreference resolution. In HLT/NAACL, pages 81?88.
Pascal Denis and Jason Baldridge. 2007. Joint de-
termination of anaphoricity and coreference resolu-
tion using integer programming. In Proceedings of
HLT/NAACL.
Pascal Denis and Jason Baldridge. 2009. Global joint
models for coreference resolution and named entity
classification. Procesamiento del Lenguaje Natural,
(42):87?96.
Charles Fillmore, Christopher Johnson, and Miriam R. L.
Petruck. 2003. Background to framenet. Interna-
tional Journal of Lexicography, 16(3).
G. G. Doddington, A. Mitchell, M. Przybocki,
L. Ramshaw, S. Strassell, and R. Weischedel.
2000. The automatic content extraction (ACE)
program-tasks, data, and evaluation. In Proceedings
of LREC.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385?393, Los An-
geles, California, June.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2009): Shared Task, pages
1?18, Boulder, Colorado, June.
Sanda M. Harabagiu, Razvan C. Bunescu, and Steven J.
Maiorano. 2001. Text and knowledge mining for
coreference resolution. In NAACL.
L. Hirschman and N. Chinchor. 1997. Coreference task
definition (v3.0, 13 jul 97). In Proceedings of the Sev-
enth Message Understanding Conference.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% solution. In Proceedings of HLT/NAACL,
pages 57?60, New York City, USA, June. Association
for Computational Linguistics.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2000. A large-scale classification of
english verbs. Language Resources and Evaluation,
42(1):21 ? 40.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of Human Language
Technology Conference and Conference on Empirical
25
Methods in Natural Language Processing, pages 25?
32, Vancouver, British Columbia, Canada, October.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics, 19(2):313?330, June.
Andrew McCallum and Ben Wellner. 2004. Conditional
models of identity uncertainty with application to noun
coreference. In Advances in Neural Information Pro-
cessing Systems (NIPS).
Joseph McCarthy and Wendy Lehnert. 1995. Using de-
cision trees for coreference resolution. In Proceedings
of the Fourteenth International Conference on Artifi-
cial Intelligence, pages 1050?1055.
Thomas S. Morton. 2000. Coreference for nlp applica-
tions. In Proceedings of the 38th Annual Meeting of
the Association for Computational Linguistics, Octo-
ber.
Vincent Ng. 2007. Shallow semantics for coreference
resolution. In Proceedings of the IJCAI.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1396?1411, Uppsala, Swe-
den, July.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. 2007. Making fine-grained and coarse-grained
sense distinctions, both manually and automatically.
R. Passonneau. 2004. Computing reliability for corefer-
ence annotation. In Proceedings of LREC.
Massimo Poesio and Ron Artstein. 2005. The reliability
of anaphoric annotation, reconsidered: Taking ambi-
guity into account. In Proceedings of the Workshop on
Frontiers in Corpus Annotations II: Pie in the Sky.
Massimo Poesio. 2004. The mate/gnome scheme for
anaphoric annotation, revisited. In Proceedings of
SIGDIAL.
Simone Paolo Ponzetto and Massimo Poesio. 2009.
State-of-the-art nlp approaches to coreference resolu-
tion: Theory and practical recipes. In Tutorial Ab-
stracts of ACL-IJCNLP 2009, page 6, Suntec, Singa-
pore, August.
Simone Paolo Ponzetto and Michael Strube. 2005. Se-
mantic role labeling for coreference resolution. In
Companion Volume of the Proceedings of the 11th
Meeting of the European Chapter of the Associa-
tion for Computational Linguistics, pages 143?146,
Trento, Italy, April.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of the HLT/NAACL, pages 192?199, New York City,
N.Y., June.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James Martin, and Dan Jurafsky. 2005.
Support vector learning for semantic argument classi-
fication. Machine Learning Journal, 60(1):11?39.
Sameer Pradhan, Eduard Hovy, Mitchell Marcus, Martha
Palmer, Lance Ramshaw, and Ralph Weischedel.
2007a. OntoNotes: A Unified Relational Semantic
Representation. International Journal of Semantic
Computing, 1(4):405?419.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007b.
Unrestricted Coreference: Indentifying Entities and
Events in OntoNotes. In in Proceedings of the
IEEE International Conference on Semantic Comput-
ing (ICSC), September 17-19.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968?977, Singapore, Au-
gust. Association for Computational Linguistics.
W. M. Rand. 1971. Objective criteria for the evaluation
of clustering methods. Journal of the American Statis-
tical Association, 66(336).
Marta Recasens and Eduard Hovy. 2011. Blanc: Im-
plementing the rand index for coreference evaluation.
Natural Language Engineering.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena,
M. Anto`nia Mart??, Mariona Taule?, Ve?ronique
Hoste, Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in
multiple languages. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 1?8,
Uppsala, Sweden, July.
W. Soon, H. Ng, and D. Lim. 2001. A machine learn-
ing approach to coreference resolution of noun phrase.
Computational Linguistics, 27(4):521?544.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-
art. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 656?664, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings
26
of the Twelfth Conference on Computational Natu-
ral Language Learning, pages 159?177, Manchester,
England, August.
Yannick Versley. 2007. Antecedent selection techniques
for high-recall coreference resolution. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model theoretic coreference
scoring scheme. In Proceedings of the Sixth Message
Undersatnding Conference (MUC-6), pages 45?52.
Ralph Weischedel and Ada Brunstein. 2005. BBN pro-
noun coreference and entity type corpus LDC catalog
no.: LDC2005T33. BBN Technologies.
Ralph Weischedel, Eduard Hovy, Martha Palmer, Mitch
Marcus, Robert Belvin, Sameer Pradhan, Lance
Ramshaw, and Nianwen Xue. 2011. OntoNotes: A
Large Training Corpus for Enhanced Processing. In
Joseph Olive, Caitlin Christianson, and John McCary,
editors, Handbook of Natural Language Processing
and Machine Translation. Springer.
27
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 117?121,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
A Machine Learning-Based Coreference Detection System For OntoNotes
Yaqin Yang
Computer Science Department
Brandeis University
Waltham, Massachusetts, USA
yaqin@brandeis.edu
Nianwen Xue
Computer Science Department
Brandeis University
Waltham, Massachusetts, USA
xuen@brandeis.edu
Peter Anick
Computer Science Department
Brandeis University
Waltham, Massachusetts, USA
peter anick@yahoo.com
Abstract
In this paper, we describe the algorithms and
experimental results of Brandeis University
in the participation of the CoNLL Task 2011
closed track. We report the features used in
our system, and describe a novel cluster-based
chaining algorithm to improve performance of
coreference identification. We evaluate the
system using the OntoNotes data set and de-
scribe our results.
1 Introduction
This paper describes the algorithms designed and
experiments finished in the participation of the
CoNLL Task 2011. The goal of the Task is to design
efficient algorithms for detecting entity candidates
and identifying coreferences. Coreference identifi-
cation is an important technical problem. Its impor-
tance in NLP applications has been observed in pre-
vious work, such as that of Raghunathan et al, Prad-
han et al, Bergsma et al, Haghighi et al, and Ng et
al.. While most of the existing work has evaluated
their systems using the ACE data set, in this work
we present our experimental results based on the
OntoNotes data set used in the CoNLL 2011 Shared
Task. We detail a number of linguistic features
that are used during the experiments, and highlight
their contribution in improving coreference identi-
fication performance over the OntoNotes data set.
We also describe a cluster-based approach to multi-
entity chaining. Finally, we report experimental re-
sults and summarize our work.
2 Data Preparation
We divide the CoNLL Task into three steps. First,
we detect entities from both the training data and
the development data. Second, we group related en-
tities into entity-pairs. Finally, we use the gener-
ated entity-pairs in the machine learning-based clas-
sifier to identify coreferences. In this section, we
describe how we extract the entities and group them
into pairs.
2.1 Generating Entity Candidates
We use the syntactic parse tree to extract four types
of entities, including noun phrase, pronoun, pre-
modifier and verb (Pradhan et al, 2007). This
method achieves 94.0% (Recall) of detection accu-
racy for gold standard trees in the development data.
When using the automatic parses, not surprisingly,
the detection accuracy becomes lower, with a per-
formance drop of 5.3% (Recall) compared with that
of using the gold standard trees. Nevertheless, this
method can still cover 88.7% of all entities existing
in the development data, thus we used it in our algo-
rithm.
2.2 Generating Entity-Pairs From Individual
Entities
In the annotated training documents, an entity has
been marked in a coreference chain that includes all
coreferential entities. In our algorithm, we only de-
tect the closest antecedent for each entity, instead
of all coreferences, of each entity. Specifically, we
define each training and testing instance as a pair
of entities. During the training process, for each
entity encountered by the system, we create a pos-
itive instance by pairing an entity with its closest
antecedent (Soon et al, 2001). In addition, a set
of negative instances are also created by pairing the
entity with any preceding entities that exist between
its closest antecedent and the entity itself (note that
the antecedent must be a coreference of the current
entity, whereas preceding entities may not be coref-
erential). For example, in the entity sequence ?A,
B, C, D, E?, let us assume that ?A? is the closest
antecedent of ?D?. Then, for entity ?D?, ?A-D? is
considered a positive instance, whereas ?B-D? and
?C-D? are two negative instances.
To generate testing data, every entity-pair within
the same sentence is considered to form positive or
negative instances, which are then used to form test-
ing data. Since occasionally the distance between an
entity and its closest antecedent can be far apart, we
handle considerably distant coreferences by consid-
117
ering each entity-pair that exists within the adjacent
N sentences. During our experiments, we observed
that the distance between an entity and its closest an-
tecedent could be as far as 23 sentences. Therefore,
in the classification process, we empirically set N as
23.
3 Machine Learning-Based Classification
After labeling entity pairs, we formalize the corefer-
ence identification problem as a binary classification
problem. We derive a number of linguistic features
based on each entity-pair, i and j, where i is the po-
tential antecedent and j the anaphor in the pair (Soon
et al, 2001). Generally, we select a set of features
that have been proved to be useful for the corefer-
ence classification tasks in previous work, includ-
ing gender, number, distance between the antecedent
and the anaphor, and WordNet (WordNet, 2010). In
addition, we design additional features that could
be obtained from the OntoNotes data, such as the
speaker or author information that is mainly avail-
able in Broadcast Conversation and Web Log data
(Pradhan et al, 2007). Moreover, we extract appo-
sition and copular structures and used them as fea-
tures. The features we used in the system are de-
tailed below.
? Independent feature: 1) if a noun phrase is defi-
nite; 2) if a noun phrase is demonstrative; 3) gender
information of each entity; 4) number information
of each entity; 5) the entity type of a noun phrase;
6) if an entity is a subject; 7) if an entity is an object;
8) if an noun phrase is a coordination, the number
of entities it has; 9) if a pronoun is preceded by a
preposition; 10) if a pronoun is ?you? or ?me?; 11)
if a pronoun is ?you? and it is followed by the word
?know?.
? Name entity feature: 1) i-j-same-entity-type-
etype=True, if i and j have the same entity type;
2) i-j-same-etype-subphrase=True, if i and j have
the same entity type and one is the subphrase of the
other.
? Syntactic feature: 1) i-j-both-subject=True, if i
and j are both subjects; 2) if i and j are in the same
sentence, record the syntactic path between i and j,
e.g. i-j-syn-path=PRP?NP!PRP; 3) i-j-same-sent-
diff-clause=True, if i and j are in the same sentence
but in different clauses.
? Gender and number feature: 1) i-j-same-
gender=True/False, by comparing if i and j have the
same gender; 2) i-j-same-num=True/False, by com-
paring if i and j have the same number; 3) i-j-same-
num-modifier=True/False, by comparing if i and j
have the same number modifier, e.g. ?two coun-
tries? and ?they both? have the same number mod-
ifier; 4) i-j-same-family=True/False, we designed
seven different families for pronouns, e.g. ?it?, ?its?
and ?itself? are in one family while ?he?, ?him?,
?his? and ?himself? are in another one.
? Distance feature: 1) i-j-sent-dist, if the sentence
distance between i and j is smaller than three, use
their sentence distance as a feature; 2) i-j-sent-
dist=medium/far: if the sentence distance is larger
than or equal to three, set the value of i-j-sent-dist
to ?medium?, otherwise set it to ?far? combined
with the part-of-speech of the head word in j.
? String and head word match feature: 1) i-j-
same-string=True, if i and j have the same string;
2) i-j-same-string-prp=True, if i and j are the
same string and they are both pronouns; 3) i-j-sub-
string=True, if one is the sub string of the other,
and neither is a pronoun; 4) i-j-same-head=True,
if i and j have the same head word; 5) i-j-prefix-
head=True, if the head word of i or j is the pre-
fix of the head word of the other; 6) i-j-loose-head,
the same as i-j-prefix-head, but comparing only the
first four letters of the head word.
? Apposition and copular feature: for each noun
phrase, if it has an apposition or is followed by
a copular verb, then the apposition or the subject
complement is used as an attribute of that noun
phrase. We also built up a dictionary where the
key is the noun phrase and the value is its apposi-
tion or the subject?s complement to define features.
1) i-appo-j-same-head=True, if i?s apposition and
j have the same head word; 2) i-j-appo-same-
head=True, if j?s apposition has the same head word
as i; we define the similar head match features for
the noun phrase and its complement; Also, if an i
or j is a key in the defined dictionary, we get the
head word of the corresponding value for that key
and compare it to the head word of the other entity.
? Alias feature: i-j-alias=True, if one entity is a
proper noun, then we extract the first letter of each
word in the other entity. ( The extraction process
skips the first word if it?s a determiner and also skips
the last one if it is a possessive case). If the proper
noun is the same as the first-letter string, it is the
alias of the other entity.
? Wordnet feature: for each entity, we used Wordnet
to generate all synsets for its head word, and for
each synset, we get al hypernyms and hyponyms.
1) if i is a hypernym of j, then i-hyper-j=True; 2)
if i is a hyponym of j, then i-hypo-j=True.
? Speaker information features: In a conversation,
a speaker usually uses ?I? to refer to himself/herself,
and most likely uses ?you? to refer to the next
speaker. Since speaker or author name informa-
tion is given in Broadcast Conversation and Web
Log data, we use such information to design fea-
tures that represent relations between pronouns and
118
speakers. 1) i-PRP1-j-PRP2-same-speaker=True,
if both i and j are pronouns, and they have the same
speaker; 2) i-I-j-I-same-speaker=True, if both i and
j are ?I?, and they have the same speaker; 3) i-I-j-
you-same-speaker=True, if i is ?I? and j is ?you?,
and they have the same speaker; 4) if i is ?I?, j
is ?you? and the speaker of j is right after that of
i, then we have feature i-I-j-you&itarget=jspeaker;
5) if i is ?you?, j is ?I? and the speaker of j is
right after that of i, then we have feature i-you-
j-I-itarget=jspeaker; 6) if both i and j are ?you?,
and they followed by the same speaker, we consider
?you? as a general term, and this information is used
as a negative feature.
? Other feature: i-j-both-prp=True, if both i and j
are pronouns.
4 Chaining by Using Clusters
After the classifier detects coreferential entities,
coreference detection systems usually need to chain
multiple coreferential entity-pairs together, forming
a coreference chain. A conventional approach is
to chain all entities in multiple coreferential entity-
pairs if they share the same entities. For example, if
?A-B?, ?B-C?, and ?C-D? are coreferential entity-
pairs, then A, B, C, and D would be chained to-
gether, forming a coreference chain ?A-B-C-D?.
One significant disadvantage of this approach is
that it is likely to put different coreference chains to-
gether in the case of erroneous classifications. For
example, suppose in the previous case, ?B-C? is ac-
tually a wrong coreference detection, then the coref-
erence chain created above will cause A and D to be
mistakenly linked together. This error can propagate
as coreference chains become larger.
To mitigate this issue, we design a cluster-based
chaining approach. This approach is based on the
observation that some linguistic rules are capable of
detecting coreferential entities with high detection
precision. This allows us to leverage these rules to
double-check the coreference identifications, and re-
ject chaining entities that are incompatible with rule-
based results.
To be specific, we design two lightweight yet ef-
ficient rules to cluster entities.
? Rule One. For the first noun phrase (NP) encoun-
tered by the system, if 1) this NP has a name entity
on its head word position or 2) it has a name en-
tity inside and the span of this entity includes the
head word position, a cluster is created for this NP.
The name entity of this NP is also recorded. For
each following NP with a name entity on its head
word position, if there is a cluster that has the same
name entity, this NP is considered as a coreference
to other NPs in that cluster, and is put into that clus-
ter. If the system cannot find such a cluster, a new
cluster is created for the current NP.
? Rule Two. In Broadcast Conversation or Web Log
data, a speaker or author would most likely use ?I?
to refer to himself/herself. Therefore, we used it
as the other rule to cluster all ?I? pronouns and the
same speaker information together.
Given the labeled entity pairs, we then link them in
different coreference chains by using the cluster in-
formation. As the Maximum Entropy classifier not
only labels each entity-pair but also returns a con-
fidence score of that label, we sort all positive pairs
using their possibilities. For each positive entity-pair
in the sorted list, if the two entities are in different
clusters, we consider this to be a conflict, and with-
draw this positive entity-pair; if one entity belongs to
one cluster whereas the other does not belong to any
cluster, the two entities will be both included in that
cluster. This process is repeated until no more enti-
ties can be included in a cluster. Finally, we chain
the rest of entity pairs together.
5 Results and Discussion
To evaluate the features and the chaining approach
described in this paper, we design experiments de-
scribed as follows. Since there are five different
data types in the provided OntoNotes coreference
data set, we create five different classifiers to pro-
cess each of the data types. We used the features
described in Section 3 to train the classifiers, and
did the experiments using a Maximum Entropy clas-
sifier trained with the Mallet package (McCallum,
2002). We use the gold-standard data in the training
set to train the five classifiers and test the classifiers
on both gold and automatically-parsed data in the
development data set. The MUC metric provided by
the Task is used to evaluate the results.
5.1 Performance without Clustering
First, we evaluate the system by turning the clus-
tering technique off during the process of creating
coreference chains. For entity detection, we ob-
serve that for all five data types, i.e. Broadcast
(BC), Broad news (BN), Newswire (NW), Magazine
(MZ), and Web blog (WB), the NW and WB data
types achieve relatively lower F1-scores, whereas
the BC, BN, and MZ data types achieve higher per-
119
BC BN NW MZ WB
Without Clustering
Gold 57.40 (64.92/51.44) 59.45 (63.53/55.86) 52.01 (59.71/46.07) 55.59 (62.90/49.80) 49.53 (61.16/41.62)
Auto 54.00 (61.28/48.26) 55.40 (59.05/52.17) 48.44 (55.32/43.09) 52.21 (59.78/46.33) 47.02 (58.33/39.39)
With Clustering
Gold 57.44 (64.12/52.03) 56.56 (58.10/55.09) 51.37 (56.64/46.99) 54.26 (60.07/49.47) 49.00 (60.09/41.36)
Auto 54.19 (60.82/48.87) 52.69 (54.07/51.37) 48.01 (52.74/44.05) 50.82 (56.76/46.01) 46.86 (57.49/39.55)
Table 1: Performance comparison of coreference identification between using and without using the clustering tech-
nique in chaining. Note that the results are listed in sequence of F1-scores (Recalls/Precisions). The results shown are
based on MUC.
formance. Due to limited space, the performance
table of entity detection is not included in this paper.
For coreference identification, as shown in Ta-
ble 1, we observe pretty similar performance gaps
among different data types. The NW and WB data
types achieve the lowest F1-scores (i.e. 52.01%
and 49.53% for gold standard data, and 48.44% and
47.02% for automatically-parsed data) among all the
five data types. This can be explained by seeing that
the entity detection performance of these two data
types are also relatively low. The other three types
achieves more than 55% and 52% F1-scores for gold
and auto data, respectively.
These experiments that are done without using
clustering techniques tend to indicate that the perfor-
mance of entity detection has a positive correlation
with that of coreference identification. Therefore, in
the other set of experiments, we enable the cluster-
ing technique to improve coreference identification
performance by increasing entity detection accuracy.
Metric Recall Precision F1
MUC 59.94 45.38 51.65
BCUBED 72.07 53.65 61.51
CEAF (M) 45.67 45.67 45.67
CEAF (E) 29.43 42.54 34.79
BLANC 70.86 60.55 63.37
Table 2: Official results of our system in the CoNLL Task
2011. Official score is 49.32. ((MUC + BCUBED +
CEAF (E))/3)
5.2 Performance with Clustering
After enabling the clustering technique, we observe
an improvement in entity detection performance.
This improvement occurs mainly in the cases of the
NW and WB data types, which show low entity
detection performance when not using the cluster-
ing technique. To be specific, the performance of
the NW type on both the gold standard and auto-
matic data improves by about 0.5%, and the perfor-
mance of the WB type on the automatic data im-
proves about 0.1%. In addition, the performance of
the BC type on both the gold standard and automatic
data also increases about 0.2% to 0.6%.
Although the clustering technique succeeds in im-
proving entity detection performance for multiple
data types, there is no obvious improvement gained
with respect to coreference identification. This is
quite incompatible with our observation in the ex-
periments that do not utilize the clustering tech-
nique. Currently, we attribute this issue to the low
accuracy rates of the clustering operation. For ex-
ample, ?H. D. Ye.? and ?Ye? can be estimated cor-
rectly to be coreferential by the Maxtent classifier,
but the clustering algorithm puts them into different
clusters since ?H. D. Ye.? is a PERSON type name
entity while ?Ye? is a ORG type name entity. There-
fore, the system erroneously considers them to be a
conflict and rejects them. We plan to investigate this
issue further in our future work.
The official results of our system in the CoNLL
Task 2011 are summarized in Table 2.
6 Conclusion
In this paper, we described the algorithm design and
experimental results of Brandeis University in the
CoNLL Task 2011. We show that several linguistic
features perform well in the OntoNotes data set.
References
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
120
Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted Coreference: Identifying Entities and Events
in OntoNotes. International Conference on Semantic
Computing (ICSC 2007), pages 446?453, September.
W.M. Soon, H.T. Ng, and D.C.Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521?
544.
WordNet. 2010. Princeton University ?About
WordNet.? WordNet. Princeton University. 2010.
http://wordnet.princeton.edu.
121
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 1?40,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
CoNLL-2012 Shared Task:
Modeling Multilingual Unrestricted Coreference in OntoNotes
Sameer Pradhan
Raytheon BBN Technologies,
Cambridge, MA 02138
USA
pradhan@bbn.com
Alessandro Moschitti
University of Trento,
38123 Povo (TN)
Italy
moschitti@disi.unitn.it
Nianwen Xue
Brandeis University,
Waltham, MA 02453
USA
xuen@cs.brandeis.edu
Olga Uryupina
University of Trento,
38123 Povo (TN)
Italy
uryupina@gmail.com
Yuchen Zhang
Brandeis University,
Waltham, MA 02453
USA
yuchenz@brandeis.edu
Abstract
The CoNLL-2012 shared task involved pre-
dicting coreference in English, Chinese, and
Arabic, using the final version, v5.0, of the
OntoNotes corpus. It was a follow-on to the
English-only task organized in 2011. Un-
til the creation of the OntoNotes corpus, re-
sources in this sub-field of language process-
ing were limited to noun phrase coreference,
often on a restricted set of entities, such as
the ACE entities. OntoNotes provides a large-
scale corpus of general anaphoric coreference
not restricted to noun phrases or to a spec-
ified set of entity types, and covers multi-
ple languages. OntoNotes also provides ad-
ditional layers of integrated annotation, cap-
turing additional shallow semantic structure.
This paper describes the OntoNotes annota-
tion (coreference and other layers) and then
describes the parameters of the shared task in-
cluding the format, pre-processing informa-
tion, evaluation criteria, and presents and dis-
cusses the results achieved by the participat-
ing systems. The task of coreference has
had a complex evaluation history. Potentially
many evaluation conditions, have, in the past,
made it difficult to judge the improvement in
new algorithms over previously reported re-
sults. Having a standard test set and stan-
dard evaluation parameters, all based on a re-
source that provides multiple integrated anno-
tation layers (syntactic parses, semantic roles,
word senses, named entities and coreference)
and in multiple languages could support joint
modeling and help ground and energize on-
going research in the task of entity and event
coreference.
1 Introduction
The importance of coreference resolution for the
entity/event detection task, namely identifying all
mentions of entities and events in text and clustering
them into equivalence classes, has been well recog-
nized in the natural language processing community.
Early work on corpus-based coreference resolu-
tion dates back to the mid-90s by McCarthy and
Lenhert (1995) where they experimented with deci-
sion trees and hand-written rules. Corpora to support
supervised learning of this task date back to the Mes-
sage Understanding Conferences (MUC) (Hirschman
and Chinchor, 1997; Chinchor, 2001; Chinchor and
Sundheim, 2003). The de facto standard datasets
for current coreference studies are the MUC and the
ACE1 (Doddington et al, 2004) corpora. These cor-
pora were tagged with coreferring entities in the
form of noun phrases in the text. The MUC corpora
cover all noun phrases in text but are relatively small
in size. The ACE corpora, on the other hand, cover
much more data, but the annotation is restricted to a
small subset of entities.
Automatic identification of coreferring entities
and events in text has been an uphill battle for sev-
eral decades, partly because it is a problem that re-
quires world knowledge to solve and word knowl-
edge is hard to define, and partly owing to the lack
of substantial annotated data. Aside from the fact
that resolving coreference in text is simply a very
hard problem, there have been other hindrances that
further contributed to the slow progress in this area:
(i) Smaller sized corpora such as MUC which cov-
ered coreference across all noun phrases. Cor-
pora such as ACE which are larger in size, but
cover a smaller set of entities; and
(ii) low consistency in existing corpora annotated
with coreference ? in terms of inter-annotator
agreement (ITA) (Hirschman et al, 1998) ?
owing to attempts at covering multiple coref-
erence phenomena that are not equally anno-
tatable with high agreement which likely less-
ened the reliability of statistical evidence in the
form of lexical coverage and semantic related-
ness that could be derived from the data and
1http://projects.ldc.upenn.edu/ace/data/
1
used by a classifier to generate better predic-
tive models. The importance of a well-defined
tagging scheme and consistent ITA has been
well recognized and studied in the past (Poe-
sio, 2004; Poesio and Artstein, 2005; Passon-
neau, 2004). There is a growing consensus that
in order to take language understanding appli-
cations such as question answering or distilla-
tion to the next level, we need more consistent
annotation for larger amounts of broad cover-
age data to train better automatic models for
entity and event detection.
(iii) Complex evaluation with multiple evaluation
metrics and multiple evaluation scenarios,
complicated with varying training and test
partitions, led to situations where many re-
searchers report results with only one or a few
of the available metrics and under a subset of
evaluation scenarios. This has made it hard to
gauge the improvement in algorithms over the
years (Stoyanov et al, 2009), or to determine
which particular areas require further attention.
Looking at various numbers reported in litera-
ture can greatly affect the perceived difficulty
of the task. It can seem to be a very hard prob-
lem (Soon et al, 2001) or one that is relatively
easy (Culotta et al, 2007).
(iv) the knowledge bottleneck which has been a
well-accepted ceiling that has kept the progress
in this task at bay.
These issues suggest that the following steps
might take the community in the right direction to-
wards improving the state of the art in coreference
resolution:
(i) Create a large corpus with high inter-
annotator agreement possibly by restricting
the coreference annotating to phenomena that
can be annotated with high consistency, and
covering an unrestricted set of entities and
events; and
(ii) Create a standard evaluation scenario with an
official evaluation setup, and possibly several
ablation settings to capture the range of perfor-
mance. This can then be used as a standard
benchmark by the research community.
(iii) Continue to improve learning algorithms that
better incorporate world knowledge and jointly
incorporate information from other layers of
syntactic and semantic annotation to improve
the state of the art.
One of the many goals of the OntoNotes
project2 (Hovy et al, 2006; Weischedel et al, 2011)
2http://www.bbn.com/nlp/ontonotes
was to explore whether it could fill this void and help
push the progress further ? not only in coreference,
but with the various layers of semantics that it tries
to capture. As one of its layers, it has created a
corpus for general anaphoric coreference that cov-
ers entities and events not limited to noun phrases
or a subset of entity types. The coreference layer
in OntoNotes constitutes just one part of a multi-
layered, integrated annotation of shallow semantic
structures in text with high inter-annotator agree-
ment. This addresses the first issue.
In the language processing community, the field
of speech recognition probably has the longest his-
tory of shared evaluations held primary by NIST3
(Pallett, 2002). In the past decade machine trans-
lation has been a topic of shared evaluations also
by NIST4. There are many syntactic and semantic
processing tasks that are not quite amenable to such
continued evaluation efforts. The CoNLL shared
tasks over the past 15 years have filled that gap, help-
ing establish benchmarks and advance the state of
the art in various sub-fields within NLP. The impor-
tance of shared tasks is now in full display in the
domain of clinical NLP (Chapman et al, 2011) and
recently a coreference task was organized as part
of the i2b2 workshop (Uzuner et al, 2012). The
computational learning community is also witness-
ing a shift towards joint inference based evaluations,
with the two previous CoNLL tasks (Surdeanu et al,
2008; Hajic? et al, 2009) devoted to joint learning of
syntactic and semantic dependencies. A SemEval-
2010 coreference task (Recasens et al, 2010) was
the first attempt to address the second issue. It
included six different Indo-European languages ?
Catalan, Dutch, English, German, Italian, and Span-
ish. Among other corpora, a small subset (?120K)
of English portion of OntoNotes was used for this
purpose. However, the lack of a strong participa-
tion prevented the organizers from reaching any firm
conclusions. The CoNLL-2011 shared task was an-
other attempt to address the second issue. It was well
received, but the shared task was only limited to the
English portion of OntoNotes. In addition, the coref-
erence portion of OntoNotes did not have a concrete
baseline prior to the 2011 evaluation, thereby mak-
ing it challenging for participants to gauge the per-
formance of their algorithms in the absence of es-
tablished state of the art on this flavor of annotation.
The closest comparison was to the results reported
by Pradhan et al (2007b) on the newswire portion of
OntoNotes. Since the corpus also covers two other
languages from completely different language fami-
lies, Chinese and Arabic, it provided a great oppor-
tunity to have a follow-on task in 2012 covering all
3http://www.itl.nist.gov/iad/mig/publications/ASRhistory/index.html
4http://www.itl.nist.gov/iad/mig/tests/mt/
2
three languages. As we will see later, peculiarities
of each of these languages had to be considered in
creating the evaluation framework.
The first systematic learning-based study in coref-
erence resolution was conducted on the MUC cor-
pora, using a decision tree learner, by Soon et al
(2001). Significant improvements have been made
in the field of language processing in general, and
improved learning techniques have pushed the state
of the art in coreference resolution forward (Mor-
ton, 2000; Harabagiu et al, 2001; McCallum and
Wellner, 2004; Culotta et al, 2007; Denis and
Baldridge, 2007; Rahman and Ng, 2009; Haghighi
and Klein, 2010). Researchers have continued to
find novel ways of exploiting ontologies such as
WordNet. Various knowledge sources from shallow
semantics to encyclopedic knowledge have been ex-
ploited (Ponzetto and Strube, 2005; Ponzetto and
Strube, 2006; Versley, 2007; Ng, 2007). Given
that WordNet is a static ontology and as such has
limitation on coverage, more recently, there have
been successful attempts to utilize information from
much larger, collaboratively built resources such as
Wikipedia (Ponzetto and Strube, 2006). More re-
cently researchers have used graph based algorithms
(Cai et al, 2011a) rather than pair-wise classifica-
tions. For a detailed survey of the progress in this
field, we refer the reader to a recent article (Ng,
2010) and a tutorial (Ponzetto and Poesio, 2009)
dedicated to this subject. In spite of all the progress,
current techniques still rely primarily on surface
level features such as string match, proximity, and
edit distance; syntactic features such as apposition;
and shallow semantic features such as number, gen-
der, named entities, semantic class, Hobbs? distance,
etc. Further research to reduce the knowledge gap is
essential to take coreference resolution techniques to
the next level.
The rest of the paper is organized as follows: Sec-
tion 2 presents an overview of the OntoNotes cor-
pus. Section 3 describes the range of phenomena
annotated in OntoNotes, and language-specific is-
sues. Section 4 describes the shared task data and
the evaluation parameters, with Section 4.4.2 exam-
ining the performance of the state-of-the-art tools
on all/most intermediate layers of annotation. Sec-
tion 5 describes the participants in the task. Sec-
tion 6 briefly compares the approaches taken by var-
ious participating systems. Section 7 presents the
system results with some analysis. Section 8 com-
pares the performance of the systems on the a subset
of the Engish test set that corresponds with the test
set used for the CoNLL-2011 evaluation. Section 9
draws some conclusions.
2 The OntoNotes Corpus
The OntoNotes project has created a large-scale
corpus of accurate and integrated annotation of mul-
tiple levels of the shallow semantic structure in text.
The English and Chinese language portion com-
prises roughly one million words per language of
newswire, magazine articles, broadcast news, broad-
cast conversations, web data and conversational
speech data. The English subcorpus also contains
an additional 200K words of the English translation
of the New Testament as Pivot Text. The Arabic por-
tion is smaller, comprising 300K words of newswire
articles. The hope is that this rich, integrated an-
notation covering many layers will allow for richer,
cross-layer models and enable significantly better
automatic semantic analysis. In addition to coref-
erence, this data is also tagged with syntactic trees,
propositions for most verb and some noun instances,
partial verb and noun word senses, and 18 named en-
tity types. Manual annotation of a large corpus with
multiple layers of syntax and semantic information
is a costly endeavor. Over the years in the devel-
opment of this corpus, there were various priorities
that came into play, and therefore not all the data in
the corpus could be annotated with all the different
layers of annotation. However, such multi-layer an-
notations, with complex, cross-layer dependencies,
demands a robust, efficient, scalable storage mech-
anism while providing efficient, convenient, inte-
grated access to the the underlying structure. To
this effect, it uses a relational database representa-
tion that captures both the inter- and intra-layer de-
pendencies and also provides an object-oriented API
for efficient, multi-tiered access to this data (Prad-
han et al, 2007a). This facilitates the extraction of
cross-layer features in integrated predictive models
that will make use of these annotations.
OntoNotes comprises the following layers of an-
notation:
? Syntax ? A layer of syntactic annotation for
English, Chinese and Arabic based on a revised
guidelines for the Penn Treebank (Marcus et
al., 1993; Babko-Malaya et al, 2006), the Chi-
nese Treebank (Xue et al, 2005) and the Arabic
Treebank (Maamouri and Bies, 2004).
? Propositions ? The proposition structure of
verbs based on revised guidelines for the En-
glish PropBank (Palmer et al, 2005; Babko-
Malaya et al, 2006), the Chinese PropBank
(Xue and Palmer, 2009) and the Arabic Prop-
Bank (Palmer et al, 2008; Zaghouani et al,
2010).
? Word Sense ? Coarse-grained word senses
are tagged for the most frequent polysemous
verbs and nouns, in order to maximize token
3
coverage. The word sense granularity is tai-
lored to achieve 90% inter-annotator agreement
as demonstrated by Palmer et al (2007). These
senses are defined in the sense inventory files.
In case of English and Arabic languages, the
sense-inventories (and frame files) are defined
separately for each part of speech that is real-
ized by the lemma in the text. For Chinese,
however the sense inventories (and frame files)
are defined per lemma ? independent of the
part of speech realized in the text. For the
English portion of OntoNotes, each individual
sense has been connected to multiple WordNet
senses. This provides users direct access to the
WordNet semantic structure. There is also a
mapping from the OntoNotes word senses to
PropBank frames and to VerbNet (Kipper et
al., 2000) and FrameNet (Fillmore et al, 2003).
Unfortunately, owing to lack of comparable re-
sources as comprehensive as WordNet in Chi-
nese or Arabic, neither language has any inter-
resource mappings available.
? Named Entities ? The corpus was tagged
with a set of 18 well-defined proper named en-
tity types that have been tested extensively for
inter-annotator agreement by Weischedel and
Burnstein (2005).
? Coreference ? This layer captures general
anaphoric coreference that covers entities and
events not limited to noun phrases or a lim-
ited set of entity types (Pradhan et al, 2007b).
It considers all pronouns (PRP, PRP$), noun
phrases (NP) and heads of verb phrases (VP)
as potential mentions. Unlike English, Chinese
and Arabic have dropped subjects and objects
which were also considered during coreference
annotation5. We will take a look at this in detail
in the next section.
3 Coreference in OntoNotes
General anaphoric coreference that spans a rich
set of entities and events ? not restricted to a few
types, as has been characteristic of most coreference
data available until now ? has been tagged with a
high degree of consistency in the OntoNotes corpus.
Two different types of coreference are distinguished:
Identity (IDENT), and Appositive (APPOS). Identity
coreference (IDENT) is used for anaphoric corefer-
ence, meaning links between pronominal, nominal,
and named mentions of specific referents. It does not
include mentions of generic, underspecified, or ab-
stract entities. Appositives (APPOS) are treated sep-
arately because they function as attributions, as de-
scribed further below. Coreference is annotated for
all specific entities and events. There is no limit on
5As we will see later these are not used during the task.
the semantic types of NP entities that can be consid-
ered for coreference, and in particular, coreference
is not limited to ACE types. The guidelines are fairly
language independent. We will look at some salient
aspects of the coreference annotation in OntoNotes.
For more details, and examples, we refer the reader
to the release documentation. We will primarily use
English examples to describe various aspects of the
annotation and use Chinese and Arabic examples es-
pecially to illustrate phenomena not observed in En-
glish, or that have some language specific peculiari-
ties.
3.1 Noun Phrases
The mentions over which IDENT coreference ap-
plies are typically pronominal, named, or definite
nominal. The annotation process begins by automat-
ically extracting all of the NP mentions from parse
trees in the syntactic layer of OntoNotes annotation,
though the annotators can also add additional men-
tions when appropriate. In the following two exam-
ples (and later ones), the phrases in bold form the
links of an IDENT chain.
(1) She had a good suggestion and it was unani-
mously accepted by all.
(2) Elco Industries Inc. said it expects net income
in the year ending June 30, 1990, to fall below a
recent analyst?s estimate of $ 1.65 a share. The
Rockford, Ill. maker of fasteners also said it
expects to post sales in the current fiscal year
that are ?slightly above? fiscal 1989 sales of $
155 million.
Noun phrases (NPs) in Chinese can be complex
noun phrases or bare nouns (nouns that lack a de-
terminer such as ?the? or ?this?). Complex noun
phrases contain structures modifying the head noun,
as in the following examples:
(3) (??????????? ? (???
????? (???))).
((His last APEC (summit meeting)) as the
President)
(4) (?? ?? ? (?? ? ?? ?? ?? ?
(????)))
((The first (U.S. president)) who went to visit
Vietnam after its unification)
In these examples, the smallest phrase in paren-
theses is the bare noun. The longer phrase in paran-
theses includes modifying structures. All the expres-
sions in the parantheses, however, share the same
head noun, i.e., ???? (summit meeting)?, and
????? (U.S. president)? respectively. Nested
noun phrases, or nested NPs, are contained within
4
longer noun phrases. In the above example, ?sum-
mit meeting? and ?U.S. president? are nested NPs.
Wherever NPs are nested, the largest logical span is
used in coreference.
3.2 Verbs
Verbs are added as single-word spans if they can
be coreferenced with a noun phrase or with another
verb. The intent is to annotate the VP, but the single-
word verb head is marked for convenience. This
includes morphologically related nominalizations as
in (5) and noun phrases that refer to the same event,
even if they are lexically distinct from the verb as in
(6). In the following two examples, only the chains
related to the growth event are shown in bold. The
Arabic translation of the same example identifies
mentions using parantheses.
(5) The European economy grew rapidly over the
past years, this growth helped raising ....
H@?
	
J??@ ?C
	
g

??Q??. ?

G
.
?P?

B@ XA?

J

?B

@ ( A? 	? ) Y??
. . . ?
	
P? ?


	
? ??A? ( ??	J? @ @ 	Y? ) , ?J

	
?A?? @
(6) Japan?s domestic sales of cars, trucks and buses
in October rose 18% from a year earlier to
500,004 units, a record for the month, the Japan
Automobile Dealers? Association said. The
strong growth followed year-to-year increases
of 21% in August and 12% in September.
3.3 Pronouns
All pronouns and demonstratives are linked to
anything that they refer to, and pronouns in quoted
speech are also marked. Expletive or pleonastic pro-
nouns (it, there) are not considered for tagging, and
generic you is not marked. In the following exam-
ple, the pronoun you and it would not be marked. (In
this and following examples, an asterisk (*) before a
boldface phrase identifies entity/event mentions that
would not be tagged in the coreference annotation.)
(7) Senate majority leader Bill Frist likes to tell
a story from his days as a pioneering heart
surgeon back in Tennessee. A lot of times,
Frist recalls, *you?d have a critical patient ly-
ing there waiting for a new heart, and *you?d
want to cut, but *you couldn?t start unless *you
knew that the replacement heart would make
*it to the operating room.
In Chinese, all the following pronouns ? ??
???, ?, ?? ????????????
?, ?, ?? (you, me, he, she, and so on), and
demonstrative pronouns ?????????,?
? (this, that, these, those) in singular, plural or pos-
sessive forms are linked to anything they refer to.
Pronouns from classical Chinese such as ? ?
(among which),? (he/she/it),? (he/she/it) are also
linked with other mentions to which they refer.
In Arabic, the following pronouns are corefer-
enced ? nominative personal pronouns (subject) and
demonstrative pronouns which are detached. Sub-
ject pronouns are often null in Arabic; overt subject
pronouns are rare, but do occur.
	?
	
K @ / ?

?
	
K @ / A?

J
	
K @ / 	?mProceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 143?152,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Towards Robust Linguistic Analysis Using OntoNotes
Sameer Pradhan1, Alessandro Moschitti2,3, Nianwen Xue4, Hwee Tou Ng5
Anders Bjo?rkelund6, Olga Uryupina2, Yuchen Zhang4 and Zhi Zhong5
1 Boston Childrens Hospital and Harvard Medical School, Boston, MA 02115, USA
2 University of Trento, University of Trento, 38123 Povo (TN), Italy
3 QCRI, Qatar Foundation, 5825 Doha, Qatar
4 Brandeis University, Brandeis University, Waltham, MA 02453, USA
5 National University of Singapore, Singapore, 117417
6 University of Stuttgart, 70174 Stuttgart, Germany
Abstract
Large-scale linguistically annotated cor-
pora have played a crucial role in advanc-
ing the state of the art of key natural lan-
guage technologies such as syntactic, se-
mantic and discourse analyzers, and they
serve as training data as well as evaluation
benchmarks. Up till now, however, most
of the evaluation has been done on mono-
lithic corpora such as the Penn Treebank,
the Proposition Bank. As a result, it is still
unclear how the state-of-the-art analyzers
perform in general on data from a vari-
ety of genres or domains. The completion
of the OntoNotes corpus, a large-scale,
multi-genre, multilingual corpus manually
annotated with syntactic, semantic and
discourse information, makes it possible
to perform such an evaluation. This paper
presents an analysis of the performance of
publicly available, state-of-the-art tools on
all layers and languages in the OntoNotes
v5.0 corpus. This should set the bench-
mark for future development of various
NLP components in syntax and semantics,
and possibly encourage research towards
an integrated system that makes use of the
various layers jointly to improve overall
performance.
1 Introduction
Roughly a million words of text from the Wall
Street Journal newswire (WSJ), circa 1989, has
had a significant impact on research in the lan-
guage processing community ? especially those
in the area of syntax and (shallow) semantics, the
reason for this being the seminal impact of the
Penn Treebank project which first selected this text
for annotation. Taking advantage of a solid syn-
tactic foundation, later researchers who wanted to
annotate semantic phenomena on a relatively large
scale, also used it as the basis of their annota-
tion. For example the Proposition Bank (Palmer et
al., 2005), BBN Name Entity and Pronoun coref-
erence corpus (Weischedel and Brunstein, 2005),
the Penn Discourse Treebank (Prasad et al, 2008),
and many other annotation projects, all annotate
the same underlying body of text. It was also con-
verted to dependency structures and other syntac-
tic formalisms such as CCG (Hockenmaier and
Steedman, 2002) and LTAG (Shen et al, 2008),
thereby creating an even bigger impact through
these additional syntactic resources. The most re-
cent one of these efforts is the OntoNotes corpus
(Weischedel et al, 2011). However, unlike the
previous extensions of the Treebank, in addition
to using roughly a third of the same WSJ subcor-
pus, OntoNotes also added several other genres,
and covers two other languages ? Chinese and
Arabic: portions of the Chinese Treebank (Xue et
al., 2005) and the Arabic Treebank (Maamouri and
Bies, 2004) have been used to sample the genre of
text that they represent.
One of the current hurdles in language process-
ing is the problem of domain, or genre adaptation.
Although genre or domain are popular terms, their
definitions are still vague. In OntoNotes, ?genre?
means a type of source ? newswire (NW), broad-
cast news (BN), broadcast conversation (BC), mag-
azine (MZ), telephone conversation (TC), web data
(WB) or pivot text (PT). Changes in the entity and
event profiles across source types, and even in the
same source over a time duration, as explicitly ex-
pressed by surface lexical forms, usually account
for a lot of the decrease in performance of mod-
els trained on one source and tested on another,
usually because these are the salient cues that are
relied upon by statistical models.
Large-scale corpora annotated with multiple
layers of linguistic information exist in various
languages, but they typically consist of a single
source or collection. The Brown corpus, which
consists of multiple genres, have been usually used
to investigate issues of genres of sensitivity, but it
is relatively small and does not include any infor-
1A portion of the English data in the OntoNotes corpus
is a selected set of sentences that were annotated for parse
and word sense information. These sentences are present in a
document of their own, and so the documents for parse layers
for English are inflated by about 3655 documents and for the
word sense are inflated by about 8797 documents.
143
Language Parse Proposition Sense Name Coreference
Documents Words Documents Verb Prop. Noun Prop. Documents Verb Sense Noun Sense Documents Words Documents Words
English 7,9671 2.6M 6,124 300K 18K 12K 173K 120K 3,637 2.0M 2,384(3493) 1.7M
Chinese 2002 1.0M 1861 148K 7K 1573 83K 1K 1,911 988K 1,729(2,280) 950K
Arabic 599 402K 599 30K - 310 4.3K 8.7K 446 298K 447(447) 300K
Table 1: Coverage for each layer in the OntoNotes v5.0 corpus, by number of documents, words, and
some other attributes. The numbers in parenthesis are the total number of parts in the documents.
mal genres such as web data. Very seldom has it
been the case that the exact same phenomena have
been annotated on a broad cross-section of the
same language before OntoNotes. The OntoNotes
corpus thus provides an opportunity for studying
the genre effect on different syntactic, semantic
and discourse analyzers.
Parts of the OntoNotes Corpus have been used
for various shared tasks organized by the language
processing community. The word sense layer was
the subject of prediction in two SemEval-2007
tasks, and the coreference layer was the subject
of prediction in the SemEval-20102 (Recasens et
al., 2010), CoNLL-2011 and 2012 shared tasks
(Pradhan et al, 2011; Pradhan et al, 2012). The
CoNLL-2012 shared task provided predicted in-
formation to the participants, however, that did not
include a few layers such as the named entities
for Chinese and Arabic, propositions for Arabic,
and for better comparison of the English data with
the CoNLL-2011 task, a smaller OntoNotes v4.0
portion of the English parse and propositions was
used for training.
This paper is a first attempt at presenting a co-
herent high-level picture of the performance of
various publicly available state-of-the-art tools on
all the layers of OntoNotes in all three languages,
so as to pave the way for further explorations in
the area of syntax and semantics processing.
The possible avenues for exploratory studies
on various fronts are enormous. However, given
space considerations, in this paper, we will re-
strict our presentation of the performance on all
layers of annotation in the data by using a strat-
ified cross-section of the corpus for training, de-
velopment, and testing. The paper is organized
as follows: Section 2 gives an overview of the
OntoNotes corpus. Section 3 explains the param-
eters of the evaluation and the various underlying
assumptions. Section 4 presents the experimental
results and discussion, and Section 5 concludes the
paper.
2 OntoNotes Corpus
The OntoNotes project has created a large-scale
corpus of accurate and integrated annotation of
2A small portion 125K words in English was used for this
evaluation.
multiple layers of syntactic, semantic and dis-
course information in text. The English lan-
guage portion comprises roughly 1.7M words and
Chinese language portion comprises roughly 1M
words of newswire, magazine articles, broadcast
news, broadcast conversations, web data and con-
versational speech data3. The Arabic portion is
smaller, comprising 300K words of newswire ar-
ticles. This rich, integrated annotation covering
many layers aims at facilitating the development
of richer, cross-layer models and enabling bet-
ter automatic semantic analysis. The corpus is
tagged with syntactic trees, propositions for most
verb and some noun instances, partial verb and
noun word senses, coreference, and named enti-
ties. Table 1 gives an overview of the number of
documents that have been annotated in the entire
OntoNotes corpus.
2.1 Layers of Annotation
This section provides a very concise overview of
the various layers of annotations in OntoNotes.
For a more detailed description, the reader is re-
ferred to (Weischedel et al, 2011) and the docu-
mentation accompanying the v5.04 release.
2.1.1 Syntax
This represents the layer of syntactic annotation
based on revised guidelines for the Penn Tree-
bank (Marcus et al, 1993; Babko-Malaya et al,
2006), the Chinese Treebank (Xue et al, 2005)
and the Arabic Treebank (Maamouri and Bies,
2004). There were two updates made to the parse
trees as part of the OntoNotes project: i) the in-
troduction of NML phrases, in the English portion,
to mark nominal sub-constituents of flat NPs that
do not follow the default right-branching structure,
and ii) re-tokenization of hyphenated tokens into
multiple tokens in English and Chinese. The Ara-
bic Treebank on the other hand was also signifi-
cantly revised in an effort to increase consistency.
2.1.2 Word Sense
Coarse-grained word senses are tagged for the
most frequent polysemous verbs and nouns, in or-
3These numbers are for the portion that has all layers of
annotations. The word count for each layer is mentioned in
Table 1
4For all the layers of data used in this study, the
OntoNotes v4.99 pre-release that was used for the CoNLL-
2012 shared task is identical to the v5.0 release.
144
der to maximize token coverage. The word sense
granularity is tailored to achieve very high inter-
annotator agreement as demonstrated by Palmer et
al. (2007). These senses are defined in the sense
inventory files. In the case of English and Arabic
languages, the sense-inventories (and frame files)
are defined separately for each part of speech that
is realized by the lemma in the text. For Chinese,
however the sense inventories (and frame files) are
defined per lemma ? independent of the part of
speech realized in the text.
2.1.3 Proposition
The propositions in OntoNotes are PropBank-style
semantic roles for English, Chinese and Arabic.
Most English verbs and few nouns were anno-
tated using the revised guidelines for the English
PropBank (Babko-Malaya et al, 2006) as part of
the OntoNotes effort. Some enhancements were
made to the English PropBank and Treebank to
make them synchronize better with each other:
one of the outcomes of this effort was that two
types of LINKs that represent pragmatic coref-
erence (LINK-PCR) and selectional preferences
(LINK-SLC) were added to the original PropBank
(Palmer et al, 2005). More details can be found in
the addendum to the PropBank guidelines5 in the
OntoNotes v5.0 release. A part of speech agnostic
Chinese PropBank (Xue and Palmer, 2009) guide-
lines were used to annotate most frequent lem-
mas in Chinese. Many verbs and some nouns and
adjectives were annotated using the revised Ara-
bic PropBank guidelines (Palmer et al, 2008; Za-
ghouani et al, 2010).
2.1.4 Named Entities
The corpus was tagged with a set of 18 well-
defined proper named entity types that have been
tested extensively for inter-annotator agreement
by Weischedel and Burnstein (2005).
2.1.5 Coreference
This layer captures general anaphoric corefer-
ence that covers entities and events not limited
to noun phrases or a limited set of entity types
(Pradhan et al, 2007). It considers all pronouns
(PRP, PRP$), noun phrases (NP) and heads of verb
phrases (VP) as potential mentions. Unlike En-
glish, Chinese and Arabic have dropped subjects
and objects which were also considered during
coreference annotation6. The mentions formed by
these dropped pronouns total roughly about 11%
for both Chinese and Arabic. Coreference is the
only document-level phenomenon in OntoNotes.
Some of the documents in the corpus ? especially
the ones in the broadcast conversation, web data,
5doc/propbank/english-propbank.pdf
6As we will see later these are not used during the task.
and telephone conversation genre ? are very long
which prohibited efficient annotation in their en-
tirety. These are split into smaller parts, and each
part is considered a separate document for the sake
of coreference evaluation.
3 Evaluation Setting
Given the scope of the corpus and the multitude of
settings one can run evaluations, we had to restrict
this study to a relatively focused subset. There has
already been evidence of models trained on WSJ
doing poorly on non-WSJ data on parses (Gildea,
2001; McClosky et al, 2006), semantic role label-
ing (Carreras and Ma`rquez, 2005; Pradhan et al,
2008), word sense (Escudero et al, 2000; ?), and
named entities. The phenomenon of coreference is
somewhat of an outlier. The winning system in the
CoNLL-2011 shared task was one that was com-
pletely rule-based and not directly trained on the
OntoNotes corpus. Given this overwhelming evi-
dence, we decided not to focus on potentially com-
plex cross-genre evaluations. Instead, we decided
on evaluating the performance on each layer of an-
notation using an appropriately selected, stratified
training, development and test set, so as to facili-
tate future studies.
3.1 Training, Development and Test
Partitions
In this section we will have a brief discussion
on the logic behind the partitioning of the data
into training, development and test sets. Before
we do that, it would help to know that given the
range and peculiarities of the layers of annota-
tion and presence of various resource and techni-
cal constraints, not all the documents in the cor-
pus are annotated with all the layers of informa-
tion, and token-centric phenomena (such as word
sense and propositions of predicates) were not an-
notated with 100% coverage. Most of the propo-
sition annotation in English and Arabic is for the
verb predicates, with a few nouns annotated in
English and some adjectives in Arabic. In Chi-
nese, the selection is part of speech agnostic, and is
based on the lemmas that can be considered predi-
cates. Some documents in the corpora are actually
snippets from larger documents, and have been an-
notated for a combination of parse, propositions,
word sense and names, but not coreference. If one
considers each layer independently, then an ideal
partitioning scheme would create a separate parti-
tion for each layer such that it maximizes the num-
ber of examples that can be extracted for that layer
from the corpus. The upside is that one would
get as much data there is to train and estimate the
performance of each layer across the entire cor-
pus. The downside is that this might cover vari-
145
ous cross sections of the documents in the corpus,
and would not provide a clean picture when look-
ing at the collective performance for all the lay-
ers. The documents that are annotated with coref-
erence correspond to the intersection of all anno-
tations. These are the documents that have also
been annotated with all the other layers of infor-
mation. The amount of data we can get together
in such a test set is big enough to be represen-
tative. Therefore, we decided that it would be
ideal to choose a portion of these documents as
the test collection for all layers. An additional ad-
vantage is that it is the exact same test set used
in the CoNLL-2012 shared task, and so in a way
is already a standard. On the training and devel-
opment side however, one can still imagine using
all possible information for training models for a
particular layer, and that is what we decided to
do. The training and development data is gener-
ated by providing all documents with all available
layers of annotation for input, however, the test
set is generated by providing as input to the algo-
rithm the set of documents in the corpus that have
been annotated for coreference. This algorithm
tries to reuse previously established partitions for
English, i.e., the WSJ portion. Unfortunately, in
the case of Chinese and Arabic, either the histor-
ical partitions were not in the selection used for
OntoNotes, or were partially overlapping with the
ones created using this scheme, and/or had a very
small portion of OntoNotes covered in the test set.
Therefore, we decided to create a fresh partition
for the Chinese and Arabic data. Note, however,
that the these test sets also match the ones used
in the CoNLL-2012 evaluation. The algorithm for
selecting the training, development and test parti-
tions is described on the CoNLL-2012 shared task
webpage, along with the list of training, develop-
ment, and test document IDs7.
3.2 Assumptions
Next we had to decide on a set of assumptions
to use while designing the experiments to mea-
sure the automatic prediction accuracy for each of
the layers. Since some of these decisions affect
more than one layer of annotation, we will de-
scribe these in this section instead of in the section
where we discuss the experiment with a particular
layer of annotation.
7http://conll.cemantix.org/2012/download/ids/
For each language there are two sub-directories ? ?all?
contains more general lists which include documents
that had at least one of the layers of annotation, and
?coref? contains the lists that include documents that
have coreference annotation. The former were used to
generate training, development, test sets for layers other
than coreference, and the latter was used to generate
training/development/test sets for the coreference layer
used in the CoNLL-2012 shared task.
Word Segmentation The three languages that
we are evaluating are from quite different lan-
guage families. Arabic has a complex morphol-
ogy, English has limited morphology, whereas
Chinese has very little morphology. English word
segmentation amounts to rule-based tokenization,
and is close to perfect. In the case of Chinese and
Arabic, although the tokenization/segmentation is
not as good as English, the accuracies are in the
high 90s. Given this we decided to use gold,
Treebank segmentation for all languages. In the
case of Chinese, the words themselves are lem-
mas, whereas in English they can be predicted
with very high accuracy. For Arabic, by default
written text is unvocalised, and lemmatization is a
complex process which we considered out of the
scope of this study, so we decided to use correct,
gold standard lemmas, along with the correct vo-
calized version of the tokens.
Traces and Function Tags Treebank traces
have hardly played a role in the mainstream parser
and semantic role labeling evaluation. Function
tags also have received similar treatment in the
parsing community, and though they are impor-
tant, there is also a significant information overlap
between them and the proposition structure pro-
vided by the PropBank layer. Whereas in English,
most traces represent syntactic phenomena such
as movement and raising, in Chinese and Arabic,
they can also represent dropped subjects/objects.
These subset of traces directly affect the corefer-
ence layer, since, unlike English, traces in Chinese
and Arabic (*pro* and * respectively) are legit-
imate targets of mentions and are considered for
coreference annotation in OntoNotes. Recovering
traces in text is a hard problem, and the most re-
cently reported numbers in literature for Chinese
are around a F-score of 50 (Yang and Xue, 2010;
Cai et al, 2011). For Arabic there have not been
much studies on recovering these. A study by
Gabbard (2010) shows that these can be recovered
with an F-score of 55 with automatic parses and
roughly 65 using gold parses. Considering the low
level of prediction accuracy of these tokens, and
their relative low frequency, we decided to con-
sider predicting traces in trees out of the scope of
this study. In other words, we removed the man-
ually identified traces and function tags from the
Treebanks across all three languages, in all the
three ? training, development and test partitions.
This meant removing any and all dependent an-
notation in layers such as PropBank and Coref-
erence. In the case of PropBank these are the
argument bearing traces, whereas in coreference
these are the mentions formed by these elided sub-
jects/objects.
146
Disfluencies One thing that needs to be dealt
with in conversational data is the presence of dis-
fluencies (restarts, etc.). In the English parses of
the OntoNotes, disfluencies are marked using a
special EDITED8 phrase tag ? as was the case for
the Switchboard Treebank. Computing the accu-
racy of identifying disfluencies is also out of the
scope of this study. Given the frequency of dis-
fluencies and the performance with which one can
identify them automatically,9 a probable process-
ing pipeline would filter them out before parsing.
We decided to remove them using oracle infor-
mation available in the English Treebank, and the
coreference chains were remapped to trees with-
out disfluencies. Owing to various technical con-
straints, we decided to retain the disfluencies in the
Chinese data.
Spoken Genre Given the scope of this study, we
make another significant assumption. For the spo-
ken genres ? BC, BN and TC ? we use the manual
transcriptions rather than the output of a speech
recognizer, as would be the case in real world. The
performance on various layers for these genres
would therefore be artificially inflated, and should
be taken into account while analyzing results. Not
many studies have previously reported on syntac-
tic and semantic analysis for spoken genre. Favre
et al (2010) report the performance on the English
subset of an earlier version of OntoNotes.
Discourse The corpus contains information on
the speaker for broadcast communication, conver-
sation, telephone conversation and writer for the
web data. This information provides an important
clue for correctly linking anaphoric pronouns with
the right antecedents. This information could be
automatically deduced, but is also not within the
scope of our study. Therefore, we decided to pro-
vide gold, instead of predicted, data both during
training and testing. Table 2 lists the status of the
layers.
4 Experiments
In this section, we will report on the experiments
carried out using all available data in the train-
ing set for training models for a particular layer,
and using the CoNLL-2012 test set as the test set.
8There is another phrase type ? EMBED in the telephone
conversation genre which is similar to the EDITED phrase
type, and sometimes identifies insertions, but sometimes con-
tains logical continuation of phrases by different speakers, so
we decided not to remove that from the data.
9A study by Charniak and Johnson (2001) shows that one
can identify and remove edits from transcribed conversational
speech with an F-score of about 78, with roughly 95 precision
and 67 recall.
10The predicted part of speech for Arabic are a mapped
down version of the richer gold version present in the Tree-
bank
Layer English Chinese Arabic
Segmentation ? ? ?
Lemma ? ? ?
Parse ? ? ?10
Proposition ? ? ?
Predicate Frame ? ? ?
Word Sense ? ? ?
Name Entities ? ? ?
Coreference ? ? ?
Speaker ? ? ?
Number ? ? ?
Gender ? ? ?
Table 2: Status of layers used during prediction
of other layers. A ??? indicates gold annotation,
a ??? indicates predicted, a ??? indicates an ab-
sence of the predicted layer, and a ??? indicates
that the layer is not applicable to the language.
The predicted annotation layers input to down-
stream models were automatically annotated by
using NLP processors learned with n-cross fold
validation on the training data. This way, the n
chunks of training data are annotated avoiding de-
pendencies with the data used for training the NLP
processors.
4.1 Syntax
Predicted parse trees for English were produced
using the Charniak parser11 (Charniak and John-
son, 2005). Some additional tag types used in
the OntoNotes trees were added to the parser?s
tagset, including the nominal (NML) tag, and the
rules used to determine head words were extended
correspondingly. Chinese and Arabic parses were
generated using the Berkeley parser (Petrov and
Klein, 2007). In the case of Arabic, the pars-
ing community uses a mapping from rich Arabic
part of speech tags to Penn-style part of speech
tags. We used the mapping that is included with
the Arabic Treebank. The predicted parses for
the training portion of the data were generated us-
ing 10-fold (5-folds for Arabic) cross-validation.
For testing, we used a model trained on the entire
training portion. Table 3 shows the precision, re-
call and F1-scores of the re-trained parsers on the
CoNLL-2012 test along with the part of speech ac-
curacies (POS) using the standard evalb scorer.
The performance on the PT genre for English is
the highest among other English genres. This is
possibly because of the professional, clean trans-
lations of the underlying text, and are mostly
shorter sentences. The MZ genre and the NW both
of which contain well edited text, share similar
scores. There is a few points gap between these
and the other genres. As for Chinese, the per-
formance on MZ is the highest followed by BN.
Surprisingly, the WB genre has a similar score and
the others are close behind except for TC. As ex-
pected, the Arabic parser performance is the low-
11http://bllip.cs.brown.edu/download/reranking-parserAug06.tar.gz
147
All Sentences
N POS P R F
English BC 2,211 97.33 86.36 86.11 86.23
BN 1,357 97.32 87.61 87.03 87.32
MZ 780 96.58 89.90 89.49 89.70
NW 2,327 97.15 87.68 87.25 87.47
TC 1,366 96.11 85.09 84.13 84.60
WB 1,787 96.03 85.46 85.26 85.36
PT 1,869 98.77 95.29 94.66 94.98
Overall 11,697 97.09 88.08 87.65 87.87
Chinese BC 885 94.79 80.17 79.35 79.76
BN 929 93.85 83.49 80.13 81.78
MZ 451 97.06 88.48 83.85 86.10
NW 481 94.07 82.26 77.28 79.69
TC 968 92.22 71.90 69.19 70.52
WB 758 92.37 82.57 78.92 80.70
Overall 4,472 94.12 82.23 78.93 80.55
Arabic NW 1,003 94.12 74.71 75.67 75.19
Table 3: Parser performance on the CoNLL-2012
test set.
est among the three languages.
4.2 Word Sense
We used the IMS12 (It Makes Sense) (Zhong and
Ng, 2010) word sense tagger. IMS was trained on
all the word sense data that is present in the train-
ing portion of the OntoNotes corpus using cross-
validated predictions on the input layers similar
to the proposition tagger. During testing, for En-
glish and Arabic, IMS must first use the auto-
matic POS information to identify the nouns and
verbs in the test data, and then assign senses to
the automatically identified nouns and verbs. In
the case of Arabic, IMS uses gold lemmas. Since
automatic POS tagging is not perfect, IMS does
not always output a sense to all word tokens that
need to be sense tagged due to wrongly predicted
POS tags. As such, recall is not the same as pre-
cision on the English and Arabic test data. For
Chinese the measure of performance is just the
accuracy since the senses are defined per lemma
rather than per part of speech. Since we provide
gold word segmentation, IMS attempts to sense
tag all correctly segmented Chinese words, so re-
call and precision are the same and so is the F1-
score. Table 4 shows the performance of this clas-
sifier aggregated over both the verbs and nouns
in the CoNLL-2012 test set and an overall score
split by nouns and verbs for English and Ara-
bic. For both nouns and verbs in English, the
F1-score is over 80%. The performance on En-
glish nouns is slightly higher than English verbs.
Comparing to the other two languages, the perfor-
mance on Arabic is relatively lower, especially the
performance on Arabic verbs, whose F1-score is
less than 70%. For English, genres PT and TC,
and for Chinese genres TC and WB, no gold stan-
dard senses were available, and so their accuracies
could not be computed. Previously, Zhong et al
(2008) reported the word sense performance on
the Wall Street Journal portion of an earlier ver-
12http://www.comp.nus.edu.sg/?nlp/sw/IMS v0.9.2.1.tar.gz
Performance
P R F A
English BC 81.2 81.3 81.2 -
BN 82.0 81.5 81.7 -
MZ 79.1 78.8 79.0 -
NW 85.7 85.7 85.7 -
WB 77.5 77.6 77.5 -
Overall 82.5 82.5 82.5 -
Nouns 83.4 83.1 83.2 -
Verbs 81.8 81.9 81.8 -
Chinese BC - - - 80.5
BN - - - 85.4
MZ - - - 82.4
NW - - - 89.1
Overall - - - 84.3
Arabic NW 75.9 75.2 75.6 -
Nouns 79.2 77.7 78.4 -
Verbs 68.8 69.5 69.1 -
Table 4: Word sense performance on the CoNLL-
2012 test set.
sion of OntoNotes, but the results are not directly
comparable.
4.3 Proposition
The revised PropBank has introduced two new
links ? LINK-SLC and LINK-PCR. Since the com-
munity is not used to the new PropBank represen-
tation which (i) relies heavily on the trace struc-
ture in the Treebank and (ii) we decided to ex-
clude, we unfold the LINKs back to their original
representation as in the PropBank 1.0 release. We
used ASSERT15 (Pradhan et al, 2005) to predict
the propositional structure for English. We made
a small modification to ASSERT, and replaced
the TinySVM classifier with a CRF16 to speed
up training the model on all the data. The Chi-
nese propositional structure was predicted with the
Chinese semantic role labeler described in (Xue,
2008), retrained on the OntoNotes v5.0 data. The
Arabic propositional structure was predicted us-
ing the system described in Diab et al (2008).
(Diab et al, 2008) Table 5 shows the detailed per-
14The Frame ID column indicates the F-score for English
and Arabic, and accuracy for Chinese for the same reasons as
word sense.
15http://cemantix.org/assert.html
16http://leon.bottou.org/projects/sgd
Frame Total Total % Perfect Argument ID + Class
ID Sent. Prop. Prop. P R F
English BC 93.2 1994 5806 52.89 80.76 69.69 74.82
BN 92.7 1218 4166 54.78 80.22 69.36 74.40
MZ 90.8 740 2655 50.77 79.13 67.78 73.02
NW 92.8 2122 6930 46.45 79.80 66.80 72.72
TC 91.8 837 1718 49.94 79.85 72.35 75.91
WB 90.7 1139 2751 42.86 80.51 69.06 74.35
PT 96.6 1208 2849 67.53 89.35 84.43 86.82
Overall 92.8 9,261 26,882 51.66 81.30 70.53 75.53
Chinese BC 87.7 885 2,323 31.34 53.92 68.60 60.38
BN 93.3 929 4,419 35.44 64.34 66.05 65.18
MZ 92.3 451 2,620 31.68 65.04 65.40 65.22
NW 96.6 481 2,210 27.33 69.28 55.74 61.78
TC 82.2 968 1,622 32.74 48.70 59.12 53.41
WB 87.8 758 1,761 35.21 62.35 68.87 65.45
Overall 90.9 4,472 14,955 32.62 61.26 64.48 62.83
Arabic NW 85.6 1,003 2337 24.18 52.99 45.03 48.68
Table 5: Proposition and frameset disambiguation
performance14 in the CoNLL-2012 test set.
148
formance numbers17. The CoNLL-2005 scorer18
was used to compute the scores. At first glance,
the performance on the English newswire genre is
much lower than what has been reported for WSJ
Section 23. This could be attributed to several fac-
tors: i) the newswire in OntoNotes not only con-
tains WSJ data, but also Xinhua news, and some
other newswire evaluation data, ii) The WSJ train-
ing and test portions in OntoNotes are a subset of
the standard ones that have been used to report
performance earlier; iii) the PropBank guidelines
were significantly revised during the OntoNotes
project in order to synchronize well with the Tree-
bank, and finally iv) it includes propositions for
be verbs missing from the original PropBank. It
looks like the newly added Pivot Text data (com-
prised of the New Testament) shows very good
performance. The Chinese and Arabic19 accuracy
is much worse. In addition to automatically pre-
dicting the arguments, we also trained the IMS
system to tag PropBank frameset IDs.
Language Genre Entity Performance
Count P R F
English BC 1671 80.17 77.20 78.66
BN 2180 88.95 85.69 87.29
MZ 1161 82.74 82.17 82.45
NW 4679 86.79 84.25 85.50
TC 362 74.09 61.60 67.27
WB 1133 77.72 68.05 72.56
Overall 11186 84.04 80.86 82.42
Chinese BC 667 72.49 58.47 64.73
BN 3158 82.17 71.50 76.46
NW 1453 86.11 76.39 80.96
MZ 1043 65.16 56.66 60.62
TC 200 48.00 60.00 53.33
WB 886 80.60 51.13 62.57
Overall 7407 78.20 66.45 71.85
Arabic NW 2550 74.53 62.55 68.02
Table 6: Performance of the named entity recog-
nizer on the CoNLL-2012 test set.
4.4 Named Entities
We retrained the Stanford named entity recog-
nizer20 (Finkel et al, 2005) on the OntoNotes data.
Table 6 shows the performance details for all the
languages across all 18 name types broken down
by genre. In English, BN has the highest perfor-
mance followed by the NW genre. There is a sig-
nificant drop from those and the TC and WB genre.
Somewhat similar trend is observed in the Chi-
nese data, with Arabic having the lowest scores.
Since the Pivot Text portion (PT) of OntoNotes
was not tagged with names, we could not com-
pute the accuracy for that cross-section of the data.
Previously Finkel and Manning (2009) performed
17The number of sentences in this table are a subset of the
ones in the table showing parser performance, since these are
the sentences for which at least one predicate has been tagged
with its arguments
18http://www.lsi.upc.es/?srlconll/srl-eval.pl
19The system could not not use the morphology features in
Diab et al (2008).
20http://nlp.stanford.edu/software/CRF-NER.shtml
a joint estimation of named entity and parsing.
However, it was on an earlier version of the En-
glish portion of OntoNotes using a different cross-
section for training and testing and therefore is not
directly comparable.
4.5 Coreference
The task is to automatically identify mentions of
entities and events in text and to link the corefer-
ring mentions together to form entity/event chains.
The coreference decisions are made using auto-
matically predicted information on other structural
and semantic layers including the parses, seman-
tic roles, word senses, and named entities that
were produced in the earlier sections. Each docu-
ment part from the documents that were split into
multiple parts during coreference annotation were
treated as separate document.
We used the number and gender predictions
generated by Bergsma and Lin (2006). Unfortu-
nately neither Arabic, nor Chinese have compara-
ble data available. Chinese, in particular, does not
have number or gender inflections for nouns, but
(Baran and Xue, 2011) look at a way to infer such
information.
We trained the Bjo?rkelund and Farkas (2012)
coreference system21 which uses a combination of
two pair-wise resolvers, the first is an incremen-
tal chain-based resolution algorithm (Bjo?rkelund
and Farkas, 2012), and the second is a best-first
resolver (Ng and Cardie, 2002). The two resolvers
are combined by stacking, i.e., the output of the
first resolver is used as features in the second one.
The system uses a large feature set tailored for
each language which, in addition to classic coref-
erence features, includes both lexical and syntactic
information.
Recently, it was discovered that there is pos-
sibly a bug in the official scorer used for the
CoNLL 2011/2012 and the SemEval 2010 corefer-
ence tasks. This relates to the mis-implementation
of the method proposed by (Cai and Strube, 2010)
for scoring predicted mentions. This issue has also
been recently reported in Recasens et al, (2013).
As of this writing, the BCUBED metric has been
fixed, and the correctness of the CEAFm, CEAFe
and BLANC metrics is being verified. We will
be updating the CoNLL shared task webpages22
with more detailed information and also release
the patched scripts as soon as they are available.
We will also re-generate the scores for previous
shared tasks, and the coreference layer in this pa-
per and make them available along with the mod-
els and system outputs for other layers. Table
7 shows the performance of the system on the
21http://www.ims.uni-stuttgart.de/?anders/coref.html
22http://conll.cemantix.org
149
CoNLL-2012 test set, broken down by genre. The
same metrics that were used for the CoNLL-2012
shared task are computed, with the CONLL col-
umn being the official CONLL measure.
Language Genre MD MUC BCUBED CEAFm CEAFe BLANC CONLL
PREDICTED MENTIONS
English BC 73.43 63.92 61.98 54.82 42.68 73.04 56.19
BN 73.49 63.92 65.85 58.93 48.14 72.74 59.30
MZ 71.86 64.94 71.38 64.03 50.68 78.87 62.33
NW 68.54 60.20 65.11 57.54 45.10 73.72 56.80
PT 86.95 79.09 68.33 65.52 50.83 77.74 66.08
TC 80.81 76.78 71.35 65.41 45.44 82.45 64.52
WB 74.43 66.86 61.43 54.76 42.05 73.54 56.78
Overall 75.38 67.58 65.78 59.20 45.87 75.8 59.74
Chinese BC 68.02 59.6 59.44 53.12 40.77 73.63 53.27
BN 68.57 61.34 67.83 60.90 48.10 77.39 59.09
MZ 55.55 48.89 58.83 55.63 46.04 74.25 51.25
NW 89.19 80.71 73.64 76.30 70.89 82.56 75.08
TC 77.72 73.59 71.65 64.30 48.52 83.14 64.59
WB 72.61 65.79 62.32 56.71 43.67 77.45 57.26
Overall 66.37 58.61 66.56 59.01 48.19 76.07 57.79
Arabic NW 60.55 47.82 61.16 53.42 44.30 69.63 51.09
GOLD MENTIONS
English BC 85.63 76.09 68.70 61.73 49.87 76.24 64.89
BN 82.11 73.56 71.52 63.67 52.29 75.70 65.79
MZ 85.65 77.73 78.82 72.75 60.09 83.88 72.21
NW 80.68 73.52 73.08 65.63 51.96 81.06 66.19
PT 93.20 85.72 73.25 70.76 58.81 79.78 72.59
TC 90.68 86.83 78.94 73.87 56.26 85.82 74.01
WB 88.12 80.61 69.86 63.45 51.13 76.48 67.20
Overall 86.16 78.7 72.67 66.32 53.23 79.22 68.2
Chinese BC 84.88 76.34 69.89 62.02 49.29 76.89 65.17
BN 80.97 74.89 76.88 68.91 55.56 81.94 69.11
MZ 78.85 73.06 70.15 61.68 46.86 78.78 63.36
NW 93.23 86.54 86.70 80.60 76.60 85.75 83.28
TC 92.91 88.31 84.51 79.49 63.87 90.04 78.90
WB 85.87 77.61 69.24 60.71 47.47 77.67 64.77
Overall 83.47 76.85 76.30 68.30 56.61 81.56 69.92
Arabic NW 76.43 60.81 67.29 59.50 49.32 74.61 59.14
Table 7: Performance of the coreference system
on the CoNLL-2012 test set.
The varying results across genres mostly meet
our expectations. In English, the system does best
on TC and the PT genres. The text in the TC set
often involve long chains where the speakers re-
fer to themselves which, given speaker informa-
tion, is fairly easy to resolve. The PT section
includes many references to god (e.g. god and
the lord) which the lexicalized resolver is quite
good at picking up during training. The more dif-
ficult genres consist of texts where references to
many entities are interleaved in the discourse and
is as such harder to resolve correctly. For Chi-
nese the numbers on the TC genre are also quite
good, and the explanation above also holds here
? many mentions refer to either of the speak-
ers. For Chinese the NW section displays by far
the highest scores, however, and the reason for
this is not clear to us. Not surprisingly, restricting
the set of mentions only to gold mentions gives
a large boost across all genres and all languages.
This shows that mention detection (MD) and sin-
gleton detection (which is not part of the annota-
tion) remain a big source of errors for the coref-
erence resolver. For these experiments we used
a combination of training and development data
for training ? following the CoNLL-2012 shared
task specification. Leaving out the development
set has a very negligible effect on the CoNLL-
score for all the languages (English: 0.14; Chi-
nese 0.06; Arabic: 0.40 F-score respectively). The
effect on Arabic is the most (0.40 F-score) most
likely because of its much smaller size. To gauge
the performance improvement between 2011 and
2012 shared tasks, we performed a clean com-
parison of over the best performing system and
an earlier version of this system (Bjo?rkelund and
Nugues, 2011) on the CoNLL 2011 test set us-
ing the CoNLL 2011 train and development set
for training. The current system has a CoNLL
score of 60.09 (64.92+69.84+45.513 )23 as opposed tothe 54.53 reported in bjo?rkelund (Bjo?rkelund and
Nugues, 2011), and the 57.79 reported for the best
performing system of CoNLL-2011. One caveat
is that these score comparison are done using the
earlier version (v4) of the CoNLL scorer. Nev-
ertheless, it is encouraging to see that within a
short span of a year, there has been significant
improvement in system performance ? partially
owing to cross-pollination of research generated
through the shared tasks.
5 Conclusion
In this paper we reported work on finding a rea-
sonable training, development and test split for
the various layers of annotation in the OntoNotes
v5.0 corpus, which consists of multiple genres in
three typologically very different languages. We
also presented the performance of publicly avail-
able, state-of-the-art algorithms on all the different
layers of the corpus for the different languages.
The trained models as well as their output will
be made publicly available24 to serve as bench-
marks for language processing community. Train-
ing so many different NLP components is very
time-consuming, thus, we hope the work reported
here has lifted the burden of having to create rea-
sonable baselines for researchers who wish to use
this corpus to evaluate their systems. We created
just one data split in training, development and test
set, covering a collection of genres for each layer
of annotation in each language in order to keep the
workload manageable However, the results do not
discriminate the performance on individual gen-
res: we believe such a setup is still a more realistic
gauge for the performance of the state-of-the-art
NLP components than a monolithic corpus such
as the Wall Street Journal section of the Penn Tree-
bank. It can be used as a starting point for devel-
oping the next generation of NLP components that
are more robust and perform well on a multitude
of genres for a variety of different languages.
23(MUC + BCUBED + CEAFe)/3
24http://cemantix.org
150
6 Acknowledgments
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
(DARPA/IPTO) under the GALE program,
DARPA/CMO Contract No. HR0011-06-C-0022for sponsoring the creation of the OntoNotes
corpus. This work was partially supported
by grants R01LM10090 and U54LM008748
from the National Library Of Medicine, and
R01GM090187 from the National Institutes ofGeneral Medical Sciences. We are indebted toSlav Petrov for helping us to retrain his syntactic
parser for Arabic. Alessandro Moschitti and
Olga Uryupina have been partially funded by
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under the grant
number 288024 (LIMOSINE). The content
is solely the responsibility of the authors and
does not necessarily represent the official views
of the National Institutes of Health. NianwenXue and Yuchen Zhang are supported in part
by the DAPRA via contract HR0011-11-C-0145
entitled ?Linguistic Resources for Multilingual
Processing.?
References
Olga Babko-Malaya, Ann Bies, Ann Taylor, Szuting Yi,
Martha Palmer, Mitch Marcus, Seth Kulick, and Libin
Shen. 2006. Issues in synchronizing the English treebank
and propbank. In Workshop on Frontiers in Linguistically
Annotated Corpora 2006, July.
Elizabeth Baran and Nianwen Xue. 2011. Singular or plural?
exploiting parallel corpora for Chinese number prediction.
In Proceedings of Machine Translation Summit XIII, Xia-
men, China.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping path-
based pronoun resolution. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computa-
tional Linguistics, pages 33?40, Sydney, Australia, July.
Anders Bjo?rkelund and Richa?rd Farkas. 2012. Data-driven
multilingual coreference resolution using resolver stack-
ing. In Joint Conference on EMNLP and CoNLL - Shared
Task, pages 49?55, Jeju Island, Korea, July. Association
for Computational Linguistics.
Anders Bjo?rkelund and Pierre Nugues. 2011. Exploring lex-
icalized features for coreference resolution. In Proceed-
ings of the Fifteenth Conference on Computational Natu-
ral Language Learning: Shared Task, pages 45?50, Port-
land, Oregon, USA, June. Association for Computational
Linguistics.
Jie Cai and Michael Strube. 2010. Evaluation metrics for
end-to-end coreference resolution systems. In Proceed-
ings of the 11th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, SIGDIAL ?10, pages
28?36.
Shu Cai, David Chiang, and Yoav Goldberg. 2011.
Language-independent parsing with empty elements. In
Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language Tech-
nologies, pages 212?216, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to
the CoNLL-2005 shared task: Semantic role labeling. In
Proceedings of the Ninth Conference on Computational
Natural Language Learning (CoNLL), Ann Arbor, MI,
June.
Eugene Charniak and Mark Johnson. 2001. Edit detection
and parsing for transcribed speech. In Proceedings of the
Second Meeting of the North American Chapter of the As-
sociation for Computational Linguistics, June.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine
n-best parsing and maxent discriminative reranking. In
Proceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Ann Arbor, MI,
June.
Mona Diab, Alessandro Moschitti, and Daniele Pighin. 2008.
Semantic role labeling systems for Arabic using kernel
methods. In Proceedings of ACL-08: HLT, pages 798?
806, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Gerard Escudero, Lluis Marquez, and German Rigau. 2000.
An empirical study of the domain dependence of super-
vised word disambiguation systems. In 2000 Joint SIG-
DAT Conference on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora, pages 172?
180, Hong Kong, China, October. Association for Com-
putational Linguistics.
Benoit Favre, Bernd Bohnet, and D. Hakkani-Tur. 2010.
Evaluation of semantic role labeling and dependency
parsing of automatic speech recognition output. In
Proceedings of 2010 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), page
5342?5345.
Jenny Rose Finkel and Christopher D. Manning. 2009. Joint
parsing and named entity recognition. In Proceedings of
Human Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Association
for Computational Linguistics, pages 326?334, Boulder,
Colorado, June. Association for Computational Linguis-
tics.
Jenny Rose Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information into in-
formation extraction systems by Gibbs sampling. In Pro-
ceedings of the 43rd Annual Meeting of the Association
for Computational Linguistics, page 363?370.
Ryan Gabbard. 2010. Null Element Restoration. Ph.D. the-
sis, University of Pennsylvania.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In 2001 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), Pittsburgh, PA.
Julia Hockenmaier and Mark Steedman. 2002. Acquir-
ing compact lexicalized grammars from a cleaner tree-
bank. In Proceedings of the Third LREC Conference, page
1974?1981.
Mohamed Maamouri and Ann Bies. 2004. Developing an
Arabic treebank: Methods, guidelines, procedures, and
tools. In Ali Farghaly and Karine Megerdoomian, edi-
tors, COLING 2004 Computational Approaches to Arabic
Script-based Languages, pages 2?9, Geneva, Switzerland,
August 28th. COLING.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: The Penn treebank. Computational Linguis-
tics, 19(2):313?330, June.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceedings
of the Human Language Technology Conference/North
American Chapter of the Association for Computational
Linguistics (HLT/NAACL), New York City, NY, June.
151
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the Association for Computational Linguistics
(ACL-02), pages 104?111.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Martha Palmer, Hoa Trang Dang, and Christiane Fellbaum.
2007. Making fine-grained and coarse-grained sense dis-
tinctions, both manually and automatically. Journal of
Natural Language Engineering, 13(2).
Martha Palmer, Olga Babko-Malaya, Ann Bies, Mona Diab,
Mohammed Maamouri, Aous Mansouri, and Wajdi Za-
ghouani. 2008. A pilot Arabic propbank. In Proceedings
of the International Conference on Language Resources
and Evaluation (LREC), Marrakech, Morocco, May 28-
30.
Slav Petrov and Dan Klein. 2007. Improved inferencing for
unlexicalized parsing. In Proc of HLT-NAACL.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne
Ward, James Martin, and Dan Jurafsky. 2005. Support
vector learning for semantic argument classification. Ma-
chine Learning, 60(1):11?39.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel, Jes-
sica MacBride, and Linnea Micciulla. 2007. Unre-
stricted coreference: Indentifying entities and events in
OntoNotes. In Proceedings of the IEEE International
Conference on Semantic Computing (ICSC), September
17-19.
Sameer Pradhan, Wayne Ward, and James H. Martin. 2008.
Towards robust semantic role labeling. Computational
Linguistics Special Issue on Semantic Role Labeling,
34(2).
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, Martha
Palmer, Ralph Weischedel, and Nianwen Xue. 2011.
CoNLL-2011 shared task: Modeling unrestricted corefer-
ence in OntoNotes. In Proceedings of the Fifteenth Con-
ference on Computational Natural Language Learning:
Shared Task, pages 1?27, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga
Uryupina, and Yuchen Zhang. 2012. CoNLL-2012 shared
task: Modeling multilingual unrestricted coreference in
OntoNotes. In Joint Conference on EMNLP and CoNLL -
Shared Task, pages 1?40, Jeju Island, Korea, July. Associ-
ation for Computational Linguistics.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki,
Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008.
The Penn discourse treebank 2.0. In Proceedings of the
Sixth International Conference on Language Resources
and Evaluation (LREC?08), Marrakech, Morocco, May.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena, M. Anto`nia
Mart??, Mariona Taule?, Ve?ronique Hoste, Massimo Poesio,
and Yannick Versley. 2010. Semeval-2010 task 1: Coref-
erence resolution in multiple languages. In Proceedings of
the 5th International Workshop on Semantic Evaluation,
pages 1?8, Uppsala, Sweden, July.
Marta Recasens, Marie-Catherine de Marneffe, and Christo-
pher Potts. 2013. The life and death of discourse enti-
ties: Identifying singleton mentions. In Proceedings of
the 2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Human
Language Technologies, pages 627?633, Atlanta, Geor-
gia, June. Association for Computational Linguistics.
Libin Shen, Lucas Champollion, and Aravind K. Joshi. 2008.
LTAG-spinal and the treebank. Language Resources and
Evaluation, 42(1):1?19, March.
Ralph Weischedel and Ada Brunstein. 2005. BBN pro-
noun coreference and entity type corpus LDC catalog no.:
LDC2005T33. BBN Technologies.
Ralph Weischedel, Eduard Hovy, Mitchell Marcus, Martha
Palmer, Robert Belvin, Sameer Pradhan, Lance Ramshaw,
and Nianwen Xue. 2011. OntoNotes: A large train-
ing corpus for enhanced processing. In Joseph Olive,
Caitlin Christianson, and John McCary, editors, Hand-
book of Natural Language Processing and Machine
Translation: DARPA Global Autonomous Language Ex-
ploitation. Springer.
Nianwen Xue and Martha Palmer. 2009. Adding semantic
roles to the Chinese Treebank. Natural Language Engi-
neering, 15(1):143?172.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha Palmer.
2005. The Penn Chinese TreeBank: phrase structure an-
notation of a large corpus. Natural Language Engineer-
ing, 11(2):207?238.
Nianwen Xue. 2008. Labeling Chinese predicates with se-
mantic roles. Computational Linguistics, 34(2):225?255.
Yaqin Yang and Nianwen Xue. 2010. Chasing the ghost:
recovering empty categories in the Chinese treebank.
In Proceedings of the 23rd International Conference on
Computational Linguistics (COLING), Beijing, China.
Wajdi Zaghouani, Mona Diab, Aous Mansouri, Sameer Prad-
han, and Martha Palmer. 2010. The revised Arabic prop-
bank. In Proceedings of the Fourth Linguistic Annotation
Workshop, pages 222?226, Uppsala, Sweden, July.
Zhi Zhong and Hwee Tou Ng. 2010. It makes sense: A wide-
coverage word sense disambiguation system for free text.
In Proceedings of the ACL 2010 System Demonstrations,
pages 78?83, Uppsala, Sweden.
Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan. 2008.
Word sense disambiguation using OntoNotes: An empiri-
cal study. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 1002?
1010.
152
LAW VIII - The 8th Linguistic Annotation Workshop, pages 29?37,
Dublin, Ireland, August 23-24 2014.
Aligning Chinese-English Parallel Parse Trees: Is it Feasible?
Dun Deng and Nianwen Xue
Computer Science Department, Brandeis University
415 South Street, Waltham MA, USA
ddeng@brandeis.edu, xuen@brandeis.edu
Abstract
We investigate the feasibility of aligning Chinese and English parse trees by examining cases of
incompatibility between Chinese-English parallel parse trees. This work is done in the context of
an annotation project wherewe construct a parallel treebank by doingword and phrase alignments
simultaneously. We discuss the most common incompatibility patterns identified within VPs and
NPs and show that most cases of incompatibility are caused by divergent syntactic annotation
standards rather than inherent cross-linguistic differences in language itself. This suggests that in
principle it is feasible to align the parallel parse trees with somemodification of existing syntactic
annotation guidelines. We believe this has implications for the use of parallel parse trees as an
important resource for Machine Translation models.
1 Introduction
Parallel treebanks have been proved to be a valuable resource inMachine Translation research (Gildea,
2003; Liu et al., 2009; Sun et al., 2010; Chiang, 2010; Xiao and Zhu, 2013), but one issue that hampers
their utility is the incompatibility between the syntactic parse trees for a sentence pair (Chiang, 2010), as
the trees are annotated based on independently developed monolingual syntactic annotation standards.
For example, even though the Penn Chinese Treebank (Xue et al., 2005) and English TreeBank (Marcus
et al., 1993) are often referred to collectively as the Penn series of treebanks and are both annotated
with phrase structure trees in very similar annotation frameworks, different annotation decisions have
led to divergent tree structures (Chiang, 2010). The purpose of this study is to investigate to what extent
the divergences between Chinese-English parallel parse trees are caused by different annotation styles
(and therefore can be avoided by revising the annotation guidelines), and to what extent they are caused
by cross-linguistic differences inherent in language. The answer to this question would shed light on
whether it is possible to align the parse trees in parallel treebanks, and on the feasibility of building
Machine Translation systems based on these aligned parallel treebanks.
The question above cannot be answered without first having a concrete alignment specification and
knowing what types of alignments are attempted. No incompatibility issue would arise for sentence-level
alignment when sentences are aligned as a whole. By contrast, both word-level alignment (or the align-
ment of terminal nodes) and phrase-level alignment (or the alignment of non-terminal nodes) interact
with syntactic structures, which could potentially cause incompatibility between the alignments and the
tree structures. In the next section, we outline an alignment approach where we perform word alignments
and phrase alignments simultaneously in a parallel Chinese-English treebank to prevent incompatibili-
ties between word alignments and syntactic structures. The alignment approach alone, however, does
not prevent incompatibilities between the two parse trees of a sentence pair, which are either due to in-
herent cross-linguistic divergences or differences in treebank annotation styles. In Section 3, we report
three types of incompatibilities between the syntactic structures of a sentence pair that prevent proper
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details:http://creativecommons.org/licenses/by/4.0/
29
phrase-level alignments. We analyze two of them and show how they make certain phrase alignments
impossible. In Section 4, we discuss the third and also the most common type of incompatibility, which
is caused by different annotation decisions as specified in the Penn Chinese and English Treebank syn-
tactic bracketing guidelines (Xue and Xia, 2000; Bies et al., 1995). We propose modifications to the
tree structures for the purpose of aligning the parse trees, which means that proper phrase alignment is
possible if certain common patterns of incompatibility in syntactic parse trees are fixed. We conclude
our paper in Section 5 and touch on the workshop theme. We argue that the quality and level of linguistic
sophistication of an linguistic annotation project is tied to the purpose of the resource, and how it is going
to be used.
2 Overview of the HACEPT Project
The purpose of the HACEPT (Hierarchically Aligned Chinese-English Parallel TreeBank) Project is
to perform word-level and phrase-level alignments between parallel parse trees to develop a linguistic re-
source for Machine Translation models. We are currently in the process of aligning about 9,000 sentence
pairs where syntactic parses already exist for sentences on both the Chinese and English side.
In our project, the annotator is presented with a pair of parallel Chinese-English sentences which have
parse trees. The task of the annotator is to do both word and phrase alignments between the two parse
trees. The reason for doing word alignments and phrase alignments simultaneously is to make sure word
alignments and syntactic structures are harmonized to avoid both redundancies and incompatibilities.
Let us use the concrete example in Figure 1 to illustrate the point.
A big challenge to word alignment comes from language-particular function words that do not have
counterparts in the translation language. Take the sentences in Figure 1 for instance, the Chinese pre-
nominal modification marker? has no English counterpart. Similarly, the English infinitive marker to
has no Chinese counterpart. Word alignments done without taking syntactic structures into considera-
tion generally glue a function word such as? and to here to a neighboring content word which has a
counterpart and align the two words together to the counterpart of the content word (Li et al., 2009).
Under this practice, the first? will be glued to??/country, and the two words??/country? as a
whole will be aligned to countries. Similarly, to will be glued to weigh in and the whole string to weigh
in will be aligned to ??/weigh in. In our project, we take a different approach to word alignments:
we leave all the words without a counterpart unaligned on the word level and mark them as "extra". For
each unaligned word, we locate the appropriate phrase which contains the unaligned word and has a
phrasal counterpart on the other side. By aligning the two phrases, the unaligned word is captured in its
appropriate context. Under this new strategy, the Chinese? and the English to are both left unaligned
on the word level. For?, we align the NP??/all??/country???/people with the NP people
in all countries, because the Chinese NP is the relevant context where? appears (? is used in the NP
to indicate that??/all??/country is the modifier of the noun??/people) and matches in meaning
with the English NP. For to, we align the VP use their own methods of expression to weigh in on thiswith
the VP??/use??/own???/expression??/method??/weigh in?/this?/thing, because to
is used in the English VP to connect use their own methods of expression and weigh in on this and also
because the English VP and the Chinese one matches in meaning.
Under our approach, word alignments and syntactic structures are harmonized, and both redundancies
and incompatibilities between the two are avoided. For example, the phrase alignment between the
two NPs??/all??/country???/people and people in all countries specifies the context for the
occurrence of the function word ?. There is no need to glue ? to the previous noun ??/country
on the word level. As a matter of fact, the host of ? (namely the modifier signaled by it) is not the
noun ??/country but the NP ??/all ??/country. Similarly, the phrase alignment between use
their own methods of expression to weigh in on this and ??/use ??/own ? ??/expression ?
?/method??/weigh in?/this?/thing captures the syntactic environment in which to appears. The
phrase alignment also avoids an incompatibility issue caused by attaching to to weigh in and aligning the
30
.IP.
.VP.
.VP.
.VP.
.VP.
.NP.
.NN
.?
.
.DP
.DT
.?
.
.VV
.??
.
.VP.
.NP.
.NP.
.NN
.??
.
.NN
.??
.
.DNP.
.DEG
.?
.NP
.PN
.??
.VV
.??
.
.VV
.??
.
.ADVP
.AD
.?
.
.NP.
.NP
.NN
.??
.DNP.
.DEG
.?
.NP.
.NN
.??
.DT
.??
.S.
.VP.
.VP.
.S
.VP.
.VP.
.PP.
.NP
.DT
.this
.
.IN
.on
.
.RP
.in
.VB
.weigh
.
.TO
.to
.
.NP.
.PP.
.NP
.NN
.expression
.
.IN
.of
.
.NP.
.NNS
.methods
.
.JJ
.own
.PRP
.their
.VB
.use
.
.MD
.can
.
.NP.
.PP.
.NP.
.NNS
.countries
.
.PDT
.all
.
.IN
.in
.
.NP
.NNS
.People
Figure 1: A hierarchically aligned sentence pair
31
string to??/weigh in since to weigh in is not even a constituent in the English parse tree. For a more
comprehensive and detailed description of the HACEPT project, see (Deng and Xue, 2014).
A natural question arises for our approach: cross-linguistic divergences between languages may cause
parse tree incompatibilities to arise, which calls into question the possibility of doing phrase alignments
to a useful extent. The fact is that we did find incompatibilities between parse trees in our annotation. In
the next section, we report three types of parse tree incompatibilities we have encountered.
3 Three types of parse tree incompatibilities
During the annotation process, we encountered three types of parse tree incompatibilities that make
some phrase alignments impossible. The three types are distinguished by the sources of their occurrence
and are listed below:
Three types of incompatibilities between parallel parse trees:
a. Incompatibilities caused by lexical-semantic differences between the two languages
b. Incompatibilities caused by translation-related reasons
c. Incompatibilities caused by different annotation standards
Let us look at the first type. On the lexical level, languages differ in terms of whether or not a piece
of semantic information is encoded in a lexical item. For instance, Chinese does not have a verb that ex-
presses the meaning of the English verb prioritize, which needs to be translated using a phrase. This does
not necessarily cause problems for phrase alignments. Taking prioritize for instance, the English phrase
prioritize transportation projects is translated as ??/arrange ??/transportation ??/project ? ?
?/priority ??/order (literally arrange transportation projects' priority order, i.e., prioritize trans-
portation projects). Note that a phrase alignment can be made between the two VPs and also the two
NPs transportation projects and ??/transportation ??/project despite the fact that the meaning of
prioritize is expressed by a discontinuous phrase in Chinese (??/arrange ????/priority??/or-
der, i.e., arrange the priority order of ...). The most extreme case in this category which usually causes
incompatibilities and makes phrase-level alignment impossible is idiomatic expressions. An idiom is
a single lexical item just like a word and its meaning generally has to be expressed literally in another
language. For instance, the idiomatic part in Markets function best so long as no one has a finger on
the scale is translated as (??/so long as)??/everyone??/justly??/act (??/market??/func-
tion?/most?/good), which literally is everyone justly acts. The parse tree for both the English idiom
and its Chinese translation is given in Figure 2. No phrase alignment is possible between the idiom and
its translation except that between the two root nodes that dominate each string. Phrase alignments are
reduced to a minimal extent in cases like this.
Now let us discuss the second type. Consider this example, where the Chinese sentence?/he??/not
??/mention?/this?/one?/point (He did't mention this point) is translated as There was no mention
made of this by him. Given this particular translation, it is impossible to make a phrase alignment between
the Chinese VP??/not??/mention?/this?/one?/point and no mention made of this although
the two strings match in meaning. This is because, as shown in Figure 3, the NP node that dominates
the English string also dominates the PP by him. Note that him in the PP corresponds to?/he, which
is outside the Chinese VP. The issue here is caused by the translation. Note that the Chinese sentence is
in active voice, but the given translation is in passive voice, which is why the PP by him appears at the
end of the sentence and causes the problem. If the more literal translation He didn't mention this point
were provided,??/not??/mention?/this?/one?/point could be aligned with didn't mention this
point, and??/mention?/this?/one?/point could be aligned with mention this point, which is also
impossible with the given translation. Phrase alignments are reduced by some extent in cases like this.
For the first two types of incompatibilities already discussed, the negative impact of them on phrase
alignments can be reduced by the enlargement of the corpus, which currently has 8, 932 sentence pairs.
32
.CP.
.IP.
.VP.
.VP
.VV
.??/act
.
.ADVP
.AD
.??/justly
.
.NP
.PN
.??/everyone
.
.ADVP
.CS
.??/so long as
(a) Chinese literal translation of the idiom
.ADVP.
.SBAR.
.S.
.VP.
.NP.
.PP.
.NP.
.NN
.scale
.
.DT
.the
.
.IN
.on
.
.NP.
.NN
.finger
.
.DT
.a
.
.VB
.has
.
.NP.
.NN
.one
.DT
.no
.
.IN
.as
.
.ADVP.
.RB
.long
.
.RB
.so
(b) An English idiom
Figure 2: Structural divergence caused by idiomatic expressions
.IP.
.VP.
.VP.
.NP.
.NP
.NN
.?/point
.
.DP.
.QP
.CD
.?/one
.DT
.?/this
.
.VV
.??/mention
.
.ADVP
.AD
.??/not
.
.NP
.PN
.?/he
(a) Chinese sentence
.S.
.VP.
.NP.
.PP.
.NP
.PRP
.him
.
.IN
.by
.
.VP.
.PP.
.NP
.DT
.this
.
.IN
.of
.
.VBN
.made
.NP.
.NN
.mention
.DT
.no
.
.VBD
.was
.
.NP
.EX
.There
(b) Non-literal English translation
Figure 3: Structural divergence caused by non-literal translations
33
.VP.
.XP
.
.NP.V.ADVP
(a) PTB structure for VP
.VP.
.VP.
.NP/QP
.
.V
.
.XP
(b) CTB structure for VP
Figure 4: Bracketing decisions for VP made by PTB and CTB. XP = {PP, ADVP, S}
Idioms which make phrase-level alignment impossible are rare in our corpus. On average, there are about
5 cases in a file of 500 sentence pairs. As for the incompatibilities caused by translation, it is possible
for the phrase alignments missed in those cases to be made up if the phrases involved reappear in a more
literal translation. These two issues do not pose a real threat to our approach. As annotators, we cannot
do much about these two issues, especially the latter one, since our data is got as is. Due to these two
reasons, we will not discuss them further in this paper.
Next let us turn to the last type of incompatibility. Use the sentence pair in Figure 1 for instance. Note
that the Chinese VP ??/use ??/own ? ??/expression ??/method matches the English string
use their own methods of expression in terms of both grammaticality and meaning. However, the English
parse tree has no phrasal node for the string that could form an alignment to the Chinese VP. Similarly,
the Chinese NP??/expression??/method corresponds to the English string methods of expression,
but again, no phrasal node is present in the English parse tree that could be aligned with the Chinese NP.
Our statistics shows that, in a file with 500 sentence pairs, there are approximately 50 instances of the
incompatibility in VPs illustrated here and 20 in NPs (an instance is a case where a legitimate phrase
alignment cannot be made). These are both quite high frequency. In the next section, we discuss the
reason for the incompatibility and give a solution to fix the issue.
4 A common incompatibility pattern and its solution
There is a pattern for the incompatibility illustrated at the end of Section 3. The cause for the incom-
patibility is the bracketing annotation of the complement-adjunct distinction made by the Penn Treebank
(PTB) bracketing guidelines (Bies et al., 1995). The pattern is found in both VPs and NPs.
Let us discuss VPs first. To see the pattern, we need some background information about the internal
composition of both English and Chinese VPs and how VPs are parsed according to PTB and CTB
annotation standards. Let us start with the English VP. Besides the verb, there can be both preverbal and
postverbal constituents in an English VP. Preverbal constituents are much more restricted than postverbal
constituents in terms of both phrase types and the number of constituents allowed. Most commonly
seen in our corpus, an ADVP is present before the verb if there is a preverbal constituent at all. By
contrast, various kinds of constituents (NP, PP, ADVP, S) can appear post-verbally and more than one
of these phrases can co-occur. When there is more than one post-verbal constituent, quite often one of
them is the complement of the verb and the others are adjuncts. Due to engineering considerations, the
PTB bracketing guidelines decided on a flat structure for the English VP, where preverbal and postverbal
constituents and the verb are treated as sisters that are directly attached to the VP-node (Bies et al., 1995).
A general structure for the English VP is given in Figure 4a, where it can be seen that the complement-
adjunct distinction is not made.
Now let us turn to the Chinese VP. In a Chinese VP, there can also be both preverbal and postver-
bal constituents, but the situation is quite different from that in English. Unlike in English VPs where
postverbal constituents are freer, postverbal constituents in Chinese VPs are restricted and can only be
34
the complement of the verb or one particular kind of phrase, namely QP, which includes counting phrases
such as three times as in went there three times, and duration phrases such as for three years as in lived
there for three years. Adjuncts including ADVP, PP, and different kinds of adverbial clauses come be-
fore the verb. The second difference is that Chinese strongly favors no more than one constituent after
the verb. In theory, a complement phrase and a QP can co-occur after the verb, but in reality, if the two
co-occur in a sentence, the complement will most likely be preposed to the left of the verb by either
topicalization or the introduction of the function word?, leaving QP the only post-verbal element. The
structure of a Chinese VP stipulated by the CTB bracketing standards (Xue and Xia, 2000) is provided
in Figure 4b.
Now let us compare the two structures in Figure 4. Note that in the English VP there is no phrasal node
that dominates the verb and its immediate sister on the right, which, in many cases, is the complement of
the verb. By contrast, there is a node in the Chinese VP (the boxed VP) that groups together the verb and
a post-verbal constituent, which could be either the complement or a QP (some QPs are complements and
some others are adjuncts, an issue that does not need to bother us here). This is where the incompatibility
arises: the boxed VP-node in the Chinese tree has no node-counterpart to align with in the English tree,
but the string dominated by that boxed VP has a match in the English sentence. The example in Figure 1
illustrates the issue, where the Chinese VP dominating the string??/use??/own???/expression
??/method has no possible phrase alignment although the string corresponds in meaning to the English
string use their own methods of expression.
To eliminate the incompatibility, an extra layer of projection is needed in the English tree. To be
specific, we need to combine the verb and its complement to create a VP node, which then can be aligned
to the boxed VP in the Chinese tree. Still using the example in Figure 1 for instance, we need to create a
VP node by combining the English verb use and its object NP their own methods of expression, so that
the Chinese VP??/use??/own???/expression??/method can be aligned with the resultant
VP. This can be done through binarization.
Now let us turn to the pattern in NPs. We will look at the English NP first. There can be constituents
both before and after the head in an English NP. Post-nominal constituents can be either a PP or an S
whereas pre-nominal constituents can be one or more than one of the following kinds of elements: deter-
miners (the/a/an), demonstratives (this/that etc.), quantifiers (some, many etc.), numerals and adjectives.
The PTB bracketing guidelines make the decision that all pre-nominal elements and the head be grouped
together using a flat structure to form a NP, which then is treated as a sister of a post-nominal constituent,
be it a complement or an adjunct. As for the Chinese NP, the major difference between a Chinese NP
and an English one is that there can only be pre-nominal constituents in Chinese NPs. In other words,
the head noun is the rightmost element in a Chinese NP and nothing comes after it.
The incompatibility has to do with the complement-adjunct distinction. The complement of an English
noun can be either a PP or an S, which always comes after the noun. Due to space limit, we only discuss
PP below. An English noun and its PP complement, because of the close semantic relationship between
the two, are usually translated as a compound noun in Chinese. For instance, student of linguistics is
translated as the N-N compound???/linguistics??/student. A compound is treated by the CTB
bracketing standard as an NP dominating all its components. Unfortunately, the English head noun and
its complement do not form a constituent, which, if present, can be aligned with the node for the Chinese
compound. This causes incompatibility to arise. Take Figure 1 for instance, the English string methods
of expression is translated as the Chinese compound noun??/expression??/method. As shown by
the structure, the noun method and its PP complement do not form a constituent. As a result, the Chinese
compound noun has no alignment.
To remove the incompatibility, we need to change the existing structure of the English NP. Still using
the example in Figure 1 for instance, if the English noun phrase has the structure in Figure 5, then we can
align the English NP methods of expression with the Chinese NP ??/expression ??/method. The
structure in Figure 5 is different from what is given by the PTB standards in that the head noun (such as
35
.NP.
.PP.
.NP
.NN
.expression
.
.IN
.of
.
.NP.
.NN
.method
.JJ
.own
.DT
.their
(a) Current structure
.NP.
.NP.
.PP.
.NP
.NN
.expression
.
.IN
.of
.
.NN
.method
.
.JJ
.own
.DT
.their
(b) Proposed new structure
Figure 5: A proposed revision for the existing structure of English NPs
method) is combined with its complement (such as the PP of expression) first to create an NP, which then
is modified by, say, an adjective (such as own) and a determiner (such as their). From the semantic point
of view, a pre-nominal adjective is an adjunct to the head noun that is not as closely related to the head
noun as its complement. The new structure given in Figure 5b reflects this semantic fact by combining
the complement with the head before the adjective.
5 Conclusion
In this paper, we argue that it is feasible to align Chinese-English parallel parse trees despite incompat-
ibility issues. We show that the most common incompatibility is caused by bracketing guideline design,
which can be fixed by changing the existing structures stipulated by the current annotation standards.
The revised structures we proposed to avoid the incompatibility are deeper than the existing PTB struc-
tures and respect the complement-adjunct distinction, which is a well-established notion in linguistics
that has been shown to manifest itself in different kinds of phenomena cross-linguistically. In syntax, the
distinction is made by combining the head and its complement first to form a constituent, which then is
combined with an adjunct. This way of representing the distinction is standard and gives arise to a struc-
ture that is binary-branching and deep. In syntactic annotation, linguistic sophistication which requires
the parse tree to reflect well-established linguistic notions such as the complement-adjunct distinction is
an important consideration and generally gives arise to deeper structures. In addition to linguistic sophis-
tication, another important consideration in syntactic annotation is engineering economy, which requires
the annotation to be economical in the sense that it can be carried out in a convenient and efficient manner
to save annotation effort and time. This means that the parse tree needs to be as flat as possible since shal-
low structures are much easier to annotate than deep ones. These two competing considerations interact
to influence the establishment of bracketing standards.
Due to engineering pressure caused by the fact that it is not easy to make a consistent distinction
between complements and adjuncts in annotation, the PTB bracketing guidelines chose a shallow struc-
ture for both VPs and NPs as shown above. The decision is understandable since no incompatibility
ever arises in the construction of a monolingual treebank like PTB. With the advent of new use cases of
monolingual treebanks such as hierarchically aligned parallel treebanks, new issues like incompatibility
emerge and call for adjustments to some decisions that have been made without such issues. As shown
in Section 4, some decisions made in existing bracketing annotation cause incompatibilities and make
legitimate phrase alignments impossible. For the purpose of aligning parallel parse trees, deeper and lin-
guistically motivated structures are needed. This raises the interesting question whether we should have
a deeper and linguistically motivated structure to start with when constructing a monolingual treebank.
Based on what we have seen in this paper, a positive answer to the question seems reasonable at least
in some cases such as VPs and NPs for the sake of better serving uses cases like constructing parallel
36
treebanks with hierarchical alignments.
Acknowledgements
The HACEPT project, under which the work presented in this paper is done, is supported by the IBM
subcontract No. 4913014934 under DARPA Prime Contract No. 0011-12-C-0015 entitled "Broad Oper-
ational Language Translation". Wewould like to thank Libin Shen and Salim Roukos for their inspiration
and discussion during early stages of the project, Abe Ittycheriah and Niyu Ge for their help with setting
up the data, Loretta Bandera for developing and maintaining the annotation tool, and three anonymous
reviewers for their helpful comments. We are grateful for the hard work of our four annotators: Hui Gao,
Shiman Guo, Tse-ming Wang and Lingya Zhou. Any opinions, findings, conclusions or recommenda-
tions expressed in this material are those of the authors and do not necessarily reflect those of the sponsor
or any of the people mentioned above.
References
Ann Bies, Mark Ferguson, Karen Katz, Robert MacIntyre, Victoria Tredinnick, Grace Kim, Mary Ann
Marcinkiewicz, and Britta Schasberger. 1995. Bracketing guidelines for Treebank II style Penn Treebank
project. Technical report, University of Pennsylvania.
David Chiang. 2010. Learning to translate with source and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics, pages 1443--1452.
Dun Deng and Nianwen Xue. 2014. Building a Hierarchically Aligned Chinese-English Parallel Treebank. In
Proceedings of the 25th International Conference on Computational Linguistics (COLING 2014).
Daniel Gildea. 2003. Loosely tree-based alignment for machine translation. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics-Volume 1, pages 80--87.
Xuansong Li, Niyu Ge, and Stephanie Strassel. 2009. Tagging guidelines for Chinese-English word alignment.
Technical report, Linguistic Data Consortium.
Yang Liu, Yajuan L?, and Qun Liu. 2009. Improving tree-to-tree translation with packed forests. In Proceedings
of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages 558--566.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a Large Annotated Corpus
of English: The Penn Treebank. Computational Linguistics, 19(2):313--330.
Jun Sun, Min Zhang, and Chew Lim Tan. 2010. Exploring syntactic structural features for sub-tree alignment
using bilingual tree kernels. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics, pages 306--315.
Tong Xiao and Jingbo Zhu. 2013. Unsupervised sub-tree alignment for tree-to-tree translation. Journal of Artifi-
cial Intelligence Research, 48:733--782.
Nianwen Xue and Fei Xia. 2000. The bracketing guidelines for Penn Chinese Treebank project. Technical report,
University of Pennsylvania.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha Palmer. 2005. The Penn Chinese Treebank: Phrase Structure
Annotation of a Large Corpus. Natural Language Engineering, 11(2):207--238.
37
