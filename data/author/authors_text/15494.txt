Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 804?813,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Linking Entities to a Knowledge Base with Query Expansion
Swapna Gottipati
School of Information Systems
Singapore Management University
Singapore
swapnag.2010@smu.edu.sg
Jing Jiang
School of Information Systems
Singapore Management University
Singapore
jingjiang@smu.edu.sg
Abstract
In this paper we present a novel approach
to entity linking based on a statistical lan-
guage model-based information retrieval with
query expansion. We use both local con-
texts and global world knowledge to expand
query language models. We place a strong
emphasis on named entities in the local con-
texts and explore a positional language model
to weigh them differently based on their dis-
tances to the query. Our experiments on
the TAC-KBP 2010 data show that incor-
porating such contextual information indeed
aids in disambiguating the named entities and
consistently improves the entity linking per-
formance. Compared with the official re-
sults from KBP 2010 participants, our system
shows competitive performance.
1 Introduction
When people read news articles, Web pages and
other documents online, they may encounter named
entities which they are not familiar with and there-
fore would like to look them up in an encyclope-
dia. It would be very useful if these entities could be
automatically linked to their corresponding encyclo-
pedic entries. This task of linking mentions of enti-
ties within specific contexts to their corresponding
entries in an existing knowledge base is called en-
tity linking and has been proposed and studied in the
Knowledge Base Population (KBP) track of the Text
Analysis Conference (TAC) (McNamee and Dang,
2009). Besides improving an online surfer?s brows-
ing experience, entity linking also has potential us-
age in many other applications such as normalizing
entity mentions for information extraction.
The major challenge of entity linking is to resolve
name ambiguities. There are generally two types of
ambiguities: (1) Polysemy: This type of ambigu-
ities refers to the case when more than one entity
shares the same name. E.g. George Bush may re-
fer to the 41st President of the U.S., the 43rd Presi-
dent of the U.S., or any other individual who has the
same name. Clearly polysemous names cause diffi-
culties for entity linking. (2) Synonymy: This type
of ambiguities refers to the case when more than
one name variation refers to the same entity. E.g.
Metro-Goldwyn-Mayer Inc. is often abbreviated as
MGM. Synonymy affects entity linking when the en-
tity mention in the document uses a name variation
not covered in the entity?s knowledge base entry.
Intuitively, to disambiguate a polysemous entity
name, we should make use of the context in which
the name occurs, and to address synonymy, exter-
nal world knowledge is usually needed to expand
acronyms or find other name variations. Indeed
both strategies have been explored in existing litera-
ture (Zhang et al, 2010; Dredze et al, 2010; Zheng
et al, 2010). However, most existing work uses
supervised learning approaches that require careful
feature engineering and a large amount of training
data. In this paper, we take a simpler unsupervised
approach using statistical language model-based in-
formation retrieval. We use the KL-divergence re-
trieval model (Zhai and Lafferty, 2001) and ex-
pand the query language models by considering both
the local contexts within the query documents and
global world knowledge obtained from the Web.
804
Symbol Description
Q Query
DQ Query document
NQ Query name string
E KB entity node
NE KB entity name string
DE KB entity disambiguation text
SQ Set of alternate query name strings
N l,iQ Local alternative name strings
NgQ Global alternative name strings
EQ Candidate KB entries for Q
?Q Query Language Model
?LQ KB entry language model using local context from DQ
?GQ KB entry language model using global knowledge
?L+GQ KB entry language model using local context and global knowledge
?NE KB entry language model with named entities only
?NE+DE KB entry language model with named entities and disambiguation text
Table 1: Notation
We evaluate our retrieval method with query ex-
pansion on the 2010 TAC-KBP data set. We find that
our expanded query language models can indeed
improve the performance significantly, demonstrat-
ing the effectiveness of our principled and yet sim-
ple techniques. Comparison with the official results
from KBP participants also shows that our system is
competitive. In particular, when no disambiguation
text from the knowledge base is used, our system can
achieve an overall 85.2% accuracy and 9.3% relative
improvement over the best performance reported in
KBP 2010.
2 Task Definition and System Overview
Following TAC-KBP (Ji et al, 2010), we define the
entity linking task as follows. First, we assume
the existence of a Knowledge Base (KB) of enti-
ties. Each KB entry E represents a unique entity
and has three fields: (1) a name string NE , which
can be regarded as the official name of the entity,
(2) an entity type TE , which is one of {PER, ORG,
GPE, UNKNOWN}, and (3) some disambiguation
text DE . Given a query Q which consists of a query
name string NQ and a query document DQ where
the name occurs, the task is to return a single KB
entry to which the query name string refers or Nil if
there is no such KB entry.
It is fairly natural to address entity linking by
ranking the KB entries given a query. In this section
we present an overview of our system, which con-
sists of two major stages: a candidate selection stage
to identify a set of candidate KB entries through
name matching, and a ranking stage to link the query
entity to the most likely KB entry. In both stages,
we consider the query?s local context in the query
document and world knowledge obtained from the
Web. It is important to note that the selection stage
is based on string matching where the order of the
word matters. It is different from the ranking stage
where a probabilistic retrieval model based on bag-
of-word representation is used. Our preliminary ex-
periments demonstrate that without the first candi-
date selection stage the linking process results in low
performance.
2.1 Selecting Candidate KB Entries
The first stage of our system aims to filter out irrel-
evant KB entries and select only a set of candidates
that are potentially the correct match to the query.
Intuitively, we determine whether two entities are
the same by comparing their name strings. We there-
fore need to compare the query name stringNQ with
the name string NE of each KB entry. However,
because of the name ambiguity problem, we cannot
expect the correct KB entry to always have exactly
the same name string as the query. To address this
problem, we use a set of alternative name strings ex-
panded from NQ and select KB entries whose name
805
strings match at least one of them. These alterna-
tive name strings come from two sources: the query
document DQ and the Web.
First, we observe that some useful alternative
name strings come from the query document. For
example, a PER query name string may contain only
a person?s last name but the query document con-
tains the person?s full name, which is clearly a less
ambiguous name string to use. Similarly, a GPE
query name string may contain only the name of a
city or town but the query document contains the
state or province, which also helps disambiguate the
query entity. Based on this observation, we do the
following. Given query Q, let SQ denote the set of
alternative query name strings. Initially SQ contains
only NQ. We then use an off-the-shelf NER tagger
to identify named entities from the query document
DQ. For PER and ORG queries, we select named
entities in DQ that contain NQ as a substring. For
GPE queries, we select named entities that are of the
type GPE, and we then combine each of them with
NQ. We denote these alternative name strings as
{N l,iQ }
KQ
i=1, where l indicates that these name strings
come locally fromDQ andKQ is the total number of
such name strings. {N l,iQ } are added to SQ. Figure
1 and Figure 2 show two example queries together
with their SQ.
Sometimes alternative name strings have to come
from external knowledge. For example, one of the
queries we have contains the name string ?AMPAS,?
and the query document also uses only this acronym
to refer to this entity. But the full name of the entity,
?Academy of Motion Pictures Arts and Sciences,? is
needed in order to locate the correct KB entry. To
tackle this problem, we leverage Wikipedia to find
the most likely official name. Given query name
string NQ, we check whether the following link ex-
ists: http://en.wikipedia.org/NQ. If NQ
is an abbreviation, Wikipedia will redirect the link
to the Wikipedia page of the corresponding entity
with its official name. So if the link exists, we use
the title of the Wikipedia page as another alternative
name string for NQ. We refer to this name string as
NgQ to indicate that it is a global name variant. NgQ is
also added to SQ. Figure 2 shows such an example.
For each name stringN in SQ, we find KB entries
whose name strings match N . We take the union of
Query name string (NQ): Mobile
Query document (DQ): The site is near Mount Ver-
non in the Calvert community on the Tombigbee River,
some 25 miles (40 kilometers) north of Mobile. It?s on
a river route to the Gulf of Mexico and near Mobile?s
rails and interstates. Along with tax breaks and $400
million (euro297 million) in financial incentives, Al-
abama offered a site with a route to a Brazil plant that
will provide slabs for processing in Mobile.
Alternative Query Strings (SQ):
from local context: Mobile, Mobile Mount Vernon,
Mobile Calvert, Mobile River, Mobile Mexico, Mobile
Alabama, Mobile Brazil
Figure 1: An example GPE query from TAC 2010.
Query name string (NQ): Coppola
Query document (DQ): I had no idea of all these
semi-obscure connections, felicia! Alex Greenwald
and Claire Oswalt aren?t names I?m at all familiar
with, but Jason Schwartzman I?ve heard of. Isn?t he
Sophia Coppola?s cousin? I think I once saw a pic-
ture of him sometime ago
Alternative Query Strings (SQ):
from local context: Coppola, Sophia Coppola, Sofia
Coppola
from world knowledge(Wikipedia): Sofia Coppola
Figure 2: An example PER query from TAC 2010.
these sets of KB entries and refer to it as EQ. These
are the candidate KB entries for query Q.
2.2 Ranking KB Entries
Given the candidate KB entries EQ, we need to
decide which one of them is the correct match.
We adopt the widely-used KL-divergence retrieval
model, a statistical language model-based retrieval
method proposed by Lafferty and Zhai (2001).
Given a KB entry E and query Q, we score E based
on the KL-divergence defined below:
s(E,Q) = ?Div(?Q??E) = ?
?
w?V
p(w|?Q) log
p(w|?Q)
p(w|?E)
.
(1)
Here ?Q and ?E are the query language model and
the KB entry language model, respectively. A lan-
guage model here is a multinomial distribution over
words (i.e. a unigram language model). V is the
vocabulary and w is a single word.
To estimate ?E , we follow the standard maxi-
mum likelihood estimation with Dirichlet smooth-
806
ing (Zhai and Lafferty, 2004):
p(w|?E) =
c(w,E) + ?p(w|?C)
|E| + ? , (2)
where c(w,E) is the count of w in E, |E| is the
number of words in E, ?C is a background lan-
guage model estimated from the whole KB, and ?
is the Dirichlet prior. Recall that E contains NE , TE
and DE . We consider using either NE only or both
NE and DE to obtain c(w,E) and |E|. We refer
to the former estimated ?E as ?NE and the latter as
?NE+DE .
To estimate ?Q, typically we can use the empirical
query word distribution:
p(w|?Q) =
c(w,NQ)
|NQ|
, (3)
where c(w,NQ) is the count of w in NQ and |NQ|
is the length of NQ. We call this model the original
query language model.
After ranking the candidate KB entries in EQ us-
ing Equation (1), we perform entity linking as fol-
lows. First, using an NER tagger, we determine the
entity type of the query name string NQ. Let TQ de-
note this entity type. We then pick the top-ranked
KB entry whose score is higher than a threshold ?
and whose TE is the same as TQ. The system links
the query entity to this KB entry. If no such entry
exists, the system returns Nil.
3 Query Expansion
We have shown in Section 2.1 that using the origi-
nal query name string NQ itself may not be enough
to obtain the correct KB entry, and additional words
from both the query document and external knowl-
edge can be useful. However, in the KB entry se-
lection stage, these additional words are only used
to enlarge the set of candidate KB entries; they have
not been used to rank KB entries. In this section, we
discuss how to expand the query language model ?Q
with these additional words in a principled way in
order to rank KB entries based on how likely they
match the query entity.
3.1 Using Local Contexts
Let us look at the example from Figure 2 again.
During the KB entry ranking stage, if we use ?Q
estimated from NQ, which contains only the word
?Coppola,? the retrieval function is unlikely to rank
the correct KB entry on the top. But if we include
the contextual word ?Sophia? from the query doc-
ument when estimating the query language model,
KL-divergence retrieval model is likely to rank the
correct KB entry on the top. This idea of using
contextual words to expand the query is very sim-
ilar to (pseudo) relevance feedback in information
retrieval. We can treat the query document DQ as
our only feedback document.
Many different (pseudo) relevance feedback
methods have been proposed. Here we apply the
relevance model (Lavrenko and Croft, 2001), which
has been shown to be effective and robust in a re-
cent comparative study (Lv and Zhai, 2009). We
first briefly review the relevance model. Given a set
of (pseudo) relevant documents Dr, where for each
D ? Dr there is a document language model ?D,
we can estimate a feedback language model ?fbQ as
follows:
p(w|?fbQ) ?
?
D?Dr
p(w|?D)p(?D)p(Q|?D). (4)
For our problem, since we have only a single feed-
back document DQ, the equation above can be sim-
plified. In fact, in this case the feedback language
model is the same as the document language model
of the only feedback document, i.e. ?DQ .
We then linearly interpolate the feedback lan-
guage model with the original query language model
to form an expanded query language model:
p(w|?LQ) = ?p(w|?Q) + (1 ? ?)p(w|?DQ), (5)
where ? is a parameter between 0 and 1, to control
the amount of feedback. The larger ? is, the less we
rely on the local context. L indicates that the query
expansion comes from local context. This ?LQ can
then replace ?Q in Equation (1) to rank KB entries.
Special Treatment of Named Entities
Usually the document language model ?DQ is es-
timated using the entire text from DQ. For entity
linking, we suspect that named entities surrounding
the query name string in DQ are particularly useful
for disambiguation and thus should be emphasized
over other words. This can be done by weighting
807
NE and non-NE words differently. In the extreme
case, we can use only NEs to estimate the document
language model ?DQ as follows:
p(w|?DQ) =
1
KQ
KQ?
i=1
c(w,N l,iQ )
|N l,iQ |
, (6)
where {N l,iQ } are defined in Section 2.
Positional Model
Another observation is that words closer to the
query name string in the query document are likely
to be more important than words farther away. Intu-
itively, we can use the distance between a word and
the query name string to help weigh the word. Here
we apply a recently proposed positional pseudo rel-
evance feedback method (Lv and Zhai, 2010). The
document language model ?DQ now has the follow-
ing form:
p(w|?DQ) =
1
KQ
KQ?
i=1
f(pi, q) ?
c(w,N l,iQ )
|N l,iQ |
, (7)
where pi and q are the absolute positions of N l,iQ
and NQ in DQ. The function f is Gaussian function
defined as follows:
f(p, q) = 1?
2pi?2
exp
(?(p? q)2
2?2
)
. (8)
where variance ? controls the spread of the curve.
3.2 Using Global World Knowledge
Similar to the way we incorporate words from DQ
into the query language model, we can also con-
struct a feedback language model using the most
likely official name of the query entity obtained from
Wikipedia. Specifically, we define
p(w|?NgQ) =
c(w,NgQ)
|NgQ|
. (9)
We can then linearly interpolate ?NgQ with the orig-inal query language model ?Q to form an expanded
query language model ?GQ:
p(w|?GQ) = ?p(w|?Q) + (1 ? ?)p(w|?NgQ). (10)
Here G indicates that the query expansion comes
from global world knowledge.
Entity Type %Nil %non-Nil
GPE 32.8% 67.2 %
ORG 59.5% 40.5 %
PER 71.7% 28.3 %
Table 2: Percentages of Nil and non-Nil queries.
3.3 Combining Local Context and World
Knowledge
We can further combine the two kinds of additional
words into the query language model as follows:
p(w|?L+GQ ) = ?p(w|?Q) + (1 ? ?)
(
?p(w|?DQ)
+(1 ? ?)p(w|?NgQ)
)
. (11)
Note that here we have two parameters ? and ? to
control the amount of contributions from the local
context and from global world knowledge.
4 Experiments
4.1 Experimental Setup
Data Set: We evaluate our system on the TAC-KBP
2010 data set (Ji et al, 2010). The knowledge base
was constructed from Wikipedia with 818,741 en-
tries. The data set contains 2250 queries and query
documents come from news wire and Web pages.
Around 45% of the queries have non-Nil entries in
the KB. Some statistics of the queries are shown in
Table 2.
Tools: In our experiments, to extract named entities
withinDQ and to determine TQ, we use the Stanford
NER tagger1. An example output of the NER tagger
is shown below:
<PERSON>Hugh Jackman<PERSON> is
Jacked!!
This piece of text comes from a query document
where the query name string is ?Jackman.? We can
see that the NER tagger can help locate the full name
of the person.
We use the Lemur/Indri2 search engine for re-
trieval. It implements the KL-divergence retrieval
model as well as many other useful functionalities.
Evaluation Metric: We adopt the Micro-averaged
accuracy metric, which is the mean accuracy over
all queries. It was used in TAC-KBP 2010 (Ji et
1http://nlp.stanford.edu/software/CRF-NER.shtml
2http://www.lemurproject.org/indri.php
808
al., 2010) as the official metric to evaluate the per-
formance of entity linking. This metric is simply
defined as the percentage of queries that have been
correctly linked.
Methods to Compare: Recall that our system con-
sists of a KB entry selection stage and a KB entry
ranking stage. At the selection stage, a set SQ of
alternative name strings are used to select candidate
KB entries. We first define a few settings where dif-
ferent alternative name string sets are used to select
candidate KB entries:
? Q represents the baseline setting which uses
only the original query name string NQ to se-
lect candidate KB entries.
? Q+L represents the setting where alternative
name strings obtained from the query docu-
ment DQ are combined with NQ to select can-
didate KB entries.
? Q+G represents the setting where the alterna-
tive name string obtained from Wikipedia is
combined with NQ to select candidate KB en-
tries.
? Q+L+G represents the setting as we described
in Section 2.1, that is, alternative name strings
from both DQ and Wikipedia are used together
with NQ to select candidate KB entries.
After selecting candidate KB entries, in the KB
entry ranking stage, we have four options for the
query language model and two options for the KB
entry language model. For the query language
model, we have (1) ?Q, the original query language
model, (2) ?LQ, an expanded query language model
using local context from DQ, (3) ?GQ, an expanded
query language model using global world knowl-
edge, and (4) ?L+GQ , an expanded query language
model using both local context and global world
knowledge. For the KB entry language model, we
can choose whether or not to use the KB disam-
biguation text DE and obtain ?NE and ?NE+DE , re-
spectively.
4.2 Results and Discussion
First, we compare the performance of KB entry se-
lection stage for all four settings on non-Nil queries.
The performance measure recall is defined as
recall =
{
1, if E that refers to Q, exists in EQ
0, otherwise
The recall statistics in Table 3 shows that, Q+L+G
has the highest recall of the KB candidate entries.
Method Recall(%)
Q 67.1
Q+L 89.7
Q+G 94.9
Q+L+G 98.2
Table 3: Comparing the effect of candidate entry selec-
tion using different methods - KB entry selection stage
recall.
Before examining the effect of query expansion
in ranking, we now compare the effect of using dif-
ferent sets of alternative query name strings in the
candidate KB entry selection stage. For this set of
experiments, we fix the query language model to ?Q
and the KB entry language model to ?NE in the rank-
ing stage.
Table 4 shows the performance of all the settings
in terms of micro-averaged accuracy. The results
shown in Tables 4, 5 and 6 are based on the opti-
mum parameter settings. We can see that in terms
of the overall performance, both Q+L and Q+G give
better performance than Q with a 7.7% and a 9.9%
relative improvement, respectively. Q+L+G gives
the best performance with a 12.8% relative improve-
ment over Q. If we further zoom into the results, we
see that for ORG and PER queries, when no correct
KB entry exists (i.e. the Nil case), the performance
of Q, Q+L, Q+G and Q+L+G is very close, indicat-
ing that the additional alternative query name strings
do not help. It shows that the alternative query name
strings are most useful for queries that do have their
correct entries in the KB.
We now further analyze the impact of the ex-
panded query language models ?LQ, ?GQ and ?L+GQ .
We first analyze the results without using the KB
disambiguation text, i.e. using ?NE . Table 5 shows
the comparison between ?Q and other expanded
query language models in terms of micro-averaged
accuracy. The results reveal that the expanded query
language models can indeed improve the overall per-
formance (the both Nil and non-Nil case) under all
settings. This shows the effectiveness of using the
principled query expansion technique coupled with
KL-divergence retrieval model to rank KB entries.
809
All Nil Non-Nil
Method ALL GPE ORG PER GPE ORG PER GPE ORG PER
Q 0.6916 0.5714 0.6533 0.8495 0.8618 0.9888 0.9963 0.4294 0.1612 0.4789
Q+L 0.7449 0.7156 0.6533 0.8655 0.9472 0.9888 0.9944 0.6024 0.1612 0.5399
Q+G 0.7604 0.7009 0.6893 0.8908 0.9431 0.9888 0.9944 0.5825 0.2500 0.6291
Q+L+G 0.7800 0.7583 0.6893 0.8921 0.9431 0.9888 0.9944 0.6680 0.2500 0.6338
Table 4: Comparing the performance of using different sets of query name strings for candidate KB entry selection.
?Q and ?NE are used in KB entry ranking.
All Nil Non-Nil
Method QueryModel ALL GPE ORG PER GPE ORG PER GPE ORG PER
Q+L ?Q 0.7449 0.7156 0.6533 0.8655 0.9472 0.9888 0.9944 0.6024 0.1612 0.5399?LQ 0.7689 0.7850 0.6533 0.8682 0.9309 0.9888 0.9944 0.7137 0.1612 0.5493
Q+G ?Q 0.7604 0.7009 0.6893 0.8908 0.9431 0.9888 0.9944 0.5825 0.2500 0.6291?GQ 0.8160 0.7423 0.7867 0.9188 0.9106 0.9372 0.9796 0.6600 0.5658 0.7653
Q+L+G ?Q 0.7800 0.7583 0.6893 0.8921 0.9431 0.9888 0.9944 0.6680 0.2500 0.6338?L+GQ 0.8516 0.8278 0.7867 0.9401 0.8821 0.9372 0.9814 0.8012 0.5658 0.8357
Table 5: Comparison between the performance of ?Q and expanded query language models in terms of micro average
accuracy. ?NE was used in ranking.
On the other hand, again we observe that the ef-
fects on the Nil and the non-Nil queries are differ-
ent. While in Table 4 the alternative name strings
do not affect the performance much for Nil queries,
now the expanded query language models actually
hurt the performance for Nil queries. It is not sur-
prising to see this result. When we expand the query
language model, we can possibly introduce noise,
especially when we use the external knowledge ob-
tained from Wikipedia, which largely depends on
what Wikipedia considers to be the most popular
official name of a query name string. With noisy
terms in the expanded query language model we in-
crease the chance to link the query to a KB entry
which is not the correct match. The challenge is that
we do not know when additional terms in the ex-
panded query language model are noise and when
they are not, because for non-Nil queries we do ob-
serve a substantial amount of improvement brought
by query expansion, especially with external world
knowledge. We will further investigate this research
question in the future.
We now further study the impact of using the KB
disambiguation text associated with each entry to es-
timate the KB entry language model used in the KL-
divergence ranking function. The results are shown
in Table 6 for all the methods on ?NE vs. ?NE+DE
using the expanded query language models. We can
see that for all methods the impact of using the KB
disambiguation text is very minimal and is observed
only for GPE and ORG queries. Table 7 shows an
example of the KL-divergence scores for a query,
Mobile whose context is previously shown in the
Figure 1. Without the KB disambiguation text both
the KB entry Mobile Alabama and the entry Mobile
River are given the same score, resulting in inaccu-
rate linking in the ?NE case. But with ?NE+DE , Mo-
bile Alabama was scored higher, resulting in an ac-
curate linking. However, we observe that such cases
are very rare in the TAC 2010 query list and thus the
overall improvement observed is minimal.
KB Entry KB Name w/o text w/ text
E0583976 Mobile Alabama -6.28514 -6.3839
E0183287 Mobile River -6.28514 -6.69372
Table 7: The KL-divergence scores of KB entities for the
query Mobile.
Finally, we compare our performance with the
highest scores from TAC-KBP 2010 as shown in the
Table 8. It is important to note that the highest TAC
results shown in the table under each setting are not
necessarily obtained by the same team. We can see
that our overall performance when KB text is used is
competitive compared with the highest TAC score,
and is substantially higher than the TAC score when
KB text is not used. Lehmann et al (2010) achieved
highest TAC scores. They used a variety of evidence
from Wikipedia like disambiguation pages, anchors,
expanded acronyms and redirects to build a rich fea-
ture set. But as we discussed, building a rich fea-
810
All Nil Non-Nil
Method KB Text ALL GPE ORG PER GPE ORG PER GPE ORG PER
Q ?NE 0.6916 0.5714 0.6533 0.8495 0.8618 0.9888 0.9963 0.4294 0.1612 0.4789?NE+DE 0.6888 0.5607 0.6533 0.8495 0.8618 0.9888 0.9963 0.4135 0.1612 0.4789
Q+L ?NE 0.7689 0.7850 0.6533 0.8682 0.9309 0.9888 0.9944 0.7137 0.1612 0.5493?NE+DE 0.7707 0.7904 0.6533 0.8682 0.9390 0.9888 0.9944 0.7177 0.1612 0.5493
Q+G ?NE 0.8160 0.7423 0.7867 0.9188 0.9106 0.9372 0.9796 0.6600 0.5658 0.7653?NE+DE 0.8222 0.7450 0.7827 0.9387 0.8902 0.9372 0.9814 0.6740 0.5559 0.8310
Q+L+G ?NE 0.8516 0.8278 0.7867 0.9401 0.8821 0.9372 0.9814 0.8012 0.5658 0.8357?NE+DE 0.8524 0.8291 0.7880 0.9401 0.8740 0.9372 0.9814 0.8072 0.5691 0.8357
Table 6: Comparing the performance using KB text and without using KB text for all methods using expanded query
models in terms of micro average accuracy on 2250 queries. ?NE+DE represents method using KB text and ?NE
represents methods without using KB text.
ture set is an expensive task. Their overall accu-
racy is 1.5% higher than our model. Table 8 shows
that the performance of ORG entities is lower when
compared with the TAC results when we used KB
text. In our analysis, we observed that, even though
some entities like AMPAS are linked correctly, the
entities like CCC (Consolidated Contractors Com-
pany) failed due to ambiguity in the title. Here, we
may benefit by leveraging more global knowledge,
i.e, we should expand the NgQ with Wikipedia global
context entities together with the title to fully benefit
from global knowledge. In particular, when KB text
is not used, our system outperforms the highest TAC
results for all three types of queries.
From the analysis by Ji et al (2010), overall the
participating teams generally performed the best on
PER queries and the worst on GPE queries. With our
system, we can achieve good performance on GPE
queries.
KB Text Usage Type Our System TAC Highest
?NE+DE
All 0.8524 0.8680
GPE 0.8291 0.7957
ORG 0.7880 0.8520
PER 0.9401 0.9601
?NE
All 0.8516 0.7791
GPE 0.8278 0.7076
ORG 0.7867 0.7333
PER 0.9401 0.9001
Table 8: Comparison of the best configuration of our sys-
tem (Q+L+Gwith ?L+GQ ) with the TAC-KBP 2010 resultsin terms of micro-averaged accuracy. ?NE+DE represents
the method using KB disambiguation text and ?NE repre-
sents the method without using KB disambiguation text.
4.3 Parameter Sensitivity
In all our experiments, we set the Dirichlet prior ? to
2500 following previous studies. For the threshold ?
we empirically set it to -12.0 in all the experiments
based on preliminary results. Recall that all the ex-
panded query language models also have a control
parameters ?. The local context-based models ?LQ
and ?L+GQ have an additional parameter ? which
controls the proximity weighing. The ?L+GQ model
has another additional parameter ? that controls the
balance between local context and world knowledge.
In this subsection, we study the sensitivity of these
parameters. We plot the sensitivity graphs for all the
methods that involve ? (? set to 0.5) in Figure 3. As
we can see, all the curves appear to be stable and
?=0.4 appears to work well.
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8
Mi
cro
 A
ve
rag
ed
 Ac
cu
rac
y
alpha
Q+LQ+GQ+L+G
Figure 3: Sensitivity of ? in regard to micro-averaged
accuracy.
Similarly, we set ?=0.4 and examine how ? af-
fects micro averaged accuracy. We plot the sensi-
tivity curve for ? for the Q+L+G setting with ?L+GQ
in Figure 5. As we can see, the best performance
is achieved when ?=0.5. This implies that the local
811
 0.76
 0.77
 0.78
 0.79
 0.8
 0.81
 0.82
 0.83
 0.84
 0.85
 0.86
 0  0.2  0.4  0.6  0.8  1
Mi
cro
 A
ve
rag
ed
 Ac
cu
rac
y
beta
Q+L+G
Figure 4: Sensitivity of ? in regard to micro-averaged
accuracy.
context and the global world knowledge are weighed
equally for aiding disambiguation and improving the
entity linking performance.
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 40  60  80  100  120
Mi
cro
 A
ve
rag
ed
 Ac
cu
rac
y
sigma
Q+LQ+L+G
Figure 5: Sensitivity of ? with respect to micro-averaged
accuracy.
Furthermore, we systematically test a fixed set of
? values from 25 to 125 with an intervals of 25 and
examine how ? affects micro averaged accuracy. We
set ?=0.4 and ?=0.5, which is the best parameter
setting as discussed above. We plot the sensitivity
curves for the parameter ? for methods that utilize
the local context, i.e. ?LQ and ?L+GQ , in Figure 5. We
observe that all the curves are stable and 75 <= ?
<= 100 appears to work well. We set ?=100 for all
our experiments. Moreover, after 100, the graph be-
comes stable, which indicates that proximity has less
impact on the method from this point on. This im-
plies that an equal weighing scheme actually would
work the same for these experiments. Part of the
reason may be that by using only named entities in
the context rather than all words, we have effectively
picked the most useful contextual terms. Therefore,
positional feedback models do have exhibit much
benefit for our problem.
5 Related Work
Bunescu and Pasca (2006) and Cucerzan (2007) ex-
plored the entity linking task using Vector Space
Models for ranking. They took a classification ap-
proach together with the novel idea of exploiting
Wikipedia knowledge. In their pioneering work,
they used Wikipedia?s category information for en-
tity disambiguation. They show that using differ-
ent background knowledge, we can find efficient ap-
proaches for disambiguation. In their work, they
took an assumption that every entity has a KB en-
try and thus the NIL entries are not handled.
Similar to other researchers, Zhang et al (2010)
took an approach of classification and used a two-
stage approach for entity liking. They proposed a
supervised model with SVM ranking to filter out the
candidates and deal with disambiguation effectively.
For entity diambiguation they used the contextual
comparisons between the Wikipedia article and the
KB article. However, their work ignores the possi-
bilities of acronyms in the entities. Also, the am-
biguous geo-political names are not handled in their
work.
Dredze et al (2010) took the approach that large
number of entities will be unlinkable, as there is
a probability that the relevant KB entry is unavail-
able. Their algorithm for learning NIL has shown
very good results. But their proposal for handling
the alias name or stage name via multiple lists is not
scalable. Unlike their approach, we use the global
knowledge to handle the stage names and thus this
gives an optimized solution to handle alias names.
Similarly, for acronyms we use the global knowl-
edge that aids unabbreviating and thus entity dis-
ambiguation. Similar to other approaches, Zheng
et al (2010) took a learning to rank approach and
compared list-wise rank model to the pair-wise rank
model. They achieved good results on the list-wise
ranking approach. They handled acronyms and dis-
ambiguity through wiki redirect pages and the an-
chor texts which is similar to others ideas.
Challenges in supervised learning includes care-
ful feature selection. The features can be selected in
ad hoc manner - similarity based or semantic based.
Also machine learning approach induces challenges
of handling heterogenous cases. Unlike their ma-
chine learning approach which requires careful fea-
812
ture engineering and heterogenous training data, our
method is simple as we use simple similarity mea-
sures. At the same time, we propose a statistical
language modeling approach to the linking prob-
lem. Many researchers have proposed efficient ideas
in their works. We integrated some of their ideas
like world knowledge with our new techniques to
achieve efficient entity linking accuracy.
6 Conclusions
In this paper we proposed a novel approach to entity
linking based on statistical language model-based
information retrieval with query expansion using the
local context from the query document as well as
world knowledge from the Web. Our model is a sim-
ple unsupervised one that follows principled exist-
ing information retrieval techniques. And yet it per-
forms the entity linking task effectively compared
with the best performance achieved in the TAC-KBP
2010 evaluation.
Currently our model does not exploit world
knowledge from the Web completely. World knowl-
edge, especially obtained from Wikipedia, has
shown to be useful in previous studies. As our future
work, we plan to explore how to further incorporate
such world knowledge into our model in a principled
way.
References
Razvan Bunescu and Marius Pasca. 2006. Using ency-
clopedic knowledge for named entity disambiguation.
In Proceesings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-06), pages 9?16, Trento, Italy.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 708?716.
Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation
for knowledge base population. In Proceedings of the
23rd International Conference on Computational Lin-
guistics, pages 277?285.
Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2010. Overview of the TAC 2010
knowledge base population track. In Proceedings of
the Third Text Analysis Conference.
John Lafferty and ChengXiang Zhai. 2001. Document
language models, query models, and risk minimiza-
tion for information retrieval. In Proceedings of the
24th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 111?119.
Victor Lavrenko and W. Bruce Croft. 2001. Rele-
vance based language models. In Proceedings of the
24th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 120?127.
John Lehmann, Sean Monahan, Luke Nezda, Arnold
Jung, and Ying Shi. 2010. Lcc approaches to knowl-
edge base population at tac 2010. In Proceedings TAC
2010 Workshop. TAC 2010.
Yuanhua Lv and ChengXiang Zhai. 2009. A compar-
ative study of methods for estimating query language
models with pseudo feedback. In Proceeding of the
18th ACM Conference on Information and Knowledge
Management, pages 1895?1898.
Yuanhua Lv and ChengXiang Zhai. 2010. Positional rel-
evance model for pseudo-relevance feedback. In Pro-
ceeding of the 33rd Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 579?586.
Paul McNamee and Hoa Trang Dang. 2009. Overview
of the TAC 2009 knowledge base population track. In
Proceedings of the Second Text Analysis Conference.
ChengXiang Zhai and John Lafferty. 2001. Model-based
feedback in the language modeling approach to infor-
mation retrieval. In Proceedings of the 10th Inter-
national Conference on Information and Knowledge
Management, pages 403?410.
Chengxiang Zhai and John Lafferty. 2004. A study of
smoothing methods for language models applied to in-
formation retrieval. ACM Transactions on Information
Systems, 22(2):179?214, April.
Wei Zhang, Jian Su, Chew Lim Tan, andWen TingWang.
2010. Entity linking leveraging automatically gener-
ated annotation. In Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics (Col-
ing 2010), pages 1290?1298.
Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xiaoyan
Zhu. 2010. Learning to link entities with knowledge
base. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
483?491.
813
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1858?1868,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Learning Topics and Positions from Debatepedia
Swapna Gottipati? Minghui Qiu? Yanchuan Sim? Jing Jiang? Noah A. Smith?
?School of Information Systems, Singapore Management University, Singapore
?Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?{swapnag.2010,minghui.qiu.2010,jingjiang}@smu.edu.sg
?{ysim,nasmith}@cs.cmu.edu
Abstract
We explore Debatepedia, a community-
authored encyclopedia of sociopolitical de-
bates, as evidence for inferring a low-
dimensional, human-interpretable representa-
tion in the domain of issues and positions. We
introduce a generative model positing latent
topics and cross-cutting positions that gives
special treatment to person mentions and opin-
ion words. We evaluate the resulting repre-
sentation?s usefulness in attaching opinionated
documents to arguments and its consistency
with human judgments about positions.
1 Introduction
The social web has evolved into a forum for large
portions of the population to discuss and debate
complex issues of societal importance. Websites like
Debatepedia,1 an online, community-authored ency-
clopedia of debates (?2), seek to organize some of
this exchange into structured information resources
that summarize arguments and link externally to
texts (editorials, blog posts, etc.) that express and
evoke them. Empirical NLP, we propose, has a
role to play in creating a more compact and easily-
interpretable way to understand the opinion space.
In particular, we envision applications to computa-
tional journalism, where there is high demand for
transformation of and pattern discovery in unman-
ageable, unstructured, evolving data (including text)
to inform the public (Cohen et al, 2011).
In this paper, we develop a generative model for
discovering such a representation (?3), using De-
batepedia as a corpus of evidence. We draw in-
spiration from Lin et al (2008) and Ahmed and
1http://dbp.idebate.org
Xing (2010), who used generative models to infer
topics?distributions over words?and other word-
associated variables representing perspectives or
ideologies. We view topics as lexicons, and propose
that grounding a topic model with evidence beyond
bags of words can lead to more lexicon-like repre-
sentations. Specifically, our generative topic model
grounds topics using the hierarchical organization
of arguments within Debatepedia. Further, we use
named entity recognition as a preprocessing step, an
existing sentiment lexicon to construct an informed
prior, and we incorporate a latent, discrete position
variable that cuts across debates.2
We evaluate the model informally and formally
(?4). Subjectively, the model identifies reasonable
topic and perspective terms, and it associates topics
sensibly with important public figures. In quanti-
tative evaluations, we find the model?s representa-
tion superior to topics from vanilla latent Dirichlet
allocation (Blei et al, 2003) and the joint sentiment
topic model (Lin and He, 2009) in matching external
texts to debates. Further, the position variables can
be used to infer the side of an argument within a de-
bate; our model performs with an accuracy of 86%
on position prediction of the debate argument. The
cross-cutting position variable is not especially con-
sistent with human judgments, suggesting that fur-
ther knowledge sources may be required to improve
interpretability across issues.
2 Data
Debatepedia, like Wikipedia, is constructed by vol-
unteer contributors and has a system of community
2This variable might serve to cluster debate sides according
to ?abstract beliefs commonly shared by a group of people,?
sometimes called ideologies (Van Dijk, 1998). We do not claim
that our model infers ideologies (see ?4).
1858
Debate: Gun control; should laws be passed to limit gun ownership further?
Question: Self-defense ? Is self-defense a good reason for gun ownership?
Side: Yes Side: No
Argument: A citizen has a ?right? to guns as a means
to self-defense: Many groups argue that a citizen
should have the ?right? to defend themselves, and that
a gun is frequently the . . .
Argument: The protection of property is not a good
justification for yielding a lethal weapon. While peo-
ple have a right to their property, this should not justify
wielding a lethal . . .
Argument: Gun restrictions and bans disadvantage cit-
izens against armed criminals. Citizens that are not al-
lowed to carry guns are disadvantaged against lawless
criminals that . . .
Argument: Robert F. Drinan, Former Democratic US
Congressman, ?Gun Control: The Good Outweighs
the Evil?, 1976 ? ?These graphic examples of individ-
ual instances of . . .
Question: Economic benefits ? Is gun control economically beneficial?
Side: Yes Side: No
Argument: Lax gun control laws are economically
costly. The Coalition for Gun Control claims that, ?in
Canada, the costs of firearms death and injury alone
have been estimated at . . .
Argument: Gun sports have economic benefits. Field
sports bring money into poor rural economies and pro-
vide a motivation for landowners to value environmen-
tal protection.
Table 1: An example of a Debatepedia debate on the topic ?Gun control.?
moderation. Many of the debate issues covered are
controversial and salient in current public discourse.
Because it is primarily expressed as text, Debatepe-
dia is a corpus of debate topics, but it is organized
hierarchically, with multiple issues in each debate
topic, questions within each issue, and arguments on
two sides of each question. An important feature of
the corpus is the widespread quotation and linking to
external articles on the web, including news stories,
blog postings, wiki pages, and social media forums;
here we use these external articles in evaluation (?4).
Table 1 shows excerpts from a debate page3 from
Debatepedia. Each debate contains ?questions,?
which reflect the different aspects of a debate. In this
particular debate, there are 13 questions (2 shown),
ranging from economic benefits to enforceability to
social impacts. For each question, there are two dis-
tinct sides, each with its own set of supporting argu-
ments. Many of these arguments also contains links
to online articles where the quotes are extracted from
(not shown in Table 1). For example, in the second
argument on the ?No? side, there is an inline link to
the article written by Congressman Drinan.4
Within a debate topic, the sides cut across differ-
ent questions, aligning arguments together. In gen-
3http://dbp.idebate.org/en/index.php/
Debate:_Gun_control
4http://www.saf.org/LawReviews/Drinan1.
html
Debates 1,303
Arguments 33,556
Articles linked by exactly one argument 3,352
Tokens 1,710,814
Types (excluding NE mentions) 59,601
Person named entity mentions 9,496
Table 2: Debatepedia corpus statistics. Types and tokens
include unigrams, bigrams and person named entities.
eral, the questions are phrased so that a consistent
?pro? and ?con? structure is apparent throughout
each debate, aligned to a high-level question (i.e.,
the ?Yes? sides of all the questions are consistent
with the same side of the larger debate). The ex-
ample of Table 1 deviates from this pattern, with the
self-defense ?Yes? arguing ?no? to the high-level de-
bate question?Should laws be passed to limit gun
ownership further??and the economic ?Yes? argu-
ing ?yes? to the high-level question.
Table 2 presents statistics of our corpus.
2.1 Preprocessing
We scraped the Debatepedia website and extracted
the debate, question, argument, and side structure
of the debate topics. We crawled the external
web articles that were linked from the Debatepe-
dia arguments. For the web articles, we extracted
the main text content (ignoring boilerplate elements
such as navigation and advertisments) using Boil-
1859
erpipe (Kohlschu?tter et al, 2010).5 We tokenized
the text and filtered stopwords.6 We considered both
unigrams and bigrams in our model, keeping all uni-
grams and removing bigram types that appeared less
than 5 times in the corpus. Although our modeling
approach ultimately treats texts as bags of terms (un-
igrams and bigrams), one important preprocessing
step was taken to further improve the interpretabil-
ity of the inferred representation: named entity men-
tions of persons. We identified these mentions of
persons using Stanford NER (Finkel et al, 2005)
and treated each person mention as a single token. In
our qualitative analysis of the model (?4.2), we will
show how this special treatment of person mentions
enables the association of well-known individuals
with debate topics. Though not part of our exper-
imental evaluation in this paper, such associations
are, we believe, an interesting direction for future
applications of the model.
3 Model
Our model defines a probability distribution over
terms7 that are observed in the corpus. Each term
occurs in a context defined by the tuple ?d, q, s, a?
(respectively, a debate, a question within the debate,
a side within the debate, and an argument). At each
level of the hierarchy is a different latent variable:
? Each question q within debate d is associated
with a distribution over topics, denoted ?d,q.8
? Each side s of the debate d is associated with a
position, denoted id,s and we posit a global dis-
tribution ? that cuts across different questions
and arguments. In our experiments, there are
two positions, and the two sides of a debate
are constrained to associate with opposing po-
sitions. As illustrated by Table 1, this assump-
5http://code.google.com/p/boilerpipe
6www.ranks.nl/resources/stopwords.html
7Recall that our model includes bigrams. We treat each un-
igram and bigram token (after filtering discussed in ?2.1) as a
separate term.
8In future work, more sharing across questions within a
debate, or more differentiation among the topic distributions
for arguments under a question, might be explored. Wallach
(2006) describes suitable techniques using hierarchical Dirich-
let draws, and Eisenstein et al (2011) suggests the use of sparse
shocks to log-odds at different levels. Here we work on the
assumption that Debatepedia?s questions are the most topically
coherent level, and work with a single topic mixture at this level.
wz y
Nd,q,s,a
Ad,q,s
?
?
Qd
?
?
?tt?
o
i,t?ii ?et
K TKT
?b
?i ?o ?t ?e
?b
i
?
?
Sd
D
Figure 1: Plate diagram. K is the number of positions,
and T is number of topics. The shaded variables are ob-
served and dashed variables are marginalized. ?,?,?
and all ? are fixed hyperparameters (?3.1).
tion is not always correct, though it tends to
hold most of the time.
? Each term wd,q,s,a,n (n is the position index
of the term within an argument) is associated
with one of five functional term types, denoted
yd,q,s,a,n. This variable is latent, except when it
takes the value ?entity? (e) for terms marked as
named entity mentions. When it is not an en-
tity, it takes one of the other four values: ?gen-
eral position? (i), ?topic-specific position? (o),
?topic? (t), or ?background? (b). Thus, every
term w is drawn from one of these 5 types of
bags, and y acts as a switching variable to se-
lect the type of bag.
? For some term types (the ones where y ?
{o, t}), each term wd,q,s,a,n is associated with
one of T discrete topics, as indexed by
zd,q,s,a,n.
Figure 1 illustrates the plate diagram for the
graphical model underlying our approach. The gen-
erative story is given in Figure 2.
3.1 Priors
Typical probabilistic topic models assume a sym-
metric Dirichlet prior over its term distributions or
1860
1. ? topics t, draw topic-term distribution ?tt ? Dirichlet(?t) and topic-entity distribution ?et ? Dirichlet(?e).
2. ? positions i, draw position-term distribution ?ii ? Dirichlet(?i).
3. ? topics t, ? positions i, draw topic-position term distribution ?oi,t ? Dirichlet(?o).
4. Draw background term distribution ?b ? Dirichlet(?b).
5. Draw functional term type distribution ? ? Dirichlet(?).
6. Draw position distribution ? ? Dirichlet(?).
7. ? debates d:
a. Draw id,1, id,2 ? Multinomial(?), assigning each of the two sides to a position.
b. ? questions q in d:
i. Draw topic mixture proportions ?d,q ? Dirichlet(?).
ii. ? arguments a under question q and term positions n in a:
A. Draw topic label zd,q,s,a ? Multinomial(?d,q).
B. Draw functional term type yd,q,s,a ? Multinomial(?).
C. Draw term wd,q,s,a ? Multinomial (?yd,q,s,a | id,1, id,2, zd,q,s,a).
Figure 2: Generative story for our model of Debatepedia.
apply empirical Bayesian techniques to estimate the
hyperparameters. Motivated by past efforts to ex-
ploit prior knowledge (Zhao et al, 2010; Lin and
He, 2009), we use the OpinionFinder sentiment lex-
icon9 (Wilson et al, 2005) to construct ?i and ?o.
Specifically, terms w in the lexicon were given pa-
rameters ?iw = ?ow = 0.01, and other terms were
given ?iw = ?ow = 0.001, capturing our prior belief
that opinion-expressing terms are likely to be used
in expressing positions. 5,451 types were given a
?boost? through this prior.
Information retrieval has long exploited the ob-
servation that a term?s document frequency (i.e., the
number of documents a term occurs in) is inversely
related its usefulness in retrieval (Jones, 1972). We
encode this in ?b, the prior over the background
term distribution, by setting each value to the log-
arithm of the term?s argument frequency.
The other priors were set to be symmetric: ?e =
0.01 (entity topics), ?t = 0.001 (topics), ? =
50/T = 1.25 (topic mixture coefficients), ? = 0.01
(positions), and ? = 0.01 (functional term types).
Preliminary tests showed that final topics are rela-
tively insensitive to the values of the hyperparame-
ters.
3.2 Inference and Parameter Estimation
Exact inference under this model, like most latent-
variable topic models, is intractable. We apply col-
lapsed Gibbs sampling, a standard approach for such
9http://mpqa.cs.pitt.edu/lexicons/subj_
lexicon/
models (Griffiths and Steyvers, 2004).10 The no-
table deviations from typical uses of collapsed Gibbs
sampling are: (i) we jointly sample id,1 and id,2 to
respect the constraint that they differ; and (ii) we
fix the priors, in some cases to be asymmetric, as
discussed in ?3.1. We perform Gibbs sampling for
2,000 iterations over the dataset, discarding the first
500 iterations for burn-in, and averaging over every
10th iteration thereafter to get estimates for our term
distributions.
3.3 T andK
In all experiments, we use T = 40 topics andK = 2
positions. We did not extensively explore different
values for T and K; preliminary exploration sug-
gested that interpretability, gauged informally by the
authors, degraded for higher values of either.
4 Evaluation
Recall that the aim of this work is to infer a low-
dimensional representation of debate text. We esti-
mated our model on the Debatepedia debates (not in-
cluding hyperlinked articles), and conducted several
evaluations of the model, each considering a differ-
ent aspect of the goal. We exploit external articles
hyperlinked from Debatepedia described in ?2 as
supporting texts for arguments, treating each one?s
association to an argument as variable to be pre-
dicted. Firstly, we evaluate our model on the article
associating task. Secondly, we evaluate our model
on the position prediction task. Then, we compare
10Because this technique is well known in NLP, details are
relegated to supplementary material.
1861
 0
 200
 400
 600
 800
 1000
 1200
 1400
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
N
o 
of
 A
rti
cle
s
JS Divergence
LDA
JST
Our Model
Figure 3: The distribution over Jensen-Shannon diver-
gences between a hyperlinked article and the correspond-
ing Debatepedia argument, n = 3, 352.
our model?s positional assignment of arguments to
human annotated clusterings. Finally, we present
qualitative discussion.
4.1 Quantitative Evaluation
4.1.1 Topics
As described in ?2, our corpus includes 3,352 ar-
ticles hyperlinked by Debatepedia arguments.11 Our
model can be used to infer the posterior over top-
ics associated with such an article, and we compare
that distribution to that of the Debatepedia article
that links to it. Calculating the similarity of these
distributions, we get an estimate of how closely our
model can associate text related to a debate with the
specific argument that linked to it. We compare with
LDA (Blei et al, 2003), which ignores sentiment,
and the joint sentiment topic (JST) model (Lin and
He, 2009), an unsupervised model that jointly cap-
tures sentiment and topic.12 Using Jensen-Shannon
divergence, we find that our approach embeds these
pairs significantly closer than LDA and JST (also
trained with 40 topics), under a Wilcoxon signed
rank test (p < 0.001). Figure 3 shows the histogram
of divergences between our model, JST, and LDA.
Associating external articles. More challenging,
of course, is selecting the argument to which an
external article should be associated. We used the
Jensen-Shannon divergence between topic distribu-
tions of articles and arguments to rank the latter,
for each article. The mean reciprocal rank scores
(Voorhees, 1999) for LDA, JST, and our model were
11We consider only those articles linked by a single Debate-
pedia argument.
12JST multiplies topics out by the set of sentiment labels, as-
signing each token to both a topic and a sentment. We use the
OpinionFinder lexicon in JST?s prior in the same way it is used
in our model.
 0.08
 0.1
 0.12
 0.14
 0.16
 0.18
5 10 15 20 25 inf
M
R
R
K
LDA
JST
Our Model
Figure 4: Mean reciprocal ranks for the association task.
0.1272, 0.1421, and 0.1507, respectively; the differ-
ence is significant (Wilcoxon signed rank test, p <
0.001). We found the same pattern for MRR@k,
k ? {5, 10, 15, 20, 25,?}, as shown in Figure 4.
It is likely possible to engineer more accurate
models for attaching articles to arguments, but the
attachment task is our aim only insofar as it con-
tributes to an overall assessment of an inferred rep-
resentation?s quality.
4.1.2 Positions
Positional distance by topic. We next consider
the JS divergences of position term distributions by
topic; for each topic t, we consider the divergence
between inferred values for ?o1,t and ?o2,t. Figure 5
shows these measurements sorted from most to least
different; these might be taken as evidence for which
issue areas? arguments are more lexically distin-
guishable by side, perhaps indicating less common
ground in discourse or (more speculatively) greater
controversy. For example, our model suggests that
debates relating to topics like presidential politics,
foreign policy, teachers, women?s health, religion,
and Israel/Palestine are more heated (within the De-
batepedia community at the time the debates took
place) than those about the minimum wage, Iran as
a nuclear threat, or immigration.
Predicting positions for arguments. We tested
our model?s ability to infer the positions of argu-
ments. In this experiment (only), we held out 3,000
arguments during parameter estimation. The held-
out arguments were selected so that every debate
side maintained at least one argument whose in-
ferred side could serve as the correct answer for the
held-out argument. We then inferred i for each held-
out argument from debate d and side s, given the
parameters, and compared it with the value of id,s
inferred during parameter estimation. The model
achieved 86% accuracy (Table 3 shows the confu-
1862
sion matrix). Note that JST does not provide a base-
line for comparison, since it does not capture debate
sides.
i = 1 i = 2
i? = 1 1,272 216
i? = 2 199 1,313
Table 3: Confusion matrix for position prediction on
held-out arguments.
Predicting positions for external articles. We
can also use the model to predict the position
adopted in an external text. For articles linked from
within Debatepedia, we have a gold standard: from
which side of a debate was it linked? After using
the model to infer a position variable for such a text,
we can check whether the inferred position variable
matches that of the argument that links to it. Table 4
shows that our model does not successfully com-
plete this task, assigning about 60% of both kinds
of articles i = 1.
i = 1 i = 2
i? = 1 1,042 623
i? = 2 1,043 644
Table 4: Confusion matrix for position prediction on hy-
perlinked articles.
Genre. We manually labeled 500 of these articles
into six genre categories. We had two annotators for
this task (Cohen?s ? = 0.856). These categories,
in increasing order of average Jensen-Shannon di-
vergence, are: blogs, editorials, wiki pages, news,
other, and government. Figure 6 shows the results.
While the only difference between the first and last
groups are surprising by chance, we are encouraged
by our model?s suggestion that blogs and editori-
als may be more ?Debatepedia argument-like? than
news and government articles.
Note that our model is learned only from text
within Debatepedia; it does not observe the text of
external linked articles. Future work might incorpo-
rate this text as additional evidence in order to cap-
ture effects on language stemming from the interac-
tion of position and genre.
0.1 0.2 0.3 0.4 0.5comment, minimum, wage, poverty, capitalism
nuclear, weapons, iran, states, threatparty, vote, republican, political, voters
energy, gas, power, fuel, windtax, economic, trade, cost, percent
immigration, cameras, police, immigrants, crimepeople, dont, time, lot, make
food, consumers, products, calorie, informationdeath, crime, punishment, penalty, justice
marijuana, drug, drugs, alcohol, agemarriage, gay, mars, space, moon
rights, law, people, individual, amendmentsouth, kosovo, independence, state, republic
human, rights, animals, life, animalchildren, child, sex, parents, sexual
school, schools, students, education, publicchina, tibet, chinese, people, tibetan
global, emissions, climate, carbon, warminginternational, court, war, crimes, icc
english, language, violence, people, videoorleans, euthanasia, city, suicide, priests
speech, corporations, corporate, public, moneyhealth, care, insurance, public, private
circumcision, men, sexual, circumcised, foreskininformation, torture, science, evidence, wikipedia
companies, market, industry, business, bailoutlaw, workers, union, rights, legal
college, cloning, game, football, incesttimes, york, ban, june, january
countries, eu, european, international, statesoil, water, production, ethanol, environmental
military, war, iraq, forces, marcheconomy, financial, spending, economic, government
government, social, governments, state, programsisrael, gaza, hamas, israeli, palestinian
women, religious, abortion, god, lifeteachers, pay, test, left, merit
peace, state, west, united, actionunited, states, president, administration, foreign
president, washington, obama, american, america
Figure 5: Jensen-Shannon divergences between topic-
specific positional term distributions, for each topic. Top-
ics are labeled by their most frequent terms from ?t.
4.1.3 Comparison to Human Judgments of
Positions
We compared our model?s inferred positions to
human judgments. For each of the 11 topics in Ta-
ble 8, we selected two associated debates with more
arguments than average (24.99). The debates were
provided to each of three human annotators,13 who
13All were native English-speaking American graduate stu-
dents not otherwise involved in this research. Each is known
by the authors to have basic literacy with issues and debates in
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
blog(12) edit(14) wiki(11) news(33) other(18) gov(12)
JS
 D
ive
rge
nc
e S
co
re
Article type(% of articles)
Figure 6: Position prediction on 500 hyperlinked articles
by genre.
1863
?Israel-Palestine? ?Same-sex marriage? ?Drugs? ?Healthcare? ?Death penalty? ?Abortion?
i1
pre emptive same sex hands free single payer anti death pro choice
israeli palestinian long term performance enhancing so called non violent pro life
open and shut second class in depth self sustaining african american non muslim
i2
two state opposite sex long term government run semi automatic would be
long term well intentioned high speed government approved high profile full time
self destructive day time short term high risk hate crime late term
a. Our model: topic-specific position bigrams associated with six selected topics.
?
war large illegal support death power
assault possibility abuse force penalty limit
disproportionate problems high threat murder civil
+
peace civil disease care power care
independence rights nature universal clean suicide
self-determination affirmative potential uninsured waste death
b. JST: sentiments associated with six selected topics manually aligned to our model?s topics.
Table 6: Terms associated with selected topics. The labels and alignments between the two models? topics were
assigned manually. (a.) Our model: topic-specific position bigrams which are ranked by comparing the log odds
conditioned on the position and topic: log ?oi1,t,w? log ?oi2,t,w. We show the top three terms for each position (b.) JST:we show the top three terms for each sentiment (negative and positive).
A1 (11) A2 (5) A3 (16)
Model (2) 3.21 2.58 3.45
A1 (11) 2.15 2.15
A2 (5) 2.63
Table 5: Variation of information scores for each pairing
of annotators and model.
were instructed to group the 44 sides of the debates.
The instructions stated:
Our goal is to see what you think about how
the different sides of different debates can be
lined up. You might find it convenient to
think of these in terms of political philoso-
phies, contemporary political party platforms,
or something else. Any of these is fine; we
want you to tell us the grouping you find most
reasonable.
All three annotators (hereafter denoted A1, A2, and
A3) used fairly involved labeling schemes; the an-
notators used 37, 30, and 16 unique labels, respec-
tively.14 A1 used keyword lists to label items; we
coarsened his labels manually by removing or merg-
ing less common keywords (resulting in: Republi-
can, Democrat, science/environment, nanny, politi-
cal reform, fiscal liberal, fiscal conservative, liber-
tarian, Israel, Palestine, and one unlabeled side).
A2 provided a coarse annotation along with each
American politics.
14In a small number of cases, an annotator declined to label
a side. Each unlabeled item received its own cluster.
fine-grained one (liberal, conservative, ?, and two
unlabeled sides). We used 100 samples from our
Gibbs sampler to estimate posteriors for each id,s;
these were always 99% or more in agreement, so we
mapped each debate side into its single most proba-
ble cluster. Recall that the two sides of each debate
must be in different clusters.
Table 5 shows the variation of information mea-
sure (Meila, 2003) for each pairing among the three
annotators and our model. The model agrees with
A2?s coarse clustering most closely, and in fact is
closer to A2?s clustering than A2 is to A3?s; it also
agrees with A2?s coarse clustering better than A2?s
coarse and fine clusterings agree (3.36, not shown
in the table). This is promising, but we do not
have confidence that the positional dimension is be-
ing captured especially well in this model; for those
debate-sides labeled liberal or conservative by A2,
the best match of our two positions was still only in
agreement only about 60% of the time, and agree-
ment with each human annotator is within the inter-
val of what would be expected if each debate?s sides
were assigned uniformly at random to positions.15
Remarks. Within debates and within topics, the
model uses the position variable to distinguish sides
well. For external text, the model performs well
on articles such as blogs and editorials but on oth-
ers the positional categories do not seem meaning-
15This was determined using a Monte Carlo simulation with
1,000 samples.
1864
Topic i = 1 i = 2
None (?i) vice president, c sections, twenty four, cross pressures,
pre dates, anti ballistic, cost effectiveness, anti land-
mine, court appointed, child poverty
cross examination, under runs, hand outs, half million,
non christians, break down, counter argument, seventy
five, co workers, run up
?Israel-
Palestine?
pre emptive, israeli palestinian, open and shut, first
time, hamas controlled, democratically elected
two state, long term, self destructive, secretary general,
right wing, all out, near daily, short term
?Same-sex
marriage?
same sex, long term, second class, blankenhorn rauch,
wrong headed, self denial, left handed
opposite sex, well intentioned, day time, planet wide,
day night, child rearing, low earth, one way, one third
?Drugs? hands free, performance enhancing, in depth, hand
held, best kept, non pharmaceutical, anti marijuana
long term, high speed, short term, peer reviewed, alco-
hol related, mind altering, inner city, long lasting
?Healthcare? single payer, so called, self sustaining, public private,
for profit, long run, high cost, multi payer
government run, government approved, high risk, two
tier, government appointed, low cost, set up
?Death
penalty?
anti death, non violent, african american, self help, cut
and cover, heavy handed, dp equivalent
semi automatic, high profile, hate crime, assault
weapons, military style, high dollar, self protective
?Abortion? pro choice, pro life, non muslim, well educated, anti
abortion, much needed, church state, birth control
would be, full time, late term, judeo christian, life
style, day to day, non christian, child bearing
Table 7: General position (first row) and topic-specific position bigrams associated with six selected topics.
Topic Terms Person entity mentions
?Israel-
Palestine?
israel, gaza, hamas, israeli, pales-
tinian
Benjamin Netanyahu, Al Jazeera, Mavi Marmara, Nicholas Kristoff,
Steven R. David
?Same-sex
marriage?
marriage, gay, mars, space, moon Buzz Aldrin, Andrew Sullivan, Moon Base, Scott Bidstrup, Ted Olson
?Drugs? marijuana, drug, drugs, alcohol, age Four Loko, Evo Morales, Toni Meyer, Sean Flynn, Robert Hahn
?Healthcare? health, care, insurance, public, pri-
vate
Kent Conrad, Paul Hsieh, Paul Krugman, Ezra Klein, Jacob Hacker
?Death
penalty?
death, crime, punishment, penalty,
justice
Adam Bedau, Thomas R. Eddlem, Jeff Jacoby, John Baer, Peter Bronson
?Abortion? women, religious, abortion, god, life Ronald Reagan, John Paul II, Sara Malkani, Mother Teresa, Marcella
Alsan
Table 8: For 6 selected topics (labels assigned manually), top terms (?t) and person entities (?e). Bigrams were
included but did not rank in the top five for these topics. The model has conflated debates relating to same-sex
marriage with the space program.
ful, perhaps due to the less argumentative nature
of other kinds of articles. Noting the vast litera-
ture focusing on ideological positions expressed in
text, we believe this failure suggests (i) that broad-
based positions that hold across many topics may
require richer textual representations (see, e.g., the
?syntactic priming? of Greene and Resnik, 2009),
or (ii) that an alternative representation of positions,
such as the spatial models favored by political sci-
entists (Poole and Rosenthal, 1991), may be more
discoverable. Aside from those issues, a stronger
theory of positions may be required. Such a the-
ory could be encoded in a more informative prior or
weaker independence assumptions across debates.
Finally, exploiting explicitly ideological texts along-
side the moderated arguments of Debatepedia might
also help to identify textual associations with gen-
eral positions (Sim et al, 2013). We leave these di-
rections to future work.
4.2 Qualitative Analysis
Of the T = 40 topics our model inferred, we subjec-
tively judged 37 to be coherent; a glimpse of each is
given in Figure 5. We manually selected six of the
most interpretable topics for further evaluation.
As a generative modeling approach, our model
was designed for the purpose of reducing the dimen-
sionality of the sociopolitical debate space, as evi-
denced by Debatepedia. It is like other topic models
in this regard, but we believe that some effects of our
design choices are noteworthy. Table 6 compares the
positional bigrams of our model to the sentiments in-
ferred by JST. We observe the benefit of our model
in identifying terms associated with positions on so-
cial issues, while JST selects more general sentiment
terms.
1865
Table 7 shows bigrams most strongly associated
with general position distributions ?i and selected
topic-position distributions ?o.16 We see the poten-
tial benefit of multiword expressions. Although we
have used frequent bigrams as a poor man?s approx-
imation to multiword expression analysis, we find
the topic-specific positions terms to be subjectively
evocative. While somewhat internally coherent, we
do not observe consistent alignment across topics,
and the general distributions ?i are not suggestive.
The separation of personal name mentions into
their own distributions, shown for some topics in
Table 8, gives a distinctive characterization of top-
ics based on relevant personalities. Subjectively, the
top individuals are relevant to the subject matter as-
sociated with each topic (though the topics are not
always pure; same-sex marriage and the space pro-
gram are merged, for example).
5 Related Work
Insofar as debates are subjective, our study is related
to opinion mining. Subjective text classification
(Wiebe and Riloff, 2005) leads to opinion mining
tasks such as opinion extraction (Dave et al, 2003),
positive and negative polarity classification (Pang et
al., 2002), sentiment target detection (Hu and Liu,
2004; Ganapathibhotla and Liu, 2008), and feature-
opinion extraction (Wu et al, 2009). The above
studies are conducted mostly on product reviews, a
domain with a simpler opinion landscape and more
concrete rationales for those opinions, compared to
sociopolitical debates.
Generative topic models have been successfully
implemented in opinion mining tasks such as feature
identification (Titov and McDonald, 2008), entity-
topic extraction (Newman et al, 2006), mining con-
tentious expressions and interactions (Mukherjee
and Liu, 2012) and specific aspect-opinion word ex-
traction from labeled data (Zhao et al, 2010). Most
relevant to this research is work on feature-sentiment
extraction (Lin and He, 2009; Mei et al, 2007). Mei
et al (2007) built on PLSI, which is problematic
for generalizing beyond the training sample. The
JST model of Lin and He (2009) is an LDA-based
topic model in which each word token is assigned
both a sentiment and a topic; they exploited a sen-
16For more topics, please refer to the supplementary notes.
timent lexicon in the prior distribution. Our model
is closely related, but introduces a switching vari-
able that assigns some tokens to positions, some to
topics, and some to both. Unlike Lin and He?s senti-
ments, our model?s positions are associated with the
two sides of a debate, and we incorporate topics at
the level of questions within debates.
Some studies have specifically analyzed con-
trastive viewpoints or stances in general discussion
text.Agrawal et al (2003) used graph mining based
method to classify authors in to opposite camps for
a given topic. Paul et al (2010) developed an unsu-
pervised method for summarizing contrastive opin-
ions from customer reviews. Abu-Jbara et al (2012)
and Dasigi et al (2012) developed techniques to ad-
dress the problem of automatically detecting sub-
groups of people holding similar stances in a dis-
cussion thread.
Several prior studies have considered debates.
Cabrio and Villata (2012) developed a system based
on argumentation theory which recognizes the en-
tailment and contradiction relationships between
two texts. Awadallah et al (2011) used a debate
corpus as a seed for extracting person-opinion-topic
tuples from news and other web documents and in
later work classified the quotations to specific top-
ics and polarity using language models (Awadal-
lah et al, 2012). Somasundaran and Wiebe (2009)
and Anand et al (2011) were interested in ideolog-
ical content in debates, relying on discourse struc-
ture and leveraging sentiment lexicons to recognize
stances.
Closer to the methodology we describe, Lin et
al. (2008) presented a statistical model for politi-
cal discourse that incorporates both topics and ide-
ologies; they used debates on the Israeli-Palestinian
conflict. Fortuna et al (2009) showed that it is pos-
sible to isolate a subset of terms from media content
that are informative of a news organization?s bias to-
wards a particular issue. Ahmed and Xing (2010) in-
troduced multi-level latent Dirichlet alocation, and
Eisenstein et al (2011) introduced sparse additive
generative models, both conceived as extensions to
well-established probabilistic modeling techniques
(Blei et al, 2003); these were applied to debates
and political blog datasets. Our approach builds on
these models (especially the switching variables of
Ahmed and Xing). We go farther in jointly modeling
1866
text across many debates evidenced by the structure
of Debatepedia, thus grounding our models more
solidly in familiar sociopolitical issues, and in mak-
ing extensive use of existing NLP resources.
6 Conclusion
Using text from Debatepedia, we inferred topics and
position term lexicons in the domain of sociopoliti-
cal debates. Our approach brings together tools from
information extraction and sentiment analysis into a
latent-variable topic model and exploits the hierar-
chical structure of the dataset. Our qualitative and
quantitative evaluations show the model?s strengths
and weaknesses.
Acknowledgments
The authors thank several anonymous reviewers,
Justin Gross, David Kaufer, and members of the
ARK group at CMU for helpful feedback on this
work and gratefully acknowledge the assistance of
the annotators. This research is supported by the
Singapore National Research Foundation under its
International Research Centre@Singapore Funding
Initiative and administered by the IDM Programme
Office, by an A?STAR fellowship to Y.S., and by
Google?s support of the Reading is Believing project
at CMU.
References
Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi, and
Dragomir Radev. 2012. Subgroup detection in ide-
ological discussions. In Proceedings of ACL.
Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social behavior. In WWW
?03.
Amr Ahmed and Eric P. Xing. 2010. Staying in-
formed: supervised and semi-supervised multi-view
topical analysis of ideological perspective. In Pro-
ceedings of EMNLP.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E. Fox
Tree, Robeson Bowmani, and Michael Minor. 2011.
Cats rule and dogs drool!: classifying stance in online
debate. In Proceedings of the Second Workshop on
Computational Approaches to Subjectivity and Senti-
ment Analysis.
Rawia Awadallah, Maya Ramanath, and Gerhard
Weikum. 2011. OpinioNetIt: Understanding the
opinions-people network for politically controversial
topics. In Proceedings of CIKM.
Rawia Awadallah, Maya Ramanath, and Gerhard
Weikum. 2012. PolariCQ: Polarity classification of
political quotations. In Proceedings of CIKM.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Elena Cabrio and Serena Villata. 2012. Combining
textual entailment and argumentation theory for sup-
porting online debates interactions. In Proceedings of
ACL.
Sarah Cohen, James T. Hamilton, and Fred Turner. 2011.
Computational journalism. Communications of the
ACM, 54(10):66?71.
Pradeep Dasigi, Weiwei Guo, and Mona Diab. 2012.
Genre independent subgroup detection in online dis-
cussion threads: a pilot study of implicit attitude using
latent textual semantics. In Proceedings of ACL.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In Pro-
ceedings of WWW.
Jacob Eisenstein, Amr Ahmed, and Eric P Xing. 2011.
Sparse additive generative models of text. In Proceed-
ings of ICML.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of ACL.
Blaz Fortuna, Carolina Galleguillos, and Nello Cristian-
ini. 2009. Detecting the bias in media with statis-
tical learning methods. In Ashok N . Srivastava and
Mehran Sahami, editors, Text Mining: Classification,
Clustering, and Applications, pages 27?50. Chapman
& Hall/CRC.
Murthy Ganapathibhotla and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings of
COLING.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment. In
Proceedings of HLT-NAACL.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(Suppl. 1):5228?5235.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of CIKM.
Karen Sparck Jones. 1972. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of documentation, 28(1):11?21.
Christian Kohlschu?tter, Peter Fankhauser, and Wolfgang
Nejdl. 2010. Boilerplate detection using shallow text
features. In Proceedings of WSDM.
1867
Chenghua Lin and Yulan He. 2009. Joint sentiment/topic
model for sentiment analysis. In Proceedings of
CIKM.
Wei-Hao Lin, Eric Xing, and Alexander Hauptmann.
2008. A joint topic and perspective model for ideo-
logical discourse. In Proceedings of ECML-PKDD.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In Proceed-
ings of WWW.
Marina Meila. 2003. Comparing clusterings by the vari-
ation of information. In Bernhard Scho?lkopf and Man-
fred K. Warmuth, editors, Learning Theory and Kernel
Machines, volume 2777 of Lecture Notes in Computer
Science, pages 173?187. Springer.
Arjun Mukherjee and Bing Liu. 2012. Mining con-
tentions from discussions and debates. In Proceedings
of KDD.
David Newman, Chaitanya Chemudugunta, and Padhraic
Smyth. 2006. Statistical entity-topic models. In Pro-
ceedings of KDD.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of EMNLP.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In Proceedings of EMNLP.
Keith Poole and Howard Rosenthal. 1991. Patterns of
congressional voting. American Journal of Political
Science, pages 118?178.
Yanchuan Sim, Brice Acree, Justin H. Gross, and
Noah A. Smith. 2013. Measuring ideological propor-
tions in political speeches. In Proceedings of EMNLP.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings of
ACL.
Ivan Titov and Ryan McDonald. 2008. Modeling online
reviews with multi-grain topic models. In Proceedings
of WWW.
Teun A. Van Dijk. 1998. Ideology: A Multidisciplinary
Approach. Sage Publications Limited.
Ellen M. Voorhees. 1999. The trec-8 question answering
track report. In Proceedings of TREC.
Hanna M. Wallach. 2006. Topic modeling: beyond bag-
of-words. In Proceedings of ICML.
Janyce Wiebe and Ellen Riloff. 2005. Creating sub-
jective and objective sentence classifiers from unan-
notated texts. In Proceedings of CICLing.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: a system for subjectivity analysis. In
Proceedings of HLT-EMNLP.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion mining.
In Proceedings of EMNLP.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaoming
Li. 2010. Jointly modeling aspects and opinions with
a maxent-lda hybrid. In Proceedings of EMNLP.
1868
