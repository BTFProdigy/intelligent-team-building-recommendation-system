Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1670?1681, Dublin, Ireland, August 23-29 2014.
Generating Supplementary Travel Guides from Social Media
Liu Yang
1,2
, Jing Jiang
2,?
, Lifu Huang
1,2
, Minghui Qiu
2
, Lizi Liao
2,3
1
Peking University / Beijing, China, 100871
2
Singapore Management University / Singapore, Singapore, 178902
3
Beijing Institute of Technology / Beijing, China, 100081
yang.liu@pku.edu.cn, jingjiang@smu.edu.sg
{warrior.fu, minghuiqiu, liaolizi.llz}@gmail.com
Abstract
In this paper we study how to summarize travel-related information in forum threads to gener-
ate supplementary travel guides. Such summaries presumably can provide additional and more
up-to-date information to tourists. Existing multi-document summarization methods have limita-
tions for this task because (1) they do not generate structured summaries but travel guides usually
follow a certain template, and (2) they do not put emphasis on named entities but travel guides
often recommend points of interest to travelers. To overcome these limitations, we propose to
use a latent variable model to align forum threads with the section structure of well-written travel
guides. The model also assigns section labels to named entities in forum threads. We then
propose to modify an ILP-based summarization method to generate section-specific summaries.
Evaluation on threads from Yahoo! Answers shows that our proposed method is able to generate
better summaries compared with a number of baselines based on ROUGE scores and coverage
of named entities.
1 Introduction
Online forums and community question answering (CQA) sites contain much useful information from
ordinary users, such as their personal experience, opinions, suggestions and recommendations. Extract-
ing and summarizing information from these rich information sources has a wide range of applications.
In this work, we study how to tap into user-generated content in forums such as Yahoo! Answers to
generate supplementary city travel guides. Travel guides published by well-known publishers such as
Lonely Planet are written by a small number of authors based on their travel experience. Presumably
if we could summarize the large amount of information given by ordinary users about a city, such a
summary could supplement the official travel guide and cover more up-to-date information.
However, social media content is diverse and noisy because it is contributed by many different au-
thors. Directly applying existing multi-document summarization methods to forum and CQA threads
may not produce good travel guides for the following reasons: (1) Summaries produced by standard
summarization methods are not structured, but travel guides usually follow a template structure. (2)
Travel guides put much emphasis on points of interest, which are usually location entities, but standard
text summarization methods are not entity-oriented.
To illustrate our points, in Table 1 we show (i) the overall structure of a travel guide for Sydney from
Lonely Planet, (ii) an excerpt from a summary generated by a state-of-the-art ILP-based summarization
method (Gillick and Favre, 2009) from a set of threads related to Sydney, and (iii) excerpts of a structured
summary generated by our proposed method. The comparison shows that the summary generated by
the standard ILP method mixes information on different topics together and does not mention many
* Corresponding author.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1670
Travel Guide from Lonely Planet (http://www.lonelyplanet.com/australia/sydney/)
Restaurants:
Sepia:There?s nothing washed out or brown-tinged about Sepia?s food: Martin Benn?s picture-perfect creations are presented in . . .
Icebergs Dining Room: Poised above the famous Icebergs swimming pool, Icebergs views sweep across the Bondi Beach arc to . . .
Shopping:
Strand Arcade: Constructed in 1891, the Strand rivals the QVB in the ornateness stakes. Three floors of designer fashions . . .
Westfield Sydney: The city?s newest shopping mall is a bafflingly large complex gobbling up Sydney Tower and a fair chunk of . . .
Transport:
Sydney Airport: Sydneys Kingsford Smith Airport , 10km south of the city centre, is Australias busiest airport, handling flights . . .
Water Taxis Combined:Fares based on up to four passengers; add $10 per person for additional passengers. Sample fares . . .
Yahoo! Answers Summary Generated by Standard ILP Method Yahoo! Answers Summary Generated by Our Method
It ?s not too far from Sydney . Sydney is the most expensive
place in Australia . They are a little lame ... Then you can go to
Darling Harbour, a beautiful habour which is a 10-minute walk
from town hall station . Make sure , if you are up to it to do the
bridge climb , this is a real treat . There are lots of interesting
things to see and do in and around Sydney . The suburbs-much
cheaper than the CBD. It was in the basement of a big shopping
mall . The only way to do that is to drive . Got to walk on top of
the Sydney harbour bridge and go up centre point tower ! Walk
around the street and see the beach . I would like to stay at a nice
hotel . My friend and I are wanting to take a trip to Sydney for
the summer . But you ?ll need to get there by taxi . Sydney is so
pretty, so you should be able to find stuff to do . And they have
many facilities . Good luck and have fun . Public transport is not
very good . Depending on what you ?re in Sydney to do it ?s hard
to say . . .
Restaurants:
Go to the two major restaurant areas close to the city Dar-
linghurst , along Oxford Street , and Newtown , along King
Street . Chinatown which is off George St. in the city look up
Dixon st. is a great place to get a cheap Chinese meal . . .
Shopping:
Queen Victoria Building and Pitt St Mall , World Square and
the Strand are good ideas to check out . Hair driers you can get
in many places , but the main places would be the department
stores such as Target , Big W , K-Mart , Myer, David Jones . . .
Transport:
The CBD is about 15 minutes by train from the airport and there
is a station at Circular Quay , right on the Harbour with access
to the bridge and the Opera House . You can catch an intercity
train with Cityrail from just about anywhere in Sydney . . .
Table 1: Comparison of different travel guides about Sydney. Top: excerpts from Lonely Planet. Bottom left: excerpt from
a summary generated by standard ILP. Bottom right: excerpts from summary generated by our method. Named entities are
highlighted in bold font.
interesting places to visit. The summary by our proposed method, in contrast, organizes the information
into sections and has a high coverage of places a tourist can visit.
To generate the kind of summaries as shown in the bottom right of Table 1, we propose to first leverage
the section structure of well-written travel guides and use a latent variable model to align forum threads
with the different sections from these travel guides. Moreover, observing that points of interest are orga-
nized by sections in these travel guides, we also identify location names from user-generated content and
try to uncover their underlying section labels. We then treat the remaining problem as a multi-document
summarization task. We modify an Integer Linear Programming (ILP)-based extractive summarization
framework (Gillick and Favre, 2009) to select sentences from forum threads to generate section-specific
summaries, where we specifically emphasize the inclusion of potential points of interest for each sec-
tion. Experiments using threads from Yahoo! Answers show that our proposed method generates better
summaries than a number of baselines in terms of ROUGE scores and coverage of named entities.
Our work makes the following contributions. First, we study a new problem of summarizing multiple
forum threads to generate city travel guides based on known template structure from well-written travel
guides. Second, we propose a principled approach based on latent variable models and Integer Linear
Programming. Third, we evaluate our method using real forum threads and human generated model
summaries, and the results are positive.
2 Overview of Our Method
Our task is to summarize travel-related information from forum threads for potential tourists. In order
to inject some structure into the generated summaries, we assume that we have a set of I well-written
travel guides that correspond to I different cities and have the same structure. We refer to these travel
guides as official travel guides. Each official travel guide consists of a fixed set of S sections such as
restaurants and shopping, and this section structure will be used to organize our generated summaries.
We further assume that each section of an official travel guide consists of a list of points of interest, each
with a name and a short description, as illustrated in Figure 1. We believe that this is a fairly common
structure followed by many if not all travel guides.
Given a target city, we assume that we can collect a set of threads about this city from travel-related
forums. In this paper we use threads from Yahoo! Answers, but our solution does not use any CQA
properties of the threads, so threads from other general forums can also be used. Our goal is to generate
a text summary with S sections from these threads, where each section has a length limit.
1671
As we have mentioned, we treat the problem as a multi-document summarization task. However,
different from standard text summarization, our generated summaries should contain S sections. To
achieve this goal, we first select a set of relevant threads for each section and then perform section-
specific summarization from the selected threads.
Thread selection: To select relevant threads given a section, a naive solution is to rank the threads based
on their relevance to the section, where relevance can be measured by, for example, cosine similarity
between a thread and all the text in the given travel guides belonging to the section. But we observe that
the language used in forum threads could be very different from that in the official travel guides, making
it hard to measure relevance purely based on lexical overlap. For example, in the entertainment section,
forum threads may contain words such as ?djs,? ?Xmas,? ?b?day? and ?anni.?, but these words do not
occur in the official travel guides. To overcome this difficulty, we propose to use a latent variable model
that jointly models official travel guides and forum threads. We treat the S sections as S latent factors
that govern the generation of the forum threads. With the latent factors observed in the official travel
guides, we receive some supervision; and yet by jointly modeling both the official travel guides and the
forum threads, we allow the latent factors to adapt to the lexical variations in user-generated content. In
the end, the learned latent factors can help us align forum threads with the sections and subsequently
select the most relevant ones for each section.
Section-specific summarization: Given the selected relevant threads for a section, we adopt an ILP-
based extractive summarization framework that has been shown to be effective (Gillick and Favre, 2009).
We modify the objective function in this framework to consider two factors: (1) Since not every sentence
in the selected threads is highly relevant to the section, we want to give preference to those more relevant
sentences in the objective function, where relevance can be measured using word distributions learned
by the latent variable model. (2) Since travel guides are expected to recommend points of interest to
readers, we try to maximize the coverage of section-specific location entities in the objective function.
3 Joint City Section Model
3.1 Model
In this section we present our Joint City Section Model (JCSM), which links official travel guides and
forum threads. The model is a typical extension of LDA, where a number of latent topics (i.e. latent
factors) are assumed to have generated the observed text. First of all, for each pre-defined section there
is a latent topic. These explain words such as ?food? and ?menu? for restaurants and ?store? and ?mall?
for shopping. In addition, in both travel guides and forum threads, some words are more related to the
city being discussed than any specific section. For example, when New York City is being discussed,
words such as ?NYC? and ?Manhattan? may frequently show up in any section. We therefore further
assume that for each city there is a city-specific topic. A switch variable is used to determine whether a
word comes from a city-specific or section-specific topic.
A special design of our model that differs from many existing LDA extensions is the treatment of
named entities. We first use a named entity recognizer to identify potential names of locations from
forum threads. We assume that each of these entities belongs to a section, which is indicated by a latent
variable. We then assume that the section labels of the non-entity words in forum threads are dependent
on the section labels of these entities. By doing so, we emphasize the importance of associating potential
points of interest with sections, which will be useful when we generate summaries.
We now formally present JCSM. To simplify the model description, we assume that we work with
I cities, each of which has a given, well-written travel guide and a set of forum threads. Note that in
practice this model can be easily extended such that a target city with forum threads does not need to have
a given travel guide to begin with. Let ?
i
denote the word distribution for the city-specific latent topic
associated with city i. Let ?
s
denote the word distribution for the section-specific latent topic for section
s. Let d
i,s,n
denote the n-th word in the s-th section of the i-th city?s travel guide. Here 1 ? d
i,s,n
? V is
an index into the vocabulary with size V . Let x
i,s,n
be a switch variable associated with d
i,s,n
to indicate
whether this word is city-specific or section-specific. For the j-th forum thread related to the i-th city, we
assume there is a distribution over sections, denoted as ?
i,j
. For the l-th location entity in the k-th post
1672
of this thread, we assume a latent variable c
i,j,k,l
(1 ? c
i,j,k,l
? S) that indicates the section label of this
entity. Then for the m-th word in this post, we first use a switch variable y
i,j,k,m
to determine whether
the word is city-specific or section-specific. If it is section-specific, we then choose one of the entities in
the same post, denoted as z
i,j,k,m
, and its corresponding section label as the section for this word.
All the binary switch variables follow a global Bernoulli distribution parameterized by pi. There are
hyperparameters ?, ?, ?
?
and ? that define the prior distributions. The complete model is depicted in
Figure 1. The generative process of JCSM is also described as follows.
S
N M L
K
J I
 d   w c ?
x y z
?
Figure 1: The plate notation of the Joint City Section Model (JCSM). Dashed variables will be integrated out in Gibbs sampling.
For clarity, the Dirichlet and Beta priors are omitted. The arrow pointing to z indicates that z is drawn from a uniform
distribution over the integers from 1 to L.
? For each city i, (i = 1, 2, ? ? ? , I), draw a city-specific word distribution ?
i
? Dir(?
?
)
? For each section s, (s = 1, 2, ? ? ? , S), draw a section-specific word distribution ?
s
? Dir(?)
? Draw a switch distribution pi ? Beta(?)
? For each city i (i = 1, 2, ? ? ? , I)
? For each section s (s = 1, 2, ? ? ? , S)
? For the n-th word in the given travel guide
- Draw x
i,s,n
? Bernoulli(pi)
- If x
i,s,n
= 1, draw d
i,s,n
? Multi(?
s
); otherwise, draw d
i,s,n
? Multi(?
i
).
? For the j-th thread
? Draw a thread specific section distribution ?
j
? Dir(?)
? For the k-th post
- For the l-th entity, draw c
i,j,k,l
? Multi(?
j
)
- For the m-th word, draw y
i,j,k,m
? Bernoulli(pi). If y
i,j,k,m
= 1, draw z
i,j,k,m
? Uniform(1, ? ? ? , L
i,j,k
)
and then draw w
i,j,k,m
? Multi(?
c
i,j,k,z
i,j,k,m
); otherwise, draw w
i,j,k,m
? Multi(?
i
).
3.2 Inference
We use collapsed Gibbs sampling to estimate the parameters in the model. The problem is to compute
the Gibbs update rules for sampling x
i,s,n
, c
i,j,k,l
, z
i,j,k,m
, y
i,j,k,m
.
Sample entity topic c
i,j,k,l
Let b denote {i, j, k, l} and u denote {i, j, k}. We can derive the Gibbs update rule for sampling entity
topic c
i,j,k,l
as follows:
p(c
b
= s|C
?
b
,W,D,X,Y,Z) =
n
s
i,j,?
b
+ ?
?
S
s
?
=1
n
s
?
i,j,?
b
+ S?
?
?
V
w=1
?
n
w
u,y=1,z=l
i
?
=1
(n
w
y=1,z=l,?
u
+ ? + i
?
? 1)
?
n
w
y=1,z=l,u
j
?
=1
(
?
V
w=1
n
w
y=1,z=l,?
u
+ V ? + j
?
? 1)
,
where n
s
i,j,?
b
denotes the number of entities whose topic assignments are s in thread {i, j} without
consideration of entity {i, j, k, l}. n
w
u,y=1,z=l
denotes the number of times term w occurs in the post
{i, j, k} with the constraint that y = 1 and z = l. n
w
y=1,z=l,?
u
is the number of times term w occurs in
all posts except the post {i, j, k} with the constraint that y = 1 and z = l.
Sample switch label x
i,s,n
We can derive the Gibbs update rule for sampling x
i,s,n
in a similar way. Note that the sampling of
x
i,s,n
is in travel guide word level. Let g denote{i, s, n}, the Gibbs update rule for sampling x
i,s,n
is as
follows:
1673
p(x
g
= 0|C,W,D
?
g
,X
?
g
,Y,Z) =
n
x=0
?
g
+ ?
?
1
x=0
n
x
?
g
+ 2?
?
n
w
g
x=0,i,?
g
+ ?
?
?
V
w=1
n
w
x=0,i,?
g
+ V ?
?
p(x
g
= 1|C,W,D
?
g
,X
?
g
,Y,Z) =
n
x=1
?
g
+ ?
?
1
x=0
n
x
?
g
+ 2?
?
n
w
g
x=1,s,?
g
+ ?
?
V
w=1
n
w
x=1,s,?
g
+ V ?
Sample post word topic z
i,j,k,m
and switch label y
i,j,k,m
For words in the thread posts, We can derive the Gibbs update rule for sampling post word topic z
i,j,k,m
and switch label y
i,j,k,m
. Note that the sampling of z
i,j,k,m
and y
i,j,k,m
is in post word level. Let f
denote{i, j, k,m}. The Gibbs update rule for sampling z
i,j,k,m
and y
i,j,k,m
is as follows:
p(z
f
= s|C,W
?
f
,D,X,Y
?
f
,Z
?
f
) =
n
w
f
y=1,s
?
,?
f
+ ?
?
V
w=1
n
w
y=1,s
?
,?
f
+ V ?
?
1
L
i,j,k
p(y
f
= 0|C,W
?
f
,D,X,Y
?
f
,Z
?
f
) =
n
y=0
?
f
+ ?
?
1
y=0
n
y
?
f
+ 2?
?
n
w
f
y=0,i,?
f
+ ?
?
?
V
w=1
n
w
y=0,i,?
f
+ V ?
?
p(y
f
= 1|C,W
?
f
,D,X,Y
?
f
,Z
?
f
) =
n
y=1
?
f
+ ?
?
1
y=0
n
y
?
f
+ 2?
?
n
w
f
y=1,s
?
,?
f
+ ?
?
V
w=1
n
w
y=1,s
?
,?
f
+ V ?
where s
?
= c
i,j,k,l
which is the topic index of the associated entity of this word.
Parameter estimation
After Gibbs Sampling, we can make the following parameter estimation:
?
i,j,s
=
n
s
i,j
+ ?
?
S
s
?
=1
n
s
?
i,j
+ S?
. thread-section distribution.
?
s,w
=
n
w
s,y=1
+ ?
?
V
w
?
=1
n
w
?
s,y=1
+ V ?
. section-word distribution.
?
i,w
=
n
w
i,y=0
+ ?
?
?
V
w
?
=1
n
w
?
i,y=0
+ V ?
?
. city-word distribution.
pi
y
=
n
y
(.)
+ ?
?
1
y
?
=0
n
y
?
(.)
+ 2?
. switch distribution.
4 Generating Section-specific Summaries
With the JCSM model presented in the last section, we can learn a word distribution for each section,
which can help us find more relevant content for the section. For each section, we rank the forum
threads by how likely the words inside a thread is generated from the corresponding section-specific word
distribution. We select the top-K threads for each section to perform section-specific summarization.
Extractive summarization has been well studied and many algorithms have been proposed. We choose
to build our solution on top of an ILP-based framework proposed by Gillick and Favre (2009), partly
because our experiments comparing this ILP framework and other existing methods show its advantage
on our data sets (see Section 5). Below we first briefly review this ILP-based summarization framework
and then present our proposed improvements.
The idea behind the ILP framework by Gillick and Favre (2009) is to maximize the coverage of so-
called ?concepts? from the original corpus in the generated summary. In practice, bigrams are used as
concepts. Specifically, let us use i to index all the concepts from the original corpus. Let w
i
denote
the weight of the i-th concept computed based on its frequency and b
i
? {0, 1} denote the absence or
1674
presence of the concept. The framework aims to maximize
?
i
w
i
b
i
, i.e. the total weighted coverage of
the concepts, subject to the following constraints:
?
j
l
j
s
j
? L, (l
j
is the length of the j-th sentence in terms of words, and L is the length limit of the summary.)
?i, j : s
j
o
i,j
? b
i
, (s
j
? {0, 1} denotes the absence or presence of the j-th sentence.)
?i :
?
j
s
j
o
i,j
? b
i
. (o
i,j
? {0, 1} denotes whether concept i occurs in sentence j.)
Although this framework works well for standard summarization, our task is different. We propose the
following changes to this framework:
Favoring relevant sentences: Recall that although we select presumably the most relevant threads for
each section, we cannot guarantee that each sentence in these threads is related to the section. For
example, we observe that the things-to-do section is often mixed with content from restaurants, sights,
transport and entertainment sections. Also, some sentences are less relevant to the target city than
others. In order to select the more relevant sentences in the summary, we propose to add the second term
in Eqn. 1 below. Here j is used to index all the candidate sentences and u
j
is a weight for sentence j
based on its relevance.
We measure relevance with respect to both the city and the section. Let LL(j, ?) denote the log like-
lihood of generating sentence j from the section-specific topic ? and LL(j, ?) denote the log likelihood
of generating sentence j from the city-specific topic ?. We define u
j
as follows:
u
j
? exp (?LL(j, ?) + (1? ?)LL(j, ?)) .
u
j
are then normalized to be between 0 and 1. Note that here ? is a manually defined parameter used to
control the tradeoff between city-specific relevance and section-specific relevance. As we will show in
Section 5, both relevance factors turn out to be useful.
Covering section-specific points of interest: We hypothesize that a good summary travel guide should
mention potential points of interest to the reader. To this end, the last term in Eqn. 1 is added. Specifically,
k is an index for unique location names we find that have been labeled as belonging to section s according
to the JCSM model. e
k
? {0, 1} denotes whether the k-th entity is present in the selected sentences, and
v
k
denotes the weight for this entity based on its frequency.
Eventually, the summarization task is formulated as the following optimization problem:
Maximize: ?
1
?
i
w
i
b
i
+ ?
2
?
j
u
j
s
j
+ (1? ?
1
? ?
2
)
?
k
v
k
e
k
(1)
Subject to:
?
j
l
j
s
j
? L,
?i :
?
j
s
j
o
i,j
? b
i
, ?i, j : s
j
o
i,j
? b
i
,
?j :
?
k
s
j
p
j,k
? e
k
, ?j, k : s
j
p
j,k
? e
k
.
Here o
i,j
denotes whether concept i occurs in sentence j, and p
j,k
denotes whether entity k occurs in
sentence j. For the weights w
i
and v
k
, we normalize them using the total occurrences of bigrams/entities
to ensure their values are between 0 and 1. We solve the above optimization problem using the IBM
ILOG CPLEX Optimizer
1
.
5 Experiments
5.1 Data and Experimental Setup
We use real data from Yahoo! Answers and Lonely Planet for evaluation. We first crawl the travel guides
for 10 cities from Lonely Planet, where each travel guide has 8 sections. We then crawl the top 60000
Q&A threads ranked by number of posts related to these 10 cities (6000 for each city) from Yahoo!
Answers under the ?travel? category where all questions have been grouped by cities. We filter out
trivial factoid questions using features used by Tomasoni and Huang (2010). We then use the Stanford
1
http://www-01.ibm.com/software/commerce/optimization/cplex-optimizer/
1675
Method Singapore Sydney New York City Los Angeles Overall Average
R-1 R-2 RSU4 R-1 R-2 RSU4 R-1 R-2 RSU4 R-1 R-2 RSU4 R-1 R-2 RSU4
Random 0.4091 0.1046 0.1576 0.4496 0.1100 0.1925 0.4442 0.1192 0.1858 0.4154 0.1130 0.1693 0.4309 0.1115 0.1771
Centroid 0.4029 0.0993 0.1484 0.4228 0.1100 0.1764 0.4235 0.1192 0.1722 0.3763 0.0787 0.1386 0.4133 0.1077 0.1640
LexRank 0.4396 0.1451 0.1891 0.4406 0.1296 0.1955 0.4304 0.1397 0.1859 0.4032 0.0992 0.1661 0.4350 0.1331 0.1894
DivRank 0.4534 0.1504 0.1888 0.4473 0.1161 0.1925 0.4391 0.1167 0.1804 0.4275 0.1180 0.1733 0.4487 0.1317 0.1888
GMDS 0.3918 0.0890 0.1415 0.4339 0.1066 0.1784 0.4064 0.0845 0.1576 0.3846 0.0809 0.1413 0.4045 0.0916 0.1553
ILP-BL 0.4635 0.1650 0.2000 0.4948 0.1731 0.2333 0.4691 0.1613 0.2073 0.4545 0.1445 0.1981 0.4755 0.1654 0.2136
Our Method 0.4723 0.1655 0.2035 0.5078 0.1787 0.2397 0.4716 0.1713 0.2086 0.4543 0.1565 0.1945 0.4804
?
0.1715
?
0.2144
?
Table 2: Comparison of the summarization results.
?
means the result is better than others except ILP-BL in the same column
at 5% significance level measured by Wilcoxon signed rank test. Note that only the average scores are tested for statistical
significance based on the 32 summarization tasks in total.
NER tool to recognize named entities in these threads. Since we notice that sometimes entities tagged as
PER are also possible points of interest, we include all entities of LOC, ORG and PER types. In order
to use higher quality threads for evaluation, for each city we pick the top 600 threads that have the most
overlapping points of interest with the Lonely Planet travel guides. On average, each thread contains 5.0
posts and 618.1 words. These 600? 10 threads are used to train the JCSM model.
We need human generated model summaries for evaluation. Since it is too time consuming to ask
human annotators to look through 600 threads and generate structured summaries, we instead opt to
first retrieve the top 30 relevant threads per section per city based on the JCSM results and then ask
human annotators to summarize these 30 threads to generate a section-specific summary. Our summa-
rization method as well as the baselines are also applied to these 30 threads per section per city for fair
comparison. We randomly select 4 cities for human annotation, giving us 8 ? 4 = 32 section-specific
summarization tasks. For each task, we ask four annotators to read all 30 threads and write a summary
as model summaries in our experiments
2
.
We use the following baseline algorithms for comparison: (1) Random, which randomly picks sum-
mary sentences. (2) Centroid (Radev et al., 2004), which selects sentences according to several features
like tfidf, cluster centroid and position. (3) LexRank (Erkan and Radev, 2004b)., which applies a graph-
based algorithm . (4) DivRank (Mei et al., 2010), which employs a time-variant random walk to enhance
diversity. (5) GMDS (Wan, 2008), which incorporates the document-level information and the sentence-
to-document relationship into the ranking process. (6) ILP-BL, which is the method proposed by Gillick
and Favre (2009).
We empirically set Dirichlet hyperparameters ? = 0.5, ? = 0.01, ? = 0.01, ?
?
= 0.1. We run JCSM
with 400 iterations of Gibbs sampling. For the weight parameters in the ILP model, we empirically set
?
1
= 0.7, ?
2
= 0.1, ? = 0.7 after we conduct multiple experiments to determine the best values of them
from 0.1 to 0.9.
5.2 Summarization Results
To compare the summaries generated by our method with those generated by the baselines, we first
compute their ROUGE scores against the human generated model summaries. ROUGE scores have
been widely used for evaluation of summarization systems (Lin and Hovy, 2003). We use the ROUGE
toolkit
3
, which provides multiple kinds of ROUGE metrics including ROUGE-N, ROUGE-L, ROUGE-
W and ROUGE-SU4. In the experiment results we report three ROUGE F-measure scores, namely,
ROUGE-1, ROUGE-2 and ROUGE-SU4. The higher the ROUGE scores, the better a summary is.
In Table 2 we show the summarization results of our method (with the optimal parameter setting) and
the baseline methods. For each city, the scores we show are averaged over the 8 sections. The overall
average scores on the right hand side are averaged over the 4 cities. We have the following findings from
the table: (1) Compared with the other baselines, the ILP-based baseline clearly shows its advantage,
justifying our our design choice of adopting an ILP-based framework as the basis of our method. (2)
Our method performs slightly better than ILP-BL based on the overall scores, but the difference is not
statistically significant.
2
The summary dataset can be found at https://sites.google.com/site/liuyang198908/code-data.
3
http://www.isi.edu/licensed-sw/see/rouge/
1676
section Singapore Sydney New York City Los Angeles
ILP-BL Our Method ILP-BL Our Method ILP-BL Our Method ILP-BL Our Method
restaurants 0.3750 0.5417 0.5714 0.7143 0.2500 0.3750 0.1053 0.2105
hotels 0.4091 0.4091 0.0000 0.5000 0.3636 0.5000 0.4500 0.5500
shopping 0.1429 0.5357 0.3750 0.3750 0.1905 0.1905 0.0455 0.1818
sights 0.5000 0.5789 0.3846 0.4615 0.3636 0.6364 0.1143 0.2571
entertainment 0.1304 0.2174 0.2500 0.7500 0.0909 0.2273 0.2500 0.4167
activities 0.4167 0.5833 0.2500 0.2500 0.1250 0.5000 0.2069 0.2759
transport 0.3889 0.5556 0.7500 0.7500 0.6000 0.8000 0.3158 0.7368
things-to-do 0.2105 0.2632 0.2500 0.5500 0.4583 0.5833 0.0000 0.2000
average 0.3217 0.4606 0.3539 0.5439 0.3052 0.4766 0.1860 0.3536
Table 3: Comparison of the recall of named entities of ILP-BL and our method.
Method Our Complete Model ?EC ?SR ?SecRel ?CityRel
R-1 0.4804 0.4520 0.4657 0.4672 0.4796
R-2 0.1715 0.1430 0.1669 0.1652 0.1685
RSU4 0.2144 0.1987 0.2028 0.2039 0.2120
Table 4: Summarization results of the degenerate versions of our method. ??? means removing this component from our
complete method. The table shows the average results over data sets of all cites.
Considering that an importance difference between our method and ILP-BL is our focus on points
of interest, we further compared ILP-BL and our method using a different metric. The objective is to
test the coverage of points of interest in our generated summaries versus the summaries generated by
ILP-BL. To this end, we first identify all the named entities in the model summaries using the Stanford
NER tool. We then check the percentage of these named entities covered in the generated summaries and
report these recall scores in Table 3. We can see that for majority of the 32 section-specific summaries,
our method clearly has a higher recall score than ILP-BL, showing that our method generates summaries
with more potential points of interest.
To further understand whether all the components of our improved ILP method have contributed to
the performance improvement, we compare our overall method with a few degenerate versions of our
method. In each degenerate version, we remove a single component of the objective function. The results
are shown in Table 4, where?EC removes the consideration of entity coverage (i.e. setting ?
1
+?
2
= 1),
?SR removes the consideration of sentence relevance (i.e. setting ?
2
= 0), ?SecRel removes only the
section-specific relevance of the sentences (i.e. setting ? = 0), and ?CityRel removes only the city-
specific relevance of the sentences (i.e. setting ? = 1). We can see that each degenerate version of our
method performs worse than the complete method, which shows that all components of the objective
function are useful. In particular, entity coverage and section-specific relevance seem to be the more
important components.
 0
 0.2
 0.4
 0.6
 0.8
 1 0
 0.2
 0.4
 0.6
 0.8
 1
 0.4
 0.5
 0.6
lambda2
lambda1
 0.445 0.45
 0.455 0.46
 0.465 0.47
 0.475 0.48
 0.485 0.49
 0.495
(a) R-1(?
1
, ?
2
)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
rho
R-1
R-2
RSU4
(b) ?
Figure 2: Summarization performance of our method by varying the value of the parameters ?
1
, ?
2
and ?.
1677
5.3 Analysis of Topic Words
We show some further analysis of our results. To begin with, we analyze the learning results of JCSM.
The top words in city-specific word distributions and section-specific word distributions learnt by JCSM
are presented in Table 5 and Table 6. Generally we observe clean top words for each city and each
section. For each city, city-specific words are those associated with the corresponding city. For example,
for Singapore, we see words such as ?s$? (Singapore dollars), ?sentosa? (an island resort in Singapore),
?orchard? (a boulevard that is the retail and entertainment hub of Singapore) and ?bugis? (a popular
shopping place). For New York City, we see ?square?, ?times? and ?manhattan?. For each section,
section-specific words are those words which frequently appear when people discuss about this section,
such as ?menu?, ?dishes? and ? seafood? for the restaurant section and ?train?, ?bus? and ?station? for
the transport section.
Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 Topic 8 Topic 9 Topic 10
Singapore SFO Chicago Boston LA NYC Seattle Pairs London Sydney
singapore sf chicago boston beach york downtown paris london sydney
s$ san downtown end hollywood nyc seattle de tube harbour
centre francisco park north los park needle metro underground beach
food gate city downtown angeles central space eiffel central manly
shopping golden neighborhood fenway la square market french centre beaches
sentosa bay north bay downtown times rain la british house
road bart lake harvard drive manhattan place du palace opera
orchard union mile place california broadway pike tower thames quay
chinese wharf loop city miles city center des end australian
mrt muni ave college hills street waterfront rue kensington rocks
bugis square field subway long east area le station bridge
Table 5: Top city specific words discovered by JCSM.
Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 Topic 8
restaurants hotels shopping sights entertainment activities transport thingsToDo
food hotel shop museum bar visit train bar
restaurant rooms store city music park bus place
menu free stores park club tour station tour
dishes wi-fi shopping art place fun airport city
place walk shops building night city time food
bar located find built dance walk line art
chicken offers clothes world beer day car day
fish station wear place clubs time walk including
fresh features mall house crowd shopping minutes music
seafood tv place area bars museum hours restaurant
Table 6: Top section specific words discovered by JCSM.
5.4 Parameter Sensitivity Analysis
We further give parameter sensitivity analysis for our proposed method. We show how sensitive our
results are with respect to the parameters ?
1
, ?
2
and ?. We choose the Sydney data set to perform
parameter sensitivity analysis. In Figure 2(a), we show how ROUGE-1 varies with respect to ?
1
and ?
2
.
We can see that the performance fluctuates within a limited range as we vary ?
1
and ?
2
. We find the
trend for ROUGE-2 and ROUGE-SU4 is similar so we leave out the figures for them. In Figure 2(b) we
see that the performance is pretty stable as we vary ?.
5.5 Sample Output and Case Study
Finally, we show a sample travel guide our method generates for Sydney in Table 7. We can see that first
of all the sentences selected by our method have high relevance to the corresponding sections. Second,
through observation we find that humans tend to select sentences containing more points of interest as
summary. Our summary sentences contain many points of interest as highlighted, showing the advantage
of our method.
1678
Sample Summary Sentences Generated from Yahoo! Answers by Our Method for Sydney
Hotel
Sorry can not recommend you a hotels as I have no idea of pricing , but if you want a nice area , check hotels in Bondi and Manly Beaches .
As for the Acer Arena , that is in the Homebush Olympic Park and you can choose to live in either Parramatta or the city .
You need to live in one of the surrounding residential suburbs , close to a train line . Try Alexandria , Newtown , Surry hills for inner suburbs . . . .
Sights
You can walk around the harbor area to the Opera House and you can see the beautiful Harbor Bridge .
All this is apart from the Opera House and the Botanical Gardens . Visit the Custom House Circular Quay and see a model of Sydney. You
must also do a day trip to the Blue Mountains . Harbour Wedding is one of the major attraction in Sydney . . . .
Entertainment
George Street has a number of bars . All the bars around the harbour are really good day and night . If you want to stay in a hotel where there is
entertainment at night , you could look at Woolloomooloo , Darlinghurst , Surry Hills , Kings Cross or Potts Point . Newtown is good for bars .
Get them to see a theatre show or something at the Opera House . . . .
Things-to-do
If you are going out for the day , starting with a walk to the city will be most enjoyable . Take a public ferry from Circular Quay to Darling
Harbour , about 15 minutes across the harbour and under the bridge , when you get to Darling Harbour go and see the Chinese Gardens . There
are lots of interesting things to see and do in and around Sydney . . . .
Activities
They have good markets at the weekend and great views of the Opera House . The Opera House is free to have a look at , if you like art then walk
through the Botanical Gardens and go and see the art gallery . If you ?re feeling brave , you can do a Harbour Bridge walk , though I think it may
be a little pricey . . . .
Table 7: Excerpts from the summary generated from Yahoo! Answers by our method for Sydney. We show summaries for the
5 sections other than the 3 sections shown in Table1. Named entities are highlighted in bold font.
6 Related Work
Multi-document summarization is a process to generate a text summary by reducing documents in size
while retaining the main points of the original documents. It has been extensively studied in the NLP
community, with most efforts on extractive summarization. Our work is also based on extractive sum-
marization. Extractive summarization essentially selects a set of sentences from the original documents
to form a summary.
To select sentences, different features and ranking strategies have been studied. Early work focuses
on finding good features to select summary sentences. Radev et al. (2004) proposed a centroid-based
summarizer which combines several pre-defined features like tfidf, cluster centroid and position to score
sentences. Lin and Hovy (2002) built the NeATS multi-document summarization system using term fre-
quency, sentence position, stigma words and simplified Maximal Marginal Relecvance (MMR). Nenkova
et al. (2006) proved that high-frequency words were significant in reflecting the focus of documents.
Ouyang et al. (2010) studied the influence of different word positions in summarization. Later, graph-
based ranking algorithms have been successfully applied to summarization. LexPageRank (Erkan and
Radev, 2004a) is a representative one based on the PageRank algorithm (Page et al., 1999). Later exten-
sions include ToPageRank (Pei et al., 2012), which incorporates topic information into the propagation
mechanism, the manifold-ranking based method for topic-focused summarization (Wan et al., 2007) and
DivRank (Mei et al., 2010), which introduces a time-variant matrix into a reinforced random walk to
balance prestige and diversity.
More recently, Integer Linear Programming (ILP) based framework was introduced as a global infer-
ence algorithm for multi-document summarization by McDonald (2007), which considers information
and redundancy at the sentence level. Gillick and Favre (2009) studied information and redundancy at a
sub-sentence, ?concept? level, modeling the value of a summary as a function of the concepts it covers.
In our work we also model concept level coverage of the summaries. Li et al. (2013) proposed a re-
gression model to estimate the frequency of bigrams in the reference summary and analyzed the impact
of bigram selection, weight estimation and ILP setup. Haghighi and Vanderwende (2009) constructed
a sequence of generative probabilistic models for multi-document summarization, exhibiting ROUGE
gains along the way. Sauper and Barzilay (2009) investigated an approach for creating a comprehensive
textual overview of subject composed of information drawn from the Internet and applied ILP to opti-
mize both local fit of information into each topic and global coherence across the entire overview. Li
et al. (2011) developed an entity-aspect LDA model to cluster sentences into aspects and then extend
LexRank algorithm to rank sentences. Hu and Wan (2013) proposed to use SVR model and ILP method
to generate presentation slides for academic papers.
Our work is different from standard ILP-based multi-document summarization. We designed a latent
variable model to first separate the threads to be summarized into sections based on model gravel guides.
1679
We also emphasized the inclusion of potential points of interest in formulating the ILP optimization
problem.
Our work is also closely related to previous work on answer summarization in community-based
QA sites. Previous work on summarizing answers is mainly based on query focused multi-document
summarization techniques to summarize multiple answer documents given a single question. Liu et al.
(2008) proposed a CQA question taxonomy to classify questions in CQA and question-type oriented
answer summarization for better reuse of answers. Tomasoni and Huang (2010) proposed two concept-
scoring functions to combine quality, coverage, relevance and novelty measures for answer summary
in response to a question and showed that their summarized answers constitute a solid complement to
best answers voted by CQA users. Chan et al. (2012) presented an answer summarization method for
complex multi-sentence questions. For our work, we study a new problem of summarizing multiple
threads to automatically generate city travel guides based on known template structure from well-written
travel guides, which is different from the setting of single Q&A thread summarization in the previous
related studies.
7 Conclusion and Future Work
In this paper we proposed a summarization framework to generate well structured supplementary travel
guides from social media based on a latent variable model and integer linear programming. The la-
tent variable model could align forum threads with the section structure of well-written travel guides.
Compared to standard concept based ILP methods, our method additionally tries to cover more named
entities as points of interest and maximizes sentence relevance scores measured by section-specific and
city-specific word distributions learnt by the latent variable model. Extensive experiments with real data
from Yahoo! Answers show that our proposed method is able to generate better summaries compared
with a number of multi-document summarization baselines measured by ROUGE scores.
Currently our generated summaries may have overlap with the well-written model travel guides. In the
future, we plan to improve our method to emphasize the selection of additional information from social
media compared with the model travel guides. We will also look into the problem of how to summarize
information that does not fit into the template structure derived from model travel guides.
Acknowledgments
This work was done during Liu Yang?s visit to Singapore Management University. The authors would
like to thank the reviewers for their valuable comments on this work.
References
Wen Chan, Xiangdong Zhou, Wei Wang, and Tat-Seng Chua. 2012. Community answer summarization for multi-
sentence question with group l1 regularization. In Proceedings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers - Volume 1, ACL ?12, pages 582?591, Stroudsburg, PA, USA.
Association for Computational Linguistics.
G?unes Erkan and Dragomir R. Radev. 2004a. Lexpagerank: Prestige in multi-document text summarization. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing, volume 4 of EMNLP
?04.
G?unes Erkan and Dragomir R. Radev. 2004b. Lexrank: Graph-based lexical centrality as salience in text summa-
rization. J. Artif. Int. Res., 22(1):457?479, December.
Dan Gillick and Benoit Favre. 2009. A scalable global model for summarization. In Proceedings of the Workshop
on Integer Linear Programming for Natural Langauge Processing, ILP ?09, pages 10?18, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In
Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter
of the Association for Computational Linguistics, NAACL ?09, pages 362?370, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
1680
Yue Hu and Xiaojun Wan. 2013. Ppsgen: Learning to generate presentation slides for academic papers. In
Proceedings of the 21st international jont conference on Artifical intelligence, IJCAI ?13, pages 2099?2105.
Peng Li, Yinglin Wang, Wei Gao, and Jing Jiang. 2011. Generating aspect-oriented multi-document summariza-
tion with event-aspect model. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing, EMNLP ?11, pages 1137?1146, Stroudsburg, PA, USA. Association for Computational Linguistics.
Chen Li, Xian Qian, and Yang Liu. 2013. Using supervised bigram-based ilp for extractive summarization. In
Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics, ACL ?13, pages
1004?1013, Stroudsburg, PA, USA. Association for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2002. From single to multi-document summarization: A prototype system and
its evaluation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL
?02, pages 457?464, Stroudsburg, PA, USA. Association for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational
Linguistics on Human Language Technology - Volume 1, NAACL ?03, pages 71?78, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin, Dingyi Han, and Yong Yu. 2008. Understanding and sum-
marizing answers in community-based question answering services. In Proceedings of the 22Nd International
Conference on Computational Linguistics - Volume 1, COLING ?08, pages 497?504, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Proceedings
of the 29th European Conference on IR Research, ECIR?07, pages 557?564, Berlin, Heidelberg. Springer-
Verlag.
Qiaozhu Mei, Jian Guo, and Dragomir Radev. 2010. Divrank: The interplay of prestige and diversity in informa-
tion networks. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, KDD ?10, pages 1009?1018, New York, NY, USA. ACM.
Ani Nenkova, Lucy Vanderwende, and Kathleen McKeown. 2006. A compositional context sensitive multi-
document summarizer: Exploring the factors that influence summarization. In Proceedings of the 29th Annual
International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ?06, pages
573?580, New York, NY, USA. ACM.
You Ouyang, Wenjie Li, Qin Lu, and Renxian Zhang. 2010. A study on position information in document
summarization. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,
COLING ?10, pages 919?927, Stroudsburg, PA, USA. Association for Computational Linguistics.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The pagerank citation ranking: Bring-
ing order to the web. Technical Report 1999-66, Stanford InfoLab, November. Previous number = SIDL-WP-
1999-0120.
Yulong Pei, Wenpeng Yin, and Lian?en Huang. 2012. Generic multi-document summarization using topic-oriented
information. In Proceedings of the 12th Pacific Rim International Conference on Trends in Artificial Intelli-
gence, PRICAI?12, pages 435?446, Berlin, Heidelberg. Springer-Verlag.
Dragomir R. Radev, Hongyan Jing, Malgorzata Sty?s, and Daniel Tam. 2004. Centroid-based summarization of
multiple documents. Inf. Process. Manage., 40(6):919?938, November.
Christina Sauper and Regina Barzilay. 2009. Automatically generating wikipedia articles: A structure-aware
approach. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL ?09, pages
208?216, Stroudsburg, PA, USA. Association for Computational Linguistics.
Mattia Tomasoni and Minlie Huang. 2010. Metadata-aware measures for answer summarization in community
question answering. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguis-
tics, ACL ?10, pages 760?769, Stroudsburg, PA, USA. Association for Computational Linguistics.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. Manifold-ranking based topic-focused multi-document
summarization. In Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI?07,
pages 2903?2908, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Xiaojun Wan. 2008. An exploration of document impact on graph-based multi-document summarization. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?08, pages
755?762, Stroudsburg, PA, USA. Association for Computational Linguistics.
1681
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 726?735,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Optimized Event Storyline Generation based on Mixture-Event-Aspect
Model
Lifu Huang
Shenzhen Key Lab for Cloud Computing
Technology and Applications,
Peking University Shenzhen Graduate School,
Shenzhen, Guangdong 518055, P.R.China
warrior.fu@gmail.com
Lian?en Huang?
Shenzhen Key Lab for Cloud Computing
Technology and Applications,
Peking University Shenzhen Graduate School,
Shenzhen, Guangdong 518055, P.R.China
hle@net.pku.edu.cn
Abstract
Recently, much research focuses on event s-
toryline generation, which aims to produce a
concise, global and temporal event summary
from a collection of articles. Generally, each
event contains multiple sub-events and the sto-
ryline should be composed by the componen-
t summaries of all the sub-events. However,
different sub-events have different part-whole
relationship with the major event, which is
important to correspond to users? interests
but seldom considered in previous work. To
distinguish different types of sub-events, we
propose a mixture-event-aspect model which
models different sub-events into local and
global aspects. Combining these local/global
aspects with summarization requirements to-
gether, we utilize an optimization method to
generate the component summaries along the
timeline. We develop experimental systems on
6 distinctively different datasets. Evaluation
and comparison results indicate the effective-
ness of our proposed method.
1 Introduction
With the rapid growth of the World Wide Web, in-
formation explosion has become an important issue
to modern people. Those who search for informa-
tion from the Internet often get lost and confused
by the overwhelmingly large collection of web doc-
uments. So how to get a concise and global pic-
ture for a given event subject is an urgent prob-
lem to be solved. Although many document un-
derstanding systems have been proposed, such as
?Corresponding author
multi-document summarization systems, to gener-
ate a compressed summary by extracting the ma-
jor information from the collection of documents,
they ignored the dynamic development information
of an event. Intuitively, each event is long-running
and contains multiple sub-events, including related
events. Users are likely to prefer a summary of all
occurrences of all the sub-events along the timeline
of the event. This motivates us to study the task of
generating event storyline from a collection of web
documents related to an event subject.
The research of event storyline summarization is
popular in recent years. Its task is to summarize a
collection of web documents by extracting represen-
tative information based on all the sub-events and
generate a global summary. Generally, generating
such a global storyline is quite interesting for the fol-
lowing main reasons: (1) It can help people catch the
whole incident based on an overall temporal struc-
tured summary for a given subject, and understand
the cause, climax, development process and result
of an event. (2) It can also make people know what
other events are related, or the effect of this incident
to subsequent events, which can present the evolu-
tion of an event along a timeline.
Though several methods of generating event sto-
ryline have been proposed recently, there are still
some problems unresolved. As event storyline sum-
marization is a process to generate component sum-
maries based on the multiple sub-events, which is d-
ifferent from traditional summarization focusing on
only one subject, so how to exactly extract all the
sub-events is the first challenge. Moreover, user-
s tend to bias to the sub-events which have global
726
consistency with the given event subject, so the sub-
events should not be considered equally when gen-
erating the component summaries. It is also a great
challenge to generate a qualified summary based on
the different types of sub-events. The componen-
t summaries should be correlative across differen-
t dates based on the global collection (Yan et al,
2011a).
Mei and Zhai (Mei and Zhai, 2005) proposed to
use theme or topic to model different sub-events,
which is to some extent similar to our method. To
be different, in this paper we introduce ?local/glob-
al? property to distinguish different part-whole rela-
tionship between the sub-events and the major even-
t, which have not been considered before in story-
line generation or summarization, to improve the
quality of the storyline. The local/global proper-
ty corresponds to the elements of an event, such
as the place, characters and other body informa-
tion. These information reflects the relationship be-
tween the sub-events and the major event. Some
sub-events have distinctive body information and lit-
tle relevance with each other. They generally occur
for a local period, which we name as ?local-sub-
events?. While other sub-events often share com-
mon properties with each other and have close re-
lationship with the major event and we call them
as ?global-sub-events?. Here we give some exam-
ples to illustrate the difference. For the event ?Con-
necticut school shooting? which occurred on Dec.14
2012, its sub-events such as ?Obama?s speech for
this massacre? or ?Gun control Act? have little word
co-occurrences and distinctive event body informa-
tion to each other, while the process and result of this
tragedy can be regarded as global-sub-events which
have a lot of word co-occurrences and share com-
mon properties with the major event.
Inspired by these, to detect different types of sub-
events based on word co-occurrences between sub-
events and the major event, we propose a mixture-
event-aspect (MEA) model to formalize differen-
t types of sub-events into local/global aspects, which
are implicated with clusters of sentences. Then com-
bining the local/global aspects with summarization
requirements together, we utilized an optimization
approach to get the optimal component summaries
along the timeline. We evaluate our method on 6 dis-
tinctively different datasets. Performance compar-
isons among different system-generated storylines
demonstrate the necessity to distinguish differen-
t types of sub-events and also indicates the effective-
ness of the proposed mixture-event-aspect model.
The rest of the paper is organized as follows. We
briefly review the related work in section 2. In sec-
tion 3 we present the details of optimized event s-
toryline generation based on mixture-event-aspect
model. Experiments and results are discussed in
Section 4. Finally we draw a conclusion of this s-
tudy in Section 5.
2 Related Work
Our work is related to several lines of research in the
literature including multi-document summarization
(MDS), topic detection and tracking (TDT), tempo-
ral text mining (TXM) and temporal news summa-
rization (TNS).
Multi-document summarization is a process to
generate a summary by reducing documents in size
while retaining the main information. To date, dif-
ferent features and ranking strategies have been s-
tudied. Radev et al (Radev et al, 2004) proposed
to implement MEAD as a centroid-based summa-
rizer by combining several predefined features to s-
core the sentence. LexPageRank (Erkan and Radev,
2004) is the representative work which is based on
PageRank (Page et al, 1999) algorithm. Somemeth-
ods have been proposed to extend the conventional
graph-based models recently including multi-layer
graph incorporated with different relationship (Wan,
2008), ToPageRank based on the topic information
(Pei et al, 2012) and DivRank (Mei et al, 2010) bal-
ancing the prestige and diversity.
Topic detection and tracking (TDT) aims to group
news articles based on the topics discussed in them,
detect some novel and previously unreported events
and track future events related to the topics (Wang
et al, 2012). Kumaran and Allan (Kumaran and Al-
lan, 2004) showed how performance on new event
detection could be improved by the use of text clas-
sification techniques as well as by using named en-
tities in a new way. Makkonen et al (Makkonen
et al, 2004) proposed a method that incorporated
simple semantics into TDT by splitting the term s-
pace into groups of terms. Krause et al Wang et al
(Wang et al, 2007) and Wang et al (Wang et al,
727
2009) worked on topic tracking from multiple news
streams. Their methods extracted meaningful top-
ics from multi-source news collections and tracked
different topics as they evolved from one to another
along the timeline.
Our work is also related to temporal text mining
and temporal news summarization. The task of tem-
poral news summarization is to generate news sum-
maries along the timeline from massive data. Chieu
et al (Chieu and Lee, 2004) built a system that ex-
tracted events relevant to a query from a collection
of related documents and placed such events along
a timeline. Yan et al (Yan et al, 2011b) designed
an evolutionary timeline summarization approach to
construct a timeline of a topic by optimizing the rel-
evance, coverage, coherence, and diversity. Lin et al
(Lin et al, 2012) explored the problem of generating
storylines from microblogs for user input queries.
They first proposed a language model with dynamic
pseudo relevance feedback to obtain relevant tweet-
s and then generated storylines via graph optimiza-
tion.
3 Approach Details
In this section, we first propose a mixture-event-
aspect model to detect local/global sub-events based
on part-whole relationship with the major event and
then present a new method to estimate the bursty of
each aspect on a certain date. Afterwards we uti-
lize an optimization method based on local/global
aspects to extract the qualified summary.
3.1 Mixture-Event-Aspect Model
The key challenge to our storyline generation task
is to detect and distinguish different types of sub-
events contained in the article collection. In the col-
lection, each sentence is assigned with a certain date
and sentences that are assigned with the same date
are grouped into the same sub-collection. Consid-
ering the consistency of content between the sub-
events and the major event, we model different sub-
events into two types: local-sub-event and global-
sub-event, and introduce local/global aspects cor-
respondingly. Generally, local aspects which cor-
respond to local-sub-events have distinctive word-
s distribution from each other and sustain for a lo-
cal context while the global aspects corresponding
to global-sub-events have coincident words distri-
bution with the major event. To capture specific
words, Titov and McDonald (Titov and McDonald,
2008) proposed a multi-grain topic model, relying
on word co-occurrences within short paragraphs and
Li et al (Li et al, 2010) proposed a entity-aspect
model based on word co-occurrences within single
sentences. Inspired by these ideas, we rely on word
co-occurrences within local period context to detec-
t mixed local and global aspects implicated in the
whole collection. We name this model as ?Mixture-
Event-Aspect (MEA)? model which can simultane-
ously detect local/global aspects and cluster sen-
tences and words into different aspects.
3.1.1 Model Description
Our mixture-event-aspect (MEA) model can be
extended from both the Probabilistic Latent Seman-
tic Analysis (PLSA) (Hofmann, 1999) and Laten-
t Dirichlet Allocation (LDA) (Blei et al, 2003). We
model two distinct types of aspects: global aspect-
s and local aspects, based on their relationship with
the major event. The distribution of global aspects is
fixed for the collection while the distribution of local
aspects is fixed to a local period of sub-collections.
That means a sentence is sampled either from the
mixture of the global aspects or from the local as-
pects specific for the local context. Here we take
the event ?Connecticut school shooting? as an exam-
ple. For the sentence ?On Sunday, President Obama
came to Connecticut to give a lecture, expressing his
sorrow for ... and calling for an end to such inci-
dents?, the words such as ?Obama?, ?lecture?, ?ex-
press? are only occurred for the local period of two
days and have no co-occurrence with other neigh-
boring period sentences, so we sample the sentence
as a local aspect sentence. But for the sentence ?All
schools in Newtown, the northeastern U.S. state of
Connecticut were in lockdown after a shooting was
reported at a local elementary school?, the word-
s such as ?Connecticut?, ?shooting?, ?elementary?
have high co-occurrence frequency in the whole col-
lection, so we sample the sentence as a global aspect
sentence.
To detect aspects, we first divide words into two
types: aspect words and background words. Back-
ground words are commonly used in the whole even-
t corpus while aspect words are clearly associat-
728
ed with the aspects of the sentences they occur in.
Stop words are removed using a standard stop word
list. In order to get the distribution of local aspect-
s, we implement a mechanism called ?Time Win-
dow? which covers Sp sequential time-based sub-
collections. We associate each time window with
a distribution over local aspects and a distribution
defining preference of local aspects versus global as-
pects.
draw ?B ? Dir(?), ?c(v) ? Dir(?), ? ? Dir(?)
draw ?gl ? Dir(?) for Agl times
draw ?loc ? Dir(?) for Aloc times
choose a distribution of global aspects ?gl ? Dir(?gl)
For each time window v in collection c
choose ?locc,v ? Dir(?loc)
choose ?c,v ? Beta(?mix)
For each sentence s in collection c
choose window ?c,s ? ?c
choose ?c,s ? ?c,?c,s
if ?c,s = gl, zc,s ? ?gl
if ?c,s = loc, zc,s ? ?locc,v
For each word w of sentence s in collection c
draw yc,s,n ?Multi(?)
draw wc,s,n ?Multi(?B) if yc,s,n = 1
draw wc,s,n ?Multi(?zc,s) if yc,s,n = 2
Figure 1: The Collection Generation Process
Formally, let C = {Ct|t = 1, 2, 3, ..., T} be
T time based sub-collections related to the even-
t subject, Ct represents the collection of sentences
which are assigned with the date t. Let v be a time
window containing Sp sequential sub-collections,
v = {Ct|t = i, i + 1, ..., i + Sp ? 1}. We draw
a background unigram language model which gen-
erates words for all sub-collections, and draw Agl
global aspect unigram language models for global
aspects andAloc word distributions for local aspects.
We assume these word distributions have a uniform
Dirichlet prior Dir(?). There is also a multinomi-
al distribution ? that controls in each sentence how
often the word occurs as a background word or an
aspect word. ? has a Dirichlet prior with parame-
ter ?. We assign each window v with an distribution
over local aspects and a distribution ? defining pref-
erence for local aspects versus global aspects. ? has
a Beta prior ?mix. A sentence can be sampled using
any window which is chosen according to a categor-
ical distribution.
In Figure 2 the corresponding graphical model is
presented. This model allows for fast approximate
inference with collapsed Gibbs sampling.
Figure 2: Mixture-Event-Aspect Model
Let SC denotes the number of sentences in col-
lection C, Nc,s denotes the number of words in sen-
tence s of collection c, and wc,s,n denotes the nth
word in sentence s. There are two kinds of hidden
variables: zc,s for each sentence to indicate the as-
pect a sentence belongs to, and yc,s,n for each word
to indicate whether a word is generated from the
background model or the aspect model.
3.1.2 Inference via Gibbs Sampling
In order to estimate the hidden parameter-
s in the model, we try to maximize distribution
p(z,y|w;?, ?, ?, ?), where z, y and w represent the
set of all z, y and w variables, respectively. Giv-
en a sentence s in the collection c, we apply Gibbs
Sampling to estimate the conditional probability for
local/global aspects using the following rules:
p(vc,s = vh, ?c,s = gl, zc,s = a|v
? , z? , y, w) ?
nch?+?
nc(.)+S
?
p?
?
nc,vhgl +?
mix
gl
nc,vh(.) +
?
r??gl,loc
?mix
r?
? n
c
gl,a+?
gl
ncgl+Agl?gl
?
?L
l=1
?E(l)?1
i=0 (C
a
(l)+i+?)
?E(.)?1i=0 (C
a
(.)+i+L?)
729
p(vc,s = vh, ?c,s = loc, zc,s = a|v
? , z? , y, w) ?
nch?+?
nc(.)+S
?
p?
? n
c,vh
loc +?
mix
loc
nc,vh(.) +
?
r??gl,loc
?mix
r?
?
nc,vhloc,a+?
loc
nc,vhloc +Aloc?loc
?
?L
l=1
?E(l)?1
i=0 (C
a
(l)+i+?)
?E(.)?1
i=0 (C
a
(.)+i+L?)
where nch? =#{si|vc,si = vi?sp+h?+1}, denotes
the number of times a sentence si in collection c is
assigned to its h?th window and for each sentence
si, we have h = i? Sp + 1 + h?. S?p is the number
of windows that contain sentence s. nc(.) denotes the
number of sentences in collection c. Sp represents
the number of dates a window covered. nc,vhgl and
nc,vhloc are the number of sentences in window vh that
are assigned to global or local aspects. nc,vh(.) is the
number of sentences assigned to window vh. ncgl,a
is the number of sentences in all global aspects that
are assigned to aspect a and nc,vhloc,a is the number of
local aspect sentences in the window vh are assigned
to aspect a. Agl is the number of global aspects in
collection C while Aloc is the number of local as-
pects. E(l) represents the number of times of word
l occurs in the current sentence and is assigned to
be an aspect word, while E(.) is the total number of
words in the current sentence that are assigned to be
an aspect word.
Then we compute the assignments of the consid-
ered word. We sample the hidden variables yc,s,n for
each word with the following rules:
p(yc,s,n = 1|z, y
?
) ?
Cpi(1) + ?
Cpi(?) + 2 ? ?
?
CB(wc,s,n) + ?
CB(?) + L ? ?
p(yc,s,n = 2|z, y
?
) ?
Cpi(2) + ?
Cpi(?) + 2 ? ?
?
Ca(wc,s,n) + ?
Ca(?) + L ? ?
where Cpi(1) and C
pi
(2) are the numbers of words as-
signed to be background words and aspect words.
Cpi(?) is the total number of words. C
B
(?) is the total
number of background words and Ca(?) is the number
of words assigned to aspect a.
By using the samples from Gibbs sampling, we
can effectively make the following estimations:
?Bw =
CBw+?
CB? +L??
, ?aw =
Caw+?
Caw+L??
, ?h? =
nch?+?
nc(.)+S
?
p?
?y =
Cpiy +?
Cpi? +2??
, ?c,v? =
nc,v? +?mix?
nc,v+
?
r?(gl,loc)
?mixr
?gla =
ncgl,a+?
gl
ncgl+Agl?gl
, ?loca =
nc,vloc,a+?
loc
nc,vloc+Aloc?loc
The hyper-parameters like ?, ?, ?, ? can be estimat-
ed using standard methods introduced in (Minka,
2000).
3.2 Bursty Period Detection
We borrow the definition of ?bursty? from (Lappas
et al, 2009) to measure the popularity of the event
on a certain date. Intuitively, each aspect have differ-
ent bursties on different dates. In this section, we try
to obtain the temporal aspect sequences of an event
based on the bursty periods of all the aspects. Dur-
ing its bursty period, one aspect should (1)be more
popular than other aspects (2) be continuously more
popular than other time. Following these intuitions,
we design a method to measure the bursty of each
aspect and get the bursty period.
Let Ak be the kth aspect obtained from the
mixture-event-aspect model, we estimate the bursty
of Ak at a certain date t as follows.
bursty(Ak,t) = p(t|Ak) =
p(Ak|t) ? p(t)
?
t?
p(Ak|t?)p(t?)
where p(Ak|t) is measured by the number of sen-
tences assigned to aspect Ak in date t divided by the
total number of sentences in date t. p(t) is estimated
by the total number of sentences in aspect Ak divid-
ed by the overall number of sentences in the collec-
tion C.
After getting the bursty of aspect Ak at each date,
we can find the most popular date and expand on
both sides to obtain the whole burst period in which
the bursties are higher than the neighboring aspects
and continuous higher than other dates.
3.3 Optimization-based Storyline Generation
With the methods discussed in previous sections, we
can get the local/global aspect sequence. Each as-
pect contains numbers of sentences and we are aim-
ing to select the most representative ones to compose
the final storyline. Considering users? bias and the
length requirement, different aspects should have d-
ifferent proportions in the last storyline. For glob-
al aspects which correspond more to users? interest,
they should share a larger proportion in the final sto-
ryline than local aspects. Thus, we use an optimiza-
tion method to determine if a sentence is selected to
be an summary sentence or to be discarded based on
the multiple local/global aspects and finally get the
optimal storyline. We formalize this problem as se-
lecting a subset of sentences S from the aspect Ak
730
to minimize the information loss.
argmin
?
Ak?C,S?Ak
?
z?Ak?S,s?S
O(z, s)
where O(z, s) is the cost function which measures
the cost of representing sentence z with sentence s.
Generally, this is an NP-hard problem (Cheung et
al., 2009) but we can use POPSTAR, an implemen-
tation of an approximate solution proposed by Re-
sende and Werneck (Resende and Werneck, 2004).
To model different costs between global or local
aspects and determine the proportions of differen-
t aspects in the final storyline, we utilize a func-
tion ?(s). When sentences z and s are local as-
pect sentences, ?(s) = ?, or, ?(s) = 1 ? ?. For-
mally, we incorporate two kinds of decreasing/in-
creasing logistic functions, ?1(x) = 1/(1 + ex) and
?2(x) = ex/(1 + ex), to define the cost function as
O(z, s) = ?(s) ? ?1(S(s)) ? ?2(S(z)) ?DKL(s, z)
where S(s) and S(z) are the ranking scores of sen-
tences s and z among the aspectAk with LexPageR-
ank algorithm. DKL(s, z) is used to measure the
similarity between sentence s and z with Kullback-
Leibler divergence here.
With this optimization method, we get the repre-
sentative sentences of each aspect for the given event
subject. Combining all the representative sentences
together based on the aspect sequence, we finally
generate the storyline.
4 Experiments and Evaluation
4.1 Datasets
To evaluate our framework for event storyline gen-
eration, we conduct our experiments on the dataset-
s amounting to 12418 articles for 6 event subjects
from 6 famous news websites, which provide date
edited by professional editors. Each article consists
of three parts, title, publish-time and news content.
Table 1 and Table 2 give the brief description of the
12418 articles. To generate reference summary, we
invite 12 undergraduate students with good English
ability to read the sentences, and for each event sub-
ject we ask two students to label human storylines.
4.2 Evaluation Metrics
We use the ROUGE1 (Lin and Hovy, 2003) (Recall
Oriented Understudy for Gisting Evaluation) toolkit
1http://www.isi/edu/licensed-sw/see/rouge/
Table 1: News sources of the 6 datasets
News Sources Number of Articles
CNN 2357
Fox News 1936
New York Times 2178
ABC 2113
Washington Post 1405
Xinhua 2429
Table 2: Event subjects of the datasets
Event Subjects Number of Articles
Connecticut school shooting 1792
The earthquake in Tokyo, Japan 2046
The U.S. presidential election 2573
Sandy hurricane attacked America 1827
American curiosity rover landed on Mars 1651
The 30th London Olympic Games 2529
to evaluate our framework, which has been widely
applied for summarization evaluation. It evaluates
the quality of a summary by counting the overlap-
ping units between the candidate summary and ref-
erence summaries. There are many kinds of ROUGE
metrics to measure the system-generated summa-
rization such as ROUGE-N, ROUGE-L, ROUGE-
W, and ROUGE-U, of which the most important one
is ROUGE-N with 3 sub-metrics: precision, recall,
and F-score.
ROUGE ? N =
?
S?RS
?
N?gram?S
Countmatch(N ? gram)
?
S?RS
?
N?gram?S
Count(N ? gram)
where RS represents the reference summaries. N-
gram?RS in the metrics denotes the N-grams in ref-
erence summaries. Countmatch(N ? gram) is the
maximum number of N-grams co-occurring in the
candidate summary and in the set of reference sum-
maries. Count(N ? gram) is the number of N-
grams in the reference summaries.
The ROUGE toolkit can report separate scores for
1, 2, 3, and 4-gram. In the experimental results we
report three ROUGE F-measure scores: ROUGE-
1, ROUGE-2, ROUGE-W metrics. The higher the
ROUGE scores, the better the summary is.
4.3 Algorithms for Comparison
Given a collection of news articles, we first decom-
pose them into sentences, and then assign each sen-
tence with a certain date, afterwards stop-words re-
moving and words stemming are performed. We
choose the following algorithms as baseline system-
s. Specifically, baseline 2 and 3 are summarization
731
systems which are similar to our storyline genera-
tion system. Then we choose baseline 4, 5 to evalu-
ate the effectiveness of the proposed method. It must
be said that all the systems are required to generate
the same number of summary words with the hu-
man reference. We conduct the same preprocessing
for all algorithms for fairness.
? Random : The method selects sentences ran-
domly from the sentence collection.
? LexPageRank (LexRank): This method applies
the graph-based multi-document summarization al-
gorithm which first constructs a sentence connec-
tivity graph based on the cosine similarity and then
chooses top-ranked sentences with PageRank.
? Chieu : This method was proposed by Chieu
(Chieu and Lee, 2004), utilizing interest and bursti-
ness to rank sentences, and choosing the top-ranked
query related sentences to construct the timeline.
? LDA+LexPageRank (LDALR) : This method
first applies standard LDA to detect latent topics
from the collection and clusters sentences to mul-
tiple aspects, then utilizes PageRank to generate the
most representative component summaries from all
the aspects.
? MEA+LexPageRank (MEALR) : This method
applies the proposed mixture-event-aspect model to
cluster sentences into multiple aspects and then uti-
lizes PageRank to generate the most representative
component summaries from all the aspects.
? MEA+Optimization (MEAOp) : This method
extracts local/global aspects with the proposed
mixture-event-aspect model, and then utilizes the
optimization method to get the qualified summary.
Figure 3: Overall performance for comparison
Table 3: Results of different systems on 6 subjects
Subject1 Subject2
Systems R-1 R-2 R-W R-1 R-2 R-W
Random 0.234 0.037 0.188 0.242 0.039 0.192
LexRank 0.317 0.045 0.257 0.326 0.051 0.262
Chieu 0.332 0.056 0.277 0.351 0.055 0.283
LDALR 0.356 0.069 0.297 0.369 0.066 0.327
MEALR 0.369 0.072 0.313 0.381 0.076 0.348
MEAOp 0.381 0.075 0.331 0.398 0.081 0.364
Subject3 Subject4
Systems R-1 R-2 R-W R-1 R-2 R-W
Random 0.258 0.042 0.191 0.234 0.036 0.179
LexRank 0.339 0.049 0.272 0.309 0.043 0.242
Chieu 0.364 0.059 0.296 0.329 0.053 0.264
LDALR 0.383 0.071 0.331 0.347 0.065 0.308
MEALR 0.396 0.076 0.3512 0.368 0.069 0.312
MEAOp 0.419 0.082 0.371 0.376 0.071 0.323
Subject5 Subject6
Systems R-1 R-2 R-W R-1 R-2 R-W
Random 0.222 0.034 0.166 0.264 0.045 0.195
LexRank 0.309 0.042 0.237 0.349 0.054 0.276
Chieu 0.319 0.049 0.258 0.371 0.062 0.293
LDALR 0.342 0.062 0.291 0.392 0.073 0.325
MEALR 0.372 0.068 0.299 0.406 0.079 0.349
MEAOp 0.384 0.070 0.309 0.427 0.087 0.368
4.4 Overall Performance Comparison
We experiment with all the baselines and our frame-
work on the 6 datasets. We take the average F-
score performance in terms of 3 ROUGE-F scores:
ROUGE-1, ROUGE-2 and ROUGE-SU4. The over-
all results are shown in Figure 3 and details are listed
in Tables 3.
Figure 3 and Table 3 show the performance of
these systems on the same datasets. The local/global
optimization balance parameter ? = 0.5. From Fig-
ure 3 and Table 3 we have following observations:
? Generally, the Random gets the worst perfor-
mance;
? The LexRank system outperforms Random al-
gorithm. This is due to the fact that LexRank ranks
all the sentences based on eigenvector centrality and
the global relationship between sentences, which
tends to select the most informative sentences as the
summary.
? The results of Chieu (Chieu and Lee, 2004) sys-
tem are better than those of LexRank. This may
be mainly for the reason that Chieu used the date
dimension to filter away uninteresting sentences by
paraphrasing and defined two different ranking mea-
sures: interest and burtiness, to select top-ranked in-
formative sentences.
? The LDALR system outperforms the Chieu sys-
732
tem. This may be for the fact that Chieu?s method is
actually based on flat clustering-based summariza-
tion, which is not as effective as LDA topic model
to extract latent sub-events.
Figure 4: Examine the performance of the balance pa-
rameter ?
? The MEALR system outperforms the LDAL-
R system. This may be mainly for the reason that
MEALR utilizes the mixture-event-aspect model to
detect the more salient sub-events based on the sub-
whole relationship, which seems to satisfy users?
bias to different sub-events.
? The MEAOp system which utilizes our method
outperforms all the baselines, indicating the effec-
tiveness of detecting different types of sub-events
with mixture-event-aspect model and the necessity
to distinguish different proportions of the compo-
nent summaries based on local/global aspects.
4.5 Parameter Tuning
Figure 5: Aspect sequence of event ?Connecticut school
shooting? (X-axis is the number of days after Dec. 14,
2012. Y-axis is bursty(Ak,t))
In this section, we compare the performance of
the parameters. The hyper-parameters such as ?, ?,
?, ? can be estimated using standard methods intro-
duced by Minka (Minka, 2000). So we mainly ex-
amine the local/global optimization balance parame-
ter ?. We try to evaluate the influence of this param-
eter on the three kinds of ROUGE measure results
respectively. Figure 4 shows the performance of the
balance parameters ?. It is obvious that when the
balance parameter ? is set to 0.7 this method per-
forms best.
4.6 Sample Output and Case Study
Figure 6: Bursties of sub-event ?Gun control debate? (X-
axis is the number of days after Dec. 14, 2012. Y-axis is
bursty(Ak,t))
We take the event ?Connecticut school shooting?
as an example to show the usefulness of our method.
Figure 5 shows the aspect sequence based on the
bursty periods of all aspects. We select a sub-
event ?Gun control debate? and Figure 6 shows the
bursties of this sub-event on the whole timeline. Ta-
ble 4 shows part of the storylines for the event ?Con-
necticut school shooting? generated by human and
our method. Through observation, we find that the
peak of the event ?Connecticut school shooting? is
around the date when it occurred, and the sub-event
?Gun control debate? has two bursty periods around
the two peaks. Compared with the human summary,
our framework can extract the important sub-events
contained in the collection, and satisfy users? inter-
est on different sub-events based on the part-whole
relationship with the event subject.
From the sample output and the human storylines,
we also get some observations. (1) The component
summary of global aspect tend to share larger pro-
733
Table 4: Selected part of storyline generated by MEAOp and human
Storyline Generated by human:
December 14, 2012 Global Aspect
Shooting massacre occurred in a primary school in Connecticut town, Sandy hoot Newton.
20-year-old Adam lanza broke into the primary school after shotting his mother, and in 10 minutes shot more than 100 times, killing twenty children
and eight adult, including himself.
The youngest death was a children in preschool students.
December 15, 2012 Global Aspect
Photos of the teachers and students shoot in the Connecticut massacre are released as well as the shooter?s.
The shooter was very smart but lonely.
December 16-17, 2012 Local Aspect
President Obama arrived in the locality of school shooting, mourned for the victims and made a speech.
December 18-20, 2012 Local Aspect
American gun control bill was put on the agenda again.
Storyline Generated by MEAOp:
December 14, 2012 Global Aspect
Children and adults gunned down in Connecticut school massacre.
20 children, six adults and the shooter are dead after shooting at Sandy Hoot Elementary School in Newtown, Connecticut.
Three law enforcement officials say Adam Lanza, 20, was the shooter, and that he died apparently by his own hand.
Suspect?s mother, Nancy Lanza, found dead in suspects home in Newtown, law enforcement source says.
Ryan Lanza, older brother of Adam Lanza, questioned by police but not labeled a suspect.
December 15-16, 2012 Global Aspect
Victims? names released Saturday; all of the slain children were either 6 or 7 years old.
Understanding school lockdowns in regards to Connecticut shooting.
Connecticut gunman recalled as intelligent but remote.
December 17, 2012 Local Aspect
President obama leads interfaith preyer vigil in newtown connecticut.
A tearful Obama says ?we?ve endured too many of these tragedies?.
December 18-20, 2012 Local Aspect
Moderate dems join gun control debate call for commission on us violence gains.
Gun debate gains traction as some lawmakers say its time to act.
portion in the final storyline. This is mainly for
the reason that when researching for an event sub-
ject, users bias more to the information about the
global-sub-events that have closely connection and
coincident properties with the major event based on
the part-whole relationship. So it is really neces-
sary to distinguish different sub-events with distinc-
tive properties. (2) Our system performs better for
the persistent event, such as ?The U.S. presidential
election?. This may be for the fact that these events
are usually long running and have more global-sub-
events than local-sub-events.
5 Conclusion
In this work, we study the task of event storyline
generation and present a novel method. We inno-
vatively introduce the properties of different sub-
events based on word co-occurrences to determine
the part-whole relationship with the major event and
develop a mixture-event-aspect (MEA) model to for-
malize different types of sub-events into local/global
aspects. Based on these local/global aspects, we uti-
lize an optimization method to get the optimal com-
ponent summaries along the aspect sequence. We
conduct experiments with our method and various
baselines on real web datasets. Through our exper-
iments we notice that our method generates over-
all better storyline than other baselines. This indi-
cates the effectiveness to detect different types of
sub-events with the proposed mixture-event-aspect
model and the necessity to distinguish different pro-
portions of the component summaries based on lo-
cal/global aspects.
Acknowledgments.
We thank the anonymous reviewers for their
valuable and constructive comments. This work
is financially supported by NSFC under the grant
NO.61272340 and 60933004.
References
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. the Journal of ma-
chine Learning research, 3:993?1022.
Jackie Chi Kit Cheung, Giuseppe Carenini, and Ray-
mond T Ng. 2009. Optimization-based content se-
734
lection for opinion summarization. In Proceedings of
the 2009 Workshop on Language Generation and Sum-
marisation, pages 7?14. ACL.
Hai Leong Chieu and Yoong Keok Lee. 2004. Query
based event extraction along a timeline. In Proceed-
ings of the 27th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 425?432. ACM.
G. Erkan and D.R. Radev. 2004. Lexpagerank: Prestige
in multi-document text summarization. In Proceed-
ings of EMNLP, volume 4.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proceedings of the Fifteenth conference
on Uncertainty in artificial intelligence, pages 289?
296. Morgan Kaufmann Publishers Inc.
Giridhar Kumaran and James Allan. 2004. Text clas-
sification and named entities for new event detection.
In Proceedings of the 27th annual international ACM
SIGIR conference on Research and development in in-
formation retrieval, pages 297?304. ACM.
Theodoros Lappas, Benjamin Arai, Manolis Platakis,
Dimitrios Kotsakos, and Dimitrios Gunopulos. 2009.
On burstiness-aware search for document sequences.
In Proceedings of the 15th ACM SIGKDD internation-
al conference on Knowledge discovery and data min-
ing, pages 477?486. ACM.
Peng Li, Jing Jiang, and Yinglin Wang. 2010. Gen-
erating templates of entity summaries with an entity-
aspect model and pattern mining. In Proceedings of
the 48th annual meeting of the Association for Com-
putational Linguistics, pages 640?649. ACL.
C.Y. Lin and E. Hovy. 2003. Automatic evaluation
of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 71?78. ACL.
Chen Lin, Chun Lin, Jingxuan Li, Dingding Wang, Yang
Chen, and Tao Li. 2012. Generating event storylines
from microblogs. In Proceedings of the 21st ACM in-
ternational conference on Information and knowledge
management, pages 175?184. ACM.
Juha Makkonen, Helena Ahonen-Myka, and Marko
Salmenkivi. 2004. Simple semantics in topic detec-
tion and tracking. Information Retrieval, 7(3-4):347?
368.
Qiaozhu Mei and ChengXiang Zhai. 2005. Discover-
ing evolutionary theme patterns from text: an explo-
ration of temporal text mining. In Proceedings of the
eleventh ACM SIGKDD international conference on
Knowledge discovery in data mining, pages 198?207.
ACM.
Q. Mei, J. Guo, and D. Radev. 2010. Divrank: the inter-
play of prestige and diversity in information networks.
In Proceedings of the 16th ACM SIGKDD internation-
al conference on Knowledge discovery and data min-
ing, pages 1009?1018. ACM.
Thomas Minka. 2000. Estimating a dirichlet distribu-
tion.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Ter-
ry Winograd. 1999. The pagerank citation ranking:
bringing order to the web.
Yulong Pei, Wenpeng Yin, et al 2012. Generic multi-
document summarization using topic-oriented infor-
mation. In PRICAI 2012: Trends in Artificial Intel-
ligence, pages 435?446. Springer.
D.R. Radev, H. Jing, M. Stys?, and D. Tam. 2004.
Centroid-based summarization of multiple documents.
Information Processing & Management, 40(6):919?
938.
Mauricio GC Resende and Renato F Werneck. 2004. A
hybrid heuristic for the p-median problem. Journal of
heuristics, 10(1):59?88.
Ivan Titov and Ryan McDonald. 2008. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of the 17th international conference on World
Wide Web, pages 111?120. ACM.
X. Wan. 2008. Document-based hits model for multi-
document summarization. PRICAI 2008: Trends in
Artificial Intelligence, pages 454?465.
Xuanhui Wang, ChengXiang Zhai, Xiao Hu, and Richard
Sproat. 2007. Mining correlated bursty topic pat-
terns from coordinated text streams. In Proceedings
of the 13th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 784?
793. ACM.
Xiang Wang, Kai Zhang, Xiaoming Jin, and Dou Shen.
2009. Mining common topics from multiple asyn-
chronous text streams. In Proceedings of the Second
ACM International Conference on Web Search and
Data Mining, pages 192?201. ACM.
Dingding Wang, Tao Li, and Mitsunori Ogihara. 2012.
Generating pictorial storylines via minimum-weight
connected dominating set approximation in multi-view
graphs. Proceddings of AAAI 2012.
Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan, X-
iaoming Li, and Yan Zhang. 2011a. Timeline gener-
ation through evolutionary trans-temporal summariza-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 433?
443. ACL.
Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011b. Evolution-
ary timeline summarization: a balanced optimization
framework via iterative substitution. In Proceedings
of the 34th international ACM SIGIR conference on
Research and development in Information Retrieval,
pages 745?754. ACM.
735
