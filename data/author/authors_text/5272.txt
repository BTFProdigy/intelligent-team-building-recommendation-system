Modelling Grounding 
Co l in  Matheson  
University of Edinburgh 
Edinburgh,  Scotland 
colin, mat  heson @ed. ac. uk 
and Discourse Obligations Using Update 
Rules 
Mass imo Poes io  
University of Edinburgh 
Edinburgh, Scotland 
massimo.poesio@ed.ac.uk 
Dav id  T raum 
University of Mary land 
Maryland,  USA 
t raum@cs.umd.edu 
Abst rac t  
This paper describes an implementation f some key 
aspects of a theory of dialogue processing whose 
main concerns are to provide models of GROUND- 
ING and of the role of DISCOURSE OBLIGATIONS in 
an agent's deliberation processes. Our system uses 
the TrindiKit dialogue move engine toolkit, which 
assumes a model of dialogue in which a participan. 
t's knowledge is characterised in terms of INFORMA- 
TION STATES which are subject o various kinds of 
updating mechanisms. 
1 I n t roduct ion  
In this paper we describe a preliminary implemen- 
tation of a 'middle-level' dialogue management sys- 
tem. The key tasks of a dialogue manager are to 
update the representation f dialogue on the basis of 
processed input (generally, but not exclusively, lan- 
guage utterances), and to decide what (if anything) 
the system should do next. There is a wide range of 
opinions concerning how these tasks should be per- 
formed, and in particular, how the ongoing dialogue 
state should be represented: e.g., as something very 
specific to a particular domain, or according to some 
more general theory of (human or human inspired) 
dialogue processing. At one extreme, some systems 
represent only the (typically very rigid) transitions 
possible in a perceived ialogue for the given task, 
often using finite states in a transition etwork to 
represent the dialogue: examples of this are sys- 
tems built using Nuance's DialogueBuilder or the 
CSLU's Rapid Application Prototyper. The other 
extreme is to build the dialogue processing theory on 
top of a full model of rational agency (e.g., (Bretier 
and Sadek, 1996)). The approach we take here lies 
in between these two extremes: we use rich repre- 
sentations of information states, but simpler, more 
dialogue-specific deliberation methods, rather than 
a deductive reasoner working on the basis of an ax- 
iomatic theory of rational agency. We show in this 
paper that the theory of information states we pro- 
pose can, nevertheless, beused to give a character- 
isation of dialogue acts such as those proposed by 
the Discourse Resource Initiative precise nough to 
formalise the deliberation process of a dialogue man- 
ager in a completely declarative fashion. 
Our implementation is based on the approach to 
dialogue developed in (Traum, 1994; Poesio and 
Traum, 1997; Poesio and Traum, 1998; Traum et al, 
1999). This theory, like other action-based theories 
of dialogue, views dialogue participation i terms of 
agents performing dialogue acts, the effects of which 
are to update the information state of the partici- 
pants in a dialogue. However, our view of dialogue 
act effects is closer in some respects to that of (All- 
wood, 1976; Allwood, 1994) and (Singh, 1998) than 
to the belief and intention model of (Sadek, 1991; 
Grosz and Sidner, 1990; Cohen and Levesque, 1990). 
Particular emphasis i placed on the social commit- 
ments of the dialogue participants (obligations to 
act and commitments to propositions) without mak- 
ing explicit claims about the actual beliefs and in- 
tentions of the participants. Also, heavy empha- 
sis is placed on how dialogue participants socially 
GROUND (Clark and Wilkes-Gibbs, 1986) the infor- 
mation expressed in dialogue: the information state 
assumed in this theory specifies which information is 
assumed to be already part of the common ground at 
a given point, and which part has been introduced, 
but not yet been established. 
The rest of this paper is structured as follows. The 
theory of dialogue underlying the implementation is 
described in more detail in Section 2. Section 3 de- 
scribes the implementation itself. Section 4 shows 
how the system updates its information state while 
participating in a fairly simple dialogue. 
2 Theoret i ca l  Background 
One basic assumption underlying this work is that 
it is useful to analyse dialogues by describing the 
relevant 'information' that is available to each par- 
ticipant. The notion of INFORMATION STATE (IS) is 
therefore mployed in deciding what the next action 
should be, and the effects of utterances are described 
in terms of the changes they bring about in ISs. A 
particular instantiation ofa dialogue manager, from 
this point of view, consists of a definition of the con- 
tents of ISs plus a description of the update processes 
which map from IS to IS. Updates are typically trig- 
gered by 'full' dialogue acts such as assertions or 
directives, 1 of course, but the theory allows parts of 
utterances, including individual words and even sub- 
parts of words, to be the trigger. The update rules 
for dialogue acts that we assume here are a simpli- 
fied version of the formalisations proposed in (Poesio 
and Traum, 1998; Traum et al, 1999) (henceforth, 
PTT). 
The main aspects of PTT which have been im- 
plemented concern the way discourse obligations are 
handled and the manner in which dialogue partic- 
ipants interact o add information to the common 
ground. Obligations are essentially social in nature, 
and directly characterise poken dialogue; a typical 
example of a discourse obligation concerns the rela- 
tionship between questions and answers. Poesio and 
Traum follow (Traum and Allen, 1994) in suggesting 
that the utterance of a question imposes an obliga- 
tion on the hearer to address the question (e.g., by 
providing an answer), irrespective of intentions. 
As for the process by which common ground is es- 
tablished, or GROUNDING (Clark and Schaefer, 1989; 
Traum, 1994), the assumption i PTT is that classi- 
cal speech act theory is inherently too simplistic in 
that it ignores the fact that co-operative interaction 
is essential in discourse; thus, for instance, simply as- 
serting something does not make it become mutually 
'known' (part of the common ground). It is actually 
necessary for the hearer to provide some kind of ac- 
knowledgement that the assertion has been received, 
understood or not understood, accepted or rejected, 
and so on. Poesio and Traum view the public in- 
formation state as including both material that has 
already been grounded, indicated by GND here, and 
material that hasn't been grounded yet. These com- 
ponents of the information state are updated when 
GROUNDING ACTS such as acknowledgement areper- 
formed. Each new contribution results in a new DIS- 
COURSE UNIT (DU) being added to the information 
state (Traum, 1994) and recorded in a list of 'un- 
grounded iscourse units' (UDUS); these DUs can 
then be subsequently grounded as the result, e.g., of 
(implicit or explicit) acknowledgements. 
3 Imp lement ing  PTT  
In this section, we describe the details of the im- 
plementation. First, in Section 3.1, we describe the 
TrindiKit tool for building dialogue managers that 
we used to build our system. In Section 3.2, we de- 
scribe the information states used in the implemen- 
tation, an extension and simplification of the ideas 
from PTT discussed in the previous ection. Then, 
in Section 3.3, we discuss how the information state 
is updated when dialogue acts are observed. Finally, 
1We assume here the DRI classification f dialogue acts 
(Discourse Resource Initiative, 1997). 
/ . ' ? "  .. -,. ' " . . ,  
I.lol'lP.lllit~ ~;l{lle (i$) 
'E 1 
Figure 1: TrindiKit Architecture 
in Section 3.4, we describe the rules used by the sys- 
tem to adopt intentions and perform its own actions. 
An extended example of how these mechanisms are 
used to track and participate in a dialogue is pre- 
sented in Section 4. 
3.1 TrindiKit 
The basis for our implementation is the TrindiKit 
dialogue move engine toolkit implemented as part 
of the TRINDI project (Larsson et al, 1999). The 
toolkit provides upport for developing dialogue sys- 
tems, focusing on the central dialogue management 
components. 
The system architecture assumed by the TrindiKit 
is shown in Figure 1. A prominent feature of this ar- 
chitecture is the information state, which serves as a 
central 'blackboard' that processing modules can ex- 
amine (by means of defined CONDITIONS) or change 
(by means of defined OPERATIONS). The structure 
of the IS for a particular dialogue system is defined' 
by the developer who uses the TrindiKit to build 
that system, on the basis of his/her own theory of 
dialogue processing; no predefined notion of infor- 
mation state is provided. 2 The toolkit provides a 
number of abstract data-types such as lists, stacks, 
and records, along with associated conditions and 
operations, that can be used to implement the user's 
theory of information states; other abstract ypes 
can also be defined. In addition to this customis- 
able notion of information state, TrindiKit provides 
a few system variables that can also used for inter- 
module communication. These include input for the 
raw observed (language) input, latest_moves which 
2In TRINDI we are experimenting with multiple instanti- 
ations of three different theories of information state (Traum 
et al, 1999). 
2 
contains the dialogue moves observed in the most 
recent urn, la tes t_speaker ,  and next_moves, con- 
taining the dialogue moves to be performed by the 
system in the next turn. 
A complete system is assumed to consist of sev- 
eral modules interacting via the IS. (See Figure 1 
again.) The central component is called the DIA- 
LOGUE MOVE ENGINE (DME). The DME performs 
the processing needed to integrate the observed i- 
alogue moves with the IS, and to select new moves 
for the system to perform. These two functions are 
encapsulated in the UPDATE and SELECTION sub- 
modules of the DME. The update and select mod- 
ules are specified by means of typed rules, as well as 
sequencing procedures to determine when to apply 
the rules. We are here mainly concerned with UP- 
DATE RULES (urules) ,  which consist of four parts: a 
name, a type, a list of conditions to check in the in- 
formation state, and a list of operations to perform 
on the information state, u ru les  are described in 
more detail below, in Section 3.3. There are also 
two modules outside the DME proper, but still cru- 
cial to a complete system: INTERPRETATION, which 
consumes the input and produces a list of dialogue 
acts in the latest_moves variable (potentially mak- 
ing reference to the current information state), and 
GENERATION, which produces NL output from the 
dialogue acts in the next_moves variable. Finally, 
there is a CONTROL module, that governs the se- 
quencing (or parallel invocation) of the other mod- 
ules. In this paper we focus on the IS and the DME; 
our current implementation only uses very simple 
interpretation and generation components. 
3.2 Information States in PTT  
In this section we discuss the information state used 
in the current implementation. The main difference 
between the implemented IS and the theoretical pro- 
posal in (Poesio and Traum, 1998) is that in the im- 
plementation the information state is partitioned in 
fields, each containing information of different ypes, 
whereas in the theoretical version the information 
state is a single repository of facts (a DISCOURSE 
REPRESENTATION STRUCTURE). Other differences 
are discussed below. An example IS with some fields 
filled is shown in Figure 2; this is the IS which results 
from the second utterance in the example dialogue 
discussed in Section 4, A route please. 3
The IS in Figure 2 is a record with two main 
parts, W and C. The first of these represents the 
system's (Wizard) view of his own mental state and 
of the (semi-)public information discussed in the di- 
alogue; the second, his view of the user's (Caller) 
information state. This second part is needed to 
3All diagrams in this paper are automatically generated 
from TrindiKit system internal representations and displayed 
using the Thistle dialogue editor (Calder, 1998). Some have 
been subsequently edited for brevity and clarity. 
r \] understandingAct( W,DU3 )\ 1 ~1 
\[OBL: ~lddre~(C,CA2 ) \] / // 
~. .  / . .  /CAS:C2 ,~..,~g.(C.DU2) \ /  I I  
. . . . .  / .... \c^:: ' / /  I I  
/ sc : < " ! H 
\[COND: < > J H 
UDUS: <DU3> H 
\[ \[OBL: <,ddri~z(C.CA2 ) > \ ] \ ]  H 
DH: <CA2: C2. into requi~t( W.?help fore1 ):> 
/ / LCOND: < > J/ If 
: | LID: DU2 J I I  
/ \[ lilt / / / /CA5: C2. dil c,(C ivemule(W)  \ / /H 
/ / /DH: (CAS: C2. a,swer( C.CA2.CA4 ) ) //11 
ko,, //11 
I / IIII / \[ LCOND: <IICIpt(W.CA6)-> obl(W~iveroutc(W))>J III 
/ LID: DU3 JII 
1 /,,,,o_,..,,,.,( w.:,,.-, )\ I I  
/ ; l i~oute(W ) 
t ~,,*~,~,d~(W.bU3)/ JI 
lINT: <letrome(C)>\] J 
Figure 2: Structure of Information States 
model misunderstandings arising from the dialogue 
participants having differing views on what has been 
grounded; as we are not concerned with this problem 
here, we will ignore C in what follows. 
w contains information on the grounded mate- 
rial (GND), on the ungrounded information (UDUS, 
PDU and CDU), and on W's intentions (INT). GND 
contains the information that has already been 
grounded; the other fields contain information about 
the contributions still to be grounded. As noticed 
above, in PTT  it is assumed that for each new ut- 
terance, a new DU is created and added to the IS. 
The current implementation differs from the full the- 
ory in that only two DUs are retained at each point; 
the current DU (CDU) and the previous DU (PDU). 
The CDU contains the information in the latest con- 
tribution, while the PDU contains information from 
the penultimate contribution. Information is moved 
f rom PDU to GND as a result of an ack (acknowl- 
edgement) dialogue act (see below.) 
The DUs and the GND field contain four fields, 
representing obligations (OBL), the dialogue history 
(DH), propositions to which agents are socially com- 
mitted (scP),  and conditional updates (COND). The 
value of OBL is a list of action types: actions that 
agents are obliged to perform. An action type is 
specified by a PREDICATE, a DIALOGUE PARTICI- 
PANT, and a list of ARGUMENTS. The value of see 
is a list of a particular type of mental states, so- 
cial commitments of agents to propositions. 4 These 
are specified by a DIALOGUE PARTICIPANT, and a 
PROPOSITION. Finally, the elements in DH are dia- 
4SCPs play much the same role in PTT as do beliefs in 
many BDI accounts of speech acts. 
3 
logue actions, which are instances of dialogue action 
types. A dialogue action is specified by an action 
type, a dialogue act id, and a confidence level CONF 
(the confidence that an agent has that that dialogue 
act has been observed). 
The situation in Figure 2 is the result of updates to 
the IS caused by utterance \[2\] in the dialogue in (6), 
which is assumed to generate a d i rect  act as well as 
an assert  act and an answer  act. 5 That utterance 
is also assumed to contain an implicit acknowledge- 
ment of the original question; this understanding act 
has resulted in the contents of DU2 being grounded 
(and subsequently merged with GND), as discussed 
below. 
GND.OBL in Figure 2 includes two obligations. 
The first is an obligation on W to perform an under- 
standing act (the predicate is unders tand ingAct ,  
the participant is W, and there is just one argument, 
DU3, which identifies the DU in CDU by referring to 
its ID). The second obligation is an obligation on C 
to address  conversational ct CA2; this ID points 
to the appropriate info_request  in the DH list by 
means of the ID number. Obligations are specified 
in CDU and PDU, as well. Those in PDU are simply 
a subset of those in GND, since at point in the up- 
date process shown in Figure 2 this field contains 
information that has already been grounded (note 
that DU2 is not in UDUS anymore); but CDU con- 
tains obligations that have not been grounded yet - 
in particular, the obligation on W to address  CA6. 
GND.DH in this IS contains a list of dialogue ac- 
tions whose occurrence has already been grounded: 
the info_request  performed by utterance 1, with ar- 
gument a question, 6 and the implicit acknowledge 
performed by utterance 2. 7 The DH field in CDU con- 
tains dialogue acts performed by utterance 2 that do 
need to be grounded: a directive by C to W to per- 
form an action of type g iveroute,  and an assert  
by C of the proposition want(C,  route), by which C 
provides an answer  to the previous info_request 
CA2. 
The COND field in CDU contains a conditional up- 
date resulting from the directive performed by that 
utterance. The idea is that directives do not imme- 
diately lead to obligations to perform the mentioned 
action: instead (in addition to an obligation to ad- 
dress the action with some sort of response), their ef- 
fect is to add to the common ground the information 
that if the directive is accepted  by the addressee, 
SThe fact that the utterance of a route please constitutes 
an answer is explicitly assumed; however, it should be possible 
to derive this information automatically (perhaps along the 
lines suggested by Kreutel (Kreutel, 1998)). 
6We use the notation ?p to indicate a question of the form 
?(\[x\],p(x)). 
7We assume here, as in (Traum, 1994) and (Poesio and 
Traum, 1998), that understanding acts do not have to be 
grounded themselves, which would result in a infinite regress. 
then he or she has the obligation to perform the ac- 
tion type requested. (In this case, to give a route to 
C.) 
3.3 Update  Rules  in PTT  
We are now in a position to examine the update 
mechanisms which are performed when new dia- 
logue acts are recognised. When a dialogue par- 
ticipant takes a turn and produces an utterance, 
the interpretation module sets the system variable 
latest_moves to contain a representation f the di- 
alogue acts performed with the utterance. The up- 
dating procedure then uses update rules to modify 
the IS on the basis of the contents of latest_moves 
and of the previous IS. The basic procedure is de- 
scribed in (1) below, s
(1) 1. Create a new DU and push it on top of 
UDUs. 
2. Perform updates on the basis of backwards 
grounding acts. 
. If any other type of act is observed, record 
it in the dialogue history in CDU and apply 
the update rules for this kind of act 
4. Apply update rules to all parts of the IS 
which contain newly added acts. 
The first step involves moving the contents of CDU 
to PDU (losing direct access to the former PDU con- 
tents) and putting in CDU a new empty DU with 
a new identifier. The second and third steps deal 
explicitly with the contents of la test .moves,  ap- 
plying one urule (of possibly a larger set) for each 
act in latest_moves. The relevant effects for each 
act are summarised in (2), where the variables have 
the following types: 
IDx 
DUx 
DP 
q 
PROP 
Act 
o(DP) 
P(ID) 
Q(ID) 
Dialogue Act Identification Number 
DU Identification Number 
Dialogue Participant (i.e., the speaker) 
A Question 
A Proposition 
An Action 
The other dialogue participant 
The content of the ID, a proposition 
The content of the ID, a question 
SSee (Poesio et al, 1999; Traum et al, 1999) for different 
versions of this update procedure used for slightly different 
versions of the theory. 
4 
(2) act ID:2, accept (DP, ID2) 
effect accomplished via rule resolution 
act ID:2, ack(DP, DU1) 
effect peRec(w.Gnd,w.pdu.tognd) 
effect remove(DU1,UDUS) 
act ID:2, agree(DP, ID2) 
effect push(scP,scp(DP,P(ID2))) 
act ID:2, answer(DP,ID2,ID3) 
effect push(scP,ans(DP, Q(ID2),P(ID2))) 
act ID:2, assert (DP,PROP) 
effect push(scP,sep(DP, PROP)) 
effect push (COND,accept (o(DP),ID)-+ 
scp(o(DP),PROP)) 
act ID:I, assert(DP,PROP) 
effect push (COND,accept (o(DP),ID)-~ 
scp(o(DP),PROP)) 
act ID:2, check(DP,PROP) 
effect push(OSL,address(o(DP),ID)) 
effect push(COND,agree(o(DP),ID) --~ 
scp(DP, PROP)) 
act ID:2, direct (DP, Act) 
effect push(OBL,address(o(DP),ID)) 
effect push(CONI),accept (o(DP),ID) -~ 
obl(o(DP),Act)) 
act ID:2, info_request (DP, Q) 
effect push(osL,address(o(DP),ID)) 
The ack act is the only backward grounding act 
implemented at the moment. The main effect of an 
ack is to merge the information i the acknowledged 
DU (assumed to be PDU) into GND, also removing 
this DU from UDUS. Unlike the other acts described 
below, ack acts are recorded irectly into GND.DH, 
rather than into CDU.TOGND.DH. 
All of the other updates are performed in the third 
step of the procedure in (1). The only effect of ac- 
cept acts is to enable the conditional rules which 
are part of the effect of assert and direct, leading 
to social commitments and obligations, respectively. 
agree acts also trigger conditional rules introduced 
by check; in addition, they result in the agent be- 
ing socially committed to the proposition i troduced 
by the act with which the agent agrees. Perform- 
ing an answer to question ID2 by asserting propo- 
sition P(ID3) commits the dialogue participant to 
the proposition that P(ID3) is indeed an answer to 
Q(ID2). 
The two rules for assert are where the confidence 
levels are actually used, to implement a simple ver- 
ification strategy. The idea is that the system only 
assumes that the user is committed to the asserted 
proposition when a confidence l vel of 2 is observed, 
while some asserts are assumed not to have been 
sufficiently well understood, and are only assigned a 
confidence l vel 1. This leads the system to perform 
a check, as we will see shortly. 
The next three update rules, for check, direct, 
and info_req, all impose an obligation on the other 
dialogue participant to address the dialogue act. In 
addition, the direct rule introduces a conditional 
act: acceptance of the directive will impose an obli- 
gation on the hearer to act on its contents. 
In addition, all FORWARD ACTS 9 in the DRI 
scheme (Discourse Resource Initiative, 1997) impose 
an obligation to perform an understanding act (e.g., 
an acknowledgement): 
(3) 1 act 
effect 
ID:c, forward-looking-act (DP) 
push(OBL,u-act (o(DP),CDU.id)) I 
The internal urules implementing the updates in 
(2) have the format shown in (4), which is the urule 
for info_request. 
(4) =uxe( doZnfoR, q. rulet~.S, 
\[ hearer(DP), 
latest_moves: in(Hove}, 
Move:valEec(pred,inforeq) \],  
\[ incr_set(update_cycles,_), 
incr_set (next.dh_id, HID), 
next _du_name (ID), 
pushRec (w'cdu'tognd'dh, 
record ( \[atype=Move, c level=2, id=HID \ ] ) ) ,  
pushRec (e'cdu~tosnd'obl, 
record ( \[pred~address, dp=DP, 
argsfstackset ( 
\[record ( \[i%em=IIID\] )\] ) \] ) ), 
pushRec (w'gnd" obl, 
record ( \[pred=uact, dp=P, 
args=stackset ( 
\[rocord(\[item=ID\] ) \] ) \] )) \ ] ) .  
As noted above, these rules have four parts; a 
name, a type, a list of conditions, and a list of ef- 
fects. The conditions in (4) state that there must be 
a move in latest_moves whose predicate is inforeq. 
The effects l? state that the move should be recorded 
in the dialogue history in CDU, that an obligation to 
address the request should be pushed into OBL in 
CDU, and that the requirement for an understand- 
ing act by W should be pushed irectly into the list 
in W.GND. 
The fourth and final step of the algorithm cycles 
through the updating process in case recently added 
facts have further implications. For instance, when 
an action has been performed that matches the an- 
tecedent of a rule in COND, the consequent is es- 
tablished. Likewise, when an action is performed 
it releases any obligations to perform that action. 
Thus, accept, answer, and agree are all ways of 
releasing an obligation to address, since these are 
all appropriate backward looking actions. Similarly, 
an agent will drop intentions to perform actions it 
has already (successfully) performed. 
3.4 Deliberation 
We assume, in common with BDI-approaches to 
agency (e.g., (Bratman et al, 1988)) that intentions 
9Forward acts include assert, check, direct, and 
info_request. 
l?The ID and HID values simply contain numbers identifying 
the discourse units and conversational acts. 
5 
are the primary mental attitude leading to an agen- 
t's actions. The main issues to explain then become 
how such intentions are adopted given the rest of 
the information state, and how an agent gets from 
intentions to actual performance. 
For the latter question, we take a fairly simplistic 
approach here: all the intentions to perform dia- 
logue acts are simply transferred to the next_moves 
system variable, with the assumption that the gen- 
eration module can realise all of them as a single ut- 
terance. A more sophisticated approach would be to 
weight the importance of (immediate) realisation of 
sets of intentions and compare this to the likelihood 
that particular utterances will achieve these effects 
at minimal cost, and choose accordingly. We leave 
this for future work (see (Traum and Dillenbourg, 
1998) for some preliminary ideas along these lines), 
concentrating here on the first issue - how the sys- 
tem adopts intentions to perform dialogue acts from 
other aspects of the mental state. 
The current system takes the following factors into 
account: 
? obligations (to perform understanding acts, to 
address previous dialogue acts, to perform other 
actions) 
? potential obligations (that would result if an- 
other act were performed, as represented in the 
COND field) 
? insufficiently understood ialogue acts (with a 
1 confidence level in CDU.DH) 
? intentions to perform complex acts 
The current deliberation process assumes maxi- 
mal cooperativity, in that the system always chooses 
to meet its obligations whenever possible, and also 
chooses to provide a maximally helpful response 
when possible. Thus, when obliged to address  a 
previous dialogue act such as a question or direc- 
tive, it will choose to actually return the answer or 
perform the action, if possible, rather than reject or 
negotiate such a performance, which would also be 
acting in accordance with the obligations (see (Kreu- 
tel, 1998) on how acts might be rejected). 
In the current implementation, the following rules 
are used to adopt new intentions (i.e., to update the 
INT field): 
(5) 1. add an intention to acknowl- 
edge(W,CDU), given an obligation to 
perform a u-act,  if everything in CDU is 
sufficiently understood (i.e., to level 2); 
2. add an intention to accept a directive or an- 
swer a question as the result of an obligation 
to address a dialogue act; 
3. add an intention to perform an action if 
COND contains a conditional that will estab- 
lish an obligation to perform the action, and 
the antecedent of this conditional is another 
action that is already intended. (This an- 
ticipatory planning allows the obligation to 
be discharged at the same time it is invoked, 
e.g., without giving an intermediate accep- 
tance of an directive.) 
4. add an intention to perform a (dialogue) ac- 
tion motivated by the intention to perform 
the current task. In the case of the Au- 
toroute domain, we have two cases: the sys- 
tem may decide 
(a) to check any dialogue acts in CDU at 
confidence level 1, which contain infor- 
mation needed to discharge the intention 
to give a route; or 
(b) to perform a question asking about a new 
piece of information that has not been es- 
tablished (this is decided by inspecting 
GND.SCP and CDU.SCP). For example, 
it may decide to ask about the starting 
point, the time of departure, etc. 
4 Extended Example  
In this section, we discuss more examples of how the 
information state changes as a result of processing 
and performing dialogue acts. It is useful to do this 
by looking briefly at a typical Autoroute dialogue, 
shown in (6). 11 Our implementation can process this 
sort of dialogue using very simple interpretation and 
generation routines that provide the dialogue acts 
in latest_moves from the text strings, and produce 
W's output text from the dialogue acts which the 
system places in next_moves. 
(6) W \[1\]: How can I help? 
C \[2\]: A route please 
W \[3\]: Where would you like to start? 
C \[4\]: Malvern 
W \[5\]: Great Malvern? 
C \[6\]: Yes 
W \[7\]: Where do you want to go? 
C \[8\]: Edwinstowe 
W \[9\]: Edwinstowe in Nottingham? 
C \[10\]: Yes 
W \[11\]: When do you want to leave? 
C \[12\]: Six pm 
W \[13\]: Leaving at 6 p.m.? 
C \[14\]: Yes 
W \[15\]: Do you want the quickest or the 
shortest route? 
C \[16\]: Quickest 
W \[17\]: Please wait while your route is cal- 
culated. 
We assume that before the dialogue starts, W has 
the intention to ask C what kind of help is required, 
liThe interchanges have been cleaned up to some extent 
here, mainly by removing pauses and hesitations. 
6 
W: 
I \[ /gi . . . . .  ,e(W, \ I OBL: ~understandlngAet(W,DU5)) / \address(C,CA8 ) \[ \[ / CA I0: C2, acknowledge(C.DU4 ) \ iGND: /DH: (CAg: C2, accept( W.CA6 ) ~/ / SCP: < ? 
\[COND: < > 
UDUS: <DU5> 
\[ /CA9:C2 . . . .  pt(W,CA6) \ / /  
TOGND: OH: \CAS: C2. info request( W,?start ) I l l  PDU: / /  
LCOND: < > 
LID: DU4 
DH: ~/CA 12: C2, a~wer( C,CA8,CAI 1 ) 
CDU: TOGND: \CA 11: Cl, assert( C.s 'tart(malvem) ) 
SCP: < > 
LCOND: < > 
LID: DU5 
/ check (W.,start(malvern ) ) \
INT: ~acknowledge( W.DU5 ) 
\giveroute( W ) / 
I INT: <getroute( C ) ? l 
Figure 3: Information State Prompting Check in \[5\] 
and that C has the intention to find a route. We also 
assume that W has the turn, and that the presence 
of the how can I help intention triggers an utterance 
directly. Figure 2, presented above, shows the in- 
formation state after utterance \[2\]. The intentions 
in that figure lead directly to the system producing 
utterance \[3\]. 
Looking a little further ahead in the dialogue, Fig- 
ure 3 shows the information state after utterance 
\[4\]. 12 Here we can see in CDU.TOGND.DH (along with 
the ack act CA10, in GND.DH) the dialogue moves 
that this utterance has generated. Note that the as- 
sert, CA l l ,  is only at confidence level 1, indicating 
lack of sufficient certainty in this interpretation as 
the town 'Great Malvern'. This lack of certainty and 
the resulting lack of a relevant SCP in CDU.TOGND 
lead the deliberation routines to produce an inten- 
tion to check this proposition rather than to move 
directly on to another information request. This 
intention leads to utterance \[5\], which, after inter- 
pretation and updating on the new dialogue acts, 
leads to the information state in Figure 4. The in- 
teresting thing here is the condition which appears 
in CDU.TOGND.COND as a result of the check; the 
interpretation of this is that, if C agrees with the 
check, then W will be committed to the proposition 
that the starting place is Malvern (C would also be 
committed to this by way of the direct effects of an 
agree act). 
12The actual information state contains all the previously 
established ialogue acts, SCPs and Obligations in GND~ from 
Figure 2 and intermediate utterances. Here we have deleted 
these aspects from the figures for brevity and clarity. 
OBL: \glveroute(W ) / \[ 
/CAI3: C2. acknowledge( W.DU5 ) \ \ [  
GND: /DH: ~ CA 12: C2, answer(C.CA8 ) }/  / \CA 11 :C 1, assert(C.start(malvem) ) / \ [SCP: < ? 
LCOND: < > J 
UDUS: <DU6> 
IOBL: < ? 
W: TOGND: DFt: \CA i i: Ci, as~rt(C,s~t(malvem) ) i i i  
PDU: SCP: < > 
LID: DU5 
r r?BL: <address(C.CAl4)> 
\]TOGND: \[DH: <CA 14: C2, check (W.,smrt(rnalvern)) > 
CDU: / /SOP: < > 
LCOND: <agree(C,CAl4 \] .> scp(W,start(malvern) }; 
L In: DU6 
INT: ,:giveroute(W )> 
C: lINT: <getroute(C)>l 
Figure 4: Information State Following Check in \[5\] 
I r /..<,.,..,.,,..,~.?,,,,,=.,:>,.,8>\ 1 11 / ?BL: \gi . . . . .  l ,(w) / / / /  
/,--,,-, /c , , . :c2  .,~,.>,,~=<W.DU~>\/ / /  
GND: \[----: \CA 16: C2. igi~e( C,CA 14 ) / /  / /  
Is<:,> ,/,,<,,< c~,.,,,.,<,.,,=,,.<,,.. > > ~, / / /  i : \scp(W,slirl( malvcrn D/  / / /  
l.CO~D: < > J N 
uous: <oul> / /  
I loB, . . . .  11 l/ W: TOGND" DH: <CA 16: C2. agree(C,CA 14 ) > 
iPDU: \[ " \[SCP: < sop( C,start( malvern ) } > H | /  
/ / LCOND: < > J/ l /  
\] LID: DO7 J / /  
/ \[ \[OBL: <Iddress(C.CAI8 )> 11ll 
/ /TOGND" / DH: <CAIS: C2. info_nequesi(W.?dest )>//// 
/ c~U: / / sc~: < > / IN  
/ / L~oNo: < > ill / 
/ uo: ,~  .ill 
C: lINT: <getroute(C)>l J 
Figure 5: Information state following \[7\] 
After C's agreement in \[6\], the deliberation rou- 
tine is able to move past discussion of the start- 
ing point, and add an intention to ask about the 
next piece of information, the destination. This 
leads to producing utterance \[7\], which also implic- 
itly acknowledges \[6\], after which C's agreement is 
grounded, leading to the IS shown in Figure 5. Note 
that the list in W.GND.SCP in Figure 5 indicates that 
both C and W are committed to the proposition that 
the starting place is Malvern. 
5 Conc lus ions  
It has only been possible here to introduce the basic 
concerns of the PTT account of dialogue modelling 
and to pick out one or two illustrative examples to 
highlight the implementational approach which has 
7 
been assumed. Current and future work is directed 
towards measuring the theory against more challeng- 
ing data to test its validity; cases where ground- 
ing is less automatic are an obvious source of such 
tests, and we have identified a few relevant problem 
cases in the Autoroute dialogues. We do claim, how- 
ever, that the implementation as it stands validates 
a number of key aspects of the theory and provides 
a good basis for future work in dialogue modelling. 
Acknowledgments  
The TRINDI (Task Oriented Instructional Dia- 
logue) project is supported by the Telematics Appli- 
cations Programme, Language Engineering Project 
LE4-8314. Massimo Poesio is supported by an EP- 
SRC Advanced Research Fellowship. 
Re ferences  
J. Allwood. 1976. Linguistic Communication as 
Action and Cooperation. Ph.D. thesis, GSteborg 
University, Department of Linguistics. 
J. Allwood. 1994. Obligations and options in dia- 
logue. Think Quarterly, 3:9-18. 
M. E. Bratman, D. J. Israel and M. E. Pollack. 1988. 
Plans and Resource-Bounded Practical Reason- 
ing. Computational Intelligence, 4(4). 
P. Bretier and M. D. Sadek. 1996. A rational agent 
as the kernel of a cooperative spoken dialogue 
system: Implementing a logical theory of inter- 
action. In J. P. Miiller, M. J. Wooldridge, and 
N. R. Jennings, editors, Intelligent Agents III -- 
Proceedings of the Third International Workshop 
on Agent Theories, Architectures, and Languages 
(ATAL-96), Lecture Notes in Artificial Intelli- 
gence. Springer-Verlag, Heidelberg. 
J. Calder. 1998. Thistle: diagram display en- 
gines and editors. Technical Report HCRC/TR- 
97, HCRC, University of Edinburgh, Edinburgh. 
H. H. Clark and E. F. Schaefer. 1989. Contributing 
to discourse. Cognitive Science, 13:259-294. 
H. H. Clark and D. Wilkes-Gibbs. 1986. Referring 
as a collaborative process. Cognition, 22:1-39. 
Also appears as Chapter 4 in (Clark, 1992). 
H. H. Clark. 1992. Arenas of Language Use. Uni- 
versity of Chicago Press. 
P. R. Cohen and H. J. Levesque. 1990. Rational in- 
teraction as the basis for communication. I  P. R. 
Cohen, J. Morgan, and M. E. Pollack, editors, In- 
tentions in Communication. MIT Press. 
Discourse Resource Initiative. 1997. Standards for 
dialogue coding in natural anguage processing. 
Report no. 167, Dagstuhl-Seminar. 
B. J. Grosz and C. L. Sidner. 1990. Plans for dis- 
course. In P. R. Cohen, J. Morgan, and M. E. Pol- 
lack, editors, Intentions in Communication. MIT 
Press. 
J. Kreutel. 1998. An obligation-driven computa- 
tional model for questions and assertions in dia- 
logue. Master's thesis, Department ofLinguistics, 
University of Edinburgh, Edinburgh. 
S. Larsson, P. Bohlin, J. Bos, and D. Traum. 1999. 
Trindikit manual. Technical Report Deliverable 
D2.2 - Manual, Trindi. 
M. Poesio, R. Cooper, S. Larsson, D. Traum, and 
C. Matheson. 1999. Annotating conversations for 
information state update. In Proceedings of Am- 
stelogue 99, 3rd Workshop on the Semantics and 
Pragmatics of Dialogues. 
M. Poesio and D. R. Tranm. 1997. Conversational 
actions and discourse situations. Computational 
Intelligence, 13(3). 
M. Poesio and D. R. Traum. 1998. Towards an ax- 
iomatization of dialogue acts. In Proceedings of 
Twendial'98, 13th Twente Workshop on Language 
Technology, pages 207-222. 
M. D. Sadek. 1991. Dialogue acts are rational plans. 
In Proceedings o\] the ESCA/ETR workshop on 
multi-modal dialogue. 
M. P. Singh. 1998. Agent communication lan- 
guages: Rethinking the principles. IEEE Com- 
puter, 31(12):40-47. 
D. R. Traum and J. F. Allen. 1992. A speech acts 
approach to grounding in conversation. In Pro- 
ceedings 2nd International Conference on Spoken 
Language Processing (ICSLP-92), pages 137-40, 
October. 
D. R. Traum and J. F. Allen. 1994. Discourse obli- 
gations in dialogue processing. In Proceedings of 
the 32nd Annual meeting of the Association for 
Computational Linguistics, pages 1-8, June. 
D. R. Traum, J. Bos, R. Cooper, S. Larsson, I. 
Lewin, C. Matheson, and M. Poesio. 1999. A 
model of dialogue moves and information state re- 
vision. Technical Report Deliverable D2.1, Trindi. 
D. R. Traum and P. Dillenbourg. 1998. Towards a 
Normative Model of Grounding in Collaboration. 
In Proceedings of the ESSLLI98 workshop on Mu- 
tual Knowledge, Common Ground and Public In- 
formation. 
D. R. Traum. 1994. A computational theory 
of grounding in natural language conversation. 
Ph.D. thesis, Computer Science, University of 
Rochester, New York, December. 
8 
NAACL HLT Demonstration Program, pages 5?6,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
Adaptive Tutorial Dialogue Systems Using Deep NLP Techniques
Myroslava O. Dzikovska, Charles B. Callaway, Elaine Farrow,
Manuel Marques-Pita, Colin Matheson and Johanna D. Moore
ICCS-HCRC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, United Kingdom
(mdzikovs,ccallawa,efarrow,mmpita,colin,jmoore)@inf.ed.ac.uk ?
Abstract
We present tutorial dialogue systems in
two different domains that demonstrate
the use of dialogue management and deep
natural language processing techniques.
Generation techniques are used to produce
natural sounding feedback adapted to stu-
dent performance and the dialogue his-
tory, and context is used to interpret ten-
tative answers phrased as questions.
1 Introduction
Intelligent tutoring systems help students improve
learning compared to reading textbooks, though not
quite as much as human tutors (Anderson et al,
1995). The specific properties of human-human di-
alogue that help students learn are still being stud-
ied, but the proposed features important for learn-
ing include allowing students to explain their actions
(Chi et al, 1994), adapting tutorial feedback to the
learner?s level, and engagement/affect. Some tuto-
rial dialogue systems use NLP techniques to analyze
student responses to ?why? questions. (Aleven et al,
2001; Jordan et al, 2006). However, for remediation
they revert to scripted dialogue, relying on short-
answer questions and canned feedback. The result-
ing dialogue may be redundant in ways detrimental
to student understanding (Jordan et al, 2005) and
allows for only limited adaptivity (Jordan, 2004).
?This work was supported under the 6th Framework Pro-
gramme of the European Commission, Ref. IST-507826, and
by a grant from The Office of Naval Research N000149910165.
We demonstrate two tutorial dialogue systems
that use techniques from task-oriented dialogue sys-
tems to improve the interaction. The systems are
built using the Information State Update approach
(Larsson and Traum, 2000) for dialogue manage-
ment and generic components for deep natural lan-
guage understanding and generation. Tutorial feed-
back is generated adaptively based on the student
model, and the interpretation is used to process
explanations and to differentiate between student
queries and hedged answers phrased as questions.
The systems are intended for testing hypotheses
about tutoring. By comparing student learning gains
between versions of the same system using different
tutoring strategies, as well as between the systems
and human tutors, we can test hypotheses about the
role of factors such as free natural language input,
adaptivity and student affect.
2 The BEEDIFF Tutor
The BEEDIFF tutor helps students solve symbolic
differentiation problems, a procedural task. Solu-
tion graphs generated by a domain reasoner are used
to interpret student actions and to generate feed-
back.1 Student input is relatively limited and con-
sists mostly of mathematical formulas, but the sys-
tem generates adaptive feedback based on the notion
of student performance and on the dialogue history.
For example, if an average student asks for a hint
on differentiating sin(x2), the first level of feedback
may be ?Think about which rule to apply?, which
1Solution graphs are generated automatically for arbitrary
expressions, with no limit on the complexity of expressions ex-
cept for possible efficiency considerations.
5
can then be specialized to ?Use the chain rule? and
then to giving away the complete answer. For stu-
dents with low performance, more specific feed-
back can be given from the start. The same strat-
egy (based on an initial corpus analysis) is used in
producing feedback after incorrect answers, and we
intend to use the system to evaluate its effectiveness.
The feedback is generated automatically from a
single diagnosis and generation techniques are used
to produce appropriate discourse cues. For example,
when a student repeats the same mistake, the feed-
back may be ?You?ve differentiated the inner layer
correctly, but you?re still missing the minus sign?.
The two clauses are joined by a contrast relationship,
and the second indicates that an error was repeated
by using the adverbial ?still?.
3 The BEETLE Tutor
The BEETLE tutor is designed to teach students ba-
sic electricity and electronics concepts. Unlike the
BEEDIFF tutor, the BEETLE tutor is built around
a pre-planned course where the students alternate
reading with exercises involving answering ?why?
questions and interacting with a circuit simulator.
Since this is a conceptual domain, for most exer-
cises there is no structured sequence of steps that the
students should follow, but students need to name a
correct set of objects and relationships in their re-
sponse. We model the process of building an answer
to an exercise as co-constructing a solution, where
the student and tutor may contribute parts of the an-
swer. For example, consider the question ?For each
circuit, which components are in a closed path?.
The solution can be built up gradually, with the stu-
dent naming different components, and the system
providing feedback until the list is complete. This
generic process of gradually building up a solution is
also applied to giving explanations. For example, in
answer to the question ?What is required for a light
bulb to light? the student may say ?The bulb must be
in a closed path?, which is correct but not complete.
The system may then say ?Correct, but is that every-
thing?? to prompt the student towards mentioning
the battery as well. The diagnosis of the student an-
swer is represented as a set of correctly given objects
or relationships, incorrect parts, and objects and re-
lationships that have yet to be mentioned, and the
system uses the same dialogue strategy of eliciting
the missing parts for all types of questions.
Students often phrase their answers tentatively,
for example ?Is the bulb in a closed path??. In the
context of a tutor question the interpretation process
treats yes-no questions from the student as poten-
tially hedged answers. The dialogue manager at-
tempts to match the objects and relationships in the
student input with those in the question. If a close
match can be found, then the student utterance is
interpreted as giving an answer rather than a true
query. In contrast, if the student said ?Is the bulb
connected to the battery??, this would be interpreted
as a proper query and the system would attempt to
answer it.
Conclusion We demonstrate two tutorial dialogue
systems in different domains built by adapting di-
alogue techniques from task-oriented dialogue sys-
tems. Improved interpretation and generation help
support adaptivity and a wider range of inputs than
possible in scripted dialogue.
References
V. Aleven, O. Popescu, and K. R. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cognitive
tutor. In Proc. AI-ED 2001.
J. R. Anderson, A. T. Corbett, K. R. Koedinger, and
R. Pelletier. 1995. Cognitive tutors: Lessons learned.
The Journal of the Learning Sciences, 4(2):167?207.
M. T. H. Chi, N. de Leeuw, M.-H. Chiu, and C. La-
Vancher. 1994. Eliciting self-explanations improves
understanding. Cognitive Science, 18(3):439?477.
P. Jordan, P. Albacete, and K. VanLehn. 2005. Taking
control of redundancy in scripted tutorial dialogue. In
Proc. of AIED2005, pages 314?321.
P. Jordan, M. Makatchev, U. Pappuswamy, K. VanLehn,
and P. Albacete. 2006. A natural language tutorial
dialogue system for physics. In Proc. of FLAIRS-06.
P. W. Jordan. 2004. Using student explanations as mod-
els for adapting tutorial dialogue. In V. Barr and
Z. Markov, editors, FLAIRS Conference. AAAI Press.
S. Larsson and D. Traum. 2000. Information state and
dialogue management in the TRINDI Dialogue Move
Engine Toolkit. Natural Language Engineering, 6(3-
4):323?340.
6
Proceedings of the EACL 2009 Demonstrations Session, pages 37?40,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
Adaptive Natural Language Interaction
Stasinos Konstantopoulos
Athanasios Tegos
Dimitris Bilidas
NCSR ?Demokritos?, Athens, Greece
Colin Matheson
Human Communication Research Centre
Edinburgh University, U.K.
Ion Androutsopoulos
Gerasimos Lampouras
Prodromos Malakasiotis
Athens Univ. of Economics and Business
Greece
Olivier Deroo
Acapela Group, Belgium
Abstract
The subject of this demonstration is natu-
ral language interaction, focusing on adap-
tivity and profiling of the dialogue man-
agement and the generated output (text
and speech). These are demonstrated in
a museum guide use-case, operating in a
simulated environment. The main techni-
cal innovations presented are the profiling
model, the dialogue and action manage-
ment system, and the text generation and
speech synthesis systems.
1 Introduction
In this demonstration we present a number of
state-of-the art language technology tools, imple-
menting and integrating the latest discourse and
knowledge representation theories into a complete
application suite, including:
? dialogue management, natural language gen-
eration, and speech synthesis, all modulated
by a flexible and highly adaptable profiling
mechanism;
? robust speech recognition and language inter-
pretation; and,
? an authoring environment for developing the
representation of the domain of discourse as
well as the associated linguistic and adaptiv-
ity resources.
The system demonstration is based on a use
case of a virtual-tour guide in a museum domain.
Demonstration visitors interact with the guide us-
ing headsets and are able to experiment with load-
ing different interaction profiles and observing the
differences in the guide?s behaviour. The demon-
stration also includes the screening of videos from
an embodied instantiation of the system as a robot
guiding visitors in a museum.
2 Technical Content
The demonstration integrates a number of state-of-
the-art language components into a highly adap-
tive natural language interaction system. Adap-
tivity here refers to using interaction profiles that
modulate dialogue management as well as text
generation and speech synthesis. Interaction pro-
files are semantic models that extend the objective
ontological model of the domain of discourse with
subjective information, such as how ?interesting?
or ?important? an entity or statement of the objec-
tive domain model is.
Advanced multimodal dialogue management
capabilities involving and combining input and
output from various interaction modalities and
technologies, such as speech recognition and syn-
thesis, natural language interpretation and gener-
ation, and recognition of/response to user actions,
gestures, and facial expressions.
State-of-the art natural language generation
technology, capable of producing multi-sentence,
coherent natural language descriptions of objects
based on their abstract semantic representation.
The resulting descriptions vary dynamically in
terms of content as well as surface language ex-
pressions used to realize each description, depend-
ing on the interaction history (e.g., comparing
to previously given information) and the adaptiv-
ity parameters (exhibiting system personality and
adapting to user background and interests).
3 System Description
The system is capable of interacting in a vari-
ety of modalities, including non-verbal ones such
as gesture and face-expression recognition, but in
this demonstration we focus on the system?s lan-
guage interaction components. In this modality,
abstract, language-independent system actions are
first planned by the dialogue and action manager
(DAM), then realized into language-specific text
37
by the natural language generation engine, and fi-
nally synthesized into speech. All three layers are
parametrized by a profiling and adaptivity module.
3.1 Profiling and Adaptation
Profiling and adaptation modulates the output of
dialogue management, generation, and speech
synthesis so that the system exhibits a synthetic
personality, while at the same time adapting to
user background and interests.
User stereotypes (e.g., ?expert? or ?child?) pro-
vide generation parameters (such as maximum de-
scription length) and also initialize the dynamic
user model with interest rates for all the ontologi-
cal entities (individuals and properties) of the do-
main of discourse. This same information is also
provided in system profiles reflecting the system?s
(as opposed to the users?) preferences; one can,
for example, define a profile that favours using
the architectural attributes to describe a building
where another profile would choose to concentrate
on historical facts regarding the same building.
Stereotypes and profiles are combined into a
single set of parameters by means of personal-
ity models. Personality models are many-valued
Description Logic definitions of the overall pref-
erence, grounded in stereotype and profile data.
These definitions model recognizable personality
traits so that, for example, an open personality will
attend more to the user?s requests than its own
interests in deriving overall preference (Konstan-
topoulos et al, 2008).
Furthermore, the system dynamically adapts
overall preference according to both interaction
history and the current dialogue state. So, for one,
the initial (static model) interest factor of an ontol-
ogy entity is reduced each time this entity is used
in a description in order to avoid repetitions. On
the other hand, preference will increase if, for ex-
ample, in the current state the user has explicitly
asked about an entity.
3.2 Dialogue and Action Management
The DAM is built around the information-state
update dialogue paradigm of the TRINDIKIT
dialogue-engine toolkit (Cooper and Larsson,
1998) and takes into account the combined user-
robot interest factor when determining informa-
tion state updates.
The DAM combines various interaction modal-
ities and technologies in both interpretation/fusion
and generation/fission. In interpreting user ac-
tions the system recognizes spoken utterances,
simple gestures, and touch-screen input, all of
which may be combined into a representation of
a multi-modal user action. Similarly, when plan-
ning robotic actions the DAM coordinates a num-
ber of available output modalities, including spo-
ken language, text (on the touchscreen), the move-
ment and configuration of the robotic platform, fa-
cial expressions, and simple head gestures.1
To handle multimodal input, the DAM uses a fu-
sion module which combines messages from the
language interpretation, gesture, and touchscreen
modules into a single XML structure. Schemati-
cally, this can be represented as:
<userAction>
<userUtterance>hello</userUtterance>
<userButton content="13"/>
</userAction>
This structure represents a user pressing some-
thing on the touchscreen and saying hello at the
same time.2
The representation is passed essentially un-
changed to the DAM, to be processed by its up-
date rules, where the ID of button press is inter-
preted in context and matched with the speech.
In most circumstances, the natural language pro-
cessing component (see 3.3) produces a seman-
tic representation of the input which appears in
the userUtterance element; the use of ?hello?
above is for illustration. An example update rule
which will fire in the context of a greeting from
the user is (in schematic form):
if
in(/latest_utterance/moves, hello)
then
output(start)
Update rules contain a list of conditions and a
list of effects. Here there is one condition (that the
latest moves from the user includes ?hello?), and
one effect (the ?start? procedure). The latter initi-
ates the dialogue by, among other things, having
the system utter a standardised greeting.
As noted above, the DAM is also multimodal
on the output side. An XML representation is
created which can contain robot utterances and
robot movements (both head movements and mo-
bile platform moves). Information can also be pre-
sented on the touchscreen.
1Expressions and gestures will not be demonstrated, as
they can not be materialized in the simulated robot.
2The precise meaning of ?at the same time? is determined
by the fusion module.
38
3.3 Natural Language Processing
The NATURALOWL natural language generation
(NLG) engine (Galanis et al 2009) produces
multi-sentence, coherent natural language descrip-
tions of objects in multiple languages from a sin-
gle semantic representation; the resulting descrip-
tions are annotated with prosodic markup for driv-
ing the speech synthesisers.
The generated descriptions vary dynamically, in
both content and language expressions, depending
on the interaction profile as well as the dynamic
interaction history. The dynamic preference factor
of the item itself is used to decide the level of de-
tail of the description being generated. The prefer-
ence factors of the properties are used to order the
contents of the descriptions to ensure that, in cases
where not all possible facts are to be presented in
a single turn, the most relevant ones are chosen.
The interaction history is used to check previously
given information to avoid repeating the same in-
formation in different contexts and to create com-
parisons with earlier objects.
NaturalOWL demonstrates the benefits of
adopting NLG on the Semantic Web. Organiza-
tions that need to publish information about ob-
jects, such as exhibits or products, can publish
OWL ontologies instead of texts. NLG engines,
embedded in browsers or Web servers, can then
render the ontologies in natural language, whereas
computer programs may access the ontologies, in
effect logical statements, directly. The descrip-
tions can be very simple and brief, relying on
question answering to provide more information
if such is requested. This way, machine-readable
information can be more naturally inspected and
consulted by users.
In order to generate a list of possible follow
up questions that the system can handle, we ini-
tially construct a list of the particular individuals
or classes that are mentioned in the generated de-
scription; the follow up questions will most likely
refer to them. Only individuals and classes for
which there is further information in the ontology
are extracted.
After identifying the referred individuals and
classes, we proceed to predict definition (e.g.,
?Who was Ares??) and property questions (e.g.,
?Where is Mount Penteli??) about them that
could be answered by the information in the on-
tology. We avoid generating questions that cannot
be answered. The expected definition questions
are constructed by inserting the names of the re-
ferred individuals and classes into templates such
as ?who is/was person X?? or ?what do you know
about class or entity Y??.
In the case of referred individuals, we also gen-
erate expected property questions using the pat-
terns NaturalOWL generates the descriptions with.
These patterns, called microplans, show how to
express the properties of the ontology as sentences
of the target languages. For example, if the indi-
vidual templeOfAres has the property excavate-
dIn, and that property has a microplan of the form
?resource was excavated in period?, we anticipate
questions such as ?when was the Temple of Ares
excavated?? and ?which period was the Temple of
Ares excavated in??.
Whenever a description (e.g., of a monument)
is generated, the expected follow up questions for
that description (e.g., about the monument?s ar-
chitect) are dynamically included in the rules of
the speech recognizer?s grammar, to increase word
recognition accuracy. The rules include compo-
nents that extract entities, classes, and properties
from the recognized questions, thus allowing the
dialogue and action manager to figure out what the
user wishes to know.
3.4 Speech Synthesis and Recognition
The natural language interface demonstrates ro-
bust speech recognition technology, capable of
recognizing spoken phrases in noisy environ-
ments, and advanced speech synthesis, capable of
producing spoken output of very high quality. The
main challenge that the automatic speech recogni-
tion (ASR) module needs to address is background
noise, especially in the robot-embodied use case.
A common technique used in order to handle this
is training acoustic models with the anticipated
background noise, but that is not always possi-
ble. The demonstrated ASR module can be trained
on noise-contaminated data where available, but
also incorporates multi-band acoustic modelling
(Dupont, 2003) for robust recognition under noisy
conditions. Speech recognition rates are also sub-
stantially improved by using the predictions made
by NATURALOWL and the DAM to dynamically
restrict the lexical and phrasal expectations at each
dialogue turn.
The speech synthesis module of the demon-
strated system is based on unit selection technol-
ogy, generally recognized as producing more nat-
39
ural output that previous technologies such as di-
phone concatenation or formant synthesis. The
main innovation that is demonstrated is support for
emotion, a key aspect of increasing the naturalness
of synthetic speech. This is achieved by combin-
ing emotional unit recordings with run-time trans-
formations. With respect to the former, a complete
?voice? now comprises three sub-voices (neutral,
happy, and sad), based on recordings of the same
speaker. The recording time needed is substan-
tially decreased by prior linguistic analysis that se-
lects appropriate text covering all phonetic units
needed by the unit selection system. In addition to
the statically defined sub-voices, the speech syn-
thesis module implements dynamic transforma-
tions (e.g., emphasis), pauses, and variable speech
speed. The system combines all these capabilities
in order to dynamically modulate the synthesised
speech to convey the impression of emotionally
modulated speech.
3.5 Authoring
The interaction system is complemented by
ELEON (Bilidas et al, 2007), an authoring tool for
annotating domain ontologies with the generation
and adaptivity resources described above. The do-
main ontology can be authored in ELEON, but any
existing OWL ontology can also annotated.
More specifically, ELEON supports author-
ing linguistic resources, including a domain-
dependent lexicon, which associates classes and
individuals of the ontology with nouns and proper
names of the target natural languages; microplans,
which provide the NLG with patterns for realizing
property instances as sentences; and a partial or-
dering of properties, which allows the system to
order the resulting sentences as a coherent text.
The adaptivity and profiling resources include
interest rates, indicating how interesting the enti-
ties of the ontology are in any given profile; and
stereotype parameters that control generation as-
pects such as the number of facts to include in a
description or the maximum sentence length.
Furthermore, ELEON supports the author with
immediate previews, so that the effect of any
change in either the ontology or the associated re-
sources can be directly reviewed. The actual gen-
eration of the preview is relegated to external gen-
eration engines.
4 Conclusions
The demonstrated system combines semantic rep-
resentation and reasoning technologies with lan-
guage technology into a human-computer interac-
tion system that exhibits a large degree of adapt-
ability to audiences and circumstances and is able
to take advantage of existing domain model cre-
ated independently of the need to build a natural
language interface. Furthermore by clearly sepa-
rating the abstract, semantic layer from that of the
linguistic realization, it allows the re-use of lin-
guistic resources across domains and the domain
model and adaptivity resources across languages.
Acknowledgements
The demonstrated system is being developed by
the European (FP6-IST) project INDIGO.3 IN-
DIGO develops and advances human-robot inter-
action technology, enabling robots to perceive nat-
ural human behaviour, as well as making them
act in ways that are more familiar to humans. To
achieve its goals, INDIGO advances various tech-
nologies, which it integrates in a robotic platform.
References
Dimitris Bilidas, Maria Theologou, and Vangelis
Karkaletsis. 2007. Enriching OWL ontologies
with linguistic and user-related annotations: the
ELEON system. In Proc. 19th Intl. Conf. on
Tools with Artificial Intelligence (ICTAI-2007).
Robin Cooper and Staffan Larsson. 1998. Dia-
logue Moves and Information States. In: Pro-
ceedings of the 3rd Intl. Workshop on Computa-
tional Semantics (IWCS-3).
Ste?phane Dupont. 2003. Robust parameters
for noisy speech recognition. U.S. Patent
2003182114.
Dimitrios Galanis, George Karakatsiotis, Gerasi-
mos Lampouras and Ion Androutsopoulos.
2009. An open-source natural language gener-
ator for OWL ontologies and its use in Prote?ge?
and Second Life. In this volume.
Stasinos Konstantopoulos, Vangelis Karkaletsis,
and Colin Matheson. 2008. Robot personality:
Representation and externalization. In Proc.
Computational Aspects of Affective and Emo-
tional Interaction (CAFFEi 08), Patras, Greece.
3http://www.ics.forth.gr/indigo/
40
Situated Reference in a Hybrid Human-Robot Interaction System
Manuel Giuliani1 and Mary Ellen Foster2 and Amy Isard3
Colin Matheson3 and Jon Oberlander3 and Alois Knoll1
1Informatik VI: Robotics and Embedded Systems, Technische Universita?t Mu?nchen
2School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh
3Institute for Communicating and Collaborative Systems, School of Informatics, University of Edinburgh
Abstract
We present the situated reference genera-
tion module of a hybrid human-robot in-
teraction system that collaborates with a
human user in assembling target objects
from a wooden toy construction set. The
system contains a sub-symbolic goal in-
ference system which is able to detect the
goals and errors of humans by analysing
their verbal and non-verbal behaviour. The
dialogue manager and reference genera-
tion components then use situated refer-
ences to explain the errors to the human
users and provide solution strategies. We
describe a user study comparing the results
from subjects who heard constant refer-
ences to those who heard references gener-
ated by an adaptive process. There was no
difference in the objective results across
the two groups, but the subjects in the
adaptive condition gave higher subjective
ratings to the robot?s abilities as a conver-
sational partner. An analysis of the objec-
tive and subjective results found that the
main predictors of subjective user satisfac-
tion were the user?s performance at the as-
sembly task and the number of times they
had to ask for instructions to be repeated.
1 Introduction
When two humans jointly carry out a mutual task
for which both know the plan?for example, as-
sembling a new shelf?it frequently happens that
one makes an error, and the other has to assist
and to explain what the error was and how it can
be solved. Humans are skilled at spotting errors
committed by another, as well as errors which
they made themselves. Recent neurological stud-
ies have shown that error monitoring?i.e., ob-
serving the errors made by oneself or by others?
plays an important role in joint activity. For ex-
ample, Bekkering et al (2009) have demonstrated
that humans show the same brain activation pat-
terns when they make an error themselves and
when they observe someone else making an error.
In this paper, we describe a human-robot inter-
action (HRI) system that is able both to analyse
the actions and the utterances of a human part-
ner to determine if the human made an error in
the assembly plan, and to explain to the human
what went wrong and what to do to solve the prob-
lem. This robot combines approaches from sub-
symbolic processing and symbolic reasoning in a
hybrid architecture based on that described in Fos-
ter et al (2008b).
During the construction process, it is frequently
necessary to refer to an object which is being used
to assemble the finished product, choosing an un-
ambigious reference to distinguish the object from
the others available. The classic reference gen-
eration algorithm, on which most subsequent im-
plementations are based, is the incremental algo-
rithm of Dale and Reiter (1995), which selects
a set of attributes of a target object to single it
out from a set of distractor objects. In real-world
tasks, the speaker and hearer often have more con-
text in common than just the knowledge of object
attributes, and several extensions have been pro-
posed, dealing with visual and discourse salience
(Kelleher and Kruijff, 2006) and the ability to pro-
duce multimodal references including actions such
as pointing (van der Sluis, 2005; Kranstedt and
Wachsmuth, 2005).
Foster et al (2008a) noted another type of mul-
timodal reference which is particularly useful in
embodied, task-based contexts: haptic-ostensive
reference, in which an object is referred to as it
is being manipulated by the speaker. Manipulat-
ing an object, which must be done in any case as
part of the task, also makes an object more salient
and therefore affords linguistic references that in-
Figure 1: The dialogue robot
dicate the increased accessibility of the referent.
This type of reference is similar to the placing-for
actions noted by Clark (1996).
An initial approach for generating referring ex-
pressions that make use of haptic-ostensive refer-
ence was described in (Foster et al, 2009a). With
this system, a study was conducted comparing the
new reference strategy to the basic Dale and Reiter
incremental algorithm. Na??ve users reported that it
was significantly easier to understand the instruc-
tions given by the robot when it used references
generated by the more sophisticated algorithm. In
this paper, we perform a similar experiment, but
making use of a more capable human-robot in-
teraction system and a more complete process for
generating situated references.
2 Hybrid Human-Robot Dialogue
System
The experiment described in this paper makes use
of a hybrid human-robot dialogue system which
supports multimodal human-robot collaboration
on a joint construction task. The robot (Figure 1)
has a pair of manipulator arms with grippers,
mounted in a position to resemble human arms,
and an animatronic talking head (van Breemen,
2005) capable of producing facial expressions,
rigid head motion, and lip-synchronised synthe-
sised speech. The subject and the robot work to-
gether to assemble wooden construction toys on
a common workspace, coordinating their actions
through speech (English or German), gestures, and
facial expressions.
The robot can pick up and move objects in the
workspace and perform simple assembly tasks. In
the scenario considered here, both of the partici-
pants know the assembly plan and jointly execute
it. The robot assists the human, explains necessary
assembly steps in case the human makes an error,
and offers pieces as required. The workspace is di-
vided into two areas?one belonging to the robot
and one to the human?to make joint action nec-
essary for task success.
The system has components which use both
sub-symbolic and symbolic processing. It in-
cludes a goal inference module based on dynamic
neural fields (Erlhagen and Bicho, 2006; Bicho
et al, 2009), which selects the robot?s next actions
based on the human user?s actions and utterances.
Given a particular assembly plan and the knowl-
edge of which objects the user has picked up, this
module can determine when the user has made
an error. The system also incorporates a dialogue
manager based on the TrindiKit dialogue manage-
ment toolkit (Larsson and Traum, 2000), which
implements the information-state based approach
to dialogue management. This unique combina-
tion of abilities means that when the robot detects
that its human partner has made an error?for ex-
ample, picking up or requesting an assembly piece
that is not needed in the current step of the building
plan?it can explain to the human what the error
was and what can be done to correct the mistake?
for example by picking up or indicating the correct
assembly piece.
Messages from all of the system?s input chan-
nels (speech, object recognition, and gesture
recognition) are processed and combined by a
multimodal fusion component based on (Giuliani
and Knoll, 2008), which is the link between the
symbolic and the sub-symbolic parts of the sys-
tem. The fusion component then communicates
with the goal inference module, which calculates
the next action instructions for the robot and also
determines if the user made an error. From there,
fusion combines the information from goal infer-
ence with the input data and sends unified hy-
potheses to the dialogue manager.
When it receives the fusion hypotheses, the dia-
logue manager uses the dialogue history and the
physical and task context to choose a response.
It then sends a high-level specification of the de-
1. System First we will build a windmill.
2. User Okay.
3. User {picks up a yellow cube, unnecessary piece for a
windmill}
4. System You don?t need a yellow cube to build a windmill.
5. System To build a windmill, you first need to build a
tower.
6. System [picking up and holding out red cube] To build
the tower, insert the green bolt through the end of this
red cube and screw it into the blue cube.
7. User [takes cube, performs action] Okay.
Figure 2: Sample human-robot dialogue, showing
adaptively-generated situated references
sired response to the output planner, which in turn
sends commands to each output channel: linguis-
tic content (including multimodal referring ex-
pressions), facial expressions and gaze behaviours
of the talking head, and actions of the robot ma-
nipulators. The linguistic outputs are realised us-
ing the OpenCCG surface realiser (White, 2006).
3 Reference Generation
In this system, two strategies were implemented
for generating references to objects in the world:
a constant version that uses only the basic incre-
mental algorithm (Dale and Reiter, 1995) to se-
lect properties, and an adaptive version that uses
more of the physical, dialogue and task context
to help select the references. The constant sys-
tem can produce a definite or indefinite reference,
and the most appropriate combination of attributes
according to the incremental algorithm. The adap-
tive system also generates pronominal and deictic
references, and introduces the concept of multiple
types of distractor sets depending on context.
Figure 2 shows a fragment of a sample interac-
tion in which the user picks up an incorrect piece:
the robot detects the error and describes the correct
assembly procedure. The underlined references
show the range of output produced by the adap-
tive reference generation module; for the constant
system, the references would all have been ?the
red cube?. The algorithms used by the adaptive
reference generation module are described below.
3.1 Reference Algorithm
The module stores a history of the referring ex-
pressions spoken by both the system and the user,
and uses these together with distractor sets to se-
lect referring expressions. In this domain there are
two types of objects which we need to refer to:
concrete objects in the world (everything which is
on the table, or in the robot?s or user?s hand), and
objects which do not yet exist, but are in the pro-
cess of being created. For non-existent objects we
do not build a distractor set, but simply use the
name of the object. In all other cases, we use one
of three types of distractor set:
? all the pieces needed to build a target object;
? all the objects referred to since the last men-
tion of this object; or
? all the concrete objects in the world.
The first type of set is used if the object under
consideration (OUC) is a negative reference to a
piece in context of the creation of a target object.
In all other cases, the second type is used if the
OUC has been mentioned before and the third type
if it has not.
When choosing a referring expression, we first
process the distractor set, comparing the proper-
ties of the OUC with the properties of all distrac-
tors. If a distractor has a different type from the
OUC, it is removed from the distractor set. With
all other properties, if the distractor has a different
value from the OUC, it is removed from the dis-
tractor set, and the OUC?s property value is added
to the list of properties to use.
We then choose the type of referring expression.
We first look for a previous reference (PR) to the
OUC, and if one exists, determine whether it was
in focus. Depending on the case, we use one of the
following reference strategies.
No PR If the OUC does not yet exist or we are
making a negative reference, we use an indef-
inite article. If the robot is holding the OUC,
we use a deictic reference. If the OUC does
exist and there are no distractors, we use a
definite; if there are distractors we use an in-
definite.
PR was focal If the PR was within the same turn,
we choose a pronoun for our next reference.
If it was in focus but in a previous turn, if
the robot is holding the OUC we use a deictic
reference, and if the robot is not holding it,
we use a pronoun.
PR was not focal If the robot is holding the
OUC, we make a deictic reference. Other-
wise, if the PR was a pronoun, definite, or de-
ictic, we use a definite article. If the PR was
indefinite and there are no distractors, we use
a definite article, if there are distractors, we
use an indefinite article.
If there are any properties in the list, and the
reference chosen is not a pronoun, we add them.
3.2 Examples of the Reference Algorithm
We will illustrate the reference-selection strategy
with two cases from the dialogue in Figure 2.
Utterance 4 ?a yellow cube?
This object is going to be referred to in a negative
context as part of a windmill under construction,
so the distractor set is the set of objects needed to
make a windmill: {red cube, blue cube, small slat,
small slat, green bolt, red bolt}.
We select the properties to use in describing the
object under consideration, processing the distrac-
tor set. We first remove all objects which do not
share the same type as our object under considera-
tion, which leaves {red cube, blue cube}. We then
compare the other attributes of our new object with
the remaining distractors - in this case ?colour?.
Since neither cube shares the colour ?yellow? with
the target object, both are removed from the dis-
tractor set, and ?yellow? is added to the list of
properties to use.
There is no previous reference to this object,
and since we are making a negative reference,
we automatically choose an indefinite article. We
therefore select the reference ?a yellow cube?.
Utterance 6 ?it? (a green bolt)
This object has been referred to before, earlier in
the same utterance, so the distractor set is all the
references between the earlier one and this one?
{red cube}. Since this object has a different type
from the bolt we want to describe, the distractor
set is now empty, and nothing is added to the list
of properties to use.
There is a previous definite reference to the ob-
ject in the same utterance: ?the green bolt?. This
reference was focal, so we are free to use a pro-
noun if appropriate. Since the previous reference
was definite, and the object being referred to does
exist, we choose to use a pronoun. We therefore
select the reference ?it?.
4 Experiment Design
In the context of the HRI system, a constant refer-
ence strategy is sufficient in that it makes it possi-
ble for the robot?s partner to know which item is
needed. On the other hand, while the varied forms
produced by the more complex mechanism can in-
crease the naturalness of the system output, they
may actually be insufficient if they are not used
in appropriate current circumstances?for exam-
ple, ?this cube? is not a particularly helpful refer-
ence if a user has no way to tell which ?this? is.
As a consequence, the system for generating such
references must be sensitive to the current state
of joint actions and?in effect?of joint attention.
The difference between the two systems is a test of
the adaptive version?s ability to adjust expressions
to pertinent circumstances. It is known that peo-
ple respond well to reduced expressions like ?this
cube? or ?it? when another person uses them ap-
propriately (Bard et al, 2008); we need to see if
the robot system can also achieve the benefits that
situated reference could provide.
To address this question, the human-robot di-
alogue system was evaluated through a user study
in which subjects interacted with the complete sys-
tem. Using a between-subjects design, this study
compared the two reference strategies, measuring
the users? subjective reactions to the system along
with their overall performance in the interaction.
Based on the findings from the user evaluation de-
scribed in (Foster et al, 2009a)?in which the pri-
mary effect of varying the reference strategy was
on the users? subjective opinion of the robot?the
main prediction for this study was as follows:
? Subjects who interact with a system using
adaptive references will rate the quality of
the robot?s conversation more highly than the
subjects who hear constant references.
We made no specific prediction regarding the
effect of reference strategy on any of the objec-
tive measures: based on the results of the user
evaluation mentioned above, there is no reason to
expect an effect either way. Note that?as men-
tioned above?if the adaptive version makes in-
correct choices, that may have a negative impact
on users? ability to understand the system?s gener-
ated references. For this reason, even a finding of
(a) Windmill (b) Railway signal
Figure 3: Target objects for the experiment
no objective difference would demonstrate that the
adaptive references did not harm the users? ability
to interact with the system, as long as it was ac-
companied by the predicted improvement in sub-
jective judgements.
4.1 Subjects
41 subjects (33 male) took part in this experiment.
The mean age of the subjects was 24.5, with a min-
imum of 19 and a maximum of 42. Of the subjects
who indicated an area of study, the two most com-
mon areas were Mathematics (14 subjects) and In-
formatics (also 14 subjects). On a scale of 1 to 5,
subjects gave a mean assessment of their knowl-
edge of computers at 4.1, of speech-recognition
systems at 2.0, and of human-robot systems at 1.7.
Subjects were compensated for their participation
in the experiment.
4.2 Scenario
This study used a between-subjects design with
one independent variable: each subject interacted
either with a system that used a constant strategy
to generate referring expressions (19 subjects), or
else with a system that used an adaptive strategy
(22 subjects).1
Each subject built two objects in collaboration
with the system, always in the same order. The
first target object was the windmill (Figure 3a);
after the windmill was completed, the robot and
human then built a railway signal (Figure 3b). For
both target objects, the user was given a building
plan (on paper). To induce an error, both of the
plans given to the subjects instructed them to use
an incorrect piece: a yellow cube instead of a red
cube for the windmill, and a long (seven-hole) slat
instead of a medium (five-hole) slat for the rail-
1The results of an additional three subjects in the constant-
reference condition could not be analysed due to technical
difficulties.
way signal. The subjects were told that the plan
contained an error and that the robot would cor-
rect them when necessary, but did not know the
nature of the error.
When the human picked up or requested an in-
correct piece during the interaction, the system de-
tected the error and explained to the human what
to do in order to assemble the target object cor-
rectly. When the robot explained the error and
when it handed over the pieces, it used referring
expressions that were generated using the constant
strategy for half of the subjects, and the adaptive
strategy for the other half of the subjects.
4.3 Experimental Set-up and Procedure
The participants stood in front of the table facing
the robot, equipped with a headset microphone for
speech recognition. The pieces required for the
target object?plus a set of additional pieces in or-
der to make the reference task more complex?
were placed on the table, using the same layout
for every participant. The layout was chosen to
ensure that there would be points in the interaction
where the subjects had to ask the robot for build-
ing pieces from the robot?s workspace, as well as
situations in which the robot automatically handed
over the pieces. Along with the building plan men-
tioned above, the subjects were given a table with
the names of the pieces they could build the ob-
jects with.
4.4 Data Acquisition
At the end of a trial, the subject responded to
a usability questionnaire consisting of 39 items,
which fell into four main categories: Intelligence
of the robot (13 items), Task ease and task suc-
cess (12 items), Feelings of the user (8 items),
and Conversation quality (6 items). The items on
the questionnaire were based on those used in the
user evaluation described in (Foster et al, 2009b),
but were adapted for the scenario and research
questions of the current study. The questionnaire
was presented using software that let the subjects
choose values between 1 and 100 with a slider. In
addition to the questionnaire, the trials were also
video-taped, and the system log files from all tri-
als were kept for further analysis.
5 Results
We analysed the data resulting from this study in
three different ways. First, the subjects? responses
Table 1: Overall usability results
Constant Adaptive M-W
Intell. 79.0 (15.6) 74.9 (12.7) p = 0.19, n.s.
Task 72.7 (10.4) 71.1 (8.3) p = 0.69, n.s.
Feeling 66.9 (15.9) 66.8 (14.2) p = 0.51, n.s.
Conv. 66.1 (13.6) 75.2 (10.7) p = 0.036, sig.
Overall 72.1 (11.2) 71.8 (9.1) p = 0.68, n.s.
to the questionnaire items were compared to de-
termine if there was a difference between the re-
sponses given by the two groups. A range of sum-
mary objective measures were also gathered from
the log files and videos?these included the dura-
tion of the interaction measured both in seconds
and in system turns, the subjects? success at build-
ing each of the target objects, the number of times
that the robot had to explain the construction plan
to the user, and the number of times that the users
asked the system to repeat its instructions. Finally,
we compared the results on the subjective and ob-
jective measures to determine which of the objec-
tive factors had the largest influence on subjective
user satisfaction.
5.1 Subjective Measures
The subjects in this study gave a generally positive
assessment of their interactions with the system on
the questionnaire?with a mean overall satisfac-
tion score of 72.0 out of 100?and rated the per-
ceived intelligence of the robot particularly highly
(overall mean of 76.8). Table 1 shows the mean
results from the two groups of subjects for each
category on the user-satisfaction questionnaire, in
all cases on a scale from 0?100 (with the scores
for negatively-posed questions inverted).
To test the effect of reference strategy on the
usability-questionnaire responses, we performed a
Mann-Whitney test comparing the distribution of
responses from the two groups of subjects on the
overall results, as well as on each sub-category of
questions. For most categories, there was no sig-
nificant difference between the responses of the
two groups, with p values ranging from 0.19 to
0.69 (as shown in Table 1). The only category
where a significant difference was found was on
the questionnaire items that asked the subjects to
assess the robot?s quality as a conversational part-
ner; for those items, the mean score from sub-
jects who heard the adaptive references was sig-
nificantly higher (p < 0.05) than the mean score
from the subjects who heard references generated
by the constant reference module. Of the six ques-
Table 2: Objective results (all differences n.s.)
Measure Constant Adaptive M-W
Duration (s.) 404.3 (62.8) 410.5 (94.6) p = 0.90
Duration (turns) 29.8 (5.02) 31.2 (5.57) p = 0.44
Rep requests 0.26 (0.45) 0.32 (0.78) p = 0.68
Explanations 2.21 (0.63) 2.41 (0.80) p = 0.44
Successful trials 1.58 (0.61) 1.55 (0.74) p = 0.93
tions that were related to the conversation quality,
the most significant impact was on the two ques-
tions which assessed the subjects? understanding
of what they were able to do at various points dur-
ing the interaction.
5.2 Objective Measures
Based on the log files and video recordings, we
computed a range of objective measures. These
measures were divided into three classes, based
on those used in the PARADISE dialogue-system
evaluation framework (Walker et al, 2000):
? Two dialogue efficiency measures: the mean
duration of the interaction as measured both
in seconds and in system turns;
? Two dialogue quality measures: the number
of times that the robot gave explanations, and
the number of times that the user asked for
instructions to be repeated; and
? One task success measure: how many of the
(two) target objects were constructed as in-
tended (i.e., as shown in Figure 3).
For each of these measures, we tested whether the
difference in reference strategy had a significant
effect, again via a Mann-Whitney test. Table 2 il-
lustrates the results on these objective measures,
divided by the reference strategy.
The results from the two groups of subjects
were very similar on all of these measures: on
average, the experiment took 404 seconds (nearly
seven minutes) to complete with the constant strat-
egy and 410 seconds with the adaptive, the mean
number of system turns was close to 30 in both
cases, just over one-quarter of all subjects asked
for instructions to be repeated, the robot gave just
over two explanations per trial, and about three-
quarters of all target objects (i.e. 1.5 out of 2)
were correctly built. The Mann-Whitney test con-
firms that none of the differences between the two
groups even came close to significance on any of
the objective measures.
5.3 Comparing Objective and Subjective
Measures
In the preceding sections, we presented results on
a number of objective and subjective measures.
While the subjects generally rated their experi-
ence of using the system positively, there was
some degree of variation, most of which could not
be attributed to the difference in reference strat-
egy. Also, the results on the objective measures
varied widely across the subjects, but again were
not generally affected by the reference strategy.
In this section, we examine the relationship be-
tween these two classes of measures in order to
determine which of the objective measures had the
largest effect on users? subjective reactions to the
HRI system.
Being able to predict subjective user satisfac-
tion from more easily-measured objective proper-
ties can be very useful for developers of interac-
tive systems: in addition to making it possible to
evaluate systems based on automatically available
data without the need for extensive experiments
with users, such a performance function can also
be used in an online, incremental manner to adapt
system behaviour to avoid entering a state that is
likely to reduce user satisfaction (Litman and Pan,
2002), or can be used as a reward function in a
reinforcement-learning scenario (Walker, 2000).
We employed the procedure used in the PAR-
ADISE evaluation framework (Walker et al,
2000) to explore the relationship between the sub-
jective and objective factors. The PARADISE
model uses stepwise multiple linear regression to
predict subjective user satisfaction based on mea-
sures representing the performance dimensions of
task success, dialogue quality, and dialogue effi-
ciency, resulting in a predictor function of the fol-
lowing form:
Satisfaction =
n
?
i=1
wi ?N (mi)
The mi terms represent the value of each measure,
while the N function transforms each measure
into a normal distribution using z-score normali-
sation. Stepwise linear regression produces coef-
ficients (wi) describing the relative contribution of
each predictor to the user satisfaction. If a predic-
tor does not contribute significantly, its wi value is
zero after the stepwise process.
Table 3 shows the predictor functions that were
derived for each of the classes of subjective mea-
sures in this study, using all of the objective mea-
sures from Table 2 as initial factors. The R2 col-
umn indicates the percentage of the variance in the
target measure that is explained by the predictor
function, while the Significance column gives sig-
nificance values for each term in the function.
In general, the two factors with the biggest in-
fluence on user satisfaction were the number of
repetition requests (which had a uniformly neg-
ative effect on user satisfaction), and the num-
ber of target objects correctly built by the user
(which generally had a positive effect). Aside
from the questions on user feelings, the R2 values
are generally in line with those found in previous
PARADISE evaluations of other dialogue systems
(Walker et al, 2000; Litman and Pan, 2002), and
in fact are much higher than those found in a pre-
vious similar study (Foster et al, 2009b).
6 Discussion
The subjective responses on the relevant items
from the usability questionnaire suggest that
the subjects perceived the robot to be a bet-
ter conversational partner if it used contextually
varied, situationally-appropriate referring expres-
sions than if it always used a baseline, constant
strategy; this supports the main prediction for this
study. The result also agrees with the findings of
a previous study (Foster et al, 2009a)?this sys-
tem did not incorporate goal inference and had a
less-sophisticated reference strategy, but the main
effect of changing reference strategy was also on
the users? subjective opinions of the robot?s inter-
active ability. These studies together support the
current effort in the natural-language generation
community to devise more sophisticated reference
generation algorithms.
On the other hand, there was no significant dif-
ference between the two groups on any of the
objective measures: the dialogue efficiency, dia-
logue quality, and task success were nearly iden-
tical across the two groups of subjects. A de-
tailed analysis of the subjects? gaze and object-
manipulation behaviour immediately after various
forms of generated references from the robot also
failed to find any significant differences between
the various reference types. These overall results
are not particularly surprising: studies of human-
human dialogue in a similar joint construction task
(Bard et al, In prep.) have demonstrated that the
collaborators preserve quality of construction in
Table 3: PARADISE predictor functions for each category on the usability questionnaire
Measure Function R2 Significance
Intelligence 76.8+7.00?N (Correct)?5.51?N (Repeats) 0.39 Correct: p < 0.001,
Repeats: p < 0.005
Task 72.4+3.54?N (Correct)?3.45?N (Repeats)?2.17?N (Explain) 0.43 Correct: p < 0.005,
Repeats: p < 0.01,
Explain: p? 0.10
Feeling 66.9?6.54?N (Repeats)+4.28?N (Seconds) 0.09 Repeats: p < 0.05,
Seconds: p? 0.12
Conversation 71.0+5.28?N (Correct)?3.08?N (Repeats) 0.20 Correct: p < 0.01,
Repeats: p? 0.10
Overall 72.0+4.80?N (Correct)?4.27?N (Repeats) 0.40 Correct: p < 0.001,
Repeats: p < 0.005
all cases, though circumstances may dictate what
strategies they use to do this. Combined with the
subjective findings, this lack of an objective effect
suggests that the references generated by the adap-
tive strategy were both sufficient and more natural
than those generated by the constant strategy.
The analysis of the relationship between the
subjective and objective measures analysis has
also confirmed and extended the findings from a
similar analysis (Foster et al, 2009b). In that
study, the main contributors to user satisfaction
were user repetition requests (negative), task suc-
cess, and dialogue length (both positive). In the
current study, the primary factors were similar,
although dialogue length was less prominent as
a factor and task success was more prominent.
These findings are generally intuitive: subjects
who are able to complete the joint construction
task are clearly having more successful interac-
tions than those who are not able to complete the
task, while subjects who need to ask for instruc-
tions to be repeated are equally clearly not hav-
ing successful interactions. The findings add ev-
idence that, in this sort of task-based, embodied
dialogue system, users enjoy the experience more
when they are able to complete the task success-
fully and are able to understand the spoken contri-
butions of their partner, and also suggest that de-
signers should concentrate on these aspects of the
interaction when designing the system.
7 Conclusions
We have presented the reference generation mod-
ule of a hybrid human-robot interaction system
that combines a goal-inference component based
on sub-symbolic dynamic neural fields with a
natural-language interface based on more tradi-
tional symbolic techniques. This combination of
approaches results in a system that is able to work
together with a human partner on a mutual con-
struction task, interpreting its partner?s verbal and
non-verbal behaviour and responding appropri-
ately to unexpected actions (errors) of the partner.
We have then described a user evaluation of this
system, concentrating on the impact of different
techniques for generating situated references in
the context of the robot?s corrective feedback. The
results of this study indicate that using an adaptive
strategy to generate the references significantly in-
creases the users? opinion of the robot as a con-
versational partner, without having any effect on
any of the other measures. This result agrees with
the findings of the system evaluation described in
(Foster et al, 2009a), and adds evidence that so-
phisticated generation techniques are able to im-
prove users? experiences with interactive systems.
An analysis of the relationship between the ob-
jective and subjective measures found that the
main contributors to user satisfaction were the
users? task performance (which had a positive ef-
fect on most measures of satisfaction), and the
number of times the users had to ask for instruc-
tions to be repeated (which had a generally neg-
ative effect). Again, these results agree with the
findings of a previous study (Foster et al, 2009b),
and also suggest priorities for designers of this
type of task-based interactive system.
Acknowledgements
This research was supported by the Euro-
pean Commission through the JAST2 (IST-
FP6-003747-IP) and INDIGO3 (IST-FP6-045388)
projects. Thanks to Pawel Dacka and Levent Kent
for help in running the experiment and analysing
the data.
2http://www.jast-project.eu/
3http://www.ics.forth.gr/indigo/
References
E. G. Bard, R. Hill, and M. E. Foster. 2008. What
tunes accessibility of referring expressions in
task-related dialogue? In Proceedings of the
30th Annual Meeting of the Cognitive Science
Society (CogSci 2008). Chicago.
E. G. Bard, R. L. Hill, M. E. Foster, and M. Arai.
In prep. How do we tune accessibility in joint
tasks: Roles and regulations.
H. Bekkering, E.R.A. de Bruijn, R.H. Cuijpers,
R. Newman-Norlund, H.T. van Schie, and
R. Meulenbroek. 2009. Joint action: Neurocog-
nitive mechanisms supporting human interac-
tion. Topics in Cognitive Science, 1(2):340?
352.
E. Bicho, L. Louro, N. Hipolito, and W. Erlhagen.
2009. A dynamic field approach to goal infer-
ence and error monitoring for human-robot in-
teraction. In Proceedings of the Symposium on
?New Frontiers in Human-Robot Interaction?,
AISB 2009 Convention. Heriot-Watt University
Edinburgh.
H. H. Clark. 1996. Using Language. Cambridge
University Press.
R. Dale and E. Reiter. 1995. Computational inter-
pretations of the Gricean maxims in the genera-
tion of referring expressions. Cognitive Science,
19(2):233?263.
W. Erlhagen and E. Bicho. 2006. The dynamic
neural field approach to cognitive robotics.
Journal of Neural Engineering, 3(3):R36?R54.
M. E. Foster, E. G. Bard, R. L. Hill, M. Guhe,
J. Oberlander, and A. Knoll. 2008a. The roles
of haptic-ostensive referring expressions in co-
operative, task-based human-robot dialogue. In
Proceedings of HRI 2008.
M. E. Foster, M. Giuliani, A. Isard, C. Matheson,
J. Oberlander, and A. Knoll. 2009a. Evaluating
description and reference strategies in a coop-
erative human-robot dialogue system. In Pro-
ceedings of IJCAI-09.
M. E. Foster, M. Giuliani, and A. Knoll. 2009b.
Comparing objective and subjective measures
of usability in a human-robot dialogue system.
In Proceedings of ACL-IJCNLP 2009.
M. E. Foster, M. Giuliani, T. Mu?ller, M. Rickert,
A. Knoll, W. Erlhagen, E. Bicho, N. Hipo?lito,
and L. Louro. 2008b. Combining goal inference
and natural-language dialogue for human-robot
joint action. In Proceedings of the 1st Interna-
tional Workshop on Combinations of Intelligent
Methods and Applications at ECAI 2008.
M. Giuliani and A. Knoll. 2008. MultiML:
A general-purpose representation language for
multimodal human utterances. In Proceedings
of ICMI 2008.
J. D. Kelleher and G.-J. M. Kruijff. 2006. Incre-
mental generation of spatial referring expres-
sions in situated dialog. In Proceedings of
COLING-ACL 2006.
A. Kranstedt and I. Wachsmuth. 2005. Incremen-
tal generation of multimodal deixis referring to
objects. In Proceedings of ENLG 2005.
S. Larsson and D. Traum. 2000. Information state
and dialogue management in the TRINDI dia-
logue move engine toolkit. Natural Language
Engineering, 6(3&4):323?340.
D. J. Litman and S. Pan. 2002. Designing and
evaluating an adaptive spoken dialogue system.
User Modeling and User-Adapted Interaction,
12(2?3):111?137.
A. J. N. van Breemen. 2005. iCat: Experimenting
with animabotics. In Proceedings of AISB 2005
Creative Robotics Symposium.
I. F. van der Sluis. 2005. Multimodal Reference:
Studies in Automatic Generation of Multimodal
Referring Expressions. Ph.D. thesis, University
of Tilburg.
M. Walker, C. Kamm, and D. Litman. 2000. To-
wards developing general models of usability
with PARADISE. Natural Language Engineer-
ing, 6(3?4):363?377.
M. A. Walker. 2000. An application of reinforce-
ment learning to dialogue strategy selection in
a spoken dialogue system for email. Journal of
Artificial Intelligence Research, 12:387?416.
M. White. 2006. Efficient realization of co-
ordinate structures in Combinatory Categorial
Grammar. Research on Language and Compu-
tation, 4(1):39?75.
