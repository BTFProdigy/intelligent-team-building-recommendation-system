Coling 2010: Poster Volume, pages 1?8,
Beijing, August 2010
Towards the Adequate Evaluation of Morphosyntactic Taggers
Szymon Acedan?ski
Institute of Computer Science,
Polish Academy of Sciences
Institute of Informatics,
University of Warsaw
accek@mimuw.edu.pl
Adam Przepi?rkowski
Institute of Computer Science,
Polish Academy of Sciences
Institute of Informatics,
University of Warsaw
adamp@ipipan.waw.pl
Abstract
There exists a well-established and almost
unanimously adopted measure of tagger
performance, namely, accuracy. Although
it is perfectly adequate for small tagsets
and typical approaches to disambiguation,
we show that it is deficient when applied
to rich morphological tagsets and propose
various extensions designed to better cor-
relate with the real usefulness of the tag-
ger.
1 Introduction
Part-of-Speech (PoS) tagging is probably the
most common and best researched NLP task, the
first step in many higher level processing solu-
tions such as parsing, but also information re-
trieval, speech recognition and machine transla-
tion. There are also well established evaluation
measures, the foremost of which is accuracy, i.e.,
the percent of words for which the tagger assigns
the correct ? in the sense of some gold standard
? interpretation.
Accuracy works well for the original PoS tag-
ging task, where each word is assumed to have ex-
actly one correct tag, and where the information
carried by a tag is limited roughly to the PoS of
the word and only very little morphosyntactic in-
formation, as in typical tagsets for English. How-
ever, there are two cases where accuracy becomes
less than adequate: the situation where the gold
standard and / or the tagging results contain mul-
tiple tags marked as correct for a single word, and
the use of a rich morphosyntactic (or morphologi-
cal) tagset.
The first possibility is discussed in detail in
(Karwan?ska and Przepi?rkowski, 2009), but the
need for an evaluation measure for taggers which
do not necessarily fully disambiguate PoS was al-
ready noted in (van Halteren, 1999), where the use
of standard information retrieval measures preci-
sion and recall (as well as their harmonic mean,
the F-measure) is proposed. Other natural gen-
eralisations of the accuracy measure, able to deal
with non-unique tags either in the gold standard1
or in the tagging results, are proposed in (Kar-
wan?ska and Przepi?rkowski, 2009).
Standard accuracy is less than adequate also
in case of rich morphosyntactic tagsets, where
the full tag carries information not only about
PoS, but also about case, number, gender, etc.
Such tagsets are common for Slavic languages,
but also for Hungarian, Arabic and other lan-
guages. For example, according to one com-
monly used Polish tagset (Przepi?rkowski and
Wolin?ski, 2003), the form uda has the follow-
ing interpretations: fin:sg:ter:perf (a fi-
nite singular 3rd person perfective form of the
verb UDAC? ?pretend?), subst:pl:nom:n and
1There are cases were it makes sense to manually assign
a number of tags as correct to a given word, as any decision
would be fully arbitrary, regardless of the amount of con-
text and world knowledge available. For example, in some
Slavic languages, incl. Polish, there are verbs which option-
ally subcategorise for an accusative or a genitive comple-
ment, without any variation in meaning, and there are nouns
which are syncretic between these two cases, so for such
?verb + nounacc/gen? sequences it is impossible to fully dis-
ambiguate case; see also (Oliva, 2001).
1
subst:pl:acc:n (nominative or accusative
plural form of the neuter noun UDO ?thigh?).
Now, assuming that the right interpretation in a
given context is subst:pl:acc:n, accuracy
will equally harshly penalise the other nominal in-
terpretation (subst:pl:nom:n), which shares
with the correct interpretation not only PoS, but
also the values of gender and number, and the
completely irrelevant verbal interpretation. A
more accurate tagger evaluation measure should
distinguish these two non-optimal assignments
and treat subst:pl:nom:n as partially correct.
Similarly, the Polish tagset mentioned above
distinguishes between nouns and gerunds, with
some forms actually ambiguous between these
two interpretations. For example, zadanie may be
interpreted as a nominative or accusative form of
the noun ZADANIE ?task?, or a nominative or ac-
cusative form of the gerund derived from the verb
ZADAC? ?assign?. Since gerunds and nouns have
very similar distributions, any error in the assign-
ment of part of speech, noun vs. gerund, will most
probably not matter for a parser of Polish ? it
will still be able to construct the right tree, pro-
vided the case is correctly disambiguated. How-
ever, the ?all-or-nothing? nature of the accuracy
measure regards the tag differing from the correct
one only in part of speech or in case as harshly,
as it would regard an utterly wrong interpretation,
say, as an adverb.
In what follows we propose various evaluation
measures which differentiate between better and
worse incorrect interpretations, cf. ? 2. The im-
plementation of two such measures is described
in ? 3. Finally, ? 4 concludes the paper.
2 Proposed Measures
2.1 Full Interpretations and PoS
The first step towards a better accuracy mea-
sure might consist in calculating two accu-
racy measures: one for full tags, and the
other only for fragments of tags represent-
ing parts of speech. Two taggers wrongly
assigning either fin:sg:ter:perf (T1) or
subst:pl:nom:n (T2) instead of the correct
subst:pl:acc:n would fare equally well with
respect to the tag-level accuracy, but T2 would be
? rightly ? evaluated as better with respect to
the PoS-level accuracy.
The second example given in ? 1 shows, how-
ever, that the problem is more general and that a
tagger which gets the PoS wrong (say, gerund in-
stead of noun) but all the relevant categories (case,
number, gender) right may actually be more use-
ful in practice than the one that gets the PoS right
at the cost of confusing cases (say, accusative in-
stead of nominative).
2.2 Positional Accuracy
A generalisation of the idea of looking separately
at parts of speech is to split tags into their compo-
nents (or positions) and measure the correctness
of the tag by calculating the F-measure. For ex-
ample, if the (perfective, affirmative) gerundial in-
terpretation ger:sg:nom:n:perf:aff is as-
signed instead of the correct nominal interpreta-
tion subst:sg:nom:n, the tags agree on 3 po-
sitions (sg, nom, n), so the precision is 36 , the re-call ? 34 , which gives the F-measure of 0.6. Obvi-ously, the assignment of the correct interpretation
results in F-measure equal 1.0, and the completely
wrong interpretation gives F-measure 0.0. Taking
these values instead of the ?all-or-nothing? 0 or 1,
accuracy is reinterpreted as the average F-measure
over all tag assignments.
Note that while this measure, let us call it po-
sitional accuracy (PA), is more fine-grained than
the standard accuracy, it wrongly treats all com-
ponents of tags as of equal importance and dif-
ficulty. For example, there are many case syn-
cretisms in Polish, but practically no ambiguities
concerning the category of negation (see the value
aff above), so case is inherently much more diffi-
cult than negation, and also much more important
for syntactic parsing, and as such it should carry
more weight when evaluating tagging results.
2.3 Weighted Positional Accuracy
In the current section we make a simplifying as-
sumption that weights of positions are absolute,
rather than conditional, i.e., that the weight of, say,
case does not depend on part of speech, word or
context. Once the weights are attained, weighted
precision and recall may be used as in the follow-
ing example.
2
Assume that PoS, case, number and gender
have the same weight, say 2.0, which is 4 times
larger than that of any other category. Then, in
case ger:sg:nom:n:perf:aff is assigned
instead of the correct subst:sg:nom:n, pre-
cision and recall are given by:
P = 3? 2.04? 2.0 + 2? 0.5 =
2
3 ,
R = 3? 2.04? 2.0 =
3
4 .
This results in a higher F-measure than in case of
non-weighted positional accuracy.
The following subsections propose various
ways in which the importance of particular gram-
matical categories and of the part of speech may
be estimated.
2.3.1 Average Ambiguity
The average number of morphosyntactic inter-
pretations per word is sometimes given as a rough
measure of the difficulty of tagging. For exam-
ple, tagging English texts with the Penn Treebank
tagset is easier than tagging Czech or Polish, as
the average number of possible tags per word is
2.32 in English (Hajic?, 2004, p. 171), while it is
3.65 (Hajic? and Hladk?, 1997, p. 113) and 3.32
(Przepi?rkowski, 2008, p. 44) for common tagsets
for Czech and Polish, respectively.
By analogy, one measure of the difficulty of as-
signing the right value of a given category or part
of speech is the average number of different val-
ues of the category per word.
2.3.2 Importance for Parsing
All measures mentioned so far are intrinsic (in
vitro) evaluation measures, independent ? but
hopefully correlated with ? the usefulness of the
results in particular applications. On the other
hand, extrinsic (in vivo) evaluation estimates the
usefulness of tagging in larger systems, e.g., in
parsers. Full-scale extrinsic evaluation is rarely
used, as it is much more costly and often requires
user evaluation of the end system.
In this and the next subsections we propose
evaluation measures which combine the advan-
tages of both approaches. They are variants of
the weighted positional accuracy (WPA) measure,
where weights correspond to the usefulness of a
given category (or PoS) for a particular task.
Probably the most common task taking advan-
tage of morphosyntactic tagging is syntactic pars-
ing. Here, weights should indicate to what extent
the parser relies on PoS and particular categories
to arrive at the correct parse. Such weights may
be estimated from an automatically parsed corpus
in the following way:
for each category (including PoS) c do
count(c) = 0 {Initialise counts.}
end for
for each sentence s do
for each rule r used in s do
for each terminal symbol (word) t in the
RHS of r do
for each category c referred to by r in t
do
increase count(c)
end for
end for
end for
end for
{Use count(c)?s as weights.}
In prose: whenever a syntactic rule is used, in-
crease counts of all morphosyntactic categories
(incl. PoS) mentioned in the terminal symbols oc-
curring in this rule. These counts may be nor-
malised or used directly as weights.
We assume here that either the parser produces
a single parse for any sentence (assumption realis-
tic only in case of shallow parsers), or that the best
or at least most probable parse may be selected au-
tomatically, as in case of probabilistic grammars,
or that parses are disambiguated manually. In case
only a non-probabilistic deep parser is available,
and parses are not disambiguated manually, the
Expectation-Maximisation method may be used
to select a probable parse (De?bowski, 2009) or all
parses might be taken into account.
Note that, once a parser is available, such
weights may be calculated automatically and used
repeatedly for tagger evaluation, so the cost of us-
ing this measure is not significantly higher than
the cost of intrinsic measures, while at the same
time the correlation of the evaluation results with
the extrinsic application is much higher.
3
2.3.3 Importance for Corpus Search
The final variant (many more are imagin-
able) of WPA that we would like to de-
scribe here concerns another application of tag-
ging, namely, for the annotation of corpora.
Various corpus search engines, including the
IMS Open Corpus Workbench (http://cwb.
sourceforge.net/) and Poliqarp (http://
poliqarp.sourceforge.net/) allow the
user to search for particular parts of speech and
grammatical categories. Obviously, the tagger
should maximise the quality of the disambigua-
tion of those categories which occur frequently
in corpus queries, i.e., the weights should corre-
spond to the frequencies of particular categories
(and PoS) in user queries. Note that the only re-
source needed to calculate weights are the logs of
a corpus search engine.
An experiment involving an implementation of
this measure is described in detail in ? 3.
2.4 Conditional Weighted Positional
Accuracy
The importance and difficulty of a category may
depend on the part of speech. For example, af-
ter case syncretisms, gender ambiguity is one of
the main problems for the current taggers of Pol-
ish. But this problem concerns mainly pronouns
and adjectives, where the systematic gender syn-
cretism is high. On the other hand, nouns do not
inflect for gender, so only some nominal forms
are ambiguous with respect to gender. Moreover,
gerunds, which also bear gender, are uniformly
neuter, so here part of speech alone uniquely de-
termines the value of this category.
A straightforward extension of WPA capitalis-
ing on these observations is what we call con-
ditional weighted positional accuracy (CWPA),
where weights of morphosyntactic categories are
conditioned on PoS.
Note that not all variants of WPA may be easily
generalised to CWPA; although such an extension
is obvious for the average ambiguity (? 2.3.1), it is
less clear for the other two variants. For parsing-
related WPA, we assume that, even if a given rule
does not mention the PoS of a terminal symbol,2
2For example, in unification grammars and constraint-
based grammars a terminal may be identified only by the
that PoS may be read off the parse tree, so the con-
ditional weights may still be calculated. On the
other hand, logs of a corpus search engine are typ-
ically not sufficient to calculate such conditional
weights; e.g., a query for a sequence of 5 genitive
words occurring in logs would have to be rerun
on the corpus again in order to find out parts of
speech of the returned 5-word sequences. For a
large number of queries on a large corpus, this is
a potentially costly operation.
It is also not immediately clear how to gener-
alise precision and recall from WPA to CWPA.
Returning to the example above, where t1 =
ger:sg:nom:n:perf:aff is assigned in-
stead of the correct t2 = subst:sg:nom:n, we
note that the weights of number, case and gender
may now (and should, at least in case of gender!)
be different for the two parts of speech involved.
Hence, precision needs to be calculated with re-
spect to the weights for the automatically assigned
part of speech, and recall ? taking into account
weights for the gold standard part of speech:
P =
?t?1t?2w(t
?
1) +
?
c?C(t1,t2) ?tc1tc2w(c|t?1)
w(t?1) +
?
c?C(t1) w(c|t?1)
,
R =
?t?1t?2w(t
?
2) +
?
c?C(t1,t2) ?tc1tc2w(c|t?2)
w(t?2) +
?
c?C(t2) w(c|t?2)
,
where t? is the PoS of tag t, w(p) is the weight
of the part of speech p, w(c|p) is the conditional
weight of the category c for PoS p, C(t) is the set
of morphosyntactic categories of tag t, C(t1, t2)
is the set of morphosyntactic categories common
to tags t1 and t2, tc is the value of category c in
tag t, and ?ij is the Kronecker delta (equal to 1 if
i = j, and to 0 otherwise). Hence, for the example
above, these formulas may be simplified to:
P =
?
c?{n,c,g}w(c|ger)
w(ger) +?c?{n,c,g,a,neg}w(c|ger)
,
R =
?
c?{n,c,g}w(c|subst)
w(subst) +?c?{n,c,g}w(c|subst)
,
where n, c, g, a and neg stand for number, case,
gender, aspect and negation.
values of some of its categories, as in the following simple
rule, specifying prepositional phrases as a preposition gov-
erning a specific case and a non-empty sequence of words
bearing that case: PPcase=C ? Pcase=C X+case=C.
4
3 Experiment
To evaluate behaviour of the proposed metrics, a
number of experiments were performed using the
manually disambiguated part of the IPI PAN Cor-
pus of Polish (Przepi?rkowski, 2005). This sub-
corpus consists of 880 000 segments. Two tag-
gers of Polish were tested. TaKIPI (Piasecki and
Godlewski, 2006) is a tagger which was used for
automatic disambiguation of the remaining part of
the aforementioned corpus. It is a statistical clas-
sifier based on decision trees combined with some
automatically extracted, hand-crafted rules. This
tagger by default sometimes assigns more than
one tag to a segment, what is consistent with the
golden standard. There is a setting which allows
this behaviour to be switched off. This tagger was
tested with both settings. The other tagger is a
prototype version of this Brill tagger, presented by
Acedan?ski and Go?uchowski in (Acedan?ski and
Go?uchowski, 2009).
For comparison, four metrics were used: stan-
dard metrics for full tags and only parts of speech,
as well as Positional Accuracy and Weighted Posi-
tional Accuracy. For the last measure, the weights
were obtained by analysing logs of user queries of
the Poliqarp corpus search engine. Occurrences
of queries involving particular grammatical cat-
egories were counted and used as weights. Ob-
tained results are presented in Table 1.
Table 1: Occurrences of particular grammatical
categories in query logs of the Poliqarp corpus
search engine.
Category # occurrences
POS 37771
CASE 14055
NUMBER 2074
GENDER 552
ASPECT 222
PERSON 186
DEGREE 81
ACCOMMODABILITY 25
POST-PREP. 8
NEGATION 7
ACCENTABILITY 5
AGGLUTINATION 4
3.1 Scored information retrieval metrics
In ? 2 a number of methods of assigning a score
to a pair of tags were presented. From now on,
let name them scoring functions. One could use
them directly for evaluation, given that both the
tagger and the golden standard always assign a
single interpretation to each segment. This is not
the case for the corpus we use, hence we pro-
pose generalisation of standard information re-
trieval metrics (precision, recall and F-measure)
as well as strong and weak correctness (Kar-
wan?ska and Przepi?rkowski, 2009) to account for
scoring functions.
Denote by Ti and Gi the sets of tags assigned by
the tagger and the golden standard, accordingly,
to the i-th segment of the tagged text. The set of
all tags in the tagset is denoted by T. The scoring
function used is score:T ? T ? [0, 1]. Also, to
save up on notation, we define
score(t, A) := max
t??A
score(t, t?)
Now, given the text has n segments, we take
P =
?n
i=1
?
t?Ti score(t, Gi)?n
i=1 |Ti|
R =
?n
i=1
?
g?Gi score(g, Ti)?n
i=1 |Gi|
F = 2 ? P ?RP +R
WC =
?n
i=1 maxt?Ti score(t, Gi)
n
SC =
?n
i=1 min({score(t, Gi): t ? Ti}
? {score(g, Ti): g ? Gi})
n
Intuitions for scored precision and recall are that
precision specifies the percent of tags assigned by
the tagger which have a high score with some cor-
responding golden tag. Analogously recall esti-
mates the percent of golden tags which have high
scores with some corresponding tag assigned by
the tagger. The definition of recall is slightly dif-
ferent than proposed by Zi??ko et al (Zi??ko et
al., 2007) so that recall is never greater than one.3
3For example if the golden standard specifies a single tag
and the tagger determines two tags which all score 0.6 when
compared with the golden, then if we used equations from
Zi??ko et al, we would get the recall of 1.2.
5
3.2 Evaluation results
Now the taggers were trained on the same data
consisting of 90% segments of the corpus and then
tested on the remaining 10%. Results were 10-
fold cross-validated. They are presented in Ta-
bles 2, 3, 4 and 5.
As expected, the values obtained with PA and
WPA fall between the numbers for standard met-
rics calculated with full tags and only the part of
speech. What is worth observing is that the use
of WPA makes values for scored precision and re-
call much closer together. This can be justified
by the fact that the golden standard relatively fre-
quently contains more than one interpretation for
some tags, which differ only in values of less im-
portant grammatical categories. WPA is resilient
to such situations.
One may argue that such scoring functions may
hide a large number of tagging mistakes occurring
in low-weighted categories. But this is not the
case as the clearly most common tagging errors
reported in both (Piasecki and Godlewski, 2006)
and (Acedan?ski and Go?uchowski, 2009) are for
CASE, GENDER and NUMBER. Also, the moti-
vation for weighting grammatical categories is to
actually ignore errors in not important ones. To
be fair, though, one should make sure that the
weights used for evaluation match the actual ap-
plication domain of the analysed tagger, and if no
specific domain is known, using a number of mea-
sures is recommended.
It should also be noted that for classic infor-
mation retrieval metrics, the result of weak cor-
rectness for TaKIPI is more similar to 92.55% re-
ported by the authors (Piasecki and Godlewski,
2006) than 91.30% shown in (Karwan?ska and
Przepi?rkowski, 2009) despite using the same test
corpus and very similar methodology4 as the sec-
ond paper presents.
4 Conclusion
This paper stems from the observation that the
commonly used measure for tagger evaluation,
i.e., accuracy, does not distinguish between com-
pletely incorrect and partially correct interpreta-
4The only difference was not contracting the grammati-
cal category of ACCOMMODABILITY present for masculine
numerals in the golden standard.
tions, even though the latter may be sufficient for
some applications. We proposed a way of grad-
ing tag assignments, by weighting the importance
of particular categories (case, number, etc.) and
the part of speech. Three variants of the weighted
positional accuracy were presented: one intrin-
sic and two application-oriented, and an extension
of WPA to conditional WPA was discussed. The
variant of WPA related to the needs of the users
of a corpus search engine for the National Corpus
of Polish was implemented and its usefulness was
demonstrated. We plan to implement the parsing-
oriented WPA in the future.
We conclude that tagger evaluation is far from
being a closed chapter and the time has come to
adopt more subtle approaches than sheer accuracy,
approaches able to cope with morphological rich-
ness and oriented towards real applications.
References
Acedan?ski, Szymon and Konrad Go?uchowski. 2009.
A morphosyntactic rule-based Brill tagger for
Polish. In K?opotek, Mieczys?aw A., Adam
Przepi?rkowski, S?awomir T. Wierzchon?, and
Krzysztof Trojanowski, editors, Advances in In-
telligent Information Systems ? Design and Ap-
plications, pages 67?76. Akademicka Oficyna
Wydawnicza EXIT, Warsaw.
De?bowski, ?ukasz. 2009. Valence extraction using
the EM selection and co-occurrence matrices. Lan-
guage Resources and Evaluation, 43:301?327.
Hajic?, Jan and Barbora Hladk?. 1997. Probabilistic
and rule-based tagger of an inflective language - a
comparison. In Proceedings of the 5th Applied Nat-
ural Language Processing Conference, pages 111?
118, Washington, DC. ACL.
Hajic?, Jan. 2004. Disambiguation of Rich Inflection.
Karolinum Press, Prague.
Janus, Daniel and Adam Przepi?rkowski. 2007.
Poliqarp: An open source corpus indexer and search
engine with syntactic extensions. In Proceedings of
the ACL 2007 Demo and Poster Sessions, pages 85?
88, Prague.
Karwan?ska, Danuta and Adam Przepi?rkowski. 2009.
On the evaluation of two Polish taggers. In Goz?dz?-
Roszkowski, Stanis?aw, editor, The proceedings of
Practical Applications in Language and Computers
PALC 2009, Frankfurt am Main. Peter Lang. Forth-
coming.
6
Oliva, Karel. 2001. On retaining ambiguity in dis-
ambiguated corpora: Programmatic reflections on
why?s and how?s. TAL (Traitement Automatique des
Langues), 42(2):487?500.
Piasecki, Maciej and Grzegorz Godlewski. 2006. Ef-
fective Architecture of the Polish Tagger. In Sojka,
Petr, Ivan Kopecek, and Karel Pala, editors, TSD,
volume 4188 of Lecture Notes in Computer Science,
pages 213?220. Springer.
Przepi?rkowski, Adam and Marcin Wolin?ski. 2003.
The unbearable lightness of tagging: A case study in
morphosyntactic tagging of Polish. In Proceedings
of the 4th International Workshop on Linguistically
Interpreted Corpora (LINC-03), EACL 2003, pages
109?116.
Przepi?rkowski, Adam. 2005. The IPI PAN Corpus in
Numbers. In Proceedings of the 2nd Language &
Technology Conference, Poznan?, Poland.
Przepi?rkowski, Adam. 2008. Powierzchniowe
przetwarzanie je?zyka polskiego. Akademicka Ofi-
cyna Wydawnicza EXIT, Warsaw.
van Halteren, Hans. 1999. Performance of taggers.
In van Halteren, Hans, editor, Syntactic Wordclass
Tagging, volume 9 of Text, Speech and Language
Technology, pages 81?94. Kluwer, Dordrecht.
Zi??ko, Bartosz, Suresh Manandhar, and Richard Wil-
son. 2007. Fuzzy Recall and Precision for
Speech Segmentation Evaluation. In Proceedings
of 3rd Language & Technology Conference, Poznan,
Poland, October.
7
Table 2: Evaluation results ? standard information retrieval metrics, full tags
Tagger C (%) WC (%) P (%) R (%) F (%)
TaKIPI ? defaults 87.67% 92.10% 89.93% 84.72% 87.25%
TaKIPI ? one tag per seg. 88.68% 91.06% 90.94% 83.78% 87.21%
Brill 90.01% 92.44% 92.26% 85.00% 88.49%
Table 3: Evaluation results ? standard information retrieval metrics, PoS only
Tagger C (%) WC (%) P (%) R (%) F (%)
TaKIPI ? defaults 95.56% 97.52% 95.71% 97.61% 96.65%
TaKIPI ? one tag per seg. 96.53% 96.54% 96.58% 96.71% 96.65%
Brill 98.17% 98.18% 98.20% 98.26% 98.23%
Table 4: Evaluation results ? scored metrics, Positional Accuracy
Tagger C (%) WC (%) P (%) R (%) F (%)
TaKIPI ? defaults 95.23% 96.58% 95.69% 95.44% 95.57%
TaKIPI ? one tag per seg. 95.69% 96.10% 96.12% 95.00% 95.56%
Brill 97.02% 97.43% 97.42% 96.27% 96.84%
Table 5: Evaluation results ? scored metrics, Weighted PA, Poliqarp weights
Tagger C (%) WC (%) P (%) R (%) F (%)
TaKIPI ? defaults 95.20% 96.62% 95.34% 96.56% 95.95%
TaKIPI ? one tag per seg. 95.88% 95.93% 95.97% 95.94% 95.95%
Brill 97.34% 97.40% 97.41% 97.34% 97.38%
8
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 42?47,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Machine Learning of Syntactic Attachment
from Morphosyntactic and Semantic Co-occurrence Statistics
Adam Slaski and Szymon Acedan?ski and Adam Przepi?rkowski
University of Warsaw
and
Institute of Computer Science
Polish Academy of Sciences
Abstract
The paper presents a novel approach to ex-
tracting dependency information in morpho-
logically rich languages using co-occurrence
statistics based not only on lexical forms
(as in previously described collocation-based
methods), but also on morphosyntactic and
wordnet-derived semantic properties of words.
Statistics generated from a corpus annotated
only at the morphosyntactic level are used
as features in a Machine Learning classifier
which is able to detect which heads of groups
found by a shallow parser are likely to be con-
nected by an edge in the complete parse tree.
The approach reaches the precision of 89%
and the recall of 65%, with an extra 6% re-
call, if only words present in the wordnet are
considered.
1 Introduction
The practical issue handled in this paper is how to
connect syntactic groups found by a shallow parser
into a possibly complete syntactic tree, i.e., how to
solve the attachment problem. To give a well-known
example from English, the task is to decide whether
in I shot an elephant in my pajamas1, the group in
my pajamas should be attached to an elephant or to
shot (or perhaps to I).
The standard approach to this problem relies on
finding collocation strengths between syntactic ob-
jects, usually between lexical items which are heads
of these objects, and resolve attachment ambigui-
ties on the basis of such collocation information.
1http://www.youtube.com/watch?v=NfN_
gcjGoJo
The current work extends this approach in two main
ways. First, we consider a very broad range of
features: not only lexical, but also lexico-semantic,
lexico-grammatical, and grammatical. Second, and
more importantly, we train classifiers based not on
these features directly, but rather on various associ-
ation measures calculated for each of the considered
features. This way the classifier selects which types
of features are important and which association mea-
sures are most informative for any feature type.
The proposed method is evaluated on Polish,
a language with rich inflection (and relatively free
word order), which exacerbates the usual data
sparseness problem in NLP.
In this work we assume that input texts are al-
ready part-of-speech tagged and chunked, the lat-
ter process resulting in the recognition of basic syn-
tactic groups. A syntactic group may, e.g., con-
sist of a verb with surrounding adverbs and particles
or a noun with its premodifiers. We assume that all
groups have a syntactic head and a semantic head. In
verbal and nominal groups both heads are the same
word, but in prepositional and numeral groups they
usually differ: the preposition and the numeral are
syntactic heads of the respective constituents, while
the semantic head is the head noun within the nomi-
nal group contained in these constituents.
To simplify some of the descriptions below, by
syntactic object we will understand either a shallow
group or a word. We will also uniformly talk about
syntactic and semantic heads of all syntactic objects;
in case of words, the word itself is its own syntac-
tic and semantic head. In effect, any syntactic ob-
ject may be represented by a pair of words (the two
42
heads), and each word is characterised by its base
form and its morphosyntactic tag.
2 Algorithm
The standard method of solving the PP-attachment
problem is based on collocation extraction (cf., e.g.,
(Hindle and Rooth, 1993)) and consists of three
main steps: first a training corpus is scanned and
frequencies of co-occurrences of pairs of words
(or more general: syntactic objects) are gathered;
then the collected data are normalised to obtain, for
each pair, the strength of their connection; finally,
information about such collocation strengths is em-
ployed to solve PP-attachment in new texts. An in-
stance of the PP-attachment problem is the choice
between two possible edges in a parse tree: (n1, pp)
and (n2, pp), where pp is the prepositional phrase,
and n1 and n2 are nodes in the tree (possible attach-
ment sites). This is solved by choosing the edge with
the node that has a stronger connection to the pp.
On this approach, collocations (defined as a rela-
tion between lexemes that co-occur more often than
would be expected by chance) are detected by taking
pairs of syntactic objects and only considering the
lemmata of their semantic heads. The natural ques-
tion is whether this could be generalised to other
properties of syntactic objects. In the following, the
term feature will refer to any properties of linguis-
tic objects taken into consideration in the process
of finding collocation strengths between pairs of ob-
jects.
2.1 Lexical and Morphosyntactic Features
To start with an example of a generalised colloca-
tion, let us consider morphosyntactic valence. In
order to extract valence links between two objects,
we should consider the lemma of one object (po-
tential predicate) and the morphosyntactic tag, in-
cluding the value of case, etc., of the other (potential
argument). This differs from standard (lexical) col-
location, where the same properties of both objects
are considered, namely, their lemmata.
Formally, we define a feature f to be a pair
of functions lf : so ? Lf and rf : so ? Rf , where
so stands for the set of syntactic objects and Lf , Rf
are the investigated properties. For example, to learn
dependencies between verbs and case values of their
objects, we can take lf (w) = base(semhead(w))
(the lemma of the semantic head of w) and rf (w) =
case(synhead(w)) (the case value of the syntactic
head of w). On the other hand, in order to obtain the
usual collocations, it is sufficient to take both func-
tions as mapping a syntactic object to a base form
of its semantic head.
What features should be considered in the task
of finding dependencies between syntactic objects?
The two features mentioned above, aimed at finding
lexical collocations and valence relations, are obvi-
ously useful. However, in a morphologically rich
language, like Polish, taking the full morphosyntac-
tic tag as the value of a feature function leads to
the data sparsity problem. Clearly, the most im-
portant valence information a tag may contribute is
part of speech and grammatical case. Hence, we
define the second function in the ?valence? feature
more precisely to be the base form and grammati-
cal case (if any), if the syntactic object is a preposi-
tion, or part of speech and grammatical case (if any),
otherwise. For example, consider the sentence Who
cares for the carers? and assume that it has already
been split into basic syntactic objects in the follow-
ing way: [Who] [cares] [for the carers] [?]. The syn-
tactic head of the third object is for and the lemma of
the semantic head is CARER. So, the valence feature
for the pair care and for the carers (both defined be-
low via their syntactic and semantic heads) will give:
lval (?CARE:verb, 3s; CARE:verb, 3s?) = CARE
rval (?FOR:prep, obj; CARER:noun, pl?) = ?FOR, obj?,
where 3s stands for the ?3rd person singular? prop-
erty of verbs and obj stands for the objective case in
English.
Additionally, 7 morphosyntactic features are de-
fined by projecting both syntactic objects onto any
(but the same of the two objects) combination
of grammatical case, gender and number. For exam-
ple one of those features is defined in the following
way:
lf (w) = rf (w) =
= ?case(synhead(w)), gender(synhead(w))?.
Another feature relates the two objects? syntactic
heads, by looking at the part of speech of the first
one and the case of the other one. The final feature
43
records information about syntactic (number, gen-
der, case) agreement between the objects.
2.2 Lexico-Semantic Features
Obviously, the semantics of syntactic objects is im-
portant in deciding which two objects are directly
connected in a syntactic tree. To this end, we utilise
a wordnet.
Ideally, we would like to represent a syntactic ob-
ject via its semantic class. In wordnets, semantic
classes are approximated by synsets (synonym sets)
which are ordered by the hyperonymy relation. We
could represent a syntactic object by its directly cor-
responding synset, but in terms of generalisation this
would hardly be an improvement over representing
such an object by its semantic head. In most cases
we need to represent the object by a hypernym of
its synset. But how far up should we go along the
hypernymy path to find a synset of the right granu-
larity? This is a difficult problem, so we leave it to
the classifier. Instead, lexico-semantic features are
defined in such a way that, for a given lexeme, all its
hypernyms are counted as observations.
After some experimentation, three features based
on this idea are defined:
1. lf (w) = base(semhead(w))
rf (w) = sset(w)
(for all sset(w) ? hypernyms(w)),
2. lf (w) = base(semhead(w))
rf (w) = ?sset(w), case(synhead(w))?
(for all sset(w) ? hypernyms(w)),
3. lf (w) = sset(w)
rf (w) = sset(w)
In the last feature, where both objects are repre-
sented by synsets, only those minimally general hy-
pernyms of the two objects are considered that co-
occur in the training corpus more than T (thresh-
old) times. In the experiments described below,
performed on a 1-million-word training corpus, the
threshold was set to 30.
2.3 Association Measures
For any two syntactic objects in the same sentence
the strength of association is computed between
them using each of the 14 features (standard col-
locations, 10 morphosyntactic features, 3 lexico-
semantic features) defined above. In fact, we use
not 1 but 6 association measures most suitable for
language analysis according to (Seretan, 2011): log
likehood ratio, chi-squared, t-score, z-score, point-
wise mutual information and raw frequency. The
last choice may seem disputable, but as was shown
in (Krenn and Evert, 2001) (and reported in vari-
ous works on valence acquisition), in some cases
raw frequency behaves better than more sophisti-
cated measures.
We are well aware that some of the employed
measures require the distribution of frequencies to
meet certain conditions that are not necessarily ful-
filled in the present case. However, as explained in
the following subsection, the decision which mea-
sures should ultimately be taken into account is left
to a classifier.
2.4 Classifiers
Let us first note that no treebank is needed for
computing the features and measures presented in
the previous section. These measures represent co-
occurrence strengths of syntactic objects based on
different grouping strategies (by lemma, by part
of speech, by case, gender, number, by wordnet
synsets, etc.). Any large, morphosyntactically an-
notated (and perhaps chunked) corpus is suitable for
computing such features. A treebank is only needed
to train a classifier which uses such measures as in-
put signals.2
In order to apply Machine Learning classifiers,
one must formally define what counts as an instance
of the classification problem. In the current case, for
each pair of syntactic objects in a sentence, a single
instance is generated with the following signals:
? absolute distance (in terms of the number of
sytnactic objects in between),
? ordering (the sign of the distance),
? 6 measures (see ? 2.3) of lexical collocation,
? 10 ? 6 = 60 values of morphosyntactic co-
occurrence measures,
? 3? 6 = 18 values of lexico-semantic (wordnet-
based) co-occurrence measures,
? a single binary signal based on 14 high-
precision low-recall handwritten syntactic de-
2We use the term signal instead of the more usual feature in
order to avoid confusion with features defined in ? 2.1 and in
? 2.2.
44
cision rules which define common grammati-
cal patterns like verb-subject agreement, geni-
tive construction, etc.; the rules look only at the
morphosyntactic tags of the heads of syntactic
objects,
? the classification target from the treebank: a bi-
nary signal describing whether the given pair of
syntactic objects form an edge in the parse tree.
The last signal is used for training the classifier and
then for evaluation. Note that lexical forms of the
compared syntactic objects or their heads are not
passed to the classifier, so the size of the training
treebank can be kept relatively small.
An inherent problem that needs to be addressed
is the imbalance between the sizes of two classifi-
cation categories. Of course, most of the pairs of
the syntactic objects do not form an edge in the
parse tree, so a relatively high classification accu-
racy may be achieved by the trivial classifier which
finds no edges at all. We experimented with various
well-known classifiers, such as decision trees, Sup-
port Vector Machines and clustering algorithms, and
also tried subsampling3 of the imbalanced data. Fi-
nally, satisfactory results were achieved by employ-
ing a Balanced Random Forest classifier.
Random Forest (Breiman, 2001) is a set of un-
pruned C4.5 (Quinlan, 1993) decision trees. When
building a single tree in the set, only a random subset
of all attributes is considered at each node and the
best is selected for splitting the data set. Balanced
Random Forest (BRF, (Chen et al, 2004)) is a mod-
ified version of the Random Forest. A single tree
of BRF is built by first randomly subsampling the
more frequent instances in the training set to match
the number of less frequent ones and then creating
a decision tree from this reduced data set.
3 Experiments and Evaluation
The approach presented above has been evaluated on
Polish.
First, a manually annotated 1-million-word
subcorpus of the National Corpus of Polish
(Przepi?rkowski et al, 2010), specifically, its mor-
phosyntactic and shallow syntactic annotation, was
3Removing enough negative instances in the training set to
balance the numbers of instances representing both classes.
used to compute the co-occurrence statistics. The
wordnet used for lexico-semantic measures was
S?owosiec? (Piasecki et al, 2009; Maziarz et al,
2012), the largest Polish wordnet.
Then a random subset of sentences from this cor-
pus was shallow-parsed by Spejd (Buczyn?ski and
Przepi?rkowski, 2009) and given to linguists, who
added annotation for the dependency links between
syntactic objects. Each sentence was processed by
two linguists, and in case of any discrepancy, the
sentence was simply rejected. The final corpus con-
tains 963 sentences comprising over 8000 tokens.
From this data we obtained over 23 500 classi-
fication problem instances. Then we performed
the classification using a BRF classifier written for
Weka (Witten and Frank, 2005) as part of the re-
search work on definition extraction with BRFs
(Kobylin?ski and Przepi?rkowski, 2008). The re-
sults were 10-fold cross-validated. A similar exper-
iment was performed taking into account only those
instances which describe syntactic objects with se-
mantic heads present in the wordnet. The results
were measured in terms of precision and recall over
edges in the syntactic tree: what percentage of found
edges are correct (precision) and what percentage of
correct edges were found by the algorithm (recall).
The obtained measures are presented in Table 1.
Expected
YES NO Classified
2674 319 YES
1781 21250 NO
Precision: 0.89
Recall: 0.60
F-measure: 0.72
Expected
YES NO Classified
1933 241 YES
1008 13041 NO
Precision: 0.89
Recall: 0.66
F-measure: 0.76
Table 1: Confusion matrix (# of instances) and measures
for the full data set and for data present in wordnet.
45
We also looked at the actual decision trees that
were generated during the training. We note that
the signal most frequently observed near the tops of
decision trees was the one from handwritten rules.
The second one was the distance. By looking at
the trees, we could not see any clear preferences for
other types of signals. This suggests that both mor-
phosyntactic and lexico-semantic signals contribute
to the accuracy of the classification.
Based on this inspection of decision trees, we per-
formed another experiment to learn how much im-
provement we get from generalised collocation sig-
nals. We evaluated ? on the same data ? a not so
trivial baseline algorithm which, for each syntactic
object, creates an edge to its nearest neighbour ac-
cepted by the handwritten rules, if any. Note that
this baseline builds on the fact that a node in a parse
tree has at most one parent, whereas the algorithm
described above does not encode this property, yet;
clearly, there is still some room for improvement.
The baseline reaches 0.78 precision and 0.47 recall
(F-measure is 0.59). Therefore, the improvement
from co-occurrence signals over this strong baseline
is 0.13, which is rather high. Also, given the high
precision, our algorithm may be suitable for using
in a cascade of classifiers.
4 Related Work
There is a plethora of relevant work on resolving PP-
attachment ambiguities in particular and finding de-
pendency links in general, and we cannot hope to do
it sufficient justice here.
One line of work, exemplified by the early influ-
ential paper (Hindle and Rooth, 1993), posits the
problem of PP-attachment as the problem of choos-
ing between a verb v and a noun n1 when attaching
a prepositional phrase defined by the syntactic head
p and the semantic head n2. Early work, including
(Hindle and Rooth, 1993), concentrated on lexical
associations, later also using wordnet information,
e.g., (Clark and Weir, 2000), in a way similar to
that described above. Let us note that this scenario
was criticised as unrealistic by (Atterer and Sch?tze,
2007), who argue that ?PP attachment should not
be evaluated in isolation, but instead as an integral
component of a parsing system, without using in-
formation from the gold-standard oracle?, as in the
approach proposed here.
Another rich thread of relevant research is con-
cerned with valence acquisition, where shallow
parsing and association measures based on mor-
phosyntactic features are often used at the stage
of collecting evidence, (Manning, 1993; Korhonen,
2002), also in work on Polish, (Przepi?rkowski,
2009). However, the aim in this task is the construc-
tion of a valence dictionary, rather than disambigua-
tion of attachment possibilities in a corpus.
A task more related to the current one is presented
in (Van Asch and Daelemans, 2009), where a PP-
attacher operates on top of a shallow parser. How-
ever, this memory-based module is fully trained on
a treebank (Penn Treebank, in this case) and is con-
cerned only with finding anchors for PPs, rather than
with linking any dependents to their heads.
Finally, much work has been devoted during the
last decade to probabilistic dependency parsing (see
(K?bler et al, 2009) for a good overview). Clas-
sifiers deciding whether ? at any stage of depen-
dency parsing ? to perform shift or reduce typically
rely on lexical and morphosyntactic, but not lexico-
semantic information (Nivre, 2006). Again, such
classifiers are fully trained on a treebank (converted
to parser configurations).
5 Conclusion
Treebanks are very expensive, morphosyntactically
annotated corpora are relatively cheap. The main
contribution of the current paper is a novel approach
to factoring out syntactic training in the process
of learning of syntactic attachment. All the fine-
grained lexical training data were collected from
a relatively large morphosyntactically annotated and
chunked corpus, and only less than 100 signals (al-
though many of them continuous) were used for
training the final classifier on a treebank. The ad-
vantage of this approach is that reasonable results
can be achieved on the basis of tiny treebanks (here,
less than 1000 sentences).
We are not aware of work fully analogous to ours,
either for Polish or for other languages, so we cannot
fully compare our results to the state of the art. The
comparison with a strong baseline algorithm which
uses handwritten rules shows a significant improve-
ment ? over 0.13 in terms of F-measure.
46
Acknowledgments
This research is supported by the POIG.01.01.02-
14-013/09 project which is co-financed by the Eu-
ropean Union under the European Regional Devel-
opment Fund.
References
Michaela Atterer and Hinrich Sch?tze. 2007. Preposi-
tional phrase attachment without oracles. Computa-
tional Linguistics, 33(4):469?476.
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45:5?32.
Aleksander Buczyn?ski and Adam Przepi?rkowski. 2009.
Spejd: A shallow processing and morphological dis-
ambiguation tool. In Zygmunt Vetulani and Hans
Uszkoreit, editors, Human Language Technology:
Challenges of the Information Society, volume 5603
of Lecture Notes in Artificial Intelligence, pages 131?
141. Springer-Verlag, Berlin.
Chao Chen, Andy Liaw, and Leo Breiman. 2004. Us-
ing random forest to learn imbalanced data. Technical
Report 666, University of California, Berkeley.
Stephen Clark and David Weir. 2000. A class-based
probabilistic approach to structural disambiguation. In
In Proceedings of the 18th International Conference
on Computational Linguistics, pages 194?200.
Donald Hindle and Mats Rooth. 1993. Structural ambi-
guity and lexical relations. Computational Linguistics,
19(1):103?120.
?ukasz Kobylin?ski and Adam Przepi?rkowski. 2008.
Definition extraction with balanced random forests.
In Bengt Nordstr?m and Aarne Ranta, editors, Ad-
vances in Natural Language Processing: GoTAL 2008,
Gothenburg, Sweden, volume 5221 of Lecture Notes
in Artificial Intelligence, pages 237?247, Berlin.
Springer-Verlag.
Anna Korhonen. 2002. Subcategorization Acquisition.
PhD Thesis, University of Cambridge.
Brigitte Krenn and Stefan Evert. 2001. Can we do better
than frequency? A case study on extracting PP-verb
collocations. In Proceedings of the ACL Workshop on
Collocations, Toulouse, France.
Sandra K?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan & Claypool.
Christopher D. Manning. 1993. Automatic acquisition of
a large subcategorization dictionary from corpora. In
Proceedings of the 31st Annual Meeting of the Associ-
ation for Computational Linguistics, pages 235?242,
Columbus, OH.
Marek Maziarz, Maciej Piasecki, and Stanis?aw Sz-
pakowicz. 2012. Approaching plWordNet 2.0. In
Proceedings of the 6th Global Wordnet Conference,
Matsue, Japan.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer-Verlag, Berlin.
Maciej Piasecki, Stanis?aw Szpakowicz, and Bartosz
Broda. 2009. A Wordnet from the Ground
Up. Oficyna Wydawnicza Politechniki Wroclawskiej,
Wroc?aw.
Adam Przepi?rkowski, Rafa? L. G?rski, Marek ?azin?ski,
and Piotr Pe?zik. 2010. Recent developments in the
National Corpus of Polish. In Proceedings of the Sev-
enth International Conference on Language Resources
and Evaluation, LREC 2010, Valletta, Malta. ELRA.
Adam Przepi?rkowski. 2009. Towards the automatic ac-
quisition of a valence dictionary for Polish. In Ma?-
gorzata Marciniak and Agnieszka Mykowiecka, edi-
tors, Aspects of Natural Language Processing, volume
5070 of Lecture Notes in Computer Science, pages
191?210. Springer-Verlag, Berlin.
John Ross Quinlan. 1993. C4.5 Programs for Machine
Learning. Morgan Kaufmann.
Violeta Seretan. 2011. Syntax-Based Collocation Ex-
traction. Text, Speech and Language Technology.
Springer, Dordrecht.
Vincent Van Asch and Walter Daelemans. 2009. Prepo-
sitional phrase attachment in shallow parsing. In
Proceedings of the International Conference RANLP-
2009, pages 12?17, Borovets, Bulgaria, September.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Morgan
Kaufmann, San Francisco, CA, 2nd edition.
47
