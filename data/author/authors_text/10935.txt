Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 382?385,
Prague, June 2007. c?2007 Association for Computational Linguistics
UC3M: Classification of Semantic Relations between Nominals using 
Sequential Minimal Optimization  
Isabel Segura Bedmar 
Computer Science Department 
University Carlos III of Madrid 
isegura@inf.uc3m.es 
Doaa Samy 
Computer Science Department 
University Carlos III of Madrid 
dsamy@inf.uc3m.es 
Jose L. Martinez 
Computer Science Department 
University Carlos III of Madrid 
jlmartinez@inf.uc3m.es 
 
Abstract 
This paper presents a method for auto-
matic classification of semantic relations 
between nominals using Sequential 
Minimal Optimization. We participated 
in the four categories of SEMEVAL task 
4 (A: No Query, No Wordnet; B: Word-
Net, No Query; C: Query, No WordNet; 
D: WordNet and Query) and for all train-
ing datasets. Best scores were achieved 
in category B using a set of feature vec-
tors including lexical file numbers of 
nominals obtained from WordNet and a 
new feature WordNet Vector designed 
for the task1. 
1 Introduction 
The survey of the state-of-art reveals an increas-
ing interest in automatically discovering the un-
derlying semantics in natural language. In this 
interdisciplinary field, the growing interest is 
justified by the number of applications which 
can directly benefit from introducing semantic 
information. Question Answering, Information 
Retrieval and Text Summarization are examples 
of these applications (Turney and Littman, 2005; 
Girju et al, 2005).  
In the present work and for the purpose of the 
SEMEVAL task 4, our scope is limited to the 
semantic relationships between nominals. By 
this definition, we understand it is the process of 
discovering the underlying relations between 
two concepts expressed by two nominals.  
                                                 
1 This work has been partially supported by the Re-
gional Government of Madrid Ander the Research 
Network MAVIR (S-0505/TIC-0267) 
Within the framework of SEMEVAL, nomi-
nals can occur either on the phrase, clause or the 
sentence level. This fact constitutes the major 
challenge in this task since most of the previous 
research limited their approaches to certain types 
of nominals mainly the ?compound nomi-
nals?(Girju et al 2005). 
The paper is divided as follows; section 2 is a 
brief introduction to SMO used as the classifier 
for the task. Section 3 is dedicated to the de-
scription of the set of features applied in our ex-
periments. In section 4, we discuss the experi-
ment?s results compared to the baselines of the 
SEMEVAL task and the top scores. Finally, we 
summarize our approach, pointing out conclu-
sions and future directions of our work. 
2 Sequential Minimal Optimization 
We decided to use Support Vector Machine 
(SVM), as one of the most successful Machine 
Learning techniques, achieving the best per-
formances for many classification tasks. Algo-
rithm performance and time efficiency are key 
issues in our task, considering that our final goal 
is to apply this classification in a Question An-
swering System.  
Sequential Minimal Optimization (SMO) is a 
fast method to train SVM. SMO breaks the large 
quadratic programming (QP) optimization prob-
lem needed to be resolved in SVM into a series 
of smallest possible QP problems. These small 
QP problems are analytically solved, avoiding, 
in this way, a time-consuming numerical QP 
optimization as an inner loop. We used Weka 
(Witten and Frank, 2005) an implementation of 
the SMO (Platt, 1998). 
382
3 Features 
Prior to the classification of semantic rela-
tions, characteristics of each sentence are auto-
matically extracted using GATE (Cunningham 
et al, 2002). GATE is an infrastructure for de-
veloping and deploying software components for 
Language Engineering. We used the following 
GATE components: English Tokenizer, Part-Of-
Speech (POS) tagger and Morphological ana-
lyser. 
The set of features used for the classification 
of semantic relations includes information from 
different levels: word tokens, POS tags, verb 
lemmas, semantic information from WordNet, 
etc. Semantic features are only applied in cate-
gories B and D.  
On the lexical level, the set of word features 
include the two nominals, their heads in case one 
of the nominals in question or both are com-
pound nominals ( e.g. the relation between 
<e1>tumor shrinkage</e1> and <e2>radiation 
therapy </e2> is actually between the head of 
the first ?shrinkage? and ?radiation_therapy?). 
More features include: the two words before the 
first nominal, the two words after the second 
nominal, and the word list in-between (Wang et 
al., 2006).  
On the POS level, we opted for using a set of 
POS features since word features are often too 
sparse. This set includes POS tags of the two 
words occurring before the first nominal and the 
two words occurring after the second nominal 
together with the tag list of the words in-
between (Wang et al, 2006). POS tags of nomi-
nals are considered redundant information.  
Information regarding verbs and prepositions, 
occurring in-between the two nominals, is highly 
considered. In case of the verb, the system takes 
into account the verb token and the information 
concerning the voice and lemma. In the same 
way, the system keeps track of the prepositions 
occurring between both nominals. In addition, a 
feature, called numinter, indicating the number 
of words between nominals is considered. 
Other important feature is the path from the 
first nominal to the second nominal. This feature 
is built by the concatenation of the POS tags be-
tween both nominals.  
The feature related to the query provided for 
each sentence is only considered in the catego-
ries C and D according to the SEMEVAL re-
strictions. 
On the semantic level, we used features ob-
tained from WordNet.  In addition to the Word-
Net sense keys, provided for each nominal, we 
extracted its synset number and its lexical file 
number.  
Based on the work of Rosario, Hearst and 
Fillmore (2002), we suppose that these lexical 
file numbers can help to determine if the nomi-
nals satisfy the restrictions for each relation. For 
example, in the relation Theme-Tool, the theme 
should be an object, an event, a state of being, an 
agent, or a substance. Else, it is possible to af-
firm that the relation is false.  
For the Part-Whole relation and due to its 
relevance in this classification task, a feature 
indicating metonymy relation in WordNet was 
taken into account. 
Furthermore, we designed a new feature, 
called WordNet vector. For constructing this 
vector, we selected the synsets of the third level 
of depth in WordNet and we detected if each is 
ancestor or not of the nominal. It is a binary vec-
tor, i.e. if the synset is ancestor of the nominal it 
is assigned the value 1, else it is assigned the 
value 0. In this way, we worked with two vec-
tors, one for each nominal. Each vector has a 
dimension of 13 coordinates. Each coordinate 
represents one of the 13 nodes in the third level 
of depth in WordNet. Our initial hypothesis con-
siders that this representation for the nominals 
could perform well on unseen data.  
4 Experiment Results 
Cross validation is a way to test the ability of 
the model to classify unseen examples. We 
trained the system using 10-fold cross-
validation; the fold number recommended for 
small training datasets. For each relation and for 
each category (A, B, C, D) we selected the set of 
features that obtained the best results using the 
indicated cross validation.  
We submitted 16 sets of results as we partici-
pated in the four categories (A, B, C, D). We 
also used all the possible sizes of the training 
dataset (1: 1 to 35, 2:1 to 70, 3:1 to 106, 4:1 to 
140). 
383
 A: No Query, No 
WordNet 
B: No Query, 
WordNet 
C: No Query, No 
WordNet 
D: Query, Word-
Net 
 Prec Rec F Prec Rec F Prec Rec F Prec Rec F 
Cause-Effect 50.0 51.2 50.6 66.7   73.2 69.8 42.9   36.6   39.5   59.0   56.1   57.5   
Instrument-Agency 47.5 50.0 48.7 73.7   73.7   73.7   51.4   50.0   50.7   67.5   71.1   69.2   
Product-Producer 65.3 51.6 57.7 83.7   66.1   73.9   67.4   50.0   57.4   74.5   61.3   67.3   
Origin-Entity 50.0 27.8 35.7 63.0   47.2   54.0   54.5   33.3   41.4   63.3   52.8   57.6   
Theme-Tool 50.0 27.6 35.6 50.0   48.3   49.1   47.4   31.0   37.5   40.9   31.0   35.3   
Part-Whole 26.5 34,6 30.0 72.4   80.8   76.4   34.0   61.5   43.8   57.1   76.9   65.6   
Content-Container 48.4 39.5 43.5 57.6   50.0   53.5   48.6   44.7   46.6   63.6   55.3   59.2   
Avg for UC3M 48.2 40.3   43.1   66.7 62.8  64.3  49.4   43.9   45.3   60.9   57.8   58.8   
Avg for all systems 59.2 58.7 58.0 65.3 64.4 63.6 59.9 59.0 58.4 64.9 60.4 60.6 
Max Avg F   64.8   72.4   65.1   62.6 
Table  1 Scores for A4, B4, C4 and D4
For some learning algorithms such as decision 
trees and rule learning, appropriate selection of 
features is crucial. For the SVM model, this is 
not so important due to its learning mechanism, 
where irrelevant features are usually balanced 
between positive and negative examples for a 
given binary classification problem. However, in 
the experiments we observed that certain fea-
tures have strong influence on the results, and its 
inclusion or elimination from the vector, influ-
enced remarkably the outcomes. 
In this section, we will briefly discuss the ex-
periments in the four categories highlighting the 
most relevant observations. 
In category A, we expected to obtain better 
results, but the overall performance of the sys-
tem has decreased in the seven relations. This 
shows that our system has over-fitted the train-
ing set. The contrast between the F score values 
in the cross-validation and the final test results 
demonstrates this fact. For all the relations in the 
category A4, we obtained an average of 
F=43.1% [average score of all participating 
teams: F=58.0% and top average score: 
F=64.8%].  
In Product-Producer relation, only two fea-
tures were used: the two heads of the nominals. 
In training, we obtained an average F= 60% us-
ing cross-validation, while in the final test data, 
we achieved an average score F=57.7%. For the 
relation Theme-Tool, other set of features was 
employed: nominals, their heads, verb, preposi-
tion and the list of word between both nominals. 
Based on the results of the 10-fold cross valida-
tion, we expected to obtain an average of the 
F=70%. Nevertheless, the score obtained is F 
=30%.   
In category B, our system has achieved better 
scores. Our average score F is 64.3% and it is 
above the average of participating teams 
(F=63.6%) and the baseline.  
Best results in this category were achieved in 
the relations: Instrument-Agency (F=73.7%), 
Product-Producer (F=73.9%), Part-Whole 
(F=76.4%). However, for the relation Theme-
Tool the system obtained lower scores 
(F=49.1%). 
It is obvious that introducing WordNet infor-
mation has improved notably the results com-
pared with the results obtained in the category 
A.  
In categories C and D, only three groups have 
participated. In category C (as in category A), 
the system results have decreased obviously 
(F=45.3%) with respect to the expected scores in 
the 10-fold cross validation. Moreover, the score 
obtained is lower than the average score of all 
participants (F=58.4%) and the best score 
(F=65.1%). For example, in training the Instru-
ment-Agent relation, the system achieved an 
average F=78% using 10-fold cross-validation, 
while for the final score it only obtained 
F=50.7%.  
Results reveal that the main reason behind the 
low scores in A and C, is the absence of infor-
mation from WordNet. Hence, the vector design 
needs further consideration in case no semantic 
information is provided. 
In category D, both WordNet senses and 
query were used, we achieved an average score 
F=58.8%. The average score for all participants 
is F=60.6% and the best system achieved 
F=62.6%. However, the slight difference shows 
that our system worked relatively well in this 
category.  
384
Both run time and accuracy depend critically 
on the values given to two parameters: the upper 
bound on the coefficient?s values in the equation 
for the hyperplane (-C), and the degree of the 
polynomials in the non-linear mapping (-E) 
(Witten and Frank, 2005). Both are set to 1 by 
default. The best settings for a particular dataset 
can be found only by experimentation.  
We made numerous experiments to find the 
best value for the parameter C (C=1, C=10, 
C=100, C=1000, C=10000), but the results were 
not remarkably affected. Probably, this is due to 
the small size of the training set. 
5 Conclusions and Future Work  
In our first approach to automatic classifica-
tion of semantic relations between nominals and 
as expected from the training phase, our system 
achieved its best performance using WordNet 
information. In general, we obtained better 
scores in category 4 (size of training: 1 to 140), 
i.e., when all the training examples are used.  
On the other hand, overfitting the training 
data (most probably due to the small size of 
training dataset) is the main reason behind the 
low scores obtained by our system. 
These facts lead us to the conclusion that se-
mantic features from WordNet, in general, play 
a key role in the classification task. However, 
the relevance of WordNet-related features var-
ies. For example, lexical file numbers proved to 
be highly effective, while the use of the Word-
Net Vector did not improve significantly the re-
sults. Thus, we consider that a level 3 WordNet 
Vector is rather abstract to represent each nomi-
nal. Developing a WordNet Vector with a deeper 
level (> 3) could be more effective as the repre-
sentation of nouns is more descriptive. 
Query features, on the other hand, did not im-
prove the performance of the system. This is due 
to the fact that the same query could represent 
both positive and negative examples of the rela-
tion. However, to improve results in categories 
A and C, more features need to introduced, es-
pecially context and syntactic information such 
as chunks or dependency relations. 
To improve results across the whole dataset, 
wider use of semantic information is necessary. 
For example, the immediate hypernym for each 
synset obtained from WordNet could help in 
improving the system performance (Nastase et 
al., 2006). Besides, information regarding the 
entity features could help in the classification of 
some relations like Origin-Entity or Product-
Producer. Other semantic resources such as 
VerbNet, FrameNet, PropBank, etc. could also 
be used. 
Furthermore, we consider introducing a Word 
Sense Disambiguation module to obtain the cor-
responding synsets of the nominals. Also, in-
formation concerning the synsets of the list of 
the context words could be of great value for the 
classification task (Wang et al, 2006). 
References 
Hamish Cunningham, Diana Maynard and Kalina 
Bontcheva, Valentin Tablan, Cristian Ursu. 2002. The 
GATE User Guide. http://gate.ac.uk/ 
Roxana Girju, Dan Moldovan, Marta Tatu and Daniel An-
tohe. 2005. On the semantics of noun compunds. Com-
puter Speech and Language 19 pp. 479-496. 
Vivi Nastase, Jelber Sayyad-Shirbad, Marina Sokolova and 
Stan Szpakowicz. 2006. Learning noun-modifier seman-
tic relations with corpus-based and WordNet-based fea-
tures. In Proc. of the 21st National Conference on Artifi-
cial Intelligence (AAAI 2006). Boston, MA. 
John C. Platt. 1998. Sequential Minimal Optimization: A 
Fast Algorithm for Training Support Vector Machines, 
Microsoft Research Technical Report MSR-TR-98-14. 
Barbara Rosario, Marti A. Hearst, and Charles Fillmore. 
2002. ?The descent of hierarchy, and selection in rela-
tions semantics?. In Proceedings of the 40 th Annual 
Meeting of the Association for Computacional Linguis-
tics (ACL?02), Philadelphia, PA, pages 417-424. 
Ian H. Witten, Eibe Frank. 2005. Data Mining: Practical 
machine learning tools and techniques. Morgan Kauf-
mann.  
Peter D. Turney and Michael L. Littman. 2005. Corpus-
based learning of analogies and semantic rela-tions. Ma-
chine Learning, in press. 
Ting Wang, Yaoyong Li, Kalina Bontcheva, Hamish Cun-
ningham and Ji Wang. 2006. Automatic Extraction of 
Hierarchical Relations from Text. In Proceedings of the 
Third European Semantic Web Conference (ESWC 
2006), Lecture Notes in Computer Science 4011, 
Springer, 2006. 
385
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 100?101,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A preliminary approach to recognize generic drug names by combining 
UMLS resources and USAN naming conventions 
 
Isabel Segura-Bedmar Paloma Mart?nez Doaa Samy 
Computer Sciences Department Computer Sciences Department Linguistic Department 
Carlos III University of Madrid Carlos III University of Madrid Cairo University 
Avd. Universidad, 30, Legan?s, 
28911, Madrid, Spain 
Avd. Universidad, 30, Legan?s, 
28911, Madrid, Spain 
Egypt 
isegura@inf.uc3m.es pmf@inf.uc3m.es dsamy@cu.edu.eg 
  
 
Abstract 
This paper presents a system1 for drug name 
identification and classification in biomedical 
texts.  
1 Introduction 
Numerous studies have tackled gene and protein 
names recognition (Collier et al 2002), (Tanabe 
and Wilbur, 2002). Nevertheless, drug names have 
not been widely addressed (Rindflesch et al, 
2000). 
Automating the process of new drugs recognition 
and classification is a challenging task. With the 
rapidly changing vocabulary, new drugs are 
introduced while old ones are made obsolete. 
Though the terminological resources are frequently 
updated, they can not follow the accelerated pace 
of the changing terminology. 
Drug receives three distinct names: the chemical 
name, the generic (or nonproprietary) name, and 
the brand (or trademark) name. The U.S. Adopted 
Name (USAN) Council establishes specific 
nomenclature rules for naming generic drugs. 
These rules rely on the use of affixes that classify 
drugs according to their chemical structure, 
indication or mechanism of action. For example, 
analgesics substances can receive affixes such as  
-adol-, -butazone, -fenine, -eridine and ?fentanil. 
In the present work, we focus, particulary, on the 
implementation of a set of 531 affixes approved by 
                                                           
1 This work has been partially supported by the projects: FIT-
350300-2007-75 (Semantic Interoperability in Electronic 
Health Care) and TIN2007-67407-C03-01 (BRAVO: 
Advanced Multimodal and Multilingual Question Answering). 
the USAN Council and published in 20072. The 
affixes allow a specific classification of drugs on 
pharmacological families, which ULMS Semantic 
NetWork is unable to provide. 
2 The System 
The system consists of four main modules: a basic 
text processing module, WordNet look-up module, 
UMLS look-up module and the USAN rules 
module, as shown in Figure 1.  
A corpus of 90 medical abstracts was compiled for 
the experiment. For the basic processing of the 
abstracts, GATE3 architecture is used. This text 
processing provides sentence segmentation, 
tokenization and POS tagging. Tokens which 
receive a noun or proper noun POS tag are 
extracted. 
The nouns found on WordNet are discarded and 
those which are not found in WordNet are looked 
up in the UMLS Metathesaurus. If a noun is found 
in UMLS, it is tagged with its corresponding 
semantic types as assigned by UMLS. A subset of 
these nouns is tagged as ?drug? if their semantic 
types are ?Pharmacological Substance? or 
?Antibiotic?. Finally, nouns which have not been 
found in UMLS are tagged as ?unknown?. 
The list of nouns tagged as ?drug? is passed to the 
rule module to detect their pharmacological 
families according to the affixes. In addition, the 
rule module processes the list of ?unknown? nouns 
which are not found in UMLS to check the 
presence of affixes, and thereby, of possible drugs. 
3 Preliminary results 
                                                           
2 http://www.ama-
assn.org/ama1/pub/upload/mm/365/usan_stem_list.pdf 
Accessed January 2008 
3 http://www.gate.ac.uk/ 
100
A manual evaluation by a domain4 expert was 
carried out. The list of nouns not found in 
WordNet contained 1885 initial candidates. This 
initial list is looked up in UMLS and 93.4% of 
them (1761) is linked with some concepts of 
UMLS. The UMLS module recognized 1400 
nouns as pharmacological substances or 
antibiotics. The rest of nouns, 361, are detected by 
UMLS but neither as pharmacological substance 
nor as antibiotics.  
The expert manually evaluated the set of nouns 
detected by UMLS as pharmacological substances 
or antibiotics (1400). Evaluation showed that only 
1100 were valid drugs.  
 
Figure 1 System Architecture 
The list of nouns (124) which have not been found 
in UMLS are processed by the rule module to 
detect new candidate drugs not included in UMLS. 
This module only detects 17 candidate drugs. The 
manual evaluation showed that 7 of them were 
valid drugs and the rest of nouns are biomedical 
concepts not included in UMLS. Some of these 
drugs are Mideplanin, Tomopenem, Elvitegravir, 
and so on. The rest of nouns neither detected by 
the UMLS module nor by the rules module, 106, 
were also validated by the expert in order to 
estimate the overall coverage of our approach. The 
evaluation of these nouns shows that only 7 of 
them are valid drugs, however, the rest of the 
nouns are named entities of the general domain 
(organization, person names or cities) or 
biomedical concepts. Introducing a module of 
generic NER should decrease the noise caused by 
such entities.  
                                                           
4 The authors are grateful to Maria Bedmar Segura, Manager 
of the Drug Information Center, Mostoles University Hospital, 
for her valuable assistance in the evaluation of the system. 
Finally, precision and recall of the overall system 
combining UMLS and rules were calculated. The 
system achieved 78% of precision and 99.3% of 
recall  
3.1 The classification in pharmacological 
families 
Once processed by the rule module, 73.8% of the 
candidate drugs recognised by UMLS were also 
classified in pharmacological families by the 
USAN naming rules. Expert?s evaluation of the 
rule-based classification showed that rules 
achieved 89% precision. Short affixes such as ?ol, 
?pin and -ox are responsible of the wrong 
classifications. Thus, additional clues are necessary 
to detect these drug families. 
4 Some Conclusions  
As a preliminary approach, it is a first step towards 
a useful Information Extraction System in the field 
of Pharmacology. Though evaluation reveals that 
rules alone are not feasible enough in detecting 
drugs, but they help to improve the coverage. In 
addition, rules provide a drug classification in 
pharmacological families. Such classification is an 
added value in the development of NLP 
applications within the pharmacological domain.  
For future work, the approach will be extended to 
address additional information about 
pharmacologic classes included in many 
biomedical terminologies integrated in the UMLS 
such as MeSH or SNOMED. 
Future work will also target a wider coverage and a 
bigger set of drug types through including more 
affixes, detecting complex entities (multi-words), 
detecting synonyms, resolving acronyms and 
ambiguities as well as using contextual information 
to disambiguate the correct semantic type of each 
term occurring in the texts.  
References  
Collier N, Takeuchi K. 2004. Comparison of characterlevel 
and part of speech features for name recognition in bio-
medical texts:423? 35. 
Rindflesch, T.C., Tanabe,L., Weinstein,J.N. and Hunter,L. 
2000. EDGAR: extraction of drugs, genes and relations 
from the biomedical literature. Pac. Symp. Biocomput. 5, 
517?528 
Tanabe, L. y Wilbur, W.J. 2002. Tagging gene and protein 
names in biomedical text. Bioinformatics 18, 1124?1132 
101
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 341?350, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 9 : Extraction of Drug-Drug Interactions from
Biomedical Texts (DDIExtraction 2013)
Isabel Segura-Bedmar, Paloma Mart??nez, Mar??a Herrero-Zazo
Universidad Carlos III de Madrid
Av. Universidad, 30, Legane?s 28911, Spain
{isegura,pmf}@inf.uc3m.es, mhzazo@pa.uc3m.es
Abstract
The DDIExtraction 2013 task concerns the
recognition of drugs and extraction of drug-
drug interactions that appear in biomedical
literature. We propose two subtasks for the
DDIExtraction 2013 Shared Task challenge:
1) the recognition and classification of drug
names and 2) the extraction and classification
of their interactions. Both subtasks have been
very successful in participation and results.
There were 14 teams who submitted a total of
38 runs. The best result reported for the first
subtask was F1 of 71.5% and 65.1% for the
second one.
1 Introduction
The definition of drug-drug interaction (DDI) is
broadly described as a change in the effects of one
drug by the presence of another drug (Baxter and
Stockely, 2010). The detection of DDIs is an im-
portant research area in patient safety since these in-
teractions can become very dangerous and increase
health care costs. Drug interactions are frequently
reported in journals, making medical literature the
most effective source for their detection (Aronson,
2007). Therefore, Information Extraction (IE) can
be of great benefit in the pharmaceutical industry al-
lowing identification and extraction of relevant in-
formation on DDIs and providing an interesting way
of reducing the time spent by health care profession-
als on reviewing the literature.
The DDIExtraction 2013 follows up on a
first event organized in 2011, DDIExtraction
2011 (Segura-Bedmar et al, 2011b) whose main
goal was the detection of drug-drug interactions
from biomedical texts. The new edition includes in
addition to DDI extraction also a supporting task,
the recognition and classification of pharmacologi-
cal substances. DDIExtraction 2013 is designed to
address the extraction of DDIs as a whole, but di-
vided into two subtasks to allow separate evaluation
of the performance for different aspects of the prob-
lem. The shared task includes two challenges:
? Task 9.1: Recognition and classification of
pharmacological substances.
? Task 9.2: Extraction of drug-drug interactions.
Additionally, while the datasets used for
the DDIExtraction 2011 task were composed
by texts describing DDIs from the DrugBank
database(Wishart et al, 2006), the new datasets for
DDIExtraction 2013 also include MedLine abstracts
in order to deal with different types of texts and
language styles.
This shared task has been conceived with a dual
objective: advancing the state-of-the-art of text-
mining techniques applied to the pharmacological
domain, and providing a common framework for
evaluation of the participating systems and other re-
searchers interested in the task.
In the next section we describe the DDI corpus
used in this task. Sections 3 and 4 focus on the de-
scription of the task 9.1 and 9.2 respectively. Finally,
Section 5 draws the conclusions and future work.
2 The DDI Corpus
The DDIExtraction 2013 task relies on the DDI cor-
pus, which is a semantically annotated corpus of
341
documents describing drug-drug interactions from
the DrugBank database and MedLine abstracts on
the subject of drug-drug interactions.
The DDI corpus consists of 1,017 texts (784
DrugBank texts and 233 MedLine abstracts) and
was manually annotated with a total of 18,491 phar-
macological substances and 5,021 drug-drug inter-
actions (see Table 1). A detailed description of the
method used to collect and process documents can
be found in (Segura-Bedmar et al, 2011a). The cor-
pus is distributed in XML documents following the
unified format for PPI corpora proposed by Pyysalo
et al, (2008) (see Figure 1). A detailed description
and analysis of the DDI corpus and its methodology
are included in an article currently under review by
BioInformatics journal.1
The corpus was split in order to build the datasets
for the training and evaluation of the different par-
ticipating systems. Approximately 77% of the DDI
corpus documents were randomly selected for the
training dataset and the remaining (142 DrugBank
texts and 91MedLine abstracts) was used for the test
dataset. The training dataset is the same for both
subtasks since it contains entity and DDI annota-
tions. The test dataset for the task 9.1 was formed by
discarding documents which contained DDI annota-
tions. Entity annotations were removed from this
dataset to be used by participants. The remaining
documents (that is, those containing some interac-
tion) were used to create the test dataset for task 9.2.
Since entity annotations are not removed from these
documents, the test dataset for the task 9.2 can also
be used as additional training data for the task 9.1.
3 Task 9.1: Recognition and classification
of pharmacological substances.
This task concerns the named entity extraction of
pharmacological substances in text. This named en-
tity task is a crucial first step for information ex-
traction of drug-drug interactions. In this task, four
types of pharmacological substances are defined:
drug (generic drug names), brand (branded drug
names), group (drug group names) and drug-n (ac-
tive substances not approved for human use). For a
1M. Herrero-Zazo, I. Segura-Bedmar, P. Mart??nez. 2013.
The DDI Corpus: an annotated corpus with pharmacological
substances and drug-drug interactions, submitted to BioInfor-
matics
Training Test for task 9.1 Test for task 9.2
D
D
I-
D
ru
gB
an
k
documents 572 54 158
sentences 5675 145 973
drug 8197 180 1518
group 3206 65 626
brand 1423 53 347
drug n 103 5 21
mechanism 1260 0 279
effect 1548 0 301
advice 819 0 215
int 178 0 94
D
D
I-
M
ed
L
in
e
documents 142 58 33
sentences 1301 520 326
drug 1228 171 346
group 193 90 41
brand 14 6 22
drug n 401 115 119
mechanism 62 0 24
effect 152 0 62
advice 8 0 7
int 10 0 2
Table 1: Basic statistics on the DDI corpus.
more detailed description, the reader is directed to
our annotation guidelines.2
For evaluation, a part of the DDI corpus consist-
ing of 52 documents from DrugBank and 58 Med-
Line abstracts, is provided with the gold annota-
tion hidden. The goal for participating systems is to
recreate the gold annotation. Each participant sys-
tem must output an ASCII list of reported entities,
one per line, and formatted as:
IdSentence|startOffset-endOffset|text|type
Thus, for each recognized entity, each line must
contain the id of the sentence where this entity ap-
pears, the position of the first character and the one
of the last character of the entity in the sentence, the
text of the entity, and its type. When the entity is a
discontinuous name (eg. aluminum and magnesium
hydroxide), this second field must contain the start
and end positions of all parts of the entity separated
by semicolon. Multiple mentions from the same sen-
tence should appear on separate lines.
3.1 Evaluation Metrics
This section describes the methodology that is used
to evaluate the performance of the participating sys-
tems in task 9.1.
The major forums of the Named Entity Recogni-
tion and Classification (NERC) research community
(such as MUC-7 (Chinchor and Robinson, 1997),
CoNLL 2003 (Tjong Kim Sang and De Meulder,
2003) or ACE07 have proposed several techniques
to assess the performance of NERC systems. While
2http://www.cs.york.ac.uk/semeval-2013/task9/
342
Figure 1: Example of an annotated document of the DDI corpus.
Team Affiliation Description
Ta
sk
9.
1
LASIGE(Grego et al, 2013) University of Lisbon, Portugal Conditional random fields
NLM LHC National Library of Medicine, USA Dictionary-based approach
UEM UC3M(Sanchez-Cisneros and Aparicio, 2013) European U. of Madrid, Carlos III University of Madrid, Spain Ontology-based approach
UMCC DLSI(Collazo et al, 2013) Matanzas University, Cuba j48 classifier
UTurku(Bjo?rne et al, 2013) University of Turku, Finland SVM classifier (TEES system)
WBI NER(Rockta?schel et al, 2013) Humboldt University of Berlin, Germany Conditional random fields
Ta
sk
9.
2
FBK-irst (Chowdhury and Lavelli, 2013c) FBK-irst, Italy hybrid kernel + scope of negations and semantic roles
NIL UCM(Bokharaeian, 2013) Complutense University of Madrid, Spain SVM classifier (Weka SMO)
SCAI(Bobic? et al, 2013) Fraunhofer SCAI, Germany SVM classifier (LibLINEAR)
UC3M(Sanchez-Cisneros, 2013) Carlos III University of Madrid, Spain Shallow Linguistic Kernel
UCOLORADO SOM(Hailu et al, 2013) University of Colorado School of Medicine, USA SVM classifier (LIBSVM)
UTurku(Bjo?rne et al, 2013) University of Turku, Finland SVM classifier (TEES system)
UWM-TRIADS(Rastegar-Mojarad et al, 2013) University of Wisconsin-Milwaukee, USA Two-stage SVM
WBI DDI(Thomas et al, 2013) Humboldt University of Berlin, Germany Ensemble of SVMs
Table 2: Short description of the teams.
ACE evaluation is very complex because its scores
are not intuitive, MUC and CoNLL 2003 used the
standard precision/recall/f-score metrics to compare
their participating systems. The main shared tasks in
the biomedical domain have continued using these
metrics to evaluate the outputs of their participant
teams.
System performance should be scored automat-
ically by how well the generated pharmacological
substance list corresponds to the gold-standard an-
notations. In our task, we evaluate the results of
the participating systems according to several evalu-
ation criteria. Firstly, we propose a strict evaluation,
which does not only demand exact boundary match,
but also requires that both mentions have the same
entity type. We are aware that this strict criterion
may be too restrictive for our overall goal (extrac-
tion of drug interactions) because it misses partial
matches, which can provide useful information for
a DDI extraction system. Our evaluation metrics
should score if a system is able to identify the ex-
act span of an entity (regardless of the type) and if
it is able to assign the correct entity type (regardless
of the boundaries). Thus, our evaluation script will
output four sets of scores according to:
1. Strict evaluation (exact-boundary and type
matching).
2. Exact boundary matching (regardless to the
type).
3. Partial boundary matching (regardless to the
type).
4. Type matching (some overlap between the
tagged entity and the gold entitity is required).
Evaluation results are reported using the standard
precision/recall/f-score metrics. We refer the reader
to (Chinchor and Sundheim, 1993) for a more de-
tailed description of these metrics.
These metrics are calculated over all entities and
on both axes (type and span) in order to evaluate
the performance of each axe separately. The final
score is the micro-averaged F-measure, which is cal-
culated over all entity types without distinction. The
main advantage of the micro-average F1 is that it
343
takes into account all possible types of errors made
by a NERC system.
Additionally, we calculate precision, recall and f-
measure for each entity type and then their macro-
average measures are provided. Calculating these
metrics for each entity type allows us to evalu-
ate the level of difficulty of recognizing each en-
tity type. In addition to this, since not all entity
types have the same frequency, we can better as-
sess the performance of the algorithms proposed by
the participating systems. This is mainly because
the results achieved on the most frequent entity type
have a much greater impact on overall performance
than those obtained on the entity types with few in-
stances.
3.2 Results and Discussion
Participants could send a maximum of three system
runs. After downloading the test datasets, they had
a maximum of two weeks to upload the results. A
total of 6 teams participated, submitting 16 system
runs. Table 2 lists the teams, their affiliations and
a brief description of their approaches. Due to the
lack of space we cannot describe them in this paper.
Tables 3, 4 and 5 show the F1 scores for each run in
alphabetic order. The reader can find the full ranking
information on the SemEval-2013 Task 9 website3.
The best results were achieved by the WBI
team with a conditional random field. They em-
ployed a domain-independent feature set alng
with features generated from the output of
ChemSpot (Rockta?schel et al, 2012), an existing
chemical named entity recognition tool, as well as
a collection of domain-specific resources. Its model
was trained on the training dataset as well as on en-
tities of the test dataset for task 9.2. The second
top best performing team developed a dictionary-
based approach combining biomedical resources
such as DrugBank, the ATC classification system,4
or MeSH,5 among others. Regarding the classifi-
cation of each entity type, we observed that brand
drugs were easier to recognize than the other types.
This could be due to the fact that when a drug is mar-
keted by a pharmaceutical company, its brand name
is carefully selected to be short, unique and easy to
3http://www.cs.york.ac.uk/semeval-2013/task9/
4http://www.whocc.no/atc ddd index/
5http://www.ncbi.nlm.nih.gov/mesh
remember (Boring, 1997). On the other hand, sub-
stances not approved for human use (drug-n) were
more difficult, due to the greater variation and com-
plexity in their naming. In fact, the UEM UC3M
team was the only team who obtained an F1 measure
greater than 0 on the DDI-DrugBank dataset. Also,
this may indicate that this type is less clearly defined
than the others in the annotation guidelines. Another
possible reason is that the presence of such sub-
stances in this dataset is very scarce (less than 1%).
It is interesting that almost every participating sys-
temwas better in detecting and classifying entities of
a particular class compared to all other systems. For
instance, on the whole dataset the dictionary-based
system from NLM LHC had it strengths at drug en-
tities, UEM UC3M at drug N entities, UTurku at
brand entities and WBI NER at group entities.
Finally, the results on the DDI-DrugBank dataset
are much better than those obtained on the DDI-
MedLine dataset. While DDI-DrugBank texts focus
on the description of drugs and their interactions, the
main topic of DDI-MedLine texts would not neces-
sarily be on DDIs. Coupled with this, it is not al-
ways trivial to distinguish between substances that
should be classified as pharmacological substances
and those who should not. This is due to the ambi-
guity of some pharmacological terms. For example,
insulin is a hormone produced by the pancreas, but
can also be synthesized in the laboratory and used
as drug to treat insulin-dependent diabetes mellitus.
The participating systems should be able to deter-
mine if the text is describing a substance originated
within the organism or, on the contrary, it describes a
process in which the substance is used for a specific
purpose and thus should be identified as pharmaco-
logical substance.
4 Task 9.2: Extraction of drug-drug
interactions.
The goal of this subtask is the extraction of drug-
drug interactions from biomedical texts. However,
while the previous DDIExtraction 2011 task focused
on the identification of all possible pairs of inter-
acting drugs, DDIExtraction 2013 also pursues the
classification of each drug-drug interaction accord-
ing to one of the following four types: advice, ef-
fect, mechanism, int. A detailed description of these
344
Team Run Rank STRICT EXACT PARTIAL TYPE DRUG BRAND GROUP DRUG N MAVG
LASIGE
1 6 0,656 0,781 0,808 0,69 0,741 0,581 0,712 0,171 0,577
2 9 0,639 0,775 0,801 0,672 0,716 0,541 0,696 0,182 0,571
3 10 0,612 0,715 0,741 0,647 0,728 0,354 0,647 0,16 0,498
NLM LHC
1 4 0,698 0,784 0,801 0,722 0,803 0,809 0,646 0 0,57
2 3 0,704 0,792 0,807 0,726 0,81 0,846 0,643 0 0,581
UMCC DLSI 1,2,3 14,15,16 0,275 0,3049 0,367 0,334 0,297 0,313 0,257 0,124 0,311
UEM UC3M
1 13 0,458 0,528 0,585 0,51 0,718 0,075 0,291 0,185 0,351
2 12 0,529 0,609 0,669 0,589 0,752 0,094 0,291 0,264 0,38
UTurku
1 11 0,579 0,639 0,719 0,701 0,721 0,603 0,478 0,016 0,468
2 8 0,641 0,659 0,731 0,766 0,784 0,901 0,495 0,015 0,557
3 7 0,648 0,666 0,743 0,777 0,783 0,912 0,485 0,076 0,604
WBI
1 5 0,692 0,772 0,807 0,729 0,768 0,787 0,761 0,071 0,615
2 2 0,708 0,831 0,855 0,741 0,786 0,803 0,757 0,134 0,643
3 1 0,715 0,833 0,856 0,748 0,79 0,836 0,776 0,141 0,652
Table 3: F1 scores for task 9.1 on the whole test dataset (DDI-MedLine + DDI-DrugBank). (MAVG for macro-
average). Each run is ranked by STRICT performance.
Team Run Rank STRICT EXACT PARTIAL TYPE DRUG BRAND GROUP DRUG N MAVG
LASIGE
1 8 0,771 0,834 0,855 0,799 0,817 0,571 0,833 0 0,563
2 9 0,771 0,831 0,852 0,799 0,823 0,553 0,824 0 0,568
3 11 0,682 0,744 0,764 0,713 0,757 0,314 0,756 0 0,47
NLM LHC
1 2 0,869 0,902 0,922 0,902 0,909 0,907 0,766 0 0,646
2 3 0,869 0,903 0,919 0,896 0,911 0,907 0,754 0 0,644
UMCC DLSI 1,2,3 14,15,16 0,424 0,4447 0,504 0,487 0,456 0,429 0,371 0 0,351
UEM UC3M
1 13 0,561 0,632 0,69 0,632 0,827 0,056 0,362 0,022 0,354
2 12 0,595 0,667 0,721 0,667 0,842 0,063 0,366 0,028 0,37
UTurku
1 10 0,739 0,753 0,827 0,864 0,829 0,735 0,553 0 0,531
2 6 0,785 0,795 0,863 0,908 0,858 0,898 0,559 0 0,581
3 7 0,781 0,787 0,858 0,905 0,847 0,911 0,551 0 0,578
WBI
1 5 0,86 0,877 0,9 0,89 0,905 0,857 0,782 0 0,636
2 4 0,868 0,894 0,914 0,897 0,909 0,865 0,794 0 0,642
3 1 0,878 0,901 0,917 0,908 0,912 0,904 0,806 0 0,656
Table 4: F1 scores for task 9.1 on the DDI-DrugBank test data. (MAVG for macro-average). Each run is ranked by
STRICT performance.
Team Run Rank STRICT EXACT PARTIAL TYPE DRUG BRAND GROUP DRUG N MAVG
LASIGE
1 4 0,567 0,74 0,772 0,605 0,678 0,667 0,612 0,183 0,577
2 8 0,54 0,733 0,763 0,576 0,631 0,444 0,595 0,196 0,512
3 6 0,557 0,693 0,723 0,596 0,702 0,667 0,56 0,171 0,554
NLM LHC
1 5 0,559 0,688 0,702 0,575 0,717 0,429 0,548 0 0,462
2 3 0,569 0,702 0,715 0,586 0,726 0,545 0,555 0 0,486
UMCC DLSI 1,2,3 14,15,16 0,187 0,2228 0,287 0,245 0,2 0,091 0,191 0,13 0,23
UEM UC3M
1 13 0,39 0,461 0,516 0,431 0,618 0,111 0,238 0,222 0,341
2 11 0,479 0,564 0,628 0,529 0,665 0,182 0,233 0,329 0,387
UTurku
1 12 0,435 0,538 0,623 0,556 0,614 0,143 0,413 0,016 0,328
2 10 0,502 0,528 0,604 0,628 0,703 0,923 0,436 0,016 0,533
3 9 0,522 0,551 0,634 0,656 0,716 0,923 0,426 0,08 0,582
WBI
1 7 0,545 0,681 0,726 0,589 0,634 0,353 0,744 0,074 0,479
2 2 0,576 0,779 0,807 0,612 0,673 0,444 0,729 0,14 0,534
3 1 0,581 0,778 0,805 0,617 0,678 0,444 0,753 0,147 0,537
Table 5: F1 scores for task 9.1 on the DDI-MedLine test data. (MAVG for macro-average). Each run is ranked by
STRICT performance.
345
types can be found in our annotation guidelines6.
Gold standard annotations (correct, human-
created annotations) of pharmacological substances
are provided to participants both for training and test
data. The test data for this subtask consists of 158
DrugBank documents and 33 MedLine abstracts.
Each participant system must output an ASCII list
including all pairs of drugs in each sentence, one per
line (multiple DDIs from the same sentence should
appear on separate lines), its prediction (1 if the pair
is a DDI and 0 otherwise) and its type (label null
when the prediction value is 0), and formatted as:
IdSentence|IdDrug1|IdDrug2|prediction|type
4.1 Evaluation Metrics
Evaluation is relation-oriented and based on the
standard precision, recall and F-score metrics. A
DDI is correctly detected only if the system is able
to assign the correct prediction label and the correct
type to it. In other words, a pair is correct only if
both prediction and type are correct. The perfor-
mance of systems to identify those pairs of drugs
interacting (regardless of the type) is also evaluated.
This allows us to assess the progress made with re-
gard to the previous edition, which only dealt with
the detection of DDIs.
Additionally, we are interested in assessing which
drug interaction types are most difficult to detect.
Thus, we calculate precision, recall and F1 for each
DDI type and then their macro-average measures are
provided. While micro-averaged F1 is calculated
by constructing a global contingency table and then
calculating precision and recall, macro-averaged F-
score is calculated by first calculating precision and
recall for each type and then taking the average of
these results.
Evaluating each DDI type separately allows us to
assess the level of difficulty of detecting and classi-
fying each type of interaction. Additionally, it is im-
portant to note that the scores achieved on the most
frequent DDI type have a much greater impact on
overall performance than those achieved on the DDI
types with few instances. Therefore, by calculating
scores for each type of DDI, we can better assess
the performance of the algorithms proposed by the
6http://www.cs.york.ac.uk/semeval-2013/task9/
participating systems.
4.2 Results and Discussion
The task of extracting drug-drug interactions from
biomedical texts has attracted the participation of 8
teams (see Table 2) who submitted 22 runs. Tables 6,
7 and 8 show the results for each run in alphabetic
order. Due to the lack of space, the performance
information is only shown in terms of F1 score. The
reader can find the full ranking information on the
SemEval-2013 Task 9 website7.
Most of the participating systems were built on
support vector machines. In general, approaches
based on non-linear kernels methods achieved better
results than linear SVMs. As in the previous edition
of DDIExtraction, most systems have used primarily
syntactic information. However, semantic informa-
tion has been poorly used.
The best results were submitted by the team from
FBK-irst. They applied a novel hybrid kernel based
RE approach described in Chowdhury (2013a).
They also exploited the scope of negations and
semantic roles for negative instance filtering as
proposed in (Chowdhury and Lavelli, 2013b) and
(Chowdhury and Lavelli, 2012). The second best
results were obtained by the WBI team from the
Humboldt University of Berlin. Its system com-
bines several kernel methods (APG (Airola et al,
2008) and Shallow Linguistic Kernel (SL) (Giuliano
et al, 2006) among others), the Turku Event Ex-
traction system (TEES) (Bjo?rne et al, 2011)8 and
the Moara system (Neves et al, 2009). These two
teams were also the top two ranked teams in DDIEx-
traction 2011. For a more detailed description, the
reader is encouraged to read the papers of the partic-
ipants in the proceedings book.
While the DDIExtraction 2011 shared task con-
centrated efforts on the detection of DDIs, this new
DDIExtraction 2013 task involved not only the de-
tection of DDIs, but also their classification. Al-
though the results of DDIExtraction 2011 are not di-
rectly comparable with the ones reported in DDIEx-
traction 2013 due to the use of different training and
test datasets in each edition, it should be noted that
there has been a significant improvement in the de-
7http://www.cs.york.ac.uk/semeval-2013/task9/
8http://jbjorne.github.io/TEES/
346
Team Run Rank CLA DEC MEC EFF ADV INT MAVG
FBK-irst
1 3 0.638 0.8 0.679 0.662 0.692 0.363 0.602
2 1 0.651 0.8 0.679 0.628 0.692 0.547 0.648
3 2 0.648 0.8 0.627 0.662 0.692 0.547 0.644
NIL UCM
1 12 0.517 0.588 0.515 0.489 0.613 0.427 0.535
2 10 0.548 0.656 0.531 0.556 0.61 0.393 0.526
SCAI
1 14 0.46 0.69 0.446 0.459 0.562 0.02 0.423
2 16 0.452 0.683 0.441 0.44 0.559 0.021 0.448
3 15 0.458 0.704 0.45 0.462 0.54 0.02 0.411
UC3M
1 11 0.529 0.676 0.48 0.547 0.575 0.5 0.534
2 21 0.294 0.537 0.268 0.286 0.325 0.402 0.335
UCOLORADO SOM
1 22 0.214 0.492 0.109 0.25 0.219 0.097 0.215
2 20 0.334 0.504 0.361 0.311 0.381 0.333 0.407
3 19 0.336 0.491 0.335 0.313 0.42 0.329 0.38
UTurku
1 9 0.581 0.684 0.578 0.585 0.606 0.503 0.572
2 7 0.594 0.696 0.582 0.6 0.63 0.507 0.587
3 8 0.582 0.699 0.569 0.593 0.608 0.511 0.577
UWM-TRIADS
1 17 0.449 0.581 0.413 0.446 0.502 0.397 0.451
2 13 0.47 0.599 0.446 0.449 0.532 0.421 0.472
3 18 0.432 0.564 0.442 0.383 0.537 0.292 0.444
WBI
1 6 0.599 0.736 0.602 0.604 0.618 0.516 0.588
2 5 0.601 0.745 0.616 0.595 0.637 0.49 0.588
3 4 0.609 0.759 0.618 0.61 0.632 0.51 0.597
Table 6: F1 scores for Task 9.2 on the whole test dataset (DDI-MedLine + DDI-DrugBank). DEC for Detection, CLA
for detection and classification, MEC for mechanism type, EFF for effect type, ADV for advice type, INT for int type
and MAVG for macro-average. Each run is ranked by CLA performance.
Team Run Rank CLA DEC MEC EFF ADV INT MAVG
FBK-irst
1 3 0.663 0.827 0.705 0.699 0.705 0.376 0.624
2 1 0.676 0.827 0.705 0.664 0.705 0.545 0.672
3 2 0.673 0.827 0.655 0.699 0.705 0.545 0.667
NIL UCM
1 12 0.54 0.615 0.527 0.525 0.625 0.444 0.565
2 10 0.573 0.68 0.552 0.597 0.619 0.408 0.55
SCAI
1 15 0.464 0.711 0.449 0.459 0.57 0.021 0.461
2 16 0.463 0.71 0.445 0.458 0.569 0.021 0.46
3 14 0.473 0.734 0.468 0.482 0.551 0.021 0.439
UC3M
1 11 0.555 0.703 0.493 0.593 0.59 0.51 0.561
2 21 0.306 0.549 0.274 0.302 0.334 0.426 0.352
UCOLORADO SOM
1 22 0.218 0.508 0.115 0.251 0.24 0.098 0.228
2 20 0.341 0.518 0.373 0.313 0.398 0.344 0.425
3 19 0.349 0.511 0.353 0.324 0.429 0.327 0.394
UTurku
1 8 0.608 0.712 0.6 0.63 0.617 0.522 0.6
2 7 0.62 0.724 0.605 0.644 0.638 0.522 0.614
3 9 0.608 0.726 0.591 0.635 0.617 0.522 0.601
UWM-TRIADS
1 17 0.462 0.596 0.43 0.459 0.509 0.405 0.463
2 13 0.485 0.616 0.467 0.466 0.536 0.425 0.486
3 18 0.445 0.573 0.469 0.39 0.544 0.29 0.46
WBI
1 6 0.624 0.762 0.621 0.645 0.634 0.52 0.61
2 5 0.627 0.775 0.636 0.636 0.652 0.5 0.611
3 4 0.632 0.783 0.629 0.652 0.65 0.513 0.617
Table 7: F1 scores for task 9.2 on the DDI-DrugBank test dataset. Each run is ranked by CLA performance.
Team Run Rank CLA DEC MEC EFF ADV INT MAVG
FBK-irst
1 4 0.387 0.53 0.383 0.436 0.286 0.211 0.406
2 3 0.398 0.53 0.383 0.407 0.286 0.571 0.436
3 2 0.398 0.53 0.339 0.436 0.286 0.571 0.44
NIL UCM
1 20 0.19 0.206 0.286 0.186 0 0 0.121
2 19 0.219 0.336 0.143 0.271 0 0 0.11
SCAI
1 1 0.42 0.462 0.412 0.458 0.2 0 0.269
2 8 0.323 0.369 0.389 0.333 0 0 0.182
3 6 0.341 0.474 0.31 0.379 0.222 0 0.229
UC3M
1 15 0.274 0.406 0.333 0.267 0 0.364 0.268
2 22 0.186 0.421 0.222 0.171 0.143 0 0.149
UCOLORADO SOM
1 21 0.188 0.37 0.042 0.241 0 0 0.073
2 14 0.275 0.394 0.258 0.302 0.138 0 0.177
3 17 0.244 0.356 0.194 0.255 0.222 0.4 0.272
UTurku
1 18 0.242 0.339 0.258 0.256 0.2 0 0.18
2 16 0.262 0.344 0.214 0.278 0.364 0 0.224
3 13 0.286 0.376 0.286 0.289 0.333 0 0.232
UWM-TRIADS
1 10 0.312 0.419 0.233 0.36 0.267 0 0.219
2 9 0.319 0.436 0.233 0.34 0.421 0.333 0.345
3 11 0.306 0.479 0.247 0.326 0.381 0.333 0.33
WBI
1 7 0.336 0.456 0.368 0.344 0.154 0.4 0.334
2 12 0.304 0.406 0.343 0.318 0.167 0 0.209
3 5 0.365 0.503 0.476 0.347 0.143 0.4 0.353
Table 8: F1 scores for task 9.2 on the DDI-MedLine test dataset. Each run is ranked by CLA performance.
347
tection of DDIs: F1 has a remarkable increase from
65.74% (the best F1-score in DDIExtraction 2011)
to 80% (see DEC column of Table 6). The increase
of the size of the corpus made for DDIExtraction
2013 and of the quality of their annotations may
have contributed significantly to this improvement.
However, the results for the detection and classifi-
cation for DDIs did not exceed an F1 of 65.1%. Ta-
ble 6 suggests that some type of DDIs are more diffi-
cult to classify than others. The best F1 ranges from
69.2% for advice to 54.7% for int. One possible ex-
planation for this could be that recommendations or
advice regarding a drug interaction are typically de-
scribed by very similar text patterns such as DRUG
should not be used in combination with DRUG or
Caution should be observed when DRUG is admin-
istered with DRUG.
Regarding results for the int relationship, it should
be noted that the proportion of instances of this re-
lationship (5.6%) in the DDI corpus is much smaller
than those of the rest of the relations (41.1% for ef-
fect, 32.3% for mechanism and 20.9% for advice).
As stated earlier, one of the differences from
the previous edition is that the corpus developed
for DDIExtraction 2013 is made up of texts from
two different sources: MedLine and the DrugBank
database. Thus, the different approaches can be
evaluated on two different styles of biomedical texts.
While MedLine abstracts are usually written in ex-
tremely scientific language, texts from DrugBank
are written in a less technical form of the language
(similar to the language used in package inserts). In-
deed, this may be the reason why the results on the
DDI-DrugBank dataset are much better than those
obtained on the DDI-MedLine dataset (see Tables 7
and 8).
5 Conclusions
The DDIExtraction 2011 task concentrated efforts
on the novel aspects of the DDI extraction task, the
drug recognition was assumed and the annotations
for drugs were provided to the participants. This
new DDIExtraction 2013 task pursues the detec-
tion and classification of drug interactions as well
as the recognition and classification of pharmaco-
logical substances. The task attracted broad interest
from the community. A total of 14 teams from 7 dif-
ferent countries participated, submitted a total of 38
runs, exceeding the participation of DDIExtraction
2011 (10 teams). The participating systems demon-
strated substantial progress at the established DDI
extraction task on DrugBank texts and showed that
their methods also obtain good results for MedLine
abstracts.
The results that the participating systems have re-
ported show successful approaches to this difficult
task, and the advantages of non-linear kernel-based
methods over linear SVMs for extraction of DDIs.
In the named entity task, the participating systems
perform well in recognizing generic drugs, brand
drugs and groups of drugs, but they fail in recogniz-
ing active substances not approved for human use.
Although the results are positive, there is still much
room to improve in both subtasks. We have ac-
complished our goal of providing a framework and
a benchmark data set to allow for comparisons of
methods for the recognition of pharmacological sub-
stances and detection and classification of drug-drug
interactions from biomedical texts.
We would like that our test dataset can still serve
as the basis for fair and stable evaluation after the
task. Thus, we have decided that the full gold an-
notations for the test data are not available for the
moment. We plan to make available a web service
where researchers can test their methods on the test
dataset and compare their results with the DDIEx-
traction 2013 task participants.
Acknowledgments
This research work has been supported by the Re-
gional Government of Madrid under the Research
Network MA2VICMR (S2009/TIC-1542), by the
Spanish Ministry of Education under the project
MULTIMEDICA (TIN2010-20644-C03-01). Addi-
tionally, we would like to thank all participants for
their efforts and to congratulate them to their inter-
esting work.
References
A. Airola, S. Pyysalo, J. Bjorne, T. Pahikkala, F. Gin-
ter, and T. Salakoski. 2008. All-paths graph kernel
for protein-protein interaction extraction with evalu-
ation of cross-corpus learning. BMC bioinformatics,
9(Suppl 11):S2.
348
JK. Aronson. 2007. Communicating information about
drug interactions. British Journal of Clinical Pharma-
cology, 63(6):637?639, June.
K. Baxter and I.H. Stockely. 2010. Stockley?s drug inter-
actions.8th ed. London:Pharmaceutical Press.
J. Bjo?rne, J. Heimonen, F. Ginter, A. Airola, T. Pahikkala,
and T. Salakoski. 2011. Extracting contextualized
complex biological events with graph-based feature
sets. Computational Intelligence, 27(4):541?557.
J. Bjo?rne, S. Kaewphan, and T. Salakoski. 2013.
UTurku: Drug Named Entity Detection and Drug-drug
Interaction Extraction Using SVM Classification and
Domain Knowledge. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013).
T. Bobic?, J. Fluck, and M. Hofmann-Apitius. 2013.
SCAI: Extracting drug-drug interactions using a rich
feature vector. In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval 2013).
A. Bokharaeian, B.and D??az. 2013. NIL UCM: Extract-
ing Drug-Drug interactions from text through combi-
nation of sequence and tree kernels. In Proceedings of
the 7th International Workshop on Semantic Evalua-
tion (SemEval 2013).
D. Boring. 1997. The development and adop-
tion of nonproprietary, established, and proprietary
names for pharmaceuticals. Drug information journal,
31(3):621?634.
N. Chinchor and P. Robinson. 1997. Muc-7 named entity
task definition. In Proceedings of the 7th Conference
on Message Understanding.
N. Chinchor and B. Sundheim. 1993. Muc-5 evalua-
tion metrics. In Proceedings of the 5th conference on
Message understanding, pages 69?78. Association for
Computational Linguistics.
MFM. Chowdhury and A. Lavelli. 2012. Impact of
less skewed distributions on efficiency and effective-
ness of biomedical relation extraction. In Proceedings
of COLING 2012.
MFM. Chowdhury and A. Lavelli. 2013b. Exploiting
the scope of negations and heterogeneous features for
relation extraction: Case study drug-drug interaction
extraction. In Proceedings of NAACL 2013.
M.F.M. Chowdhury and A. Lavelli. 2013c. FBK-irst
: A Multi-Phase Kernel Based Approach for Drug-
Drug Interaction Detection and Classification that Ex-
ploits Linguistic Information. In Proceedings of the
7th International Workshop on Semantic Evaluation
(SemEval 2013).
MFM. Chowdhury. 2013a. Improving the Effectiveness
of Information Extraction from Biomedical Text. Ph.d.
dissertation, University of Trento.
A. Collazo, A. Ceballo, D Puig, Y. Gutie?rrez, J. Abreu,
J Pe?rez, A. Ferna?ndez-Orqu??n, A. Montoyo, R. Mun?oz,
and F. Camara. 2013. UMCC DLSI-(DDI): Seman-
tic and Lexical features for detection and classification
Drugs in biomedical texts. In Proceedings of the 7th
International Workshop on Semantic Evaluation (Se-
mEval 2013).
C. Giuliano, A. Lavelli, and L. Romano. 2006. Ex-
ploiting shallow linguistic information for relation ex-
traction from biomedical literature. In Proceedings of
the Eleventh Conference of the European Chapter of
the Association for Computational Linguistics (EACL-
2006), pages 401?408.
T. Grego, F. Pinto, and F.M. Couto. 2013. LASIGE: us-
ing Conditional Random Fields and ChEBI ontology.
In Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013).
N.D. Hailu, L.E. Hunter, and K.B. Cohen. 2013.
UColorado SOM: Extraction of Drug-Drug Interac-
tions from Biomedical Text using Knowledge-rich and
Knowledge-poor Features. In Proceedings of the 7th
International Workshop on Semantic Evaluation (Se-
mEval 2013).
ML. Neves, JM. Carazo, and A. Pascual-Montano. 2009.
Extraction of biomedical events using case-based rea-
soning. In Proceedings of the Workshop on BioNLP:
Shared Task, pages 68?76. Association for Computa-
tional Linguistics.
S. Pyysalo, A. Airola, J. Heimonen, J. Bjorne, F. Gin-
ter, and T. Salakoski. 2008. Comparative analysis of
five protein-protein interaction corpora. BMC bioin-
formatics, 9(Suppl 3):S6.
M. Rastegar-Mojarad, R. D. Boyce, and R. Prasad. 2013.
UWM-TRIADS: Classifying Drug-Drug Interactions
with Two-Stage SVM and Post-Processing. In Pro-
ceedings of the 7th International Workshop on Seman-
tic Evaluation (SemEval 2013).
T. Rockta?schel, M. Weidlich, and U. Leser. 2012.
Chemspot: a hybrid system for chemical named entity
recognition. Bioinformatics, 28(12):1633?1640.
T. Rockta?schel, T. Huber, M. Weidlich, and U. Leser.
2013. WBI-NER: The impact of domain-specific fea-
tures on the performance of identifying and classifying
mentions of drugs. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013).
D. Sanchez-Cisneros and F. Aparicio. 2013. UEM-
UC3M: An Ontology-based named entity recognition
system for biomedical texts. In Proceedings of the 7th
International Workshop on Semantic Evaluation (Se-
mEval 2013).
D. Sanchez-Cisneros. 2013. UC3M: A kernel-based ap-
proach for identify and classify DDIs in biomedical
349
texts. In Proceedings of the 7th International Work-
shop on Semantic Evaluation (SemEval 2013).
I. Segura-Bedmar, P. Mart??nez, and C. de Pablo-Sa?nchez.
2011a. Using a shallow linguistic kernel for drug-drug
interaction extraction. Journal of Biomedical Infor-
matics, 44(5):789 ? 804.
I. Segura-Bedmar, P. Mart?nez, and D. Sa?nchez-Cisneros.
2011b. The 1st ddiextraction-2011 challenge task:
Extraction of drug-drug interactions from biomedical
texts. In Proceedings of DDIExtraction-2011 chal-
lenge task, pages 1?9.
P. Thomas, M. Neves, T. Rockta?schel, and U. Leser.
2013. WBI-DDI: Drug-Drug Interaction Extraction
usingMajority Voting. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013).
E.F. Tjong Kim Sang and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proceedings
of the seventh conference on Natural language learn-
ing at HLT-NAACL 2003-Volume 4, pages 142?147.
Association for Computational Linguistics.
D.S. Wishart, C. Knox, A.C. Guo, S. Shrivastava,
M. Hassanali, P. Stothard, Z. Chang, and J. Woolsey.
2006. Drugbank: a comprehensive resource for in sil-
ico drug discovery and exploration. Nucleic acids re-
search, 34(suppl 1):D668?D672.
350
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 106?115,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Detecting drugs and adverse events from Spanish health social media streams 
  Isabel Segura-Bedmar, Ricardo Revert, Paloma Mart?nez Computer Science Department,  Carlos III University of Madrid, Spain {isegura,rrevert,pmf}@inf.uc3m.es     Abstract 
To the best of our knowledge, this is the first work that does drug and adverse event detection from Spanish posts collected from a health social media. First, we created a gold-standard corpus annotated with drugs and adverse events from social media. Then, Textalytics, a multilingual text analysis engine, was applied to identify drugs and possible adverse events. Overall recall and precision were 0.80 and 0.87 for drugs, and 0.56 and 0.85 for adverse events. 1 Introduction 
It is well-known that adverse drug reactions (ADRs) are an important health problem. Indeed, ADRs are the 4th cause of death in hospitalized patients (Wester et al., 2008). Thus, the field of pharmacovigilance has received a great deal of attention due to the high and growing incidence of drug safety incidents (Bond and Raehl, 2006) as well as to their high associated costs (van Der Hooft et al., 2006). Since many ADRs are not captured during clinical trials, the major medicine regulatory agencies such as the US Food and Drug Administration (FDA) or the European Medicines Agency (EMA) require healthcare professionals to report all suspected adverse drug reactions. However, some studies have shown 
that ADRs are under-estimated due to the fact that they are reported by voluntary reporting systems (Bates et al., 2003; van Der Hooft et al., 2006; McClellan, 2007). In fact, it is estimated that only between 2 and 10 per cent of ADRs are reported (Rawlins, 1995). Healthcare professionals must perform many tasks during their workdays and thus finding the time to use these surveillance reporting systems is very difficult. Also, healthcare professionals tend to report only those ADRs on which they have absolute certainty of their existence. Several medicines agencies have implemented spontaneous patient reporting systems in order for patients to report ADRs themselves. Some of these systems are the MedWatch from the FDA, the Yellow Cards  from the UK Medicines agency (MHRA) or the website1 developed by the Spanish Agency of Medicines and Medical devices (AEMPS). Unlike reports from healthcare professionals, patient reports often provide more detailed and explicit information about ADRs (Herxheimer et al., 2010). Another important contribution of spontaneous patient reporting systems is to achieve patients having a more central role in their treatments. However, despite the fact that these systems are well-established, the rate of spontaneous patient reporting is very low probably because many                                                             
1 https://www.notificaram.es/  
106
patients are still unaware of their existence and even may feel embarrassed when describing their symptoms.  In this study, our hypothesis is that health-related social media can be used as a complementary data source to spontaneous reporting systems in order to detect unknown ADRs and thereby to increase drug safety. In recent days, social media on health information, just like has happened in other areas, have seen a tremendous growth (Hill et al., 2013). Examples of social media sites include blogs, online forums, social networking, and wikis, among many others. In this work, we focus on health forums where patients often exchange information about their personal medical experiences with other patients who suffer the same illness or receive similar treatment. Some patients may feel more comfortable sharing their medical experiences with each other rather than with their healthcare professionals. These forums contain a large number of comments describing patient experiences that would be a fertile source of data to detect unknown ADRs. Although there have been several research efforts devoted to developing systems for extracting ADRs from social media, all studies have focused on social media in English, and none of them have addressed the extraction from Spanish social media. Moreover, the problem is that these studies have not been compared with each other, and hence it is very difficult to determine the current ?state-of-art? of the techniques for ADRs extraction from social media. This comparison has not been performed due to the lack of a gold-standard corpus for ADRs. Thus, the goal of our work is twofold: i) to create a gold-standard corpus annotated with drugs and adverse events and ii) to develop a system to automatically extract mentions of drugs and adverse events from Spanish health-related social media sites. The corpus is composed by patients? comments from Forumclinic2, a health online networking website                                                             
2 http://www.forumclinic.org  
in Spanish. This is the first corpus of patient comments annotated with drugs and adverse events in Spanish. Also, we believe that this corpus will facilitate comparison for future ADRs detection from Spanish social media.  This is a preliminary work, in which we have only focused on the automatic detection of mentions of drugs and adverse events. Our final goal will be to develop a system to automatically extract drugs and their side effects. We hope our system will be beneficial to AEMPS as well as to the pharmaceutical industry in the improvement of their pharmacovigilance systems. 2 Related Work In recent years, the application of Natural Language Processing (NLP) techniques to mine adverse reactions from texts has been explored with promising results, mainly in the context of drug labels (Gurulingappa et al., 2013; Li et al., 2013; Kuhn et al., 2010), biomedical literature (Xu and Wang, 2013), medical case reports (Gurulingappa et al., 2012) and health records (Friedman, 2009; Sohn et al., 2011). However, as it will be described below, the extraction of adverse reactions from social media has received much less attention. In general, medical literature, such as scientific publications and drug labels, contains few grammatical and spelling mistakes. Another important advantage is that this type of texts can be easily linked to biomedical ontologies. Similarly, clinical records present specific medical terminology and can also be mapped to biomedical ontologies and resources. Meanwhile social media texts are markedly different from clinical records and scientific articles, and thereby the processing of social media texts poses additional challenges such as the management of meta-information included in the text (for example as tags in tweets) (Bouillot et al., 2013), the detection of typos and unconventional spelling, word shortenings (Neunedert et al, 2013; Moreira et al., 2013) and slang and emoticons (Balahur, 2013), among others. Moreover, these texts are often very short 
107
and with an informal nature, making the processing task extremely challenging. Regarding the identification of drug names in text, during the last four years there has been significant research efforts directed to encourage the development of systems for detecting these entities. Concretely, shared tasks such as DDIExtraction 2013 (Segura-Bedmar et al., 2013), CHEMDNER 2013 (Krallinger et al., 2013) or the i2b2 Medication Extraction challenge (Uzuner et al., 2010) have been held for the advancement of the state of the art in this problem. However, most of the work on recognizing drugs concerns either biomedical literature (for example, MedLine articles) or clinical records, thus leaving unexplored this task in social media streams.  Leaman et al., (2010) developed a system to automatically recognize adverse effects in user comments. A corpus of 3,600 comments from the DailyStrength health-related social network was collected and manually annotated with a total of 1,866 drug conditions, including beneficial effects, adverse effects, indications and others. To identify the adverse effects in the user comments, a lexicon was compiled from the following resources: (1) the COSTART vocabulary (National Library of Medicine, 2008), (2) the SIDER database (Kuhn et al., 2010), (3) MedEffect3 and (4) a list of colloquial phrases which were manually collected from the DailyStrength comments. The final lexicon consisted of 4,201 concepts (terms with the same CUI were grouped in the same concept). Finally, the terms in the lexicon were mapped against user comments to identify the adverse effects. In order to distinguish adverse effects from the other drug conditions (beneficial effects, indications and others), the systems used a list of verbs denoting indications (for example, help, work, prescribe). Drug name recognition was not necessary because the evaluation focused only on a set of four drugs: carbamazepine, olanzapine,                                                             
3 http://www.hc-sc.gc.ca/dhp-mps/medeff/index-eng.php 
trazodone and ziprasidone. The system achieved a good performance, with a precision of 78.3% and a recall of 69.9%.  An extension of this system was accomplished by Nikfarjam and Gonzalez (2011). The authors applied association rule mining to extract frequent patterns describing opinions about drugs. The rules were generated using the Apriori tool4, an implementation of the Apriori algorithm (Agrawal and Srikant, 1994) for association rule mining. The system was evaluated using the same corpus created for their previous work (Leaman et al., 2010), and which has been described above. The system achieved a precision of 70.01% and a recall of 66.32%. The main advantage of this system is that it can be easily adapted for other domains and languages. Another important advantage of this approach over a dictionary based approach is that the system is able to detect terms not included in the dictionary.  Benton et al., (2011) created a corpus of posts from several online forums about breast cancer, which later was used to extract potential adverse reactions from the most commonly used drugs to treat this disease: tamoxifen, anastrozole, letrozole and axemestane. The authors collected a lexicon of lay medical terms from websites and databases about drugs and adverse events. The lexicon was extended with the Consumer Health Vocabulary (CHV)5, a vocabulary closer to the lay terms, which patients usually use to describe their medical experiences. Then, pairs of terms co-occurring within a window of 20 tokens were considered. The Fisher?s exact test (Fisher, 1922) was used to calculate the probability that the two terms co-occurred independently by chance. To evaluate the system, the authors focused on the four drugs mentioned above, and then collected their adverse effects from their drug labels. Then, precision and recall were calculated by comparing the adverse effects from drug labels and the adverse effects obtained by the system.                                                             
4 http://www.borgelt.net/apriori.html 5 http://consumerhealthvocab.org 
108
The system obtained an average precision of 77% and an average recall of 35.1% for all four drugs.  UDWarning (Wu et al., 2012) is an ongoing prototype whose main goal is to extract adverse drug reactions from Google discussions. A knowledge base of drugs and their adverse effects was created by integrating information from different resources such as SIDER, DailyMed6, Drugs.com7 and MedLinePlus. The authors hypothesized that unknown adverse drug effects would have a high volume of discussions over the time. Thus, the systems should monitor the number of relevant discussions for each adverse drug effect. However, to the best of our knowledge, the UDWarning?s component devoted to the detection of unrecognized adverse drug effects has not been developed yet.  Bian et al., (2012) developed a system to detect tweets describing adverse drug reactions. The systems used a SVM classifier trained on a corpus of tweets, which were manually labeled by two experts. MetaMap (Aronson and Lang, 2010) was used to analyze the tweets and to find the UMLS concepts present in the tweets. The system produced poor results, mainly because tweets are riddled with spelling and grammar mistakes. Moreover, MetaMap is not a suitable tool to analyze this type of texts since patients do not usually use medical terminology to describe their medical experiences.  As it was already mentioned, the recognition of drugs in social media texts has hardly been tackled and little research has been conducted to extract relationships between drugs and their side effects, since most systems were focused on a given and fixed set of drugs. Most systems for extracting ADRs follow a dictionary-based approach. The main drawback of these systems is that they fail to recognize terms which are not included in the dictionary.  In addition, the dictionary-based approach is not able to handle the large number of spelling and grammar errors in social media texts. Moreover, the detection of                                                             
6 http://dailymed.nlm.nih.gov/dailymed/ 7 http://www.drugs.com/ 
ADRs has not been attempted for languages other than English. Indeed, automatic information extraction from Spanish-language social media in the field of health remains largely unexplored. Additionally, to the best of our knowledge, there is no corpus annotated with ADRs in social media texts available today. 3 Method  3.1 Corpus creation In order to create the first corpus in Spanish annotated with drugs and adverse events, we reviewed the main health-related social networks in Spanish language to select the most appropriate source of user comments. This corpus will be used to evaluate our system. Twitter was initially our preferred option due to the tremendous amount of tweets published each day (nearly 400 millions). However, we decided to discard it because Twitter does not seem to be the preferred source for users to describe their ADRs. Gonzalez et al. (2013) gathered a total of 42,327 in a one-month period, from which only 216 described ADRs. Although Facebook is the most popular social media and many Facebook groups dedicated to specific diseases have emerged in the last years, we discarded it because most of these groups usually have restricted access to their members.  Online health-related forums are an attractive source of data for our corpus due to their high dynamism, their great number of users as well as their easy access. After reviewing the main health forums in Spanish, we chose ForumClinic, an interactive program for patients, whose main goal is to provide rigorous information about specific diseases (such as breast cancer, HIV, bipolar disorder, depression, schizophrenia, ischemic heart disease, among others) and their treatments. Also, this platform aims to increase the participation of patients maintaining a discussion forum where patients can exchange information about their experiences. Figure 1 shows the distribution of user comments across the main twelve categories defined in the forum. We 
109
implemented a web crawler to gather all user comments published in ForumClinic to date. 
 Figure 1 Distribution of user comments. Then, we randomly selected a sample of 400 comments that were manually labeled with drugs and adverse events by two annotators with expertise in Pharmacovigilance. It should be noted that adverse events and ADRs do not refer to the same: while an adverse event may or may not be caused by a drug, an ADR is an adverse event that is suspected to be caused by a drug. A drug is a substance used in the treatment, cure, prevention or diagnosis of diseases. The corpus includes generic and brand drugs as well as drug families. Disagreements between the annotators were discussed and reconciled during the harmonization process, where a third annotator helped to make the final decision (some examples are shown in Table 1). All the mentions of drugs and adverse events were annotated, even those containing spelling or grammatical errors (for example, hemorrajia). Nominal anaphoric expressions, which refer to previous adverse events or drugs in the comment, were also included in the annotation. The annotators found 187 drugs (from which 40 were nominal anaphors and 14 spelling errors) and 636 adverse events (from which 48 were nominal anaphors and 17 spelling errors). The corpus is available for academic purposes8. To measure the inter-annotator agreement we used the F-measure metric. This metric approximates the kappa coefficient (Cohen, 1960) 
                                                            
8 http://labda.inf.uc3m.es/SpanishADRCorpus 
when the number of true negatives (TN) is very large (Hripcsak and Rothschild, 2005). In our case, we can state that the number of TN is very high since TN are all the terms that are not true positives, false positives nor false negatives. The F-measure was calculated by comparing the two corpora created by the two first annotators. The corpus labelled by the first annotator was considered the gold-standard. As it was expected, drugs exhibit a high IAA (0.89), while adverse events point to moderate agreement (0.59). As drugs have specific names and there are a limited number of them, it is possible to create a limited and controlled vocabulary to gather many of the existing drugs. On the other hand, patients can express their adverse events in many different ways due to the variability and richness of natural language. Sentence Final Decision 
De entre los distintos antiretrovirales, transcriptasa inversa, proteasa, integrasa y fusi?n, qu? grupo ser?a el m?s potente y cual el menos. 
Names in bold type refer to four families of inhibitors (that is, drug families), and thereby, they should be annotated. 
Como complemento proteico recomendamos el de los laboratorio Vegenat. Si compras los complementos del Decathlon, aseg?rate que contenga prote?nas. 
The mention ?complementos del Decathlon? should not be annotated as a drug since it is not a brand-marked drug.   
Table 1: Some examples of disagreements between annotators  3.2 Constructing a dictionary for drugs and adverse events Since our goal is to identify drugs and adverse events from user comments, the first challenge is to create a dictionary that contains all of the drugs and known adverse events.  CIMA9 is an online information center about medicines that provides all the daily updated official information about drugs. CIMA is                                                             
9 http://www.aemps.gob.es/cima/ 
110
maintained by the Spanish Agency for Medicines and Health Products (AEMPS). It includes information on all drugs authorized in Spain and their current authorization status. CIMA contains a total of 16,418 brand drugs and 2,228 generic drugs. Many brand drug names include additional information such as dosages, mode and route of administration, laboratory, among others (for example, ?ESPIDIFEN 400 mg GRANULADO PARA SOLUCION ORAL SABOR ALBARICOQUE? or ?ESPIDIFEN 600 mg GRANULADO PARA SOLUCION ORAL SABOR LIMON EFG, 20 sobres?). Since it is unlikely that these long names are used by patients, we implemented a method to shorten them by removing their additional information (for example, ?ESPIDIFEN?). After applying this method, the resulting list of brand drug names consisted of 3,662 terms. The main limitation of CIMA is that it only provides information about drugs authorized in Spain. That is, CIMA does not contain information about drugs approved only in Latin America. CIMA is free and offers a downloadable version in XML format. Thus, it provides the information in a well-structured format that makes it possible to directly extract generic and brand drug names as well as other related information such as their ATC codes, their pharmaceutical company, among others.  Unfortunately, CIMA does not provide information about drug groups. For this reason, we decided to consider the WHO ATC system10, a classification system of drugs, as an additional resource to obtain a list of drug groups.  MedDRA 11  is a medical terminology dictionary about events associated with drugs. It is a multilingual terminology, which includes the following languages: Chinese, Czech, Dutch, French, German, Hungarian, Italian, Japanese, Portuguese and Spanish. Its main goal is to provide a classification system for efficient communication of ADRs data between countries. The main advantage of MedDRA is that its                                                             
10 http://www.whocc.no/atc_ddd_index/ 11 http://www.meddra.org/ 
structured format allows easily obtaining a list of possible adverse events. MedDRA is composed of a five levels hierarchy. We collected the terms from the most specific level, "Lowest Level Terms" (LLTs)?. This level contains a total of 72,072 terms, which express how information is communicated in practice.  By analyzing the information from these resources, we found that none of them contained all of the drugs and adverse events. Patients usually use lay terms to describe their symptoms and their treatments. Unfortunately, many of these lay terms are not included in the above mentioned resources. Therefore, we decided to integrate additional information from other resources devoted to patients to build a more complete and comprehensive dictionary. There are several online websites that provide information to patients on drugs and their side effects in Spanish language. For example, MedLinePlus and Vademecum contain information about drugs and their side effects. These websites allow users to browse by generic or drug name, providing an information leaflet for each drug in a HTML page. Since these leaflets are unstructured, the extraction of drugs and their adverse effects is a challenging task. While drug names are often located in specific fields (such as title), their adverse events are usually descriptions of harmful reactions in natural language. We only developed a web crawler to browse and download pages related to drugs from Vademecum since this website provided an easier access to its drug pages than MedLinePlus. We plan to augment the list of drugs and adverse events by crawling MedLinePlus in future work.  After extracting drugs and adverse events from these different resources, we created a dictionary of drugs and adverse events. Table 2 shows the statistics of our final dictionary. 
Resource Total 
Generic drugs from CIMA 2,228 
Brand drugs from CIMA 3,662   
111
Drug group names from the ATC system 466 
Drug names (which are not in CIMA) from Vademecum 1,237 
Total Drugs: 7,593 
Table 2: Number of drugs in the dictionary. Resource Total 
Adverse events from MedDRA 72,072 
Adverse events from Vademecum (which are not in MedDRA) 2,793 
Total adverse events: 74,865 
Table 3: Number of adverse events in the dictionary. 3.3 Using Textalytics and gazetteers to identify drugs and adverse events Textalytics 12  is a multilingual text analysis engine to extract information from any type of texts such as tweets, posts, comments, news, contracts, etc. This tool offers a wide variety of functionalities such as text classification, entity recognition, concept extraction, relation extraction and sentiment analysis, among others. We used a plugin that integrates Textalytics with GATE. In this paper, we applied entity recognition provided by Textalytics, which follows a dictionary-based approach to identify entities in texts. We created a dictionary for drugs and adverse events from CIMA and MedDRA. This dictionary was integrated into Textalytics. Additionally, the lists of drugs and adverse events collected from the others resources (ATC system and Vademecum) were used to create GATE gazetteers.  4 Results and error analysis We evaluated the system on the corpus annotated with drugs and adverse events.  The results of this study show a precision of 87% for drugs and 85% for adverse events, and a recall of 80% for drugs and 56% for adverse events.  
                                                            
12 https://textalytics.com/ 
We performed an analysis to determine the main sources of error in the system. A sample of 50 user comments were randomly selected and analyzed. Regarding the detection of adverse events, the major cause of false negatives was the use of colloquial expressions to describe an adverse event. Phrases like ?me deja ko (it makes me KO)? or ?me cuesta m?s levantarme (it?s harder for me to wake up)? were used by patients for expressing their adverse events. These phrases are not included in our dictionary. A possible solution may be to create a lexicon containing this kind of idiomatic expressions. The second highest cause of false negatives for adverse events was due to the different lexical variations of the same adverse event. For example, ?depresi?n (depression)? is included in our dictionary, but their lexical variations such as ?depremido (depress)?, ?me deprimo (I get depressed)?, ?depresivo (depressive)? or ?deprimente (depressing)? were not detected by our system since they are not in our dictionary. Nominalization may be used to identify all the possible lexical variations of a same adverse event. Another important error source of false negatives was spelling mistakes (eg. hemorrajia instead of hemorragia). Many users have great difficulty in spelling unusual and complex technical terms. This error source may be handled by a more advanced matching method capable of dealing with the spelling error problem. The use of abbreviations (?depre? is an abbreviation for ?depression?) also produces false negatives. Techniques such as lemmatization and stemming may help to resolve this kind of abbreviations.  False positives for adverse events were mainly due to the inclusion of MedDRA terms referring to procedures (such as therapeutic, preventive or laboratory procedures) and tests in our dictionary. MedDRA includes terms for diseases, signs, abnormalities, procedures and tests.  We should have not included those terms referring to procedures and tests since they do not represent adverse events.  
112
The main source of false negatives for drugs seems to be that users often misspelled drug names. Some generic and brand drugs have complex names for patients. Some examples of misspelled drugs are avilify (Abilify) or rivotril (ribotril). Another important cause of false negatives was due to the fact that our dictionary does not include drugs approved in other countries than Spain (for example, Clorimipramina, Ureadin or Paxil). However, ForumClinic has a large number of users in Latin America. It is possible that these users have posted comments about some drugs that have only been approved in their countries. The third largest source of errors was the abbreviations for drug families. For instance, benzodiacepinas (benzodiazepine) is commonly used as benzos, which is not included in our dictionary. An interesting source of errors to point out is the use of acronyms referring to a combination of two or more drugs. For instance, FEC is a combination of Fluorouracil, Epirubicin and Cyclophosphamide, three chemotherapy drugs used to treat breast cancer. This combination of drugs is not registered in the resources (CIMA and Vademecum) used to create our dictionary. Most false positives for drugs were due to a lack of ambiguity resolution. Some drug names are common Spanish words such as ?All?? (a slimming drug) or ?Puntual? (a laxative). These terms are ambiguous and resolve to multiple senses, depending on the context in which they are used. Similarly, some drug names such as ?alcohol? or ?oxygen? can take a meaning different than the one of pharmaceutical substance. Another important cause of false positives is due to the use of drug family names as adjectives that specify an effect. This is the case of sedante (sedative) or antidepresivo (antidepressant), which can refer to a family of drugs, but also to the definition of an effect or disorder caused by a drug (sedative effects). 5 Conclusion  In this research, we created the first Spanish corpus of health user comments annotated with drugs and adverse events. The corpus is available 
for research. In this work, we only focused on the detection of the mentions of drugs and adverse events, but not the relationships among them. In future work, we plan to extend the system to detect the relationships between drugs and their side effects. Also, we would like to identify their indications and beneficial effects.  Acknowledgments This work was supported by the EU project TrendMiner [FP7-ICT287863], by the project MULTIMEDICA [TIN2010-20644-C03-01], and by the Research Network MA2VICMR [S2009/TIC-1542]. References  Rakesh Agrawal and Ramakrishnan Srikant. 1994. Fast algorithms for mining association rules. In Proc. 20th Int. Conf. Very Large Data Bases, 1215:487-499. Alan R Aronson and Francois-Michel Lang. 2010. An overview of MetaMap: historical perspective and recent advances. Journal of the American Medical Informatics Association, 17(3):229-236.  Alexandra Balahur. 2013. Sentiment Analysis in Social Media Texts. WASSA 2013, 120. David W. Bates, R Scott Evans, Harvey Murff, Peter D. Stetson, Lisa Pizziferri and George Hripcsak. 2003. Detecting adverse events using information technology. Journal of the American Medical Informatics Association, 10(2):115-128. Adrian Benton, Lye Ungar, Shawndra Hill, Sean Hennessy, Jun Mao, Annie Chung, Charles E. Leonarda and John H. Holmes. 2011. Identifying potential adverse effects using the web: A new approach to medical hypothesis generation. Journal of biomedical informatics, 44(6): 989-996. Jiang Bian, Umit Topaloglu and Fan Yu. 2012. Towards large-scale twitter mining for drug-related adverse events. In Proceedings of the 2012 international workshop on Smart health and wellbeing, 25-32. CA. Bond and Cynthia L. Raehl. 2006. Adverse drug reactions in United States hospitals. Pharmacotherapy: The Journal of Human Pharmacology and Drug Therapy, 26(5):601-608.  
113
Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychol Meas ;20:37e46. Ronald A. Fisher. 1922. On the interpretation of ? 2 from contingency tables, and the calculation of P. Journal of the Royal Statistical Society, 85(1):87-94. Flavien Bouillot, Phan N. Hai, Nicolas B?chet, Sandra Bringay, Dino Ienco, Stan Matwin, Pascal Poncelet, Mathiue Roche and Maguelonne Teisseire. 2013. How to Extract Relevant Knowledge from Tweets?. Communications in Computer and Information Science.  Carol Friedman. 2009. Discovering novel adverse drug events using natural language processing and mining of the electronic health record. In Artificial Intelligence in Medicine. LNAI 5651:1 -5. Graciela H. Gonzalez, Matthew L Scotch and Garrick L Wallstrom. Mining Social Network Postings for Mentions of Potential Adverse Drug Reactions. HHS-NIH-NLM (9/10/2012 - 8/31/2016). Harsha Gurulingappa, Abdul Mateen-??Rajput and Luca Toldo. 2012. Extraction of potential adverse drug events from medical case reports. Journal of biomedical semantics. 3(1):15. Harsha Gurulingappa, Luca Toldo, Abdul Mateen-Rajput, Jan A. Kors, Adel Taweel and Yorki Tayrouz. 2013. Automatic detection of adverse events to predict drug label changes using text and data mining techniques. Pharmacoepidemiology and drug safety, 22(11):1189-1194. A Herxheimer, MR Crombag and TL Alves. 2010. Direct patient reporting of adverse drug reactions. A twelve-country survey & literature review. Health Action International (HAI). Europe. Paper Series Reference 01-2010/01. Shawndra Hill, Raina Merchant and Lile Ungar. (2013). Lessons Learned About Public Health from Online Crowd Surveillance. Big Data, 1(3):160-167.  George Hripcsak and Adam S. Rothschild. 2005. Agreement, the F-measure, and reliability in information retrieval. J Am Med Inform Assoc.12:296e8.  Martin Krallinger, Florian Leitner, Obdulia Rabal, Miguel Vazquez, Julen Oyarzabal and Alfonso Valencia. 2013. Overview of the chemical compound and drug name recognition 
(CHEMDNER) task. In BioCreative Challenge Evaluation Worksho. 2:2-33. Michael Kuhn, Monica Campillos, Ivica Letunic, Lars J. Jensen and Peer Bork. 2010. A side effect resource to capture phenotypic effects of drugs. Molecular systems biology, 6(343):1-6. Robert Leaman, Laura Wojtulewicz, Ryan Sullivan, Annie Skariah, Jian Yang and Graciela Gonzalez. 2010. Towards internet-age pharmacovigilance: extracting adverse drug reactions from user posts to health-related social networks. In Proceedings of the 2010 workshop on biomedical natural language processing. 117-125. Association for Computational Linguistics. Anne J. Leendertse, Antoine C. Egberts, Lennar J. Stoker, & Patricia M.L.A. van den Bemt. 2008. Frequency of and risk factors for preventable medication-related hospital admissions in the Netherlands. Archives of internal medicine, 168(17), 1890.  Qi Li, Louise Deleger, Todd Lingren, Haijun Zhai, Megan Kaiser, Laura Stoutenborough Anil G Jegga, Kevin B Cohen and Imre Solti. 2013. Mining FDA drug labels for medical conditions. BMC medical informatics and decision making, 13(1):53. Mark McClellan. 2007. Drug Safety Reform at the FDA-Pendulum Swing or Systematic Improvement?. New England Journal of Medicine, 356(17):1700-1702. Silvio Moreira, Joao Filgueiras, Bruno Martins, Francisco Couto and Mario J. Silva. 2013. REACTION: A naive machine learning approach for sentiment classification. In 2nd Joint Conference on. Lexical and Computational Semantics. 2:490-494.  Melanie Neunerdt, Michael Reyer and Rudolf Mathar. 2013. A POS Tagger for Social Media Texts trained on Web Comments. Polibits, 48:59-66. Azadeh Nikfarjam and Graciela H. Gonzalez. 2011. Pattern mining for extraction of mentions of adverse drug reactions from user comments. In AMIA Annual Symposium Proceedings, 2011:1019-1026. American Medical Informatics Association.  Isabel Segura-Bedmar, Paloma Mart?nez and Mar?a Herrero-Zazo. 2013. SemEval-2013 Task 9: Extraction of Drug-Drug Interactions from 
114
Biomedical Texts (DDIExtraction 2013). 3206(65): 341-351. Cornelis S. van Der Hooft, Miriam CJM Sturkenboom, Kees van Grootheest, Herre J. Kingma and Bruno HCh Stricker. 2006. Adverse drug reaction-related hospitalisations. Drug Safety, 29(2):161-168.  Hamish Cunningham. 2002. GATE, a general architecture for text engineering. Computers and the Humanities, 36(2):223-254. M Rawlins. 1995. Pharmacovigilance: paradise lost, regained or postponed? The William Withering Lecture 1994. Journal of the Royal College of Physicians of London, 29(1): 41-49.  Sunghwan Sohn, Jean-Pierre A. Kocher, Christopher G. Chute and Guergana K. Savova. 2011. Drug side effect extraction from clinical narratives of 
psychiatry and psychology patients. Journal of the American Medical Informatics Association, 18(Suppl 1):i144-i149. ?zlem Uzuner, Imre Solti and Eithon Cadag. 2010. Extracting medication information from clinical text. Journal of the American Medical Informatics Association. 17(5):514-518. Rong Xu and QuanQiu Wang. 2013. Large-scale extraction of accurate drug-disease treatment pairs from biomedical literature for drug repurposing. BMC Bioinformatics, 14(1):181. Karin Wester, Anna K. J?nsson, Olav Spigset, Henrik Druid and Staffan H?gg. 2008. Incidence of fatal adverse drug reactions: a population based study. British journal of clinical pharmacology, 65(4):573-579.  
115
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 98?106,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Extracting drug indications and adverse drug reactions from Spanish
health social media
Isabel Segura-Bedmar, Santiago de la Pe
?
na, Paloma Mart??nez
Computer Science Department
Carlos III University of Madrid, Spain
{isegura|spena|pmf}@inf.uc3m.es
Abstract
In this paper, we present preliminary re-
sults obtained using a system based on co-
occurrence of drug-effect pairs as a first
step in the study of detecting adverse drug
reactions and drug indications from social
media texts. To the best of our knowl-
edge, this is the first work that extracts
this kind of relationships from user mes-
sages that were collected from an online
Spanish health-forum. In addition, we also
describe the automatic construction of the
first Spanish database for drug indications
and adverse drug reactions.
1 Introduction
The activity of Pharmacovigilance (science de-
voted to the detection and prevention of any possi-
ble drug-related problem, including adverse drug
effects) has gained significant importance in the
recent decades, due to the growing number of drug
safety incidents (Bond and Raehl, 2006) as well as
to their high associated costs (van Der Hooft et al.,
2006).
Nowadays, the major medicine regulatory agen-
cies such as the US Food and Drug Administra-
tion (FDA) or the European Medicines Agency
(EMA) are working to create policies and prac-
tices to facilitate the reporting of adverse drug re-
actions (ADRs) by healthcare professionals and
patients. However, several studies have shown that
ADRs are under-estimated because many health-
care professionals do not have enough time to use
the ADR reporting systems (Bates et al., 2003;
van Der Hooft et al., 2006; McClellan, 2007) .
In addition, healthcare professionals tend to re-
port only those ADRs on which they have abso-
lute certainty of their existence. Unlike reports
from healthcare professionals, patient reports of-
ten provide more detailed and explicit information
about ADRs (Herxheimer et al., 2010). Neverthe-
less, the rate of ADRs reported by patients is still
very low probably because many patients are still
unaware of the existence of ADR reporting sys-
tems. In addition, patients may feel embarrassed
when describing their symptoms.
In this paper, we pose the hypothesis that
health-related social media can be used as a com-
plementary data source to the ADR reporting sys-
tems. In particular, health forums contain a large
number of comments describing patient experi-
ences that would be a fertile source of data to de-
tect unknown ADRs.
Several systems have been developed for ex-
tracting ADRs from social media (Leaman et al.,
2010; Nikfarjam and Gonzalez, 2011). However
to the best of our knowledge, only one work in the
literature has focused on the detection of ADRs
from social media in Spanish (Segura-Bedmar et
al., 2014). Indeed, it is only concerned with the
detection of mentions of drugs and their effects,
without dealing with the extraction of the relation-
ships between them. In this paper, we extend this
existing work in order to extract drug indications
and adverse drug reactions from user comments in
a Spanish health-forum.
The remaining of this paper is structured as fol-
lows: the next section surveys related work on
ADR detection from social media. Section 3 de-
scribes the creation of a gold-standard corpus we
used for our experiments. Sections 4 and 5 re-
spectively describe the techniques employed and
their results. Lastly, some conclusive remarks and
future perspectives are given in Section 6.
2 Related Work
In recent years, the application of Natural Lan-
guage Processing (NLP) techniques to mine drug
indications and adverse drug reactions from texts
has been explored with promising results, mainly
in the context of drug labels (Gurulingappa et al.,
98
2013; Li et al., 2013; Kuhn et al., 2010; Fung et al.,
2013), biomedical literature (Xu and Wang, 2013),
medical case reports (Gurulingappa et al., 2012)
and health records (Friedman, 2009; Sohn et al.,
2011). However, as it will be described below, the
extraction of these drug relationships from social
media has received much less attention.
To date, most of research on drug name recogni-
tion concerns either biomedical literature (Segura-
Bedmar et al., 2013; Krallinger et al., 2013) or
clinical records (Uzuner et al., 2010), thus leaving
unexplored this task in social media texts.
To our knowledge, there is no work in the lit-
erature that addresses the extraction of drug in-
dications from social media texts. Regarding the
detection of ADRs, Leaman et al., (2010) devel-
oped a system to automatically recognize adverse
effects in user comments from the DailyStrength
1
health-related social network. A corpus of 3,600
comments was manually annotated with a total
of 1,866 drug conditions, including beneficial ef-
fects, adverse effects, indications and others. This
study focused only on a set of four drugs, and
thereby, drug name recognition was not addressed.
The system used a dictionary-based approach to
identify adverse effects and a set of keywords in
order to distinguish adverse effects from the other
drug conditions. The dictionary consisted of 4,201
concepts, which were collected from several re-
sources such as the COSTART vocabulary (FDA,
1970), the SIDER database (Kuhn et al., 2010),
the MedEffect database
2
and a list of colloquial
phrases manually collected from the comments.
The system achieved a precision of 78.3% and a
recall of 69.9% (an f-measure of 73.9%).
Later, Nikfarjam and Gonzalez (2011) applied
association rule mining to extract frequent pat-
terns describing opinions about drugs. The rules
were generated using the Apriori tool, an imple-
mentation of the Apriori algorithm (Agrawal et al.,
1994) for association rule mining. The main ad-
vantage of this approach over the dictionary based
approach is that the system is able to detect terms
not included in the dictionary. The results of this
study were 70.01% precision and 66.32% recall,
for an f-measure of 67.96%.
Benton et al.,(2011) collected a lexicon of lay
medical terms from websites and databases about
drugs and their adverse effects to identify drug ef-
1
http://www.dailystrength.org/
2
http://www.hc-sc.gc.ca/dhp-mps/medeff/index-eng.php
fects. Then, the authors applied the Fishers exact
test (Fisher, 1922) to find all the drug-effect pairs
that co-occurred independently by chance in a cor-
pus of user comments. To evaluate the system, the
authors focused only on the four most commonly
used drugs to treat breast cancer. Precision and
recall were calculated by comparing the adverse
effects from their drug labels and the adverse ef-
fects obtained by the system. The system obtained
an average precision of 77% and an average recall
of 35.1% for all four drugs.
To the best of our knowledge, the system de-
scribed in (Segura-Bedmar et al., 2014) is the only
one that has dealt with the detection of drugs and
their effects from Spanish social media streams.
The system used the Textalytics tool
3
, which fol-
lows a dictionary-based approach to identify en-
tities in texts. The dictionary was constructed
based on the following resources: CIMA
4
and
MedDRA
5
. CIMA is an online information center
maintained by the Spanish Agency for Medicines
and Health Products (AEMPS). CIMA provides
information on all drugs authorized in Spain,
though it does not include drugs approved only in
Latin America. CIMA contains a total of 16,418
brand drugs and 2,228 generic drugs. Many brand
drugs have very long names because they include
additional information such as dosages, mode and
route of administration, laboratory, among oth-
ers (for example, ESPIDIFEN 400 mg GRANU-
LADO PARA SOLUCION ORAL SABOR ALBARI-
COQUE). For this reason, brand drug names were
simplified before being included in the dictionary.
After removing the additional information, the re-
sulting list of brand drug names consisted of 3,662
terms. Thus, the dictionary contained a total of
5,890 drugs. As regards to the effects, the au-
thors decided to use MedDRA, a medical multi-
lingual terminology dictionary about events asso-
ciated with drugs. MedDRA is composed of a five
levels hierarchy. A total of 72,072 terms from the
most specific level, ?Lowest Level Terms? (LLTs),
were integrated into the dictionary. In addition,
several gazetteers including drugs and effects were
collected from websites such as Vademecum
6
, a
Spanish online website that provides information
to patients on drugs and their side effects, and
3
https://textalytics.com/
4
http://www.aemps.gob.es/cima/
5
http://www.meddra.org/
6
http://www.vademecum.es/
99
the ATC system
7
, a classification system of drugs.
Thus, the dictionary and the two gazetteers con-
tained a total of 7,593 drugs and 74,865 effects.
The system yielded a precision of 87% for drugs
and 85% for effects, and a recall of 80% for drugs
and 56% for effects.
3 The SpanishADR corpus
Segura-Bedmar et al., (2014) created the first
Spanish corpus of user comments annotated with
drugs and their effects. The corpus consists of
400 comments, which were gathered from Fo-
rumClinic
8
, an interactive health social platform,
where patients exchange information about their
diseases and their treatments. The texts were man-
ually annotated by two annotators with expertise
in Pharmacovigilance. All the mentions of drugs
and effects were annotated, even those contain-
ing spelling or grammatical errors (for example,
hemorrajia (haemorrhage)). An assessment of the
inter-annotator agreement (IAA) was based on the
F-measure metric, which approximates the kappa
coefficient (Cohen, 1960) when the number of true
negatives (TN) is very large (Hripcsak and Roth-
schild, 2005). This assessment revealed that while
drugs showed a high IAA (0.89), their effects point
to moderate agreement (0.59). This may be due
to drugs have specific names and there are a lim-
ited number of them, however their effects are ex-
pressed by patients in many different ways due to
the variability and richness of natural language.
The corpus is available for academic purposes
9
.
In this paper, we extend the Spanish corpus to
incorporate the annotation of the relationships be-
tween drugs and their effects. In particular, we
annotated drug indications and adverse drug reac-
tions. These relationships were annotated at com-
ment level rather than sentence level, because de-
termining sentence boundaries in this kind of texts
can be problematic since many users often write
ungrammatical sentences. Guidelines were cre-
ated by two annotators (A1, A2) and a third an-
notator (A3) was trained on the annotation guide-
lines. Then, we split the corpus in three subsets,
and each subset was annotated by one annotator.
Finally, IAA was measured using kappa-statistic
on a sample of 97 documents randomly selected.
These documents were annotated by the three an-
7
http://www.whocc.no/atc ddd index/
8
http://www.forumclinic.org/
9
http://labda.inf.uc3m.es/SpanishADRCorpus
notators and annotation differences were analysed.
As Table 1 shows, the resulting corpus has 61
drug indications and 103 adverse drug reactions.
The average size of a comment is 72 tokens. The
average size of a text fragment describing a drug
indication is 34.7 tokens and 28.2 tokens for ad-
verse drug reactions.
Annotation Size
drugs 188
effect 545
drug indication 61
adverse drug reaction 103
Table 1: Size of the extended SpanishADR corpus.
As it is shown in Table 2, the IAA figures clearly
suggest that the annotators have high agreement
among them. We think that the IAA figures were
lower with the third annotator because he did not
participate in the guidelines development process,
and maybe, he was not trained well enough to per-
form the task. The main source of disagreement
among the annotators could arise from consider-
ing whether a term refers to a drug effect or not.
This is due to some terms are too general (such as
trastorno (upset), enfermedad (disease), molestia
(ache)). The annotators A1 and A2, in general,
ruled out all the relation instances where these
general terms occur, however they were consid-
ered and annotated by the third annotator.
A2 A3
A1 0.8 0.69
A2 - 0.68
Table 2: Pairwise IAA for each combination of
two annotators. IAA was measured using Cohens?
kappa statistic
4 Methods
In this contribution, some refinements to the sys-
tem (Segura-Bedmar et al., 2014) are proposed.
The error analysis performed in (Segura-Bedmar
et al., 2014) showed that most of false positives
for drug effects were mainly due to the inclu-
sion of MedDRA terms referring to procedures
and tests in the dictionary. MedDRA includes
terms for diseases, signs, abnormalities, proce-
dures and tests. Therefore, we decided not to in-
clude terms corresponding to the ?Procedimientos
100
m?edicos y quir?urgicos? and ?Exploraciones com-
plementarias? categories since they do not repre-
sent drug effects. Thus, we created a new dic-
tionary that only includes those terms from Med-
DRA that actually refer to drug effects. As in the
system (Segura-Bedmar et al., 2014), we applied
the Textalytics tool, which follows a dictionary-
based approach, to identify drugs and their ef-
fects occurring in the messages. We created a
GATE
10
pipeline application integrating the Tex-
talytic module and the gazetteers collected from
the Vademecum website and the ATC system pro-
posed in (Segura-Bedmar et al., 2014).
In addition, we created an additional gazetteer
in order to increase the coverage. We developed a
web crawler to browse and download pages related
to drugs from the MedLinePlus website
11
. Un-
like Vademecum, which only contains information
for drugs approved in Spain, MedLinePlus also
includes information about drugs only approved
in Latin America. Terms describing drug effects
were extracted by regular expressions from these
pages and then were incorporated into a gazetteer.
Then, the new gazetteer was also integrated into
the GATE pipeline application to identify drugs
and effects. Several experiments with different
settings of this pipeline are described in the fol-
lowing section.
The main contribution of this paper is to pro-
pose an approach for detecting relationships be-
tween drugs and their effects from user comments
in Spanish. The main difficulty in this task is that
although there are several English databases such
as SIDER or MedEffect with information about
drugs and their side effects, none of them are avail-
able for Spanish. Moreover, these resources do
not include drug indications. Thus, we have au-
tomatically built the first database, SpanishDrug-
EffectBD, with information about drugs, their drug
indications as well as their adverse drug reactions
in Spanish. Our first step was to populate the
database with all drugs and effects from our dic-
tionary. Figure 1 shows the database schema.
Active ingredients are saved into the Drug ta-
ble, and their synonyms and brand names into the
DrugSynset table. Likewise, concepts from Med-
DRA are saved into the Effect table and their syn-
onyms are saved into the EffectSynset table. As
it is shown in Figure 1, the database is also de-
10
http://gate.ac.uk/
11
http://www.nlm.nih.gov/medlineplus/spanish/
signed to store external ids from other databases.
Thus, drugs and effects can be linked to external
databases by the tables has externalIDDrug and
has externalIDDrug, respectively.
To obtain the relationships between drugs
and their effects, we developed several web
crawlers in order to gather sections describing
drug indications and adverse drug reactions from
drug package leaflets contained in the follow-
ing websites: MedLinePlus, Prospectos.Net
12
and
Prospectos.org
13
. Once these sections were down-
loaded, their texts were processed using the Text-
Alyticis tool to recognize drugs and their effects.
As each section (describing drug indications or ad-
verse drug effects) is linked to one drug, we de-
cided to consider the effects contained in the sec-
tion as possible relationships with this drug. The
type of relationship depends on the type of section:
drug indication or adverse drug reaction. Thus for
example, a pair (drug, effect) from a section de-
scribing drug indications is saved into the DrugEf-
fect table as a drug indication relationship, while if
the pair is obtained from a section describing ad-
verse drug reactions, then it is saved as an adverse
drug reaction. This database can be used to au-
tomatically identify drug indications and adverse
drug reactions from texts. Table 3 shows the num-
ber of drugs, effects and their relationships stored
into the database.
Concepts Synonyms
drugs 3,244 7,378
effects 16,940 52,199
drug indications 4,877
adverse drug reactions 58,633
Table 3: Number of drugs, effects, drug indica-
tions and adverse drug effects in the SpanishDrug-
EffectBD database.
As regards to the extraction of the relationships
between drugs and their effects occurring in the
corpus, first of all, texts were automatically an-
notated with drugs and effects using the GATE
pipeline application. Then, in order to generate
all possible relation instances between drugs and
their effects, we considered several sizes of win-
dow: 10, 20, 30, 40 and 50. Given a size n, any
pair (drug, effect) co-occurring within a window
of n-tokens are treated as a relation instance. Af-
12
http://www.prospectos.net/
13
http://prospectos.org/
101
Figure 1: The SpanishDrugEffectBD database schema
terwards, each relation instance is looked up in the
DrugEffect table in order to determine if it is a pos-
itive instance and if this is the case, its type: drug
indication or adverse drug reaction.
5 Experiments
Several experiments have been performed in order
to evaluate the contribution of the proposed meth-
ods and resources. Table 4 shows the results for
the named entity recognition task of drugs and ef-
fects using the dictionary integrated into the Tex-
tAlytic tool. The first row shows the results with
the dictionary built from the CIMA and MedDRA
resources, while the second one shows the results
obtained using the new dictionary in which those
MedDRA terms corresponding to ?Procedimien-
tos m?edicos y quir?urgicos? and ?Exploraciones
complementarias? categories were ruled out. As
it can be seen in this table, the new dictionary per-
mits to obtain a significant improvement with re-
spect to the original dictionary. For effect type,
precision was increased almost a 40% and re-
call a 7%. As regards to the contribution of the
gazetteers, the coverage for effects improves al-
most a 6% but with significant decrease in preci-
sion of almost 21%. Regarding to the detection of
drugs, the use of gazetteers improves slightly the
precision and achieves a significant improvement
in the recall of almost 35%.
The major cause of false negatives for drug ef-
fects was the use of colloquial expressions (such
as ?me deja ko? (it makes me ko)) to describe an
adverse effect. These phrases are not included in
our dictionary. Another important cause was the
dictionary and gazetteers do not cover all the lex-
ical variations of a same effect (for example de-
presi?on (depression), depresivo (depress), me de-
primo (I get depressed)). In addition, many false
negatives were due to spelling mistakes (for ex-
ample hemorrajia instead of hemorragia (haemor-
rhage)) and abbreviations (depre is an abbreviation
for depresi?on (depression)).
Regarding to the results for the relation extrac-
tion task, Table 5 shows the overall results ob-
tained using a baseline system, which considers
all pairs (drug, effect) occurring in messages as
positive relation instances, and a second approach
using the SpanishDrugEffectBD database (a rela-
tion instance is positive only if it is found into the
database). In both experiments, a window size of
250 tokens was used. The database provides a high
precision but with a very low recall of only 15%.
102
Approach Entity P R F1
Dictionary
drugs 0.84 0.46 0.60
effect 0.45 0.38 0.41
New dictionary
drugs 0.84 0.46 0.60
effect 0.84 0.45 0.59
New dictionary plus gazetteers
drugs 0.86 0.81 0.84
effect 0.63 0.51 0.57
Table 4: Precision, Recall and F-measure for named entity recognition task.
As it can be seen in Table 6, when the type of the
relationship is considered, the performance is even
lower.
Approach P R F1
Baseline 0.31 1.00 0.47
SpanishDrugEffectBD 0.83 0.15 0.25
Table 5: Overall results for relation extraction task
(window size of 250 tokens).
Relation P R F1
Drug indication 0.50 0.02 0.03
Adverse drug reaction 0.65 0.11 0.18
Table 6: Results for drug indications and adverse
drug reactions using only the database (window
size of 50 tokens).
Figure 2 shows an example of the output of our
system using the database. The system is able to
detect the relationship of indication between al-
prazolman and ansiedad (anxiety), but fails in de-
tecting the adverse drug reaction between alpra-
zolman and dependencia (dependency). The ad-
verse drug reaction between lamotrigina and ver-
tigo is detected.
The co-occurrence approach provides better re-
sults than the use of the database. Table 7 shows
the results for different size of windows. As it was
expected, small sizes provide better precision but
lower recall.
6 Conclusion
In this paper we present the first corpus where
400 user messages from a Spanish health social
network have been annotated with drug indica-
tions and adverse drug reactions. In addition, we
present preliminary results obtained using a very
simple system based on co-occurrence of drug-
effect pairs as a first step in the study of detecting
Size of window P R F1
10 0.71 0.24 0.36
20 0.59 0.53 0.56
30 0.52 0.69 0.59
40 0.47 0.77 0.58
50 0.44 0.84 0.58
Table 7: Overall results for relation extraction task
using the co-occurrence approach considering dif-
ferent window sizes.
adverse drug reactions and drug indications from
social media streams. Results show that there is
still much room for improvement in the identifica-
tion of drugs and effects, as well as in the extrac-
tion of drug indications and adverse drug rections.
As it was already mentioned in Section 2, the
recognition of drugs in social media texts has
hardly been tackled since most systems were fo-
cused on a given and fixed set of drugs. Moreover,
little research has been conducted to extract rela-
tionships between drugs and their effects from so-
cial media. Most systems for extracting ADRs fol-
low a dictionary-based approach. The main draw-
back of these systems is that they fail to recog-
nize terms which are not included in the dictio-
nary. In addition, the dictionary-based approach
is not able to handle the large number of spelling
and grammar errors in social media texts. More-
over, the detection of ADRs and drug indications
has not been attempted for languages other than
English. Indeed, automatic information extraction
from Spanish-language social media in the field of
health remains largely unexplored.
Social media texts pose additional challenges
to those associated with the processing of clin-
ical records and medical literature. These new
challenges include the management of meta-
information included in the text (for example as
tags in tweets)(Bouillot et al., 2013), the detection
of typos and unconventional spelling, word short-
103
Figure 2: An example of the output of the system using the database.
enings (Neunerdt et al., 2013; Moreira et al., 2013)
and slang and emoticons (Balahur, 2013), among
others. Another challenge that should be taken
into account is that while clinical records and med-
ical literature can be mapped to terminological re-
sources or biomedical ontologies, lay terminology
used by patients to describe their treatments and
their effects, in general, is not collected in any ter-
minological resource, which would facilitate the
automatic processing of this kind of texts.
In this paper, we also describe the automatic
creation of a database for drug indications and ad-
verse drug reactions from drug package leaflets.
To the best of our knowledge, this is the first
database available for Spanish. Although the use
of this database did not improve the results due
to its limited coverage, we think that the database
could be a valuable resource for future efforts.
Thus, we plan to translate the database into an on-
tology and to populate it with more entities and re-
lationships. As future work, we plan the following
tasks:
? To create a lexicon containing idiomatic ex-
pressions used by patients to express drug ef-
fects.
? To use techniques such as lemmatization and
stemming to cope with the problem of lexical
variability and to resolve abbreviations.
? To integrate advanced matching methods ca-
pable of dealing with the spelling error prob-
lem.
? To increase the size of the corpus.
? To apply a SVM classification approach to
extract relationships between drugs and their
effects.
We hope our research will be beneficial to
AEMPS as well as to the pharmaceutical indus-
try in the improvement of their pharmacovigilance
systems. Both the corpus and the database are
freely available online
14
for research purposes.
Acknowledgments
This work was supported by the EU project Trend-
Miner [FP7-ICT287863], by the project MUL-
TIMEDICA [TIN2010-20644-C03-01], and by
the Research Network MA2VICMR [S2009/TIC-
1542].
References
Rakesh Agrawal, Ramakrishnan Srikant, et al. 1994.
Fast algorithms for mining association rules. In
Proc. 20th int. conf. very large data bases, VLDB,
volume 1215, pages 487?499.
Alexandra Balahur. 2013. Sentiment analysis in social
media texts. WASSA 2013, page 120.
David W Bates, R Scott Evans, Harvey Murff, Peter D
Stetson, Lisa Pizziferri, and George Hripcsak. 2003.
Detecting adverse events using information technol-
ogy. Journal of the American Medical Informatics
Association, 10(2):115?128.
Adrian Benton, Lyle Ungar, Shawndra Hill, Sean Hen-
nessy, Jun Mao, Annie Chung, Charles E Leonard,
and John H Holmes. 2011. Identifying potential
adverse effects using the web: A new approach to
14
http://labda.inf.uc3m.es/SpanishADRCorpus
104
medical hypothesis generation. Journal of biomedi-
cal informatics, 44(6):989?996.
CA Bond and Cynthia L Raehl. 2006. Adverse drug
reactions in united states hospitals. Pharmacother-
apy: The Journal of Human Pharmacology and
Drug Therapy, 26(5):601?608.
Flavien Bouillot, Phan Nhat Hai, Nicolas B?echet, San-
dra Bringay, Dino Ienco, Stan Matwin, Pascal Pon-
celet, Mathieu Roche, and Maguelonne Teisseire.
2013. How to extract relevant knowledge from
tweets? In Information Search, Integration and Per-
sonalization, pages 111?120. Springer.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
FDA. 1970. National adverse drug reaction directory:
Costart (coding symbols for thesaurus of adverse re-
action terms). Rock-Irvine, Charles F, Sharp,) r,
MD, Huntington Memorial Hospital, Stuart l, Sil-
verman, MD, University of California, los Angeles,
West los Angeles-Veterans Affairs Medical Center,
Osteoporosis Medical Center.
Ronald A Fisher. 1922. On the interpretation of chi-
squared from contingency tables, and the calcula-
tion of p. Journal of the Royal Statistical Society,
85(1):87?94.
Carol Friedman. 2009. Discovering novel adverse
drug events using natural language processing and
mining of the electronic health record. In Artificial
Intelligence in Medicine, pages 1?5. Springer.
Kin Wah Fung, Chiang S Jao, and Dina Demner-
Fushman. 2013. Extracting drug indication infor-
mation from structured product labels using natural
language processing. Journal of the American Med-
ical Informatics Association, 20(3):482?488.
Harsha Gurulingappa, Abdul Mateen-Rajput, Luca
Toldo, et al. 2012. Extraction of potential adverse
drug events from medical case reports. J Biomed
Semantics, 3(1):15.
Harsha Gurulingappa, Luca Toldo, Abdul Mateen Ra-
jput, Jan A Kors, Adel Taweel, and Yorki Tayrouz.
2013. Automatic detection of adverse events to pre-
dict drug label changes using text and data min-
ing techniques. Pharmacoepidemiology and drug
safety, 22(11):1189?1194.
A Herxheimer, MR Crombag, and TL Alves. 2010.
Direct patient reporting of adverse drug reactions. a
twelve-country survey & literature review. Health
Action International (HAI)(Europe). Amsterdam.
George Hripcsak and Adam S Rothschild. 2005.
Agreement, the f-measure, and reliability in infor-
mation retrieval. Journal of the American Medical
Informatics Association, 12(3):296?298.
Martin Krallinger, Florian Leitner, Obdulia Rabal,
Miguel Vazquez, Julen Oyarzabal, and Alfonso Va-
lencia. 2013. Overview of the chemical compound
and drug name recognition (chemdner) task. In
BioCreative Challenge Evaluation Workshop vol. 2,
page 2.
Michael Kuhn, Monica Campillos, Ivica Letunic,
Lars Juhl Jensen, and Peer Bork. 2010. A side ef-
fect resource to capture phenotypic effects of drugs.
Molecular systems biology, 6(1).
Robert Leaman, Laura Wojtulewicz, Ryan Sullivan,
Annie Skariah, Jian Yang, and Graciela Gonzalez.
2010. Towards internet-age pharmacovigilance: ex-
tracting adverse drug reactions from user posts to
health-related social networks. In Proceedings of
the 2010 workshop on biomedical natural language
processing, pages 117?125. Association for Compu-
tational Linguistics.
Qi Li, Louise Deleger, Todd Lingren, Haijun Zhai,
Megan Kaiser, Laura Stoutenborough, Anil G Jegga,
Kevin Bretonnel Cohen, and Imre Solti. 2013. Min-
ing fda drug labels for medical conditions. BMC
medical informatics and decision making, 13(1):53.
Mark McClellan. 2007. Drug safety reform at
the fdapendulum swing or systematic improvement?
New England Journal of Medicine, 356(17):1700?
1702.
Silvio Moreira, Joao Filgueiras, and Bruno Martins.
2013. Reaction: A naive machine learning approach
for sentiment classification. In Proceedings of the
7th InternationalWorkshop on Semantic Evaluation
(SemEval 2013), page 490.
Melanie Neunerdt, Michael Reyer, and Rudolf Mathar.
2013. A pos tagger for social media texts trained on
web comments. Polibits, 48:59?66.
Azadeh Nikfarjam and Graciela H Gonzalez. 2011.
Pattern mining for extraction of mentions of adverse
drug reactions from user comments. In AMIA An-
nual Symposium Proceedings, volume 2011, page
1019. American Medical Informatics Association.
Isabel Segura-Bedmar, Paloma Mart??nez, and Mar?a
Herrero-Zazo. 2013. Semeval-2013 task 9: Ex-
traction of drug-drug interactions from biomedical
texts (ddiextraction 2013). Proceedings of Semeval,
pages 341?350.
Isabel Segura-Bedmar, Ricardo Revert, and Paloma
Martnez. 2014. Detecting drugs and adverse events
from spanish social media streams. In Proceedings
of the 5th International Louhi Workshop on Health
Document Text Mining and Information Analysis
(Louhi 2014).
Sunghwan Sohn, Jean-Pierre A Kocher, Christopher G
Chute, and Guergana K Savova. 2011. Drug side ef-
fect extraction from clinical narratives of psychiatry
and psychology patients. Journal of the American
Medical Informatics Association, 18(Suppl 1):i144?
i149.
105
?Ozlem Uzuner, Imre Solti, and Eithon Cadag. 2010.
Extracting medication information from clinical
text. Journal of the American Medical Informatics
Association, 17(5):514?518.
Cornelis S van Der Hooft, Miriam CJM Sturkenboom,
Kees van Grootheest, Herre J Kingma, and Bruno
H Ch Stricker. 2006. Adverse drug reaction-related
hospitalisations. Drug Safety, 29(2):161?168.
Rong Xu and QuanQiu Wang. 2013. Large-scale
extraction of accurate drug-disease treatment pairs
from biomedical literature for drug repurposing.
BMC bioinformatics, 14(1):181.
106
