Semantic Role Labeling: An Introduction to
the Special Issue
Llu??s Ma`rquez?
Universitat Polite`cnica de Catalunya
Xavier Carreras??
Massachusetts Institute of Technology
Kenneth C. Litkowski?
CL Research
Suzanne Stevenson?
University of Toronto
Semantic role labeling, the computational identification and labeling of arguments in text,
has become a leading task in computational linguistics today. Although the issues for this
task have been studied for decades, the availability of large resources and the development of
statistical machine learning methods have heightened the amount of effort in this field. This
special issue presents selected and representative work in the field. This overview describes
linguistic background of the problem, the movement from linguistic theories to computational
practice, the major resources that are being used, an overview of steps taken in computational
systems, and a description of the key issues and results in semantic role labeling (as revealed in
several international evaluations). We assess weaknesses in semantic role labeling and identify
important challenges facing the field. Overall, the opportunities and the potential for useful
further research in semantic role labeling are considerable.
1. Introduction
The sentence-level semantic analysis of text is concerned with the characterization of
events, such as determining ?who? did ?what? to ?whom,? ?where,? ?when,? and
?how.? The predicate of a clause (typically a verb) establishes ?what? took place,
and other sentence constituents express the participants in the event (such as ?who? and
?where?), as well as further event properties (such as ?when? and ?how?). The primary
task of semantic role labeling (SRL) is to indicate exactly what semantic relations hold
among a predicate and its associated participants and properties, with these relations
? Departament de Llenguatges i Sistemes Informa`tics, Universitat Polite`cnica de Catalunya, Jordi Girona
Salgado 1?3, 08034 Barcelona, Spain. E-mail: lluism@lsi.upc.edu.
?? Computer Science and Artificial Intelligence Laboratory (CSAIL), MIT, 32 Vassar St., Cambridge, MA
02139, USA. E-mail: carreras@csail.mit.edu.
? CL Research, 9208 Gue Road, Damascus, MD 20872 USA. E-mail: ken@clres.com.
? Department of Computer Science, 6 King?s College Road, Toronto, ON M5S 3G4, Canada.
E-mail: suzanne@cs.toronto.edu.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 2
drawn from a pre-specified list of possible semantic roles for that predicate (or class of
predicates). In order to accomplish this, the role-bearing constituents in a clause must
be identified and their correct semantic role labels assigned, as in:
[The girl on the swing]Agent [whispered]Pred to [the boy beside her]Recipient
Typical roles used in SRL are labels such as Agent, Patient, and Location for the entities
participating in an event, and Temporal and Manner for the characterization of other
aspects of the event or participant relations. This type of role labeling thus yields a first-
level semantic representation of the text that indicates the basic event properties and
relations among relevant entities that are expressed in the sentence.
Research has proceeded for decades on manually created lexicons, grammars, and
other semantic resources (Hirst 1987; Pustejovsky 1995; Copestake and Flickinger 2000)
in support of deep semantic analysis of language input, but such approaches have been
labor-intensive and often restricted to narrow domains. The 1990s saw a growth in
the development of statistical machine learning methods across the field of computa-
tional linguistics, enabling systems to learn complex linguistic knowledge rather than
requiring manual encoding. These methods were shown to be effective in acquiring
knowledge necessary for semantic interpretation, such as the properties of predicates
and the relations to their arguments?for example, learning subcategorization frames
(Briscoe and Carroll 1997) or classifying verbs according to argument structure prop-
erties (Merlo and Stevenson 2001; Schulte im Walde 2006). Recently, medium-to-large
corpora have been manually annotated with semantic roles in FrameNet (Fillmore,
Ruppenhofer, and Baker 2004), PropBank (Palmer, Gildea, and Kingsbury 2005), and
NomBank (Meyers et al 2004), enabling the development of statistical approaches
specifically for SRL.
With the advent of supporting resources, SRL has become a well-defined task with
a substantial body of work and comparative evaluation (see, among others, Gildea and
Jurafsky [2002], Surdeanu et al [2003], Xue and Palmer [2004], Pradhan et al [2005a],
the CoNLL Shared Task in 2004 and 2005, and Senseval-3 and SemEval-2007). The
identification of event frames may potentially benefit many natural language processing
(NLP) applications, such as information extraction (Surdeanu et al 2003), question
answering (Narayanan and Harabagiu 2004), summarization (Melli et al 2005), and
machine translation (Boas 2002). Related work on classifying the semantic relations in
noun phrases has also been encouraging for NLP tasks (Moldovan et al 2004; Rosario
and Hearst 2004).
Although the use of SRL systems in real-world applications has thus far been
limited, the outlook is promising for extending this type of analysis to many appli-
cations requiring some level of semantic interpretation. SRL represents an excellent
framework with which to perform research on computational techniques for acquiring
and exploiting semantic relations among the different components of a text.
This special issue of Computational Linguistics presents several articles represent-
ing the state-of-the-art in SRL, and this overview is intended to provide a broader
context for that work. First, we briefly discuss some of the linguistic views on se-
mantic roles that have had the most influence on computational approaches to SRL
and related NLP tasks. Next, we show how the linguistic notions have influenced
the development of resources that support SRL. We then provide an overview of
SRL methods and describe the state-of-the-art as well as current open problems in the
field.
146
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
2. Semantic Roles in Linguistics
Since the foundational work of Fillmore (1968), considerable linguistic research has been
devoted to the nature of semantic roles. Although there is substantial agreement on
major semantic roles, such as Agent and Theme, there is no consensus on a definitive
list of semantic roles, or even whether such a list exists. Proposed lists range from a
large set of situation-specific roles, such as Suspect, Authorities, and Offense (Fillmore,
Ruppenhofer, and Baker 2004), to a relatively small set of general roles, such as Agent,
Theme, Location, and Goal (typically referred to as thematic roles, as in Jackendoff
[1990]), to the set of two core roles, Proto-Agent and Proto-Theme, whose entailments
determine the precise relation expressed (Dowty 1991). This uncertainty within linguis-
tic theory carries over into computational work on SRL, where there is much variability
on the roles assumed in different resources.
A major focus of work in the linguistics community is on the mapping between the
predicate?argument structure that determines the roles, and the syntactic realization of
the recipients of those roles (Grimshaw 1990; Levin 1993; Levin and Rappaport Hovav
2005). Semantic role lists are generally viewed as inadequate for explaining the mor-
phosyntactic behavior of argument expression, with argument realization dependent
on a deeper lexical semantic representation of the components of the event that the
predicate describes. Although much of the mapping from argument structure to syntax
is predictable, this mapping is not completely regular, nor entirely understood. An
important question for SRL, therefore, is the extent to which performance is degraded
by the irregularities noted in linguistic studies of semantic roles.
Nonetheless, sufficient regularity exists to provide the foundation for meaningful
generalizations. Much research has focused on explaining the varied expression of verb
arguments within syntactic positions (Levin 1993). A major conclusion of that work is
that the patterns of syntactic alternation exhibit regularity that reflects an underlying
semantic similarity among verbs, forming the basis for verb classes. Such classes, and
the argument structure specifications for them, have proven useful in a number of NLP
tasks (Habash, Dorr, and Traum 2003; Shi and Mihalcea 2005), including SRL (Swier and
Stevenson 2004), and have provided the foundation for the computational verb lexicon
VerbNet (Kipper, Dang, and Palmer 2000).
This approach to argument realization focuses on the relation of morphosyntactic
behavior to argument semantics, and typically leads to a general conceptualization of
semantic roles. In frame semantics (Fillmore 1976), on the other hand, a word activates
a frame of semantic knowledge that relates linguistic semantics to encyclopedic knowl-
edge. This effort has tended to focus on the delineation of situation-specific frames (e.g.,
an Arrest frame) and correspondingly more specific semantic roles (e.g., Suspect and
Authorities) that codify the conceptual structure associated with lexical items (Fillmore,
Ruppenhofer, and Baker 2004). With a recognition that many lexical items could activate
any such frame, this approach leads to lexical classes of a somewhat different nature
than those of Levin (1993). Whereas lexical items in a Levin class are syntactically
homogeneous and share coarse semantic properties, items in a frame may syntactically
vary somewhat but share fine-grained, real-world semantic properties.
A further difference in these perspectives is the view of the roles themselves. In
defining verb classes that capture argument structure similarities, Levin (1993) does not
explicitly draw on the notion of semantic role, instead basing the classes on behavior
that is hypothesized to reflect the properties of those roles. Other work also eschews
the notion of a simple list of roles, instead postulating underlying semantic structure
that captures the relevant properties (Levin and Rappaport Hovav 1998). Interestingly,
147
Computational Linguistics Volume 34, Number 2
as described in Fillmore, Ruppenhofer, and Baker (2004), frame semantics also avoids a
predefined list of roles, but for different reasons. The set of semantic roles, called frame
elements, are chosen for each frame, rather than being selected from a predefined list
that may not capture the relevant distinctions in that particular situation. Clearly, to
the extent that disagreement persists on semantic role lists and the nature of the roles
themselves, SRL may be working on a shifting target.
These approaches also differ in the broad characterization of event participants
(and their roles) as more or less essential to the predicate. In the more syntactic-oriented
approaches, roles are typically divided into two categories: arguments, which cap-
ture a core relation, and adjuncts, which are less central. In frame semantics, the roles
are divided into core frame elements (e.g., Suspect, Authorities, Offense) and periph-
eral or extra-thematic elements (e.g., Manner, Time, Place). These distinctions carry
over into SRL, where we see that systems generally perform better on the more central
arguments.
Finally, although predicates are typically expressed as verbs, and thus much work
in both linguistics and SRL focuses on them, some nouns and adjectives may be used
predicatively, assigning their own roles to entities (as in the adjective phrase proud that
we finished the paper, where the subordinate clause is a Theme argument of the adjective
proud). Frame semantics tends to include in a frame relevant non-verb lexical items,
due to the emphasis on a common situation semantics. In contrast, the morphosyntactic
approaches have focused on defining classes of verbs only, because they depend on
common syntactic behavior that may not be apparent across syntactic categories.
Interestingly, prepositions have a somewhat dual status with regard to role labeling.
In languages like English, prepositions serve an important function in signaling the rela-
tion of a participant to a verb. For example, it is widely accepted that to in give the book to
Mary serves as a grammatical indicator of the Recipient role assigned by the verb, rather
than as a role assigner itself. In other situations, however, a preposition can be viewed
as a role-assigning predicate in its own right. Although some work in computational
linguistics is tackling the issue of the appropriate characterization of prepositions and
their contribution to semantic role assignment (as we see subsequently), much work
remains in order to fully integrate linguistic theories of prepositional function and
semantics into SRL.
3. From Linguistic Theory to Computational Resources
The linguistic approaches to semantic roles discussed previously have greatly influ-
enced current work on SRL, leading to the creation of significant computational lexicons
capturing the foundational properties of predicate?argument relations.
In the FrameNet project (Fillmore, Ruppenhofer, and Baker 2004), lexicographers
define a frame to capture some semantic situation (e.g., Arrest), identify lexical items
as belonging to the frame (e.g., apprehend and bust), and devise appropriate roles for
the frame (e.g., Suspect, Authorities, Offense). They then select and annotate example
sentences from the British National Corpus and other sources to illustrate the range of
possible assignments of roles to sentence constituents for each lexical item (at present,
over 141,000 sentences have been annotated).
FrameNet thus consists of both a computational lexicon and a role-annotated cor-
pus. The existence of such a corpus enabled Gildea and Jurafsky (2002) to develop the
first statistical machine learning approach to SRL, using various lexical and syntactic
features such as phrase type and grammatical function calculated over the annotated
constituents. Although this research spurred the current wave of SRL work that has
148
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
refined and extended Gildea and Jurafsky?s approach, the FrameNet data has not been
used extensively. One issue is that the corpus is not a representative sample of the
language, but rather consists of sentences chosen manually to illustrate the possible
role assignments for a given lexical item. Another issue is that the semantic roles are
situation-specific, rather than general roles like Agent, Theme, and Location that can be
used across many situations and genres.
The computational verb lexicon, VerbNet (Kipper, Dang, and Palmer 2000), instead
builds on Levin?s (1993) work on defining verb classes according to shared argument re-
alization patterns. VerbNet regularizes and extends the original Levin classes; moreover,
each class is explicitly associated with argument realization specifications that state the
constituents that a verb can occur with and the role assigned to each. The roles are
mostly drawn from a small set (around 25) of general roles widely used in linguistic
theory. This lexicon has been an important resource in computational linguistics, but
because of the lack of an associated role-annotated corpus, it has only been used directly
in SRL in an unsupervised setting (Swier and Stevenson 2004).
Research on VerbNet inspired the development of the Proposition Bank (PropBank;
Palmer, Gildea, and Kingsbury 2005), which has emerged as a primary resource for
research in SRL (and used in four of the articles in this special issue). PropBank ad-
dresses some of the issues for SRL posed by the FrameNet data. First, the PropBank
project has annotated the semantic roles for all verbs in the Penn Treebank corpus (the
Wall Street Journal [WSJ] news corpus). This provides a representative sample of text
with role-annotations, in contrast to FrameNet?s reliance on manually selected, illus-
trative sentences. Importantly, PropBank?s composition allows for consideration of the
statistical patterns across natural text. Although there is some concern about the limited
genre of its newspaper text, this aspect has the advantage of allowing SRL systems to
benefit from the state-of-the-art syntactic parsers and other resources developed with
the WSJ TreeBank data. Moreover, current work is extending the PropBank annotation
to balanced corpora such as the Brown corpus.
The lexical information associated with verbs in PropBank also differs significantly
from the situation-specific roles of FrameNet. At the same time, the PropBank designers
recognize the difficulty of providing a small, predefined list of semantic roles that is suf-
ficient for all verbs and predicate?argument relations, as in VerbNet. PropBank instead
takes a ?theory-neutral? approach to the designation of core semantic roles. Each verb
has a frameset listing its allowed role labelings in which the arguments are designated
by number (starting from 0). Each numbered argument is provided with an English-
language description specific to that verb. Participants typically considered as adjuncts
are given named argument roles, because there is more general agreement on such
modifiers as Temporal or Manner applying consistently across verbs. Different senses
for a polysemous verb have different framesets; however, syntactic alternations which
preserve meaning (as identified in Levin [1993]) are considered to be a single frameset.
While the designations of Arg0 and Arg1 are intended to indicate the general roles of
Agent and Theme/Patient across verbs, other argument numbers do not consistently
correspond to general (non-verb-specific) semantic roles.
Given the variability in the sets of roles used across the computational resources,
an important issue is the extent to which different role sets affect the SRL task, as well
as subsequent use of the output in other NLP applications. Gildea and Jurafsky (2002)
initiated this type of investigation by exploring whether their results were dependent
on the set of semantic roles they used. To this end, they mapped the FrameNet frame
elements into a set of abstract thematic roles (i.e., more general roles such as Agent,
Theme, Location), and concluded that their system could use these thematic roles
149
Computational Linguistics Volume 34, Number 2
without degradation. Similar questions must be investigated in the context of PropBank,
where the framesets for the verbs may have significant domain-specific meanings and
arguments due to the dependence of the project on WSJ data. Given the uncertainty in
the linguistic status of semantic role lists, and the lack of evidence about which types
of roles would be most useful in various NLP tasks, an important ongoing focus of
attention is the value of mapping between the role sets of the different resources (Swier
and Stevenson 2005; Loper, Yi, and Palmer 2007; Yi, Loper, and Palmer 2007).
We noted previously the somewhat special part that prepositions play in marking
semantic relations, in some sense mediating the role assignment of a verb to an argu-
ment. The resources noted earlier differ in their treatment of prepositions. In VerbNet,
for example, prepositions are listed explicitly as part of the syntactic context in which
a role is assigned (e.g., Agent V Prep(for) Recipient), but it is the NP object of the prep-
osition that receives the semantic role. In FrameNet and PropBank, on the other hand,
the full prepositional phrase is considered as the frame element (the constituent re-
ceiving the role). Clearly, further work needs to proceed on how to best capture the in-
teraction between verbs and prepositions in SRL. This is especially complex given
the high polysemy of prepositions, and work has proceeded on relating preposition
disambiguation to role assignment (e.g., O?Hara and Wiebe 2003). For such approaches
to make meaningful progress, resources are needed that elaborate the senses of prepo-
sitions and relate those senses to semantic roles. In The Preposition Project (TPP;
Litkowski and Hargraves 2005), a comprehensive, hierarchical characterization of the
semantic roles for all preposition senses in English is being developed. TPP has sense-
tagged more than 25,000 preposition instances in FrameNet sentences, allowing for
comprehensive investigation of the linking between preposition sense and semantic role
assignment.
4. Approaches to Automatic SRL
The work on SRL has included a broad spectrum of probabilistic and machine-learning
approaches to the task. We focus here on supervised systems, because most SRL research
takes an approach requiring training on role-annotated data. We briefly survey the main
approaches to automatic SRL, and the types of learning features used.
4.1 SRL Step by Step
Given a sentence and a designated verb, the SRL task consists of identifying the bound-
aries of the arguments of the verb predicate (argument identification) and labeling
them with semantic roles (argument classification). The most common architecture for
automatic SRL consists of the following steps to achieve these subtasks.
The first step in SRL typically consists of filtering (or pruning) the set of argu-
ment candidates for a given predicate. Because arguments may be a continuous or
discontinuous sequence of words, any subsequence of words in the sentence is an
argument candidate. Exhaustive exploration of this space of candidates is not feasible,
because it is both very large and imbalanced (i.e., the vast majority of candidates are
not actual arguments of the verb). The simple heuristic rules of Xue and Palmer (2004)
are commonly used to perform filtering because they greatly reduce the set of candidate
arguments, while maintaining a very high recall.
The second step consists of a local scoring of argument candidates by means of
a function that outputs probabilities (or confidence scores) for each of the possible
150
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
role labels, plus an extra ?no-argument? label indicating that the candidate should
not be considered an argument in the solution. In this step, candidates are usually
treated independently of each other. A crucial aspect in local scoring (see Section 4.2)
is the representation of candidates with features, rather than the particular choice of
classification algorithm.
Argument identification and classification may be treated jointly or separately in
the local scoring step. In the latter case, a pipeline of two subprocesses is typically
applied, first scoring between ?argument? and ?no-argument? labels, and then scoring
the particular argument labels. Because argument identification is closely related to
syntax and argument classification is more a semantic issue, useful features for the two
subtasks may be very different?that is, a good feature for addressing recognition may
hurt classification and vice versa (Pradhan et al 2005a).
The third step in SRL is to apply a joint scoring (or global scoring) in order to
combine the predictions of local scorers to produce a good structure of labeled argu-
ments for the predicate. In this step, dependencies among several arguments of the same
predicate can be exploited. For instance, Punyakanok, Roth, and Yih (this issue) ensure
that a labeling satisfies a set of structural and SRL-dependent constraints (arguments
do not overlap, core arguments do not repeat, etc.). Also in this issue, Toutanova,
Haghighi, and Manning apply re-ranking to select the best among a set of candidate
complete solutions produced by a base SRL system. Finally, probabilistic models have
also been applied to produce the structured output, for example, generative models
(Thompson, Levy, and Manning 2003), sequence tagging with classifiers (Ma`rquez et al
2005; Pradhan et al 2005b), and Conditional Random Fields on tree structures (Cohn
and Blunsom 2005). These approaches at a global level may demand considerable extra
computation, but current optimization techniques help solve them quite efficiently.
Some variations in the three-step architecture are found. Systems may bypass one
of the steps, by doing only local scoring, or skipping directly to joint scoring. A fourth
step may consist of fixing common errors or enforcing coherence in the final solution.
This postprocess usually consists of a set of hand-developed heuristic rules that are
dependent on a particular architecture and corpus of application.
An important consideration within this general SRL architecture is the combination
of systems and input annotations. Most SRL systems include some kind of combi-
nation to increase robustness, gain coverage, and reduce effects of parse errors. One
may combine: (1) the output of several independent SRL basic systems (Surdeanu
et al 2007; Pradhan et al 2005b), or (2) several outputs from the same SRL system
obtained by changing input annotations or other internal parameters (Koomen et al
2005; Toutanova, Haghighi, and Manning 2005). The combination can be as simple as
selecting the best among the set of complete candidate solutions, but usually consists of
combining fragments of alternative solutions to construct the final output. Finally, the
combination component may involve machine learning or not. The gain in performance
from the combination step is consistently between two and three F1 points. However, a
combination approach increases system complexity and penalizes efficiency.
Several exceptions to this described architecture for SRL can be found in the lit-
erature. One approach entails joint labeling of all predicates of the sentence, instead
of proceeding one by one. This opens the possibility of exploiting dependencies among
the different verbs in the sentence. However, the complexity may grow significantly, and
results so far are inconclusive (Carreras, Ma`rquez, and Chrupa?a 2004; Surdeanu et al
2007). Other promising approaches draw on dependency parsing rather than traditional
phrase structure parsing (Johansson and Nugues 2007), or combine parsing and SRL
into a single step of semantic parsing (Musillo and Merlo 2006).
151
Computational Linguistics Volume 34, Number 2
4.2 Feature Engineering
As previously noted, devising the features with which to encode candidate arguments
is crucial for obtaining good results in the SRL task. Given a verb and a candidate argu-
ment (a syntactic phrase) to be classified in the local scoring step, three types of features
are typically used: (1) features that characterize the candidate argument and its context;
(2) features that characterize the verb predicate and its context; and (3) features that cap-
ture the relation (either syntactic or semantic) between the candidate and the predicate.
Gildea and Jurafsky (2002) presented a compact set of features across these three
types, which has served as the core of most of the subsequent SRL work: (1) the phrase
type, headword, and governing category of the constituent; (2) the lemma, voice, and
subcategorization pattern of the verb; and (3) the left/right position of the constituent
with respect to the verb, and the category path between them. Extensions to these fea-
tures have been proposed in various directions. Exploiting the ability of some machine
learning algorithms to work with very large feature spaces, some authors have largely
extended the representation of the constituent and its context, including among others:
first and last words (and part-of-speech) in the constituent, bag-of-words, n-grams of
part of speech, and sequence of top syntactic elements in the constituent. Parent and
sibling constituents in the tree may also be codified with all the previous structural and
lexical features (Pradhan et al 2005a; Surdeanu et al 2007). Other authors have designed
new features with specific linguistic motivations. For instance, Surdeanu et al (2003)
generalized the concept of headword with the content word feature. They also used
named entity labels as features. Xue and Palmer (2004) presented the syntactic frame
feature, which captures the overall sentence structure using the verb predicate and the
constituent as pivots. All these features resulted in a significant increase in performance.
Finally, regarding the relation between the constituent and the predicate, several
variants of Gildea and Jurafsky?s syntactic path have been proposed in the literature
(e.g., generalizations to avoid sparsity, and adaptations to partial parsing). Also, some
attempts have been made at characterizing the semantic relation between the predicate
and the constituent. In Zapirain, Agirre, and Ma`rquez (2007) and Erk (2007), selectional
preferences between predicate and headword of the constituent are explored to generate
semantic compatibility features. Using conjunctions of several of the basic features is
also common practice. This may be very relevant when the machine learning method
used is linear in the space of features.
Joint scoring and combination components open the door to richer types of fea-
tures, which may take into account global properties of the candidate solution plus de-
pendencies among the different arguments. The most remarkable work in this direction
is the reranking approach by Toutanova, Haghighi, and Manning in this issue. When
training the ranker to select the best candidate solution they codify pattern features as
strings containing the whole argument structure of the candidate. Several variations
of this type of feature (with different degrees of generalization to avoid sparseness)
allow them to significantly increase the performance of the base system. Also related,
Pradhan et al (2005b) and Surdeanu et al (2007) convert the confidence scores of several
base SRL systems into features for training a final machine learning?based combination
system. Surdeanu et al (2007) develop a broad spectrum of features, with sentence-
based information, describing the role played by the candidate argument in every
solution proposed by the different base SRL systems.
A completely different approach to feature engineering is the use of kernel meth-
ods to implicitly exploit all kinds of substructures in the syntactic representation of
the candidates. This knowledge poor approach intends to take advantage of a massive
152
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
quantity of features without the need for manual engineering of specialized features.
This motivation might be relevant for fast system development and porting, especially
when specialized linguistic knowledge of the language of application is not available.
The most studied approach consists of using some variants of the ?all subtrees kernel?
applied to the sentence parse trees. The work by Moschitti, Pighin, and Basili in this
issue is the main representative of this family.
5. Empirical Evaluations of SRL Systems
Many experimental studies have been conducted since the work of Gildea and Jurafsky
(2002), including seven international evaluation tasks in ACL-related conferences and
workshops: the SIGNLL CoNLL shared tasks in 2004 and 2005 (Carreras and Ma`rquez
2004, 2005), the SIGLEX Senseval-3 in 2004 (Litkowski 2004), and four tasks in the
SIGLEX SemEval in 2007 (Pradhan et al 2007; Ma`rquez et al 2007; Baker, Ellsworth, and
Erk 2007; Litkowski and Hargraves 2007). In the subsequent sections, we summarize
their main features, results, and conclusions, although note that the scores are not
directly comparable across different exercises, due to differences in scoring and in the
experimental methodologies.
5.1 Task Definition and Evaluation Metrics
The standard experiment in automatic SRL can be defined as follows: Given a sentence
and a target predicate appearing in it, find the arguments of the predicate and label
them with semantic roles. A system is evaluated in terms of precision, recall, and F1 of
the labeled arguments. In evaluating a system, an argument is considered correct when
both its boundaries and the semantic role label match a gold standard. Performance
can be divided into two components: (1) the precision, recall, and F1 of unlabeled
arguments, measuring the accuracy of the system at segmenting the sentence; and (2)
the classification accuracy of assigning semantic roles to the arguments that have been
correctly identified. In calculating the metrics, the de facto standard is to give credit only
when a proposed argument perfectly matches an argument in the reference solution;
nonetheless, variants that give some credit for partial matching also exist.
5.2 Shared Task Experiments Using FrameNet, PropBank, and VerbNet
To date, most experimental work has made use of English data annotated either with
PropBank or FrameNet semantic roles.
The CoNLL shared tasks in 2004 and 2005 were based on PropBank (Carreras and
Ma`rquez 2004, 2005), which is the largest evaluation benchmark available today, and
also the most used by researchers?all articles in this special issue dealing with English
use this benchmark. In the evaluation, the best systems obtained an F1 score of ?80%,
and have achieved only minimal improvements since then. The articles in this issue by
Punyakanok, Roth, and Yih; Toutanova, Haghighi, and Manning; and Pradhan, Ward,
and Martin describe such efforts. An analysis of the outputs in CoNLL-2005 showed
that argument identification accounts for most of the errors: a system will recall ?81%
of the correct unlabeled arguments, and ?95% of those will be assigned the correct
semantic role. The analysis also showed that systems recognized core arguments better
than adjuncts (with F1 scores from the high 60s to the high 80s for the former, but below
60% for the latter). Finally, it was also observed that, although systems performed better
153
Computational Linguistics Volume 34, Number 2
on verbs appearing frequently in training, the best systems could recognize arguments
of unseen verbs with an F1 in the low 70s, not far from the overall performance.
1
SemEval-2007 included a task on semantic evaluation for English, combining word
sense disambiguation and SRL based on PropBank (Pradhan et al 2007). Unlike the
CoNLL tasks, this task concentrated on 50 selected verbs. Interestingly, the data was
annotated using verb-independent roles using the PropBank/VerbNet mapping from
Yi, Loper, and Palmer (2007). The two participating systems could predict VerbNet roles
as accurately as PropBank verb-dependent roles.
Experiments based on FrameNet usually concentrate on a selected list of frames.
In Senseval-3, 40 frames were selected for an SRL task with the goal of replicating
Gildea and Jurafsky (2002) and improving on them (Litkowski 2004). Participants were
evaluated on assigning semantic roles to given arguments, with best F1 of 92%, and on
the task of segmenting and labeling arguments, with best F1 of 83%.
SemEval-2007 also included an SRL task based on FrameNet (Baker, Ellsworth, and
Erk 2007). It was much more complete, realistic, and difficult than its predecessor in
Senseval-3. The goal was to perform complete analysis of semantic roles on unseen
texts, first determining the appropriate frames of predicates, and then determining their
arguments labeled with semantic roles. It also involved creating a graph of the sentence
representing part of its semantics, by means of frames and labeled arguments. The
test data of this task consisted of novel manually-annotated documents, containing a
number of frames and roles not in the FrameNet lexicon. Three teams submitted results,
with precision percentages in the 60s, but recall percentages only in the 30s.
To our knowledge, there is no evidence to date on the relative difficulty of assigning
FrameNet or PropBank roles.
5.3 Impact of Syntactic Processing in SRL
Semantic roles are closely related to syntax, and, therefore, automatic SRL heavily relies
on the syntactic structure of the sentence. In PropBank, over 95% of the arguments
match with a single constituent of the parse tree. If the output produced by a statistical
parser is used (e.g., Collins?s or Charniak?s) the exact matching is still over 90%. More-
over, some simple rules can be used to join constituents and fix a considerable portion
of the mismatches (Toutanova, Haghighi, and Manning 2005). Thus, it has become a
common practice to use full parse trees as the main source for solving SRL.
The joint model presented in this issue by Toutanova, Haghighi, and Manning
obtains an F1 at ?90% on the WSJ test of the CoNLL-2005 evaluation when using gold-
standard trees; but with automatic syntactic analysis, its best result falls to ?80%. This
and other work consistently show that the drop in performance occurs in identifying
argument boundaries; when arguments are identified correctly with predicted parses,
the accuracy of assigning semantic roles is similar to that with correct parses.
A relevant question that has been addressed in experimental work concerns the
use of a partial parser instead of a parser that produces full WSJ trees. In the CoNLL-
2004 task, systems were restricted to the use of base syntactic phrases (i.e., chunks)
and clauses, and the best results that could be obtained were just below 70%. But the
training set in that evaluation was about five times smaller than that of the 2005 task.
Punyakanok, Roth, and Yih (this issue) and Surdeanu et al (2007) have shown that, in
1 The analysis summarized here was presented in the oral session at CoNLL-2005. The slides of the session,
containing the results supporting this analysis, are available in the CoNLL-2005 shared task Web site.
154
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
fact, a system working with partial parsing can do almost as well as a system working
with full parses, with differences in F1 of only ?2?3 points.
Currently, the top-performing systems on the CoNLL data make use of several
outputs of syntactic parsers, as discussed in Section 4. It is clear that many errors in
SRL are caused by having incorrect syntactic constituents, as reported by Punyakanok,
Roth, and Yih in this issue. By using many parses, the recognition of semantic roles is
more robust to parsing errors. Yet, it remains unanswered what is the most appropriate
level of syntactic analysis needed in SRL.
5.4 Generalization of SRL Systems to New Domains
Porting a system to a new domain, different than the domain used to develop and train
the system, is a challenging question in NLP. SRL is no exception, with the particular
difficulty that a predicate in a new domain may exhibit a behavior not contemplated
in the dictionary of frames at training time. This difficulty was identified as a major
challenge in the FrameNet-based task in SemEval-2007 (Baker, Ellsworth, and Erk 2007).
In the CoNLL-2005 task, WSJ-trained systems were tested on three sections of
the Brown corpus annotated by the PropBank team. The performance of all systems
dropped dramatically: The best systems scored F1 below 70%, as opposed to figures at
?80% when testing on WSJ data. This is perhaps not surprising, taking into account that
the pre-processing systems involved in the analysis (tagger and parser) also experienced
a significant drop in performance. The article in this issue by Pradhan, Ward, and
Martin further investigates the robustness across text genres when porting a system
from WSJ to Brown. Importantly, the authors claim that the loss in accuracy takes place
in assigning the semantic roles, rather than in the identification of argument boundaries.
5.5 SRL on Languages Other Than English
SemEval-2007 featured the first evaluation exercise of SRL systems for languages other
than English, namely for Spanish and Catalan (Ma`rquez et al 2007). The data was part
of the CESS-ECE corpus, consisting of ?100K tokens for each language. The semantic
role annotations are similar to PropBank, in that role labels are specific to each verb,
but also include a verb-independent thematic role label similar to the scheme proposed
in VerbNet. The task consisted of assigning semantic class labels to target verbs, and
identifying and labeling arguments of such verbs, in both cases using gold-standard
syntax. Only two teams participated, with best results at ?86% for disambiguating
predicates, and at ?83% for labeling arguments.
The work by Xue in this issue studies semantic role labeling for Chinese, using the
Chinese PropBank and NomBank corpora. Apart from working also with nominalized
predicates, this work constitutes the first comprehensive study on SRL for a language
different from English.
5.6 SRL with Other Parts-of-Speech
The SemEval-2007 task on disambiguating prepositions (Litkowski and Hargraves 2007)
used FrameNet sentences as the training and test data, with over 25,000 sentences for
the 34 most common English prepositions. Although not overtly defined as semantic
role labeling, each instance was characterized with a semantic role name and also had
an associated FrameNet frame element. Almost 80% of the prepositional phrases in the
instances were identified as core frame elements, and are likely to be closely associated
155
Computational Linguistics Volume 34, Number 2
with arguments of the words to which they are attached. The three participants used a
variety of methods, with the top performing team using machine learning techniques
similar to those in other semantic role labeling tasks.
6. Final Remarks
To date, SRL systems have been shown to perform reasonably well in some controlled
experiments, with F1 measures in the low 80s on standard test collections for English.
Still, a number of important challenges exist for future research on SRL. It remains
unclear what is the appropriate level of syntax needed to support robust analysis of
semantic roles, and to what degree improved performance in SRL is constrained by the
state-of-the-art in tagging and parsing. Beyond syntax, the relation of semantic roles to
other semantic knowledge (such as WordNet, named entities, or even a catalogue of
frames) has scarcely been addressed in the design of current SRL models. A deeper
understanding of these questions could help in developing methods that yield im-
proved generalization, and that are less dependent on large quantities of role-annotated
training data.
Indeed, the requirement of most SRL approaches for such training data, which is
both difficult and highly expensive to produce, is the major obstacle to the widespread
application of SRL across different genres and different languages. Given the degrada-
tion of performance when a supervised system is faced with unseen events or a testing
corpus different from training, this is a major impediment to increasing the application
of SRL even within English, a language for which two major annotated corpora are
available. It is critical for the future of SRL that research broadens to include wider
investigation of unsupervised and minimally supervised learning methods.
In addition to these open research problems, there are also methodological issues
that need to be addressed regarding how research is conducted and evaluated. Shared
task frameworks have been crucial in SRL development by supporting explicit compar-
isons of approaches, but such benchmark testing can also overly focus research efforts
on small improvements in particular evaluation measures. Improving the entire SRL
approach in a significant way may require more open-ended investigation and more
qualitative analysis.
Acknowledgments
We are grateful for the insightful comments
of two anonymous reviewers whose input
helped us to improve the article. This work
was supported by the Spanish Ministry of
Education and Science (Ma`rquez); the
Catalan Ministry of Innovation, Universities
and Enterprise; and a grant from NTT, Agmt.
Dtd. 6/21/1998 (Carreras); and NSERC of
Canada (Stevenson).
References
Baker, C., M. Ellsworth, and K. Erk. 2007.
SemEval-2007 Task 19: Frame semantic
structure extraction. In Proceedings of the
4th International Workshop on Semantic
Evaluations (SemEval-2007), pages 99?104,
Prague, Czech Republic.
Boas, H. C. 2002. Bilingual framenet
dictionaries for machine translation.
In Proceedings of the Third International
Conference on Language Resources and
Evaluation (LREC), pages 1364?1371,
Las Palmas de Gran Canaria, Spain.
Briscoe, T. and J. Carroll. 1997. Automatic
extraction of subcategorization from
corpora. In Proceedings of the 5th ACL
Conference on Applied Natural Language
Processing (ANLP), pages 356?363,
Washington, DC.
Carreras, X. and L. Ma`rquez. 2004.
Introduction to the CoNLL-2004 Shared
Task: Semantic role labeling. In Proceedings
of the Eighth Conference on Computational
Natural Language Learning (CoNLL-2004),
pages 89?97, Boston, MA.
Carreras, X. and L. Ma`rquez. 2005.
Introduction to the CoNLL-2005 Shared
156
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
Task: Semantic role labeling. In Proceedings
of the Ninth Conference on Computational
Natural Language Learning (CoNLL-2005),
pages 152?164, Ann Arbor, MI.
Carreras, X., L. Ma`rquez, and G. Chrupa?a.
2004. Hierarchical recognition of
propositional arguments with perceptrons.
In Proceedings of the Eighth Conference on
Computational Natural Language Learning
(CoNLL-2004), pages 106?109, Boston, MA.
Cohn, T. and P. Blunsom. 2005. Semantic role
labelling with tree conditional random
fields. In Proceedings of the Ninth Conference
on Computational Natural Language
Learning (CoNLL-2005), pages 169?172,
Ann Arbor, MI.
Copestake, A. and D. Flickinger. 2000. An
open-source grammar development
environment and broad-coverage English
grammar using HPSG. In Proceedings
of the Second International Conference on
Language Resources and Evaluation (LREC),
pages 591?600, Athens, Greece.
Dowty, D. 1991. Thematic proto-roles and
argument selection. Language, 67:547?619.
Erk, K. 2007. A simple, similarity-based
model for selectional preferences. In
Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics,
pages 216?223, Prague, Czech Republic.
Fillmore, C. 1968. The case for case. In
E. Bach and R. T. Harms, editors, Universals
in Linguistic Theory. Holt, Rinehart &
Winston, New York, pages 1?88.
Fillmore, C. J. 1976. Frame semantics and the
nature of language. Annals of the New York
Academy of Sciences: Conference on the Origin
and Development of Language and Speech,
280:20?32.
Fillmore, C. J., J. Ruppenhofer, and C. F.
Baker. 2004. Framenet and representing
the link between semantic and syntactic
relations. In Churen Huang and Winfried
Lenders, editors, Frontiers in Linguistics,
volume I of Language and Linguistics
Monograph Series B. Institute of Linguistics,
Academia Sinica, Taipei, pages 19?59.
Gildea, D. and D. Jurafsky. 2002. Automatic
labeling of semantic roles. Computational
Linguistics, 28(3):245?288.
Grimshaw, J. 1990. Argument Structure. MIT
Press, Cambridge, MA.
Habash, N., B. J. Dorr, and D. Traum. 2003.
Hybrid natural language generation from
lexical conceptual structures. Machine
Translation, 18(2):81?128.
Hirst, G. 1987. Semantic Interpretation and
the Resolution of Ambiguity. Cambridge
University Press, Cambridge.
Jackendoff, R. 1990. Semantic Structures. MIT
Press, Cambridge, MA.
Johansson, R. and P. Nugues. 2007. LTH:
Semantic structure extraction using
nonprojective dependency trees. In
Proceedings of the 4th International
Workshop on Semantic Evaluations
(SemEval-2007), pages 227?230, Prague,
Czech Republic.
Kipper, K., H. T. Dang, and M. Palmer. 2000.
Class-based construction of a verb lexicon.
In Proceedings of the 17th National Conference
on Artificial Intelligence (AAAI-2000),
Austin, TX.
Koomen, P., V. Punyakanok, D. Roth, and
W. Yih. 2005. Generalized inference
with multiple semantic role labeling
systems. In Proceedings of the Ninth
Conference on Computational Natural
Language Learning (CoNLL-2005),
pages 181?184, Ann Arbor, MI.
Levin, B. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
The University of Chicago Press,
Chicago, IL.
Levin, B. and M. Rappaport Hovav. 1998.
Building verb meanings. In M. Butt and
W. Geuder, editors, The Projection of
Arguments: Lexical and Compositional
Factors. CSLI Publications, Stanford, CA,
pages 97?134.
Levin, B. and M. Rappaport Hovav. 2005.
Argument Realization. Cambridge
University Press, Cambridge.
Litkowski, K. C. 2004. Senseval-3 task:
Automatic labeling of semantic roles. In
Proceedings of the 3rd International Workshop
on the Evaluation of Systems for the Semantic
Analysis of Text (Senseval-3), pages 9?12,
Barcelona, Spain.
Litkowski, K. C. and O. Hargraves. 2005.
The preposition project. In Proceedings
of the ACL-SIGSEM Workshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistic
Formalisms and Applications, pages 171?179,
Colchester, UK.
Litkowski, K. C. and O. Hargraves. 2007.
SemEval-2007 Task 06: Word-sense
disambiguation of prepositions. In
Proceedings of the 4th International Workshop
on Semantic Evaluations (SemEval-2007),
pages 24?29, Prague, Czech Republic.
Loper, E., S. Yi, and M. Palmer. 2007.
Combining lexical resources: Mapping
between PropBank and VerbNet. In
Proceedings of the 7th International Workshop
on Computational Semantics, pages 118?128,
Tilburg, The Netherlands.
157
Computational Linguistics Volume 34, Number 2
Ma`rquez, L., P. R. Comas, J. Gime?nez, and
N. Catala`. 2005. Semantic role labeling
as sequential tagging. In Proceedings
of the Ninth Conference on Computational
Natural Language Learning (CoNLL-2005),
pages 193?196, Ann Arbor, MI.
Ma`rquez, L., L. Villarejo, M.A. Mart??, and
M. Taule?. 2007. SemEval-2007 Task 09:
Multilevel semantic annotation of Catalan
and Spanish. In Proceedings of the 4th
International Workshop on Semantic
Evaluations (SemEval-2007), pages 42?47,
Prague, Czech Republic.
Melli, G., Y. Wang, Y. Liu, M. M. Kashani,
Z. Shi, B. Gu, A. Sarkar, and F. Popowich.
2005. Description of SQUASH, the SFU
question answering summary handler
for the DUC-2005 Summarization Task.
In Proceedings of the HLT/EMNLP
Document Understanding Workshop (DUC),
Vancouver, Canada, available at
http://duc.nist.gov/pubs/2005papers/
simonfraseru.sarkar.pdf.
Merlo, P. and S. Stevenson. 2001. Automatic
verb classification based on statistical
distributions of argument structure.
Computational Linguistics, 27(3):373?408.
Meyers, A., R. Reeves, C. Macleod,
R. Szekely, V. Zielinska, B. Young, and
R. Grishman. 2004. The NomBank Project:
An interim report. In Proceedings of
the HLT-NAACL 2004 Workshop: Frontiers
in Corpus Annotation, pages 24?31,
Boston, MA.
Moldovan, D., A. Badulescu, M. Tatu,
D. Antohe, and R. Girju. 2004. Models for
the semantic classification of noun
phrases. In Proceedings of the HLT-NAACL
2004 Workshop on Computational Lexical
Semantics, pages 60?67, Boston, MA.
Musillo, G. and P. Merlo. 2006. Accurate
parsing of the proposition bank. In
Proceedings of the Human Language
Technology Conference of the NAACL,
pages 101?104, New York, NY.
Narayanan, S. and S. Harabagiu. 2004.
Question answering based on semantic
structures. In Proceedings of the 20th
International Conference on Computational
Linguistics (COLING), pages 693?701,
Geneva, Switzerland.
O?Hara, T. and J. Wiebe. 2003. Preposition
semantic classification via Penn Treebank
and FrameNet. In Proceedings of the
Seventh Conference on Computational
Natural Language Learning (CoNLL-2003),
pages 79?86, Edmonton, Canada.
Palmer, M., D. Gildea, and P. Kingsbury.
2005. The Proposition Bank: An annotated
corpus of semantic roles. Computational
Linguistics, 31(1):71?105.
Pradhan, S., K. Hacioglu, V. Krugler,
W. Ward, J. Martin, and D. Jurafsky. 2005a.
Support vector learning for semantic
argument classification. Machine Learning,
60(1):11?39.
Pradhan, S., K. Hacioglu, W. Ward, J. H.
Martin, and D. Jurafsky. 2005b. Semantic
role chunking combining complementary
syntactic views. In Proceedings of the
Ninth Conference on Computational
Natural Language Learning (CoNLL-2005),
pages 217?220, Ann Arbor, MI.
Pradhan, S., E. Loper, D. Dligach, and
M. Palmer. 2007. SemEval-2007 Task 17:
English lexical sample, SRL and all words.
In Proceedings of the 4th International
Workshop on Semantic Evaluations
(SemEval-2007), pages 87?92, Prague,
Czech Republic.
Pustejovsky, J. 1995. The Generative Lexicon.
MIT Press, Cambridge, MA.
Rosario, B. and M. Hearst. 2004. Classifying
semantic relations in bioscience text. In
Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics,
pages 430?437, Barcelona, Spain.
Schulte im Walde, S. 2006. Experiments
on the automatic induction of German
semantic verb classes. Computational
Linguistics, 32(2):159?194.
Shi, L. and R. Mihalcea. 2005. Putting pieces
together: Combining FrameNet, VerbNet
and WordNet for robust semantic parsing.
In Computational Linguistics and Intelligent
Text Processing; Sixth International
Conference, CICLing 2005, Proceedings,
LNCS, vol 3406, pages 100?111, Mexico
City, Mexico.
Surdeanu, M., S. Harabagiu, J. Williams,
and P. Aarseth. 2003. Using
predicate-argument structures for
information extraction. In Proceedings of the
41st Annual Meeting of the Association for
Computational Linguistics, pages 8?15,
Sapporo, Japan.
Surdeanu, M., L. Ma`rquez, X. Carreras, and
P. R. Comas. 2007. Combination strategies
for semantic role labeling. Journal of
Artificial Intelligence Research (JAIR),
29:105?151.
Swier, R. and S. Stevenson. 2004.
Unsupervised semantic role labelling. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP), pages 95?102, Barcelona, Spain.
Swier, R. and S. Stevenson. 2005. Exploiting
a verb lexicon in automatic semantic role
158
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
labelling. In Proceedings of the Human
Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing (HLT/EMLNP), pages
883?890, Vancouver, B.C., Canada.
Thompson, C. A., R. Levy, and C. Manning.
2003. A generative model for semantic role
labeling. In Proceedings of the 14th European
Conference on Machine Learning (ECML),
pages 397?408, Dubrovnik, Croatia.
Toutanova, K., A. Haghighi, and C. Manning.
2005. Joint learning improves semantic role
labeling. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 589?596, Ann Arbor, MI.
Xue, N. and M. Palmer. 2004. Calibrating
features for semantic role labeling. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP), pages 88?94, Barcelona, Spain.
Yi, S., E. Loper, and M. Palmer. 2007. Can
semantic roles generalize across corpora?
In Human Language Technologies 2007:
The Conference of the North American
Chapter of the Association for Computational
Linguistics; Proceedings of the Main
Conference, pages 548?555, Rochester, NY.
Zapirain, B., E. Agirre, and L. Ma`rquez. 2007.
UBC-UPC: Sequential SRL using
selectional preferences: an approach with
maximum entropy Markov models. In
Proceedings of the 4th International Workshop
on Semantic Evaluations (SemEval-2007),
pages 354?357, Prague, Czech Republic.
159

Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 13?16, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
CL Research?s Knowledge Management System
Kenneth C. Litkowski
CL Research
9208 Gue Road
Damascus, MD 20872
ken@clres.com
http://www.clres.com
Abstract
CL Research began experimenting with
massive XML tagging of texts to answer
questions in TREC 2002. In DUC 2003, the
experiments were extended into text
summarization. Based on these experiments,
The Knowledge Management System (KMS)
was developed to combine these two
capabilities and to serve as a unified basis for
other types of document exploration. KMS has
been extended to include web question
answering, both general and topic-based
summarization, information extraction, and
document exploration. The document
exploration functionality includes identification
of semantically similar concepts and dynamic
ontology creation. As development of KMS has
continued, user modeling has become a key
research issue: how will different users want to
use the information they identify.
1 Introduction
In participating the TREC question-answering track,
CL Research began by parsing full documents and
developing databases consisting of semantic relation
triples (Litkowski, 1999). The database approach
proved to be quite confining, with time requirements
expanding exponentially trying to maintain larger sets
of documents and increasingly complex procedures to
answer questions. A suggestion was made to tag text
with the type of questions they could answer (e.g.,
tagging time phrases as answering when questions
and person names as answering who questions). This
led to the general approach of analyzing parse trees to
construct an XML representation of texts (i.e.,
attaching metadata to the text) and examining these
representations with XPath expressions to answer
questions.
Litkowski (2003a) demonstrated the viability of
this approach by showing that XPath expressions
could be used to answer questions at a level above the
highest performing team. Many issues and problems
were identified: (1) The necessary level of analysis to
meet the needs of particular applications; (2) tagging
alternatives; and (3) the viability of the using the
XML representation for text summarization,
information extraction, novelty detection, and text
mining. Subsequent efforts showed that XML
representations could be effectively used in
summarization (Litkowski, 2003b) and novelty
detection (Litkowski, 2005).
Initially, CL Research developed an interface for
examining question-answering performance. This
interface has since evolved into a Knowledge
Management System (KMS) that provides a single
platform for examining English documents (e.g.,
newswire and research papers) and for generating
different types of output (e.g., answers to questions,
summaries, and document ontologies), also in XML
representations. In this demonstration, CL Research
will describe many parts of KMS, particularly the
approaches used for analyzing texts.
1
 The
demonstration will particularly focus on the value of
XML in providing a flexible and extensible
mechanism for implementing the various NLP
functionalities. In addition, the demonstration will
identify the emerging issue of user modeling to
determine exactly how knowledge will be used, since
1
Screen shots of KMS in performing the
functions as described below are can be seen at
http://www.clres.com/kmsscreen.html.
13
the primary purpose of KMS is to serve as a tool that
will enable users (such as scientists and intelligence
analysts) to accumulate and manage knowledge
(including facts, such as described in Fiszman et al,
2003) about topics of interest.
2
2 Parsing and Creation of XML Tagging
KMS and each of its application areas is based on
parsing text and then transforming parse trees into an
XML representation. CL Research uses the Proximity
Parser, developed by an inventor of top-down syntax-
directed parsing (Irons, 1961).
3
  The parser output
consists of bracketed parse trees, with leaf nodes
describing the part of speech and lexical entry for
each sentence word. Annotations, such as number and
tense information, may be included at any node.
(Litkowski (2002) and references therein provide
more details on the parser.)
After each sentence is parsed, its parse tree is
traversed in a depth-first recursive function. During
this traversal, each non-terminal and terminal node is
analyzed to identify discourse segments (sentences
and clauses), noun phrases, verbs, adjectives, and
prepositional phrases. These items are maintained in
lists; the growing lists constitute a document?s
discourse structure and are used, e.g., in resolving
anaphora and establishing coreferents (implementing
techniques inspired by Marcu (2000) and Tetreault
(2001)). As these items are identified, they are
subjected to a considerable amount of analysis to
characterize them syntactically and semantically. The
analysis includes word-sense disambiguation of
nouns, verbs (including subcategorization
identification), and adjectives and semantic analysis
of prepositions to establish their semantic roles (such
as described in Gildea & Jurafsky, 2002).
When all sentences of a document have been
parsed and components identified and analyzed, the
various lists are used to generate the XML
representation. Most of the properties of the
components are used as the basis for establishing
XML attributes and values in the final representation.
(Litkowski 2003a provides further details on this
process.) This representation then becomes the basis
for question answering, summarization, information
extraction, and document exploration.
The utility of the XML representation does not
stem from an ability to use XML manipulation
technologies, such as XSLT and XQuery. In fact,
these technologies seem to involve too much
overhead. Instead, the utility arises within a
Windows-based C++ development environment with
a set of XML functions that facilitate working with
node sets from a document?s XML tree.
3 Question Answering
As indicated above, the initial implementation of the
question-answering component of KMS was designed
primarily to determine if suitable XPath expressions
could be created for answering questions. CL
Research?s XML Analyzer was developed for this
purpose.
4
 XML Analyzer is constructed in a C++
Windows development environment to which a
capability for examining XML nodes has been added.
With this capability, a document can be loaded with
one instruction and an XPath expression can be
applied against this document in one more instruction
to obtain a set of nodes which can be examined in
more detail. Crucially, this enables low-level control
over subsequent analysis steps (e.g., examining the
text of a node with Perl regular expressions).
XML Analyzer first loads an XML file (which
can include many documents, such as the ?top 50?
used in TREC). The user then presents an XPath
expression and discourse components (typically, noun
phrases) satisfying that expression are returned. XML
Analyzer includes the document number, the sentence
number, and the full sentence for each noun phrase.
Several other features were added to XML Analyzer
to examine characteristics of the documents and
sentences (particularly to identify why an answer
2
The overall design of KMS is based on
requirements enunciated by intelligence analysts and
question-answering researchers  in a workshop on
Scenario-Based Question Answering sponsored by the
Advanced Research and Development Agency in 2003.
3
An online demonstration of the parser is
available at http://www.zzcad.com/parse.htm. A demo
version of the parser is available for download at
http://www.clres.com/demos.html.
4
A demo version of XML Analyzer is available
for download at http://www.clres.com/demos.html.
14
wasn?t retrieved by an XPath expression).
XML Analyzer does not include the automatic
creation of an XPath expression. KMS was created
for TREC 2003 as the initial implementation of a
complete question-answering system. In KMS, the
question itself is parsed and transformed into an
XML representation (using the same underlying
functionality for processing documents) and then used
to construct an XPath expression.
An XPath expression consists of two parts. The
first part is a ?passage retrieval? component, designed
to retrieve sentences likely to contain the answer. This
basic XPath is then extended for each question type
with additional specifications, e.g., to ask for noun
phrases that have time, location, or other semantic
attributes. Experiments have shown that there is a
tradeoff involved in these specifications. If they are
very exacting, few possible answers are returned.
Backoff strategies are used to return a larger set of
potential answers and to analyze the context of these
potential answers in more detail. The development of
routines for automatic creation of XPath expressions
is an ongoing process, but has begun to yield more
consistent results (Litkowski, 2005).
In preparation for TREC 2004, KMS was further
extended to incorporate a web-based component.
With a check box to indicate whether the web or a
document repository should be used, additional
functionality was used to pose questions to Google. In
web mode, an XML representation of a question is
still developed, but then it is analyzed to present an
optimal query to Google, typically, a pattern that will
provide an answer. This involves the use of an
integrated dictionary, particularly for creating
appropriate inflected forms in the search query. KMS
only uses the first page of Google results, without
going into the source documents, extracting sentences
from the Google results and using these as the
documents. (A user can create a new ?document
repository? consisting of the documents from which
answers have been obtained.) Many additional
possibilities have emerged from initial explorations in
using web-based question answering.
4 Summarization
Litkowski (2003a) indicated the possibility that the
XML representation of documents could be used for
summarization. To investigate this possibility, XML
Analyzer was extended to include summarization
capabilities for both general and topic-based
summaries, including headline and keyword
generation. Summarization techniques crucially take
into account anaphora, coreferent, and definite noun
phrase resolutions. As intimated in the analysis of the
parse output, the XML representation for a referring
expression is tagged with antecedent information,
including both an identifying number and the full text
of the antecedent. As a result, in examining a
sentence, it is possible to consider the import of all its
antecedents, instead of simply the surface form.
At the present time, only extractive
summarization is performed in KMS. The basis for
identifying important sentences is simply a frequency
count of its words, but using antecedents instead of
referring expressions. Stopwords and some other
items are eliminated from this count.
In KMS, the user has the option for creating
several kinds of summaries. The user specifies the
type of summary (general, topic-based, headline, or
keyword), which documents to summarize (one or
many), and the length. Topic-based summaries
require the user to enter search terms. The search
terms can be as simple as a person?s name or a few
keywords or can be several sentences in length.
Topic-based summaries use the search terms to give
extra weight to sentences containing the search terms.
Sentences are also evaluated for their novelty, with
redundancy and overlap measures based on
examining their noun phrases. KMS summarization
procedures are described in more detail in Litkowski
(2003b); novelty techniques are described in
Litkowski (2005).
In KMS, summaries are saved in XML files as
sets of sentences, each characterized by its source and
sentence number. Each summary uses XML
attributes containing the user?s specifications and the
documents included in the search. generated quickly
but in whole form.
5 Document Exploration
KMS includes two major components for
exploring the contents of a document. The first is
based on the semantic types attached to nouns and
verbs. The second is based on analyzing noun phrases
to construct a document hierarchy or ontology.
As noted above, each noun phrase and each verb
15
is tagged with its semantic class, based on WordNet.
A user can explore one or more documents in three
stages. First, a semantic category is specified.
Second, the user pushes a button to obtain all the
instances in the documents in that category. The
phraseology in the documents is examined so that
similar words (e.g., plurals and singulars and/or
synonyms) are grouped together and then presented in
a drop-down box by frequency. Finally, the user can
select any term set and obtain all the sentences in the
documents containing any of the terms.
KMS provides the capability for viewing a
?dynamic? noun ontology of a document set. All noun
phrases are analyzed into groups in a tree structure
that portrays the ontology that is instantiated by these
phrases. Noun phrases are reduced to their base
forms (in cases of plurals) and grouped together first
on the basis of their heads. Synonym sets are then
generated and a further grouping is made. Algorithms
from Navigli & Velardi (2004) are being modified
and implemented in KMS. The user can then select a
node in the ontology hierarchy and create a summary
based on sentences containing any of its terms or
children.
6 Dictionaries and Thesauruses in KMS
KMS makes extensive use of integrated dictionaries
and thesauruses, in addition to a comprehensive
dictionary used in parsing (which makes use of about
30 subcategorization patterns for verbs). This
dictionary is supplemented with other dictionaries that
are first used in dynamically extending the parser?s
dictionary for parsing, but then more extensively in
semantic analysis. WordNet is used for many
functions, as is a Roget-style thesaurus. KMS also
uses a full machine-readable dictionary, dictionaries
and semantic networks from the Unified Medical
Language System, and a specially constructed
dictionary of prepositions for semantic role analysis.
7 Summary
The preceding sections have focused on particular
prominent functionalities (question-answering,
summarization, and document exploration) in KMS.
Each of these components is part of the whole, in
which the main objective is to allow a user to explore
documents in a variety of ways to identify salient
portions of one or more documents. KMS is designed
to identify relevant documents, to build a repository
of these documents, to explore the documents, and to
extract relevant pieces of information.
References
Fiszman, M., Rindflesch, T., & Kilicoglu, H. (2003).
Integrating a Hypernymic Proposition Interpreter
into a Semantic Processor for Biomedical Texts.
Proceedings of the AMIA Annual Symposium on
Medical Informatics.
Gildea, Daniel, and Daniel Jurafsky. (2002) Automatic
Labeling of Semantic Roles. Computational
Linguistics, 28 (3), 245-288.
Irons, E. T. (1961) A Syntax Directed Compiler for
ALGOL-60. Communications of the ACM, 4, 51-
55.
Litkowski, K. C. (1999). Question Answering Using
Semantic Relation Triples. In E. M. Voorhees &
D. K. Harman (eds.), The Eighth Text Retrieval
Conference (TREC-8). NIST Special Publication
500-246. Gaithersburg, MD., 349-56.
Litkowski, K. C. (2002). CL Research Experiments in
TREC-10 Question-Answering. In E. M. Voorhees
& D. K. Harman (eds.), The Tenth Text Retrieval
Conference (TREC 2001). NIST Special
Publication 500-250. Gaithersburg, MD., 122-131.
Litkowski, K. C. (2003a). Question Answering Using
XML-Tagged Documents. In E. M. Voorhees & L.
P. Buckland (eds.), The Eleventh Text Retrieval
Conference (TREC 2002). NIST Special
Publication 500-251. Gaithersburg, MD., 122-131.
Litkowski, K. C. (2003b). Text Summarization Using
XML-Tagged Documents. Available:
http://nlpir.nist.gov/projects/duc/pubs.html.
Litkowski, K. C. (2005). Evolving XML and Dictionary
Strategies for Question Answering and Novelty
Tasks. Available:
http://trec.nist.gov/pubs/trec13/t13_proceedings.ht
ml.
Marcu, Daniel. (2000) The Rhetorical Parsing of
Unrestricted Texts: A Surface-based Approach.
Computational Linguistics, 26 (3), 395-448.
Navigli, R. & P. Velardi (2004) Learning Domain
Ontologies from Document Warehouses and
Dedicated Web Sites. Computational Linguistics
30, 151-180.
Tetreault, Joel. (2001) A Corpus-Based Evaluation of
Centering and Pronoun Resolution. Computational
Linguistics, 27 (4), 507-520.
16
Digraph Analysis of Dictionary Preposition Definitions
Kenneth C. Litkowski
CL Research
9208 Gue Road
Damascus, MD 20872
ken@clres.com
Abstract
We develop a model of preposition definitions
in a machine-readable dictionary using the
theory of labeled directed graphs and analyze
the resulting digraphs to determine a primitive
set of preposition senses. We characterize
these primitives and show how they can be
used to develop an inheritance hierarchy for
prepositions, representing the definitions by a
type and slots for its arguments. By analyzing
the definitions, we develop criteria for
disambiguating among the highly polysemous
primitives. We show how these criteria can be
used in developing the inheritance hierarchy
and how they may be used in assigning theta
roles to the objects of transitive verbs. Finally,
we describe the use of the disambiguation
criteria to parse and represent the meaning of
the prepositions as used in encyclopedia
articles.
1??????????????Introduction
Prepositions have generally been viewed as
function words to be discarded in many natural
language processing applications.  However,
prepositions have considerable importance as
identifiers of semantic relations tying various
elements of a sentence together. Since many
prepositions are highly polysemous, it is necessary
to develop a finer-grained analyses of their
meanings so that the semantic relations can be
more accurately identified.
We have modeled preposition definitions in a
dictionary using directed labeled graphs (digraphs)
to identify primitive preposition senses. We have
used the definitions from a machine-readable
version of a comprehensive English dictionary.
2??????????????Modeling Preposition Definitions
A preposition is ?a word governing, and usually
preceding, a noun or pronoun and expressing a
relation to another word or element in the clause.?
The definition of a preposition takes two principal
forms: (1) a usage expression characterizing the
relation or (2) an expression that can be substituted
for the preposition. A substituting preposition
definition usually consists of a prepositional phrase
(including both a preposition and a noun phrase)
and a terminating preposition (e.g., for around, one
definition is ?on every side of?).
2.1????????Headwords as Digraph Nodes
A digraph consists of nodes and directed arcs
between the nodes. In general, an arc should
correspond to a transitive relation. Modeling a
dictionary with a digraph entails assigning an
interpretation to the nodes and arcs. For our initial
model, we subsume all the definitions of a
preposition as one node in the digraph, labeled by
the preposition. An arc is drawn from one node
(e.g., of) to another (e.g., around) if the preposition
represented by the first node contributes a typed
meaning component with an open slot to the
preposition represented by the second node, e.g.,
?part-of of around? would arise from the
definition of around (?on every side of?).
Loosely, for our purposes, the terminating
preposition acts as a genus term in an ISA
hierarchy and makes it possible to use the results
from digraph theory to analyze the relationships
between definitions. In particular, digraph analysis
identifies definitional cycles and ?primitives? and
arranges the nodes into an inheritance hierarchy.
When a dictionary is modeled like this, digraph
theory (Harary, et al 1965) indicates that there is
a ?basis set? of nodes, which may be viewed as a
                      July 2002, pp. 9-16.  Association for Computational Linguistics.
                 Disambiguation: Recent Successes and Future Directions, Philadelphia,
                             Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense
set of primitives.
1
Many prepositions are not used as the final
preposition of other preposition definitions
(specifically, their nodes have an outdegree of 0).
These are the leaves of the inheritance hierarchy.
When these are removed from the dictionary, other
prepositions will come to have outdegree 0, and
may in turn be removed. After all such iterations,
the remaining nodes are ?strongly connected?, that
is, for every node, there is a path to each other
node; a strong component is an equivalence class
and corresponds to a definitional cycle.
Each strong component may now be viewed as
a node. Some of these nodes also have the property
that they have outdegree 0; these strong component
may also be removed from the dictionary. This may
introduce a new round where individual nodes or
strong components have outdegree 0 and hence
may be removed from the dictionary.
After all removals, what is left is a set of one
or more strong components, each of which is
unreachable from the other. This final set is viewed
as the set of primitives. What this means is that we
have converted the preposition dictionary into an
inheritance hierarchy. If we can characterize the
meanings of the primitives, we can then inherit
these meanings in all the words and definitions that
have been previously been removed.
2.2????????Definitions as Digraph Nodes
This model of prepositions is very coarse, lumping
all senses into one node. Having reduced the set of
prepositions with this model, we can initiate a new
round of digraph analysis by disambiguating the
final preposition. In this new model, each node
represents a single sense and the arc between two
nodes indicates that one specific sense is used to
define one specific sense of another word (i.e.,
?contributes a typed meaning component with an
open slot to?).
With this new model, we can enter into a
further round of digraph analysis. In this round,
which proceeds as above, instead of a set of
primitive prepositions, the outcome will be a set of
primitive preposition definitions. However, as
mentioned above, preposition definitions come in
two flavors. The usage expressions are lumped into
the digraph analysis when a node corresponded to
all definitions, but they do not do so in the
definition digraph analysis.
3   NODE Prepositions
As the data for the digraph analysis, we began with
the 155 prepositions identified in a
machine-readable dictionary (The New Oxford
Dictionary of English, 1998) (NODE). Additional
prepositions are found as unmarked phrases under
noun or adjective headwords, but not so labeled,
e.g., in spite of under the headword spite. To find
these prepositions, we developed a more rigorous
specification of a preposition signature. A
preposition definition is either (1) a preposition; (2)
a prepositional phrase + a preposition; (3) (an
optional leading string) + a transitive present
participle; or (4) a leading string + an infinitive of
a transitive verb. This led to the addition of 218
phrasal prepositions, for a total of 373 entries, with
847 senses, shown in the Appendix.
We may have missed other subsenses that have
a preposition signature. In all likelihood, these
patterns would enter the digraph analysis as nodes
with outdegree 0 and hence would be eliminated in
the first stage of the primitive analysis.
3.1????????Substitutable Definitions
Most preposition definitions are in a form that can
be substituted for the preposition. For a sense of
against (?as protection from?), with an example
?he turned up his collar against the wind?, the
definition can be fully substituted to obtain ?he
turned up his collar as protection from the wind.?
The preposition definitions were parsed,
putting them into a generic sentence frame, usually
?Something is [prepdef] something.? For example,
the definition of ahead of (?in store for?) would be
parsed as ?Something is in store for something.?
1
The determination of the ?basis set? of a digraph is
NP-complete However, as pointed out in (Litkowski,
1988), this process will not involve millions of
nodes. In our implementation of the algorithm for
finding strong components (Even 1980), the digraph
analysis of prepositions takes less than two seconds.
For definitions with a selectional restriction on the
preposition?s object (identifiable by a parenthesized
expression in the definition), the parentheses were
removed in the sentence frame, e.g., above (?higher
than (a specified amount, rate, or norm)?) would be
parsed as ?Something is higher than a specified
amount, rate, or norm.?
The parse tree would then be analyzed to
obtain the final preposition, treated as the
hypernym. For definitions containing a verb at the
end, e.g., another sense of above (?overlooking?,
parsed as ?Something is overlooking something?)
would yield ?overlooking? as the hypernym.
3.2????????Usage Note Definitions
Many preposition definitions are not substitutable,
but rather characterize how the preposition is used
syntactically and semantically. One sense of of
(?expressing the relationship between a part and a
whole?) characterizes the semantic relationship (in
this case, the partitive). One of its subsenses (?with
the word denoting the part functioning as the head
of the phrase?) indicates syntactic characteristics
when this sense is used. These definitions are not
parsed and do not lead to the identification of
hypernyms. As shown below, these definitions will
emerge as the primitives.
3.3????????Definition Modifications
The automatic generation of preposition hypernyms
was less than perfect. We examined each definition
and made various hand modifications. Our editing
process included hand entry of hypernyms: adding
or modifying automatically generated hypernyms,
making hypernymic links for ?non-standard?
entries (e.g., making upon the hypernym of ?pon),
and creating hypernymic links from a subsense to
a supersense
4 Digraph Analysis Results
The digraph analysis described above eliminated
309 of the 373 entries. The remaining 64 entries
were grouped into 25 equivalence classes, as
shown in Table 1 and portrayed in Figure 1 in the
appendix. Figure 1 shows how these strong
components are related to one another. The strong
components highlighted in the table are primitives.
Seven of the primitive strong components (in, of,
than, as, from, as far as, and including) have
paths into strong component 12. Strong
components 14 to 18 arise essentially from the
primitive strong component of. The eighth strong
component (23) and other entries defined by words
in this class exist somewhat independently.
It would seem that the largest strong
component (12, with 33 entries) should be broken
down into smaller classes; this would occur in the
sense-specific digraph analysis. Specialized senses
of with, by, to, for, and before give rise to
definitional cycles within this strong component.
In addition to the strong components shown
above, 62 non-prepositional primitives have been
identified. The first 42 of these primitives were
used in defining entries that were removed in the
first phase of the digraph analysis. The 20
beginning with affect were used in defining entries
in the primitive strong components.
There are 155 preposition senses (out of 847)
that are defined solely with usage notes. Of these,
71 are subsenses, leaving 74 senses in 26 entries
(as shown in Table 3) that can be considered the
most primitive senses and deserving initial focus in
attempting to lay out the meanings of all
preposition senses.
5 Interpretation of Results
The digraph analysis of prepositions provides
additional perspectives in understanding their
meanings and their use. To begin with, the analysis
enables us to identify definitional cycles and move
toward the creation of an inheritance hierarchy.
The large number of senses that have verb
hypernymic roots indicates a close kinship between
prepositions and verbs, suggesting that a verb
hierarchy may provide an organizing principle for
prepositions (discussed further below). The large
number of senses rooted in usage notes, which
essentially characterize how these senses function,
encapsulates the role of prepositions as ?function
words;? however, as described below, these
functions are not simply syntactic in nature, but
also capture semantic roles.
Table 1 Strong Components
Entries
1 over, above
2 against
3 but
4 along
5 on
6 via, by way of
7 through
8 touching
9 until, up to
10 below, underneath
11 inside, within
12 in favour of, along with, with respect to, in
proportion to, in relation to, in connection
with, with reference to, in respect of, as
regards, concerning, about, with, in place of,
instead of, in support of, except, other than,
apart from, in addition to, behind, beside, next
to, following, past, beyond, after, to, before, in
front of, ahead of, for, by, according to
13 in
14 across
15 by means of
16 in the course of
17 during
18 on behalf of
19 of
20 than
21 as
22 from
23 by reason of, because of, on account of
24 as far as
25 including
Table 2
Non-Prepositional Primitives
embrace, incur, lose, injure, called, taking into
consideration, taking account of, help, guide, interest,
impress, providing, exceeding, requiring, needing,
losing, injuring, restrain, see, attaining, support,
defend, award, subtracting, nearly, cover, exclude,
involving, undergoing, do, encircle, separating, taking
into account, concerns, lacking, encircling, hit,
achieving, using, involve, affect, overlooking,
awaiting, having, being, reach, preceding, constituting,
affecting, representing, facing, promote, obtain,
containing, approaching, almost, taking, complete,
reaching, concern, possessing, wearing
The frequency with which the various
prepositions are used as hypernyms in defining
other prepositions reveals something about their
relative importance.  The most frequent hypernyms
are of (175), to (74), than (45), with (44), by (39),
from (30), for (22), as (20), and in (12). These
prepositions correspond to the primitives identified
in Table 1, as well as those with the largest number
of usage notes shown in Table 3.
Table 3
Usage-Note Primitives
about (2), as (1), as from (1), as of (1), at (6), between
(1), but (1), by (7), for (6), from (11), in (7), in relation
to (1), into (8), like (1), of (9), on (1), on the part of
(1), out of (1), over (1), than (2), this side of (1), to (7),
towards (1), under (1), up to (1), with (4)
On the other hand, the relative frequencies may
not correspond well with our intuitions about a
semantic classification of prepositions. (Quirk, et
al. 1985) give the greatest prominence to spatial
and temporal meanings, followed by the
cause/purpose spectrum, the means/agentive
spectrum, accompaniment, and support and
opposition, and finally, several miscellaneous
categories. In the semantic relations hierarchy of
the Unified Medical Language System (UMLS)
(Unified Medical Language System 2002), five
general types of associations are identified:
physical, spatial, functional (causal), temporal, and
conceptual. The leaves of the UMLS hierarchy are
realized as verbs, but have a strong correspondence
to the classification in (Quirk, et al 1985).
In our identification of primitives, including the
usage notes, spatial and temporal senses are
conspicuously reduced in significance, while a
comparative term (than) seems to have a much
greater presence. The explanation for these two
observations is that (1) many of the basic spatial
and temporal prepositions were located in the
largest strong component (12 in Table 1) or were
derived from it and (2) many of the senses of these
spatial and temporal prepositions have ?than? as
hypernym. This suggests that a considerable
amount of the meaning of such prepositions lie
principally in describing relative position in a
spatio-temporal continuum.
6 Developing an Inheritance
Hierarchy
As suggested earlier, the next stage of digraph
analysis involves disambiguating the hypernymic
preposition, so that individual nodes of the digraph
represent senses or concepts. As suggested in
(Litkowski, 1978), these nodes will consist of a
gloss and the various lexicalizations the concept,
much like the synsets in WordNet (Fellbaum
1998). A prototypical case would be strong
component 23 which may be lexicalized as {by
reason of, because of, on account of}; our
analysis suggests that, in this case, some further
characterization of the usage of this concept by the
lexicographers would be desirable, since otherwise
we have only a vicious definitional cycle.
The creation of the hierarchy would involve
assigning a label or type to the individual concepts
and then characterizing the information that is to be
inherited. The typology can be developed from the
bottom up, rather than developing some a priori
structure. In other words, since the digraph
analysis has identified primitive senses, these
provide an appropriate starting point. Each sense
can be examined on its own merits with an initial
assignment of a type and later examination of the
full set of primitives for organization into a data-
driven set of types and subtypes.
As to what gets inherited, we begin with the
fact that in general, each preposition has two
arguments, arg1 (the object of the preposition) and
arg2 (the attachment point, or head, of the
prepositional phrase). We may take these as the
two slots associated with each representation and
we may give the slots names according to the type
(or just implicitly understand that a type has
particular types of arguments). When considering
the general structure of a non-primitive preposition
definition (a prepositional phrase with an ending
preposition), the NP of the prepositional phrase is
the value of arg2. This value will be useful in
disambiguating the hypernymic preposition (as
described in the next section). In considering the
slots for prepositions whose hypernym is a verb (as
identified in Table 2), arg1 will be the object of the
verb.
7 Definition Use
To describe the process by which preposition
senses will be disambiguated and also how the
representations of their meaning will be used in
processing text, Table 4 shows the definitions for
?of?, the most frequently used hypernym and
perhaps the second most frequent word in the
English language. In the table, we have assigned a
type to each of nine main senses. In the definition
column, the main sense is given first, with any
subsenses given in parentheses, separated by
semicolons if there is more than one subsense.
First, we consider the disambiguation of
hypernyms in preposition definitions, that is, those
whose final word is ?of?. One sense of ?after? is
?in imitation of? (e.g., ?a mystery story after
Poe?); examining the table suggests that this is a
deverbal use of ?of?, where the object of ?after?
would be the object of the underlying verb of
?imitation?, so that when ?after? is used in this
sense, its arg1 is the object of the verb ?imitate?. A
sense of ?on behalf of? is ?as a representative of?;
this is the partitive sense, so that arg1 of ?on
behalf of? is a ?whole?. Finally, one sense of ?like?
is ?characteristic of?; this is the predicative
deverbal. Carrying out this process throughout the
preposition definitions will thus enable us not only
to disambiguate them, but also to identify
characteristics of their arguments when the
prepositions they define are used in some text.
In addition, prepositions very often appear at
the end of the definitions of transitive verbs. For
example, one sense of ?accommodate? is ?provide
lodging or sufficient space for?, where the sense of
?for? is ?to the benefit of?, where ?of? is used in
the genitive sense (i.e., ?someone?s or something?s
benefit). With this interpretation, we can say that
the object of ?accommodate? is a benefactive and
that a benefactive role has been lexicalized into the
meaning of ?accommodate?. With disambiguation
of the final preposition in such definitions, we will
be able to characterize the objects of these verbs
with some theta role.
The ultimate objective of this analysis of
prepositions is to be able to characterize their
occurrences in processing text. Specifically, we
would like to disambiguate a preposition, so that
we can assign each instance a type and characterize
its arguments. In this way, processing a text would
identify the semantic relations present in the text.
We have performed some initial investigations into
the viability of this goal.
We have begun implementing a discourse
analysis of encyclopedia articles. At the base of
this analysis, we are identifying and characterizing
discourse entities, essentially the noun phrases. Our
Table 4. Definitions of ?of?
Type Definition (Subsense(s))
1. Partitive relationship between a part and a whole (part functioning as head; after a number, quantifier, or
partitive noun, with the word denoting the whole functioning as the head of the phrase)
2. Scale-Value relationship between a scale or measure and a value (an age)
3. Genitive association between two entities, typically one of belonging (relationship between an author, artist, or
composer and their works collectively)
4. Direction relationship between a direction and a point of reference
5. Hypernym relationship between a general category and the thing being specified which belongs to such a
category (governed by a noun expressing the fact that a category is vague)
6. Deverbal relationship between an abstract concept having a verb-like meaning and (a noun denoting the subject
of the underlying verb; the second noun denotes the object of the underlying verb; head of the phrase
is a predicative adjective)
7. Indirect Object relationship between a verb and an indirect object (a verb expressing a mental state; expressing a
cause)
8. Substance the material or substance constituting something
9. Time time in relation to the following hour
analysis includes identification of the syntactic role
and semantic type of the noun phrases, along with
attributes such as number and gender. The analysis
also includes resolution of anaphora, coreferences,
and definite noun phrases. The modules analyzing
the discourse entities come after a full parse of
each sentence. We have now introduced a module
to examine prepositions and build semantic
relations. The results of these analyses generate an
XML representation of discourse segments,
discourse entities, and semantic relations, each with
an accompanying set of attributes.
Our implementation of the semantic relation
module has identified several issues of interest.
First, the characterization of the semantic relation
needs to come after the object of the prepositional
phrase has been analyzed for its discourse entity
properties. For example, if the object is an
anaphor, the antecedent needs to be established.
Second, the attachment points of the prepositional
phrase need to be identified; our parser establishes
a stack of possible attachment points (index
positions in the sentence), with the most likely at
the top of the stack. (Attachment tests could be
implemented at this point, although we have not yet
done so.) The attachment point is necessary to
identify the arguments to be analyzed.
Having identified the arguments, the
information subject to analysis includes the literal
arguments (both the full phrase and their roots), the
parts of speech of the arguments, any semantic
characterizations of the arguments that are
available (such as the WordNet file number), and
access to the dictionary definitions of the root
heads. The analysis for the semantic relation is
specific to the preposition. We are encoding a
semantic relation type and one or more tests with
each sense.  Some of these tests are simple, such as
string matches, and others are complex, involving
function calls to examine semantic relationships
between the arguments.
In the case of ?of?, the first test was whether
arg2 is an adjective, in which case we assigned a
type of ?predicative?. Next, if arg2 was a vague
general category (?form?, ?type?, or ?kind?), we
set the type to ?hypernymic?. If neither of these
conditions was satisfied, we looked up the root of
arg2 in WordNet to determine if the word had a
?part-of? relation (resulting in a ?partitive? type)
or ?member-of? relation (resulting in a
?hypernymic? type). If a type had not been
established by this point, we used the WordNet file
number to establish an intermediate type. Thus, for
example, if arg2 was an ?action? or ?process?
word, we set the type for the semantic relation to
?deverbal?; for a ?quantity?, we set the type to
?partitive?. Finally, we can make use of the
definition for arg1 (parsed to identify its
hypernym) to determine if arg2 is the hypernym of
arg1. When these criteria are not sufficient, we
label the type ?undetermined?.
In our encyclopedia project, we parse and
process the articles to generate XML files. We then
apply an XSL transformation to extract all the
semantic relations that were identified, including
the preposition, the type assigned, and the values of
Figure 1. Basis Digraph of NODE Prepositions
arg1 and arg2. We can sort on these fields to
facilitate analysis of our success and to identify
situations in need of further work.
After the initial implementation, we were able
to assign semantic relations to 50 percent of the
instances of ?of?, although many of these were
given incorrect assignments. However, the method
is useful for identifying instances for which
improved analysis is necessary. For example, we
can identify where improved characterization of
discourse entities is needed, or where additional
lexical information might be desirable (such as how
to identify a partitive noun).
8 Conclusions and Further Work
We have shown that a digraph analysis of
preposition definitions provides a useful organizing
principle for analyzing and understanding the
meanings of prepositions. The definitions
themselves provide sufficient information for
developing an inheritance hierarchy within a typed-
feature structure arrangement and also provide a
rich set of criteria for disambiguating among the
many senses. By incorporating these criteria in a
text processing system, it is possible to develop
semantic triples that characterize intrasentential
relationships among discourse entities. Further, the
characterization of meanings may prove useful in
identifying theta roles implied by the ending
prepositions of transitive verb definitions 
Much work remains to be done to develop the
full set of information for all prepositions. We
believe we have established a suitable framework
for carrying out this work.
References
Even, S. (1980). Graph Algorithms. Rockville, MD:
Computer Science Press.
Fellbaum, C. (1998). (Ed.), WordNet: An Electronic
Lexical Database (pp. 69-104). Cambridge,
Massachusetts: The MIT Press.
Harary, F., Norman, R. Z., & Cartwright, D. (1965).
Structural models: An introduction to the theory
of directed graphs. New York: John Wiley and
Sons, Inc.
Litkowski, K. C. (1988). On the search for semantic
primitives. Computational Linguistics, 14(1),
52.
Litkowski, K. C. (1978). Models of the semantic
structure of dictionaries. American Journal of
Computational Linguistics, Mf.81, 25-74.
The New Oxford Dictionary of English. (1998) (J.
Pearsall, Ed.). Oxford: Clarendon Press.
Quirk, R., Greenbaum, S., Leech, G., & Svartik, J.
(1985). A comprehensive grammar of the
English language. London: Longman.
Unified Medical Language System Knowledge
Sources. (13
th
 ed.). (2002). Bethesda, MD:
National Library of Medicine.
#? la
#'cept
#'gainst
#'mongst
#'pon
a cut above
abaft
abaht
aboard
about
above
absent
according to
across
afore
after
after the fashion of
against
agin
ahead of
all for
all of
all over
along
along of
along with
alongside
amid
amidst
among
amongst
an apology for
anent
anti
anything like
anywhere near
apart from
apropos
around
as
as far as
as for
as from
as of
as regards
as to
aside from
aslant
astraddle
astride
at
at a range of
at peril of
at right angles to
at the expense of
at the hand of
at the hands of
at the heels of
at the instance of
at the mercy of
athwart
atop
back of
bar
bare of
barring
because of
before
behind
below
beneath
beside
besides
between
betwixt
beyond
but
but for
by
by courtesy of
by dint of
by force of
by means of
by reason of
by the hand of
by the hands of
by the name of
by the side of
by virtue of
by way of
care of
chez
circa
come
complete with
con
concerning
considering
contrary to
counting
courtesy of
cum
dan
dehors
depending on
despite
despite of
down
due to
during
ere
even as
every bit as
ex
except
except for
excepting
excluding
exclusive of
failing
following
for
for all
for the benefit of
for the love of
forby
forbye
fore
fornenst
fornent
frae
from
give or take
given
gone
good for
having regard to
head and shoulders above
in
in accord with
in addition to
in advance of
in aid of
in answer to
in back of
in bed with
in behalf of
in case of
in common with
in company with
in connection with
in consideration of
in contravention of
in consequence of
in default of
in despite of
in excess of
in face of
in favor of
in favour of
in front of
in honor of
in honour of
in keeping with
in lieu of
in light of
in line with
in memoriam
in need of
in obedience to
in peril of
in place of
in proportion to
in re
in reference to
in regard to
in relation to
in respect of
in restraint of
in sight of
in spite of
in succession to
in support of
in terms of
in the act of
in the cause of
in the course of
in the face of
in the fashion of
in the gift of
in the grip of
in the heat of
in the interest of
in the interests of
in the light of
in the matter of
in the midst of
in the name of
in the nature of
in the pay of
in the person of
in the shape of
in the teeth of
in the throes of
in the way of
in token of
in view of
in virtue of
in with
including
inclusive of
inshore of
inside
inside of
instead of
into
into the arms of
irrespective of
less
like
little short of
mid
midst
minus
mod
modulo
more like
near
near to
neath
next
next door to
next to
nigh
none the worse for
not a patch on
not someone's idea of
nothing short of
notwithstanding
o'
o'er
of
of the name of
of the order of
off
offa
on
on a level with
on a par with
on account of
on behalf of
on pain of
on the order of
on the part of
on the point of
on the right side of
on the score of
on the strength of
on the stroke of
on the wrong side of
on top of
onto
opposite
other than
out
out for
out of
out of keeping with
out of line with
outa
outboard of
outside
outside of
outta
outwith
over
over against
over and above
overtop
owing to
pace
past
pending
per
plus
preparatory to
previous to
prior to
pro
pursuant to
qua
re
regarding
regardless of
relative to
respecting
round
round about
sans
save
saving
short for
short of
shot through with
sick and tired of
since
strong on
subsequent to
than
thanks to
the better part of
this side of
thro'
through
throughout
thru
thwart
till
to
to the accompaniment of
to the exclusion of
to the tune of
to windward of
together with
touching
toward
towards
uh
under
under pain of
under cover of
under sentence of
under the auspices of
under the banner of
under the baton of
under the heel of
underneath
unknown to
unlike
until
unto
up
up against
up and down
up before
up for
up on
up to
up to one's elbows in
up to one's neck in
upon
upward of
upwards of
v
v.
versus
via
vice
vis-?-vis
vs
while
with
with a view to
with one eye on
with reference to
with regard to
with respect to
with the exception of
withal
within
within a measurable distance of
within sight of
without
Table A-2 Prepositions in the New Oxford Dictionary of English
Sense Information for Disambiguation:
Confluence of Supervised and Unsupervised Methods
Kenneth C. Litkowski
CL Research
9208 Gue Road
Damascus, MD 20872
ken@clres.com
Abstract
For SENSEVAL-2, we disambiguated the lexical
sample using two different sense inventories.
Official SENSEVAL-2 results were generated
using WordNet, and separately using the New
Oxford Dictionary of English (NODE). Since
our initial submission, we have implemented
additional routines and have now examined the
differences in the features used for making sense
selections. We report here the contribution of
default sense selection, idiomatic usage, syntactic
and semantic clues, subcategorization patterns,
word forms, syntactic usage, context, selectional
preferences, and topics or subject fields. We also
compare the differences between WordNet and
NODE. Finally, we compare these features to
those identified as significant in supervised
learning approaches.
1 Introduction
CL Research?s official submission for SENSEVAL-2
used WordNet as the lexical inventory. Separately,
we also used a machine-readable dictionary (The
New Oxford Dictionary of English, 1998) (NODE),
mapping NODE senses automatically into WordNet
senses. We did not submit these results, since we
were not sure of the feasibility of using one
dictionary mapped into another. Our initial results
(Litkowski, 2001)  proved to be much better than
anticipated, achieving comparable levels of precision
although at lower levels of recall, since not all senses
in NODE mapped into WordNet senses.
Subsequently, we examined our results in more detail
(Litkowski, 2002), primarily focusing on the quality
of the mapping and its effect on our performance
using NODE. This led us to the conclusion that we
had likely performed at a considerably higher level
using the NODE inventory, with an opportunity for
even better performance as we were able to exploit
much more information available in NODE.
We have now identified what features (i.e., sense
information) were used in our disambiguation. In
particular, we have examined the role of (1) default
sense selection, (2) idiomatic usage, (3) typing (e.g.,
transitivity), (4) syntactic and semantic clues, (5)
subcategorization patterns, (6) word form (e.g.,
capitalization, tense, or number), (7) selectional
preferences (for verbs and adjectives), (8) syntactic
usage (e.g., nouns as modifiers), (9) context (in
definitions and in examples), and (10) topic area
(e.g., subject fields associated with definitions).
Our methodology enables us to compare the
features relevant to disambiguation in WordNet and
in NODE, allowing us to pinpoint differences
between the two sense inventories.
1
 In addition,
comparing our findings with those identified in
supervised machine learning algorithms, we can see
patterns of similarity with our features.
In the following sections, we describe our
methods of dictionary preparat ion, our
disambiguation techniques, our methodology for
analyzing features and our results. We discuss these
findings in terms of what they say about the
differences between the information available in each
of the two sense inventories, the possible
generalizability of our analysis technique, and how
our features relate to those used by other
SENSEVAL  participants who used supervised
learning techniques. Finally, we describe our future
plans of analysis, based on attempting to merge
1
We have not yet determined how decisive these
features are in making correct sense selections. The
present study should be viewed as an examination of
sense distinctions in lexical resources.
                     July 2002, pp. 47-53.  Association for Computational Linguistics.
                 Disambiguation: Recent Successes and Future Directions, Philadelphia,
                             Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense
supervised and unsupervised word-sense
disambiguation.
2 Dictionary Preparation
CL Research?s DIMAP (Dictionary Maintenance
Programs) disambiguates open text against WordNet
or any other dictionary converted to DIMAP. The
dictionaries used for disambiguation operate in the
background (as distinguished from the foreground
development and maintenance of a dictionary), with
rapid lookup to access and examine the multiple
senses of a word after a sentence has been parsed.
DIMAP allows multiple senses for each entry, with
fields for definitions, usage notes, hypernyms,
hyponyms, other semantic relations, and feature
structures containing arbitrary information.
For SENSEVAL-2, WordNet was entirely
converted to alphabetic format for use as the
disambiguation dictionary. Details of this conversion
(which captures all WordNet information) and the
creation of a separate ?phrase? dictionary for all
noun and verb multiword units (MWUs) are
described in Litkowski (2001). In disambiguation, the
phrase dictionary is examined first for a match, with
the full phrase then used to identify the sense
inventory rather than a single word.
NODE was prepared in a similar manner, with
several additions. A conversion program transformed
the MRD files into various fields in DIMAP, the
notable difference being the much richer and more
formal structure (e.g., lexical preferences, grammar
fields, and subsensing). Conversion also
automatically created ?kind? and ?clue? regular
expression phrases under individual headwords, e.g.,
?(as) happy as a sandboy (or Larry or a clam)?
under happy was converted into a collocation pattern
for a sense under happy, written ?(as|?) ~ as (a
sandboy | Larry | a clam)?, with the tilde marking the
target word. Further details on this conversion and
definition parsing to enrich the sense information are
also provided in Litkowski (2001). After parsing was
completed, a phrase dictionary was also created for
NODE.
2
The SENSEVAL lexical sample tasks
(disambiguating one of 73 target words within a text
of several sentences) were run independently against
the WordNet and NODE sense inventories, with the
WordNet results submitted. To investigate the
viability of mapping for WSD, subdictionaries were
created for each of the lexical sample words. For
each word, the subdictionaries consisted of the main
word and all entries identifiable from the phrase
dictionary for that word. (For bar, in NODE, there
were 13 entries where bar was the first word in an
MWU and 50 entries where it was the head noun; for
begin, there was only one entry.)
The NODE dictionaries were then mapped into
the WordNet dictionaries (see Litkowski, 1999),
using overlap among words and semantic relations.
The 73 dictionaries for the lexical sample words gave
rise to 1372 WordNet entries and 1722 NODE
entries. Only 491 entries (of which, 418 were
MWUs) were common (i.e., no mappings were
available for the remaining 1231 NODE entries, all
of which were MWUs); 881 entries in WordNet were
therefore inaccessible through NODE. For the entries
in common, there was an average of 5.6 senses, of
which only 64% were mappable into WordNet, thus
creating our initial impression that use of NODE
would not be feasible.
3
3 Disambiguation Techniques
Details of the disambiguation process are provided in
Litkowski (2001). In general, for the lexical sample,
the sentence containing the target word was first
parsed and the part of speech of the target word was
used to select the sense inventory. If the tagged word
was part of an MWU, the MWU's sense inventory
was used. The dictionary entry for the word was then
accessed. Before evaluating the senses, the topic area
of the context provided by the sentence was
?established?. Subject labels for all senses of all
content words in the context were tallied.
Each sense of the target was then evaluated,
based on the available information for the sense,
including type restrictions such as transitivity,
presence of accompanying grammatical constituents
such as infinitives or complements, selectional2WordNet definitions were not parsed. An experiment
showed the semantic relations identifiable through
parsing were frequently inconsistent with those in
WordNet.
3
Note that a mapping from WordNet to NODE
generates similar mismatch statistics.
preferences for verbs and adjectives, form restrictions
such as number and tense, grammatical roles,
collocation patterns, contextual clue words,
contextual overlap with definitions and examples, and
topical area matches. Points were given to each sense
and the sense with the highest score was selected; in
case of a tie, the first sense was selected.
The top line of Table 1 shows our official results
using WordNet as the disambiguation dictionary,
with an overall precision (and recall) of 0.293 at the
fine-grained level and 0.367 at the coarse-grained
level. Disambiguating with NODE immediately after
the official submission and mapping its senses into
WordNet senses achieved comparable levels of
precision, with a coverage of 75% based on the
senses that could be mapped into WordNet, even
though the NODE coverage was 100%.
Since our original submission, we have
implemented many additional routines and improved
our NODE mapping to WordNet; our revised
precision shown in Table 1 are now 0.368 at the fine-
grained level and 0.462 at the coarse-grained level
using WordNet and 0.337 and 0.427 using NODE.
Of particular note are the facts that the mapping from
NODE to WordNet is now 89% and that precision is
comparable except for the verbs.
In Litkowski (2002), we examined the mapping
from NODE to WordNet in considerable detail.
Several of our findings are pertinent to our analysis
of the features affecting disambiguation. Table 1
reflects changes to the automatic mapping along with
hand changes. The automatic mapping changes
account for the change in coverage. The hand
mapping shows that the automatic mapping was
about 70% accurate. Interestingly, the hand changes
did not affect precision. In general, the fact that we
were able to achieve a level of precision comparable
to WordNet suggests the most frequent senses of the
lexical sample words were able to be disambiguated
and mapped correctly into WordNet.
The significant discrepancy between the entries
(all MWUs, 1231 entries in NODE not in WordNet
and 871 entries in WordNet not in NODE) in part
reflects the usual editorial decisions that would be
found in examining any two dictionaries. However,
since WordNet is not lexicographically based, many
of the differences are indicative of the idiosyncratic
development of WordNet. WordNet may identify
several types of an entity (e.g., apricot bar and
nougat bar), where NODE may use one sense (?an
amount of food or another substance formed into a
regular narrow block?) without creating separate
entries that follow this regular lexical rule.
For the most part, verb phrases containing
particles are equally present in both dictionaries (e.g.,
draw out and draw up), but NODE contains several
more nuanced phrases (e.g., draw in one's horns,
draw someone aside, keep one's figure, and pull
oneself together). NODE also contains many idioms
where a noun is used in a verb phrase (e.g., call it a
day, keep one's mouth shut, and go back to nature).
About 100 of our disambiguations using NODE were
to MWUs not present in WordNet (20% of our
coverage gap).
Of most significance to the sense mapping is the
classical problem of splitting (attaching more
importance to differences than to similarities,
resulting in more senses) and lumping (attaching
more significance to similarities than to differences,
resulting in fewer senses). Splitting accounts for the
remaining 80% gap in our coverage (where NODE
identified senses not present in WordNet). The effect
of lumping is more difficult to assess. When a NODE
definition corresponds to more than one sense in
WordNet, we may disambiguate correctly in NODE,
but receive no score since we have mapped into the
wrong definition; the WordNet sense groupings may
allow us to receive credit at the coarse grain, but not
at the fine grain. We have examined this issue in
more detail in Litkowski (2002), with the conclusion
that lumping reduces our NODE score since we are
unable to pick out the single WordNet sense answer.
More problematic for our mapping was the
absence of crucial information in WordNet. Delfs
Table 1. Lexical Sample Precision
Run
Adjectives Nouns Verbs Total
Items Fine Coarse Items Fine Coarse Items Fine Coarse Items Fine Coarse
WordNet Test 768 0.354 0.354 1726 0.338 0.439 1834 0.225 0.305 4328 0.293 0.367
NODE Test 420 0.288 0.288 1403 0.402 0.539 1394 0.219 0.305 3217 0.308 0.405
WordNet Test (R) 768 0.435 0.435 1726 0.430 0.535 1834 0.267 0.387 4328 0.368 0.462
NODE Test (R) 684 0.472 0.472 1567 0.429 0.537 1605 0.189 0.300 3856 0.337 0.427
(2001) described a sense for begin that has an
infinitive complement, but present only in an example
sentence and not explicitly encoded with the usual
WordNet verb frame. Similarly, for train, two
sentences were ?tagged to transitive senses despite
being intransitive because again we were dealing with
an implied direct object, and the semantics of the
sense that was chosen fit; we just pretended that the
object was there.? In improving our disambiguation
routines, it will be much more difficult to glean the
appropriate criteria for sense selection in WordNet
without this explicit information than to obtain it in
NODE and map it into WordNet. Much of this
information is either not available in WordNet,
available only in an unstructured way, only implicitly
present, or inconsistently present.
4 Feature Analysis Methodology
4.1 Identifying Disambiguation Features
As indicated above, our disambiguation routines
assign point values based on a judgment of how
important each feature seems to be. The weighting
scheme is ad-hoc. For the feature analysis, we simply
recorded a binary variable for each feature that had
made a contribution to the final sense selection. In
particular, we identified the following features: (1)
whether the sense selected was the default (first)
sense (i.e., no other features were identified in
examining any of the senses), (2) whether the
identified sense was based on the occurrence of the
target word in an idiom, (3) whether a type
(specifically, transitivity) factored into the sense
selection, (4) whether the selected sense had any
syntactic or semantic clues, (5) whether a
subcategorization pattern figured into the sense
selection, (6) whether the sense had a specified word
form (e.g., capitalization, tense, or number), (7)
whether a syntactic usage was relevant (e.g., nouns
as modifiers or an adjective being used as a noun,
such as ?the blind?), (8) whether a selectional
preference was satisfied (for verb subjects and
objects and adjective modificands), (9) whether we
were able to use a Lesk-style context clue from the
definitions or an example, and (10) topic area (e.g.,
subject fields, usage labels, or register labels
associated with definitions).
As the disambiguation algorithm proceeded, we
recorded each of the features associated with each
sense. After a sense was selected, the features
associated with that sense were written to a file (as a
hexadecimal number) for subsequent analysis. We
sorted the senses for each target word in the lexical
sample and summarized the features that were used
for all instances that had the same sense. We then
summarized the features over all senses and further
summarized them by part of speech. These results are
shown in Table 2.
The first column shows the number of instances
for each part of speech and overall. The second
column shows the number of instances where the
disambiguation algorithm selected the default sense.
These cases indicate the absence of positive
information for selecting a sense and may be
construed as indicating that the sense inventory may
not make sufficient sense distinctions. The default
numbers are somewhat misleading for verbs, where
the mere presence of an object (recorded in the ?with?
column) sufficed to make a selection ?non-default?.
As well, the default selections may indicate that our
disambiguation does not yet make full use of the
distinctions that are available. As we make
improvements in our algorithm, we would expect the
number of default selections to decrease.
Table 2. Comparative Analysis of Features Used in WordNet and NODE Disambiguation
Instance Default Idiom Kind Clue Context Topics Form With As Prefs POS
WordNet
768 556 79 0 0 190 0 0 15 0 1 Adjectives
1754 1140 293 0 0 536 0 0 29 0 0 Nouns
1804 436 161 0 2 576 0 0 984 0 0 Verbs
4326 2132 533 0 2 1302 0 0 1028 0 1 Total
NODE
768 324 81 0 2 249 168 14 11 11 33 Adjectives
1754 456 269 14 94 546 364 317 28 136 3 Nouns
1804 175 105 61 124 564 285 353 573 187 108 Verbs
4326 955 455 75 220 1359 817 684 612 334 144 Total
The significant difference in the number of
default selections between WordNet and NODE is a
broad indicator that there is more information
available in NODE than in WordNet. In examining
the results for individual words, even in cases where
the ?default? (or first) sense was being selected, the
decision was being made in NODE based on positive
information rather than the absence of information.
Generally (but not absolutely), the intent of the
compilers of both WordNet and NODE is that the
first sense correspond to the most frequent sense. The
relative importance of the default sense indicated by
our results suggests the importance of ensuring that
this is the case. In a few instances, the first NODE
sense did not correspond to the first WordNet sense,
and we were able to obtain a much better result
disambiguating in NODE than in WordNet by using
an appropriate mapping from NODE to a second or
third WordNet sense. The significance of the default
sense is important in the selection of instances in an
evaluation such as SENSEVAL; if the instances do
not reflect common usage, WSD results may be
biased simply because of the instance selection.
The ?idiom? column indicates those cases where
a phrasal entry was used to provide the sense
inventory. As pointed out above, these correspond to
the MWUs that were created and account for over
10% of the lexical instances.
The ?kind? and ?clue? columns correspond to
either strong or slightly weaker collocational patterns
that have been associated with individual senses.
These correspond to similarly named sense attributes
used in the Hector database for SENSEVAL-1,
which was the experimental basis for NODE. As can
be seen in the table, these were relevant to the sense
selection for about 6.5 percent of the instances for
NODE. We converted several of WordNet?s verb
frames into clue format; however, they did not show
up as features in our analysis, probably because our
implementation needs to be improved. We expect that
further improvements will obtain some cases where
these are relevant in the WordNet disambiguation (as
well as increasing the number of cases where these
are relevant to NODE senses).
The context column reflects the significance of
Lesk-style information available in the definitions and
examples. In general, it appears that about a third of
the lexical instances were able to use this
information. This reflects the extent to which the
dictionary compilers are able to provide good
examples for the individual senses. Since space is
limited for such examples, our results indicate that
there will an inevitable upper limit of the extent to
which disambiguation can rely on such information
(a conclusion also reached by (Haynes 2001)).
The potential significance of subject or topic
fields associated with individual senses is indicated
by the number of cases where NODE was able to use
this information (nearly 20 percent of the instances).
NODE makes extensive use of subject labels,
particularly in the MRD. We included many subject
labels, usage labels, and register labels in our
WordNet conversion, but these did not surface in our
disambiguation with WordNet. They were very rare
for the lexical items used in SENSEVAL. The value
shown here is similar to the results obtained by
Magnini, et al (2001), but their low recall suggests
that for more common words, there will be a lower
opportunity for their use.
The word form of a lexical item also emerged as
being of some significance when disambiguating with
NODE, slightly over 16 percent. In NODE, this is
captured by such labels as ?often capitalized? or
?often in plural form?. No comparable information is
available in WordNet.
Subcategorization patterns (indicated under the
?with? column) were very important in both
WordNet (based on the verb frames) and NODE,
relevant in 55% and 32% of the sense selections,
respectively. As indicated, the ?with? category is also
important for nouns. For the most part, this indicates
that a given noun sense is usually accompanied by a
noun modifier (e.g., ?metal fatigue?).
The ?as? column corresponds to nouns used as
modifiers, verbs used as adjectives, and adjectives
used as nouns. These were fairly important for nouns
(7.7%) and verbs (10.3%).
The final column, ?prefs?, corresponds to
selectional preferences for verb subjects and objects
and adjective modificands. In these cases, a match
occurred when the head noun in these positions either
matched literally or was a synonym or within two
synsets in the WordNet hierarchy. Although the
results were relatively small, this demonstrates the
viability of using such preferences.
Finally, anomalous entries in the table (e.g.,
nouns having subcategorization patterns used in the
sense selection) generally correspond to our parser
incorrectly assigning a part of speech (i.e., treating
the noun as a verb sense).
4.2 Variation in Disambiguation Features
Space precludes showing the variation in features by
lexical item. The attributes in NODE for individual
items varies considerably and the differences were
reflected in which features emerged as important.
For adjectives, idiomatic usages were significant
for free, green, and natural. Topics were important
for fine, free, green, local, natural, oblique, and
simple, indicating that many senses of these words
have specialized meanings. Form was important for
blind, arising from the collocation ?the blind?. The
default sense was most prominent for colorless,
graceful (with only one sense in NODE), and
solemn. Context was important for blind, cool, fine,
free, green, local, natural, oblique, and simple,
suggesting that these words participate in common
expressions that can be captured well in a few choice
examples. Selectional preferences on the modificands
were useful in several instances.
For nouns, idioms were important for art, bar,
channel, church, circuit, and post. Clues (i.e., strong
collocations) were important for art, bar, chair, grip,
post, and sense. Topics were important for bar,
channel, church, circuit, day, detention, mouth,
nation, post, spade, stress, and yew (even though
yew had only one sense in NODE). Context was
important for art, authority, bar, chair, channel,
child, church, circuit, day, detention, facility,
fatigue, feeling, grip, hearth, lady, material, mouth,
nature, post, and restraint. The presence of
individual lexical items in several of these groupings
shows the richness of variations in characteristics,
particularly into specialized usages and collocations.
For verbs, idioms were important for call, carry,
draw, dress, live, play, pull, turn, wash, and work, a
reflection of the many entries where these words were
paired with a particle. Form was an important feature
for begin (over 50% of the instances), develop, face,
find, leave, match, replace, treat, and work.
Subcategorization patterns were important for all the
verbs. However, many verb senses in both WordNet
and NODE do not show wide variation in their
subcategorization patterns and are insufficient in
themselves to distinguish senses. Strong (?kind?) and
weak (?clue?) collocations are relatively less
important, except for a few verbs (collaborate, serve,
and work). Topics are surprisingly significant for
several verbs (call, carry, develop, dress, drive, find,
play, pull, serve, strike, and train), indicating the
presence of specialized senses. Context does not vary
significantly among the set of verbs, but it is a
feature in one-third of the sense selections. Finally,
selectional preferences on verb subjects and objects
emerged as having some value.
5 Generalizability of Feature Analysis,
Relation to Supervised Learning, and
Implications for Future Studies
The use of feature analysis has advanced our
perception of the disambiguation process. To begin
with, by summarizing the features used in the sense
selection, the technique identifies overall differences
between sense inventories. While our comments have
focused on information available in NODE, they
reflect only what we have implemented. Many
opportunities still exist and the results will help us
identify them.
In developing our feature analysis techniques, we
made lists of features available for the senses of a
given word. This gradually gave rise to the notion of
a ?feature signature? associated with each sense. In
examining the set of definitions for each lexical item,
an immediate question is how the feature signatures
differ from one another. This allows us to focus on
the issue of adequate sense distinctions: what is it
that distinguishes each sense.
The notion of feature signatures also raises the
question of their correspondence to supervised
learning techniques such as the feature selection of
(Mihalcea & Moldovan, 2001) and the decision lists
used in WASPS (Tugwell & Kilgarriff 2001). This
raises the possibility of precompiling a sense
inventory and revising our disambiguation strategy to
identify the characteristics of an instance?s use and
then simply to perform a boolean conjunction to
narrow the set of viable senses.
The use of feature signatures also allows us to
examine our mapping functionality. As indicated
above, we are unable to map 10 percent of the senses
from NODE to WordNet, and of our mappings,
approximately 33 percent have appeared to be
inaccurate when examined by hand. When we
examine the instances where we selected a sense in
NODE, but were unable to map to a WordNet sense,
we can use these instances either to identify clear
cases where there is no WordNet sense.
In connection with the use of supervised learning
techniques, participants of other teams have provided
us with the raw data with which their systems made
their sense selections. The feature arrays from
(Mihalcea & Moldovan, forthcoming) identify many
features in common with our set. For example, they
used the form and part of speech of the target word;
this corresponds to our ?form?. Their collocations,
prepositions after the target word, nouns before and
after, and prepositions before and after correspond to
our idioms, ?clues?, and ?with? features.
The array of grammatical relations used with
WASPS (Tugwell & Kilgarriff,  2001)  (such as
bare-noun, plural, passive, ing-complement, noun-
modifier, PP-comp) correspond to our ?form?,
?clue?, ?with?, and ?as? features.
The data from these teams also identifies bigrams
and other context information. Pedersen (2001) also
provided us with the output of several classification
methods, identifying unigrams and bigrams found to
be significant in sense selection. These data
correspond to our ?context? feature.
We have begun to array all these data by sense,
corresponding to our detailed feature analysis. Our
initial qualitative assessment is that there are strong
correspondences among the different data set. We
will examine these quantitatively to assess the
significance of the various features. In addition, while
several features are already present in WordNet and
NODE, we fully expect that these other results will
help us to identify features that can be added to the
NODE sense inventory.
6 Conclusions
Our analysis has identified many characteristics of
sense distinctions, but indicates many difficulties in
making such distinctions in WordNet (but also
NODE). It is questionable whether WSD has been
fully tested without a carefully drawn sense
inventory. A lexicographically-based sense inventory
shows considerable promise and invites the WSD
community to pool its resources to come up with
such an inventory.
Acknowledgments
I wish to thank Oxford University Press for allowing
me to use their data, and particularly to Rob Scriven,
Judy Pearsall, Glynnis Chantrell, Patrick Hanks,
Catherine Soanes, Angus Stevenson, Adam
Kilgarriff, and James McCracken for their invaluable
discussions, to Rada Mihalcea, Ted Pedersen, and
David Tugwell for making their data available, and
to the anonymous reviewers.
References
Delfs, L. (2001, 6 Sep). Verb keys. (Personal communication)
Haynes, S. (2001, July). Semantic Tagging Using WordNet
Examples. In Association for Computational Linguistics
SIGLEX Workshop (pp. 79-82). Toulouse, France.
Litkowski, K. C. (2002). SENSEVAL Word-Sense
Disambiguation Using a Different Sense Inventory and
Mapping to WordNet (CL Research No. 02-01). .
Damascus, MD.
Litkowski, K. C. (2001, July). Use of Machine Readable
Dictionaries for Word-Sense in SENSEVAL-2. In
Association for Computational Linguistics SIGLEX
Workshop (pp. 107-110). Toulouse, France.
Litkowski, K. C. (1999, June). Towards a Meaning-Full
Comparison of Lexical Resources. In Association for
Computational Linguistics SIGLEX Workshop (pp. 30-7).
College Park, MD.
Magnini, B., Strapparava, C., Pezzulo, G., & Gliozzo, A.
(2001, July). Using Domain Information for Word Sense
Disambiguation. In Association for Computational
Linguistics SIGLEX Workshop (pp. 111-4). Toulouse,
France.
Mihalcea, R., & Moldovan, D. (2001, July). Pattern Learning
and Active Feature Selection for Word Sense
Disambiguation. In Association for Computational
Linguistics SIGLEX Workshop (pp. 127-30). Toulouse,
France.
Mihalcea, R., & Moldovan, D. (Forthcoming). Word Sense
Disambiguation with Pattern Learning and Active Feature
Selection. Journal of Natural Language Engineering.
The New Oxford Dictionary of English. (1998) (J. Pearsall,
Ed.). Oxford: Clarendon Press.
Pedersen, T. (2001, July). Machine Learning with Lexical
Features: The Duluth Approach to SENSEVAL-2. In
Association for Computational Linguistics SIGLEX
Workshop (pp. 139-142). Toulouse, France.
Tugwell, D., & Kilgarriff, A. (2001, July). WASP-Bench: A
Lexicographic Tool Supporting Word Sense
Disambiguation. In Association for Computational
Linguistics SIGLEX Workshop (pp. 151-4). Toulouse,
France.
SENSEVAL-3 TASK
Automatic Labeling of Semantic Roles
Kenneth C. Litkowski
CL Research
9208 Gue Road
Damascus, MD 20872
ken@clres.com
Abstract
The SENSEVAL-3 task to perform automatic
labeling of semantic roles was designed to
encourage research into and use of the FrameNet
dataset. The task was based on the considerable
expansion of the FrameNet data since the
baseline study of automatic labeling of semantic
roles by Gildea and Jurafsky. The FrameNet data
provide an extensive body of ?gold standard?
data that can be used in lexical semantics
research, as the basis for its further exploitation
in NLP applications. Eight teams participated in
the task, with a total of 20 runs. Discussions
among participants during development of the
task and the scoring of their runs contributed to
a successful task. Participants used a wide
variety of techniques, investigating many aspects
of the FrameNet data. They achieved results
showing considerable improvements from Gildea
and Jurafsky?s baseline study. Importantly, their
efforts have contributed considerably to making
the complex FrameNet dataset more accessible.
They have amply demonstrated that FrameNet is
a substantial lexical resource that will permit
extensive further research and exploitation in
NLP applications in the future.
Introduction
Word-sense disambiguation has frequently been
criticized as a task in search of a reason. Since a
considerable portion of a sense inventory has only a
single sense, the question has been raised whether the
amount of effort required by disambiguation is
worthwhile. Heretofore, the focus of disambiguation
has been on the sense inventory and has not examined
the major reason why we would have lexical
knowledge bases: how the meanings would be
represented and thus, available for use in natural
language processing applications. At the present
time, a major paradigm for representing meaning has
emerged in frame semantics, specifically in the
FrameNet project.
A worthy objective for the Senseval community
is the development of a wide range of methods for
automating frame semantics, specifically identifying
and labeling semantic roles in sentences. An
important baseline study of this process has recently
appeared in the literature (Gildea and Jurafsky,
2002). The FrameNet project (Johnson et al, 2003)
has put together a body of hand-labeled data and the
Gildea and Jurafsky study has put together a set of
suitable metrics for evaluating the performance of an
automatic system.
1 The Senseval-3 Task
This Senseval-3 task calls for the development of
systems to meet the same objectives as the Gildea and
Jurafsky study. The data for this task is a sample of
the FrameNet hand-annotated data. Evaluation of
systems is measured using precision and recall of
frame elements and overlap of a system?s frame
element sentence positions with those identified in the
FrameNet data.
The basic task for Senseval-3 is: Given a
sentence, a target word and its frame, identify the
frame elements within that sentence and tag them
with the appropriate frame element name.
The FrameNet project has just released a major
revision (FrameNet 1.1) to its database, with 487
frames using 696 distinctly-named  frame elements
(although it is not guaranteed that frame elements
with the same name have the same meaning). This
release includes 132,968 annotated sentences (mostly
taken from the British National Corpus). The
Senseval-3 task used 8,002 of these sentences
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
selected randomly from 40 frames (also selected
randomly) having at least 370 annotations (out of the
100 frames having the most annotations).1
Participants were provided with a training set
that identified, for each of the 40 frames, the lexical
unit identification number (which equates to a file
name) and a sentence identification name. They were
also provided with the answers, i.e., the frame
element names and their beginning and ending
positions. Since the training set was much larger than
the test set, participants were required to use the
FrameNet 1.1 dataset to obtain the full sentence, its
target word, and the tagged frame elements.
For the test data, participants were provided, for
each frame, with sentence instances that identified the
lexical unit, the lexical unit identification number, the
sentence identification number, the full sentence, and
a specification of the target alng with its start and
end positions.
Participants were required to submit their
answers in a text file, with one answer per line. Each
line was to identify the frame name and the sentence
identifier and then all the frame elements with their
start and end positions that their systems were able to
identify. For example, for the sentence
However, its task is made much more
difficult by the fact that derogations
granted to the Welsh water authority allow
<Agent>it</> to <Target>pump</>
<Fluid>raw sewage</> <Goal>into both
those rivers</>.
the correct answer would appear as follows:
Cause_fluidic_motion.256263 Agent (119,120)
Fluid (130,139) Goal (141,162)
The sentences provided to participants were not
presegmented (as defined in the Gildea and Jurafsky
study); this was left to the participants' systems. The
FrameNet dataset contains considerable information
that was tagged by the FrameNet lexicographers.
Participants could use (and were strongly encouraged
to use) any and all of the FrameNet data in
developing and training their systems. In the test,
participants could use any of this data, but were
strongly encouraged to use only data available in the
sentence itself and in the frame that is identified.
(This corresponds to the ?more difficult task?
identified by Gildea and Jurafsky.) Participants could
submit two runs, one with (non-restrictive case) and
one without (restrictive case) using the additional
data; these were scored separately.
FrameNet recognizes the permissibility of
?conceptually salient? frame elements that have not
been instantiated in a sentence; these are called null
instantiations (see Johnson et al for a fuller
description). An example occurs in the following
sentence (sentID="1087911") from the Motion
frame: ?I went and stood in the sitting room doorway,
but I couldn't get any further -- my legs wouldn't
move.? In this case, the FrameNet taggers considered
the Path frame element to be an indefinite null
instantiation (INI). Frame elements that have been so
designated for a particular sentence appear to be
Core frame elements, but not all core frame elements
missing from a sentence have designated as null
instantiations. The correct answer for this case, based
on the tagging, is as follows:
Motion.1087911 Theme (82,88) Path (0,0)
Participants were instructed to identify null
instantiations in submissions by giving a (0,0) value
for the frame element?s position.2 Participants were
told in the task description that null instantiations
would be analyzed separately.3
For this Senseval task, participants were allowed
to download the training data at any time; the 21-day
1The test set was generated with the Windows-based
program FrameNet Explorer, available at
http://www.clres.com/SensSemRoles.html. FrameNet
Explorer provides several facilities for examining the
FrameNet data: by frame, frame element, and lexical
units. For each unit, a user can explore a frame?s
elements, associated lexical units, frame-to-frame
relations, frame and frame element definitions, lexical
units and their definitions, and all sentences.
2This turned out to be an incorrect method, since some
frame elements (notably ?I? at the beginning of a
sentence) would have a position of (0,0), i.e, the
beginning and ending positions are both 0. Such
instances in the test set were identified and handled
separately to distinguish them from null instantiations.
3No analysis of null instantiations has yet been
performed.
restriction on submission of results after downloading
the training data was waived since this is a new
Senseval task and the dataset is very complex.
Participants could work with the training data as long
as they wished. The 7-day restriction of submitting
results after downloading the test data still applied.
In general, FrameNet frames contain many frame
elements (perhaps an average of 10), most of which
are not instantiated in a given sentence. Systems were
not penalized if they returned more frame elements
than those identified by the FrameNet taggers. For
the 8002 sentences in the test set, only 16212 frame
elements constituted the answer set.
In scoring the runs, each frame element (not a
null instantiation) returned by a system was counted
as an item attempted. If the frame element was one
that had been identified by the FrameNet taggers, the
answer was scored as correct. In addition, however,
the scoring program required that the frame
boundaries identified by the system?s answer had to
overlap with the boundaries identified by FrameNet.
An additional measure of system performance was
the degree of overlap. If a system?s answer coincided
exactly to FrameNet?s start and end position, the
system received an overlap score of 1.0. If not, the
overlap score was the number of characters
overlapping divided by the length of the FrameNet
start and end positions (i.e., end-start+1)4
The number attempted was the number of non-
null frame elements generated by a system. Precision
was computed as the number of correct answers
divided by the number attempted. Recall was
computed as the number of correct answers divided
by the number of frame elements in the test set.
Overlap was the average overlap of all correct
answers. The percent Attempted was the number of
frame elements generated divided by the number of
frame elements in the test set, multiplied by 100. If a
system returned frame elements not identified in the
test set, its precision would be lower.
2 Results
Eight teams submitted 20 runs. Three teams
submitted runs only for the restricted case (no prior
knowledge about frame boundaries). The other five
teams submitted at least two runs, with one team
submitting 8 runs and another submitting 4 runs.
Four of these five teams submitted a restricted run
and an unrestricted run (frame boundaries were
identified, i.e., the task was a classification task of
identifying the applicable frame element).
The results for the classification task are shown
in Table 1. The average precision over all these runs
is 0.803 and the average recall is 0.757. The overlap
in each run is almost identical to the precision, and
differs slightly because there may have been some
slight positional errors in either the FrameNet data or
the sentence string provided in the test data.
Table 1. System Performance (Unrestricted)
Run Prec Over Rec Att
01b (HKPolyU) 0.874 0.873 0.867 99.2
01c (HKPolyU) 0.905 0.904 0.846 93.5
01d (HKPolyU) 0.859 0.858 0.852 99.2
01e (HKPolyU) 0.902 0.901 0.849 94.1
01f (HKPolyU) 0.908 0.907 0.846 93.2
01g1 (HKPolyU) 0.819 0.817 0.812 99.2
01g2 (HKPolyU) 0.819 0.817 0.812 99.2
01h (HKPolyU) 0.926 0.925 0.705 76.1
02b (InfoSciInst) 0.867 0.866 0.858 99.0
04b (UTDMorarescu) 0.946 0.946 0.907 95.8
07a (UTDMoldovan) 0.898 0.897 0.839 93.4
08b (UUtah) 0.728 0.725 0.721 99.1
08c (UUtah) 0.858 0.857 0.849 98.9
The results for the restricted case are shown in
Table 2. The average precision over all these runs is
0.595 and the average recall is 0.481. The average
overlap is noticeably lower than the precision,
indicating the additional difficulty for these runs of
identifying the frame element boundaries.
Table 2. System Performance (Restricted)
Run Prec Over Rec Att
02a (InfoSciInst) 0.802 0.784 0.654 81.5
03 (CLResearch) 0.583 0.480 0.111 19.0
04a (UTDMorarescu) 0.899 0.882 0.772 85.9
05a (USaarland) 0.654 0.602 0.471 72.0
05b (USaarland) 0.736 0.675 0.594 80.7
06 (UAmsterdam) 0.869 0.847 0.752 86.4
07b (UTDMoldovan) 0.807 0.777 0.780 96.7
08a (UUtah) 0.355 0.255 0.453 127.9
08e (UUtah) 0.387 0.295 0.335 86.74Hence the problem with an element having (0,0) as
the start and end positions.
In both cases, the percent attempted is quite high,
except for one system in the restricted runs.  This
indicates that systems were able to identify potential
frame elements in quite a large percentage of the
cases. Systems were allowed to return any number of
frame elements for a sentence and it is possible for a
system to identify more frame elements than were
identified by the FrameNet taggers. For example, run
08a asserted many more frame elements than were
identified in the answer key. As a result, its percent
attempted was much higher than 100 percent. The
number of frame elements in other runs not identified
in the answer key is unknown. The effect of a higher
number attempted lowers the precision for a run and
increases the percent attempted.
3 Discussion
Overall, the results achieved in this SENSEVAL-3
task were quite high. Several teams achieved results
much better than those obtained by Gildea and
Jurafsky. The average precision of 0.80 for all runs
in the unrestricted case  is only slightly lower than the
82% accuracy achieved in that study when using
presegmented constituents. Many teams achieved
precision at or above 0.90, indicating that their
routines for classifying constituents is quite good. In
view of the fact that the number of frames and frame
elements in FrameNet has expanded considerably
since the Gildea and Jurafsky study, it appears that
the methods employed have become quite accurate in
classifying constituents.5
Results for the restricted were also quite good in
comparison with the Gildea and Jurafsky study,
which achieved 65% precision and 61% recall at the
?more difficult task of simultaneously segmenting
constituents and identifying their semantic role.? In
this task, four teams achieved results between 80 and
90 percent for precision and between 65 and 78
percent for recall.
The participants in this task used a wide variety
of methods and data in their systems. In addition,
they used the FrameNet dataset from a wide diversity
of perspectives. In some cases, they developed
mechanisms for grouping the FrameNet data by part
of speech or making use of the nascent inheritance
hierarchy in FrameNet. In some cases, they used all
frames as a basis for training and in others, they
employed novel grouping methods based on the
similarities among different frames.
The successes of many teams seems to indicate
that the FrameNet dataset is an excellent lexical
resource and that the resources devoted to its
development have been quite valuable. The collective
efforts of the participants have contributed greatly to
making this complex database more accessible and
more amenable to even further development, not only
for research purposes, but also for use in many NLP
applications.
References
Gildea, Daniel, and Daniel Jurafsky. Automatic Labeling
of Semantic Roles. Computational Linguistics, 28 (3),
245-288.
Johnson, Christopher; Miriam Petruck, Collin Baker,
Michael Ellsworth, Josef Ruppenhofer, and Charles
Fillmore, (2003). FrameNet: Theory and Practice.
Berkeley, California.
5The diversity of frame elements in the test data has
not yet been investigated, so the assertion that this task
is more difficult is based solely on the general
expansion of FrameNet.
SENSEVAL-3 TASK
Word-Sense Disambiguation of WordNet Glosses
Kenneth C. Litkowski
CL Research
9208 Gue Road
Damascus, MD 20872
ken@clres.com
Abstract
The SENSEVAL-3 task to perform word-sense
disambiguation of WordNet glosses was
designed to encourage development of technology
to make use of standard lexical resources. The
task was based on the availability of sense-
disambiguated hand-tagged glosses created in the
eXtended WordNet project. The hand-tagged
glosses provided a ?gold standard? for judging
the performance of automated disambiguation
systems. Seven teams participated in the task,
with a total of 10 runs. Scoring these runs as an
?all-words? task, along with considerable
discussions among participants, provided more
insights than just the underlying technology. The
task identified several issues about the nature of
the WordNet sense inventory and the underlying
use of wordnet design principles, particularly the
significance of WordNet-style relations.
Introduction
In SENSEVAL-2, performance in the lexical sample
task dropped considerably (Kilgarriff, 2001).
Kilgarriff suggested that using WordNet (Fellbaum,
1998) for SENSEVAL has drawbacks. WordNet was
not designed to serve as a lexical resource, but its
public availability and reasonable comprehensiveness
have been dominant factors in its selection as the
lexical resource of choice for Senseval and for many
applications. These factors have led to further
funding by U.S. government agencies and many
improvements are currently underway. Among these
improvements is a planned hand-tagging of the
WordNet glosses with their WordNet senses. At the
same time, sense-tagging of the glosses is being
performed in the Extended WordNet (XWN) project
under development at the University of Texas at
Dallas (Mihalcea and Moldovan, 2001)1. The XWN
project also parses the WordNet glosses into a part of
speech tree and transforms them into a logical
predicate form.
More generally, sense disambiguation of
definitions in any lexical resource is an important
objective in the language engineering community.
The first significant disambiguation of dictionary
definitions and creation of a hierarchy took place 25
years ago in the groundbreaking work of Amsler
(1980). However, while substantial research has been
performed on machine-readable dictionaries since
that time, technology has not yet been developed to
make systematic use of these resources. This
SENSEVAL task was designed to encourage the
lexical research community to take up the challenge
of disambiguating dictionary definitions.
XWN is used as a core knowledge base for
applications such as question answering, information
retrieval, information extraction, summarization,
natural language generation, inferences, and other
knowledge intensive applications. The glosses contain
a part of the world knowledge since they define the
most common concepts of the English language. In
the XWN project, many open-class words in
WordNet glosses have been hand-tagged and provide
a ?gold standard? against which disambiguation
systems can be judged. The SENSEVAL-3 task is to
replicate the hand-tagged results.
The Extended WordNet (XWN) project has
disambiguated the content words (nouns, verbs,
adjectives, and adverbs) of all glosses, combining
human annotation and automated methods using
WordNet 1.7.1. A ?quality? attribute was given to
each lemma. XWN used two automatic systems to
disambiguate the content words. When the two
systems agreed, the lemma was given a ?silver?
1http://www.hlt.utdallas.edu/
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
quality. Otherwise, a lemma was given a ?normal?
quality (even when there was only one sense in
WordNet). In a complex process described in more
detail below, certain glosses or lemmas were selected
for hand annotation. Lemmas which were hand-
tagged were given a ?gold? tag.
The WordNet 1.7.1 data were next converted to
use WordNet 2.0 glosses. Word senses have been
assigned to 630,599 open class words, with 15,179
(less than 2.5 percent) of the open-class words in
these glosses assigned manually. Many glosses have
more than one word given a ?gold? assignment.  The
resultant test set provided to participants consists of
9,257 glosses, containing 15,179 ?gold? tagged
content words and a total of 42,491 content words,
distributed as follows:
Table 1. Gloss Test Set
POS Glosses Golds Words
Adjective 94 263 370
Adverb 1684 1826 3719
Noun 6706 10985 35539
Verb 773 2105 2863
Total 9257 15179 42491
The disambiguations (and hence the answer key) are
available at the XWN web site. Participants were
encouraged to investigate the XWN data as well as
the methods followed by the XWN team. However,
participants were expected to develop their own
systems, for comparison with the XWN manual
annotations.
1 The Senseval-3 Task
Participants were provided with all glosses from
WordNet in which at least one open-class word was
given a "Gold" quality assignment. These glosses
were provided in an XML file, each with its WordNet
synset number, its part of speech, and the gloss itself.
Glosses frequently include sample uses. The samples
uses were not parsed in the XWN project and were
not to be included in the submissions.
The task was configured as essentially identical
to the SENSEVAL-2 and SENSEVAL-3 "all-words"
tasks, except without any context and with the gloss
not constituting a complete sentence. Unlike the
all-words task, individual tokens to be disambiguated
were not identified, so that participants were required
to perform their own tokenization and identification
of multiword units. The number of words in a gloss
is quite small, but a few glosses do contain the same
word more than once. Participants were encouraged
to consider a synset's placement within WordNet (its
hypernyms, hyponyms, and other relations) to assist
in disambiguation. The XWN data contains part of
speech tags for each word in the glosses, as well as
parses and logical forms, which participants were
allowed to use. Most of the glosses in the test set
have hand-tagged words as well as words tagged by
the automatic XWN systems. The senses assigned to
other open-class words have a tag of ?silver? or
?normal?. In submitting test runs, participants did not
know which of the words had been assigned a ?gold?
quality, but were only scored for the ?gold? quality
words.2
No training data was available for this task since
the number of items in the test set was so small.
Participants were encouraged to become familiar
with the XWN dataset and to make use of it in ways
that would not compromise their performance of the
task.
2 Submissions
Seven teams participated in the task with one team
submitting two runs and one team submitting three
runs. A submission contained an identifier (the part
of speech of the gloss and its synset number) and a
WordNet sense for each content word or phrase
identified by the system. The answer key contains
part of speech/synset identifier, the XWN quality
assignment, the lemma and the word form from the
XWN data, and the WordNet sense. The scoring
program (a Perl script) stored the answers in three
hashes according to quality (?gold?, ?silver?, and
?normal?) and then also stored the system?s answers
in a hash. The program then proceeded through the
?gold? answers and determined if a system?s answers
included a match for that answer, equaling either the
(lemma, sense) or (word form, sense). No system
submitted more than one sense for each of its word
forms. An exact match received a score of 1.0. If a
2The answer key contains all assignments, so it is
possible that runs can be analyzed with these other
sense assignments with a voting system. However,
such an analysis has not yet been performed.
system returned either the lemma or the word form,
but had assigned an incorrect sense, the item was
counted as attempted.
Precision was computed as the number correct
divided by the number attempted. Recall was
computed as the number correct divided by the total
number of ?gold? items. The percent attempted was
computed as the number attempted divided by the
total number of ?gold? items. Results for all runs are
shown in Table 2.
Table 2. System Performance (All Items)
Run Prec Rec Att
01 (UPolit?cnica de Valencia) 0.534 0.405 76.0
02 (CL Research) 0.449 0.345 76.8
03 (LanguageComputerCorp) 0.701 0.504 71.9
04a (TALP Research Center) 0.686 0.683 99.5
04b (TALP Research Center) 0.574 0.558 97.2
05 (IRIT-ERSS) 0.388 0.385 99.1
06a (Uni-Roma1-DI) 0.777 0.311 40.0
06b (Uni-Roma1-DI) 0.668 0.667 99.9
06c (Uni-Roma1-DI) 0.716 0.362 50.5
07 (Indian Inst Technology) 0.343 0.301 87.8
Systems 04a and 06b used the part of speech tags
available in the XWN files, while the other runs did
not.
3 Discussion
During discussions on the SENSEVAL-3 mailing list
and in interchanges assessing the scoring of the
systems, several issues of some importance arose.
Most of these concerned the nature of the XWN
annotation process and the ?correctness? of the
?gold? quality assignments.
Since glosses (or definitions) are only ?sentence?
fragments, parsing them poses some inherent
difficulties. In theory, a proper lexicographically-
based definition is one that contains a genus term
(hypernym or superordinate) and differentiae. A
gloss? hypernym is somewhat easily identified as the
head of the first phrase, particularly in noun and verb
definitions. Since most WordNet synsets have a
hypernym, a heuristic for disambiguating the head of
the first phrase would be to use the hypernym as the
proper disambiguated sense. And, indeed, the
instructions for the task encouraged participants to
make use of WordNet relations in their
disambiguation.
However, the XWN annotators were not given
this heuristic, but rather were presented with the set
of WordNet senses without awareness of the
WordNet relations. As a result, many glosses had
?gold? assignments that seemed incorrect when
considering WordNet?s own hierarchy. For example,
naught is defined as ?complete failure?; in WordNet,
its hypernym failure is sense 1 (?an act that fails?),
but the XWN annotators tagged it with sense 2 (?an
event that does not accomplish its intended
purpose?).
To investigate the use of WordNet relations
heuristics, we considered a set of 313 glosses
containing 867 ?gold? assignments which team 06
submitted as highly reliant on these relations. As
shown in Table 3 (scored on 8944 glosses with
14312 ?gold? assignments), precision scores changed
most for 03 (0.020), 06b (0.017), and 04a (0.016);
these runs had correspondingly much lower scores
for the 313 glosses in this set (results not shown).
These differences do not appear to be significant. A
more complete assessment of the significance of
WordNet relations in disambiguation would require
a more complete identification of glosses where
systems relied on such information.
Table 3. System Performance (Reduced Set)
Run Prec Rec Att
01 (UPolit?cnica de Valencia) 0.538 0.407 75.6
02 (CL Research) 0.446 0.342 76.6
03 (LanguageComputerCorp) 0.721 0.516 71.6
04a (TALP Research Center) 0.702 0.698 99.5
04b (TALP Research Center) 0.585 0.568 97.2
05 (IRIT-ERSS) 0.395 0.391 99.1
06a (Uni-Roma1-DI) 0.826 0.323 39.1
06b (Uni-Roma1-DI) 0.685 0.684 99.9
06c (Uni-Roma1-DI) 0.753 0.375 49.7
07 (Indian Inst Technology) 0.346 0.302 87.2
Further discussion with members of the XWN
project about the annotation process revealed some
factors that should be taken into account when
assessing the various systems? performances. Firstly,
the annotations of the 9257 glosses with ?gold?
assignments were annotated using three different
methods. The first group of 1032 glosses were fully
hand-tagged by two graduate students, with 80
percent agreement and with the project leader
choosing a sense when there was disagreement.
For the remaining glosses in WordNet, two
automated disambiguation programs were run. When
both programs agreed on a sense, they were given a
?silver? quality. In those glosses for which all but one
or two words had been assigned a ?silver? quality,
the one or two words were hand-tagged by a graduate
student, without any interannotator check or review.
There are 4077 noun glosses in this second set.
A third set, the remaining 4738 among the test
set, were glosses for which all the words but one had
been assigned a ?silver? quality. The single word was
then hand-tagged by a graduate student, and in some
cases by the project leader (particularly when a word
had been mistagged by the Brill tagger).
To assess the effect of these three different styles
of annotation, we ran the scoring program, restricting
the items scored to those in each of the three
annotation sets. The scores were changed much more
significantly for the various teams for the different
sets. For the first set, precision was down
approximately 0.07 for three runs, with much lower
changes for the other runs. For the second set,
precision was up approximately 0.075 for two runs,
down approximately 0.08 for two runs, and relatively
unchanged for the remaining runs. For the third set,
there was relatively little changes in the precision for
all runs (with a maximum change of 0.03).
4 Conclusions
The underlying guidance for this SENSEVAL-3 task
that, in the absence of significant context,
participants make use of WordNet relations for
disambiguating glosses has led to some significant
insights about the use and importance of wordnets.
These insights emerge from the tension between the
reliance on WordNet relations and the imprecision of
the tagging process.
Many investigators, including several of the
participants in this task, are attempting to exploit the
kinds of relations between lexical entries that are
embodied in WordNet. The use of wordnets in NLP
applications has become an important basic construct
and increasingly valuable.  However, the construction
of wordnets is expensive and time-consuming, and
without any significant prospects for commercial
support. While some dictionary publishers are
increasingly incorporating wordnet principles into
their lexical resources, this process is slow. At
present, the publicly available WordNet remains the
wordnet of choice.
The annotation process followed by the XWN
project, with the taggings used in this task, has again
indicated difficulties with the WordNet sense
inventory. The fact remains that WordNet has not
had the benefit of sufficient lexicographic resources
in the construction of its glosses and in the
acquisition of other lexicographic information in its
entries. The WordNet project continues its efforts to
add information, but with limited resources.
With the diverse set of approaches represented by
the participants in this task, it is possible to envision
sets of steps that might be employed to improve the
details of the WordNet sense inventory. One step
would include continued hand-tagging of WordNet
glosses without consideration of WordNet relations.
Another step would be the use of automated
disambiguation routines to act as checks on
consistency. Such systems would include those that
rely on WordNet relations as well as those that do
not, acting as checks on one another.
References
Amsler, Robert A. 1980. The Structure of the Merriam-
Webster Pocket Dictionary. Ph.D. Thesis., Austin:
University of Texas.
Fellbaum, Christiane (ed.). 1998. WordNet: An
Electronic Lexical Database. The MIT Press:
Cambridge, MA.
Kilgarriff, Adam. 2001. English Lexical Sample Task
Description. In Proceedings of SENSEVAL-2: Second
International Workshop on Evaluating Word Sense
Disambiguation Systems, Toulouse, France.
Mihalcea, Rada and Dan Moldovan. 2001. eXtended
WordNet: Progress Report. In Proceedings of NAACL
Workshop on WordNet and Other Lexical Resources,
Pittsburgh, PA.
Explorations in Disambiguation Using XML Text Representation
Kenneth C. Litkowski
CL Research
9208 Gue Road
Damascus, MD 20872
ken@clres.com
Abstract
In SENSEVAL-3, CL Research participated in
four tasks: English all-words, English lexical
sample, disambiguation of WordNet glosses, and
automatic labeling of semantic roles. This
participation was performed within the
development of CL Research?s Knowledge
Management System, which massively tags texts
with syntactic, semantic, and discourse
characterizations and attributes. This System is
fully integrated with CL Research?s DIMAP
dictionary maintenance software, which provides
access to one or more dictionaries for
disambiguation and representation. Our core
disambiguation functionality, unchanged since
SENSEVAL-2, performed at a level comparable
to our previous performance. Our participation
in the SENSEVAL-3 tasks was concerned
primarily with text processing and representation
issues and did not advance our disambiguation
capabilities.
Introduction
CL Research participated in four SENSEVAL-3
tasks: English all-words, English lexical sample,
disambiguation of WordNet glosses, and automatic
labeling of semantic roles. We also ran the latter two
tasks, but since their test sets were generated blindly,
our results did not involve use of any prior
information.
Our participation in these tasks is a continuation
and extension of our efforts to perform NLP tasks
within an integrated text processing system known as
the Knowledge Management System (KMS). KMS
parses and processes text into an XML representation
tagged with syntactic, semantic, and discourse
properties. This representation is then used for such
tasks as question answering and text summarization
(Litkowski, 2004a; Litkowski, 2004b).
The SENSEVAL-3 tasks were performed as part
of CL Research?s efforts to extend and improve the
semantic characterizations in the KMS XML
representations. For each SENSEVAL-3 task, the
corresponding texts in the test sets were processed
using the general KMS functionality. However, since
the texts involved in the SENSEVAL tasks were
quite small, the amount of processing was quite
minimal. The descriptions below focus on the
integration of disambiguation technology in a larger
system and do not present any advancements in this
technology.
1 The SENSEVAL-3 All-Words Task
Our procedures for performing this task and our
results were largely unchanged from SENSEVAL-2
(Litkowski, 2001; Litkowski, 2002). Our system is
unsupervised, instead relying on information in
whatever dictionary is being used to disambiguate the
words. In this case, as in SENSEVAL-2, WordNet
1.7.1 was used.
The main types of information used are default
sense selection, idiomatic usage, syntactic and
semantic clues, subcategorization patterns, word
forms, syntactic usage, context, and topics or subject
fields. As pointed out in Litkowski (2002), the
amount of information available in WordNet is
problematic. Additional information suitable for
disambiguation is available in WordNet 2.0, but we
were unable to test the effect of the changes, even
though we could have easily switched our system to
use this later version.
In performing this task, we spent some time
cleaning the text files, removing extraneous material
and creating a more natural text file (e.g., joining
contractions). Use of a preprocessed file is somewhat
difficult. Since some tokens to be disambiguated were
unnatural (e.g., ?that?s? broken into two tokens, with
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
only the ??s? to be disambiguated), this affected the
quality of our parse output.
After removing extraneous material, KMS parsed
and processed the XML source file, treating the text
in its ordinary manner. The first step of KMS
involves splitting a text into sentences and then
parsing each sentence. To customize KMS for this
task, we had to create a list of tokens, advancing
through this list in concert with the parse output. This
process was different from the normal processing of
KMS where every word is disambiguated in an
integrated fashion. Our results are shown in Table 1,
broken down by part of speech as indicated in the
answer key.
Table 1. All-Words Results
Run Items Precision
Nouns 895 0.523
Verbs 731 0.361
Adjectives 346 0.413
Adverbs 13 0.077
Hyphenated/U 56 0.179
Total 2041 0.434
These results are similar to our performance in
Senseval-2, where our precision was 0.451. Our
recall is the same, since we attempted each item.
As indicated, several factors degraded our
performance, primarily the quality of the information
available in the dictionary used for disambiguation.
We have not attempted to optimize our system for
WordNet, but rather emphasize use of
lexicographically-based dictionaries. KMS can use
several dictionaries at the same time, and the
additional effort to disambiguate against several
sense inventories at the same time is not demanding.
Our system?s performance was also degraded by
a difficulty in advancing through the token list, so
that we did not return a sense for 305 items (some of
which were due to our parser?s performance). We
also did not deal properly with the adverbs (most of
which were adverbial phrases) and hyphenated words
(which we learned about only after downloading the
test set).
As indicated in Table 1, our system?s
performance was lowest for verbs. We believe, based
on our earlier studies, that this lower score is affected
by the WordNet verb sense inventory.
2 The SENSEVAL-3 Lexical Sample Task
Disambiguation for the lexical sample task is quite
similar to that used for the all-words task. The effort
is somewhat easier in preparation, since the text for
each instance is generally in a form that has not been
preprocessed to an extensive degree. Each instance in
the test set generally consisted of a paragraph which
could be processed immediately within KMS. It was
only necessary to modify KMS in a minor way to
recognize and keep track of the target word to be
disambiguated.
The major difference in the SENSEVAL-3 task
from SENSEVAL-2 is the sense inventory. WordNet
1.7.1 was used for nouns and adjective, while
Wordsymth provided the verb senses. As indicated
above, we were able to use WordNet immediately.
For the Wordsmyth sense inventory, we had to
create a new dictionary with CL Research?s DIMAP
dictionary maintenance software. The Wordsmyth
definitions were very uncomplicated, and we were
able to create this dictionary quickly after
downloading the task training data. On the other
hand, the Wordsmyth data is not as rich as would be
found in ordinary dictionaries, particularly the
machine-readable versions of these dictionaries.
Nonetheless, we analyzed the dictionary data to
extract nuggets of information about each sense. This
included creation of synsets (as in WordNet),
identification of the definition proper, creation of
examples where provided, identification of ?clues?
(e.g., ?followed by ?to??), identification of typical
subjects and objects, and identification of a sense?s
topical area. We also used the online version of
Wordsmyth to identify the transitivity of each sense.
We ran our system first on the trial data and
obtained the results shown in Table 2, essentially
using the identical disambiguation routines developed
for SENSEVAL-2. We intended to use the training
data, not for use as in supervised systems, but to
analyze our results using methods we had established
for identifying factors significant in disambiguation
(Litkowski, 2002). We also briefly investigated the
value of using (1) the topical area characterization of
preceding sentences, (2) WordNet relations among
words in the sentences (including the target), and (3)
prepositions following the target in examples. Our
investigations indicated that only negligible changes
would occur from these possibilities.
Table 2. Lexical Sample Recall (Training)
Run Items Fine Coarse
Adjectives 314 0.382 0.516
Nouns 3593 0.490 0.561
Verbs 3961 0.409 0.525
Total 7868 0.445 0.541
We compared the results from the training data
with our performance in SENSEVAL-2 (Litkowski,
2001). In all categories, the recall was considerably
improved, on average about 0.15. This suggests that
the lexical sample task for SENSEVAL-3 is much
easier. The improvement was relatively greater for
verbs, suggesting that the sense inventory for
Wordsmyth is much closer to what might be found in
ordinary dictionaries.
As a result of these preliminary investigations,
we did not further modify our system for the test run.
Our results for the test data are shown in Table 3. As
is clear, the results are nearly identical with the test
data. These patterns also hold for the individual
lexical items (not shown), where there is much more
variation in performance. The major reason for the
variations appears to lie primarily in the ordering of
the senses in the dictionaries. In other words, the
sense inventories provide little discriminating
information, with the result that sense selection is
primarily to the default first sense. This indicates that
the sense inventories do not reflect the frequencies in
the training and test data.
Table 3. Lexical Sample Recall (Test)
Run Items Fine Coarse
Adjectives 159 0.409 0.503
Nouns 1806 0.488 0.576
Verbs 1977 0.419 0.540
Total 3942 0.450 0.555
3 Disambiguation of WordNet Glosses
The SENSEVAL-3 task to disambiguate content
words in WordNet glosses was a slight modification
of the all-words task. One main difference was that
tokens to be disambiguated were not identified,
requiring the systems to identify content words and
phrases. Content words were considered to be any of
the four major parts of speech, i.e., words or phrases
that could be found in WordNet. Another major
difference was that minimal context was provided,
i.e., only the gloss itself (although examples were
also available). The WordNet synset was also given,
providing some ?context? within the WordNet
network of synsets.
This task had no training data, but only test data
based on the tagging of content words by the
eXtended WordNet (XWN) project (Mihalcea and
Moldovan, 2001). The test data consisted of only and
all those glosses from WordNet for which one or
more word forms (a single word or a multiword unit)
had received a ?gold? quality WordNet sense
assignment. Scoring for this task is based only on a
system?s performance in assigning a sense to these
word forms. The test set consisted of 9257 glosses
containing 15179 ?gold? assignments (out of 42491
word forms in these glosses).
To perform this task1, we used KMS to process
each gloss (treated by KMS as a ?text?). Each gloss
was parsed and processed and converted into an
XML representation. (No gloss was a sentence, so
each parse was ?degenerate? in that only sentence
fragments were identified.)
KMS has only recently been modified to
incorporate ?all-words? disambiguation in the XML
representation. At present, the disambiguation has
only been partially implemented. One aspect still in
development is a determination of exactly which
items in the representation should be given a
disambiguation and represented (e.g., exactly how to
treat multiword units or verbs with particles). Also,
we have not yet integrated the full disambiguation
machinery (as used in the all-words and lexical
sample tasks) into KMS. As a result, only the first
(or default) sense of a word is selected.
CL Research?s DIMAP dictionary software
includes considerable functionality to parse and
analyze dictionary definitions. Part of the analysis
functionality makes use of WordNet relations in
order to propagate information to features associated
with a sense. CL Research has previously parsed
WordNet glosses as part of an investigation into
1Note that, although CL Research ran this task, and we
had access to the test data beforehand, we did not
actually work with the data until the date indicated for
other participants to download and work with the data
prior to submission. In any event, our participation in
this task was primarily to investigate the parsing and
processing of sentence fragments in KMS.
WordNet?s internal consistency. However, we did not
incorporate any of this experience in performing this
task. We also did not incorporate any routines that
make use of WordNet relations for disambiguation
(as enabled by identification of the WordNet synset
identifier). Determining the extent to which these
functionalities are relevant for KMS is a matter for
future investigation.
Our performance for this task reflects our
somewhat limited implementation, as shown in Table
4. Among 10 participating runs, our precision was
the second lowest and our recall was the third lowest.
We were only able to identify 76.8 percent of the test
items with our current implementation. However, in
comparing our results with our performance in the
all-words and lexical sample tasks, the results here
are not significantly different. Moreover, these results
suggest a minimum that might be obtained with a
disambiguation system that relies only on picking the
first sense.
Table 4. Disambiguation of WordNet Glosses
Items Precision Recall
?Gold? words 15179 0.449 0.345
4 Automatic Labeling of Semantic Roles
The SENSEVAL-3 task to label sentence
constituents with semantic roles was designed to
replicate the tagging and identification of frame
elements performed in the FrameNet project (Johnson
et al, 2003). This task was modeled on the study of
automatic labeling by Gildea & Jurafsky (2002), to
allow other participants to investigate methods for
assigning semantic roles. That study was based on
FrameNet 1.0, whereas this task used data from
FrameNet 1.1, which considerably expanded the
number of frames and the corpus sentences that were
tagged by FrameNet lexicographers.
The test data for this task consisted of 200
sentences that had been labeled with frame elements
for 40 different frames. Participants were provided
with the sentences, the target word (along with its
beginning and ending positions in the sentence), and
the frame name (i.e., no attempt was made to
determine the applicable frame). Specific training
data for the task consisted of all sentences not in the
test set for the individual frame (ranging from slightly
fewer than 200 sentences to as many as 1500
sentences). In addition, participants could use the
remainder of the FrameNet corpus for training
purposes (another 447 frames and nearly 133,000
sentences). Participants could submit two types of
runs: unrestricted (in which frame element
boundaries, but not frame element names, could be
used, i.e., essentially a classification task) and
restricted (in which these boundaries could not be
used, i.e., the more difficult task of segmenting
constituents and identifying their semantic role). CL
Research submitted only one run, for the restricted
task.
To perform this task2, we used KMS to parse and
process the sentences (where each sentence was
treated as a ?text?). We made a slight modification to
our system to enable to identify the applicable frame
and to keep track of the target word. We also created
a special dictionary for FrameNet frames. This
dictionary was put into an XML file and consisted
only of the frame name, the frame elements, the type
of frame element (a classification used by FrameNet
as ?core?, ?peripheral?, or ?extra-thematic?), and a
characterization or ?definition? of the frame element.
?Definitions? of frame elements were written as
specifications for the type of syntactic constituent
that was expected to instantiate a frame element in a
sentence. Thus, for frames usually associated with
verbs, a specification for a frame element might be
?subject? or ?object?. More generally, many frame
elements specified ?prepositional phrases? headed by
one of a set of prepositions (such as ?about? or
?with?). The basic structure of the FrameNet
dictionary was created automatically. The
specifications for each frame element was created
manually after inspecting the training set for only the
40 frames in the task (which we had processed to
show what frame elements had been identified for
2Note that, again, although CL Research ran this task,
and we had access to the test data beforehand, we did
not actually work with the data until the date indicated
for other participants to download and work with the
data prior to submission. We used only the training
data for development of our system. Our participation
in this task was exploratory in nature, designed to
examine the feasibility and issues involved in
integrating frame semantics into KMS. This involves
development of processing routines and examination of
methods for including frame elements in our XML
representation.
each sentence).
To process the test data and create answers, we
first parsed and processed each sentence with KMS
to create an XML representation using the full set of
tags and attributes normally generated. Then, we
used the applicable FrameNet ?definition? for the
frame, the XML representation of the sentence, and
the identification of the target word. We iterated
through the frame elements and if we had a
specification for that element, we used this
specification to create an XPath expression used to
query the XML representation of the sentence to
determine if the sentence contained a constituent of
the desired type. If a frame element was labeled as a
?core? element for the frame, but no constituent was
identified, KMS treated this a ?null? instantiation
(i.e., a situation where linguistic principles allow
frame elements to be omitted within a sentence). Each
frame element identified in the sentence was
appended to a growing list and the full list was
returned as the set of labeled semantic roles for the
sentence.
Our results for this task are shown in Table 5.
Precision and recall reflect standard measures of how
well we were able to identify frame elements. The
low recall is a reflection of the small percentage of
items attempted. The overlap indicates how well we
were able to identify the beginning and ending
positions of the constituents we identified.
Table 5. Automatic Labeling of Semantic Roles 
Items Precision Overlap Recall Attempted
16279 0.583 0.480 0.111 19.0
Our poor results stem in large part from only a
cursory development of our FrameNet dictionary. We
only created substantial entries for 16 of the 40
frames, minimal entries for another 11, and no
detailed specifications at all for the remaining 13.
The minimal entries were created on the basis of
frame elements with the same name (such as time,
manner, and duration), which appear in more than
one frame. In addition, our method of specification is
still somewhat limiting. For example, in frames
associated with both nouns and verbs, our method
only permitted us to specify the subject or object for
a verb and not also a prepositional phrase following
a noun. Another deficiency of our system was seen in
cases where a long constituent (such as a noun phrase
with multiple attached prepositional phrases) was
required. Notwithstanding, with only a limited time
for development, we able to obtain substantial
results, suggesting that simple methods may plausibly
be used for a large percentage of cases.
It appears that most participants in this task used
statistical methods in training their systems and
achieved results better than those obtained by Gildea
& Jurafsky. It is possible that these improved results
stem from the much larger corpus available in
FrameNet 1.1. These results suggest the possibility
that it may be feasible and more appropriate to
include statistical bases for identifying frame
elements in KMS.
Conclusions
In participating in four tasks of SENSEVAL-3, we
examined several aspects of disambiguation within
the framework of massive tagging of text with
syntactic, semantic, and discourse characterizations
and attributes. We established basic mechanisms for
integrating disambiguation and representational
procedures into a larger text processing and analysis
system. Our results further demonstrated difficulties
in using the WordNet sense inventory, but have
further illuminated a number of important issues in
disambiguation and representation. At the same time,
we have identified a significant number of
shortcomings in our system, but with considerable
opportunities for further refinement and development.
References
Gildea, Daniel, and Daniel Jurafsky. Automatic Labeling
of Semantic Roles. Computational Linguistics, 28 (3),
245-288.
Johnson, Christopher; Miriam Petruck, Collin Baker,
Michael Ellsworth, Josef Ruppenhofer, and Charles
Fillmore, (2003). FrameNet: Theory and Practice.
Berkeley, California.
Litkowski, K. C. (2001, 5-6 July). Use of Machine-
Readable Dictionaries for Word-Sense Disambiguation
in SENSEVAL-2. Proceedings of SENSEVAL-2: 2nd
International Workshop on Evaluating Word Sense
Disambiguation Systems. Toulouse, France, pp. 107-
110.
Litkowski, K. C. (2002, 11 July). Sense Information for
Disambiguation: Confluence of Supervised and
Unsupervised Methods. Word Sense Disambiguation:
Recent Successes and Future Directions. Philadelphia,
PA, pp. 47-53.
Litkowski, Kenneth. C. (2004a). Use of Metadata for
Question Answering and Novelty Tasks. In E. M.
Voorhees & L. P. Buckland (eds.), The Twelfth Text
Retrieval Conference (TREC 2003). (In press.)
Litkowski, Kenneth. C. (2004b). Summarization
Experiments in DUC 2004. (In press.)
Mihalcea, Rada and Dan Moldovan. (2001). EXtended
WordNet: Progress Report. In: WordNet and Other
Lexical Resources: Applications, Extensions, and
Customizations. NAACL 2001 SIGLEX Workshop.
Pittsburgh, PA.: Association for Computational
Linguistics.
Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, pages 37?44,
Trento, Italy, April 2006. c?2006 Association for Computational Linguistics
Coverage and Inheritance in The Preposition Project
                      Ken Litkowski                        
                       CL Research                          
                     9208 Gue Road                        
      Damascus, MD 20872       
ken@clres.com 
                       Orin Hargraves                      
                 5130 Band Hall Hill Road            
        Westminster, MD 21158     
orinkh@carr.org  
Abstract
In The Preposition Project (TPP), 13
prepositions have now been analyzed and
considerable data made available. These
prepositions, among the most common words
in English, contain 211 senses. By analyzing
the coverage of these senses, it is shown that
TPP provides potentially greater breadth and
depth than other inventories of the range of
semantic roles. Specific inheritance
mechanisms are developed within the
preposition sense inventory and shown to be
viable and provide a basis for the
rationalization of the range of preposition
meaning. In addition, this rationalization can
be used for developing a data-driven mapping
of a semantic role hierarchy. Based on these
findings and methodology, the broad
structure of a WordNet-like representation of
preposition meaning, with self-contained
disambiguation tests, is outlined.
1 Introduction
The Preposition Project (TPP, Litkowski &
Hargraves, 2005)
1
 provides a large amount of data
for a small number of prepositions. To date, 13 out
of 373 prepositions (among the most frequent in
English) have been analyzed. We examined the data
for these prepositions to determine (1) their
coverage of the semantic space of semantic
relations, (2) the extent to which these data could be
extrapolated to prepositions not yet covered, and (3)
what types of analyses might be useful to fill
shortcomings in the data. Examining these issues
seems important to determining the extent to which
the data in the project can be used in NLP
applications.
TPP is designed to provide a comprehensive
database of preposition senses, so it is useful to
provide a mechanism for assessing the extent of
coverage, not only in comparison with the range of
meanings described in traditional grammar, but also
in comparison with analyses within the
computational linguistics community. Similarly, it
seems important to determine how, if at all, the data
developed thus far can be leveraged for use with
other preposition meanings not yet analyzed, e.g.,
through mechanisms of inheritance. Finally, through
these analyses, it is useful to identify any
shortcomings in data being developed in TPP and
what further should be undertaken.
In the following sections, we first provide an
overview of TPP and extensions to its available
data that have occurred since its inception. Next, we
examine issues of coverage in relation to the range
of preposition meaning contained in Quirk et al
(1985), alongside the ranges in other resources such
as the Penn Treebank, FrameNet, and Lexical
Conceptual Structures. This analysis also considers
accounts of semantic relations that have been
presented in literature that has used these other
resources. Next, we critically examine claims of the
inheritance of preposition meaning as described in
Litkowski (2002), including consideration of
inheritance mechanisms in FrameNet. This analysis
suggests some mechanisms for a data-driven or
corpus-based approach to the identification of a
semantic relation inventory. Finally, based on these
analyses of coverage and inheritance, we identify
some next steps TPP needs to take.
2 The Preposition Project
The primary objective of TPP is to characterize
each of 847 preposition senses for 373 prepositions
(including 220 phrasal prepositions with 309
senses) with a semantic role name and the syntactic
and semantic properties of its complement and
attachment point. The preposition sense inventory
is taken from the Oxford Dictionary of English
1
http://www.clres.com/prepositions.html.
37
(2004).
2
 Starting from the senses for a particular
preposition, a set of instances of that preposition
are extracted from the FrameNet database. A
lexicographer then assigns a sense from the
inventory to each instance. While engaged in this
sense assignment, the lexicographer accumulates an
understanding of the behavior of the preposition,
assigns a name to each sense (characterizing its
semantic type), and characterizes the syntactic and
semantic properties of the preposition complement
and its point of attachment or head. Each sense is
also characterized by its syntactic function and its
meaning, identifying the relevant paragraph(s)
where it is discussed in Quirk et al
TPP then makes available the sense analysis
(including the lexicographer?s overview) and the set
of instances for each preposition that is analyzed. In
addition, the disambiguated instances are then
analyzed to provide the set of FrameNet frames and
frame elements associated with each sense. The set
of sentences is provided in Senseval format, along
with an answer key, for use in development of
preposition disambiguation routines (ranging from
300 to over 4000 sentences for ?of?). Finally, using
the FrameNet frame and frame element of the
tagged instances, syntactic alternation patterns
(other syntactic forms in which the semantic role
may be realized) are provided for each FrameNet
target word; this data constitutes a suitable corpus
for use in studying, for example, English verb
classes (see Levin, 1993).
An important next step for TPP is the use of
these disambiguated instances to refine the
characterization of the syntactic and semantic
properties of the complement and the point of
attachment. As the lexicographer has analyzed the
sense inventory for a preposition, the question of its
use in relation to other words is continually raised.
In particular, the question is whether a sense stands
alone or is selected for by a verb or other word
(most frequently, an adjective).
3
 The lexicographer
has observed that selection might be occurring. The
extent to which this occurs will be examined when
an attempt is made, for example, to develop
decision lists for disambiguating among a
preposition?s senses.
4
 We hope, as a result, that the
number of instances available for disambiguation
will permit a more definitive characterization of
selection.
Since Litkowski & Hargraves (2005), several
additions have been made to the data and analyses
available under TPP. First, Oxford University Press
has granted permission to provide the definitions
and examples of the senses for each definition from
the Oxford Dictionary of English (ODE, 2003)
(and its predecessor, the New Oxford Dictionary of
English (NODE, 1997)). Second, a summary file of
all senses has been prepared from the individual
preposition sense analyses, facilitating overview
analysis of the full sense inventory (e.g., sorting the
table on different columns). Third, the
lexicographer has disambiguated the ending
preposition of definitions as those prepositions are
analyzed (e.g., in sense 1 of about, on the subject
of, identifying the applicable sense of of); 451
prepositions have been so tagged.
At present, the following 13 prepositions have
been analyzed (with the initial number of senses in
parentheses): about (6), against (10), at (12), by
(22), for (14), from (14), in (11), of (18), on (23),
over (16), through (13), to (17), and with (16).
The number of senses has changed based on
changes from NODE to ODE and based on
evidence developed in the project (adding 19 senses
that are attested with the FrameNet data). These
prepositions include the most frequent in English
(see Boonthum et al, 2006 for the top 10 based on
the Brown corpus). In summary, the 13 prepositions
(out of 373 identified in Litkowski, 2002) have 210
senses (19 have been added during the course of
TPP) out of the original 847 senses.
It is noteworthy also that in moving from
NODE to ODE, 60 prepositions have been
removed. Some of these prepositions are variant
spellings (e.g. abaht for about). Most are phrasal
prepositions, e.g., to the accompaniment of. In
2
TPP does not include particle senses of such words
as in or over (or any other particles) used with verbs
to make phrasal verbs. In this context, phrasal verbs
are to be distinguished from verbs that select a
preposition (such as on in rely on), which may be
characterized as a collocation. We are grateful to an
anonymous reviewer for raising this issue.
3
We are grateful to an anonymous reviewer for this
characterization.
4
The anonymous reviewer asked whether TPP
excludes senses that are selected for. This prompted
an examination of whether this might be the case.
Although it is the intent that such senses be included,
an examination of how FrameNet instances are
generated raises the possibility that such instances
may have excluded. Procedures are currently being
developed to ensure that such instances are not
excluded.
38
NODE, the definitions constitute a lexicographic
statement that the meaning of the phrase has an
idiomatic status, i.e., is not solely recoverable based
on an understanding of the meanings of its
constituents. In ODE, such phrases are identified as
having collocative status and thereby rendered in
example usages with italics, but not given a
definition. Such phrases will be retained in TPP.
Litkowski & Hargraves (2005) provides more
details on the methodology used in TPP and the
databases that are available.
3 Semantic Coverage of TPP
Although only a small percentage of the
prepositions have as yet been analyzed,
approximately 25 percent of the total number of
senses are included in the 13 prepositions. This
percentage is sufficient to assess their coverage of
the semantic space of prepositional meaning.
3.1 Assessing the Broad Spectrum of Semantic
Space
To assess the coverage, the first question is what
inventory should be used. The linguistics and
computational linguistics literatures are replete with
introspective lists of semantic roles. Gildea &
Jurafsky (2002) present a list of 18 that may be
viewed as reasonably well-accepted. O?Hara (2005)
provides several compilations based on Penn
Treebank annotations, FrameNet, OpenCyc, and
Factotum. Boonthum et al (2006) includes an
assessment of semantic roles in Jackendoff, Dorr?s
Lexical Conceptual Structures preposition
database, and Barker?s analysis of preposition
meaning; she posits a list of 7 overarching semantic
roles (although specifically intended for use in
paraphrase analysis). Without going into a detailed
analysis of each of these lists, all of which are
relatively small in number, the semantic relations
included in TPP clearly cover each of the lists.
However, since the semantic relations in these lists
are relatively coarse-grained, this assessment is not
sufficient.
Quirk et al (1985) is arguably the most
comprehensive introspective compilation of the
range of preposition meaning. As indicated above,
in analyzing the senses for a preposition, the
lexicographer includes a reference to a section in
Quirk et al(specifically in Chapter 9). Quirk et al
describe the meanings of prepositions in 50
sections, with the majority of discussion devoted to
spatial and temporal prepositions. By comparing
the references in the spreadsheets for each
preposition (i.e., a data-driven approach), we find
that only 4 sections are not yet mentioned. These
are 9.21 (between), 9.56 (concession), 9.58
(exception and addition), and 9.59 (negative
condition). In general, then, TPP broadly covers the
full range of meanings expressed by prepositions as
described in Quirk et al.
However, for almost half of the senses analyzed
in TPP (100 of 210), the lexicographer was unable
to assign a Quirk paragraph in Chapter 9 or
elsewhere. This raises the question of whether
Quirk et al can be viewed as comprehensive. A
preliminary examination of the semantic relations
assigned by the lexicographer and not assigned a
Quirk paragraph indicates that the range of
prepositional meaning is more extensive than what
is provided in Quirk et al
Two major categories of missing semantic
relations emerge from this analysis. Of the 100
senses without a Quirk paragraph, 28 involve
prepositional usages pertaining to quantities. These
include the semantic relations like Age (?at six he
contracted measles?, ScaleValue (?an increase of
5%?), RatioDenominator (?ten miles to the
gallon?), Exponent (?10 to the fourth power?),
ValueBasis (?a tax on tea?), Price (?copies are
available for $5"), and UnitSize (?billing is by the
minute?). Another 32 involve prepositions used to
establish a point of reference, similar to the
Standard in Quirk (section 9.62), except indicating
a much broader set. These include semantic
relations like FormerState (?wakened from a
dream?), KnowledgeSource (?information from
books?), NameUsed (?call him by his last name?),
ParentName (?a child by her first husband?),
Experiencer (?a terrible time for us?), and
Comparator (?that?s nothing compared to this?).
The remaining 40 semantic relations, such as
MusicalKey (?in F minor?), Drug (?on dope?), and
ProfessionAspect (?a job in publishing?), appear to
represent finer-grained points of prepositional
meaning.
This assessment of coverage suggests that TPP
currently not only covers the broad range of
semantic space, but also identifies gaps that have
not received adequate treatment in the linguistic
literature. Perhaps such gaps may be viewed as
?beneath the radar? and not warranting elaborate
treatment. However, it is highly likely that these
39
Semantic
Relation Frequency Definitions Examples
Location 0.404 expressing location or arrival in a
particular place or position
crouched at the edge of the track
Temporal 0.072 expressing the time when an event
takes place
avoid confusiong at this late stage
Level 0.039 denoting a particular point or segment
on a scale
charged at two percent
Skill 0.038 expressing a particular state or
condition, or a relationship between an
individual and a skill
brilliant at the job
ActionObject 0.276 expressing the object of a look,
gesture, thought, action, or plan
moaned at him
Stimulus 0.171 expressing the means by which
something is done or the cause of an
action or reaction
boiled at his lack of thought
Table 1. Frequency of ?at? FrameNet Instances in The Preposition Project
senses occur with considerable frequency and
should be treated.
It is somewhat premature to perform a
comprehensive analysis of coverage that provides a
full characterization of the semantic space of
preposition meaning based on the 25 percent of
senses that have been analyzed thus far. However,
the available data are sufficient to begin such an
effort; this issue is further discussed below.
3.2 Assessing Finer-Grained Spectra of
Prepositional Meaning
While examining the broad coverage of preposition
meaning, several issues affecting the treatment of
individual prepositions in the computational
linguistics literature emerged. These issues also
provide a perspective on the potential value of the
analyses being performed in TPP.
O?Hara (2005), in attempting to create a
framework for analysis and identification of
semantic relations, examined the utility of Penn
Treebank II annotations and FrameNet frame
elements. He examined sentences containing at in
both corpora. In Treebank, he noted that there were
four senses: locative (0.732), temporal (0.239),
manner (0.020), and direction (0.006). In
FrameNet, with some combination of frame
elements, he identified five major senses: addressee
(0.315), other (0.092), phenomenon (0.086), goal
(0.079), and content (0.051).
Table 1 provides a coarse-grained analysis of at
developed in TPP (6 additional subsenses are not
shown). Although frequencies are shown in the
table, they should not be taken seriously, since the
FrameNet instances on which they are based makes
no claim to be representative. In particular,
FrameNet seldom annotates temporal references
since they are usually viewed as peripheral frame
elements that may occur with virtually all frames.
Nonetheless, the frequencies in the FrameNet
instances does indicate that each of the at senses is
likely to occur at levels that should not be ignored
or glossed over.
In comparing TPP results with Penn Treebank
characterizations, it seems that, not only might the
corpus be unrepresentative, but that the linguistic
introspection does not capture the more natural
array of senses. Thus, by combining corpus
evidence (from FrameNet) with a lexicographic
perspective for carving out sense distinctions, an
improved balance results. It should also be noted
that in Table 1, the final sense for Stimulus
emerged from the FrameNet data and from Quirk
and was not identified in the ODE sense inventory.
Comparing TPP results with O?Hara?s
aggregation of FrameNet frame elements indicates
the difficulty of working directly with the large
number of frame elements (currently over 700). As
Gildea & Jurafsky noted, it is difficult to map these
frame elements into higher level semantic roles.
Some assistance is available from the FrameNet
inheritance hierarchy, but this is still not well-
developed. This issue is taken up further below in
describing how TPP?s data-driven approach may
facilitate this kind of mapping.
In summary, the methodology being followed in
TPP arguably provides a more natural and a more
assuredly complete coverage of the fine-grained
senses associated with an individual preposition.
40
4 Inheritance Within the Preposition
Sense Inventory
The preceding discussion provides some assurance
that TPP provides broad coverage of the range of
prepositional meaning and fine-grained analysis of
the behavior of individual prepositions. However,
the large number of preposition senses requires
some additional work to manage these broad and
fine-grained spectra. Litkowski (2002) provided a
graph-theoretical analysis that arranged
prepositions into a hierarchy. However, that
analysis treated individual prepositions as
aggregations, i.e., all senses were combined into
nodes in a digraph. With the finer-grained analysis
now available in TPP data, a more in-depth
examination of inheritance within the preposition
sense inventory is possible.
4.1 Initial Considerations for Mapping Out the
Inheritance Hierarchy
Of the 847 senses described in Litkowski (2002),
and used as the starting point for the analysis in
TPP, most follow the prototypical form of a
prepositional phrase followed by a terminal
(dangling) preposition, e.g., for sense 1 of about,
on the subject of. Litkowski viewed the terminal
preposition as a hypernym. However, 62 senses do
not have terminal prepositions (but rather usually
verbs) and an additional 164 senses are usage notes
describing behavior (such as the senses of at shown
in Table 1). These 226 senses were viewed as being
primitive, while the remaining 621 were viewed as
being derived in some way dependent on the
putative hypernym.
Among the 13 prepositions that have been
analyzed thus far, 11 senses having a non-
preposition hypernym and 100 senses with usage
notes have been characterized. Thus, only about
half of the so-called primitives have been assigned
a semantic relation type. Further analysis of the
range of meaning of these primitives should await
a more complete coverage of these senses. The kind
of analysis envisioned among these senses is
determining how they group together and what
range of semantic meaning they express. This will
be discussed further below.
Of the 621 senses with a preposition hypernym,
411 end in one of the 13 prepositions that have been
analyzed, with 175 ending in of and 74 in to. The
remaining 210 senses end in prepositions with at
most a few cases of the same preposition. Most of
these remaining senses, in fact, are the ones that
gave rise to the definitional cycles and hierarchical
analysis of the digraph described in Litkowski
(2002). As a result, senses with a preposition
hypernym form a set sufficient in size for a more
detailed analysis of inheritance within the
preposition inventory.
4.2 The Meaning of an Inheritance Hierarchy
for Prepositions
The assumption underlying an inheritance analysis
of preposition definitions with a terminal
preposition is that such definitions are substitutable
for the preposition that is defined. For example, in
a book about ancient Greece, about can be
replaced by its definition to obtain a book on the
subject of ancient Greece. This sense of about has
been labeled SubjectConsidered (or equivalently,
Topic or Subject) by the lexicographer. In the
inheritance analysis, this definition of about is said
to have of as its hypernym.
Clearly, the hypernymic ascription for
prepositions is by analogy only. To say that about
isa of makes little sense. In TPP, the lexicographer
develops three pieces of information about each
sense: a semantic relation name, the properties of
the prepositional object, and the properties of the
word to which the prepositional phrase is attached.
In analyzing the definition for about, of is attached
to the word subject. Thus, nothing about the
attachment properties of of can be inherited into
saying anything about the attachment properties of
about. At best, then, the semantic relation name and
complement properties of the applicable sense of of
can be inherited. Indeed, this can be put into the
form of a hypothesis: the semantic relation name
and the complement properties of an inherited
sense are more general than those of the inheriting
sense.
As mentioned above, the lexicographer has
disambiguated the terminal preposition in senses
that use one of the 13 prepositions that have been
analyzed. This has been done for 451 definitions in
the 411 senses. It is noteworthy that in only 29
cases did the lexicographer assign multiple senses
(i.e., viewing the applicable sense as ambiguous). In
other words, despite the fact that most of these
definitions contained only 4 or 5 words, sufficient
context enabled resolution to a specific sense of the
hypernym. In 8 cases, the multiple inheritance was
41
Semantic
Relation Preposition
Complement
Properties Definition
Hypernym
Semantic
Relation
Hypernym
Complement
Properties
Opposing
Force
against sth actively resisted in resistance to; as
protection from
Thing
Prevented
participle or noun
denoting thing
prevented
Thing
Surmounted
over a physical entity that
can have sth above it
extending directly
upwards from
Space
Origin
point in space or
abstraction
identified as origin
Thing Bored through permeable or breakable
physical object
so as to make a hole
or opening in (a
physical object)
Thing
Entered
sth capable of
being entered or of
incorporating or
enveloping input
Beneficiary for usually a person;
otherwise, sth capable of
benefitting
on behalf of or to the
benefit of (someone or
something)
Recipient noun representing
the obj. of action
denoted in the
POA
Feature
Backdrop
on background on which
the POA is located
forming a distinctive
or marked part of (the
surface of something)
Whole object of which the
POA is a part,
piece, or sample
Downside against downside; the con in a
pro/con situation
in conceptual contrast
to
Comparator second term of a
comparison
Table 2. Inheritance of Semantic Relations and Complement Properties
for all senses, as in the case of frae, a Scottish
dialectical form of from.
In making the sense assignments, 175 of which
(39 percent) involved of, the lexicographer noted
that a large number of cases (132 of 373) involved
phrasal prepositions that ended in of, e.g., into the
arms of and in the name of. In these cases, the
definition (as developed by Oxford lexicographers)
merely substituted one phrase ending in of for the
phrase being defined (into the possession or control
of and for the sake of for the two examples). This
observation was a major reason for requiring that
any hypernymic ascription within the preposition
inventory could not be based on the prototypical isa
hierarchy applicable to nouns.
Among the 411 senses for which the terminal
preposition had been disambiguated, 48 senses
occurred as definitions of the 13 prepositions that
have been analyzed in TPP. For these 48 senses,
each of which was fully characterized, the
characterization of the terminal prepositions was
also available, thus enabling us to test the
hypothesis about what could be inherited. Table 2
shows the results for 6 of these senses, giving first
the semantic relation assigned to the sense by the
lexicographer, the preposition, the characterization
of the complement properties for that sense, the
definition (with the hypernymic preposition in bold),
the semantic relation of the sense that the
lexicographer judged to be the appropriate sense of
the hypernymic preposition, and the complement
properties of that sense.
The examples in Table 2 support the hypothesis
about inheritance. The other 42 cases are similar,
although for some, the hypernymic semantic
relation or hypernymic complement properties are
not as close to the preposition sense being
examined. In a few cases, for example, the
complement properties are as general as ?any
noun.? In such cases, what gets inherited may not
provide much in the way of specificity to aid in
analyzing the behavior of the inheriting preposition.
However, viewed from the perspective of the
digraph analysis performed in Litkowski (2002),
this inheritance analysis provides confidence that
there is an ordering relationship within the
preposition sense inventory that can be exploited.
In the digraph analysis in Litkowski (2002),
where the prepositions were analyzed as aggregated
nodes, the inheritance mechanism provides the basis
for splitting nodes based on the specific sense
assignments that can now be made. In particular, in
Table 3, showing one node of the preposition
digraph that was characterized as a single strong
component (number 12) containing 33 prepositions,
the sense-specific assignments will permit the
disaggregation of these prepositions into smaller
groups that are closely related.
42
Table 3. Strong Components
Entries
12 in favour of, along with, with respect to,
in proportion to, in relation to, in
connection with, with reference to, in
respect of, as regards, concerning, about,
with, in place of, instead of, in support of,
except, other than, apart from, in addition
to, behind, beside, next to, following,
past, beyond, after, to, before, in front of,
ahead of, for, by, according to
In considering the type of analysis described by
Table 2, it is important to note that the results
followed from the reliance on a data-driven
approach. Characterizations of individual senses are
made locally with respect to observed behavior of a
single preposition. It is only after these analyses
that results from several tables and spreadsheets
can be conjoined to produce something like Table 2.
It is also important to note that the results in
Table 2 must be viewed as preliminary. Although it
is expected that the central hypothesis about
inheritance will remain valid, it is expected that the
characterizations of the complement properties will
undergo considerable refinement. One of the
primary goals of TPP is to develop a data-driven set
of disambiguation criteria for distinguishing among
preposition senses. Methods such as those
developed by O?Hara (2005) and Boonthum et al
(2006) suggest that refined characterizations will
emerge. The large instance sets (in Senseval format)
will provide an ample data set for this analysis.
Finally, it is expected that the semantic relation
names will also undergo some additional revisions.
Again, since these names are developed locally with
respect to single prepositions, they do not reflect
what may be a final set when they are analyzed
together. This is discussed in the next section.
5 Next Steps for The Preposition Project
The analyses of issues concerning coverage and
inheritance within the preposition sense inventory
suggest at least two major new goals for TPP. One
is the rationalization of the semantic relation types
and the other is the aggregation of characterizations
about the senses into a convenient and usable data
structure, perhaps following WordNet.
5.1 Rationalization of Semantic Relation Types
The semantic relation types that have been
developed thus far in TPP have been extremely
useful in assessing the current coverage of the
semantic space of prepositions and in examining the
possibilities of an inheritance structure for the
senses. However, the analyses have shown that
there are some gaps in broad coverage and some
that will affect fine-grained characterizations of the
semantic space.
In performing the analyses of the 13
prepositions and their 211 senses, the names for the
semantic relations for an individual preposition
have been developed without regard to those from
other prepositions or the linguistic literature, based
on the individual definitions in ODE and the
instances from FrameNet that have been tagged.
Although frame element names are available to the
lexicographer when examining FrameNet instances,
they are only in the background. As a result, these
names provide a data-driven basis for
characterizing the semantic space of prepositions.
Given the importance of these names for the
types of analyses described above, it is valuable to
complete the assignment of names, even without the
full-scale analysis of sentence instances.
Completion of this task would represent only a
preliminary assignment, modifiable when instances
are more fully analyzed.
With a relatively complete set of semantic
relation types, ?rationalization? of the set, i.e.,
reorganization in such a way to make it more
logical and consistent can be performed. At present,
among the 211 semantic relation types, there are 36
duplicate names, some appearing multiple times,
e.g., AgentName appears 5 times. Some names are
only slight variants of one another, such as
Timeframe and TimePeriod. Many names can be
grouped together for analysis. For example, in the
time space, such semantic relations as
ActivePeriod, ClockHour, CreationDate,
FutureTime, Hour, PeriodBisected, PointInTime,
TargetTime, TimeOrigin, and TimePeriod would
be examined together.
In pursuing this rationalization, outside
resources can be used more efficiently. In
particular, the FrameNet naming conventions and
inheritance hierarchy can be examined in more
detail (as well as critiqued). In addition, it will be
possible to take into account other treatments of
particular prepositions or fine-grained areas of
semantic space more easily.
Rationalization not only will ensure consistency
in naming, but provide a vehicle for appropriate
43
data-driven mapping. This will provide a basis for
or against conventional groupings that have been
posited in the linguistics and computational
linguistics literature. It is not expected that this
rationalization will produce anything unexpected,
but it will provide an underlying support for
characterizing the range of prepositional meaning.
5.2 Towards a WordNet Representation of
Prepositional Meaning
The amount of data generated in TPP has been
prodigious and is difficult to comprehend and
exploit. With a firmer basis established in section 4
above for inheritance mechanisms, combined with
the digraph analysis described in Litkowski (2002),
it seems possible to move toward a representation
that is similar to WordNet.
By following the inheritance structure, based on
the analyses described in section 4, combined with
a rationalization of semantic relation names, it
seems likely that there will be a relatively small
number of primitive concepts. The digraph analysis
yields synsets in the manner of WordNet, so we can
visualize that nodes in a WordNet preposition
network will consist of preposition names and
preposition glosses (i.e., definitions). In addition,
the objective will be to provide an improved
characterization of complement and attachment
properties that will accompany each node. Thus,
such a WordNet-like preposition network will
represent not only meanings, but also provide the
capability for disambiguation.
6 Conclusions
Although only a small number of prepositions have
been analyzed in The Preposition Project, the data
that has been generated has proved sufficient for a
broad assessment of the range of preposition
meaning. Not only has it been possible to
demonstrate that the project currently provides a
comparable broad coverage, but also that it reveals
potential gaps in previous analyses of coverage.
The data has also proved sufficient for the
articulation of appropriate inheritance mechanisms
within the preposition sense inventory. These results
have permitted the development of procedures that
can be used for mapping out the space of semantic
roles. In addition, with these results, it is possible to
lay out steps toward a WordNet-like representation
of prepositions and their behavior.
References
Bonnie Dorr. 1996. Lexical Conceptual Structures for
Prepositions
(http://www.umiacs.umd.edu/~bonnie/AZ-preps-
English.lcs)
Chutima Boonthum, Shunichi Toida, & Irwin
Levinstein. 2006. Preposition Senses:
Generalized Disambiguation Model. Conference
on Intelligent Text Processing and
Computational Linguistics (CICLING-2006).
Mexico City.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
Labeling of Semantic Roles. Computational
Linguistics, 28 (3), 245-288.
Kenneth C. Litkowski. 2002. Digraph Analysis of
Dictionary Preposition Definitions. Word Sense
Disambiguation: Recent Success and Future
Directions. Philadelphia, PA: Association for
Computational Linguistics.
Kenneth C. Litkowski & Orin Hargraves. 2005. The
Preposition Project. ACL-SIGSEM Workshop on
?The Linguistic Dimensions of Prepositions and
their Use in Computational Linguistic
Formalisms and Applications?, University of
Essex - Colchester, United Kingdom. 171-179. 
The New Oxford Dictionary of English. 1998. (J.
Pearsall, Ed.). Oxford: Clarendon Press.
Thomas P. O?Hara. 2005. Empirical Acquisition of
Conceptual Distinctions via Dictionary
Definitions. Ph.D. Thesis. New Mexico State
University.
The Oxford Dictionary of English. 2003. (A.
Stevension and C. Soanes, Eds.). Oxford:
Clarendon Press.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
& Jan Svartik. (1985). A comprehensive
grammar of the English language. London:
Longman.
44
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 24?29,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 06: Word-Sense Disambiguation of Prepositions
                      Ken Litkowski                           
                        CL Research                            
                     9208 Gue Road                          
      Damascus, MD 20872          
ken@clres.com 
                       Orin Hargraves                         
              5130 Band Hall Hill Road                  
  Westminster, MD 21158     
orinhargraves@googlemail.com  
Abstract
The SemEval-2007 task to disambiguate
prepositions was designed as a lexical sample
task. A set of over 25,000 instances was
developed, covering 34 of the most frequent
English prepositions, with two-thirds of the
instances for training and one-third as the test
set. Each instance identified a preposition to be
tagged in a full sentence taken from the
FrameNet corpus (mostly from the British
National Corpus). Definitions from the Oxford
Dictionary of English formed the sense
inventories. Three teams participated, with all
achieving supervised results significantly
better than baselines, with a high fine-grained
precision of 0.693. This level is somewhat
similar to results on lexical sample tasks with
open class words, indicating that significant
progress has been made. The data generated in
the task provides ample opportunitites for
further investigations of preposition behavior.
1 Introduction
The SemEval-2007 task to disambiguate prepositions
was designed as a lexical sample task to investigate
the extent to which an important  closed class of
words could be disambiguated. In addition, because
they are a closed class, with stable senses, the
requisite datasets for this task are enduring and can
be used as long as the problem of preposition
disambiguation remains. The data used in this task
was developed in The Preposition Project (TPP,
Litkowski & Hargraves (2005) and Litkowski &
Hargraves (2006)),
1
 with further refinements to fit
the requirements of a SemEval task. 
In the following sections, we first describe the
motivations for a preposition disambiguation task.
Next, we describe the development of the datasets
used for the task, i.e., the instance sets and the sense
inventories. We describe how the task was performed
and how it was evaluated (essentially using the same
scoring methods as previous Senseval lexical sample
tasks). We present the results obtained from the
participating teams and provide an initial analysis of
these results. Finally, we identify several further
types of analyses that will provide further insights
into the characterization of preposition behavior.
2 Motivation
Prepositions are a closed class, meaning that the
number of prepositions remains relatively constant
and that their meanings are relatively stable. Despite
this, their treatment in computational linguistics has
been somewhat limited. In the Penn Treebank, only
two types of prepositions are recognized (IN
(locative, temporal, and manner) and TO (direction))
(O?Hara, 2005). Prepositions are viewed as function
words that occur with high frequency and therefore
carry little meaning. A task to disambiguate
prepositions would, in the first place, allow this
limited treatment to be confronted more fully.
Preposition behavior has been the subject of
much research, too voluminous to cite here. Three
recent workshops on prepositions have been
sponsored by the ACL-SIGSEM: Toulouse in 2003,
Colchester in 2005, and Trento in 2006. For the most
part, these workshops have focused on individual
prepositions, with various investigations of more
generalized behavior. The SemEval preposition
disambiguation task provides a vehicle to examine
whether these behaviors are substantiated with a
well-defined set of corpus instances.
Prepositions assume more importance when they1http://www.clres.com/prepositions.html.
24
are considered in relation to verbs. While linguistic
theory focuses on subjects and objects as important
verb arguments, quite frequently there is an
additional oblique argument realized in a
prepositional phrase. But with the focus on the verbs,
the prepositional phrases do not emerge as having
more than incidental importance. However, within
frame semantics (Fillmore, 1976), prepositions rise
to a greater prominence; frequently, two or three
prepositional phrases are identified as constituting
frame elements. In addition, frame semantic analyses
indicate the possibility of a greater number of
prepositional phrases acting as adjuncts (particularly
identifying time and location frame elements). While
linguistic theories may identify only one or two
prepositions associated with an argument of a verb,
frame semantic analyses bring in the possibility of a
greater variety of prepositions introducing the same
type of frame element. The preposition
disambiguation task provides an opportunity to
examine this type of variation.
The question of prepositional phrase attachment
is another important issue. Merlo & Esteve Ferrer
(2006) suggest that this problem is a four-way
disambiguation task, depending on the properties of
nouns and verbs and whether the prepositional
phrases are arguments or adjuncts. Their analysis
relied on Penn Treebank data. Further insights may
be available from the finer-grained data available in
the preposition disambiguation task.
Another important thread of investigation
concerning preposition behavior is the task of
semantic role (and perhaps semantic relation)
labeling (Gildea & Jurafsky, 2002). This task has
been the subject of a previous Senseval task
(Automatic Semantic Role Labeling, Litkowski
(2004)) and two shared tasks on semantic role
labeling in the Conference on Natural Language
Learning (Carreras & Marquez (2004) and Carreras
& Marquez (2005)). In addition, three other tasks in
SemEval-2007 (semantic relations between nominals,
task 4; temporal relation labeling, task 15; and frame
semantic structure extraction, task 19) address issues
of semantic role labeling. Since a great proportion of
these semantic roles are realized in prepositional
phrases, this gives greater urgency to understanding
preposition behavior.
Despite the predominant view of prepositions as
function words carrying little meaning, this view is
not borne out in dictionary treatment of their
definitions. To all appearances, prepositions exhibit
definitional behavior similar to that of open class
words. There is a reasonably large number of distinct
prepositions and they show a range of polysemous
senses. Thus, with a suitable set of instances, they
may be amenable to the same types of analyses as
open class words.
3 Preparation of Datasets
The development of the datasets for the preposition
disambiguation task grew directly out of TPP. This
project essentially articulates the corpus selection, the
lexicon choice, and the production of the gold
standard. The primary objective of TPP is to
characterize each of 847 preposition senses for 373
prepositions (including 220 phrasal prepositions with
309 senses)
2
 with a semantic role name and the
syntactic and semantic properties of its complement
and attachment point. The preposition sense
inventory is taken from the Oxford Dictionary of
English (ODE, 2004).
3
 
3.1 Corpus Development
For a particular preposition, a set of instances is
extracted from the FrameNet database.
4
 FrameNet
was chosen since it provides well-studied sentences
drawn from the British National Corpus (as well as
a limited set of sentences from other sources). Since
the sentences to be selected for frame analysis were
generally chosen for some open class verb or noun,
these sentences would be expected to provide no bias
with respect to prepositions. In addition, the use of
this resource makes available considerable
information for each sentence in its identification of
2
The number of prepositions and the number of senses
is not fixed, but has changed during the course of the
project, as will become clear.
3
TPP does not include particle senses of such words as
in or over (or any other particles) used with verbs to
make phrasal verbs. In this context, phrasal verbs are
to be distinguished from verbs that select a preposition
(such as on in rely on), which may be characterized as
a collocation.
4
http://framenet.icsi.berkeley.edu/ 
25
frame elements, their phrase type, and their
grammatical function. The FrameNet data was also
made accessible in a form (FrameNet Explorer)
5
 to
facilitate a lexicographer?s examination of
preposition instances.
Each sentence in the FrameNet data is labeled
with a subcorpus name. This name is generally
intended only to capture some property of a set of
instances. In particular, many of these subcorpus
names include a string ppprep and this identification
was used for the selection of instances. Thus,
searching the FrameNet corpus for subcorpora
labeled ppof or ppafter would yield sentences
containing a prepositional phrase with a desired
preposition. This technique was used for many
common prepositions, yielding 300 to 4500
instances. The technique was modified for
prepositions with fewer instances. Instead, all
sentences having a phrase beginning with a desired
preposition were selected. 
The number of sentences eventually used in the
SemEval task is shown in Table 1. More than 25,000
instances for 34 prepositions were tagged in TPP and
used for the SemEval-2007 task.
3.2 Lexicon Development
As mentioned above, ODE (and its predecessor, the
New Oxford Dictionary of English (NODE, 1997))
was used as the sense inventory for the prepositions.
ODE is a corpus-based, lexicographically-drawn
sense inventory, with a two-level hierarchy,
consisting of a set of core senses and a set of
subsenses (if any) that are semantically related to the
core sense. The full set of information, both printed
and in electronic form, containing additional
lexicographic information, was made publicly
available for TPP, and hence, the SemEval
disambiguation task.
The sense inventory was not used as absolute and
further information was added during TPP. The
lexicographer (Hargraves) was free to add senses,
particularly as the corpus evidence provided by the
FrameNet data suggested. The process of refining the
sense inventory was performed as the lexicographer
assigned a sense to each instance. While engaged in
this sense assignment, the lexicographer accumulated
an understanding of the behavior of the preposition,
assigning a name to each sense (characterizing its
semantic type), and characterizing the syntactic and
semantic properties of the preposition complement
and its point of attachment or head. Each sense was
also characterized by its syntactic function and its
meaning, identifying the relevant paragraph(s) where
it is discussed in Quirk et al(1985).
After sense assignments were completed, the set
of instances for each preposition was analyzed
against the FrameNet database. In particular, the
FrameNet frames and frame elements associated with
each sense was identified. The set of sentences was
provided in SemEval format in an XML file with the
preposition tagged as <head>, along with an answer
key (also identifying the FrameNet frame and frame
element). Finally, using the FrameNet frame and
frame element of the tagged instances, syntactic
alternation patterns (other syntactic forms in which
the semantic role may be realized) are provided for
each FrameNet target word for each sense.
All of the above information was combined into
a preposition database.
6
 For SemEval-2007, entries
for the target prepositions were combined into an
XML file as the ?Definitions? to be used as the sense
inventory, where each sense was given a unique
identifier. All prepositions for which a set of
instances had been analyzed in TPP were included.
These 34 prepositions are shown in Table 1 (below,
beyond, and near were used in the trial set).
3.3 Gold Standard Production
Unlike previous Senseval lexical sample tasks,
tagging was not performed as a separate step. Rather,
sense tagging was completed as an integral part of
TPP. Funding was unavailable to perform additional
tagging with other lexicographers and the appropriate
interannotator agreement studies have not yet been
completed. At this time, only qualitative assessments
of the tagging can be given.
As indicated, the sense inventory for each
preposition evolved as the lexicographer examined
5
Available for the Windows operating system at
http://www.clres.com for those with access to the
FrameNet data.
6
The full database is viewable in the Online TPP
(http://www.clres.com/cgi-bin/onlineTPP/find_prep.cgi
).
26
the set of FrameNet instances. Multiple sources (such
as Quirk et al) and lexicographic experience were
important components of the sense tagging. The
tagging was performed without any deadlines and
with full adherence to standard lexicographic
principles. Importantly, the availability of the
FrameNet corpora facilitated the sense assignment,
since many similar instances were frequently
contiguous in the instance set (e.g., associated with
the same target word and frame).
Another important factor suggesting higher
quality in the sense assignment is the quality of the
sense inventory. Unlike previous Senseval lexical
sample tasks, the sense inventory was developed
using lexicographic principles and was quite stable.
In arriving at the sense inventory, the lexicographer
was able to compare ODE with its predecessor
NODE, noting in most cases that the senses had not
changed or had changed in only minor ways. 
Finally, the lexicographer had little difficulty in
making sense assignments. The sense distinctions
were well enough drawn that there was relatively
little ambiguity given a sentence context. The
lexicographer was not constrained to selecting one
sense, but could tag a preposition with multiple
senses as deemed necessary. Out of 25,000 instances,
only 350 instances received multiple senses.
4 Task Organization and Evaluation
The organization followed standard SemEval
(Senseval) procedures. The data were prepared in
XML, using Senseval DTDs. That is, each instance
was labeled with an instance identifier as an XML
attribute. Within the <instance> tag, the FrameNet
sentence was labeled as the <context> and included
one item, the target preposition, in the <head> tag.
The FrameNet sentence identifier was used as the
instance identifier, enabling participants to make use
of other FrameNet data. Unlike lexical sample tasks
for open class words, only one sentence was provided
as the context. Although no examination of whether
this is sufficient context for prepositions, it seems
likely that all information necessary for preposition
disambiguation is contained in the local context.
A trial set of three prepositions was provided (the
three smallest instance sets that had been developed).
For each of the remaining 34 prepositions, the data
was split in a ratio of two to one between training and
test data. The training data included the sense
identifier. Table 1 shows the total number of
instances for each preposition, along with the number
in the training and the test sets.
Answers were submitted in the standard Senseval
format, consisting of the lexical item name, the
instance identifier, the system sense assignments, and
optional comments. Although participants were not
restricted to selecting only one sense, all did so and
did not provide either multiple senses or weighting of
different senses. Because of this, a simple Perl script
was used to score the results, giving precision, recall,
and F-score.
7
 The answers were also scored using the
standard Senseval scoring program, which records a
result for ?attempted? rather than F-score, with
precision interpreted as percent of attempted
instances that are correct and recall as percent of
total instances that are correct.
8
 Table 1 reports the
standard SemEval recall, while Tables 2 and 3 use
the standard notions of precision and recall.
5 Results
Tables 2 and 3 present the overall fine-grained and
coarse-grained results, respectively, for the three
participating teams (University of Melbourne, Ko?
University, and Instituto Trentino di Cultura, IRST).
The tables show the team designator, and the results
over all prepositions, giving the precision, the recall,
and the F-score. The table also shows the results for
two baselines. The FirstSense baseline selects the
first sense of each preposition as the answer (under
the assumption that the senses are organized
somewhat according to prominence). The FreqSense
baseline selects the most frequent sense from the
training set. Table 1 shows the fine-grained recall
scores for each team for each preposition. Table 1
also shows the entropy and perplexity for each
preposition, based on the data from the training sets.
7
Precision is the percent of total correct instances and
recall is the percent of instances attempted, so that an
F-score can be computed.
8
The standard SemEval (Senseval) scoring program,
scorer2, does not work to compute a coarse-grained
score for the preposition instances, since senses are
numbers such as ?4(2a)? and not alphabetic.
27
Table 2. Fine-Grained Scores
(All Prepositions - 8096 Instances)
Team Prec Rec F
MELB-YB 0.693 1.000 0.818
KU 0.547 1.000 0.707
IRST-BP 0.496 0.864 0.630
FirstSense 0.289 1.000 0.449
FreqSense 0.396 1.000 0.568
Table 3. Coarse-Grained Scores
(All Prepositions - 8096 Instances)
Team Prec Rec F
MELB-YB 0.755 1.000 0.861
KU 0.642 1.000 0.782
IRST-BP 0.610 0.864 0.715
FirstSense 0.441 1.000 0.612
FreqSense 0.480 1.000 0.649
As can be seen, all participating teams performed
significantly better than the baselines. Additional
improvements occurred at the coarse grain, although
the differences are not dramatically higher.
All participating teams used supervised systems,
using the training data for their submissions. The
University of Melbourne used a maximum entropy
system using a wide variety of syntactic and semantic
features. Ko? University used a statistical language
model (based on Google ngram data) to measure the
likelihood of various substitutes for various senses.
IRST-BP used Chain Clarifying Relationships, in
which contextual lexical and syntactic features of
representative contexts are used for learning sense
discriminative patterns. Further details on their
methods are available in their respective papers.
6 Discussion
Examination of the detailed results by preposition in
Table 1 shows that performance is inversely related
to polysemy. The greater number of senses leads to
reduced performance. The first sense heuristic has a
correlation of -0.64; the most frequent sense heuristic
has a correlation of -0.67. the correlations for
MELB, KU, and IRST are -0.40, -0.70, and -0.56,
respectively. The scores are also negatively
correlated with the number of test instances. The
correlations are -0.34 and -0.44 for the first sense
and the most frequent sense heuristics. For the
systems, the scores are -0.17, -0.48, and -0.39 for
Melb, KU, and IRST.
The scores for each preposition are strongly
negatively correlated with entropy and perplexity, as
frequently observed in lexical sample disambiguation.
For MELB-YB and IRST-BP, the correlation with
entropy is about -0.67, while for KU, the correlation
is -0.885. For perplexity, the correlation is -0.55 for
MELB-YB, -0.62 for IRST-ESP , and -0.82 for KU.
More detailed analysis is required to examine the
performance for each preposition, particularly for the
most frequent prepositions (of, in, from, with, to, for,
on, at, into, and by). Performance on these
prepositions ranged from fairly good to mediocre to
relatively poor. In addition, a comparison of the
various attributes of the TPP sense information with
the different performances might be fruitful. Little of
this information was used by the various systems.
7 Conclusions
The SemEval-2007 preposition disambiguation task
can be considered successful, with results that can be
exploited in general NLP tasks. In addition, the task
has generated considerable information for further
examination of preposition behavior.
References
Xavier Carreras and Lluis Marquez. 2004.
Introduction to the CoNLL-2004 Shared Task:
Semantic Role Labeling. In: Proceedings of
CoNLL-2004.
Xavier Carreras and Lluis Marquez. 2005.
Introduction to the CoNLL-2005 Shared Task:
Semantic Role Labeling. In: Proceedings of
CoNLL-2005.
Charles Fillmore. 1976. Frame Semantics and the
Nature of Language. Annals of the New York
Academy of Sciences, 280: 20-32.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
Labeling of Semantic Roles. Computational
Linguistics, 28 (3), 245-288.
Kenneth C. Litkowski. 2004. Senseval-3 Task:
Automatic Labeling of Semantic Roles. In
Senseval-3: Third International Workshop on the
Evaluation of Systems for the Semantic Analysis of
Text. ACL. 9-12. 
Kenneth C. Litkowski & Orin Hargraves. 2005. The
Preposition Project. In: ACL-SIGSEM Workshop
on the Linguistic Dimensions of Prepositions and
their Use in Computational Linguistic Formalisms
28
and Applications, University of Essex -
Colchester, United Kingdom. 171-179. 
Kenneth C. Litkowski.& Orin Hargraves.  2006.
Coverage and Inheritance in The Preposition
Project. In: Proceedings of the Third ACL-
SIGSEM Workshop on Prepositions. Trento, Italy.
ACL. 89-94.
Paola Merlo and Eva Esteve Ferrer. 2006. The Notion
of Argument in Prepositional Phrase Attachment.
Computational Linguistics, 32 (3), 341-377.
The New Oxford Dictionary of English. 1998. (J.
Pearsall, Ed.). Oxford: Clarendon Press.
Thomas P. O?Hara. 2005. Empirical Acquisition of
Conceptual Distinctions via Dictionary
Definitions. Ph.D. Thesis. New Mexico State .
The Oxford Dictionary of English. 2003. (A.
Stevension and C. Soanes, Eds.). Oxford:
Clarendon Press.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
& Jan Svartik. (1985). A comprehensive grammar
of the English language. London: Longman.
Table 1. SemEval-2007 Preposition Disambiguation
Prepostition Senses Ent Perp
Number of Instances
Fine-Grained Recall
Participating Teams Baselines
Total Training Test Melb KU IRST
First
Sense
Freq
Sense
about 6 0.63 1.54 1074 710 364 0.885 0.934 0.780 0.885 0.885
above 9 1.80 3.49 71 48 23 0.652 0.522 0.565 0.043 0.609
across 3 0.23 1.17 470 319 151 0.960 0.960 0.914 0.960 0.960
after 11 2.15 4.44 156 103 53 0.472 0.585 0.585 0.434 0.434
against 10 1.89 3.69 287 195 92 0.880 0.793 0.826 0.446 0.435
along 4 0.30 1.23 538 365 173 0.954 0.954 0.936 0.954 0.954
among 4 1.55 2.93 150 100 50 0.660 0.680 0.620 0.300 0.300
around 6 2.05 4.13 490 335 155 0.561 0.535 0.381 0.155 0.452
as 2 0.00 1.00 258 174 84 1.000 1.000 0.988 1.000 1.000
at 12 2.38 5.21 1082 715 367 0.790 0.662 0.646 0.425 0.425
before 4 1.33 2.51 67 47 20 0.600 0.850 0.800 0.450 0.450
behind 9 1.31 2.47 206 138 68 0.662 0.676 0.471 0.662 0.662
beneath 6 1.22 2.33 85 57 28 0.714 0.679 0.750 0.571 0.571
beside 3 0.00 1.00 91 62 29 1.000 1.000 1.000 1.000 1.000
between 9 2.11 4.31 313 211 102 0.814 0.765 0.892 0.422 0.422
by 22 2.53 5.77 758 510 248 0.730 0.556 0.391 0.000 0.371
down 5 1.18 2.26 485 332 153 0.654 0.647 0.680 0.438 0.438
during 2 1.00 2.00 120 81 39 0.769 0.564 0.667 0.615 0.385
for 15 2.84 7.17 1429 951 478 0.573 0.395 0.456 0.036 0.238
from 16 2.85 7.21 1784 1206 578 0.642 0.415 0.512 0.279 0.279
in 15 2.81 7.01 2085 1397 688 0.561 0.436 0.494 0.362 0.362
inside 5 1.63 3.10 105 67 38 0.579 0.579 0.605 0.368 0.526
into 10 2.14 4.41 901 604 297 0.616 0.539 0.586 0.290 0.451
like 7 1.26 2.40 391 266 125 0.856 0.808 0.592 0.120 0.768
of 20 3.14 8.80 4482 3004 1478 0.681 0.374 0.144 0.000 0.205
off 7 1.16 2.23 237 161 76 0.658 0.776 0.408 0.171 0.763
on 25 3.42 10.68 1313 872 441 0.624 0.469 0.351 0.218 0.206
onto 3 0.60 1.52 175 117 58 0.879 0.879 0.776 0.879 0.879
over 17 2.52 5.73 298 200 98 0.510 0.510 0.480 0.010 0.327
round 8 2.31 4.95 263 181 82 0.610 0.512 0.000 0.037 0.378
through 16 2.71 6.54 649 441 208 0.524 0.538 0.481 0.322 0.495
to 17 2.43 5.38 1755 1183 572 0.745 0.579 0.558 0.322 0.322
towards 6 0.71 1.63 316 214 102 0.931 0.873 0.833 0.873 0.873
with 18 3.05 8.27 1769 1191 578 0.699 0.455 0.635 0.149 0.249
Total 332 24653 16557 8096 0.693 0.547 0.496 0.289 0.396
29
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 30?35,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 07: Coarse-Grained English All-Words Task
Roberto Navigli
Universita` di Roma ?La Sapienza?
Dipartimento di Informatica
Via Salaria, 00198 - Roma Italy
navigli@di.uniroma1.it
Kenneth C. Litkowski
CL Research
9208 Gue Road
Damascus MD 20872
ken@clres.com
Orin Hargraves
Lexicographer
orinhargraves
@googlemail.com
Abstract
This paper presents the coarse-grained En-
glish all-words task at SemEval-2007. We
describe our experience in producing a
coarse version of the WordNet sense inven-
tory and preparing the sense-tagged corpus
for the task. We present the results of par-
ticipating systems and discuss future direc-
tions.
1 Introduction
It is commonly thought that one of the major obsta-
cles to high-performance Word Sense Disambigua-
tion (WSD) is the fine granularity of sense inven-
tories. State-of-the-art systems attained a disam-
biguation accuracy around 65% in the Senseval-3
all-words task (Snyder and Palmer, 2004), where
WordNet (Fellbaum, 1998) was adopted as a ref-
erence sense inventory. Unfortunately, WordNet is
a fine-grained resource, encoding sense distinctions
that are difficult to recognize even for human an-
notators (Edmonds and Kilgarriff, 2002). Making
WSD an enabling technique for end-to-end applica-
tions clearly depends on the ability to deal with rea-
sonable sense distinctions.
The aim of this task was to explicitly tackle the
granularity issue and study the performance of WSD
systems on an all-words basis when a coarser set
of senses is provided for the target words. Given
the need of the NLP community to work on freely
available resources, the solution of adopting a dif-
ferent computational lexicon is not viable. On the
other hand, the production of a coarse-grained sense
inventory is not a simple task. The main issue
is certainly the subjectivity of sense clusters. To
overcome this problem, different strategies can be
adopted. For instance, in the OntoNotes project
(Hovy et al, 2006) senses are grouped until a 90%
inter-annotator agreement is achieved. In contrast,
as we describe in this paper, our approach is based
on a mapping to a previously existing inventory
which encodes sense distinctions at different levels
of granularity, thus allowing to induce a sense clus-
tering for the mapped senses.
We would like to mention that another SemEval-
2007 task dealt with the issue of sense granularity
for WSD, namely Task 17 (subtask #1): Coarse-
grained English Lexical Sample WSD. In this paper,
we report our experience in organizing Task 07.
2 Task Setup
The task required participating systems to annotate
open-class words (i.e. nouns, verbs, adjectives, and
adverbs) in a test corpus with the most appropriate
sense from a coarse-grained version of the WordNet
sense inventory.
2.1 Test Corpus
The test data set consisted of 5,377 words of run-
ning text from five different articles: the first three
(in common with Task 17) were obtained from the
WSJ corpus, the fourth was the Wikipedia entry for
computer programming1, the fifth was an excerpt of
Amy Steedman?s Knights of the Art, biographies of
Italian painters2. We decided to add the last two
1http://en.wikipedia.org/wiki/Computer programming
2http://www.gutenberg.org/etext/529
30
article domain words annotated
d001 JOURNALISM 951 368
d002 BOOK REVIEW 987 379
d003 TRAVEL 1311 500
d004 COMPUTER SCIENCE 1326 677
d005 BIOGRAPHY 802 345
total 5377 2269
Table 1: Statistics about the five articles in the test
data set.
texts to the initial dataset as we wanted the corpus to
have a size comparable to that of previous editions
of all-words tasks.
In Table 1 we report the domain, number of run-
ning words, and number of annotated words for the
five articles. We observe that articles d003 and d004
are the largest in the corpus (they constitute 51.87%
of it).
2.2 Creation of a Coarse-Grained Sense
Inventory
To tackle the granularity issue, we produced a
coarser-grained version of the WordNet sense inven-
tory3 based on the procedure described by Navigli
(2006). The method consists of automatically map-
ping WordNet senses to top level, numbered entries
in the Oxford Dictionary of English (ODE, (Soanes
and Stevenson, 2003)). The semantic mapping be-
tween WordNet and ODE entries was obtained in
two steps: first, we disambiguated with the SSI algo-
rithm (Navigli and Velardi, 2005) the definitions of
the two dictionaries, together with additional infor-
mation (hypernyms and domain labels); second, for
each WordNet sense, we determined the best match-
ing ODE coarse entry. As a result, WordNet senses
mapped to the same ODE entry were assigned to the
same sense cluster. WordNet senses with no match
were associated with a singleton sense.
In contrast to the automatic method above, the
sense mappings for all the words in our test cor-
pus were manually produced by the third author, an
expert lexicographer, with the aid of a mapping in-
terface. Not all the words in the corpus could be
mapped directly for several reasons: lacking entries
in ODE (e.g. adjectives underlying and shivering),
3We adopted WordNet 2.1, available from:
http://wordnet.princeton.edu
different spellings (e.g. after-effect vs. aftereffect,
halfhearted vs. half-hearted, etc.), derivatives (e.g.
procedural, gambler, etc.). In most of the cases, we
asked the lexicographer to map senses of the orig-
inal word to senses of lexically-related words (e.g.
WordNet senses of procedural were mapped to ODE
senses of procedure, etc.). When this mapping was
not straightforward, we just adopted the WordNet
sense inventory for that word.
We released the entire sense groupings (those in-
duced from the manual mapping for words in the
test set plus those automatically derived on the other
words) and made them available to the participants.
2.3 Sense Annotation
All open-class words (i.e. nouns, verbs, adjectives,
and adverbs) with an existing sense in the WordNet
inventory were manually annotated by the third au-
thor. Multi-word expressions were explicitly iden-
tified in the test set and annotated as such (this was
made to allow a fair comparison among systems in-
dependent of their ability to identify multi-word ex-
pressions).
We excluded auxiliary verbs, uncovered phrasal
and idiomatic verbs, exclamatory uses, etc. The
annotator was allowed to tag words with multiple
coarse senses, but was asked to make a single sense
assignment whenever possible.
The lexicographer annotated an overall number
of 2,316 content words. 47 (2%) of them were ex-
cluded because no WordNet sense was deemed ap-
propriate. The remaining 2,269 content words thus
constituted the test data set. Only 8 of them were as-
signed more than one sense: specifically, two coarse
senses were assigned to a single word instance4 and
two distinct fine-grained senses were assigned to 7
word instances. This was a clear hint that the sense
clusters were not ambiguous for the vast majority of
words.
In Table 2 we report information about the pol-
ysemy of the word instances in the test set. Over-
all, 29.88% (678/2269) of the word instances were
monosemous (according to our coarse sense inven-
tory). The average polysemy of the test set with the
coarse-grained sense inventory was 3.06 compared
to an average polysemy with the WordNet inventory
4d005.s004.t015
31
polysemy N V A R all
monosemous 358 86 141 93 678
polysemous 750 505 221 115 1591
total 1108 591 362 208 2269
Table 2: Statistics about the test set polysemy (N =
nouns, V = verbs, A = adjectives, R = adverbs).
of 6.18.
2.4 Inter-Annotator Agreement
Recent estimations of the inter-annotator agreement
when using the WordNet inventory report figures of
72.5% agreement in the preparation of the English
all-words test set at Senseval-3 (Snyder and Palmer,
2004) and 67.3% on the Open Mind Word Expert an-
notation exercise (Chklovski and Mihalcea, 2002).
As the inter-annotator agreement is often consid-
ered an upper bound for WSD systems, it was de-
sirable to have a much higher number for our task,
given its coarse-grained nature. To this end, beside
the expert lexicographer, a second author indepen-
dently performed part of the manual sense mapping
(590 word senses) described in Section 2.2. The
pairwise agreement was 86.44%.
We repeated the same agreement evaluation on
the sense annotation task of the test corpus. A sec-
ond author independently annotated part of the test
set (710 word instances). The pairwise agreement
between the two authors was 93.80%. This figure,
compared to those in the literature for fine-grained
human annotations, gives us a clear indication that
the agreement of human annotators strictly depends
on the granularity of the adopted sense inventory.
3 Baselines
We calculated two baselines for the test corpus: a
random baseline, in which senses are chosen at
random, and the most frequent baseline (MFS), in
which we assign the first WordNet sense to each
word in the dataset.
Formally, the accuracy of the random baseline
was calculated as follows:
BLRand = 1|T |
|T |?
i=1
1
|CoarseSenses(wi)|
where T is our test corpus, wi is the i-th word
instance in T , and CoarseSenses(wi) is the set of
coarse senses for wi according to the sense cluster-
ing we produced as described in Section 2.2.
The accuracy of the MFS baseline was calculated
as:
BLMFS = 1|T |
|T |?
i=1
?(wi, 1)
where ?(wi, k) equals 1 when the k-th sense of
word wi belongs to the cluster(s) manually associ-
ated by the lexicographer to word wi (0 otherwise).
Notice that our calculation of the MFS is based on
the frequencies in the SemCor corpus (Miller et al,
1993), as we exploit WordNet sense rankings.
4 Results
12 teams submitted 14 systems overall (plus two
systems from a 13th withdrawn team that we will
not report). According to the SemEval policy for
task organizers, we remark that the system labelled
as UOR-SSI was submitted by the first author (the
system is based on the Structural Semantic Inter-
connections algorithm (Navigli and Velardi, 2005)
with a lexical knowledge base composed by Word-
Net and approximately 70,000 relatedness edges).
Even though we did not specifically enrich the al-
gorithm?s knowledge base on the task at hand, we
list the system separately from the overall ranking.
The results are shown in Table 3. We calcu-
lated a MFS baseline of 78.89% and a random base-
line of 52.43%. In Table 4 we report the F1 mea-
sures for all systems where we used the MFS as a
backoff strategy when no sense assignment was at-
tempted (this possibly reranked 6 systems - marked
in bold in the table - which did not assign a sense
to all word instances in the test set). Compared
to previous results on fine-grained evaluation exer-
cises (Edmonds and Kilgarriff, 2002; Snyder and
Palmer, 2004), the systems? results are much higher.
On the other hand, the difference in performance
between the MFS baseline and state-of-the-art sys-
tems (around 5%) on coarse-grained disambiguation
is comparable to that of the Senseval-3 all-words ex-
ercise. However, given the novelty of the task we
believe that systems can achieve even better perfor-
32
System A P R F1
NUS-PT 100.0 82.50 82.50 82.50
NUS-ML 100.0 81.58 81.58 81.58
LCC-WSD 100.0 81.45 81.45 81.45
GPLSI 100.0 79.55 79.55 79.55
BLMFS 100.0 78.89 78.89 78.89
UPV-WSD 100.0 78.63 78.63 78.63
TKB-UO 100.0 70.21 70.21 70.21
PU-BCD 90.1 69.72 62.80 66.08
RACAI-SYNWSD 100.0 65.71 65.71 65.71
SUSSX-FR 72.8 71.73 52.23 60.44
USYD 95.3 58.79 56.02 57.37
UOFL 92.7 52.59 48.74 50.60
SUSSX-C-WD 72.8 54.54 39.71 45.96
SUSSX-CR 72.8 54.30 39.53 45.75
UOR-SSI? 100.0 83.21 83.21 83.21
Table 3: System scores sorted by F1 measure (A =
attempted, P = precision, R = recall, F1 = F1 mea-
sure, ?: system from one of the task organizers).
mance by heavily exploiting the coarse nature of the
sense inventory.
In Table 5 we report the results for each of the
five articles. The interesting aspect of the table is
that documents from some domains seem to have
predominant senses different from those in Sem-
Cor. Specifically, the MFS baseline performs more
poorly on documents d004 and d005, from the
COMPUTER SCIENCE and BIOGRAPHY domains
respectively. We believe this is due to the fact that
these documents have specific predominant senses,
which correspond less often to the most frequent
sense in SemCor than for the other three documents.
It is also interesting to observe that different systems
perform differently on the five documents (we high-
light in bold the best performing systems on each
article).
Finally, we calculated the systems? performance
by part of speech. The results are shown in Table
6. Again, we note that different systems show dif-
ferent performance depending on the part-of-speech
tag. Another interesting aspect is that the perfor-
mance of the MFS baseline is very close to state-of-
the-art systems for adjectives and adverbs, whereas
it is more than 3 points below for verbs, and around
5 for nouns.
System F1
NUS-PT 82.50
NUS-ML 81.58
LCC-WSD 81.45
GPLSI 79.55
BLMFS 78.89
UPV-WSD 78.63
SUSSX-FR 77.04
TKB-UO 70.21
PU-BCD 69.72
RACAI-SYNWSD 65.71
SUSSX-C-WD 64.52
SUSSX-CR 64.35
USYD 58.79
UOFL 54.61
UOR-SSI? 83.21
Table 4: System scores sorted by F1 measure with
MFS adopted as a backoff strategy when no sense
assignment is attempted (?: system from one of the
task organizers). Systems affected are marked in
bold.
System N V A R
NUS-PT 82.31 78.51 85.64 89.42
NUS-ML 81.41 78.17 82.60 90.38
LCC-WSD 80.69 78.17 85.36 87.98
GPLSI 80.05 74.45 82.32 86.54
BLMFS 77.44 75.30 84.25 87.50
UPV-WSD 79.33 72.76 84.53 81.25
TKB-UO 70.76 62.61 78.73 74.04
PU-BCD 71.41 59.69 66.57 55.67
RACAI-SYNWSD 64.02 62.10 71.55 75.00
SUSSX-FR 68.09 51.02 57.38 49.38
USYD 56.06 60.43 58.00 54.31
UOFL 57.65 48.82 25.87 60.80
SUSSX-C-WD 52.18 35.64 42.95 46.30
SUSSX-CR 51.87 35.44 42.95 46.30
UOR-SSI? 84.12 78.34 85.36 88.46
Table 6: System scores by part-of-speech tag (N
= nouns, V = verbs, A = adjectives, R = adverbs)
sorted by overall F1 measure (best scores are marked
in bold, ?: system from one of the task organizers).
33
d001 d002 d003 d004 d005
System P R P R P R P R P R
NUS-PT 88.32 88.32 88.13 88.13 83.40 83.40 76.07 76.07 81.45 81.45
NUS-ML 86.14 86.14 88.39 88.39 81.40 81.40 76.66 76.66 79.13 79.13
LCC-WSD 87.50 87.50 87.60 87.60 81.40 81.40 75.48 75.48 80.00 80.00
GPLSI 83.42 83.42 86.54 86.54 80.40 80.40 73.71 73.71 77.97 77.97
BLMFS 85.60 85.60 84.70 84.70 77.80 77.80 75.19 75.19 74.20 74.20
UPV-WSD 84.24 84.24 80.74 80.74 76.00 76.00 77.11 77.11 77.10 77.10
TKB-UO 78.80 78.80 72.56 72.56 69.40 69.40 70.75 70.75 58.55 58.55
PU-BCD 77.16 67.94 75.52 67.55 64.96 58.20 68.86 61.74 64.42 60.87
RACAI-SYNWSD 71.47 71.47 72.82 72.82 66.80 66.80 60.86 60.86 59.71 59.71
SUSSX-FR 79.10 57.61 73.72 53.30 74.86 52.40 67.97 48.89 65.20 51.59
USYD 62.53 61.69 59.78 57.26 60.97 57.80 60.57 56.28 47.15 45.51
UOFL 61.41 59.24 55.93 52.24 48.00 45.60 53.42 47.27 44.38 41.16
SUSSX-C-WD 66.42 48.37 61.31 44.33 55.14 38.60 50.72 36.48 42.13 33.33
SUSSX-CR 66.05 48.10 60.58 43.80 59.14 41.40 48.67 35.01 40.29 31.88
UOR-SSI? 86.14 86.14 85.49 85.49 79.60 79.60 86.85 86.85 75.65 75.65
Table 5: System scores by article (best scores are marked in bold, ?: system from one of the task organizers).
5 Systems Description
In order to allow for a critical and comparative in-
spection of the system results, we asked the partici-
pants to answer some questions about their systems.
These included information about whether:
1. the system used semantically-annotated and
unannotated resources;
2. the system used the MFS as a backoff strategy;
3. the system used the coarse senses provided by
the organizers;
4. the system was trained on some corpus.
We believe that this gives interesting information
to provide a deeper understanding of the results. We
summarize the participants? answers to the question-
naires in Table 7. We report about the use of seman-
tic resources as well as semantically annotated cor-
pora (SC = SemCor, DSO = Defence Science Organ-
isation Corpus, SE = Senseval corpora, OMWE =
Open Mind Word Expert, XWN = eXtended Word-
Net, WN = WordNet glosses and/or relations, WND
= WordNet Domains), as well as information about
the use of unannotated corpora (UC), training (TR),
MFS (based on the SemCor sense frequencies), and
the coarse senses provided by the organizers (CS).
As expected, several systems used lexico-semantic
information from the WordNet semantic network
and/or were trained on the SemCor semantically-
annotated corpus.
Finally, we point out that all the systems perform-
ing better than the MFS baseline adopted it as a
backoff strategy when they were not able to output a
sense assignment.
6 Conclusions and Future Directions
It is commonly agreed that Word Sense Disambigua-
tion needs emerge and show its usefulness in end-
to-end applications: after decades of research in the
field it is still unclear whether WSD can provide
a relevant contribution to real-world applications,
such as Information Retrieval, Question Answering,
etc. In previous Senseval evaluation exercises, state-
of-the-art systems achieved performance far below
70% and even the agreement between human anno-
tators was discouraging. As a result of the discus-
sion at the Senseval-3 workshop in 2004, one of the
aims of SemEval-2007 was to tackle the problems
at the roots of WSD. In this task, we dealt with the
granularity issue which is a major obstacle to both
system and human annotators. In the hope of over-
coming the current performance upper bounds, we
34
System SC DSO SE OMWE XWN WN WND OTHER UC TR MFS CS
GPLSI
? ? ? ? ? ? ? ? ? ? ? ?
LCC-WSD
? ? ? ? ? ? ? ? ? ? ? ?
NUS-ML
? ? ? ? ? ? ? ? ? ? ? ?
NUS-PT
? ? ? ? ? ? ? Parallel corpus ? ? ? ?
PU-BCD
? ? ? ? ? ? ? ? ? ? ? ?
RACAI-SYNWSD ? ? ? ? ? ? ? ? ? ? ? ?
SUSSX-C-WD ? ? ? ? ? ? ? ? ? ? ? ?
SUSSX-CR ? ? ? ? ? ? ? ? ? ? ? ?
SUSSX-FR ? ? ? ? ? ? ? ? ? ? ? ?
TKB-UO ? ? ? ? ? ? ? ? ? ? ? ?
UOFL ? ? ? ? ? ? ? ? ? ? ? ?
UOR-SSI? ? ? ? ? ? ? ? SSI LKB ? ? ? ?
UPV-WSD ? ? ? ? ? ? ? ? ? ? ? ?
USYD
? ? ? ? ? ? ? ? ? ? ? ?
Table 7: Information about participating systems (SC = SemCor, DSO = Defence Science Organisation
Corpus, SE = Senseval corpora, OMWE = Open Mind Word Expert, XWN = eXtended WordNet, WN =
WordNet glosses and/or relations, WND = WordNet Domains, UC = use of unannotated corpora, TR = use
of training, MFS = most frequent sense backoff strategy, CS = use of coarse senses from the organizers, ?:
system from one of the task organizers).
proposed the adoption of a coarse-grained sense in-
ventory. We found the results of participating sys-
tems interesting and stimulating. However, some
questions arise. First, it is unclear whether, given
the novelty of the task, systems really achieved the
state of the art or can still improve their performance
based on a heavier exploitation of coarse- and fine-
grained information from the adopted sense inven-
tory. We observe that, on a technical domain such
as computer science, most supervised systems per-
formed worse due to the nature of their training set.
Second, we still need to show that coarse senses can
be useful in real applications. Third, a full coarse
sense inventory is not yet available: this is a major
obstacle to large-scale in vivo evaluations. We be-
lieve that these aspects deserve further investigation
in the years to come.
Acknowledgments
This work was partially funded by the Interop NoE (508011),
6th European Union FP. We would like to thank Martha Palmer
for providing us the first three texts of the test corpus.
References
Tim Chklovski and Rada Mihalcea. 2002. Building a sense
tagged corpus with open mind word expert. In Proc. of ACL
2002 Workshop on WSD: Recent Successes and Future Di-
rections. Philadelphia, PA.
Philip Edmonds and Adam Kilgarriff. 2002. Introduction to the
special issue on evaluating word sense disambiguation sys-
tems. Journal of Natural Language Engineering, 8(4):279?
291.
Christiane Fellbaum, editor. 1998. WordNet: an Electronic
Lexical Database. MIT Press.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The
90% solution. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Comp. Volume, pages 57?
60, New York City, USA.
George A. Miller, Claudia Leacock, Randee Tengi, and Ross T.
Bunker. 1993. A semantic concordance. In Proceedings of
the ARPA Workshop on Human Language Technology, pages
303?308, Princeton, NJ, USA.
Roberto Navigli and Paola Velardi. 2005. Structural seman-
tic interconnections: a knowledge-based approach to word
sense disambiguation. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence (PAMI), 27(7):1063?1074.
Roberto Navigli. 2006. Meaningful clustering of senses helps
boost word sense disambiguation performance. In Proc. of
the 44th Annual Meeting of the Association for Computa-
tional Linguistics joint with the 21st International Confer-
ence on Computational Linguistics (COLING-ACL 2006),
pages 105?112. Sydney, Australia.
Benjamin Snyder and Martha Palmer. 2004. The english all-
words task. In Proc. of ACL 2004 SENSEVAL-3 Workshop,
pages 41?43. Barcelona, Spain.
Catherine Soanes and Angus Stevenson, editors. 2003. Oxford
Dictionary of English. Oxford University Press.
35
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 113?116,
Prague, June 2007. c?2007 Association for Computational Linguistics
CLR: Integration of FrameNet in a Text Representation System
Ken. Litkowski
CL Research
9208 Gue Road
Damascus, MD 20872
ken@clres.com
Abstract
In SemEval-2007, CL Research participated in
the task for Frame Semantic Structure
Extraction. Participation in this task was used as
the vehicle for efforts to integrate and exploit
FrameNet in a comprehensive text processing
system. In particular, this involved steps to build
a FrameNet dictionary with CL Research?s
DIMAP dictionary software and to use this
dictionary (along with its semantic network
processing capabilities) in processing text into
XML representations. Implementation of the
entire integrated package is only in its initial
stages and was used to make only a bare
submission of frame identification. On this task,
over all texts, a recall of 0.372, a precision of
0.553, and an F-score of 0.445 were achieved.
Considering only targets included in the DIMAP
FrameNet dictionary, the overall F-score is
0.605. These results, competitive with the top
scoring system, support continued attempts at a
dictionary-based approach to frame structure
extraction.
1 Introduction
CL Research participated in the SemEval-2007 task
for Frame Semantic Structure Extraction. In
participating in this task, we integrated the use of
FrameNet in the Text Parser component of the CL
Research Knowledge Management System (KMS). In
particular, we created a FrameNet dictionary from
the FrameNet databases with the CL Research
DIMAP dictionary software and used this dictionary
as a lexical resource. This new lexical resource was
integrated in the same manner as other lexical
resources (including WordNet and the Oxford
Dictionary of English (ODE, 2004)). As such, the
FrameNet dictionary was available as the basis for
sense disambiguation. In the CL Research Text
Parser, this integration was seamless, in which
disambiguation can be performed against several
lexical resources. This work attempts to expand on
semantic role labeling experiments in Senseval-3
(Litkowski, 2004a, and Litkowski, 2004b).
In the following sections, we first describe the
overall structure of the CL Research Knowledge
Management System and Text Parser, describing
their general parsing and text analysis routines. Next,
we describe the creation of the FrameNet dictionary,
particularly identifying design considerations to
exploit the richness of the FrameNet data. In section
4, we describe our submission for the SemEval task.
In section 5, we describe our results. Finally, we
identify next steps that can be taken within the CL
Research KMS and DIMAP environments to extend
the FrameNet data.
2 CL Research Text Processing
The CL Research Knowledge Management System
(KMS) is an integrated environment for performing
several higher level applications, particularly
question answering and summarization. The
underlying architecture of KMS relies on an XML
representation of texts that captures discourse
structure and discourse elements, particularly noun
phrases, verbs, and semantic roles (predominantly as
reified in prepositions). The texts that are represented
include primarily full texts as they may appear in
several forms, but also include questions, topic
specifications for which summaries are desired, and
keyword search expressions.
Text processing is an integrated component of
KMS, but for large-scale processing, a separate
system, the CL Research Text Parser is frequently
used. The same modules are used for both, with
different interfaces. Text processing is performed in
two stages: (1) syntactic parsing, generating a parse
113
tree as output; and (2) discourse analysis, analyzing
the parse tree and building sets of data used to record
information about discourse segments (i.e., clauses),
discourse entities (primarily noun phrases, but also
including predicate adjective and adverb phrases),
verbs, and semantic relations (prepositions). After the
data structures are completed for an entire text during
the discourse analysis phase, they are used to create
a nested XML representation showing all the
elements and providing attributes of each component.
The parser is grammar-based and produces a
constituent structure, with non-terminals representing
syntactic components and leaves corresponding to the
words of the sentence. The parser generates some
dependency relationships by using dynamic grammar
rules added during parsing, particularly through sets
of subcategorization patterns associated with verbs
(and some other words in the dictionary). This allows
the identification of such things as sentence subjects,
preposition phrase attachments, and clause
attachments. Syntactic ambiguity is handled by
carrying forward a variable number of possible
parses (usually 40, but user adjustable for any
number), eliminating parses that are less well-formed.
The discourse analysis phase includes an
anaphora resolution component and detailed semantic
analyses of each sentence element. Many dependency
relationships are identified during this phase. The
semantic analysis includes a disambiguation
component for all words (using one or more of the
integrated dictionaries). The semantic analysis also
identifies (for later use in the XML representation)
relations between various sentence elements,
particularly identifying the complement and
attachment point for prepositions.
1
To make use of the FrameNet data, it is first
necessary to put it into a form that can be used
effectively. For this purpose, a DIMAP dictionary is
used. Such dictionaries are accessible using btree
lookup, so rapid access is ensured during large-scale
text processing. Syntactic parsing proceeds at about
eight or nine hundred sentences per minute; the
discourse analysis phase is roughly the same
complexity. The result is that sentences are normally
processed at 300 to 500 sentences per minute.
3 A FrameNet Dictionary
The integration of FrameNet into KMS and Text
Parser is generally handled in the same way that
other dictionaries are used. Specifically, there is a
call to a disambiguation component to identify the
applicable sense. After this, FrameNet data are used
in a slightly different way. Disambiguation proceeds
sequentially through the words in a sentence, but the
labeling of components with frame elements is
performed only after a sentence has been fully
discourse-analyzed. This is necessary because the
location of frame elements requires full knowledge of
all components in a sentence, not just those which
precede a given target (i.e., in left-to-right parsing
and discourse analysis).
The main issue is the design of a FrameNet
dictionary; DIMAP provides sufficient capability to
capture all aspects of the FrameNet data
(Ruppenhofer, et al, 2006) in various types of built-
in data structures. First, it is necessary to capture
each lexical unit and to create a distinct sense for
each frame in which a lexeme is used. The current
FrameNet DIMAP dictionary contains 7575 entries,
with many entries having multiple senses.
2
 For each
sense, the FrameNet part of speech, the definition, the
frame name, the ID number, and the definition source
(identified as FN or COD, the Concise Oxford
Dictionary) are captured from the FrameNet files.
3
If there is an associated FrameNet lexical entry
file that contains frame element realizations, this
information is also captured in the appropriate sense.
In DIMAP, this is done in an attribute-value feature
structure. Each non-empty feature element realization
in the FrameNet data is captured. A DIMAP feature
attribute is constructed as a conflation of the phrase
type and the grammatical function, e.g. ?NP (Dep)?.
The feature value is a conflation of the valence unit
1
At present, the analysis of the complement and
attachment points examines only the highest ranked
attachment point, rather than examining other
possibilities (which are frequently identified in
parsing).
2
We unwittingly used an August 2006 version of
FrameNet, not the latest version that incorporated
frames developed in connection with full-text
annotation. This affects our results, as described below.
3
The FrameNet dictionary data is captured using
FrameNet Explorer, a Windows interface for exploring
FrameNet frames, available for free download at CL
Research (http://www.clres.com).
114
frame element name and the number of annotations in
the FrameNet corpus, e.g., ?Cognizer (28)?. This
manner of capturing FrameNet information is done to
facilitate processing; the DIMAP feature structure is
frequently used to access information about lexical
items. Further experience will assess the utility of this
format.
Frames and frame elements are captured in the
same dictionary. However, they are not treated as
lexical units, but rather as ?meta-entries?. In the
DIMAP dictionary, frame names are entered as
dictionary entries beginning with the symbol ?#? and
frame elements are entered beginning with the symbol
?@?. In these entries, different data structures of a
DIMAP entry are used to capture the different kinds
of relations between frames and frame elements (i.e.,
the frame-to-frame relations) that are found in the
FrameNet data. Thus, a frame will have a ?frame-
element? link to each of its frame elements. It will
also have attribute-value features listing its frame
elements and their type (core, peripheral, or extra-
thematic).
With a dictionary structured as described, it is
possible not only to look up a lexical unit, but also to
traverse the various links that are reachable from a
given entry. Specifically, when a lexical unit is
recognized in processing the text, the first step is to
retrieve the entry for that item and to use the frame
element realization patterns to disambiguate among
the senses (if more than one of the same part of
speech). After a sentence has been completely
processed (as described above), the meta-entries
associated with each lexical unit can be examined
(and appropriate traversals to other meta-entries can
be followed) in order to identify which sentence
constituents fill the frame elements.
Specific routines for traversing the various
FrameNet links have not yet been developed.
However, this is primarily a matter of assessing
which traversals would be useful. Similar traversals
are used with other lexical resources, such as
WordNet, where, for example, inheritance hierarchies
and other WordNet relation links are routinely
traversed.
4 The SemEval FrameNet Submission
To participate in the SemEval FrameNet task, the
three test texts were wrapped into a standard XML
representation used in processing texts. This wrapper
consists only of an overall <DOCS> tag, a subtag
<DOC> for each document, and a <TEXT> tag
surrounding the actual text. The text was included
with some minor changes. Since Text Parser includes
a sentence splitter, we had to make sure that the texts
would split into the identifiable sentences as given on
each line of the texts. Thus, for headers in the text,
we added a period at the end. Once we were sure that
the same number of sentences would be recognized,
we processed the texts using Text Parser, as
described in section 2.
4
As mentioned above, the FrameNet dictionary
lookup occurred in a separate traversal of the parse
tree after the discourse analysis phase. During this
traversal, the base form of each noun, verb, adjective,
or adverb content word was looked up in the
FrameNet dictionary. If there was no entry for the
word, no further FrameNet processing was
performed. When an entry was found, each sense of
the appropriate part of speech is examined in order to
disambiguate among multiple senses. A score is
computed for each sense and the score with the
highest sense was selected.
5
Having identified a sense in the FrameNet
dictionary, this was interpreted as finding a
FrameNet target, with the FrameNet frame as
identified in the lexical entry. Since the character
positions of each word in the source sentence are
included in the parse tree information, this
information was captured for inclusion in the output.
(Further implementation to identify the frame
elements associated with the target has not been
completed at this time. As a result, our submission
was only a partial completion of the FrameNet task.)
After completing the processing of each sentence,
4
To make a submission for the FrameNet task, it was
necessary to initialize an XML object into which the
results could be inserted after processing each
sentence. This is not a usual component of Text Parser,
but was implemented solely for the purpose of
participating in this task.
5
At this time, all senses receive an identical score. The
first sense is selected. Senses are unsystematically
ordered as they were encountered in creating the
FrameNet dictionary. This will be extended to compute
a score based on the various frame element realization
patterns associated with each sense.
115
all FrameNet frame information that had been
identified was processed for inclusion in the XML
submission for this task. In particular, the annotation
sets required were incorporated into the XML object
that had been initialized. (Our annotation sets
included only the ?Target? layer.)  After all sentences
had been completed, the XML object was printed to
a file for submission.
5 Results
Our results are shown in Table 1, giving the recall,
precision, and F-score for each text and over all
texts. As indicated, these results are for only the
target identification subtask.
6
Table 1. Target Identification Scores
Text Recall Precision F-Score
Dublin 0.33403 0.53572 0.41237
China 0.51148 0.52525 0.51827
Iran 0.44828 0.66102 0.53425
All 0.37240 0.55337 0.44520
As indicated above, we used an early version of
the FrameNet databases that did not include all the
lexical units in the training and test texts. As a result,
we did not have FrameNet entries for 30 percent of
the words identified as targets in the test texts. Table
2 shows an estimate of the adjusted scores that would
result if those lexical items were included..
Table 2. Adjusted Target Identification Scores
Text Recall Precision F-Score
Dublin 0.53445 0.65140 0.58716
China 0.57037 0.62097 0.59459
Iran 0.61494 0.72789 0.66667
All 0.56144 0.65132 0.60305
The results in Table 1 rank third of the four
teams participating in this subtask. With the results
in Table 2, our performance would improve to first
for two of the texts and just below the top team for
the other text.
6 Future Steps
Participation in the FrameNet frame structure
extraction task has demonstrated the basic viability
of our approach. Many of the frames have been
recognized successfully. We have not yet examined
the extent to which the disambiguation among frames
is significant, particularly since there are not many
entries that have several senses. We have yet to
develop specific techniques for making use of the
frame element realization patterns. However, we
believe that a reasonable performance can be
expected since KMS and Text Parser produce output
that breaks sentences down into the types of
components that should be included as frame
elements.
The architecture of KMS, Text Parser, and
DIMAP provide significant opportunities for
extending our performance. In particular, since these
systems include the Oxford Dictionary of English, a
superset of the Concise Oxford Dictionary, there is
an opportunity for extending the FrameNet datasets.
The COD definitions in FrameNet can be mapped to
those in ODE and can be exploited to extend
FrameNet frames to lexical items not yet covered in
FrameNet.
References
Kenneth C. Litkowski. 2004a. Senseval-3 Task:
Automatic Labeling of Semantic Roles. In Senseval-
3: Third International Workshop on the Evaluation
of Systems for the Semantic Analysis of Text.
Association for Computational Linguistics. 9-12. 
Kenneth C. Litkowski. 2004b. Explorations in
Disambiguation Using XML Text Representation. In
Senseval-3: Third International Workshop on the
Evaluation of Systems for the Semantic Analysis of
Text. Association for Computational Linguistics.
141-146.
The Oxford Dictionary of English. 2003. (A. Stevension
and C. Soanes, Eds.). Oxford: Clarendon Press.
Josef Ruppenhofer, Michael Ellsworth, Miriam Petruck,
Christopher Johnson, and Jan Scheffxzyk. 2006.
FrameNet II: Extended Theory and Practice.
International Computer Science Institute, University
of California at Berkeley.
6
Corresponding to the ?-e -n -t? options of the scoring
program. In these tables, ?Dublin? refers to
IntroOfDublin, ?China? to ChinaOverview, and
?Iran? to workAdvances.
116
