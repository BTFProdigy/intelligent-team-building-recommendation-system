Recognizing Expressions of Commonsense Psychology in English Text
Andrew Gordon, Abe Kazemzadeh, Anish Nair and Milena Petrova
University of Southern California
Los Angeles, CA 90089 USA
gordon@ict.usc.edu, {kazemzad, anair, petrova}@usc.edu
Abstract
Many applications of natural language
processing technologies involve analyzing
texts that concern the psychological states
and processes of people, including their
beliefs, goals, predictions, explanations,
and plans. In this paper, we describe our
efforts to create a robust, large-scale lexi-
cal-semantic resource for the recognition
and classification of expressions of com-
monsense psychology in English Text.
We achieve high levels of precision and
recall by hand-authoring sets of local
grammars for commonsense psychology
concepts, and show that this approach can
achieve classification performance greater
than that obtained by using machine
learning techniques. We demonstrate the
utility of this resource for large-scale cor-
pus analysis by identifying references to
adversarial and competitive goals in po-
litical speeches throughout U.S. history.
1 Commonsense Psychology in Language
Across all text genres it is common to find words
and phrases that refer to the mental states of people
(their beliefs, goals, plans, emotions, etc.) and their
mental processes (remembering, imagining, priori-
tizing, problem solving). These mental states and
processes are among the broad range of concepts
that people reason about every day as part of their
commonsense understanding of human psychol-
ogy. Commonsense psychology has been studied
in many fields, sometimes using the terms Folk
psychology or Theory of Mind, as both a set of be-
liefs that people have about the mind and as a set
of everyday reasoning abilities.
Within the field of computational linguistics,
the study of commonsense psychology has not re-
ceived special attention, and is generally viewed as
just one of the many conceptual areas that must be
addressed in building large-scale lexical-semantic
resources for language processing. Although there
have been a number of projects that have included
concepts of commonsense psychology as part of a
larger lexical-semantic resource, e.g. the Berkeley
FrameNet Project (Baker et al, 1998), none have
attempted to achieve a high degree of breadth or
depth over the sorts of expressions that people use
to refer to mental states and processes.
The lack of a large-scale resource for the analy-
sis of language for commonsense psychological
concepts is seen as a barrier to the development of
a range of potential computer applications that in-
volve text analysis, including the following:
?  Natural language interfaces to mixed-initiative
planning systems (Ferguson & Allen, 1993;
Traum, 1993) require the ability to map ex-
pressions of users? beliefs, goals, and plans
(among other commonsense psychology con-
cepts) onto formalizations that can be ma-
nipulated by automated planning algorithms.
?  Automated question answering systems
(Voorhees & Buckland, 2002) require the abil-
ity to tag and index text corpora with the rele-
vant commonsense psychology concepts in
order to handle questions concerning the be-
liefs, expectations, and intentions of people.
? Research efforts within the field of psychology
that employ automated corpus analysis tech-
niques to investigate developmental and men-
tal illness impacts on language production, e.g.
Reboul & Sabatier?s (2001) study of the dis-
course of schizophrenic patients, require the
ability to identify all references to certain psy-
chological concepts in order to draw statistical
comparisons.
In order to enable future applications, we un-
dertook a new effort to meet this need for a lin-
guistic resource. This paper describes our efforts in
building a large-scale lexical-semantic resource for
automated processing of natural language text
about mental states and processes. Our aim was to
build a system that would analyze natural language
text and recognize, with high precision and recall,
every expression therein related to commonsense
psychology, even in the face of an extremely broad
range of surface forms. Each recognized expres-
sion would be tagged with an appropriate concept
from a broad set of those that participate in our
commonsense psychological theories.
Section 2 demonstrates the utility of a lexical-
semantic resource of commonsense psychology in
automated corpus analysis through a study of the
changes in mental state expressions over the course
of over 200 years of U.S. Presidential State-of-the-
Union Addresses. Section 3 of this paper describes
the methodology that we followed to create this
resource, which involved the hand authoring of
local grammars on a large scale. Section 4 de-
scribes a set of evaluations to determine the per-
formance levels that these local grammars could
achieve and to compare these levels to those of
machine learning approaches. Section 5 concludes
this paper with a discussion of the relative merits
of this approach to the creation of lexical-semantic
resources as compared to other approaches.
2 Applications to corpus analysis
One of the primary applications of a lexical-
semantic resource for commonsense psychology is
toward the automated analysis of large text cor-
pora. The research value of identifying common-
sense psychology expressions has been
demonstrated in work on children?s language use,
where researchers have manually annotated large
text corpora consisting of parent/child discourse
transcripts (Barsch & Wellman, 1995) and chil-
dren?s storybooks (Dyer et al, 2000). While these
previous studies have yielded interesting results,
they required enormous amounts of human effort
to manually annotate texts. In this section we aim
to show how a lexical-semantic resource for com-
monsense psychology can be used to automate this
annotation task, with an example not from the do-
main of children?s language acquisition, but rather
political discourse.
We conducted a study to determine how politi-
cal speeches have been tailored over the course of
U.S. history throughout changing climates of mili-
tary action. Specifically, we wondered if politi-
cians were more likely to talk about goals having
to do with conflict, competition, and aggression
during wartime than in peacetime. In order to
automatically recognize references to goals of this
sort in text, we used a set of local grammars
authored using the methodology described in Sec-
tion 3 of this paper. The corpus we selected to ap-
ply these concept recognizers was the U.S. State of
the Union Addresses from 1790 to 2003. The rea-
sons for choosing this particular text corpus were
its uniform distribution over time and its easy
availability in electronic form from Project Guten-
berg (www.gutenberg. net). Our set of local gram-
mars identified 4290 references to these goals in
this text corpus, the vast majority of them begin
references to goals of an adversarial nature (rather
than competitive). Examples of the references that
were identified include the following:
? They sought to use the rights and privileges
they had obtained in the United Nations, to
frustrate its purposes [adversarial-goal] and
cut down its powers as an effective agent of
world progress. (Truman, 1953)
? The nearer we come to vanquishing [adver-
sarial-goal] our enemies the more we inevita-
bly become conscious of differences among
the victors. (Roosevelt, 1945)
? Men have vied [competitive-goal] with each
other to do their part and do it well. (Wilson,
1918)
? I will submit to Congress comprehensive leg-
islation to strengthen our hand in combating
[adversarial-goal] terrorists. (Clinton, 1995)
Figure 1 summarizes the results of applying our
local grammars for adversarial and competitive
goals to the U.S. State of the Union Addresses. For
each year, the value that is plotted represents the
number of references to these concepts that were
identified per 100 words in the address. The inter-
esting result of this analysis is that references to
adversarial and competitive goals in this corpus
increase in frequency in a pattern that directly cor-
responds to the major military conflicts that the
U.S. has participated in throughout its history.
Each numbered peak in Figure 1 corresponds to
a period in which the U.S. was involved in a mili-
tary conflict. These are: 1) 1813, War of 1812, US
and Britain; 2) 1847, Mexican American War; 3)
1864, Civil War; 4) 1898, Spanish American War;
5) 1917, World War I; 6) 1943, World War II; 7)
1952, Korean War; 8) 1966, Vietnam War; 9)
1991, Gulf War; 10) 2002, War on Terrorism.
The wide applicability of a lexical-semantic re-
source for commonsense psychology will require
that the identified concepts are well defined and
are of broad enough scope to be relevant to a wide
range of tasks. Additionally, such a resource must
achieve high levels of accuracy in identifying these
concepts in natural language text. The remainder of
this paper describes our efforts in authoring and
evaluating such a resource.
3 Authoring recognition rules
The first challenge in building any lexical-semantic
resource is to identify the concepts that are to be
recognized in text and used as tags for indexing or
markup. For expressions of commonsense psy-
chology, these concepts must describe the broad
scope of people?s mental states and processes. An
ontology of commonsense psychology with a high
degree of both breadth and depth is described by
Gordon (2002). In this work, 635 commonsense
psychology concepts were identified through an
analysis of the representational requirements of a
corpus of 372 planning strategies collected from 10
real-world planning domains. These concepts were
grouped into 30 conceptual areas, corresponding to
various reasoning functions, and full formal mod-
els of each of these conceptual areas are being
authored to support automated inference about
commonsense psychology (Gordon & Hobbs,
2003). We adopted this conceptual framework in
our current project because of the broad scope of
the concepts in this ontology and its potential for
future integration into computational reasoning
systems.
The full list of the 30 concept areas identified is
as follows: 1) Managing knowledge, 2) Similarity
comparison, 3) Memory retrieval, 4) Emotions, 5)
Explanations, 6) World envisionment, 7) Execu-
tion envisionment, 8) Causes of failure, 9) Man-
aging expectations, 10) Other agent reasoning, 11)
Threat detection, 12) Goals, 13) Goal themes, 14)
Goal management, 15) Plans, 16) Plan elements,
17) Planning modalities, 18) Planning goals, 19)
Plan construction, 20) Plan adaptation, 21) Design,
22) Decisions, 23) Scheduling, 24) Monitoring, 25)
Execution modalities, 26) Execution control, 27)
Repetitive execution, 28) Plan following, 29) Ob-
servation of execution, and 30) Body interaction.
Our aim for this lexical-semantic resource was
to develop a system that could automatically iden-
tify every expression of commonsense psychology
in English text, and assign to them a tag corre-
sponding to one of the 635 concepts in this ontol-
ogy. For example, the following passage (from
William Makepeace Thackeray?s 1848 novel,
Vanity Fair) illustrates the format of the output of
this system, where references to commonsense
psychology concepts are underlined and followed
by a tag indicating their specific concept type de-
limited by square brackets:
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
1
7
9
0
1
7
9
8
1
8
0
6
1
8
1
4
1
8
2
2
1
8
3
0
1
8
3
8
1
8
4
6
1
8
5
4
1
8
6
2
1
8
7
0
1
8
7
8
1
8
8
6
1
8
9
8
1
9
0
6
1
9
1
4
1
9
2
2
1
9
3
0
1
9
3
9
1
9
4
7
1
9
5
5
1
9
6
3
1
9
7
1
1
9
7
9
1
9
8
7
1
9
9
7
Year
F
e
a
t
u
r
e
s
 
p
e
r
 
1
0
0
 
w
o
r
d
s
1
2
7
6
5
4
3
8
9 10
Figure 1.  Adversarial and competitive goals in the U.S. State of the Union Addresses from 1790-2003
Perhaps [partially-justified-proposition] she
had mentioned the fact [proposition] already to
Rebecca, but that young lady did not appear to
[partially-justified-proposition] have remem-
bered it [memory-retrieval]; indeed, vowed and
protested that she expected [add-expectation] to
see a number of Amelia's nephews and nieces.
She was quite disappointed [disappointment-
emotion] that Mr. Sedley was not married; she
was sure [justified-proposition] Amelia had said
he was, and she doted so on [liking-emotion] lit-
tle children.
The approach that we took was to author (by
hand) a set of local grammars that could be used to
identify each concept. For this task we utilized the
Intex Corpus Processor software developed by the
Laboratoire d'Automatique Documentaire et Lin-
guistique (LADL) of the University of Paris 7 (Sil-
berztein, 1999). This software allowed us to author
a set of local grammars using a graphical user in-
terface, producing lexical/syntactic structures that
can be compiled into finite-state transducers. To
simplify the authoring of these local grammars,
Intex includes a large-coverage English dictionary
compiled by Blandine Courtois, allowing us to
specify them at a level that generalized over noun
and verb forms. For example, there are a variety of
ways of expressing in English the concept of reaf-
firming a belief that is already held, as exemplified
in the following sentences:
1) The finding was confirmed by the new
data. 2) She told the truth, corroborating his
story. 3) He reaffirms his love for her. 4) We
need to verify the claim. 5) Make sure it is true.
Although the verbs in these sentences differ in
tense, the dictionaries in Intex allowed us to recog-
nize each using the following simple description:
(<confirm> by | <corroborate> | <reaffirm> |
<verify> | <make> sure)
While constructing local grammars for each of
the concepts in the original ontology of common-
sense psychology, we identified several conceptual
distinctions that were made in language that were
not expressed in the specific concepts that Gordon
had identified. For example, the original ontology
included only three concepts in the conceptual area
of memory retrieval (the sparsest of the 30 areas),
namely memory, memory cue, and memory re-
trieval. English expressions such as ?to forget? and
?repressed memory? could not be easily mapped
directly to one of these three concepts, which
prompted us to elaborate the original sets of con-
cepts to accommodate these and other distinctions
made in language. In the case of the conceptual
area of memory retrieval, a total of twelve unique
concepts were necessary to achieve coverage over
the distinctions evident in English.
These local grammars were authored one con-
ceptual area at a time. At the time of the writing of
this paper, our group had completed 6 of the origi-
nal 30 commonsense psychology conceptual areas.
The remainder of this paper focuses on the first 4
of the 6 areas that were completed, which were
evaluated to determine the recall and precision per-
formance of our hand-authored rules. These four
areas are Managing knowledge, Memory, Expla-
nations, and Similarity judgments. Figure 2 pre-
sents each of these four areas with a single
fabricated example of an English expression for
each of the final set of concepts. Local grammars
for the two additional conceptual areas, Goals (20
concepts) and Goal management (17 concepts),
were authored using the same approach as the oth-
ers, but were not completed in time to be included
in our performance evaluation.
After authoring these local grammars using the
Intex Corpus Processor, finite-state transducers
were compiled for each commonsense psychology
concept in each of the different conceptual areas.
To simplify the application of these transducers to
text corpora and to aid in their evaluation, trans-
ducers for individual concepts were combined into
a single finite state machine (one for each concep-
tual area). By examining the number of states and
transitions in the compiled finite state graphs, some
indication of their relative size can be given for the
four conceptual areas that we evaluated: Managing
knowledge (348 states / 932 transitions), Memory
(203 / 725), Explanations (208 / 530), and Similar-
ity judgments (121 / 500).
4 Performance evaluation
In order to evaluate the utility of our set of hand-
authored local grammars, we conducted a study of
their precision and recall performance. In order to
calculate the performance levels, it was first neces-
sary to create a test corpus that contained refer-
ences to the sorts of commonsense psychological
concepts that our rules were designed to recognize.
To accomplish this, we administered a survey to
1. Managing knowledge (37 concepts)
He?s got a logical mind (managing-knowledge-ability). She?s very gullible (bias-toward-belief). He?s skepti-
cal by nature (bias-toward-disbelief). It is the truth (true). That is completely false (false). We need to know
whether it is true or false (truth-value). His claim was bizarre (proposition). I believe what you are saying (be-
lief). I didn?t know about that (unknown). I used to think like you do (revealed-incorrect-belief). The assumption
was widespread (assumption). There is no reason to think that (unjustified-proposition). There is some evidence
you are right (partially-justified-proposition). The fact is well established (justified-proposition). As a rule, stu-
dents are generally bright (inference). The conclusion could not be otherwise (consequence). What was the rea-
son for your suspicion (justification)? That isn?t a good reason (poor-justification). Your argument is circular
(circular-justification). One of these things must be false (contradiction). His wisdom is vast (knowledge). He
knew all about history (knowledge-domain). I know something about plumbing (partial-knowledge-domain).
He?s got a lot of real-world experience (world-knowledge). He understands the theory behind it (world-model-
knowledge). That is just common sense (shared-knowledge). I?m willing to believe that (add-belief). I stopped
believing it after a while (remove-belief). I assumed you were coming (add-assumption). You can?t make that
assumption here (remove-assumption). Let?s see what follows from that (check-inferences). Disregard the con-
sequences of the assumption (ignore-inference). I tried not to think about it (suppress-inferences). I concluded
that one of them must be wrong (realize-contradiction). I realized he must have been there (realize). I can?t think
straight (knowledge-management-failure). It just confirms what I knew all along (reaffirm-belief).
2. Memory (12 concepts)
He has a good memory (memory-ability). It was one of his fondest memories (memory-item). He blocked out
the memory of the tempestuous relationship (repressed-memory-item). He memorized the words of the song
(memory-storage). She remembered the last time it rained (memory-retrieval). I forgot my locker combination
(memory-retrieval-failure). He repressed the memories of his abusive father (memory-repression). The widow
was reminded of her late husband (reminding). He kept the ticket stub as a memento (memory-cue). He intended
to call his brother on his birthday (schedule-plan).  He remembered to set the alarm before he fell asleep (sched-
uled-plan-retrieval). I forgot to take out the trash (scheduled-plan-retrieval-failure).
3. Explanations (20 concepts)
He?s good at coming up with explanations (explanation-ability). The cause was clear (cause). Nobody knew
how it had happened (mystery). There were still some holes in his account (explanation-criteria). It gave us the
explanation we were looking for (explanation). It was a plausible explanation (candidate-explanation). It was
the best explanation I could think of (best-candidate-explanation). There were many contributing factors (fac-
tor). I came up with an explanation (explain). Let?s figure out why it was so (attempt-to-explain). He came up
with a reasonable explanation (generate-candidate-explanation). We need to consider all of the possible expla-
nations (assess-candidate-explanations). That is the explanation he went with (adopt-explanation). We failed to
come up with an explanation (explanation-failure). I can?t think of anything that could have caused it (explana-
tion-generation-failure). None of these explanations account for the facts (explanation-satisfaction-failure).
Your account must be wrong (unsatisfying-explanation). I prefer non-religious explanations (explanation-
preference). You should always look for scientific explanations (add-explanation-preference). We?re not going
to look at all possible explanations (remove-explanation-preference).
4. Similarity judgments (13 concepts)
She?s good at picking out things that are different (similarity-comparison-ability). Look at the similarities
between the two (make-comparison). He saw that they were the same at an abstract level (draw-analogy). She
could see the pattern unfolding (find-pattern). It depends on what basis you use for comparison (comparison-
metric). They have that in common (same-characteristic). They differ in that regard (different-characteristic). If
a tree were a person, its leaves would correspond to fingers (analogical-mapping). The pattern in the rug was
intricate (pattern). They are very much alike (similar). It is completely different (dissimilar). It was an analogous
example (analogous).
Figure 2. Example sentences referring to 92 concepts in 4 areas of commonsense psychology
collect novel sentences that could be used for this
purpose.
This survey was administered over the course
of one day to anonymous adult volunteers who
stopped by a table that we had set up on our uni-
versity?s campus. We instructed the survey taker to
author 3 sentences that included words or phrases
related to a given concept, and 3 sentences that
they felt did not contain any such references. Each
survey taker was asked to generate these 6 sen-
tences for each of the 4 concept areas that we were
evaluating, described on the survey in the follow-
ing manner:
? Managing knowledge: Anything about the
knowledge, assumptions, or beliefs that people
have in their mind
? Memory: When people remember things, for-
get things, or are reminded of things
? Explanations: When people come up with pos-
sible explanations for unknown causes
? Similarity judgments: When people find simi-
larities or differences in things
A total of 99 people volunteered to take our
survey, resulting in a corpus of 297 positive and
297 negative sentences for each conceptual area,
with a few exceptions due to incomplete surveys.
Using this survey data, we calculated the preci-
sion and recall performance of our hand-authored
local grammars. Every sentence that had at least
one concept detected for the corresponding concept
area was treated as a ?hit?. Table 1 presents the
precision and recall performance for each concept
area. The results show that the precision of our
system is very high, with marginal recall perform-
ance.
The low recall scores raised a concern over the
quality of our test data. In reviewing the sentences
that were collected, it was apparent that some sur-
vey participants were not able to complete the task
as we had specified. To improve the validity of the
test data, we enlisted six volunteers (native English
speakers not members of our development team) to
judge whether or not each sentence in the corpus
was produced according to the instructions. The
corpus of sentences was divided evenly among
these six raters, and each sentence that the rater
judged as not satisfying the instructions was fil-
tered from the data set. In addition, each rater also
judged half of the sentences given to a different
rater in order to compute the degree of inter-rater
agreement for this filtering task. After filtering
sentences from the corpus, a second preci-
sion/recall evaluation was performed. Table 2 pre-
sents the results of our hand-authored local
grammars on the filtered data set, and lists the in-
ter-rater agreement for each conceptual area among
our six raters. The results show that the system
achieves a high level of precision, and the recall
performance is much better than earlier indicated.
The performance of our hand-authored local
grammars was then compared to the performance
that could be obtained using more traditional ma-
chine-learning approaches. In these comparisons,
the recognition of commonsense psychology con-
cepts was treated as a classification problem,
where the task was to distinguish between positive
Concept area Correct Hits
(a)
Wrong hits
(b)
Total positive
sentences (c)
Total negative
sentences
Precision
(a/(a+b))
Recall
(a/c)
Managing knowledge 205 16 297 297 92.76% 69.02%
Memory 240 4 297 297 98.36% 80.80%
Explanations 126 7 296 296 94.73% 42.56%
Similarity judgments 178 18 296 297 90.81% 60.13%
749 45 1186 1187 94.33% 63.15%
Table 1. Precision and recall results on the unfiltered data set
Concept area Inter-rater
agreement (K)
Correct
Hits (a)
Wrong
hits (b)
Total positive
sentences (c)
Total negative
 sentences
Precision
(a/(a+b))
Recall
(a/c)
Managing knowledge 0.5636 141 12 168 259 92.15% 83.92%
Memory 0.8069 209 0 221 290 100% 94.57%
Explanations 0.7138 83 5 120 290 94.21% 69.16%
Similarity judgments 0.6551 136 12 189 284 91.89% 71.95%
0.6805 569 29 698 1123 95.15% 81.51%
Table 2. Precision and recall results on the filtered data set, with inter-rater agreement on filtering
and negative sentences for any given concept area.
Sentences in the filtered data sets were used as
training instances, and feature vectors for each
sentence were composed of word-level unigram
and bi-gram features, using no stop-lists and by
ignoring punctuation and case. By using a toolkit
of machine learning algorithms (Witten & Frank,
1999), we were able to compare the performance
of a wide range of different techniques, including
Na?ve Bayes, C4.5 rule induction, and Support
Vector Machines, through stratified cross-
validation (10-fold) of the training data. The high-
est performance levels were achieved using a se-
quential minimal optimization algorithm for
training a support vector classifier using polyno-
mial kernels (Platt, 1998). These performance re-
sults are presented in Table 3. The percentage
correctness of classification (Pa) of our hand-
authored local grammars (column A) was higher
than could be attained using this machine-learning
approach (column B) in three out of the four con-
cept areas.
We then conducted an additional study to de-
termine if the two approaches (hand-authored local
grammars and machine learning) could be com-
plimentary. The concepts that are recognized by
our hand-authored rules could be conceived as ad-
ditional bimodal features for use in machine
learning algorithms. We constructed an additional
set of support vector machine classifiers trained on
the filtered data set that included these additional
concept-level features in the feature vector of each
instance along side the existing unigram and bi-
gram features. Performance of these enhanced
classifiers, also obtained through stratified cross-
validation (10-fold), are also reported in Table 3 as
well (column C). The results show that these en-
hanced classifiers perform at a level that is the
greater of that of each independent approach.
5 Discussion
The most significant challenge facing developers
of large-scale lexical-semantic resources is coming
to some agreement on the way that natural lan-
guage can be mapped onto specific concepts. This
challenge is particularly evident in consideration of
our survey data and subsequent filtering. The
abilities that people have in producing and recog-
nizing sentences containing related words or
phrases differed significantly across concept areas.
While raters could agree on what constitutes a
sentence containing an expression about memory
(Kappa=.8069), the agreement on expressions of
managing knowledge is much lower than we
would hope for (Kappa=.5636). We would expect
much greater inter-rater agreement if we had
trained our six raters for the filtering task, that is,
described exactly which concepts we were looking
for and gave them examples of how these concepts
can be realized in English text. However, this ap-
proach would have invalidated our performance
results on the filtered data set, as the task of the
raters would be biased toward identifying exam-
ples that our system would likely perform well on
rather than identifying references to concepts of
commonsense psychology.
Our inter-rater agreement concern is indicative
of a larger problem in the construction of large-
scale lexical-semantic resources. The deeper we
delve into the meaning of natural language, the less
we are likely to find strong agreement among un-
trained people concerning the particular concepts
that are expressed in any given text. Even with
lexical-semantic resources about commonsense
knowledge (e.g. commonsense psychology), finer
distinctions in meaning will require the efforts of
trained knowledge engineers to successfully map
between language and concepts. While this will
certainly create a problem for future preci-
A. Hand authored local
grammars
B. SVM with word level
features
C. SVM with word and
concept features
Concept area Pa K Pa K Pa K
Managing knowledge 90.86% 0.8148 86.0789% 0.6974 89.5592% 0.7757
Memory 97.65% 0.8973 93.5922% 0.8678 97.4757% 0.9483
Explanations 89.75% 0.7027 85.9564% 0.6212 89.3462% 0.7186
Similarity judgments 86.25% 0.7706 92.4528% 0.8409 92.0335% 0.8309
Table 3. Percent agreement (Pa) and Kappa statistics (K) for classification using hand-authored local
grammars (A), SVMs with word features (B), and SVMs with word and concept features (C)
sion/recall performance evaluations, the concern is
even more serious for other methodologies that
rely on large amounts of hand-tagged text data to
create the recognition rules in the first place. We
expect that this problem will become more evident
as projects using algorithms to induce local gram-
mars from manually-tagged corpora, such as the
Berkeley FrameNet efforts (Baker et al, 1998),
broaden and deepen their encodings in conceptual
areas that are more abstract (e.g. commonsense
psychology).
The approach that we have taken in our re-
search does not offer a solution to the growing
problem of evaluating lexical-semantic resources.
However, by hand-authoring local grammars for
specific concepts rather than inducing them from
tagged text, we have demonstrated a successful
methodology for creating lexical-semantic re-
sources with a high degree of conceptual breadth
and depth. By employing linguistic and knowledge
engineering skills in a combined manner we have
been able to make strong ontological commitments
about the meaning of an important portion of the
English language. We have demonstrated that the
precision and recall performance of this approach
is high, achieving classification performance
greater than that of standard machine-learning
techniques. Furthermore, we have shown that
hand-authored local grammars can be used to
identify concepts that can be easily combined with
word-level features (e.g. unigrams, bi-grams) for
integration into statistical natural language proc-
essing systems. Our early exploration of the appli-
cation of this work for corpus analysis (U.S. State
of the Union Addresses) has produced interesting
results, and we expect that the continued develop-
ment of this resource will be important to the suc-
cess of future corpus analysis and human-computer
interaction projects.
Acknowledgments
This paper was developed in part with funds from
the U.S. Army Research Institute for the Behav-
ioral and Social Sciences under ARO contract
number DAAD 19-99-D-0046. Any opinions,
findings and conclusions or recommendations ex-
pressed in this paper are those of the authors and
do not necessarily reflect the views of the Depart-
ment of the Army.
References
Baker, C., Fillmore, C., & Lowe, J. (1998) The Ber-
keley FrameNet project. in Proceedings of the
COLING-ACL, Montreal, Canada.
Bartsch, K. & Wellman, H. (1995) Children talk about
the mind. New York: Oxford University Press.
Dyer, J., Shatz, M., & Wellman, H. (2000) Young chil-
dren?s storybooks as a source of mental state infor-
mation. Cognitive Development 15:17-37.
Ferguson, G. & Allen, J. (1993) Cooperative Plan Rea-
soning for Dialogue Systems, in AAAI-93 Fall Sym-
posium on Human-Computer Collaboration:
Reconciling Theory, Synthesizing Practice, AAAI
Technical Report FS-93-05. Menlo Park, CA: AAAI
Press.
Gordon, A. (2002) The Theory of Mind in Strategy
Representations. 24th Annual Meeting of the Cogni-
tive Science Society. Mahwah, NJ: Lawrence Erl-
baum Associates.
Gordon, A. & Hobbs (2003) Coverage and competency
in formal theories: A commonsense theory of mem-
ory. AAAI Spring Symposium on Formal Theories of
Commonsense knowledge, March 24-26, Stanford.
Platt, J. (1998). Fast Training of Support Vector Ma-
chines using Sequential Minimal Optimization. In B.
Sch?lkopf, C. Burges, and A. Smola (eds.) Advances
in Kernel Methods - Support Vector Learning, Cam-
bridge, MA: MIT Press.
Reboul A., Sabatier P., No?l-Jorand M-C. (2001) Le
discours des schizophr?nes: une ?tude de cas. Revue
fran?aise de Psychiatrie et de Psychologie M?dicale,
49, pp 6-11.
Silberztein, M. (1999) Text Indexing with INTEX.
Computers and the Humanities 33(3).
Traum, D. (1993) Mental state in the TRAINS-92 dia-
logue manager. In Working Notes of the AAAI Spring
Symposium on Reasoning about Mental States: For-
mal Theories and Applications, pages 143-149, 1993.
Menlo Park, CA: AAAI Press.
Voorhees, E. & Buckland, L. (2002) The Eleventh Text
REtrieval Conference (TREC 2002). Washington,
DC: Department of Commerce, National Institute of
Standards and Technology.
Witten, I. & Frank, E. (1999) Data Mining: Practical
Machine Learning Tools and Techniques with Java
Implementations. Morgan Kaufman.
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 811?818,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Comparison of Alternative Parse Tree Paths for Labeling Semantic Roles 
 Reid Swanson and Andrew S. Gordon Institute for Creative Technologies University of Southern California 13274 Fiji Way, Marina del Rey, CA 90292 USA swansonr@ict.usc.edu, gordon@ict.usc.edu     Abstract The integration of sophisticated infer-ence-based techniques into natural lan-guage processing applications first re-quires a reliable method of encoding the predicate-argument structure of the pro-positional content of text. Recent statisti-cal approaches to automated predicate-argument annotation have utilized parse tree paths as predictive features, which encode the path between a verb predicate and a node in the parse tree that governs its argument. In this paper, we explore a number of alternatives for how these parse tree paths are encoded, focusing on the difference between automatically generated constituency parses and de-pendency parses. After describing five al-ternatives for encoding parse tree paths, we investigate how well each can be aligned with the argument substrings in annotated text corpora, their relative pre-cision and recall performance, and their comparative learning curves. Results in-dicate that constituency parsers produce parse tree paths that can more easily be aligned to argument substrings, perform better in precision and recall, and have more favorable learning curves than those produced by a dependency parser. 1 Introduction A persistent goal of natural language processing research has been the automated transformation of natural language texts into representations that unambiguously encode their propositional content in formal notation. Increasingly, first-order predicate calculus representations of 
textual meaning have been used in natural lanugage processing applications that involve automated inference. For example, Moldovan et al (2003) demonstrate how predicate-argument formulations of questions and candidate answer sentences are unified using logical inference in a top-performing question-answering application. The importance of robust techniques for predicate-argument transformation has motivated the development of large-scale text corpora with predicate-argument annotations such as PropBank (Palmer et al, 2005) and FrameNet (Baker et al, 1998). These corpora typically take a pragmatic approach to the predicate-argument representations of sentences, where predicates correspond to single word triggers in the surface form of the sentence (typically verb lemmas), and arguments can be identified as substrings of the sentence. Along with the development of annotated corpora, researchers have developed new techniques for automatically identifying the arguments of predications by labeling text segments in sentences with semantic roles. Both Gildea & Jurafsky (2002) and Palmer et al (2005) describe statistical labeling algorithms that achieve high accuracy in assigning semantic role labels to appropropriate constituents in a parse tree of a sentence. Each of these efforts employed the use of parse tree paths as predictive features, encoding the series of up and down transitions through a parse tree to move from the node of the verb (predicate) to the governing node of the constituent (argument). Palmer et al (2005) demonstrate that utilizing the gold-standard parse trees of the Penn tree-bank (Marcus et al, 1993) to encode parse tree paths yields significantly better labeling accuracy than when using an automatic syntactical parser, namely that of Collins (1999). 
811
Parse tree paths (between verbs and arguments that fill semantic roles) are particularly interest-ing because they symbolically encode the rela-tionship between the syntactic and semantic as-pects of verbs, and are potentially generalized across other verbs within the same class (Levin, 1993). However, the encoding of individual parse tree paths for predicates is wholly depend-ent on the characteristics of the parse tree of a sentence, for which competing approaches could be taken.  The research effort described in this paper fur-ther explores the role of parse tree paths in iden-tifying the argument structure of verb-based predications. We are particularly interested in exploring alternatives to the constituency parses that were used in previous research, including parsing approaches that employ dependency grammars. Specifically, our aim is to answer four important questions: 1. How can parse tree paths be encoded when employing different automated constituency parsers, i.e. Charniak (2000), Klein & Manning (2003), or a dependency parser (Lin, 1998)? 2. Given that each of these alternatives creates a different formulation of the parse tree of a sen-tence, which of them encodes branches that are easiest to align with substrings that have been annotated with semantic role information? 3. What is the relative precision and recall per-formance of parse tree paths formulated using these alternative automated parsing techniques, and do the results vary depending on argument type? 4. How many examples of parse tree paths are necessary to provide as training examples in or-der to achieve high labeling accuracy when em-ploying each of these parsing alternatives? Each of these four questions is addressed in the four subsequent sections of this paper, fol-lowed by a discussion of the implications of our findings and directions for future work.  2 Alternative Parse Tree Paths Parse tree paths were introduced by Gildea & Jurafsky (2002) as descriptive features of the syntactic relationship between predicates and arguments in the parse tree of a sentence. Predi-cates are typically assumed to be specific target words (usually verbs), and arguments are as-sumed to be a span of words in the sentence that are governed by a single node in the parse tree. A parse tree path can be described as a sequence of transitions up and down a parse tree from the 
target word to the governing node, as exempli-fied in Figure 1. The encoding of the parse tree path feature is dependent on the syntactic representation that is produced by the parser. This, in turn, is depend-ant on the training corpus used to build the parser, and the conditioning factors in its prob-ability model. As result, encodings of parse tree paths can vary greatly depending on the parser that is used, yielding parse tree paths that vary in their ability to generalize across sentences. In this paper we explore the characteristics of parse tree paths with respect to different ap-proaches to automated parsing. We were particu-larly interested in comparing traditional constitu-ency parsing (as exemplified in Figure 1) with dependency parsing, specifically the Minipar system built by Lin (1998). Minipar is increas-ingly being used in semantics-based nlp applica-tions (e.g. Pantel & Lin, 2002). Dependency parse trees differ from constituency parses in that they represent sentence structures as a set of de-pendency relationships between words, typed asymmetric binary relationships between head words and modifying words. Figure 2 depicts the output of Minipar on an example sentence, where each node is a word or an empty node along with the word lemma, its part of speech, and the relationship type to its governing node. Our motivation for exploring the use of Mini-par in for the creation of parse tree paths can be seen by comparing Figure 1 and Figure 2, where 
 Figure 1: An example parse tree path from the predicate ate to the argument NP He, rep-resented as VB?VP?S?NP.   
 Figure 2. An example dependency parse, with a parse tree path from the predicate ate to the argument He. 
812
the Minipar path is both shorter and simpler for the same predicate-argument relationship, and could be encoded in various ways that take ad-vantage of the additional semantic and lexical information that is provided. To compare traditional constituency parsing with dependency parsing, we evaluated the accu-racy of argument labeling using parse tree paths generated by two leading constituency parsers and three variations of parse tree paths generated by Minipar, as follows:  Charniak: We used the Charniak parser (2000) to extract parse tree paths similar to those found in Palmer et al (2005), with some slight modifications. In cases where the last node in the path was a non-branching pre-terminal, we added the lexical information to the path node. In addi-tion, our paths led to the lowest governing node, rather than the highest. For example, the parse tree path for the argument in Figure 1 would be  encoded as:  VB?VP?S?NP?PRP:he  Stanford: We also used the Stanford parser developed by Klein & Manning (2003), with the same path encoding as the Charniak parser.  Minipar A: We used three variations of parse tree path encodings based on Lin?s dependency parser, Minipar (1998). Minipar A is the first and most restrictive path encoding, where each is annotated with the entire information output by Minpar at each node. A typical path might be: ate:eat,V,i?He:he,N,s  Minipar B: A second parse tree path encoding was generated from Minipar parses that relaxes some of the constraints used in Minpar A. In-stead of using all the information contained at a node, in Minipar B we only encode a path with its part of speech and relational information. For example: V,i?N,s  Minipar C: As the converse to Minipar A we also tried one other Minipar encoding. As in Minipar A, we annotated the path with all the information output, but instead of doing a direct string comparison during our search, we consid-ered two paths matching when there was a match between either the word, the stem, the part of speech, or the relation. For example, the follow-ing two parse tree paths would be considered a match, as both include the relation i. 
ate:eat,V,i?He:he,N,s was:be,VBE,i?He:he,N,s  We explored other combinations of depend-ency relation information for Minipar-derived parse tree paths, including the use of the deep relations. However, results obtained using these other combinations were not notably different from those of the three base cases listed above, and are not included in the evaluation results re-ported in this paper. 3 Aligning arguments to parse trees nodes in a training / testing corpus We began our investigation by creating a training and testing corpus of 400 sentences each contain-ing an inflection of one of four target verbs (100 each), namely believe, think, give, and receive. These sentences were selected at random from the 1994-07 section of the New York Times gi-gaword corpus from the Linguistic Data Consor-tium. These four verbs were chosen because of the synonymy among the first two, and the re-flexivity of the second two, and because all four have straightforward argument structures when viewed as predicates, as follows:  predicate: believe arg0: the believer arg1: the thing that is believed  predicate: think arg0: the thinker arg1: the thing that is thought  predicate: give arg0: the giver arg1: the thing that is given arg2: the receiver  predicate: receive arg0: the receiver arg1: the thing that is received arg2: the giver  This corpus of sentences was then annotated with semantic role information by the authors of this paper. All annotations were made by assign-ing start and stop locations for each argument in the unparsed text of the sentence. After an initial pilot annotation study, the following annotation policy was adopted to overcome common dis-agreements: (1) When the argument is a noun and it is part of a definite description then in-
813
clude the entire definite description. (2) Do not include complementizers such as ?that? in ?be-lieve that? in an argument. (3) Do include prepo-sitions such as ?in? in ?believe in?. (4) When in doubt, assume phrases attach locally. Using this policy, an agreement of 92.8% was achieved among annotators for the set of start and stop locations for arguments. Examples of semantic role annotations in our corpus for each of the four predicates are as follows:  1. [Arg0Those who excavated the site in 1907] believe [Arg1 it once stood two or three stories high.] 2. Gus is in good shape and [Arg0 I] think [Arg1 he's happy as a bear.] 3. If successful, [Arg0 he] will give [Arg1 the funds] to [Arg2 his Vietnamese family.]  4. [Arg0 The Bosnian Serbs] have received [Arg1 military and economic support] from [Arg2 Ser-bia.] The next step was to parse the corpus of 400 sentences using each of three automated parsing systems (Charniak, Stanford, and Minipar), and align each of the annotated arguments with its closest matching branch in the resulting parse trees. Given the differences in the parsing models used by these three systems, each yield parse tree nodes that govern different spans of text in the sentence. Often there exists no parse tree node that governs a span of text that exactly matches the span of an argument in the annotated corpus. Accordingly, it was necessary to identify the closest match possible for each of the three pars-ing systems in order to encode parse tree paths for each. We developed a uniform policy that would facilitate a fair comparison between pars-ing techniques. Our approach was to identify a single node in a given parse tree that governed a string of text with the most overlap with the text of the annotated argument. Each of the parsing methods tokenizes the input string differently, so in order to simplify the selection of the govern-ing node with the most overlap, we made this selection based on lowest minimum edit distance (Levenshtein distance). All three of these different parsing algorithms produced single governing nodes that overlapped well with the human-annotated corpus. However, it appeared that the two constituency parsers pro-duced governing nodes that were more closely aligned, based on minimum edit distance. The Charniak parser aligned best with the annotated text, with an average of 2.40 characters for the lowest minimum edit distance (standard de-viation = 8.64). The Stanford parser performed 
slightly worse (average = 2.67, standard devia-tion = 8.86), while distances were nearly two times larger for Minipar (average = 4.73, standard deviation = 10.44).  In each case, the most overlapping parse tree node was treated as correct for training and test-ing purposes.  4 Comparative Performance Evaluation In order to evaluate the comparative performance of the parse tree paths for each of the five encod-ings, we divided the corpus in to equal-sized training and test sets (50 training and 50 test ex-amples for each of the four predicates). We then constructed a system that identified the parse tree paths for each of the 10 arguments in the training sets, and applied them to the sentences in each corresponding test sets. When applying the 50 training parse tree paths to any one of the 50 test sentences for a given predicate-argument pair, a set of zero or more candidate answer nodes were returned. For the purpose of calculating precision and recall scores, credit was given when the cor-rect answer appeared in this set. Precision scores were calculated as the number of correct answers found divided by the number of all candidate answer nodes returned. Recall scores were calcu-lated as the number of correct answers found di-vided by the total number of correct answers possible. F-scores were calculated as the equally-weighted harmonic mean of precision and recall.  Our calculation of recall scores represents the best-possible performance of systems using only these types of parse-tree paths. This level of per-formance could be obtained if a system could always select the correct answer from the set of candidates returned. However, it is also informa-tive to estimate the performance that could be achieved by randomly selecting among the can-didate answers, representing a lower-bound on performance. Accordingly, we computed an ad-justed recall score that awarded only fractional credit in cases where more than one candidate answer was returned (one divided by the set size). Adjusted recall is the sum of all of these adjusted credits divided by the total number of correct answers possible. Figure 3 summarizes the comparative recall, precision, f-score, and adjusted recall perform-ance for each of the five parse tree path formula-tions. The Charniak parser achieved the highest overall scores (precision=.49, recall=.68, f-score=.57, adjusted recall=.48), followed closely 
814
by the Stanford parser (precision=.47, recall=.67, f-score=.55, adjusted recall=.48). Our expectation was that the short, semanti-cally descriptive parse tree paths produced by Minipar would yield the highest performance. However, these results indicate the opposite; the constituency parsers produce the most accurate parse tree paths. Only Minipar C offers better recall (0.71) than the constituency parsers, but at the expense of extremely low precision. Minipar A offers excellent precision (0.62), but with ex-tremely low recall. Minipar B provides a balance between recall and precision performance, but falls short of being competitive with the parse tree paths generated by the two constituency parsers, with an f-score of .44. We utilized the Sign Test in order to deter-mine the statistical significance of these differ-ences. Rank orderings between pairs of systems were determined based on the adjusted credit that each system achieved for each test sentence. Sig-nificant differences were found between the per-formance of every system (p<0.05), with the ex-ception of the Charniak and Stanford parsers. Interestingly, by comparing weighted values for each test example, Minipar C more frequently scores higher than Minipar A, even though the 
sum of these scores favors Minipar A. In addition to overall performance, we were interested in determining whether performance varied depending on the type of the argument that is being labeled. In assigning labels to argu-ments in the corpus, we followed the general principles set out by Palmer et al (2005) for la-beling arguments arg0, arg1 and arg2. Across each of our four predicates, arg0 is the agent of the predication (e.g. the person that has the belief or is doing the giving), and arg1 is the thing that is acted upon by the agent (e.g. the thing that is believed or the thing that is given). Arg2 is used only for the predications based on the verbs give and receive, where it is used to indicate the other party of the action.  Our interest was in determining whether these five approaches yielded different results depend-ing on the semantic type of the argument. Fig-ure 4 presents the f-scores for each of these en-codings across each argument type.  Results indicate that the Charniak and Stan-ford parsers continue to produce parse tree paths that outperform each of the Minipar-based ap-proaches. In each approach argument 0 is the easiest to identify. Minipar A retains the general trends of Charniak and Stanford, with argument 
 Figure 3. Precision, recall, f-scores, and adjusted recall for five parse tree path types 
 Figure 4. Comparative f-scores for arguments 0, 1, and 2 for five parse tree path types  
815
1 easier to identify than argument 2, while Mini-par B and C show the reverse. The highest f-scores for argument 0 were achieved Stanford (f=.65), while Charniak achieved the highest scores for argument 1 (f=.55) and argument 2 (f=.49). 5 Learning Curve Comparisons The creation of large-scale text corpora with syn-tactic and/or semantic annotations is difficult, expensive, and time consuming. The PropBank effort has shown that producing this type of cor-pora is considerably easier once syntactic analy-sis has been done, but substantial effort and re-sources are still required. Better estimates of total costs could be made if it was known exactly how many annotations are necessary to achieve ac-ceptable levels of performance. Accordingly, we investigated the learning curves of precision, re-call, f-score, and adjusted recall achieved using the five different parse tree path encodings. For each encoding approach, learning curves were created by applying successively larger subsets of the training parse tree paths to each of the items in the corresponding test set. Precision, recall, f-scores, and adjusted recall were com-puted as described in the previous section, and identical subsets of sentences were used across parsers, in one-sentence increments. Individual learning curves for each of the five approaches are given in Figures 5, 6, 7, 8, and 9. Figure 10 presents a comparison of the f-score learning curves for all five of the approaches.  In each approach, the precision scores slowly degrade as more training examples are provided, due to the addition of new parse tree paths that yield additional candidate answers. Conversely, the recall scores of each system show their great-est gains early, and then slowly improve with the addition of more parse tree paths. In each ap-proach, the recall scores (estimating best-case performance) have the same general shape as the adjusted recall scores (estimating the lower-bound performance). The divergence between these two scores increases with the addition of more training examples, and is more pronounced in systems employing parse tree paths with less specific node information. The comparative f-score curves presented in Figure 10 indicate that Minipar B is competitive with Charniak and Stanford when only a small number of training examples is available. There is some evidence here that the performance of Minipar A would continue to improve with the addition of more 
training data, suggesting that this approach might be well-suited for applications where lots of training data is available.  6 Discussion Annotated corpora of linguistic phenomena en-able many new natural language processing ap-plications and provide new means for tackling difficult research problems. Just as the Penn Treebank offers the possibility of developing systems capable of accurate syntactic parsing, corpora of semantic role annotations open up new possibilities for rich textual understanding and integrated inference. In this paper, we compared five encodings of parse tree paths based on two constituency pars-ers and a dependency parser. Despite our expec-tations that the semantic richness of dependency parses would yield paths that outperformed the others, we discovered that parse tree paths from Charniak?s constituency parser performed the best overall. In applications where either preci-sion or recall is the only concern, then Minipar-derived parse tree paths would yield the best re-sults. We also found that the performance of all of these systems varied across different argument types.    In contrast to the performance results reported by Palmer et al (2005) and Gildea & Jurafsky (2002), our evaluation was based solely on parse tree path features. Even so, we were able to ob-tain reasonable levels of performance without the use of additional features or stochastic methods. Learning curves indicate that the greatest gains in performance can be garnered from the first 10 or so training examples. This result has implica-tions for the development of large-scale corpora of semantically annotated text. Developers should distribute their effort in order to maxi-mize the number of predicate-argument pairs with at least 10 annotations.  An automated semantic role labeling system could be constructed using only the parse tree path features described in this paper, with esti-mated performance between our recall scores and our adjusted recall scores. There are several ways to improve on the random selection approach used in the adjusted recall calculation. For exam-ple, one could simply select the candidate answer with the most frequent parse tree path.  The results presented in this paper help inform the design of future automated semantic role la-beling systems that improve on the best-performing systems available today (Gildea &  
816
    
 Figure 5. Charniak learning curves    
 Figure 6. Stanford learning curves    
 Figure 7. Minipar A learning curves        
    
 Figure 8. Minipar B learning curves    
 Figure 9. Minipar C learning curves    
 Figure 10. Comparative F-score curves   
817
Jurafsky, 2002; Moschitti et al, 2005). We found that different parse tree paths encode different types of linguistic information, and exhibit dif-ferent characteristics in the tradeoff between pre-cision and recall. The best approaches in future systems will intelligently capitalize on these dif-ferences in the face of varying amounts of train-ing data.  In our own future work, we are particularly in-terested in exploring the regularities that exist among parse tree paths for different predicates. By identifying these regularities, we believe that we will be able to significantly reduce the total number of annotations necessary to develop lexi-cal resources that have broad coverage over natu-ral language.  Acknowledgments The project or effort depicted was sponsored by the U. S. Army Research, Development, and En-gineering Command (RDECOM). The content or information does not necessarily reflect the posi-tion or the policy of the Government, and no of-ficial endorsement should be inferred. References Baker, Collin, Charles J. Fillmore and John B. Lowe. 1998. The Berkeley FrameNet Project, In Proceed-ings of COLING-ACL, Montreal. Charniak, Eugene. 2000. A maximum-entropy-inspired parser, In Proceedings NAACL. Collins, Michael. 1999. Head-Driven Statistical Mod-els for Natural Language Parsing, PhD thesis, Uni-versity of Pennsylvania. Gildea, Daniel and Daniel Jurafsky. 2002. Automatic labeling of semantic roles, Computational Linguis-tics, 28(3):245--288. Klein, Dan and Christopher Manning. 2003. Accurate Unlexicalized Parsing, In Proceedings of the 41st Annual Meeting of the Association for Computa-tional Linguistics, 423-430. Levin, Beth. 1993. English Verb Classes and Alterna-tions: A Preliminary Investigation. Chicago, IL: University of Chicago Press. Lin, Dekang. 1998. Dependency-Based Evaluation of MINIPAR, In Proceedings of the Workshop on the Evaluation of Parsing Systems, First International Conference on Language Resources and Evaluation, Granada, Spain. Marcus, Mitchell P., Beatrice Santorini and Mary Ann Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank, Computa-tional Linguistics, 19(35):313-330 
Moldovan, Dan I., Christine Clark, Sanda M. Hara-bagiu & Steven J. Maiorano. 2003. COGEX: A Logic Prover for Question Answering, HLT-NAACL. Moschitti, A., Giuglea, A., Coppola, B. & Basili, R. 2005. Hierarchical Semantic Role Labeling. In Proceedings of the 9th Conference on Computa-tional Natural Language Learning (CoNLL 2005 shared task), Ann Arbor(MI), USA. Palmer, Martha, Dan Gildea and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles, Computational Linguistics. Pantel, Patrick and Dekang Lin. 2002. Document clustering with committees, In Proceedings of SIGIRO2, Tampere, Finland.  
818
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 192?199,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Generalizing Semantic Role Annotations  
Across Syntactically Similar Verbs 
 
 
Andrew S. Gordon Reid Swanson 
Institute for Creative Technologies Institute for Creative Technologies 
University of Southern California University of Southern California 
Marina del Rey, CA 90292 USA Marina del Rey, CA 90292 USA 
gordon@ict.usc.edu swansonr@ict.usc.edu 
 
 
 
 
Abstract 
Large corpora of parsed sentences with 
semantic role labels (e.g. PropBank) pro-
vide training data for use in the creation 
of high-performance automatic semantic 
role labeling systems. Despite the size of 
these corpora, individual verbs (or role-
sets) often have only a handful of in-
stances in these corpora, and only a 
fraction of English verbs have even a sin-
gle annotation. In this paper, we describe 
an approach for dealing with this sparse 
data problem, enabling accurate semantic 
role labeling for novel verbs (rolesets) 
with only a single training example. Our 
approach involves the identification of 
syntactically similar verbs found in Prop-
Bank, the alignment of arguments in their 
corresponding rolesets, and the use of 
their corresponding annotations in Prop-
Bank as surrogate training data. 
1 Generalizing Semantic Role Annotations 
A recent release of the PropBank (Palmer et al, 
2005) corpus of semantic role annotations of Tree-
bank parses contained 112,917 labeled instances of 
4,250 rolesets corresponding to 3,257 verbs, as 
illustrated by this example for the verb buy. 
 
[arg0 Chuck] [buy.01 bought] [arg1 a car] [arg2 from 
Jerry] [arg3 for $1000]. 
 
Annotations similar to these have been used to cre-
ate automated semantic role labeling systems 
(Pradhan et al, 2005; Moschitti et al, 2006) for 
use in natural language processing applications that 
require only shallow semantic parsing. As with all 
machine-learning approaches, the performance of 
these systems is heavily dependent on the avail-
ability of adequate amounts of training data. How-
ever, the number of annotated instances in 
PropBank varies greatly from verb to verb; there 
are 617 annotations for the want roleset, only 7 for 
desire, and 0 for any sense of the verb yearn. Do 
we need to keep annotating larger and larger cor-
pora in order to generate accurate semantic label-
ing systems for verbs like yearn? 
A better approach may be to generalize the data 
that exists already to handle novel verbs. It is rea-
sonable to suppose that there must be a number of 
verbs within the PropBank corpus that behave 
nearly exactly like yearn in the way that they relate 
to their constituent arguments. Rather than annotat-
ing new sentences that contain the verb yearn, we 
could simply find these similar verbs and use their 
annotations as surrogate training data. 
This paper describes an approach to generalizing 
semantic role annotations across different verbs, 
involving two distinct steps. The first step is to 
order all of the verbs with semantic role annota-
tions according to their syntactic similarity to the 
target verb, followed by the second step of aligning 
argument labels between different rolesets. To 
evaluate this approach we developed a simple 
automated semantic role labeling algorithm based 
on the frequency of parse-tree paths, and then 
compared its performance when using real and sur-
rogate training data from PropBank. 
192
2 Parse Tree Paths 
A key concept in understanding our approach to 
both automated semantic role annotation and gen-
eralization is the notion of a parse tree path. Parse 
tree paths were used for semantic role labeling by 
Gildea and Jurafsky (2002) as descriptive features 
of the syntactic relationship between predicates 
and their arguments in the parse tree of a sentence. 
Predicates are typically assumed to be specific tar-
get words (verbs), and arguments are assumed to 
be spans of words in the sentence that are domi-
nated by nodes in the parse tree. A parse tree path 
can be described as a sequence of transitions up 
from the target word then down to the node that 
dominates the argument span (e.g. Figure 1). 
 
 
Figure 1: An example parse tree path from the 
predicate ate to the argument NP He, represented 
as VBVPSNP 
 
Parse tree paths are particularly interesting for 
automated semantic role labeling because they 
generalize well across syntactically similar sen-
tences. For example, the parse tree path in Figure 1 
would still correctly identify the ?eater? argument 
in the given sentence if the personal pronoun ?he? 
were swapped with a markedly different noun 
phrase, e.g. ?the attendees of the annual holiday 
breakfast.? 
3 A Simple Semantic Role Labeler 
To explore issues surrounding the generalization of 
semantic role annotations across verbs, we began 
by authoring a simple automated semantic role la-
beling algorithm that assigns labels according to 
the frequency of the parse tree paths seen in train-
ing data. To construct a labeler for a specific role-
set, training data consisting of parsed sentences 
with role-labeled parse tree constituents are ana-
lyzed to identify all of the parse tree paths between 
predicates and arguments, which are then tabulated 
and sorted by frequency. For example, Table 1 lists 
the 10 most frequent pairs of arguments and parse 
tree paths for the want.01 roleset in a recent release 
of PropBank. 
 
Count Argument Parse tree path 
189 ARG0 VBPVPSNP  
159 ARG1 VBPVPS  
125 ARG0 VBZVPSNP  
110 ARG1 VBZVPS  
102 ARG0 VBVPVPSNP  
98 ARG1 VBVPS  
96 ARG0 VBDVPSNP  
79 ARGM VBVPVPRB  
76 ARG1 VBDVPS  
43 ARG1 VBPVPNP  
Table 1. Top 10 most frequent parse tree paths for 
arguments of the PropBank want.01 roleset, based 
on 617 annotations 
  
To automatically assign role labels to an unla-
beled parse tree, each entry in the table is consid-
ered in order of highest frequency. Beginning from 
the target word in the sentence (e.g. wants) a check 
is made to determine if the entry includes a possi-
ble parse tree path in the parse tree of the sentence. 
If so, then the constituent is assigned the role label 
of the entry, and all subsequent entries in the table 
that have the same argument label or lead to sub-
constituents of the labeled node are invalidated. 
Only subsequent entries that assign core arguments 
of the roleset (e.g. ARG0, ARG1) are invalidated, 
allowing for multiple assignments of non-core la-
bels (e.g. ARGM) to a test sentence. In cases 
where the path leads to more than one node in a 
sentence, the leftmost path is selected. This process 
then continues down the list of valid table entries, 
assigning additional labels to unlabeled parse tree 
constituents, until the end of the table is reached. 
This approach also offers a simple means of 
dealing with multiple-constituent arguments, 
which occasionally appear in PropBank data. In 
these cases, the data is listed as unique entries in 
the frequency table, where each of the parse tree 
paths to the multiple constituents are listed as a set. 
The labeling algorithm will assign the argument of 
the entry only if all parse tree paths in the set are 
present in the sentence. 
The expected performance of this approach to 
semantic role labeling was evaluated using the 
PropBank data using a leave-one-out cross-
validation experimental design. Precision and re-
call scores were calculated for each of the 3,086 
193
rolesets with at least two annotations. Figure 2 
graphs the average precision, recall, and F-score 
for rolesets according to the number of training 
examples of the roleset in the PropBank corpus. 
An additional curve in Figure 2 plots the percent-
age of these PropBank rolesets that have the given 
amount of training data or more. For example, F-
scores above 0.7 are first reached with 62 training 
examples, but only 8% of PropBank rolesets have 
this much training data available. 
 
 
Figure 2. Performance of our semantic role label-
ing approach on PropBank rolesets 
4 Identifying Syntactically Similar Verbs 
A key part of generalizing semantic role annota-
tions is to calculate the syntactic similarity be-
tween verbs. The expectation here is that verbs that 
appear in syntactically similar contexts are going 
to behave similarly in the way that they relate to 
their arguments. In this section we describe a fully 
automated approach to calculating the syntactic 
similarity between verbs. 
Our approach is strictly empirical; the similarity 
of verbs is determined by examining the syntactic 
contexts in which they appear in a large text cor-
pus. Our approach is analogous to previous work 
in extracting collocations from large text corpora 
using syntactic information (Lin, 1998). In our 
work, we utilized the GigaWord corpus of English 
newswire text (Linguistic Data Consortium, 2003), 
consisting of nearly 12 gigabytes of textual data. 
To prepare this corpus for analysis, we extracted 
the body text from each of the 4.1 million entries 
in the corpus and applied a maximum-entropy al-
gorithm to identify sentence boundaries (Reynar 
and Ratnaparkhi, 1997). 
Next we executed a four-step analysis process 
for each of the 3,257 verbs in the PropBank cor-
pus. In the first step, we identified each of the sen-
tences in the prepared GigaWord corpus that 
contained any inflection of the given verb. To 
automatically identify all verb inflections, we util-
ized the English DELA electronic dictionary 
(Courtois, 2004), which contained all but 21 of the 
PropBank verbs (for which we provided the inflec-
tions ourselves), with old-English verb inflections 
removed. We extracted GigaWord sentences con-
taining these inflections by using the GNU grep 
program and a template regular expression for each 
inflection list. The results of these searches were 
collected in 3,257 files (one for each verb). The 
largest of these files was for inflections of the verb 
say (15.9 million sentences), and the smallest was 
for the verb namedrop (4 sentences). 
The second step was to automatically generate 
syntactic parse trees for the GigaWord sentences 
found for each verb. It was our original intention to 
parse all of the found sentences, but we found that 
the slow speed of contemporary syntactic parsers 
made this impractical. Instead, we focused our ef-
forts on the first 100 sentences found for each of 
the 3,257 verbs with 100 or fewer tokens: a total of 
324,461 sentences (average of 99.6 per verb). For 
this task we utilized the August 2005 release of the 
Charniak parser with the default speed/accuracy 
settings (Charniak, 2000), which required roughly 
360 hours of processor time on a 2.5 GHz 
PowerPC G5. 
The third step was to characterize the syntactic 
context of the verbs based on where they appeared 
within the parse trees. For this purpose, we utilized 
parse tree paths as a means of converting tree 
structures into a flat, feature-vector representation. 
For each sentence, we identified all possible parse 
tree paths that begin from the verb inflection and 
terminate at a constituent that does not include the 
verb inflection. For example, the syntactic context 
of the verb in Figure 1 can be described by the fol-
lowing five parse tree paths: 
1. VBVPSNP 
2. VBVPSNPPRP 
3. VBVPNP 
4. VBVPNPDT 
5. VBVPNPNN 
Possible parse tree paths were identified for 
every parsed sentence for a given verb, and the 
frequencies of each unique path were tabulated 
194
into a feature vector representation. Parse tree 
paths where the first node was not a Treebank part-
of-speech tag for a verb were discarded, effectively 
filtering the non-verb homonyms of the set of in-
flections. The resulting feature vectors were nor-
malized by dividing the values of each feature by 
the number of verb instances used to generate the 
parse tree paths; the value of each feature indicates 
the proportion of observed inflections in which the 
parse tree path is possible. As a representative ex-
ample, 95 verb forms of abandon were found in 
the first 100 GigaWord sentences containing any 
inflection of this verb. For this verb, 4,472 possible 
parse tree paths were tabulated into 3,145 unique 
features, 2501 of which occurred only once. 
The fourth step was to compute the distance be-
tween a given verb and each of the 3,257 feature 
vector representations describing the syntactic con-
text of PropBank verbs. We computed and com-
pared the performance of a wide variety of possible 
vector-based distance metrics, including Euclidean, 
Manhattan, and Chi-square (with un-normalized 
frequency counts), but found that the ubiquitous 
cosine measure was least sensitive to variations in 
sample size between verbs. To facilitate a com-
parative performance evaluation (section 6), pair-
wise cosine distance measures were calculated 
between each pair of PropBank verbs and sorted 
into individual files, producing 3,257 lists of 3,257 
verbs ordered by similarity. 
Table 2 lists the 25 most syntactically similar 
pairs of verbs among all PropBank verbs. There 
are a number of notable observations in this list. 
First is the extremely high similarity between bind 
and bound. This is partly due to the fact that they 
share an inflection (bound is the irregular past 
tense form of bind), so the first 100 instances of 
GigaWord sentences for each verb overlap signifi-
cantly, resulting in overlapping feature vector rep-
resentations. Although this problem appears to be 
restricted to this one pair of verbs, it could be 
avoided in the future by using the part-of-speech 
tag in the parse tree to help distinguish between 
verb lemmas. 
A second observation of Table 2 is that several 
verbs appear multiple times in this list, yielding 
sets of verbs that all have high syntactic similarity. 
Three of these sets account for 19 of the verbs in 
this list: 
1. plunge, tumble, dive, jump, fall, fell, dip 
2. assail, chide, lambaste 
3. buffet, embroil, lock, superimpose, whip-
saw, pluck, whisk, mar, ensconce 
The appearance of these sets suggests that our 
method of computing syntactic similarity could be 
used to identify distinct clusters of verbs that be-
have in very similar ways. In future work, it would 
be particularly interesting to compare empirically-
derived verb clusters to verb classes derived from 
theoretical considerations (Levin, 1993), and to the 
automated verb classification techniques that use 
these classes (Joanis and Stevenson, 2003). 
A third observation of Table 2 is that the verb 
pairs with the highest syntactic similarity are often 
synonyms, e.g. the cluster of assail, chide, and 
lambaste. As a striking example, the 14 most syn-
tactically similar verbs to believe (in order) are 
think, guess, hope, feel, wonder, theorize, fear, 
reckon, contend, suppose, understand, know, 
doubt, and suggest ? all mental action verbs. This 
observation further supports the distributional hy-
pothesis of word similarity and corresponding 
technologies for identifying synonyms by similar-
ity of lexical-syntactic context (Lin, 1998). 
   
Verb pairs (instances) Cosine 
bind (83) bound (95) 0.950 
plunge (94) tumble (87) 0.888 
dive (36) plunge (94) 0.867 
dive (36) tumble (87) 0.866 
jump (79) tumble (87) 0.865 
fall (84) fell (102) 0.859 
intersperse (99) perch (81) 0.859 
assail (100) chide (98) 0.859 
dip (81) fell (102) 0.858 
buffet (72) embroil (100) 0.856 
embroil (100) lock (73) 0.856 
embroil (100) superimpose (100) 0.856 
fell (102) jump (79) 0.855 
fell (102) tumble (87) 0.855 
embroil (100) whipsaw (63) 0.850 
pluck (100) whisk (99) 0.849 
acquit (100) hospitalize (99) 0.849 
disincline (70) obligate (94) 0.848 
jump (79) plunge (94) 0.848 
dive (36) jump (79) 0.847 
assail (100) lambaste (100) 0.847 
festoon (98) strew (100) 0.846 
mar (78) whipsaw (63) 0.846 
pluck (100) whipsaw (63) 0.846 
ensconce (101) whipsaw (63) 0.845 
Table 2. Top 25 most syntactically similar pairs of 
the 3257 verbs in PropBank. Each verb is listed 
with the number of inflection instances used to 
calculate the cosine measurement. 
195
5 Aligning Arguments Across Rolesets 
The second key aspect of our approach to general-
izing annotations is to make mappings between the 
argument roles of the novel target verb and the 
roles used for a given roleset in the PropBank cor-
pus. For example, if we?d like to apply the training 
data for a roleset of the verb desire in PropBank to 
a novel roleset for the verb yearn, we need to know 
that the desirer corresponds to the yearner, the de-
sired to the yearned-for, etc. In this section, we 
describe an approach to argument alignment that 
involves the application of the semantic role label-
ing approach described in section 3 to a single 
training example for the target verb. 
To simplify the process of aligning argument la-
bels across rolesets, we make a number of assump-
tions. First, we only consider cases where two 
rolesets have exactly the same number of argu-
ments. The version of the PropBank corpus that we 
used in this research contained 4250 rolesets, each 
with 6 or fewer roles (typically two or three). Ac-
cordingly, when attempting to apply PropBank 
data to a novel roleset with a given argument count 
(e.g. two), we only consider the subset of Prop-
Bank data that labels rolesets with exactly the same 
count. 
Second, our approach requires at least one fully-
annotated training example for the target roleset. A 
fully-annotated sentence is one that contains a la-
beled constituent in its parse tree for each role in 
the roleset. As an illustration, the example sentence 
in section 1 (for the roleset buy.01) would not be 
considered a fully-annotated training example, as 
only four of the five arguments of the PropBank 
buy.01 roleset are present in the sentence (it is 
missing a benefactor, as in ?Chuck bought his 
mother a car from Jerry for $1000?). 
In both of these simplifying requirements, we 
ignore role labels that may be assigned to a sen-
tence but that are not defined as part of the roleset, 
specifically the ARGM labels used in PropBank to 
label standard proposition modifiers (e.g. location, 
time, manner).  
Our approach begins with a list of verbs ordered 
by their calculated syntactic similarity to the target 
verb, as described in section 4 of this paper. We 
subsequently apply two steps that transform this 
list into an ordered set of rolesets that can be 
aligned with the roles used in one or more fully-
annotated training examples of the target verb. In 
describing these two steps, we use instigate as an 
example target verb. Instigate already appears in 
the PropBank corpus as a two-argument roleset, 
but it has only a single training example: 
 
[arg0 The Mahatma, or "great souled one,"] 
[instigate.01 instigated] [arg1 several campaigns of 
passive resistance against the British 
government in India]. 
 
The syntactic similarity of instigate to all Prop-
Bank verbs was calculated in the manner described 
in the previous section. This resulting list of 3,180 
entries begins with the following fourteen verbs: 
orchestrate, misrepresent, summarize, wreak, rub, 
chase, refuse, embezzle, harass, spew, thrash, un-
earth, snub, and erect. 
The first step is to replace each of the verbs in 
the ordered list with corresponding rolesets from 
PropBank that have the same number of roles as 
the target verb. As an example, our target roleset 
for the verb instigate has two arguments, so each 
verb in the ordered list is replaced with the set of 
corresponding rolesets that also have two argu-
ments, or removed if no two-argument rolesets 
exist for the verb in the PropBank corpus. The or-
dered list of verbs for instigate is transformed into 
an ordered list of 2,115 rolesets with two argu-
ments, beginning with the following five entries: 
orchestrate.01, chase.01, unearth.01, snub.01, and 
erect.01.  
The second step is to identify the alignments be-
tween the arguments of the target roleset and each 
of the rolesets in the ordered list. Beginning with 
the first roleset on the list (e.g. orchestrate.01), we 
build a semantic role labeler (as described in sec-
tion 3) using its available training annotations from 
the PropPank corpus. We then apply this labeler to 
the single, fully-annotated example sentence for 
the target verb, treating it as if it were a test exam-
ple of the same roleset. We then check to see if any 
of the core (numbered) role labels overlap with the 
annotations that are provided. In cases where an 
annotated constituent of the target test sentence is 
assigned a label from the source roleset, then the 
roleset mappings are noted along with the entry in 
the ordered list. If no mappings are found, the role-
set is removed from the ordered list. 
For example, the roleset for orchestrate.01 con-
tains two arguments (ARG0 and ARG1) that corre-
spond to the ?conductor, manager? and the ?things 
196
being coordinated or managed?. This roleset is 
used for only three sentence annotations in the 
PropBank corpus. Using these annotations as train-
ing data, we build a semantic role labeler for this 
roleset and apply it to the annotated sentence for 
instigate.01, treating it as if it were a test sentence 
for the roleset orchestrate.01. The labeler assigns 
the orchestrate.01 label ARG1 to the same con-
stituent labeled ARG1 in the test sentence, but fails 
to assign a label to the other argument constituent 
in the test sentence. Therefore, a single mapping is 
recorded in the ordered list of rolesets, namely that 
ARG1 of orchestrate.01 can be mapped to ARG1 
of instigate.01. 
After all of the rolesets are considered, we are 
left with a filtered list of rolesets with their argu-
ment mappings, ordered by their syntactic similar-
ity to the target verb. For the roleset instigate.01, 
this list consists of 789 entries, beginning with the 
following 5 mappings. 
1. orchestrate.01, 1:1 
2. chase.01, 0:0, 1:1  
3. unearth.01, 0:0, 1:1  
4. snub.01, 1:1  
5. erect.01, 0:0, 1:1  
Given this list, arbitrary amounts of PropBank 
annotations can be used as surrogate training data 
for the instigate.01 roleset, beginning at the top of 
the list. To utilize surrogate training data in our 
semantic role labeling approach (Section 3), we 
combine parse tree path information for a selected 
portion of surrogate training data into a single list 
sorted by frequency, and apply these files to test 
sentences as normal.  
Although we use an existing PropBank roleset 
(instigate.01) as an example in this section, this 
approach will work for any novel roleset where 
one fully-annotated training example is available. 
For example, arbitrary amounts of surrogate Prop-
Bank data can be found for the novel verb yearn by 
1) searching for sentences with the verb yearn in 
the GigaWord corpus, 2) calculating the syntactic 
similarity between yearn and all PropBank verbs 
as described in Section 4, 3) aligning the argu-
ments in a single fully-annotated example of yearn 
with ProbBank rolesets with the same number of 
arguments using the method described in this sec-
tion, and 4) selecting arbitrary amounts of Prop-
Bank annotations to use as surrogate training data, 
starting from the top of the resulting list. 
6 Evaluation 
We conducted a large-scale evaluation to deter-
mine the performance of our semantic role labeling 
algorithm when using variable amounts of surro-
gate training data, and compared these results to 
the performance that could be obtained using vari-
ous amounts of real training data (as described in 
section 3). Our hypothesis was that learning-curves 
for surrogate-trained labelers would be somewhat 
less steep, but that the availability of large-amounts 
of surrogate training data would more than make 
up for the gap.  
To test this hypothesis, we conducted an evalua-
tion using the PropBank corpus as our testing data 
as well as our source for surrogate training data. As 
described in section 5, our approach requires the 
availability of at least one fully-annotated sentence 
for a given roleset. Only 28.5% of the PropBank 
annotations assign labels for each of the numbered 
arguments in their given roleset, and only 2,858 of 
the 4,250 rolesets used in PropBank annotations 
(66.5%) have at least one fully-annotated sentence. 
Of these, 2,807 rolesets were for verbs that ap-
peared at least once in our analysis of the Giga-
Word corpus (Section 4). Accordingly, we 
evaluated our approach using the annotations for 
this set of 2,807 rolesets as test data. For each of 
these rolesets, various amounts of surrogate train-
ing data were gathered from all 4,250 rolesets rep-
resented in PropBank, leaving out the data for 
whichever roleset was being tested. 
For each of the target 2,807 rolesets, we gener-
ated a list of semantic role mappings ordered by 
syntactic similarity, using the methods described in 
sections 4 and 5. In aligning arguments, only a sin-
gle training example from the target roleset was 
used, namely the first annotation within the Prop-
Bank corpus where all of the rolesets arguments 
were assigned. Our approach failed to identify any 
argument mappings for 41 of the target rolesets, 
leaving them without any surrogate training data to 
utilize. Of the remaining 2,766 rolesets, the num-
ber of mapped rolesets for a given target ranged 
from 1,041 to 1 (mean = 608, stdev = 297). 
For each of the 2,766 target rolesets with aligna-
ble roles, we gathered increasingly larger amounts 
of surrogate training data by descending the or-
dered list of mappings translating the PropBank 
data for each entry according to its argument map-
pings. Then each of these incrementally larger sets 
197
of training data was then used to build a semantic 
role labeler as described in section 3. The perform-
ance of each of the resulting labelers was then 
evaluated by applying it to all of the test data 
available for target roleset in PropBank, using the 
same scoring methods described in section 3. The 
performance scores for each labeler were recorded 
along with the total number of surrogate training 
examples used to build the labeler. 
Figure 3 presents the performance result of our 
semantic role labeling approach using various 
amounts of surrogate training data. Along with 
precision, recall, and F-score data, Figure 3 also 
graphs the percentage of PropBank rolesets for 
which a given amount of training data had been 
identified using our approach, of the 2,858 rolesets 
with at least one fully-annotated training example. 
For instance, with 120 surrogate annotations our 
system achieves an F-score above 0.5, and we 
identified this much surrogate training data for 
96% of PropBank rolesets with at least one fully-
annotated sentence. This represents 64% of all 
PropBank rolesets that are used for annotation. 
Beyond 120 surrogate training examples, F-scores 
remain around 0.6 before slowly declining after 
around 700 examples. 
 
 
Figure 3. Performance of our semantic role label-
ing approach on PropBank rolesets using various 
amounts of surrogate training data 
  
Several interesting comparisons can be made be-
tween the results presented in Figure 3 and those in 
Figure 2, where actual PropBank training data is 
used instead of surrogate training data. First, the 
precision obtained with surrogate training data is 
roughly 10% lower than with real data. Second, the 
recall performance of surrogate data performs 
similar to real data at first, but is consistently 10% 
lower than with real data after the first 50 training 
examples. Accordingly, F-scores for surrogate 
training data are 10% lower overall.  
Even though the performance obtained using 
surrogate training data is less than with actual data, 
there is abundant amounts of it available for most 
PropBank rolesets. Comparing the ?% of rolesets? 
plots in Figures 2 and 3, the real value of surrogate 
training data is apparent. Figure 2 suggests that 
over 20 real training examples are needed to 
achieve F-scores that are consistently above 0.5, 
but that less than 20% of PropBank rolesets have 
this much data available. In contrast, 64% of all 
PropBank rolesets can achieve this F-score per-
formance with the use of surrogate training data. 
This percentage increases to 96% if every Prop-
Bank roleset is given at least one fully annotated 
sentence, where all of its numbered arguments are 
assigned to constituents.  
In addition to supplementing the real training 
data available for existing PropBank rolesets, these 
results predict the labeling performance that can be 
obtained by applying this technique to a novel 
roleset with one fully-annotated training example, 
e.g. for the verb yearn. Using the first 120 surro-
gate training examples and our simple semantic 
role labeling approach, we would expect F-scores 
that are above 0.5, and that using the first 700 
would yield F-scores around 0.6. 
7 Discussion 
The overall performance of our semantic role la-
beling approach is not competitive with leading 
contemporary systems, which typically employ 
support vector machine learning algorithms with 
syntactic features (Pradhan et al, 2005) or syntac-
tic tree kernels (Moschitti et al, 2006). However, 
our work highlights a number of characteristics of 
the semantic role labeling task that will be helpful 
in improving performance in future systems. Parse 
tree paths features can be used to achieve high pre-
cision in semantic role labeling, but much of this 
precision may be specific to individual verbs. By 
generalizing parse tree path features only across 
syntactically similar verbs, we have shown that the 
drop in precision can be limited to roughly 10%. 
The approach that we describe in this paper is 
not dependent on the use of PropBank rolesets; any 
large corpus of semantic role annotations could be 
198
generalized in this manner. In particular, our ap-
proach would be applicable to corpora with frame-
specific role labels, e.g. FrameNet (Baker et al, 
1998). Likewise, our approach to generalizing 
parse tree path feature across syntactically similar 
verbs may improve the performance of automated 
semantic role labeling systems based on FrameNet 
data. Our work suggests that feature generalization 
based on verb-similarity may compliment ap-
proaches to generalization based on role-similarity 
(Gildea and Jurafsky, 2002; Baldewein et al, 
2004). 
There are a number of improvements that could 
be made to the approach described in this paper. 
Enhancements to the simple semantic role labeling 
algorithm would improve the alignment of argu-
ments across rolesets, which would help align role-
sets with greater syntactic similarity, as well as 
improve the performance obtained using the surro-
gate training data in assigning semantic roles.  
This research raises many questions about the 
relationship between syntactic context and verb 
semantics. An important area for future research 
will be to explore the correlation between our dis-
tance metric for syntactic similarity and various 
quantitative measures of semantic similarity 
(Pedersen, et al, 2004). Particularly interesting 
would be to explore whether different senses of a 
given verb exhibited markedly different profiles of 
syntactic context. A strong syntactic/semantic cor-
relation would suggest that further gains in the use 
of surrogate annotation data could be gained if syn-
tactic similarity was computed between rolesets 
rather than their verbs. However, this would first 
require accurate word-sense disambiguation both 
for the test sentences as well as for the parsed cor-
pora used to calculate parse tree path frequencies. 
Alternatively, parse tree path profiles associated 
with rolesets may be useful for word sense disam-
biguation, where the probability of a sense is com-
puted as the likelihood that an ambiguous verb's 
parse tree paths are sampled from the distributions 
associated with each verb sense. These topics will 
be the focus of our future work in this area. 
Acknowledgments 
The project or effort depicted was or is sponsored 
by the U.S. Army Research, Development, and 
Engineering Command (RDECOM), and that the 
content or information does not necessarily reflect 
the position or the policy of the Government, and 
no official endorsement should be inferred. 
References  
Baker, C., Fillmore, C., and Lowe, J. 1998. The Ber-
keley FrameNet Project, In Proceedings of COLING-
ACL, Montreal. 
Baldewein, U., Erk, K., Pado, S., and Prescher, D. 2004. 
Semantic role labeling with similarity-based gener-
alization using EM-based clustering. Proceedings of 
Senseval-3, Barcelona. 
Charniak, E. 2000. A maximum-entropy-inspired 
parser, Proceedings NAACL-ANLP, Seattle. 
Courtois, B. 2004. Dictionnaires ?lectroniques DELAF 
anglais et fran?ais. In C. Lecl?re, E. Laporte, M. Piot 
and M. Silberztein (eds.) Syntax, Lexis and Lexicon-
Grammar: Papers in Honour of Maurice Gross. Am-
sterdam: John Benjamins. 
Gildea, D. and Jurafsky, D. 2002. Automatic Labeling 
of Semantic Roles. Computational Linguistics 28:3, 
245-288. 
Joanis, E. and Stevenson, S. 2003. A general feature 
space for automatic verb classification. Proceedings 
EACL, Budapest. 
Levin, B. 1993. English Verb Classes and Alterna-tions: 
A Preliminary Investigation. Chicago, IL: University 
of Chicago Press. 
Lin, D. 1998. Automatic Retrieval and Clustering of 
Similar Words. COLING-ACL, Montreal. 
Linguistic Data Consortium. 2003. English Gigaword. 
Catalog number LDC2003T05. Available from LDC 
at http://www.ldc.upenn.edu. 
Moschitti, A., Pighin, D. and Basili, R. 2006. Semantic 
Role Labeling via Tree Kernel joint inference. Pro-
ceedings of CoNLL, New York. 
Palmer, M., Gildea, D., and Kingsbury, P. 2005. The 
Proposition Bank: An Annotated Corpus of Semantic 
Roles. Computational Linguistics 31(1):71-106. 
Pedersen, T., Patwardhan, S. and Michelizzi, J. 2004. 
WordNet::Similarity - Measuring the Relatedness of 
Concepts. Proceedings NAACL-04, Boston, MA. 
Pradhan, S., Ward, W., Hacioglu, K., Martin, J., and 
Jurafsky, D. 2005. Semantic role labeling using dif-
ferent syntactic views. Proceedings ACL-2005, Ann 
Arbor, MI. 
Reynar, J. and Ratnaparkhi, A. 1997. A Maximum En-
tropy Approach to Identifying Sentence Boundaries. 
Proceedings of ANLP, Washington, D.C. 
199
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 192?201,
Paris, October 2009. c?2009 Association for Computational Linguistics
Clustering Words by Syntactic Similarity Improves Dependency   Parsing of Predicate-Argument Structures 
 Kenji Sagae and Andrew S. Gordon Institute for Creative Technologies University of Southern California 13274 Fiji Way Marina del Rey, CA 90292 {sagae,gordon}@ict.usc.edu     Abstract 
We present an approach for deriving syntactic word clusters from parsed text, grouping words according to their unlexicalized syntac-tic contexts.  We then explore the use of these syntactic clusters in leveraging a large corpus of trees generated by a high-accuracy parser to improve the accuracy of another parser based on a different formalism for representing a dif-ferent level of sentence structure.  In our ex-periments, we use phrase-structure trees to produce syntactic word clusters that are used by a predicate-argument dependency parser, significantly improving its accuracy. 1 Introduction Syntactic parsing of natural language has ad-vanced greatly in recent years, in large part due to data-driven techniques (Collins, 1999; Charniak, 2000; Miyao and Tsujii, 2005; McDonald et al, 2005; Nivre et al, 2007) cou-pled with the availability of large treebanks. Sev-eral recent efforts have started to look for ways to go beyond what individual annotated data sets and individual parser models can offer, looking to combine diverse parsing models, develop cross-framework interoperability and evaluation, and leverage the availability of large amounts of text available.  Two research directions that have produced promising improvements on the accu-racy of data-driven parsing are: (1) combining different parsers using ensemble techniques, such as voting (Henderson and Brill, 1999; Sagae and Lavie, 2006; Hall et al, 2007) and stacking (Nivre and McDonald, 2008; Martins et al, 2008), and (2) semi-supervised learning, where unlabeled data (plain text) is used in addition to a 
treebank (McClosky et al, 2006; Koo et al, 2008). In this paper we explore a new way to obtain improved parsing accuracy by using a large amount of unlabeled text and two parsers that use different ways of representing syntactic structure.  In contrast to previous work where automatically generated constituent trees were used directly to train a constituent parsing model (McClosky et al, 2006), or where word clusters were derived from a large corpus of plain text to improve a dependency parser (Koo et al, 2008), we use a large corpus of constituent trees (previously gen-erated by an accurate constituent parser), which we use to produce syntactically derived clusters that are then used to improve a transition-based parser that outputs dependency graphs that re-flect predicate-argument structure where words may be dependents of more than one parent.  This type of representation is more general than dependency trees (Sagae and Tsujii, 2008; Henderson et al, 2008), and is suitable for repre-senting both surface relations and long-distance dependencies (such as control, it-cleft and tough movement). The first contribution of this work is a novel approach for deriving syntactic word clusters from parsed text, grouping words by the general syntactic contexts where they appear, and not by n-gram word context (Brown et al, 1992) or by immediate dependency context (Lin, 1998).  Un-like in clustering approaches that rely on lexical context (either linear or grammatical) to group words, resulting in a notion of word similarity that blurs syntactic and semantic characteristics of lexical items, we use unlexicalized syntactic context, so that words are clustered based only on their syntactic behavior.  This way, we at-tempt to generate clusters that are more concep-tually similar to part-of-speech tags or supertags 
192
(Bangalore and Joshi, 1999), but organized hier-archically to provide tagsets with varying levels of granularity. Our second contribution is a methodology for leveraging a high-accuracy parser to improve the accuracy of a parser that uses a different formal-ism (that represents different structural informa-tion), without the need to process the input with both parsers at run-time.  In our experiments, we show that we can improve the accuracy of a fast dependency parser for predicate-argument struc-tures by using a corpus which was previously automatically annotated using a highly accurate but considerably slower phrase-structure tree parser.  This is accomplished by using the slower parser only to parse the data used to create the syntactic word clusters.  During run-time, the dependency parser uses these clusters, which encapsulate syntactic knowledge from the phrase-structure parser.  Although our experi-ments focus on the use of phrase-structure and dependency parsers, the same framework can be easily applied to data-driven parsing using other syntactic formalisms, such as CCG or HPSG. 2 Clustering by Syntactic Similarity We developed a new approach to clustering words according to their syntactic similarity. Our method involves the use of hierarchical agglom-erate clustering techniques using the calculated syntactic distance between words. Syntactic dis-tance between words is computed as the cosine distance between vector representations of the frequency of unique parse tree paths emanating from the word in a corpus of parse trees. In this research, we employ a novel encoding of syntac-tic parse tree paths that includes direction infor-mation and non-terminal node labels, but does not include lexical information or part-of-speech tags. Consequently, the resulting hierarchy groups words that appear in similar places in similar parse trees, regardless of its assigned part-of-speech tag.  In this section we describe our approach in detail. 2.1 Parse tree path representation Gordon and Swanson (2007) first described a corpus-based method for calculating a measure of syntactic similarity between words, and dem-onstrated its utility in improving the performance of a syntax-based Semantic Role Labeling sys-tem. The central idea behind their approach was that parse tree paths could be used as features for describing a word?s grammatical behavior. 
Parse tree paths are descriptions of tree transi-tions from a terminal (e.g. a verb) to a different node in a constituent parse tree of a sentence. Parse tree paths gained popularity in early Se-mantic Role Labeling research (Gildea and Juraf-sky, 2002), where they were used as features de-scribing the relationship between a verb and a particular semantic role label. For example, Fig-ure 1 illustrates a parse tree path between a verb and a semantically related noun phrase. Gordon and Swanson viewed parse tree paths as features that could be used to describe the syn-tactic contexts of words in a corpus. In their ap-proach, all of the possible parse tree paths that begin at a given word were identified in a large set of automatically generated constituent parse trees. The normalized frequency counts of unique parse tree paths were combined into a feature vector that describes the location that the given word appears in the set of parse trees. This syntactic profile was then compared with other profiles using a cosine distance function, produc-ing a quantitative value of word similarity. In this manner, the syntactic similarity between the verb ?pluck? and the verb ?whisk? was calcu-lated as 0.849. One drawback of the approach of Gordon and Swanson was the inclusion of part-of-speech tags in the encoding of the parse tree paths. As a con-sequence, the cosine distance between words of different classes was always zero, regardless of their similarities in the remainder of the paths. To correct this problem in our current research, we removed part-of-speech tags from the encod-ing of parse tree paths, deleting the tag that be-gins each path and replacing tags when they ap-pear at the end of a path with a generic terminal label.  A second drawback of the approach of Gordon and Swanson is that the path directionality is un-derspecified. Consider the parse tree paths that 
 Figure 1: An example parse tree path from the verb ate to the argument NP He, repre-sented as ?VBD?VP?S?NP.  
193
emanate from each of the words ?some? and ?pancakes? in Figure 1. In the original encoding, the paths for each of these words would be iden-tical (if the part of speech tags were removed), despite their unique locations in this parse tree. To correct this problem in our current research, we elaborated the original set of two path identi-fiers (? and ?) to a set of six tags that included information about the direction of the transition. Up-Right (?) and Down-Left (?) transition are used to and from nodes that are the first constitu-ent of a non-terminal. Up-Left (?) and Down-Right (?) transitions are used to and from nodes that are the last constituent of a non-terminal. Transitions to and from all other constituent nodes are labeled Up-Middle (?) or Down-Middle (?), accordingly. For example, we repre-sent the parse tree path depicted in Figure 1 as: ?VP?S?NP. 2.2 Profiles for BLLIP WSJ Corpus words As in the previous work of Gordon and Swanson (2007), we characterize the syntactic properties of words as the normalized frequency of unique parse tree paths emanating from the word in a large corpus of syntactic parse trees.  In our research, we used the Brown Labora-tory for Linguistic Information Processing (BLLIP) 1987-89 WSJ Corpus Release 1 (Charniak et al, 2000), which contains approxi-mately 30 million words of Wall Street Journal news articles, parsed with Charniak (2000) parser.  Although the trees in the BLLIP corpus are enriched with function tags and empty nodes, we remove this information, leaving only the trees produced by the Charniak parser.  We iden-tified the top five thousand most frequent words (or, more generally, types, since these also in-clude other sequences of characters, such as numbers and punctuation) in this corpus, treating words that differed in capitalization or in as-signed part-of-speech tag as separate types. These five thousand types correspond to ap-proximately 85% of the tokens in the BLLIP corpus.  For each token instance of each of these five thousand types, we identified every occur-ring parse tree path emanating from the token in each of the sentences in which it appeared. The most frequent type was the comma, which ap-peared 2.2 million times and produced 118 mil-lion parse tree paths. The least frequent token in this set was the singular noun ?pollution,? with 731 instances producing 35,185 parse tree paths. To generate syntactic profiles for a given type, the frequency of unique parse tree paths was ta-
tabulated, and then normalized by dividing this frequency by the number of tokens of that type in the corpus. To reduce the dimensionality of these normalized frequency vectors, parse tree paths that appeared in less than 0.2% of the instances were ignored. This threshold value produced vectors with dimensionality that was comparable across all five thousand types, and small enough to process given our available computational re-sources.  The mean vector size was 2,228 dimen-sions, with a standard deviation of 734. 2.3 Distance calculation and clustering Pairwise distances between each of the five thou-sand types were computed as the cosine distance between their profile vectors. We then grouped similar types using hierarchical agglomerate clustering techniques, where distance between clusters is calculated as mean distance between elements of each cluster (average link cluster-ing).  The three most similar types (the first 2 clus-tering steps) consisted of the capitalized subordi-nating conjunctions ?Although,? ?While,? and ?Though.? The two most dissimilar types (the last to be included in any existing cluster) were the symbol ?@? and the question mark. 2.4 Cluster label selection Hierarchical agglomerate clustering produces a binary-branching tree structure, where each branch point is ordered according to a similarity value between 0 and 1. In our clustering of the top five thousand most frequent types in the BLLIP corpus, there are five thousand leaf nodes representing individual tokens, and 4999 branch points that cluster these types into a single tree. We label each of these 4999 branch points, and treat these cluster labels as features of the types that they dominate. For example, the singular noun ?house? participates in 114 clusters of in-creasing size. The syntactic features of this type can therefore be characterized by 114 cluster la-bels, which overlap with varying degrees with other tokens in the set. We view these cluster labels as conceptually similar to traditional part-of-speech tags in that they are indicative of the syntactic contexts in which words are likely to appear.  Because words are clustered based on their unlexicalized syntactic contexts, the resulting clusters are more likely to reflect purely syntactic information than are clusters derived from lexical context, such as adjacent words (Brown et al, 1992) or immedi-ate head-word (Lin, 1998).  However, the extent 
194
to which these syntactic contexts are specified can vary from a more general to a more fine-grained level than that of parts-of-speech.  As clusters become more fine-grained, they become more similar to supertags (Bangalore and Joshi, 1999).  Clusters that represent more specific syn-tactic contexts can encode information about, for example, subcategorization.  As these labels are derived empirically from a large corpus of syn-tactic parse trees, they accurately represent syn-tactic distinctions in real discourse at different granularities, in contrast to the single arbitrary granularity of theoretically derived part-of-speech tags used in existing treebanks (Marcus et al, 1993).  While it is sometimes useful to view types as having multiple part-of-speech tags at different levels of granularity (e.g. the 114 tags for the token ?house?), it is often useful to select a sin-gle level of granularity to use across all tokens. For example, it is useful to know which one of the 114 cluster labels for ?house? to use if ex-actly 100 part-of-speech distinctions are to be made among tokens in the set. These cluster la-bels can be identified by slicing the tree at the level for which there are exactly 100 branches, then using the label of the first branch point in each branch as the label for all of its leaf-node types, or the leaf-node itself in the case where no further branching exists. Given our hierarchical clustering, there are five thousand different ways to slice the tree in this manner, yielding sets of cluster labels (and un-clustered types) that vary in size from 1 to 5000. We identified these sets for use in the experiments described in the next sections. Figure 2 shows a dendrogram representation of the cluster tree when it is sliced to produce exactly 60 clusters, 19 of which are individual types. For the other 41 clusters, we show only the most frequent word in the cluster and the number of additional words in the cluster.  The scale line in the lower left of Figure 2 indicates the horizontal length of a calculated similarity between clusters of 0.1. 3 Transition-based dependency parsing with word clusters The clusters obtained with the approach de-scribed in section 2 provide sets of syntactic tags with varying levels of granularity.  Previous work by Koo et al (2008) and Miller et al (2004) suggests that different levels of cluster granularity may be useful in natural language 
   Figure 2: A hierarchical clustering of the top five thousand tokens in the BLLIP corpus, cut at 60 clusters. 
195
processing tasks with discriminative training.  We add the syntactic clusters as features in a transition-based parser that uses a classifier to decide among shift/reduce parser actions based on the local context of the decision.  This transi-tion-based parsing approach has been found to be efficient and accurate in dependency parsing of surface syntactic dependencies (Yamada and Matsumoto, 2003; Nivre et al, 2004; Hall et al, 2007) and predicate-argument parsing (Hender-son et al, 2008; Sagae and Tsujii, 2008). Our experiments are based on an implementa-tion of Sagae and Tsujii (2008)?s algorithm for basic shift-reduce parsing with multiple heads, which we use to identify predicate-argument de-pendencies extracted from the HPSG Treebank developed by Miyao et al (2004).  Using this data set alows for a comparison of our results with those obtained in previous work on data-driven HPSG predicate-argument analysis, while demonstrating the use of our clustering approach for cross-framework parser improvement, since the clusters were derived from syntactic trees in Penn Treebank format (as produced by the Char-niak parser, without empty nodes, co-indexation or function tags), and used in the identification of HPSG Treebank predicate-argument dependencies.  Figure 3 shows a predicate-argument dependency structure following the annotation standard of the HPSG Treebank, where arrows point from head to modifier.  We note that unlike in the widely known PropBank (Palmer et al, 2005) predicate-argument struc-tures, argument labels start from ARG1 (not ARG0), and predicate-argument relationships are annotated for all words.  One difference between in our implementation is that, instead of maxi-mum entropy classification used by Sagae and Tsujii, we perform parser action classification using the averaged perceptron (Freund and 
Schapire, 1999; Collins, 2002), which allows for the inclusion of all of Sagae and Tsujii?s fea-tures, in addition to a set of cluster-based fea-tures, while retaining fast training times. We now describe the parsing approach, start-ing with the dependency DAG parser that we use as a baseline, followed by how the syntactic clus-ter features were added to the baseline parser. 3.1 Arc-standard parsing for dependency DAGs Sagae and Tsujii (2008) describe two algorithms for dependency parsing with words that have multiple heads.  Each corresponds to extensions of Nivre (2004)?s arc-standard and arc-eager al-gorithms for dependency (tree) parsing.  In our experiments, we used an implementation of the arc-standard extension.   Nivre?s arc-standard dependency parsing algo-rithm uses a stack to process the input string one word at a time, from left to right, using two gen-eral types of parser action: shift (push the next input token onto the stack), and reduce (create a dependency arc between the top two items on the stack, and pop the item marked as the depend-ent).  Reduce actions are subdivided into reduce-right and reduce-left, indicating which of the two items on the top of the stack is the head, and which is the dependent in the newly formed de-pendency arc.  These two reduce actions can be further subdivided to reflect what type of de-pendency arc is created, in the case of labeled dependency parsing.  The extension for allowing multiple heads per word consists of the addition a new type of parser action: attach, which creates a dependency arc without removing anything from the stack.  As with reduce actions, there are two types of attach: attach-left which creates a dependency arc between the top two items on the stack such that the item on top is the head, and 
           Figure 3: Predicate-argument dependency structure following the HPSG Treebank standard. 
196
right-attach, which creates a dependency arc be-tween the top two items on the stack such that the top item is the dependent, then pops it from the stack and unshifts it back into the input.  Fi-nally, this algorithm for unlabeled graphs can be extended to produce labeled dependencies in the same way as Nivre?s algorithm, by replacing the reduce and attach actions with sets of actions that perform the reduce or attach operation and also name the label of the arc created.  Sagae and Tsujii (2008) provide a more detailed description of the algorithm, including an example that illus-trates the new attach actions. This basic algorithm is only capable of pro-ducing labeled directed acyclic graphs where, if the nodes (which correspond to words) are placed on a left to right sequence on a horizontal line in the order in which the words appear in the input sentence, all arcs can be drawn above the nodes without crossing.  This corresponds to the notion of projectivity that similarly limits the types of trees produced by Nivre?s algorithm.  Just as in dependency parsing with tree struc-tures, a way to effectively remove this limitation is the use of pseudo-projective transformations (Nivre and Nilsson, 2005), where arcs that cross have their heads moved towards the root and have their labels edited to reflect this change, often making it reversible.  Once crossing arcs have been ?lifted? so that no crossing arcs re-main, the ?projectivized? structures are used to train a parsing model.  Projective structures pro-duced by this model can be ?deprojectivized? through the use of the edits in the arc labels, in an attempt to produce structures that conform to the scheme in the original data.  Sagae and Tsujii also propose a simple arc reversal transform, which simply reverses the direction of a depend-ency arc (editing the arc label to note this change).  This transformation, which can be re-versed trivially, makes it possible to remove cy-cles in dependency graphs. 3.2 Baseline features  To create output graph structures for an input sentence, the algorithm described in section 3.1 relies on an oracle that tells it what action to take at each parser state, where the state is the con-tents of the stack, remaining words in the input, and the dependency arcs formed so far.  In grammar-based shift-reduce parsing, this oracle may take the form of a look-up table derived from grammar rules.  In our data-driven setting, where the parser learns to choose actions based on examples of correctly parsed data, the (likely 
imperfect) substitute for the oracle is a classifier that takes features that represent the parser state as input, and produces a matching parser action as output.  These features should represent as-pects of the parser state that may be informative as to what the corresponding appropriate action is.  Our baseline model uses the averaged percep-tron with a core set of features derived from the following templates, where S(n) denotes the n-th item from the top of the stack (for example, S(1) is the item on top of the stack), and I(n) denotes the next n-th input token: 1. For the items S(1) and S(2): a. the total number of dependents; b. the number of dependents to the right of the item; c. the number of dependents to the left of the item; d. the part-of-speech tag of the right-most dependent of the item; e. the part-of-speech tag of the leftmost dependent of the item; f. the arc label of the rightmost de-pendent of the item; g. the arc label of the leftmost depend-ent of the item; 2. the words in items S(1), S(2), S(3), I(1) and I(2); 3. the part-of-speech tags in items S(1), S(2), S(3), I(1), I(2) and I(3); 4. the part-of-speech tag of the word i mmedi-aely to the right of S(2); 5. the part-of-speech tag of the word immedi-ately to the left of S(1); 6. whether an arc exists between S(1) and S(2); 7. whether an arc exists between S(1) and I(1); 8. the direction of the arc between S(1) and S(2), if any; 9. the label of the arc between S(1) and S(2), if any; 10. the label of the arc between S(1) and I(1), if any; 11. the distance, in linear sequence of words, between S(1) and S(2); 12. the distance, in linear sequence of words, between S(1) and I(1); 
197
13. the previous parser action. In addition to the core set of features, we also use features obtained by concatenating the part-of-speech tags in S(1), S(2) and I(1) with the fea-tures derived from templates 1-6, and additional features derived from selected concatenation of two or three core features. 3.3 Cluster-based features To take advantage of the clusters that reflect syn-tactic similarity between words, we assign arbi-trary unique labels to each of the hierarchical clusters obtained using the procedure described in section 2.  These cluster labels are used to generate additional features that help the parser make its decisions base on the syntactic profile of words.  As explained in section 2.4, each there may be several cluster labels (corresponding to clusters of different granularities) associated with each word.  To select the set of cluster labels to be used to generate features, we first select a de-sired granularity for the clusters, and use the set of labels resulting from slicing the cluster tree at the appropriate level, as discussed in section 2.4.  We experimented with several levels of cluster granularity using development data, and follow-ing Koo et al (2008), we also experimented with using two sets of cluster labels with different levels of granularity at the same time.  Given a specific level of granularity, the cluster-based features we used are: 14. the cluster labels for the words in items S(1), S(2), S(3), I(1), I(2), I(3); 15. the cluster labels for the words in the right-most and leftmost dependents of S(1) and S(2); 16. the concatenation of the cluster labels for the words in S(1), S(2) and I(1), and the features derived from feature templates 1-15. In experiments where we used two sets of cluster labels corresponding to different levels of granularity, we added all the cluster-based fea-tures in 14 and 15 for both sets of labels, and the features in 16 only for the set corresponding to the coarser-grained clusters. 4 Experiments Following previous experiments with Penn Tree-bank WSJ data, or annotations derived from it, we used sections 02-21 of the HPSG Treebank as training material, section 22 for development, and section 23 for testing. Only the predicate-
argument dependencies were used, not the phrase structures or other information from the HPSG analyses. For all experiments described here, part-of-speech tagging was done separately using a CRF tagger with accuracy of 97.3% on sections 22-24.  Our evaluation is based on labeled preci-sion and recall of predicate-argument dependen-cies.  Although accuracy is commonly used for evaluation of dependency parsers, in our task the parser is not restricted to output a fixed number of dependencies.  Labeled precision and recall of predicate-argument pairs are also the standard evaluation metrics for data-driven HPSG and CCG parsers (although the predicate-argument pairs extracted from the HPSG Treebank and the CCGBank are specific to their formalisms and not quantitatively comparable). We started by eliminating cycles from the de-pendency graphs extracted from the HPSG Tree-bank by using the arc reversal transform in the following way: for each cycle detected in the data, the shortest arc in the cycle was reversed until no cycles remained. We then applied pseudo-projective transformation to create data that can be used to train our parser, described in section 3.  By detransforming the projective graphs generated from gold-standard dependen-cies, we obtain labeled precision of 98.1% and labeled recall of 97.7%, which is below the accu-racy expected for detransformation of syntactic dependency trees.  This is expected, since arc crossing occurs more frequently in predicate-argument graphs in the HPSG Treebank than in surface syntactic dependencies. We first trained a parsing model without clus-ter-based features, using only the baseline set of features, which was the product of experimenta-tion using the development set.  On the test set, this baseline model has labeled precision and recall of 88.7 and 88.2, respectively, slightly be-low the precision and recall obtained by Sagae and Tsujii on the same data (89.0 precision and 88.5 recall). We then used the development set to explore the effects of cluster sets with different levels of granularity.  The baseline model has precision and recall of 88.6 and 88.0 on the development set.  We found that by slicing the cluster tree relatively close to the root, resulting in a set of 50 to 100 distinct cluster labels (corresponding to relatively coarse clusters), we obtain small (0.3 to 0.4), but statistically significant (p < 0.005) improvements on precision and recall over the baseline model on the development set.  By in-creasing the number of cluster labels (making the 
198
distinctions among members of different clusters more fine-grained) in steps of 100, we observed improvements in precision and recall until the point where there were 600 distinct cluster la-bels.  This set of 600 cluster labels produced the highest values of precision and recall (89.5 and 89.0) that we obtained for the development set using only one set of cluster labels.  Figure 4 shows how precision, recall and F-score on the development set varied with the number of clus-ter labels used. Following Koo et al (2008), we also experi-mented with using two sets of cluster labels with different levels of granularity.  We found that using the set of 600 labels and an additional set with fewer than 600 labels did not improve or hurt precision and recall.  Finer grained clusters with more than 1,000 labels (combined with the set of 600 labels) improved results further.  The highest precision and recall figures of 90.1 and 89.6 were obtained with the sets of 600 and 1,400 labels. We parsed the test set using the best configu-ration of cluster-based features as determined using the development set (the sets with 600 and 1,400 cluster labels) and obtained 90.2 precision, 89.8 recall and 90.0 f-score, a 13.8% reduction in error over a strong baseline.  Table 1 summarizes our results on the test set.  For comparison, we also shows results published by Sagae and Tsujii (2008), to our knowledge the highest f-score re-ported for this test set, and Miyao and Tsujii (2005), who first reported results on this data set. 
4.1 Surface dependency parsing with clus-ter-based features The parser used in our experiments with HPSG Treebank predicate-argument structures can as-sign more than one head for a single word, but when the parser is trained using only dependency trees, it behaves in exactly the same way as a parser based on Nivre?s arc-standard algorithm, since it never sees examples of attach actions during training.  To see whether our clusters can improve surface dependency parsing, and to al-low for comparison of our results to a larger body of research on surface dependency parsing, we used dependency trees extracted from the Penn Treebank using the Yamada and Matsu-moto (2003) version of the Penn Treebank head-percolation rules to train parsing models that produce dependency trees.  However, no tuning of the features or metaparameters was per-formed; the parser was trained as-is on depend-ency trees. We used the standard train, development and test sets splits to train two models, as in our ex-periments with predicate-argument dependen-cies: a baseline that uses no cluster information, and a model that uses two sets of clusters that were found to improve results in the develop-ment set.  The unlabeled accuracy of our baseline model on the test set is 89.96%, which is consid-erably lower than the best current results.  Koo et al (2008) report 90.84% for a first-order edge-factored model, and 92.02% for a second-order model (and as high as 93.16% with a second-order model enriched with cluster features de-rived from plain text).  Using two sets of clus-ters, one with 600 and one with 1,200 labels, ac-curacy improves by 1.32%, to reach 91.28% (a 13.15% reduction in error compared to our base-line).  While still below the level of the strongest results for this dataset, it is interesting to see that 
 Precision Recall F-score Baseline 88.7 88.2 88.4 Clusters 90.2 89.8 90.0 S & T 89.9 88.5 88.7 Miyao et al 85.0 84.3 84.6  Table 1: Results obtained on the test set us-ing our baseline model and our best cluster-based features.  The results in the bottom two rows are from Sagae and Tsujii (2008) and Miyao and Tsujii (2005).  Figure 4: Effect of cluster granularity on labeled the precision and recall of predicate-argument pairs in the development set.  The improvement in precision and recall between the baseline (zero cluster labels, where no cluster information is added) and 600 cluster labels is statistically significant (p < 0.0005).  
199
the improvement in accuracy over the baseline observed for surface dependency trees is similar to the improvement observed for predicate-argument dependency graphs. 5 Related work Many aspects of this research were inspired by the recent work of Koo et al (2008), who re-ported impressive results on improving depend-ency parsing accuracy using a second order edge-factored model and word clusters derived from plain text using the Brown et al (1992) al-gorithm.  Our clustering approach is significantly different, focusing on the use of parsed data to produce strictly syntactic clusters.  It is possible that using both types of clusters may be benefi-cial. McClosky et al (2006) used a large corpus of parsed text to obtain improved parsing results through self-training.  A key difference in our general framework is that it allows for a parser with one type of syntactic representation to im-prove the accuracy of a different parser with a different type of formalism.  In this regard, our work is related to that of Sagae et al (2007), who used a stacking-like framework to allow a sur-face dependency parser to improve an HPSG parser.  In that work, however, as in other work that combines different parsers through stacking (Martins et al, 2008; Nivre and McDonald, 2008) or voting (Henderson and Brill, 1999), multiple parsers need to process new text at run-time.  In our approach for leveraging diverse parsers, one of the parsers is used only to create a parsed corpus from which we extract clusters of words that have similar syntactic behaviors, and only one parser is needed at run-time. 6 Conclusion We have presented a novel approach for deriving word clusters based on syntactic similarity, and shown how these word clusters can be applied in a transition-based dependency parser. Our experiments focused on predicate-argument structures extracted from the HPSG Treebank, which demonstrates that the syntactic clusters are effective in leveraging cross-framework parser representations to improve parsing accuracy.  However, we expect that simi-lar accuracy improvements can be obtained in parsing using other frameworks and formalisms, and possibly in other natural language processing tasks. 
Acknowledgments The project or effort described here has been sponsored by the U.S. Army Research, Devel-opment, and Engineering Command (RDE-COM). Statements and opinions expressed do not necessarily reflect the position or the policy of the United States Government, and no official endorsement should be inferred. References  Srinivas Bangalore and Aravind K. Joshi. 1999. Su-pertagging: an approach to almost parsing. Compu-tational Linguistics 25, 2 (Jun. 1999), 237-265. Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Class-Based n-gram Models of Natural Lan-guage. Computational Linguistics, 18(4):467?479. Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the First Meet-ing of the North American Chapter of the Associa-tion for Computational Linguistics (NAACL), pages 132?139. Charniak, E., Blaheta, D., Ge, N., Hall, K., Hale, J., and Johnson, M. (2000) BLLIP 1987-89 WSJ Cor-pus Release 1. Philadelphia, PA: Linguistic Data Consortium. Michael Collins. 1999. Head-Driven Statistical Mod-els for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania. Michael Collins. 2002. Discriminative Training Me-thods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Pro-ceedings of EMNLP, pages 1?8. Yoav Freund and Robert E. Schapire. 1999. Large Margin Classification Using the Perceptron Algo-rithm. Machine Learning, 37(3):277?296. Daniel Gildea and Daniel Jurafsky. 2002. Automatic Labeling of Semantic Roles. Computational Lin-guistics 28(3): 245-288. Andrew Gordon and Reid Swanson. 2007. Generaliz-ing semantic role annotations across syntactically similar verbs. Proceedings of the 2007 meeting of the Association for Computational Linguistics (ACL-07), Prague, Czech Republic, June 23-30, 2007. Johan Hall, Jens Nilsson, Joakim Nivre, Gulsen Ery-igit, Beata Megyesi, Mattias Nilsson, and Markus Saers. 2007. Single malt or blended? A study in multilingual parser optimization. In Proceedings of EMNLP-CoNLL. James Henderson, Paola Merlo, G. Musillo, and Ivan Titov. 2008. A latent variable model of synchro-nous parsing for syntactic and semantic dependen-
200
cies. In Proceedings of the Shared Task of the Con-ference on Computational Natural Language Learning (CoNLL), pages 178-182. Manchester, UK. John Henderson and Eric Brill. 1999. Exploiting di-versity in natural language processing: combining parsers. In Proceedings of the Fourth Conference on Empirical Methods in Natural Language Proc-essing (EMNLP). Terry Koo, Xavier Carreras and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-08:HLT), pages 595-603.  Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th inter-national Conference on Computational Linguistics - Volume 2. Montreal, Quebec, Canada. Mitchell P. Marcus, Mary A. Marcinkiewicz, Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank, Computational Linguistics, 19(2), June 1993. Andr? F. T. Martins, Dipanjan Das, Noah A. Smith, and Eric P. Xing. 2008. Stacking Dependency Parsers.  In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing, Waikiki, HI. David McClosky, Eugene Charniak, and Mark John-son. 2006. Effective Self-Training for Parsing. In Proceedings of HLT-NAACL, pages 152?159. Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of de-pendency parsers. In Proceedings of ACL, pages 91?98. Scott Miller, Jethran Guinness and Alex Zamanian. 2004. Name Tagging withWord Clusters and Dis-criminative Training. In Proceedings of HLT-NAACL, pages 337?342. Miyao, Yusuke, Takashi Ninomiya, and Jun?ichi Tsu-jii. 2004. Corpus-oriented grammar development for acquiring a Head-driven Phrase Structure Grammar from the Penn Treebank. In Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP).  Miyao Yusuke and Jun'ichi Tsujii. 2005. Probabilistic disambiguation models for wide-coverage HPSG parsing. In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics. Joakim Nivre.2004. Incrementality in Deterministic Dependency Parsing. In Incremental Parsing: Bringing Engineering and Cognition Together (Workshop at ACL-2004). 
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-based dependency parsing. In Proceed-ings of CoNLL, pages 49?56. Joakim Nivre. and Jens Nilsson. 2005. Pseudo-Projective Dependency Parsing. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pp. 99-106. Joakim Nivre, Johan Hall, Sandra Kubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of EMNLP-CoNLL, pages 915-932. Nivre, J. and McDonald, R. (2008) Integrating Graph-Based and Transition-Based Dependency Parsers. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-08: HLT), 950-958. Martha Palmer, Dan Gildea and Paul Kingsbury. 2005. The Proposition Bank: A Corpus Annotated with Semantic Roles. Computational Linguistics, 31:1. Kenji Sagae and Alon Lavie. 2006. Parser combina-tion by reparsing. In Proceedings of NAACL: Short Papers, pages 129?132. Kenji Sagae, Yusuke Miyao Jun?ichi and Tsujii. 2007. HPSG Parsing with shallow dependency con-straints. In Proceedings of the 44th Meeting of the Association for Computational Linguistics. Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-reduce dependency DAG parsing. In Proceedings of the International Conference on Computational Lin-guistics (COLING 2008). Hiroyasu Yamada and Y. Matsumoto. 2003. Statisti-cal Dependency Analysis With Support Vector Machines. In Proceedings of the Eighth Interna-tional Workshop on Parsing Technologies (IWPT), pages 195?206.  
201
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 394?398,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
SemEval-2012 Task 7: Choice of Plausible Alternatives:  An Evaluation of Commonsense Causal Reasoning   Andrew S. Gordon Zornitsa Kozareva Melissa Roemmele Institute for Creative Technologies Information Sciences Institute Department of Linguistics University of Southern California University of Southern California Indiana University Los Angeles, CA Marina del Rey, CA Bloomington, IN gordon@ict.usc.edu kozareva@isi.edu msroemme@gmail.com       Abstract 
SemEval-2012 Task 7 presented a deceptively simple challenge: given an English sentence as a premise, select the sentence amongst two alternatives that more plausibly has a causal relation to the premise. In this paper, we de-scribe the development of this task and its mo-tivation. We describe the two systems that competed in this task as part of SemEval-2012, and compare their results to those achieved in previously published research. We discuss the characteristics that make this task so difficult, and offer our thoughts on how progress can be made in the future. 
1 Motivation Open-domain commonsense reasoning is one of the grand challenges of artificial intelligence, and has been the subject of research since the inception of the field. Until recently, this research history has been dominated by formal approaches (e.g. Lenat, 1995), where logical formalizations of com-monsense theories were hand-authored by expert logicians and evaluated using a handful of com-monsense challenge problems (Morgenstern, 2012). Progress via this approach has been slow, both because of the inherent difficulties in author-ing suitably broad-coverage formal theories of the commonsense world and the lack of evaluation metrics for comparing systems from different labs and research traditions. 
Radically different approaches to the com-monsense reasoning problem have recently been explored by natural language processing research-ers. Speer et al (2008) describe a novel reasoning approach that applies dimensionality reduction to the space of millions of English-language com-monsense facts in a crowd-sourced knowledge base (Liu & Singh, 2004). Gordon et al, (2010) describe a method for extracting millions of com-monsense facts from parse trees of English sen-tences. Jung et al (2010) describe a novel approach to the extraction of commonsense knowledge about activities by mining online how-to articles. We believe that these new NLP-based approaches hold enormous potential for overcom-ing the knowledge acquisition bottleneck that has limited progress in commonsense reasoning in pre-vious decades. Given the growth and enthusiasm for these new approaches, there is increasing need for a common metric for evaluation. A common evaluation suite would allow researchers to gauge the performance of new versions of their own systems, and to com-pare their approaches with those of other research groups. Evaluations for these new NLP-based ap-proaches should themselves be based in natural language, and must be suitably large to truly eval-uate the breadth of different reasoning approaches. Still, each evaluation should be focused on one dimension of the overall commonsense reasoning task, so as not to create a new challenge that no single research group could hope to succeed.  In SemEval-2012 Task 7, we presented a new evaluation for open-domain commonsense reason-
394
ing, focusing specifically on commonsense causal reasoning about everyday events. 2 Choice of Plausible Alternatives Consider the following English sentence, describ-ing a hypothetical state of the world: The man lost his balance on the ladder.  In addition to parsing this sentence, resolving ambiguities, and constructing a semantic interpre-tation, human readers also imagine the causal ante-cedents and consequents that would follow if the statement were true. With such a brief description, readers are left with many questions. How high up on the ladder was this man? What was he doing on the ladder in the first place? How much experience does he have using ladders? Was he intoxicated? The answers to these questions help readers formu-late hypotheses for the two central concerns when reasoning about events: What was the cause of this? and What happened as a result?  As computational linguists, we imagine that our automated natural language processing algorithms will also, eventually, need to engage in similar rea-soning processes in order to achieve human-like performance on text understanding tasks. Progress toward the goal of deep semantic interpretation of text has been slow. However, the last decade of natural language processing research has shown that enormous gains can be achieved when there is a clear evaluation metric. A shared task with an automated scoring mechanism allows researchers to compare different approaches, tune system pa-rameters to maximize performance, and assess progress toward broader research objectives. De-veloping an evaluation metric for causal reasoning poses a number of challenges. It is necessary to formulate a question with answers that can be au-tomatically graded, but can still serve as a proxy for the complex, generative imagination of readers. Roemmele et al (2011) offered a solution in the form of a simple binary-choice question. Presented with an English sentence describing a premise, systems must select between two alternatives (also sentences) the one that more plausibly has a causal relation to the premise, as in the following exam-ple: Premise: The man lost his balance on the lad-der. What happened as a result? Alternative 1: He fell off the ladder. 
Alternative 2: He climbed up the ladder. Both of these alternatives are conceivable, and neither is entailed by the premise. However, hu-man readers have no difficulty selecting the alter-native that is the more plausible of the two. This question asks about a causal consequent, and a complimentary formulation asks for the causal an-tecedent, as in the following example: Premise: The man fell unconscious. What was the cause of this? Alternative 1: The assailant struck the man on the head. Alternative 2: The assailant took the man's wal-let. Roemmele et al describe their efforts to author a collection of 1000 questions of these two types to create a new causal reasoning evaluation tool: the Choice of Plausible Alternatives (COPA). When presented to humans to select the correct alterna-tive, the inter-rater agreement was extremely high (Cohen's kappa = 0.965). Where disagreements between two raters were found (in 26 of 1000 items), questions were removed and replaced with new ones with perfect agreement. To develop an automated evaluation tool, the 1000 questions were randomly ordered and sorted into two equally sized sets of 500 questions to serve as development and test sets. The order of the correct alternative was also randomized, such that the expected accuracy of a random baseline would be 50%. Gold-standard answers for each split are used to automatically evaluate a given system's performance.  The distribution of the COPA evaluation in-cludes an automated test of statistical significance of differences seen between two competing sys-tems. This software tool implements a compute-intensive randomized test of statistical significance using stratified shuffling, as described by Noreen (1989). By randomly sorting answers between two systems over thousands of trials, this test computes the likelihood that differences as great as observed differences could be obtained by random chance. The COPA evaluation is most similar in style to the Recognizing Textual Entailment challenge (Degan et al, 2006), but differs in its focus on causal implication rather than entailment. Instead of asking whether the interpretation of a sentence necessitates the truth of another, COPA concerns 
395
the defeasible inferences that can be drawn from the interpretation of a sentence. In this respect, COPA overlaps in its aims with the task of recog-nizing causal relations in text through automated discourse processing (e.g. Marcu, 1999). Some progress in automated discourse processing has been made using supervised machine learning methods, where system learn the lexical-syntactic patterns that are most correlated with causal rela-tions from a large annotated corpus (Sagae, 2009). Lacking a dedicated training corpus, the COPA evaluation encourages competitors to capture commonsense causal knowledge from any availa-ble corpus or existing knowledge repository. 3 SemEval-2012 Systems and Results The COPA evaluation was accepted as Task 7 of the 6th International Workshop on Semantic Eval-uation (SemEval-2012). In several respects, the COPA evaluation was different than the typical shared task offered as part of this series of work-shops. First, the task materials were available and distributed long before the evaluation period be-gan, and there were published results of previous systems using this evaluation.1 Second, the task included no training data, only sets of development and test questions (500 each). Participants were encouraged to use any available text corpus or knowledge repositories in the construction of their systems. Success on the task would not be possible simply through the selection of machine learning algorithms and feature encodings. Instead, some creativity and ingenuity was needed to find a suita-ble source of commonsense causal information, and determine an automated mechanism for apply-ing this information to COPA questions. Only one team successfully completed the task and submitted results during the official two-week SemEval-2012 evaluation period. This team was Travis Goodwin, Bryan Rink, Kirk Roberts, and Sanda M. Harabagiu from the University of Texas at Dallas, Human Language Technology Research Institute. This team submitted results from two different systems (Goodwin et al, 2012), which they described to us as follows: UTDHLT Bigram PMI: The team's first ap-proach selects the alternative with the maximum Pointwise Mutual Information (PMI) statistic                                                             1 http://www.ict.usc.edu/~gordon/copa.html 
(Church & Hanks, 1990) over all pairs of bigrams (at the token level) between the candidate alterna-tive and the premise. PMI statistics were collected using 8.4 million documents from the LDC Giga-word corpus (Graff & Cieri, 2003). A window of 100 terms was used for finding pairs of co-occurring bigrams, and a window/slop size of 2 for the bigram itself. UTDHLT SVM Combined: The team's second approach augments the first by combining it with several other features and casting the task as a classification problem. To this end, they consider the PMI between events participating in a temporal link on a Time-ML annotated Gigaword corpus. That is, events that occur together frequently will have a higher PMI. They also consider the differ-ence between the number of positive and negative polarity words between an alternative and premise using information from the Harvard Inquisitor. In addition, they used the count of matching cause-effect pairs extracted using patterns on dependency structures from the Gigaword corpus. Combining all of these sources of information, they trained a support vector machine (SVM) learning algorithm to classify the alternative that is most causally re-lated to the premise. These systems were assessed based on their ac-curacy on the 500 questions in the test split of the COPA evaluation, presented in Table 1. Both sys-tems significantly outperformed the random base-line (50% accuracy), but the gains seen in the second approach were not significantly different than those of the first.   System Accuracy UTDHLT Bigram PMI 61.8% UTDHLT SVM Combined 63.4%  Table 1. SemEval-2012 Task 7 system accuracy on 500 questions in the COPA test split 4 Comparison to Previous Results In order to better evaluate the success of these two systems, we compared these results with the pub-lished results of other systems that have used the COPA evaluation. Three other systems were con-sidered. PMI Gutenberg (W=5): Described in Roem-mele et al (2011), this approach calculated the PMI between words (unigrams) in the premise and 
396
each alternative, and selected the alternative with the stronger correlation. The PMI statistic was cal-culated using every English-language document in Project Gutenberg (16GB of text), using a window of 5 words. PMI Story 1M (W=25): Described in Gordon et al (2011), this approach was identical to that of Roemmele et al (2011) except that the PMI statis-tic was calculated using a corpus of nearly one mil-lion personal stories extracted from Internet weblogs (Gordon & Swanson, 2009), with 1.9 GB of text. Using this corpus instead of Project Guten-berg, the best results were obtained by using a window of 25 words for the PMI statistic.  PMI Story 10M (W=25): Also described in Gordon et al (2011), this approach explores the gains that can be achieved by calculating the PMI statistic using a much larger corpus of weblog sto-ries. The story extraction technology used by Gor-don and Swanson (2009) was applied to 621 million English-language weblog entries posted to the Internet in 2010 to create a corpus of 10.4 mil-lion personal stories (37GB of text). Again, the best results were obtained by using a window of 25 words for the PMI statistic.  Table 2 compares the results of these three pre-vious systems with the two SemEval-2012 sys-tems. Although the last two of these three previous systems achieved higher scores than both of the SemEval-2012 submissions, the differences are not statistically significant.  System Accuracy PMI Gutenberg (W=5) 58.8% UTDHLT Bigram PMI 61.8% UTDHLT SVM Combined 63.4% PMI Story 1M (W=25) 65.2% PMI Story 10M (W=25) 65.4%  Table 2. Comparison of SemEval-2012 Task 7 sys-tems (in bold) with previously published results on the 500 questions in the COPA test split 5 Discussion The two systems from the University of Texas at Dallas make an important contribution to progress on open-domain commonsense reasoning. Some lessons are evident from the short descriptions of their systems that they provided to us. 
As in each of the previously successful systems, this team focused their efforts on calculating corre-lational statistics between words in COPA ques-tions using very large text corpora. In this case, the Gigaword corpus is used, and the calculation is based on bigrams rather than unigrams. We believe that the content of the news articles that comprise the Gigaword corpus is a step further away from the concerns of COPA questions than both the Pro-ject Gutenberg corpus and the weblog story corpo-ra used in previous efforts. Indeed, the gains achieved by Gordon et al (2011) appear to be en-tirely due to the relationship between COPA ques-tions and the personal stories that people write about in their public weblogs. However, the use of a large news corpus affords the use of more sophis-ticated analysis techniques that have been devel-oped for this genre. Here, the Gigaword corpus is annotated using Time-ML relationships, which in turn are used to modify the PMI strength between words. The use of bigrams is an additional enhancement explored by this team, as is the casting of COPA questions as a classification task using a diverse set of lexical and discourse features. Such an approach can facilitate the combining of diverse systems in the future, where correlational statistics are gath-ered from a diverse set of text corpora, each suited for specific domains of COPA questions or yield-ing complimentary feature sets. Still, the modest COPA performance seen from all existing systems is somewhat discouraging. With the best systems performing in the 60-65% range, we remain much closer to random perfor-mance (50%) than human performance (99%). These results cast some doubt that the information necessary to answer COPA questions can be readi-ly obtained from large text corpora. Certainly the use of simple correlational statistics between near-by words is not enough. In the best case, we might wish for perfect identification of causal relation-ships between events in an extremely large text corpus of narratives similar in content to COPA questions. Semantic similarity between these events and COPA sentences could be computed to gather evidence to select the best alternative. Even if it were possible to achieve this ideal, it is diffi-cult to imagine that such an approach could mirror human performance on this task. To move closer to human performance, systems may need to stretch beyond corpus statistics into 
397
the realm of automated reasoning. Just as human readers do when hearing that ?the man lost his bal-ance on the ladder,? successful systems may need to treat COPA premises as novel world states, and imagine a broad range of interconnected causal antecedents and consequents. Useful knowledge bases will be those that have adequate coverage over commonsense concerns, but also adequate competency to support generative inference of the sort more commonly seen in deductive and abduc-tive automated reasoning frameworks. This knowledge may or may not be represented as text, but any successful system must have the capacity to apply this knowledge to the understanding of COPA's textual premises and alternatives. We con-sider the successful application of commonsense inference to text understanding to be one of the grand challenges of natural language processing, and hope that the COPA evaluation continues to be a useful tool for benchmarking progress toward this goal. Acknowledgments The projects or efforts depicted were or are spon-sored by the U. S. Army. The content or infor-mation presented does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred. References  Church, K. and Hanks, P. (1990) Word Association Norms, Mutual Information, and Lexicography. Computational Linguistics, 16(1):22-29.  Dagan, I., Glickman, O., and Magnini, B. (2006) The PASCAL Recognising Textual Entailment Chal-lenge. In Qui?onero-Candela, J.; Dagan, I.; Magnini, B.; d'Alch?-Buc, F. (Eds.), Machine Learning Chal-lenges. Lecture Notes in Computer Science, Vol. 3944, pp. 177-190, Springer, 2006. Goodwin, T., Rink, B., Roberts, K., and Harabagiu, S. (2012) UTDHLT: COPACETIC System for Choos-ing Plausible Alternatives. Proceedings of the 6th In-ternational Workshop on Semantic Evaluation (SemEval 2012), June 7-8, 2012, Montreal, Canada. Gordon, A., Bejan, C., and Sagae, K. (2011) Com-monsense Causal Reasoning Using Millions of Per-sonal Stories. Twenty-Fifth Conference on Artificial Intelligence (AAAI-11), August 7?11, 2011, San Francisco, CA.  
Gordon, A. and Swanson, R. (2009) Identifying Person-al Stories in Millions of Weblog Entries. Internation-al Conference on Weblogs and Social Media, Data Challenge Workshop, San Jose, CA.  Gordon, J., Van Durme, B., and K. Schubert, L. (2010) Learning from the Web: Extracting General World Knowledge from Noisy Text. Proceedings of the AAAI 2010 Workshop on Collaboratively-built Knowledge Sources and Artificial Intelligence (WikiAI 2010). Graff, D. and Cieri, C. (2003) English Gigaword. Lin-guistic Data Consortium, Philadelphia. Jung, Y., Ryu, J., Kim., K. and Myaeng, S.(2010). Au-tomatic Construction of a Large-Scale Situation On-tology by Mining How-to Instructions from the Web. Journal of Web Semantics 8(2-3):110-124. Lenat, D. (1995) CYC: A Large-Scale Investment in Knowledge Infrastructure, Communications of the ACM 38:33-38. Liu, H. and Singh, P. (2004) ConceptNet: A Practical Commonsense Reasoning Toolkit. BT Technology Journal 22(4):211-226. Marcu, D. (1999). A decision-based approach to rhetor-ical parsing. The 37th Annual Meeting of the Associ-ation for Computational Linguistics (ACL'99), pages 365-372, Maryland, June 1999. Morgenstern, L. (2012) Common Sense Problem Page. Retrieved April 2012 at http://www-formal.stanford.edu/leora/commonsense/ Noreen, E. (1989) Computer-Intensive Methods for Testing Hypotheses: An Introduction. New York: John Wiley & Sons. Roemmele, M., Bejan, C., and Gordon, A. (2011) Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning. AAAI Spring Symposium on Logical Formalizations of Com-monsense Reasoning, Stanford University, March 21-23, 2011. Sagae, K. (2009) Analysis of discourse structure with syntactic dependencies and data-driven shift-reduce parsing. In Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 81--84. 2009.  Speer, R., Havasi, C. and Lieberman, H. (2008) Analo-gySpace: Reducing the Dimensionality of Common Sense Knowledge. Proceedings of AAAI 2008.  
398
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 43?51,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Open-domain Commonsense Reasoning Using Discourse Relations from a
Corpus of Weblog Stories
Matt Gerber
Department of Computer Science
Michigan State University
gerberm2@msu.edu
Andrew S. Gordon and Kenji Sagae
Institute for Creative Technologies
University of Southern California
{gordon,sagae}@ict.usc.edu
Abstract
We present a method of extracting open-
domain commonsense knowledge by apply-
ing discourse parsing to a large corpus of per-
sonal stories written by Internet authors. We
demonstrate the use of a linear-time, joint syn-
tax/discourse dependency parser for this pur-
pose, and we show how the extracted dis-
course relations can be used to generate open-
domain textual inferences. Our evaluations
of the discourse parser and inference models
show some success, but also identify a num-
ber of interesting directions for future work.
1 Introduction
The acquisition of open-domain knowledge in sup-
port of commonsense reasoning has long been a
bottleneck within artificial intelligence. Such rea-
soning supports fundamental tasks such as textual
entailment (Giampiccolo et al, 2008), automated
question answering (Clark et al, 2008), and narra-
tive comprehension (Graesser et al, 1994). These
tasks, when conducted in open domains, require vast
amounts of commonsense knowledge pertaining to
states, events, and their causal and temporal relation-
ships. Manually created resources such as FrameNet
(Baker et al, 1998), WordNet (Fellbaum, 1998), and
Cyc (Lenat, 1995) encode many aspects of com-
monsense knowledge; however, coverage of causal
and temporal relationships remains low for many do-
mains.
Gordon and Swanson (2008) argued that the
commonsense tasks of prediction, explanation, and
imagination (collectively called envisionment) can
be supported by knowledge mined from a large cor-
pus of personal stories written by Internet weblog
authors.1 Gordon and Swanson (2008) identified
three primary obstacles to such an approach. First,
stories must be distinguished from other weblog
content (e.g., lists, recipes, and reviews). Second,
stories must be analyzed in order to extract the im-
plicit commonsense knowledge that they contain.
Third, inference mechanisms must be developed that
use the extracted knowledge to perform the core en-
visionment tasks listed above.
In the current paper, we present an approach to
open-domain commonsense inference that addresses
each of the three obstacles identified by Gordon and
Swanson (2008). We built on the work of Gordon
and Swanson (2009), who describe a classification-
based approach to the task of story identification.
The authors? system produced a corpus of approx-
imately one million personal stories, which we used
as a starting point. We applied efficient discourse
parsing techniques to this corpus as a means of ex-
tracting causal and temporal relationships. Further-
more, we developed methods that use the extracted
knowledge to generate textual inferences for de-
scriptions of states and events. This work resulted
in an end-to-end prototype system capable of gen-
erating open-domain, commonsense inferences us-
ing a repository of knowledge extracted from un-
structured weblog text. We focused on identifying
1We follow Gordon and Swanson (2009) in defining a story
to be a ?textual discourse that describes a specific series of
causally related events in the past, spanning a period of time
of minutes, hours, or days, where the author or a close associate
is among the participants.?
43
strengths and weaknesses of the system in an effort
to guide future work.
We structure our presentation as follows: in Sec-
tion 2, we present previous research that has inves-
tigated the use of large web corpora for natural lan-
guage processing (NLP) tasks. In Section 3, we de-
scribe an efficient method of automatically parsing
weblog stories for discourse structure. In Section 4,
we present a set of inference mechanisms that use
the extracted discourse relations to generate open-
domain textual inferences. We conclude, in Section
5, with insights into story-based envisionment that
we hope will guide future work in this area.
2 Related work
Researchers have made many attempts to use the
massive amount of linguistic content created by
users of the World Wide Web. Progress and chal-
lenges in this area have spawned multiple workshops
(e.g., those described by Gurevych and Zesch (2009)
and Evert et al (2008)) that specifically target the
use of content that is collaboratively created by In-
ternet users. Of particular relevance to the present
work is the weblog corpus developed by Burton et
al. (2009), which was used for the data challenge
portion of the International Conference on Weblogs
and Social Media (ICWSM). The ICWSM weblog
corpus (referred to here as Spinn3r) is freely avail-
able and comprises tens of millions of weblog en-
tries posted between August 1st, 2008 and October
1st, 2008.
Gordon et al (2009) describe an approach to
knowledge extraction over the Spinn3r corpus using
techniques described by Schubert and Tong (2003).
In this approach, logical propositions (known as fac-
toids) are constructed via approximate interpreta-
tion of syntactic analyses. As an example, the sys-
tem identified a factoid glossed as ?doors to a room
may be opened?. Gordon et al (2009) found that
the extracted factoids cover roughly half of the fac-
toids present in the corresponding Wikipedia2 arti-
cles. We used a subset of the Spinn3r corpus in
our work, but focused on discourse analyses of en-
tire texts instead of syntactic analyses of single sen-
tences. Our goal was to extract general causal and
temporal propositions instead of the fine-grained
2http://en.wikipedia.org
properties expressed by many factoids extracted by
Gordon et al (2009).
Clark and Harrison (2009) pursued large-scale
extraction of knowledge from text using a syntax-
based approach that was also inspired by the work
of Schubert and Tong (2003). The authors showed
how the extracted knowledge tuples can be used
to improve syntactic parsing and textual entailment
recognition. Bar-Haim et al (2009) present an ef-
ficient method of performing inference with such
knowledge.
Our work is also related to the work of Persing
and Ng (2009), in which the authors developed a
semi-supervised method of identifying the causes of
events described in aviation safety reports. Simi-
larly, our system extracts causal (as well as tem-
poral) knowledge; however, it does this in an open
domain and does not place limitations on the types
of causes to be identified. This greatly increases
the complexity of the inference task, and our results
exhibit a corresponding degradation; however, our
evaluations provide important insights into the task.
3 Discourse parsing a corpus of stories
Gordon and Swanson (2009) developed a super-
vised classification-based approach for identifying
personal stories within the Spinn3r corpus. Their
method achieved 75% precision on the binary task
of predicting story versus non-story on a held-out
subset of the Spinn3r corpus. The extracted ?story
corpus? comprises 960,098 personal stories written
by weblog users. Due to its large size and broad
domain coverage, the story corpus offers unique op-
portunities to NLP researchers. For example, Swan-
son and Gordon (2008) showed how the corpus can
be used to support open-domain collaborative story
writing.3
As described by Gordon and Swanson (2008),
story identification is just the first step towards com-
monsense reasoning using personal stories. We ad-
dressed the second step - knowledge extraction -
by parsing the corpus using a Rhetorical Structure
Theory (Carlson and Marcu, 2001) parser based on
the one described by Sagae (2009). The parser
performs joint syntactic and discourse dependency
3The system (called SayAnything) is available at
http://sayanything.ict.usc.edu
44
parsing using a stack-based, shift-reduce algorithm
with runtime that is linear in the input length. This
lightweight approach is very efficient; however, it
may not be quite as accurate as more complex, chart-
based approaches (e.g., the approach of Charniak
and Johnson (2005) for syntactic parsing).
We trained the discourse parser over the causal
and temporal relations contained in the RST corpus.
Examples of these relations are shown below:
(1) [cause Packages often get buried in the load]
[result and are delivered late.]
(2) [before Three months after she arrived in L.A.]
[after she spent $120 she didn?t have.]
The RST corpus defines many fine-grained rela-
tions that capture causal and temporal properties.
For example, the corpus differentiates between re-
sult and reason for causation and temporal-after and
temporal-before for temporal order. In order to in-
crease the amount of available training data, we col-
lapsed all causal and temporal relations into two
general relations causes and precedes. This step re-
quired normalization of asymmetric relations such
as temporal-before and temporal-after.
To evaluate the discourse parser described above,
we manually annotated 100 randomly selected we-
blog stories from the story corpus produced by Gor-
don and Swanson (2009). For increased efficiency,
we limited our annotation to the generalized causes
and precedes relations described above. We at-
tempted to keep our definitions of these relations
in line with those used by RST. Following previous
discourse annotation efforts, we annotated relations
over clause-level discourse units, permitting rela-
tions between adjacent sentences. In total, we an-
notated 770 instances of causes and 1,009 instances
of precedes.
We experimented with two versions of the RST
parser, one trained on the fine-grained RST rela-
tions and the other trained on the collapsed relations.
At testing time, we automatically mapped the fine-
grained relations to their corresponding causes or
precedes relation. We computed the following ac-
curacy statistics:
Discourse segmentation accuracy For each pre-
dicted discourse unit, we located the reference
discourse unit with the highest overlap. Accu-
racy for the predicted discourse unit is equal to
the percentage word overlap between the refer-
ence and predicted discourse units.
Argument identification accuracy For each dis-
course unit of a predicted discourse relation,
we located the reference discourse unit with the
highest overlap. Accuracy is equal to the per-
centage of times that a reference discourse rela-
tion (of any type) holds between the reference
discourse units that overlap most with the pre-
dicted discourse units.
Argument classification accuracy For the subset
of instances in which a reference discourse re-
lation holds between the units that overlap most
with the predicted discourse units, accuracy is
equal to the percentage of times that the pre-
dicted discourse relation matches the reference
discourse relation.
Complete accuracy For each predicted discourse
relation, accuracy is equal to the percentage
word overlap with a reference discourse rela-
tion of the same type.
Table 1 shows the accuracy results for the fine-
grained and collapsed versions of the RST discourse
parser. As shown in Table 1, the collapsed version
of the discourse parser exhibits higher overall ac-
curacy. Both parsers predicted the causes relation
much more often than the precedes relation, so the
overall scores are biased toward the scores for the
causes relation. For comparison, Sagae (2009) eval-
uated a similar RST parser over the test section of
the RST corpus, obtaining precision of 42.9% and
recall of 46.2% (F1 = 44.5%).
In addition to the automatic evaluation described
above, we also manually assessed the output of the
discourse parsers. One of the authors judged the
correctness of each extracted discourse relation, and
we found that the fine-grained and collapsed ver-
sions of the parser performed equally well with a
precision near 33%; however, throughout our exper-
iments, we observed more desirable discourse seg-
mentation when working with the collapsed version
of the discourse parser. This fact, combined with the
results of the automatic evaluation presented above,
45
Fine-grained RST parser Collapsed RST parser
Accuracy metric causes precedes overall causes precedes overall
Segmentation 36.08 44.20 36.67 44.36 30.13 43.10
Argument identification 25.00 33.33 25.86 26.15 23.08 25.87
Argument classification 66.15 50.00 64.00 79.41 83.33 79.23
Complete 22.20 28.88 22.68 31.26 21.21 30.37
Table 1: RST parser evaluation. All values are percentages.
led us to use the collapsed version of the parser in
all subsequent experiments.
Having developed and evaluated the discourse
parser, we conducted a full discourse parse of the
story corpus, which comprises more than 25 million
sentences split into nearly 1 million weblog entries.
The discourse parser extracted 2.2 million instances
of the causes relation and 220,000 instances of the
precedes relation. As a final step, we indexed the
extracted discourse relations with the Lucene infor-
mation retrieval engine.4 Each discourse unit (two
per discourse relation) is treated as a single docu-
ment, allowing us to query the extracted relations
using information retrieval techniques implemented
in the Lucene toolkit.
4 Generating textual inferences
As mentioned previously, Gordon and Swan-
son (2008) cite three obstacles to performing com-
monsense reasoning using weblog stories. Gordon
and Swanson (2009) addressed the first (story col-
lection). We addressed the second (story analysis)
by developing a discourse parser capable of extract-
ing causal and temporal relations from weblog text
(Section 3). In this section, we present a prelimi-
nary solution to the third problem - reasoning with
the extracted knowledge.
4.1 Inference method
In general, we require an inference method that takes
as input the following things:
1. A description of the state or event of interest.
This is a free-text description of any length.
2. The type of inference to perform, either causal
or temporal.
4Available at http://lucene.apache.org
3. The inference direction, either forward or back-
ward. Forward causal inference produces the
effects of the given state or event. Backward
causal inference produces causes of the given
state or event. Similarly, forward and back-
ward temporal inferences produce subsequent
and preceding states and events, respectively.
As a simple baseline approach, we implemented the
following procedure. First, given a textual input de-
scription d, we query the extracted discourse units
using Lucene?s modified version of the vector space
model over TF-IDF term weights. This produces a
ranked list Rd of discourse units matching the input
description d. We then filterRd, removing discourse
units that are not linked to other discourse units by
the given relation and in the given direction. Each el-
ement of the filtered Rd is thus linked to a discourse
unit that could potentially satisfy the inference re-
quest.
To demonstrate, we perform forward causal infer-
ence using the following input description d:
(3) John traveled the world.
Below, we list the three top-ranked discourse units
that matched d (left-hand side) and their associated
consequents (right-hand side):
1. traveling the world? to murder
2. traveling from around the world to be there ?
even though this crowd was international
3. traveled across the world? to experience it
In a na??ve way, one might simply choose the top-
ranked clause in Rd and select its associated clause
as the answer to the inference request; however, in
the example above, this would incorrectly generate
?to murder? as the effect of John?s traveling (this is
46
more appropriately viewed as the purpose of trav-
eling). The other effect clauses also appear to be
incorrect. This should not come as much of a sur-
prise because the ranking was generated soley from
the match score between the input description and
the causes in Rd, which are quite relevant.
One potential problem with the na??ve selection
method is that it ignores information contained in
the ranked list R?d of clauses that are associated with
the clauses in Rd. In our experiments, we often
observed redundancies in R?d that captured general
properties of the desired inference. Intuitively, con-
tent that is shared across elements ofR?d could repre-
sent the core meaning of the desired inference result.
In what follows, we describe various re-rankings
of R?d using this shared content. For each model
described, the final inference prediction is the top-
ranked element of R?d.
Centroid similarity To approximate the shared
content of discourse units in R?d, we treat each
discourse unit as a vector of TF scores. We then
compute the average vector and re-rank all dis-
course units in R?d based on their cosine simi-
larity with the average vector. This favors infer-
ence results that ?agree? with many alternative
hypotheses.
Description score scaling In this approach, we in-
corporate the score from Rd into the centroid
similarity score, multiplying the two and giving
equal weight to each. This captures the intu-
ition that the top-ranked element of R?d should
represent the general content of the list but
should also be linked to an element of Rd that
bears high similarity to the given state or event
description d.
Log-length scaling When working with the cen-
troid similarity score, we often observed top-
ranked elements of R?d that were only a few
words in length. This was typically the case
when components from sparse TF vectors in
R?d matched well with components from the
centroid vector. Ideally, we would like more
lengthy (but not too long) descriptions. To
achieve this, we multiplied the centroid simi-
larity score by the logarithm of the word length
of the discourse unit in R?d.
Description score/log-length scaling In this ap-
proach, we combine the description score scal-
ing and log-length scaling, multiplying the cen-
troid similarity by both and giving equal weight
to all three factors.
4.2 Evaluating the generated textual inferences
To evaluate the inference re-ranking models de-
scribed above, we automatically generated for-
ward/backward causal and temporal inferences for
five documents (265 sentences) drawn randomly
from the story corpus. For simplicity, we gener-
ated an inference for each sentence in each docu-
ment. Each inference re-ranking model is able to
generate four textual inferences (forward/backward
causal/temporal) for each sentence. In our experi-
ments, we only kept the highest-scoring of the four
inferences generated by a model. One of the authors
then manually evaluated the final predictions for cor-
rectness. This was a subjective process, but it was
guided by the following requirements:
1. The generated inference must increase the lo-
cal coherence of the document. As described
by Graesser et al (1994), readers are typically
required to make inferences about the text that
lead to a coherent understanding thereof. We
required the generated inferences to aid in this
task.
2. The generated inferences must be globally
valid. To demonstrate global validity, consider
the following actual output:
(4) I didn?t even need a jacket (until I got
there).
In Example 4, the system-generated forward
temporal inference is shown in parentheses.
The inference makes sense given its local con-
text; however, it is clear from the surround-
ing discourse (not shown) that a jacket was not
needed at any point in time (it happened to be
a warm day). As a result, this prediction was
tagged as incorrect.
Table 2 presents the results of the evaluation. As
shown in the table, the top-performing models are
those that combine centroid similarity with one or
both of the other re-ranking heuristics.
47
Re-ranking model Inference accuracy (%)
None 10.19
Centroid similarity 12.83
Description score scaling 17.36
Log-length scaling 12.83
Description score/log-length scaling 16.60
Table 2: Inference generation evaluation results.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.0
5 0.1 0.1
5 0.2 0.2
5 0.3 0.3
5 0.4 0.4
5 0.5 0.5
5 0.6 0.6
5 0.7 0.7
5 0.8 0.8
5 0.9 0.9
5 1
Confidence-ordered percentage of all 
inferences
In
fe
re
nc
e 
ac
cu
ra
cy None
Centroid similarity
Description score scaling
Log-length scaling
Combined scaling
Figure 1: Inference rate versus accuracy. Values along the x-axis indicate that the top-scoring x% of all inferences
were evaluated. Values along the y-axis indicate the prediction accuracy.
The analysis above demonstrates the relative per-
formance of the models when making inferences for
all sentences; however it is probably the case that
many generated inferences should be rejected due to
their low score. Because the output scores of a single
model can be meaningfully compared across predic-
tions, it is possible to impose a threshold on the in-
ference generation process such that any prediction
scoring at or below the threshold is withheld. We
varied the prediction threshold from zero to a value
sufficiently large that it excluded all predictions for
a model. Doing so demonstrates the trade-off be-
tween making a large number of textual inferences
and making accurate textual inferences. Figure 1
shows the effects of this variable on the re-ranking
models. As shown in Figure 1, the highest infer-
ence accuracy is reached by the re-ranker that com-
bines description score and log-length scaling with
the centroid similarity measure. This accuracy is at-
tained by keeping the top 25% most confident infer-
ences.
5 Conclusions
We have presented an approach to commonsense
reasoning that relies on (1) the availability of a large
corpus of personal weblog stories and (2) the abil-
ity to analyze and perform inference with these sto-
ries. Our current results, although preliminary, sug-
gest novel and important areas of future exploration.
We group our observations according to the last two
problems identified by Gordon and Swanson (2008):
story analysis and envisioning with the analysis re-
sults.
5.1 Story analysis
As in other NLP tasks, we observed significant per-
formance degradation when moving from the train-
ing genre (newswire) to the testing genre (Internet
48
weblog stories). Because our discourse parser relies
heavily on lexical and syntactic features for classi-
fication, and because the distribution of the feature
values varies widely between the two genres, the
performance degradation is to be expected. Recent
techniques in parser adaptation for the Brown corpus
(McClosky et al, 2006) might be usefully applied to
the weblog genre as well.
Our supervised classification-based approach to
discourse parsing could also be improved with ad-
ditional training data. Causal and temporal relations
are instantiated a combined 2,840 times in the RST
corpus, with a large majority of these being causal.
In contrast, the Penn Discourse TreeBank (Prasad et
al., 2008) contains 7,448 training instances of causal
relations and 2,763 training instances of temporal
relations. This represents a significant increase in
the amount of training data over the RST corpus. It
would be informative to compare our current results
with those obtained using a discourse parser trained
on the Penn Discourse TreeBank.
One might also extract causal and temporal rela-
tions using traditional semantic role analysis based
on FrameNet (Baker et al, 1998) or PropBank
(Kingsbury and Palmer, 2003). The former defines a
number of frames related to causation and temporal
order, and roles within the latter could be mapped to
standard thematic roles (e.g., cause) via SemLink.5
5.2 Envisioning with the analysis results
We believe commonsense reasoning based on we-
blog stories can also be improved through more so-
phisticated uses of the extracted discourse relations.
As a first step, it would be beneficial to explore alter-
nate input descriptions. As presented in Section 4.2,
we make textual inferences at the sentence level for
simplicity; however, it might be more reasonable to
make inferences at the clause level, since clauses are
the basis for RST and Penn Discourse TreeBank an-
notation. This could result in the generation of sig-
nificantly more inferences due to multi-clause sen-
tences; thus, more intelligent inference filtering will
be required.
Our models use prediction scores for the tasks
of rejecting inferences and selecting between mul-
tiple candidate inferences (i.e., forward/backward
5Available at http://verbs.colorado.edu/semlink
causal/temporal). Instead of relying on prediction
scores for these tasks, it might be advantageous to
first identify whether or not envisionment should be
performed for a clause, and, if it should, what type
and direction of envisionment would be best. For
example, consider the following sentence:
(5) [clause1 John went to the store] [clause2
because he was hungry].
It would be better - from a local coherence perspec-
tive - to infer the cause of the second clause instead
of the cause of the first. This is due to the fact that a
cause for the first clause is explicitly stated, whereas
a cause for the second clause is not. Inferences made
about the first clause (e.g., that John went to the store
because his dog was hungry), are likely to be unin-
formative or in conflict with explicitly stated infor-
mation.
Example 5 raises the important issue of context,
which we believe needs to be investigated further.
Here, context refers to the discourse that surrounds
the clause or sentence for which the system is at-
tempting to generate a textual inference. The con-
text places a number of constraints on allowable in-
ferences. For example, in addition to content-based
constraints demonstrated in Example 5, the context
limits pronoun usage, entity references, and tense.
Violations of these constraints will reduce local co-
herence.
Finally, the story corpus, with its vast size, is
likely to contain a significant amount of redundancy
for common events and states. Our centroid-based
re-ranking heuristics are inspired by this redun-
dancy, and we expect that aggregation techniques
such as clustering might be of some use when ap-
plied to the corpus as a whole. Having identified
coherent clusters of causes, it might be easier to find
a consequence for a previously unseen cause.
In summary, we have presented preliminary re-
search into the task of using a large, collaboratively
constructed corpus as a commonsense knowledge
repository. Rather than relying on hand-coded on-
tologies and event schemas, our approach relies on
the implicit knowledge contained in written natu-
ral language. We have demonstrated the feasibility
of obtaining the discourse structure of such a cor-
pus via linear-time parsing models. Furthermore,
49
we have introduced inference procedures that are ca-
pable of generating open-domain textual inferences
from the extracted knowledge. Our evaluation re-
sults suggest many opportunities for future work in
this area.
Acknowledgments
The authors would like to thank the anonymous
reviewers for their helpful comments and sugges-
tions. The project or effort described here has
been sponsored by the U.S. Army Research, Devel-
opment, and Engineering Command (RDECOM).
Statements and opinions expressed do not necessar-
ily reflect the position or the policy of the United
States Government, and no official endorsement
should be inferred.
References
Collin Baker, Charles Fillmore, and John Lowe. 1998.
The Berkeley FrameNet project. In Christian Boitet
and PeteWhitelock, editors, Proceedings of the Thirty-
Sixth Annual Meeting of the Association for Computa-
tional Linguistics and Seventeenth International Con-
ference on Computational Linguistics, pages 86?90,
San Francisco, California. MorganKaufmann Publish-
ers.
Roy Bar-Haim, Jonathan Berant, and Ido Dagan. 2009.
A compact forest for scalable inference over entail-
ment and paraphrase rules. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1056?1065, Singapore, Au-
gust. Association for Computational Linguistics.
K. Burton, A. Java, and I. Soboroff. 2009. The icwsm
2009 spinn3r dataset. In Proceedings of the Third An-
nual Conference on Weblogs and Social Media.
Lynn Carlson and Daniel Marcu. 2001. Discourse tag-
ging manual. Technical Report ISI-TR-545, ISI, July.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics.
Peter Clark and Phil Harrison. 2009. Large-scale extrac-
tion and use of knowledge from text. In K-CAP ?09:
Proceedings of the fifth international conference on
Knowledge capture, pages 153?160, New York, NY,
USA. ACM.
Peter Clark, Christiane Fellbaum, Jerry R. Hobbs, Phil
Harrison, William R. Murray, and John Thompson.
2008. Augmenting WordNet for Deep Understanding
of Text. In Johan Bos and Rodolfo Delmonte, editors,
Semantics in Text Processing. STEP 2008 Conference
Proceedings, volume 1 of Research in Computational
Semantics, pages 45?57. College Publications.
Stefan Evert, Adam Kilgarriff, and Serge Sharoff, edi-
tors. 2008. 4th Web as Corpus Workshop Can we beat
Google?
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database (Language, Speech, and Communi-
cation). The MIT Press, May.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, and Bill Dolan. 2008. The
fourth fascal recognizing textual entailment challenge.
In Proceedings of the First Text Analysis Conference.
Andrew Gordon and Reid Swanson. 2008. Envision-
ing with weblogs. In International Conference on New
Media Technology.
Andrew Gordon and Reid Swanson. 2009. Identifying
personal stories in millions of weblog entries. In Third
International Conference on Weblogs and Social Me-
dia.
Jonathan Gordon, Benjamin Van Durme, and Lenhart
Schubert. 2009. Weblogs as a source for extracting
general world knowledge. In K-CAP ?09: Proceed-
ings of the fifth international conference on Knowledge
capture, pages 185?186, New York, NY, USA. ACM.
A. C. Graesser, M. Singer, and T. Trabasso. 1994. Con-
structing inferences during narrative text comprehen-
sion. Psychological Review, 101:371?395.
Iryna Gurevych and Torsten Zesch, editors. 2009. The
Peoples Web Meets NLP: Collaboratively Constructed
Semantic Resources.
Paul Kingsbury and Martha Palmer. 2003. Propbank: the
next level of treebank. In Proceedings of Treebanks
and Lexical Theories.
Douglas B. Lenat. 1995. Cyc: a large-scale investment
in knowledge infrastructure. Communications of the
ACM, 38(11):33?38.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In ACL-44: Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Compu-
tational Linguistics, pages 337?344, Morristown, NJ,
USA. Association for Computational Linguistics.
Isaac Persing and Vincent Ng. 2009. Semi-supervised
cause identification from aviation safety reports. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 843?851, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Rashmi Prasad, Alan Lee, Nikhil Dinesh, Eleni Milt-
sakaki, Geraud Campion, Aravind Joshi, and Bonnie
50
Webber. 2008. Penn discourse treebank version 2.0.
Linguistic Data Consortium, February.
Kenji Sagae. 2009. Analysis of discourse structure with
syntactic dependencies and data-driven shift-reduce
parsing. In Proceedings of the 11th International Con-
ference on Parsing Technologies (IWPT?09), pages
81?84, Paris, France, October. Association for Com-
putational Linguistics.
Lenhart Schubert and Matthew Tong. 2003. Extract-
ing and evaluating general world knowledge from the
brown corpus. In Proceedings of the HLT-NAACL
2003 workshop on Text meaning, pages 7?13, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Reid Swanson and AndrewGordon. 2008. Say anything:
A massively collaborative open domain story writing
companion. In First International Conference on In-
teractive Digital Storytelling.
51
