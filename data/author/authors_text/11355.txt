Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 190?199,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Subjectivity Word Sense Disambiguation
Cem Akkaya and Janyce Wiebe
University of Pittsburgh
{cem,wiebe}@cs.pitt.edu
Rada Mihalcea
University of North Texas
rada@cs.unt.edu
Abstract
This paper investigates a new task, subjec-
tivity word sense disambiguation (SWSD),
which is to automatically determine which
word instances in a corpus are being used
with subjective senses, and which are be-
ing used with objective senses. We pro-
vide empirical evidence that SWSD is
more feasible than full word sense dis-
ambiguation, and that it can be exploited
to improve the performance of contextual
subjectivity and sentiment analysis sys-
tems.
1 Introduction
The automatic extraction of opinions, emotions,
and sentiments in text (subjectivity analysis) to
support applications such as product review min-
ing, summarization, question answering, and in-
formation extraction is an active area of research
in NLP.
Many approaches to opinion, sentiment, and
subjectivity analysis rely on lexicons of words that
may be used to express subjectivity. Examples of
such words are the following (in bold):
(1) He is a disease to every team he has gone to.
Converting to SMF is a headache.
The concert left me cold.
That guy is such a pain.
Knowing the meaning (and thus subjectivity) of
these words would help a system recognize the
negative sentiments in these sentences.
Most subjectivity lexicons are compiled as lists
of keywords, rather than word meanings (senses).
However, many keywords have both subjective
and objective senses. False hits ? subjectivity
clues used with objective senses ? are a signifi-
cant source of error in subjectivity and sentiment
analysis. For example, even though the follow-
ing sentence contains all of the negative keywords
above, it is nevertheless objective, as they are all
false hits:
(2) Early symptoms of the disease include severe
headaches, red eyes, fevers and cold chills, body
pain, and vomiting.
To tackle this source of error, we define a
new task, subjectivity word sense disambigua-
tion (SWSD), which is to automatically determine
which word instances in a corpus are being used
with subjective senses, and which are being used
with objective senses. We hypothesize that SWSD
is more feasible than full word sense disambigua-
tion, because it is more coarse grained ? often, the
exact sense need not be pinpointed. We also hy-
pothesize that SWSD can be exploited to improve
the performance of contextual subjectivity analy-
sis systems via sense-aware classification.
The paper consists of two parts. In the first
part, we build and evaluate a targeted supervised
SWSD system that aims to disambiguate members
of a subjectivity lexicon. It labels clue instances as
having a subjective sense or an objective sense in
context. The system relies on common machine
learning features for word sense disambiguation
(WSD). The performance is substantially above
both baseline and the performance of full WSD
on the same data, suggesting that the task is feasi-
ble, and that subjectivity provides a natural coarse-
grained grouping of senses.
The second part demonstrates the promise of
SWSD for contextual subjectivity analysis. First,
we show that subjectivity sense ambiguity is
highly prevalent in the MPQA opinion-annotated
corpus (Wiebe et al, 2005; Wilson, 2008), thus
establishing the potential benefit of performing
SWSD. Then, we exploit SWSD to improve per-
formance on several subjectivity analysis tasks,
from subjective/objective sentence-level classi-
fication to positive/negative/neutral expression-
level classification. To our knowledge, this is the
190
first attempt to explicitly use sense-level subjec-
tivity tags in contextual subjectivity and sentiment
analysis.
2 Background
We adopt the definitions of subjective and objec-
tive from (Wiebe et al, 2005; Wiebe and Mi-
halcea, 2006; Wilson, 2008). Subjective expres-
sions are words and phrases being used to ex-
press mental and emotional states, such as spec-
ulations, evaluations, sentiments, and beliefs. A
general covering term for such states is private
state (Quirk et al, 1985), an internal state that
cannot be directly observed or verified by others.
(Wiebe and Mihalcea, 2006) give the following
examples:
(3) His alarm grew.
He absorbed the information quickly.
UCC/Disciples leaders roundly condemned the
Iranian President?s verbal assault on Israel.
What?s the catch?
Polarity (also called semantic orientation) is
also important to NLP applications. In review
mining, for example, we want to know whether
an opinion about a product is positive or negative.
Nonetheless, as argued by (Wiebe and Mihalcea,
2006; Su and Markert, 2008), there are also mo-
tivations for a separate subjective/objective (S/O)
classification.
First, expressions may be subjective but not
have any particular polarity. An example given by
(Wilson et al, 2005a) is Jerome says the hospi-
tal feels no different than a hospital in the states.
An NLP application system may want to find a
wide range of private states attributed to a person,
such as their motivations, thoughts, and specula-
tions, in addition to their positive and negative sen-
timents. Second, benefits for sentiment analysis
can be realized by decomposing the problem into
S/O (or neutral versus polar) and polarity classifi-
cation (Yu and Hatzivassiloglou, 2003; Pang and
Lee, 2004; Wilson et al, 2005a; Kim and Hovy,
2006). We will see further evidence of this in Sec-
tion 4.2.3 in this paper.
The contextual subjectivity analysis experi-
ments in Section 4 include both S/O and polarity
classifications. The data used in those experiments
is from the MPQA Corpus (Wiebe et al, 2005;
Wilson, 2008),1 which consists of texts from the
world press annotated for subjective expressions.
1Available at http://www.cs.pitt.edu/mpqa
In the MPQA Corpus, subjective expressions of
varying lengths are marked, from single words to
long phrases. In addition, other properties are an-
notated, including polarity.
For SWSD, we need the notions of subjective
and objective senses of words in a dictionary. We
adopt the definitions from (Wiebe and Mihalcea,
2006), who describe the annotation scheme as fol-
lows. Classifying a sense as S means that, when
the sense is used in a text or conversation, one ex-
pects it to express subjectivity, and also that the
phrase or sentence containing it expresses subjec-
tivity. As noted in (Wiebe and Mihalcea, 2006),
sentences containing objective senses may not be
objective. Thus, objective senses are defined as
follows: Classifying a sense as O means that,
when the sense is used in a text or conversation,
one does not expect it to express subjectivity and,
if the phrase or sentence containing it is subjective,
the subjectivity is due to something else. Finally,
classifying a sense as B means it covers both sub-
jective and objective usages.
The following subjective examples are given in
(Wiebe and Mihalcea, 2006):
His alarm grew.
alarm, dismay, consternation ? (fear resulting from the aware-
ness of danger)
=> fear, fearfulness, fright ? (an emotion experienced in
anticipation of some specific pain or danger (usually ac-
companied by a desire to flee or fight))
What?s the catch?
catch ? (a hidden drawback; ?it sounds good but what?s the
catch??)
=> drawback ? (the quality of being a hindrance; ?he
pointed out all the drawbacks to my plan?)
They give the following objective examples:
The alarm went off.
alarm, warning device, alarm system ? (a device that signals
the occurrence of some undesirable event)
=> device ? (an instrumentality invented for a particu-
lar purpose; ?the device is small enough to wear on your
wrist?; ?a device intended to conserve water?)
He sold his catch at the market.
catch, haul ? (the quantity that was caught; ?the catch was
only 10 fish?)
=> indefinite quantity ? (an estimated quantity)
Wiebe and Mihalcea performed an agreement
study and report that good agreement (?=0.74) can
be achieved between human annotators labeling
the subjectivity of senses. For a similar task, (Su
and Markert, 2008) also report good agreement
(?=0.79).
191
3 Subjectivity Word Sense
Disambiguation
3.1 Task Definition and Method
We now turn to SWSD, and our method for per-
forming it.
Note that SWSD is midway between pure dic-
tionary classification and pure contextual interpre-
tation. For SWSD, the context of the word is con-
sidered in order to perform the task, but the sub-
jectivity is determined solely by the dictionary. In
contrast, full contextual interpretation can deviate
from a sense?s subjectivity label in the dictionary.
As noted above, words used with objective senses
may appear in subjective expressions. For exam-
ple, an SWSD system would label the following
examples of alarm as S, O and O, respectively. On
the other hand, a sentence-level subjectivity clas-
sifier would label the sentences as S, S, and O, re-
spectively.
(4) His alarm grew.
Will someone shut that darn alarm off?
The alarm went off.
We use a supervised approach to SWSD. We
train a different classifier for each lexicon entry
for which we have training data. Thus, our ap-
proach is like targeted WSD (in contrast to all-
words WSD), with two labels: S and O.
We borrow machine learning features which
have been successfully used in WSD. Specifically,
given an ambiguous target word, we use the fol-
lowing features from (Mihalcea, 2002):
CW : the target word itself
CP : POS of the target word
CF : surrounding context of 3 words and their POS
HNP : the head of the noun phrase to which the
target word belongs
NB : the first noun before the target word
VB : the first verb before the target word
NA : the first noun after the target word
VA : the first verb after the target word
SK : at most 10 context words occurring at least 5
times; determined for each sense
3.2 Lexicon and Data
Our target words are members of a subjectivity
lexicon, because, since they are in such a lexicon,
we know they have subjective usages. Specifically,
we use the lexicon of (Wilson et al, 2005b; Wil-
son, 2008).2 The entries have been divided into
2Available at http://www.cs.pitt.edu/mpqa
those that are strongly subjective (strongsubj) and
those that are weakly subjective (weaksubj), re-
flecting their reliability as subjectivity clues. The
sources of the entries in the lexicon are identified
in (Wilson, 2008). In the second part of this pa-
per, we evaluate systems against the MPQA cor-
pus. Wilson also uses this corpus for her eval-
uations. To enable this, entries were added to
the lexicon independently from the MPQA corpus
(that is, none of the entries were derived using the
MPQA corpus).
The training and test data for SWSD consists of
word instances in a corpus labeled as S or O, indi-
cating whether they are used with a subjective or
objective sense. Because we do not have data la-
beled with the S/O coarse-grained senses and we
did not want to undertake the annotation effort at
this stage, we created an annotated corpus by com-
bining two types of sense annotations: (1) labels
of senses within a dictionary as S or O (i.e., sub-
jectivity sense labels), and (2) sense tags of word
instances in a corpus (i.e., sense-tagged data). The
subjectivity sense labels are used to collapse the
sense labels in the sense-tagged data into the two
new senses, S and O.
Our sense-tagged data are the lexical sample
corpora (training and test data) from SENSEVAL1
(Kilgarriff and Palmer, 2000), SENSEVAL2 (Preiss
and Yarowsky, 2001), and SENSEVAL3 (Mihal-
cea and Edmonds, 2004). We selected all of the
SENSEVAL words that are also in the subjectivity
lexicon, and labeled their dictionary senses as S,
O, or B according to the annotation scheme de-
scribed above in Section 2. We did this subjectiv-
ity sense labeling according to the sense inventory
of the underlying corpus (Hector for SENSEVAL1;
WordNet1.7 for SENSEVAL2; and WordNet1.7.1
for SENSEVAL3).
Among the words, we found that 11 are not
ambiguous - either they have only S or only O
senses (in the corresponding sense inventory), or
the senses of their instances in the SENSEVAL data
are all S or all O. So as not to inflate our results, we
removed those 11 from the data, leaving 39 words.
In addition, we excluded the senses labeled B (a to-
tal of 10 senses). This leaves a total of 372 senses:
9 words (64 senses) from SENSEVAL1, 18 words
(201 senses) from SENSEVAL2, and 12 words (107
senses) from SENSEVAL3.
192
Base Acc SP SR SF OP OR OF IB EB(%)
All 79.9 88.3 89.3 89.1 89.2 87.1 87.4 87.2 8.4 41.8
S1 57.9 80.7 81.1 78.3 79.7 80.2 82.9 81.5 22.8 54.2
S2 81.1 87.3 86.5 85.2 85.8 87.9 89.0 88.4 6.2 32.8
S3 95.0 96.4 96.5 99.0 97.7 96.3 87.8 91.8 1.4 28.0
Table 1: Overall SWSD results (micro averages). Base is majority-class baseline; Acc is accuracy; SP,
SR, and SF are subjective precision, recall and F-measure; similarly for OP, OR, and OF. IB is absolute
improvement in Acc over Base; EB is percent error reduction in Acc.
3.3 SWSD Experiments
In this section, we evaluate our SWSD system, and
compare its performance to an WSD system on the
same data.
Note that, although generally in the SENSEVAL
datasets, training and test data are provided sep-
arately, a few target words from SENSEVAL1 do
not have both training and testing data. Thus, we
opted to combine the training and test data into one
dataset, and then perform 10-fold cross validation
experiments.
For our classifier, we use the SVM classifier
from the Weka package (Witten and Frank., 2005)
with its default settings.
We were interested in how well the system
would perform on more and less ambiguous
words. Thus, we split the words into three sub-
sets according to their majority-class baselines,
and report separate results: S1 (9 words), S2 (18
words), and S3 (12 words) have majority-class
baselines in the intervals [50%,70%) , [70%,90%),
and [90%,100%), respectively.
Table 1 contains the results, giving the overall
results (micro averages), as well as results for the
subsets S1, S2, and S3.
The improvement for SWSD over baseline is
especially high for the less skewed set, S1. This
is very encouraging because these words are the
more ambiguous words, and thus are the ones that
most need SWSD (assuming the SENSEVAL pri-
ors are similar to the priors in the corpus). The
average error reduction over baseline for S1 words
is 54.2%. Even for the more skewed sets S2 and
S3, reductions are 32.8% and 28.0%, respectively,
with an overall reduction of 41.8%.
To compare SWSD with WSD, we re-ran the
10-fold cross validation experiments, but this time
using the original sense labels, rather than S
and O. The (micro-averaged) accuracy is 67.9%,
much lower than the overall accuracy for SWSD
(88.3%).
The positive results provide evidence that
SWSD is a feasible variant of WSD, and that the
S/O sense groupings are natural ones, since the
system is able to learn to distinguish between them
with high accuracy. There is also potential for im-
provement by using a richer feature set, including
subjectivity features.
4 Opinion Analysis with Subjectivity
Word Sense Disambiguation
In this section, we explore the promise of SWSD
for contextual subjectivity analysis. First, we pro-
vide evidence that a subjectivity lexicon can have
substantial coverage of the subjective expressions
in a corpus, yet still be responsible for significant
subjectivity sense ambiguity in that corpus. Then,
we exploit SWSD in several contextual opinion
analysis systems, comparing the performance of
sense-aware and non-sense-aware versions. They
are all variations of components of the Opinion-
Finder opinion recognition system.3
4.1 Coverage and Ambiguity of Lexicon
Entries in the MPQA Corpus
In this section, we consider the distribution of lex-
icon entries in the MPQA corpus.
The lexicon covers a substantial subset of the
subjective expressions in the corpus: 67.1% of the
subjective expressions contain one or more lexi-
con entries.
On the other hand, fully 42.9% of the instances
of the lexicon entries in the MPQA corpus are
not in subjective expressions. An instance that
is not in a subjective expression is, by definition,
being used with an objective sense. Thus, these
instances are false hits of subjectivity clues. As
mentioned above, the entries in the lexicon have
been pre-classified as either more (strongsubj) or
less (weaksubj) reliable. We see this difference re-
flected in their degree of ambiguity ? 53% of the
3Available at http://www.cs.pitt.edu/opin
193
weaksubj instances are false hits, while only 22%
of the strongsubj instances are.
The high coverage of the lexicon demonstrates
its potential usefulness for opinion analysis sys-
tems, while its degree of ambiguity, in the form of
false hits in a subjectivity annotated corpus, shows
the potential benefit to opinion analysis of per-
forming SWSD.
As mentioned above, our experiments involve
only lexicon entries that are covered by the SEN-
SEVAL data, as we did not perform manual sense
tagging for this work. We have hope to expand
the system?s coverage in the future, as more word-
sense tagged data is produced (e.g., ONTONOTES
(Hovy et al, 2006)). We also have evidence that a
moderate amount of manual annotation would be
worth the effort. For example, let us order the lexi-
con entries from highest to lowest by frequency in
the MPQA corpus. The top 20 are responsible for
25% of all false hits in the corpus; the top 40 are
responsible for 34%; and the top 80 are responsi-
ble for 44%. If the SWSD system could be trained
for these words, the potential impact on reducing
false hits could be substantial, especially consid-
ering the good performance of the SWSD system
on the more ambiguous words. Note that we do
not want to simply discard these clues. The top 20
cover 9.4% of all subjective expressions; the top
40 cover 15.4%; and the top 80 cover 29.5%. Note
that SWSD only needs the data annotated with the
coarse-grained binary labels, which should be less
time consuming to produce than full word sense
tags.
4.2 Contextual Classification
We found in Section 3.3 that SWSD is a feasible
task and then in Section 4.1 that there is a great
deal of subjectivity sense ambiguity in a standard
subjectivity-annotated corpus (MPQA). We now
turn to exploiting the results of SWSD to automat-
ically recognize subjectivity and sentiment in the
MPQA corpus.
A motivation for using the MPQA data is that
many types of classifiers have been evaluated on
it, and we can directly test the effect of SWSD on
these classifiers.
Note that, for the SWSD experiments, the num-
ber of words does not limit the amount of data,
as SENSEVAL provides data for each word. How-
ever, the only parts of the MPQA corpus for which
SWSD could affect performance is the subset con-
taining instances of the words in the SWSD sys-
tem?s coverage. Thus, for the classifiers in this
section, the data used is the SenMPQA dataset,
which consists of the sentences in the MPQA Cor-
pus that contain at least one instance of the 39 key-
words. There are 689 such sentences (containing,
in total, 723 instances of the 39 keywords).
Even though this dataset is smaller than the one
used above, it gives us enough data to draw con-
clusions according to McNemar?s test for statisti-
cal significance.
4.2.1 Rule-based Classifier
We first apply SWSD to the rule-based classifier
from (Riloff and Wiebe, 2003). The classifier,
which is a sentence-level S/O classifier, has low
subjective and objective recall but high subjective
and objective precision. It is useful for creating
training data for subsequent processing by apply-
ing it to large amounts of unannotated data.
The classifier is a good candidate for directly
measuring the effects of SWSD on contextual sub-
jectivity analysis, because it classifies sentences
only by looking for the presence of subjectivity
keywords. Performance will improve if false hits
can be ignored.
The classifier labels a sentence as S if it contains
two or more strongsubj clues. On the other hand,
it considers three conditions to classify a sentence
as O: there are no strongsubj clues in the current
sentence, there are together at most one strongsubj
clue in the previous and next sentence, and there
are together at most 2 weaksubj clues in the cur-
rent, previous, and next sentence. A sentence that
is not labeled S or O is labeled unknown.
The rule-based classifier is made sense aware
by making it blind to the target word instances la-
beled O by the SWSD system, as these represent
false hits of subjectivity keywords. We compare
this sense-aware method (SE), with the original
classifier (O
RB
), in order to see if SWSD would
improve performance. We also built another modi-
fied rule-based classifier RE to demonstrate the ef-
fect of randomly ignoring subjectivity keywords.
RE ignores a keyword instance randomly with a
probability of 0.429, the expected value of false
hits in the MPQA corpus. The results are listed in
Table 2.
The rule-based classifier looks for the presence
of the keywords to find subjective sentences and
for the absence of the keywords to find objective
sentences. It is obvious that a variant working on
194
Acc OP OR OF SP SR SF
O
RB
27.0 50.0 4.1 7.6 92.7 36.0 51.8
SE 28.3 62.1 9.3 16.1 92.7 35.8 51.6
RE 27.6 48.4 7.7 13.3 92.6 35.4 51.2
Table 2: Effect of SWSD on the rule-based classi-
fiers.
fewer keyword instances than O
RB
will always
have the same or higher objective recall and the
same or lower subjective recall than O
RB
. That is
the case for both SE and RE. The real benefit we
see is in objective precision, which is substantially
higher for SE than O
RB
. For our experiments, OP
gives a better idea of the impact of SWSD, be-
cause most of the keyword instances SWSD dis-
ambiguates are weaksubj clues, and weaksubj key-
words figure more prominently in objective classi-
fication. On the other hand, RE has both lower OP
and SP than O
RB
. Note that accuracy for all three
systems is low, because all unknown predictions
are counted as incorrect.
These findings suggest that SWSD performs
well on disambiguating keyword instances in the
MPQA corpus,4 and demonstrates a positive im-
pact of SWSD on sentence-level subjectivity clas-
sification.
4.2.2 Subjective/Objective Classifier
We now move to more fine-grained expression-
level subjectivity classification. Since sentences
often contain multiple subjective expressions,
expression-level classification is more informative
than sentence-level classification.
The classifier in this section is an implementa-
tion of the neutral/polar supervised classifier of
(Wilson et al, 2005a) (using the same features),
except that the classes are S/O rather than neu-
tral/polar. These classifiers label instances of lex-
icon entries. The gold standard is defined on the
MPQA Corpus as follows: If an instance is in a
subjective expression, it is contextually S. If the
instance is in an objective expression, it is contex-
tually O. We evaluate the system on the 723 clue
instances in the SenMPQA dataset.
We incorporate SWSD information into the
contextual subjectivity classifier in a straight-
forward fashion: outputs are modified according
to simple, intuitive rules.
4which we cannot evaluate directly, as the MPQA corpus
is not sense tagged.
Our strategy is defined by the relation between
sense subjectivity and contextual subjectivity and
involves two rules, R1 and R2.
We know that a keyword instance used with a
S sense must be in a subjective expression. R1 is
to simply trust SWSD: If the contextual classifier
labels an instance as O, but SWSD determines that
it has an S sense, then R1 flips the contextual clas-
sifier?s label to S.
Things are not as simple in the case of O senses,
since they may appear in both subjective and ob-
jective expressions. We will state R2, and then ex-
plain it: If the contextual classifier labels an in-
stance as S, but (1) SWSD determines that it has
an O sense, (2) the contextual classifier?s confi-
dence is low, and (3) there is no other subjective
keyword in the same expression, then R2 flips the
contextual classifier?s label to O. First, consider
confidence: though a keyword with an O sense
may appear in either subjective or objective ex-
pressions, it is more likely to appear in an objec-
tive expression. We assume that this is reflected
to some extent in the contextual classifier?s confi-
dence. Second, if a keyword with an O sense ap-
pears in a subjective expression, then the subjec-
tivity is not due to that keyword but rather due to
something else. Thus, the presence of another lex-
icon entry ?explains away? the presence of the O
sense in the subjective expression, and we do not
want SWSD to overrule the contextual classifier.
Only when the contextual classifier isn?t certain
and only when there isn?t another keyword does
R2 flip the label to O.
Our definition of low confidence is in terms
of the label weights assigned by BoosTexter
(Schapire and Singer, 2000), which is the under-
lying machine learning algorithm of the classifier.
We use the difference between the largest label
weight and the second largest label weight as a
measure of confidence, as suggested in the Boos-
Texter documentation. The threshold we use is
0.0008.5
We apply the contextual classifier and the
SWSD system to the data, and compare the per-
formance of the original system (O
S/O
) and three
sense-aware variants: one using only R1, one us-
5As will be noted below, we experimented with three
thresholds for the classifier in Section 4.2.3, with no signif-
icant difference in accuracy. Here, we simply adopt 0.0008,
without further experimentation. In addition, we did not ex-
periment with other conditions than those incorporated in the
two rules in this section and the two rules in Section 4.2.3
below.
195
Acc OP OR OF SP SR SF
O
S/O
75.4 68.0 62.9 65.4 79.2 82.7 80.9
R1 77.7 75.5 58.8 66.1 78.6 88.8 83.4
R2 79.0 67.3 83.9 74.7 89.0 76.1 82.0
R1R2 81.3 72.5 79.8 75.9 87.4 82.2 84.8
Table 3: Effect of SWSD on the subjec-
tive/objective classifier
ing only R2, and one using both (R1R2). The re-
sults are in Table 3. The R1 variant shows an im-
provement of 2.3 points in accuracy (a 9.4% error
reduction). The R2 variant shows an improvement
of 3.6 points in accuracy (a 14.6% error reduc-
tion). Applying both rules (R1R2) gives an im-
provement of 5.9 percentage points in accuracy (a
24% error reduction).
In our case, a paired t-test is not appropriate
to measure statistical significance, as we are not
doing multiple runs. Thus, we apply McNemar?s
test, which is a non-parametric method for algo-
rithms that can be executed only once, meaning
training once and testing once (Dietterich, 1998).
For R1, the improvement in accuracy is statisti-
cally significant at the p < .05 level. For R2 and
R1R2, the improvement in accuracy is statistically
significant at the p < .01 level. Moreover, in all
cases, we see improvement in both objective and
subjective F-measure.
4.2.3 Contextual Polarity Classifier
We now apply SWSD to contextual polarity clas-
sification (positive/negative/neutral), in the hope
that avoiding false hits of subjectivity keywords
will also lead to performance improvement in con-
textual sentiment analysis.
We use an implementation of the classifier of
(Wilson et al, 2005a). This classifier labels in-
stances of lexicon entries. The gold standard is
defined on the MPQA Corpus as follows: If an
instance is in a positive subjective expression, it
is contextually positive (Ps); if in a negative sub-
jective expression, it is contextually negative (Ng);
and if it is in an objective expression or a neu-
tral subjective expression, then it is contextually
N(eutral). As above, we evaluate the system on
the keyword instances in the SenMPQA dataset.
Wilson et al use a two step approach. The first
step classifies keyword instances as being in a po-
lar (positive or negative) or a neutral context. The
first step is performed by the neutral/polar classi-
fier mentioned above in Section 4.2.2. The sec-
ond step decides the contextual polarity (positive
or negative) of the instances classified as polar in
the first step, and is performed by a separate clas-
sifier.
To make a sense-aware version of the system,
we use rules to change some of the answers of the
neutral/polar classifier.
Unfortunately, we cannot simply trust SWSD
when it labels a keyword as an S sense, because an
S sense might be in a N(eutral) expression (since
there are neutral subjective expressions). But, an
S sense is more likely to appear in a P(olar) ex-
pression. Thus, we consider confidence (rule R3):
If the contextual classifier labels an instance as N,
but SWSD determines it has an S sense and the
contextual classifier?s confidence is low,6 then R3
flips the contextual classifier?s label to P.
Rule R4 is analogous to R2 in the previous sec-
tion: If the contextual classifier labels an instance
as P, but (1) SWSD determines that it has an O
sense, (2) the contextual classifier?s confidence is
low, and (3) there is no other subjective keyword in
the same expression, then R2 flips the contextual
classifier?s label to N.
We compare the performance of the original
neutral/polar classifier (O
N/P
) and sense-aware
variants using R3 and R4. The results are in Table
4. This time, the table does not include a combined
method, because only R4 improves performance.
This is consistent with the finding in (Wilson et
al., 2005a) that most errors are caused by subjec-
tivity keywords with non-neutral prior polarity ap-
pearing in phrases with neutral contextual polarity.
R4 targets these cases. It is promising to see that
SWSD provides enough information to fix some of
them. There is a 2.6 point improvement in accu-
racy (a 12.4% error reduction). The improvement
in accuracy is statistically significant at the p <
.01 level with McNemar?s test. The improvement
in accuracy is accompanied by improvements in
both neutral and polar F-measure.
We wanted to see if the improvements in the
6As in the previous section, low confidence is defined
in terms of the difference between the largest label weight
and the second largest label weight assigned by BoosTexter.
We tried three thresholds, 0.0007, 0.0008, and 0.0009, re-
sulting in only a slight difference in accuracy: 0.0007 and
0.0009 both give 81.5 accuracy compared to 81.6 accuracy
for 0.0008. We report results using 0.0008, though the ac-
curacy using the other thresholds is statistically significantly
better than the accuracy of the original classifier at the same
level.
196
Acc NP NR NF NgP NgR NgF PsP PsR PsF
O
Ps/Ng/N
77.6 80.9 94.6 87.2 60.4 29.4 39.5 52.2 32.4 40.0
R4 80.6 81.2 98.7 89.1 82.1 29.4 43.2 68.6 32.4 44.0
Table 5: Effect of SWSD on the contextual polarity classifier
Acc NP NR NF PP PR PF
O
N/P
79.0 81.5 92.5 86.7 65.8 40.7 50.3
R3 70.0 83.7 73.8 78.4 44.4 59.3 50.8
R4 81.6 81.7 96.8 88.6 81.1 38.6 52.3
Table 4: Effect of SWSD on the neutral/polar clas-
sifier
first step of Wilson et als system can be propa-
gated to their second step, yielding an overall im-
provement in positive /negative/neutral (Ps/Ng/N)
classification.
The sense-aware variant of the overall two-part
system is the same as the original except that we
apply R4 to the output of the first step (flipping
some of the neutral/polar classifier?s P labels to
N). Thus, since the second step in Wilson et al?s
classifier processes only those instances labeled P
in the first step, in the sense-aware system, fewer
instances are passed from the first to the second
step.
Table 5 reports results for the original sys-
tem (O
Ps/Ng/N
) and the sense-aware variant (R4).
These results are for the entire SenMPQA dataset,
not just those labeled P in the first step.
The accuracy improves 3 percentage points (a
13.4% error reduction). The improvement in accu-
racy is statistically significant at the p < .01 level
with McNemar?s test. We see the real benefit when
we look at the precision of the positive and neg-
ative classes. Negative precision goes from 60.4
to 82.1 and positive precision goes from 52.2 to
68.6, with no loss in recall. This is evidence that
the SWSD system is doing a good job of removing
some false hits of subjectivity clues that harm the
original version of the system.
5 Comparisons to Previous Work
Several researchers exploit lexical resources for
contextual subjectivity and sentiment analysis.
These systems typically look for the presence of
subjective or sentiment-bearing words in the text.
They may rely only on this information (e.g.,
(Turney, 2002; Whitelaw et al, 2005; Riloff and
Wiebe, 2003)), or they may combine it with addi-
tional information as well (e.g., (Yu and Hatzivas-
siloglou, 2003; Kim and Hovy, 2004; Bloom et al,
2007; Wilson et al, 2005a)). We apply SWSD to
some of those systems to show the effect of SWSD
on contextual subjectivity and sentiment analysis.
Another set of related work is on subjectivity
and polarity labeling of word senses (e.g. (Esuli
and Sebastiani, 2006; Andreevskaia and Bergler,
2006; Wiebe and Mihalcea, 2006; Su and Markert,
2008)). They label senses of words in a dictionary.
In comparison, we label senses of word instances
in a corpus.
Moreover, our work extends findings in (Wiebe
and Mihalcea, 2006) and (Su and Markert, 2008).
(Wiebe and Mihalcea, 2006) demonstrates that
subjectivity is a property that can be associated
with word senses. We show that it is a natural
grouping of word senses and that it provides a
principled way for clustering senses. They also
demonstrate that subjectivity helps with WSD. We
show that a coarse-grained WSD variant (SWSD)
helps with subjectivity and sentiment analysis.
Both (Wiebe and Mihalcea, 2006) and (Su and
Markert, 2008) show that even reliable subjectiv-
ity clues have objective senses. We demonstrate
that this ambiguity is also prevalent in a corpus.
Several researchers (e.g., (Palmer et al, 2004;
Navigli, 2006; Snow et al, 2007; Hovy et al,
2006)) work on reducing the granularity of sense
inventories for WSD. They aim for a more coarse-
grained sense inventory to overcome performance
shortcomings related to fine-grained sense distinc-
tions. Our work is similar in the sense that we
reduce all senses of a word to two senses (S/O).
The difference is the criterion driving the group-
ing. Related work concentrates on syntactic and
semantic similarity between senses to group them.
In contrast, our grouping is driven by subjectivity
with a specific application area in mind, namely
subjectivity and sentiment analysis.
6 Conclusions and Future Work
We introduced the task of subjectivity word sense
disambiguation (SWSD), and evaluated a super-
vised method inspired by research in WSD. The
197
system achieves high accuracy, especially on
highly ambiguous words, and substantially outper-
forms WSD on the same data. The positive results
provide evidence that SWSD is a feasible variant
of WSD, and that the S/O sense groupings are nat-
ural ones.
We also explored the promise of SWSD for con-
textual subjectivity analysis. We showed that a
subjectivity lexicon can have substantial coverage
of the subjective expressions in the corpus, yet
still be responsible for significant sense ambiguity.
This demonstrates the potential benefit to opin-
ion analysis of performing SWSD. We then ex-
ploit SWSD in several contextual opinion analysis
systems, including positive/negative/neutral senti-
ment classification. Improvements in performance
were realized for all of the systems.
We plan several future directions which promise
to further increase the impact of SWSD on sub-
jectivity and sentiment analysis. We will manu-
ally annotate a moderate number of strategically
chosen words, namely frequent ones which are
highly ambiguous. In addition, we will add fea-
tures to the SWSD system reflecting the subjec-
tivity of the surrounding context. Finally, there
are more sophisticated strategies to explore for
improving subjectivity and sentiment analysis via
SWSD than the simple, intuitive rules we began
with in this paper.
Acknowledgments
This material is based in part upon work supported
by National Science Foundation awards #0840632
and #0840608. Any opinions, findings, and con-
clusions or recommendations expressed in this
material are those of the authors and do not nec-
essarily reflect the views of the National Science
Foundation.
References
A. Andreevskaia and S. Bergler. 2006. Mining word-
net for a fuzzy sentiment: Sentiment tag extraction
from wordnet glosses. In (EACL-2006).
K. Bloom, N. Garg, and S. Argamon. 2007. Extracting
appraisal expressions. In HLT-NAACL 2007, pages
308?315, Rochester, NY.
T. G. Dietterich. 1998. Approximate statistical tests
for comparing supervised classification learning al-
gorithms. Neural Computation, 10:1895?1923.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A
publicly available lexical resource for opinion min-
ing. In (LREC-06), Genova, IT.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: The 90% solu-
tion. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Companion Vol-
ume: Short Papers, New York City.
A. Kilgarriff and M. Palmer, editors. 2000. Com-
puter and the Humanities. Special issue: SENSE-
VAL. Evaluating Word Sense Disambiguation pro-
grams, volume 34, April.
S.-M. Kim and E. Hovy. 2004. Determining the senti-
ment of opinions. In (COLING 2004), pages 1267?
1373, Geneva, Switzerland.
S.-M. Kim and E. Hovy. 2006. Identifying and analyz-
ing judgment opinions. In (HLT/NAACL-06), pages
200?207, New York, New York.
R. Mihalcea and P. Edmonds, editors. 2004. Pro-
ceedings of SENSEVAL-3, Association for Compu-
tational Linguistics Workshop, Barcelona, Spain.
R. Mihalcea. 2002. Instance based learning with
automatic feature selection applied to Word Sense
Disambiguation. In Proceedings of the 19th Inter-
national Conference on Computational Linguistics
(COLING 2002), Taipei, Taiwan, August.
R. Navigli. 2006. Meaningful clustering of senses
helps boost word sense disambiguation perfor-
mance. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics, Sydney,
Australia.
M. Palmer, O. Babko-Malaya, and H. T. Dang. 2004.
Different sense granularities for different applica-
tions. In HLT-NAACL 2004 Workshop: 2nd Work-
shop on Scalable Natural Language Understanding,
Boston, Massachusetts.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In (ACL-04), pages 271?
278, Barcelona, ES. Association for Computational
Linguistics.
J. Preiss and D. Yarowsky, editors. 2001. Pro-
ceedings of SENSEVAL-2, Association for Compu-
tational Linguistics Workshop, Toulouse, France.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985. A Comprehensive Grammar of the English
Language. Longman, New York.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In (EMNLP-2003),
pages 105?112, Sapporo, Japan.
R. E. Schapire and Y. Singer. 2000. BoosTexter: A
boosting-based system for text categorization. Ma-
chine Learning, 39(2/3):135?168.
R. Snow, S. Prakash, D. Jurafsky, and A. Ng. 2007.
Learning to merge word senses. In Proceedings of
the Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL), Prague,
Czech Republic.
F. Su and K. Markert. 2008. From word to sense: a
case study of subjectivity recognition. In (COLING-
2008), Manchester.
198
P. Turney. 2002. Thumbs up or thumbs down? seman-
tic orientation applied to unsupervised classification
of reviews. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), pages 417?424, Philadelphia.
C. Whitelaw, N. Garg, and S. Argamon. 2005. Us-
ing appraisal groups for sentiment analysis. In Pro-
ceedings of CIKM-05, the ACM SIGIR Conference
on Information and Knowledge Management, Bre-
men, DE.
J. Wiebe and R. Mihalcea. 2006. Word sense and sub-
jectivity. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics, Syd-
ney, Australia.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Anno-
tating expressions of opinions and emotions in lan-
guage. Language Resources and Evaluation (for-
merly Computers and the Humanities), 39(2/3):164?
210.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005a. Recog-
nizing contextual polarity in phrase-level sentiment
analysis. In (HLT/EMNLP-2005), pages 347?354,
Vancouver, Canada.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler, J.
Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Patward-
han. 2005b. OpinionFinder: A system for subjec-
tivity analysis. In Proc. Human Language Technol-
ogy Conference and Conference on Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP-
2005) Companion Volume (software demonstration).
T. Wilson. 2008. Fine-grained Subjectivity and Sen-
timent Analysis: Recognizing the Intensity, Polarity,
and Attitudes of private states. Ph.D. thesis, Intelli-
gent Systems Program, University of Pittsburgh.
I. Witten and E. Frank. 2005. Data Mining: Practi-
cal Machine Learning Tools and Techniques, Second
Edition. Morgan Kaufmann, June.
H. Yu and V. Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP-03), pages 129?
136, Sapporo, Japan.
199
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 10?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Integrating Knowledge for Subjectivity Sense Labeling
Yaw Gyamfi and Janyce Wiebe
University of Pittsburgh
{anti,wiebe}@cs.pitt.edu
Rada Mihalcea
University of North Texas
rada@cs.unt.edu
Cem Akkaya
University of Pittsburgh
cem@cs.pitt.edu
Abstract
This paper introduces an integrative approach
to automatic word sense subjectivity annota-
tion. We use features that exploit the hier-
archical structure and domain information in
lexical resources such as WordNet, as well as
other types of features that measure the sim-
ilarity of glosses and the overlap among sets
of semantically related words. Integrated in a
machine learning framework, the entire set of
features is found to give better results than any
individual type of feature.
1 Introduction
Automatic extraction of opinions, emotions, and
sentiments in text (subjectivity analysis) to support
applications such as product review mining, sum-
marization, question answering, and information ex-
traction is an active area of research in NLP.
Many approaches to opinion, sentiment, and sub-
jectivity analysis rely on lexicons of words that may
be used to express subjectivity. However, words may
have both subjective and objective senses, which is
a source of ambiguity in subjectivity and sentiment
analysis. We show that even words judged in pre-
vious work to be reliable clues of subjectivity have
significant degrees of subjectivity sense ambiguity.
To address this ambiguity, we present a method
for automatically assigning subjectivity labels to
word senses in a taxonomy, which uses new features
and integrates more diverse types of knowledge than
in previous work. We focus on nouns, which are
challenging and have received less attention in auto-
matic subjectivity and sentiment analysis.
A common approach to building lexicons for sub-
jectivity analysis is to begin with a small set of
seeds which are prototypically subjective (or posi-
tive/negative, in sentiment analysis), and then fol-
low semantic links in WordNet-like resources. By
far, the emphasis has been on horizontal relations,
such as synonymy and antonymy. Exploiting vertical
links opens the door to taking into account the infor-
mation content of ancestor concepts of senses with
known and unknown subjectivity. We develop novel
features that measure the similarity of a target word
sense with a seed set of senses known to be sub-
jective, where the similarity between two concepts
is determined by the extent to which they share in-
formation, measured by the information content as-
sociated with their least common subsumer (LCS).
Further, particularizing the LCS features to domain
greatly reduces calculation while still maintaining
effective features.
We find that our new features do lead to signif-
icant improvements over methods proposed in pre-
vious work, and that the combination of all features
gives significantly better performance than any sin-
gle type of feature alone.
We also ask, given that there are many approaches
to finding subjective words, if it would make sense
for word- and sense-level approaches to work in tan-
dem, or should we best view them as competing ap-
proaches? We give evidence suggesting that first
identifying subjective words and then disambiguat-
ing their senses would be an effective approach to
subjectivity sense labeling.
10
There are several motivations for assigning sub-
jectivity labels to senses. First, (Wiebe and Mi-
halcea, 2006) provide evidence that word sense la-
bels, together with contextual subjectivity analysis,
can be exploited to improve performance in word
sense disambiguation. Similarly, given subjectivity
sense labels, word-sense disambiguation may poten-
tially help contextual subjectivity analysis. In addi-
tion, as lexical resources such as WordNet are devel-
oped further, subjectivity labels would provide prin-
cipled criteria for refining word senses, as well as for
clustering similar meanings to create more course-
grained sense inventories.
For many opinion mining applications, polarity
(positive, negative) is also important. The overall
framework we envision is a layered approach: clas-
sifying instances as objective or subjective, and fur-
ther classifying the subjective instances by polar-
ity. Decomposing the problem into subproblems has
been found to be effective for opinion mining. This
paper addresses the first of these subproblems.
2 Background
We adopt the definitions of subjective and objective
from Wiebe and Mihalcea (2006) (hereafter WM).
Subjective expressions are words and phrases being
used to express opinions, emotions, speculations,
etc. WM give the following examples:
His alarm grew.
He absorbed the information quickly.
UCC/Disciples leaders roundly condemned the
Iranian President?s verbal assault on Israel.
What?s the catch?
Polarity (also called semantic orientation) is also
important to NLP applications in sentiment analysis
and opinion extraction. In review mining, for exam-
ple, we want to know whether an opinion about a
product is positive or negative. Even so, we believe
there are strong motivations for a separate subjec-
tive/objective (S/O) classification as well.
First, expressions may be subjective but not have
any particular polarity. An example given by (Wil-
son et al, 2005) is Jerome says the hospital feels
no different than a hospital in the states. An NLP
application system may want to find a wide range
of private states attributed to a person, such as their
motivations, thoughts, and speculations, in addition
to their positive and negative sentiments.
Second, distinguishing S and O instances has of-
ten proven more difficult than subsequent polarity
classification. Researchers have found this at vari-
ous levels of analysis, including the manual anno-
tation of phrases (Takamura et al, 2006), sentiment
classification of phrases (Wilson et al, 2005), sen-
timent tagging of words (Andreevskaia and Bergler,
2006b), and sentiment tagging of word senses (Esuli
and Sebastiani, 2006a). Thus, effective methods for
S/O classification promise to improve performance
for sentiment classification. In fact, researchers in
sentiment analysis have realized benefits by decom-
posing the problem into S/O and polarity classifica-
tion (Yu and Hatzivassiloglou, 2003; Pang and Lee,
2004; Wilson et al, 2005; Kim and Hovy, 2006).
One reason is that different features may be relevant
for the two subproblems. For example, negation fea-
tures are more important for polarity classification
than for subjectivity classification.
Note that some of our features require vertical
links that are present in WordNet for nouns and
verbs but not for other parts of speech. Thus we ad-
dress nouns (leaving verbs to future work). There
are other motivations for focusing on nouns. Rela-
tively little work in subjectivity and sentiment anal-
ysis has focused on subjective nouns. Also, a study
(Bruce and Wiebe, 1999) showed that, of the major
parts of speech, nouns are the most ambiguous with
respect to the subjectivity of their instances.
Turning to word senses, we adopt the definitions
from WM. First, subjective: ?Classifying a sense as
S means that, when the sense is used in a text or con-
versation, we expect it to express subjectivity; we
also expect the phrase or sentence containing it to
be subjective [WM, pp. 2-3].?
In WM, it is noted that sentences containing ob-
jective senses may not be objective, as in the sen-
tence Will someone shut that darn alarm off? Thus,
objective senses are defined as follows: ?Classifying
a sense as O means that, when the sense is used in a
text or conversation, we do not expect it to express
subjectivity and, if the phrase or sentence containing
it is subjective, the subjectivity is due to something
else [WM, p 3].?
The following subjective examples are given in
11
WM:
His alarm grew.
alarm, dismay, consternation ? (fear resulting from the aware-
ness of danger)
=> fear, fearfulness, fright ? (an emotion experienced in an-
ticipation of some specific pain or danger (usually accompa-
nied by a desire to flee or fight))
What?s the catch?
catch ? (a hidden drawback; ?it sounds good but what?s the
catch??)
=> drawback ? (the quality of being a hindrance; ?he
pointed out all the drawbacks to my plan?)
The following objective examples are given in WM:
The alarm went off.
alarm, warning device, alarm system ? (a device that signals the
occurrence of some undesirable event)
=> device ? (an instrumentality invented for a particular pur-
pose; ?the device is small enough to wear on your wrist?; ?a
device intended to conserve water?)
He sold his catch at the market.
catch, haul ? (the quantity that was caught; ?the catch was only
10 fish?)
=> indefinite quantity ? (an estimated quantity)
WM performed an agreement study and report
that good agreement (?=0.74) can be achieved be-
tween human annotators labeling the subjectivity of
senses. For a similar task, (Su and Markert, 2008)
also report good agreement.
3 Related Work
Many methods have been developed for automati-
cally identifying subjective (opinion, sentiment, at-
titude, affect-bearing, etc.) words, e.g., (Turney,
2002; Riloff and Wiebe, 2003; Kim and Hovy, 2004;
Taboada et al, 2006; Takamura et al, 2006).
Five groups have worked on subjectivity sense la-
beling. WM and Su and Markert (2008) (hereafter
SM) assign S/O labels to senses, while Esuli and Se-
bastiani (hereafter ES) (2006a; 2007), Andreevskaia
and Bergler (hereafter AB) (2006b; 2006a), and
(Valitutti et al, 2004) assign polarity labels.
WM, SM, and ES have evaluated their systems
against manually annotated word-sense data. WM?s
annotations are described above; SM?s are similar.
In the scheme ES use (Cerini et al, 2007), senses
are assigned three scores, for positivity, negativity,
and neutrality. There is no unambiguous mapping
between the labels of WM/SM and ES, first because
WM/SM use distinct classes and ES use numerical
ratings, and second because WM/SM distinguish be-
tween objective senses on the one hand and neutral
subjective senses on the other, while those are both
neutral in the scheme used by ES.
WM use an unsupervised corpus-based approach,
in which subjectivity labels are assigned to word
senses based on a set of distributionally similar
words in a corpus annotated with subjective expres-
sions. SM explore methods that use existing re-
sources that do not require manually annotated data;
they also implement a supervised system for com-
parison, which we will call SMsup. The other three
groups start with positive and negative seed sets and
expand them by adding synonyms and antonyms,
and traversing horizontal links in WordNet. AB, ES,
and SMsup additionally use information contained
in glosses; AB also use hyponyms; SMsup also uses
relation and POS features. AB perform multiple
runs of their system to assign fuzzy categories to
senses. ES use a semi-supervised, multiple-classifier
learning approach. In a later paper, (Esuli and Se-
bastiani, 2007), ES again use information in glosses,
applying a random walk ranking algorithm to a
graph in which synsets are linked if a member of
the first synset appears in the gloss of the second.
Like ES and SMsup, we use machine learning, but
with more diverse sources of knowledge. Further,
several of our features are novel for the task. The
LCS features (Section 6.1) detect subjectivity by
measuring the similarity of a candidate word sense
with a seed set. WM also use a similarity measure,
but as a way to filter the output of a measure of distri-
butional similarity (selecting words for a given word
sense), not as we do to cumulatively calculate the
subjectivity of a word sense. Another novel aspect
of our similarity features is that they are particular-
ized to domain, which greatly reduces calculation.
The domain subjectivity LCS features (Section 6.2)
are also novel for our task. So is augmenting seed
sets with monosemous words, for greater coverage
without requiring human intervention or sacrificing
quality. Note that none of our features as we specif-
ically define them has been used in previous work;
combining them together, our approach outperforms
previous approaches.
12
4 Lexicon and Annotations
We use the subjectivity lexicon of (Wiebe and Riloff,
2005)1 both to create a subjective seed set and to
create the experimental data sets. The lexicon is a
list of words and phrases that have subjective uses,
though only word entries are used in this paper (i.e.,
we do not address phrases at this point). Some en-
tries are from manually developed resources, includ-
ing the General Inquirer, while others were derived
from corpora using automatic methods.
Through manual review and empirical testing on
data, (Wiebe and Riloff, 2005) divided the clues into
strong (strongsubj) and weak (weaksubj) subjectiv-
ity clues. Strongsubj clues have subjective meanings
with high probability, and weaksubj clues have sub-
jective meanings with lower probability.
To support our experiments, we annotated the
senses2 of polysemous nouns selected from the lex-
icon, using WM?s annotation scheme described in
Section 2. Due to time constraints, only some of the
data was labeled through consensus labeling by two
annotators; the rest was labeled by one annotator.
Overall, 2875 senses for 882 words were anno-
tated. Even though all are senses of words from the
subjectivity lexicon, only 1383 (48%) of the senses
are subjective.
The words labeled strongsubj are in fact less am-
biguous than those labeled weaksubj in our analysis,
thus supporting the reliability classifications in the
lexicon. 55% (1038/1924) of the senses of strong-
subj words are subjective, while only 36% (345/951)
of the senses of weaksubj words are subjective.
For the analysis in Section 7.3, we form subsets
of the data annotated here to test performance of our
method on different data compositions.
5 Seed Sets
Both subjective and objective seed sets are used to
define the features described below. For seeds, a
large number is desirable for greater coverage, al-
though high quality is also important. We begin to
build our subjective seed set by adding the monose-
mous strongsubj nouns of the subjectivity lexicon
(there are 397 of these). Since they are monose-
mous, they pose no problem of sense ambiguity. We
1Available at http://www.cs.pitt.edu/mpqa
2In WordNet 2.0
then expand the set with their hyponyms, as they
were found useful in previous work by AB (2006b;
2006a). This yields a subjective seed set of 645
senses. After removing the word senses that belong
to the same synset, so that only one word sense per
synset is left, we ended up with 603 senses.
To create the objective seed set, two annotators
manually annotated 800 random senses from Word-
Net, and selected for the objective seed set the ones
they both agreed are clearly objective. This creates
an objective seed set of 727. Again we removed
multiple senses from the same synset leaving us with
722. The other 73 senses they annotated are added
to the mixed data set described below. As this sam-
pling shows, WordNet nouns are highly skewed to-
ward objective senses, so finding an objective seed
set is not difficult.
6 Features
6.1 Sense Subjectivity LCS Feature
This feature measures the similarity of a target sense
with members of the subjective seed set. Here, sim-
ilarity between two senses is determined by the ex-
tent to which they share information, measured by
using the information content associated with their
least common subsumer. For an intuition behind this
feature, consider this example. In WordNet, the hy-
pernym of the ?strong criticism? sense of attack is
criticism. Several other negative subjective senses
are descendants of criticism, including the relevant
senses of fire, thrust, and rebuke. Going up one
more level, the hypernym of criticism is the ?ex-
pression of disapproval? meaning of disapproval,
which has several additional negative subjective de-
scendants, such as the ?expression of opposition and
disapproval? sense of discouragement. Our hypoth-
esis is that the cases where subjectivity is preserved
in the hypernym structure, or where hypernyms do
lead from subjective senses to others, are the ones
that have the highest least common subsumer score
with the seed set of known subjective senses.
We calculate similarity using the information-
content based measure proposed in (Resnik, 1995),
as implemented in the WordNet::Similarity pack-
age (using the default option in which LCS values
are computed over the SemCor corpus).3 Given a
3http://search.cpan.org/dist/WordNet-Similarity/
13
taxonomy such as WordNet, the information con-
tent associated with a concept is determined as the
likelihood of encountering that concept, defined as
?log(p(C)), where p(C) is the probability of see-
ing concept C in a corpus. The similarity between
two concepts is then defined in terms of information
content as: LCSs(C1, C2) = max[?log(p(C))],
where C is the concept that subsumes both C1 and
C2 and has the highest information content (i.e., it is
the least common subsumer (LCS)).
For this feature, a score is assigned to a target
sense based on its semantic similarity to the mem-
bers of a seed set; in particular, the maximum such
similarity is used.
For a target sense t and a seed set S, we could
have used the following score:
Score(t, S) = max
s?S
LCSs(t, s)
However, several researchers have noted that sub-
jectivity may be domain specific. A version of
WordNet exists, WordNet Domains (Gliozzo et al,
2005), which associates each synset with one of the
domains in the Dewey Decimal library classifica-
tion. After sorting our subjective seed set into differ-
ent domains, we observed that over 80% of the sub-
jective seed senses are concentrated in six domains
(the rest are distributed among 35 domains).
Thus, we decided to particularize the semantic
similarity feature to domain, such that only the sub-
set of the seed set in the same domain as the tar-
get sense is used to compute the feature. This in-
volves much less calculation, as LCS values are cal-
culated only with respect to a subset of the seed set.
We hypothesized that this would still be an effec-
tive feature, while being more efficient to calculate.
This will be important when this method is applied
to large resources such as the entire WordNet.
Thus, for seed set S and target sense t which is
in domain D, the feature is defined as the following
score:
SenseLCSscore(t,D, S) = max
d?D?S
LCSs(t, d)
The seed set is a parameter, so we could have
defined a feature reflecting similarity to the objec-
tive seed set as well. Since WordNet is already
highly skewed toward objective noun senses, any
naive classifier need only guess the majority class
for high accuracy for the objective senses. We in-
cluded only a subjective feature to put more empha-
sis on the subjective senses. In the future, features
could be defined with respect to objectivity, as well
as polarity and other properties of subjectivity.
6.2 Domain Subjectivity LCS Score
We also include a feature reflecting the subjectivity
of the domain of the target sense. Domains are
assigned scores as follows. For domain D and seed
set S:
DomainLCSscore(D,S) =
aved?D?SMemLCSscore(d,D, S)
where:
MemLCSscore(d,D, S) =
max
di?D?S,di 6=d
LCSs(d, di)
The value of this feature for a sense is the score
assigned to that sense?s domain.
6.3 Common Related Senses
This feature is based on the intersection between the
set of senses related (via WordNet relations) to the
target sense and the set of senses related to members
of a seed set. First, for the target sense and each
member of the seed set, a set of related senses is
formed consisting of its synonyms, antonyms and di-
rect hypernyms as defined by WordNet. For a sense
s, R(s) is s together with its related senses.
Then, given a target sense t and a seed set S we
compute an average percentage overlap as follows:
RelOverlap(t, S) =
?
si?S
|R(t)?R(si)|
max (|R(t)|,|R(si)|)
|S|
The value of a feature is its score. Two features
are included in the experiments below, one for each
of the subjective and objective seed sets.
6.4 Gloss-based features
These features are Lesk-style features (Lesk, 1986)
that exploit overlaps between glosses of target and
seed senses. We include two types in our work.
6.4.1 Average Percentage Gloss Overlap
Features
For a sense s, gloss(s) is the set of stems in the
gloss of s (excluding stop words). Then, given a tar-
14
get sense t and a seed set S, we compute an average
percentage overlap as follows:
GlOverlap(t, S) =
?
si?S
|gloss(t)??r?R(si)gloss(r)|
max (|gloss(t)|,|?r?R(si)gloss(r)|)
|S|
As above, R(s) is considered for each seed sense
s, but now only the target sense t is considered, not
R(t). We did this because we hypothesized that the
gloss can provide sufficient context for a given target
sense, so that the addition of related words is not
necessary.
We include two features, one for each of the sub-
jective and objective seed sets.
6.4.2 Vector Gloss Overlap Features
For this feature we also consider overlaps of
stems in glosses (excluding stop words). The over-
laps considered are between the gloss of the tar-
get sense t and the glosses of R(s) for all s in a
seed set (for convenience, we will refer to these as
seedRelationSets).
A vector of stems is created, one for each stem
(excluding stop words) that appears in a gloss of
a member of seedRelationSets. If a stem in the
gloss of the target sense appears in this vector, then
the vector entry for that stem is the total count of
that stem in the glosses of the target sense and all
members of seedRelationSets.
A feature is created for each vector entry whose
value is the count at that position. Thus, these fea-
tures consider counts of individual stems, rather than
average proportions of overlaps, as for the previous
type of gloss feature.
Two vectors of features are used, one where the
seed set is the subjective seed set, and one where it
is the objective seed set.
6.5 Summary
In summary, we use the following features (here, SS
is the subjective seed set and OS is the objective
one).
1. SenseLCSscore(t,D, SS)
2. DomainLCSscore(D,SS)
3. RelOverlap(t, SS)
4. RelOverlap(t, OS)
5. GlOverlap(t, SS)
6. GlOverlap(t, OS)
Features Acc P R F
All 77.3 72.8 74.3 73.5
Standalone Ablation Results
All 77.3 72.8 74.3 73.5
LCS 68.2 69.3 44.2 54.0
Gloss vector 74.3 71.2 68.5 69.8
Overlaps 69.4 75.8 40.6 52.9
Leave-One-Out Ablation Results
All 77.3 72.8 74.3 73.5
LCS 75.2 70.9 70.6 70.7
Gloss vector 75.0 74.4 61.8 67.5
Overlaps 74.8 71.9 73.8 72.8
Table 1: Results for the mixed corpus (2354 senses,
57.82% O))
7. Vector of gloss words (SS)
8. Vector of gloss words (OS)
7 Experiments
We perform 10-fold cross validation experiments
on several data sets, using SVM light (Joachims,
1999)4 under its default settings.
Based on our random sampling of WordNet, it
appears that WordNet nouns are highly skewed to-
ward objective senses. (Esuli and Sebastiani, 2007)
argue that random sampling from WordNet would
yield a corpus mostly consisting of objective (neu-
tral) senses, which would be ?pretty useless as a
benchmark for testing derived lexical resources for
opinion mining [p. 428].? So, they use a mixture of
subjective and objective senses in their data set.
To create a mixed corpus for our task, we anno-
tated a second random sample from WordNet (which
is as skewed as the previously mentioned one). We
added together all of the senses of words in the lexi-
con which we annotated, the leftover senses from the
selection of objective seed senses, and this new sam-
ple. We removed duplicates, multiple senses from
the same synset, and any senses belonging to the
same synset in either of the seed sets. This resulted
in a corpus of 2354 senses, 993 (42.18%) of which
are subjective and 1361 (57.82%) of which are ob-
jective.
The results with all of our features on this mixed
corpus are given in Row 1 of Table 1. In Table 1, the
4http://svmlight.joachims.org/
15
first column identifies the features, which in this case
is all of them. The next three columns show overall
accuracy, and precision and recall for finding sub-
jective senses. The baseline accuracy for the mixed
data set (guessing the more frequent class, which is
objective) is 57.82%. As the table shows, the accu-
racy is substantially above baseline.5
7.1 Analysis and Discussion
In this section, we seek to gain insights by perform-
ing ablation studies, evaluating our method on dif-
ferent data compositions, and comparing our results
to previous results.
7.2 Ablation Studies
Since there are several features, we divided them
into sets for the ablation studies. The vector-of-
gloss-words features are the most similar to ones
used in previous work. Thus, we opted to treat
them as one ablation group (Gloss vector). The
Overlaps group includes the RelOverlap(t, SS),
RelOverlap(t, OS), GlOverlap(t, SS), and
GlOverlap(t, OS) features. Finally, the LCS
group includes the SenseLCSscore and the
DomainLCSscore features.
There are two types of ablation studies. In the
first, one group of features at a time is included.
Those results are in the middle section of Table 1.
Thus, for example, the row labeled LCS in this sec-
tion is for an experiment using only the LCS fea-
tures. In comparison to performance when all fea-
tures are used, F-measure for the Overlaps and LCS
ablations is significantly different at the p < .01
level, and, for the Gloss Vector ablation, it is sig-
nificantly different at the p = .052 level (one-tailed
t-test). Thus, all of the features together have better
performance than any single type of feature alone.
In the second type of ablation study, we use all
the features minus one group of features at a time.
The results are in the bottom section of Table 1.
Thus, for example, the row labeled LCS in this sec-
tion is for an experiment using all but the LCS fea-
tures. F-measures for LCS and Gloss vector are sig-
nificantly different at the p = .056 and p = .014 lev-
els, respectively. However, F-measure for the Over-
laps ablation is not significantly different (p = .39).
5Note that, because the majority class is O, baseline recall
(and thus F-measure) is 0.
Data (#senses) Acc P R F
mixed (2354 57.8% O) 77.3 72.8 74.3 73.5
strong+weak (1132) 77.7 76.8 78.9 77.8
weaksubj (566) 71.3 70.3 71.1 70.7
strongsubj (566) 78.6 78.8 78.6 78.7
Table 2: Results for different data sets (all are 50% S,
unless otherwise notes)
These results provide evidence that LCS and Gloss
vector are better together than either of them alone.
7.3 Results on Different Data Sets
Several methods have been developed for identify-
ing subjective words. Perhaps an effective strategy
would be to begin with a word-level subjectivity lex-
icon, and then perform subjectivity sense labeling
to sort the subjective from objective senses of those
words. We also wondered about the relative effec-
tiveness of our method on strongsubj versus weak-
subj clues.
To answer these questions, we apply the full
model (again in 10-fold cross validation experi-
ments) to data sets composed of senses of polyse-
mous words in the subjectivity lexicon. To support
comparison, all of the data sets in this section have
a 50%-50% objective/subjective distribution.6 The
results are presented in Table 2.
For comparison, the first row repeats the results
for the mixed corpus from Table 1. The second
row shows results for a corpus of senses of a mix-
ture of strongsubj and weaksubj words. The corpus
was created by selecting a mixture of strongsubj and
weaksubj words, extracting their senses and the S/O
labels applied to them in Section 4, and then ran-
domly removing senses of the more frequent class
until the distribution is uniform. We see that the
results on this corpus are better than on the mixed
data set, even though the baseline accuracy is lower
and the corpus is smaller. This supports the idea
that an effective strategy would be to first identify
opinion-bearing words, and then apply our method
to those words to sort out their subjective and objec-
tive senses.
The third row shows results for a weaksubj subset
6As with the mixed data set, we removed from these data
sets multiple senses from the same synset and any senses in the
same synset in either of the seed sets.
16
Method P R F
Our method 56.8 66.0 61.1
WM, 60% recall 44.0 66.0 52.8
SentiWordNet mapping 60.0 17.3 26.8
Table 3: Results for WM Corpus (212 senses, 76% O)
Method A P R F
Our Method 81.3% 60.3% 63.3% 61.8%
SM CV* 82.4% 70.8% 41.1% 52.0%
SM SL* 78.3% 53.0% 57.4% 54.9%
Table 4: Results for SM Corpus (484 senses, 76.9% O)
of the strong+weak corpus and the fourth shows re-
sults for a strongsubj subset that is of the same size.
As expected, the results for the weaksubj senses
are lower while those for the strongsubj senses are
higher, as weaksubj clues are more ambiguous.
7.4 Comparisons with Previous Work
WM and SM address the same task as we do. To
compare our results to theirs, we apply our full
model (in 10-fold cross validation experiments) to
their data sets.7
Table 3 has the WM data set results. WM rank
their senses and present their results in the form of
precision recall curves. The second row of Table 3
shows their results at the recall level achieved by our
method (66%). Their precision at that level is sub-
stantially below ours.
Turning to ES, to create S/O annotations, we ap-
plied the following heuristic mapping (which is also
used by SM for the purpose of comparison): any
sense for which the sum of positive and negative
scores is greater than or equal to 0.5 is S, otherwise
it is O. We then evaluate the mapped tags against the
gold standard of WM. The results are in Row 3 of
Table 3. Note that this mapping is not fair to Sen-
tiWordNet, as the tasks are quite different, and we
do not believe any conclusions can be drawn. We
include the results to eliminate the possibility that
their method is as good ours on our task, despite the
differences between the tasks.
Table 4 has the results for the noun subset of SM?s
7The WM data set is available at
http://www.cs.pitt.edu/www.cs.pitt.edu/?wiebe. ES applied
their method in (2006b) to WordNet, and made the results
available as SentiWordNet at http://sentiwordnet.isti.cnr.it/.
data set, which is the data set used by ES, reanno-
tated by SM. CV* is their supervised system and
SL* is their best non-supervised one. Our method
has higher F-measure than the others.8 Note that the
focus of SM?s work is not supervised machine learn-
ing.
8 Conclusions
In this paper, we introduced an integrative approach
to automatic subjectivity word sense labeling which
combines features exploiting the hierarchical struc-
ture and domain information of WordNet, as well
as similarity of glosses and overlap among sets
of semantically related words. There are several
contributions. First, we learn several things. We
found (in Section 4) that even reliable lists of sub-
jective (opinion-bearing) words have many objec-
tive senses. We asked if word- and sense-level ap-
proaches could be used effectively in tandem, and
found (in Section 7.3) that an effective strategy is to
first identify opinion-bearing words, and then apply
our method to sort out their subjective and objective
senses. We also found (in Section 7.2) that the entire
set of features gives better results than any individ-
ual type of feature alone.
Second, several of the features are novel for
our task, including those exploiting the hierarchical
structure of a lexical resource, domain information,
and relations to seed sets expanded with monose-
mous senses.
Finally, the combination of our particular features
is effective. For example, on senses of words from
a subjectivity lexicon, accuracies range from 20 to
29 percentage points above baseline. Further, our
combination of features outperforms previous ap-
proaches.
Acknowledgments
This work was supported in part by National Sci-
ence Foundation awards #0840632 and #0840608.
The authors are grateful to Fangzhong Su and Katja
Markert for making their data set available, and to
the three paper reviewers for their helpful sugges-
tions.
8We performed the same type of evaluation as in SM?s paper.
That is, we assign a subjectivity label to one word sense for each
synset, which is the same as applying a subjectivity label to a
synset as a whole as done by SM.
17
References
Alina Andreevskaia and Sabine Bergler. 2006a. Mining
wordnet for a fuzzy sentiment: Sentiment tag extrac-
tion from wordnet glosses. In Proceedings of the 11rd
Conference of the European Chapter of the Associa-
tion for Computational Linguistics.
Alina Andreevskaia and Sabine Bergler. 2006b. Sen-
timent tag extraction from wordnet glosses. In Pro-
ceedings of 5th International Conference on Language
Resources and Evaluation.
Rebecca Bruce and Janyce Wiebe. 1999. Recognizing
subjectivity: A case study of manual tagging. Natural
Language Engineering, 5(2):187?205.
S. Cerini, V. Campagnoni, A. Demontis, M. Formentelli,
and C. Gandini. 2007. Micro-wnop: A gold standard
for the evaluation of automatically compiled lexical re-
sources for opinion mining. In Language resources
and linguistic theory: Typology, second language ac-
quisition, English linguistics. Milano.
Andrea Esuli and Fabrizio Sebastiani. 2006a. Determin-
ing term subjectivity and term orientation for opinion
mining. In 11th Meeting of the European Chapter of
the Association for Computational Linguistics.
Andrea Esuli and Fabrizio Sebastiani. 2006b. Senti-
WordNet: A publicly available lexical resource for
opinion mining. In Proceedings of the 5th Conference
on Language Resources and Evaluation, Genova, IT.
Andrea Esuli and Fabrizio Sebastiani. 2007. PageRank-
ing wordnet synsets: An application to opinion min-
ing. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 424?
431, Prague, Czech Republic, June.
A. Gliozzo, C. Strapparava, E. d?Avanzo, and
B. Magnini. 2005. Automatic acquisition of
domain specific lexicons. Tech. report, IRST, Italy.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scholkopf, C. Burgess, and A. Smola,
editors, Advances in Kernel Methods ? Support Vector
Learning, Cambridge, MA. MIT-Press.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In Proceedings of the Twentieth
International Conference on Computational Linguis-
tics, pages 1267?1373, Geneva, Switzerland.
Soo-Min Kim and Eduard Hovy. 2006. Identifying
and analyzing judgment opinions. In Proceedings of
Empirical Methods in Natural Language Processing,
pages 200?207, New York.
M.E. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
SIGDOC Conference 1986, Toronto, June.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics , pages 271?278, Barcelona, ES. Association for
Computational Linguistics.
Philip Resnik. 1995. Using information content to eval-
uate semantic similarity in a taxonomy. In Proc. Inter-
national Joint Conference on Artificial Intelligence.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Conference on
Empirical Methods in Natural Language Processing,
pages 105?112.
Fangzhong Su and Katja Markert. 2008. From word
to sense: a case study of subjectivity recognition. In
Proceedings of the 22nd International Conference on
Computational Linguistics, Manchester.
M. Taboada, C. Anthony, and K. Voll. 2006. Methods
for creating semantic orientation databases. In Pro-
ceedings of 5th International Conference on Language
Resources and Evaluation .
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2006. Latent variable models for semantic orienta-
tions of phrases. In Proceedings of the 11th Meeting
of the European Chapter of the Association for Com-
putational Linguistics , Trento, Italy.
P. Turney. 2002. Thumbs up or thumbs down? semantic
orientation applied to unsupervised classification of re-
views. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics, pages
417?424, Philadelphia.
Alessandro Valitutti, Carlo Strapparava, and Oliviero
Stock. 2004. Developing affective lexical resources.
PsychNology Journal, 2(1):61?83.
J. Wiebe and R. Mihalcea. 2006. Word sense and subjec-
tivity. In Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics, Sydney, Aus-
tralia.
Janyce Wiebe and Ellen Riloff. 2005. Creating sub-
jective and objective sentence classifiers from unan-
notated texts. In Proceedings of the 6th International
Conference on Intelligent Text Processing and Com-
putational Linguistics , pages 486?497, Mexico City,
Mexico.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the Human Lan-
guage Technologies Conference/Conference on Empir-
ical Methods in Natural Language Processing , pages
347?354, Vancouver, Canada.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Conference on Empirical Methods in Nat-
ural Language Processing , pages 129?136, Sapporo,
Japan.
18
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 269?278,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Iterative Constrained Clustering for Subjectivity Word Sense
Disambiguation
Cem Akkaya, Janyce Wiebe
University of Pittsburgh
Pittsburgh PA, 15260, USA
{cem,wiebe}@cs.pitt.edu
Rada Mihalcea
University of North Texas
Denton TX, 76207, USA
rada@cs.unt.edu
Abstract
Subjectivity word sense disambiguation
(SWSD) is a supervised and application-
specific word sense disambiguation task
disambiguating between subjective and
objective senses of a word. Not sur-
prisingly, SWSD suffers from the knowl-
edge acquisition bottleneck. In this work,
we use a ?cluster and label? strategy to
generate labeled data for SWSD semi-
automatically. We define a new algo-
rithm called Iterative Constrained Cluster-
ing (ICC) to improve the clustering purity
and, as a result, the quality of the gener-
ated data. Our experiments show that the
SWSD classifiers trained on the ICC gen-
erated data by requiring only 59% of the
labels can achieve the same performance
as the classifiers trained on the full dataset.
1 Introduction
Subjectivity lexicons (e.g., (Turney, 2002;
Whitelaw et al., 2005; Riloff and Wiebe, 2003; Yu
and Hatzivassiloglou, 2003; Kim and Hovy, 2004;
Bloom et al., 2007; Andreevskaia and Bergler,
2008; Agarwal et al., 2009)) play an important
role in opinion, sentiment, and subjectivity
analysis. These systems typically look for the
presence of clues in text. Recently, in (Akkaya
et al., 2009), we showed that subjectivity clues
are fairly ambiguous as to whether they express
subjectivity or not ? words in such lexicons may
have both subjective and objective usages. We
call this problem subjectivity sense ambiguity.
Consider the following sentence containing the
clue ?attack?:
(1) He was attacked by Milosevic for at-
tempting to carve out a new party from the
Socialists.
Knowing that ?attack? is a subjectivity clue with
negative polarity will help a system recognize the
negative sentiment in the sentence. But for (2), the
same information is simply misleading, because
the clue is used with an objective meaning.
(2) A new treatment based on training T-cells
to attack cancerous cells ...
Any opinion analysis system which relies on a
subjectivity lexicon will be misled by subjectiv-
ity clues used with objective senses (false hits).
In (Akkaya et al., 2009), we introduced the task,
Subjectivity Word Sense Disambiguation, which is
to automatically determine which word instances
in a corpus are being used with subjective senses,
and which are being used with objective senses.
SWSD can be considered as a coarse-grained
and application-specific word sense disambigua-
tion task. We showed that sense subjectivity in-
formation about clues can be fed to subjectiv-
ity and sentiment analysis resulting in substantial
improvement for both subjectivity and sentiment
analysis by avoiding false hits.
Although SWSD is a promising tool, it suf-
fers from the knowledge acquisition bottleneck.
SWSD is defined as a supervised task, and fol-
lows a targeted approach common in the WSD lit-
erature for performance reasons. This means, for
each target clue, a different classifier is trained re-
quiring separate training data for each target clue.
It is expensive and time-consuming to obtain an-
notated datasets to train SWSD classifiers limit-
ing scalability. As a countermeasure, in (Akkaya
et al., 2011), we showed that non-expert annota-
tions collected through Amazon Mechanical Turk
(MTurk) can replace expert annotations success-
fully and might be used to apply SWSD on a large
scale.
Although non-expert annotations are cheap and
fast, they still incur some cost. In this work, we
aim to reduce the human annotation effort needed
269
to generate the same amount of subjectivity sense
tagged data by using a ?cluster and label? strategy.
We hypothesize that we can obtain large sets of
labeled data by labelling clusters of instances of a
target word instead of single instances.
The main contribution of this work is a novel
constrained clustering algorithm called Iterative
Constrained Clustering (ICC) utilizing an active
constraint selection strategy. A secondary con-
tribution is a mixed word representation that is a
combination of previously proposed context rep-
resentations. We show that a ?cluster and label?
strategy relying on these two proposed compo-
nents generates training data of good purity. The
resulting data has sufficient purity to train reli-
able SWSD classifiers. SWSD classifiers trained
on only 59% of the data achieve the same perfor-
mance as classifiers trained on 100% of the data,
resulting in a significant reduction in the annota-
tion effort. Our results take SWSD another step
closer to large scale application.
2 Cluster and Label
Our approach is inspired by a method lexicogra-
phers commonly employ to create sense invento-
ries, where they create inventories based on ev-
idence found in corpora. They use concordance
information to mine frequent usage patterns. (Kil-
garriff, 1997) describes this process in detail. A
lexicographer collects usages of a word in cor-
pora and groups them into coherent sets. The in-
stances in a set should have more in common with
each other than with the instances in other sets,
according to the criteria the lexicographer consid-
ers. After generating the sets, the lexicographer
codes each set as a dictionary definition based on
the common attributes of the instances. Our goal
is similar. Instead of generating dictionary defini-
tions, we are only interested in generating coher-
ent sets of usages of a word, so that we can label
each induced set ? with its instances ? to obtain
labeled data for SWSD. Our high-level grouping
criterion is that the instances in a cluster should be
similar subjective (objective) usages of the word.
Training data for an SWSD classifier consists
of instances of the target word tagged as having
a subjective sense (S) or an objective sense (O)
(subjectivity sense tagged data). We train a dif-
ferent SWSD classifier for each target word as in
(Akkaya et al., 2009). Thus, we need a different
training dataset for each target word. Our ultimate
goal is to reduce the human annotation effort re-
quired to create training data for SWSD classifiers.
For this purpose, we utilize a ?cluster and label?
strategy relying on context clustering. Each in-
stance of a word is represented as a feature vector
(i.e., a context vector). The annotation process has
the following steps: (1) cluster the context vectors
of word instances, (2) label the induced clusters
as S or O, (3) propagate the given label to all in-
stances in a cluster.
The induced clusters represent different usage
patterns of a word. Thus, we build more than two
clusters, even though SWSD is a binary task. This
implies that two different instances of a word can
both be subjective, but end up in different clusters,
if they are different usages of the word.
Since we are labelling clusters as a whole, we
will introduce noise in the labeled data. Thus, in
developing the clustering process, we need to min-
imize that noise and find as pure clusters as possi-
ble.
The first step is to define the context representa-
tion of the instances. This is addressed in Section
3. Then, we turn in Section 4.2 to the clustering
process itself.
To evaluate our ?cluster and label? strategy, we
use two gold standard subjectivity sense tagged
datasets.
1
. The first one is called senSWSD gen-
erated in (Akkaya et al., 2009) and the second
one is called mturkSWSD generated in (Akkaya
et al., 2011). They consist of subjectivity sense
tagged data for disjoint sets of 39 and 90 words,
respectively. In this paper, we opt to use the
smaller dataset senSWSD as our development set,
on which we evaluate various context representa-
tions (in Section 3) and our proposed constrained
clustering algorithm (in Section 4.2). Then, on
mturkSWSD, we evaluate the quality of semi-
automatically generated data for SWSD classifi-
cation (in Section 4.3.2).
3 Context Representations
There has been much work on context representa-
tions of words for various NLP tasks. Clustering
word instances in order to discriminate senses of
a word is called Word Sense Discrimination. Con-
text representations for this task rely on two main
types of models: distributional semantic models
(DSM) and feature-based models.
1
Available at http://mpqa.cs.pitt.edu/
corpora
270
(Schutze, 1998), which is still a competi-
tive model for word-sense discrimination by con-
text clustering, relies on a distributional semantic
model (DSM) (Turney and Pantel, 2010; Sahlgren,
2006; Bullinaria and Levy, 2007). A DSM is usu-
ally a word-to-word co-occurrence matrix ? also
called semantic space ? such that each row repre-
sents the distribution of a target word in a large
text corpus. Each row gives the semantic sig-
nature of a word, which is basically a high di-
mensional numeric vector. Note that this high di-
mensional vector represents word types, not word
tokens. Thus, it cannot model a word instance
in context. For token-based treatment, (Schutze,
1998) utilizes a second-order representation by av-
eraging co-occurrence vectors of the words (cor-
responding to rows of the co-occurrence matrix)
that occur in that particular context. It is impor-
tant to note that (Schutze, 1998) uses an addi-
tive model for compositional representation. Re-
cently, in (Akkaya et al., 2012), we found that a
DSM built using multiplicative composition ? pro-
posed by (Mitchell and Lapata, 2010) for a differ-
ent task ? gives better performance than the model
described by (Schutze, 1998).
We test both methods in this paper, using the
same semantic space. The space is built from a
corpus consisting of 120 million tokens. The rows
of the space correspond to word forms and the
columns correspond to word lemmas present in the
corpus. We adopt the parameters for our semantic
space from (Mitchell and Lapata, 2010): window
size of 10 and dimension size of 2000 (i.e., the
2000 most frequent lemmas). We do not filter out
stop words, since they have been shown to be use-
ful for various semantic similarity tasks in (Bulli-
naria and Levy, 2007). We use positive point-wise
mutual information to compute values of the vec-
tor components, which has also been shown to be
favourable in (Bullinaria and Levy, 2007).
Purandere and Pedersen is the prominent repre-
sentative of feature-based models. (Purandare and
Pedersen, 2004) creates context vectors from local
feature representations similar to the feature vec-
tors found in supervised WSD. In this work, we
use the following features from (Mihalcea, 2002)
to build the local feature representation: (1) the
target word itself and its part of speech, (2) sur-
rounding context of 3 words and their part of
speech, (3) the head of the noun phrase, (4) the
first noun and verb before the target word, (5) the
first noun and verb after the target word.
skew local dsm add dsm mul mix rep
average 79.90 80.50 80.50 83.53 85.23
appear-v 53.83 54.85 54.85 57.40 69.39
fine-a 70.07 72.26 70.07 74.45 75.18
interest-n 54.41 54.78 55.88 81.62 81.62
restraint-n 70.45 71.97 75.00 71.21 81.82
Table 1: Evaluation of Various Context Representations
3.1 Evaluation of Context Representations
In this section, we evaluate context representations
for the context clustering task on the subjectivity
sense tagged data, senSWSD. The evaluation is
done separately for each word.
We use the same clustering algorithm for all
context representations: agglomerative hierarchi-
cal clustering with average linkage criteria. In all
our experiments throughout the paper, we fix the
cluster size to 7 as it is done in (Purandare and
Pedersen, 2004). We think that is reasonable num-
ber since SENSEVAL III reports that the average
number of senses per word is 6.47. We choose
cluster purity as our evaluation metric. To com-
pute cluster purity, we assign each cluster to a
sense label, which is the most frequent one in the
cluster. The number of the correctly assigned in-
stances divided by the number of all the clustered
instances gives us cluster purity.
Row 1 of Table 1 holds the cumulative results
over all the words in senSWSD (micro averages).
The table also reports detailed results for 4 sample
selected words from senSWSD. skew stands for
the percentage of the most frequent label. dsm add
is the representation based on (Schutze, 1998),
dsm mul stands for the representation as described
in (Akkaya et al., 2012) and local features is the
local feature representation based on (Purandare
and Pedersen, 2004). The results show that among
dsm mul, dsm add, and local features; dsm mul
performs the best.
When we look at the context clustering re-
sults for single words separately, we observe
that the performance of different representations
vary. There is not a single winner among all
words. Thus, perhaps choosing one single repre-
sentation for all the words is not optimal. Hav-
ing that in mind, we try merging the dsm mul
and local features representations. We leave out
dsm add representation, since both dsm mul and
dsm add rely on the same type of semantic infor-
mation (i.e., a DSM). We hypothesize that the two
271
representations, one relying on a semantic space
and the other relying on local WSD features, may
complement each other.
To merge the representations, we concatenate
the two feature vectors into one. First, however,
we normalize each vector to unit length, since the
individual vectors have different scales and would
have unequal contribution, otherwise. We call this
mixed representation mix rep.
In Table 1, we see that, overall, mix rep per-
forms better than all the other representations. The
improvement is statistically significant at the p <
.05 level on a paired t-test. We observe that, even
when mix rep does not perform the best, it is never
bad. mix rep is the winner or ties for the winner
for 25 out of 39 words. This number is 13, 13, and
15 for dsm add, dsm mul and local features, re-
spectively. For the words for which mix rep is not
the winner, it is, on average, 1.47 points lower than
the winner. This number is 4.22, 6.83, and 7.07
for the others. The results provide evidence that
mix rep is consistently good and reliable. Thus, in
our experiments, mix rep will be our choice as the
context representation.
4 Clustering Process
We now turn to the clustering process. In a ?clus-
ter and label? strategy, in order to be able to label
clusters, we need to annotate some of the instances
in each cluster. Then, we can accept the majority
label found in a cluster as its label. Thus, some
manual labelling is required, preferably a small
amount.
We propose to provide this small amount of an-
notated data prior to clustering, and then perform
semi-supervised clustering. This way the provided
labels will guide the clustering algorithm to gener-
ate the clusters that are more suitable for our end
task, namely clusters where subjective and objec-
tive instances are grouped together.
4.1 Constrained Clustering
Constrained clustering (Grira et al., 2004) also
known as semi-supervised clustering is a recent
development in the clustering literature. In addi-
tion to the similarity information required by un-
supervised clustering, constrained clustering re-
quires pairwise constraints. There are two types
of constraints: (1) must-link and (2) cannot-link
constraints. A must-link constraint dictates that
two instances should be in the same cluster and a
cannot-link dictates that two instances should not
be in the same cluster. In this work, we only con-
sider cannot-links, because of the definition of our
SWSD task. Two instances sharing the same label
do not need to be in the same cluster, since the in-
duced clusters represent different usage patterns of
a word. For example, two instances labeled S need
not be similar to each other. They can be different
usages, both having a subjective meaning. On the
other hand, if two instances are labeled having op-
posing labels, we do not want them to be in the
same cluster. Thus, we utilize cannot-links but not
must-links.
Constraints can be obtained from domain
knowledge or from available instance labels. In
our work, constraints are generated from instance
labels. Each instance pair with opposing labels is
considered to be cannot-linked.
There are two general strategies to incorporate
constraints into clustering. The first is to adapt
the similarity between instances (Xing et al., 2002;
Klein et al., 2002) by adjusting the underlying dis-
tance metric. The main idea is to make the dis-
tance between must-linked instances ? their neigh-
bourhoods ? smaller and the distance between
cannot-linked instances ? their neighbourhoods ?
larger. The second strategy is modifying the clus-
tering algorithm itself so that search is biased to-
wards a partitioning for which the constraints hold
(Wagstaff and Cardie, 2000; Basu et al., 2002;
Demiriz et al., 1999).
Our proposed constrained clustering method re-
lies on some ideas from (Klein et al., 2002). Thus,
we explain it in more detail. (Klein et al., 2002)
utilizes agglomerative hierarchical clustering with
complete-linkage. The algorithm imposes con-
straints by changing the distance matrix accord-
ing to the given constraints. The distances be-
tween must-linked instances are set to 0. That is
not enough by itself, since if two instances are
must-linked, other instances close to them should
also get closer to each other. This means there is
a need to propagate the constraints. This is done
by calculating the shortest path between all the in-
stances and updating the distance matrix accord-
ingly. To impose cannot-links, the distance be-
tween two cannot-linked instances is set to some
large number. The complete-linkage property
indirectly propagates the cannot-link constraints,
since it will not allow two clusters to be merged if
they contain instances that are cannot-linked.
Although previous work report on average sub-
272
stantial improvement in the clustering purity,
(Davidson et al., 2006) shows that even if the
constraints are generated from gold-standard data,
some constraint sets can decrease clustering pu-
rity. The results vary significantly depending on
the specific set of constraints used. To our knowl-
edge, there have been two approaches for select-
ing informative constraint sets (Basu et al., 2004;
Klein et al., 2002). The method described in
(Basu et al., 2004) uses the farthest-first traversal
scheme. That strategy is not suitable in our setting,
since we have only two labels. After selecting
just one instance from both labels, this method be-
comes the same as random selection. The strategy
described in (Klein et al., 2002) is more general.
At first, the hierarchical clustering algorithm fol-
lows in a unconstrained fashion until some moder-
ate number of clusters are remaining. Then, the al-
gorithm starts to request constraints between roots
whenever two clusters are merged.
4.2 Iterative Constrained Clustering
Our proposed algorithm is closely related to (Klein
et al., 2002). We share the same backbone:
(1) the agglomerative hierarchical clustering with
complete-linkage and (2) the mechanism to im-
pose cannot-link constraints described in Section
4.1. For our algorithm, we implement a second
mechanism for imposing constraints proposed by
(Xing et al., 2002) (Section 4.2.1) and use both
mechanisms in combination. We also propose a
novel constraint selection method (Section 4.2.2).
4.2.1 Imposing Constraints
(Klein et al., 2002) imposes cannot-link con-
straints by adjusting the distance between cannot-
linked pairs heuristically and by relying on com-
plete linkage for propagation. Although this ap-
proach was shown to be effective, we believe it
does not make full use of the provided constraints.
We believe that learning a new distance metric will
result in more reliable distance estimates between
all instances. For this purpose, we learn a Maha-
lanobis distance function following the method de-
scribed in (Davis et al., 2007). (Davis et al., 2007)
formulate the problem of distance metric learn-
ing as minimizing the differential relative entropy
between two multivariate Gaussians under con-
straints. Note that using distance metric learning
for imposing constraints was previously proposed
by (Xing et al., 2002). (Xing et al., 2002) pose
metric learning as a convex optimization problem.
The reason we choose the metric learning method
(Davis et al., 2007) over (Xing et al., 2002) is that
it is computationally more efficient.
(Klein et al., 2002) has a favourable property we
want to keep. The constraints are imposed strictly,
meaning that no cannot-linked instances can ap-
pear in the same cluster. I.e., they are hard con-
straints. In the case of metric learning, the con-
straints are not imposed strictly. In a new learned
distance metric, two cannot-linked instances will
be relatively distant, but there is no guarantee they
will not end up in the same cluster. Although we
think that metric learning makes a better use of
provided constraints, we do not want to lose the
benefit of hard constraints. Thus, we use both
mechanisms in combination to impose constraints.
We first learn a Mahalanobis distance based on the
provided constraints. Then, we compute distance
matrix and employ the mechanism proposed by
(Klein et al., 2002) on the learned distance matrix.
4.2.2 Active Constraint Generation
As mentioned before, the choice of the set of con-
straints affects the quality of the end clustering. In
this work, we define a novel method to choose in-
formative instances, which we believe will have
maximum impact on the end cluster quality, when
they are labeled and used to generate constraints
for our task. We use an iterative approach. Each
iteration consists of three steps: (1) generating
clusters by the process described in Section 4.2.1
imposing available constraints, (2) choosing the
most informative instance, considering the cluster
boundaries, and acquiring its label, (3) extending
the available constraints with the ones we generate
from the newly labeled instance.
We consider an instance to be informative if
there is a high probability that the knowledge of
its label may change the cluster boundaries. The
more probable that change is, the more informa-
tive is the instance. The basic idea is that if an
instance is in a cluster holding instances of type
a and it is close to another cluster holding in-
stances of type b, that instance is most likely mis-
clustered. Thus, it should be queried. Our hypoth-
esis is that, in each iteration, the algorithm will
choose the most problematic ? informative ? in-
stance that will end up changing cluster bound-
aries. This will result in each iteration in a more
reliable distance metric, which in return will pro-
vide more reliable estimates of problematic in-
stances in future iterations. The imposed con-
273
Algorithm 1 Iterative Constrained Clustering
1: C = cluster(I)
2: I
{L}
= labelprototypes(C)
3: while
?
?
I
{L}
?
?
< stop do
4: Con = createconstraints(I
{L}
)
5: Matrix
dist
= learnmetric(I,Con)
6: C = constraintedcluster(Matrix
dist
,Con)
7: L = labelmostinformative(C)
8: I
{L}
= I
{L}
? L
9: end while
10: propagatelabels(I
{L}
, C) {C...Clusters; Con...Constraints;
I...Instances; I
{L}
...Labeled Instances; Matrix
dist
...Distance Matrix}
straints will move the clustering in each iteration
towards better separation of S and O instances.
To define informativeness, we define a scoring
function, which is used to score each data point on
its goodness. The lower the score, the more likely
it is that the instance is mis-clustered. Choosing
the data point with the lowest score will likely
change clustering borders in the next iteration.
Our scoring function is based on the silhouette co-
efficient, a popular unsupervised cluster validation
metric to measure goodness (Tan et al., 2005) of
a cluster member. Basically, the silhouette score
assigns a cluster member that is close to another
cluster a lower score, and a cluster member that
is closer to the cluster center a higher score. That
is partly what we want. In addition, we do not
want to penalize a cluster member that is close to
another cluster having members with the same la-
bel. For this purpose, we calculate the silhouette
score only over clusters with an opposing label
(i.e., holding members with an opposing label). In
addition, we consider only instances labeled so far
when computing the score. We call this new coef-
ficient silh
const
. It is computed as follows: (1) for
an instance i, compute its average distance from
the other instances in its cluster x
i
which are al-
ready labeled, (2) for an instance i, compute its
average distance from the labeled instances of the
clusters from an opposing label and take the mini-
mum of these averages y
i
, (3) compute the silhou-
ette coefficient as (y
i
-x
i
) / max(y
i
,x
i
).
The silh
const
coefficient has favourable proper-
ties. First, it scores members that are close to
a cluster with an opposing label lower than the
members that are close to a cluster with the same
label. According to our definition, these mem-
bers are more informative. Figure 1 holds a sam-
ple cluster setting. The shape of a member de-
notes its label and its fill denotes whether or not it
has been queried. In this example, silh
const
scores
3 1 
2 
Figure 1: Behaviour of selection function
members 2 and 3 lower than 1. Thus, member 1
will not be selected, which is the right decision in
this example. Both members 2 and 3 are close to
clusters with an opposing label. In this example
silh
const
scores member 3 lower, which is farther
away from already labeled members in the clus-
ter. Thus, member 3 will be selected to be labeled.
This type of behaviour results in an explorative
strategy.
The active selection strategy proposed by (Klein
et al., 2002) is single pass. Thus, it does not have
the opportunity to observe the complete cluster
structure before choosing constraints. We hypoth-
esize that our strategy will provide more informa-
tive constraints, since it has the advantage of be-
ing able to base the decision of which constraints
to generate on fully observed cluster structure in
each iteration.
We call our proposed algorithm Iterative Con-
strained Clustering (ICC). In our final implemen-
tation, ICC starts by simply clustering the in-
stances without any constraints. The algorithm
queries the label of the prototypical member ?
the member closest to the cluster center ? of each
cluster. Then, the described iterations begin. Al-
gorithm 1 contains the complete ICC algorithm.
Note that line 6 is equivalent to the algorithm of
(Klein et al., 2002).
4.3 Experiments
This section gives details on experiments to evalu-
ate the purity of the semi-automatically generated
subjectivity sense tagged data by our ?cluster and
label? strategy. We carry out detailed analysis to
quantify the effect of the proposed active selec-
tion strategy and of metric learning on the purity
of the generated data. We compare our active se-
lection strategy to random selection and also to
(Klein et al., 2002). The comparison is done on
the senSWSD dataset. SenSWSD consists of three
subsets, SENSEVAL I,II and III. Since we devel-
274
Figure 2: Label Purity ? ICC vs. random selection
oped our active selection algorithm on the SEN-
SEVAL I subset, we use only SENSEVAL II and
III subsets for comparison. We apply ICC to each
word in the comparison set separately, and report
cumulative results for the purity of the generated
data. We report results for different percentages of
the queried data amount (e.g. 10% means that the
algorithm queried 10% of the data to create con-
straints). This way, we obtain a learning curve.
We fix the cluster number to 7 as in the context
representation experiments.
4.3.1 Effect of Active Selection Strategy
Figure 2 holds the comparison of ICC with
silh
const
selection to a random selection baseline.
?majority? stands for majority label frequency in
the dateset. We see that silh
const
performs better
than the random selection. By providing labels to
only 25% of the data, we can achieve 87.67% pure
fully labeled data.
For comparison, we also evaluate the perfor-
mance of (Klein et al., 2002) with their active con-
straint selection strategy as described in Section
4.1. Note that originally (Klein et al., 2002) re-
quests the constraint between two roots. In our
setting, it requests labels of the roots and then gen-
erates constraints from the obtained labels. Since
we have a binary task, querying labels makes more
sense. This has the advantage that more con-
straints from each request are obtained. More-
over, it allows a direct comparison to our algo-
rithm. (Klein et al., 2002) does not use any metric
learning. Thus, we run our algorithm also without
metric learning, in order to compare the effective-
ness of both active selection strategies fairly. In
Figure 3, we see that silh
const
performs better than
the active selection strategy described in (Klein et
al., 2002). We also see that metric learning results
Figure 3: Label Purity ? ICC vs. Klein
Figure 4: SWSD accuracy on ICC generated data
in a big improvement. In addition, metric learn-
ing results in a smoother learning curve, which is
a favourable property for a real-world application.
4.3.2 SWSD on semi-automatically generated
annotations
Now that we have a tool to generate training data
for SWSD, we want to evaluate it on the actual
SWSD task. We want to see if the obtained purity
is enough to create reliable SWSD classifiers. For
this purpose, we test ICC on mturkSWSD dataset.
For each word in our dataset, we conduct 10-
fold cross-validation experiments. ICC is ap-
plied to training folds to label instances semi-
automatically. We train SWSD classifiers on the
generated training fold labels and test the classi-
fiers on the corresponding test fold. We distin-
guish between queried instances and propagated
labels. The queried instances are weighted as
1 and the instances with propagated labels are
weighted by their silh
const
score, since that mea-
sure gives the goodness of an instance. The score
is defined between -1 and 1. This score is normal-
ized between 0 and 1, before it is used as a weight.
SVM classifiers from the Weka package (Witten
and Frank., 2005) with its default settings are used
275
as in (Akkaya et al., 2011).
We implement two baselines. The first is sim-
ple random sampling and the second is uncer-
tainty sampling, which is an active learning (AL)
method. We use ?simple margin? selection as de-
scribed in (Tong and Koller, 2001). It selects, in
each iteration, the instance closest to the decision
boundary of the trained SVM. Each method is run
until it reaches the accuracy of training fully on
the gold-standard data. ICC reaches that bound-
ary when provided only 59% of the labels in the
dataset. For uncertainty sampling and random
sampling, these values are 92% and 100%, respec-
tively. In Figure 4, we see the SWSD accuracy for
different queried data percentages. ?full? stands
for training fully on gold-standard data. We see
that training SWSD on semi-automatically labeled
data by ICC does consistently better than uncer-
tainty sampling and random sampling.
It is surprising to see that uncertainty sampling
overall does not do better than random sampling.
We believe that it might be because of sampling
bias. During AL, as more and more labels are
obtained, the training set quickly diverges from
the underlying data distribution. (Sch?utze et al.,
2006) states that AL can explore the feature space
in such a biased way that it can end up ignoring en-
tire clusters of unlabeled instances. We think that
SWSD is highly prone for the mentioned missed
cluster problem because of its unique nature. As
mentioned, SWSD is a binary task where we dis-
tinguish between subjective and objective usages
of a subjectivity word. Although the classifica-
tion is binary, the underlying usages are grouped
into multiple clusters corresponding to senses of
the word. It is possible that two groups of usages
which are represented quite differently in the fea-
ture space are both subjective or objective. More-
over, one usage group might be closer to a usage
group from the opposing label than to a group with
the same label.
We see that our method reduces the annotation
amount by 36% in comparison to uncertainty sam-
pling and by 41% in comparison to random sam-
pling to reach the performance of the SWSD sys-
tem trained on fully annotated data.
5 Related Work
One related line of research is constrained clus-
tering also known as semi-supervised clustering
(Xing et al., 2002; Wagstaff and Cardie, 2000;
Grira et al., 2004; Demiriz et al., 1999). It has
been applied to various datasets and tasks such
as image and document categorization. To our
knowledge, we are the first to utilize constrained
clustering for a difficult NLP task.
There have been only two previous works se-
lecting constraints for constrained clustering ac-
tively (Basu et al., 2004; Klein et al., 2002). The
biggest difference of our approach is that it is iter-
ative as opposed to single pass.
Active Learning (AL) (Settles, 2009; Settles
and Craven, 2008; Hwa, 2004; Tong and Koller,
2001) builds another important set of related work.
Our method is inspired by uncertainty sampling.
We accomplish active selection in the clustering
setting.
6 Conclusions
In this paper, we explore a ?cluster and la-
bel? strategy to reduce the human annotation ef-
fort needed to generate subjectivity sense-tagged
data. In order to keep the noise in the semi-
automatically labeled data minimal, we investigate
different feature space types and evaluate their ex-
pressiveness. More importantly, we define a new
algorithm called iterative constrained clustering
(ICC) with an active constraint selection strategy.
We show that we can obtain a fairly reliable la-
beled data when we utilize ICC.
We show that the active selection strategy
we propose outperforms a previous approach by
(Klein et al., 2002) for generating subjectivity
sense-tagged data. Training SWSD classifiers on
ICC generated data improves over random sam-
pling and uncertainty sampling (Tong and Koller,
2001). We can achieve on mturkSWSD 36% an-
notation reduction over uncertainty sampling and
41% annotation reduction over random sampling
in order to reach the performance of SWSD clas-
sifiers trained on fully annotated data.
To our knowledge, this work is the first applica-
tion of constrained clustering to a hard NLP prob-
lem. We showcase the power of constrained clus-
tering. We hope that the same ?cluster and label?
strategy will be applicable to Word Sense Disam-
biguation. This will be part of our future work.
7 Acknowledgments
This material is based in part upon work supported
by National Science Foundation awards #0917170
and #0916046.
276
References
Apoorv Agarwal, Fadi Biadsy, and Kathleen Mckeown.
2009. Contextual phrase-level polarity analysis us-
ing lexical affect scoring and syntactic N-grams. In
Proceedings of the 12th Conference of the Euro-
pean Chapter of the ACL (EACL 2009), pages 24?
32, Athens, Greece, March. Association for Compu-
tational Linguistics.
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2009. Subjectivity word sense disambiguation. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
190?199, Singapore, August. Association for Com-
putational Linguistics.
Cem Akkaya, Janyce Wiebe, Alexander Conrad, and
Rada Mihalcea. 2011. Improving the impact of
subjectivity word sense disambiguation on contex-
tual opinion analysis. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning, pages 87?96, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2012. Utilizing semantic composition in distribu-
tional semantic models for word sense discrimina-
tion and word sense disambiguation. In ICSC, pages
45?51.
Alina Andreevskaia and Sabine Bergler. 2008. When
specialists and generalists work together: Over-
coming domain dependence in sentiment tagging.
In Proceedings of ACL-08: HLT, pages 290?298,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Sugato Basu, Arindam Banerjee, and R. Mooney.
2002. Semi-supervised clustering by seeding. In
In Proceedings of 19th International Conference on
Machine Learning (ICML-2002).
Sugato Basu, Arindam Banerjee, and Raymond J.
Mooney. 2004. Active semi-supervision for pair-
wise constrained clustering. In SDM.
Kenneth Bloom, Navendu Garg, and Shlomo Argamon.
2007. Extracting appraisal expressions. In HLT-
NAACL 2007, pages 308?315, Rochester, NY.
John Bullinaria and Joseph Levy. 2007. Ex-
tracting semantic representations from word
co-occurrence statistics: A computational
study. Behavior Research Methods, 39:510?526.
10.3758/BF03193020.
Ian Davidson, Kiri Wagstaff, and Sugato Basu. 2006.
Measuring constraint-set utility for partitional clus-
tering algorithms. In PKDD, pages 115?126.
Jason V. Davis, Brian Kulis, Prateek Jain, Suvrit Sra,
and Inderjit S. Dhillon. 2007. Information-theoretic
metric learning. In Proceedings of the 24th interna-
tional conference on Machine learning, ICML ?07,
pages 209?216, New York, NY, USA. ACM.
Ayhan Demiriz, Kristin Bennett, and Mark J. Em-
brechts. 1999. Semi-supervised clustering using
genetic algorithms. In In Artificial Neural Networks
in Engineering (ANNIE-99, pages 809?814. ASME
Press.
Nizar Grira, Michel Crucianu, and Nozha Boujemaa.
2004. Unsupervised and semi-supervised cluster-
ing: a brief survey. In in A Review of Ma-
chine Learning Techniques for Processing Multime-
dia Content, Report of the MUSCLE European Net-
work of Excellence.
Rebecca Hwa. 2004. Sample selection for statis-
tical parsing. Comput. Linguist., 30(3):253?276,
September.
Adam Kilgarriff. 1997. I dont believe in word senses.
Computers and the Humanities, 31(2):91?113.
Soo-Min Kim and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the Twentieth International Conference on Compu-
tational Linguistics (COLING 2004), pages 1267?
1373, Geneva, Switzerland.
D. Klein, K. Toutanova, I.T. Ilhan, S.D. Kamvar, and
C. Manning. 2002. Combining heterogeneous clas-
sifiers for word-sense disambiguation. In Proceed-
ings of the ACL Workshop on ?Word Sense Dis-
ambiguatuion: Recent Successes and Future Direc-
tions, pages 74?80, July.
R. Mihalcea. 2002. Instance based learning with
automatic feature selection applied to Word Sense
Disambiguation. In Proceedings of the 19th Inter-
national Conference on Computational Linguistics
(COLING 2002), Taipei, Taiwan, August.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and sim-
ilarity spaces. In Proceedings of the Conference on
Computational Natural Language Learning (CoNLL
2004), Boston.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP-2003), pages
105?112, Sapporo, Japan.
Magnus Sahlgren. 2006. The Word-Space Model: us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stockholm
University.
Hinrich Sch?utze, Emre Velipasaoglu, and Jan O. Ped-
ersen. 2006. Performance thresholding in practical
text classification. In Proceedings of the 15th ACM
international conference on Information and knowl-
edge management, CIKM ?06, pages 662?671, New
York, NY, USA. ACM.
277
H. Schutze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?124.
Burr Settles and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?08, pages 1070?1079, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Burr Settles. 2009. Active Learning Literature Survey.
Technical Report 1648, University of Wisconsin?
Madison.
Pang-Ning Tan, Michael Steinbach, and Vipin Kumar.
2005. Introduction to Data Mining, (First Edi-
tion). Addison-Wesley Longman Publishing Co.,
Inc., Boston, MA, USA.
Simon Tong and Daphne Koller. 2001. Support vec-
tor machine active learning with applications to text
classification. Journal of Machine Learning Re-
search, 2:45?66.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. March.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th An-
nual Meeting of the Association for Computational
Linguistics (ACL-02), pages 417?424, Philadelphia,
Pennsylvania.
Kiri Wagstaff and Claire Cardie. 2000. Clustering
with instance-level constraints. In Proceedings of
the Seventeenth International Conference on Ma-
chine Learning (ICML-2000), pages 1103?1110.
Casey Whitelaw, Navendu Garg, and Shlomo Arga-
mon. 2005. Using appraisal taxonomies for sen-
timent analysis. In Proceedings of CIKM-05, the
ACM SIGIR Conference on Information and Knowl-
edge Management, Bremen, DE.
I. Witten and E. Frank. 2005. Data Mining: Practi-
cal Machine Learning Tools and Techniques, Second
Edition. Morgan Kaufmann, June.
Eric P. Xing, Andrew Y. Ng, Michael I. Jordan, and
Stuart J. Russell. 2002. Distance metric learning
with application to clustering with side-information.
In NIPS, pages 505?512.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating facts
from opinions and identifying the polarity of opin-
ion sentences. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2003), pages 129?136, Sapporo, Japan.
278
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 195?203,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Amazon Mechanical Turk for Subjectivity Word Sense Disambiguation
Cem Akkaya
University of Pittsburgh
cem@cs.pitt.edu
Alexander Conrad
University of Pittsburgh
conrada@cs.pitt.edu
Janyce Wiebe
University of Pittsburgh
wiebe@cs.pitt.edu
Rada Mihalcea
University of North Texas
rada@cs.unt.edu
Abstract
Amazon Mechanical Turk (MTurk) is a mar-
ketplace for so-called ?human intelligence
tasks? (HITs), or tasks that are easy for hu-
mans but currently difficult for automated pro-
cesses. Providers upload tasks to MTurk
which workers then complete. Natural lan-
guage annotation is one such human intelli-
gence task. In this paper, we investigate us-
ing MTurk to collect annotations for Subjec-
tivity Word Sense Disambiguation (SWSD),
a coarse-grained word sense disambiguation
task. We investigate whether we can use
MTurk to acquire good annotations with re-
spect to gold-standard data, whether we can
filter out low-quality workers (spammers), and
whether there is a learning effect associated
with repeatedly completing the same kind of
task. While our results with respect to spam-
mers are inconclusive, we are able to ob-
tain high-quality annotations for the SWSD
task. These results suggest a greater role for
MTurk with respect to constructing a large
scale SWSD system in the future, promising
substantial improvement in subjectivity and
sentiment analysis.
1 Introduction
Many Natural Language Processing (NLP) systems
rely on large amounts of manually annotated data
that is collected from domain experts. The anno-
tation process to obtain this data is very laborious
and expensive. This makes supervised NLP systems
subject to a so-called knowledge acquisition bottle-
neck. For example, (Ng, 1997) estimates an effort of
16 person years to construct training data for a high-
accuracy domain independent Word Sense Disam-
biguation (WSD) system.
Recently researchers have been investigating
Amazon Mechanical Turk (MTurk) as a source of
non-expert natural language annotation, which is a
cheap and quick alternative to expert annotations
(Kaisser and Lowe, 2008; Mrozinski et al, 2008).
In this paper, we utilize MTurk to obtain training
data for Subjectivity Word Sense Disambiguation
(SWSD) as described in (Akkaya et al, 2009). The
goal of SWSD is to automatically determine which
word instances in a corpus are being used with sub-
jective senses, and which are being used with ob-
jective senses. SWSD is a new task which suffers
from the absence of a substantial amount of anno-
tated data and thus can only be applied on a small
scale. SWSD has strong connections to WSD. Like
supervised WSD, it requires training data where tar-
get word instances ? words which need to be dis-
ambiguated by the system ? are labeled as having
an objective sense or a subjective sense. (Akkaya
et al, 2009) show that SWSD may bring substantial
improvement in subjectivity and sentiment analysis,
if it could be applied on a larger scale. The good
news is that training data for 80 selected keywords is
enough to make a substantial difference (Akkaya et
al., 2009). Thus, large scale SWSD is feasible. We
hypothesize that annotations for SWSD can be pro-
vided by non-experts reliably if the annotation task
is presented in a simple way.
The annotations obtained from MTurk workers
are noisy by nature, because MTurk workers are
not trained for the underlying annotation task. That
is why previous work explored methods to assess
annotation quality and to aggregate multiple noisy
annotations for high reliability (Snow et al, 2008;
Callison-Burch, 2009). It is understandable that not
every worker will provide high-quality annotations,
195
depending on their background and interest. Un-
fortunately, some MTurk workers do not follow the
annotation guidelines and carelessly submit annota-
tions in order to gain economic benefits with only
minimal effort. We define this group of workers
as spammers. We believe it is essential to distin-
guish between workers as well-meaning annotators
and workers as spammers who should be filtered out
as a first step when utilizing MTurk. In this work,
we investigate how well the built-in qualifications in
MTurk function as such a filter.
Another important question about MTurk workers
is whether they learn to provide better annotations
over time in the absence of any interaction and feed-
back. The presence of a learning effect may support
working with the same workers over a long time and
creating private groups of workers. In this work, we
also examine if there is a learning effect associated
with MTurk workers.
To summarize, in this work we investigate the fol-
lowing questions:
? Can MTurk be utilized to collect reliable train-
ing data for SWSD ?
? Are the built-in methods provided by MTurk
enough to avoid spammers ?
? Is there a learning effect associated with MTurk
workers ?
The remainder of the paper is organized as fol-
lows. In Section 2, we give general background in-
formation on the Amazon Mechanical Turk service.
In Section 3, we discuss sense subjectivity. In Sec-
tion 4, we describe the subjectivity word sense dis-
ambiguation task. In Section 5, we discuss the de-
sign of our experiment and our filtering mechanisms
for workers. In Section 6, we evaluate MTurk anno-
tations and relate results to our questions. In Section
7, we review related work. In Section 8, we draw
conclusions and discuss future work.
2 Amazon Mechanical Turk
Amazon Mechanical Turk (MTurk)1 is a market-
place for so-called ?human intelligence tasks,? or
HITs. MTurk has two kinds of users: providers and
1http://mturk.amazon.com
workers. Providers create HITs using the Mechan-
ical Turk API and, for a small fee, upload them to
the HIT database. Workers search through the HIT
database, choosing which to complete in exchange
for monetary compensation. Anyone can sign up as
a provider and/or worker. Each HIT has an associ-
ated monetary value, and after reviewing a worker?s
submission, a provider may choose whether to ac-
cept the submission and pay the worker the promised
sum or to reject it and pay the worker nothing. HITs
typically consist of tasks that are easy for humans
but difficult or impossible for computers to complete
quickly or effectively, such as annotating images,
transcribing speech audio, or writing a summary of
a video.
One challenge for requesters using MTurk is that
of filtering out spammers and other workers who
consistently produce low-quality annotations. In or-
der to allow requesters to restrict the range of work-
ers who can complete their tasks, MTurk provides
several types of built-in statistics, known as quali-
fications. One such qualification is approval rating,
a statistic that records a worker?s ratio of accepted
HITs compared to the total number of HITs sub-
mitted by that worker. Providers can require that a
worker?s approval rating be above a certain threshold
before allowing that worker to submit one of his/her
HITs. Country of residence and lifetime approved
number of HITs completed also serve as built-in
qualifications that providers may check before al-
lowing workers to access their HITs.2 Amazon also
allows providers to define their own qualifications.
Typically, provider-defined qualifications are used to
ensure that HITs which require particular skills are
only completed by qualified workers. In most cases,
workers acquire provider-defined qualifications by
completing an online test.
Amazon also provides a mechanism by which
multiple unique workers can complete the same HIT.
The number of times a HIT is to be completed is
known as the number of assignments for the HIT.
By having multiple workers complete the same HIT,
2According to the terms of use, workers are prohibited from
having more than one account, but to the writer?s knowledge
there is no method in place to enforce this restriction. Thus,
a worker with a poor approval rating could simply create a
new account, since all accounts start with an approval rating
of 100%.
196
Subjective senses:
His alarm grew.
alarm, dismay, consternation ? (fear resulting from the aware-
ness of danger)
=> fear, fearfulness, fright ? (an emotion experienced in an-
ticipation of some specific pain or danger (usually accompa-
nied by a desire to flee or fight))
What?s the catch?
catch ? (a hidden drawback; ?it sounds good but what?s the
catch??)
=> drawback ? (the quality of being a hindrance; ?he
pointed out all the drawbacks to my plan?)
Objective senses:
The alarm went off.
alarm, warning device, alarm system ? (a device that signals the
occurrence of some undesirable event)
=> device ? (an instrumentality invented for a particular pur-
pose; ?the device is small enough to wear on your wrist?; ?a
device intended to conserve water?)
He sold his catch at the market.
catch, haul ? (the quantity that was caught; ?the catch was only
10 fish?)
=> indefinite quantity ? (an estimated quantity)
Figure 1: Subjective and objective word sense examples.
techniques such as majority voting among the sub-
missions can be used to aggregate the results for
some types of HITs, resulting in a higher-quality
final answer. Previous work (Snow et al, 2008)
demonstrates that aggregating worker submissions
often leads to an increase in quality.
3 Word Sense Subjectivity
(Wiebe and Mihalcea, 2006) define subjective ex-
pressions as words and phrases being used to ex-
press mental and emotional states, such as specula-
tions, evaluations, sentiments, and beliefs. Many ap-
proaches to sentiment and subjectivity analysis rely
on lexicons of such words (subjectivity clues). How-
ever, such clues often have both subjective and ob-
jective senses, as illustrated by (Wiebe and Mihal-
cea, 2006). Figure 1 provides subjective and objec-
tive examples of senses.
(Akkaya et al, 2009) points out that most sub-
jectivity lexicons are compiled as lists of keywords,
rather than word meanings (senses). Thus, subjec-
tivity clues used with objective senses ? false hits ?
are a significant source of error in subjectivity and
sentiment analysis. SWSD specifically deals with
this source of errors. (Akkaya et al, 2009) shows
that SWSD helps with various subjectivity and sen-
timent analysis systems by ignoring false hits.
4 Annotation Task
4.1 Subjectivity Word Sense Disambiguation
Our target task is Subjectivity Word Sense Disam-
biguation (SWSD). SWSD aims to determine which
word instances in a corpus are being used with sub-
jective senses and which are being used with ob-
jective senses. It can be considered to be a coarse-
grained application-specific WSD that distinguishes
between only two senses: (1) the subjective sense
and (2) the objective sense.
Subjectivity word sense annotation is done in the
following way. We try to keep the annotation task
for the worker as simple as possible. Thus, we do
not directly ask them if the instance of a target word
has a subjective or an objective sense (without any
sense inventory), because the concept of subjectivity
is fairly difficult to explain to someone who does not
have any linguistics background. Instead we show
MTurk workers two sets of senses ? one subjective
set and one objective set ? for a specific target word
and a text passage in which the target word appears.
Their job is to select the set that best reflects the
meaning of the target word in the text passage. The
specific sense set automatically gives us the subjec-
tivity label of the instance. This makes the annota-
tion task easier for them as (Snow et al, 2008) shows
that WSD can be done reliably by MTurk workers.
This approach presupposes a set of word senses that
have been annotated as subjective or objective. The
annotation of senses in a dictionary for subjectivity
is not difficult for an expert annotator. Moreover,
it needs to be done only once per target word, al-
lowing us to collect hundreds of subjectivity labeled
instances for each target word through MTurk.
In this annotation task, we do not inform the
MTurk workers about the nature of the sets. This
means the MTurk workers have no idea that they are
annotating subjectivity of senses; they are just se-
lecting the set which contains a sense matching the
usage in the sentence or being as similar to it as pos-
sible. This ensures that MTurk workers are not bi-
ased by the contextual subjectivity of the sentence
while tagging the target word instance.
197
Sense Set1 (Subjective)
{ look, appear, seem } ? give a certain impression or have a
certain outward aspect; ?She seems to be sleeping?; ?This ap-
pears to be a very difficult problem?; ?This project looks fishy?;
?They appeared like people who had not eaten or slept for a
long time?
{ appear, seem } ? seem to be true, probable, or apparent; ?It
seems that he is very gifted?; ?It appears that the weather in
California is very bad?
Sense Set2 (Objective)
{ appear } ? come into sight or view; ?He suddenly appeared
at the wedding?; ?A new star appeared on the horizon?
{ appear, come out } ? be issued or published, as of news in a
paper, a book, or a movie; ?Did your latest book appear yet??;
?The new Woody Allen film hasn?t come out yet?
{ appear, come along } ? come into being or existence, or ap-
pear on the scene; ?Then the computer came along and changed
our lives?; ?Homo sapiens appeared millions of years ago?
{ appear } ? appear as a character on stage or appear in a play,
etc.; ?Gielgud appears briefly in this movie?; ?She appeared in
?Hamlet? on the London
{ appear } ? present oneself formally, as before a (judicial) au-
thority; ?He had to appear in court last month?; ?She appeared
on several charges of theft?
Figure 2: Sense sets for target word ?appear?.
Below, we describe a sample annotation problem.
An MTurk worker has access to the following two
sense sets of the target word ?appear?, as seen in
Figure 2. The information that the first sense set is
subjective and second sense set is objective is not
available to the worker. The worker is presented
with the following text passage holding the target
word ?appear?.
It?s got so bad that I don?t even know what
to say. Charles |target| appeared |target|
somewhat embarrassed by his own behav-
ior. The hidden speech was coming, I
could tell.
In this passage, the MTurk worker should be able
to understand that ?appeared? refers to the outward
impression given by ?Charles?. This use of appear is
most similar to the first entry in sense set one; thus,
the correct answer for this problem is Sense Set-1.
4.2 Gold Standard
The gold standard dataset, on which we evaluate
MTurk worker annotations, is provided by (Akkaya
et al, 2009). This dataset (called subjSENSEVAL)
consists of target word instances in a corpus labeled
as S or O, indicating whether they are used with
a subjective or objective sense. It is based on the
lexical sample corpora from SENSEVAL1 (Kilgar-
riff and Palmer, 2000), SENSEVAL2 (Preiss and
Yarowsky, 2001), and SENSEVAL3 (Mihalcea and
Edmonds, 2004). SubjSENSEVAL consists of in-
stances for 39 ambiguous (having both subjective
and objective meanings) target words.
(Akkaya et al, 2009) also provided us with sub-
jectivity labels for word senses which are used in the
creation of subjSENSEVAL. Sense labels of the tar-
get word senses are defined on the sense inventory
of the underlying corpus (Hector for SENSEVAL1;
WordNet1.7 for SENSEVAL2; and WordNet1.7.1
for SENSEVAL3). This means the target words
from SENSEVAL1 have their senses annotated in
the Hector dictionary, while the target words from
SENSEVAL2 and SENSEVAL3 have their senses
annotated in WordNet1.7. We make use of these la-
beled sense inventories to build our subjective and
objective sets of senses, which we present to the
MTurk worker as Sense Set1 and Sense Set2 re-
spectively. We want to have a uniform sense rep-
resentation for the words we ask subjectivity sense
labels for. Thus, we consider only SENSEVAL2 and
SENSEVAL3 subsets of subjSENSEVAL, because
SENSEVAL1 relies on a sense inventory other than
WordNet.
5 Experimental Design
We chose randomly 8 target words that have a distri-
bution of subjective and objective instances in sub-
jSENSEVAL with less skew than 75%. That is, no
more than 75% of a word?s senses are subjective or
objective. Our concern is that using skewed data
might bias the workers to choose from the more fre-
quent label without thinking much about the prob-
lem. Another important fact is that these words with
low skew are more ambiguous and responsible for
more false hits. Thus, these target words are the ones
for which we really need subjectivity word sense
disambiguation. For each of these 8 target words, we
select 40 passages from subjSENSEVAL in which
the target word appears, to include in our experi-
ments. Table 1 summarizes the selected target words
198
Word FLP Word FLP
appear 55% fine 72.5%
judgment 65% solid 55%
strike 62.5% difference 67.5%
restraint 70% miss 50%
Average 62.2%
Table 1: Frequent label percentages for target words.
and their label distribution. In this table, frequent la-
bel percentage (FLP) represents the skew for each
word. A word?s FLP is equal to the percent of the
senses that are of the most frequently occurring type
of sense (subjective or objective) for that word.
We believe this annotation task is a good candi-
date for attracting spammers. This task requires only
binary annotations, where the worker just chooses
from one of the two given sets, which is not a dif-
ficult task. Since it is easy to provide labels, we
believe that there will be a distinct line, with re-
spect to quality of annotations, between spammers
and mediocre annotators.
For our experiments, we created three different
HIT groups each having different qualification re-
quirements but sharing the same data. To be con-
crete, each HIT group consists of the same 320 in-
stances: 40 instances for each target word listed in
Table 1. Each HIT presents an MTurk worker with
four instances of the same word in a text passage
? this makes 80 HITs for each HIT group ? and
asks him to choose the set to which the activated
sense belongs. We know for each HIT the mapping
between sense set numbers and subjectivity. Thus,
we can evaluate each HIT response on our gold-
standard data, as discussed in Section 4.2. We pay
seven cents per HIT. We consider this to be generous
compensation for such a simple task.
There are many builtin qualifications in MTurk.
We concentrated only on three of them: location,
HIT approval rate, and approved HITs, as discussed
in Section 2. In our experience, these qualifications
are widely used for quality assurance. As mentioned
before, we created three different HIT groups in or-
der to see how well different built-in qualification
combinations do with respect to filtering spammers.
These groups ? starting from the least constrained to
the most constrained ? are listed in Table 2.
Group1 Location: USA
Group2 Location: USAHIT Approval Rate > 96%
Group3
Location: USA
HIT Approval Rate > 96%
Approved HITs > 500
Table 2: Constraints for each HIT group.
Group1 required only that the MTurk workers are
located in the US. This group is the least constrained
one. Group2 additionally required an approval rate
greater than 96%. Group3 is the most constrained
one, requiring a lifetime approved HIT number to
be greater than 500, in addition to the qualifications
in Group1 and Group2.
We believe that neither location nor approval rate
and location together is enough to avoid spammers.
While being a US resident does to some extent guar-
antee English proficiency, it does not guarantee well-
thought answers. Since there is no mechanism in
place preventing users from creating new MTurk
worker accounts at will and since all worker ac-
counts are initialized with a 100% approval rate, we
do not think that approval rate is sufficient to avoid
serial spammers and other poor annotators. We hy-
pothesize that the workers with high approval rate
and a large number of approved HITs have a reputa-
tion to maintain, and thus will probably be careful in
their answers. We think it is unlikely that spammers
will have both a high approval rate and a large num-
ber of completed HITs. Thus, we anticipated that
Group3?s annotations will be of higher quality than
those of the other groups.
Note that an MTurk worker who has access to the
HITs in one of the HIT groups also has access to
HITs in less constrained groups. For example, an
MTurk worker who has access to HITs in Group3
also has access to HITs in Group2 and Group1. We
did not prevent MTurk workers from working in
multiple HIT groups because we did not want to
influence worker behavior, but instead simulate the
most realistic annotation scenario.
In addition to the qualifications described above,
we also required each worker to take a qualification
test in order to prove their competence in the anno-
tation task. The qualification test consists of 10 sim-
199
Figure 3: Venn diagram illustrating worker distribution.
ple annotation questions identical in form to those
present in the HITs. These questions are split evenly
between two target words, ?appear? and ?restraint?.
There are a total of five subjective and five objective
usages in the test. We required an accuracy of 90%
in the qualification test, corresponding to a Kappa
score of .80, before a worker was allowed to submit
any of our HITs. If a worker failed to achieve a score
of 90% on an attempt, that worker could try the test
again after a delay of 4 hours.
We collected three sets of assignments within
each HIT group. In other words, each HIT was com-
pleted three times by three different workers in each
group. This gives us a total of 960 assignments in
each HIT group. A total of 26 unique workers par-
ticipated in the experiment: 17 in Group1, 17 in
Group2 and 8 in Group3. As mentioned before, a
worker is able to participate in all the groups for
which he is qualified. Thus the unique worker num-
bers in each group does not sum up to the total num-
ber of workers in the experiment, since some work-
ers participated in the HITs for more than one group.
Figure 3 summarizes how workers are distributed
between groups.
6 Evaluation
We are interested in how accurate the MTurk annota-
tions are with respect to gold-standard data. We are
also interested in how the accuracy of each group
differs from the others. We evaluate each group it-
self separately on the gold-standard data. Addition-
ally, we evaluate each worker?s performance on the
gold-standard data and inspect their distribution in
various groups.
6.1 Group Evaluation
As mentioned in the previous section, we collect
three annotations for each HIT. They are assigned to
respective trials in the order submitted by the work-
ers. The results are summarized in Table 3. Trials
are labeled as TX and MV is the majority vote an-
notation among the three trials. The final column
contains the baseline agreement where a worker la-
bels each instance of a word with the most frequent
label of that word in the gold-standard data. It is
clear from this table that, since worker accuracy
always exceeds the baseline agreement, subjectiv-
ity word sense annotation can be done reliably by
MTurk workers. This is very promising. Consid-
ering the low cost and low time required to obtain
MTurk annotations, a large scale SWSD is realis-
tic. For example, (Akkaya et al, 2009) shows that
the most frequent 80 lexicon keywords are respon-
sible for almost half of the false hits in the MPQA
Corpus3 (Wiebe et al, 2005; Wilson, 2008), a cor-
pus annotated for subjective expressions. Utilizing
MTurk to collect training data for these 80 lexicon
keywords will be quick and cheap and most impor-
tantly reliable.
When we compare groups with each other, we
see that the best trial result is achieved in Group3.
However, according to McNemar?s test (Dietterich,
1998), there is no statistically significant difference
between any trial of any group. On the other hand,
the best majority vote annotation is achieved in
Group2, but again there is no statistically significant
difference between any majority vote annotation of
any group. These results are surprising to us, since
we do not see any significant difference in the qual-
ity of the data throughout different groups.
6.2 Worker Evaluation
In this section, we evaluate all 26 workers and group
them as either spammers or well-meaning workers.
All workers who deviate from the gold-standard by a
3http://www.cs.pitt.edu/mpqa/
200
Group3 Group2 Group1 baseline
T1 T2 T3 MV T1 T2 T3 MV T1 T2 T3 MV
Accuracy 89.7 86.9 86.6 88.4 87.2 86.3 88.1 90.3 84.4 87.5 87.5 88.4 62.2
Kappa .79 .74 .73 .77 .74 .73 .76 .81 .69 .75 .75 .77
Table 3: Accuracy and kappa scores for each group of workers.
Threshold 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75
Spammer Count
G1 2 2 2 2 2 4 7 9
G2 1 2 2 2 2 3 5 8
G3 0 0 0 0 0 0 2 2
Spammer Percentage
G1 12% 12% 12% 12% 12% 24% 41% 53%
G2 6% 12% 12% 12% 12% 12% 29% 42%
G3 0% 0% 0% 0% 0% 0% 25% 25%
Table 4: Spammer representation in groups.
large margin beyond a certain threshold will be con-
sidered to be spammers. As discussed in Section 5,
we require all participating workers to pass a quali-
fication test before answering HITs. Thus, we know
that they are competent to do subjectivity sense an-
notations, and providing consistently erroneous an-
notations means that they are probably spammers.
We think a kappa score of 0.6 is a good threshold
to distinguish spammers from well-meaning work-
ers. For this threshold, we had 2 spammers par-
ticipating in Group1, 2 spammers in Group2 and
0 spammers in Group3. Table 4 presents spammer
count and spammer percentage in each group for
various threshold values. We see that Group3 has
consistently fewer spammers and a smaller spammer
percentage. The lowest kappa scores for Group1,
Group2, and Group3 are .35, .40, and .69, respec-
tively. The mean kappa scores for Group1, Group2,
and Group3 are .73, .75, and .77, respectively.
These results indicate that Group3 is less prone
to spammers, apparently contradicting Section 6.1.
We see the reason when we inspect the data more
closely. It turns out that spammers contributed in
Group1 and Group2 only minimally. On the other
hand there are two mediocre workers (Kappa of
0.69) who submit around 1/3 of the HITs in Group3.
This behavior might be a coincidence. In the face of
contradicting results, we think that we need a more
extensive study to derive conclusions about the rela-
tion between spammer distribution and built-in qual-
ification.
6.3 Learning Effect
Expert annotators can learn to provide more accu-
rate annotations over time. (Passonneau et al, 2006)
reports a learning effect early in the annotation pro-
cess. This might be due to the formal and informal
interaction between annotators. Another possibility
is that the annotators might get used to the annota-
tion task over time. This is to be expected if there is
not an extensive training process before the annota-
tion takes place.
On the other hand, the MTurk workers have no
interaction among themselves. They do not receive
any formal training and do not have access to true
annotations except a few examples if provided by
the requester. These properties make MTurk work-
ers a unique annotation workforce. We are interested
if the learning effect common to expert annotators
holds in this unique workforce in the absence of any
interaction and feedback. That may justify working
with the same set of workers over a long time by
creating private groups of workers.
We sort annotations of a worker after the submis-
sion date. This way, we get for each worker an or-
dered list of annotations. We split the list into bins
of size 40 and we test for an increasing trend in
the proportion of successes over time. We use the
Chi-squared Test for binomial proportions (Rosner,
2006). Using this test, we find that all of the p-values
201
are substantially larger than 0.05. Thus, there is no
increasing trend in the proportion of successes and
no learning effect. This is true for both mediocre
workers and very reliable workers. We think that the
results may differ for harder annotation tasks where
the input is more complex and requires some adjust-
ment.
7 Related Work
There has been recently an increasing interest in
Amazon Mechanical Turk. Many researchers have
utilized MTurk as a source of non-expert natural
language annotation to create labeled datasets. In
(Mrozinski et al, 2008), MTurk workers are used to
create a corpus of why-questions and corresponding
answers on which QA systems may be developed.
(Kaisser and Lowe, 2008) work on a similar task.
They make use of MTurk workers to identify sen-
tences in documents as answers and create a corpus
of question-answer sentence pairs. MTurk is also
considered in other fields than natural language pro-
cessing. For example, (Sorokin and Forsyth, 2008)
utilizes MTurk for image labeling. Our ultimate goal
is similar; namely, to build training data (in our case
for SWSD).
Several studies have concentrated specifically on
the quality aspect of the MTurk annotations. They
investigated methods to assess annotation quality
and to aggregate multiple noisy annotations for high
reliability. (Snow et al, 2008) report MTurk an-
notation quality on various NLP tasks (e.g. WSD,
Textual Entailment, Word Similarity) and define
a bias correction method for non-expert annota-
tors. (Callison-Burch, 2009) uses MTurk workers
for manual evaluation of automatic translation qual-
ity and experiments with weighed voting to com-
bine multiple annotations. (Hsueh et al, 2009) de-
fine various annotation quality measures and show
that they are useful for selecting annotations leading
to more accurate classifiers. Our work investigates
the effect of built-in qualifications on the quality of
MTurk annotations.
(Hsueh et al, 2009) applies MTurk to get senti-
ment annotations on political blog snippets. (Snow
et al, 2008) utilizes MTurk for affective text annota-
tion task. In both works, MTurk workers annotated
larger entities but on a more detailed scale than we
do. (Snow et al, 2008) also provides a WSD anno-
tation task which is similar to our annotation task.
The difference is the MTurk workers are choosing
an exact sense not a sense set.
8 Conclusion and Future Work
In this paper, we address the question of whether
built-in qualifications are enough to avoid spam-
mers. The investigation of worker performances
indicates that the lesser constrained a group is the
more spammers it attracts. On the other hand, we did
not find any significant difference between the qual-
ity of the annotations for each group. It turns out that
workers considered as spammers contributed only
minimally. We do not know if it is just a coincidence
or if it is correlated to the task definition. We did not
get conclusive results. We need to do more extensive
experiments before arriving at conclusions.
Another aspect we investigated is the learning ef-
fect. Our results show that there is no improvement
in annotator reliability over time. We should not ex-
pect MTurk workers to provide more consistent an-
notations over time. This will probably be the case
in similar annotation tasks. For harder annotation
tasks (e.g. parse tree annotation) things may be dif-
ferent. An interesting follow-up would be whether
showing the answers of other workers on the same
HIT will promote learning.
We presented our subjectivity sense annotation
task to the worker in a very simple way. The an-
notation results prove that subjectivity word sense
annotation can be done reliably by MTurk workers.
This is very promising since the MTurk annotations
can be collected for low costs in a short time pe-
riod. This implies that a large scale general SWSD
component, which can help with various subjectivity
and sentiment analysis tasks, is feasible. We plan to
work with selected workers to collect new annotated
data for SWSD and use this data to train a SWSD
system.
Acknowledgments
This material is based in part upon work sup-
ported by National Science Foundation awards IIS-
0916046 and IIS-0917170 and by Department of
Homeland Security award N000140710152. The au-
thors are grateful to the three paper reviewers for
their helpful suggestions.
202
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009.
Subjectivity word sense disambiguation. In Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2009).
Chris Callison-Burch. 2009. Fast, cheap, and creative:
evaluating translation quality using amazon?s mechan-
ical turk. In EMNLP ?09: Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 286?295, Morristown, NJ,
USA. Association for Computational Linguistics.
Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learning
algorithms. Neural Computation, 10:1895?1923.
Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.
2009. Data quality from crowdsourcing: a study of
annotation selection criteria. In HLT ?09: Proceedings
of the NAACL HLT 2009 Workshop on Active Learning
for Natural Language Processing, pages 27?35, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Michael Kaisser and John Lowe. 2008. Creat-
ing a research collection of question answer sen-
tence pairs with amazons mechanical turk. In Pro-
ceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08). http://www.lrec-
conf.org/proceedings/lrec2008/.
Joanna Mrozinski, Edward Whittaker, and Sadaoki Furui.
2008. Collecting a why-question corpus for develop-
ment and evaluation of an automatic QA-system. In
Proceedings of ACL-08: HLT, pages 443?451, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Hwee Tou Ng. 1997. Getting serious about word sense
disambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why,What, and How?
Rebecca Passonneau, Nizar Habash, and Owen Rambow.
2006. Inter-annotator agreement on a multilingual se-
mantic annotation task. In Proceedings of the Fifth
International Conference on Language Resources and
Evaluation (LREC).
Bernard Rosner. 2006. Fundamentals of Biostatistics.
Thompson Brooks/Cole.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In EMNLP ?08: Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 254?263, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
A. Sorokin and D. Forsyth. 2008. Utility data annotation
with amazon mechanical turk. pages 1 ?8, june.
J. Wiebe and R. Mihalcea. 2006. Word sense and subjec-
tivity. In (ACL-06), Sydney, Australia.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation (for-
merly Computers and the Humanities), 39(2/3):164?
210.
Theresa Wilson. 2008. Fine-grained Subjectivity and
Sentiment Analysis: Recognizing the Intensity, Polar-
ity, and Attitudes of private states. Ph.D. thesis, Intel-
ligent Systems Program, University of Pittsburgh.
203
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 87?96,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Improving the Impact of Subjectivity Word Sense Disambiguation on
Contextual Opinion Analysis
Cem Akkaya, Janyce Wiebe, Alexander Conrad
University of Pittsburgh
Pittsburgh PA, 15260, USA
{cem,wiebe,conrada}@cs.pitt.edu
Rada Mihalcea
University of North Texas
Denton TX, 76207, USA
rada@cs.unt.edu
Abstract
Subjectivity word sense disambiguation
(SWSD) is automatically determining which
word instances in a corpus are being used with
subjective senses, and which are being used
with objective senses. SWSD has been shown
to improve the performance of contextual
opinion analysis, but only on a small scale and
using manually developed integration rules.
In this paper, we scale up the integration of
SWSD into contextual opinion analysis and
still obtain improvements in performance,
by successfully gathering data annotated by
non-expert annotators. Further, by improving
the method for integrating SWSD into con-
textual opinion analysis, even greater benefits
from SWSD are achieved than in previous
work. We thus more firmly demonstrate the
potential of SWSD to improve contextual
opinion analysis.
1 Introduction
Often, methods for opinion, sentiment, and sub-
jectivity analysis rely on lexicons of subjective
(opinion-carrying) words (e.g., (Turney, 2002;
Whitelaw et al, 2005; Riloff and Wiebe, 2003; Yu
and Hatzivassiloglou, 2003; Kim and Hovy, 2004;
Bloom et al, 2007; Andreevskaia and Bergler, 2008;
Agarwal et al, 2009)). Examples of such words are
the following (in bold):
(1) He is a disease to every team he has gone to.
Converting to SMF is a headache.
The concert left me cold.
That guy is such a pain.
However, even manually developed subjectiv-
ity lexicons have significant degrees of subjectivity
sense ambiguity (Su and Markert, 2008; Gyamfi et
al., 2009). That is, many clues in these lexicons have
both subjective and objective senses. This ambiguity
leads to errors in opinion and sentiment analysis, be-
cause objective instances represent false hits of sub-
jectivity clues. For example, the following sentence
contains the keywords from (1) used with objective
senses:
(2) Early symptoms of the disease include severe
headaches, red eyes, fevers and cold chills, body
pain, and vomiting.
Recently, in (Akkaya et al, 2009), we introduced
the task of subjectivity word sense disambiguation
(SWSD), which is to automatically determine which
word instances in a corpus are being used with sub-
jective senses, and which are being used with objec-
tive senses. We developed a supervised system for
SWSD, and exploited the SWSD output to improve
the performance of multiple contextual opinion anal-
ysis tasks.
Although the reported results are promising, there
are three obvious shortcomings. First, we were able
to apply SWSD to contextual opinion analysis only
on a very small scale, due to a shortage of anno-
tated data. While the experiments show that SWSD
improves contextual opinion analysis, this was only
on the small amount of opinion-annotated data that
was in the coverage of our system. Two questions
arise: is it feasible to obtain greater amounts of
the needed data, and do SWSD performance im-
provements on contextual opinion analysis hold on a
87
larger scale. Second, the annotations in (Akkaya et
al., 2009) are piggy-backed on SENSEVAL sense-
tagged data, which are fine-grained word sense an-
notations created by trained annotators. A concern
is that SWSD performance improvements on con-
textual opinion analysis can only be achieved using
such fine-grained expert annotations, the availability
of which is limited. Third, (Akkaya et al, 2009) uses
manual rules to apply SWSD to contextual opinion
analysis. Although these rules have the advantage
that they transparently show the effects of SWSD,
they are somewhat ad hoc. Likely, they are not opti-
mal and are holding back the potential of SWSD to
improve contextual opinion analysis.
To address these shortcomings, in this paper, we
investigate (1) the feasibility of obtaining a substan-
tial amount of annotated data, (2) whether perfor-
mance improvements on contextual opinion analy-
sis can be realized on a larger scale, and (3) whether
those improvements can be realized with subjectiv-
ity sense tagged data that is not built on expert full-
inventory sense annotations. In addition, we explore
better methods for applying SWSD to contextual
opinion analysis.
2 Subjectivity Word Sense Disambiguation
2.1 Annotation Tasks
We adopt the definitions of subjective (S) and ob-
jective (O) from (Wiebe et al, 2005; Wiebe and Mi-
halcea, 2006; Wilson, 2007). Subjective expressions
are words and phrases being used to express mental
and emotional states, such as speculations, evalua-
tions, sentiments, and beliefs. A general covering
term for such states is private state (Quirk et al,
1985), an internal state that cannot be directly ob-
served or verified by others. Objective expressions
instead are words and phrases that lack subjectivity.
The contextual opinion analysis experiments de-
scribed in Section 3 include both S/O and polar-
ity (positive,negative, neutral) classifications. The
opinion-annotated data used in those experiments is
from the MPQA Corpus (Wiebe et al, 2005; Wilson,
2007),1 which consists of news articles annotated for
subjective expressions, including polarity.
1Available at http://www.cs.pitt.edu/mpqa
2.1.1 Subjectivity Sense Labeling
For SWSD, we need the notions of subjective
and objective senses of words in a dictionary. We
adopt the definitions from (Wiebe and Mihalcea,
2006), who describe the annotation scheme as fol-
lows. Classifying a sense as S means that, when
the sense is used in a text or conversation, one ex-
pects it to express subjectivity, and also that the
phrase or sentence containing it expresses subjectiv-
ity. As noted in (Wiebe and Mihalcea, 2006), sen-
tences containing objective senses may not be objec-
tive. Thus, objective senses are defined as follows:
Classifying a sense as O means that, when the sense
is used in a text or conversation, one does not expect
it to express subjectivity and, if the phrase or sen-
tence containing it is subjective, the subjectivity is
due to something else.
Both (Wiebe and Mihalcea, 2006) and (Su and
Markert, 2008) performed agreement studies of the
scheme and report that good agreement can be
achieved between human annotators labeling the
subjectivity of senses (? values of 0.74 and 0.79, re-
spectively).
(Akkaya et al, 2009) followed the same annota-
tion scheme to annotate the senses of the words used
in the experiments. For this paper, we again use
the same scheme and annotate WordNet senses of
90 new words (the process of selecting the words is
described in Section 2.4).
2.1.2 Subjectivity Sense Tagging
The training and test data for SWSD consists of
word instances in a corpus labeled as S or O, in-
dicating whether they are used with a subjective or
objective sense.
Because there was no such tagged data at the time,
(Akkaya et al, 2009) created a data set by com-
bining two types of sense annotations: (1) labels of
senses within a dictionary as S or O (i.e., the subjec-
tivity sense labels of the previous section), and (2)
sense tags of word instances in a corpus (i.e., SEN-
SEVAL sense-tagged data).2 The subjectivity sense
labels were used to collapse the sense labels in the
sense-tagged data into the two new senses, S and O.
The target words (Akkaya et al, 2009) chose are the
words tagged in SENSEVAL that are also members
2Please see the paper for details on the SENSEVAL data
used in the experiments.
88
Sense Set1 (Subjective)
{ attack, round, assail, lash out, snipe, assault } ? attack in
speech or writing; ?The editors attacked the House Speaker?
{ assail, assault, set on, attack } ? attack someone emotionally;
?Nightmares assailed him regularly?
Sense Set2 (Objective)
{ attack } ? begin to injure; ?The cancer cells are attacking his
liver?; ?Rust is attacking the metal?
{ attack, aggress } ? take the initiative and go on the offensive;
?The visiting team started to attack?
Figure 1: Sense sets for target word ?attack? (abridged).
of the subjectivity lexicon of (Wilson et al, 2005;
Wilson, 2007).3 There are 39 such words. (Akkaya
et al, 2009) chose words from a subjectivity lexicon
because such words are known to have subjective
usages.
For this paper, subjectivity sense-tagged data was
obtained from the MTurk workers using the anno-
tation scheme of (Akkaya et al, 2010). A goal is to
keep the annotation task as simple as possible. Thus,
the workers are not directly asked if the instance of
a target word has a subjective or an objective sense,
because the concept of subjectivity would be diffi-
cult to explain in this setting. Instead the workers
are shown two sets of senses ? one subjective set and
one objective set ? for a specific target word and a
text passage in which the target word appears. Their
job is to select the set that best reflects the meaning
of the target word in the text passage. The set they
choose gives us the subjectivity label of the instance.
A sample annotation task is shown below. An
MTurk worker has access to two sense sets of the
target word ?attack? as seen in Figure 1. The S and
O labels appear here only for the purpose of this pa-
per; the workers do not see them. The worker is pre-
sented with the following text passage holding the
target word ?attack?:
Ivkovic had been a target of intra-party
feuding that has shaken the party. He was
attacked by Milosevic for attempting to
carve out a new party from the Socialists.
In this passage, the use of ?attack? is most similar
to the first entry in sense set one; thus, the correct
answer for this problem is Sense Set-1.
3Available at http://www.cs.pitt.edu/mpqa
(Akkaya et al, 2010) carried out a pilot study
where a subjectivity sense-tagged dataset was cre-
ated for eight SENSEVAL words through MTurk.
(Akkaya et al, 2010) evaluated the non-expert la-
bel quality against gold-standard expert labels which
were obtained from (Akkaya et al, 2009) relying
on SENSEVAL. The non-expert annotations are reli-
able, achieving ? scores around 0.74 with the expert
annotations.
For some words, there may not be a clean split be-
tween the subjective and objective senses. For these,
we opted for another strategy for obtaining MTurk
annotations. Rather than presenting the workers
with WordNet senses, we show them a set of objec-
tive usages, a set of subjective usages, and a text pas-
sage in which the target word appears. The workers?
job is to judge which set of usages the target instance
is most similar to.
2.2 SWSD System
We follow the same approach as in (Akkaya et al,
2009) to build our SWSD system. We train a differ-
ent supervised SWSD classifier for each target word
separately. This means the overall SWSD system
consists of as many SWSD classifiers as there are
target words. We utilize the same machine learning
features as in (Akkaya et al, 2009), which are com-
monly used in Word Sense Disambiguation (WSD).
2.3 Expert SWSD vs. Non-expert SWSD
Before creating a large subjectivity sense-tagged
corpus via MTurk, we want to make sure that non-
expert annotations are good enough to train reliable
SWSD classifiers. Thus, we decided to compare
the performance of a SWSD system trained on non-
expert annotations and on expert annotations. For
this purpose, we need a subjectivity sense-tagged
corpus where word instances are tagged both by ex-
pert and non-expert annotations. Fortunately, we
have such a corpus. As discussed in Section 3,
(Akkaya et al, 2009) created a subjecvitivity sense-
tagged corpus piggybacked on SENSEVAL. This
gives us a gold-standard corpus tagged by experts.
There is also a small subjectivity sense-tagged cor-
pus consisting of eight target words obtained from
non-expert annotators in (Akkaya et al, 2010). This
corpus is a subset of the gold-standard corpus from
(Akkaya et al, 2009) and it consists of 60 tagged
89
Acc p-value
SWSDGOLD 79.2 -
SWSDMJL 78.4 0.542
SWSDMJC 78.8 0.754
Table 1: Comparison of SWSD systems
instances for each target word.
Actually, (Akkaya et al, 2010) gathered three la-
bels for each instance. This gives us two options
to train the non-expert SWSD system: (1) training
the system on the majority vote labels (SWSDMJL)
(2) training three systems on the three separate la-
bel sets and taking the majority vote prediction
(SWSDMJC). Additionally, we train an expert SWSD
system (SWSDGOLD) ? a system trained on gold
standard expert annotations. All these systems are
trained on 60 instances of the eight target words for
which we have both non-expert and expert annota-
tions and are evaluated on the remaining instances
of the gold-standard corpus. This makes a total of
923 test instances for the eight target words with a
majority class baseline of 61.8.
Table 1 reports micro-average accuracy of each
system and the two-tailed p-value between the ex-
pert SWSD system and the two non-expert SWSD
systems. The p-value is calculated with McNemar?s
test. It shows that there is no statistically signif-
icant difference between classifiers trained on ex-
pert gold-standard annotations and non-expert anno-
tations. We adopt SWSDMJL in all our following ex-
periments, because it is more efficient.
2.4 Corpus Creation
For our experiments, we have multiple goals, which
effect our decisions on how to create the subjectiv-
ity sense-tagged corpus via MTurk. First, we want
to be able to disambiguate more target words than
(Akkaya et al, 2009). This way, SWSD will be able
to disambiguate a larger portion of the MPQA Cor-
pus allowing us to evaluate the effect of SWSD on
contextual opinion analysis on a larger scale. This
will also allow us to investigate additional integra-
tion methods of SWSD into contextual opinion anal-
ysis rather than simple ad hoc manual rules utilized
in (Akkaya et al, 2009). Second, we want to show
that we can rely on non-expert annotations instead of
expert annotations, which will make an annotation
effort on a larger-scale both practical and feasible,
timewise and costwise. Optimally, we could have
annotated via MTurk the same subjectivity sense-
tagged corpus from (Akkaya et al, 2009) in order to
compare the effect of a non-expert SWSD system on
contextual opinion analysis directly with the results
reported for an expert SWSD system in (Akkaya et
al., 2009). But, this would have diverted our re-
sources to reproduce the same corpus and contradict
our goal to extend the subjectivity sense-tagged cor-
pus to new target words. Moreover, we have already
shown in Section 2.3 that non-expert annotations can
be utilized to train reliable SWSD classifiers. It is
reasonable to believe that similar performance on
the SWSD task will reflect to similar improvements
on contextual opinion analysis. Thus, we decided
to prioritize creating a subjectivity sense-tagged cor-
pus for a totally new set of words. We aim to show
that the favourable results reported in (Akkaya et al,
2009) will still hold on new target words relying on
non-expert annotations.
We chose our target words from the subjectivity
lexicon of (Wilson et al, 2005), because we know
they have subjective usages. The contextual opin-
ion systems we want to improve rely on this lexicon.
We call the words in the lexicon subjectivity clues.
At this stage, we want to concentrate on the fre-
quent and ambiguous subjectivity clues. We chose
frequent ones, because they will have larger cov-
erage in the MPQA Corpus. We chose ambiguous
ones, because these clues are the ones that are most
important for SWSD. Choosing most frequent and
ambiguous subjectivity clues guarantees that we uti-
lize our limited resources in the most efficient way.
We judge a clue to be ambiguous if it appears more
than 25% and less than 75% of the times in a sub-
jective expression. We get these statistics by simply
counting occurrences in the MPQA Corpus inside
and outside of subjective expressions.
There are 680 subjectivity clues that appear in the
MPQA Corpus and are ambiguous. Out of those, we
selected the 90 most frequent that have to some ex-
tent distinct objective and subjective senses in Word-
Net, as judged by the co-authors. The co-authors an-
notated the WordNet senses of those 90 target words.
For each target word, we selected approximately 120
instances randomly from the GIGAWORD Corpus.
In a first phase, we collected three sets of MTurk an-
90
notations for the selected instances. In this phase,
MTurk workers base their judgements on two sense
sets they observe. This way, we get training data to
build SWSD classifiers for these 90 target words.
The quality of these classifiers is important, be-
cause we will exploit them for contextual opinion
analysis. Thus, we evaluate them by 10-fold cross-
validation. We split the target words into three
groups. If the majority class baseline of a word is
higher than 90%, it is considered as skewed (skewed
words have a performance at least as good as the ma-
jority class baseline). If a target word improves over
its majority class baseline by 25% in accuracy, it is
considered as good. Otherwise, it is considered as
mediocre. This way, we end up with 24 skewed, 35
good, and 31 mediocre words. There are many pos-
sible reasons for the less reliable performance for
the mediocre group. We hypothesize that a major
problem is the similarity between the objective and
subjective sense sets of a word, thus leading to poor
annotation quality. To check this, we calculate the
agreement between three annotation sets and report
averages. The agreement in the mediocre group is
78.68%, with a ? value of 0.57, whereas the aver-
age agreement in the good group is 87.51%, with
a ? value of 0.75. These findings support our hy-
pothesis. Thus, the co-authors created usage inven-
tories for the words in the mediocre group as de-
scribed in Section 2.1.1. We initiated a second phase
of MTurk annotations. We collect for the mediocre
group another three sets of MTurk annotations for
120 instances, this time utilizing usage inventories.
The 10-fold cross-validation experiments show that
nine of the 31 words in the mediocre group shift to
the good group. Only for these nine words, we ac-
cept the annotations collected via usage inventories.
For all other words, we use the annotations collected
via sense inventories. From now on, we will refer
to this non-expert subjectivity sense-tagged corpus
consisting of the tagged data for all 90 target words
as the MTurkSWSD Corpus (agreement on the entire
MTurkSWSD corpus is 85.54%, ?:0.71).
3 SWSD Integration
Now that we have the MTurkSWSD Corpus, we
are ready to evaluate the effect of SWSD on con-
textual opinion analysis. In this section, we ap-
ply our SWSD system trained on MTurkSWSD to
both expression-level classifiers from (Akkaya et al,
2009): (1) the subjective/objective (S/O) classifier
and (2) the contextual polarity classifier. Both clas-
sifiers are introduced in Section 3.1
Our SWSD system can disambiguate 90 target
words, which have 3737 instances in the MPQA
Corpus. We refer to this subset of the MPQA Corpus
as MTurkMPQA. This subset makes up the cover-
age of our SWSD system. Note that MTurkMPQA
is 5.2 times larger than the covered MPQA subset
in (Akkaya et al, 2009) referred as senMPQA. We
try different strategies to integrate SWSD into the
contextual classifiers. In Section 3.2, we follow the
same rule-based strategy as in (Akkaya et al, 2009)
for completeness. In Section 3.3, we introduce two
new learning strategies for SWSD integration out-
performing existing rule-based strategy. We evalu-
ate the improvement gained by SWSD on MTurkM-
PQA.
3.1 Contextual Classifiers
The original contextual polarity classifier is intro-
duced in (Wilson et al, 2005). We use the same im-
plementation as in (Akkaya et al, 2009). This classi-
fier labels clue instances in text as contextually neg-
ative/positive/neutral. The gold standard is defined
on the MPQA Corpus as follows. If a clue instance
appears in a positive expression, it is contextually
positive (Ps). If it appears in a negative expression,
it is contextually negative (Ng). If it is in an objec-
tive expression or in a neutral subjective expression,
it is contextually neutral (N). The contextual polar-
ity classifier consists of two separate steps. The first
step is an expression-level neutral/polar (N/P) clas-
sifier. The second step classifies only polar instances
further into positive and negative classes. This way,
the overall system performs a three-way classifica-
tion (Ng/Ps/N).
The subjective/objective classifier is introduced in
(Akkaya et al, 2009). It relies on the same machine
learning features as the N/P classifier (i.e. the first
step of the contextual polarity classifier). The only
difference is that the classes are S/O instead of N/P.
The gold standard is defined on the MPQA Corpus
in the following way. If a clue instance appears in
a subjective expression, it is contextually S. If it ap-
pears in an objective expression, it is contextually O.
Both contextual classifiers are supervised.
91
Baseline Acc OF SF
MTurkMPQA 52.4% (O)
OS/O 67.1 68.9 65.0
R1R2 71.1 72.7 69.2
senMPQA 63.1% (O)
OS/O 75.4 65.4 80.9
R1R2 81.3 75.9 84.8
Table 2: S/O classifier with and without SWSD.
3.2 Rule-Based SWSD Integration
(Akkaya et al, 2009) integrates SWSD into a con-
textual classifier by simple rules. The rules flip the
output of the contextual classifier if some conditions
hold. They make use of following information: (1)
SWSD output, (2) the contextual classifier?s confi-
dence and (3) the presence of another subjectivity
clue ? any clue from the subjectivity lexicon ? in the
same expression.
For the contextual S/O classifier, (Akkaya et al,
2009) defines two rules: one flipping the S/O classi-
fier?s output from O to S (R1) and one flipping from
S to O (R2). R1 is defined as follows : if the contex-
tual classifier decides a target word instance is con-
textually O and SWSD decides that it is used in a S
sense, then SWSD overrules the contextual S/O clas-
sifier?s output and flips it from O to S, because an
instance in a S sense will make the surrounding ex-
pression subjective. R2 is a little bit more complex.
It is defined as follows: If the contextual classifier la-
bels a clue instance as S but (1) SWSD decides that
it is used in an O sense, (2) the contextual classifier?s
confidence is low, and (3) there is no other subjec-
tivity clue in the same expression, then R2 flips the
contextual classifier?s output from S to O. The ra-
tionale behind R2 is that even if the target word in-
stance has an O sense, there might be another reason
(e.g. the presence of another subjectivity clue in the
same expression) for the expression enclosing it to
be subjective.
We use the exact same rules and adopt the same
confidence threshold. Table 2 holds the comparison
of the original contextual classifier and the classi-
fier with SWSD support on senMPQA as reported in
(Akkaya et al, 2009) and on MTurkMPQA. OS/O is
the original S/O classifier; R1R2 is the system with
SWSD support utilizing both rules. We report only
R1R2, since (Akkaya et al, 2009) gets highest im-
provement utilizing both rules.
Baseline Acc NF PF
MTurkMPQA 70.6% (P)
ON/P 72.3 82.0 39.8
R4 74.5 84.0 37.8
senMPQA 73.9% (P)
ON/P 79.0 86.7 50.3
R4 81.6 88.6 52.3
Table 3: N/P classifier with and without SWSD
In Table 2 we see that R1R2 achieves 4% percent-
age points improvement in accuracy over OS/O on
MTurkMPQA. The improvement is statistically sig-
nificant at the p < .01 level with McNemar?s test. It
is accompanied with improvements both in subjec-
tive F-measure (SF) and objective F-measure (OF).
It is not possible to directly compare improvements
on senMPQA and MTurkMPQA since they are dif-
ferent subsets of the MPQA Corpus. SWSD support
brings 24% error reduction on senMPQA over the
original S/O classifier. In comparison, on MTurkM-
PQA, the error reduction is 12%. We see that the im-
provements on the large MTurkMPQA set still hold,
but not as strong as in (Akkaya et al, 2009).
(Akkaya et al, 2009) uses a similar rule to
make the contextual polarity classifier sense-aware.
Specifically, the rule is applied to the output of the
first step (N/P classifier). The rule, R4, flips P to N
and is analogous to R2. If the contextual classifier
labels a clue instance as P but (1) SWSD decides
that it is used in an O sense, (2) the contextual clas-
sifier?s confidence is low, and (3) there is no other
clue instance in the same expression, then R4 flips
the contextual classifier?s output from P to N.
Table 3 holds the comparison of the original N/P
classifier with and without SWSD support on sen-
MPQA as reported in (Akkaya et al, 2009) and on
MTurkMPQA. ON/P is the original N/P classifier; R4
is the system with SWSD support utilizing rule R4.
Since our main focus is not rule-based integration,
we did not run the second step of the polarity classi-
fier. We report the second step result below for the
learning-based SWSD integration in section 3.4.
In Table 3, we see that R4 achieves 2.2 percent-
age points improvement in accuracy over ON/P on
MTurkMPQA. The improvement is statistically sig-
nificant at the p < .01 level with McNemar?s test.
It is accompanied with improvement only in objec-
tive F-measure (OF). SWSD support brings 12.4%
error reduction on senMPQA (Akkaya et al, 2009).
92
On MTurkMPQA, the error reduction is 8%. We see
that the rule-based SWSD integration still improves
both contextual classifiers on MTurkMPQA, but the
gain is not as large as on senMPQA. This might be
due to the brittleness of the rule-based integration.
3.3 Learning SWSD Integration
Now that we can disambiguate a larger portion of
the MPQA Corpus than in (Akkaya et al, 2009),
we can investigate machine learning methods for
SWSD integration to deal with the brittleness of the
rule-based integration. In this section, we introduce
two learning methods to apply SWSD to the contex-
tual classifiers. For the learning methods, we rely on
exactly the same information as the rule-based inte-
gration: (1) SWSD output, (2) the contextual clas-
sifier?s output, (3) the contextual classifier?s confi-
dence, and (4) the presence of another clue instance
in the same expression. The rationale is the same as
for the rule-based integration, namely to relate sense
subjectivity and contextual subjectivity.
3.3.1 Method1
In the first method, we extend the machine learn-
ing features of the underlying contextual classifiers
by adding (1) and (4) from above. We evaluate the
extended contextual classifiers on MTurkMPQA via
10-fold cross-validation. Tables 4 and 5 hold the
comparison of Method1 (EXTS/O, EXTN/P) to the
original contextual classifiers (OS/O, ON/P) and to the
rule-based SWSD integration (R1R2, R4). We see
substantial improvement for Method1. It achieves
39% error reduction over OS/O and 25% error reduc-
tion over ON/P. For both classifiers, the improvement
in accuracy over the rule-based integration is statisti-
cally significant at the p< .01 level with McNemar?s
test.
3.3.2 Method2
This method defines a third classifier that accepts
as input the contextual classifier?s output and the
SWSD output and predicts what the contextual clas-
sifier?s output should have been. We can think of
this third classifier as the learning counterpart of
the manual rules from Section 3.2, since it actu-
ally learns when to flip the contextual classifier?s
output considering SWSD evidence. Specifically,
this merger classifier relies on four machine learn-
ing features (1), (2), (3), (4) from above (the ex-
Acc OF SF
OS/O 67.1 68.9 65.0
R1R2 71.1 72.7 69.2
EXTS/O 80.0 81.4 78.3
MERGERS/O 78.2 80.3 75.5
Table 4: S/O classifier with learned SWSD integration
Acc NF PF
ON/P 72.3 82.0 39.8
R4 74.5 84.0 37.8
EXTN/P 79.1 85.7 61.1
MERGERN/P 80.4 86.7 62.8
Table 5: N/P classifier with learned SWSD integration
act same information used in rule-based integration).
Because it is a supervised classifier, we need train-
ing data where we have clue instances with cor-
responding contextual classifier and SWSD predic-
tions. Fortunately, we can use senMPQA for this
purpose. We train our merger classifier on senM-
PQA (we get contextual classifier predictions via 10-
fold cross-validation on the MPQA Corpus) and ap-
ply it to MTurkMPQA. We use SVM classifier from
the Weka package (Witten and Frank., 2005) with
its default settings. Tables 4 and 5 hold the com-
parison of Method2 (MERGERS/O, MERGERN/P) to
the original contextual classifiers (Oo/s, ON/P) and
the rule-based SWSD integration (R1R2, R4). It
achieves 29% error reduction over OS/O and 29% er-
ror reduction over ON/P. The improvement on the
rule-based integration is statistically significant at
the p < .01 level with McNemar?s test. Method2
performs better (statistically significant at the p <
.05 level) than Method1 for the N/P classifier but
worse (statistically significant at the p < .01 level)
for the S/O classifier.
3.4 Improving Contextual Polarity
Classification
We have seen that Method2 is the best method to
improve the N/P classifier, which is the first step
of the contextual polarity classifier. To assess the
overall improvement in polarity classification, we
run the second step of the contextual polarity clas-
sifier after correcting the first step with Method2.
Table 6 summarizes the improvement propagated to
93
Acc NF NgF PsF
MTurkMPQA
OPs/Ng/N 72.1 83.0 34.2 15.0
MERGERN/P 77.8 87.4 53.0 27.7
senMPQA
OPs/Ng/N 77.6 87.2 39.5 40.0
R4 80.6 89.1 43.2 44.0
Table 6: Polarity classifier with and without SWSD.
Ps/Ng/N classification. For comparison, we also
include results from (Akkaya et al, 2009) on sen-
MPQA. Method2 results in 20% error reduction in
accuracy over OPs/Ng/N (R4 achieves 13.4% error
reduction on senMPQA). The improvement on the
rule-based integration is statistically significant at
the p < .01 level with McNemar?s test. More im-
portantly, the F-measure for all the labels improves.
This indicates that non-expert MTurk annotations
can replace expert annotations for our end-goal ? im-
proving contextual opinion analysis ? while reduc-
ing time and cost requirements by a large margin.
Moreover, we see that the improvements in (Akkaya
et al, 2009) scale up to new subjectivity clues.
4 Related Work
One related line of research is to automatically
assign subjectivity and/or polarity labels to word
senses in a dictionary (Valitutti et al, 2004; An-
dreevskaia and Bergler, 2006; Wiebe and Mihalcea,
2006; Esuli and Sebastiani, 2007; Su and Markert,
2009). In contrast, the task in our paper is to auto-
matically assign labels to word instances in a corpus.
Recently, some researchers have exploited full
word sense disambiguation in methods for opinion-
related tasks. For example, (Mart??n-Wanton et al,
2010) exploit WSD for recognizing quotation polar-
ities, and (Rentoumi et al, 2009; Mart??n-Wanton et
al., 2010) exploit WSD for recognizing headline po-
larities. None of this previous work investigates per-
forming a coarse-grained variation of WSD such as
SWSD to improve their application results, as we do
in this work.
A notable exception is (Su and Markert, 2010),
who exploit SWSD to improve the performance on
a contextual NLP task, as we do. While the task
in our paper is subjectivity and sentiment analy-
sis, their task is English-Chinese lexical substitu-
tion. As (Akkaya et al, 2009) did, they anno-
tated word senses, and exploited SENSEVAL data
as training data for SWSD. They did not directly an-
notate words in context with S/O labels, as we do in
our work. Further, they did not separately evaluate a
SWSD system component.
Many researchers work on reducing the granular-
ity of sense inventories for WSD (e.g., (Palmer et al,
2004; Navigli, 2006; Snow et al, 2007; Hovy et al,
2006)). Their criteria for grouping senses are syn-
tactic and semantic similarities, while the groupings
in work on SWSD are driven by the goals to improve
contextual subjectivity and sentiment analysis.
5 Conclusions and Future Work
In this paper, we utilized a large pool of non-expert
annotators (MTurk) to collect subjectivity sense-
tagged data for SWSD. We showed that non-expert
annotations are as good as expert annotations for
training SWSD classifiers. Moreover, we demon-
strated that SWSD classifiers trained on non-expert
annotations can be exploited to improve contextual
opinion analysis.
The additional subjectivity sense-tagged data en-
abled us to evaluate the benefits of SWSD on con-
textual opinion analysis on a corpus of opinion-
annotated data that is five times larger. Using the
same rule-based integration strategies as in (Akkaya
et al, 2009), we found that contextual opinion anal-
ysis is improved by SWSD on the larger datasets.
We also experimented with new learning strategies
for integrating SWSD into contextual opinion analy-
sis. With the learning strategies, we achieved greater
benefits from SWSD than the rule-based integration
strategies on all of the contextual opinion analysis
tasks.
Overall, we more firmly demonstrated the poten-
tial of SWSD to improve contextual opinion analy-
sis. We will continue to gather subjectivity sense-
tagged data, using sense inventories for words that
are well represented in WordNet for our purposes,
and with usage inventories for those that are not.
6 Acknowledgments
This material is based in part upon work supported
by National Science Foundation awards #0917170
and #0916046.
94
References
Apoorv Agarwal, Fadi Biadsy, and Kathleen Mckeown.
2009. Contextual phrase-level polarity analysis us-
ing lexical affect scoring and syntactic N-grams. In
Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 24?32. Asso-
ciation for Computational Linguistics.
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009.
Subjectivity word sense disambiguation. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing, pages 190?199, Singa-
pore, August. Association for Computational Linguis-
tics.
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon mechanical turk for
subjectivity word sense disambiguation. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechani-
cal Turk, pages 195?203, Los Angeles, June. Associa-
tion for Computational Linguistics.
Alina Andreevskaia and Sabine Bergler. 2006. Mining
wordnet for a fuzzy sentiment: Sentiment tag extrac-
tion from wordnet glosses. In Proceedings of the 11rd
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL-2006).
Alina Andreevskaia and Sabine Bergler. 2008. When
specialists and generalists work together: Overcom-
ing domain dependence in sentiment tagging. In Pro-
ceedings of ACL-08: HLT, pages 290?298, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Kenneth Bloom, Navendu Garg, and Shlomo Argamon.
2007. Extracting appraisal expressions. In HLT-
NAACL 2007, pages 308?315, Rochester, NY.
Andrea Esuli and Fabrizio Sebastiani. 2007. Pagerank-
ing wordnet synsets: An application to opinion min-
ing. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
424?431, Prague, Czech Republic, June. Association
for Computational Linguistics.
Yaw Gyamfi, Janyce Wiebe, Rada Mihalcea, and Cem
Akkaya. 2009. Integrating knowledge for subjectiv-
ity sense labeling. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT 2009), pages
10?18, Boulder, Colorado, June. Association for Com-
putational Linguistics.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: The 90% solution.
In Proceedings of the Human Language Technology
Conference of the NAACL, Companion Volume: Short
Papers, New York City.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In Proceedings of the Twen-
tieth International Conference on Computational Lin-
guistics (COLING 2004), pages 1267?1373, Geneva,
Switzerland.
Tamara Mart??n-Wanton, Aurora Pons-Porrata, Andre?s
Montoyo-Guijarro, and Alexandra Balahur. 2010.
Opinion polarity detection - using word sense disam-
biguation to determine the polarity of opinions. In
ICAART 2010 - Proceedings of the International Con-
ference on Agents and Artificial Intelligence, Volume
1, pages 483?486.
R. Navigli. 2006. Meaningful clustering of senses helps
boost word sense disambiguation performance. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics, Sydney, Australia.
M. Palmer, O. Babko-Malaya, and H. T. Dang. 2004.
Different sense granularities for different applications.
In HLT-NAACL 2004 Workshop: 2nd Workshop on
Scalable Natural Language Understanding, Boston,
Massachusetts.
Randolph Quirk, Sidney Greenbaum, Geoffry Leech, and
Jan Svartvik. 1985. A Comprehensive Grammar of the
English Language. Longman, New York.
Vassiliki Rentoumi, George Giannakopoulos, Vangelis
Karkaletsis, and George A. Vouros. 2009. Sentiment
analysis of figurative language using a word sense
disambiguation approach. In Proceedings of the In-
ternational Conference RANLP-2009, pages 370?375,
Borovets, Bulgaria, September. Association for Com-
putational Linguistics.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP-2003), pages 105?
112, Sapporo, Japan.
R. Snow, S. Prakash, D. Jurafsky, and A. Ng. 2007.
Learning to merge word senses. In Proceedings of
the Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), Prague, Czech
Republic.
Fangzhong Su and Katja Markert. 2008. From word
to sense: a case study of subjectivity recognition. In
Proceedings of the 22nd International Conference on
Computational Linguistics (COLING-2008), Manch-
ester.
Fangzhong Su and Katja Markert. 2009. Subjectivity
recognition on word senses via semi-supervised min-
cuts. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 1?9, Boulder, Colorado, June. Associ-
ation for Computational Linguistics.
95
Fangzhong Su and Katja Markert. 2010. Word sense
subjectivity for cross-lingual lexical substitution. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 357?
360, Los Angeles, California, June. Association for
Computational Linguistics.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-02), pages 417?424, Philadelphia, Pennsyl-
vania.
Alessandro Valitutti, Carlo Strapparava, and Oliviero
Stock. 2004. Developing affective lexical resources.
PsychNology, 2(1):61?83.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal taxonomies for sentiment anal-
ysis. In Proceedings of CIKM-05, the ACM SIGIR
Conference on Information and Knowledge Manage-
ment, Bremen, DE.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1065?1072, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. Language Resources and Evaluation,
39(2/3):164?210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Hu-
man Language Technologies Conference/Conference
on Empirical Methods in Natural Language Process-
ing (HLT/EMNLP-2005), pages 347?354, Vancouver,
Canada.
Theresa Wilson. 2007. Fine-grained Subjectivity and
Sentiment Analysis: Recognizing the Intensity, Polar-
ity, and Attitudes of private states. Ph.D. thesis, Intel-
ligent Systems Program, University of Pittsburgh.
I. Witten and E. Frank. 2005. Data Mining: Practical
Machine Learning Tools and Techniques, Second Edi-
tion. Morgan Kaufmann, June.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2003), pages 129?136, Sapporo, Japan.
96
