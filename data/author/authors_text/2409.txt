Automatically Inducing a Part-of-Speech Tagger
by Projecting from Multiple Source Languages
Across Aligned Corpora
Victoria Fossum1 and Steven Abney2
1 Dept. of EECS, University of Michigan, Ann Arbor MI 48105
vfossum@umich.edu
2 Dept. of Linguistics, University of Michigan, Ann Arbor MI 48105
abney@umich.edu
Abstract. We implement a variant of the algorithm described by
Yarowsky and Ngai in [21] to induce an HMM POS tagger for an ar-
bitrary target language using only an existing POS tagger for a source
language and an unannotated parallel corpus between the source and tar-
get languages. We extend this work by projecting from multiple source
languages onto a single target language. We hypothesize that systematic
transfer errors from differing source languages will cancel out, improving
the quality of bootstrapped resources in the target language. Our exper-
iments confirm the hypothesis. Each experiment compares three cases:
(a) source data comes from a single language A, (b) source data comes
from a single language B, and (c) source data comes from both A and B,
but half as much from each. Apart from the source language, other condi-
tions are held constant in all three cases ? including the total amount of
source data used. The null hypothesis is that performance in the mixed
case would be an average of performance in the single-language cases,
but in fact, mixed-case performance always exceeds the maximum of
the single-language cases. We observed this effect in all six experiments
we ran, involving three different source-language pairs and two different
target languages.
1 Introduction
1.1 Background
Statistical NLP techniques typically require large amounts of annotated data.
Labelling data by hand is time-consuming; a natural goal is therefore to generate
text analysis tools automatically, using minimal resources. Yarowsky et al [22]
present methods for automatically inducing various monolingual text analysis
tools for an arbitrary target language, using only the corresponding text analysis
tool for a source language and a parallel corpus between the source and target
languages. Hwa et al [15] induce a parser for Chinese text via projection from
English using a similar method to that of [22]. Cucerzan and Yarowsky [8] present
a method for bootstrapping a POS tagger for an arbitrary target language using
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 862?873, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Automatically Inducing a Part-of-Speech Tagger 863
only a bilingual dictionary between the source and target languages, a ?basic
library reference grammar? for the target language, and an existing corpus in
the target language.
While automatically induced text analysis tools use fewer resources, their
accuracy lags behind that of more resource-intensive tools. One solution to the
problem of error reduction on NLP tasks is to train multiple classifiers, then
compute a consensus classifier. Combining multiple classifiers is an effective way
to reduce error if the errors made by each classifier are independently distributed.
Such approaches have been successfully applied to a range of NLP tasks. Brill
and Wu [4], van Halteren et al [19], and Zavrel and Daelemans [23] investigate
various methods for improving the performance of statistical POS taggers by
combining multiple such taggers. Henderson and Brill [14] combine the Char-
niak, Collins, and Ratnaparkhi parsers to achieve an accuracy surpassing the
best previous results on the WSJ. Gollins and Sanderson [10] apply projec-
tion via multiple source languages to reduce error in cross-linguistic information
retrieval.
1.2 Motivation
We hypothesize that a large component of the error rate in the automatically in-
duced text analysis tools generated by [22] is due to morphosyntactic differences
between the source and target languages that are specific to each source-target
language pair. Therefore, training POS taggers on additional source languages
should result in multiple classifiers which produce independently distributed er-
rors on the target language.
Previous research in classifier combination for POS tagging has focused pri-
marily on combining various statistical classifiers trained on data in the same
language. Thus, our approach is novel in its exploitation of differences across
languages, rather than differences across statistical methods, to improve per-
formance on POS tagging. Our method is general in that it does not rely on
language-specific information, and requires no annotated resources in the target
language.
Our method is easily extensible to new languages. While it requires a parallel
corpus between each source language and the target language, the corpora used
to train each single-source tagger need not be translations of the same text.
Furthermore, our algorithm is applicable even to target languages belonging to
distinct language families from those of the source languages.
1.3 Task Overview
Using existing POS taggers for English, German, and Spanish, we generate
single-source taggers for Czech and French via projection across parallel trans-
lations of the Bible. To obtain a theoretical upper bound on the performance
improvement that is possible by combining multiple POS taggers, we measure
the complementarity between each pair of single-source taggers. We examine
864 V. Fossum and S. Abney
various ways to combine the output of these single-source taggers into a consen-
sus tagger, and measure the resulting performance improvement.
2 Methods
2.1 Single-Source POS Tagger Induction
We implement a variant of the algorithm described in [21] for constructing a
single-source bigram-based HMM POS tagger for a target language. First, we
identify a language (the ?source language?) for which a POS tagger exists, and
a sentence-aligned parallel corpus consisting of text in the source language and
its translation in the target language. We then align the parallel corpora at
the word-level using GIZA++ [1]. Next, we annotate the source text using an
existing POS tagger. Finally, we project these annotations across the parallel
text from the source text to the target text, smooth these projections, and use
the projected annotations to train an HMM POS tagger for the target language.
In more detail, we implement the following procedure, based on [21]:
1. Obtain a sentence-aligned parallel corpus in the source and target languages
(see Section 2.4).
2. Align the parallel corpus at the word-level using GIZA++1.
English: He(1) likes(2) cats(4).
French: Il(1) aime(2) les(3) chats(4).
3. Tag the source portion of the parallel corpus using an existing POS tagger for
the source language2. We use the Brill tagger3 for English [3], the TNT tagger4
for German, and the SVMTool tagger5 for Spanish.
English: He/PRP likes/VBP cats/NNS.
Since the POS tagger for each source language uses its own distinct tagset, we
convert the output of each tagger to a ?generic? tagset for comparison purposes.
Additionally, we label each POS tag as belonging to one of several more general
?core? tagset categories (see Table 1).
4. Using the mapping induced by the word-level alignments, project the POS
tags from the source language onto the target language.
French: Il/PRP aime/VB les/NULL chats/NNS.
1 GIZA++ is a component of EGYPT, an open-source implementation of IBM?s sta-
tistical machine translation system [1].
2 For all existing POS taggers, we use the default models provided with the tagger
for training in the source language. For taggers with variable parameter settings, we
use the default settings for all parameters.
3 A transformation-based tagger [3].
4 A bigram-based Markov tagger[2].
5 An SVM-based tagger [9].
Automatically Inducing a Part-of-Speech Tagger 865
Note that tag projection is complicated by the occurrence of many-to-one word
alignments from source to target. To handle such cases, we compute two esti-
mates of tag probabilities, P (ti|wi): one using only 1-to-1 alignments, and the
other using 1-to-n alignments. We then linearly combine the two estimators.
5. Before computing the P (wi|ti) model, several steps must be taken to smooth
the initial, noisy tag projections. First, P (wi|ti) can be decomposed as follows:
P (wi|ti) =
P (ti|wi) ? P (wi)
P (ti)
To smooth P (ti|wi), the simplifying assumption is made that in most natural
languages, each word has at most two possible POS tags at the core tagset
granularity. We count the relative frequency of each tag that is assigned to that
French word by the tag projection from English, then discard all but the two most
frequently assigned core tags. We then recursively smooth the tag probabilities
in favor of the two most probable subtags for each of the core tags, where the
subtags are members of the more finely grained ?generic? tagset. We compute
P (ti) and P (wi) using corpus frequency.
6. We estimate the probability of unknown words using the probability of words
appearing only once in the training corpus. We replace all words occurring only
once in the training corpus by the ?UNK? token.
7. Before computing the P (tj |ti) model, we filter the training data to remove
those sentence pairs whose alignment score (as determined by GIZA++) falls
into the lowest 25% of alignment scores. To estimate the probability of unknown
state transitions, we perform Witten-Bell smoothing [20] on P (tj |ti) to assign
non-zero probabilities to state transitions not seen in the training data.
8. The resulting model defines an HMM bigram-based tagger in the target lan-
guage. We use the Viterbi algorithm to determine the most likely sequence of
tags given a sentence in the target language [17].
2.2 Multiple-Source POS Tagger Induction
To compute a multiple-source consensus tagger, we train n single-source taggers
using n parallel texts, each pairing one of the source languages with the target
language. We then apply each single-source tagger to the test sentences. For
each word in the test sentences, we record the probability distribution Pi(t|w)
over possible tags that the ith single-source tagger produces. We then compute
two consensus taggers, Majority Tag and Linear Combination, by combining the
output from each of the n taggers, P1(t|w) . . . Pn(t|w) as follows:
Majority Tag: Each tagger outputs the most likely tag
tbesti = argmax
t
(Pi(t|w))
for w. We select the tag from tbest1 , . . . , t
best
n that receives the greatest number
of votes from single-source taggers. To break ties, we select the tag chosen with
the highest probability by the taggers that selected it.
866 V. Fossum and S. Abney
Linear Combination: Each tagger outputs a vector of probabilities over pos-
sible tags t given w. We take a linear combination of these vectors to compute
Plinear(T |w), then select the tag tlinear with the highest probability.
Plinear(T |w) =
n
?
i=1
ki ? (Pi(T |w))
tlinear = argmax
t
(Plinear(t|w))
In our experiments, we set ki = 1n , so we effectively average the probability
distributions of each tagger over possible tags t for w.
2.3 Tagsets
Two tagsets of different granularities are used in the experiments: the coarse-
grained ?core? and fine-grained ?generic? tagsets (see Table 1). While it can be
difficult to map fine-grained POS tags from one language directly onto another
another because of morphological differences between languages, languages tend
to agree on tags at a coarse-grained level.
2.4 Data Sets
We use two corpora in our experiments: the Bible (with translations in English,
Czech, French, German, and Spanish), and the Hansards parallel corpus of Cana-
dian parliamentary proceedings (with translations in English and French). For
Table 1. Generic and Core Tagsets
POS Generic Core
Noun NN N
Proper Noun NNP N
Verb, Inf. VB V
Verb, Present VBP V
Verb, Present Part. VBG V
Verb, Past Part. VBN V
Verb, Past VBD V
Determiner DT D
Wh-Determiner WDT D
Conjunction CC C
Number CD NUM
Adverb RB R
Wh-Adverb WRB R
Adjective JJ J
Pronoun PRP P
Preposition IN I
Automatically Inducing a Part-of-Speech Tagger 867
the Bible experiments, we use the entire 31,100-line text: training data con-
sists of either one 31,000-line excerpt or two 15,500-line excerpts, while testing
data consists of a held-out 100-sentence excerpt. For the Hansards experiments,
training data consists of a 85,000-line excerpt; testing data consists of a held-out
100-sentence excerpt.
We perform the following pre-processing steps. Each text is filtered to remove
punctuation and converted to lower case; accents are preserved. The English,
French, German, and Spanish texts are tokenized to expand elisions.6
3 Results
We report percent agreement with the correct tags, determined by compari-
son with the output of the Treetag tagger7 for French, and a hybrid rule-
based/HMM-based tagger8 for Czech. For French, agreement with the correctly
tagged text is measured on the generic and core tagsets. For Czech, agreement
is measured on the core tagset only, since this is the POS tagset provided by
the tagger we use for evaluation purposes. All experiments use 5-fold cross-
validation. For each iteration, the parallel corpus is divided randomly into train-
ing and testing sets. The accuracy of each single-source tagger is limited by the
accuracy of the tagger used to tag the source training text; the accuracy of the
evaluation of each tagger?s performance on French and Czech text is limited by
the accuracy of the reference tagger against which it is compared (Table 2).
Table 2. Reported Accuracy of Existing POS Taggers used to Train Single-Source
Taggers
Tagger Language % Accuracy F-measure Test Corpus
Brill English 96.6% ?- Penn Treebank (English)
TNT German ?- ?- ?-
SVMTool Spanish 96.89% ?- LEXESP (Spanish)
TreeTag French 96.36% ?- Penn Treebank (English)
Rules + HMM Czech ?- 95.38% PDT (Czech)
3.1 Single-Source
For each single-source tagger, we train on 31,000 lines of the parallel Bible be-
tween the source and target languages and test on 100 held-out lines of the Bible
in the target language. We report the accuracy of the induced taggers on French
(Tables 5 and 4) and Czech (Table 6).
6 e.g. ?doesn?t? ? ?does not?, ?qu?il? ? ?que il?, ?zum? ? ?zu dem?, and ?del? ?
?de el?. This tokenization represents the only step of our algorithm that requires
additional language-specific knowledge beyond the resources already given.
7 A decision-tree-based tagger [18].
8 [13].
868 V. Fossum and S. Abney
To compare our baseline single-source tagger performance against that of [21],
we conduct the following experiment, after the experimental procedure used by
[21]. We train a single-source English-projected tagger for French on a 2,000,000-
word (approximately 85,000-line) excerpt of the French-English Hansards cor-
pus and test it on a 100-line excerpt of the same corpus. We obtain accuracies
of 86.5% and 91.1% on the generic and core tagsets, respectively; [21] report
accuracies of 91% and 94% on the ?English Equivalent? and ?core? tagsets,
respectively.9
3.2 Multiple-Source
Complementarity: We compute the pairwise complementarity of each pair of
single-source taggers. Brill and Wu [4] define the complementarity of a pair of
taggers i and j as the percentage of cases when tagger i is wrong that tagger j
is correct (See Table 3):
Comp(i, j) = (1 ? errorsi ? errorsj
errorsi
) ? 100
Table 3. Complementarity (row,col) of Single-Source Taggers
French Bible Czech Bible
Generic Tagset Core Tagset Core Tagset
Source English German Spanish English German Spanish English German Spanish
English 0 38.95 32.87 0 32.75 37.13 0 22.08 18.71
German 42.40 0 44.93 30.49 0 38.95 15.47 0 17.31
Spanish 41.12 48.83 0 35.64 39.51 0 19.95 24.98 0
PairwiseCombination: To determine whether tagger performance improves by
using training data from two different source languages, without increasing the to-
tal amount of training data, we perform the following experiments. For each possi-
ble combination of two single-source taggers,wepartition theBible into two15,500-
line training sets (the first, a parallel corpus between one source language and the
target language; the second, a parallel corpus between the other source language
and the target language), and a 100-line held-out testing set. We train the first
single-source tagger on one half, train the second single-source tagger on the sec-
ond half, combine their output using the methods described in Section 2.2, and test
the resulting consensus tagger on a held-out 100-line excerpt of the French (Tables
4 and 5) or Czech (Table 6) Bibles. For each pairwise combination of taggers, we
report the percent error reduction of the combined tagger in comparison to the
average accuracy of the constituent single-source taggers.
9 Our ?generic? and ?core? tagsets correspond approximately to the ?English Equiva-
lent? and ?core? tagsets used by [21]. Since we do not have access to the same testing
set used by [21], we report results on a held-out excerpt of the Hansards corpus.
Automatically Inducing a Part-of-Speech Tagger 869
Table 4. % Accuracy of Single-Source, Pairwise-Combined, and n-way Combined Tag-
gers Using Generic Tagset on French Bible
Sources % Accuracy % Error Rate Reduction
Linear Majority
English 81.95 81.95 ?
German 81.21 81.21 ?
Spanish 79.76 79.76 ?
Eng. + Ger. 84.52 84.30 15.96
Eng. + Span. 84.42 84.48 18.91
Ger. + Span. 83.89 84.09 18.45
E. + G. + S. 85.80 85.61 25.38
Table 5. % Accuracy of Single-Source, Pairwise-Combined, and n-way Combined Tag-
gers Using Core Tagset on French Bible
Sources % Accuracy % Error Rate Reduction
Linear Majority
English 85.67 85.67 ?
German 86.66 86.66 ?
Spanish 86.54 86.54 ?
Eng. + Ger. 88.06 88.05 13.67
Eng. + Span. 88.13 88.12 14.54
Ger. + Span. 89.12 89.19 19.33
E. + G. + S. 89.87 89.43 26.11
n-Way Combination: To examine how much tagger performance can be im-
proved by increasing the total amount of training data n-fold and training each
of n single-source taggers on the full 31,000 lines of the Bible, then computing
a consensus tagger, we perform the following experiment. We train each single-
source tagger on 31,000 lines of the Bible, then compute the consensus output of
all 3 single-source taggers on a held-out 100-line excerpt of the French (Tables
4 and 5) or Czech (Table 6) Bibles. For each n-way combination of taggers, we
report the percent error reduction of the combined tagger in comparison to the
average accuracy of the constituent single-source taggers.
4 Discussion
All multiple-source taggers outperform the corresponding single-source taggers?
thus, incorporating multiple source languages improves performance, even when
the total amount of training data is held constant (as in the pairwise combination
experiments).
4.1 Single-Source Taggers
We expect performance to be highest for those source-target language pairs that
are most similar to each other, linguistically. At the generic tagset level, the
870 V. Fossum and S. Abney
Table 6. % Accuracy of Single-Source, Pairwise-Combined, and n-way Combined Tag-
gers Using Core Tagset on Czech Bible
Sources % Accuracy % Error Rate Reduction
Linear Majority
English 62.53 62.53 ?
German 65.27 65.27 ?
Spanish 63.27 63.27 ?
Eng. + Ger. 65.44 65.98 5.76
Eng. + Span. 65.41 65.28 6.77
Ger. + Span. 67.18 67.75 9.74
E. + G. + S. 67.13 67.36 10.12
poor performance of the Spanish-projected single-source tagger on French text
is partially due to a discrepancy between the SVMTool tagset [9] and our generic
tagset10. At the core tagset level, the distinction between verb tenses becomes
irrelevant, and the performance of the Spanish-projected tagger matches that of
the other single-source taggers more closely on French data; still, its performance
is lower than expected given the close morphosyntactic correspondence between
Spanish and French.11
For several reasons, we expect single-source tagger performance to be poorer
on Czech (Table 6) than on French (Tables 5 and 4). First, Czech is a ?highly
inflected? language: the role of function words in the Germanic and Romance
languages is typically filled by suffixes in Czech. Second, Czech exhibits a ?rela-
tively free word order? [7]. Since a great deal of the POS information exploited
by an HMM tagger is contained in sequences of function words12, these features
of Czech hinder the performance of an HMM POS tagger.13 Finally, Czech be-
longs to the Slavic language family, and is therefore further removed than French
from the Germanic and Romance families of the source languages used to train
the single-source taggers.
Although our single-source taggers do not replicate the performance results
reported by [21] (91% and 94% accuracy on generic and core tagsets, respec-
tively), our primary concern is not their absolute performance but rather their
10 e.g., SVMTool [9] does not make certain distinctions in verb tense that we make in
our generic tagset.
11 One likely explanation for this discrepancy is that we do not optimize the parameters
of the Spanish POS tagger used to annotate the source corpus to suit the input format
of our data set, but instead use the default settings. We estimate that optimizing
these parameters to match our data set could result in an increase of 1-2% accuracy in
the Spanish-projected source tagger for French and Czech; however, such an increase
in performance of one of the baseline experiments would not change our conclusion
in a significant way.
12 e.g., a ?DT? is likely to be followed by a ?NN? in English.
13 The Czech tagger we use for reference [13] combines a rule-based morphological
analyzer with an HMM POS tagger to combat these problems; our induced HMM
POS taggers, lacking any morphological analysis component, may not exploit the
correct type of information for such languages.
Automatically Inducing a Part-of-Speech Tagger 871
performance relative to the multiple-source taggers. We think it plausible that
the improvements we observe would also be observed with Yarowsky?s single-
source taggers, but it remains an open question.
4.2 Multiple-Source Taggers
Complementarity. Pairwise complementarity among single-source taggers is
relatively high on French at both tagset granularities (Table 3). The low pairwise
complementarity of taggers on Czech may indicate the existence of a ceiling on
the performance of the single-source tagger induction algorithm, imposed by the
limited degree of similarity between any of the source languages with the target
language. Even under such circumstances, we still see improvement (though
diminished) by combining single-source taggers for Czech.
One factor whose influence upon tagger complementarity must be acknowl-
edged is the diversity of the statistical models underlying each of the POS taggers
used to tag the source portion of the training text. Since we use a different type
of tagger to tag each source language, we cannot separate the component of
complementarity that is caused by the difference in statistical models among
sources from the component caused by the difference in languages.
Pairwise Combination. All pairwise combined taggers outperform the cor-
responding single-source taggers, though the total amount of training data is
unchanged. We observe this improvement on both French and Czech. This sug-
gests that our approach is likely to improve performance over single-source tag-
gers on a wide range of target languages, and does not depend upon a close
correspondence between any of the source and target languages.
n-Way Combination. As expected (given the n-fold increase in training data),
all n-way combined taggers outperform the corresponding single-source taggers,
suggesting that when parallel training data between a particular source-language
pair is limited, the performance of a POS tagger projected across that language
pair can be improved by the use of a parallel corpus between the target language
and a different source language.
5 Conclusion
Projection from multiple source languages significantly improves the perfor-
mance of automatically induced POS taggers on a target language. We observe
performance gains from incorporating multiple source taggers even when the to-
tal amount of training data is held constant, indicating that multiple languages
provide sources of information whose errors are independent and randomly dis-
tributed to a large extent. The approach presented here is general in that it does
not depend on any language-specific resources in the target language beyond
parallel corpora. Our results suggest that the performance of text analysis tools
induced using parallel corpora can benefit from the incorporation of resources
in other languages, even in the case of source languages belonging to distinct
linguistic families from the target language.
872 V. Fossum and S. Abney
6 Future Work
To further improve the accuracy of induced multiple-source taggers, we plan to in-
vestigate othermethods for combining the output of single-sourcePOS taggers.We
hypothesize that combining the models constructed by each tagger before applying
each tagger to the testing set would result in greater performance gains.
References
1. Yasser Al-Onaizan , Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan
Melamed, Franz-Josef Och, David Purdy, Noah Smith and David Yarowsky: Sta-
tistical machine translation. Johns Hopkins University 1999 Summer Workshop on
Language Engineering (1999)
2. Thorsten Brants: TnT ? a statistical part-of-speech tagger. In Proceedings of the
6th Applied NLP Conference, ANLP-2000, April 29 ? May 3, 2000, Seattle, WA.
(2000)
3. Eric Brill: Transformation-Based Error-Driven Learning and Natural Language
Processing: A Case Study in Part-of-Speech Tagging. Computational Linguistics
Vol. 21 No. 4 (1995) 543-565
4. Eric Brill and Jun Wu: Classifier Combination for Improving Lexical Disambigua-
tion. Proceedings of the ACL (1998)
5. Peter F. Brown, John Cocke, Stephen Della Pietra, Vincent J. Della Pietra, Freder-
ick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin: A Statistical
Approach to Machine Translation. Computational Linguistics Vol. 16 No. 2 (1990)
79?85
6. S. Clark, J. Curran, and M. Osborne: Bootstrapping POS taggers using unlabelled
data. In Walter Daelemans and Miles Osborne, editors, Proceedings of CoNLL-
2003, Edmonton, Canada (2003) 49?55
7. Michael Collins, Jan Hajic, Lance Ramshaw, and Christoph Tillmann: A Statistical
Parser for Czech. Proceedings of the 37th Annual Meeting of the ACL, College
Park, Maryland (1999)
8. Silviu Cucerzan and David Yarowsky: Bootstrapping a Multilingual Part-of-speech
Tagger in One Person-day. Proceedings of the Sixth Conference on Natural Lan-
guage Learning (CoNLL) (2002)
9. Jesus Gimenez and Lluis Marquez: SVMTool: A general POS tagger generator
based on Support Vector Machines. Proceedings of the 4th International Confer-
ence on Language Resources and Evaluation (LREC?04), Lisbon, Portugal (2004)
10. Tim Gollins and Mark Sanderson: Improving Cross Language Information Retrieval
with Triangulated Translation. Proceedings of the 24th annual international ACM
SIGIR conference 90?95 (2001)
11. French-English Hansards Corpus of Canadian Parliamentary Proceedings.
12. Jan Hajic and Barbora Hladka: Tagging Inflective Languages: Prediction of Mor-
phological Categories for a Rich, Structured Tagset, COLING-ACL (1998) 483?490
13. Jan Hajic, Pavel Krbec, Pavel Kevton, Karel Oliva, and Vladimir Petkevic: Serial
Combination of Rules and Statistics: A Case Study in Czech Tagging. Proceedings
of the ACL (2001)
14. John C. Henderson and Eric Brill: Exploiting Diversity in Natural Language Pro-
cessing: Combining Parsers. Proceedings of the 1999 Joint SIGDAT Conference
on Empirical Methods in Natural Language Processing and Very Large Corpora
(1999) 187?194
Automatically Inducing a Part-of-Speech Tagger 873
15. Rebecca Hwa, Philip Resnik, and Amy Weinberg: Breaking the Resource Bottle-
neck for Multilingual Parsing. Proceedings of the Workshop on Linguistic Knowl-
edge Acquisition and Representation: Bootstrapping Annotated Language Data
(2002)
16. Gideon Mann and David Yarowsky: Multipath translation lexicon induction via
bridge languages. In Proceedings of NAACL 2001: 2nd Meeting of the North Amer-
ican Chapter of the Association for Computational Linguistics (2001) 151?158
17. Lawrence Rabiner: A tutorial on hidden Markov models and selected applications
in speech recognition. Proceedings of the IEEE Vol. 77 No. 2 (1989)
18. Helmut Schmid: Probabilistic Part-of-Speech Tagging Using Decision Trees. Inter-
national Conference on New Methods in Language Processing, Manchester, UK.
(1994)
19. Hans van Halteren, Jakub Zavrel, and Walter Daelemans: Improving Data Driven
Wordclass Tagging by System Combination. Proceedings of the Thirty-Sixth An-
nual Meeting of the Association for Computational Linguistics (1998) 491?497
20. Ian Witten and Timothy Bell: The zero-frequency problem: Estimating the prob-
abilities of novel events in adaptive text compression. IEEE Transactions in Infor-
mation Theory, Vol. 37 No. 4 1085?1094 (1991)
21. David Yarowsky and Grace Ngai: Inducing Multilingual POS Taggers and NP
Bracketers via Robust Projection Across Aligned Corpora. Proceedings of NAACL
(2001) 200?207
22. David Yarowsky, Grace Ngai, and Richard Wicentowski: Inducing Multilingual
Text Analysis Tools via Robust Projection across Aligned Corpora. Proceedings of
HLT (2001)
23. Jakub Zavrel and Walter Daelemans: Bootstrapping a Tagged Corpus through
Combination of Existing Heterogeneous Taggers. Proceedings of LREC-2000,
Athens (2000)
Proceedings of NAACL HLT 2009: Short Papers, pages 253?256,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Combining Constituent Parsers
Victoria Fossum
Dept. of Computer Science
University of Michigan
Ann Arbor, MI 48104
vfossum@umich.edu
Kevin Knight
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
knight@isi.edu
Abstract
Combining the 1-best output of multiple
parsers via parse selection or parse hybridiza-
tion improves f-score over the best indi-
vidual parser (Henderson and Brill, 1999;
Sagae and Lavie, 2006). We propose three
ways to improve upon existing methods for
parser combination. First, we propose a
method of parse hybridization that recom-
bines context-free productions instead of con-
stituents, thereby preserving the structure of
the output of the individual parsers to a greater
extent. Second, we propose an efficient linear-
time algorithm for computing expected f-score
using Minimum Bayes Risk parse selection.
Third, we extend these parser combination
methods from multiple 1-best outputs to mul-
tiple n-best outputs. We present results on
WSJ section 23 and also on the English side
of a Chinese-English parallel corpus.
1 Introduction
Parse quality impacts the quality of downstream ap-
plications such as syntax-based machine translation
(Quirk and Corston-Oliver, 2006). Combining the
output of multiple parsers can boost the accuracy
of such applications. Parses can be combined in
two ways: parse selection (selecting the best parse
from the output of the individual parsers) or parse
hybridization (constructing the best parse by recom-
bining sub-sentential components from the output of
the individual parsers).
1.1 Related Work
(Henderson and Brill, 1999) perform parse selec-
tion by maximizing the expected precision of the
selected parse with respect to the set of parses be-
ing combined. (Henderson and Brill, 1999) and
(Sagae and Lavie, 2006) propose methods for parse
hybridization by recombining constituents.
1.2 Our Work
In this work, we propose three ways to improve upon
existing methods for parser combination.
First, while constituent recombination (Hender-
son and Brill, 1999; Sagae and Lavie, 2006) gives a
significant improvement in f-score, it tends to flatten
the structure of the individual parses. To illustrate,
Figures 1 and 2 contrast the output of the Charniak
parser with the output of constituent recombination
on a sentence from WSJ section 24. We recombine
context-free productions instead of constituents, pro-
ducing trees containing only context-free produc-
tions that have been seen in the individual parsers?
output (Figure 3).
Second, the parse selection method of (Hender-
son and Brill, 1999) selects the parse with maxi-
mum expected precision; here, we present an effi-
cient, linear-time algorithm for selecting the parse
with maximum expected f-score within the Mini-
mum Bayes Risk (MBR) framework.
Third, we extend these parser combination meth-
ods from 1-best outputs to n-best outputs. We
present results on WSJ section 23 and also on the
English side of a Chinese-English parallel corpus.
253
SBAR
IN
although
FRAG
RB
not
ADJP
RB
as
RB
sharply
PP
IN
as
NP
NP
DT
the
NN
gain
VP
VBN
reported
NP
NNP
Friday
Figure 1: Output of Charniak Parser
SBAR
IN
although
RB
not
RB
as
RB
sharply
IN
as
NP
DT
the
NN
gain
VP
VBN
reported
NP
NNP
Friday
Figure 2: Output of Constituent Recombination
2 Parse Selection
In the MBR framework, although the true reference
parse is unknown, we assume that the individual
parsers? output forms a reasonable distribution over
possible reference parses. We compute the expected
f-score of each parse tree pi using this distribution:
expected f(pi) =
?
pj
f(pi, pj) ? pr(pj)
where f(pi, pj) is the f-score of parse pi with
respect to parse pj and pr(pj) is the prior prob-
ability of parse pj . We estimate pr(pj) as fol-
lows: pr(pj) = pr(parserk) ? pr(pj |parserk),
where parserk is the parser generating pj . We
set pr(parserk) according to the proportion of sen-
tences in the development set for which the 1-best
output of parserk achieves the highest f-score of
any individual parser, breaking ties randomly.
When n = 1, pr(pj |parserk) = 1 for all pj ;
when n > 1 we must estimate pr(pj |parserk), the
distribution over parses in the n-best list output by
any given parser. We estimate this distribution us-
ing the model score, or log probability, given by
parserk to each entry pj in its n-best list:
pr(pj |parserk) = e
??scorej,k
?n
j?=1 e??scorej?,k
SBAR
IN
although
S
ADVP
ADVP
RB
not
RB
as
ADVP
RB
sharply
PP
IN
as
NP
DT
the
NN
gain
VP
VBN
reported
NP
NNP
Friday
Figure 3: Output of Context-Free Production Recombi-
nation
Parser wsj cedev test dev test
Berkeley 88.6 89.3 82.9 83.5(Petrov and Klein, 2007)
Bikel?Collins Model 2 87.0 88.2 81.2 80.6(Bikel, 2002)
Charniak 90.6 91.4 84.7 84.1(Charniak and Johnson, 2005)
Soricut?Collins Model 2 87.3 88.4 82.3 82.1(Soricut, 2004)
Stanford 85.4 86.4 81.3 80.1(Klein and Manning, 2003)
Table 1: F-Scores of 1-best Output of Individual Parsers
We tune ? on a development set to maximize f-
score,1 and select the parse pi with highest expected
f-score.
Computing exact expected f-score requires
O(m2) operations per sentence, where m is the
number of parses being combined. We can compute
an approximate expected f-score in O(m) time. To
do so, we compute expected precision for all parses
in O(m) time by associating with each unique
constituent ci a list of parses in which it occurs,
plus the total probability qi of those parses. For
each parse p associated with ci, we increment the
expected precision of that parse by qi/size(p). This
computation yields the same result as the O(m2)
algorithm. We carry out a similar operation for
expected recall. We then compute the harmonic
mean of expected precision and expected recall,
which closely approximates the true expected
f-score.
1A low value of ? creates a uniform distribution, while a
high value concentrates probability mass on the 1-best entry in
the n-best list. In practice, tuning ? produces a higher f-score
than setting ? to the value that exactly reproduces the individual
parser?s probability distribution.
254
Parse Selection: Minimum Bayes Risk
System wsj-dev wsj-test ce-dev ce-testP R F P R F P R F P R F
best individual 91.3 89.9 90.6 91.8 91.0 91.4 86.1 83.4 84.7 85.6 82.6 84.1parser
n=1 91.7 90.5 91.1 92.5 91.8 92.0 87.1 84.6 85.8 86.7 83.7 85.2
n=10 92.1 90.8 91.5 92.4 91.7 92.0 87.9 85.3 86.6 87.7 84.4 86.0
n=25 92.1 90.9 91.5 92.4 91.7 92.0 88.0 85.4 86.7 87.4 84.2 85.7
n=50 92.1 91.0 91.5 92.4 91.7 92.1 88.0 85.3 86.6 87.6 84.3 85.9
Table 2: Precision, Recall, and F-score Results from Parse Selection
3 Constituent Recombination
(Henderson and Brill, 1999) convert each parse into
constituents with syntactic labels and spans, and
weight each constituent by summing pr(parserk)
over all parsers k in whose output the constituent
appears. They include all constituents with weight
above a threshold t = m+12 , where m is the number
of input parses, in the combined parse.
(Sagae and Lavie, 2006) extend this method by
tuning t on a development set to maximize f-
score.2 They populate a chart with constituents
whose weight meets the threshold, and use a CKY-
style parsing algorithm to find the heaviest tree,
where the weight of a tree is the sum of its con-
stituents? weights. Parsing is not constrained by a
grammar; any context-free production is permitted.
Thus, the combined parses may contain context-free
productions not seen in the individual parsers? out-
puts. While this failure to preserve the structure of
individual parses does not affect f-score, it may hin-
der downstream applications.
To extend this method from 1-best to n-best
lists, we weight each constituent by summing
pr(parserk)?pr(pj |parserk) over all parses pj gen-
erated by parserk in which the constituent appears.
4 Context-Free Production Recombination
To ensure that all context-free productions in the
combined parses have been seen in the individual
parsers? outputs, we recombine context-free produc-
tions rather than constituents. We convert each parse
into context-free productions, labelling each con-
stituent in the production with its span and syntac-
tic category and weighting each production by sum-
2A high threshold results in high precision, while a low
threshold results in high recall.
ming pr(parserk) ? pr(pj |parserk) over all parses
pj generated by parserk in which the production ap-
pears. We re-parse the sentence with these produc-
tions, returning the heaviest tree (where the weight
of a tree is the sum of its context-free productions?
weights). We optimize f-score by varying the trade-
off between precision and recall using a derivation
length penalty, which we tune on a development
set.3
5 Experiments
Table 1 illustrates the 5 parsers used in our combi-
nation experiments and the f-scores of their 1-best
output on our data sets. We use the n-best output
of the Berkeley, Charniak, and Soricut parsers, and
the 1-best output of the Bikel and Stanford parsers.
All parsers were trained on the standard WSJ train-
ing sections. We use two corpora: the WSJ (sec-
tions 24 and 23 are the development and test sets, re-
spectively) and English text from the LDC2007T02
Chinese-English parallel corpus (the development
and test sets contain 400 sentences each).
6 Discussion & Conclusion
Results are shown in Tables 2, 3, and 4. On both
test sets, constituent recombination achieves the best
f-score (1.0 points on WSJ test and 2.3 points on
Chinese-English test), followed by context-free pro-
duction combination, then parse selection, though
the differences in f-score among the combination
methods are not statistically significant. Increasing
the n-best list size from 1 to 10 improves parse se-
lection and context-free production recombination,
3By subtracting higher(lower) values of this length penalty
from the weight of each production, we can encourage the com-
bination method to favor trees with shorter(longer) derivations
and therefore higher precision(recall) at the constituent level.
255
Parse Hybridization: Constituent Recombination
System wsj-dev wsj-test ce-dev ce-testP R F P R F P R F P R F
best individual 91.3 89.9 90.6 91.8 91.0 91.4 86.1 83.4 84.7 85.6 82.6 84.1parser
n=1 92.5 90.3 91.4 93.0 91.6 92.3 89.2 84.6 86.8 89.1 83.6 86.2
n=10 92.6 90.5 91.5 93.1 91.7 92.4 89.9 84.4 87.1 89.9 83.2 86.4
n=25 92.6 90.5 91.5 93.2 91.7 92.4 89.9 84.4 87.0 89.7 83.4 86.4
n=50 92.6 90.5 91.5 93.1 91.7 92.4 89.9 84.4 87.1 89.7 83.2 86.3
Table 3: Precision, Recall, and F-score Results from Constituent Recombination
Parse Hybridization: Context-Free Production Recombination
System wsj-dev wsj-test ce-dev ce-testP R F P R F P R F P R F
best individual 91.3 89.9 90.6 91.8 91.0 91.4 86.1 83.4 84.7 85.6 82.6 84.1parser
n=1 91.7 91.0 91.4 92.1 91.9 92.0 86.9 85.4 86.2 86.2 84.3 85.2
n=10 92.1 90.9 91.5 92.5 91.8 92.2 87.8 85.1 86.4 86.2 84.3 86.1
n=25 92.2 91.0 91.6 92.5 91.8 92.2 87.8 85.1 86.4 87.6 84.6 86.1
n=50 92.1 90.8 91.4 92.4 91.7 92.1 87.6 84.9 86.2 87.7 84.6 86.1
Table 4: Precision, Recall, and F-score Results from Context-Free Production Recombination
though further increasing n does not, in general,
help.4 Chinese-English test set f-score gets a bigger
boost from combination than WSJ test set f-score,
perhaps because the best individual parser?s baseline
f-score is lower on the out-of-domain data.
We have presented an algorithm for parse hy-
bridization by recombining context-free produc-
tions. While constituent recombination results in
the highest f-score of the methods explored, context-
free production recombination produces trees which
better preserve the syntactic structure of the indi-
vidual parses. We have also presented an efficient
linear-time algorithm for selecting the parse with
maximum expected f-score.
Acknowledgments
We thank Steven Abney, John Henderson, and
Kenji Sagae for helpful discussions. This research
was supported by DARPA (contract HR0011-06-C-
0022) and by NSF ITR (grant IIS-0428020).
4These diminishing gains in f-score as n increases reflect
the diminishing gains in f-score of the oracle parse produced by
each individual parser as n increases.
References
Daniel M. Bikel. 2004. Design of a Multi-lingual,
Parallel-processing Statistical Parsing Engine. In Pro-
ceedings of HLT.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of ACL.
Michael Collins and Terry Koo. 2005. Discriminative
Reranking for Natural Language Parsing. Computa-
tional Linguistics, 31(1):25-70.
John C. Henderson and Eric Brill. 2000. Exploiting Di-
versity in Natural Language Processing: Combining
Parsers. In Proceedings of EMNLP.
Dan Klein and Christopher D. Manning. 2003. Accurate
Unlexicalized Parsing. In Proceedings of ACL.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Proceedings of HLT-
NAACL.
Chris Quirk and Simon Corston-Oliver. 2006. The Im-
pact of Parse Quality on Syntactically-Informed Statis-
tical Machine Translation. In Proceedings of EMNLP.
Kenji Sagae and Alon Lavie. 2006. Parser Combination
by Reparsing. In Proceedings of HLT-NAACL.
Radu Soricut. 2004. A Reimplementation of Collins?
Parsing Models. Technical report, Information Sci-
ences Institute, Department of Computer Science, Uni-
versity of Southern California.
256
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 48?55,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Latent Features in Automatic Tense Translation between Chinese and
English
Yang Ye?, Victoria Li Fossum?, Steven Abney ? ?
? Department of Linguistics
? Department of Electrical Engineering and Computer Science
University of Michigan
Abstract
On the task of determining the tense to use
when translating a Chinese verb into En-
glish, current systems do not perform as
well as human translators. The main focus
of the present paper is to identify features
that human translators use, but which are
not currently automatically extractable.
The goal is twofold: to test a particu-
lar hypothesis about what additional infor-
mation human translators might be using,
and as a pilot to determine where to focus
effort on developing automatic extraction
methods for features that are somewhat be-
yond the reach of current feature extrac-
tion. The paper shows that incorporating
several latent features into the tense clas-
sifier boosts the tense classifier?s perfor-
mance, and a tense classifier using only the
latent features outperforms one using only
the surface features. Our findings confirm
the utility of the latent features in auto-
matic tense classification, explaining the
gap between automatic classification sys-
tems and the human brain.
1 Introduction
Language speakers make two types of distinctions
about temporal relations: the first type of relation
is based on precedence between events and can be
expanded into a finer grained taxonomy as pro-
posed by (Allen, 1981). The second type of re-
lation is based on the relative positioning between
the following three time parameters proposed by
(Reichenbach, 1947): speech time (S), event time
(E) and reference time (R). In the past couple of
decades, the NLP community has seen an emer-
gent interest in the first type of temporal relation.
In the cross-lingual context, while the first type of
relationship can be easily projected across a lan-
guage pair, the second type of relationship is of-
ten hard to be projected across a language pair. In
contrast to this challenge, cross-lingual temporal
reference distinction has been poorly explored.
Languages vary in the granularity of their
tense and aspect representations; some have finer-
grained tenses or aspects than others. Tense gener-
ation and tense understanding in natural language
texts are highly dynamic and context-dependent
processes, since any previously established time
point or interval, whether explicitly mentioned in
the context or not, could potentially serve as the
reference time for the event in question. (Bruce,
1972) captures this nature of temporal reference
organization in discourse through a multiple tem-
poral reference model. He defines a set (S
1
, S
2
, ...,
Sn) that is an element of tense. S1 corresponds to
the speech time, Sn is the event time, and (Si, i=2,
..., n-1) stand for a sequence of time references
from which the reference time of a particular event
could come. Given the elusive nature of reference
time shift, it is extremely hard to model the ref-
erence time point directly in temporal information
processing. The above reasons motivate classify-
ing temporal reference distinction automatically,
using machine learning algorithms such as Con-
ditional Random Fields (CRFs).
Many researchers in Natural Language Process-
ing seem to believe that an automatic system does
not have to follow the mechanism of human brain
in order to optimize its performance, for example,
the feature space for an automatic classification
system does not have to replicate the knowledge
sources that human beings utilize. There has been
very little research that pursues to testify this faith.
The current work attempts to identify which
features are most important for tense generation
in Chinese to English translation scenario, which
can point to direction of future research effort for
automatic tense translation between Chinese and
English.
48
The remaining part of the paper is organized
as follows: Section 2 summarizes the significant
related works in temporal information annotation
and points out how this study relates to yet dif-
fers from them. Section 3 formally defines the
problem, tense taxonomy and introduces the data.
Section 4 discusses the feature space and proposes
the latent features for the tense classification task.
Section 5 presents the classification experiments
in Conditional Random Fields as well as Classifi-
cation Tree and reports the evaluation results. Sec-
tion 6 concludes the paper and section 7 points out
directions for future research.
2 Related Work
There is an extensive literature on temporal infor-
mation processing. (Mani, et al, 2005) provides
a survey of works in this area. Here, we high-
light several works that are closely related to Chi-
nese temporal information processing. (Li, 2001)
describes a model of mining and organizing tem-
poral relations embedded in Chinese sentences,
in which a set of heuristic rules are developed to
map linguistic patterns to temporal relations based
on Allen?s thirteen relations. Their work shows
promising results via combining machine learning
techniques and linguistic features for successful
temporal relation classification, but their work is
concerned with another type of temporal relation-
ship, namely, the precedence-based temporal rela-
tion between a pair of events explicitly mentioned
in text.
A significant work worth mentioning is (Olsen
et. al. 2001)?s paper, where the authors exam-
ine the determination of tense for English verbs
in Chinese-to-English translation. In addition to
the surface features such as the presence of aspect
markers and certain adverbials, their work makes
use of the telicity information encoded in the lexi-
cons through the use of Lexical Conceptual Struc-
tures (LCS). Based on the dichotomy of grammat-
ical aspect and lexical aspect, they propose that
past tense corresponds to the telic (either inher-
ently or derived) LCS. They propose a heuristic
algorithm in which grammatical aspect markings
supersede the LCS, and in the absence of gram-
matical aspect marking, verbs that have telic LCS
are translated into past tense and present tense oth-
erwise. They report a significant performance im-
provement in tense resolution from adding a verb
telicity feature. They also achieve better perfor-
mance than the baseline system using the telic-
ity feature alone. This work, while alerting re-
searchers to the importance of lexical aspectual
feature in determination of tense for English verbs
in Chinese-to-English machine translation, is sub-
ject to the risk of adopting a one-to-one mapping
between grammatical aspect markings and tenses
hence oversimplifies the temporal reference prob-
lem in Chinese text. Additionally, their binary
tense taxonomy is too coarse for the rich tempo-
ral reference system in Chinese.
(Ye, et al 2005) reported a tense tagging case
study of training Conditional Random Fields on
a set of shallow surface features. The low inter-
annotator agreement rate reported in the paper il-
lustrates the difficulty of tense tagging. Neverthe-
less, the corpora size utilized is too small with only
52 news articles and none of the latent features was
explored, so the evaluation result reported in the
paper leaves room for improvement.
3 Problem Definition
3.1 Problem Formulation
The problem we are interested in can be formal-
ized as a standard classification or labeling prob-
lem, in which we try to learn a classifier
C : V ? T (1)
where V is a set of verbs (each described by a
feature vector), and T is the set of possible tense
tags.
Tense and aspect are morphologically merged
in English and coarsely defined, there can be
twelve combinations of the simple tripartite tenses
(present, past and future) with the progressive and
perfect grammatical aspects. For our classification
experiments, in order to combat sparseness, we ig-
nore the aspects and only deal with the three sim-
ple tenses: present, past and future.
3.2 Data
We use 152 pairs of parallel Chinese-English arti-
cles from LDC release. The Chinese articles come
from two news sources: Xinhua News Service and
Zaobao News Service, consisting of 59882 Chi-
nese characters in total with roughly 350 charac-
ters per article. The English parallel articles are
from Multiple-Translation Chinese (MTC) Corpus
from LDC with catalog number LDC2002T01.
We chose to use the best human translation out
49
of 9 translation teams as our gold-standard par-
allel English data. The verb tenses are obtained
through manual alignment between the Chinese
source articles and the English translations. In or-
der to avoid the noise brought by errors and be fo-
cused on the central question we try to answer in
the paper, we did not use automatic tools such as
GIZA++ to obtain the verb alignments, which typ-
ically comes with significant amount of errors. We
ignore Chinese verbs that are not translated into
English as verbs because of ?nominalization? (by
which verbal expressions in Chinese are translated
into nominal phrases in English). This exclusion is
based on the rationale that another choice of syn-
tactic structure might retain the verbal status in the
target English sentence, but the tense of those po-
tential English verbs would be left to the joint de-
cision of a set of disparate features. Those tenses
are unknown in our training data. This preprocess-
ing yields us a total of 2500 verb tokens in our data
set.
4 Feature Space
4.1 Surface Features
There are many heterogeneous features that con-
tribute to the process of tense generation for Chi-
nese verbs in the cross-lingual situation. Tenses in
English, while manifesting a distinction in tempo-
ral reference, do not always reflect this distinction
at the semantic level, as is shown in the sentence ?I
will leave when he comes.? (Hornstein, 1990) ac-
counts for this phenomenon by proposing the Con-
straints on Derived Tense Structures. Therefore,
the feature space we use includes the features that
contribute to the semantic level temporal reference
construction as well as those contributing to tense
generation from that semantic level. The follow-
ing is a list of the surface features that are directly
extractable from the training data:
1. Feature 1: Whether the verb is in quoted
speech or not.
2. Feature 2: The syntactic structure in which
the current verb is embedded. Possible struc-
tures include sentential complements, rel-
ative clauses, adverbial clauses, appositive
clauses, and null embedding structure.
3. Feature 3: Which of the following signal
adverbs occur between the current verb
and the previous verb: yi3jing1(already),
ceng2jing1(once), jiang1(future tense
marker), zheng4zai4(progressive aspect
marker), yi4zhi2(have always been).
4. Feature 4: Which of the following aspect
markers occur between the current verb and
the subsequent verb: le0, zhe0, guo4.
5. Feature 5: The distance in characters between
the current verb and the previously tagged
verb (We descretize the continuous distance
into three ranges: 0 < distance < 5, 5 ?
distance < 10, or 10 ? distance <?).
6. Feature 6: Whether the current verb is in the
same clause as the previous verb.
Feature 1 and feature 2 are used to capture the
discrepancy between semantic tense and syntactic
tense. Feature 3 and feature 4 are clues or triggers
of certain aspectual properties of the verbs. Fea-
ture 5 and feature 6 try to capture the dependency
between tenses of adjacent verbs.
4.2 Latent Features
The bottleneck in Artificial Intelligence is the un-
balanced knowledge sources shared by human be-
ings and a computer system. Only a subset of
the knowledge sources used by human beings can
be formalized, extracted and fed into a computer
system. The rest are less accessible and are very
hard to be shared with a computer system. De-
spite their importance in human language process-
ing, latent features have received little attention in
feature space exploration in most NLP tasks be-
cause they are impractical to extract. Although
there have not yet been rigorous psycholinguis-
tic studies demonstrating the extent to which the
above knowledge types are used in human tempo-
ral relation processing, we hypothesize that they
are very significant in assisting human?s temporal
relation decision. Nevertheless, a quantitative as-
sessment of the utility of the latent features in NLP
tasks has yet to be explored. (Olsen, et al, 2001)
illustrates the value of latent features by showing
how the telicity feature alone can help with tense
resolution in Chinese to English machine transla-
tion. Given the prevalence of latent features in hu-
man language processing, in order to emulate hu-
man beings performance of the disambiguation, it
is crucial to experiment with the latent features in
automatic tense classification.
(Pustejovsky, 2004) discusses the four basic
problems in event-temporal identification:
50
??????????????????????????????????
??????????
He said that Henan Province not only possesses the hardwares necessary for foreign 
investment, but also has, on the basis of the State policies and Henan's specific 
conditions, formulated its own preferential policies.
? ?? ???? ??
N/A include subsume
?? ??
Figure 1: Temporal Relations between Adjacent Events
1. Time-stamping of events (identifying an
event and anchoring it in time)
2. Ordering events with respect to one another
3. Reasoning with contextually under-specified
temporal expressions
4. Reasoning about the persistence of events
(how long does an event or the outcome of
an event last?)
While time-stamping of the events and reason-
ing with contextually under-specified temporal ex-
pressions might be too informative to be features
in tense classification, information concerning or-
derings between events and persistence of events
are relatively easier to be encoded as features in
a tense classification task. Therefore, we exper-
iment with these two latent knowledge sources,
both of which are heavily utilized by human be-
ings in tense resolution.
4.3 Telicity and Punctuality Features
Following (Vendler, 1947), temporal information
encoded in verbs is largely captured by some in-
nate properties of verbs, of which telicity and
punctuality are two very important ones. Telic-
ity specifies a verb?s ability to be bound in a cer-
tain time span, while punctuality specifies whether
or not a verb is associated with a point event in
time. Telicity and punctuality prepare verbs to be
assigned different tenses when they enter the con-
text in the discourse. While it is true that isolated
verbs are typically associated with certain telicity
and punctuality features, such features are contex-
tually volatile. In reaction to the volatility exhib-
ited in verb telicity and punctuality features, we
propose that verb telicity and punctuality features
should be evaluated only at the clausal or senten-
tial level for the tense classification task. We man-
ually obtained these two features for both the En-
glish and the Chinese verbs. All verbs in our data
set were manually tagged as ?telic? or ?atelic?, and
?punctual? or ?apunctual?, according to context.
4.4 Temporal Ordering Feature
(Allen, 1981) defines thirteen relations that could
possibly hold between any pair of situations. We
experiment with six temporal relations which we
think represent the most typical temporal relation-
ships between two events. We did not adopt all
of the thirteen temporal relationships proposed by
Allen for the reason that some of them would re-
quire excessive deliberation from the annotators
and hard to implement. The six relationships we
explore are as follows:
1. event A precedes event B
2. event A succeeds event B
3. event A includes event B
4. event A subsumes event B
5. event A overlaps with event B
6. no temporal relations between event A and
event B
For each Chinese verb in the source Chinese
texts, we annotate the temporal relation between
the verb and the previously tagged verb as belong-
ing to one of the above classes. The annotation
of the temporal relation classes mimics a deeper
semantic analysis of the Chinese source text. Fig-
ure 1 illustrates a sentence in which each verb is
tagged by the temporal relation class that holds be-
tween it and the previous verb.
51
5 Experiments and Evaluation
5.1 CRF learning algorithms
Conditional Random Fields (CRFs) are a formal-
ism well-suited for learning and prediction on se-
quential data in many NLP tasks. It is a prob-
abilistic framework proposed by (Lafferty et al,
2001) for labeling and segmenting structured data,
such as sequences, trees and lattices. The condi-
tional nature of CRFs relaxes the independence as-
sumptions required by traditional Hidden Markov
Models (HMMs). This is because the conditional
model makes it unnecessary to explicitly represent
and model the dependencies among the input vari-
ables, thus making it feasible to use interacting and
global features from the input. CRFs also avoid
the label bias problem exhibited by maximum en-
tropy Markov models (MEMMs) and other con-
ditional Markov models based on directed graph-
ical models. CRFs have been shown to perform
well on a number of NLP problems such as shal-
low parsing (Sha and Pereira, 2003), table extrac-
tion (Pinto et al, 2003), and named entity recog-
nition (McCallum and Li, 2003). For our exper-
iments, we use the MALLET implementation of
CRF?s (McCallum, 2002).
5.2 Experiments
5.2.1 Human Inter-Annotator Agreement
All supervised learning algorithms require a
certain amount of training data, and the reliability
of the computational solutions is intricately tied
to the accuracy of the annotated data. Human an-
notations typically suffer from errors, subjectivity,
and the expertise effect. Therefore, researchers
use consistency checking to validate human an-
notation experiments. The Kappa Statistic (Co-
hen, 1960) is a standard measurement of inter-
annotator agreement for categorical data annota-
tion. The Kappa score is defined by the following
formula, where P(A) is the observed agreement
rate from multiple annotators and P(E) is the ex-
pected rate of agreement due to pure chance:
k = P (A)? P (E)
1? P (E)
(2)
Since tense annotation requires disambiguating
grammatical meaning, which is more abstract than
lexical meaning, one would expect the challenge
posed by human annotators in a tense annota-
tion experiment to be even greater than for word
sense disambiguation. Nevertheless, the tense an-
notation experiment carried as a precursor to our
tense classification task showed a kappa Statistic
of 0.723 on the full taxonomy, with an observed
agreement of 0.798. In those experiments, we
asked three bilingual English native speakers who
are fluent in Chinese to annotate the English verb
tenses for the first 25 Chinese and English parallel
news articles from our training data.
We could also obtain a measurement of reliabil-
ity by taking one annotator as the gold standard
at one time, then averaging over the precisions of
the different annotators across different gold stan-
dards. While it is true that numerically, precision
would be higher than Kappa score and seems to
be inflating Kappa score, we argue that the dif-
ference between Kappa score and precision is not
limited to one measure being more aggressive than
the other. Rather, the policies of these two mea-
surements are different. The Kappa score cares
purely about agreement without any consideration
of trueness or falseness, while the procedure we
described above gives equal weight to each anno-
tator being the gold standard, and therefore con-
siders both agreement and truthness of the annota-
tion. The advantage of the precision-based agree-
ment measurement is that it makes comparison of
the system performance accuracy to the human
performance accuracy more direct. The precision
under such a scheme for the three annotators is
80% on the full tense taxonomy.
5.2.2 CRF Learning Experiments
We train a tense classifier on our data set in two
stages: first on the surface features, and then on
the combined space of both surface features (dis-
cussed in 4.1) and latent features (discussed in 4.2-
4.4). It is conceivable that the granularity of se-
quences may matter in learning from data with se-
quential relationship, and in the context of verb
tense tagging, it naturally maps to the granularity
of discourse. (Ye, et al, 2005) shows that there
is no significant difference between sentence-level
sequences and paragraph-level sequences. There-
fore, we experiment with only sentence-level se-
quences.
5.2.3 Classification Tree Learning
Experiments
To verify the stability of the utility of the la-
tent features, we also experiment with classifica-
tion tree learning on the same features space as
52
Tense Precision Recall F
Present tense 0.662 0.661 0.627
Past tense 0.882 0.915 0.896
Future tense 0.758 0.487 0.572
Table 1: Evaluation Results for CRFs Classifier in Precision, Recall and F Using All Features
Surface Features Latent Features Surface and Latent Features
Accuracy for Training Data 79.3% 82.9% 85.9%
Table 2: Apparent Accuracy for the Training Data of the Classification Tree Classifiers
discussed above. Classification Trees are used
to predict membership of cases or objects in the
classes of a categorical dependent variable from
their measurements on one or more predictor vari-
ables. The main idea of Classification Tree is to
do a recursive partitioning of the variable space
to achieve good separation of the classes in the
training dataset. We use the Recursive Partition-
ing and Regression Trees(Rpart) package provided
by R statistical computing software for the imple-
mentation of classification trees. In order to avoid
over-fitting, we prune the tree by setting the min-
imum number of objects in a node to attempt a
split and the minimum number of objects in any
terminal node to be 10 and 3 respectively. In the
constructed classification tree when we use all fea-
tures including both surface and latent features,
the top split at the root node in the tree is based
on telicity feature of the English verb, indicating
the importance of telicity feature for English verb
among all of the features.
5.3 Evaluation Results
All results are obtained by 5-fold cross validation.
The classifier?s performance is evaluated against
the tenses from the best-ranked human-generated
English translation. To evaluate the performance
of the CRFs tense classifier, we compute the pre-
cision, recall, general accuracy and F, which are
defined as follow.
Accuracy =
nprediction
Nprediction
(3)
Recall = nhit
S
(4)
Precision = nhit
Nhit
(5)
F = 2? Precision ?Recall
Precision + Recall
(6)
where
1. Nprediction: Total number of predictions;
2. nprediction: Number of correct predictions;
3. Nhit: Total number of hits;
4. nhit: Number of correct hits;
5. S: Size of perfect hitlist;
From Table 1, we see that past tense, which oc-
curs most frequently in the training data, has the
highest precision, recall and F. Future tense, which
occurs least frequently, has the lowest F. Precision
and recall do not show clear pattern across differ-
ent tense classes.
Table 2 presents the apparent classification ac-
curacies for the training data, we see that latent
features still outperform the surface features. Ta-
ble 3 summarizes the general accuracies of the
tense classification systems for CRFs and Classifi-
cation Trees. The CRFs classifier and the Classifi-
cation Tree classifier demonstrate similar scales of
improvement from surface features, latent features
to both surface and latent features.
53
Methodology Surface Features Latent Features Surface and Latent Features
CRFs 75.8% 80% 83.4%
Classification Tree 74.1% 81% 84.5%
Table 3: Evaluations in General Accuracy
5.4 Baseline Systems
To better evaluate our tense classifiers, we provide
two baseline systems here. The first baseline sys-
tem is the tense resolution from the best ranked
machine translation system?s translation results in
the MTC corpus mentioned above. When evalu-
ated against the reference tense tags from the best
ranked human translation team, the best MT sys-
tem yields a accuracy of 47%. The second base-
line system is a naive system that assigns the most
frequent tense in the training data set, which in our
case is past tense, to all verbs in the test data set.
Given the fact that we are deadling with newswire
data, this baseline system yields a high baseline
system with an accuracy of 69.5%.
6 Discussion and Conclusions
To the best of our knowledge, the current paper
is the first work investigating the utility of latent
features in the task of machine-learning based au-
tomatic tense classification. We significantly out-
perform the two baseline systems as well as the
automatic tense classifier performance reported by
(Ye, et al, 2005) by 15% in general accuracy. A
crucial finding of our experiments is that utility of
only three latent features, i.e. verb telicity, verb
punctuality and temporal ordering between adja-
cent events, outperforms that of all the surface
linguistic features we discussed earlier in the pa-
per. While one might think that the lack of exist-
ing techonology of latent feature extraction would
discount research effort on latent features? utili-
ties, we believe that such efforts guide the research
community to determine where to focus effort on
developing automatic extraction methods for fea-
tures that are beyond the reach of current tech-
nologies. Such research effort will also help to
shed light on the enigmatic research question of
whether automatic NLP systems should take ef-
fort to make use of the features employed by hu-
man beings to optimize the system performance
and shorten the gap between the system and hu-
man brain. The results of the current paper point
to the fact that bottleneck of cross-linguistic tense
classification is acquisition and modeling of the
more latent linguistic knowledge. To our surprise,
CRF tense classifier performance is consistently
tied with classification tree tense classifier perfor-
mance in all of our experiments. One might expect
that CRFs would accurately capture sequential de-
pendencies among verbs. Reflecting upon the sim-
ilar evaluation results of the CRFs classifier and
the Classification Tree classifier, it is unlikely for
this to be due to the over-fitting of the Classifi-
cation Tree because of the pruning we did to the
Classification Trees. Therefore, we speculate that
the dependencies between the tense tags of verbs
in the texts may not be strong enough for CRFs
to outperform Classification Tree. This might also
be contributable to the built-in variable selection
procedures of Classification Trees, which makes
it more robust to interacting and interdependent
features. A confirmative explanation towards the
equal performances between the CRFs and the
Classification Tree classifiers requires more exper-
iments with other machine learning algorithms.
In conclusion, this paper makes the following
contributions:
1. It demonstrates that an accurate tense classi-
fier can be constructed automatically by com-
bining off-the-shelf machine learning tech-
niques and inexpensive linguistic features.
2. It shows that latent features (such as verb
telicity, verb punctuality and temporal order-
ing between adjacent events) have higher util-
ity in tense classification than the surface lin-
guistic features.
3. It reveals that the sequential dependency be-
tween tenses of adjacent verbs in the dis-
course may be rather weak.
54
7 Future Work
Temporal reference is a complicated semantic do-
main with rich connections among the disparate
features. We investigate three latent features:
telicity, punctuality, and temporal ordering be-
tween adjacent verbs. We summarize several in-
teresting questions for future research in this sec-
tion. First, besides the latent features we examined
in the current paper, there are other interesting
latent features to be investigated under the same
theme, e.g. classes of temporal expression associ-
ated with the verbs and causal relationships among
disparate events. Second, currently, the latent fea-
tures are obtained through manual annotation by
a single annotator. In an ideal situation, multi-
ple annotators are desired to provide the reliabil-
ity of the annotations as well as reduce the noise
in annotations. Thirdly, it would be interesting to
examine the utility of the same latent features for
classification in the opposite direction, namely, as-
pect marker classification for Chinese verbs in the
English-to-Chinese translation scenario. Finally,
following our discussion of the degree of depen-
dencies among verb tenses in the texts, it is desir-
able to study rigorously the dependencies among
tenses and aspect markers for verbs in extensions
of the current research.
References
James Allen. 1981. Towards a General Theory of Ac-
tion and Time. Artificial Intelligence, 23(2): 123-
160.
Hans Reichenbach. 1947. Elements of Symbolic Logic.
Macmillan, New York, N.Y.
B. Bruce. 1972. A Model for Temporal Reference and
its Application in a Question Answering System, Ar-
tificial Intelligence. Vol. 3, No. 1, 1-25.
Inderjeet Mani, James Pustejovsky and Robert
Gaizauskas. 2005. The Language of Time, Oxford
Press.
Wenjie Li, Kam-Fai Wong, Caogui Hong, Chunfa
Yuan. 2004. Applying Machine Learning to Chinese
Temporal Relation Resolution. Proceedings of the
42nd Annual Meeting of the Association for Com-
putational Linguistics, 582-588.
Mari Olson, David Traum, Carol Van-ess Dykema,
and AmyWeinberg. 2001. Implicit Cues for Explicit
Generation: Using Telicity as a Cue for Tense Struc-
ture in a Chinese to English MT System. Proceed-
ings Machine Translation Summit VIII, Santiago de
Compostela, Spain.
Yang Ye, Zhu Zhang. 2005. Tense Tagging for Verbs in
Cross-Lingual Context: a Case Study. Proceedings
of IJCNLP 2005, 885-895
Norbert Hornstein. 1990. As Time Goes By: Tense and
Universal Grammar. The MIT Press.
James Pustejovsky, Robert Ingria, Roser Sauri, Jose
Castano, Jessica Littman, Rob Gaizauskas, Andrea
Setzer, Graham Katz, and Inderjeet Mani. 2004. The
Specification Language TimeML. The Language of
Time: A Reader. Oxford, 185-96.
Zeno Vendler. 1967. Verbs and Times. Linguistics in
Philosophy, 97-121.
Lafferty, J., McCallum, A. and Pereira, F. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML-01, 282-289.
Sha, F. and Pereira, F. 2003. Shallow Parsing with
Conditional Random Fields. Proceedings of the
2003 Human Language Technology Conference and
North American Chapter of the Association for
Computational Linguistics (HLT/NAACL-03)
Pinto, D., McCallum, A., Lee, X. and Croft, W. B.
2003. Table Extraction Using Conditional Random
Fields. Proceedings of the 26th Annual International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval (SIGIR 2003)
McCallum, A. and Li, W. 2003. Early Results for
Named Entity Recognition with Conditional Ran-
dom Fields, Feature Induction and Web-Enhanced
Lexicons. Proceedings of the Seventh Conference on
Natural Language Learning (CoNLL)
McCallum, A. K. 2002. MALLET: A Ma-
chine Learning for Language Toolkit,
http://mallet.cs.umass.edu.
Jacob Cohen, 1960. A Coefficient of Agreement for
Nominal Scales, Educational and Psychological
Measurement, 20, 37-46.
Ross Ihaka and Robert Gentleman. 1996. R: A Lan-
guage for Data Analysis and Graphics, Journal
of Computational and Graphical Statistics, Vol. 5.
299?14.
55
Proceedings of the Third Workshop on Statistical Machine Translation, pages 44?52,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Using Syntax to Improve Word Alignment Precision for Syntax-Based
Machine Translation
Victoria Fossum
Dept. of Computer Science
University of Michigan
Ann Arbor, MI 48104
vfossum@umich.edu
Kevin Knight
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
knight@isi.edu
Steven Abney
Dept. of Linguistics
University of Michigan
Ann Arbor, MI 48104
abney@umich.edu
Abstract
Word alignments that violate syntactic cor-
respondences interfere with the extraction
of string-to-tree transducer rules for syntax-
based machine translation. We present an
algorithm for identifying and deleting incor-
rect word alignment links, using features of
the extracted rules. We obtain gains in both
alignment quality and translation quality in
Chinese-English and Arabic-English transla-
tion experiments relative to a GIZA++ union
baseline.
1 Introduction
1.1 Motivation
Word alignment typically constitutes the first stage
of the statistical machine translation pipeline.
GIZA++ (Och and Ney, 2003), an implementation
of the IBM (Brown et al, 1993) and HMM (?)
alignment models, is the most widely-used align-
ment system. GIZA++ union alignments have been
used in the state-of-the-art syntax-based statistical
MT system described in (Galley et al, 2006) and in
the hierarchical phrase-based system Hiero (Chiang,
2007). GIZA++ refined alignments have been used
in state-of-the-art phrase-based statistical MT sys-
tems such as (Och, 2004); variations on the refined
heuristic have been used by (Koehn et al, 2003)
(diag and diag-and) and by the phrase-based system
Moses (grow-diag-final) (Koehn et al, 2007).
GIZA++ union alignments have high recall but
low precision, while intersection or refined align-
ments have high precision but low recall.1 There are
two natural approaches to improving upon GIZA++
alignments, then: deleting links from union align-
ments, or adding links to intersection or refined
alignments. In this work, we delete links from
GIZA++ union alignments to improve precision.
The low precision of GIZA++ union alignments
poses a particular problem for syntax-based rule ex-
traction algorithms such as (Quirk et al, 2005; Gal-
ley et al, 2006; Huang et al, 2006; Liu et al,
2006): if the incorrect links violate syntactic corre-
spondences, they force the rule extraction algorithm
to extract rules that are large in size, few in number,
and poor in generalization ability.
Figure 1 illustrates this problem: the dotted line
represents an incorrect link in the GIZA++ union
alignment. Using the rule extraction algorithm de-
scribed in (Galley et al, 2004), we extract the rules
shown in the leftmost column (R1?R4). Rule R1 is
large and unlikely to generalize well. If we delete
the incorrect link in Figure 1, we can extract the
rules shown in the rightmost column (R2?R9): Rule
R1, the largest rule from the initial set, disappears,
and several smaller, more modular rules (R5?R9) re-
place it.
In this work, we present a supervised algorithm
that uses these two features of the extracted rules
(size of largest rule and total number of rules), as
well as a handful of structural and lexical features,
to automatically identify and delete incorrect links
from GIZA++ union alignments. We show that link
1For a complete discussion of alignment symmetrization
heuristics, including union, intersection, and refined, refer to
(Och and Ney, 2003).
44
VP
VBZ
starts
PRT
RP
out
PP
IN
from
NP
NP
DT
the
NNS
needs
PP
IN
of
NP
PRP
its
JJ
own
NN
country
,
? ) ?  ? 
FROM OWN-COUNTRY NEEDS STARTS-OUT
Rules Extracted Using GIZA++ Union Alignments Rules Extracted After Deleting Dotted Link
R1: VP
VBZ
starts
PRT
RP
out
PP
x0:IN NP
NP
DT
the
NNS
needs
x1:PP
? x0 x1 ?  ?  R2: IN
from
? ,
R2: IN
from
? , R3: PP
IN
of
x0:NP
? x0
R3: PP
IN
of
x0:NP
? x0 R4: NP
PRP
its
JJ
own
NN
country
? ? )
R4: NP
PRP
its
JJ
own
NN
country
? ? ) R5: PP
x0:IN x1:NP
?x0 x1
R6: NP
x0:NP x1:PP
? x1 x0
R7: NP
DT
the
x0:NNS
? x0
R8: NNS
needs
? ? 
R9: VP
VBZ
starts
PRT
RP
out
x0:PP
? x0 ? 
Figure 1: The impact of incorrect alignment links upon rule extraction. Using the original alignment (including all
links shown) leads to the extraction of the tree-to-string transducer rules whose left hand sides are rooted at the solid
boxed nodes in the parse tree (R1, R2, R3, and R4). Deleting the dotted alignment link leads to the omission of rule
R1, the extraction of R9 in its place, the extraction of R2, R3, and R4 as before, and the extraction of additional rules
whose left hand sides are rooted at the dotted boxed nodes in the parse tree (R5, R6, R7, R8).
45
deletion improves alignment quality and translation
quality in Chinese-English and Arabic-English MT,
relative to a strong baseline. Our link deletion al-
gorithm is easy to implement, runs quickly, and has
been used by a top-scoring MT system in the Chi-
nese newswire track of the 2008 NIST evaluation.
1.2 Related Work
Recently, discriminative methods for alignment
have rivaled the quality of IBM Model 4 alignments
(Liu et al, 2005; Ittycheriah and Roukos, 2005;
Taskar et al, 2005; Moore et al, 2006; Fraser and
Marcu, 2007b). However, except for (Fraser and
Marcu, 2007b), none of these advances in align-
ment quality has improved translation quality of a
state-of-the-art system. We use a discriminatively
trained model to identify and delete incorrect links,
and demonstrate that these gains in alignment qual-
ity lead to gains in translation quality in a state-
of-the-art syntax-based MT system. In contrast to
the semi-supervised LEAF alignment algorithm of
(Fraser and Marcu, 2007b), which requires 1,500-
2,000 CPU days per iteration to align 8.4M Chinese-
English sentences (anonymous, p.c.), link deletion
requires only 450 CPU hours to re-align such a cor-
pus (after initial alignment by GIZA++, which re-
quires 20-24 CPU days).
Several recent works incorporate syntactic fea-
tures into alignment. (May and Knight, 2007) use
syntactic constraints to re-align a parallel corpus that
has been aligned by GIZA++ as follows: they extract
string-to-tree transducer rules from the corpus, the
target parse trees, and the alignment; discard the ini-
tial alignment; use the extracted rules to construct a
forest of possible string-to-tree derivations for each
string/tree pair in the corpus; use EM to select the
Viterbi derivation tree for each pair; and finally, in-
duce a new alignment from the Viterbi derivations,
using the re-aligned corpus to train a syntax-based
MT system. (May and Knight, 2007) differs from
our approach in two ways: first, the set of possible
re-alignments they consider for each sentence pair is
limited by the initial GIZA++ alignments seen over
the training corpus, while we consider all alignments
that can be reached by deleting links from the ini-
tial GIZA++ alignment for that sentence pair. Sec-
ond, (May and Knight, 2007) use a time-intensive
training algorithm to select the best re-alignment
for each sentence pair, while we use a fast greedy
search to determine which links to delete; in con-
trast to (May and Knight, 2007), who require 400
CPU hours to re-align 330k Chinese-English sen-
tence pairs (anonymous, p.c), link deletion requires
only 18 CPU hours to re-align such a corpus.
(Lopez and Resnik, 2005) and (Denero and Klein,
2007) modify the distortion model of the HMM
alignment model (Vogel et al, 1996) to reflect tree
distance rather than string distance; (Cherry and
Lin, 2006) modify an ITG aligner by introducing
a penalty for induced parses that violate syntac-
tic bracketing constraints. Similarly to these ap-
proaches, we use syntactic bracketing to constrain
alignment, but our work extends beyond improving
alignment quality to improve translation quality as
well.
2 Link Deletion
We propose an algorithm to re-align a parallel bitext
that has been aligned by GIZA++ (IBM Model 4),
then symmetrized using the union heuristic. We then
train a syntax-based translation system on the re-
aligned bitext, and evaluate whether the re-aligned
bitext yields a better translation model than a base-
line system trained on the GIZA++ union aligned
bitext.
2.1 Link Deletion Algorithm
Our algorithm for re-alignment proceeds as follows.
We make a single pass over the corpus. For each sen-
tence pair, we initialize the alignment A = Ainitial
(the GIZA++ union alignment for that sentence
pair). We represent the score of A as a weighted
linear combination of features hi of the alignment
A, the target parse tree parse(e) (a phrase-structure
syntactic representation of e), and the source string
f :
score(A) =
n
?
i=0
?i ? hi(A, parse(e), f)
We define a branch of links to be a contiguous 1-
to-many alignment.2 We define two alignments, A
2In Figure 1, the 1-to-many alignment formed by {? )-
its, ? )- own,? )-country} constitutes a branch, but the
1-to-many alignment formed by {? -starts,? -out,? -
needs} does not.
46
and A?, to be neighbors if they differ only by the
deletion of a link or branch of links. We consider all
alignments A? in the neighborhood of A, greedily
deleting the link l or branch of links b maximizing
the score of the resulting alignment A? = A \ l or
A? = A \ b. We delete links until no further increase
in the score of A is possible.3
In section 2.2 we describe the features hi, and in
section 2.4 we describe how to set the weights ?i.
2.2 Features
2.2.1 Syntactic Features
We use two features of the string-to-tree trans-
ducer rules extracted from A, parse(e), and f ac-
cording to the rule extraction algorithm described in
(Galley et al, 2004):
ruleCount: Total number of rules extracted from
A, parse(e), and f . As Figure 1 illustrates, in-
correct links violating syntactic brackets tend to de-
crease ruleCount; ruleCount increases from 4 to 8
after deleting the incorrect link.
sizeOfLargestRule: The size, measured in terms
of internal nodes in the target parse tree, of the single
largest rule extracted from A, parse(e), and f . In
Figure 1, the largest rules in the leftmost and right-
most columns are R1 (with 9 internal nodes) and R9
(with 4 internal nodes), respectively.
2.2.2 Structural Features
wordsUnaligned: Total number of unaligned
words.
1-to-many Links: Total number of links for which
one word is aligned to multiple words, in either di-
rection. In Figure 1, the links {? -starts,? -
out,? -needs} represent a 1-to-many alignment.
1-to-many links appear more frequently in GIZA++
union alignments than in gold alignments, and are
therefore good candidates for deletion. The cate-
gory of 1-to-many links is further subdivided, de-
pending on the degree of contiguity that the link ex-
hibits with its neighbors.4 Each link in a 1-to-many
3While using a dynamic programming algorithm would
likely improve search efficiency and allow link deletion to find
an optimal solution, in practice, the greedy search runs quickly
and improves alignment quality.
4(Deng and Byrne, 2005) observe that, in a manually aligned
Chinese-English corpus, 82% of the Chinese words that are
alignment can have 0, 1, or 2 neighbors, according
to how many links are adjacent to it in the 1-to-many
alignment:
zeroNeighbors: In Figure 1, the link ? -needs
has 0 neighbors.
oneNeighbor: In Figure 1, the links ? -starts
and ? -out each have 1 neighbor?namely, each
other.
twoNeighbors: In Figure 1, in the 1-to-many
alignment formed by {? )-its,? )-own,? )-
country}, the link ? )-own has 2 neighbors,
namely ? )-it and ? )-country.
2.2.3 Lexical Features
highestLexProbRank: A link ei-fj is ?max-
probable from ei to fj? if p(fj |ei) > p(fj? |ei) for
all alternative words fj? with which ei is aligned
in Ainitial. In Figure 1, p(? |needs) > p(?
|needs), so ? -needs is max-probable for
?needs?. The definition of ?max-probable from fj to
ei? is analogous, and a link is max-probable (nondi-
rectionally) if it is max-probable in either direction.
The value of highestLexProbRank is the total num-
ber of max-probable links. The conditional lexical
probabilities p(ei|fj) and p(fj |ei) are estimated us-
ing frequencies of aligned word pairs in the high-
precision GIZA++ intersection alignments for the
training corpus.
2.2.4 History Features
In addition to the above syntactic, structural,
and lexical features of A, we also incorporate
two features of the link deletion history itself into
Score(A):
linksDeleted: Total number of links deleted
Ainitial thus far. At each iteration, either a link or
a branch of links is deleted.
aligned to multiple English words are aligned to a contiguous
block of English words; similarly, 88% of the English words
that are aligned to multiple Chinese words are aligned to a con-
tiguous block of Chinese words. Thus, if a Chinese word is cor-
rectly aligned to multiple English words, those English words
are likely to be ?neighbors? of each other, and if an English
word is correctly aligned to multiple Chinese words, those Chi-
nese words are likely to be ?neighbors? of each other.
47
stepsTaken: Total number of iterations thus far in
the search; at each iteration, either a link or a branch
is deleted. This feature serves as a constant cost
function per step taken during link deletion.
2.3 Constraints
Protecting Refined Links from Deletion: Since
GIZA++ refined links have higher precision than
union links5, we do not consider any GIZA++ re-
fined links for deletion.6
Stoplist: In our Chinese-English corpora, the 10
most common English words (excluding punc-
tuation marks) include {a,in,to,of,and,the}, while
the 10 most common Chinese words include
{?,4,?,Z,{}. Of these, {a,the} and {?,{}
have no explicit translational equivalent in the other
language. These words are aligned with each other
frequently (and erroneously) by GIZA++ union, but
rarely in the gold standard. We delete all links in
the set {a, an, the} ? {{, ?} from Ainitial as a
preprocessing step.7
2.4 Perceptron Training
We set the feature weights ? using a modified ver-
sion of averaged perceptron learning with structured
outputs (Collins, 2002). Following (Moore, 2005),
we initialize the value of our expected most infor-
mative feature (ruleCount) to 1.0, and initialize all
other feature weights to 0. During each pass over the
discriminative training set, we ?decode? each sen-
tence pair by greedily deleting links from Ainitial in
order to maximize the score of the resulting align-
ment using the current settings of ? (for details, refer
to section 2.1).
5On a 400-sentence-pair Chinese-English data set, GIZA++
union alignments have a precision of 77.32 while GIZA++ re-
fined alignments have a precision of 85.26.
6To see how GIZA++ refined alignments compare to
GIZA++ union alignments for syntax-based translation, we
compare systems trained on each set of alignments for Chinese-
English translation task A. Union alignments result in a test set
BLEU score of 41.17, as compared to only 36.99 for refined.
7The impact upon alignment f-measure of deleting these
stoplist links is small; on Chinese-English Data Set A, the f-
measure of the baseline GIZA++ union alignments on the test
set increases from 63.44 to 63.81 after deleting stoplist links,
while the remaining increase in f-measure from 63.81 to 75.14
(shown in Table 3) is due to the link deletion algorithm itself.
We construct a set of candidate alignments
Acandidates for use in reranking as follows. Starting
with A = Ainitial, we iteratively explore all align-
ments A? in the neighborhood of A, adding each
neighbor to Acandidates, then selecting the neigh-
bor that maximizes Score(A?). When it is no
longer possible to increase Score(A) by deleting
any links, link deletion concludes and returns the
highest-scoring alignment, A1-best.
In general, Agold /? Acandidates; following
(Collins, 2000) and (Charniak and Johnson, 2005)
for parse reranking and (Liang et al, 2006) for trans-
lation reranking, we define Aoracle as alignment in
Acandidates that is most similar to Agold.8 We up-
date each feature weight ?i as follows: ?i = ?i +
hAoraclei ? h
A1-best
i .
9
Following (Moore, 2005), after each training
pass, we average all the feature weight vectors seen
during the pass, and decode the discriminative train-
ing set using the vector of averaged feature weights.
When alignment quality stops increasing on the dis-
criminative training set, perceptron training ends.10
The weight vector returned by perceptron training is
the average over the training set of all weight vectors
seen during all iterations; averaging reduces overfit-
ting on the training set (Collins, 2002).
3 Experimental Setup
3.1 Data Sets
We evaluate the effect of link deletion upon align-
ment quality and translation quality for two Chinese-
English data sets, and one Arabic-English data set.
Each data set consists of newswire, and contains a
small subset of manually aligned sentence pairs. We
divide the manually aligned subset into a training set
(used to discriminatively set the feature weights for
link deletion) and a test set (used to evaluate the im-
pact of link deletion upon alignment quality). Table
1 lists the source and the size of the manually aligned
training and test sets used for each alignment task.
8We discuss alignment similarity metrics in detail in Section
3.2.
9(Liang et al, 2006) report that, for translation reranking,
such local updates (towards the oracle) outperform bold updates
(towards the gold standard).
10We discuss alignment quality metrics in detail in Section
3.2.
48
Using the feature weights learned on the manually
aligned training set, we then apply link deletion to
the remainder (non-manually aligned) of each bilin-
gual data set, and train a full syntax-based statistical
MT system on these sentence pairs. After maximum
BLEU tuning (Och, 2003a) on a held-out tuning set,
we evaluate translation quality on a held-out test set.
Table 2 lists the source and the size of the training,
tuning, and test sets used for each translation task.
3.2 Evaluation Metrics
AER (Alignment Error Rate) (Och and Ney, 2003)
is the most widely used metric of alignment qual-
ity, but requires gold-standard alignments labelled
with ?sure/possible? annotations to compute; lack-
ing such annotations, we can compute alignment f-
measure instead.
However, (Fraser and Marcu, 2007a) show that,
in phrase-based translation, improvements in AER
or f-measure do not necessarily correlate with im-
provements in BLEU score. They propose two mod-
ifications to f-measure: varying the precision/recall
tradeoff, and fully-connecting the alignment links
before computing f-measure.11
Weighted Fully-Connected F-Measure Given a
hypothesized set of alignment links H and a gold-
standard set of alignment links G, we define H+ =
fullyConnect(H) and G+ = fullyConnect(G),
and then compute:
f -measure(H+) = 1?
precision(H+) +
1??
recall(H+)
For phrase-based Chinese-English and Arabic-
English translation tasks, (Fraser and Marcu, 2007a)
obtain the closest correlation between weighted
fully-connected alignment f-measure and BLEU
score using ?=0.5 and ?=0.1, respectively. We
use weighted fully-connected alignment f-measure
as the training criterion for link deletion, and to eval-
uate alignment quality on training and test sets.
Rule F-Measure To evaluate the impact of link
deletion upon rule quality, we compare the rule pre-
cision, recall, and f-measure of the rule set extracted
11In Figure 1, the fully-connected version of the alignments
shown would include the links ? -starts and ? - out.
Language Train Test
Chinese-English A 400 400
Chinese-English B 1500 1500
Arabic-English 1500 1500
Table 1: Size (sentence pairs) of data sets used in align-
ment link deletion tasks
from our hypothesized alignments and a Collins-
style parser against the rule set extracted from gold
alignments and gold parses.
BLEU For all translation tasks, we report case-
insensitive NIST BLEU scores (Papineni et al,
2002) using 4 references per sentence.
3.3 Experiments
Starting with GIZA++ union (IBM Model 4) align-
ments, we use perceptron training to set the weights
of each feature used in link deletion in order to opti-
mize weighted fully-connected alignment f-measure
(?=0.5 for Chinese-English and ?=0.1 for Arabic-
English) on a manually aligned discriminative train-
ing set. We report the (fully-connected) precision,
recall, and weighted alignment f-measure on a held-
out test set after running perceptron training, relative
to the baseline GIZA++ union alignments. Using
the learned feature weights, we then perform link
deletion over the GIZA++ union alignments for the
entire training corpus for each translation task. Us-
ing these alignments, which we refer to as ?GIZA++
union + link deletion?, we train a syntax-based trans-
lation system similar to that described in (Galley et
al., 2006). After extracting string-to-tree translation
rules from the aligned, parsed training corpus, the
system assigns weights to each rule via frequency
estimation with smoothing. The rule probabilities,
as well as trigram language model probabilities and
a handful of additional features of each rule, are used
as features during decoding. The feature weights are
tuned using minimum error rate training (Och and
Ney, 2003) to optimize BLEU score on a held-out
development set. We then compare the BLEU score
of this system against a baseline system trained us-
ing GIZA++ union alignments.
To determine which value of ? is most effective
as a training criterion for link deletion, we set ?=0.4
(favoring recall), 0.5, and 0.6 (favoring precision),
49
Language Train Tune Test1 Test2
Chinese-English A 9.8M/newswire 25.9k/NIST02 29.0k/NIST03 ?
Chinese-English B 12.3M/newswire 42.9k/newswire 42.1k/newswire ?
Arabic-English 174.8M/newswire 35.8k/NIST04-05 40.3k/NIST04-05 53.0k/newswire
Table 2: Size (English words) and source of data sets used in translation tasks
and compare the effect on translation quality for
Chinese-English data set A.
4 Results
For each translation task, link deletion improves
translation quality relative to a GIZA++ union base-
line. For each alignment task, link deletion tends to
improve fully-connected alignment precision more
than it decreases fully-connected alignment recall,
increasing weighted fully-connected alignment f-
measure overall.
4.1 Chinese-English
On Chinese-English translation task A, link deletion
increases BLEU score by 1.26 points on tuning and
0.76 points on test (Table 3); on Chinese-English
translation task B, link deletion increases BLEU
score by 1.38 points on tuning and 0.49 points on
test (Table 3).
4.2 Arabic-English
On the Arabic-English translation task, link dele-
tion improves BLEU score by 0.84 points on tuning,
0.18 points on test1, and 0.56 points on test2 (Ta-
ble 3). Note that the training criterion for Arabic-
English link deletion uses ?=0.1; because this pe-
nalizes a loss in recall more heavily than it re-
wards an increase in precision, it is more difficult
to increase weighted fully-connected alignment f-
measure using link deletion for Arabic-English than
for Chinese-English. This difference is reflected in
the average number of links deleted per sentence:
4.19 for Chinese-English B (Table 3), but only 1.35
for Arabic-English (Table 3). Despite this differ-
ence, link deletion improves translation results for
Arabic-English as well.
4.3 Varying ?
On Chinese-English data set A, we explore the ef-
fect of varying ? in the weighted fully-connected
93 187 375 750 1500
46
48
50
52
54
56
58
60
62
64
Training Sentence Pairs
Te
st
 S
et
 W
ei
gh
te
d 
Fu
lly
?C
on
ne
ct
ed
 A
lig
nm
en
t F
?M
ea
su
re
 
 
GIZA++ union
GIZA++ union + link deletion
Figure 2: Effect of discriminative training set size on link
deletion accuracy for Chinese-English B, ?=0.5
alignment f-measure used as the training criterion
for link deletion. Using ?=0.5 leads to a higher gain
in BLEU score on the test set relative to the base-
line (+0.76 points) than either ?=0.4 (+0.70 points)
or ?=0.6 (+0.67 points).
4.4 Size of Discriminative Training Set
To examine how many manually aligned sentence
pairs are required to set the feature weights reli-
ably, we vary the size of the discriminative training
set from 2-1500 sentence pairs while holding test
set size constant at 1500 sentence pairs; run per-
ceptron training; and record the resulting weighted
fully-connected alignment f-measure on the test set.
Figure 2 illustrates that using 100-200 manually
aligned sentence pairs of training data is sufficient
for Chinese-English; a similarly-sized training set is
also sufficient for Arabic-English.
4.5 Effect of Link Deletion on Extracted Rules
Link deletion increases the size of the extracted
grammar. To determine how the quality of the ex-
tracted grammar changes, we compute the rule pre-
50
Language Alignment Prec Rec ? F-measure Links Del/ Grammar BLEUSent Size Tune Test1 Test2
Chi-Eng A GIZA++ union 54.76 75.38 0.5 63.44 ? 23.4M 41.80 41.17 ?
Chi-Eng A GIZA++ union + 79.59 71.16 0.5 75.14 4.77 59.7M 43.06 41.93 ?link deletion
Chi-Eng B GIZA++ union 36.61 66.28 0.5 47.16 ? 28.9M 39.59 41.39 ?
Chi-Eng B GIZA++ union + 65.52 59.28 0.5 62.24 4.19 73.0M 40.97 41.88 ?link deletion
Ara-Eng GIZA++ union 35.34 84.05 0.1 73.87 ? 52.4M 54.73 50.9 38.16
Ara-Eng GIZA++ union + 52.68 79.75 0.1 75.85 1.35 64.9M 55.57 51.08 38.72link deletion
Table 3: Results of link deletion. Weighted fully-connected alignment f-measure is computed on alignment test sets
(Table 1); BLEU score is computed on translation test sets (Table 2).
Alignment Parse RulePrecision Recall F-measure Total Non-Unique
gold gold 100.00 100.00 100.00 12,809
giza++ union collins 50.49 44.23 47.15 11,021
giza++ union+link deletion, ?=0.5 collins 47.51 53.20 50.20 13,987
giza++ refined collins 44.20 54.06 48.64 15,182
Table 4: Rule precision, recall, and f-measure of rules extracted from 400 sentence pairs of Chinese-English data
cision, recall, and f-measure of the GIZA++ union
alignments and various link deletion alignments on
a held-out Chinese-English test set of 400 sentence
pairs. Table 4 indicates the total (non-unique) num-
ber of rules extracted for each alignment/parse pair-
ing, as well as the rule precision, recall, and f-
measure of each pair. As more links are deleted,
more rules are extracted?but of those, some are of
good quality and others are of bad quality. Link-
deleted alignments produce rule sets with higher rule
f-measure than either GIZA++ union or GIZA++ re-
fined.
5 Conclusion
We have presented a link deletion algorithm that im-
proves the precision of GIZA++ union alignments
without notably decreasing recall. In addition to lex-
ical and structural features, we use features of the ex-
tracted syntax-based translation rules. Our method
improves alignment quality and translation quality
on Chinese-English and Arabic-English translation
tasks, relative to a GIZA++ union baseline. The
algorithm runs quickly, and is easily applicable to
other language pairs with limited amounts (100-200
sentence pairs) of manually aligned data available.
Acknowledgments
We thank Steven DeNeefe and Wei Wang for assis-
tance with experiments, and Alexander Fraser and
Liang Huang for helpful discussions. This research
was supported by DARPA (contract HR0011-06-C-
0022) and by a fellowship from AT&T Labs.
51
References
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. The Mathematics of Sta-
tistical Machine Translation: Parameter Estimation.
Computational Linguistics, Vol. 19, No. 2, 1993.
Eugene Charniak and Mark Johnson. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking.
Proceedings of ACL, 2005.
Colin Cherry and Dekang Lin. Soft Syntactic Constraints
for Word Alignment through Discriminative Training.
Proceedings of ACL (Poster), 2006.
David Chiang. A Hierarchical Phrase-Based Model for
Statistical Machine Translation. Proceedings of ACL,
2005.
David Chiang. Hierarchical phrase-based translation.
Computational Linguistics, 2007.
Michael Collins. Discriminative Reranking for Natural
Language Parsing. Proceedings of ICML, 2000.
Michael Collins. Discriminative training methods for
hidden Markov models: theory and experiments with
perceptron algorithms. Proceedings of EMNLP,
2002.
John DeNero and Dan Klein. Tailoring Word Align-
ments to Syntactic Machine Translation. Proceedings
of ACL, 2007.
Yonggang Deng and William Byrne. HMM word and
phrase alignment for statistical machine translation.
Proceedings of HLT/EMNLP, 2005.
Alexander Fraser and Daniel Marcu. Measuring Word
Alignment Quality for Statistical Machine Translation.
Computational Linguistics, Vol. 33, No. 3, 2007.
Alexander Fraser and Daniel Marcu. Getting the Struc-
ture Right for Word Alignment: LEAF. Proceedings of
EMNLP, 2007.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. What?s in a Translation Rule? Proceedings of
HLT/NAACL-04, 2004.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. Scalable Inference and Training of Context-
Rich Syntactic Translation Models. Proceedings of
ACL, 2006.
Liang Huang, Kevin Knight, and Aravind Joshi. Statis-
tical Syntax-Directed Translation with Extended Do-
main of Locality. Proceedings of AMTA, 2006.
Abraham Ittycheriah and Salim Roukos. A Maximum En-
tropy Word Aligner for Arabic-English Machine Trans-
lation. Proceedings of HLT/EMNLP, 2005.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
Statistical Phrase-Based Translation. Proceedings of
HLT/NAACL, 2003.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, Evan Herbst. Moses: Open Source Toolkit for
Statistical Machine Translation. Proceedings of ACL
(demo), 2007.
Percy Liang, Alexandre Bouchard-Cote, Dan Klein, and
Ben Taskar. An end-to-end discriminative approach to
machine translation. Proceedings of COLING/ACL,
2006.
Yang Liu, Qun Liu, and Shouxun Lin. Log-linear Models
for Word Alignment. Proceedings of ACL, 2005.
Yang Liu, Qun Liu, and Shouxun Lin. Tree-to-String
Alignment Template for Statistical Machine Transla-
tion. Proceedings of ACL, 2006.
Adam Lopez and Philip Resnik. Improved HMM Align-
ment Models for Languages with Scarce Resources.
Proceedings of the ACL Workshop on Parallel Text,
2005.
Jonathan May and Kevin Knight. Syntactic Re-Alignment
Models for Machine Translation. Proceedings of
EMNLP-CoNLL, 2007.
Robert C. Moore. A Discriminative Framework for Bilin-
gual Word Alignment. Proceedings of HLT/EMNLP,
2005.
Robert C. Moore, Wen-tau Yih, and Andreas Bode. Im-
proved discriminative bilingual word alignment. Pro-
ceedings of ACL, 2006.
Franz Josef Och. Minimum Error Rate Training in Sta-
tistical Machine Translation. Proceedings of ACL,
2003.
Franz Josef Och and Hermann Ney. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, Vol. 29, No. 1, 2003.
Franz Josef Och and Hermann Ney. The alignment
template approach to statistical machine translation.
Computational Linguistics, 2004.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU:
a Method for Automatic Evaluation of Machine Trans-
lation. Proceedings of ACL, 2002.
Chris Quirk, Arul Menezes, and Colin Cherry. De-
pendency Treelet Translation: Syntactically Informed
Phrasal SMT. Proceedings of ACL, 2005.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein. A
Discriminative Matching Approach to Word Align-
ment. Proceedings of HTL/EMNLP, 2005.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
HMM-Based Word Alignment in Statistical Transla-
tion Proceedings of COLING, 1996.
52
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1387?1392,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Decoding with Large-Scale Neural Language Models
Improves Translation
Ashish Vaswani
University of Southern California
Department of Computer Science
avaswani@isi.edu
Yinggong Zhao
Nanjing University, State Key Laboratory
for Novel Software Technology
zhaoyg@nlp.nju.edu.cn
Victoria Fossum and David Chiang
University of Southern California
Information Sciences Institute
{vfossum,chiang}@isi.edu
Abstract
We explore the application of neural language
models to machine translation. We develop a
new model that combines the neural proba-
bilistic language model of Bengio et al, rec-
tified linear units, and noise-contrastive esti-
mation, and we incorporate it into a machine
translation system both by reranking k-best
lists and by direct integration into the decoder.
Our large-scale, large-vocabulary experiments
across four language pairs show that our neu-
ral language model improves translation qual-
ity by up to 1.1 Bleu.
1 Introduction
Machine translation (MT) systems rely upon lan-
guage models (LMs) during decoding to ensure flu-
ent output in the target language. Typically, these
LMs are n-gram models over discrete representa-
tions of words. Such models are susceptible to data
sparsity?that is, the probability of an n-gram ob-
served only few times is difficult to estimate reli-
ably, because these models do not use any informa-
tion about similarities between words.
To address this issue, Bengio et al (2003) pro-
pose distributed word representations, in which each
word is represented as a real-valued vector in a
high-dimensional feature space. Bengio et al (2003)
introduce a feed-forward neural probabilistic LM
(NPLM) that operates over these distributed repre-
sentations. During training, the NPLM learns both a
distributed representation for each word in the vo-
cabulary and an n-gram probability distribution over
words in terms of these distributed representations.
Although neural LMs have begun to rival or even
surpass traditional n-gram LMs (Mnih and Hin-
ton, 2009; Mikolov et al, 2011), they have not yet
been widely adopted in large-vocabulary applica-
tions such as MT, because standard maximum like-
lihood estimation (MLE) requires repeated summa-
tions over all words in the vocabulary. A variety of
strategies have been proposed to combat this issue,
many of which require severe restrictions on the size
of the network or the size of the data.
In this work, we extend the NPLM of Bengio et
al. (2003) in two ways. First, we use rectified lin-
ear units (Nair and Hinton, 2010), whose activa-
tions are cheaper to compute than sigmoid or tanh
units. There is also evidence that deep neural net-
works with rectified linear units can be trained suc-
cessfully without pre-training (Zeiler et al, 2013).
Second, we train using noise-contrastive estimation
or NCE (Gutmann and Hyva?rinen, 2010; Mnih and
Teh, 2012), which does not require repeated summa-
tions over the whole vocabulary. This enables us to
efficiently build NPLMs on a larger scale than would
be possible otherwise.
We then apply this LM to MT in two ways. First,
we use it to rerank the k-best output of a hierarchi-
cal phrase-based decoder (Chiang, 2007). Second,
we integrate it directly into the decoder, allowing the
neural LM to more strongly influence the model. We
achieve gains of up to 0.6 Bleu translating French,
German, and Spanish to English, and up to 1.1 Bleu
on Chinese-English translation.
1387
u1 u2
input
words
input
embeddings
hidden
h1
hidden
h2
output
P(w | u)
D?
M
C1 C2
D
Figure 1: Neural probabilistic language model (Bengio et
al., 2003).
2 Neural Language Models
Let V be the vocabulary, and n be the order of
the language model; let u range over contexts, i.e.,
strings of length (n?1), and w range over words. For
simplicity, we assume that the training data is a sin-
gle very long string, w1 ? ? ?wN , where wN is a special
stop symbol, </s>. We write ui for wi?n+1 ? ? ?wi?1,
where, for i ? 0, wi is a special start symbol, <s>.
2.1 Model
We use a feedforward neural network as shown in
Figure 1, following Bengio et al (2003). The input
to the network is a sequence of one-hot represen-
tations of the words in context u, which we write
u j (1 ? j ? n ? 1). The output is the probability
P(w | u) for each word w, which the network com-
putes as follows.
The hidden layers consist of rec-
tified linear units (Nair and Hinton,
2010), which use the activation func-
tion ?(x) = max(0, x) (see graph at
right).
The output of the first hidden layer h1 is
h1 = ?
?
????????
n?1?
j=1
C jDu j
?
????????
(1)
where D is a matrix of input word embeddings
which is shared across all positions, the C j are the
context matrices for each word in u, and ? is applied
elementwise. The output of the second layer h2 is
h2 = ? (Mh1) ,
where M is the matrix of connection weights be-
tween h1 and h2. Finally, the output layer is a soft-
max layer,
P(w | u) ? exp
(
D?h2 + b
)
(2)
where D? is the output word embedding matrix and b
is a vector of biases for every word in the vocabulary.
2.2 Training
The typical way to train neural LMs is to maximize
the likelihood of the training data by gradient ascent.
But the softmax layer requires, at each iteration, a
summation over all the units in the output layer, that
is, all words in the whole vocabulary. If the vocabu-
lary is large, this can be prohibitively expensive.
Noise-contrastive estimation or NCE (Gutmann
and Hyva?rinen, 2010) is an alternative estimation
principle that allows one to avoid these repeated
summations. It has been applied previously to log-
bilinear LMs (Mnih and Teh, 2012), and we apply it
here to the NPLM described above.
We can write the probability of a word w given a
context u under the NPLM as
P(w | u) =
1
Z(u)
p(w | u)
p(w | u) = exp
(
D?h2 + b
)
Z(u) =
?
w?
p(w? | u) (3)
where p(w | u) is the unnormalized output of the unit
corresponding to w, and Z(u) is the normalization
factor. Let ? stand for the parameters of the model.
One possibility would be to treat Z(u), instead of
being defined by (3), as an additional set of model
parameters which are learned along with ?. But it is
easy to see that we can make the likelihood arbitrar-
ily large by making the Z(u) arbitrarily small.
In NCE, we create a noise distribution q(w).
For each example uiwi, we add k noise samples
w?i1, . . . , w?ik into the data, and extend the model to
account for noise samples by introducing a random
1388
variable C which is 1 for training examples and 0 for
noise samples:
P(C = 1,w | u) =
1
1 + k
?
1
Z(u)
p(w | u)
P(C = 0,w | u) =
k
1 + k
? q(w).
We then train the model to classify examples as
training data or noise, that is, to maximize the con-
ditional likelihood,
L =
N?
i=1
(
log P(C = 1 | uiwi) +
k?
j=1
log P(C = 0 | uiw?i j)
)
with respect to both ? and Z(u).
We do this by stochastic gradient ascent. The gra-
dient with respect to ? turns out to be
?L
??
=
N?
i=1
(
P(C = 0 | uiwi)
?
??
log p(wi | ui) ?
k?
j=1
P(C = 1 | uiw?i j)
?
??
log p(w?i j | ui)
)
and similarly for the gradient with respect to Z(u).
These can be computed by backpropagation. Unlike
before, the Z(u) will converge to a value that normal-
izes the model, satisfying (3), and, under appropriate
conditions, the parameters will converge to a value
that maximizes the likelihood of the data.
3 Implementation
Both training and scoring of neural LMs are compu-
tationally expensive at the scale needed for machine
translation. In this section, we describe some of the
techniques used to make them practical for transla-
tion.
3.1 Training
During training, we compute gradients on an en-
tire minibatch at a time, allowing the use of matrix-
matrix multiplications instead of matrix-vector mul-
tiplications (Bengio, 2012). We represent the inputs
as a sparse matrix, allowing the computation of the
input layer (1) to use sparse matrix-matrix multi-
plications. The output activations (2) are computed
only for the word types that occur as the positive ex-
ample or one of the noise samples, yielding a sparse
matrix of outputs. Similarly, during backpropaga-
tion, sparse matrix multiplications are used at both
the output and input layer.
In most of these operations, the examples in a
minibatch can be processed in parallel. However, in
the sparse-dense products used when updating the
parameters D and D?, we found it was best to di-
vide the vocabulary into blocks (16 per thread) and
to process the blocks in parallel.
3.2 Translation
To incorporate this neural LM into a MT system, we
can use the LM to rerank k-best lists, as has been
done in previous work. But since the NPLM scores
n-grams, it can also be integrated into a phrase-based
or hierarchical phrase-based decoder just as a con-
ventional n-gram model can, unlike a RNN.
The most time-consuming step in computing n-
gram probabilities is the computation of the nor-
malization constants Z(u). Following Mnih and Teh
(2012), we set al the normalization constants to one
during training, so that the model learns to produce
approximately normalized probabilities. Then, when
applying the LM, we can simply ignore normaliza-
tion. A similar strategy was taken by Niehues and
Waibel (2012). We find that a single n-gram lookup
takes about 40 ?s.
The technique, described above, of grouping ex-
amples into minibatches works for scoring of k-best
lists, but not while decoding. But caching n-gram
probabilities helps to reduce the cost of the many
lookups required during decoding.
A final issue when decoding with a neural LM
is that, in order to estimate future costs, we need
to be able to estimate probabilities of n?-grams for
n? < n. In conventional LMs, this information is
readily available,1 but not in NPLMs. Therefore, we
defined a special word <null> whose embedding is
the weighted average of the (input) embeddings of
all the other words in the vocabulary. Then, to esti-
mate the probability of an n?-gram u?w, we used the
probability of P(w | <null>n?n
?
u?).
1However, in Kneser-Ney smoothed LMs, this information
is also incorrect (Heafield et al, 2012).
1389
setting dev 2004 2005 2006
baseline 38.2 38.4 37.7 34.3
reranking 38.5 38.6 37.8 34.7
decoding 39.1 39.5 38.8 34.9
Table 1: Results for Chinese-English experiments, with-
out neural LM (baseline) and with neural LM for rerank-
ing and integrated decoding. Reranking with the neural
LM improves translation quality, while integrating it into
the decoder improves even more.
4 Experiments
We ran experiments on four language pairs ? Chi-
nese to English and French, German, and Spanish
to English ? using a hierarchical phrase-based MT
system (Chiang, 2007) and GIZA++ (Och and Ney,
2003) for word alignments.
For all experiments, we used four LMs. The base-
lines used conventional 5-gram LMs, estimated with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998) on the English side of the bitext and the
329M-word Xinhua portion of English Gigaword
(LDC2011T07). Against these baselines, we tested
systems that included the two conventional LMs as
well as two 5-gram NPLMs trained on the same
datasets. The Europarl bitext NPLMs had a vocab-
ulary size of 50k, while the other NPLMs had a vo-
cabulary size of 100k. We used 150 dimensions for
word embeddings, 750 units in hidden layer h1, and
150 units in hidden layer h2. We initialized the net-
work parameters uniformly from (?0.01, 0.01) and
the output biases to ? log |V |, and optimized them by
10 epochs of stochastic gradient ascent, using mini-
batches of size 1000 and a learning rate of 1. We
drew 100 noise samples per training example from
the unigram distribution, using the alias method for
efficiency (Kronmal and Peterson, 1979).
We trained the discriminative models with MERT
(Och, 2003) and the discriminative rerankers on
1000-best lists with MERT. Except where noted, we
ran MERT three times and report the average score.
We evaluated using case-insensitive NIST Bleu.
4.1 NIST Chinese-English
For the Chinese-English task (Table 1), the training
data came from the NIST 2012 constrained track,
excluding sentences longer than 60 words. Rules
Fr-En De-En Es-En
setting dev test dev test dev test
baseline 33.5 25.5 28.8 21.5 33.5 32.0
reranking 33.9 26.0 29.1 21.5 34.1 32.2
decoding 34.12 26.12 29.3 21.9 34.22 32.12
Table 2: Results for Europarl MT experiments, without
neural LM (baseline) and with neural LM for reranking
and integrated decoding. The neural LM gives improve-
ments across three different language pairs. Superscript 2
indicates a score averaged between two runs; all other
scores were averaged over three runs.
without nonterminals were extracted from all train-
ing data, while rules with nonterminals were ex-
tracted from the FBIS corpus (LDC2003E14). We
ran MERT on the development data, which was the
NIST 2003 test data, and tested on the NIST 2004?
2006 test data.
Reranking using the neural LM yielded improve-
ments of 0.2?0.4 Bleu, while integrating the neural
LM yielded larger improvements, between 0.6 and
1.1 Bleu.
4.2 Europarl
For French, German, and Spanish translation, we
used a parallel text of about 50M words from Eu-
roparl v7. Rules without nonterminals were ex-
tracted from all the data, while rules with nonter-
minals were extracted from the first 200k words. We
ran MERT on the development data, which was the
WMT 2005 test data, and tested on the WMT 2006
news commentary test data (nc-test2006).
The improvements, shown in Table 2, were more
modest than on Chinese-English. Reranking with
the neural LM yielded improvements of up to 0.5
Bleu, and integrating the neural LM into the decoder
yielded improvements of up to 0.6 Bleu. In one
case (Spanish-English), integrated decoding scored
higher than reranking on the development data but
lower on the test data ? perhaps due to the differ-
ence in domain between the two. On the other tasks,
integrated decoding outperformed reranking.
4.3 Speed comparison
We measured the speed of training a NPLM by NCE,
compared with MLE as implemented by the CSLM
toolkit (Schwenk, 2013). We used the first 200k
1390
10 20 30 40 50 60 70
0
1,
00
0
2,
00
0
3,
00
0
4,
00
0
Vocabulary size (?1000)
T
ra
in
in
g
tim
e
(s
)
CSLM
NCE k = 1000
NCE k = 100
NCE k = 10
Figure 2: Noise contrastive estimation (NCE) is much
faster, and much less dependent on vocabulary size, than
MLE as implemented by the CSLM toolkit (Schwenk,
2013).
lines (5.2M words) of the Xinhua portion of Giga-
word and timed one epoch of training, for various
values of k and |V |, on a dual hex-core 2.67 GHz
Xeon X5650 machine. For these experiments, we
used minibatches of 128 examples. The timings are
plotted in Figure 2. We see that NCE is considerably
faster than MLE; moreover, as expected, the MLE
training time is roughly linear in |V |, whereas the
NCE training time is basically constant.
5 Related Work
The problem of training with large vocabularies in
NPLMs has received much attention. One strategy
has been to restructure the network to be more hi-
erarchical (Morin and Bengio, 2005; Mnih and Hin-
ton, 2009) or to group words into classes (Le et al,
2011). Other strategies include restricting the vocab-
ulary of the NPLM to a shortlist and reverting to a
traditional n-gram LM for other words (Schwenk,
2004), and limiting the number of training examples
using resampling (Schwenk and Gauvain, 2005) or
selecting a subset of the training data (Schwenk et
al., 2012). Our approach can be efficiently applied
to large-scale tasks without limiting either the model
or the data.
NPLMs have previously been applied to MT, most
notably feed-forward NPLMs (Schwenk, 2007;
Schwenk, 2010) and RNN-LMs (Mikolov, 2012).
However, their use in MT has largely been limited
to reranking k-best lists for MT tasks with restricted
vocabularies. Niehues and Waibel (2012) integrate a
RBM-based language model directly into a decoder,
but they only train the RBM LM on a small amount
of data. To our knowledge, our approach is the first
to integrate a large-vocabulary NPLM directly into a
decoder for a large-scale MT task.
6 Conclusion
We introduced a new variant of NPLMs that com-
bines the network architecture of Bengio et al
(2003), rectified linear units (Nair and Hinton,
2010), and noise-contrastive estimation (Gutmann
and Hyva?rinen, 2010). This model is dramatically
faster to train than previous neural LMs, and can be
trained on a large corpus with a large vocabulary and
directly integrated into the decoder of a MT system.
Our experiments across four language pairs demon-
strated improvements of up to 1.1 Bleu. Code for
training and using our NPLMs is available for down-
load.2
Acknowledgements
We would like to thank the anonymous reviewers for
their very helpful comments. This research was sup-
ported in part by DOI IBC grant D12AP00225. This
work was done while the second author was visit-
ing USC/ISI supported by China Scholarship Coun-
cil. He was also supported by the Research Fund for
the Doctoral Program of Higher Education of China
(No. 20110091110003) and the National Fundamen-
tal Research Program of China (2010CB327903).
References
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research.
Yoshua Bengio. 2012. Practical recommendations for
gradient-based training of deep architectures. CoRR,
abs/1206.5533.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
2http://nlg.isi.edu/software/nplm
1391
eling. Technical Report TR-10-98, Harvard University
Center for Research in Computing Technology.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Gutmann and Aapo Hyva?rinen. 2010. Noise-
contrastive estimation: A new estimation principle for
unnormalized statistical models. In Proceedings of
AISTATS.
Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2012.
Language model rest costs and space-efficient storage.
In Proceedings of EMNLP-CoNLL, pages 1169?1178.
Richard Kronmal and Arthur Peterson. 1979. On the
alias method for generating random variables from
a discrete distribution. The American Statistician,
33(4):214?218.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In Proceedings
of ICASSP, pages 5524?5527.
Toma?s? Mikolov, Anoop Deoras, Stefan Kombrink, Luka?s?
Burget, and Jan ?Honza? C?ernocky?. 2011. Em-
pirical evaluation and combination of advanced lan-
guage modeling techniques. In Proceedings of IN-
TERSPEECH, pages 605?608.
Toma?s? Mikolov. 2012. Statistical Language Models
Based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Andriy Mnih and Geoffrey Hinton. 2009. A scalable
hierarchical distributed language model. In Advances
in Neural Information Processing Systems.
Andriy Mnih and Yee Whye Teh. 2012. A fast and sim-
ple algorithm for training neural probabilistic language
models. In Proceedings of ICML.
Frederic Morin and Yoshua Bengio. 2005. Hierarchical
probabilistic neural network language model. In Pro-
ceedings of AISTATS, pages 246?252.
Vinod Nair and Geoffrey E. Hinton. 2010. Rectified lin-
ear units improve restricted Boltzmann machines. In
Proceedings of ICML, pages 807?814.
Jan Niehues and Alex Waibel. 2012. Continuous
space language models using Restricted Boltzmann
Machines. In Proceedings of IWSLT.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Holger Schwenk and Jean-Luc Gauvain. 2005. Training
neural network language models on very large corpora.
In Proceedings of EMNLP.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space lan-
guage models on a GPU for statistical machine trans-
lation. In Proceedings of the NAACL-HLT 2012 Work-
shop: Will We Ever Really Replace the N-gramModel?
On the Future of Language Modeling for HLT, pages
11?19.
Holger Schwenk. 2004. Efficient training of large neural
networks for language modeling. In Proceedings of
IJCNN, pages 3059?3062.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Holger Schwenk. 2010. Continuous-space language
models for statistical machine translation. Prague Bul-
letin of Mathematical Linguistics, 93:137?146.
Holger Schwenk. 2013. CSLM - a modular open-source
continuous space language modeling toolkit. In Pro-
ceedings of Interspeech.
M.D. Zeiler, M. Ranzato, R. Monga, M. Mao, K. Yang,
Q.V. Le, P. Nguyen, A. Senior, V. Vanhoucke, J. Dean,
and G.E. Hinton. 2013. On rectified linear units for
speech processing. In Proceedings of ICASSP.
1392
In: R. Levy & D. Reitter (Eds.), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 61?69,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Sequential vs. Hierarchical Syntactic Models of Human Incremental
Sentence Processing
Victoria Fossum and Roger Levy
Department of Linguistics
University of California, San Diego
9500 Gilman Dr.
La Jolla, CA 92093
{vfossum,rlevy}@ucsd.edu
Abstract
Experimental evidence demonstrates that syn-
tactic structure influences human online sen-
tence processing behavior. Despite this ev-
idence, open questions remain: which type
of syntactic structure best explains observed
behavior?hierarchical or sequential, and lexi-
calized or unlexicalized? Recently, Frank and
Bod (2011) find that unlexicalized sequen-
tial models predict reading times better than
unlexicalized hierarchical models, relative to
a baseline prediction model that takes word-
level factors into account. They conclude that
the human parser is insensitive to hierarchi-
cal syntactic structure. We investigate these
claims and find a picture more complicated
than the one they present. First, we show that
incorporating additional lexical n-gram prob-
abilities estimated from several different cor-
pora into the baseline model of Frank and Bod
(2011) eliminates all differences in accuracy
between those unlexicalized sequential and hi-
erarchical models. Second, we show that lexi-
calizing the hierarchical models used in Frank
and Bod (2011) significantly improves pre-
diction accuracy relative to the unlexicalized
versions. Third, we show that using state-
of-the-art lexicalized hierarchical models fur-
ther improves prediction accuracy. Our results
demonstrate that the claim of Frank and Bod
(2011) that sequential models predict reading
times better than hierarchical models is pre-
mature, and also that lexicalization matters for
prediction accuracy.
1 Introduction
Various factors influence human reading times dur-
ing online sentence processing, including word-level
factors such as word length, unigram and bigram
probabilities, and position in the sentence. Yet word-
level factors cannot explain many observed process-
ing phenomena; ample experimental evidence ex-
ists for the influence of syntax on human behav-
ior during online sentence processing, beyond what
can be predicted using word-level factors alone.
Examples include the English subject/object rela-
tive clause asymmetry (Gibson et al, 2005; King
and Just, 1991) and anti-locality effects in German
(Konieczny, 2000; Konieczny and Do?ring, 2003),
Hindi (Vasishth and Lewis, 2006), and Japanese
(Nakatani and Gibson, 2008). Levy (2008) shows
that these processing phenomena can be explained
by surprisal theory under a hierarchical probabilis-
tic context-free grammar (PCFG). Other evidence
of syntactic expectation in sentence processing in-
cludes the facilitation of processing at ?or? follow-
ing ?either? (Staub and Clifton, 2006); expectations
of heavy noun phrase shifts (Staub et al, 2006); el-
lipsis processing (Lau et al, 2006); and syntactic
priming (Sturt et al, 2010).
Experimental evidence for the influence of syn-
tax on human behavior is not limited to experiments
carefully designed to isolate a particular processing
phenomenon. Several broad-coverage experimental
studies have shown that surprisal under hierarchi-
cal syntactic models predicts human processing dif-
ficulty on large corpora of naturally occurring text,
even after word-level factors have been taken into
61
account (Boston et al, 2008; Demberg and Keller,
2008; Roark et al, 2009).
Despite this evidence, in recent work Frank and
Bod (2011) challenge the notion that hierarchical
syntactic structure is strictly necessary to predict
reading times. They compare per-word surprisal
predictions from unlexicalized hierarchical and se-
quential models of syntactic structure along two
axes: linguistic accuracy (how well the model pre-
dicts the test corpus) and psychological accuracy
(how well the model predicts observed reading times
on the test corpus). They find that, while hierar-
chical phrase-structure grammars (PSG?s) achieve
better linguistic accuracy, sequential echo state net-
works (ESN?s) achieve better psychological accu-
racy on the English Dundee corpus (Kennedy and
Pynte, 2005). Frank and Bod (2011) do not in-
clude lexicalized syntactic models in the compar-
ison on the grounds that, once word-level factors
have been included as control predictors in the read-
ing times model, lexicalized syntactic models do not
predict reading times better than unlexicalized syn-
tactic models (Demberg and Keller, 2008). Based on
the results of their comparisons between unlexical-
ized models, they conclude that the human parser is
insensitive to hierarchical syntactic structure.
In light of the existing evidence that hierarchical
syntax influences human sentence processing, the
claim of Frank and Bod (2011) is surprising. In this
work, we investigate this claim, and find a picture
more complicated than the one they present. We
first replicate the results of Frank and Bod (2011)
using the dataset provided by the authors, verifying
that we obtain the same linguistic and psychologi-
cal accuracies reported by the authors. We then ex-
tend their work in several ways. First, we repeat
their comparisons using additional, more robustly
estimated lexical n-gram probabilities as control pre-
dictors in the baseline model.1 We show that when
these additional lexical n-gram probabilities are used
as control predictors, any differences in psycholog-
ical accuracy between the hierarchical and sequen-
tial models used in Frank and Bod (2011) vanish.
Second, while they restrict their comparisons to un-
1By robustly estimated, we mean that these probabilities
are estimated from larger corpora and use a better smoothing
method (Kneser-Ney) than the lexical n-grams of Frank and
Bod (2011).
lexicalized models over part-of-speech (POS) tags,
we investigate the lexicalized versions of each hi-
erarchical model, and show that lexicalization sig-
nificantly improves psychological accuracy. Third,
while they explore only a subset of the PSG?s im-
plemented under the incremental parser of Roark
(2001), we explore a state-of-the-art lexicalized hi-
erarchical model that conditions on richer contexts,
and show that this model performs still better. Our
findings demonstrate that Frank and Bod (2011)?s
strong claim that sequential models predict reading
times better than hierarchical models is premature,
and also that lexicalization improves the psycholog-
ical accuracy of hierarchical models.
2 Related Work
Several broad-coverage experimental studies
demonstrate that surprisal under a hierarchical syn-
tactic model predicts human processing difficulty
on a corpus of naturally occurring text, even after
word-level factors have been taken into account.
Under surprisal theory (Hale, 2001; Levy, 2008),
processing difficulty at word wi is proportional to
reading time at wi, which in turn is proportional to
the surprisal of wi in the context in which it is ob-
served: surprisal(wi) = ?log(pr(wi|context)).
Typically, context ? w1...wi?1. Comput-
ing surprisal(wi) thus reduces to computing
?log(pr(wi|w1...wi? 1)). Henceforth, we refer
to this original formulation of surprisal as total
surprisal.
Boston et al (2008) show that surprisal estimates
from a lexicalized dependency parser (Nivre, 2006)
and an unlexicalized PCFG are significant predic-
tors of reading times on the German Potsdam Cor-
pus. Demberg and Keller (2008) propose to isolate
syntactic surprisal from total surprisal by replacing
each word with its POS tag, then calculating sur-
prisal as usual under the incremental probabilistic
phrase-structure parser of Roark (2001). (Following
Roark et al (2009), we hereafter refer to this type of
surprisal as POS surprisal.) They find that only POS
surprisal, not total surprisal, is a significant predictor
of reading time predictions on the English Dundee
corpus.
Demberg and Keller (2008)?s definition of POS
surprisal introduces two constraints. First, by omit-
62
ting lexical information from the conditioning con-
text, they ignore differences among words within a
syntactic category that can influence syntactic ex-
pectations about upcoming material. Second, by re-
placing words with their most likely POS tags, they
treat POS tags as veridical, observed input rather
than marginalizing over all possible latent POS tag
sequences consistent with the observed words.
Roark et al (2009) propose a more principled way
of decomposing total surprisal into its syntactic and
lexical components, defining the syntactic surprisal
of wi as:
?log
?
D:yield(D)=w1...wi pr(D minus last step)
?
D:yield(D)=w1...wi?1 pr(D)
and the lexical surprisal of wi as:
?log
?
D:yield(D)=w1...wi pr(D)
?
D:yield(D)=w1...wi pr(D minus last step)
where D is the set of derivations in the parser?s
beam at any given point; D : yield(D) = w1...wi
is the set of all derivations in D consistent with
w1...wi; and D minus last step includes all steps
in the derivation except for the last step, in which wi
is generated by conditioning upon all previous steps
of D (including ti).
Roark et al (2009) show that syntactic surprisal
produces more accurate reading time predictions on
an English corpus than POS surprisal, and that de-
composing total surprisal into its syntactic and lex-
ical components produces more accurate reading
time predictions than total surprisal taken as a single
quantity. In this work, we compare not only differ-
ent types of syntactic models, but also different mea-
sures of surprisal under each of those models (total,
POS, syntactic-only, and lexical-only).
3 Models
Estimating surprisal(wi) amounts to calculating
?log(pr(wi|w1...wi?1)). Language models differ
in the way they estimate the conditional proba-
bility of the event wi given the observed context
w1...wi?1. In the traditional formulation of surprisal
under a hierarchical model, the event wi is condi-
tioned not only on the observed context w1...wi?1
but also on the latent context consisting of the syn-
tactic trees T whose yield is w1...wi?1; computing
pr(wi|w1...wi?1) therefore requires marginalizing
over all possible latent contexts T . In this formu-
lation of surprisal, the context includes lexical infor-
mation (w1...wi?1) as well as syntactic information
(T : yield(T ) = w1...wi?1), and the predicted event
itself (wi) contains lexical information.
Other formulations of surprisal are also possible,
in which the event, observed context, and latent con-
text are otherwise defined. In this work, we classify
syntactic models as follows: lexicalized models in-
clude lexical information in the context, in the pre-
dicted event, or both; unlexicalized models include
lexical information neither in the context nor in the
predicted event; hierarchical models induce a latent
context of trees compatible with the input; sequen-
tial models either induce no latent context at all,
or induce a latent sequence of POS tags compati-
ble with the input. Table 1 summarizes the syntactic
models and various formulations of surprisal used in
this work.
Following Frank and Bod (2011), we consider one
type of hierarchical model (PSG?s) and two types of
sequential models (Markov models and ESN?s).
3.1 Phrase-Structure Grammars
PSG?s consists of rules expanding a parent node into
children nodes in the syntactic tree, with associ-
ated probabilities. Frank and Bod (2011) use PSG?s
that generate POS tag sequences, not words. Under
such grammars, the prefix probability of a tag se-
quence t is the sum of the probabilities of all trees
T : yield(T ) = t1...ti, where the probability of
each tree T is the product of the probabilities of the
rules used in the derivation of T .
Vanilla PCFG?s, a special case of PSG?s in which
the probability of a rule depends only on the identity
of the parent node, achieve sub-optimal parsing ac-
curacy relative to grammars in which the probability
of each rule depends on a richer context (Charniak,
1996; Johnson, 1998; Klein and Manning, 2003).
To this end, Frank and Bod (2011) explore several
variants of PSG?s conditioned on successively richer
contexts, including ancestor models (which condi-
tion rule expansions on ancestor nodes from 1-4
levels up in the tree) and ancestor+sibling models
(which condition rule expansions on the ancestor?s
left sibling as well). Both sets of grammars also con-
63
Authors Model Surprisal Observed Latent Predicted
Context Context Event
Boston et al (2008) Hier. POS ti....ti?1 Trees T with yield t1...ti?1 ti
Demberg and Keller (2008)
Roark et al (2009)
Frank and Bod (2011)
This Work
Demberg and Keller (2008) Hier. Total w1...wi?1 Trees T with yield t1...ti?1 wi
Roark et al (2009)
This Work
Roark et al (2009) Hier. Syntactic- w1...wi?1 Trees T with yield w1...wi?1 ti
This Work Only
Roark et al (2009) Hier. Lexical- w1...wi?1 Trees T with yield w1...wi?1; ti wi
This Work Only
Frank and Bod (2011) Seq. POS ti....ti?1 ? ti
This Work
? Seq. Total w1...wi?1 t1...ti?1 with yield w1...wi?1 wi
Table 1: Contexts and events used to produce surprisal measures under various probabilistic syntactic models. T refers
to trees; t refers to POS tags; and w refers to words.
dition rule expansions on the current head node2.
In addition to the grammars over POS tag se-
quences used by Frank and Bod (2011), we evalu-
ate PSG?s over word sequences. We also include
the state-of-the-art Berkeley grammar (Petrov and
Klein, 2007) in our comparison. Syntactic cate-
gories in the Berkeley grammar are automatically
split into fine-grained subcategories to improve the
likelihood of the training corpus under the model.
This increased expressivity allows the parser to
achieve state-of-the-art automatic parsing accuracy,
but increases grammar size considerably.3
3.2 Markov Models
Frank and Bod (2011) use Markov models over
POS tag sequences, where the prefix probability
of a sequence t is
?
i pr(ti|ti?n+1, ti?n+2...ti?1).
They use three types of smoothing: additive, Good-
Turing, and Witten-Bell, and explore values of n
from 1 to 3.
2or rightmost child node, if the head node is not yet avail-
able(Roark, 2001).
3To make parsing with the Berkeley grammar tractable un-
der the prefix probability parser, we prune away all rules with
probability less than 10?4.
3.3 Echo State Networks
Unlike Markov models, ESN?s (Ja?ger, 2001) can
capture long-distance dependencies. ESN?s are a
type of recurrent neural network (Elman, 1991) in
which only the weights from the hidden layer to the
output layer are trained; the weights from the input
layer to the hidden layer and from the hidden layer
to itself are set randomly and do not change. In re-
current networks, the activation of the hidden layer
at tag ti depends not only on the activation of the in-
put layer at tag ti, but also on the activation of the
hidden layer at tag ti?1, which in turn depends on
the activation of the hidden layer at tag ti?2, and so
forth. The activation of the output layer at tag ti is
therefore a function of all previous input symbols
t1...ti?1 in the sequence. The prefix probability of
a sequence t under this model is
?
i pr(ti|t1...ti?1),
where pr(ti|t1...ti?1) is the normalized activation of
the output layer at tag ti. Frank and Bod (2011) eval-
uate ESN?s with 100, 200...600 hidden nodes.
4 Methods
We use two incremental parsers to calculate sur-
prisals under the hierarchical models. For the PSG?s
available under the Roark et al (2009) parser, we
use that parser to calculate approximate prefix prob-
64
abilities using beam search. For the Berkeley gram-
mar, we use a probabilistic Earley parser modified
by Levy4 to calculate exact prefix probabilities us-
ing the algorithm of Stolcke (1995). We evaluate
each hierarchical model under each type of surprisal
(POS, total, lexical-only, and syntactic-only), where
possible.
4.1 Data Sets
Each syntactic model is trained on sections 2-21 of
the Wall Street Journal (WSJ) portion of the Penn
Treebank (Marcus et al, 1994), and tested on the
Dundee Corpus (Kennedy and Pynte, 2005), which
contains reading time measures for 10 subjects over
a corpus of 2,391 sentences of naturally occurring
text. Gold-standard POS tags for the Dundee cor-
pus are obtained automatically using the Brill tagger
(Brill, 1995).
Frank and Bod (2011) exclude subject/word pairs
from evaluation if any of the following conditions
hold true: ?the word was not fixated, was presented
as the first or last on a line, was attached to punc-
tuation, contained more than one capital letter, or
contained a non-letter (this included clitics)?. This
leaves 191,380 subject/word pairs in the data set
published by Frank and Bod (2011). Because we
consider lexicalized hierarchical models in addition
to unlexicalized ones, we additionally exclude sub-
ject/word pairs where the word is ?unknown? to the
model.5 This leaves us with a total of 148,829 sub-
ject/word pairs; all of our reported results refer to
this data set.
4.2 Evaluation
Following Frank and Bod (2011), we compare the
per-word surprisal predictions from hierarchical and
sequential models of syntactic structure along two
axes: linguistic accuracy (how well the model ex-
plains the test corpus) and psychological accuracy
(how well the model explains observed reading
times on the test corpus).
4The prefix parser is available at:
www.http://idiom.ucsd.edu/ rlevy/prefixprobabilityparser.html
5We consider words appearing fewer than 5 times in the
training data to be unknown.
4.2.1 Linguistic Accuracy
Each model provides surprisal estimates
surprisal(wi). The linguistic accuracy over
the test corpus is 1n
?n
i=1 surprisal(wi), where n
is the number of words in the test corpus.
4.2.2 Psychological Accuracy
We add each model?s per-word surprisal predic-
tions to a linear mixed-effects model of first-pass
reading times, then measure the improvement in
reading time predictions (according to the de-
viance information criterion) relative to a baseline
model; the resulting decrease in deviance is the
psychological accuracy of the language model.
Using the lmer package for linear mixed-effects
models in R (Baayen et al, 2008), we first fit a
baseline model to first-pass readings times over
the test corpus. Each baseline model contains
the following control predictors for each sub-
ject/word pair: sentpos (position of the word in
the sentence), nrchar (number of characters in
the word), prevnonfix (whether the previous
word was fixated by the subject), nextnonfix
(whether the next word was fixated by the subject),
logwordprob (log(pr(wi))), logforwprob
(log(pr(wi|wi?1))), and logbackprob
(log(pr(wi|wi+1))). When fitting each base-
line model, we include all control predictors; all
significant two-way interactions between them
(|t| ? 1.96); by-subject and by-word intercepts;
and a by-subject random slope for the predictor that
shows the most significant effect (nrchar).6
We evaluate the statistical significance of the dif-
ference in psychological accuracy between two pre-
dictors using a nested model comparison. If the
model containing both predictors performs signifi-
cantly better than the model containing only the first
predictor under a ?2 test (p ? 0.05), then the sec-
ond predictor accounts for variance in reading times
above and beyond the first predictor, and vice versa.
6In accordance with the methods of Frank and Bod (2011),
?Surprisal was not included as a by-subject random slope be-
cause of the possibility that participants? sensitivity to surprisal
varies more strongly for some sets of surprisal estimates than
for others, making the comparisons between language models
unreliable. Since subject variability is not currently of interest,
it is safer to leave out random surprisal effects.?
65
5 Results
We first replicate the results of Frank and Bod
(2011) by obtaining POS surprisal values directly
from the authors? published dataset for each syntac-
tic model, then evaluating the psychological accu-
racy of each of those models relative to the baseline
model defined above.7
Baseline Model with Additional Lexical N-grams
Next, we explore the impact of the lexical n-gram
probabilities used as control predictors upon psy-
chological accuracy. Frank and Bod (2011) state
that they compute lexical unigram and bigram prob-
abilities via linear interpolation between estimates
from the British National Corpus and the Dundee
corpus itself (p.c.); upon inspection, we find that the
bigram probabilities released in their published data
set (which are consistent with their published exper-
imental results) more closely resemble probabilities
estimated from the Dundee corpus alone. Because of
the small size of the Dundee corpus, lexical bigrams
from this corpus alone are unlikely to be representa-
tive of a human?s language experience.
We augment the lexical bigram probabilities used
in the baseline model of Frank and Bod (2011)
with additional lexical unigram and bigrams esti-
mated using the SRILM toolkit (Stolcke, 2002) with
Kneser-Ney smoothing from three corpora: sec-
tions 2-21 of the WSJ portion of the Penn Tree-
bank, the Brown corpus, and the British National
corpus. We include these additional predictors and
all two-way interactions between them in the base-
line model. Figure 1 shows that the relative differ-
ences in psychological accuracy between unlexical-
ized hierarchical and sequential models vanish under
this stronger baseline condition.8
Unlexicalized Hierarchical Models We then cal-
culate POS surprisal values under each of the ances-
tor (a1-a4) and the ancestor+sibling (s1-s4) hierar-
chical models ourselves, using the parser of Roark
7The only difference between our results and the original
results in Figure 2 of Frank and Bod (2011) is that we evaluate
accuracy over a subset of the subject/items pairs used in Frank
and Bod (2011) (see Section 4.1 for details).
8The psychological accuracies of the best sequential model
(e4) and the best hierarchical model (s3) used in Frank and Bod
(2011) relative to the stronger baseline with additional lexical
n-grams are not significantly different, according to a ?2 test.
et al (2009). We also calculate POS surprisal un-
der the Berkeley grammar (b) using the Levy prefix
probability parser. Figure 2 shows the accuracies of
these models.9
Lexicalized Hierarchical Models Next, we lex-
icalize the hierarchical models. Figure 3 shows
the results of computing total surprisal under
each lexicalized hierarchical model (a1-a4T, s1-s4T,
and bT). The lexicalized models improve signifi-
cantly upon their unlexicalized counterparts (?2 =
7.52 to 12.47, p ? 0.01) in all cases; by con-
trast, the unlexicalized models improve signifi-
cantly upon their lexicalized counterparts (?2 =
4.05 to 5.92, p ? 0.05) only in some cases (s1-
s4). Each lexicalized model improves significantly
upon e4, the best unlexicalized model of Frank
and Bod (2011) (?2 = 6.96 to 23.45, p ? 0.01),
though e4 also achieves a smaller but still signifi-
cant improvement upon each of the lexicalized mod-
els (?2 = 4.49 to 7.58, p ? 0.05). The lexical-
ized Berkeley grammar (bT) achieves the highest
linguistic and psychological accuracy; the improve-
ment of bT upon e4 is substantial and significant
(?2(1) = 23.45, p ? 0.001), while the improve-
ment of e4 upon bT is small but still significant
(?2(1) = 4.50, p ? 0.1). Estimated coefficients
for surprisal estimates under each lexicalized hierar-
chical model are shown in Table 2.10
Decomposing Total Surprisal Figure 3 shows the
results of decomposing total surprisal (a1-a4T, s1-
s4T) into its lexical and syntactic components, then
entering both components as predictors into the
mixed-effects model (a1-a4LS, s1-s4LS).11 For each
grammar, the psychological accuracy of the surprisal
estimates is slightly higher when both lexical and
syntactic surprisal are entered as predictors, though
the differences are not statistically significant.
9Our POS surprisal estimates have slightly worse linguistic
accuracy but slightly better psychological accuracy than Frank
and Bod (2011); these differences are likely due to differences
in beam settings and in the subset of the WSJ used as training
data.
10Each surprisal estimate predicts reading times in the ex-
pected (positive) direction.
11Decomposing surprisal into its lexical and syntactic com-
ponents is possible with the Levy prefix probability parser as
well, but requires modifications to the parser; the Roark et al
(2009) parser computes these quantities explicitly by default.
66
Figure 1: Psychological vs. linguistic accuracy of POS sur-
prisal estimates from unlexicalized sequential and hierar-
chical models of Frank and Bod (2011) relative to baseline
system of Frank and Bod (2011) (shown above dotted line),
and relative to a baseline system including additional lex-
ical unigrams and bigrams (shown below dotted line). In-
corporating additional lexical n-grams into baseline system
virtually eliminates all differences in psychological accu-
racy among models.
Figure 2: Psychological vs. linguistic accuracy of POS
surprisal estimates from unlexicalized hierarchical models
used in this work, relative to a baseline system with ad-
ditional lexical unigrams and bigrams. Horizontal line in-
dicates most psychologically accurate model of Frank and
Bod (2011) for ease of comparison.
POS vs. Syntactic-only Surprisal Figures 2 and
4 show the results of computing POS surprisal (a1-
a4, s1-s4) and syntactic-only surprisal (a1-a4S, s1-
s4S), respectively, under each of the Roark gram-
mars. While syntactic surprisal achieves slightly
higher psychological accuracy than POS surprisal
for each model, the difference is statistically signifi-
cant in only one case (s1).
6 Discussion
In the presence of additional lexical n-gram control
predictors, all gaps in performance between the un-
lexicalized sequential and hierarchical models used
in Frank and Bod (2011) vanish (Figure 1). Frank
and Bod (2011) do not include lexicalized hierarchi-
cal models in their study; our results indicate that
lexicalizing hierarchical models improves their psy-
chological accuracy significantly compared to the
unlexicalized versions. Overall, the lexicalized hier-
archical model with the highest linguistic accuracy
(Berkeley) also achieves the highest psychological
accuracy.
Decomposing total surprisal into its lexical- and
syntactic-only components improves psychological
accuracy, but this improvement is not statistically
significant. Computing syntactic-only surprisal in-
stead of POS surprisal improves psychological accu-
racy, but this improvement is statistically significant
in only one case (s1).
7 Conclusion and Future Work
Frank and Bod (2011) claim that sequential unlexi-
calized syntactic models predict reading times bet-
ter than hierarchical unlexicalized syntactic models,
and conclude that the human parser is insensitive
to hierarchical syntactic structure. We find that the
picture is more complicated than this. We show,
first, that the gap in psychological accuracy between
the unlexicalized hierarchical and sequential models
of Frank and Bod (2011) vanishes when additional,
67
Figure 3: Psychological vs. linguistic accuracy of lexi-
cal+syntactic (LS) and total (T) surprisal estimates from
lexicalized hierarchical models used in this work, relative
to baseline system with additional lexical unigrams and bi-
grams as control predictors. Decomposing total surprisal
into lexical-only and syntactic-components improves psy-
chological accuracy. Horizontal line indicates most psy-
chologically accurate model of (Frank and Bod, 2011).
Figure 4: Psychological vs. linguistic accuracy of lexical-
only (L) and syntactic-only (S) surprisal estimates from
lexicalized hierarchical models used in this work, relative
to baseline system with additional lexical unigrams and bi-
grams as control predictors. On its own, syntactic-only sur-
prisal predicts reading times better than lexical-only sur-
prisal. Horizontal line indicates most psychologically ac-
curate model of (Frank and Bod, 2011).
Surprisal Coef. |t| Surprisal Coef. |t|
a1LS 0.82 2.61 a1T 1.30 2.98
a2LS 1.01 3.24 a2T 1.38 3.19
a3LS 1.14 3.65 a3T 1.56 3.60
a4LS 1.17 3.76 a4T 1.56 3.64
s1LS 1.38 4.43 s1T 1.71 4.00
s2LS 1.37 4.44 s2T 1.75 4.16
s3LS 1.20 3.90 s3T 1.64 3.91
s4LS 1.21 3.97 s4T 1.62 3.89
bT 3.15 5.34
Table 2: Estimated coefficients and |t|-values for sur-
prisal estimates shown in Figure 3. Coefficients are es-
timated by adding each surprisal estimate, one at a time,
to the baseline model of reading times used in Figure 3.
robustly estimated lexical n-gram probabilities are
incorporated as control predictors into the baseline
model of reading times. Next, we show that lexical-
izing hierarchical grammars improves psychological
accuracy significantly. Finally, we show that using
better lexicalized hierarchical models improves psy-
chological accuracy still further. Our results demon-
strate that the claim of Frank and Bod (2011) that
sequential models predict reading times better than
hierarchical models is premature, and that further in-
vestigation is required.
In future work, we plan to incorporate lexical in-
formation into the sequential syntactic models used
in Frank and Bod (2011) so that we can compare
the hierarchical lexicalized models described here
against sequential lexicalized models.
Acknowledgments
The authors thank Stefan Frank for providing the
dataset of Frank and Bod (2011) and a detailed spec-
ification of their experimental configuration. This
research was supported by NSF grant 0953870,
NIH grant 1R01HD065829, and funding from the
Army Research Laboratory?s Cognition & Neuroer-
gonomics Collaborative Technology Alliance.
68
References
R. H. Baayen, D. J. Davidson, and D. M. Bates. 2008.
Mixed-effects modeling with crossed random effects
for subjects and items. In Journal of Memory and Lan-
guage, 59, pp. 390-412.
Marisa Ferrara Boston, John Hale, Reinhold Kliegl,
Umesh Patil, and Shravan Vasishth. 2008. Parsing
costs as predictors of reading difficulty: An evaluation
using the potsdam sentence corpus. In Journal of Eye
Movement Research, 2(1):1, pages 1-12.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational lin-
guistics, 21(4).
Eugene Charniak. 1996. Tree-bank grammars. In AAAI.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. In Cognition, Volume 109, Is-
sue 2, pages 193-210.
J.L. Elman. 1991. Distributed representations, simple re-
current networks, and grammatical structure. Machine
Learning, 7(2).
Stefan Frank and Rens Bod. 2011. Insensitivity of
the human sentence-processing system to hierarchical
structure. In Psychological Science.
Edward Gibson, Timothy Desmet, Daniel Grodner, Du-
ane Watson, and Kara Ko. 2005. Reading relative
clauses in english. Cognitive Linguistics, 16(2).
John Hale. 2001. A probabilistic earley parser as a psy-
cholinguistic model. In Proceedings of NAACL.
Herbert Ja?ger. 2001. The? echo state? approach to
analysing and training recurrent neural networks. In
Technical Report GMD 148, German National Re-
search Center for Information Technology.
Mark Johnson. 1998. Pcfg models of linguistic tree rep-
resentations. Computational Linguistics, 24.
A. Kennedy and J. Pynte. 2005. Parafoveal-on-foveal
effects in normal reading. Vision research, 45(2).
Jonathan King and Marcel Just. 1991. Individual dif-
ferences in syntactic processing: The role of working
memory. Journal of memory and language, 30(5).
Dan Klein and Chris Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of ACL.
Lars Konieczny and Philipp Do?ring. 2003. Anticipation
of clause-final heads: Evidence from eye-tracking and
srns. In Proceedings of ICCS/ASCS.
Lars Konieczny. 2000. Locality and parsing complexity.
Journal of Psycholinguistic Research, 29(6).
E. Lau, C. Stroud, S. Plesch, and C. Phillips. 2006. The
role of structural prediction in rapid syntactic analysis.
Brain and Language, 98(1).
Roger Levy. 2008. Expectation-based syntactic compre-
hension. Cognition.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert Macintyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The penn tree-
bank: Annotating predicate argument structure,. In
Proceedings of ARPA Human Language Technology
Workshop.
Kentaro Nakatani and Edward Gibson. 2008. Distin-
guishing theories of syntactic expectation cost in sen-
tence comprehension: Evidence from japanese. Lin-
guistics, 46(1).
Joakim Nivre. 2006. Inductive dependency parsing, vol-
ume 34. Springer Verlag.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedinngs of HLT-
NAACL.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and syn-
tactic expectation-based measures for psycholinguistic
modeling via incremental top-down parsing. In Pro-
ceedings of EMNLP.
Brian Roark. 2001. Probabilistic top-down parsing and
language modeling. Computational linguistics, 27(2).
A. Staub and C. Clifton. 2006. Syntactic prediction in
language comprehension: Evidence from either... or.
Journal of Experimental Psychology: Learning, Mem-
ory, and Cognition, 32(2).
A. Staub, C. Clifton, and L. Frazier. 2006. Heavy np shift
is the parsers last resort: Evidence from eye move-
ments. Journal of memory and language, 54(3).
Andreas Stolcke. 1995. An efficient probabilistic
context-free parsing algorithm that computes prefix
probabilities. Computational Linguistics, 21(2).
A. Stolcke. 2002. Srilm-an extensible language mod-
eling toolkit. In Seventh International Conference on
Spoken Language Processing.
P. Sturt, F. Keller, and A. Dubey. 2010. Syntactic prim-
ing in comprehension: Parallelism effects with and
without coordination. Journal of Memory and Lan-
guage, 62(4).
Shravan Vasishth and Richard Lewis. 2006. Argument-
head distance and processing complexity: Explaining
both locality and antilocality effects. Linguistic Soci-
ety of America, 82(4).
69
