Integrated Feasibility Experiment for Bio-Security: IFE-Bio
A TIDES Demonstration
Lynette Hirschman, Kris Concepcion, Laurie Damianos, David Day, John Delmore, Lisa Ferro,
John Griffith, John Henderson, Jeff Kurtz, Inderjeet Mani, Scott Mardis, Tom McEntee, Keith
Miller, Beverly Nunan, Jay Ponte, Florence Reeder, Ben Wellner, George Wilson, Alex Yeh
The MITRE Corporation
Bedford, Massachusetts, USA and
McLean, Virginia, USA
781-271-7789
lynette@mitre.org
ABSTRACT
As part of MITRE?s work under the DARPA TIDES
(Translingual Information Detection, Extraction and
Summarization) program, we are preparing a series of
demonstrations to showcase the TIDES Integrated Feasibility
Experiment on Bio-Security (IFE-Bio).  The current
demonstration illustrates some of the resources that can be made
available to analysts tasked with monitoring infectious disease
outbreaks and other biological threats.
Keywords
Translation, information extraction, summarization, topic
detection and tracking, system integration.
1. INTRODUCTION
The long-term goal of TIDES is to provide delivery of
information on demand in real-time from live on-line sources. For
IFE-Bio, the resources made available to the analyst include e-
mail, news groups, digital library resources, and eventually (in
later versions), topic-specific segments from broadcast news.
Because of the emphasis on global monitoring, there is a need to
process incoming information in multiple languages.  The system
must deliver the appropriate information content in the
appropriate form and in the appropriate language (taken for now
to be English). This means that the IFE-Bio system will have to
deliver news stories, clusters of relevant documents, threaded
discussions, alerts on new events, tables, summaries (particularly
over document collections), answers to questions, graphs and geo-
spatial  temporal displays of information.
The demonstration system for the Human Language Technology
Conference in March 2001 represents an early stage of the full
IFE-Bio system, with an emphasis on end-to-end processing.
Future demonstrations will make use of MITRE?s Catalyst
architecture, providing an efficient, scalable architecture to
facilitate  integration of multiple stages of linguistic processing.
By June 2001, the IFE-Bio system will provide richer linguistic
processing through the integration of modules contributed by
other TIDES participants. By June 2002, the IFE-Bio system will
include additional functionality, such as real-time broadcast news
feeds, new machine translation components, support for question-
answering, cross-language information retrieval, multi-document
summarization, automatic extraction and normalization of
temporal and spatial information, and automated geospatial and
temporal displays.
2. The IFE-Bio System
The current demonstration (March 2001) highlights the basic
functionality required by an analyst, including:
? Capture of sources, including e-mail, digital library
material, news groups, and web-based resources;
? Categorizing of the sources into multiple orthogonal
hierarchies useful to the analyst, e.g., disease, region, news
source, language;
? Processing of the information through various stages,
including ?zoning? of the text to select the relevant portions
for processing; named entity detection, event detection,
extraction of temporal information, summarization, and
translation from Spanish, Portuguese, and Chinese into
English;
? Access to the information through use of any mail and news
group reader, which allows the analyst to organize, save, and
share the information in a familiar, readily accessible
environment;
? Display of the information in alternate forms, including
color-tagged documents, tables, summaries, graphs, and
geospatial, map-based displays.
Figure 1 below shows the overall functionality envisioned
for the IFE-Bio system, including capture, categorizing,
processing, access and display.
Collection capability for the current IFE-Bio system includes
email, news groups, journals, and Web resources. We have a
complete copy of the ProMED mailings (a moderated source
tracking global infectious disease outbreaks), and are routinely
collecting other information sources from the World Health
Organization and CDC.  In addition, we are collecting several
general global news feeds. Current volume is around 2000
messages per day; we estimate capacity for the current system at
around 4500 messages/day. Once we have integrated a filtering
capability, we expect the volume of messages saved in IFE-Bio
should drop significantly, since many of the global news services
report on a wide range of events and not all need to be passed on
to IFE-Bio analysts.  The categorizing of sources is done based on
the message header. The header is synthesized by extracting key
information about disease name, the country, and other relevant
information such as type of victim and source of information, as
well as date of message receipt.
The processing for the current demonstration system uses a
limited subset of the Catalyst architecture capabilities and a
number of in-house linguistic modules. The linguistic modules in
the current demonstration system include tokenization, sentence
segmentation, part-of-speech tagging, named entity detection,
temporal extraction (Mani and Wilson 2000) and source-specific
event detection.  In addition, we have incorporated the
CyberTrans embedded machine translation system which ?wraps?
available machine translation engines to make them available via
an e-mail or Web interface (Reeder 2000). Single document
summarization is performed by the MITRE WebSumm system
(Mani and Bloedorn 1999).
We carefully chose a light-weight interface mechanism for
delivery of the information to the analyst.  By treating the
incoming streams of data as feeds to a news server, the analyst can
inspect and organize the information using a familiar news and e-
mail browser. The analyst can subscribe to areas of interest, flag
important messages, watch specific threads, and create tailored
filters for monitoring outbreaks. The stories are crossed-posted to
multiple relevant news groups, based on the information in the
header, e.g., a story on Ebola in Africa would be cross posted to
the Africa regional newsgroup and to the Ebola disease
newsgroup. Search by subject and date allow the analyst to select
subsets of the messages for further processing, annotation or
sharing.  The news client provides notification of incoming
messages. In later versions, we plan to integrate topic detection
and tracking capabilities, to provide improved filtering and
routing of messages, as well as detection of new topics.  The use
of this simple delivery mechanism provides a familiar
environment with almost no learning curve, and it avoids issues of
platform and operating system dependence.
Finally, the system makes use of several different devices to
display the information appropriately. Figure 2 shows the layout
of the Netscape news browser interface.  It includes the list of
newsgroups that have been subscribed to (on the left), the list of
messages from the chosen newsgroup (on top), and a particular
message with color-coded named entities (including disease terms
displayed in red, so that they are easy to spot in the message).
What is the status of the
current Ebola outbreak?
The epidemic is contained;
as of 12/22/00, there were 
421 cases with 162 deaths
Interaction
CDC
WHO
Medical
literatureEmail:
ProMed
~ 2500
 stories/day
Internl
News
Sources  Capture
Translingual 
Information 
Detection 
Extraction 
Summarization 
U niden tif ied h emor rhagic  f
U niden tif ied h emor rhagic  f
Ebola hemorr hagic  fever  in
Re :  Ebo la hemorrhagi. ..
R e: Ebola hemo rrha gi...
ProMED
A nnotator
Ja ne Analyst
10 /17/00 1 9:37
10 /17/00 2 0:42
10 /18/00 7 :42
High
Norm al
Normal
read
rep lied
ProMED
10 /18/00 1 2:3 4 High un read
Ebola hemorr hagic  fever  in
Sour ce
D ate
Priority Status
10   99
0   105
1   57
0   10
2   34
0   50
1   1
0   25
5   200
0   45
0   0
0   0
0   0
0   0
0   6
0   32
0   3
0   1
High
Norm al
High
High
Ebola hemorrhagic feve r  -  Ugan da
U nf ilte red
O utbr eak
     C holer a
     D engue  Fe ve r
     Eb ola
I nfras tructure?
N atu ral  Di sas. ..
Spi lls
A cc id en ts
W M D Tra ckin. ..
Sus picious Il ln. ..
Sus picious De.. .
Pos sible  Biol o. ..
Pathogen threa ?
- --- --- ---------------------
W orkspa ce
      E bola
      D ra fts
      Re ports
D isease
R e: Ebola hemo rrha gi...
Location
U NK
U NK
Ebola
Ebola
Ebola
Ebola
Rabies
Rabies
U gan da
U gan da
U gan da
K eny a
U gan da
IHT
ProMED
WHO
Jo e Analyst
D ate
10 /14/00 2 3:06
10 /15/00 1 0:50
10 /16/00 2 1:45
10 /17/00 1 9:12
read
read
read
read
un read
Date: 10/16/00
Disease: Ebo la
Descripto r: hem orrh agic fever
Locatio n:          Ugan da
Disease Date:     10/14/00
Ho spital: mission ary hosp ital  in Gulu
New cases:  at least  7
Total  cases: 51
Total  dead:       31
Ebola hemorr hagic  fever  -
Ugand an M ini stry  ide ntif ies Eb ola  virus as t he c ause of  the outbreak.  KA MP ALA :
The  dreade d Eb ola  virus that struck over 300  peopl e i n Kikwit,  in  t he D emocratic
Rep ub lic  of Con go  in  1995, has ki lled 31  people in northe rn Ugan da.  A  U gandan
M ini s tr y of Heal th  sta tement  said l aboratory test s had r eveale d that  the Ebola vi rus
was  t he caus e of the  epidemi c hemorr hagic feve r whi ch has been r agi ng in the  G ulu
dis trict  since Septe mbe r.   Thr ee  of the dea d wer e s tud ent nur ses , who tre ated the first
Eb ola  patients admitt ed to a  Lac or  mis sionary hosp it al in  Gu lu  tow n.  A  task force
he ade d by G ul u dis trict adm ini str ator, Walte r O ch ora , has bee n se t up to co-or dina te
efforts to control the epi demi c.  F ie ld offic ials i n  Gul u tol d the Ka mpala-based Ne w
H ttp: //ti des2000.mi tre.org/
Pr oM ED /10162000/34n390h.ht ml
U gan da
News Repository
CATALYSTEntity Tagging
Event Extraction
Translation
Summarization
Alerting
Change detection
Threading
Cross-language IR
Topic clustering
Figure 1: Overview of the IFE-Bio Demonstration System
Local,
private
workspace
Documents
automatically
categorized
into shared,
tailorable
hierarchy
Sort by disease, location, source, date, etc.
Associated
meta-data:
header,
event,
summary,
named-entity
Figure 2: Screenshot of IFE-Bio Interface Using News Group Reader
Figure 3: Sample Summarization Automatically Generated by WebSumm
There are multiple display modalities available. The message in
Figure 2 contains a short tabular display in the beginning,
identifying disease, region and victim type. Below that is a URL
to a document summary, created by MITRE?s WebSumm system
(see Figure 3 for a sample summary).   If an incoming message is
in a language other than English, then CyberTrans is called to run
code set and language identification modules, and the language is
translated into English for further processing. Figure 4 below
shows a sample translated message; note that there are a number
of untranslated words, but it is still possible to get the gist of the
message.
In addition, we are working on a mechanism to provide
geographic and eventually, temporal display of outbreak
information. Figure 5 shows the stages of processing involved.
Stage 1 shows onamed entity and temporal tagging to identify the
items of interest. These are combined into disease events by
further linguistic processing; the result is shown in the table in
Stage 2. This spreadsheet of events serves as input for a map-
based display, shown in Stage 3. The graph plots number of new
cases and number of cumulative cases over time.  In the map, the
size of the outer dot represents total number of cases to date, and
the inner dot represents new cases.  This allows the analyst to
visualize spread of the disease, as well as the stage of the outbreak
(spreading or subsiding).
3. REFERENCES
[1] Mani, I. and Bloedorn, E. (1999). "Summarizing
Similarities and Among Related Documents".
Information Retrieval 1(1): 35-67.
[2] Mani, I. and Wilson, G. (2000). "Robust Temporal
Processing of News," Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics (ACL'2000), 69-76. New Brunswick, New
Jersey. Association for Computational Linguistics.
[3] Reeder, F.  (2000) "At Your Service:  Embedded MT
as a Service",  NAACL Workshop on Embedded MT,
March, 2000.
Figure 4: Translation from Portuguese to English Produced by CyberTrans
1. Annotate entities of interest via XML
Dise a se Source Country City_na m eDa te Ca se s Ne w _ca se s De a d
Ebola PROM ED Uganda G ula 26-O ct-2000 182 17 64
Ebola PROM ED Uganda G ula 5-Nov-2000 280 14 89
Ebola PROM ED Uganda G ulu 13-O ct-2000 42 9 30
Ebola PROM ED Uganda G ulu 15-O ct-2000 51 7 31
Ebola PROM ED Uganda G ulu 16-O ct-2000 63 12 33
Ebola PROM ED Uganda G ulu 17-O ct-2000 73 2 35
Ebola PROM ED Uganda G ulu 18-O ct-2000 94 21 39
Ebola PROM ED Uganda G ulu 19-O ct-2000 111 17 41
2. Assemble entities into events
0
50
100
150
200
250
300
350
400
10/
13/
200
0
10/
20/
200
0
10/
27/
200
0
11/
3/2
000
11/
10/
200
0
11/
17/
200
0
11/
24/
200
0
T IME
Nu
m
be
r C
as
es
Cases
New_cases
Dead
3. Display events...
   Total Cases   New Cases
Figure 5: Steps in Extraction to Support Temporal and Geospatial Displays of Disease Outbreak
Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 299?300,
New York City, June 2006. c?2006 Association for Computational Linguistics
1. What?s in a Name: Current Methods, Applications, and Evaluation
in Multilingual Name Search and Matching
Sherri Condon and Keith J. Miller, MITRE
Names of people, places, and organizations have unique linguistic properties, and they typically require
special treatment in automatic processes. Appropriate processing of names is essential to achieve high-
quality information extraction, speech recognition, machine translation, and information management, yet
most HLT applications provide limited specialized processing of names. Variation in the forms of names can
make it difficult to retrieve names from data sources, to perform co-reference resolution across documents,
or to associate instances of names with their representations in gazetteers and lexicons. Name matching
has become critical in government contexts for checking watchlists and maintaining tax, health, and So-
cial Security records. In commercial contexts, name matching is essential in credit, insurance, and legal
applications.
This tutorial will focus on personal names, with special attention given to Arabic names, though it will
be clear that much of the material applies to other languages and to names of places and organizations. Case
studies will be used to illustrate problems and approaches to solutions. Arabic names illustrate many of the
issues encountered in multilingual name matching, among which are complex name structures and spelling
variation due to morphophonemic alternation and competing transliteration conventions.
1.1 Tutorial Outline
1. Name matching across languages, scripts, and cultures
? Survey of problems using Arabic case study
* Name parts and structure (titles, initials, particles, prefixes, suffixes, nicknames,
tribal names)
* Transliteration complications (segmentation, ambiguity, incompleteness, dialect
variation, acoustic mismatches, competing standards)
* Other difficulties presented by personal names
? Survey of approaches to solutions, advantages/disadvantages of each:
* SOUNDEX, generic string matching (Levenshtein, n-gram, Jaro-Winkler),
* Variant generation (pattern matching, dictionaries, gazetteers),
* Normalization (morphological analysis, rewriting, ?deep? structures)
* Intelligent-search algorithms that incorporate linguistic knowledge in selection of
string-similarity measures, parameters, and lists
? Matching across scripts
* Methods for data acquisition
* Transliteration
* Phonological interlingua
2. Evaluation of Name Search and Matching Systems
? Development of ground-truth sets
* Human adjudication
* Estimation techniques
? Case study: adjudication exercises
? Issues in establishing ground truth: different truth for different applications
? Metrics (precision, recall, F scores, others)
? Case study comparing matching systems for Romanized Arabic names (based on MITRE
evaluation of 9 name matching products)
? Inter-adjudicator agreement
? Performance and other considerations
299
1.2 Target Audience
This tutorial is intended for those with interest in information retrieval and entity extraction, identity reso-
lution, Arabic computational linguistics, and related language-processing applications. As a relatively un-
studied domain, name matching is a promising area for innovation and for researchers seeking new projects.
Keith J. Miller received his Ph.D. in Computational Linguistics from Georgetown University. He spent
several years working on various large-scale name matching systems. His current research activities cen-
ter around multicultural name matching, machine translation, embedded HLT systems, and component and
system-level evaluation of systems involving HLT components.
Sherri Condon received her Ph.D. in Linguistics from the University of Texas at Austin. In addition to
several years of work in multilingual name matching and cross script name matching, she is a researcher in
discourse/dialogue, entity extraction, and evaluation of machine translation and dialogue systems.
300
Sharing Problems and Solutions for Machine Translation of
Spoken and Written Interaction
Sherri Condon             Keith Miller
The MITRE Corporation
7515 Colshire Drive
McLean, VA 22102-7508
{scondon, keith}@mitre.org
Abstract
Examples from chat interaction are
presented to demonstrate that machine
translation of written interaction
shares many problems with translation
of spoken interaction. The potential
for common solutions to the problems
is illustrated by describing operations
that normalize and tag input before
translation.  Segmenting utterances
into small translation units and
processing short turns separately  are
also motivated using data from chat.
1 Introduction
The informal, dialogic character of oral
interaction imposes demands on translation
systems that are not encountered in well-formed,
monologic texts.  These differences make it
appear that any similarities between the machine
translation of text and speech will be limited to
core translation components, as opposed to pre-
and post-processing operations that are linked to
the medium.
In this paper, we demonstrate that many
challenges of translating spoken interaction are
also encountered in translating written
interaction such as chat or instant messaging.
Consequently, it is proposed that solutions
developed for these common problems can be
shared by researchers engaged in applying
machine translation technologies to both types
of interaction.  Specifically, preprocessing
operations can address many of the problems
that make dialogic interaction difficult to
translate in both spoken and written media.
After surveying the challenges that are
shared in machine translation of spoken and
written interaction, we identify several areas in
which preprocessing solutions have been
proposed that could be fruitfully adopted for
either spoken or written input.  The speech
recognition problem of discriminating out of
vocabulary words from unrecognized
vocabulary words is equivalent to the problem
of discriminating novel forms that emerge in
chat environments from words that are
unrecognized due to nonstandard spellings.  We
suggest that a solution based on templates like
those used in example-based translation could
be a useful approach to the problem for both
spoken and written input.  Similarly, other
preprocessing operations that tag input for
special processing can be used to facilitate
translation of problematic phenomena such as
discourse markers and vocatives.  Finally, we
explore the possibility that the complexity of
translating interaction can be reduced by
translating smaller packages of input and
exploiting participants? strategies for packaging
certain discourse functions in smaller turn units.
2 Challenges for translation of
spoken and written interaction
In illustrating the problems for machine
translation that are shared by both spoken and
written interactions, we take for granted that
readers are aware of examples that occur in
spoken interaction because these are available in
the literature and from direct observation of
personal experience.  Therefore, we focus on
providing examples of written interaction to
demonstrate that the same kinds of challenges
arise in translation of chat and instant messages.
Most of the examples we present are taken from
logs of chat interactions collected from 10 chat
channels in 8 languages during July of 2001.
The examples are presented exactly as they
appeared in the logs.
                                            Association for Computational Linguistics.
                          Algorithms and Systems, Philadelphia, July 2002, pp. 93-100.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
2.1 Ellipsis and fragments
The elliptical and fragmentary quality of
ordinary spoken dialogue is well-known and is
characteristic of chat interaction, too, as in (1).
(1) a. faut voir
b. voir koi?
The French expression il faut ?it is necessary? is
used without the pleonastic pronoun il, and the
verb voir ?to see? is used without a direct object
expressing what it is necessary to see.  The
writer may have intended to use voir
intransitively, as in the English expression we?ll
have to see, but the interlocutor who responded
with (1b) asks ?to see what?? and omits both the
pronoun and the verb faut.  (Creative spelling
such as the convention that replaces the qu in
quoi with k in koi is discussed in section 3.3.)
Though it is unlikely that preprocessing
operations will be able to add information that is
missing from fragments and elliptical
expressions, these problems interact with
preprocessing operations such as segmentation
of units for translation (see 2.9).
2.2 High function/low content terms
Spoken interaction is replete with formulaic
expressions that serve significant interactional
functions, but have little or no internal structure.
These include greetings, leave-takings,
affirmations, negations, and other interjections,
some of which are illustrated in (2).
(2) a. re esselamu aleyk?m
b. in like califonia
c. jose?lets make 1
The expression re is a conventional greeting in
chat interaction with the function re-greet, as in
hello again.  In (2a) it is used on a Turkish chat
channel preceding a greeting borrowed from
Arabic with the literal meaning ?peace to you.?
The example in (2b) demonstrates that chat
interaction includes expressions such as like that
are usually associated exclusively with speech.
Like and discourse markers such as well, now, so
and anyway occur frequently in chat interaction.
Wiebe et al (1995) identify discourse markers
as a major area of difficulty for translation of
spoken interaction.  Many discourse markers are
homophonous and/or homographic with lexical
items that have very different meanings, and the
need to disambiguate polysemous words has
received much attention in the language
processing and machine translation literature.
The use of vocative proper names illustrated
in (2c) is frequent in chat interaction, where
participants? nicknames are used to direct
messages to specific interlocutors.  In a small
sample of 76 messages from our chat logs, 31%
included vocative uses of participants?
nicknames.  In speech and in chat interaction
like (2), where punctuation is unpredictable (see
3.2), capitalization cannot be relied on to
identify proper names.  The complexity of
translating proper names has also received
considerable attention in the machine translation
research community, and translation of proper
names has been proposed as an evaluation
measure for machine translation (Papineni et al,
2002; Vanni and Miller, 2002).
2.3 Vagueness
Though vagueness is a problem in all language
use, Wiebe et al (1995) identify it as a major
problem for translation of spoken interaction,
citing metaphorical expressions such as de alli,
literally, ?from there,? translated as after that.
More pervasive are the deixis and vagueness
that result from the shared online context that
participants in real time interaction can rely on,
compared to communication environments in
which relevant context must be encoded in the
text itself.   Researchers have demonstrated the
increased explicitness and structural complexity
of asynchronous interaction, in which delays
between the transmission of messages preclude
immediate feedback, compared to synchronous
interaction, in which it is expected that messages
will be responded to immediately (Chafe, 1982;
Condon and Cech, forthcoming; Sotillo, 2000).
Similarly, Popowich et al (2000) report that a
high degree of semantic vagueness is a problem
for translating closed captions because the visual
context provided by the television screen
supplies missing details.
2.4 Anaphora
Another consequence of the synchronous
communication environments in written and
spoken interaction is the high frequency of
pronouns and deictic forms.  Wiebe et al (1995)
report that 64% of utterances in a corpus of
spoken Spanish contained pronominals
compared to 48% of sentences in a written
corpus.  Similarly, numerous studies have
demonstrated the high frequency of personal
pronouns, especially first person pronouns, in
chat interaction compared to other types of
written texts (Ferrara, Brunner and Whittemore,
1991; Flanagan 1996; Yates, 1996).  Pronouns
are particularly problematic for translation when
the target language makes distinctions such as
gender that are not present in the source
language.  To determine the appropriate
inflection, the antecedent of the pronoun must
be identified, and resolution of pronoun
antecedents is another thorny problem that has
attracted much attention from researchers in
machine translation.
2.5  Juncture
Along with the liberties that participants in chat
interaction take with spelling and punctuation
conventions (see 3.1,2), they also deliberately
(and undoubtedly sometimes accidentally) omit
spaces between words, as in (3).
(3) a. selamunaleykum  (= selamun aleykum)
b. aleykumselam   (= aleykum selam)
The Turkish ?peace to you? greeting in (3a) is a
variant of (2a) and is usually represented as two
words, though the merged forms in (3a) and in
the conventional reply (3b) occur several times
in a sample of our corpus.  Consequently, one of
the basic challenges for speech recognition,
identification of word boundaries, is also a
problem in chat interaction.
2.6 Colloquial terms, idioms and slang
Wiebe et al (1995) use the term conventional
constructions to refer to idiosyncratic
collocations and tense/aspect usage. Colloquial
or idiomatic usage complicates translation of
both spoken and chat interaction, though it is
less frequent in formal writing.  (4) provides
some examples from chat.
(4) a. have a ball, y?all
b. do u sleep there n stuff
Like discourse markers, expressions such as
have a ball in (4a) and [and] stuff in (4b), have
both compositional and idiomatic meanings,
which causes ambiguity that must be resolved.
2.7 Code-switching
Code-switching is common in multilingual
speech communities, and the participants in
communication environments like chat tend to
be multilingual.  The Turkish to English switch
in (5a) illustrates.
(5) a. anlamami istedigin seyi anlamadim sorry
b. salam mon frere
In (5b) from the #paris chat channel, Arabic
salam ?peace? is used as a greeting.  Not only
are these switches problematic for translation
engines designed to map a single source
language into a single target language, but also
translation into a single language eliminates the
sociolinguistic and pragmatic effects of code-
switching.
2.8  Language play
Another consequence of the informal contexts in
which speech and chat interaction occur is the
playful use of language for entertainment, and in
online environments like chat, where fun often
is the primary attraction,  humor and language
play are valued (Danet et al, 1995).  In addition
to play with identity and typographic symbols,
which have become conventional in chat
interaction, novel games like (6) emerge.
(6) a. wew
b. wiw
c. wow
(6) is part of a sequence on a Cebuano language
channel in which the game seems to be to
produce consonant frames with different vowels.
It ocurred after another game in which
participants inserted a vocative use of baby (as
in hey baby) into almost every message, often
accompanied by additional codeswitching into
English, and finally prompting the protest, ?you
guys have been on that baby thing for ages.?
2.9 Segmentation of translation units
Just as spoken interaction does not include clear
delimiters for word boundaries, it also lacks
conventional means of marking larger units of
discourse that would be analogous to sentence
punctuation in written texts.  Similarly, though
chat interaction is written, punctuation is often
inconsistent or absent.  For example, vocative
nicknames used to address messages to specific
participants may not be separated from the
remainder of the message by any punctuation or
they may be separated by commas, colons,
ellipses, parentheses, brackets, and emoticons.
The same range of possibilities occurs for
punctuation between sentences, which is
frequently absent.   Consequently, it is difficult
to segment input into consistent units that
translation components can anticipate.
3 Analogous Challenges in Spoken
and Written Interaction
Another set of problems that arise in translation
of written interaction are not found in spoken
interaction because they involve the typographic
symbols that render language in written form.
However, most of the problems have analogies
in spoken interaction, just as lack of punctuation
in writing causes the same juncture and
segmentation problems encountered in speech.
Most of the challenges in Section 2 represent
problems for translation from the source
language to the target language, whereas the
challenges in this section primarily complicate
the problem of recognizing the source message.
3.1 Unintentional misspellings and
typographical errors
Nonstandard spellings occur so frequently in
chat interaction that it is difficult to find
examples that do not contain them, as (2b)
illustrates above.  Online interaction also
contains many deliberate misspellings that are
discussed in the next section.  In addition to
misspellings like (2b) and (7a), we classify as
typographic errors the many instances like (7b)
in which participants fail to punctuate
contractions (though these may be deliberate).
(7) a. hi evenybody
b. bon jai mon vrai nick crisse
?good, I have my true nickname crisse?
Unlike the English contraction I?ve, the French
contraction j?ai ?I have? is not optional:  it is
always spelled j?ai and neither je ai nor jai exist
in the French language.  This kind of
misspelling is analogous to mispronunciations
and speech errors like slips of the tongue in
speech, though clearly these anomalous forms
are much more frequent in chat interaction than
in speech.
Another type of problem is the failure to use
diacritic symbols associated with letters in some
orthographic systems.  For example, in French
the letter ?a? without an accent represents the 3rd
person singular present tense form of the verb
?have,? while the form ? is the preposition ?to.?
Both of these forms are pronounced the same,
but in other cases the diacritic signifies a change
in both pronunciation and meaning.  For
example, marche is the 3rd person singular
present tense form of the French verb marcher
?to work, go? and is pronounced like English
marsh with one syllable, but march? ?market? is
a noun pronounced with a second syllable [e].
Consequently, the failure to follow orthographic
conventions, creates homographs that present
the same identification and ambiguity problems
as homophones do in speech.
3.2 Creative spelling, rebus, and
abbreviations
Online interaction is famous for the creative and
playful conventions that have emerged for
frequently used expressions, and though most of
these originated in English, it is now possible to
observe French mdr (mort de rire ?dying of
laughter?) with English lol (laughing out loud)
or amha (? mon humble avis ?in my humble
opinion?) like English imho (in my humble
opinion) and even Portuguese vc (voce ?you?).
Like the nonstandard spellings that are
unintentional, these deliberate departures from
convention are so frequent that we have already
seen several instances of rebus forms in (2c) and
(4b) and the replacement of Romance qu by k in
(1). Other examples include English pls "please"
and ur ?your,? Turkish slm (selam ?peace?) and
the French forms in (8).
(8) a. ah wi snooppy ?   "ah yes, snooppy?"
b. et c pa toi     "and it is not you"
In (8a) oui ?yes? is spelled wi, which reflects the
pronunciation, and in (8b) the rebus form c
represents c?est ?it is,? both pronounced [se],
while pas is also spelled as pronounced, without
the silent s.  In the creative and rebus spellings,
the nonstandard forms typically reflect the
pronunciation of the word and the pronunciation
is often a reduced one, as in hiya ?hi you? and
cyah ?see you.? Consequently, these forms can
be viewed as analogous to the variation in
speech that is caused by use of reduced forms.
Alternatively, these representations might be
viewed as analogous to the out of vocabulary
words that plague current speech recognizers.
3.3 Register and dialect differences
Chat interaction is subject to the same kinds of
register and dialect variation that occurs in
speech.  For example (2a) employs a form of the
standard Turkish greeting esselamu aleyk?m that
is closer to the original Arabic because it uses
the Arabic definite article es-, though the umlaut
is not Arabic.  In contrast the variant in (3a),
selamun aleykum, employs the Turkish suffix on
selam, but omits the umlaut.  (8) illustrates other
variants that occurred in a sample of chat from
the #ankara channel.
(9) a. selamun aleyk?m
b. selam?n aleykum
c. Selammmmmmmmm
d. selamlar
e. selam all
Another example is the variable use of ne in
French constructions with negative polarity.
Though formal French uses ne before the verb
and pas after the verb for sentence negation,
most varieties omit the ne in everyday contexts,
as observed in (8b), where the absence of both
ne and the s on pas creates serious problems for
any translation engine that expects negation to
appear as ne and pas in French.  This variation
combines with a creative spelling based on
reduction to produce examples like (10).
(10)  shuis pa interess?   ?I am not interested?
The standard form of (10) is je ne suis pas
interess?, but in casual speech, ne is dropped,
the vowel in je is omitted and the two adjacent
fricatives merge to produce the sound that is
typically spelled sh in English (though it is
usually spelled ch in French).
3.4 Emotives and repeated letters
Two challenges for speech recognition are non-
lexical sounds such as laughter or grunts and the
distortions of pronunciation that are caused by
emphasis, fatigue, or emotions such as anger
and boredom.  These complications have
analogies in written interaction when
participants attempt to render the same sounds
orthographically, producing forms like those in
(9c) and (10).
(10) a. merhabaaaaaaaaaaaaaaaaaaaaaa
 b. ewww
c. eeeeeeeeeeeeeeeeeeeeeeeeeee
 d. hehehe
In (10a) the final syllable of the Turkish greeting
merhaba is lengthened in the same way that it
would be in an enthusiastic and expansive
greeting, and (10b) effectively communicates a
typical expression of disgust.  Laughter is
rendered in a variety of ways including ha ha,
heh heh, and (10d).  The variability of spellings
in these cases resembles the variability of
nonverbal sounds in speech.
3.5 Emoticons
Another way that chat participants express
emotion is by using emoticons and messages
that consist entirely of punctuation, as in (11).
(11) a. hey Pipes` >:) how u doing?
b. o)))*******
c. !!!!
 d. ????
Like the emotives and repeated letters described
in 3.4, these can be viewed as analogous to the
non-lexical sounds that occur in speech.
However, they are probably more easily
identified because they are drawn from a very
limited set of symbols.
4 Sharing solutions to shared
problems
Because machine translations of spoken and
written interaction share so many challenges, it
is likely that solutions to the problems might
also be shared in ways that will allow research
on the newer phenomenon of written interaction
to benefit from the years of experience with
spoken interaction.  Conversely, approaches to
written interaction, not biased by previous
efforts, can provide fresh perspectives on
familiar problems.  We present some examples
in which there appears to be strong potential for
this kind of mutual benefit, drawing on our
efforts to improve the performance of TrIM,
MITRE?s Translingual Instant Messenger
prototype.  TrIM is an instant messaging
environment in which participants are able to
interact by reading and typing in their own
preferred languages.  The system translates each
user?s messages into the language of the other
participants and displays both the source
language and target language versions.
TrIM?s translation services are provided by
the CyberTrans system, which provides a
common interface for various commercial text
translation systems and several types of text
documents (e.g. e-mail, web, FrameMaker).  It
incorporates text normalization tools that can
improve the quality of the input text and thus the
resultant translation.  Specifically, preprocessing
systems provide special handling for
punctuation and normalize spelling, such as
adding diacritics that have been omitted.
4.1 Spelling and recognition problems
Closer consideration of the problems created by
nonstandard spellings  reveals the strong
similarities between the complexity of speech
recognition and recognition of written words in
?noisy? communication environments such as
chat.  In both cases, there is a need to
discriminate between words that are not
recognized because they are not in the system
and words that are in the system, but are not
recognized for other reasons, such as variation
in phonetic form or a problem in the recognition
process.  Two properties of chat interaction
make this problem as serious for identifying
written words as it is for spoken input.  First,
though a much larger vocabulary can be
maintained in digital memory than in the models
of speech recognition systems, the creativity and
innovation that is valued in chat environments
provides a constant source of new vocabulary:
it is guaranteed that there will always be out of
vocabulary (OOV) words.  Second,  the high
frequency of intentional and unintentional
departures from standard spelling matches the
variability of speech and makes it essential that
the system be able to normalize spellings so that
messages are not obscured by large numbers of
unidentified words.
A variety of methods have been proposed in
the speech recognition literature for detecting
OOV words. Fetter (1998) reviews four
approaches to the problem and observes that
they can be classified in two broad groups:
explicit acoustic and language models of OOV
words ?compete against models of in-
vocabulary words during a word-based
search?Implicit models use information
derived from other recognition parameters to
compute the likelihood of OOV-words? (Fetter,
1998: 104).  In the latter group, he classifies
approaches that use confidence measures, online
garbage modeling in keyword spotting, and the
use of an additional phoneme recognizer
running in parallel to a word recognizer. These
approaches might be adapted to the problem of
discriminating misspelled and OOV words in
chat interaction, just as approaches to spelling
correction might provide alternative solutions to
the analogous problem in speech recognition.
For example, models of OOV words might
compete with models of in-vocabulary
recognition errors using Brill and Moore?s
(2000) error model for noisy channel spelling
correction that takes into account the
probabilities of errors occurring in specific
positions in the word.  By modeling recognition
errors, the model captures the stochastic
properties of both the language and the
individual recognition system.
Because our goal is not only recognizing,
but also translating messages, we are especially
interested in solutions that will facilitate the
translation system and process.  Consequently,
solutions based on modeling the contexts of
OOV word use and the contexts of nonstandard
spellings seem most promising.  For example, it
would be worth exploring whether the templates
used in example-based translation could be used
to model these contexts.
4.2 Preprocessing for special cases
Seligman (2000) observes that current spoken
language translation systems use very different
methods for recognizing phones, words, and
syntactic structures, and he envisions systems in
which these processes are integrated, proposing
alternatives that range from architectures which
support a common central data structure to
grammars whose terminal symbols are phones.
The latter approach appears to be too narrow
because it precludes the possibility of
employing preprocessing operations that
structure input to facilitate translation.
The success of TrIm and CyberTrans
suggests that preprocessing operations offer
useful approaches to the challenges we have
identified.  For example, a preprocessing system
in CyberTrans identifies words which are likely
to be missing diacritic symbols and inserts them
before the input is sent to the translation
engines.  As a result, the chat message in (12a)
is correctly translated as (12b) rather than (12c)
or (12d), which are the results from two systems
that did not benefit from preprocessing.
(12) a. et la ca va mieux
 b. and there that is better
c. and Ca is better
 d. and the ca goes better
The French form la is the feminine definite
article, whereas the form l? is the demonstrative
deictic ?there.?  CyberTrans recognized that the
form should be l? and that ca should be ?a.
Another example concerns the problems of
forms such as discourse markers, vocatives, and
greetings.  (13) shows that when the discourse
marker well is separated by a comma, as in
(13a), it is correctly translated as a discourse
marker in (13b), whereas without the comma in
(13c), the translation uses puits, the word that
would be used if referring to a water well.
(13) a. Well, I'm feeling great!
b. Bien, je me sens grand!
c. well aren't we happy today?
d. puits ne sommes-nous pas heureux
aujourd'hui?
Therefore, the comma served as a signal to the
system to translate the discourse marker
differently, and clearly, a preprocessing
operation that identifies and tags items like
discourse markers can facilitate their translation.
4.3 Segmentation of translation units
Though clause or sentence units are not clearly
marked in speech, the grammars on which
analysis and translation rely typically operate
with the clause or sentence as the primary unit
of analysis.  Consequently, the issue of
segmenting speech into sentence-like units has
received considerable attention in the speech
translation community, especially the possibility
that prosodic information can be used to identify
appropriate boundaries (Kompe, 1997; Shriberg
et al, 2000).  Other efforts identify lexical items
with high probabilities of occurring at sentence
boundaries and incorporate probabilistic models,
taking into account acoustic features (Lavie et
al. 1996) or part of speech information
(Zechner, 2001).
In contrast, some researchers have proposed
that identifying sentence boundaries is less
important than finding appropriate packaging
sizes for translation.  For example, Seligman
(2000) reports that pause units divided a corpus
into units that were smaller than sentence units,
but could be parsed and yielded understandable
translations when translated by hand.  Popowich
et al (2000) deliberately segment closed caption
input into small packages for translation,
claiming that it provides a basis for handling the
vocatives, false starts, tag questions, and other
non-canonical structures encountered in
captions.  They also claim that the procedure
reduces the scope of translation failures and
constrains the generation problem.  Since much
interaction is already fragmented, processing
that relies on smaller units rather than sentences
seems worth investigating.
A related possibility is that much of the
problematic input is already packaged in small
units of short turns or messages.  Yu et al
(2000) report that 54% of turns with a duration
of 0.7 seconds or less in their data consisted of
either yeah or mmhmm and about 70% of the
turns contained the same 40 words or phrasal
expressions.  They took advantage of these facts
by building a language model tailored
specifically for short turns.
In a pilot study, we examined a small
sample of chat data in order to determine
whether short messages were more likely to
contain the problematic features described in
sections 2 and 3.  Of 76 chat messages, 46 or
61% were 3 units or less, where units were
delineated by space and punctuation (without
separation of contractions) and emoticons
counted as a unit.  The frequency of items such
as greetings, vocatives and acronyms was
counted for each message size.  Of 8 greetings
in the corpus, 7 occurred in messages of 3 or
fewer words, and all 9 greeting-plus-vocative
structures occurred in messages of 3 or fewer
words.  Messages with 3 or fewer words also
contained 11 of the 14 emotives in the corpus
(including emoticons), 8 of the 10 messages
with repeated letters, 3 of the 3 acronyms, 2 of
the 3 discourse markers, and both of the
interjections in the corpus.  These results
support the claim that much of the problematic
usage in chat interaction is limited to short turns
that can be identified and processed separately.
5 Conclusions
This paper demonstrates that many of the
problems which complicate translation of
spoken interaction are shared by written
interaction such as chat and instant messaging.
It is proposed that solutions developed to solve
similar problems in the two communication
environments can be profitably shared, and
several examples are presented where mutually
beneficial approaches might be developed.
Specifically, we noted that the problem of
discriminating unrecognized OOV words  from
in-vocabularly words in spoken interaction is
analogous to the problem of discriminating
unrecognized OOV words from misspelled
words in written interaction.  We suggested that
some of the same methods used in spelling
correction might be adapted to speech
recognition, especially language models that
incorporate probabilities of errors in specific
positions in the word.  We also observed the
potential of preprocessing operations that
structure input for translation systems to allow
special treatment of problematic language,
including the possibility that much complexity
can be avoided by processing and translating
smaller units separately.  We look forward to
exploring these possibilities in future work.
References
Brill, Eric and Moore, Robert C.  2000. An improved
error model for noisy channel spelling
correction.  Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics.
Chafe, Wallace. 1982. Integration and involvement in
speaking, writing, and oral literature.  In Spoken
and Written Language:  Exploring Orality and
Literacy, Deborah Tannen (Ed.), Norwoord, NJ:
Ablex, pp. 35-53.
Condon, Sherri and Cech, Claude.  (Forthcoming)
Discourse management in three modalities.  In
Computer-Mediated Conversation, Susan
Herring (Ed.), Hampton Press.
Danet, Brenda, Ruedenberg-Wright, Lucia, and
Rosenbaum-Tamari, Yehudit. 1995. Hmmm...
Where's that smoke coming from?" Writing,
Play and Performance on Internet Relay. Journal
of Computer-Mediated Communication, 1 (2).
Ferrara, Kathleen, Brunner, Hans, and Whittemore,
Greg.  1991.  Interactive written discourse as an
emergent register.  Written Communication, 8
(1), 8-34.
Fetter, Pablo. 1998.  Detection and transcription of
OOV words.  Verbmobil Technical Report 231.
Flanagan, Mary.  1996.  Two years online:
Experiences, challenges and trends.  Expanding
MT Horizons:  Proceedings of the Second
Conference of the Association for Machine
translation in the Americas, 2-5 October,  pp.
192-197.
Kompe, Ralf. 1997. Prosody in Speech
Understanding Systems.  Berlin:  Springer.
Lavie, Alon, Gates, Donna, Coccaro, Noah and
Levin, Lori.  1996. Input segmentation of
spontaneous speech in Janus: A speech-to-
speech translation system. Proceedings of the
ECAI 96, Budapest, Hungary.
Papineni, K., Roukos, S., Ward, T., Henderson, J.,
and Reeder, Florence. 2002. Corpus-Based
comprehensive and diagnostic MT evaluation :
Initial Arabic, Chinese, French, and Spanish
results. Proceedings of the Human Language
Technology Conference.  San Diego, California.
Popowich, Fred, McFetridge, Paul, Turcato, Davide,
and Toole, Janine. 2000. Machine translation of
closed captions.  Machine Translation, 15, 311-
341.
Shriberg, Elizabeth, Stolcke, Andreas, Hakkani-Tur,
Dilek, and Tur, Gokhan. 2000.  Prosody-based
automatic segmentation of speech into sentences
and topics.  Speech Communication 32(1-2).
Seligman, Mark.  2000.  Nine issues in speech
translation.  Machine Translation, 15, 149-185.
Sotillo, Susana M. 2000. Discourse functions and
syntactic complexity in synchronous and
asynchronous communication. Language
Learning & Technology, 4 (1), pp. 82-119.
Vanni, Michelle. and Miller, Keith.  2002. Scaling
the ISLE framework: Use of existing corpus
resources for validation of MT evaluation
metrics across languages. In Proceedings of
LREC 2002.  Las Plamas, Canary Islands, Spain.
Wiebe, Janice, Farwell, David, Villa, Daniel, Chen,
J-L, Sinclaire, R., Sandgren, Thorsten, Stein, G.,
Zarazua, David, and Ohara, Tom. 1995.
ARTWORK:  Discourse Processing in Machine
Translation of Dialog.   Final Report Year 1.
New Mexico State University:  Computing
Research Lab.  http://crl.NMSU.Edu/Research/
Projects/artwork/index.html
Yates, Simeon. 1996.  Oral and written linguistic
aspects of computer conferencing:  A corpus
based study. In Computer-Mediated
Communication:  Linguistic, Social and Cross-
Cultural Perspectives., Susan Herring (Ed.),
Philadelphia:  John Benjamins, pp. 29-46.
Yu, Hua, Tomokiyo, Takashi, Wang, Zhirong, and
Waibel, Alex.  2000.  New developments in
automatic meeting transcription.  International
Conference on Speech and Language Processing,
Beijing, China.
Zechner, Klaus. 2001. Automatic Generation of
Concise Summaries of Spoken Dialogues in
Unrestricted Domains. Proceedings of the 24th
ACM-SIGIR International Conference on
Research and Development in Information
Retrieval, New Orleans, Louisiana.
Interlingual Annotation of Multilingual Text Corpora 
 
Stephen Helmreich 
David Farwell 
Computing Research Laboratory 
New Mexico State University 
david@crl.nmsu.edu
shelmrei@crl.nmsu.edu
Florence Reeder 
Keith Miller 
Information Discovery & Understanding 
MITRE Corporation 
freeder@mitre.org
keith@mitre.org   
Bonnie Dorr 
Nizar Habash 
Institute for Advanced Computer Studies 
University of Maryland 
bonnie@umiacs.umd.edu
habash@umiacs.umd.edu
 
Eduard Hovy 
Information Sciences Institute 
University of Southern California 
hovy@isi.edu 
Lori Levin 
Teruko Mitamura 
Language Technologies Institute 
Carnegie Mellon University 
lsl@cs.cmu.edu
teruko@cs.cmu.edu 
Owen Rambow 
Advaith Siddharthan 
Department of Computer Science 
Columbia University 
rambow@cs.columbia.edu
as372@cs.columbia.edu  
 
 
Abstract 
This paper describes a multi-site project to 
annotate six sizable bilingual parallel corpora 
for interlingual content. After presenting the 
background and objectives of the effort, we 
describe the data set that is being annotated, 
the interlingua representation language used, 
an interface environment that supports the an-
notation task and the annotation process itself. 
We will then present a preliminary version of 
our evaluation methodology and conclude 
with a summary of the current status of the 
project along with a number of issues which 
have arisen.  
1 Introduction 
This paper describes a multi-site National Science 
Foundation project focusing on the annotation of six 
sizable bilingual parallel corpora for interlingual content 
with the goal of providing a significant data set for im-
proving knowledge-based approaches to machine trans-
lation (MT) and a range of other Natural Language 
Processing (NLP) applications. The project participants 
include the Computing Research Laboratory at NMSU, 
the Language Technologies Institute at CMU, the In-
formation Science Institute at USC, UMIACS at the 
University of Maryland, the MITRE Corporation and 
Columbia University. In the remainder of the paper, we 
first present the background and objectives of the pro-
ject. We then describe the data set that is being anno-
tated, the interlingual representation language being 
used, an interface environment that is designed to sup-
port the annotation task, and the process of annotation 
itself. We will then outline a preliminary version of our 
evaluation methodology and conclude with a summary 
of the current status of the project along with a set of 
issues that have arisen since the project began.  
2 Project Goals and Expected Outcomes 
The central goals of the project are: 
? to produce a practical, commonly-shared system 
for representing the information conveyed by a 
text, or ?interlingua?, 
? to develop a methodology for accurately and 
consistently assigning such representations to 
texts across languages and across annotators, 
? to annotate a sizable multilingual parallel corpus 
of source language texts and translations for IL 
content. 
This corpus is expected to serve as a basis for improving 
meaning-based approaches to MT and a range of other 
natural language technologies.  The tools and annotation 
standards will serve to facilitate more rapid annotation 
of texts in the future. 
3 Corpus 
The target data set is modeled on and an extension of 
the DARPA MT Evaluation data set (White and 
O?Connell 1994) and includes data from the Linguistic 
Data Consortium (LDC) Multiple Translation Arabic, 
Part 1 (Walker et al, 2003). The data set consists of 6 
bilingual parallel corpora. Each corpus is made up of 
125 source language news articles along with three 
translations into English, each produced independently 
by different human translators. However, the source 
news articles for each individual language corpus are 
different from the source articles in the other language 
corpora.  Thus, the 6 corpora themselves are comparable 
to each other rather than parallel. The source languages 
are Japanese, Korean, Hindi, Arabic, French and Span-
ish.  Typically, each article is between 300 and 400 
words long (or the equivalent) and thus each corpus has 
between 150,00 and 200,000 words. Consequently, the 
size of the entire data set is around 1,000,000 words. 
Thus, for any given corpus, the annotation effort is 
to assign interlingual content to a set of 4 parallel texts, 
3 of which are in the same language, English, and all of 
which theoretically communicate the same information. 
The following is an example set from the Spanish cor-
pus: 
S: Atribuy? esto en gran parte a
una pol?tica que durante muchos a?os
tuvo un "sesgo concentrador" y repre-
sent? desventajas para las clases me-
nos favorecidas.
T1: He attributed this in great
part to a type of politics that
throughout many years possessed a
"concentrated bias" and represented
disadvantages for the less favored
classes.
T2: To a large extent, he attrib-
uted that fact to a policy which had
for many years had a "bias toward
concentration" and represented disad-
vantages for the less favored
classes.
T3: He attributed this in great
part to a policy that had a "centrist
slant" for many years and represented
disadvantages for the less-favored
classes.
 
The annotation process involves identifying the 
variations between the translations and then assessing 
whether these differences are significant. In this case, 
the translations are, for the most part, the same although 
there are a few interesting variations.  
For instance, where this appears as the translation 
of esto in the first and third translations, that fact 
appears in the second. The translator choice potentially 
represents an elaboration of the semantic content of the 
source expression and the question arises as to whether 
the annotation of the variation in expressions should be 
different or the same.  
More striking perhaps is the variation between 
concentrated bias, bias toward concen-
tration and centrist slant as the translation 
for sesgo concentrador. Here, the third transla-
tion offers a clear interpretation of the source text au-
thor?s intent. The first two attempt to carry over the 
vagueness of the source expression assuming that the 
target text reader will be able to figure it out. But even 
here, the two translators appear to differ as to what the 
source language text author?s intent actually was, the 
former referring to bias of a certain degree of strength 
and the second to a bias  in a certain direction. Seem-
ingly, then, the annotation of each of these expressions 
should differ. 
Furthermore, each source language has different 
methods of encoding meaning linguistically. The resul-
tant differing types of translation mismatch with English 
should provide insight into the appropriate structure and 
content for an interlingual representation. 
The point is that a multilingual parallel data set of 
source language texts and English translations offers a 
unique perspective and unique problem for annotating 
texts for meaning. 
4 Interlingua 
Due to the complexity of an interlingual annotation as 
indicated by the differences described in the previous 
section, the representation has developed through three 
levels and incorporates knowledge from sources such as 
the Omega ontology and theta grids.  Since this is an 
evolving standard, the three levels will be presented in 
order as building on one another. Then the additional 
data components will be described.  
4.1 Three Levels of Representation 
We now describe three levels of representation, referred 
to as IL0, IL1 and IL2. The aim is to perform the annota-
tion process incrementally, with each level of represen-
tation incorporating additional semantic features and 
removing existing syntactic ones. IL2 is intended as the 
interlingua, that abstracts away from (most) syntactic 
idiosyncrasies of the source language. IL0 and IL1 are 
intermediate representations that are useful starting 
points for annotating at the next level. 
4.1.1 IL0 
IL0 is a deep syntactic dependency representation. It 
includes part-of-speech tags for words and a parse tree 
that makes explicit the syntactic predicate-argument 
structure of verbs. The parse tree is labeled with syntac-
tic categories such as Subject or Object , which refer to 
deep-syntactic grammatical function (normalized for 
voice alternations).  IL0 does not contain function words 
(determiners, auxiliaries, and the like): their contribu-
tion is represented as features.  Furthermore, semanti-
cally void punctuation has been removed.  While this 
representation is purely syntactic, many disambiguation 
decisions, relative clause and PP attachment for exam-
ple, have been made, and the presentation abstracts as 
much as possible from surface-syntactic phenomena.  
Thus, our IL0 is intermediate between the analytical and 
tectogrammatical levels of the Prague School (Haji? et 
al 2001). IL0 is constructed by hand-correcting the out-
put of a dependency parser (details in section 6) and is a 
useful starting point for semantic annotation at  IL1, 
since it allows annotators to see how textual units relate 
syntactically when making semantic judgments.  
4.1.2 IL1 
IL1 is an intermediate semantic representation. It asso-
ciates semantic concepts with lexical units like nouns, 
adjectives,  adverbs and verbs (details of the ontology in 
section 4.2). It also replaces the syntactic relations in 
IL0, like subject and object, with thematic roles, like 
agent, theme and goal (details in section 4.3). Thus, like 
PropBank (Kingsbury et al2002), IL1 neutralizes dif-
ferent alternations for argument realization.  However, 
IL1 is not an interlingua; it does not normalize over all 
linguistic realizations of the same semantics. In particu-
lar, it does not address how the meanings of individual 
lexical units combine to form the meaning of a phrase or 
clause. It also does not address idioms, metaphors and 
other non-literal uses of language.  Further, IL1 does not 
assign semantic features to prepositions; these continue 
to be encoded as syntactic heads of their phrases, al-
though these might have been annotated with thematic 
roles such as location or time. 
4.1.3 IL2 
IL2 is intended to be an interlingua, a representation of 
meaning that is reasonably independent of language. IL2 
is intended to capture similarities in meaning across 
languages and across different lexical/syntactic realiza-
tions within a language. For example, IL2 is expected to 
normalize over conversives (e.g. X bought a book from 
Y vs. Y sold a book to X)  (as does FrameNet (Baker et 
al 1998)) and non-literal language usage (e.g. X started 
its business vs. X opened its doors to customers).  The 
exact definition of IL2 will be the major research con-
tribution of this project. 
4.2 The Omega Ontology 
In progressing from IL0 to IL1, annotators have to se-
lect semantic terms (concepts) to represent the nouns, 
verbs, adjectives, and adverbs present in each sentence.  
These terms are represented in the 110,000-node ontol-
ogy Omega (Philpot et al, 2003), under construction at 
ISI.  Omega has been built semi-automatically from a 
variety of sources, including Princeton's WordNet (Fell-
baum, 1998), NMSU?s Mikrokosmos (Mahesh and Ni-
renburg, 1995), ISI's Upper Model (Bateman et al, 
1989) and ISI's SENSUS (Knight and Luk, 1994).  After 
the uppermost region of Omega was created by hand, 
these various resources? contents were incorporated and, 
to some extent, reconciled.  After that, several million 
instances of people, locations, and other facts were 
added (Fleischman et al, 2003).  The ontology, which 
has been used in several projects in recent years (Hovy 
et al, 2001), can be browsed using the DINO browser at 
http://blombos.isi.edu:8000/dino; this browser forms a 
part of the annotation environment.  Omega remains 
under continued development and extension.  
4.3 The Theta Grids 
Each verb in Omega is assigned one or more theta grids 
specifying the arguments associated with a verb and 
their theta roles (or thematic role).  Theta roles are ab-
stractions of deep semantic relations that generalize 
over verb classes.  They are by far the most common 
approach in the field to represent predicate-argument 
structure.  However, there are numerous variations with 
little agreement even on terminology (Fillmore, 1968; 
Stowell, 1981; Jackendoff, 1972; Levin and Rappaport-
Hovav, 1998). 
The theta grids used in our project were extracted 
from the Lexical Conceptual Structure Verb Database 
(LVD) (Dorr, 2001).  The WordNet senses assigned to 
each entry in the LVD were then used to link the theta 
grids to the verbs in the Omega ontology.  In addition to 
the theta roles, the theta grids specify the mapping be-
tween theta roles and their syntactic realization in argu-
ments, such as Subject, Object or Prepositional Phrase, 
and the Obligatory/Optional nature of the argument, 
thus facilitating IL1 annotation.  For example, one of the 
theta grids for the verb ?load? is listed in Table 1 (at the 
end of the paper). 
Although based on research in LCS-based MT 
(Dorr, 1993; Habash et al 2002), the set of theta roles 
used has been simplified for this project.  This list (see 
Table 2 at the end of the paper), was used in the Inter-
lingua Annotation Experiment 2002 (Habash and 
Dorr).1  
4.4 Incremental Annotation 
As described earlier, the development and annota-
tion of the interlingual notation is incremental in nature.  
This necessitates constraining the types and categories 
of attributes included in the annotation during the be-
ginning phases.  Other topics not addressed here, but 
considered for future work include time, aspect, loca-
tion, modality, type of reference, types of speech act, 
causality, etc.  
Thus, IL2 itself is not a final interlingual representa-
tion, but one step along the way. IL0 and IL1 are also 
intermediate representations, and as such are an occa-
sionally awkward mixture of syntactic and semantic 
information. The decisions as to what to annotate, what 
to normalize, what to represent as features at each level 
are semantically and syntactically principled, but also 
governed by expectations about reasonable annotator 
tasks. What is important is that at each stage of trans-
formation, no information is lost, and the original lan-
guage recoverable in principle from the representation. 
5 Annotation Tool 
We have assembled a suite of tools to be used in the 
annotation process.  Some of these tools are previously 
existing resources that were gathered for use in the pro-
ject, and others have been developed specifically with 
the annotation goals of this project in mind.  Since we 
are gathering our corpora from disparate sources, we 
need to standardize the text before presenting it to 
automated procedures.  For English, this involves sen-
tence boundary detection, but for other languages, it 
may involve segmentation, chunking of text, or other 
?text ecology? operations.  The text is then processed 
with a dependency parser, the output of which is viewed 
and corrected in TrED (Haji?, et al, 2001), a graphi-
cally-based tree editing program, written in Perl/Tk2.  
The revised deep dependency structure produced by this 
process is the IL0 representation for that sentence. 
In order to derive IL1 from the IL0 representation, 
annotators use Tiamat, a tool developed specifically for 
                                                           
1 Other contributors to this list are Dan Gildea and Karin 
Kipper Schuler. 
2 http://quest.ms.mff.cuni.cz/pdt/Tools/Tree_Editors/Tre
d/ 
this project.  This tool enables viewing of the IL0 tree 
with easy reference to all of the IL resources described 
in section 4 (the current IL representation, the ontology, 
and the theta grids).  This tool provides the ability to 
annotate text via simple point-and-click selections of 
words, concepts, and theta-roles.  The IL0 is displayed 
in the top left pane, ontological concepts and their asso-
ciated theta grids, if applicable, are located in the top 
right, and the sentence itself is located in the bottom 
right pane.  An annotator may select a lexical item (leaf 
node) to be annotated in the sentence view; this word is 
highlighted, and the relevant portion of the Omega on-
tology is displayed in the pane on the left.  In addition, 
if this word has dependents, they are automatically un-
derlined in red in the sentence view.  Annotators can 
view all information pertinent to the process of deciding 
on appropriate ontological concepts in this view.  Fol-
lowing the procedures described in section 6, selection 
of concepts, theta grids and roles appropriate to that 
lexical item can then be made in the appropriate panes. 
Evaluation of the annotators? output would be daunt-
ing based solely on a visual inspection of the annotated 
IL1 files.  Thus, a tool was also developed to compare 
the output and to generate the evaluation measures that 
are described in section 7.  The reports generated by the 
evaluation tool allow the researchers to look at both 
gross-level phenomena, such as inter-annotator agree-
ment, and at more detailed points of interest, such as 
lexical items on which agreement was particularly low, 
possibly indicating gaps or other inconsistencies in the 
ontology being used. 
6 Annotation Task 
To describe the annotation task, we first present the 
annotation process and tools used with it as well as the 
annotation manuals.  Finally, setup issues relating to 
negotiating multi-site annotations are discussed. 
6.1 Annotation process 
The annotation process was identical for each text. For 
the initial testing period, only English texts were anno-
tated, and the process described here is for English text. 
The process for non-English texts will be, mutatis mu-
tandis, the same. 
Each sentence of the text is parsed into a depend-
ency tree structure. For English texts, these trees were 
first provided by the Connexor parser at UMIACS 
(Tapanainen and Jarvinen, 1997), and then corrected by 
one of the team PIs. For the initial testing period, anno-
tators were not permitted to alter these structures. Al-
ready at this stage, some of the lexical items are 
replaced by features (e.g., tense), morphological forms 
are replaced by features on the citation form, and certain 
constructions are regularized (e.g., passive) and empty 
arguments inserted.  It is this dependency structure that 
is loaded into the annotation tool and which each anno-
tator then marks up. 
The annotator was instructed to annotate all nouns, 
verbs, adjectives, and adverbs. This involves annotating 
each word twice ? once with a concept from Wordnet 
SYNSET and once with a Mikrokosmos concept; these 
two units of information are merged, or at least inter-
twined in Omega. One of the goals and results of this 
annotation process will be a simultaneous coding of 
concepts in both ontologies, facilitating a closer union 
between them.  
In addition, users were instructed to provide a se-
mantic case role for each dependent of a verb. In many 
cases this was ?NONE? since adverbs and conjunctions 
were dependents of verbs in the dependency tree. LCS 
verbs were identified with Wordnet classes and the LCS 
case frames supplied where possible. The user, how-
ever, was often required to determine the set of roles or 
alter them to suit the text. In both cases, the revised or 
new set of case roles was noted and sent to a guru for 
evaluation and possible permanent inclusion. Thus the 
set of event concepts in the ontology supplied with roles 
will grow through the course of the project. 
6.2 The annotation manuals 
Markup instructions are contained in three manuals: a 
users guide for Tiamat (including procedural instruc-
tions), a definitional guide to semantic roles, and a 
manual for creating a dependency structure (IL0). To-
gether these manuals allow the annotator to (1) under-
stand the intention behind aspects of the dependency 
structure; (2) how to use Tiamat to mark up texts; and 
(3) how to determine appropriate semantic roles and 
ontological concepts. In choosing a set of appropriate 
ontological concepts, annotators were encouraged to 
look at the name of the concept and its definition, the 
name and definition of the parent node, example sen-
tences, lexical synonyms attached to the same node, and 
sub- and super-classes of the node. All these manuals 
are available on the IAMTC website3. 
6.3 The multi-site set up 
For the initial testing phase of the project, all annotators 
at all sites worked on the same texts. Two texts were 
provided by each site as were two translations of the 
same source language (non-English) text. To test for the 
effects of coding two texts that are semantically close, 
since they are both translations of the same source 
document, the order in which the texts were annotated 
differed from site to site, with half the sites marking one 
translation first, and the other half of the sites marking 
the second translation first. Another variant tested was 
                                                           
3 http://sparky.umiacs.umd.edu:8000/IAMTC/annotation
_manual.wiki?cmd=get&anchor=Annotation+Manual 
to interleave the two translations, so that two similar 
sentences were coded consecutively. 
During the later production phase, a more complex 
schedule will be followed, making sure that many texts 
are annotated by two annotators, often from different 
sites, and that regularly all annotators will mark the 
same text. This will help ensure continued inter-coder 
reliability. 
In the period leading up to the initial test phase, 
weekly conversations were held at each site by the an-
notators, going over the texts coded. This was followed 
by a weekly conference call among all the annotators. 
During the test phase, no discussion was permitted. 
One of the issues that arose in discussion was how 
certain constructions should be displayed and whether 
each word should have a separate node or whether cer-
tain words should be combined into a single node. In 
view of the fact that the goal was not to tag individual 
words, but entities and relations, in many cases words 
were combined into single nodes to facilitate this proc-
ess. For instance, verb-particle constructions were com-
bined into a single node. In a sentence like ?He threw it 
up?, ?throw? and ?up? were combined into a single 
node ?throw up? since one action is described by the 
combined words. Similarly, proper nouns, compound 
nouns and copular constructions required specialized 
handling.    In addition, issues arose about whether an-
notators should change dependency trees; and in in-
structing the annotators on how best to determine an 
appropriate ontology node.    
7 Evaluation 
The evaluation criteria and metrics continue to evolve 
and are in the early stages of formation and implementa-
tion.  Several possible courses for evaluating the annota-
tions and resulting structures exist.  In the first of these, 
the annotations are measured according to inter-
annotator agreement.  For this purpose, data is collected 
reflecting the annotations selected, the Omega nodes 
selected and the theta roles assigned.  Then, inter-coder 
agreement is measured by a straightforward match, with 
agreement calculated by a Kappa measure (Carletta, 
1993) and a Wood standard similarity (Habash and 
Dorr, 2002).  This is done for three agreement points:  
annotations, Omega selection and theta role selection.  
At this time, the Kappa statistic?s expected agreement is 
defined as 1/(N+1) where N is the number of choices at 
a given data point.  In the case of Omega nodes, this 
means the number of matched Omega nodes (by string 
match) plus one for the possibility of the annotator trav-
ersing up or down the hierarchy. Multiple measures are 
used because it is important to have a mechanism for 
evaluating inter-coder consistency in the use of the IL 
representation language which does not depend on the 
assumption that there is a single correct annotation of a 
given text.  The tools for evaluation have been modified 
from pervious use (Habash and Dorr, 2002). 
Second, the accuracy of the annotation is measured.  
Here accuracy is defined as correspondence to a prede-
fined baseline.  In the initial development phase, all 
sites annotated the same texts and many of the varia-
tions were discussed at that time, permitting the devel-
opment of a baseline annotation.  While not a useful 
long-term strategy, this produced a consensus baseline 
for the purpose of measuring the annotators? task and 
the solidity of the annotation standard.  
The final measurement technique derives from the 
ultimate goal of using the IL representation for MT, 
therefore, we are measuring the ability to generate accu-
rate surface texts from the IL representation as anno-
tated.  At this stage, we are using an available generator, 
Halogen (Knight and Langkilde, 2000).  A tool to con-
vert the representation to meet Halogen requirements is 
being built.  Following the conversion, surface forms 
will be generated and then compared with the originals 
through a variety of standard MT metrics (ISLE, 2003).   
8 Accomplishments and Issues 
In a short amount of time, we have identified languages 
and collected corpora with translations.  We have se-
lected representation elements, from parser outputs to 
ontologies, and have developed an understanding of 
how their component elements fit together.  A core 
markup vocabulary (e.g., entity-types, event-types and 
participant relations) was selected.  An initial version of 
the annotator?s toolkit (Tiamat) has been developed and 
has gone through alpha testing.  The multi-layered ap-
proach to annotation  decided upon reduces the burden 
on the annotators for any given text as annotations build 
upon one another.  In addition to developing individual 
tools, an infrastructure exists for carrying out a multi-
site annotation project.   
In the coming months we will be fleshing out the 
current procedures for evaluating the accuracy of an 
annotation and measuring inter-coder consistency.  
From this, a multi-site evaluation will be produced and  
results reported.  Regression testing, from the interme-
diate stages and representations will be able to be car-
ried out.  Finally, a growing corpus of annotated texts 
will become available.   
In addition to the issues discussed throughout the 
paper, a few others have not yet been identified.  From a 
content standpoint, looking at IL systems for time and 
location should utilize work in personal name, temporal 
and spatial annotation (e.g., Ferro et al, 2001).  Also, an 
ideal IL representation would also account for causality, 
co-reference, aspectual content, modality, speech acts, 
etc.  At the same time, while incorporating these items, 
vagueness and redundancy must be eliminated from the 
annotation language.  Many inter-event relations would 
need to be captured such as entity reference, time refer-
ence, place reference, causal relationships, associative 
relationships, etc.  Finally, to incorporate these, cross-
sentence phenomena remain a challenge.     
From an MT perspective, issues include evaluating 
the consistency in the use of an annotation language 
given that any source text can result in multiple, differ-
ent, legitimate translations (see Farwell and Helmreich, 
2003) for discussion of evaluation in this light.  Along 
these lines, there is the problem of annotating texts for 
translation without including in the annotations infer-
ences from the source text.   
9 Conclusions  
This is a radically different annotation project from 
those that have focused on morphology, syntax or even 
certain types of semantic content (e.g., for word sense 
disambiguation competitions). It is most similar to 
PropBank (Kingsbury et al2002) and FrameNet (Baker 
et al1998).  However, it is novel in its emphasis on:  (1) 
a more abstract level of mark-up (interpretation); (2) the 
assignment of a well-defined meaning representation to 
concrete texts; and (3) issues of a community-wide con-
sistent and accurate annotation of meaning. 
By providing an essential, and heretofore non-
existent, data set for training and evaluating natural lan-
guage processing systems, the resultant annotated multi-
lingual corpus of translations is expected to lead to 
significant research and development opportunities for 
Machine Translation and a host of other Natural Lan-
guage Processing technologies including Question-
Answering and Information Extraction.  
References 
Baker, C., J. Fillmore and J B. Lowe, 1998.  The Berke-
ley FrameNet Project.  Proceedings of ACL. 
Bateman, J.A., R. Kasper, J. Moore, and R. Whitney. 
1989. A General Organization of Knowledge for 
Natural Language Processing: The Penman Upper 
Model. Unpublished research report, USC / Informa-
tion Sciences Institute, Marina del Rey, CA.  
Carletta, J. C. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Lin-
guistics, 22(2), 249-254 
Conceptual Structures and Documentation, UMCP. 
http://www.umiacs.umd.edu/~bonnie/LCS_Database
_Documentation.html  
Dorr, B. J. 2001.  LCS Verb Database, Online Software 
Database of Lexical  
Dorr, B. J., 1993. Machine Translation: A View from the 
Lexicon, MIT Press, Cambridge, MA. 
Farwell, D., and S. Helmreich.  2003.  Pragmatics-based 
Translation and MT Evaluation.  In Proceedings of 
Towards Systematizing MT Evaluation.  MT-Summit 
Workshop, New Orleans, LA. 
Fellbaum, C. (ed.). 1998. WordNet: An On-line Lexical 
Database and Some of its Applications. MIT Press, 
Cambridge, MA. 
Ferro, L., I. Mani, B. Sundheim and G. Wilson.  2001. 
TIDES Temporal Annotation Guidelines. Version 
1.0.2 MITRE Technical Report, MTR 01W0000041 
Fillmore, C..  1968. The Case for Case. In E. Bach and 
R. Harms, editors, Universals in Linguistic Theory, 
pages 1--88. Holt, Rinehart, and Winston.  
Fleischman, M., A. Echihabi, and E.H. Hovy. 2003. 
Offline Strategies for Online Question Answering: 
Answering Questions Before They Are Asked.  Pro-
ceedings of the ACL Conference. Sapporo, Japan. 
Habash, N. and B. Dorr. 2002. Interlingua Annotation 
Experiment Results. AMTA-2002 Interlingua Reli-
ability Workshop. Tiburon, California, USA. 
Habash, N., B. J. Dorr, and D. Traum, 2002. "Efficient 
Language Independent Generation from Lexical 
Conceptual Structures," Machine Translation, 17:4. 
Haji?, J.; B. Vidov?-Hladk?; P. Pajas.  2001: The Pra-
gue Dependency Treebank: Annotation Structure and 
Support. In Proceeding of the IRCS Workshop on 
Linguistic Databases, pp. . University of Pennsyl-
vania, Philadelphia, USA, pp. 105-114. 
Hovy, E., A. Philpot, J. Ambite, Y. Arens, J. Klavans, 
W. Bourne, and D. Saroz.  2001. Data Acquisition 
and Integration in the DGRC's Energy Data Collec-
tion Project, in Proceedings of the NSF's dg.o 2001. 
Los Angeles, CA. 
ISLE 2003.  Framework for Evaluation of Machine 
Translation in ISLE.  
http://www.issco.unige.ch/projects/isle/femti/ 
Jackendoff, R. 1972. Grammatical Relations and Func-
tional Structure. Semantic Interpretation in Genera-
tive Grammar. The MIT Press, Cambridge, MA. 
Kingsbury, P and M Palmer and M Marcus , 2002.  
Adding Semantic Annotation to the Penn TreeBank. 
Proceedings of the Human Language Technology 
Conference (HLT 2002).  
Knight, K., and I. Langkilde. 2000.  Preserving Ambi-
guities in Generation via Automata Intersection. 
American Association for Artificial Intelligence con-
ference (AAAI). 
Knight, K, and S. K. Luk.  1994. Building a Large-Scale 
Knowledge Base for Machine Translation.  Proceed-
ings of AAAI. Seattle, WA. 
Levin, B. and M. Rappaport-Hovav. 1998. From Lexical 
Semantics to Argument Realization. Borer, H. (ed.) 
Handbook of Morphosyntax and Argument Structure. 
Dordrecht: Kluwer Academic Publishers. 
Mahesh, K., and Nirenberg, S.  1995. A Situated Ontol-
ogy for Practical NLP, in Proceedings on the Work-
shop on Basic Ontological Issues in Knowledge 
Sharing at IJCAI-95. Montreal, Canada. 
Philpot, A., M. Fleischman, E.H. Hovy. 2003. Semi-
Automatic Construction of a General Purpose Ontol-
ogy.  Proceedings of the International Lisp Confer-
ence.  New York, NY. Invited. 
Stowell, T. 1981. Origins of Phrase Structure. PhD the-
sis, MIT, Cambridge, MA.  
Tapanainen, P. and T Jarvinen.  1997.  A non-projective 
dependency parser.  In the 5th Conference on Applied 
Natural Language Processing / Association for Com-
putational Linguistics, Washington, DC. 
White, J., and T. O?Connell.  1994.  The ARPA MT 
evaluation methodologies: evolution, lessons, and fu-
ture approaches.  Proceedings of the 1994 Confer-
ence, Association for Machine Translation in the 
Americas 
Walker, K., M. Bamba, D. Miller, X. Ma, C. Cieri, and 
G. Doddington 2003.  Multiple-Translation Arabic 
Corpus, Part 1. Linguistic Data Consortium (LDC) 
catalog num. LDC2003T18 & ISBN 1-58563-276-7. 
 
 
Role Description Grid Syntax Type 
Agent The entity that does the action Agent:  load 
Theme  with possessed 
SUBJ OBLIGATORY 
Theme The entity that is worked on Agent:  load 
Theme with possessed 
OBJ OBLIGATORY 
Possessed The entity controlled or owned Agent:  load 
Theme  with possessed 
PP OPTIONAL 
Table 1 :  A theta grid for the verb "load" 
 
 
Role and Definition Examples 
Agent:  Agents have the features of volition, sentience, causation and 
independent exist 
? Henry pushed/broke the vase. 
Instrument: An instrument should have causation but no volition. Its 
sentience and existence are not relevant. 
? The Hammer broke the vase. 
? She hit him with a baseball bat 
Experiencer: An experiencer has no causation but is sentient and 
exists independently. Typically an experiencer is the subject of verbs 
like feel, hear, see, sense, smell, notice, detect, etc. 
? John heard the vase shatter.   
? John shivered. 
Theme: The theme is typically causally affected or experiences a 
movement and/or change in state. The theme can appear as the infor-
mation in verbs like acquire, learn, memorize, read, study, etc. It can 
also be a thing, event or state (clausal complement). 
? John went to school.  
? John broke the vase.   
? John memorized his lines.  
? She buttered the bread with marga-
rine.   
Perceived: Refers to a perceived entity that isn't required by the verb 
but further characterizes the situation. The perceived is neither caus-
ally affected nor causative. It doesn't experience a movement or 
change in state. Its volition and sentience are irrelevant. Its existence 
is independent of an experiencer. 
? He saw the play.   
? He looked into the room.  
? The cat's fur feels good to John.   
? She imagined the movie to be loud.    
Predicate: Indicates new modifying information about other thematic 
roles. 
? We considered him a fool.   
? She acted happy.   
Source: Indicates where/when the theme started in its motion, or 
what its original state was, or where its original (possibly abstract) 
location/time was. 
? John left the house. 
Goal: Indicates where the theme ends up in its motion, or what its 
final state is, or where/when its final (possibly abstract) location/time 
is. It also can indicate the thing/event resulting from the verb's occur-
rence (the result). 
? John ran home.   
? John ran to the store.  
? John gave a book to Mary.   
? John gave Mary a book. 
Location: Indicates static locations---as opposed to a source or goal, 
i.e., the (possibly abstract) location of the theme or event. 
? He lived in France.   
? The water fills the box.   
? This cabin sleeps five people 
Time Indicates time. ? John sleeps for five hours.   
? Mary ate during the meeting. 
Beneficiary: Indicates the thing that receives the benefit/result of the 
event/state. 
? John baked the cake for Mary.   
? John baked Mary a cake.  
? An accident happened to him.   
Purpose: Indicates the purpose/reason behind an event/state ? He studied for the exam.  
? He searched for rabbits.  
Possessed: Indicates the possessed entity in verbs such as own, have, 
possess, fit, buy, and carry. 
? John has five bucks.  
? He loaded the cart with hay.   
? He bought it for five dollars 
Proposition: Indicates the secondary event/state ? He wanted to study for the exam. 
Modifier: Indicates a property of a thing such as color, taste, size, 
etc. 
? The red book sitting on the table is 
old.  
Null Indicates no thematic contribution. Typical examples are imper-
sonal it and there. 
? It was raining all morning in Miami. 
 
TABLE 2:  List of Theta Roles 
