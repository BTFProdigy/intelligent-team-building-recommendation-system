A grammar formalism and parser for linearization-based HPSG
Michael W. Daniels and W. Detmar Meurers
Department of Linguistics
The Ohio State University
222 Oxley Hall
1712 Neil Avenue
Columbus, OH 43210
daniels|dm@ling.osu.edu
Abstract
Linearization-based HPSG theories are widely
used for analyzing languages with relatively
free constituent order. This paper introduces
the Generalized ID/LP (GIDLP) grammar for-
mat, which supports a direct encoding of such
theories, and discusses key aspects of a parser
that makes use of the dominance, precedence,
and linearization domain information explicitly
encoded in this grammar format. We show
that GIDLP grammars avoid the explosion in
the number of rules required under a traditional
phrase structure analysis of free constituent or-
der. As a result, GIDLP grammars support more
modular and compact grammar encodings and
require fewer edges in parsing.
1 Introduction
Within the framework of Head-Driven Phrase Struc-
ture Grammar (HPSG), the so-called linearization-
based approaches have argued that constraints on
word order are best captured within domains that
extend beyond the local tree. A range of analyses
for languages with relatively free constituent order
have been developed on this basis (see, for example,
Reape, 1993; Kathol, 1995; Mu?ller, 1999; Donohue
and Sag, 1999; Bonami et al, 1999) so that it is at-
tractive to exploit these approaches for processing
languages with relatively free constituent order.
This paper introduces a grammar format that sup-
ports a direct encoding of linearization-based HPSG
theories. The Generalized ID/LP (GIDLP) format
explicitly encodes the dominance, precedence, and
linearization domain information and thereby sup-
ports the development of efficient parsing algorithm
making use of this information. We make this
concrete by discussing key aspects of a parser for
GIDLP grammars that integrates the word order do-
mains and constraints into the parsing process.
2 Linearization-based HPSG
The idea of discontinuous constituency was first in-
troduced into HPSG in a series of papers by Mike
Reape (see Reape, 1993 and references therein).1
The core idea is that word order is determined not
at the level of the local tree, but at the newly intro-
duced level of an order domain, which can include
elements from several local trees. We interpret this
in the following way: Each terminal has a cor-
responding order domain, and just as constituents
combine to form larger constituents, so do their or-
der domains combine to form larger order domains.
Following Reape, a daughter?s order domain
enters its mother?s order domain in one of two
ways. The first possibility, domain union, forms
the mother?s order domain by shuffling together its
daughters? domains. The second option, domain
compaction, inserts a daughter?s order domain into
its mother?s. Compaction has two effects:
Contiguity: The terminal yield of a compacted
category contains all and only the terminal yield of
the nodes it dominates; there are no holes or addi-
tional strings.
LP Locality: Precedence statements only con-
strain the order among elements within the same
compacted domain. In other words, precedence
constraints cannot look into a compacted domain.
Note that these are two distinct functions of do-
main compaction: defining a domain as covering a
contiguous stretch of terminals is in principle in-
dependent of defining a domain of elements for
LP constraints to apply to. In linearization-based
HPSG, domain compaction encodes both aspects.
Later work (Kathol and Pollard, 1995; Kathol,
1995; Yatabe, 1996) introduced the notion of partial
compaction, in which only a portion of the daugh-
ter?s order domain is compacted; the remaining ele-
ments are domain unioned.
1Apart from Reape?s approach, there have been proposals
for a more complete separation of word order and syntactic
structure in HPSG (see, for example, Richter and Sailer, 2001
and Penn, 1999). In this paper, we focus on the majority of
linearization-based HPSG approaches, which follow Reape.
3 Processing linearization-based HPSG
Formally, a theory in the HPSG architecture con-
sists of a set of constraints on the data structures
introduced in the signature; thus, word order do-
mains and the constraints thereon can be straight-
forwardly expressed. On the computational side,
however, most systems employ parsers to efficiently
process HPSG-based grammars organized around a
phrase structure backbone. Phrase structure rules
encode immediate dominance (ID) and linear prece-
dence (LP) information in local trees, so they cannot
directly encode linearization-based HPSG, which
posits word order domains that can extend the lo-
cal trees.
The ID/LP grammar format (Gazdar et al, 1985)
was introduced to separate immediate dominance
from linear precedence, and several proposals have
been made for direct parsing of ID/LP grammars
(see, for example, Shieber, 1994). However, the
domain in which word order is determined still is
the local tree licensed by an ID rule, which is insuf-
ficient for a direct encoding of linearization-based
HPSG.
The LSL grammar format as defined by Suhre
(1999) (based on Go?tz and Penn, 1997) allows el-
ements to be ordered in domains that are larger
than a local tree; as a result, categories are not re-
quired to cover contiguous strings. Linear prece-
dence constraints, however, remain restricted to lo-
cal trees: elements that are linearized in a word
order domain larger than their local tree cannot
be constrained. The approach thus provides valu-
able worst-case complexity results, but it is inade-
quate for encoding linearization-based HPSG theo-
ries, which crucially rely on the possibility to ex-
press linear precedence constraints on the elements
within a word order domain.
In sum, no grammar format is currently available
that adequately supports the encoding of a process-
ing backbone for linearization-based HPSG gram-
mars. As a result, implementations of linearization-
based HPSG grammars have taken one of two op-
tions. Some simply do not use a parser, such as the
work based on ConTroll (Go?tz and Meurers, 1997);
as a consequence, the efficiency and termination
properties of parsers cannot be taken for granted in
such approaches.
The other approaches use a minimal parser that
can only take advantage of a small subset of the req-
uisite constraints. Such parsers are typically limited
to the general concept of resource sensitivity ? ev-
ery element in the input needs to be found exactly
once ? and the ability to require certain categories
to dominate a contiguous segment of the input.
Some of these approaches (Johnson, 1985; Reape,
1991) lack word order constraints altogether. Others
(van Noord, 1991; Ramsay, 1999) have the gram-
mar writer provide a combinatory predicate (such
as concatenate, shuffle, or head-wrap) for each rule
specifying how the string coverage of the mother is
determined from the string coverages of the daugh-
ter. In either case, the task of constructing a word
order domain and enforcing word order constraints
in that domain is left out of the parsing algorithm;
as a result, constraints on word order domains either
cannot be stated or are tested in a separate clean-up
phase.
4 Defining GIDLP Grammars2
To develop a grammar format for linearization-
based HPSG, we take the syntax of ID/LP rules
and augment it with a means for specifying which
daughters form compacted domains. A Generalized
ID/LP (GIDLP) grammar consists of four parts: a
root declaration, a set of lexical entries, a set of
grammar rules, and a set of global order constraints.
We begin by describing the first three parts, which
are reminiscent of context-free grammars (CFGs),
and then address order constraints in section 4.1.
The root declaration has the form root(S , L) and
states the start symbol S of the grammar and any
linear precedence constraints L constraining the root
domain.
Lexical entries have the form A ? t and link the
pre-terminal A to the terminal t, just as in CFGs.
Grammar rules have the form A ? ?;C. They
specify that a non-terminal A immediately domi-
nates a list of non-terminals ? in a domain where
a set of order constraints C holds.
Note that in contrast to CFG rules, the order of
the elements in ? does not encode immediate prece-
dence or otherwise contribute to the denotational
meaning of the rule. Instead, the order can be used
to generalize the head marking used in grammars
for head-driven parsing (Kay, 1990; van Noord,
1991) by additionally ordering the non-head daugh-
ters.3
2Due to space limitations, we focus here on introducing the
syntax of the grammar formalism and giving an example. We
will also base the discussion on simple term categories; noth-
ing hinges on this, and when using the formalism to encode
linearization-based HPSG grammars, one will naturally use the
feature descriptions known from HPSG as categories.
3By ordering the right-hand side of a rule so that those cate-
gories come first that most restrict the search space, it becomes
possible to define a parsing algorithm that makes use of this
information. For an example of a construction where order-
ing the non-head daughters is useful, consider sentences with
AcI verbs like I see him laugh. Under the typical HPSG analy-
If the set of order constraints is empty, we obtain
the simplest type of rule, exemplified in (1).
(1) S? NP, VP
This rule says that an S may immediately dominate
an NP and a VP, with no constraints on the relative
ordering of NP and VP. One may precede the other,
the strings they cover may be interleaved, and mate-
rial dominated by a node dominating S can equally
be interleaved.
4.1 Order Constraints
GIDLP grammars include two types of order con-
straints: linear precedence constraints and com-
paction statements.
4.1.1 Linear Precedence Constraints
Linear precedence constraints can be expressed in
two contexts: on individual rules (as rule-level con-
straints) and in compaction statements (as domain-
level constraints). Domain-level constraints can
also be specified as global order constraints, which
has the effect that they are specified for each single
domain.
All precedence constraints enforce the following
property: given any appropriate pair of elements in
the same domain, one must completely precede the
other for the resulting parse to be valid. Precedence
constraints may optionally require that there be no
intervening material between the two elements: this
is referred to as immediate precedence. Precedence
constraints are notated as follows:
? Weak precedence: A<B.
? Immediate precedence: AB.
A pair of elements is considered appropriate
when one element in a domain matches the symbol
A, another matches B, and neither element domi-
nates the other (it would otherwise be impossible to
express an order constraint on a recursive rule).
The symbols A and B may be descriptions or to-
kens. A category in a domain matches a descrip-
tion if it is subsumed by it; a token refers to a spe-
cific category in a rule, as discussed below. A con-
straint involving descriptions applies to any pair of
elements in any domain in which the described cat-
egories occur; it thus can also apply more than once
within a given rule or domain. Tokens, on the other
hand, can only occur in rule-level constraints and
sis (Pollard and Sag, 1994), see combines in a ternary structure
with him and laugh. Note that the constituent that is appropriate
in the place occupied by him here can only be determined once
one has looked at the other complement, laugh, from which it
is raised.
refer to particular RHS members of a rule. In this
paper, tokens are represented by numbers referring
to the subscripted indices on the RHS categories.
In (2) we see an example of a rule-level linear
precedence constraint.
(2) A? NP1, V2, NP3; 3<V
This constraint specifies that the token 3 in the rule?s
RHS (the second NP) must precede any constituents
described as V occurring in the same domain (this
includes, but is not limited to, the V introduced by
the rule).
4.1.2 Compaction Statements
As with LP constraints, compaction statements ex-
ist as rule-level and as global order constraints; they
cannot, however, occur within other compaction
statements. A rule-level compaction statement has
the form ??, A, L?, where ? is a list of tokens,
A is the category representing the compacted do-
main, and L is a list of domain-level precedence
constraints. Such a statement specifies that the con-
stituents referenced in ? form a compacted domain
with category A, inside of which the order con-
straints in L hold. As specified in section 2, a com-
pacted domain must be contiguous (contain all and
only the terminal yield of the elements in that do-
main), and it constitutes a local domain for LP state-
ments.
It is because of partial compaction that the second
component A in a compaction statement is needed.
If only one constituent is compacted, the resulting
domain will be of the same category; but when mul-
tiple categories are fused in partial compaction, the
category of the resulting domain needs to be deter-
mined so that LP constraints can refer to it.
The rule in (3) illustrates compaction: each of the
S categories forms its own domain. In (4) partial
compaction is illustrated: the V and the first NP
form a domain named VP to the exclusion of the
second NP.
(3) S? S1, Conj2, S3;
12, 23, ?[1], S, ?[]??, ?[3], S, ?[]??
(4) VP? V1, NP2, NP3; ?[1, 2], VP, ?[]??
One will often compact only a single category
without adding domain-specific LP constraints, so
we introduce the abbreviatory notation of writing
such a compacted category in square brackets. In
this way (3) can be written as (5).
(5) S? [S1], Conj2, [S3]; 12, 23
A final abbreviatory device is useful when the en-
tire RHS of a rule forms a single domain, which
Suhre (1999) refers to as ?left isolation?. This is de-
noted by using the token 0 in the compaction state-
ment if linear precedence constraints are attached,
or by enclosing the LHS category in square brack-
ets, otherwise. (See rules (13d) and (13j) in sec-
tion 6 for an example of this notation.)
The formalism also supports global compaction
statements. A global compaction statement has the
form ?A, L?, where A is a description specifying a
category that always forms a compacted domain,
and L is a list of domain-level precedence con-
straints applying to the compacted domain.
4.2 Examples
We start with an example illustrating how a CFG
rule is encoded in GIDLP format. A CFG rule en-
codes the fact that each element of the RHS imme-
diately precedes the next, and that the mother cat-
egory dominates a contiguous string. The context-
free rule in (6) is therefore equivalent to the GIDLP
rule shown in (7).
(6) S? Nom V Acc
(7) [S]? V1, Nom2, Acc3; 21, 13
In (8) we see a more interesting example of a
GIDLP grammar.
(8) a) root(A, [])
b) A? B1, C2, [D3]; 2<3
c) B? F1, G2, E3
d) C? E1, D2, I3; ?[1,2], H, ?[]??
e) D? J1, K2
f) Lexical entries: E? e, . . .
g) E<F
(8a) is the root declaration, stating that an input
string must parse as an A; the empty list shows that
no LP constraints are specifically declared for this
domain. (8b) is a grammar rule stating that an A
may immediately dominate a B, a C, and a D; it fur-
ther states that the second constituent must precede
the third and that the third is a compacted domain.
(8c) gives a rule for B: it dominates an F, a G, and an
E, in no particular order. (8d) is the rule for C, illus-
trating partial compaction: its first two constituents
jointly form a compacted domain, which is given
the name H. (8e) gives the rule for D and (8f) spec-
ifies the lexical entries (here, the preterminals just
rewrite to the respective lowercase terminal). Fi-
nally, (8g) introduces a global LP constraint requir-
ing an E to precede an F whenever both elements
occur in the same domain.
Now consider licensing the string efjekgikj with
the above grammar. The parse tree, recording which
rules are applied, is shown in (9). Given that the
domains in which word order is determined can be
larger than the local trees, we see crossing branches
where discontinuous constituents are licensed.
(9) A
B C [D]
E F [D E]H G I K J
J K
e f j e k g i k j
To obtain a representation in which the order do-
mains are represented as local trees again, we can
draw a tree with the compacted domains forming
the nodes, as shown in (10).
(10) A
H D
e f j e k g i k j
There are three non-lexical compacted domains
in the tree in (9): the root A, the compacted D, and
the partial compaction of D and E forming the do-
main H within C. In each domain, the global LP
constraint E < F must be obeyed. Note that the
string is licensed by this grammar even though the
second occurrence of E does not precede the F. This
E is inside a compacted domain and therefore is not
in the same domain as the F, so that the LP con-
straint does not apply to those two elements. This
illustrates the property of LP locality: domain com-
paction acts as a ?barrier? to LP application.
The second aspect of domain compaction, con-
tiguity, is also illustrated by the example, in con-
nection with the difference between total and partial
compaction. The compaction of D specified in (8b)
requires that the material it dominates be a contigu-
ous segment of the input. In contrast, the partial
compaction of the first two RHS categories in rule
(8d) requires that the material dominated by D and
E, taken together, be a continuous segment. This
allows the second e to occur between the two cate-
gories dominated by D.
Finally, the two tree representations above illus-
trate the separation of the combinatorial potential
of rules (9) from the flatter word order domains
(10) that the GIDLP format achieves. It would, of
course, be possible to write phrase structure rules
that license the word order domain tree in (10) di-
rectly, but this would amount to replacing a set of
general rules with a much greater number of flatter
rules corresponding to the set of all possible ways
in which the original rules could be combined with-
out introducing domain compaction. Mu?ller (2004)
discusses the combinatorial explosion of rules that
results for an analysis of German if one wants to
flatten the trees in this way. If recursive rules such as
adjunction are included ? which is necessary since
adjuncts and complements can be freely intermixed
in the German Mittelfeld ? such flattening will not
even lead to a finite number of rules. We will return
to this issue in section 6.
5 A Parsing Algorithm for GIDLP
We have developed a GIDLP parser based on Ear-
ley?s algorithm for context-free parsing (Earley,
1970). In Earley?s original algorithm, each edge
encodes the interval of the input string it covers.
With discontinuous constituents, however, that is no
longer an option. In the spirit of Johnson (1985)
and Reape (1991), and following Ramsay (1999),
we represent edge coverage with bitvectors, stored
as integers. For instance, 00101 represents an edge
covering words one and three of a five-word sen-
tence.4
Our parsing algorithm begins by seeding the chart
with passive edges corresponding to each word in
the input and then predicting a compacted instance
of the start symbol covering the entire input; each
final completion of this edge will correspond to a
successful parse.
As with Earley?s algorithm, the bulk of the work
performed by the algorithm is borne by two steps,
prediction and completion. Unlike the context-free
case, however, it is not possible to anchor these
steps to string positions, proceeding from left to
right. The strategy for prediction used by Suhre
(1999) for his LSL parser is to predict every rule
at every position. While this strategy ensures that
no possibility is overlooked, it fails to integrate and
use the information provided by the word order con-
straints attached to the rules ? in other words, the
parser receives no top-down guidance. Some of the
edges generated by prediction therefore fall prey to
the word order constraints later, in a generate-and-
test fashion. This need not be the case. Once one
daughter of an active edge has been found, the other
daughters should only be predicted to occur in string
positions that are compatible with the word order
constraints of the active edge. For example, con-
sider the edge in (11).
(11) A? B1 ? C2 ; 1<2
4Note that the first word is the rightmost bit.
This notation represents the point in the parse dur-
ing which the application of this rule has been pre-
dicted, and a B has already been located. Assuming
that B has been found to cover the third position of a
five-word string, two facts are known. From the LP
constraint, C cannot precede B, and from the gen-
eral principle that the RHS of a rule forms a parti-
tion of its LHS, C cannot overlap B. Thus C cannot
cover positions one, two, or three.
5.1 Compiling LP Constraints into Bitmasks
We can now discuss the integration of GIDLP word
order constraints into the parsing process. A central
insight of our algorithm is that the same data struc-
ture used to describe the coverage of an edge can
also encode restrictions on the parser?s search space.
This is done by adding two bitvectors to each edge,
in addition to the coverage vector: a negative mask
(n-mask) and a positive mask (p-mask). Efficient
bitvector operations can then be used to compute,
manipulate, and test the encoded constraints.
Negative Masks The n-mask constrains the set of
possible coverage vectors that could complete the
edge. The 1-positions in a masking vector represent
the positions that are masked out: the positions that
cannot be filled when completing this edge. The 0-
positions in the negative mask represent positions
that may potentially be part of the edge?s cover-
age. For the example above, the coverage vector for
the edge is 00100 since only the third word B has
been found so far. Assuming no restrictions from a
higher rule in the same domain, the n-mask for C is
00111, encoding the fact that the final coverage vec-
tor of the edge for A must be either 01000, 10000,
or 11000 (that is, C must occupy position four, po-
sition five, or both of these positions). The negative
mask in essence encodes information on where the
active category cannot be found.
Positive Masks The p-mask encodes information
about the positions the active category must occupy.
This knowledge arises from immediate precedence
constraints. For example, consider the edge in (12).
(12) D? E1 ? F2 ; 12
If E occupies position one, then F must at least oc-
cupy position two; the second position in the posi-
tive mask would therefore be occupied.
Thus in the prediction step, the parser considers
each rule in the grammar that provides the symbol
being predicted, and for each rule, it generates bit-
masks for the new edge, taking both rule-level and
domain-level order constraints into account. The
resulting masks are checked to ensure that there is
enough space in the resulting mask for the minimum
number of categories required by the rule.5
Then, as part of each completion step, the parser
must update the LP constraints of the active edge
with the new information provided by the passive
edge. As edges are initially constructed from gram-
mar rules, all order constraints are initially ex-
pressed in terms of either descriptions or tokens. As
the parse proceeds, these constraints are updated in
terms of the actual locations where matching con-
stituents have been found. For example, a constraint
like 1 < 2 (where 1 and 2 are tokens) can be up-
dated with the information that the constituent cor-
responding to token 1 has been found as the first
word, i.e. as position 00001.
In summary, compiling LP constraints into bit-
masks in this way allows the LP constraints to be
integrated directly into the parser at a fundamental
level. Instead of weeding out inappropriate parses
in a cleanup phase, LP constraints in this parser can
immediately block an edge from being added to the
chart.
6 Evaluation
As discussed at the end of section 4.2, it is possible
to take a GIDLP grammar and write out the discon-
tinuity. All non-domain introducing rules must be
folded into the domain-introducing rules, and then
each permitted permutation of a RHS must become
a context-free rule on its own ? generally, at the cost
of a factorial increase in the number of rules.
This construction indicates the basis for a prelim-
inary assessment of the GIDLP formalism and its
parser. The grammar in (13) recognizes a very small
fragment of German, focusing on the free word or-
der of arguments and adjuncts in the so-called Mit-
telfeld that occurs to the right of either the finite verb
in yes-no questions or the complementizer in com-
plementized sentences.6
(13) a) root(s, [])
b) s? s(cmp)1
c) s? s(que)1
d) s(cmp)? cmp1, clause2;
?[0], s(cmp), ?cmp< , <v( )??
e) s(que)? clause1; ?[0], s(que), ?v( )< ??
f) clause? np(n)1, vp2
5This optimization only applies to epsilon-free grammars.
Further work in this regard can involve determining the minu-
mum and maximum yields of each category; some opti-
mizations involving this information can be found in (Haji-
Abdolhosseini and Penn, 2003).
6The symbol is used to denote the set of all categories.
g) vp? v(ditr)1, np(a)2, np(d)3
h) vp? adv1, vp2
i) vp? v(cmp)1, s(cmp)2
j) [np(Case)]? det(Case)1, n(Case)2;
12
k) v(ditr)? gab q) v(cmp)? denkt
l) comp? dass r) det(nom)? der
m) det(dat)? der s) det(acc)? das
n) n(nom)? Mann t) n(dat)? Frau
o) n(acc)? Buch u) adv? gestern
p) adv? dort
The basic idea of this grammar is that domain com-
paction only occurs at the top of the head path, af-
ter all complements and adjuncts have been found.
When the grammar is converted into a CFG, the
effect of the larger domain can only be mimicked
by eliminating the clause and vp constituents alto-
gether.
As a result, while this GIDLP grammar has 10
syntactic rules, the corresponding flattened CFG (al-
lowing for a maximum of two adverbs) has 201
rules. In an experiment, the four sample sentences
in (14)7 were parsed with both our prototype GIDLP
parser (using the GIDLP grammar) as well as a
vanilla Earley CFG parser (using the CFG); the re-
sults are shown in (15).
(14) a) Gab der Mann der Frau das Buch?
b) dass das Buch der Mann der Frau gab.
c) dass das Buch gestern der Mann dort der
Frau gab.
d) Denkt der Mann dass das Buch gestern
der Mann dort der Frau gab?
(15)
Active Edges Passive Edges
Sentence GIDLP CFG GIDLP CFG
a) 18 327 16 15
b) 27 338 18 16
c) 46 345 27 27
d) 75 456 36 24
Averaging over the four sentences, the GIDLP
grammar requires 89% fewer active edges. It also
generates additional passive edges corresponding to
the extra non-terminals vp and clause. It is impor-
tant to keep in mind that the GIDLP grammar is
more general than the CFG: in order to obtain a fi-
nite number of CFG rules, we had to limit the num-
ber of adverbs. When using a grammar capable of
7The grammar and example sentences are intended as a for-
mal illustration, not a linguistic theory; because of this and
space limitations, we have not provided glosses.
handling longer sentences with more adverbs, the
number of CFG rules (and active edges, as a conse-
quence) increases factorially.
Timings have not been included in (15); it is gen-
erally the case that the GIDLP parser/grammar com-
bination was slower than the CFG/Earley parser.
This is an artifact of the use of atomic categories,
however. For the large feature structures used as
categories in HPSG, we expect the larger numbers
of edges encountered while parsing with the CFG to
have a greater impact on parsing time, to the point
where the GIDLP grammar/parser is faster.
7 Summary
In this abstract, we have introduced a grammar for-
mat that can be used as a processing backbone for
linearization-based HPSG grammars that supports
the specification of discontinuous constituents and
word order constraints on domains that extend be-
yond the local tree. We have presented a prototype
parser for this format illustrating the use of order
constraint compilation techniques to improve effi-
ciency. Future work will concentrate on additional
techniques for optimized parsing as well as the ap-
plication of the parser to feature-based grammars.
We hope that the GIDLP grammar format will en-
courage research on such optimizations in general,
in support of efficient processing of relatively free
constituent order phenomena using linearization-
based HPSG.
References
Olivier Bonami, Danie`le Godard, and Jean-Marie
Marandin. 1999. Constituency and word order
in French subject inversion. In Gosse Bouma et
al., editor, Constraints and Resources in Natural
Language Syntax and Semantics. CSLI.
Cathryn Donohue and Ivan A. Sag. 1999. Domains
in Warlpiri. In Abstracts of the Sixth Int. Confer-
ence on HPSG, pages 101?106, Edinburgh.
Jay Earley. 1970. An efficient context-free parsing
algorithm. Communications of the ACM, 13(2).
Gerald Gazdar, Ewan Klein, Geoffrey K. Pullum,
and Ivan A. Sag. 1985. Generalized Phrase
Structure Grammar. Harvard University Press.
Thilo Go?tz and W. Detmar Meurers. 1997. The
ConTroll system as large grammar development
platform. In Proceedings of the EACL Workshop
?Computational Environments for Grammar De-
velopment and Linguistic Engineering?, Madrid.
Thilo Go?tz and Gerald Penn. 1997. A proposed
linear specification language. Volume 134 in Ar-
beitspapiere des SFB 340, Tu?bingen.
Mohammad Haji-Abdolhosseini and Gerald Penn.
2003. ALE reference manual. Univ. Toronto.
Mark Johnson. 1985. Parsing with discontinuous
constituents. In Proceedings of ACL, Chicago.
Andreas Kathol and Carl Pollard. 1995. Extraposi-
tion via complex domain formation. In Proceed-
ings of ACL, pages 174?180, Boston.
Andreas Kathol. 1995. Linearization-Based Ger-
man Syntax. Ph.D. thesis, Ohio State University.
Martin Kay. 1990. Head-driven parsing. In Masaru
Tomita, editor, Current Issues in Parsing Tech-
nology. Kluwer, Dordrecht.
Stefan Mu?ller. 1999. Deutsche Syntax deklarativ.
Niemeyer, Tu?bingen.
Stefan Mu?ller. 2004. Continuous or discontinuous
constituents? A comparison between syntactic
analyses for constituent order and their process-
ing systems. Research on Language and Compu-
tation, 2(2):209?257.
Gerald Penn. 1999. Linearization and WH-
extraction in HPSG: Evidence from Serbo-
Croatian. In Robert D. Borsley and Adam
Przepio?rkowski, editors, Slavic in HPSG. CSLI.
Carl Pollard and Ivan A. Sag. 1994. Head-
Driven Phrase Structure Grammar. University of
Chicago Press, Chicago.
Allan M. Ramsay. 1999. Direct parsing with dis-
continuous phrases. Natural Language Engi-
neering, 5(3):271?300.
Mike Reape. 1991. Parsing bounded discontinuous
constituents: Generalisations of some common
algorithms. In Mike Reape, editor, Word Order
in Germanic and Parsing. DYANA R1.1.C.
Mike Reape. 1993. A Formal Theory of Word Or-
der: A Case Study in West Germanic. Ph.D. the-
sis, University of Edinburgh.
Frank Richter and Manfred Sailer. 2001. On the left
periphery of German finite sentences. In W. Det-
mar Meurers and Tibor Kiss, editors, Constraint-
Based Approaches to Germanic Syntax. CSLI.
Stuart M. Shieber. 1984. Direct parsing of ID/LP
grammars. Linguistics & Philosophy, 7:135?154.
Oliver Suhre. 1999. Computational aspects of
a grammar formalism for languages with freer
word order. Diplomarbeit. (= Volume 154 in Ar-
beitspapiere des SFB 340, 2000).
Gertjan van Noord. 1991. Head corner parsing for
discontinuous constituency. In ACL Proceedings.
Shuichi Yatabe. 1996. Long-distance scrambling
via partial compaction. In Masatoshi Koizumi,
Masayuki Oishi, and Uli Sauerland, editors,
Formal Approaches to Japanese Linguistics 2.
MITWPL.
107
108
109
110
111
112
113
114
Proceedings of the 43rd Annual Meeting of the ACL, pages 322?329,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Detecting Errors in Discontinuous Structural Annotation
Markus Dickinson
Department of Linguistics
The Ohio State University
dickinso@ling.osu.edu
W. Detmar Meurers
Department of Linguistics
The Ohio State University
dm@ling.osu.edu
Abstract
Consistency of corpus annotation is an
essential property for the many uses of
annotated corpora in computational and
theoretical linguistics. While some re-
search addresses the detection of inconsis-
tencies in positional annotation (e.g., part-
of-speech) and continuous structural an-
notation (e.g., syntactic constituency), no
approach has yet been developed for au-
tomatically detecting annotation errors in
discontinuous structural annotation. This
is significant since the annotation of po-
tentially discontinuous stretches of ma-
terial is increasingly relevant, from tree-
banks for free-word order languages to se-
mantic and discourse annotation.
In this paper we discuss how the variation
n-gram error detection approach (Dickin-
son and Meurers, 2003a) can be extended
to discontinuous structural annotation. We
exemplify the approach by showing how it
successfully detects errors in the syntactic
annotation of the German TIGER corpus
(Brants et al, 2002).
1 Introduction
Annotated corpora have at least two kinds of uses:
firstly, as training material and as ?gold standard?
testing material for the development of tools in com-
putational linguistics, and secondly, as a source of
data for theoretical linguists searching for analyti-
cally relevant language patterns.
Annotation errors and why they are a problem
The high quality annotation present in ?gold stan-
dard? corpora is generally the result of a manual
or semi-automatic mark-up process. The annota-
tion thus can contain annotation errors from auto-
matic (pre-)processes, human post-editing, or hu-
man annotation. The presence of errors creates prob-
lems for both computational and theoretical linguis-
tic uses, from unreliable training and evaluation of
natural language processing technology (e.g., van
Halteren, 2000; Kve?to?n and Oliva, 2002, and the
work mentioned below) to low precision and recall
of queries for already rare linguistic phenomena. In-
vestigating the quality of linguistic annotation and
improving it where possible thus is a key issue for
the use of annotated corpora in computational and
theoretical linguistics.
Illustrating the negative impact of annotation er-
rors on computational uses of annotated corpora,
van Halteren et al (2001) compare taggers trained
and tested on the Wall Street Journal (WSJ, Marcus
et al, 1993) and the Lancaster-Oslo-Bergen (LOB,
Johansson, 1986) corpora and find that the results for
the WSJ perform significantly worse. They report
that the lower accuracy figures are caused by incon-
sistencies in the WSJ annotation and that 44% of the
errors for their best tagging system were caused by
?inconsistently handled cases.?
Turning from training to evaluation, Padro and
Marquez (1998) highlight the fact that the true ac-
curacy of a classifier could be much better or worse
than reported, depending on the error rate of the cor-
pus used for the evaluation. Evaluating two taggers
on the WSJ, they find tagging accuracy rates for am-
322
biguous words of 91.35% and 92.82%. Given the
estimated 3% error rate of the WSJ tagging (Marcus
et al, 1993), they argue that the difference in perfor-
mance is not sufficient to establish which of the two
taggers is actually better.
In sum, corpus annotation errors, especially er-
rors which are inconsistencies, can have a profound
impact on the quality of the trained classifiers and
the evaluation of their performance. The problem is
compounded for syntactic annotation, given the dif-
ficulty of evaluating and comparing syntactic struc-
ture assignments, as known from the literature on
parser evaluation (e.g., Carroll et al, 2002).
The idea that variation in annotation can indicate
annotation errors has been explored to detect errors
in part-of-speech (POS) annotation (van Halteren,
2000; Eskin, 2000; Dickinson and Meurers, 2003a)
and syntactic annotation (Dickinson and Meurers,
2003b). But, as far as we are aware, the research
we report on here is the first approach to error detec-
tion for the increasing number of annotations which
make use of more general graph structures for the
syntactic annotation of free word order languages or
the annotation of semantic and discourse properties.
Discontinuous annotation and its relevance The
simplest kind of annotation is positional in nature,
such as the association of a part-of-speech tag with
each corpus position. On the other hand, struc-
tural annotation such as that used in syntactic tree-
banks (e.g., Marcus et al, 1993) assigns a syntactic
category to a contiguous sequence of corpus posi-
tions. For languages with relatively free constituent
order, such as German, Dutch, or the Slavic lan-
guages, the combinatorial potential of the language
encoded in constituency cannot be mapped straight-
forwardly onto the word order possibilities of those
languages. As a consequence, the treebanks that
have been created for German (NEGRA, Skut et al,
1997; VERBMOBIL, Hinrichs et al, 2000; TIGER,
Brants et al, 2002) have relaxed the requirement that
constituents have to be contiguous. This makes it
possible to syntactically annotate the language data
as such, i.e., without requiring postulation of empty
elements as placeholders or other theoretically mo-
tivated changes to the data. We note in passing that
discontinuous constituents have also received some
support in theoretical linguistics (cf., e.g., the arti-
cles collected in Huck and Ojeda, 1987; Bunt and
van Horck, 1996).
Discontinuous constituents are strings of words
which are not necessarily contiguous, yet form a
single constituent with a single label, such as the
noun phrase Ein Mann, der lacht in the German rel-
ative clause extraposition example (1) (Brants et al,
2002).1
(1) Ein
a
Mann
man
kommt
comes
,
,
der
who
lacht
laughs
?A man who laughs comes.?
In addition to their use in syntactic annotation,
discontinuous structural annotation is also rele-
vant for semantic and discourse-level annotation?
essentially any time that graph structures are needed
to encode relations that go beyond ordinary tree
structures. Such annotations are currently employed
in the mark-up for semantic roles (e.g., Kings-
bury et al, 2002) and multi-word expressions (e.g.,
Rayson et al, 2004), as well as for spoken language
corpora or corpora with multiple layers of annota-
tion which cross boundaries (e.g., Blache and Hirst,
2000).
In this paper, we present an approach to the de-
tection of errors in discontinuous structural annota-
tion. We focus on syntactic annotation with poten-
tially discontinuous constituents and show that the
approach successfully deals with the discontinuous
syntactic annotation found in the TIGER treebank
(Brants et al, 2002).
2 The variation n-gram method
Our approach builds on the variation n-gram al-
gorithm introduced in Dickinson and Meurers
(2003a,b). The basic idea behind that approach is
that a string occurring more than once can occur
with different labels in a corpus, which we refer to as
variation. Variation is caused by one of two reasons:
i) ambiguity: there is a type of string with multiple
possible labels and different corpus occurrences of
that string realize the different options, or ii) error:
the tagging of a string is inconsistent across compa-
rable occurrences.
1The ordinary way of marking a constituent with brack-
ets is inadequate for discontinuous constituents, so we instead
boldface and underline the words belonging to a discontinuous
constituent.
323
The more similar the context of a variation, the
more likely the variation is an error. In Dickin-
son and Meurers (2003a), contexts are composed
of words, and identity of the context is required.
The term variation n-gram refers to an n-gram (of
words) in a corpus that contains a string annotated
differently in another occurrence of the same n-gram
in the corpus. The string exhibiting the variation is
referred to as the variation nucleus.
2.1 Detecting variation in POS annotation
In Dickinson and Meurers (2003a), we explore this
idea for part-of-speech annotation. For example, in
the WSJ corpus the string in (2) is a variation 12-
gram since off is a variation nucleus that in one cor-
pus occurrence is tagged as a preposition (IN), while
in another it is tagged as a particle (RP).2
(2) to ward off a hostile takeover attempt by two
European shipping concerns
Once the variation n-grams for a corpus have
been computed, heuristics are employed to classify
the variations into errors and ambiguities. The first
heuristic encodes the basic fact that the label assign-
ment for a nucleus is dependent on the context: vari-
ation nuclei in long n-grams are likely to be errors.
The second takes into account that natural languages
favor the use of local dependencies over non-local
ones: nuclei found at the fringe of an n-gram are
more likely to be genuine ambiguities than those oc-
curring with at least one word of surrounding con-
text. Both of these heuristics are independent of a
specific corpus, annotation scheme, or language.
We tested the variation error detection method on
the WSJ and found 2495 distinct3 nuclei for the vari-
ation n-grams between the 6-grams and the 224-
grams. 2436 of these were actual errors, making for
a precision of 97.6%, which demonstrates the value
of the long context heuristic. 57 of the 59 genuine
ambiguities were fringe elements, confirming that
fringe elements are more indicative of a true ambi-
guity.
2To graphically distinguish the variation nucleus within a
variation n-gram, the nucleus is shown in grey.
3Being distinct means that each corpus position is only taken
into account for the longest variation n-gram it occurs in.
2.2 Detecting variation in syntactic annotation
In Dickinson and Meurers (2003b), we decompose
the variation n-gram detection for syntactic annota-
tion into a series of runs with different nucleus sizes.
This is needed to establish a one-to-one relation be-
tween a unit of data and a syntactic category annota-
tion for comparison. Each run detects the variation
in the annotation of strings of a specific length. By
performing such runs for strings from length 1 to
the length of the longest constituent in the corpus,
the approach ensures that all strings which are ana-
lyzed as a constituent somewhere in the corpus are
compared to the annotation of all other occurrences
of that string.
For example, the variation 4-gram from a year
earlier appears 76 times in the WSJ, where the nu-
cleus a year is labeled noun phrase (NP) 68 times,
and 8 times it is not annotated as a constituent and
is given the special label NIL. An example with
two syntactic categories involves the nucleus next
Tuesday as part of the variation 3-gram maturity
next Tuesday, which appears three times in the WSJ.
Twice it is labeled as a noun phrase (NP) and once as
a prepositional phrase (PP).
To be able to efficiently calculate all variation nu-
clei of a treebank, in Dickinson and Meurers (2003b)
we make use of the fact that a variation necessar-
ily involves at least one constituent occurrence of
a nucleus and calculate the set of nuclei for a win-
dow of length i by first finding the constituents of
that length. Based on this set, we then find non-
constituent occurrences of all strings occurring as
constituents. Finally, the variation n-grams for these
variation nuclei are obtained in the same way as for
POS annotation.
In the WSJ, the method found 34,564 variation
nuclei, up to size 46; an estimated 71% of the 6277
non-fringe distinct variation nuclei are errors.
3 Discontinuous constituents
In Dickinson and Meurers (2003b), we argued that
null elements need to be ignored as variation nuclei
because the variation in the annotation of a null el-
ement as the nucleus is largely independent of the
local environment. For example, in (3) the null el-
ement *EXP* (expletive) can be annotated a. as a
sentence (S) or b. as a relative/subordinate clause
324
(SBAR), depending on the properties of the clause
it refers to.
(3) a. For cities losing business to suburban shop-
ping centers , it *EXP* may be a wise busi-
ness investment [S * to help * keep those
jobs and sales taxes within city limits] .
b. But if the market moves quickly enough , it
*EXP* may be impossible [SBAR for the
broker to carry out the order] because the in-
vestment has passed the specified price .
We found that removing null elements as variation
nuclei of size 1 increased the precision of error de-
tection to 78.9%.
Essentially, null elements represent discontinu-
ous constituents in a formalism with a context-free
backbone (Bies et al, 1995). Null elements are co-
indexed with a non-adjacent constituent; in the pred-
icate argument structure, the constituent should be
interpreted where the null element is.
To be able to annotate discontinuous material
without making use of inserted null elements, some
treebanks have instead relaxed the definition of a lin-
guistic tree and have developed more complex graph
annotations. An error detection method for such cor-
pora thus does not have to deal with the problems
arising from inserted null elements discussed above,
but instead it must function appropriately even if
constituents are discontinuously realized.
A technique such as the variation n-gram method
is applicable to corpora with a one-to-one map-
ping between the text and the annotation. For
corpora with positional annotation?e.g., part-of-
speech annotated corpora?the mapping is triv-
ial given that the annotation consists of one-to-
one correspondences between words (i.e., tokens)
and labels. For corpora annotated with more
complex structural information?e.g., syntactically-
annotated corpora?the one-to-one mapping is ob-
tained by considering every interval (continuous
string of any length) which is assigned a category
label somewhere in the corpus.
While this works for treebanks with continuous
constituents, a one-to-one mapping is more com-
plicated to establish for syntactic annotation involv-
ing discontinuous constituents (NEGRA, Skut et al,
1997; TIGER, Brants et al, 2002). In order to apply
the variation n-gram method to discontinuous con-
stituents, we need to develop a technique which is
capable of comparing labels for any set of corpus
positions, instead of for any interval.
4 Extending the variation n-gram method
To extend the variation n-gram method to handle
discontinuous constituents, we first have to define
the characteristics of such a constituent (section 4.1),
in other words our units of data for comparison.
Then, we can find identical non-constituent (NIL)
strings (section 4.2) and expand the context into
variation n-grams (section 4.3).
4.1 Variation nuclei: Constituents
For traditional syntactic annotation, a variation nu-
cleus is defined as a contiguous string with a sin-
gle label; this allows the variation n-gram method
to be broken down into separate runs, one for each
constituent size in the corpus. For discontinuous
syntactic annotation, since we are still interested in
comparing cases where the nucleus is the same, we
will treat two constituents as having the same size if
they consist of the same number of words, regard-
less of the amount of intervening material, and we
can again break the method down into runs of differ-
ent sizes. The intervening material is accounted for
when expanding the context into n-grams.
A question arises concerning the word order of
elements in a constituent. Consider the German ex-
ample (4) (Mu?ller, 2004).
(4) weil
because
der
the
Mann
mannom
der
the
Frau
womandat
das
the
Buch
bookacc
gab.
gave
?because the man gave the woman the book.?
The three arguments of the verb gab (?give?) can be
permuted in all six possible ways and still result in a
well-formed sentence. It might seem, then, that we
would want to allow different permutations of nuclei
to be treated as identical. If das Buch der Frau gab
is a constituent in another sentence, for instance, it
should have the same category label as der Frau das
Buch gab.
Putting all permutations into one equivalence
class, however, amounts to stating that all order-
325
ings are always the same. But even ?free word or-
der? languages are more appropriately called free
constituent order; for example, in (4), the argument
noun phrases can be freely ordered, but each argu-
ment noun phrase is an atomic unit, and in each unit
the determiner precedes the noun.
Since we want our method to remain data-driven
and order can convey information which might be
reflected in an annotation system, we keep strings
with different orders of the same words distinct, i.e.,
ordering of elements is preserved in our method.
4.2 Variation nuclei: Non-constituents
The basic idea is to compare a string annotated as a
constituent with the same string found elsewhere?
whether annotated as a constituent or not. So we
need to develop a method for finding all string oc-
currences not analyzed as a constituent (and assign
them the special category label NIL). Following
Dickinson and Meurers (2003b), we only look for
non-constituent occurrences of those strings which
also occur at least once as a constituent.
But do we need to look for discontinuous NIL
strings or is it sufficient to assume only continuous
ones? Consider the TIGER treebank examples (5).
(5) a. in
on
diesem
this
Punkt
point
seien
are
sich
SELF
Bonn
Bonn
und
and
London
London
nicht
not
einig
agreed
.
.
?Bonn and London do not agree on this point.?
b. in
on
diesem
this
Punkt
point
seien
are
sich
SELF
Bonn
Bonn
und
and
London
London
offensichtlich
clearly
nicht einig
not agreed
.
.
In example (5a), sich einig (?SELF agree?) forms
an adjective phrase (AP) constituent. But in ex-
ample (5b), that same string is not analyzed as a
constituent, despite being in a nearly identical sen-
tence. We would thus like to assign the discontinu-
ous string sich einig in (5b) the label NIL, so that the
labeling of this string in (5a) can be compared to its
occurrence in (5b).
In consequence, our approach should be able to
detect NIL strings which are discontinuous?an is-
sue which requires special attention to obtain an al-
gorithm efficient enough to handle large corpora.
Use sentence boundary information The first
consideration makes use of the fact that syntactic an-
notation by its nature respects sentence boundaries.
In consequence, we never need to search for NIL
strings that span across sentences.4
Use tries to store constituent strings The sec-
ond consideration concerns how we calculate the
NIL strings. To find every non-constituent string in
the corpus, discontinuous or not, which is identical
to some constituent in the corpus, a basic approach
would first generate all possible strings within a sen-
tence and then test to see which ones occur as a
constituent elsewhere in the corpus. For example,
if the sentence is Nobody died when Clinton lied, we
would see if any of the 31 subsets of strings occur
as constituents (e.g., Nobody, Nobody when, Clin-
ton lied, Nobody when lied, etc.). But such a gener-
ate and test approach clearly is intractable given that
it generates generates 2n? 1 potential matches for a
sentence of n words.
We instead split the task of finding NIL strings into
two runs through the corpus. In the first, we store
all constituents in the corpus in a trie data structure
(Fredkin, 1960), with words as nodes. In the sec-
ond run through the corpus, we attempt to match the
strings in the corpus with a path in the trie, thus iden-
tifying all strings occurring as constituents some-
where in the corpus.
Filter out unwanted NIL strings The final con-
sideration removes ?noisy? NIL strings from the can-
didate set. Certain NIL strings are known to be use-
less for detecting annotation errors, so we should re-
move them to speed up the variation n-gram calcu-
lations. Consider example (6) from the TIGER cor-
pus, where the continuous constituent die Menschen
is annotated as a noun phrase (NP).
(6) Ohne
without
diese
these
Ausgaben,
expenses
so
according to
die
the
Weltbank,
world bank
seien
are
die Menschen
the people
totes
dead
Kapital
capital
?According to the world bank, the people are dead capital
without these expenses.?
4This restriction clearly is syntax specific and other topo-
logical domains need to be identified to make searching for NIL
strings tractable for other types of discontinuous annotation.
326
Our basic method of finding NIL strings would de-
tect another occurrence of die Menschen in the same
sentence since nothing rules out that the other occur-
rence of die in the sentence (preceding Weltbank)
forms a discontinuous NIL string with Menschen.
Comparing a constituent with a NIL string that con-
tains one of the words of the constituent clearly goes
against the original motivation for wanting to find
discontinuous strings, namely that they show varia-
tion between different occurrences of a string.
To prevent such unwanted variation, we eliminate
occurrences of NIL-labeled strings that overlap with
identical constituent strings from consideration.
4.3 Variation n-grams
The more similar the context surrounding a varia-
tion nucleus, the more likely it is for a variation in
its annotation to be an error. For detecting errors in
traditional syntactic annotation (see section 2.2), the
context consists of the elements to the left and the
right of the nucleus. When nuclei can be discontinu-
ous, however, there can also be internal context, i.e.,
elements which appear between the words forming
a discontinuous variation nucleus.
As in our earlier work, an instance of the a pri-
ori algorithm is used to expand a nucleus into a
longer n-gram by stepwise adding context elements.
Where previously it was possible to add an element
to the left or the right, we now also have the option of
adding it in the middle?as part of the new, internal
context. But depending on how we fill in the internal
context, we can face a serious tractability problem.
Given a nucleus with j gaps within it, we need to
potentially expand it in j + 2 directions, instead of
in just 2 directions (to the right and to the left).
For example, the potential nucleus was werden
appears as a verb phrase (VP) in the TIGER corpus in
the string was ein Seeufer werden; elsewhere in the
corpus was and werden appear in the same sentence
with 32 words between them. The chances of one of
the middle 32 elements matching something in the
internal context of the VP is relatively high, and in-
deed the twenty-sixth word is ein. However, if we
move stepwise out from the nucleus in order to try
to match was ein Seeufer werden, the only options
are to find ein directly to the right of was or Seeufer
directly to the left of werden, neither of which oc-
curs, thus stopping the search.
In conclusion, we obtain an efficient application
of the a priori algorithm by expanding the context
only to elements which are adjacent to an element
already in the n-gram. Note that this was already
implicitly assumed for the left and the right context.
There are two other efficiency-related issues
worth mentioning. Firstly, as with the variation nu-
cleus detection, we limit the n-grams expansion to
sentences only. Since the category labels do not rep-
resent cross-sentence dependencies, we gain no new
information if we find more context outside the sen-
tence, and in terms of efficiency, we cut off what
could potentially be a very large search space.5
Secondly, the methods for reducing the number
of variation nuclei discussed in section 4.2 have the
consequence of also reducing the number of possi-
ble variation n-grams. For example, in a test run
on the NEGRA corpus we allowed identical strings
to overlap; this generated a variation nucleus of size
63, with 16 gaps in it, varying between NP and NIL
within the same sentence. Fifteen of the gaps can be
filled in and still result in variation. The filter for un-
wanted NIL strings described in the previous section
eliminates the NIL value from consideration. Thus,
there is no variation and no tractability problem in
constructing n-grams.
4.3.1 Generalizing the n-gram context
So far, we assumed that the context added around
variation nuclei consists of words. Given that tree-
banks generally also provide part-of-speech infor-
mation for every token, we experimented with part-
of-speech tags as a less restrictive kind of context.
The idea is that it should be possible to find more
variation nuclei with comparable contexts if only the
part-of-speech tags of the surrounding words have to
be identical instead of the words themselves.
As we will see in section 5, generalizing n-gram
contexts in this way indeed results in more variation
n-grams being found, i.e., increased recall.
4.4 Adapting the heuristics
To determine which nuclei are errors, we can build
on the two heuristics from previous research (Dick-
5Note that similar sentences which were segmented differ-
ently could potentially cause varying n-gram strings not to be
found. We propose to treat this as a separate sentence segmen-
tation error detection phase in future work.
327
inson and Meurers, 2003a,b)?trust long contexts
and distrust the fringe?with some modification,
given that we have more fringe areas to deal with
for discontinuous strings. In addition to the right
and the left fringe, we also need to take into account
the internal context in a way that maintains the non-
fringe heuristic as a good indicator for errors. As
a solution that keeps internal context on a par with
the way external context is treated in our previous
work, we require one word of context around every
terminal element that is part of the variation nucleus.
As discussed below, this heuristic turns out to be a
good predictor of which variations are annotation er-
rors; expanding to the longest possible context, as in
Dickinson and Meurers (2003a), is not necessary.
5 Results on the TIGER Corpus
We ran the variation n-grams error detection method
for discontinuous syntactic constituents on v. 1 of
TIGER (Brants et al, 2002), a corpus of 712,332
tokens in 40,020 sentences. The method detected
a total of 10,964 variation nuclei. From these we
sampled 100 to get an estimate of the number of er-
rors in the corpus which concern variation. Of these
100, 13 variation nuclei pointed to an error; with this
point estimate of .13, we can derive a 95% confi-
dence interval of (0.0641, 0.1959),6 which means
that we are 95% confident that the true number of
variation-based errors is between 702 and 2148. The
effectiveness of a method which uses context to nar-
row down the set of variation nuclei can be judged
by how many of these variation errors it finds.
Using the non-fringe heuristic discussed in the
previous section, we selected the shortest non-fringe
variation n-grams to examine. Occurrences of the
same strings within larger n-grams were ignored, so
as not to artificially increase the resulting set of n-
grams.
When the context is defined as identical words,
we obtain 500 variation n-grams. Sampling 100 of
these and labeling for each position whether it is an
error or an ambiguity, we find that 80 out of the 100
samples point to at least one token error. The 95%
confidence interval for this point estimate of .80 is
6The 95% confidence interval was calculated using the stan-
dard formula of p?1.96
q
p(1?p)
n , where p is the point estimate
and n the sample size.
(0.7216, 0.8784), so we are 95% confident that the
true number of error types is between 361 and 439.
Note that this precision is comparable to the esti-
mates for continuous syntactic annotation in Dick-
inson and Meurers (2003b) of 71% (with null ele-
ments) and 78.9% (without null elements).
When the context is defined as identical parts of
speech, as described in section 4.3.1, we obtain 1498
variation n-grams. Again sampling 100 of these, we
find that 52 out of the 100 point to an error. And
the 95% confidence interval for this point estimate
of .52 is (0.4221, 0.6179), giving a larger estimated
number of errors, between 632 and 926.
Context Precision Errors
Word 80% 361?439
POS 52% 632?926
Figure 1: Accuracy rates for the different contexts
Words convey more information than part-of-
speech tags, and so we see a drop in precision when
using part-of-speech tags for context, but these re-
sults highlight a very practical benefit of using a
generalized context. By generalizing the context, we
maintain a precision rate of approximately 50%, and
we substantially increase the recall of the method.
There are, in fact, likely twice as many errors when
using POS contexts as opposed to word contexts.
Corpus annotation projects willing to put in some
extra effort thus can use this method of finding vari-
ation n-grams with a generalized context to detect
and correct more errors.
6 Summary and Outlook
We have described the first method for finding er-
rors in corpora with graph annotations. We showed
how the variation n-gram method can be extended
to discontinuous structural annotation, and how this
can be done efficiently and with as high a preci-
sion as reported for continuous syntactic annotation.
Our experiments with the TIGER corpus show that
generalizing the context to part-of-speech tags in-
creases recall while keeping precision above 50%.
The method can thus have a substantial practical
benefit when preparing a corpus with discontinuous
annotation.
Extending the error detection method to handle
328
discontinuous constituents, as we have done, has
significant potential for future work given the in-
creasing number of free word order languages for
which corpora and treebanks are being developed.
Acknowledgements We are grateful to George
Smith and Robert Langner of the University of Pots-
dam TIGER team for evaluating the variation we de-
tected in the samples. We would also like to thank
the three ACL reviewers for their detailed and help-
ful comments, and the participants of the OSU CLip-
pers meetings for their encouraging feedback.
References
Ann Bies, Mark Ferguson, Karen Katz and Robert
MacIntyre, 1995. Bracketing Guidelines for Tree-
bank II Style Penn Treebank Project. University
of Pennsylvania.
Philippe Blache and Daniel Hirst, 2000. Multi-level
annotation for spoken-language corpora. In Pro-
ceedings of ICSLP-00. Beijing, China.
Sabine Brants, Stefanie Dipper, Silvia Hansen,
Wolfgang Lezius and George Smith, 2002. The
TIGER Treebank. In Proceedings of TLT-02. So-
zopol, Bulgaria.
Harry Bunt and Arthur van Horck (eds.), 1996. Dis-
continuous Constituency. Mouton de Gruyter,
Berlin and New York.
John Carroll, Anette Frank, Dekang Lin, Detlef
Prescher and Hans Uszkoreit (eds.), 2002. Pro-
ceedings of the LREC Workshop ?Beyond PAR-
SEVAL. Towards Improved Evaluation Measures
for Parsing Systems?, Las Palmas, Gran Canaria.
Markus Dickinson and W. Detmar Meurers, 2003a.
Detecting Errors in Part-of-Speech Annotation. In
Proceedings of EACL-03. Budapest, Hungary.
Markus Dickinson and W. Detmar Meurers, 2003b.
Detecting Inconsistencies in Treebanks. In Pro-
ceedings of TLT-03. Va?xjo?, Sweden.
Eleazar Eskin, 2000. Automatic Corpus Correc-
tion with Anomaly Detection. In Proceedings of
NAACL-00. Seattle, Washington.
Edward Fredkin, 1960. Trie Memory. CACM,
3(9):490?499.
Erhard Hinrichs, Julia Bartels, Yasuhiro Kawata,
Valia Kordoni and Heike Telljohann, 2000. The
Tu?bingen Treebanks for Spoken German, En-
glish, and Japanese. In Wolfgang Wahlster (ed.),
Verbmobil: Foundations of Speech-to-Speech
Translation, Springer, Berlin, pp. 552?576.
Geoffrey Huck and Almerindo Ojeda (eds.), 1987.
Discontinuous Constituency. Academic Press,
New York.
Stig Johansson, 1986. The Tagged LOB Corpus:
Users? Manual. Norwegian Computing Centre for
the Humanities, Bergen.
Paul Kingsbury, Martha Palmer and Mitch Marcus,
2002. Adding Semantic Annotation to the Penn
TreeBank. In Proceedings of HLT-02. San Diego.
Pavel Kve?to?n and Karel Oliva, 2002. Achieving
an Almost Correct PoS-Tagged Corpus. In Petr
Sojka, Ivan Kopec?ek and Karel Pala (eds.), TSD
2002. Springer, Heidelberg, pp. 19?26.
M. Marcus, Beatrice Santorini and M. A.
Marcinkiewicz, 1993. Building a large an-
notated corpus of English: The Penn Treebank.
Computational Linguistics, 19(2):313?330.
Stefan Mu?ller, 2004. Continuous or Discontinu-
ous Constituents? A Comparison between Syn-
tactic Analyses for Constituent Order and Their
Processing Systems. Research on Language and
Computation, 2(2):209?257.
Lluis Padro and Lluis Marquez, 1998. On the Eval-
uation and Comparison of Taggers: the Effect of
Noise in Testing Corpora. In COLING/ACL-98.
Paul Rayson, Dawn Archer, Scott Piao and Tony
McEnery, 2004. The UCREL Semantic Analy-
sis System. In Proceedings of the Workshop on
Beyond Named Entity Recognition: Semantic la-
belling for NLP tasks. Lisbon, Portugal, pp. 7?12.
Wojciech Skut, Brigitte Krenn, Thorsten Brants and
Hans Uszkoreit, 1997. An Annotation Scheme
for Free Word Order Languages. In Proceedings
of ANLP-97. Washington, D.C.
Hans van Halteren, 2000. The Detection of Inconsis-
tency in Manually Tagged Text. In Anne Abeille?,
Thorsten Brants and Hans Uszkoreit (eds.), Pro-
ceedings of LINC-00. Luxembourg.
Hans van Halteren, Walter Daelemans and Jakub Za-
vrel, 2001. Improving Accuracy in Word Class
Tagging through the Combination of Machine
Learning Systems. Computational Linguistics,
27(2):199?229.
329
A Web-based Instructional Platform for Constraint-Based Grammar
Formalisms and Parsing
W. Detmar Meurers
Dept. of Linguistics
Ohio State University
dm@ling.osu.edu
Gerald Penn
Dept. of Computer Science
University of Toronto
gpenn@cs.toronto.edu
Frank Richter
Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
fr@sfs.uni-tuebingen.de
Abstract
We propose the creation of a web-based
training framework comprising a set of
topics that revolve around the use of fea-
ture structures as the core data structure
in linguistic theory, its formal foundations,
and its use in syntactic processing.
1 Introduction
Feature structures have been used prolifically at ev-
ery level of linguistic theory, and they form the
mathematical foundation of our most comprehen-
sive and rigorous schools of syntactic theory, includ-
ing Lexical-Functional Grammar and Head-driven
Phrase Structure Grammar. This data structure is
popular because it shares many properties with the
first-order terms of classical logic, and in addi-
tion provides named access to substructures through
paths of features. Often it also includes a type sys-
tem reminiscent of the taxonomical classification
systems that are widely used in knowledge represen-
tation, psychology and the natural sciences.
For teaching a subject like computational linguis-
tics, which draws on a broad curriculum from many
traditional disciplines to audiences with mixed back-
grounds themselves, feature-structure-based theo-
retical and computational linguistics have three im-
portant properties. First, they are a mature disci-
pline, in which a great deal of accomplishments have
been made over the last 20 years, spanning from em-
pirical and conceptual advances in linguistic theory
to its mathematical and computational foundations,
to grammar development and efficient processing.
Second, they are pervasive as an already existing
representation standard for many levels of linguistic
study. Third, they are transparent, reducing com-
plex theories of grammar to a basic collection of
mathematical concepts and algorithms for answer-
ing formal questions about those theories. One can
address the distinction between descriptions of ob-
jects and the objects themselves, the difference be-
tween consistency and truth, and what it means for a
syntactic theory to be not only elegant but correct in
a precise and provable sense.
The purpose of this paper is to discuss how these
three properties can be cast into an instructional set-
ting to arrive at a framework for teaching computa-
tional linguistics that highlights the integrated nature
and precision with which work in this very hetero-
geneous discipline can be presented. In principle,
the framework we are proposing is open-ended, in
the sense that additional modules should be added
by students and other researchers, subject to the de-
sign principles given in Section 3. We are currently
designing three of the core modules for this frame-
work: formal foundations, constraint-based gram-
mar implementation, and parsing.
2 Problems of seminar-style courses
The contents of our core modules are based on a
series of previous seminar-style courses, in partic-
ular on constraint-based grammar implementation,
which also started integrating interactive compo-
nents and web-based materials into traditional face-
to-face teaching. These are described in detail in
Section 5. The traditional seminar-style teaching
method underlying the courses mentioned therein
                     July 2002, pp. 19-26.  Association for Computational Linguistics.
              Natural Language Processing and Computational Linguistics, Philadelphia,
         Proceedings of the Workshop on Effective Tools and Methodologies for Teaching
has a number of inherent problems, however. These
problems become particularly pressing when topics
as diverse as linguistic theory, grammar implemen-
tation, parsing, mathematical foundations of linguis-
tic theory and feature logics are combined in a single
course that is addressed to a mixed audience with
varying backgrounds in computer science, knowl-
edge representation, artificial intelligence and lin-
guistics, in any combination of these subjects.
First, the seminar-style teaching format as used in
those grammar implementation courses presupposes
a fairly coherent audience of linguists with a shared
background of linguistic knowledge. Second, since
computers are only used as a medium to implement
grammars and since the implementation platform is
not optimized for web-based training, it is neces-
sary that there be a relatively low number of stu-
dents per teacher. Third, the theoretical material is
in the form of overheads and research papers, which
are in electronic form but not easily accessible with-
out the accompanying lecture as part of a seminar-
style course. Fourth, the background lectures of the
courses lack the support of the kind of graphical,
interactive visualization that teaching software can
in principle offer. Finally, the courses follow a sin-
gle path through the materials as determined by the
teacher, which the student cannot change according
to their specific interests and their prior knowledge.
We believe that these shortcomings can be over-
come by shifting from a seminar-style to a web-
based training format in a way that preserves the
positive aspects of successful hands-on courses. On
the other hand, to successfully shift from seminar-
style to web-based training we believe it is essential
to do this based on a scientific understanding of the
nature and possibilities of web-based learning. In
the next section we therefore embed our work in the
context of education and collaborate learning tech-
nology research.
3 Education and collaborative learning
technology research
Our perspective on web-based training draws its in-
spiration primarily from work in building ?learn-
ing communities? in education research (Lin et al,
1995; Nonaka, 1994), in which:
1. a precise context is established to introduce
tacit knowledge and experience, in this case
on subjects in computational linguistics and the
traditional disciplines it draws from,
2. conflicting perspectives are shared, concepts
are objectified and submitted to a process of
justification and arbitration, and
3. the concepts are then integrated into the knowl-
edge base as modules upon which further in-
structional material or grammar implementa-
tions can be constructed.
We thus intend to provide an environment that
teaches students by actively encouraging them to
participate in research that extends our collective
knowledge in this area. In principle, there are no
boundaries to the material that could be included in
the evolving framework. We intend to make it avail-
able as an open-source standard for grammar de-
velopment and instruction in the hope that this will
encourage researchers and educators to contribute
modules to it, and to use a feature-structure based
approach for their own research and courses.
Scardamalia and Bereiter (1993) identify seven
global characteristics that technologies must have to
support this kind of participation:
Balance: a distinction between public and private
and between individual and group knowledge pro-
cesses. That includes free access to others? work, in-
cluding implementations of concepts as algorithms
or grammars, and opportunities to borrow ideas into
their own work that would be prohibitively time-
consuming or otherwise advanced to formulate on
their own. Such technologies must also encour-
age time for personal ?reflection and refinement?
and anonymous public or private contribution to the
knowledge space. The present framework achieves
this by providing an open-source setting combined
with a web-based instructional tool for self-paced
learning and individual design of both the contents
and order of the curriculum.
Contribution and notification: to prevent ideas
from being presented in an insulated structure that
discourages questioning, debate, or revision. As dis-
cussed in Section 4.2, this is achieved by providing
extensive linking and annotation of resources using
web-compatible metalanguages for integrating mod-
ules at the implementational, formal and instruc-
tional levels.
Source referencing: a means of preserving the
boundaries of a contributor?s idea and its credit as
well as a history of prior accounts and antecedents
to the idea. In the present framework, this is pro-
vided by means of a requirements analysis compo-
nent that requires contributed modules to identify
the contribution by new concepts or resources pro-
vided, existing concepts or resources imported for it
to work, and an account of existing alternatives with
a description of its distinction from them.
Storage and retrieval: which places contribu-
tions in a ?communal context? of related contribu-
tions by others to encourage joint work between con-
tributors working on problems with significant over-
lap. The present framework must organize the pre-
sentation of existing modules along several thematic
dimensions to accomplish this.
Multiple points of entry: for stu-
dents/contributors with different backgrounds
and levels of experience. Material is made acces-
sible in more basic or fundamental modules by
projecting the formal content of the subject into a
graphically based common-sense domain at which
it can be grasped more intuitively (see Section 4.3).
Accessibility in more advanced modules is provided
by links specified in the requirements analysis
component to more basic modules that the former
rely upon.
Coherence-producing mechanisms: feedback
to contributors and framework moderators of mod-
ules that are ?fading? for lack of attention or further
development. These can either be reinstated or refor-
mulated, moved to a private space of more periph-
eral modules, or deleted outright. This is a way of
encouraging activity that is productive, and restrict-
ing the chance of confusion or information overload.
Such a coherence mechanism must exist within this
framework.
Links to external resources: to situate the justifi-
cation and discussion of contributions in a wide con-
text. We make use of the web-based training plat-
form ILIAS1 which is available as open source soft-
ware and offers a high degree of flexibility in terms
of the integration of internal and external resources.
1http://www.ilias.uni-koeln.de/ios/index-e.html
4 Integration of the framework
The goal of our current work is to transform previ-
ous, seminar-style courses and new input into teach-
ing materials that are fit for web-based training in the
general framework outlined in the previous section.
This clearly involves much more than simply refor-
matting old teaching materials into web-compatible
formats. Instead, it requires an analysis of the con-
tents of the courses, the interleaving and hyperlink-
ing of the textual materials, and the development
of graphical, interactive solutions for presenting and
interacting with the content of the material. Since
the nature of the textual material as such is familiar
(instructional notes, reference guides to major sec-
tions with indices, system documentation, annotated
system source code, and annotated grammar source
code), we use the limited space in this paper to high-
light the integrated nature of the approach as well as
the web-based training specific issues of hyperlink-
ing and visualization.
4.1 Integration of linguistic and computational
aspects
Our approach is distinguished by its integration of
grammars, the parsers that use them and the on-
line instructional materials. Compared to the LKB
system2, which as mentioned in Section 5.2 has
also been used successfully in teaching grammar
development, the greater range of formal expres-
sive devices available to our parsing system, called
TRALE, allows for more readable and compact
grammars, which we believe to be of central impor-
tance in a teaching context. To illustrate this, we
are currently porting the LinGO3 English Resource
Grammar (ERG) from the LKB (on which the ERG
was designed) to the TRALE system.
Given the scope of our web-based training frame-
work as including an integrated module on parsing,
it is also relevant that the TRALE system itself can
be relatively compact and transparent at the source-
code level since it exploits its close affinity to the
underlying Prolog on which it is implemented. This
contrasts with the perspective of Copestake et al
(2001), who concede that the LKB is unsuitable for
teaching parsing.
2http://www-csli.stanford.edu/?aac/lkb.html
3http://lingo.stanford.edu/csli/
4.2 The use of hyperlinks
Several different varieties of links are distinguished
within the course material, giving a first-class repre-
sentation to the transfer of knowledge between the
linguistic, computational and mathematical sources
that inform this interdisciplinary area. We intend to
distinguish the following kinds of links:
Conceptual/taxonomical: connecting instances
of key concepts and terms used throughout the
course material with their definitions and prove-
nience;
Empirical context: connecting instances of de-
sign decisions, algorithms and formal definitions to
encyclopedic discussions of their linguistic motiva-
tion and empirical significance;
Denotational: connecting instances of construc-
tional terms and issues within linguistics as well as
correctness conditions of algorithms to the mathe-
matical definitions that formalize them within the
foundations of constraint-based linguistics;
Operational: connecting mathematical defini-
tions and instances of related linguistic discussions
to computational instructional material describing
the algorithms used to construct, refute or transform
the formal objects representing them in a practical
system;
Implementational: connecting discussions of al-
gorithms to the actual annotated system source code
in the TRALE system used to implement them, and
mathematical definitions and discussions of linguis-
tic constructions to the actual annotated grammar
source code used to represent them in a typical im-
plementation.
The idea behind this classification is that when
more course material is added to the web-based
training framework we are proposing, the new mate-
rial will take into account these distinctions to obtain
a conceptually coherent use of hyperlinks through-
out the framework.
4.3 Visualization
Our three core modules make use of a number of
graphical user interfaces: a tool for interleaved vi-
sualization and interaction with trees and attribute
value matrices, one for the presentation of lexical
rules and their interaction, an Emacs-based source-
level debugger, and a program for the graphical ex-
ploration of the formal foundations of typed feature
logic. The first two are extensions of tools we al-
ready used for our previous courses, and the third is
an extension of the ALE source-level debugger, so
we here focus on the last, new development.
The main goal of the MorphMoulder (MoMo) is
to project the formality of its subject, the formal
foundations of constraint languages over typed fea-
ture structures, onto a graphical level at which it can
be grasped more intuitively.4 The transparency of
this level is essential for providing multiple points
of entry (Section 3) to this fundamentally impor-
tant module. The MoMo tool allows the user to
explore the relationship between the two levels of
the formal architecture: the descriptions and the el-
ements described. To this end, the user works with
a graphical interface on a whiteboard. Labeled di-
rected graphs representing feature structures can be
constructed on the whiteboard from their basic com-
ponents, nodes and arcs. The nodes are depicted
as colored balls, which are assigned types, and the
arcs are depicted as arrows that may be labeled by
feature names. Once a feature structure has been
constructed, the user may examine its logical prop-
erties. The three main functions of the MoMo tool
allow one to check (1) whether a feature structure
complies with a given signature, (2) whether a well-
formed feature structure satisfies a description or a
set of descriptions, and (3) whether a well-formed
feature structure is a model of a description or a set
of descriptions. In the context of the course, the
functions of MoMo thus lead the user from under-
standing the well-formedness of feature structures
with respect to a signature to an understanding of
feature structures in their role as a logical model of
a theory. If a student has chosen course modules that
include a focus on formal foundations of feature log-
ics or feature logics based linguistic theory, the first
introduction to the subject by MoMo can easily be
followed up by a course module with rigorous math-
ematical definitions.
In constraint-based frameworks, the user declares
the primitives of the empirical domain in terms of
a type hierarchy with appropriate attributes and at-
tribute values. Consider a signature that licenses
lists of various birds, which may then be classified
according to certain properties. First of all, the sig-
4MoMo is written by Ekaterina Ovchinnikova, U. Tu?bingen.
nature needs to comprise a type hierarchy and fea-
ture appropriateness conditions for lists. Let type list
be an immediate supertype of the types non-empty-
list and empty-list in the type hierarchy (henceforth
abbreviated as nelist and elist). Let the appropri-
ateness conditions declare the attributes HEAD and
TAIL appropriate for (objects of) type nelist, the val-
ues of TAIL at nelist be of type list, and the values
of HEAD at type nelist be of type bird (for lists of
birds). Finally no attributes are appropriate for the
type elist. A typical choice for the interpretation of
that kind of signature in constraint-based formalisms
is the collection of totally well-typed and sort re-
solved feature structures. All nodes of totally well-
typed and sort resolved feature structures are of a
maximally specific type (types with no subtypes);
and they have outgoing arcs for all and only those
features that are appropriate to their type, with the
feature values again obeying appropriateness. Our
signature for lists thus declares an ontology of fea-
ture structures with nodes of type nelist or elist (but
never of type list), where the former must bear the
outgoing arcs HEAD and TAIL, and the latter have no
outgoing arcs. They signal the end of the list. The
HEAD values of non-empty lists must be in the de-
notation of the type bird.
Figure 1 illustrates how the MoMo tool can be
used to study the relationship between signatures
and the feature structures they license by letting
the user construct feature structures and interac-
tively explore whether particular feature structures
are well-formed according to the signature. To the
left of the whiteboard there are two clickable graph-
ics consoles of possible nodes and arcs from which
the user may choose to draw feature structures. The
consoles offer nodes of all maximally specific types
and arcs of all attributes that are declared in the
signature. In the present example, parrot, wood-
pecker, and canary are the maximally specific sub-
types of bird.
Each color of edge represents a different attribute,
and each color of node represents a different type.
The grayed outlines on edges and nodes indicate that
all of the respective edges and nodes in this partic-
ular example are licensed by the signature that was
provided. The HEAD arc originating at the node of
type elist, however, violates the appropriateness con-
ditions of the signature. The feature structure de-
Figure 1: Graphically evaluating well-typedness of
feature structures.
picted here, therefore, is not well-formed. The sig-
nature check thus fails on the given feature structure,
as indicated by the red light in the upper function
console to the right of the whiteboard.
Similarly, MoMo can graphically depict satisfia-
bility and modellability of a single description or set
of descriptions. To this end, the user may be asked to
construct a description that a given feature structure
satisfies or models; or she may be asked to construct
feature structures that satisfy or model a given de-
scription (or set of descriptions). The system will
give systematic feedback on the correct or incorrect
usage of the syntax of the description language as
well as on to which extent a feature structure satis-
fies or models descriptions, systematically guiding
the user to correct solutions.
Figure 2 shows a successful satisfiability check of
a well-formed feature structure. The feature struc-
ture is derived from the one in Figure 1 by re-
moving the incorrect HEAD arc and its substructure
from the elist node. The query, asked in a sepa-
rate window, is whether the feature structure satis-
fies the constraint (nelist, head:(parrot,
color:green), tail:nelist). Since this
is the case, the green light on the function console to
the right is signaling succeed. If we were to perform
model checking of the same feature structure against
the same constraint, checking would fail, and MoMo
would indicate the nodes of the feature structure that
do not satisfy the given constraint.
Figure 2: Graphically evaluating constraint satisfac-
tion of feature structures.
MoMo?s descriptions are a syntactic parallel to
TRALE?s descriptions, thus introducing the student
not only to the syntax and semantics of constraint
languages but also to the language that will be used
for the implementation of grammars later in the
course. The close relationship of description lan-
guages also facilitates a comparison of their model-
theoretic semantics and the truth conditions of gram-
mars with the structure and semantics of algorithms
that use descriptions for constraint resolution and in
parsing. Finally, their common structure allows for a
tight network of hyperlinks across the boundaries of
different course modules and course topics, linking
them to a common source of mathematical, imple-
mentational and linguistic indices, which explain the
usage of common mathematical concepts across the
different areas of application of typed feature struc-
tures.
5 From seminar-style courses to
web-based training
Having discussed the ideas driving the web-based
teaching platform and exemplified one of the tools,
we now return to the courses which have informed
our work on the three core modules currently being
developed in terms of their content and the use of a
web- and implementation environment they make.
5.1 Grammar implementation in ALE
ALE5 (Carpenter and Penn, 1996) is a conserva-
tive extension of Prolog based on typed feature
structures, with a built-in parser and semantic-head-
driven generator. The demand for such a utility
was so great when it was beta-released in 1992
that it immediately became the subject of early
work in graphical front-end development for large
constraint-based grammars: first with the Pleuk sys-
tem (Calder, 1993), then as one of several systems
supported by Gertjan van Noord?s HDrug6, followed
by an ALE-mode Emacs user interface (Laurens,
1995). It also provided the computational support
for one of the very first web-based computational
linguistics courses, Colin Matheson?s widely used
HPSG Development in ALE7. A follow-up course on
computational morphology8, also by Colin Mathe-
son, was based on ALE-RA9, a morphological ex-
tension of ALE by Tomaz Erjavec.
Our current web-based training module is sup-
ported by an extension of ALE, called TRALE,
that uses a slightly different interpretation of typing
found in many linguistic theories and an enhanced
constraint language that supports constraints with
complex antecedents (Penn, 2000).
5http://www.cs.toronto.edu/?gpenn/ale.html
6http://grid.let.rug.nl/?vannoord/hdrug/
7http://www.ltg.hcrc.ed.ac.uk/projects/ledtools/ale-hpsg/
8http://www.ltg.ed.ac.uk/projects/ledtools/ale-ra/
9http://nl.ijs.si/et/Thesis/ALE-RA/
5.2 Constraint-based grammar
implementation
Over the past five years, we have held another course
on Constraint-Based Grammar Implementation in
a variety of settings, from summer schools to reg-
ular curriculum courses.10 It offers hands-on ex-
perience to linguists interested in the formalization
of linguistic knowledge in a constraint-based gram-
mar formalism. The course is taught in an interac-
tive fashion in a computer laboratory and combines
background lectures with practical exercises on how
to specify grammars in ConTroll11 (Go?tz and Meur-
ers, 1997), a processing system for constraint-based
grammars intended to process with HPSG theories
directly from the form in which they are constructed
by linguists.
The background lectures of the Constraint-based
grammar implementation courses introduce the rel-
evant mathematical and computational knowledge
and focus on the main ingredients of constraint-
based grammars: highly structured lexical represen-
tations, constituent structures, and the encoding of
well-formedness constraints on grammatical repre-
sentations. In the lab, students work on exercises
exploring the theoretical concepts covered in the lec-
tures. In a later part of the course, they are given
the opportunity to undertake individualized gram-
mar projects for modeling theoretically and empir-
ically significant syntactic constructions of their na-
tive language.
This course was the first hands-on computational
syntax course at the European Summer School
in Language, Logic, and Information (ESSLLI,
1997: Aix-en-Provence), and was also offered at the
LSA Linguistic Institute (1999: University of Illi-
nois, Urbana-Champaign)12 and the Computational
Linguistics and Represented Knowledge (CLaRK)
Summer School (1999: Eberhard-Karls Universita?t,
Tu?bingen)13. Generally regarded as a highly suc-
cessful course and teaching method, every subse-
quent ESSLLI summer school has offered at least
one similar course: Practical HPSG Grammar Engi-
neering (1998: Ann Copestake, Dan Flickinger, and
10The courses were taught by E. Hinrichs and D. Meurers.
11http://www.sfs.uni-tuebingen.de/controll/
12http://ling.osu.edu/?dm/lehre/lsa99/
13http://ling.osu.edu/?dm/lehre/clark99/
Stephan Oepen)14, Development of large scale LFG
grammars: Linguistics, Engineering and Resources
(1999: Miriam Butt, Annette Frank, and Jonas
Kuhn)15, Grammatical Resources: Logic, Struc-
ture, Control (1999: Michael Moortgat and Richard
T. Oehrle)16, An Introduction to Grammar Engi-
neering using HPSG (2000: Ann Copestake, Rob
Malouf)17, Advanced Grammar Engineering using
HPSG (2000: Dan Flickinger, Stephan Oepen)18,
and An Introduction to Stochastic Attribute-Value
Grammars (2001: Rob Malouf, Miles Osborne)19.
5.3 Introduction to theory-driven CL
A further source of material for the core modules
of our web-based training framework is the graduate
level Introduction to Theory-driven Computational
Linguistics at the Ohio State University.20 It covers
the basic issues of the following topics: finite state
automata and transducers, formal language theory,
computability and complexity, recognizers/parsers
for context free grammars, memoization, and pars-
ing with complex categories.
The theoretical material is combined with prac-
tical exercises in Prolog implementing different as-
pects of parsers. At the end of the course, students
complete a project consisting of building and testing
a grammar fragment for a short English text of their
choice. The traditional one-quarter course includes
weekly exercises, extensive web-based course mate-
rial for students, and a course workbook21 as a guide
through the theoretical material.
5.4 Model-theoretic introduction to Syntax
Our approach to teaching the fundamentals of math-
ematical theories through graphical metaphors in
the context of syntax derives from our experience
with this method in teaching Syntax I (HPSG) at
the Eberhard-Karls Universita?t Tu?bingen in 1998,
14http://www.coli.uni-sb.de/esslli/Seiten/Oepen.html
15http://www.let.uu.nl/esslli/Courses/butt.html
16http://www.let.uu.nl/esslli/Courses/moortgat-oehrle.html
17http://www.cs.bham.ac.uk/?esslli/notes/copestake.html
18http://www.cs.bham.ac.uk/?esslli/notes/oepen.html
19http://odur.let.rug.nl/?malouf/esslli01/
20The course was taught by D. Meurers; see http://ling.osu.
edu/?dm/2001/winter/684.01/
21This workbook is based, with kind permission from the
authors, on the module workbook for ?Techniques in Natural
Language Processing 1? by Chris Mellish, Pete Whitelock and
Graeme Ritchie, 1994, Dept. of AI, University of Edinburgh.
1999 and 2001.22 In these seminars, which did not
presuppose any prior knowledge of model-theoretic
methods in logic, the mathematical foundations of
feature logic were introduced by intuitive means but
with as much precision as possible without strict for-
malization. An introduction to a standardized ver-
sion of the logical description language of HPSG
was accompanied with problem sets that required
the students to construct three-dimensional feature
structure models (made of styrofoam and wires) of
descriptions and sets of descriptions. The informal
but very concrete understanding of the relationship
between a theory cast in a constraint language and its
feature structure models had a very positive result on
students? ability to grasp and build working analyses
of unseen constructions compared to the results of
the more traditional method of teaching constraint-
based syntax used in previous years. At the same
time, the teaching method successfully used an ap-
peal to prior world knowledge rather than unfamiliar
mathematical notation in order to make the students
familiar with the basic concepts of constraint satis-
faction and truth in feature logics.
6 Summary and Outlook
The interdisciplinary nature of computational lin-
guistics and the diverse backgrounds of the student
audience makes it particularly attractive to teach a
subject like constraint-based grammar formalisms
and parsing using a web-based instructional plat-
form which integrates formal and computational
foundations, linguistic theory, and grammar im-
plementation. We discussed several seminar-style
courses which have informed our proposal in terms
of content, highlighted the problems of the tradi-
tional face-to-face teaching, and described our en-
vironment of web-based teaching materials plus im-
plementational support. We argued that a web-based
training framework for the topic can be organized
around feature structures as a central data structure
in formal foundations, linguistics and implementa-
tion. We outlined the educational and collaborative
learning background in which an informed proposal
on web-based training must be embedded and used
the newly developed tool MoMo as an illustration
22The courses were taught by F. Richter and M. Sailer; see
http://www.sfs.uni-tuebingen.de/?fr/teaching/
of how we envisage projecting the formal content of
the subject into a graphically based common-sense
domain in which it can be grasped more intuitively.
The three core modules on formal founda-
tions, constraint-based grammar implementation,
and parsing will be completed and made publicly
available at the end of 2003. The joint project
is funded by the German Federal Ministry for Re-
search Technology (BMBF) as part of the consor-
tium Media-intensive teaching modules in the com-
putational linguistics curriculum (MiLCA).23
References
J. Calder. 1993. Graphical interaction with constraint-
based grammars. In Proceedings of PACLING ?93,
pages 160?168, Vancouver, British Columbia.
B. Carpenter and G. Penn. 1996. Compiling typed
attribute-value logic grammars. In H. Bunt and
M. Tomita, editors, Recent Advances in Parsing Tech-
nologies, pages 145?168. Kluwer, Dordrecht.
A. Copestake, J. Carroll, D. Flickinger, R. Malouf, and
S. Oepen. 2001. Using an open-source unification-
based system for CL/NLP teaching. In Proceedings
of the EACL/ACL Workshop on Sharing Tools and Re-
sources for Research and Education, pages 35?38.
T. Go?tz and W. D. Meurers. 1997. The ConTroll system
as large grammar development platform. In Proceed-
ings of the EACL/ACL Workshop on Computational
Environments for Grammar Development and Linguis-
tic Engineering, pages 38?45. http://ling.osu.edu/?dm/
papers/envgram.html.
O. Laurens. 1995. An Emacs user interface for ALE.
Technical Report CSS-IS TR 95-07, School of Com-
puting Science, Simon Fraser University.
X. Lin, J.D. Bransford, and C.E. Hmelo. 1995. Instruc-
tional design and development of learning communi-
ties: an invitation to dialogue. Educational Technol-
ogy, 35(5):53?63.
I. Nonaka. 1994. A dynamic theory of organizational
knowledge creation. Organizational Science, 5(1).
G. Penn. 2000. Applying Constraint Handling Rules
to HPSG. In Proceedings of the Workshop on Rule-
Based Constraint Reasoning and Programming, CL
2000.
M. Scardamalia and C. Bereiter. 1993. Technologies
for knowledge-building discourse. Communications
of the ACM, 36(5):37?41.
23http://milca.sfs.uni-tuebingen.de/A4/HomePage/top.html
Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 15?22,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
?Language and Computers?
Creating an Introduction for a General Undergraduate Audience
Chris Brew
Department of Linguistics
The Ohio State University
cbrew@ling.osu.edu
Markus Dickinson
Department of Linguistics
The Ohio State University
dickinso@ling.osu.edu
W. Detmar Meurers
Department of Linguistics
The Ohio State University
dm@ling.osu.edu
Abstract
This paper describes the creation
of Language and Computers, a new
course at the Ohio State University de-
signed to be a broad overview of topics
in computational linguistics, focusing
on applications which have the most
immediate relevance to students. This
course satisfies the mathematical and
logical analysis requirement at Ohio
State by using natural language sys-
tems to motivate students to exercise
and develop a range of basic skills in
formal and computational analysis. In
this paper we discuss the design of the
course, focusing on the success we have
had in offering it, as well as some of the
difficulties we have faced.
1 Introduction
In the autumn of 2003, we created Language
and Computers (Linguistics 384), a new course
at the Ohio State University that is designed to
be a broad overview of topics in computational
linguistics, focusing on applications which have
the most immediate relevance to students. Lan-
guage and Computers is a general enrollment
course designed to meet the Mathematical and
Logical Analysis requirement that is mandated
for all undergraduates at the Ohio State Uni-
versity (OSU), one of the largest universities in
the US. We are committed to serving the av-
erage undergraduate student at OSU, including
those for whom this is the first and last Lin-
guistics course. Some of the students take the
course because it is an alternative to calculus,
others because of curiosity about the subject
matter. The course was first taught in Win-
ter 2004, drawing a wide range of majors, and
has since expanded to three sections of up to 35
students each. In this paper we will discuss the
design of the course, focusing on the success we
have had in offering it, as well as some of the
difficulties we have faced.
2 General Context
The Linguistics Department at OSU is the home
of a leading graduate program in which 17 grad-
uate students are currently specializing in com-
putational linguistics. From the perspective of
the graduate program, the goal of the new course
development was to create more appropriate
teaching opportunities for the graduate students
specializing in computational linguistics. Much
of the undergraduate teaching load in Linguis-
tics at OSU is borne by graduate teaching assis-
tants (GTAs) who receive stipends directly from
the department. After a training course in the
first year, most such GTAs act as instructors on
the Department?s ?Introduction to Language,?
which is taught in multiple small sections. In-
structors are given considerable responsibility
for all aspects of course design, preparation, de-
livery, and grading. This works very well and
produces many superb instructors, but by 2003
it was apparent that increasing competition was
reducing the pool of undergraduates who want
to take this general overview course.
The Ohio State University has a distribution
requirement, the General Education Curricu-
15
lum (GEC), that is designed to ensure adequate
breadth in undergraduate education. The twin
demands of the student?s major and the distri-
bution requirement are sufficient to take up the
vast majority of the credit hours required for
graduation. In practice this means that students
tend to make course selections motivated pri-
marily by the goal of completing the necessary
requirements as quickly and efficiently as they
can, possibly at the expense of curiosity-driven
exploration. Linguistics, as an interdisciplnary
subject, can create courses that satisfy both cu-
riosity and GEC requirements.
To fill this interdisciplinary niche, the OSU
Department of Linguistics has created a range
of new courses such as Language and Gender,
Language and the Mind, Language and the Law,
and the Language and Computers course dis-
cussed in this paper. In addition to filling a dis-
tribution requirement niche for undergraduates,
the courses also allow the linguistics GTAs to
teach courses on topics that are related to their
area of specialization, which can be beneficial
both to the instructors and to those instructed.
Prior to creation of the new Language and Com-
puters course, there were virtually no opportu-
nities for student members of the computational
linguistics group to teach material close to their
focus.
3 Course overview
The mission statement for our course reads:
In the past decade, the widening use
of computers has had a profound influ-
ence on the way ordinary people com-
municate, search and store informa-
tion. For the overwhelming majority
of people and situations, the natural
vehicle for such information is natu-
ral language. Text and to a lesser ex-
tent speech are crucial encoding for-
mats for the information revolution.
This course will give students insight
into the fundamentals of how comput-
ers are used to represent, process and
organize textual and spoken informa-
tion, as well as providing tips on how
to effectively integrate this knowledge
into their working practice. The course
will cover the theory and practice of
human language technology.
The course was designed to meet the Math-
ematical and Logical Analysis (MLA) require-
ment for students at the Ohio State University,
which is characterized in the following way:
A student in a B.A. program must take
one course that focuses on argument in
a context that emphasizes natural lan-
guage, mathematics, computer science
or quantitative applications not pri-
marily involving data. Courses which
emphasize the nature of correct argu-
mentation either in natural languages
or in symbolic form would satisfy this
requirement, as would many mathe-
matics or computer science courses.
. . . The courses themselves should em-
phasize the logical processes involved
in mathematics, inductive or deductive
reasoning, or computing and the the-
ory of algorithms.
Linguistics 384 responds to this specification
by using natural language systems to motivate
students to exercise and develop a range of ba-
sic skills in formal and computational analysis.
The course combines lectures with group work
and in-class discussions, resulting in a seminar-
like environment. We enrol no more than 35
students per section, often significantly fewer at
unpopular times of day.
The course philosophy is to ground abstract
concepts in real world examples. We intro-
duce strings, regular expressions, finite-state
and context-free grammars, as well as algo-
rithms defined over these structures and tech-
niques for probing and evaluating systems that
rely on these algorithms. This meets the MLA
objective to emphasize the nature of correct ar-
gumentation in symbolic form as well as the logi-
cal processes involved in computing and the the-
ory of algorithms. These abstract ideas are em-
bedded in practical applications: web searching,
16
spelling correction, machine translation and di-
alogue systems. By covering the technologies
behind these applications, the course addresses
the requirement to sharpen a student?s ability
to reason critically, construct valid arguments,
think creatively, analyze objectively, assess ev-
idence, perceive tacit assumptions, and weigh
evidence.
Students have impressions about the quality
of such systems, but the course goes beyond
merely subjective evaluation of systems and em-
phasizes the use of formal reasoning to draw and
argue for valid conclusions about the design, ca-
pabilities and behavior of natural language sys-
tems.
In ten weeks, we cover eight topics, using a
data projector in class, with copies of the slides
being handed out to the student before each
class. There is no textbook, and there are rel-
atively few assigned readings, as we have been
unable to locate materials appropriate for an av-
erage student without required background who
may never take another (computational) linguis-
tics class. The topics covered are the following,
in this order:
? Text and speech encoding
? (Web-)Searching
? Spam filtering (and other classification
tasks, such as language identification)
? Writers? aids (Spelling and grammar correc-
tion)
? Machine translation (2 weeks)
? Dialogue systems (2 weeks)
? Computer-aided language learning
? Social context of language technology use
In contrast to the courses of which we are
aware that offer computational linguistics to un-
dergraduates, our Language and Computers is
supposed to be accessible without prerequisites
to students from every major (a requirement for
GEC courses). For example, we cannot assume
any linguistic background or language aware-
ness. Like Lillian Lee?s Cornell course (Lee,
2002), the course cannot presume programming
ability. But the GEC regulations additionally
prohibit us from requiring anything beyond high
school level abilities in algebraic manipulation.
We initially hoped that this meant that we
would be able to rely on the kind of math knowl-
edge that we ourselves acquired in secondary
school, but soon found that this was not real-
istic. The sample questions from Lee?s course
seem to us to be designed for students who ac-
tively enjoy math. Our goal is different: we
want to exercise and extend the math skills of
the general student population, ensuring that
the course is as accessible to the well-motivated
dance major as it is to the geekier people with
whom we are somewhat more familiar. This is
hard, but worthwhile.
The primary emphasis is on discrete math-
ematics, especially with regard to strings and
grammars. In addition, the text classification
and spam-filtering component exercise the abil-
ity to reason clearly using probabilities. All of
this can be achieved for students with no colle-
giate background in mathematics.
Specifically, Linguistics 384 uses non-trivial
mathematics at a level at or just beyond algebra
1 in the following contexts:
? Reasoning about finite-state automata and
regular expressions (in the contexts of web
searching and of information management).
Students reason about relationships be-
tween specific and general search terms.
? Reasoning about more elaborate syntactic
representations (such as context-free gram-
mars) and semantic representations (such
as predicate calculus), in order to better
understand grammar checking and machine
translation errors.
? Reasoning about the interaction between
components of natural language systems (in
the contexts of machine translation and of
dialog systems).
? Understanding the basics of dynamic pro-
gramming via spelling correction (edit dis-
tance) and applying algebraic thinking to
algorithm design.
17
? Simple probabilistic reasoning (in the con-
text of text classification).
There is also an Honors version of the course,
which is draws on a somewhat different pool
of students. In 2004 the participants in Hon-
ors 384 were equally split between Linguistics
majors looking for a challenging course, people
with a computer background and some interest
in language and people for whom the course was
a good way of meeting the math requirement at
Honors level. Most were seniors, so there was lit-
tle feed-through to further Linguistics courses.
The Honors course, which used to be
called Language Processing Technology, pre-
dates Language and Computers, and includes
more hands-on material. Originally the first half
of this course was an introduction to phonetics
and speech acoustics through Praat, while the
second was a Prolog-based introduction to sym-
bolic NLP. We took the opportunity to redesign
this course when we created the non-honors ver-
sion. In the current regime, the hands-on aspect
is less important than the opportunities offered
by the extra motivation and ability of these stu-
dents. Two reading assignments in the honors
version were Malcolm Gladwell?s book review on
the Social Life of Paper (Gladwell, 2001) and
Turing?s famous paper on the Imitation Game
(Turing, 1950). We wondered whether the ec-
centricity and dated language of the latter would
be a problem, but it was not.
Practical assignments in the laboratory are
possible in the honors course, because the class
size can be limited. One such assignment was
a straightforward run-through of the clock tu-
torial from the Festival speech synthesis system
and another a little machine translation system
between digits and number expressions. Having
established that they can make a system that
turns 24 into ?twenty four?, and so on, the stu-
dents are challenged to adapt it to speak ?Fairy
Tale English?: that is, to make it translate 24
into ?four and twenty?, and vice-versa.
1
1For a complete overview of the course materials,
there are several course webpages to check out. The web-
page for the first section of the course (Winter 2004)
4 General themes of the course
Across the eight different topics that are taught,
we try to maintain a cohesive feel by emphasiz-
ing and repeating different themes in computa-
tional linguistics. Each theme allows the stu-
dents to see that certain abstract ideas are quite
powerful and can inform different concrete tasks.
The themes which have been emphasized to this
point are as follows:
? There are both statistical and rule-based
methods for approaching a problem in nat-
ural language processing. We show this
most clearly in the spam filtering unit and
the machine translation unit with different
types of systems.
? There is a tension between developing tech-
nology in linguistically-informed ways and
developing technology so that a product is
effective. In the context of dialogue sys-
tems, for example, the lack of any linguistic
knowledge in ELIZA makes it fail quickly,
but an ELIZA with a larger database and
still no true linguistic knowledge could have
more success.
? Certain general techniques, such as n-gram
analysis, can be applied to different compu-
tational linguistic applications.
? Effective technology does not have to solve
every problem; focusing on a limited do-
main is typically more practical for the ap-
plications we look at. In machine transla-
tion, this means that a machine translation
system translating the weather (e.g., the
METEO system) will perform better than
a general-purpose system.
? Intelligent things are being done to improve
natural language technology, but the task is
a very difficult one, due to the complexities
of language. Part of each unit is devoted to
is at http://ling.osu.edu/~dickinso/384/wi04/. A
more recent section (Winter 2005) can be found at http:
//ling.osu.edu/~dm/05/winter/384/. For the honors
course, the most recent version is located at http:
//ling.osu.edu/~cbrew/2005/spring/H384/. A list of
weblinks to demos, software, and on-line tutorials cur-
rently used in connection with the course can be found
at http://ling.osu.edu/~xflu/384/384links.html
18
showing that the problem the technology is
addressing is a complex one.
5 Aspects of the course that work
The course has been a positive experience, and
students overall seemed pleased with it. This
is based on the official student evaluation of
instruction, anonymous, class specific question-
naires we handed out at the end of the class,
personal feedback, and new students enrolling
based on recommendations from students who
took the course. We attribute the positive re-
sponse to several different aspects of the course.
5.1 Topics they could relate to
Students seem to most enjoy those topics which
were most relevant to their everyday life. On the
technological end, this means that the units on
spam filtering, web searching, and spell check-
ing are generally the most well-received. The
more practical the focus, the more they seem
to appreciate it; for web searching, for instance,
they tend to express interest in becoming better
users of the web. On the linguistic end, discus-
sions of how dialogue works and how language
learning takes place, as part of the units on di-
alogue systems and CALL, respectively, tend to
resonate with many students. These topics are
only sketched out insofar as they were relevant
to the NLP technology in question, but this has
the advantage of not being too repetitive for the
few students who have had an introductory lin-
guistics class before.
5.2 Math they can understand
Students also seem to take pride in being able
to solve what originally appear to be difficult
mathematical concepts. To many, the concept
and look of a binary number is alien, but they
consistently find this to be fairly simple. The
basics of finite-state automata and boolean ex-
pressions (even quite complicated expressions)
provide opportunities for students to understand
that they are capable of learning concepts of log-
ical thinking. Students with more interest and
more of an enjoyment for math are encouraged
to go beyond the material and, e.g., figure out
the nature of more complicated finite-state au-
tomata. In this way, more advanced students are
able to stay interested without losing the other
students.
More difficult topics, such as calculating the
minimum edit distance between a word and its
misspelling via dynamic programming, can be
frustrating, but they just as often are a source
of a greater feeling of success for students. After
some in-class exercises, when it becomes appar-
ent that the material is learnable and that there
is a clear, well-motivated point to it, students
generally seem pleased in conquering somewhat
more difficult mathematical concepts.
5.3 Interactive demos
In-class demos of particular software are also
usually well-received, in particular when they
present applications that students themselves
can use. These demos often focus on the end
result of a product, such as simply listening to
the output of several text-to-speech synthesiz-
ers, but they can also be used for understanding
how the applications works. For example, some
sections attempt to figure out as a class where
a spelling checker fails and why. Likewise, an
in-class discussion with ELIZA has been fairly
popular, and students are able to deduce many
of the internal properties of ELIZA.
5.4 Fun materials
In many ways, we have tried to keep the tone
of the course fairly light. Even though we
are teaching mathematical and logical concepts,
these concepts are still connected to the real
world, and as such, there is much opportunity
to present the material in a fun and engaging
manner.
Group work One such way to make the learn-
ing process more enjoyable was to use group
work. In the past few quarters, we have been
refining these exercises. Because of the nature
of the topics, some topics are easier to derive
group exercises for than others. The more math-
ematical topics, such as regular expressions, suit
themselves well for straightforward group work
on problem sets in class; others can be more
19
creative. The group exercises usually serve as a
way for students to think about issues they al-
ready know something about, often as a way to
introduce the topic.
For example, on the first day, they are given
a sheet and asked to evaluate sets of opposing
claims, giving arguments for both sides, such as
the following:
1. A person will have better-quality papers if
they use a spell checker.
A person will have worse-quality papers if
they use a spell checker.
2. An English-German dictionary is the main
component needed to automatically trans-
late from English to German.
An English-German dictionary is not the
main component needed to automatically
translate from English to German.
3. Computers can make you sound like a na-
tive speaker of another language.
Computers cannot make you sound like a
native speaker of another language.
To take another example, to get students
thinking about the social aspects of the use of
language technology, they are asked in groups to
consider some of the implications of a particu-
lar technology. The following is an excerpt from
one such handout.
You work for a large software company
and are in charge of a team of com-
putational linguists. One day, you are
told: ?We?d like you and your team to
develop a spell checker for us. Do you
have any questions?? What questions
do you have for your boss?
...
Somehow or another, the details of
your spell checker have been leaked to
the public. This wouldn?t be too bad,
except that it?s really ticked some lin-
guists off. ?It?s just a big dictionary!?
they yell. ?It?s like you didn?t know
anything about morphology or syntax
or any of that good stuff.? There?s
a rumor that they might sue you for
defamation of linguistics. What do you
do?
Although the premise is somewhat ridiculous,
with such group work, students are able to con-
sider important topics in a relaxed setting. In
this case, they have to first consider the speci-
fications needed for a technology to work (who
will be using it, what the expectations are, etc.)
and, secondly, what the relationship is between
the study of language and designing a product
which is functional.
Fun homework questions In the home-
works, students are often instructed to use a
technology on the internet, or in some way to
take the material presented in class a step far-
ther. Additionally, most homework assignments
had at least one lighter question which allowed
students to be more creative in their responses
while at the same time reinforcing the material.
For example, instructors have asked students
to send them spam, and the most spam-worthy
message won a prize. Other homework ques-
tions have included sketching out what it would
take to convert an ELIZA system into a hostage
negotiator?and what the potential dangers are
in such a use. Although some students put down
minimal answers, many students offer pages of
detailed suggestions to answer such a question.
This gives students a taste of the creativity in-
volved in designing new technology without hav-
ing to deal with the technicalities.
6 Challenges for the course
Despite the positive response, there are several
aspects to the course which have needed im-
provement and continue to do so. Teaching
to a diverse audience of interests and capabili-
ties presents obstacles which are not easily over-
come. To that end, here we will review aspects
of the course which students did not generally
enjoy and which we are in the process of adapt-
ing to better suit our purposes and our students?
needs.
20
6.1 Topics they do not relate to
For such a range of students, there is the diffi-
culty of presenting abstract concepts. Although
we try to relate everything to something which
students actually use or could readily use, we
sometimes include topics from computational
linguistics that make one better able to think
logically in general and which we feel will be
of future use for our students. One such topic
is that of regular expressions, in the context of
searching for text in a document or corpus. As
most students only experience searching as part
of what they do on the web, and no web search
engine (to the best of our knowledge) currently
supports regular expression searching, students
often wonder what the point of the topic is. In
making most topics applicable to everyday life,
we had raised expect. In this particular case,
students seemed to accept regular expressions
more once it they saw that Microsoft Word has
something roughly analogous.
Another difficulty that presented itself for a
subset of the students was that of using for-
eign language text to assist in teaching ma-
chine translation and computer-aided language
learning. Every example was provided with an
English word-by-word gloss, as well as a para-
phrase, yet the examples can still be difficult to
understand without a basic appreciation for the
relevant languages. If the students know Span-
ish, the example is in Spanish and the instruc-
tor has a decent Spanish accent, things can go
well. But students tend to blame difficulties in
the machine translation homework on not know-
ing the languages used in the examples. Under-
standing the distinction between different kinds
of machine translation systems requires some
ability to grasp how languages can differ, so we
certainly must (unless we use proxies like fairy-
tale English) present some foreign material, but
we are in dire need of means to do this as gently
as possible
6.2 Math they do not understand
While some of the more difficult mathemati-
cal concepts were eventually understood, oth-
ers continued to frustrate students. The al-
ready mentioned regular expressions, for exam-
ple, caused trouble. Firstly, even if you do
understand them, they are not necessarily life-
enhancing, unless you are geeky enough to write
your papers in a text editor that properly sup-
ports them. Secondly, and more importantly,
many students saw them as unnecessarily ab-
stract and complex. For instance, some stu-
dents were simply unable to understand the no-
tion that the Kleene star is to be interpreted as
an operator rather than as a special character
occurring in place of any string.
Even though we thought we had calibrated
our expectations to respect the fact that our
students knew no math beyond high school, the
amount that they had retained from high school
was often less than we expected. For exam-
ple, many students behaved exactly as if they
had never seen Venn diagrams before, so time
had to be taken away from the main material
in order to explain them. Likewise, figuring
out how to calculate probabilities for a bag of
words model of statistical machine translation
required a step-by-step explanation of where
each number comes from. A midterm ques-
tion on Bayesian spam filtering needed the same
treatment, revealing that even good students
may have significant difficulties in deploying the
high school math knowledge they almost cer-
tainly possess.
6.3 Technology which did not work
Most assignments required students to use the
internet or the phone in some capacity, usu-
ally to try out a demo. With such tasks, there
is always the danger that the technology will
not work. For example, during the first quar-
ter the course was taught, students were asked
to call the CMU Communicator system and in-
teract with it, to get a feel for what it is like
to interact with a computer. As it turns out,
halfway through the week the assignment was
due, the system was down, and thus some stu-
dents could not finish the exercise. Follow-
ing this episode, homework questions now come
with alternate questions. In this case, if the sys-
tem is down, the first alternate is to listen to a
pre-recorded conversation to see how the Com-
21
municator works. Since some students are un-
able to listen to sounds in the campus computer
labs, the second alternate is to read a transcript.
Likewise, students were instructed to view the
page source code for ELIZA. However, some
campus computer labs at OSU do not allow stu-
dents to view the source of a webpage. In re-
sponse to this, current versions of the assign-
ment have a separate webpage with the source
code written out as plain text, so all students
can view it.
One final note is that students have often com-
plained of weblinks failing to work, but this ?fail-
ure? is most often due to students mistyping
the link provided in the homework. Providing
links directly on the course webpage or including
them in the web- or pdf-versions of the home-
work sheets is the simplest solution for this prob-
lem.
7 Summary and Outlook
We have described the course Language and
Computers (Linguistics 384), a general introduc-
tion to computational linguistics currently being
taught at OSU. While there are clear lessons
to be learned for developing similar courses at
other universities, there are also more general
points to be made. In courses which assume
some CS background, for instance, it is still
likely the case that students will want to see
some practical use of what they are doing and
learning.
There are several ways in which this course
can continue to be improved. The most pressing
priority is to develop a course packet and pos-
sibly a textbook. Right now, students rely only
on the instructor?s handouts, and we would like
to provide a more in-depth and cohesive source
of material. Along with this, we want to de-
velop a wider range of readings for students (e.g.
Dickinson, to appear) to provide students with
a wider variety of perspectives and explanations
for difficult concepts.
To address the wide range of interests and ca-
pabilities of the students taking this course as a
general education requirement, it would be good
to tailor some of the sections to audiences with
specific backgrounds?but given the lack of a
dedicated free time slot for all students of a par-
ticular major, etc., it is unclear whether this is
feasible in practice.
We are doing reasonably well in integrating
mathematical thinking into the course, but we
would like to give students more experience of
thinking about algorithms. Introducing a ba-
sic form of pseudocode might go some way to-
wards achieving this, provided we can find a mo-
tivating linguistic example that is both simple
enough to grasp and complex enough to justify
the overhead of introducing a new topic. Fur-
ther developments might assist us in developing
a course between Linguistics 384 and Linguistics
684, our graduate-level computational linguis-
tics course, as we currently have few options for
advanced undergraduates.
Acknowledgements We would like to thank
the instructors of Language and Computers for
their discussions and insights into making it a
better course: Stacey Bailey, Anna Feldman, Xi-
aofei Lu, Crystal Nakatsu, and Jihyun Park. We
are also grateful to the two ACL-TNLP review-
ers for their detailed and helpful comments.
References
Markus Dickinson, to appear. Writers? Aids. In
Keith Brown (ed.), Encyclopedia of Language
and Linguistics. Second Edition, Elsevier, Ox-
ford.
Malcolm Gladwell, 2001. The Social Life
of Paper. New Yorker . available from
http://www.gladwell.com/archive.html.
Lillian Lee, 2002. A non-programming introduc-
tion to computer science via NLP, IR, and
AI. In ACL Workshop on Effective Tools
and Methodologies for Teaching Natural Lan-
guage Processing and Computational Linguis-
tics. pp. 32?37.
A.M. Turing, 1950. Computing Machinery and
Intelligence. Mind , 59(236):433?460.
22
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 107?115,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Diagnosing meaning errors in
short answers to reading comprehension questions
Stacey Bailey
Department of Linguistics
The Ohio State University
1712 Neil Avenue
Columbus, Ohio 43210, USA
s.bailey@ling.osu.edu
Detmar Meurers
Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
Wilhelmstrasse 19
72074 Tu?bingen, Germany
dm@sfs.uni-tuebingen.de
Abstract
A common focus of systems in Intelli-
gent Computer-Assisted Language Learning
(ICALL) is to provide immediate feedback to
language learners working on exercises. Most
of this research has focused on providing feed-
back on the form of the learner input. Foreign
language practice and second language acqui-
sition research, on the other hand, emphasizes
the importance of exercises that require the
learner to manipulate meaning.
The ability of an ICALL system to diag-
nose and provide feedback on the mean-
ing conveyed by a learner response depends
on how well it can deal with the response
variation allowed by an activity. We focus
on short-answer reading comprehension ques-
tions which have a clearly defined target re-
sponse but the learner may convey the mean-
ing of the target in multiple ways. As empiri-
cal basis of our work, we collected an English
as a Second Language (ESL) learner corpus
of short-answer reading comprehension ques-
tions, for which two graders provided target
answers and correctness judgments. On this
basis, we developed a Content-Assessment
Module (CAM), which performs shallow se-
mantic analysis to diagnose meaning errors. It
reaches an accuracy of 88% for semantic error
detection and 87% on semantic error diagno-
sis on a held-out test data set.
1 Introduction
Language practice that includes meaningful interac-
tion is a critical component of many current lan-
guage teaching theories. At the same time, exist-
ing research on intelligent computer-aided language
learning (ICALL) systems has focused primarily on
providing practice with grammatical forms. For
most ICALL systems, although form assessment of-
ten involves the use of natural language processing
(NLP) techniques, the need for sophisticated con-
tent assessment of a learner response is limited by
restricting the kinds of activities offered in order to
tightly control the variation allowed in learner re-
sponses, i.e., only one or very few forms can be used
by the learner to express the correct content. Yet
many of the activities that language instructors typ-
ically use in real language-learning settings support
a significant degree of variation in correct answers
and in turn require both form and content assess-
ment for answer evaluation. Thus, there is a real
need for ICALL systems that provide accurate con-
tent assessment.
While some meaningful activities are too unre-
stricted for ICALL systems to provide effective con-
tent assessment, where the line should be drawn on
a spectrum of language exercises is an open ques-
tion. Different language-learning exercises carry
different expectations with respect to the level and
type of linguistic variation possible across learner
responses. In turn, these expectations may be linked
to the learning goals underlying the activity design,
the cognitive skills required to respond to the ac-
tivity, or other properties of the activity. To de-
velop adequate processing strategies for content as-
sessment, it is important to understand the connec-
tion between exercises and expected variation, as
conceptualized by the exercise spectrum shown in
Figure 1, because the level of variation imposes re-
107
Tightly Restricted Responses Loosely Restricted Responses
Decontextualized 
grammar fill-in-
the-blanks
Short-answer reading 
comprehension 
questions
Essays on 
individualized 
topics
The Middle Ground
Viable Processing Ground
Figure 1: Language Learning Exercise Spectrum
quirements and limitations on different processing
strategies. At one extreme of the spectrum, there are
tightly restricted exercises requiring minimal analy-
sis in order to assess content. At the other extreme
are unrestricted exercises requiring extensive form
and content analysis to assess content. In this work,
we focus on determining whether shallow content-
analysis techniques can be used to perform content
assessment for activities in the space between the
extremes. A good test case in this middle ground
are loosely restricted reading comprehension (RC)
questions. From a teaching perspective, they are a
task that is common in real-life learning situations,
they combine elements of comprehension and pro-
duction, and they are a meaningful activity suited
to an ICALL setting. From a processing perspec-
tive, responses exhibit linguistic variation on lexical,
morphological, syntactic and semantic levels ? yet
the intended contents of the answer is predictable so
that an instructor can define target responses.
Since variation is possible across learner re-
sponses in activities in the middle ground of the
spectrum, we propose a shallow content assessment
approach which supports the comparison of target
and learner responses on several levels including to-
ken, chunk and relation. We present an architec-
ture for a content assessment module (CAM) which
provides this flexibility using multiple surface-based
matching strategies and existing language process-
ing tools. For an empirical evaluation, we collected
a corpus of language learner data consisting exclu-
sively of responses to short-answer reading compre-
hension questions by intermediate English language
learners.
2 The Data
The learner corpus consists of 566 responses to
short-answer comprehension questions. The re-
sponses, written by intermediate ESL students as
part of their regular homework assignments, were
typically 1-3 sentences in length. Students had ac-
cess to their textbooks for all activities. For devel-
opment and testing, the corpus was divided into two
sets. The development set contains 311 responses
from 11 students answering 47 different questions;
the test set contains 255 responses from 15 students
to 28 questions. The development and test sets were
collected in two different classes of the same inter-
mediate reading/writing course.
Two graders annotated the learner answers with
a binary code for semantic correctness and one of
several diagnosis codes to be discussed below. Tar-
get responses (i.e., correct answers) and keywords
from the target responses were also identified by
the graders.1 Because we focus on content assess-
ment, learner responses containing grammatical er-
rors were only marked as incorrect if the grammat-
ical errors impacted the understanding of the mean-
ing.
The graders did not agree on correctness judg-
ments for 31 responses (12%) in the test set. These
were eliminated from the test set in order to obtain a
gold standard for evaluation.
The remaining responses in the development and
test sets showed a range of variation for many of the
prompts. As the following example from the corpus
illustrates, even straightforward questions based on
1Keywords refer to terms in the target response essential to
a correct answer.
108
an explicit short reading passage yield both linguis-
tic and content variation:
CUE: What are the methods of propaganda men-
tioned in the article?
TARGET: The methods include use of labels, visual
images, and beautiful or famous people promoting
the idea or product. Also used is linking the product
to concepts that are admired or desired and to create
the impression that everyone supports the product or
idea.
LEARNER RESPONSES:
? A number of methods of propaganda are used
in the media.
? Bositive or negative labels.
? Giving positive or negative labels. Using vi-
sual images. Having a beautiful or famous per-
son to promote. Creating the impression that
everyone supports the product or idea.
While the third answer was judged to be correct,
the syntactic structures, word order, forms, and lexi-
cal items used (e.g., famous person vs. famous peo-
ple) vary from the string provided as target. Of the
learner responses in the corpus, only one was string
identical with the teacher-provided target and nine
were identical when treated as bags-of-words. In the
test set, none of the learner responses was string or
bag-of-word identical with the corresponding target
sentence.
To classify the variation exhibited in learner re-
sponses, we developed an annotation scheme based
on target modification, with the meaning error la-
bels being adapted from those identified by James
(1998) for grammatical mistakes. Target modifica-
tion encodes how the learner response varies from
the target, but makes the sometimes incorrect as-
sumption that the learner is actually trying to ?hit?
the meaning of the target. The annotation scheme
distinguishes correct answers, omissions (of rele-
vant concepts), overinclusions (of incorrect con-
cepts), blends (both omissions and overinclusions),
and non-answers. These error types are exempli-
fied below with examples from the corpus. In ad-
dition, the graders used the label alternate answer
for responses that were correct given the question
and reading passage, but that differed significantly
in meaning from what was conveyed by the target
answer.2
1. Necessary concepts left out of learner response.
CUE: Name the features that are used in the
design of advertisements.
TARGET: The features are eye contact, color,
famous people, language and cultural refer-
ences.
RESPONSE: Eye contact, color
2. Response with extraneous, incorrect concepts.
CUE: Which form of programming on TV
shows that highest level of violence?
TARGET: Cartoons show the most violent acts.
RESPONSE: Television drama, children?s pro-
grams and cartoons.
3. An incorrect blend/substitution (correct con-
cept missing, incorrect one present).
CUE: What is alliteration?
TARGET: Alliteration is where sequential
words begin with the same letter or sound.
RESPONSE: The worlds are often chosen to
make some pattern or play on works. Sequen-
tial works begins with the same letter or sound.
4. Multiple incorrect concepts.
CUE: What was the major moral question
raised by the Clinton incident?3
TARGET: The moral question raised by the
Clinton incident was whether a politician?s
personal life is relevant to their job perfor-
mance.
RESPONSE: The scandal was about the rela-
tionship between Clinton and Lewinsky.
3 Method
The CAM design integrates multiple matching
strategies at different levels of representation and
various abstractions from the surface form to com-
pare meanings across a range of response varia-
tions. The approach is related to the methods used in
2We use the term concept to refer to an entity or a relation
between entities in a representation of the meaning of a sen-
tence. Thus, a response generally contains multiple concepts.
3Note the incorrect presupposition in the cue provided by
the instructor.
109
machine translation evaluation (e.g., Banerjee and
Lavie, 2005; Lin and Och, 2004), paraphrase recog-
nition (e.g., Brockett and Dolan, 2005; Hatzivas-
siloglou et al, 1999), and automatic grading (e.g.,
Leacock, 2004; Mar??n, 2004).
To illustrate the general idea, consider the exam-
ple from our corpus in Figure 2.
Figure 2: Basic matching example
We find one string identical match between the token
was occurring in the target and the learner response.
At the noun chunk level we can match home with
his house. And finally, after pronoun resolution it is
possible to match Bob Hope with he.
The overall architecture of CAM is shown in Fig-
ure 3. Generally speaking, CAM compares the
learner response to a stored target response and de-
cides whether the two responses are possibly differ-
ent realizations of the same semantic content. The
design relies on a series of increasingly complex
comparison modules to ?align? or match compatible
concepts. Aligned and unaligned concepts are used
to diagnose content errors. The CAM design sup-
ports the comparison of target and learner responses
on token, chunk and relation levels. At the token
level, the nature of the comparison includes abstrac-
tions of the string to its lemma (i.e., uninflected root
form of a word), semantic type (e.g., date, location),
synonyms, and a more general notion of similarity
supporting comparison across part-of-speech.
The system takes as input the learner response and
one or more target responses, along with the ques-
tion and the source reading passage. The compari-
son of the target and learner input pair proceeds first
with an analysis filter, which determines whether
linguistic analysis is required for diagnosis. Essen-
tially, this filter identifies learner responses that were
copied directly from the source text.
Then, for any learner-target response pair that
requires linguistic analysis, CAM assessment pro-
ceeds in three phases ? Annotation, Alignment and
Diagnosis. The Annotation phase uses NLP tools to
enrich the learner and target responses, as well as
the question text, with linguistic information, such
as lemmas and part-of-speech tags. The question
text is used for pronoun resolution and to eliminate
concepts that are ?given? (cf. Halliday, 1967, p. 204
and many others since). Here ?given? information
refers to concepts from the question text that are re-
used in the learner response. They may be neces-
sary for forming complete sentences, but contribute
no new information. For example, if the question is
What is alliteration? and the response is Allitera-
tion is the repetition of initial letters or sounds, then
the concept represented by the word alliteration is
given and the rest is new. For CAM, responses are
neither penalized nor rewarded for containing given
information.
Table 1 contains an overview of the annotations
and the resources, tools or algorithms used. The
choice of the particular algorithm or implementation
was primarily based on availability and performance
on our development corpus ? other implementations
could generally be substituted without changing the
overall approach.
Annotation Task Language Processing Tool
Sentence Detection, MontyLingua (Liu, 2004)
Tokenization,
Lemmatization
Lemmatization PC-KIMMO (Antworth, 1993)
Spell Checking Edit distance (Levenshtein, 1966),
SCOWL word list (Atkinson, 2004)
Part-of-speech Tagging TreeTagger (Schmid, 1994)
Noun Phrase Chunking CASS (Abney, 1997)
Lexical Relations WordNet (Miller, 1995)
Similarity Scores PMI-IR (Turney, 2001;
Mihalcea et al, 2006)
Dependency Relations Stanford Parser
(Klein and Manning, 2003)
Table 1: NLP Tools used in CAM
After the Annotation phase, Alignment maps new
(i.e., not given) concepts in the learner response to
concepts in the target response using the annotated
information. The final Diagnosis phase analyzes
the alignment to determine whether the learner re-
110
Annotation Alignment Diagnosis
Punctuation
Input
Learner Response
Target Response(s)
Question
Output
Source Text
Activity Model
Settings
Sentence Detection
Tokenization
Lemmatization
POS Tagging
Chunking
Dependency Parsing
Spelling Correction
Similarity Scoring
Pronoun Resolution
Type Recognition
Analysis Filter
Givenness
Pre-Alignment Filters
Token-level 
Alignment
Chunk-level 
Alignment
Relation-level 
Alignment
Error
Reporting
Detection
Classification
Diagnosis
Classification
Figure 3: Architecture of the Content Assessment Module (CAM)
sponse contains content errors. If multiple target re-
sponses are supplied, then each is compared to the
learner response and the target response with the
most matches is selected as the model used in di-
agnosis. The output is a diagnosis of the input pair,
which might be used in a number of ways to provide
feedback to the learner.
3.1 Combining the evidence
To combine the evidence from these different lev-
els of analysis for content evaluation and diagno-
sis, we tried two methods. In the first, we hand-
wrote rules and set thresholds to maximize perfor-
mance on the development set. On the development
set, the hand-tuned method resulted in an accuracy
of 81% for the semantic error detection task, a bi-
nary judgment task. However, performance on the
test set (which was collected in a later quarter with
a different instructor and different students) made
clear that the rules and thresholds thus obtained were
overly specific to the development set, as accuracy
dropped down to 63% on the test set. The hand-
written rules apparently were not general enough to
transfer well from the development set to the test set,
i.e., they relied on properties of the development set
that where not shared across data sets. Given the va-
riety of features and the many different options for
combining and weighing them that might have been
explored, we decided that rather than hand-tuning
the rules to additional data, we would try to machine
learn the best way of combining the evidence col-
lected. We thus decided to explore machine learn-
ing, even though the set of development data for
training clearly is very small.
Machine learning has been used for equivalence
recognition in related fields. For instance, Hatzivas-
siloglou et al (1999) trained a classifier for para-
phrase detection, though their performance only
reached roughly 37% recall and 61% precision. In
a different approach, Finch et al (2005) found that
MT evaluation techniques combined with machine
learning improves equivalence recognition. They
used the output of several MT evaluation approaches
based on matching concepts (e.g., BLEU) as fea-
tures/values for training a support vector machine
(SVM) classifier. Matched concepts and unmatched
111
concepts alike were used as features for training the
classifier. Tested against the Microsoft Research
Paraphrase (MSRP) Corpus, the SVM classifier ob-
tained 75% accuracy on identifying paraphrases.
But it does not appear that machine learning tech-
niques have so far been applied to or even discussed
in the context of language learner corpora, where the
available data sets typically are very small.
To begin to address the application of machine
learning to meaning error diagnosis, the alignment
data computed by CAM was converted into features
suitable for machine learning. For example, the first
feature calculated is the relative overlap of aligned
keywords from the target response. The full list of
features are listed in Table 2.
Features Description
1. Keyword Overlap Percent of keywords aligned
(relative to target)
2. Target Overlap Percent of aligned target tokens
3. Learner Overlap Percent of aligned learner tokens
4. T-Chunk Percent of aligned target chunks
5. L-Chunk Percent of aligned learner chunks
6. T-Triple Percent of aligned target triples
7. L-Triple Percent of aligned learner triples
8. Token Match Percent of token alignments
that were token-identical
9. Similarity Match Percent of token alignments
that were similarity-resolved
10. Type Match Percent of token alignments
that were type-resolved
11. Lemma Match Percent of token alignments
that were lemma-resolved
12. Synonym Match Percent of token alignments
that were synonym-resolved
13. Variety of Match Number of kinds of token-level
(0-5) alignments
Table 2: Features used for Machine Learning
Features 1-7 reflect relative numbers of matches (rel-
ative to length of either the target or learner re-
sponse). Features 2, 4, and 6 are related to the target
response overlap. Features 3, 5, and 7 are related to
overlap in the learner response. Features 8?13 re-
flect the nature of the matches.
The values for the 13 features in Table 2 were used
to train the detection classifier. For diagnosis, a four-
teenth feature ? a detection feature (1 or 0 depending
on whether the detection classifier detected an error)
? was added to the development data to train the di-
agnosis classifier. Given that token-level alignments
are used in identifying chunk- and triple-level align-
ments, that kinds of alignments are related to variety
of matches, etc., there is clear redundancy and inter-
dependence among features. But each feature adds
some new information to the overall diagnosis pic-
ture.
The machine learning suite used in all the devel-
opment and testing runs is TiMBL (Daelemans et al,
2007). As with the NLP tools used, TiMBLwas cho-
sen mainly to illustrate the approach. It was not eval-
uated against several learning algorithms to deter-
mine the best performing algorithm for the task, al-
though this is certainly an avenue for future research.
In fact, TiMBL itself offers several algorithms and
options for training and testing. Experiments with
these options on the development set included vary-
ing how similarity between instances was measured,
how importance (i.e., weight) was assigned to fea-
tures and how many neighbors (i.e., instances) were
examined in classifying new instances. Given the
very small development set available, making em-
pirical tuning on the development set difficult, we
decided to use the default learning algorithm (k-
nearest neighbor) and majority voting based on the
top-performing training runs for each available dis-
tance measure.
4 Results
Turning to the results obtained by the machine-
learning based CAM, for the binary semantic error
detection task, the system obtains an overall 87% ac-
curacy on the development set (using the leave-one-
out option of TiMBL to avoid training on the test
item). Interestingly, even for this small development
set, machine learning thus outperforms the accuracy
obtained for the manual method of combining the
evidence reported above. On the test set, the final
TiMBL-based CAM performance for detection im-
proved slightly to 88% accuracy. These results sug-
gest that detection using the CAM design is viable,
though more extensive testing with a larger corpus
is needed.
Balanced sets Both the development and test sets
contained a high proportion of correct answers ?
71% of the development set and 84% of the test set
were marked as correct by the human graders. Thus,
112
we also sampled a balanced set consisting of 50%
correct and 50% incorrect answers by randomly in-
cluding correct answers plus all the incorrect an-
swers to obtain a set with 152 cases (development
subset) and 72 (test subset) sentences. The accuracy
obtained for this balanced set was 78% (leave-one-
out-testing with development set) and 67% (test set).
The fact that the results for the balanced develop-
ment set using leave-one-out-testing are comparable
to the general results shows that the machine learner
was not biased towards the ratio of correct and in-
correct responses, even though there is a clear drop
from development to test set, possibly related to the
small size of the data sets available for training and
testing.
Alternate answers Another interesting aspect to
discuss is the treatment of alternate answers. Recall
that alternate answers are those learner responses
that are correct but significantly dissimilar from the
given target. Of the development set response pairs,
15 were labeled as alternate answers. One would
expect that given that these responses violate the as-
sumption that the learner is trying to hit the given
target, using these items in training would negatively
effect the results. This turns out to be the case; per-
formance on the training set drops slightly when the
alternate answer pairs are included. We thus did not
include them in the development set used for train-
ing the classifier. In other words, the diagnosis clas-
sifier was trained to label the data with one of five
codes ? correct, omissions (of relevant concepts),
overinclusions (of incorrect concepts), blends (both
omissions and overinclusions), and non-answers.
Because it cannot be determined beforehand which
items in unseen data are alternate answer pairs, these
pairs were not removed from the test set in the final
evaluation. Were these items eliminated, the detec-
tion performance would improve slightly to 89%.
Form errors Interestingly, the form errors fre-
quently occurring in the student utterances did not
negatively impact the CAM results. On average, a
learner response in the test set contained 2.7 form
errors. Yet, 68% of correctly diagnosed sentences
included at least one form error, but only 53% of
incorrectly diagnosed ones did so. In other words,
correct responses had more form errors than incor-
rect responses. Looking at numbers and combina-
tions of form errors, no clear pattern emerges that
would suggest that form errors are linked to mean-
ing errors in a clear way. One conclusion to draw
based on these data is that form and content assess-
ment can be treated as distinct in the evaluation of
learner responses. Even in the presence of a range
of form-based errors, human graders can clearly ex-
tract the intended meaning to be able to evaluate se-
mantic correctness. The CAM approach is similarly
able to provide meaning evaluation in the presence
of grammatical errors.
Diagnosis For diagnosis with five codes, CAM
obtained overall 87% accuracy both on the devel-
opment and on the test set. Given that the number of
labels increases from 2 to 5, the slight drop in overall
performance in diagnosis as compared to the detec-
tion of semantic errors (from 88% to 87%) is both
unsurprising in the decline and encouraging in the
smallness of the decline. However, given the sample
size and few numbers of instances of any given error
in the test (and development) set, additional quanti-
tative analysis of the diagnosis results would not be
particularly meaningful.
5 Related Work
The need for semantic error diagnosis in previous
CALL work has been limited by the narrow range
of acceptable response variation in the supported
language activity types. The few ICALL systems
that have been successfully integrated into real-life
language teaching, such as German Tutor (Heift,
2001) and BANZAI (Nagata, 2002), also tightly
control expected response variation through delib-
erate exercise type choices that limit acceptable re-
sponses. Content assessment in the German Tutor
is performed by string matching against the stored
targets. Because of the tightly controlled exercise
types and lack of variation in the expected input,
the assumption that any variation in a learner re-
sponse is due to form error, rather than legitimate
variation, is a reasonable one. The recently de-
veloped TAGARELA system for learners of Por-
tuguese (Amaral and Meurers, 2006; Amaral, 2007)
lifts some of the restrictions on exercise types, while
relying on shallow semantic processing. Using
strategies inspired by our work, TAGARELA in-
corporates simple content assessment for evaluating
113
learner responses in short-answer questions.
ICALL system designs that do incorporate more
sophisticated content assessment include FreeText
(L?Haire and Faltin, 2003), the Military Language
Tutor (MILT) Program (Kaplan et al, 1998), and
Herr Kommissar (DeSmedt, 1995). These systems
restrict both the exercise types and domains to make
content assessment feasible using deeper semantic
processing strategies.
Beyond the ICALL domain, work in automatic
grading of short answers and essays has addressed
whether the students answers convey the correct
meaning, but these systems focus on largely scor-
ing rather than diagnosis (e.g., E-rater, Burstein
and Chodorow, 1999), do not specifically address
language learning contexts and/or are designed to
work specifically with longer texts (e.g., AutoTu-
tor, Wiemer-Hastings et al, 1999). Thus, the extent
to which ICALL systems can diagnose meaning er-
rors in language learner responses has been far from
clear.
As far as we are aware, no directly comparable
systems performing content-assessment on related
language learner data exist. The closest related sys-
tem that does a similar kind of detection is the C-
rater system (Leacock, 2004). That system obtains
85% accuracy. However, the test set and scoring sys-
tem were different, and the system was applied to
responses from native English speakers. In addition,
their work focused on detection of errors rather than
diagnosis. So, the results are not directly compara-
ble. Nevertheless, the CAM detection results clearly
are competitive.
6 Summary
After motivating the need for content assessment in
ICALL, in this paper we have discussed an approach
for content assessment of English language learner
responses to short answer reading comprehension
questions, which is worked out in detail in Bailey
(2008). We discussed an architecture which relies on
shallow processing strategies and achieves an accu-
racy approaching 90% for content error detection on
a learner corpus we collected from learners complet-
ing the exercises assigned in a real-life ESL class.
Even for the small data sets available in the area of
language learning, it turns out that machine learn-
ing can be effective for combining the evidence from
various shallow matching features. The good perfor-
mance confirms the viability of using shallow NLP
techniques for meaning error detection. By devel-
oping and testing this model, we hope to contribute
to bridging the gap between what is practical and
feasible from a processing perspective and what is
desirable from the perspective of current theories of
language instruction.
References
Steven Abney, 1997. Partial Parsing via Finite-State Cas-
cades. Natural Language Engineering, 2(4):337?344.
http://vinartus.net/spa/97a.pdf.
Luiz Amaral, 2007. Designing Intelligent Language Tu-
toring Systems: Integrating Natural Language Pro-
cessing Technology into Foreign Language Teaching.
Ph.D. thesis, The Ohio State University.
Luiz Amaral and Detmar Meurers, 2006. Where
does ICALL Fit into Foreign Language Teaching?
Presentation at the 23rd Annual Conference of the
Computer Assisted Language Instruction Consortium
(CALICO), May 19, 2006. University of Hawaii.
http://purl.org/net/icall/handouts/
calico06-amaral-meurers.pdf.
Evan L. Antworth, 1993. Glossing Text with the PC-
KIMMO Morphological Parser. Computers and the
Humanities, 26:475?484.
Kevin Atkinson, 2004. Spell Checking Oriented
Word Lists (SCOWL). http://wordlist.
sourceforge.net/.
Stacey Bailey, 2008. Content Assessment in Intelligent
Computer-Aided Language Learning: Meaning Error
Diagnosis for English as a Second Language. Ph.D.
thesis, The Ohio State University.
Satanjeev Banerjee and Alon Lavie, 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of Workshop on Intrinsic and Extrinsic Eval-
uation Measures for Machine Translation and/or Sum-
marization at the 43th Annual Meeting of the Associ-
ation of Computational Linguistics (ACL-2005). Ann
Arbor, Michigan, pp. 65?72. http://aclweb.
org/anthology/W05-0909.
Chris Brockett andWilliam B. Dolan, 2005. Support Vec-
tor Machines for Paraphrase Identification and Cor-
pus Construction. In Proceedings of the Third In-
ternational Workshop on Paraphrasing (IWP2005).
pp. 1?8. http://aclweb.org/anthology/
I05-5001.
Jill Burstein and Martin Chodorow, 1999. Automated
Essay Scoring for Nonnative English Speakers. In
Proceedings of a Workshop on Computer-Mediated
Language Assessment and Evaluation of Natural Lan-
guage Processing, Joint Symposium of the Asso-
ciation of Computational Linguistics (ACL-99) and
the International Association of Language Learning
Technologies. pp. 68?75. http://aclweb.org/
anthology/W99-0411.
114
Walter Daelemans, Jakub Zavrel, Kovan der Sloot and
Antal van den Bosch, 2007. TiMBL: Tilburg Memory-
Based Learner Reference Guide, ILK Technical Re-
port ILK 07-03. Induction of Linguistic Knowledge
Research Group Department of Communication and
Information Sciences, Tilburg University, P.O. Box
90153, NL-5000 LE, Tilburg, The Netherlands, ver-
sion 6.0 edition.
William DeSmedt, 1995. Herr Kommissar: An ICALL
Conversation Simulator for Intermediate German. In
V. Melissa Holland, Jonathan Kaplan and Michelle
Sams (eds.), Intelligent Language Tutors: Theory
Shaping Technology, Lawrence Erlbaum Associates,
pp. 153?174.
Andrew Finch, Young-Sook Hwang and Eiichiro Sumita,
2005. Using Machine Translation Evaluation Tech-
niques to Determine Sentence-level Semantic Equiva-
lence. In Proceedings of the Third International Work-
shop on Paraphrasing (IWP2005). pp. 17?24. http:
//aclweb.org/anthology/I05-5003.
Michael Halliday, 1967. Notes on Transitivity and Theme
in English. Part 1 and 2. Journal of Linguistics, 3:37?
81, 199?244.
Vasileios Hatzivassiloglou, Judith Klavans and Eleazar
Eskin, 1999. Detecting Text Similarity over Short Pas-
sages: Exploring Linguistic Feature Combinations via
Machine Learning. In Proceedings of Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora (EMNLP?99). College Park, Maryland, pp.
203?212. http://aclweb.org/anthology/
W99-0625.
Trude Heift, 2001. Intelligent Language Tutoring
Systems for Grammar Practice. Zeitschrift fu?r In-
terkulturellen Fremdsprachenunterricht, 6(2). http:
//www.spz.tu-darmstadt.de/projekt_
ejournal/jg-06-2/beitrag/heift2.htm.
Carl James, 1998. Errors in Language Learning and Use:
Exploring Error Analysis. Longman Publishers.
Jonathan Kaplan, Mark Sobol, Robert Wisher and Robert
Seidel, 1998. The Military Language Tutor (MILT)
Program: An Advanced Authoring System. Computer
Assisted Language Learning, 11(3):265?287.
Dan Klein and Christopher D. Manning, 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the 41st
Meeting of the Association for Computational Linguis-
tics (ACL 2003). Sapporo, Japan, pp. 423?430. http:
//aclweb.org/anthology/P03-1054.
Claudia Leacock, 2004. Scoring Free-Responses Auto-
matically: A Case Study of a Large-Scale Assessment.
Examens, 1(3).
Vladimir I. Levenshtein, 1966. Binary Codes Capable of
Correcting Deletions, Insertions, and Reversals. Soviet
Physics Doklady, 10(8):707?710.
Se?bastien L?Haire and Anne Vandeventer Faltin, 2003.
Error Diagnosis in the FreeText Project. CALICO
Journal, 20(3):481?495.
Chin-Yew Lin and Franz Josef Och, 2004. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statistics. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-04). pp. 605?612. http://aclweb.org/
anthology/P04-1077.
Hugo Liu, 2004. MontyLingua: An End-to-
End Natural Language Processor with Common
Sense. http://web.media.mit.edu/?hugo/
montylingua, accessed October 30, 2006.
Diana Rosario Pe?rez Mar??n, 2004. Automatic Evaluation
of Users? Short Essays by Using Statistical and Shal-
low Natural Language Processing Techniques. Mas-
ter?s thesis, Universidad Auto?noma deMadrid. http:
//www.ii.uam.es/?dperez/tea.pdf.
Rada Mihalcea, Courtney Corley and Carlo Strapparava,
2006. Corpus-based and Knowledge-based Measures
of Text Semantic Similarity. In Proceedings of the Na-
tional Conference on Artificial Intelligence. American
Association for Artificial Intelligence (AAAI) Press,
Menlo Park, CA, volume 21(1), pp. 775?780.
George Miller, 1995. WordNet: A Lexical Database for
English. Communications of the ACM, 38(11):39?41.
Noriko Nagata, 2002. BANZAI: An Application of Nat-
ural Language Processing to Web-Based Language
Learning. CALICO Journal, 19(3):583?599.
Helmut Schmid, 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In International Con-
ference on New Methods in Language Processing.
Manchester, United Kingdom, pp. 44?49.
Peter Turney, 2001. Mining theWeb for Synonyms: PMI-
IR Versus LSA on TOEFL. In Proceedings of the
Twelfth European Conference on Machine Learning
(ECML-2001). Freiburg, Germany, pp. 491?502.
Peter Wiemer-Hastings, Katja Wiemer-Hastings and
Arthur Graesser, 1999. Improving an Intelligent Tu-
tor?s Comprehension of Students with Latent Seman-
tic Analysis. In Susanne Lajoie and Martial Vivet
(eds.), Artificial Intelligence in Education, IOS Press,
pp. 535?542.
115
Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 24?32,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Revisiting the impact of different annotation schemes on PCFG parsing:
A grammatical dependency evaluation
Adriane Boyd
Department of Linguistics
The Ohio State University
1712 Neil Avenue
Columbus, Ohio 43210, USA
adriane@ling.osu.edu
Detmar Meurers
Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
Wilhelmstrasse 19
72074 Tu?bingen, Germany
dm@sfs.uni-tuebingen.de
Abstract
Recent parsing research has started addressing
the questions a) how parsers trained on differ-
ent syntactic resources differ in their perfor-
mance and b) how to conduct a meaningful
evaluation of the parsing results across such
a range of syntactic representations. Two Ger-
man treebanks, Negra and Tu?Ba-D/Z, consti-
tute an interesting testing ground for such re-
search given that the two treebanks make very
different representational choices for this lan-
guage, which also is of general interest given
that German is situated between the extremes
of fixed and free word order. We show that
previous work comparing PCFG parsing with
these two treebanks employed PARSEVAL
and grammatical function comparisons which
were skewed by differences between the two
corpus annotation schemes. Focusing on the
grammatical dependency triples as an essen-
tial dimension of comparison, we show that
the two very distinct corpora result in compa-
rable parsing performance.
1 Introduction
Syntactically annotated corpora have been produced
for a range of languages and they differ significantly
regarding which language properties are encoded
and how they are represented. Between the two ex-
tremes of constituency treebanks for English and de-
pendency treebanks for free word order languages
such as Czech lie languages such as German, for
which two different treebanks have explored differ-
ent options for encoding topology and dependency,
Negra (Brants et al, 1999) and Tu?Ba-D/Z (Telljo-
hann et al, 2005).
Recent research has started addressing the ques-
tion of how parsers trained on these different syntac-
tic resources differ in their performance. Such work
must also address the question of how to conduct a
meaningful evaluation of the parsing results across
such a range of syntactic representations. In this pa-
per, we show that previous work comparing PCFG
parsing for the two German treebanks used represen-
tations which cannot adequately be compared using
the given PARSEVAL measures and that a grammat-
ical dependency evaluation is more meaningful than
the grammatical function evaluation provided.
We present the first comparison of Negra and
Tu?Ba-D/Z using a labeled dependency evaluation
based on the grammatical function labels provided
in the corpora. We show that, in contrast to previ-
ous literature, a labeled dependency evaluation es-
tablishes that PCFG parsers trained on the two cor-
pora give similar parsing performance. The focus on
labeled dependencies also provides a direct link to
recent work on dependency-based evaluation (e.g.,
Clark and Curran, 2007) and dependency parsing
(e.g., CoNLL shared tasks 2006, 2007).
1.1 Previous work
The question of how to evaluate parser output has
naturally already arisen in earlier work on parsing
English. As discussed by Lin (1995) and others, the
PARSEVAL evaluation typically used to analyze the
performance of statistical parsing models has many
drawbacks. Bracketing evaluation may count a sin-
gle error multiple times and does not differentiate
between errors that significantly affect the interpre-
tation of the sentence and those that are less crucial.
24
It also does not allow for evaluation of particular
syntactic structures or provide meaningful informa-
tion about where the parser is failing. In addition,
and most directly relevant for this paper, PARSE-
VAL scores are difficult to compare across syntactic
annotation schemes (Carroll et al, 2003).
At the same time, previous research on PCFG
parsing using treebank training data present PAR-
SEVAL measures in comparing the parsing per-
formance for different languages and annotation
schemes, reporting a number of striking differences.
For example, Levy and Manning (2003), Ku?bler
(2005), and Ku?bler et al (2006) highlight the sig-
nificant effect of language properties and annotation
schemes for German and Chinese treebanks. In re-
lated work, parser enhancements that provide a sig-
nificant performance boost for English, such as head
lexicalization, are reported not to provide the same
kind of improvement, if any, for German (Dubey and
Keller, 2003; Dubey, 2004; Ku?bler et al, 2006).
Previous work has compared the similar Negra
and Tiger corpora of German to the very different
Tu?Ba-D/Z corpus. Ku?bler et al (2006) compares
the Negra and Tu?Ba-D/Z corpora of German using
a PARSEVAL evaluation and an evaluation on core
grammatical function labels that is included to ad-
dress concerns about the PARSEVAL measure.1 Us-
ing the Stanford Parser (Klein and Manning, 2002),
which employs a factored PCFG and dependency
model, they claim that the model trained on Tu?Ba-
D/Z consistently outperforms that trained on Ne-
gra in PARSEVAL and grammatical function evalu-
ations. Dubey (2004) also includes an evaluation on
grammatical function for statistical models trained
on Negra, but obtains very different results from
Ku?bler et al (2006).2
In recent related work, Rehbein and van Genabith
(2007a) demonstrate using the Tiger and Tu?Ba-D/Z
1The evaluation is based only on the grammatical function;
it does not identify the dependency pair that it labels.
2While the focus of Ku?bler et al (2006) is on comparing
parsing results across corpora, Dubey (2004) focuses on im-
proving parsing for Negra, including corpus-specific enhance-
ments leading to better results. This difference in focus and
additional differences in experimental setup mean that a fine-
grained comparison of the results is inappropriate ? the rele-
vant point here is that the gap between the results (23% for sub-
jects, 35% for accusative objects) warrants further attention in
the context of comparing parsing results across corpora.
corpora of German that PARSEVAL is inappropri-
ate for comparisons of the output of PCFG parsers
trained on different treebank annotation schemes be-
cause PARSEVAL scores are affected by the ratio
of terminal to non-terminal nodes. A dependency-
based evaluation on triples of the form word-POS-
head shows better results for the parser trained
on Tiger even though the much lower PARSEVAL
scores, if meaningful, would predict that the out-
put for Tiger is of lower quality. However, their
dependency-based evaluation does not make use
of the grammatical function labels, which are pro-
vided in the corpora and closely correspond to the
representations used in recent work on formalism-
independent evaluation of parsers (e.g., Clark and
Curran, 2007).3
Addressing these issues, we resolve the apparent
discrepancy between Ku?bler et al (2006) and Dubey
(2004) and establish a firm grammatical function
comparison of Negra and Tu?Ba-D/Z. We also ex-
tend the evaluation to a labeled dependency evalu-
ation based on grammatical relations for both cor-
pora. Such an evaluation, which abstracts away from
the specifics of the annotation schemes, shows that,
in contrast to the claims made in Ku?bler et al (2006),
the parsing results for PCFG parsers trained on these
heterogeneous corpora are very similar.
2 The corpora used
As motivated in the introduction, the work discussed
in this paper is based on two German corpora, Ne-
gra and Tu?Ba-D/Z, which differ significantly in the
syntactic representations used ? thereby offering an
interesting test bed for investigating the influence of
an annotation scheme on the parsers trained.
2.1 Negra
The Negra corpus (Brants et al, 1999) consists of
newspaper text from the Frankfurter Rundschau, a
German newspaper. Version 2 of the corpus contains
20,602 sentences. It uses the STTS tag set (Schiller
et al, 1995) for part-of-speech annotation. There are
25 non-terminal node labels and 46 edge labels.
The syntactic annotation of Negra combines fea-
tures from phrase structure grammar and depen-
3Their evaluation also introduces an additional level of com-
plexity by finding heads heuristically rather than relying on the
head labels present on some elements in each corpus.
25
dency grammar using a tree-like syntactic structure
with grammatical functions labeled on the edges of
the tree. Flat sentence structures are used in many
places to avoid attachment ambiguities and non-
branching phrases are not used.
The annotation scheme emphasizes the use of the
tree structure to encode grammatical dependencies,
representing a head and all its dependents within a
local tree regardless of whether a dependent is real-
ized near its head or not, e.g., because it has been
extraposed or fronted. Since traditional syntax trees
do not permit the crossing branches needed to li-
cense discontinuous constituents, Negra uses a ?syn-
tax graph? data structure to represent the annotation.
An example of a syntax graph with a discontinuous
constituent (VP) due to a fronted dative object (NP)
is shown in Figure 1.
Dieser
PDAT
Meinung
NN
kann
VMFIN
ich
PPER
nur
ADV
voll
ADJD
zustimmen
VVINF
.
$.
NK NK
NP
DA MO HD
VP
OCHD SB MO
S
VROOT
this opinion can I only completely agree
Figure 1: Negra tree for ?I can only agree with this opin-
ion completely.?
Negra uses flat NP and PP annotation with no
marked heads. For example, both Dieser and Mein-
ung in Figure 1 have the grammatical function label
?NK?. Since unary branching is not used in Negra, a
bare noun or pronoun argument is not dominated by
an NP node, as shown by the pronoun ich above.
A verbal head in Negra is always marked with the
edge label ?HD? and its arguments are its sisters in
the local tree. The subject is always the sister of the
finite verb, which is a daughter of S. If the finite verb
is the main verb in the clause, the objects are also its
sisters, i.e., the finite verb, subject and objects are
all daughters of S. If the main verb is an auxiliary
governing a non-finite main verb, the non-finite verb
and its objects and modifiers form a VP where the
objects are sisters of the non-finite verb as in Fig-
ure 1. The VP is then a sister of the finite verb.
The finite verb in a German declarative clause ap-
pears in the so-called verb-second position, immedi-
ately following the fronted constituent. As a result,
the VP in Negra is discontinuous whenever one of
its children has been fronted, as in the common word
orders exemplified in (1a) and (1b).
(1) a. Die
the
Tu?r
door
hat
has
Anna
Anna
wieder
again
zugeschlagen.
slammed-shut
?Anna slammed the door shut again.?
b. Wieder
again
hat
has
Anna
Anna
die
the
Tu?r
door
zugeschlagen.
slammed-shut
?Anna slammed the door shut again.?
The sentence we saw in Figure 1 contains a dis-
continuous VP with a fronted dative object (Dieser
Meinung). The dative object and a modifier (voll)
form a VP with the non-finite verb (zustimmen).
2.2 Tu?Ba-D/Z
The Tu?Ba-D/Z corpus, version 2, (Telljohann et al,
2005) consists of 22,091 sentences of newspaper
text from the German newspaper die tageszeitung.
Like Negra, it uses the STTS tag set (Schiller et al,
1995) for part-of-speech annotation. Syntactically it
uses 27 non-terminal node labels and 47 edge labels.
The syntactic annotation incorporates a topologi-
cal field analysis of the German clause (Reis, 1980;
Ho?hle, 1986), which segments a sentence into topo-
logical units depending on the position of the finite
verb (verb-first, verb-second, verb-last). In a verb-
first and verb-second sentence, the finite verb is the
left bracket (LK), whereas in a verb-last subordinate
clause, the subordinating conjunction occupies that
field. In all clauses, the non-finite verb cluster forms
the right bracket (VC), and arguments and modifiers
can appear in the middle field (MF) between the two
brackets. Extraposed material is found to the right
of the right bracket, and in a verb-second sentence
one constituent appears in the fronted field (VF) pre-
ceding the finite verb. By specifying constraints on
the elements that can occur in the different fields,
the word order in any type of German clause can be
concisely characterized.
Each clause in the Tu?Ba-D/Z corpus is divided
into topological fields at the top level, and each topo-
logical field contains phrase-level annotation. An
26
example sentence from Tu?Ba-D/Z is shown in Fig-
ure 2, where the topological fields VF, LK, MF, and
VC are visible under the SIMPX clause node.
Daf?r
PROP
wird
VAFIN
Andrea
NE
Fischer
NE
wenig
PIAT
Zeit
NN
haben
VAINF
.
$.
HD
PX
HD
VXFIN
- -
NX
- HD
NX
HD
VXINF
OA-MOD
VF
HD
LK
-
EN-ADD
OV
VC
ON OA
MF
- - - -
SIMPX
VROOT
for it will Andrea Fischer little time have
Figure 2: Tu?Ba-D/Z tree for ?Andrea Fischer will have
little time for it.?
Edge labels are used to mark heads and gram-
matical functions, even though it can be nontrivial
to figure out which grammatical function belongs
to which head given that heads and their arguments
often are in separate topological fields. For exam-
ple, in Figure 2 the subject noun chunk (NX) has
the edge label ON (object - nominative) and the ob-
ject noun chunk has the edge label OA (object - ac-
cusative); both are realized within the middle field
(MF), while the finite verb (VXFIN) marked as HD
(head) is in the left sentence bracket (LK). This is-
sue becomes relevant in section 3.4.2, discussing an
evaluation based on labeled dependency triples.
Where Negra uses discontinuous constituents,
Tu?Ba-D/Z uses special edge labels to annotate gram-
matical relations which are not locally realized. For
example, the fronted prepositional phrase (PX) in
Figure 2 has the edge label OA-MOD which needs
to be matched with the noun phrase (NX) with label
OA that is found in the MF field.
2.3 Comparing Negra and Tu?Ba-D/Z
To give an impression of how the different anno-
tation schemes affect the appearance of a typical
tree in the two corpora, Table 1 provides statistics
on average sentence length and the number of non-
terminals per sentence.
Negra Tu?Ba-D/Z
No. of Sentences 20,602 22,091
Terminals/Sentence 17.2 17.3
Non-terminals/Sentence 7.0 20.7
Table 1: General Characteristics of the Corpora
While the sentences in Negra and Tu?Ba-D/Z on
average have the same number of words, the average
Tu?Ba-D/Z sentence has nearly three times as many
non-terminal nodes as the average Negra sentence.
This difference is mainly due to the extra level of
topological fields annotation and the use of more
contoured structures in many places where Negra
uses flatter structures.
3 Experiments
The goal of the following experiments is a compar-
ison of parsing performance across different types
of evaluation metrics for parsers trained on Negra
(Ver. 2) and Tu?Ba-D/Z (Ver. 2).
3.1 Data Preparation
Following Ku?bler et al (2006), only sentences with
fewer than 35 words were used, which results in
20,002 sentences for Negra and 21,365 sentences
for Tu?Ba-D/Z. Because punctuation is not attached
within the sentence in the corpus annotation, punc-
tuation was removed.
To be able to train PCFG parsing models, it is nec-
essary to convert the syntax graphs encoding trees
with discontinuities in Negra into traditional syntax
trees. Around 30% of sentences in Negra contain at
least one discontinuity. To remove discontinuities,
we used the conversion program included with the
Negra corpus annotation tools (Brants and Plaehn,
2000), the same tool used in Ku?bler et al (2006),
which raises non-head elements to a higher tree un-
til there are no more discontinuities. For example,
for the discontinuous tree with a fronted object we
saw in Figure 1, the PP containing the fronted NP
Dieser Meinung is raised to become a daughter of
the top S node.4
Additionally, the edge labels used in both corpora
need to be folded into the node labels to become a
4An alternate method that avoids certain problems with this
raising method is discussed in Boyd (2007).
27
part of context-free grammar rules used by a PCFG
parser. In the Penn Treebank-style versions of the
corpora appropriate for training a PCFG parser, each
edge label is joined with the phrase or POS label
on the phrase or word immediately below it. Both
corpora include edge labels above all phrases and
words. However the flatter structures in Negra result
in 39 different edge labels on words while Tu?Ba-D/Z
has only 5.
Unlike Ku?bler et al (2006), which ignored edge
labels on words, we incorporate all edge labels
present in both corpora. As a consequence of this,
providing a parser with perfect lexical tags would
also provide the edge label for that word. Tu?Ba-D/Z
does not annotate grammatical functions other than
HD on words, but Negra includes many grammati-
cal functions on words. Including edge labels in the
perfect lexical tags would artificially boost the re-
sults of a grammatical function evaluation for Negra
since it amounts to providing the correct grammati-
cal function for the 38% of arguments in Negra that
are single words.
To avoid this problem, we introduced non-
branching phrasal nodes into Negra to prevent the
correct grammatical function label from being pro-
vided with the perfect lexical tag in the cases
of single-word arguments, which are mostly bare
nouns and pronouns. We added phrasal nodes above
all single-word subject, accusative object, dative ob-
ject, and genitive object5 arguments, with the cate-
gory of the inserted phrase depending on the POS
tag on the word. The introduced phrasal node is
given the word?s original grammatical function la-
bel; the grammatical function label of the word itself
becomes NK for NPs and HD for APs and VPs. In
total, 14,580 nodes were inserted into Negra in this
way. Tu?Ba-D/Z has non-branching phrases above all
single-word arguments, so that no such modification
was needed.6
3.2 Experimental Setup
We trained unlexicalized PCFG parsing models us-
ing LoPar (Schmid, 2000). Unlexicalized models
5Genitive objects are modified for the sake of consistency
among arguments even though there are too few genitive objects
to provide reliable results in the evaluation.
6The addition of edge labels to terminal POS labels results
in 337 lexical tags for Negra and 91 for Tu?Ba-D/Z.
were used to minimize the impact of other corpus
differences on parsing. A ten-fold cross validation
was performed for all experiments.7
3.3 PARSEVAL Evaluation
As a reference point for comparison with previous
work, the PARSEVAL results8 are given in Table 2.
Negra Tu?Ba-D/Z
Unlabeled Precision 78.69 89.92
Unlabeled Recall 82.29 86.48
Labeled Precision 64.08 75.36
Labeled Recall 67.01 72.47
Coverage 97.00 99.90
Table 2: PARSEVAL Evaluation
The parser trained on Tu?Ba-D/Z performs much
better than the one trained on Negra on all labeled
and unlabeled bracketing scores. As we saw in
section 2, Negra and Tu?Ba-D/Z use very different
syntactic annotation schemes, resulting in over 2.5
times as many non-terminals per sentence in Tu?Ba-
D/Z as in Negra with the additional unary nodes.
As mentioned previously, Rehbein and van Genabith
(2007a) showed that PARSEVAL is affected by the
ratio of terminal to non-terminal nodes, so these re-
sults are not expected to indicate the quality of the
parses. The comparison with grammatical function
and dependency evaluations we turn to next show-
cases that PARSEVAL does not provide a meaning-
ful evaluation metric across annotation schemes.
3.4 Dependency Evaluation
Complementing the issue of the ratio of terminals
to non-terminals raised in the last section, one can
question whether counting all brackets in the sen-
tence equally, as done by the PARSEVAL metric,
provides a good measure of how accurately the ba-
sic functor-argument structure of the sentence has
been captured in a parse. Thus, it is useful to per-
7Our experimental setup is designed to support a compari-
son between Negra and Tu?Ba-D/Z for the three evaluation met-
rics and is intended to be comparable to the setup of Ku?bler
et al (2006). For Negra, Dubey (2004) explores a range of pars-
ing models and the corpus preparation he uses differs from the
one discussed in this paper so that a discussion of his results is
beyond the scope of the corpus comparison in this paper.
8Scores were calculated using evalb.
28
form an evaluation based on the grammatical func-
tion labels that are important for determining the
functor-argument structure of the sentence: subjects,
accusative objects, and dative objects.9 The first
step in an evaluation of functor-argument structure
is to identify whether an argument bears the correct
grammatical function label.
3.4.1 Grammatical Function Label Evaluation
Ku?bler et al (2006) present the results shown in Ta-
ble 3 for the parsing performance of the unlexical-
ized model of the Stanford Parser (Klein and Man-
ning, 2002). In this grammatical function label eval-
uation, Tu?Ba-D/Z outperforms Negra for subjects,
accusative objects, and dative objects based on an
evaluation of phrasal arguments.
Negra Tu?Ba-D/Z
Prec Rec F Prec Rec F
Subj 52.50 58.02 55.26 66.82 75.93 72.38
Acc 35.14 36.30 35.72 43.84 47.31 45.58
Dat 8.38 3.58 5.98 24.46 9.96 17.21
Table 3: Grammatical Function Label Evaluation for
Phrasal Arguments from Ku?bler et al (2006)
Note that this grammatical function label evalua-
tion is restricted to labels on phrases; grammatical
function labels on words are ignored in training and
testing. This results in an unbalanced comparison
between Negra and Tu?Ba-D/Z since, as discussed
in section 2, Tu?Ba-D/Z includes unary-branching
phrases above all single-word arguments whereas
Negra does not. In effect, single-word arguments
in Negra ? mainly pronouns and bare nouns ? are
not considered in the evaluation from Ku?bler et al
(2006). The result is thus a comparison of multi-
word arguments in Negra to both single- and multi-
word arguments in Tu?Ba-D/Z. Recall from section
3.1 that this is not a minor difference: single-word
arguments account for 38% of subjects, accusative
objects, and dative objects in Negra.
As discussed in the data preparation section, Ne-
gra was modified for our experiment so as not to
9Genitive objects are also annotated in both corpora, but
they are too infrequent to provide meaningful results. As dis-
cussed in Rehbein and van Genabith (2007b), labels such as
subject (SB for Negra, ON for Tu?Ba-D/Z) are not necessarily
comparable in all instances, but such cases are infrequent.
provide the parser with the grammatical function la-
bels for single word phrases as part of the perfect
tags provided. This evaluation handles multiple cat-
egories of arguments, not just NPs, so it focuses
solely on the grammatical function labels, ignoring
the phrasal categories. For example, in Negra an NP-
OA in a parse is considered a correct accusative ob-
ject even if the OA label in the gold standard has the
category MPN. The results are shown in Table 4.
Negra Tu?Ba-D/Z
Prec Rec F Prec Rec F
Subj 69.69 69.12 69.42 65.74 72.24 68.99
Acc 48.17 50.97 49.57 41.37 46.81 44.09
Dat 20.93 15.22 18.08 21.40 11.51 16.46
Table 4: Grammatical Function Label Evaluation
In contrast to the results for NP grammatical func-
tions of Ku?bler et al (2006) we saw in Table 3, Ne-
gra and Tu?Ba-D/Z perform quite similarly overall,
with Negra slightly outperforming Tu?Ba-D/Z for all
types of arguments.
These results also form a clear contrast to the
PARSEVAL results we saw in Table 2. Contrary
to the finding in Ku?bler et al (2006), the PAR-
SEVAL evaluation does not echo the grammatical
function label evaluation. In keeping with the re-
sults from Rehbein and van Genabith (2007a), we
find that PARSEVAL is not an adequate predictor of
performance in an evaluation targeting the functor-
argument structure of the sentence for comparisons
between PCFG parsers trained on corpora with dif-
ferent annotation schemes.
3.4.2 Labeled Dependency Triple Evaluation
While determining the grammatical function of an
element is an important part of determining the
functor-argument structure of a sentence, the other
necessary component is determining the head of
each function. To evaluate whether both the functor
and the argument have been correctly found, an eval-
uation of labeled dependency triples is needed. As
in the previous section, we focus on the grammatical
function labels for arguments of verbs. To complete
a labeled dependency triple for each argument, we
additionally need to locate the lexical verbal head.
In Negra, the head is the sister of an argu-
ment marked with the function label ?HD?, however
29
heads are only marked for a subset of the phrase cat-
egories: S, VP, AP, and AVP.10 This subset includes
the phrase categories that contain verbs and their ar-
guments, S and VP. In our experiment, the parser
finds the HD grammatical function labels with a very
high f-score: 99.5% precision and 96.5% recall. If
the sister with the label HD is a word, then that word
is the lexical head for the purposes of this depen-
dency evaluation. If the sister with the label HD is
a phrase, then a recursive search for heads within
that phrase finds a lexical head. In 3.2% of cases in
the gold standard, it is not possible to find a lexical
head for an argument. Further methods could be ap-
plied to find the remaining heads heuristically, but
we avoid the additional parameters this introduces
for this evaluation by ignoring these cases.
For Tu?Ba-D/Z, finding the head is not as simple
because the verbal head and its arguments are in dif-
ferent topological fields. To create a parallel com-
parison to Negra, the finite verb from the local clause
is chosen as the head for all subjects. The (finite or
non-finite) main full verb is designated as the head
for the accusative and dative objects. It is possible
to automatically find an appropriate head verb for all
but 2.7% of subjects, accusative objects, and dative
objects.11 As with Negra, only cases where a head
verb can be found in the gold standard are consid-
ered in the evaluation.
As in the grammatical function evaluation in the
previous section, only the grammatical function la-
bel, not the phrase category is considered in the eval-
uation. The results for the labeled dependency eval-
uation are shown in Table 5. The parser trained on
Negra outperforms the one trained on Tu?Ba-D/Z for
all types of arguments.
4 Discussion of Results
Comparing PARSEVAL scores for a parser trained
on the Negra and the Tu?Ba-D/Z corpus with a gram-
matical function and a labeled dependency evalua-
10However, some strings labeled as S and VP do not contain
a head and thus lack a daughter with a HD function label.
11The relative numbers of instances where a lexical head is
not found are comparable for Negra and Tu?Ba-D/Z. Heads are
not found for approximately 4% of subjects, 1% of accusative
objects, and 1% of dative objects. These instances are fre-
quently due to elision of the verb in headlines and coordinated
clauses.
Negra Tu?Ba-D/Z
Prec Rec F Prec Rec F
Subj 72.84 69.03 70.93 60.52 65.98 63.25
Acc 47.96 48.80 48.38 37.39 40.83 39.11
Dat 19.56 14.01 16.79 19.32 10.39 14.85
Table 5: Labeled Dependency Evaluation
tion, we confirm that the PARSEVAL scores do not
correlate with the scores in the other two evalua-
tions, which given their closeness to the semantic
functor argument structure make meaningful targets
for evaluating parsers.
Shifting the focus to the grammatical function
evaluation, we showed that a grammatical function
evaluation based on phrasal arguments as provided
by Ku?bler et al (2006) is inadequate for compar-
ing parsers trained on the Negra and Tu?Ba-D/Z cor-
pora. By introducing non-branching phrase nodes
above single-word arguments in Negra, it is possi-
ble to provide a balanced comparison for the gram-
matical function label evaluation between Negra and
Tu?Ba-D/Z on both phrasal and single-word argu-
ments. The models trained on both corpora perform
very similarly in the grammatical function evalua-
tion, in contrast to the claims in Ku?bler et al (2006).
When the grammatical function label evaluation
is extended into a labeled dependency evaluation by
finding the verbal head to complete the labeled de-
pendency triple, the parser trained on Negra outper-
forms that trained on Tu?Ba-D/Z. The more signifi-
cant drop in results for Tu?Ba-D/Z compared to the
grammatical function label evaluation may be due
to the fact that a verbal lexical head in Tu?Ba-D/Z is
not in the same local tree as its dependents, whereas
it is in Negra. The presence of intervening topolog-
ical field nodes in Tu?Ba-D/Z may make it difficult
for the parser to consistently identify the elements
of the dependency triple across several subtrees.
The Negra corpus annotation scheme makes it
simple to identify the heads of verb arguments, but
the flat NP and PP structures make it difficult to ex-
tend a labeled dependency analysis beyond verb ar-
guments. On the other hand, Tu?Ba-D/Z has marked
heads in NPs and PPs, but it is not as easy to pair
verb arguments with their heads because the verbs
are in separate topological fields from their argu-
30
ments. For a constituent-based corpus annotation
scheme to lend itself to a thorough labeled depen-
dency evaluation, heads should be marked clearly
for all phrase categories and all non-head elements
need to have marked grammatical functions.
The presence of topological field nodes in Tu?Ba-
D/Z deserves more discussion in relation to a gram-
matical dependency evaluation. The corpus con-
tains two very different types of nodes in its syntac-
tic trees: nodes such as NP and PP that correspond
to constituents and nodes such as VF (Vorfeld) and
MF (Mittelfeld) that correspond to word order do-
mains. Constituents such as NP have grammatical
relations to other elements in the sentence and have
identifiable heads within them, whereas nodes en-
coding word order domains have neither.12 While
constituents and word order domains sometimes co-
incide, such as the Vorfeld normally consisting of a
single constituent, this is not the general case. For
example, the Mittelfeld often contains multiple con-
stituents which each stand in different grammatical
relations to the verb(s) in the left and right sentence
brackets (LK and VC).
Returning to the issue of finding dependencies be-
tween constituents, the intervening word order do-
main nodes can make it non-trivial to determine
these relations in Tu?Ba-D/Z. For example, word or-
der domain nodes will always intervene between a
verb and its arguments. In order to have all gram-
matical dependencies directly encoded in the tree-
bank, it would be preferable for corpus annotation
schemes to ensure that a homogeneous constituency
representation can be easily obtained.
5 Future Work
An evaluation on arguments of verbs is just a first
step in working towards a more complete labeled
dependency evaluation. Because Negra and Tu?Ba-
D/Z do not have parallel uses of many grammatical
function labels beyond arguments of verbs, a more
detailed evaluation on more types of dependency re-
lations will require a complex dependency conver-
sion method to provide comparable results.
12While the focus in this work is on unlexicalized parsing,
this also calls into question the effect of head lexicalization for
a corpus that contains elements that by their nature are not the
types of elements that have heads.
Since previous work on head-lexicalized pars-
ing models for German has focused on PARSEVAL
evaluations, it would also be useful to perform a la-
beled dependency evaluation to determine what ef-
fect head lexicalization has on particular construc-
tions for the parsers. Because of the concerns dis-
cussed in the previous section and the difference in
which types of clauses have marked heads in Negra
and Tu?Ba-D/Z, the effect of head lexicalization on
the parsing results may differ for the two corpora.
6 Conclusion
Addressing the general question of how to compare
parsing results for different annotation schemes, we
revisited the comparison of PCFG parsing results for
the Negra and Tu?Ba-D/Z corpora. We show that
these different annotation schemes lead to very sig-
nificant differences in PARSEVAL scores for un-
lexicalized PCFG parsing models, but grammatical
function label and labeled dependency evaluations
for arguments of verbs show that this difference does
not carry over to measures which are relevant to the
semantic functor-argument structure. In contrast to
Ku?bler et al (2006) a grammatical function evalua-
tion on subjects, accusative objects, and dative ob-
jects establishes that Negra and Tu?Ba-D/Z perform
similarly when all types of words and phrases ap-
pearing as arguments are taken into consideration. A
labeled dependency evaluation based on grammati-
cal relations, which links this work to current work
on formalism-independent parser evaluation (e.g.,
Clark and Curran, 2007), shows that the parsing per-
formance for Negra and Tu?Ba-D/Z is comparable.
References
Adriane Boyd, 2007. Discontinuity Revisited: An Im-
proved Conversion to Context-Free Representations.
In Proceedings of the Linguistic Annotation Workshop
(LAW). Prague, Czech Republic.
Thorsten Brants and Oliver Plaehn, 2000. Interactive
Corpus Annotation. In Proceedings of the Second In-
ternational Conference on Language Resources and
Evaluation (LREC). Athens, Greece.
Thorsten Brants, Wojciech Skut and Hans Uszkoreit,
1999. Syntactic Annotation of a German Newspaper
Corpus. In Proceedings of the ATALA Treebank Work-
shop. Paris, France.
John Carroll, Guido Minnen and Ted Briscoe, 2003.
Parser evaluation: using a grammatical relation anno-
tation scheme. In A. Abeille? (ed.), Treebanks: Build-
ing and Using Parsed Corpora, Kluwer, Dordrecht.
31
Stephen Clark and James Curran, 2007. Formalism-
Independent Parser Evaluation with CCG and Dep-
Bank. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics (ACL).
Prague, Czech Republic.
Amit Dubey, 2004. Statistical Parsing for German: Mod-
eling Syntactic Properties and Annotation Differences.
Ph.D. thesis, Universita?t des Saarlandes.
Amit Dubey and Frank Keller, 2003. Probabilistic Pars-
ing Using Sister-Head Dependencies. In Proceedings
of the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL). Sapporo, Japan.
Tilman Ho?hle, 1986. Der Begriff ?Mittelfeld?, An-
merkungen u?ber die Theorie der topologischen Felder.
In Akten des Siebten Internationalen Germanistenkon-
gresses 1985. Go?ttingen, Germany.
Dan Klein and Christopher D. Manning, 2002. Fast Exact
Inference with a Factored Model for Natural Language
Parsing. In Advances in Neural Information Process-
ing Systems 15 (NIPS). Vancouver, British Columbia,
Canada.
Sandra Ku?bler, 2005. How do treebank annotation
schemes influence parsing results? Or how not to com-
pare apples and oranges. In Proceedings of the Con-
ference on Recent Advances in Natural Language Pro-
cessing (RANLP). Borovets, Bulgaria.
Sandra Ku?bler, Erhard W. Hinrichs and Wolfgang Maier,
2006. Is it really that difficult to parse German? In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP). Sydney,
Australia.
Roger Levy and Christopher Manning, 2003. Is it harder
to parse Chinese, or the Chinese Treebank? In Pro-
ceedings of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL).
Dekang Lin, 1995. A Dependency-based Method for
Evaluating Broad-Coverage Parsers. In Proceedings
of the International Joint Conference on Artificial In-
telligence (IJCAI). Montreal, Quebec, Canada.
Ines Rehbein and Josef van Genabith, 2007a. Treebank
Annotation Schemes and Parser Evaluation for Ger-
man. In Proceedings of the Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning. Prague,
Czech Republic.
Ines Rehbein and Josef van Genabith, 2007b. Why is it so
difficult to compare treebanks? TIGER and Tu?Ba-D/Z
revisited. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories (TLT). Bergen, Norway.
Marga Reis, 1980. On Justifying Topological Frames:
?Positional Field? and the Order of Nonverbal Con-
stituents in German. Documentation et Recherche
en Linguistique Allemande Contemporaine Vincennes
(DRLAV), 22/23.
Anne Schiller, Simone Teufel and Christine Thielen,
1995. Guidelines fu?r das Tagging deutscher Textcor-
pora mit STTS. Technical report, Universita?t Stuttgart,
Universita?t Tu?bingen, Germany.
Helmut Schmid, 2000. LoPar: Design and Implemen-
tation. Arbeitspapiere des Sonderforschungsbereiches
340 No. 149, Universita?t Stuttgart.
Heike Telljohann, ErhardW. Hinrichs, Sandra Ku?bler and
Heike Zinsmeister, 2005. Stylebook for the Tu?bingen
Treebank of Written German (Tu?Ba-D/Z). Technical
report, Seminar fu?r Sprachwissenschaft, Universita?t
Tu?bingen, Germany.
32
Coling 2010: Poster Volume, pages 267?275,
Beijing, August 2010
Exploring the Data-Driven Prediction of Prepositions in English
Anas Elghafari Detmar Meurers Holger Wunsch
Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
{aelgafar,dm,wunsch}@sfs.uni-tuebingen.de
Abstract
Prepositions in English are a well-known
challenge for language learners, and the
computational analysis of preposition us-
age has attracted significant attention.
Such research generally starts out by de-
veloping models of preposition usage for
native English based on a range of fea-
tures, from shallow surface evidence to
deep linguistically-informed properties.
While we agree that ultimately a com-
bination of shallow and deep features is
needed to balance the preciseness of ex-
emplars with the usefulness of generaliza-
tions to avoid data sparsity, in this paper
we explore the limits of a purely surface-
based prediction of prepositions.
Using a web-as-corpus approach, we in-
vestigate the classification based solely on
the relative number of occurrences for tar-
get n-grams varying in preposition usage.
We show that such a surface-based ap-
proach is competitive with the published
state-of-the-art results relying on complex
feature sets.
Where enough data is available, in a sur-
prising number of cases it thus is possible
to obtain sufficient information from the
relatively narrow window of context pro-
vided by n-grams which are small enough
to frequently occur but large enough
to contain enough predictive information
about preposition usage.
1 Introduction
The correct use of prepositions is a well-known
difficulty for learners of English, and correspond-
ingly the computational analysis of preposition
usage has attracted significant attention in re-
cent years (De Felice and Pulman, 2007; De Fe-
lice, 2008; Lee and Knutsson, 2008; Gamon et
al., 2008; Chodorow et al, 2007; Tetreault and
Chodorow, 2008a, 2008b).
As a point of reference for the detection of
preposition errors in learner language, most of
the research starts out by developing a model of
preposition usage for native English. For this
purpose, virtually all previous approaches em-
ploy a machine learning setup combining a range
of features, from surface-based evidence to deep
linguistically-informed properties. The overall
task is approached as a classification problem
where the classes are the prepositions and the in-
stances to be classified are the contexts, i.e., the
sentences with the prepositions omitted.
A focus of the previous literature is on the ques-
tion which linguistic and lexical features are the
best predictors for preposition usage. Linguistic
features used include the POS tags of the sur-
rounding words, PP attachment sites, WordNet
classes of PP object and modified item. Lexical
features used include the object of the PP and the
lexical item modified by the PP. Those syntactic,
semantic and lexical features are then extracted
from the training instances and used by the ma-
chine learning tool to predict the missing preposi-
tion in a test instance.
While we agree that ultimately a combination
of shallow and linguistically informed features is
needed to balance the preciseness of exemplars
267
with the usefulness of generalizations to avoid
data sparsity problems, in this paper we want to
explore the limits of a purely surface-based pre-
diction of prepositions. Essentially, our ques-
tion is how much predictive information can be
found in the immediate distributional context of
the preposition. Is it possible to obtain n-gram
contexts for prepositions which are small enough
to occur frequently enough in the available train-
ing data but large enough to contain enough pre-
dictive information about preposition usage?
This perspective is related to that underlying
the variation-n-gram approach for detecting errors
in the linguistic annotation of corpora (Dickin-
son and Meurers, 2003; Dickinson and Meurers,
2005; Boyd et al, 2008). Under that approach, er-
rors in the annotation of linguistic properties (lexi-
cal, constituency, or dependency information) are
detected by identifying units which recur in the
corpus with sufficient identical context so as to
make variation in their annotation unlikely to be
correct. In a sense, the recurring n-gram contexts
are used as exemplar references for the local do-
mains in which the complex linguistic properties
are established. The question now is to what ex-
tent basic1 n-gram contexts can also be success-
fully used to capture the linguistic properties and
relations determining preposition usage, explor-
ing the trade-off expressed in the question ending
the previous paragraph.
To address this question, in this paper we make
use of a web-as-corpus approach in the spirit of
Lapata and Keller (2005). We employ the Yahoo
search engine to investigate a preposition classifi-
cation setup based on the relative number of web
counts obtained for target n-grams varying in the
preposition used. We start the discussion with a
brief review of key previous approaches and the
results they obtain for the preposition classifica-
tion task in native English text. In section 2,
we then describe the experimental setup we used
1While Dickinson and Meurers (2005) also employ dis-
continuous n-grams, we here focus only on contiguous n-
gram contexts. Using discontinuous n-gram contexts for
preposition prediction could be interesting to explore in the
future, once, as a prerequisite for the effective generation
of discontinuous n-grams, heuristics have been identified for
when which kind of discontinuities should be allowed to arise
for preposition classification contexts.
for our exploration and discuss our results in sec-
tion 3.
1.1 Previous work and results
The previous work on the preposition prediction
task varied in i) the features selected, ii) the num-
ber of prepositions tackled, and iii) the training
and testing corpora used.
De Felice (2008) presents a system that (among
other things) is used to predict the correct prepo-
sition for a given context. The system tackles the
nine most frequent prepositions in English: of, to,
in, for, on, with, at, by, from. The approach uses a
wide variety of syntactic and semantic features:
the lexical item modified by the PP, the lexical
item that occurs as the object of the preposition,
the POS tags of three words to the left and three
words to the right of the preposition, the grammat-
ical relation that the preposition is in with its ob-
ject, the grammatical relation the preposition is in
with the word modified by the PP, and the Word-
Net classes of the preposition?s object and the lex-
ical item modified by the PP. De Felice (2008) also
used a named entity recognizer to extract general-
izations about which classes of named entities can
occur with which prepositions. Further, the verbs?
subcategorization frames were taken as features.
For features that used lexical sources (WordNet
classes, verbs subcategorization frames), only par-
tial coverage of the training and testing instances
is available.
The overall accuracy reported by De Felice
(2008) for this approach is 70.06%, testing on sec-
tion J of the British National Corpus (BNC) after
training on the other sections. As the most exten-
sive discussion of the issue, using an explicit set
of prepositions and a precisely specified and pub-
licly accessible test corpus, De Felice (2008) is
well-suited as a reference approach. Correspond-
ingly, our study in this paper is based on the same
set of prepositions and the same test corpus.
Gamon et al (2008) introduce a system for the
detection of a variety of learner errors in non-
native English text, including preposition errors.
For the preposition task, the authors combine the
outputs of a classifier and a language model. The
language model is a 5-gram model trained on the
English Gigaword corpus. The classifier is trained
268
on Encarta encyclopedia and Reuters news text.
It operates in two stages: The presence/absence
classifier predicts first whether a preposition needs
to be inserted at a given location. Then, the choice
classifier determines which preposition is to be in-
serted. The features that are extracted for each
possible insertion site come from a six-token win-
dow around the possible insertion site. Those fea-
tures are the relative positions, POS tags, and sur-
face forms of the tokens in that window. The
choice classifier predicts one of 13 prepositions:
in, for, of, on, to, with, at, by, as, from, since,
about, than, and other. The accuracy of the choice
classifier, the part of the system to which the work
at hand is most similar, is 62.32% when tested on
text from Encarta and Reuters news.
Tetreault and Chodorow (2008a) present a sys-
tem for detecting preposition errors in learner text.
Their approach extracts a total of 25 features from
the local contexts: the adjacent words, the heads
of the nearby phrases, and the POS tags of all
those. They combine word-based features with
POS tag features to better handle cases where a
word from the test instance has not been seen
in training. For each test instance, the system
predicts one of 34 prepositions. In training and
testing performed on the Encarta encyclopedia,
Reuters news text and additional training material
an accuracy figure of 79% is achieved.
Bergsma et al (2009) extract contextual fea-
tures from the Google 5-gram corpus to train an
SVM-based classifier for predicting prepositions.
They evaluate on 10 000 sentences taken from the
New York Times section of the Gigaword corpus,
and achieve an accuracy of 75.4%.
Following De Felice (2008, p. 66), we summa-
rize the main results of the mentioned approaches
to preposition prediction for native text in Fig-
ure 1.2 Since the test sets and the prepositions tar-
geted differ between the approaches, such a com-
parison must be interpreted with caution. In terms
of the big picture, it is useful to situate the results
with respect to the majority baseline reported by
De Felice (2008). It is obtained by always choos-
ing of as the most common preposition in section
J of the BNC. De Felice also reports another inter-
2The Gamon et al (2008) result differs from the one re-
ported in De Felice (2008); we rely on the original paper.
esting figure included in Figure 1, namely the ac-
curacy of the human agreement with the original
text, averaged over two English native-speakers.
Approach Accuracy
Gamon et al (2008) 62.32%
Tetreault and Chodorow (2008a) 79.00%
Bergsma et al (2009) 75.50%
De Felice (2008) system 70.06%
Majority baseline (of) 26.94%
Human agreement 88.60%
Figure 1: Preposition prediction results
2 Experiments
2.1 Data
As our test corpus, we use section J of the BNC,
the same corpus used by De Felice (2008). Based
on the tokenization as given in the corpus, we
join the tokens with a single space, which also
means that punctuation characters end up as sep-
arate, white-space separated tokens. We select all
sentences that contain one or more prepositions,
using the POS annotation in the corpus to iden-
tify the prepositions. The BNC is POS-annotated
with the CLAWS-5 tagset, which distinguishes the
two tags PRF for of and PRP for all other preposi-
tions.3 We mark every occurrence of these prepo-
sition tags in the corpus, yielding one prediction
task for each marked preposition. For example,
the sentence (1) yields four prediction tasks, one
for each of the prepositions for, of, from, and in in
the sentence.
(1) But for the young, it is rather a question
of the scales falling from their eyes, and
having nothing to believe in any more.
In each task, one preposition is masked using
the special marker -*-MASKED-*-. Figure 2
shows the four marked-up prediction tasks result-
ing for example (1).
Following De Felice (2008), we focus our ex-
periments on the top nine prepositions in the
BNC: of, to, in, for, on, with, at, by, from. For
3http://www.natcorp.ox.ac.uk/docs/URG/
posguide.html#guidelines
269
But -*-MASKED-*-for the young , it is
rather a question of the scales falling
from their eyes , and having nothing to
believe in any more .
But for the young , it is rather a
question -*-MASKED-*-of the scales
falling from their eyes , and having
nothing to believe in any more .
But for the young , it is rather
a question of the scales falling
-*-MASKED-*-from their eyes , and having
nothing to believe in any more .
But for the young , it is rather a
question of the scales falling from
their eyes , and having nothing to
believe -*-MASKED-*-in any more .
Figure 2: Four prediction tasks for example (1)
each occurrence of these nine prepositions in sec-
tion J of the BNC, we extract one prediction task,
yielding a test set of 522 313 instances.
Evaluating on this full test set would involve a
prohibitively large number of queries to the Ya-
hoo search engine. We therefore extract a ran-
domly drawn subset of 10 000 prediction tasks.
From this subset, we remove all prediction tasks
which are longer than 4000 characters in length,
as Yahoo only supports queries up to that length.
Finally, in a web-as-corpus setup, the indexing of
the web pages performed by the search engine es-
sentially corresponds to the training step in a typi-
cal machine learning setup. In order to avoid test-
ing on the training data, we thus need to ensure
that the test cases are based on text not indexed by
the search engine. To exclude any such cases, we
query the search engine with each complete sen-
tence that a prediction task is based on and remove
any prediction task for which the search engine re-
turns hits for the complete sentence. The final test
set consists of 8060 prediction tasks.4
2.2 Experimental Setup
Recall that the general issue we are interested in
is whether one can obtain sufficient information
from the relatively narrow distributional window
of context provided by n-grams which are small
enough to occur frequently enough in the training
data but large enough to contain enough predic-
4For a copy of the test set, just send us an email.
tive information about preposition usage for the
instances to be classified. By using a web-as-
corpus approach we essentially try to maximize
the training data size. For the n-gram size, we ex-
plore the use of a maximum order of 7, containing
the preposition in the middle and three words of
context on either side.
For each prediction task, we successively insert
one of the nine most frequent prepositions into
the marked preposition slot of the 8060 n-grams
obtained from the test set. Thus, for each pre-
diction task, we get a cohort consisting of nine
different individual queries, one query for each
potential preposition. For example, the second
prediction task of Figure 2 yields the cohort of
nine queries in Figure 3 below, where the candi-
date prepositions replace the location marked by
-*-MASKED-*-of. The correct preposition of
is stripped off and kept for later use in the evalua-
tion step.
1. rather a question of the scales
falling
2. rather a question to the scales
falling
3. rather a question in the scales
falling
...
9. rather a question from the scales
falling
Figure 3: Cohort of nine queries resulting for the
second prediction task of Figure 2
In cases where a preposition is closer than four
words to the beginning or the end of the corre-
sponding sentence, a lower-order n-gram results.
For example, in the first prediction task in Fig-
ure 2, the preposition occurs already as the sec-
ond word in the sentence, thus not leaving enough
context to the left of the preposition for a sym-
metric 7-gram. Here, the truncated asymmetric 5-
gram ?But <prep> the young ,? includ-
ing only one word of context on the left would
get used.
We issue each query in a cohort to the Ya-
hoo search engine, and determine the number
of hits returned for that query. To that end,
we use Yahoo?s BOSS service, which offers a
270
JSON interface supporting straightforward auto-
mated queries. As part of its response to a query,
the BOSS service includes the deephits field,
which gives an ?approximate count that reflects
duplicate documents and all documents from a
host?.5 In other words, this number is an approx-
imate measure of how many web pages there are
that contain the search pattern.
With the counts for all nine queries in a cohort
retrieved from Yahoo, we select the preposition of
the query with the highest count. For the cases
in which none of the counts in a 7-gram cohort is
greater than zero, we use one of two strategies:
In the baseline condition, for all n-gram cohorts
with zero counts (5160 out of the 8060 cases) we
predict the most frequent preposition of, i.e., the
majority baseline. This results in an overall accu-
racy of 50%.
In the full back-off condition, we explore the
trade-off between the predictive power of the n-
gram as context and the likelihood of having seen
this n-gram in the training material, i.e., finding
it on the web. In this paper we never abstract or
generalize away from the surface string (e.g., by
mapping all proper names to an abstract name tag;
but see the outlook discussion at the end of the pa-
per), so the only option for increasing the number
of occurrences of an n-gram is to approximate it
with multiple shorter n-grams.
Concretely, if no hits could be found for any of
the queries in a cohort, we back off to the sum
of the hits for the two overlapping 6-grams con-
structed in the way illustrated in Figure 4.
[rather a question of the scales falling]
?
[rather a question of the scales]
[a question of the scales falling]
Figure 4: Two overlapping 6-grams approximate
a 7-gram for back-off.
If still no hits can be obtained after backing off
to 6-grams for any of the queries in a cohort, the
system backs off further to overlapping 5-grams,
and so on, down to trigrams.6
5Cited from http://developer.yahoo.com/
search/boss/boss_guide/ch02s02.html
6When backing off, the left-most and the right-most tri-
3 Results
Figure 5 shows the results of the full back-off
approach. Compared to the baseline condition,
accuracy goes up significantly to 76.5%. Thus,
the back-off strategy is effective in increasing the
amount of available data using lower-order n-
grams. This increase of data is also reflected in
the number of cases with zero counts for a cohort,
which goes down to none.
Full back-off
Correct 6166
Incorrect 1894
Total 8060
Accuracy 76.5%
Figure 5: Overall results of our experiments.
Figure 6 provides a detailed analysis of the
back-off experiment. It lists back-off sequences
separately for each maximum n-gram order. The
prediction tasks for which a full 7-gram can be
extracted are displayed in the third column, with
back-off orders of 6 down to 3. Prediction tasks
for which only asymmetric 6-grams can be ex-
tracted follow in column 4, and so on until 4-
grams. There are no predictions tasks that are
shorter than four words. Therefore, n-grams with
a length of less than 4 do not occur.
The ?sum? column shows the combined results
of the full 7-gram prediction tasks and the pre-
diction tasks involving truncated, asymmetric n-
grams of lower orders.
There are 6999 prediction tasks for which full
7-grams can be extracted. The remaining 1061
of the 8060 prediction tasks are the cases where
the system extracts only asymmetric lower-order
n-grams, for the reasons explained in section 2.2.
For 2195 of the 6999 7-gram prediction tasks,
we find full 7-gram contexts on the web, of which
1931 lead to a correct prediction, and 264 to an
incorrect one, leaving 4804 prediction tasks still
to be solved through the back-off approach. Thus,
full 7-gram contexts lead to high-quality predic-
tions at 88% precision, but they are rare and with
a recall of 28,7% cover only a fraction of all cases.
gram do not include the target preposition of the original 7-
gram. However, this only affects 13 cases, cf. Figure 6.
271
sum 7-grams 6-grams 5-grams 4-grams
(3 + prep + 3) (truncated 7-gram) (truncated 7-gram) (truncated 7-gram)
Total 8060 6999 656 182 223
Predictions 2900 2195 379 119 207
correct 2495 1931 326 91 147
incorrect 405 264 53 28 60
Requiring back-off 5160 4804 277 63 16
Precision 86% 88% 86% 76.5% 71%
Recall 32.6% 28.7% 79.6% 59.1% 90.2%
Back-off order 6
Predictions 2028 2028
correct 1620 1620
incorrect 408 408
Still requiring back-off 2776 2776
Predict. orders 7+6 4223 4223
correct 3551 3551
incorrect 672 672
Precision 84.1% 84.1%
Recall 56.1% 56.1%
Back-off order 5
Predictions 2180 2020 160
correct 1542 1411 131
incorrect 638 609 29
Still requiring back-off 873 756 117
Predict. orders 7 ? 5 6782 6243 539
correct 5419 4962 457
incorrect 1363 1281 82
Precision 79.9% 79.5% 84.8%
Recall 86.1% 86.8% 79.6%
Back-off order 4
Predictions 905 743 106 56
correct 488 382 68 38
incorrect 417 361 38 18
Still requiring back-off 31 13 11 7
Predict. orders 7 ? 4 7806 6986 645 175
correct 5998 5344 525 129
incorrect 1808 1642 120 46
Precision 76.8% 76.5% 81.4% 73.7%
Recall 99.5% 99.8% 97.9% 94.9%
Back-off order 3
Predictions 47 13 11 7 16
correct 21 5 7 3 6
incorrect 26 8 4 4 10
Still requiring back-off 0 0 0 0 0
Predict. orders 7 ? 3 8060 6999 656 182 223
correct 6166 5349 532 132 153
incorrect 1894 1650 124 50 70
Precision 76.5% 76.4% 81.1% 72.5% 68.6%
Recall 100% 100% 100% 100% 100%
Figure 6: The results of our experiments
272
Figure 7: Development of precision and recall in
relation to back-off order
Approximating 7-grams with two overlapping
6-grams as the first back-off step provides the
evidence needed to correctly predict 1620 addi-
tional prepositions, with 408 additional false pre-
dictions. The number of correctly solved predic-
tion tasks thus rises to 3551, and the number of
incorrect predictions rises to 672. This back-off
step almost doubles recall (56.1%). At the same
time, precision drops to 84.1%. For 2776 pre-
diction tasks, a further back-off step is necessary
since still no evidence can be found for them. This
pattern repeats with the back-off steps that fol-
low. To summarize, by adding more data using
less restricted contexts, more prediction tasks can
be solved. The better coverage however comes at
the price of reduced precision: Less specific con-
texts are worse predictors of the correct preposi-
tion than more specific contexts.
Figure 7 visualizes the development of preci-
sion and recall with full and truncated 7-grams
counted together as in the ?sum? column in Fig-
ure 6. With each back-off step, more prediction
tasks can be solved (as shown by the rising recall
curve). At the same time, the overall quality of
the predictions drops due to the less specific con-
texts (as shown by the slightly dropping precision
curve). While the curve for recall rises steeply,
the curve for precision remains relatively flat. The
back-off approach thus succeeds in adding data
while preserving prediction quality.
As discussed above, we use the same set of
prepositions and test corpus as De Felice (2008),
but only make use of 8060 test cases. Figure 8
shows that the accuracy stabilizes quickly after
about 1000 predictions, so that the difference in
the size of the test set should have no impact on
the reported results.
Figure 8: The accuracy of the n-gram prediction
stabilizes quickly.
4 Conclusions and Outlook
In this paper, we explored the potential and the
limits of a purely surface-based strategy of pre-
dicting prepositions in English. The use of
surface-based n-grams ensures that fully specific
exemplars of a particular size are stored in train-
ing, but avoiding abstractions in this way leads to
the well-known data sparsity issues. We showed
that using a web-as-corpus approach maximizing
the size of the ?training data?, one can work with
n-grams which are large enough to predict the oc-
currence of prepositions with significant precision
while at the same time ensuring that these specific
n-grams have actually been encountered during
?training?, i.e., evidence for them can be found
on the web.
For the random sample of the BNC section J
we tested on, the surface-based approach results
in an accuracy of 77% for the 7-gram model with
back-off to overlapping shorter n-grams. It thus
outperforms De Felice?s (2008) machine learning
273
approach which uses the same set of prepositions
and the full BNC section J as test set. In broader
terms, the result of our surface-based approach
is competitive with the state-of-the art results for
preposition prediction in English using machine
learning to combine sophisticated sets of lexical
and linguistically motivated features.
In this paper, we focused exclusively on the
impact of n-gram size on preposition prediction.
Limiting ourselves to pure surface-based informa-
tion made it possible to maximize the ?training
data? by using a web-as-corpus approach. Return-
ing from this very specific experiment to the gen-
eral issue, there are two well-known approaches
to remedy the data sparseness problem arising
from storing large, specific surface forms in train-
ing. On the one hand, one can use smaller ex-
emplars, which is the method we used as back-
off in our experiments in this paper. This only
works if the exemplars contain enough context for
the linguistic property or relation that we need to
capture the predictive power. On the other hand,
one can abstract parts of the surface-based train-
ing instances to more general classes. The cru-
cial question this raises is which generalizations
preserve the predictive power of the exemplars
and can reliably be identified. The linguistically-
informed features used in the previous approaches
in the literature naturally provide interesting in-
stances of answers to this question. In the fu-
ture, we intend to compare the results we ob-
tained using the web-as-corpus approach with one
based on the Google-5-gram corpus to study us-
ing controlled, incremental shallow-to-deep fea-
ture development which abstractions or linguistic
generalizations best preserve the predictive con-
text while lowering the demands on the size of the
training data.
Turning to a linguistic issue, it could be use-
ful to distinguish between lexical and functional
prepositions when reporting test results. This is
an important distinction because the information
needed to predict functional prepositions typically
is in the local context, whereas the information
needed to predict lexical prepositions is not nec-
essarily present locally. To illustrate, a competent
human speaker presented with the sentence John
is dependent his brother and asked to fill in
the missing preposition, would correctly pick on.
This is a case of a functional preposition where
the relevant information is locally present: the ad-
jective dependent selects on. On the other hand,
the sentence John put his bag the table is
more problematic, even for a human, since both
on and under are reasonable choices; the infor-
mation needed to predict the omitted preposition
in this case is not locally present. In line with
the previous research, in the work in this paper
we made predictions for all prepositions alike. In
the future, it could be useful to annotate the test
set so that one can distinguish functional and lex-
ical uses and report separate figures for these two
classes in order to empirically confirm their dif-
ferences with respect to locality.
References
Bergsma, Shane, Dekang Lin, and Randy Goebel.
2009. Web-scale n-gram models for lexical disam-
biguation. In IJCAI?09: Proceedings of the 21st in-
ternational jont conference on Artifical intelligence,
pages 1507?1512, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
Boyd, Adriane, Markus Dickinson, and Detmar Meur-
ers. 2008. On detecting errors in dependency tree-
banks. Research on Language and Computation,
6(2):113?137.
Chodorow, Martin, Joel Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involv-
ing prepositions. In Proceedings of the 4th ACL-
SIGSEM Workshop on Prepositions, pages 25?30,
Prague, Czech Republic, June.
De Felice, Rachele and Stephen Pulman. 2007. Au-
tomatically acquiring models of preposition use. In
Proceedings of the 4th ACL-SIGSEM Workshop on
Prepositions, pages 45?50, Prague, Czech Republic,
June. Association for Computational Linguistics.
De Felice, Rachele. 2008. Automatic Error Detection
in Non-native English. Ph.D. thesis, St Catherine?s
College, University of Oxford.
Dickinson, Markus and W. Detmar Meurers. 2003.
Detecting errors in part-of-speech annotation. In
Proceedings of the 10th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL-03), pages 107?114, Budapest,
Hungary.
Dickinson, Markus and W. Detmar Meurers. 2005.
Detecting errors in discontinuous structural anno-
274
tation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 322?329.
Gamon, Michael, Jianfeng Gao, Chris Brockett,
Alexander Klementiev, William Dolan, Dmitriy Be-
lenko, and Lucy Vanderwende. 2008. Using con-
textual speller techniques and language modeling
for esl error correction. In Proceedings of IJCNLP,
Hyderabad, India.
Lapata, Mirella and Frank Keller. 2005. Web-
based models for natural language processing. ACM
Transactions on Speech and Language Processing,
2(1):1?30, February.
Lee, John and Ola Knutsson. 2008. The role of pp
attachment in preposition generation. In Gelbukh,
A., editor, Proceedings of CICLing 2008.
Tetreault, Joel and Martin Chodorow. 2008a. Na-
tive judgments of non-native usage: Experiments
in preposition error detection. In Proceedings of
COLING-08, Manchester.
Tetreault, Joel and Martin Chodorow. 2008b. The ups
and downs of preposition error detection in esl writ-
ing. In Proceedings of COLING-08, Manchester.
275
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1962?1973, Dublin, Ireland, August 23-29 2014.
Exploring Syntactic Features for Native Language Identification:
A Variationist Perspective on Feature Encoding and
Ensemble Optimization
Serhiy Bykh
Seminar f?ur Sprachwissenschaft
Universit?at T?ubingen
sbykh@sfs.uni-tuebingen.de
Detmar Meurers
Seminar f?ur Sprachwissenschaft
Universit?at T?ubingen
dm@sfs.uni-tuebingen.de
Abstract
In this paper, we systematically explore lexicalized and non-lexicalized local syntactic features
for the task of Native Language Identification (NLI). We investigate different types of feature
representations in single- and cross-corpus settings, including two representations inspired by a
variationist perspective on the choices made in the linguistic system. To combine the different
models, we use a probabilities-based ensemble classifier and propose a technique to optimize and
tune it. Combining the best performing syntactic features with four types of n-grams outperforms
the best approach of the NLI Shared Task 2013.
1 Introduction and related work
Native Language Identification (NLI) is the task of identifying the native language of a writer by analyz-
ing texts written by this writer in a non-native language. NLI started to attract attention in computational
linguistics with the work of Koppel et al. (2005). Since then, the interest has increased steadily, leading
to the First NLI Shared Task in 2013, with 29 participating teams (Tetreault et al., 2013).
The task of NLI is usually treated as a text classification problem with the L1s as classes. A wide range
of features, reaching from character or word-based n-grams to different types of syntactic models have
been employed in NLI. For example, Wong and Dras (2011) utilized character and part-of-speech (POS)
n-grams as well as cross-sections of parse trees and Context-Free Grammar (CFG) features, i.e., local
trees. Their approach with a binary representation of non-lexicalized rules (except for those rules lexi-
calized with function words and punctuation) outperformed a setup using only lexical features, such as
n-grams, on data from the International Corpus of Learner English (ICLE; Granger et al., 2002). Swanson
and Charniak (2012) used binary feature representations of CFG and Tree Substitution Grammar (TSG)
rules replacing terminals (except for function words) by a special symbol. TSG outperformed CFG fea-
tures in their settings. Among several options, Brooke and Hirst (2012) explored using non-lexicalized
CFG production rules in a binary feature encoding on three corpora: ICLE, FCE (Yannakoudakis et al.,
2011), and Lang-8 (Brooke and Hirst, 2013a). The authors conclude that including CFG features gen-
erally boosts the performance of the system. In the context of the First NLI Shared Task, in Bykh et al.
(2013) we showed that non-lexicalized frequency-based CFG features contribute relevant information.
Other recent work has focused on TSGs (Tetreault et al., 2012; Brooke and Hirst, 2013b; Swanson and
Charniak, 2012; Swanson and Charniak, 2013; Swanson, 2013; Malmasi et al., 2013).
Before extending syntactic modeling further, in this paper we want to systematically explore the range
of options involving CFG rule features for NLI. We consider non-lexicalized and lexicalized CFG fea-
tures, and different feature representations, from binary encodings to a normalized frequency encoding
inspired by a variationist sociolinguistic perspective.
Previous research in this domain often limited the use of lexicalized rules given that the lexicalization
may lead to an unintended topic or domain dependence. Yet, NLI research has since established that
lexical features, such as word-based n-grams, are among the best performing features both in single-
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1962
and in cross-corpus settings (Brooke and Hirst, 2012; Bykh and Meurers, 2012; Jarvis and Crossley,
2012; Brooke and Hirst, 2013b; Bykh et al., 2013; Gebre et al., 2013; Jarvis et al., 2013; Lynum, 2013),
making them an essential component of any approach with state-of-the-art performance. At the same
time, the question whether an NLI approach and its results capture general characteristics of language
and language learning instead of only encoding the characteristics of a specific data set remains an
essential concern. In the experiments in this paper, we thus include experiments on both a topic-balanced
single-corpus and on a highly heterogeneous cross-corpus data set.
The range of feature types used in NLI research raises a further question, namely how the different
sources of information are best combined. The most simple solution is to put all features into a single
vector. However, Tetreault et al. (2012) pointed out that the performance can be increased by using a
probability-estimate based ensemble (meta-classifier), which was confirmed in Bykh et al. (2013) and
Cimino et al. (2013). But which models are worth integrating into such a meta-classifier? Some of
the models may be redundant despite performing well individually; on the other hand, some models
may improve the ensemble despite performing relatively poorly by itself. We explore this issue by
implementing a basic ensemble optimization algorithm performing model selection.
In terms of the structure of the paper, in section 2 we first introduce the corpora used in the single-
corpus and cross-corpus settings. Section 3 then presents the first set of experiments, systematically
exploring lexicalized and unlexicalized Context-Free Grammar Rules (CFGR) as features. Given the
significant complexity of the overall feature space, we then explore model selection for optimizing the
ensemble classifier in section 4. In section 5, we combine the CFGR features with n-grams, resulting in
the best accuracy reported for the standard TOEFL11 test set. Section 6 sums up the paper and sketches
some directions for future research.
2 Data
The research in this paper makes use of two sets of data:
First, there is the TOEFL11 (T11) data set (Blanchard et al., 2013), which was introduced for the NLI
Shared Task 2013 and has become a standard frame of reference for NLI research. We use this standard
setup for single-corpus evaluation, where each L1 is represented by 1100 essays, of which 100 essays
are singled out in the standard test set. The remaining 1000 essays per L1 (= T11 train ? dev) constitute
our training data in the single-corpus settings.
Second, we make use of a range of other learner corpora to study how well the results generalize.
Concretely, for our cross-corpus settings we employ the NT11 corpus of Bykh et al. (2013), which
consists of the ICLE (Granger et al., 2009), FCE (Yannakoudakis et al., 2011), BALC (Randall and
Groom, 2009), ICNALE (Ishikawa, 2011), and T
?
UTEL-NLI (Bykh et al., 2013) corpora. In total NT11
includes 5843 texts, with the following division into languages: Arabic (846), Chinese (1048), French
(456), German (500), Hindu (400), Italian (467), Japanese (447), Korean (684), Spanish (446), Telugu
(200), Turkish (349). In the cross-corpus settings, we train on NT11 and test on the standard T11 test set.
3 Systematically exploring Context-Free Grammar Rules (CFGR)
3.1 Features
In this paper, we focus on the CFG production rules (CFGR) as syntactic features for the task of NLI.
CFG rules are the most basic and widely used local syntactic units modularizing the overall syntactic
analysis of a sentence. We parsed the T11 and NT11 corpora using the Stanford Parser (Klein and
Manning, 2002) and extracted all CFG rules from the T11 and NT11 training sets. On this basis we
defined the following tree feature types:
1. CFGR
ph
: Only phrasal CFG production rules excluding all terminals
? S? NP VP, NP? D NN, . . .
2. CFGR
lex
: Only lexicalized CFG production rules of the type preterminal? terminal
? JJ? nice, JJ? quick, NN? vacation, . . .
3. CFGR
ph?lex
= CFGR
ph
? CFGR
lex
(i.e., the union of the above two)
1963
A variationist perspective on feature representation We explore four different feature representa-
tions: The two standard ones are a frequency-based (freq) representation, where the values are the raw
counts of the occurrences of the rule in the given parsed document, and a binary (bin) representation,
which only indicates whether a rule is present or absent in that document.
Complementing these standard feature representations, we explored two options that take as starting
point the observation that CFG rules with the same left-hand side category represent different ways to
rewrite that category. So in a sense, under a top-down perspective, there is a choice between different
ways of realizing a given category.
This is reminiscent of variationist sociolinguistic analysis, where one studies the linguistic choices
made by a given speaker and connects the choices with extra-linguistic variables such as the age or
gender of a speaker. For example, in William Labov?s field-defining study ?The Social Stratification
of (r) in New York City Department Stores? from his book ?Sociolinguistic Patterns? (Labov, 1972),
he found that the presence or absence of the consonant [r] in postvocalic position (e.g., car, fourth)
correlates with the ranking of people in status or prestige, i.e., social stratification. Speakers thus make
choices in how to realize a given variable by producing one of the variants (see also Tagliamonte, 2011).
Inspired by this perspective, in Meurers et al. (2013) we discussed how a variationist perspective on
syntactic alternations can provide interpretable features for NLI classification.
Under a variationist perspective, producing one of the variants of a given variable also means not
choosing the other variants of that variable. So it is this grouping of observations that we want to take
into account in terms of encoding local trees as features when we interpret the mother category as the
variable to be realized and the different CFG rules with that left-hand side as variants of that variable.
This results in two feature representations, a simple one (var
s
) and a weighted one (var
w
).
The var
s
and var
w
frequency normalizations for each variant v from the set of variants V realizing a
particular variable out of the set of variables V is defined as follows:
var
s
(v ? V ) =
f(v)
F (V )
var
w
(v ? V ) = var
s
(v) ? w(V )
Here, f(v) yields the frequency x of a particular variant v, F (V ) is the sum over the frequencies of
all variants v realizing the variable V , and w(V ) is the weight for the variable V :
f(v) = x
F (V ) =
?
v?V
f(v)
w(V ? V ) =
F (V )
n
?
i=1
F (V
i
)
The weighting applied in var
w
takes into account the frequency proportion of each variable V in the
overall variables set V , assigning higher weights for more frequent variables. Mathematically it reduces
to normalizing each variant by the sum of the frequencies over all variants across all variables, i.e., to
the relative frequency of each variant v with respect to the set of all variables V . At the same time,
we will see in the next section that the individual variables keep an independent status in terms of the
classification setup, where we train a separate classifier for each variable.
1964
3.2 Results
Classifier We use the L2-regularized Logistic Regression from the LIBLINEAR package (Fan et al.,
2008), which we accessed through WEKA (Hall et al., 2009). To obtain results for all feature repre-
sentations which are comparable across the different settings we uniformly scale all values employing
the -Z option of WEKA. This means that the freq feature representation based on the raw frequencies
in essence also becomes normalized. This is particularly relevant in the context of the cross-corpus
evaluation, where raw frequencies are particularly questionable given highly variable text sizes.
Single- vs. cross-corpus results The results for the three feature types using the four different feature
representations are presented in Table 1. The chance baseline for the given data setup is 9.1%. There
are big accuracy differences between the single- and cross-corpus settings despite very similar feature
counts. The drop for the cross-corpus settings is roughly around
1
2
compared to the single-corpus settings.
This is in line with previous results on the same data sets using a wide range of features (Bykh et al.,
2013), confirming the fact that obtaining high cross-corpus results remains challenging in NLI.
features single-corpus (sc): T11 training
freq bin var
s
var
w
feat. #
CFGR
ph
50.00% 44.27% 48.45% 49.82% 14,713
CFGR
lex
75.73% 72.45% 71.00% 76.91% 83,402
CFGR
ph?lex
78.18% 73.55% 75.36% 78.82% 98,115
features cross-corpus (cc): NT11 training
freq bin var
s
var
w
feat. #
CFGR
ph
21.27% 22.91% 26.27% 27.73% 15,253
CFGR
lex
26.73% 32.00% 28.82% 36.82% 78,923
CFGR
ph?lex
28.27% 34.27% 32.55% 38.82% 94,176
Table 1: Results for the CFGR feature variants obtained on the standard T11 test set
Best feature type The CFGR
lex
feature type clearly outperforms the more abstract CFGR
ph
feature
type, yielding up to 28% difference in accuracy for the single-corpus and up to 9% for the cross-corpus
settings. In contrast to previous research assuming that lexicalized trees are too topic-specific, the results
show that CFGR
lex
is a valuable feature type in both the single-corpus and the cross-corpus settings.
The CFGR
lex
features combine syntactic and lexical information, such as the fact that a given token with
a particular POS is used, e.g., the token can being used as a noun in There is a can of beer in the fridge
instead of as the more frequent modal verb use in He can dance. Note that this is different from using
word and POS unigrams as features, where the relevant connection is lost. In both the T11 data, which
is topic balanced, for single-corpus evaluation and the very heterogeneous NT11 data containing a wide
range of topics for cross-corpus evaluation, we obtained consistently better results for CFGR
lex
than for
CFGR
ph
. Some syntactic rules including lexical information thus seem to generalize well across topics.
Combining CFGR
ph
and CFGR
lex
into CFGR
ph?lex
gives an additional boost in performance.
Best feature representation There are clear differences in Table 1 between the results for the four
feature representations. var
w
yields the best accuracies in five out of six settings, across different feature
types and corpora.
The results show that WEKA-normalized raw frequencies such as freq yield the worst results in a
cross-corpus setting but perform very well single-corpus, which is in line with the assumption that raw
frequency features do not generalize well. In our experiments, the performance of freq in a cross-corpus
setting is up to 10.55% worse than what is yielded by var
w
, despite comparable single-corpus perfor-
mance. freq also consistently performs worse than var
s
in the cross-corpus setting, despite outperforming
var
s
single-corpus.
1965
Using binary features (bin) yields better results cross-corpus than freq, whereas in the single-corpus
setting it is the other way round. The abstraction introduced by the binary feature representation thus
shows a positive effect in terms of the capability of the features to generalize to other data sets.
For the abstract CFGR
ph
features, var
s
performs better than freq or bin in the cross-corpus setting.
The fact that the var
w
is performing consistently better than var
s
shows that weighting is important.
Hence, incorporating the insight from variationist sociolinguistics is not only conceptually interesting as
a theoretical perspective, but also provides a quantitative advantage in terms of performance.
CFGR categories as variables As mentioned above, the best performance is achieved by combining
CFGR
ph
and CFGR
lex
into the CFGR
ph?lex
feature type using the weighted variationist feature rep-
resentation var
w
. Thus, we focused on that feature type and explored it more in depth. We did so by
splitting the overall var
w
normalized CFGR
ph?lex
feature set by the variable, i.e., the different mother
nodes. We trained separate models, where each of those models consists of features encoding the differ-
ent variants, i.e., the different realizations in which a given mother node can be rewritten. Our aim was
to investigate the accuracy of the individual variable-based models and their contribution to the overall
performance. Figures 1 and 2 depict the single-corpus (sc) and cross-corpus (cc) accuracies yielded by
each individual variable-based model, for presentation reasons shown separately for the CFGR
ph
and
the CFGR
lex
subsets.
ADJ
P
ADV
P
CON
JP
FRA
G INT
J LST NAC NP NX PP PRN PRT QP RRC S SBA
R
SBA
RQ SIN
V SQ UCP VP
WH
ADJ
P
WH
ADV
P
WH
NP
WH
PP X
0,00%
5,00%
10,00%
15,00%
20,00%
25,00%
30,00%
35,00%
40,00%
45,00%
50,00%
sccc
models
accu
racy
Figure 1: Accuracy for the individual CFGR
ph
variable based models, var
w
normalized
The CFGR
ph
results in Figure 1 show that a small subset of variables performs relatively well. Most
of the models perform poorly, yielding accuracies close to the chance baseline. The best performing
variables are essentially the main phrasal categories, such as S, NP, VP, PP, ADJP, ADVP or SBAR.
The results for the CFGR
lex
in Figure 2 show a similar pattern. There is a subset of variables which
perform relatively well, usually models based on the main POS categories, such as the nominal (NN) and
verbal (VB) categories as well as adjectives (JJ), prepositions (IN) and adverbs (RB). Some punctuation
marks also seem to play a role. The rest of the models yields accuracies around the chance baseline.
This might be due to data sparsity given that the main POS categories also are the most frequent. But
those main categories also have the highest number of variants through which they can be realized. The
good performance of the models for the variables with the highest number of variants thus confirms the
assumption that the choice of one of the realization options of a given category is influenced by the L1.
Should we focus only on those high-performing models ? or do the other models also contain relevant,
independent information which is worth preserving? We address that question in the next section.
1966
AA AD DJ PV CO NF RR RRG RRI TI LD FF FFS FFS
I FFI SDJ SXI SGS SGS
Q GB GBG GBI GS IUL JX WH 0B 0BD 0B, 0BF 0BS 0B% OD
J OS OS
Q
OG
B
5TG
B5
5GG
B5 1 Q 22 3 4 s ccm3mmo
d3mmo
em3mmo
ed3mmo
lm3mmo
ld3mmo
am3mmo
ad3mmo
um3mmo
ud3mmo
dm3mmo
ryyy
?????r
?yy?
??y?
Figure 2: Accuracy for the individual CFGR
lex
variable based models, var
w
normalized
4 Ensemble optimization and tuning
Ensemble generation To combine the individual models, we employ a probability-estimate-based en-
semble approach, following Tetreault et al. (2012) and Bykh et al. (2013). This meta-classifier combines
the probability distributions provided by the individual classifier for each of the incorporated models as
features. To obtain the ensemble training files, we performed 10-fold cross-validation for each model on
the corresponding training set and took the probability estimate distributions. For testing, we took the
probability estimate distribution yielded by each individual model trained on the corresponding training
set and tested on the T11 test set. To obtain the probability estimates for the individual models we used
LIBLINEAR as described in section 3.2. The ensembles were trained and tested using LIBSVM with an
RBF kernel (Chang and Lin, 2011), which outperformed LIBLINEAR for this purpose.
Ensemble optimization (+opt) The growing range of features used for NLI raises the question of how
to perform model selection. Even when analyzing a single feature type in depth, as we do in section 3.2,
we already must determine which of the low-performing models to keep in an ensemble. We approach
the question with a simple incremental ensemble optimization algorithm performing model selection.
Algorithm 1 Ensemble Optimization / Ensemble Model Selection
M
a
? {m
1
, ...,m
n
} . overall ensemble, i.e., all ensemble models
M
b
? ? . current best performing ensemble
while M
a
6= ? do . iterate until M
a
is empty
m
b
? MAX(M
a
) . get the model with the highest accuracy m
b
out of M
a
M
t
?M
b
? {m
b
} . join the previous best performing ensemble M
b
and {m
b
}
if ACC(M
t
) > ACC(M
b
) then . check if the new ensemble is performing better than M
b
M
b
?M
t
. if the accuracy improves, store the new ensemble in M
b
end if
REMOVE(m
b
,M
a
) . remove m
b
from M
a
end while
1967
In each iteration step the optimization algorithm shown in Algorithm 1 retrieves the current best single
model m
b
out of the model set M
a
(which is initialized with the overall model set for a particular setting),
joins it with the previous best performing ensemble M
b
(which is initialized to ?), compares the accuracy
of that new ensemble with the accuracy of the previous best ensemble. It retains the new ensemble as the
best ensemble if the accuracy improves, or keeps the previous best ensemble as best ensemble otherwise.
In Algorithm 1, we describe only the gist of the optimization, omitting some details to keep it transparent.
Some ambiguities have to be resolved. If there are several models in M
a
yielding the same accuracy, one
has to decide, which of them to pick as the next m
b
. We resolve that issue by always picking the model
with the least number of features. When several models yield the same accuracy and have the same
number of features, we resort to alphabetical order. The optimization is always carried out using 10-fold
cross-validation results on the training data (to obtain the accuracy ranking on M
a
and to perform each
optimization step). The test set is not part of the optimization at any point. Only after optimization is the
resulting ensemble applied to the test set and we report the corresponding accuracies.
Ensemble tuning (+all) In order to further tune the ensemble, we explore the following idea: We
generate a single ensemble model m
n+1
based on all of the features used in a particular setting, i.e., all the
features incorporated by the models m
1
. . .m
n
. Then we include that m
n+1
model in the M
a
ensemble
as just another model, and use that new M
+1
a
ensemble either directly or as basis for the optimization.
Since m
n+1
incorporates all of the features of interest for a particular setting, it is expected to yield more
reliable probability estimates than the other individual ensemble models in M
+1
a
, each covering only
a subset of that feature set. Incorporating such an m
n+1
into the ensemble may stabilize the resulting
system, i.e., the machine learning algorithms may learn to rely on m
n+1
in settings, where the rest of
the included models m
1
. . .m
n
show a rather poor individual performance and are of limited use. In the
tables and explanations below, we refer to the model m
n+1
as [all] and to the M
+1
a
ensemble as +all.
For building the m
n+1
model included in the M
+1
a
ensemble there are two options. We can build
it on the basis of the probabilities of the models or on the union of the original feature values of those
models. In the former case, the final ensemble model essentially is a meta-meta-classifier. For the settings
integrating the same type of feature representations (cf. results in Tables 2 and 4), we use the original
feature values merged into a single vector to build m
n+1
. For the settings integrating different feature
types (cf. results in Table 6), we use the probability estimates from the models m
1
. . .m
n
to build m
n+1
.
Ensemble results for the CFGR variables The ensemble results for the separate variable-based mod-
els for the CFGR
ph?lex
feature type are presented in Table 2. We provide single-corpus (sc) and cross-
corpus (cc) results for different ensemble settings, where +/- opt states whether ensemble optimization
was performed, and +/- all whether tuning was employed. Concretely, (-opt, -all) means that the ensem-
ble M
a
was used without any optimization or tuning, and correspondingly (+opt, +all) means that the
optimized and tuned version of M
a
(i.e., the optimized version of the ensemble M
+1
a
) was employed. In
the remaining two cases (+opt, -all) and (-opt, +all) either optimization or tuning was used, respectively.
The column baseline lists the corresponding results from Table 1, which were obtained by putting all the
features in a single vector. The number in parentheses specifies the number of models combined in the
ensemble: in the features column, it shows the overall number of separate variable-based models, and in
the +opt columns, it is the number of models selected by the optimization algorithm.
features data baseline ensemble
-opt +opt
-all +all -all +all
CFGR
ph?lex
(71) sc 78.82% 66.00% 79.18% 71.27% (14) 79.64% (8)
cc 38.82% 18.09% 34.18% 32.55% (10) 39.00% (1)
Table 2: Results for the CFGR
ph?lex
ensembles with different optimization settings
The results show that generating an ensemble using all of the individual variable-based models without
optimization and tuning (-opt, -all) leads to a big accuracy drop compared to the baseline. The fact that
1968
the drop in the cross-corpus setting is more than 20% is particularly striking. We assume that this is due
to the poor performance of most of the individual models, yielding probabilities of little use overall. The
few relatively well-performing models we discussed in section 3.2 apparently are flooded by the noise
introduced by the others. Thus, for a set of rather low-performing models without any optimization, it
seems preferable to provide the classifier with access to the individual features instead of to the noisy
probability estimates. The optimization (+opt, -all) leads to a clear improvement over the non-optimized
settings. In the single-corpus setting only 14 of the 71 models were kept and in cross-corpus only 10.
Table 3 shows the selected models in the order in which they are selected by the ensemble optimization
algorithm. For (+opt, -all), the table basically consists of the best performing variables (i.e., the models
containing as features the different ways to rewrite the given mother category) as discussed in section 3.2,
suggesting that the algorithm makes meaningful choices.
data CFGR
ph?lex
: selected models
+opt, -all +opt, +all
sc [NN]+[JJ]+[RB]+[NNS]+[VB]+[NP]+[S]+[VP] [all]+[NN]+[JJ]+[RB]+[PRP]+[VBN]+[NNP]+[WDT] (8)
+[IN]+[VBP]+[VBG]+[VBN]+[NNP]+[,] (14)
cc [NN]+[JJ]+[NNS]+[NP]+[RB]+[VB]+[VP]+[NNP] [all] (1)
+[S]+[IN] (10)
Table 3: The CFGR
ph?lex
model sets selected by optimization
The flipside of the coin is that low-performing models generally were not found to have a positive
effect and thus were not included. Yet, optimization by itself is not successful overall given that the
(+opt, -all) accuracy remains below the single feature set baseline.
Applying tuning without optimization (-opt, +all) outperforms the optimization result. Thus, includ-
ing the overall model [all] in the ensemble improves the meta-classifier. In the single-corpus setting, the
accuracy is slightly higher than the baseline, in cross-corpus it remains below the baseline.
Turning on both optimization and tuning (+opt, +all) yields the overall best results of Table 2, 79.64%
for single-corpus and 39% for the cross-corpus setting. The corresponding entry in Table 3 shows that
tuning significantly reduces the number of selected models. This is not unexpected given that the overall
model [all] essentially includes all the information. In the cross-corpus setting, [all] indeed is the only
model selected. Interestingly, in the single-corpus setting, the optimization algorithm identifies some
additional models to improve the accuracy, mainly ones that also perform well individually. While this
amounts to adding information that in principle is already available to the [all] model, the improvement
may stem from the abstract nature of the probability estimates used as features of the meta-classifier.
When both optimization and tuning are applied, the tuning apparently stabilizes the ensemble leading to
higher performance, and the optimization algorithm further improves the result by reducing the noise.
5 Combining CFGR with four types of n-grams
Based on the systematic exploration of the CFGR domain, we turn to combining our new feature type
CFGR
ph?lex
with n-gram features as the best performing features for NLI (Tetreault et al., 2013; Jarvis
et al., 2013). Adapting the n-gram approach we presented in Bykh and Meurers (2012), we use all
recurring n-grams with 1 ? n ? 10 at different levels of representation, including the word-based (W),
open-class POS-based (OP) and POS-based (P) n-grams from our previous work as well as lemma-based
(L) n-grams (Jarvis et al., 2013). We employ binary feature encoding for all n-gram types.
For POS-tagging we use the OpenNLP
1
toolkit, for lemmatizing we employ the MATE
2
tools
(Bj?orkelund et al., 2010). To obtain a fine grained, flexible n-gram setting, we generate an ensemble
model for each n-gram type and each n, which results in 40 n-gram models.
1
http://opennlp.apache.org
2
https://code.google.com/p/mate-tools
1969
Table 4 provides the results for the n-gram ensembles built on the basis of the recurring word-, lemma-,
POS-, OCPOS-based n-grams with 1 ? n ? 10 in the same format as Table 2 for CFGR
ph?lex
.
3
Different from the CFGR
ph?lex
case, the results for the n-gram ensemble model without optimization
or tuning (-opt, -all) already are 4?5% higher than the single vector baseline.
features data baseline ensemble
-opt +opt
-all +all -all +all
N-GRAMS (40) sc 77.09% 82.27% 82.55% 83.00% (13) 82.27% (8)
cc 31.00% 34.91% 34.55% 36.45% (6) 35.45% (6)
Table 4: Results for the n-gram ensembles with different optimization settings
The best results, 83% for single-corpus and 36.45% for the cross-corpus setting, are obtained by ap-
plying the optimization. The n-gram ensembles seem to benefit more from optimization than from tuning
in general. The feature counts for the n-grams (single-corpus: 4,822,874; cross-corpus: 3,687,375) are
far higher than for CFGR
ph?lex
(single-corpus: 98,115; cross-corpus: 94,176), so there may be more
noise in the [all] model, making it less useful for the tuning step.
Table 5 lists the models selected by the optimization algorithm in order in which they are selected.
The n-gram types and the n of the model is indicated, e.g., ?[OP-3]? means ?OCPOS-based trigrams?.
data N-GRAMS: selected models
+opt, -all +opt, +all
sc [W-2]+[L-2]+[W-1]+[L-1]+[L-3]+[W-3]+[OP-3] [all]+[W-2]+[L-2]+[W-1]+[L-1]+[L-3]+[OP-4]+[L-4] (8)
+[OP-1]+[OP-5]+[P-3]+[P-5]+[P-2]+[OP-8] (13)
cc [W-2]+[W-1]+[L-1]+[L-3]+[W-3]+[OP-2] (6) [W-2]+[W-1]+[all]+[L-1]+[L-3]+[P-4] (6)
Table 5: The n-gram model sets selected by optimization
For the more surface-based n-gram (word- and lemma-based), the optimizer selected only up to n = 3,
whereas for the more abstract ones (POS- and OCPOS-based), models up to n = 8 were included. Thus,
when abstracting from the surface, one can get some useful information out of longer n-grams that
apparently is not contained in the short surface-based ones. Different from the CFGR
ph?lex
variables-
based ensemble, we here find that relatively low-performing models such as those considering longer n
n-grams are kept when optimizing the ensemble.
Having established the performance of the n-gram ensembles, we can turn to combining the
CFGR
ph?lex
and n-gram models. The results are presented in Table 6.
features data ensemble
-opt +opt
-all +all -all +all
(a) CFGR
ph?lex
(71) + N-GRAMS (40) sc 82.09% 82.91% 82.91% (20) 83.55% (6)
cc 34.09% 36.00% 36.73% (8) 38.45% (3)
(b) CFGR
ph?lex
(71) + N-GRAMS [+opt, -all] (ME) sc 83.09% 83.73% 82.64% (4) 84.18% (5)
cc 37.36% 39.55% 38.00% (3) 40.27% (3)
(c) CFGR
ph?lex
[+opt, +all] (ME) + N-GRAMS (40) sc 83.73% 84.82% 84.73% (13) 83.82% (13)
cc 36.82% 38.91% 42.00% (5) 43.00% (4)
(d) CFGR
ph?lex
[+opt, +all] (ME) + N-GRAMS [+opt, -all] (ME) sc 83.45% 83.45% 83.45% (2) 83.36% (2)
cc 41.27% 42.00% 41.27% (2) 40.55% (2)
Table 6: Optimization results combining n-grams and CFGR
ph?lex
3
For space reasons, we cannot present the individual results for the separate n-gram models here, but interested readers
can consult Bykh and Meurers (2012), where word-, POS- and OCPOS-based n-gram results are discussed in detail. The
lemma-based n-grams we are adding here perform very much like the word-based n-grams.
1970
We explore four different ways to combine the two model sets, and the table shows the best results for
each of the setups in bold, once for the single-corpus and once for the cross-corpus setting.
For the results of setup (a), we use the ensemble consisting of all individual models separately.
In (b), the CFGR
ph?lex
models are included as in (a), but we replace the n-gram models by a single
meta-ensemble model (ME) generated using the best n-grams setting (+opt, -all), which consists of 13
models for single-corpus and six models for the cross-corpus setting (see Table 4). ME thus is a meta-
meta-classifier, generated by applying the ensemble model generation routine to an ensemble.
In (c), we invert the (b) setting: The CFGR
ph?lex
features are replaced by a meta-ensemble generated
using the best performing CFGR
ph?lex
setting (+opt, +all), which consists of eight models for the
single-corpus, and one model for the cross-corpus setting (see Table 2).
Finally, in (d) we combine the meta-ensemble for CFGR
ph?lex
with the meta-ensemble for the n-
grams obtaining an ensemble consisting of two models
The best results of 84.82% in the single-corpus setting and 43% cross-corpus, underlined in the table,
are obtained in setup (c). These are the overall best results across all experiments described in this paper.
The best result in the single-corpus setting involves tuning only, whereas in the cross-corpus setting it
involves tuning and optimization selecting the models [all]+[CFGR +all +opt]+[W-2]+[W-1].
The single-corpus accuracy of 84.82% is the best result reported so far for the NLI Shared Task 2013
data with the T11 train ? dev set for training and the T11 test set for testing. The best previous result
was 83.6% (Jarvis et al., 2013).
In the cross-corpus setting, the 43% accuracy also outperforms the previous best result on the
NT11 data (Bykh et al., 2013) by 4.5%.
In sum, the overall best results in the single-corpus and cross-corpus settings are obtained starting with
the whole n-gram model set plus an optimized CFGR
ph?lex
meta-ensemble. This confirms the useful-
ness of the optimized ensemble setup and underlines that combining a range of linguistic properties, from
n-grams at different levels of abstraction to local syntactic trees characteristics, is a particularly fruitful
approach for native language identification as a good example of an experimental task putting linguistic
modeling to the test with real-life data.
6 Conclusions
In the research presented, we systematically explored non-lexicalized and lexicalized CFG production
rules (CFGR) as features for the task of NLI using both single-corpus and cross-corpus settings. Includ-
ing lexicalized CFG rule features clearly improved the results in both setting so that it seems worthwhile
not to discard them a priori, which was the standard in previous research.
Pursuing a variationist perspective to CFGR feature representation resulted in improved performance
and it supported an in-depth exploration of the contribution of the different variables and variants as
well as of the value of local syntactic features for NLI in general. Training a separate classifier for each
variable provides quantitative advantages by facilitating high-performing ensemble setups and supports
a qualitative discussion of the categories reflecting the choices made by the learners with a given L1.
Investigating different meta-classifier setups, we explored ensemble optimization and tuning tech-
niques that improved the accuracy over putting all features in a single vector or a basic ensemble setup.
Combining the syntactic CFGR with four types of n-grams yielded a single-corpus accuracy of 84.82%
on the TOEFL11 test set. To the best of our knowledge this is the highest accuracy reported so far on
this standard data set of the NLI Shared Task 2013. The combined model also outperformed our best
previous cross-corpus result on the NT11 corpus.
In terms of future work, we intend to explore a broader range of linguistic features from a variationist
perspective, for example on the morphological level. To investigate the generalizability of the types of
features used, we also plan to apply our approach to NLI targeting second langauges other than English.
1971
References
Anders Bj?orkelund, Bernd Bohnet, Love Hafdell, and Pierre Nugues. 2010. A high-performance syntactic and
semantic dependency parser. In Demonstration Volume of the 23rd International Conference on Computational
Linguistics (COLING 2010), Beijing, pages 23?27. https://code.google.com/p/mate-tools/.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife Cahill, and Martin Chodorow. 2013. TOEFL11: A
corpus of non-native english. Technical report, Educational Testing Service.
Julian Brooke and Graeme Hirst. 2012. Robust, lexicalized native language identification. In Proceedings of the
24th International Conference on Computational Linguistics (COLING), pages 391?408, Mumbai, India.
Julian Brooke and Graeme Hirst. 2013a. Native language detection with ?cheap? learner corpora. In Sylviane
Granger, Ga?etanelle Gilquin, and Fanny Meunier, editors, Twenty Years of Learner Corpus Research. Looking
Back, Moving Ahead. Proceedings of the First Learner Corpus Research Conference (LCR 2011), Louvain-la-
Neuve, Belgium. Presses universitaires de Louvain.
Julian Brooke and Graeme Hirst. 2013b. Using other learner corpora in the 2013 nli shared task. In Proceedings
of the 8th Workshop on Innovative Use of NLP for Building Educational Applications (BEA-8) at NAACL-HLT
2013, Atlanta, GA.
Serhiy Bykh and Detmar Meurers. 2012. Native language identification using recurring n-grams ? investigating
abstraction and domain dependence. In Proceedings of the 24th International Conference on Computational
Linguistics (COLING), pages 425?440, Mumbay, India.
Serhiy Bykh, Sowmya Vajjala, Julia Krivanek, and Detmar Meurers. 2013. Combining shallow and linguistically
motivated features in native language identification. In Proceedings of the 8th Workshop on Innovative Use of
NLP for Building Educational Applications (BEA-8) at NAACL-HLT 2013, Atlanta, GA.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions
on Intelligent Systems and Technology, 2:27:1?27:27. Software available at http://www.csie.ntu.edu.
tw/
?
cjlin/libsvm.
Andrea Cimino, Felice Dell?Orletta, Giulia Venturi, and Simonetta Montemagni. 2013. Linguistic profiling based
on general?purpose features and native language identification. In Proceedings of the 8th Workshop on Innova-
tive Use of NLP for Building Educational Applications (BEA-8) at NAACL-HLT 2013, Atlanta, GA.
R.E. Fan, K.W. Chang, C.J. Hsieh, X.R. Wang, and C.J. Lin. 2008. Liblinear: A library for large linear
classification. The Journal of Machine Learning Research, 9:1871?1874. Software available at http:
//www.csie.ntu.edu.tw/
?
cjlin/liblinear.
Binyam Gebrekidan Gebre, Marcos Zampieri, Peter Wittenburg, and Tom Heskes. 2013. Improving native lan-
guage identification with tf-idf weighting. In Proceedings of the 8th Workshop on Innovative Use of NLP for
Building Educational Applications (BEA-8) at NAACL-HLT 2013, Atlanta, GA.
S. Granger, E. Dagneaux, and F. Meunier. 2002. International Corpus of Learner English. Presses Universitaires
de Louvain, Louvain-la-Neuve.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier, and Magali Paquot, 2009. International Corpus of Learner
English, Version 2. Presses Universitaires de Louvain, Louvain-la-Neuve.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The
weka data mining software: An update. In The SIGKDD Explorations, volume 11, pages 10?18.
Shin?ichiro Ishikawa. 2011. A new horizon in learner corpus studies: The aim of the ICNALE projects. In
G. Weir, S. Ishikawa, and K. Poonpon, editors, Corpora and language technologies in teaching, learning and
research, pages 3?11. University of Strathclyde Publishing, Glasgow, UK. http://language.sakura.
ne.jp/icnale/index.html.
Scott Jarvis and Scott A. Crossley, editors. 2012. Approaching Language Transfer through Text Classification:
Explorations in the Detection-based Approach. Second Language Acquisition. Multilingual Matters.
Scott Jarvis, Yves Bestgen, and Steve Pepper. 2013. Maximizing classification accuracy in native language iden-
tification. In Proceedings of the 8th Workshop on Innovative Use of NLP for Building Educational Applications
(BEA-8) at NAACL-HLT 2013, Atlanta, GA.
Dan Klein and Christopher D. Manning. 2002. Fast exact inference with a factored model for natural language
parsing. In Advances in Neural Information Processing Systems 15 (NIPS 2002).
1972
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005. Determining an author?s native language by mining a text
for errors. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in
data mining (KDD ?05), pages 624?628, New York.
William Labov. 1972. Sociolinguistic Patterns. University of Pennsylvania Press.
Andr?e Lynum. 2013. Native language identification using large scale lexical features. In Proceedings of the
8th Workshop on Innovative Use of NLP for Building Educational Applications (BEA-8) at NAACL-HLT 2013,
Atlanta, GA.
Shervin Malmasi, Sze-Meng Jojo Wong, and Mark Dras. 2013. Nli shared task 2013: Mq submission. In
Proceedings of the 8th Workshop on Innovative Use of NLP for Building Educational Applications (BEA-8) at
NAACL-HLT 2013, Atlanta, GA.
Detmar Meurers, Julia Krivanek, and Serhiy Bykh. 2013. On the automatic analysis of learner corpora: Native
language identification as experimental testbed of language modeling between surface features and linguistic
abstraction. In Diachrony and Synchrony in English Corpus Studies, Frankfurt am Main. Peter Lang.
Mick Randall and Nicholas Groom. 2009. The BUiD Arab learner corpus: a resource for studying the acquisition
of L2 english spelling. In Proceedings of the Corpus Linguistics Conference (CL), Liverpool, UK.
Benjamin Swanson and Eugene Charniak. 2012. Native language detection with tree substitution grammars. In
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short
Papers), pages 193?197, Jeju Island, Korea, July. Association for Computational Linguistics.
Ben Swanson and Eugene Charniak. 2013. Extracting the native language signal for second language acquisition.
In Proceedings of NAACL-HLT. Association for Computational Linguistics.
Ben Swanson. 2013. Exploring syntactic representations for native language identification. In Proceedings of the
8th Workshop on Innovative Use of NLP for Building Educational Applications (BEA-8) at NAACL-HLT 2013,
Atlanta, GA.
Sali A. Tagliamonte. 2011. Variationist Sociolinguistics: Change, Observation, Interpretation. John Wiley &
Sons.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Martin Chodorow. 2012. Native tongues, lost and found:
Resources and empirical evaluations in native language identification. In Proceedings of the 24th International
Conference on Computational Linguistics (COLING), pages 2585?2602, Mumbai, India.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013. A report on the first native language identification shared
task. In Proceedings of the Eighth Workshop on Building Educational Applications Using NLP, Atlanta, GA,
USA, June. Association for Computational Linguistics.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting parse structures for native language identification. In
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1600?1610,
Edinburgh, Scotland, UK., July.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock. 2011. A new dataset and method for automatically
grading ESOL texts. In Proceedings of the 49th Annual Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Volume 1, HLT ?11, pages 180?189, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics. Corpus available: http://ilexir.co.uk/applications/
clc-fce-dataset.
1973
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 288?297,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Assessing the relative reading level of sentence pairs for text simplification
Sowmya Vajjala and Detmar Meurers
LEAD Graduate School, Seminar f?ur Sprachwissenschaft
Universit?at T?ubingen
{sowmya,dm}@sfs.uni-tuebingen.de
Abstract
While the automatic analysis of the read-
ability of texts has a long history, the use
of readability assessment for text simplifi-
cation has received only little attention so
far. In this paper, we explore readability
models for identifying differences in the
reading levels of simplified and unsimpli-
fied versions of sentences.
Our experiments show that a relative rank-
ing is preferable to an absolute binary one
and that the accuracy of identifying rel-
ative simplification depends on the ini-
tial reading level of the unsimplified ver-
sion. The approach is particularly success-
ful in classifying the relative reading level
of harder sentences.
In terms of practical relevance, the ap-
proach promises to be useful for identi-
fying particularly relevant targets for sim-
plification and to evaluate simplifications
given specific readability constraints.
1 Introduction
Text simplification essentially is the process of
rewriting a given text to make it easier to process
for a given audience. The target audience can ei-
ther be human users trying to understand a text or
machine applications, such as a parser analyzing
text. Text simplification has been used in a vari-
ety of application scenarios, from providing sim-
plified newspaper texts for aphasic readers (Can-
ning and Tait, 1999) to supporting the extraction of
protein-protein interactions in the biomedical do-
main (Jonnalagadda and Gonzalez, 2009).
A related field of research is automatic readabil-
ity assessment, which can be useful for evaluating
text simplification. It can also be relevant for in-
termediate simplification steps, such as the identi-
fication of target sentences for simplification. Yet,
so far there has only been little research connect-
ing the two subfields, possibly because readability
research typically analyzes documents, whereas
simplification approaches generally targeted lex-
ical and syntactic aspects at the sentence level. In
this paper, we attempt to bridge this gap between
readability and simplification by studying read-
ability at a sentence level and exploring how well
can a readability model identify the differences be-
tween unsimplified and simplified sentences.
Our main research questions in this paper are:
1. Can the readability features that worked at the
document level successfully be used at the sen-
tence level? 2. How accurately can we identify the
differences in the sentential reading level before
and after simplification? To pursue these ques-
tions, we started with constructing a document-
level readability model. We then applied it to nor-
mal and simplified versions of sentences drawn
from Wikipedia and Simple Wikipedia.
As context of our work, we first discuss rel-
evant related research. Section 2 then describes
the corpora and the features we used to construct
our readability model. Section 3 discusses the
performance of our readability model in compari-
son with other existing systems. Sections 4 and 5
present our experiments with sentence level read-
ability analysis and the results. In Section 6 we
present our conclusions and plans for future work.
1.1 Related Work
Research into automatic text simplification essen-
tially started with the idea of splitting long sen-
tences into multiple shorter sentences to improve
parsing efficiency (Chandrasekar et al., 1996;
Chandrasekar and Srinivas, 1996). This was
followed by rule-based approaches targeting hu-
man and machine uses (Carroll et al., 1999; Sid-
dharthan, 2002, 2004).
With the availability of a sentence-aligned cor-
pus based on Wikipedia and SimpleWikipedia
288
texts, data-driven approaches, partly inspired by
statistical machine translation, appeared (Specia,
2010; Zhu et al., 2010; Bach et al., 2011; Coster
and Kauchak, 2011; Woodsend and Lapata, 2011).
While simplification methods have evolved, un-
derstanding which parts of a text need to be sim-
plified and methods for evaluating the simplified
text so far received only little attention. The use
of readability assessment for simplification has
mostly been restricted to using traditional read-
ability formulae for evaluating or generating sim-
plified text (Zhu et al., 2010; Wubben et al.,
2012; Klerke and S?gaard, 2013; Stymne et al.,
2013). Some recent work briefly addresses issues
such as classifying sentences by their reading level
(Napoles and Dredze, 2010) and identifying sen-
tential transformations needed for text simplifica-
tion using text complexity features (Medero and
Ostendorf, 2011). Some simplification approaches
for non-English languages (Aluisio et al., 2010;
Gasperin et al., 2009;
?
Stajner et al., 2013) also
touch on the use of readability assessment.
In the present paper, we focus on the neglected
connection between readability analysis and sim-
plification. We show through a cross-corpus eval-
uation that a document level, regression-based
readability model successfully identifies the dif-
ferences between simplified vs. unsimplified sen-
tences. This approach can be useful in various
stages of simplification ranging from identifying
simplification targets to the evaluation of simplifi-
cation outcomes.
2 Corpora and Features
2.1 Corpora
We built and tested our document and sentence
level readability models using three publicly avail-
able text corpora with reading level annotations.
WeeBit Corpus: The WeeBit corpus (Vajjala
and Meurers, 2012) consists of 3,125 articles be-
longing to five reading levels, with 625 articles
per reading level. The texts compiled from the
WeeklyReader and BBC Bitesize target English
language learners from 7 to 16 years of age. We
used this corpus to build our primary readability
model by mapping the five reading levels in the
corpus to a scale of 1?5 and considered readabil-
ity assessment as a regression problem.
Common Core Standards Corpus: This cor-
pus consists of 168 English texts available from
the Appendix B of the Common Core Standards
reading initiative of the U.S. education system
(CCSSO, 2010). They are annotated by experts
with grade bands that cover the grades 1 to 12.
These texts serve as exemplars for the level of
reading ability at a given grade level. This corpus
was introduced as an evaluation corpus for read-
ability models in the recent past (Sheehan et al.,
2010; Nelson et al., 2012; Flor et al., 2013), so we
used it to compare our model with other systems.
Wiki-SimpleWiki Sentence Aligned Corpus:
This corpus was created by Zhu et al. (2010) and
consists of ?100k aligned sentence pairs drawn
from Wikipedia and Simple English Wikipedia.
We removed all pairs of identical sentences, i.e.,
where the Wiki and the SimpleWiki versions are
the same. We used this corpus to study reading
level assessment at the sentence level.
2.2 Features
We started with the feature set described in Vajjala
and Meurers (2012) and added new features fo-
cusing on the morphological and psycholinguistic
properties of words. The features can be broadly
classified into four groups.
Lexical richness and POS features: We
adapted the lexical features from Vajjala and
Meurers (2012). This includes measures of lexical
richness from Second Language Acquisition
(SLA) research and measures of lexical variation
(noun, verb, adjective, adverb and modifier vari-
ation). In addition, this feature set also includes
part-of-speech densities (e.g., the average # of
nouns per sentence). The information needed to
calculate these features was extracted using the
Stanford Tagger (Toutanova et al., 2003). None
of the lexical richness and POS features we used
refer to specific words or lemmas.
Syntactic Complexity features: Parse tree
based features and some syntactic complexity
measures derived from SLA research proved
useful for readability classification in the past, so
we made use of all the syntactic features from
Vajjala and Meurers (2012): mean lengths of
various production units (sentence, clause, t-unit),
measures of coordination and subordination
(e.g., # of coordinate clauses per clause), the
presence of particular syntactic structures (e.g.,
VPs per t-unit), the number of phrases of various
categories (e.g., NP, VP, PP), the average lengths
289
of phrases, the parse tree height, and the number
of constituents per subtree. None of the syntactic
features refer to specific words or lemmas. We
used the BerkeleyParser (Petrov and Klein, 2007)
for generating the parse trees and the Tregex tool
(Levy and Andrew, 2006) to count the occurrences
of the syntactic patterns.
While the first two feature sets are based on our
previous work, as far as we know the next two are
used in readability assessment for the first time.
Features from the Celex Lexical Database:
The Celex Lexical Database (Baayen et al., 1995)
is a database consisting of information about mor-
phological, syntactic, orthographic and phonolog-
ical properties of words along with word frequen-
cies in various corpora. Celex for English contains
this information for more than 50,000 lemmas. An
overview of the fields in the Celex database is pro-
vided online
1
and the Celex user manual
2
.
We used the morphological and syntactic prop-
erties of lemmas as features. We excluded word
frequency statistics and properties which consisted
of word strings. In all, we used 35 morphologi-
cal and 49 syntactic properties that were expressed
using either character or numeric codes in this
database as features for our task.
The morphological properties in Celex include
information about the derivational, inflectional
and compositional features of the words, their
morphological origins and complexity. The syn-
tactic properties of the words in Celex describe
the attributes of a word depending on its parts of
speech. For the morphological and syntactic prop-
erties from this database, we used the proportion
of occurrences per text as features. For example,
the ratio of transitive verbs, complex morphologi-
cal words, and vocative nouns to number of words.
Lemmas from the text that do not have entries in
the Celex database were ignored.
Word frequency statistics from Celex have been
used before to analyze text difficulty in the past
(Crossley et al., 2007). However, to our knowl-
edge, this is the first time morphological and syn-
tactic information from the Celex database is used
for readability assessment.
Psycholinguistic features: The MRC Psy-
cholinguistic Database (Wilson, 1988) is a freely
available, machine readable dictionary annotated
1
http://celex.mpi.nl/help/elemmas.html
2
http://catalog.ldc.upenn.edu/docs/LDC96L14
with 26 linguistic and psychological attributes of
about 1.5 million words.
3
We used the measures
of word familiarity, concreteness, imageability,
meaningfulness, and age of acquisition from
this database as our features, by encoding their
average values per text.
Kuperman et al. (2012) compiled a freely avail-
able database that includes Age of Acquisition
(AoA) ratings for over 50,000 English words.
4
This database was created through crowd sourcing
and was compared with several other AoA norms,
which are also included in the database. For each
of the five AoA norms, we computed the average
AoA of words per text.
Turning to the final resource used, we included
the average number of senses per word as calcu-
lated using the MIT Java WordNet Interface as a
feature.
5
We excluded auxiliary verbs for this cal-
culation as they tend to have multiple senses that
do not necessarily contribute to reading difficulty.
Combining the four feature groups, we encode
151 features for each text.
3 Document-Level Readability Model
In our first experiment, we tested the document-
level readability model based on the 151 features
using the WeeBit corpus. Under a regression per-
spective on readability, we evaluated the approach
using Pearson Correlation and Root Mean Square
Error (RMSE) in a 10-fold cross-validation set-
ting. We used the SMO Regression implementa-
tion from WEKA (Hall et al., 2009) and achieved a
Pearson correlation of 0.92 and an RMSE of 0.53.
The document-level performance of our 151
feature model is virtually identical to that of the re-
gression model we presented in Vajjala and Meur-
ers (2013). But compared to our previous work,
the Celex and psycholinguistic features we in-
cluded here provide more lexical information that
is meaningful to compute even for the sentence-
level analysis we turn to in the next section.
To be able to compare our document-level
results with other contemporary readability ap-
proaches, we need a common test corpus. Nel-
son et al. (2012) compared several state of the art
readability assessment systems using five test sets
and showed that the systems that went beyond tra-
ditional formulae and wordlists performed better
3
http://www.psych.rl.ac.uk
4
http://crr.ugent.be/archives/806
5
http://projects.csail.mit.edu/jwi
290
on these real-life test sets. We tested our model
on one of the publicly accessible test corpora from
this study, the Common Core Standards Corpus.
Flor et al. (2013) used the same test set to study
a measure of lexical tightness, providing a further
performance reference.
Table 1 compares the performance of our model
to that reported for several commercial (indicated
in italics) and research systems on this test set.
Nelson et al. (2012) used Spearman?s Rank Cor-
relation and Flor et al. (2013) used Pearson Corre-
lation as evaluation metrics. To facilitate compar-
ison, for our approach we provide both measures.
System Spearman Pearson
Our System 0.69 0.61
Nelson et al. (2012):
REAP
6
0.54 ?
ATOS
7
0.59 ?
DRP
8
0.53 ?
Lexile
9
0.50 ?
Reading Maturity
10
0.69 ?
SourceRater
11
0.75 ?
Flor et al. (2013):
Lexical Tightness ? -0.44
Flesch-Kincaid ? 0.49
Text length ? 0.36
Table 1: Performance on CommonCore data
As the table shows, our model is the best non-
commercial system and overall second (tied with
the Reading Maturity system) to SourceRater as
the best performing commercial system on this
test set. These results on an independent test set
confirm the validity of our document-level read-
ability model. With this baseline, we turned to a
sentence-level readability analysis.
4 Sentence-Level Binary Classification
For each of the pairs in the Wiki-SimpleWiki Sen-
tence Aligned Corpus introduced above, we la-
beled the sentence from Wikipedia as hard and
that from Simple English Wikipedia as simple.
The corpus thus consisted of single sentences,
each labeled either simple or hard. On this basis,
we constructed a binary classification model.
6
http://reap.cs.cmu.edu
7
http://renlearn.com/atos
8
http://questarai.com/Products/DRPProgram
9
http://lexile.com
10
http://readingmaturity.com
11
http://naeptba.ets.org/SourceRater3
Our document-level readability model does not
include discourse features, so all 151 features can
also be computed for individual sentences. We
built a binary sentence-level classification model
using WEKA?s Sequential Minimal Optimization
(SMO) for training an SVM in WEKA on the
Wiki-SimpleWiki sentence aligned corpus. The
choice of algorithm was primarily motivated by
the fact that it was shown to be efficient in previ-
ous work on readability classification (Feng, 2010;
Hancke et al., 2012; Falkenjack et al., 2013).
The accuracy of the resulting classifier deter-
mining whether a given sentence is simple or
hard was disappointing, reaching only 66% accu-
racy in a 10-fold cross-validation setting. Exper-
iments with different classification algorithms did
not yield any more promising results. To study
how the classification performance is impacted by
the size of the training data, we experimented with
different sizes, using SMO as the classification al-
gorithm. Figure 1 shows the classification accu-
racy with different training set sizes.
 65
 65.5
 66
 66.5
 67
 67.5
 68
 68.5
 0  10  20  30  40  50  60  70  80  90  100
clas
sific
atio
n ac
cura
cy (
in %
)
% of training data used
Relation between Binary Sentence Classification Accuracy and Training Data size
 
Figure 1: Training size vs. classification accuracy
The graph shows that beyond 10% of the training
data, more training data did not result in signifi-
cant differences in classification accuracy. Even
at 10%, the training set contains around 10k in-
stances per category, so the variability of any of
the patterns distinguished by our features is suffi-
ciently represented.
We also explored whether feature selection
could be useful. A subset of features chosen by re-
moving correlated features using the CfsSubsetE-
val method in WEKA did not improve the results,
yielding an accuracy of 65.8%. A simple base-
line based on the sentence length as single feature
results in an accuracy of 60.5%, underscoring the
291
limited value of the rich feature set in this binary
classification setup.
For the sake of a direct comparison with the
document-level model, we also explored modeling
the task as a regression on a 1?2 scale. In compar-
ison to the document-level model, which as dis-
cussed in section 3 had a correlation of 0.92, the
sentence-level model achieves only a correlation
of 0.4. A direct comparison is also possible when
we train the document-level model as a five-class
classifier with SMO. This model achieved a clas-
sification accuracy of ?90% on the documents,
compared to the 66% accuracy of the sentence-
level model classifying sentences. So under each
of these perspectives, the sentence-level models on
the sentence task are much less successful than the
document-level models on the document task.
But does this indicate that it is not possible to
accurately identify the reading level distinctions
between simplified and unsimplified versions at
the sentence level? Is there not enough informa-
tion available when considering a single sentence?
We hypothesized that the drop in the classi-
fication accuracy instead results from the rela-
tive nature of simplification. For each pair of
the Wiki-SimpleWiki sentence aligned corpus we
used, the Wiki sentence was harder than the Sim-
pleWikipedia sentence. But this does not neces-
sarily mean that each of the Wikipedia sentences
is harder than each of the SimpleWikipedia sen-
tences. The low accuracy of the binary classi-
fier may thus simply result from the inappropriate
assumption of an absolute, binary classification
viewing each of the sentences originating from
SimpleWikipedia as simple and each from the reg-
ular Wiki as hard.
The confusion matrices of the binary classifi-
cation suggests some support for this hypothesis,
as more simple sentences were classified as hard
compared to the other way around. This can result
when a simple sentence is simpler than its hard
version, but could actually be simplified further ?
and as such may still be harder than another un-
simplified sentence. The hypothesis thus amounts
to saying that the two-class classification model
mistakenly turned the relative difference between
the sentence pairs into a global classification of in-
dividual sentences, independent of the pairs they
occur in.
How can we verify this hypothesis? The sen-
tence corpus only provides the relative ranking of
the pairs, but we can try to identify more fine-
grained readability levels for sentences by apply-
ing the five class readability model for documents
that was introduced in section 3.
5 Relative Reading Levels of Sentences
We applied the document-level readability model
to the individual sentences from the Wiki-
SimpleWiki corpus to study which reading levels
are identified by our model. As we are using a re-
gression model, the values sometimes go beyond
the training corpus? scale of 1?5. For ease of com-
parison, we rounded off the reading levels to the
five level scale, i.e., 1 means 1 or below, and 5
means 5 or above. Figure 2 shows the distribution
of Wikipedia and SimpleWikipedia sentences ac-
cording to the predictions of our document-level
readability model trained on the WeeBit corpus.
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 1  1.5  2  2.5  3  3.5  4  4.5  5
Per
cen
tage
 of t
he t
otal
 sen
tenc
es a
t tha
t lev
el
Reading level
Distribution of reading levels of Normal and Simplified Sentences
WikiSimple Wiki
Figure 2: Reading level distribution of the
Wikipedia and SimpleWikipedia sentences
The model determines that a high percentage of
the SimpleWiki sentences belong to lower reading
levels, with over 45% at the lowest reading level;
yet there also are some SimpleWikipedia sen-
tences which are aligned even to the highest read-
ability level. In contrast, the regular Wikipedia
sentences are evenly distributed across all reading
levels.
The distributions identified by the model sup-
port our hypothesis that some Wiki sentences are
simpler than some SimpleWikipedia sentences.
Note that this is fully compatible with the fact that
for each pair of (SimpleWiki,Wiki) sentences in-
cluded in the corpus, the former is higher in read-
ing level than the latter; e.g., just consider two sen-
tence pairs with the levels (1, 2) and (3, 5).
292
5.1 On the discriminating power of the model
Zooming in on the relative reading levels of the
paired unsimplified and simplified sentences, we
wanted to determine for how many sentence pairs
the sentence reading levels determined by our
model are compatible with the pair?s ranking. In
other words, we calculated the percentage of pairs
(S,N) in which the reading level of a simplified
sentence (S) is identified as less than, equal to, or
greater than the unsimplified (normal) version of
the sentence (N ), i.e., S<N , S=N , and S>N .
Where simplification split a sentence into multiple
sentences, we computed S as the average reading
level of the split sentences.
Given the regression model setup, we can con-
sider how big the difference between two reading
levels determined by the model should be in or-
der for us to interpret it as a categorical difference
in reading level. Let us call this discriminating
reading-level difference the d-level. For example,
with d = 0.3, a sentence pair determined to be
at levels (3.4, 3.2) would be considered a case of
S=N , whereas (3.4, 3.7) would be an instance of
S <N . The d-value can be understood as a mea-
sure of how fine-grained the model is in identify-
ing reading-level differences between sentences.
If we consider the percentage of samples identi-
fied as S <=N as an accuracy measure, Figure 3
shows the accuracy for different d-values.
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  0.2  0.4  0.6  0.8  1
Per
cen
tage
 of t
he t
otal
 sam
ples
 
d-value
Comparison of Normal and Simplified Sentences
S<=N
Figure 3: Accurately identified S<=N
We can observe that the percentage of instances
that the model correctly identifies as S <= N
steadily increases from 70% to 90% as d increases.
While the value of d in theory can be anything,
values beyond 1 are uninteresting in the context of
this study. At d = 1, most of the sentence pairs
already belong to S=N , so increasing this further
would defeat the purpose of identifying reading-
level differences. The higher the d-value, the more
of the simplified and unsimplified pairs are lumped
together as indistinguishable.
Spelling out the different cases from Figure 3,
the number of pairs identified correctly, equated,
and misclassified as a function of the d-value is
shown in Figure 4.
 10
 15
 20
 25
 30
 35
 40
 45
 50
 55
 60
 0  0.2  0.4  0.6  0.8  1
Per
cen
tage
 of t
he t
otal
 sam
ples
 
d-value
Comparison of Normal and Simplified Sentences
S<NS=NS>N
Figure 4: Correctly (S < N ), equated (S = N ),
and incorrectly (S>N ) identified sentence pairs
At d = 0.4, around 50% of the pairs are cor-
rectly classified, 20% are misclassified, and 30%
equated. At d=0.7, the rate of pairs for which no
distinction can be determined already rises above
50%. For d-values between 0.3 and 0.6, the per-
centage of correctly identified pairs exceeds the
percentage of equated pairs, which in turn exceeds
the percentage of misclassified pairs.
5.2 Influence of reading-level on accuracy
We saw in Figure 2 that the Wikipedia sentences
are uniformly distributed across the reading lev-
els, and for each of these sentences, a human sim-
plified version is included in the corpus. Even
sentences identified by our readability model as
belonging to the lower reading levels thus were
further simplified. This leads us to investigate
whether the reading level of the unsimplified sen-
tence influences the ability of our model to cor-
rectly identify the simplification relationship.
To investigate this, we separately analyzed pairs
where the unsimplified sentences had a higher
reading level and those where it had a lower read-
ing level, taking the middle of the scale (2.5) as the
293
cut-off point. Figure 5 shows the accuracies ob-
tained when distinguishing unsimplified sentences
of two readability levels.
 55
 60
 65
 70
 75
 80
 85
 90
 95
 100
 0  0.2  0.4  0.6  0.8  1
Per
cen
tage
 of t
he t
otal
 sam
ples
 hav
ing 
S<=
N
d-value
S<=N vs d, when N >=2.5 and N<2.5
N>=2.5N<2.5
Figure 5: Accuracy (S<=N) for different N types
For the pairs where the reading level of the unsim-
plified version is high, the accuracy of the read-
ability model is high (80?95%). In the other case,
the accuracy drops to 65?75% (for 0.3<= d <=
0.6). Presumably the complex sentences for which
the model performs best offer more syntactic and
lexical material informing the features used.
When we split the graph into the three cases
again (S < N , S = N , S > N ), the pairs with a
high-level unsimplified sentence in Figure 6 fol-
low the overall picture of Figure 4.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  0.2  0.4  0.6  0.8  1
Per
cen
tage
 of t
he t
otal
 sam
ples
 
d-value
Comparison at Higher Values of N
S<NS=NS>N
Figure 6: Results for N>=2.5
On the other hand, the results in Figure 7 for the
pairs with an unsimplified sentence at a low read-
ability level establish that the model essentially is
incapable to identify readability differences.
 10
 20
 30
 40
 50
 60
 70
 0  0.2  0.4  0.6  0.8  1
Per
cen
tage
 of t
he t
otal
 sam
ples
 
d-value
Comparison at Lower Values of N
S<NS=NS>N
Figure 7: Results for N<2.5
The correctly identified S<N and the incorrectly
identified S >N cases mostly overlap, indicating
chance-level performance. Increasing the d-level
only increases the number of equated pairs, with-
out much impact on the number of correctly dis-
tinguished pairs.
In real-world terms, this means that it is diffi-
cult to identify simplifications of an already sim-
ple sentence. While some of this difficulty may
stem from the fact that simple sentences are likely
to be shorter and thus offer less linguistic material
on which an analysis can be based, it also points
to a need for more research on features that can
reliably distinguish lower levels of readability.
Summing up, the experiments discussed in this
section show that a document-level readability
model trained on the WeeBit corpus can provide
insightful perspectives on the nature of simplifica-
tion at the sentence level. The results emphasize
the relative nature of readability and the need for
more features capable of identifying characteris-
tics distinguishing sentences at lower levels.
6 Conclusions
We started with constructing a document-level
readability model and compared its performance
with other readability systems on a standard test
set. Having established the state-of-the-art perfor-
mance of our document-level model, we moved on
to investigate the use of the features and the model
at the sentence level.
In the sentence-level research, we first used the
same feature set to construct a two-class readabil-
ity model on the sentences from the Wikipedia-
SimpleWikipedia sentence aligned corpus. The
294
model only achieved a classification accuracy of
66%. Exploring the causes for this low perfor-
mance, we studied the sentences in the aligned
pairs through the lens of our document-level read-
ability model, the regression model based on the
five level data of the WeeBit corpus. Our ex-
periment identifies most of the Simple Wikipedia
sentences as belonging to the lower levels, with
some sentences also showing up at higher lev-
els. The sentences from the normal Wikipedia,
on the other hand, display a uniform distribution
across all reading levels. A simplified sentence
(S) can thus be at a lower reading level than its
paired unsimplified sentence (N) while also being
at a higher reading level than another unsimplified
sentence. Given this distribution of reading lev-
els, the low performance of the binary classifier
is expected. Instead of an absolute, binary differ-
ence in reading levels that counts each Wikipedia
sentence from the corpus as hard and each Simple
Wikipedia sentence as simple, a relative ranking
of reading levels seems to better suit the data.
Inspecting the relative difference in the read-
ing levels of the aligned unsimplified-simplified
sentence pairs, we characterized the accuracy of
predicting the relative reading level ranking in a
pair correctly depending on the reading-level dif-
ference d required to required to identify a cate-
gorical difference. While the experiments were
performed to verify the hypothesis that simpli-
fication is relative, they also confirm that the
document-level readability model trained on the
WeeBit corpus generalized well to Wikipedia-
SimpleWikipedia as a different, sentence-level
corpus.
The analysis revealed that the accuracy depends
on the initial reading level of the unsimplified
sentence. The model performs very well when
the reading level of the unsimplified sentence is
higher, but the features seem limited in their abil-
ity to pick up on the differences between sentences
at the lowest levels. In future work, we thus in-
tend to add more features identifying differences
between lower levels of readability.
Taking the focus on the relative ranking of
the readability of sentences one step further, we
are currently studying if modeling the readability
problem as preference learning or ordinal regres-
sion will improve the accuracy in predicting the
relation between simplified and unsimplified sen-
tence versions.
Overall, the paper contributes to the state of the
art by providing a methodology to quantitatively
evaluate the degree of simplification performed
by an automatic system. The results can also be
potentially useful in providing assistive feedback
for human writers preparing simplified texts given
specific target user constraints. We plan to explore
the idea of generating simplified text with read-
ability constraints as suggested in Stymne et al.
(2013) for Machine Translation.
Acknowledgements
We thank the anonymous reviewers for their de-
tailed comments. Our research was funded by
the LEAD Graduate School (GSC 1028, http:
//purl.org/lead), a project of the Excellence
Initiative of the German federal and state gov-
ernments, and the European Commission?s 7th
Framework Program under grant agreement num-
ber 238405 (CLARA).
References
Sandra Aluisio, Lucia Specia, Caroline Gasperin, and
Carolina Scarton. 2010. Readability assessment for
text simplification. In Proceedings of the NAACL
HLT 2010 Fifth Workshop on Innovative Use of NLP
for Building Educational Applications, pages 1?9.
R. H. Baayen, R. Piepenbrock, and L. Gulikers.
1995. The CELEX lexical databases. CDROM,
http://www.ldc.upenn.edu/Catalog/
readme_files/celex.readme.html.
Nguyen Bach, Qin Gao, Stephan Vogel, and Alex
Waibel. 2011. Tris: A statistical sentence simplifier
with log-linear models and margin-based discrimi-
native training. In Proceedings of 5th International
Joint Conference on Natural Language Processing,
pages 474?482. Asian Federation of Natural Lan-
guage Processing.
Yvonne Canning and John Tait. 1999. Syntactic sim-
plification of newspaper text for aphasic readers. In
Proceedings of SIGIR-99 Workshop on Customised
Information Delivery, pages 6?11.
John Carroll, Guido Minnen, Darren Pearce, Yvonne
Canning, Siobhan Devlin, and John Tait. 1999.
Simplifying text for language-impaired readers. In
Proceedings of the 9th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 269?270.
CCSSO. 2010. Common core state standards for en-
glish language arts & literacy in history/social stud-
ies, science, and technical subjects. appendix B: Text
exemplars and sample performance tasks. Technical
report, National Governors Association Center for
295
Best Practices, Council of Chief State School Of-
ficers. http://www.corestandards.org/
assets/Appendix_B.pdf.
R. Chandrasekar and B. Srinivas. 1996. Automatic in-
duction of rules for text simplification. Technical
Report IRCS Report 96?30, Upenn, NSF Science
and Technology Center for Research in Cognitive
Science.
R. Chandrasekar, Christine Doran, and B. Srinivas.
1996. Motivations and methods for text simplifica-
tion. In Proceedings of the 16th International Con-
ference on Computational Linguistics (COLING),
pages 1041?1044.
William Coster and David Kauchak. 2011. Simple en-
glish wikipedia: A new text simplification task. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 665?669, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Scott A. Crossley, David F. Dufty, Philip M. McCarthy,
and Danielle S. McNamara. 2007. Toward a new
readability: A mixed model approach. In Danielle S.
McNamara and Greg Trafton, editors, Proceedings
of the 29th annual conference of the Cognitive Sci-
ence Society. Cognitive Science Society.
Johan Falkenjack, Katarina Heimann M?uhlenbock, and
Arne J?onsson. 2013. Features indicating readability
in swedish text. In Proceedings of the 19th Nordic
Conference of Computational Linguistics (NODAL-
IDA).
Lijun Feng. 2010. Automatic Readability Assessment.
Ph.D. thesis, City University of New York (CUNY).
Michael Flor, Beata Beigman Klebanov, and Kath-
leen M. Sheehan. 2013. Lexical tightness and text
complexity. In Proceedings of the Second Workshop
on Natural Language Processing for Improving Tex-
tual Accessibility.
Caroline Gasperin, Lucia Specia, Tiago F. Pereira, and
Sandra M. Aluisio. 2009. Learning when to sim-
plify sentences for natural text simplification. In
Encontro Nacional de Intelig?encia Artificial (ENIA-
2009).
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
In The SIGKDD Explorations, volume 11, pages 10?
18.
Julia Hancke, Detmar Meurers, and Sowmya Vajjala.
2012. Readability classification for german using
lexical, syntactic, and morphological features. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING), pages 1063?
1080, Mumbay, India.
Siddhartha Jonnalagadda and Graciela Gonzalez.
2009. Sentence simplification aids protein-protein
interaction extraction. In Proceedings of The 3rd
International Symposium on Languages in Biology
and Medicine, Jeju Island, South Korea, November
8-10, 2009.
Sigrid Klerke and Anders S?gaard. 2013. Simple,
readable sub-sentences. In Proceedings of the ACL
Student Research Workshop.
Victor Kuperman, Hans Stadthagen-Gonzalez, and
Marc Brysbaert. 2012. Age-of-acquisition ratings
for 30,000 english words. Behavior Research Meth-
ods, 44(4):978?990.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In 5th International Conference on Lan-
guage Resources and Evaluation, Genoa, Italy.
Julie Medero and Marie Ostendorf. 2011. Identifying
targets for syntactic simplification. In ISCA Interna-
tional Workshop on Speech and Language Technol-
ogy in Education (SLaTE 2011).
Courtney Napoles and Mark Dredze. 2010. Learn-
ing simple wikipedia: a cogitation in ascertaining
abecedarian language. In Proceedings of the NAACL
HLT 2010 Workshop on Computational Linguistics
and Writing: Writing Processes and Authoring Aids,
CL&W ?10, pages 42?50, Stroudsburg, PA, USA.
Association for Computational Linguistics.
J. Nelson, C. Perfetti, D. Liben, and M. Liben. 2012.
Measures of text difficulty: Testing their predic-
tive value for grade levels and student performance.
Technical report, The Council of Chief State School
Officers.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Kathleen M. Sheehan, Irene Kostin, Yoko Futagi, and
Michael Flor. 2010. Generating automated text
complexity classifications that are aligned with tar-
geted text complexity standards. Technical Report
RR-10-28, ETS, December.
Advaith Siddharthan. 2002. An architecture for a text
simplification system. In In Proceedings of the Lan-
guage Engineering Conference 2002 (LEC 2002).
Advaith Siddharthan. 2004. Syntactic simplification
and text cohesion. Technical Report UCAM-CL-
TR-597, University of Cambridge Computer Labo-
ratory.
Lucia Specia. 2010. Translating from complex to sim-
plified sentences. In Proceedings of the 9th interna-
tional conference on Computational Processing of
the Portuguese Language (PROPOR?10).
296
Sara Stymne, J?org Tiedemann, Christian Hardmeier,
and Joakim Nivre. 2013. Statistical machine trans-
lation with readability constraints. In Proceedings of
the 19th Nordic Conference of Computational Lin-
guistics (NODALIDA 2013).
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In HLT-NAACL, pages
252?259, Edmonton, Canada.
Sowmya Vajjala and Detmar Meurers. 2012. On im-
proving the accuracy of readability classification us-
ing insights from second language acquisition. In
In Proceedings of the 7th Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 163?-173.
Sowmya Vajjala and Detmar Meurers. 2013. On the
applicability of readability models to web texts. In
Proceedings of the Second Workshop on Predicting
and Improving Text Readability for Target Reader
Populations.
M.D. Wilson. 1988. The MRC psycholinguistic
database: Machine readable dictionary, version 2.
Behavioural Research Methods, Instruments and
Computers, 20(1):6?11.
Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous
grammar and integer programming. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Sander Wubben, Antal van den Bosch, and Emiel
Krahmer. 2012. Sentence simplification by mono-
lingual machine translation. In Proceedings of ACL
2012.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of The
23rd International Conference on Computational
Linguistics (COLING), August 2010. Beijing, China.
Sanja
?
Stajner, Biljana Drndarevic, and Horaccio Sag-
gion. 2013. Corpus-based sentence deletion and
split decisions for spanish text simplification. In CI-
CLing 2013: The 14th International Conference on
Intelligent Text Processing and Computational Lin-
guistics.
297
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 608?616, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
CoMeT: Integrating different levels of linguistic modeling for
meaning assessment
Niels Ott Ramon Ziai Michael Hahn Detmar Meurers
Sonderforschungsbereich 833
Eberhard Karls Universita?t Tu?bingen
{nott,rziai,mhahn,dm}@sfs.uni-tuebingen.de
Abstract
This paper describes the CoMeT system, our
contribution to the SemEval 2013 Task 7 chal-
lenge, focusing on the task of automatically
assessing student answers to factual questions.
CoMeT is based on a meta-classifier that uses
the outputs of the sub-systems we developed:
CoMiC, CoSeC, and three shallower bag ap-
proaches. We sketch the functionality of all
sub-systems and evaluate their performance
against the official test set of the challenge.
CoMeT obtained the best result (73.1% accu-
racy) for the 3-way unseen answers in Beetle
among all challenge participants. We also dis-
cuss possible improvements and directions for
future research.
1 Introduction
Our contribution to the SemEval 2013 Task 7 chal-
lenge (Dzikovska et al, 2013) presented here is based
on our research in the A4 project1 of the SFB 833,
which is dedicated to the question how meaning can
be computationally compared in realistic situations.
In realistic situations, utterances are not necessarily
well-formed or complete, there may be individual
differences in situative and world knowledge among
the speakers. This can complicate or even preclude
a complete linguistic analysis, leading us to the fol-
lowing research question: Which linguistic repre-
sentations can be used effectively and robustly for
comparing the meaning of sentences and text frag-
ments computationally?
1http://purl.org/dm/projects/sfb833-a4
In order to work on effective and robust processing,
we base our work on reading comprehension exer-
cises for foreign language learners, of which we are
also collecting a large corpus (Ott et al, 2012). Our
first system, CoMiC, is an alignment-based approach
which exists in English and German variants (Meur-
ers et al, 2011a; Meurers et al, 2011b). CoMiC
uses various levels of linguistic abstraction from sur-
face tokens to dependency parses. Further work that
we are starting to tackle includes the utilization of
Information Structure (Krifka, 2007) in the system.
The second approach emerging from the research
project is CoSeC (Hahn and Meurers, 2011; Hahn
and Meurers, 2012), a semantics-based system for
meaning comparison that was developed for German
from the start and was ported to operate on English
for this shared task. As a novel contribution in this
paper, we present CoMeT (Comparing Meaning in
Tu?bingen), a system that employs a meta-classifier
for combining the output of CoMiC and CoSeC and
three shallower bag approaches.
In terms of the general context of our work, short
answer assessment essentially comes in the two fla-
vors of meaning comparison and grading, the first
trying to determine whether or not two utterances
convey the same meaning, the latter aimed at grading
the abilities of students (cf. Ziai et al, 2012). Short
answer assessment is also closely related to the field
of Recognizing Textual Entailment (RTE, Dagan et
al., 2009), which this year is directly reflected by
the fact that SemEval 2013 Task 7 is the Joint Stu-
dent Response Analysis and 8th Recognizing Textual
Entailment Challenge.
608
Turning to the organization of this paper, section 2
introduces the three types of sub-systems and the
meta-classifier. In section 3, we report on the evalu-
ation results of each sub-system both for our devel-
opment set as well as for the official test set of the
shared task. We then discuss possible causes and
implications of the findings we made by participating
in the shared task.
2 Systems
The CoMeT system that we describe in this paper
is a combination of three types of sub-systems in
one meta-classifier. CoSeC and CoMiC are sys-
tems that align linguistic units in the student answer
to those in the reference answer. In contrast, the
bag-based approaches employ a vocabulary of words,
lemmas, and Soundex hashes constructed from all
of the student answers in the training data. In the
meta-classifier, we tried to combine the benefits of the
named sub-systems into one large system that eventu-
ally computed our submission to the SemEval 2013
Task 7 challenge.
2.1 CoMiC
CoMiC (Comparing Meaning in Context) is an
alignment-based system, i.e., it operates on a map-
ping of linguistic units found in a student answer to
those given in a reference answer. CoMiC started off
as a re-implementation of the Content Assessment
Module (CAM) of Bailey and Meurers (2008). It
exists in two flavors: CoMiC-DE for German, de-
scribed in Meurers et al (2011b), and CoMiC-EN for
English, described in Meurers et al (2011a). Both
systems are positioned in the landscape of the short
answer assessment field in Ziai et al (2012). In this
paper, we refer to CoMiC-EN simply as CoMiC.
Sketched briefly, CoMiC operates in three stages:
1. Annotation uses various NLP modules to equip
student answers and reference answers with lin-
guistic abstractions of several types.
2. Alignment creates links between these linguistic
abstractions from the reference answer to the
student answer.
3. Classification uses summary statistics of these
alignment links in machine learning in order to
assign labels to each student answer.
Automatic annotation and alignment are imple-
mented in the Unstructured Information Management
Architecture (UIMA, Ferrucci and Lally, 2004). Our
UIMA modules mainly wrap around standard NLP
tools of which we provide an overview in Table 1.
We used the standard statistical models which are
provided with the NLP tools.
Annotation Task NLP Component
Sentence Detection OpenNLP2
Tokenization OpenNLP
Lemmatization morpha (Minnen et al, 2001)
Spell Checking Edit distance (Levenshtein, 1966),
SCOWL word list3
Part-of-speech Tagging TreeTagger (Schmid, 1994)
Noun Phrase Chunking OpenNLP
Synonyms and WordNet (Fellbaum, 1998)
Semantic Types
Similarity Scores PMI-IR (Turney, 2001)
on UkWaC (Baroni et al, 2009)
Dependency Relations MaltParser (Nivre et al, 2007)
Keyword extraction Heads from dependency parse
Table 1: NLP tools used for CoMiC and Bag Approaches
Annotation ranges from very basic linguistic units
such as sentences and tokens with POS and lemmas,
over NP chunks, up to full dependency parses of
the input. For distributional semantic similarity via
PMI-IR (Turney, 2001), a local search engine based
on Lucene (Gospodnetic? and Hatcher, 2005) querying
the UkWaC corpus (Baroni et al, 2009) was used,
since all major search engines meanwhile have shut
down their APIs.
After the annotation of linguistic units has taken
place, candidate alignment links are created within
UIMA. In a simple example case, a candidate align-
ment link is a pair of tokens that is token identical
in the student answer and in the reference answer.
The same token in the student answer may also be
part of a candidate alignment link that maps to an-
other token in the reference answer that, e.g., has the
same lemma, or is a possible synonym, or again is
token identical. Other possible links are based on
spelling-corrected tokens, semantic types, or high
values of the PMI-IR similarity measure.
Words that are present in the reading comprehen-
sion question and that are also found in the student an-
swer are excluded from alignment, resulting in a very
2http://incubator.apache.org/opennlp
3http://wordlist.sourceforge.net
609
basic implementation of an approach to givenness
(cf. Halliday, 1967, p. 204 and many others since).
Subsequently, a globally optimal alignment of lin-
guistic units in the reference answer and student an-
swer is determined using the Traditional Marriage
Algorithm (Gale and Shapley, 1962).
At this point, processing within UIMA comes to
an end with an output module that generates the files
containing the features for machine learning. These
features basically are summary statistics of the types
of alignment links. An overview of these numeric
features used is given in Table 2.
Feature Description
1. Keyword Overlap Percent of keywords aligned
(relative to target)
2./3. Token Overlap Percent of aligned
target/learner tokens
4./5. Chunk Overlap Percent of aligned
target/learner chunks
6./7. Triple Overlap Percent of aligned
target/learner triples
8. Token Match Percent of token alignments
that were token-identical
9. Similarity Match Percent of token alignments
that were similarity-resolved
10. Type Match Percent of token alignments
that were type-resolved
11. Lemma Match Percent of token alignments
that were lemma-resolved
12. Synonym Match Percent of token alignments
that were synonym-resolved
13. Variety of Match Number of kinds of
(0-5) token-level alignments
Table 2: Features used in CoMiC?s classification phase
Current versions of CoMiC use the WEKA toolkit
(Hall et al, 2009), allowing us to experiment with
different machine learning strategies. In general, any
type of classification can be trained in this machine
learning phase, a binary correct vs. incorrect de-
cision as in the 2-way task being the simplest case.
The best results with CoMiC on our held-out develop-
ment set were achieved using WEKA?s J48 classifier,
which is an implementation of decision tree based on
Quinlan (1993).
In terms of linguistic abstractions, CoMiC leaves
the choice of representations used to its alignment
step. However, in the final machine learning step, no
concrete information about linguistic units is present
any more. The machine learning component only
sees alignment configurations which are indepen-
dent of concrete words, phrases, or any other lin-
guistic information. This high level of abstraction
suggests that CoMiC should perform better than other
approaches on unseen topics and unseen questions,
since it does not rely on concrete units as, e.g., a
bag-of-words approach does.
2.2 CoSeC
CoSeC (Comparing Semantics in Context) performs
meaning comparison on the basis of an underspec-
ified semantic representation robustly derived from
the learner and the reference answers. The sys-
tem was developed for German (Hahn and Meurers,
2012), on the basis of which we created the English
CoSeC-EN for the SemEval 2013 Task 7 challenge.
Using an explicit semantic formalism in principle
makes it possible to precisely represent meaning dif-
ferences. It also supports a direct representation of
Information Structure as a structuring of semantics
representations (Krifka, 2007).
CoSeC is based on Lexical Resource Semantics
(LRS, Richter and Sailer, 2004). Being an under-
specified semantic formalism, LRS avoids the costly
computation of all readings and provides access to
the building blocks of the semantic representation,
while additional constraints provide the information
about their composition.
As described in Hahn and Meurers (2011), LRS
representations can be derived automatically using
a two-step approach based on part-of-speech tags
assigned by TreeTagger (Schmid, 1994) and depen-
dency parses by MaltParser (Nivre et al, 2007). First,
the dependency structure is transformed into a com-
pletely lexicalized syntax-semantics interface rep-
resentation, which abstracts away from some form
variation at the surface. These representations are
then mapped to LRS representations. The approach
is robust in that it always results in an LRS structure,
even for ill-formed sentences.
CoSeC then aligns the LRS representations of the
reference answer and the student answer to each other
and also to the representation of the question. The
alignment approach takes into account local criteria,
namely the semantic similarity of pairs of elements
that are linked by the alignment, as well as global
criteria measuring the extent to which the alignment
610
preserves structure at the levels of variables and the
subterm structure of the semantic formulas.
Local similarity of semantic expressions is esti-
mated using WordNet (Fellbaum, 1998), FrameNet
(Baker et al, 1998), PMI-IR (Turney, 2001) on the
UkWaC (Baroni et al, 2009) as used in CoMiC, the
Minimum Edit Distance (Levenshtein, 1966), and
special parameters for comparing functional elements
such as quantifiers and grammatical function labels.
Based on the alignments, the system marks ele-
ments which are not linked to elements in the ques-
tion or which are linked to the semantic contribution
of an alternative in an alternative question as ?fo-
cused?. This is intended as a first approximation of
the concept of focus in the sense of Information Struc-
ture (von Heusinger, 1999; Kruijff-Korbayova? and
Steedman, 2003; Krifka, 2007), an active field of re-
search in linguistics addressing the question how the
information in sentences is packaged and integrated
into discourse. Focus elements are expected to be
particularly relevant for determining the correctness
of an answer (Meurers et al, 2011b).
Overall meaning comparison is then done based
on a set of numerical scores computed from the align-
ments and their quality. For each of these scores, a
threshold is empirically determined, over which the
student answer is considered to be correct. Among
the scores discussed by Hahn and Meurers (2011),
weighted-target focus, consistently scored best in the
development set. This score measures the percent-
age of terms in the semantic representation of the
reference answer which are linked to elements of
the student answer in relation to the number of all
elements in the representation of the reference an-
swer. Only terms that were marked as focused in
the preceding step are counted. Functional elements,
i.e., quantifiers, predicates representing grammatical
function labels, or the lambda operator, are weighted
differently from other elements.
This threshold method can only be used to perform
2-way classification. Unlike the machine learning
step in CoMiC, it does not generalize to 3-way or
5-way classification.
The alignment algorithm uses several numerical
parameters, such as weights for the different compo-
nents measuring semantic similarities, weights for
the different overall local and global criteria, and
the weight of the weighted-target focus score. These
parameters are optimized using Powells algorithm
combined with grid-based line optimization (Press et
al., 2002). To avoid overfitting, the parameters and
the threshold are determined on disjoint partitions of
the training set.
In terms of linguistic abstractions, meaning assess-
ment in CoSeC is based entirely on underspecified
semantic representations. Surface forms are indi-
rectly encoded by the structure of the representation
and the predicate names, which are usually derived
from the lemmas. As with CoMiC, parameter opti-
mization and the determination of the thresholds for
the numerical scores do not involve concrete infor-
mation about linguistic objects. Again, the high level
of abstraction suggests that CoSeC should perform
better than other approaches on unseen topics and
unseen questions.
2.3 The Bag Approaches
Inspired by the bag-of-words concept that emerged
from information retrieval (Salton and McGill, 1983),
we designed a system that uses bag representations
of student answers. For each student answer, there
are three bags, each containing one of the following
representations: words, lemmas and Soundex hashes
of that answer. The question ID corresponding to
the answer is added to each bag as a pseudo-word,
allowing the machine learner to adjust to question-
specific properties. Based on the bag representations,
the approach compares a given student answer to a
model trained on all other known student answers.
On the one hand, this method ignores the presence of
reference answers (although they could be added to
the training set as additional correct answers), on the
other hand it makes use of information not taken into
account by alignment-based systems such as CoMiC
or CoSeC.
Concerning pre-processing, the linguistic anal-
yses such as tokenization and lemmatization are
identical to those of CoMiC, since the bag gener-
ator technically is just another output module of the
UIMA-based pipeline used there. No stop-word list
is used. The bags are fed into a support vector-based
machine learner. We used WEKA?s Sequential Min-
imal Optimization (SMO, Platt, 1998) implementa-
tion with the radial basis function (RBF) kernel, since
it yielded good results on our development set and
since it supports output of the estimated probabilities
611
for each class. The optimal gamma parameter and
complexity constant were estimated via 10-fold grid
search.
In terms of abstractions, all bag-based approaches
simply disregard word order and in case of binary
bags even word frequency. Still, a bit of the relation
between words is essentially encoded in their mor-
phology. This piece of information is discarded in
the bags of lemmas, eventually, e.g., putting words
like ?bulb? and ?bulbs? in the same vector slot. Fur-
ther away from the surface are the Soundex hashes,
a phonetic representation of English words patented
by Russell (1918). The well-known algorithm trans-
forms similar-sounding English words into the same
representation of characters and numbers, thereby
ironing out many spelling mistakes and common
confusion cases of homophones such as ?there? vs.
?their?. The MorphAdorner4 implementation we used
returns empty Soundex hashes for input tokens that
do not start with a letter of the alphabet. However,
we found in our experiments, that the presence of
these empty hashes in the bags has a positive impact
on performance. This is most likely due to the fact
that it discriminates answers containing punctuation
(not a letter of the alphabet) from those which do not.
Since the bag approaches use Soundex as pho-
netic equivalence classes, but no semantic equiva-
lence classes, they should perform best on the unseen
answers data in which most lexical material from the
test set is likely to already be present in the training
set.
2.4 CoMeT: A Meta-Classifier
As described in the previous sections, our sub-
systems perform short answer evaluation on differ-
ent representations and at different levels of abstrac-
tion. The bag approaches are very surface-oriented,
whereas CoSeC uses a semantic formalism to com-
pare answers to each other. We expected each system
to show its strengths in different test scenarios, so a
way was needed to combine the predictions of differ-
ent systems into the final result.
CoMeT (Comparing Meaning in Tu?bingen) is a
meta-classifier which builds on the predictions of
our individual systems (feature stacking, see Wolpert,
1992). The rationale is that if systems are comple-
4http://morphadorner.northwestern.edu
mentary, their combination will perform better (or at
least as good) than any individual system on its own.
The design is as follows:
Each system produces predictions on the training
set, using 10-fold cross-validation, and on the test set.
In addition to the predicted class, each system was
also made to output probabilities for each possible
class (cf., e.g., Tetreault et al, 2012a). The class
probabilities were then used as features in the meta
classifier to train a model for the test data. In addition
to the probabilities, we also used the question ID and
module ID in the meta-classifier, in the hope that they
would allow differentiation between scenarios. For
example, an unseen question ID means that we are
not testing on unseen answers and thus predictions
from systems with more abstraction from the surface
may be preferred.
The class probabilities come from different
sources, depending on the system. In the case of
CoMiC, they are extracted directly from the decision
trees. For the bag approaches, we used WEKA?s op-
tion to fit logistic models to the SVM output after
classification in order to estimate probabilities. Fi-
nally, the CoSeC probabilities are derived directly
from its final score. As mentioned in section 2.2,
CoSeC only does binary classification, so those prob-
abilities are used in the meta-classifier for all tasks.
Based on the results on our internal development
set (see section 3.1), we chose different system com-
binations for different scenarios. For unseen topics
and unseen questions, we used only CoMiC in com-
bination with CoSeC, since the inclusion of the bag
approaches had a negative impact on results. For un-
seen answers, we additionally included the bag mod-
els. All meta-classification was done using WEKA?s
Logistic Regression implementation. The results are
discussed in section 3.
3 Evaluation
In this section, we present the results for each of the
sub-systems, both on the custom-made split of the
training data we used in our development, as well as
on the official test data of the SemEval 2013 Task 7
challenge. Subsequently, we discuss possible causes
for issues raised by our evaluation results.
612
3.1 Development Set
In order to be as close as possible to the final test
setting, we replicated the official test scenarios on
the training set, resulting in a train/dev/test split for
each of the corpora. For Beetle, we held out all an-
swers to two random questions for each module to
form the unseen questions scenario, and five random
answers from each remaining question to form the
unseen answers scenario. For SciEntsBank, we held
out module LF for dev and module VB for test to
form the unseen topics scenario, because they have
an average number of questions (11). The LF module
turned out to be far more skewed towards incorrect
answers (76.8%) than the training set on average
(57.5%). While this skewedness needs to be taken
into account for the interpretation of the development
results, it did not have a negative effect on our fi-
nal test results. Furthermore, analogous to Beetle,
we held out all answers to one random question for
each remaining module for unseen-questions, and
two random answers from each remaining question
for unseen answers.
The dev set was used for tuning and design deci-
sions concerning which individual systems to com-
bine in the stacked classifier, while we envisaged
the test set to be used as a final checkpoint before
submission.
The accuracy results for all sub-systems on the
development set are reported in detail in Table 3.
The majority baseline reflects the accuracy a system
would achieve by always labelling any student answer
as ?incorrect?, hence it is equivalent to the percentage
of incorrect answers in the data. The lexical baseline
is the performance of the system provided by the
challenge organizers.
Beetle SciEntsBank
System d-uA d-uQ d-uA d-uQ d-uT
Maj. Baseline 57.14% 59.28% 54.30% 60.70% 76.84%
Lex. Baseline 75.43% 71.10% 63.44% 66.05% 59.54%
CoMiC 76.57% 71.52% 67.20% 70.23% 64.63%
Bag of Words 85.14% 62.03% 80.65% 54.65% 73.79%
? of Lemmas 85.71% 58.02% 80.11% 52.33% 74.55%
? of Soundex 86.86% 60.76% 81.18% 53.95% 72.77%
CoSeC 76.00% 74.89% 64.52% 73.49% 68.96%
CoMeT 88.00% 75.95% 81.18% 66.74% 68.45%
Table 3: Development set: accuracy for 2-way task (uA:
unseen answers, uQ: unseen questions, uT: unseen topics)
The systems presented in section 2 performed as
expected: The Bag-of-Soundex system achieved its
best scores on the unseen answers where overlap of
vocabulary was most likely, outperforming CoMiC
and CoSeC with accuracy values as high as 86.86%.
For Beetle unseen answers, the meta-classifier op-
erated as expected and improved the overall results
to 88.86%. For SciEntsBank unseen answers, it re-
mained stable at 81.18%.
As expected, CoMiC and CoSeC with their align-
ment not depending on vocabulary outperformed the
bag approaches in the other scenarios, in which the
question or even the domain were not known during
training. However, both alignment-based systems
failed on SciEntsBank?s unseen topics in comparison
to the rather high majority baseline.
3.2 Official Test Set
For our submission to the SemEval 2013 Task 7 chal-
lenge, we trained our sub-systems on the entire of-
ficial training set. The overall performance of the
CoMeT system on all sub-tasks is shown in Table 4.
Beetle SciEntsBank
uA uQ uA uQ uT
Lexical 2-way 79.7% 74.0% 66.1% 67.4% 67.6%
Overlap 3-way 59.5% 51.2% 55.6% 54.0% 57.7%
Baseline 5-way 51.9% 48.0% 43.7% 41.3% 41.5%
Best 2-way 84.5% 74.1% 77.6% 74.5% 71.1%
System 3-way 73.1% 59.6% 72.0% 66.3% 63.7%
5-way 71.5% 62.1% 64.3% 53.2% 51.2%
CoMeT 2-way 83.8% 70.2% 77.4% 60.3% 67.6%
3-way 73.1% 51.8% 71.3% 54.6% 57.9%
5-way 68.8% 48.8% 60.0% 43.7% 42.1%
Table 4: Official test set: overall accuracy of CoMeT (uA:
unseen answers, uQ: unseen questions, uT: unseen topics)
While CoMeT won the Beetle 3-way task in unseen
answers, our main focus is on the 2-way task. The
results for the 2-way task of our sub-systems on the
official test set are shown in Table 5.
The first row of the table reports the results of the
winning system of the challenge; the two baselines
are computed as before. In general, the accuracy val-
ues of CoMeT exhibit a drop of around 5% from
our development set to the official test set. The
meta-classifier was unable to benefit from the dif-
ferent sub-systems except for the unseen answers in
SciEntsBank that slightly outperformed the best bag
approach.
613
Beetle SciEntsBank
System uA uQ uA uQ uT
Best 84.50% 74.10% 77.60% 74.50% 71.10%
Maj. Baseline 59.91% 58.00% 56.85% 58.94% 57.98%
Lex. Baseline 79.70% 74.00% 66.10% 67.40% 67.60%
CoMiC 76.08% 70.57% 67.96% 66.30% 67.97%
Bag of Words 83.14% 67.52% 75.93% 57.84% 59.84%
? of Lemmas 83.60% 67.16% 76.67% 58.25% 58.81%
? of Soundex 84.05% 68.38% 75.93% 57.57% 58.02%
CoSeC 62.19% 63.61% 67.22% 58.94% 62.36%
CoMeT 83.83% 70.21% 77.41% 60.30% 67.62%
CoSeC* 75.40% 70.82% 72.04% 64.94% 70.60%
CoMeT* 84.51% 71.43% 79.26% 65.35% 69.53%
Table 5: Official test set: accuracy for 2-way task (uA:
unseen answers, uQ: unseen questions, uT: unseen topics)
Even though it does not live up to the standards of
the bag approaches in their area of expertise (unseen
answers), the CoMiC systems outperforms the bags
on the unseen question and unseen topic sub-sets as
expected. Note that on unseen topics, CoMiC still
scores 10% above the majority baseline on the official
test set, in contrast to the drop of more than 10%
below the baseline for the corresponding (skewed)
development set.
However, the results for CoSeC are around 10%
lower on the unseen questions, and almost 7% lower
on the unseen topics of the test data than on the de-
velopment set, a drop that the overall meta-classifier
(CoMeT) was unable to catch. Investigating this drop
in comparison to our development set, we checked
the correctness of the training script and discovered a
bug in the CoSeC setup that led to the parameters and
the thresholds being computed on the same partition
of the training set, i.e., the system overfitted to this
partition, while the remainder of the training set was
not used for training. Correcting the bug resulted in
CoSeC accuracy values broadly comparable to those
of CoMiC, as was the case on the development set.
This confirms that the reason for the drop in the sub-
mission was not a flaw in the CoSeC system as such,
but a programming bug in a peripheral component.
With this bug fixed, CoSeC performs 5%?13%
better on the test set, and the meta-classifier would
have been able to benefit from the regularly perform-
ing CoSeC, improving in performance up to 5%.
These two amended systems are listed as CoSeC*
and CoMeT* in Table 5. For the two unseen an-
swers scenarios, CoMeT* would outperform the best
scoring systems of the challenge in the 2-way task.
3.3 Discussion
In this section, we try to identify some general ten-
dencies from studying the results. Firstly, we can
observe that due to the strong performance of the bag
models, unseen answers scores are generally higher
than their counterparts. It seems that if questions
have been seen before, surface-oriented methods out-
perform more abstract approaches. However, the
picture is different for unseen domains and unseen
questions. We are generally puzzled by the fact that
many systems in the shared task scored worse on
unseen questions, where in-domain training data is
available, than on unseen domains, where this is not
the case. The CoMeT classifier suffered especially in
unseen questions of SciEntsBank, scoring lower than
our best system would have on its own (see Table 5);
even after the CoSeC bug was fixed, CoMeT* still
scored worse there than CoMiC on its own.
In general, we likely would have benefited from
domain adaptation, as described in, e.g., Daume III
(2007). Consider that the input for the meta-classifier
always consists of the same set of features produced
via standard cross-validation, regardless of the test
scenario. Instead, the trained model should have dif-
ferent feature weights depending on what the model
will be tested on.
4 Conclusion and Outlook
We presented our approach to Task 7 of SemEval
2013, consisting of a combination of surface-oriented
bag models and the increasingly abstract alignment-
based systems CoMiC and CoSeC. Predictions of
all systems were combined using a meta classifier in
order to produce the final result for CoMeT.
The results presented show that our approach per-
forms competitively, especially in the unseen answers
test scenarios, where we obtained the best result of all
participants in the 3-way task with the Beetle corpus
(73.1% accuracy). As expected, the unseen topics
scenario proved to be more challenging, with results
at 67.6% accuracy in the 2-way task for CoMeT. Sur-
prisingly, CoMeT performed consistently worse in
the unseen questions scenarios, which we attribute
to rather low CoSeC results there and to the way the
meta classifier is trained, which currently does not
take into account the test scenario it is trained for
and instead uses the module and question IDs as fea-
614
tures, which turned out not to be an effective domain
adaptation approach.
In our future research, work on CoMiC will con-
centrate on integrating two aspects of the context:
First, we are planning to develop an automatic ap-
proach to focus identification in order to pinpoint the
essential parts of the student answers. Second, for
data sets where a reading text is available, we will
try to automatically determine the location of the rel-
evant source information given the question, which
can then be used as alternative or additional reference
material for answer evaluation.
The CoMiC system currently also relies on the
Traditional Marriage Algorithm to select the optimal
global alignment between student answer and refer-
ence answer. We plan to replace this algorithm by
a machine learning component that can handle this
selection in a data-driven way.
For CoSeC, we plan to develop an extension that
allows for n-to-m mappings, hence improving the
alignment performance for multi-word units such as,
e.g., phrasal verb constructions.
The bag approaches could be augmented by explor-
ing additional levels of abstractions, e.g., semantic
equivalence classes constructed via WordNet lookup.
In sum, while we will also plan to explore opti-
mizations to the training setup of the meta-classifier
(e.g., domain adaptation along the lines of Daume
III, 2007), the main focus of our further research lies
in improving the individual sub-systems, which then
again are expected to push the overall performance
of the CoMeT meta-classifier system.
Acknowledgements
We are thankful to Sowmya Vajjala and Serhiy Bykh
for their valuable advice on meta-classifiers and other
machine learning techniques. We also thank the re-
viewers for their comments; in consultation with the
SemEval organizers we kept the length at 8 pages
plus references, the page limit for papers describing
multiple systems.
References
Stacey Bailey and Detmar Meurers. 2008. Diagnosing
meaning errors in short answers to reading compre-
hension questions. In Joel Tetreault, Jill Burstein,
and Rachele De Felice, editors, Proceedings of the
3rd Workshop on Innovative Use of NLP for Building
Educational Applications (BEA-3) at ACL?08, pages
107?115, Columbus, Ohio. http://aclweb.org/
anthology/W08-0913.pdf.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceedings of
the 36th Annual Meeting of the Association for Compu-
tational Linguistics and 17th International Conference
on Computational Linguistics, volume 1, pages 86?90,
Montreal, Quebec, Canada.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A collec-
tion of very large linguistically processed web-crawled
corpora. Journal of Language Resources and Evalua-
tion, 3(43):209?226.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch, 2007. TiMBL: Tilburg
Memory-Based Learner Reference Guide, ILK Techni-
cal Report ILK 07-03. Induction of Linguistic Knowl-
edge Research Group Department of Communication
and Information Sciences, Tilburg University, Tilburg,
The Netherlands, July 11. Version 6.0.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rational, evalu-
ation and approaches. Natural Language Engineering,
15(4):i?xvii, 10.
Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic, June. Association
for Computational Linguistics.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.
2013. Semeval-2013 task 7: The joint student response
analysis and 8th recognizing textual entailment chal-
lenge. In *SEM 2013: The First Joint Conference on
Lexical and Computational Semantics, Atlanta, Geor-
gia, USA, 13-14 June.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, Mas-
sachusetts.
David Ferrucci and Adam Lally. 2004. UIMA: An ar-
chitectural approach to unstructured information pro-
cessing in the corporate research environment. Natural
Language Engineering, 10(3?4):327?348.
David Gale and Lloyd S. Shapley. 1962. College admis-
sions and the stability of marriage. American Mathe-
matical Monthly, 69:9?15.
Otis Gospodnetic? and Erik Hatcher. 2005. Lucene in
Action. Manning, Greenwich, CT.
Michael Hahn and Detmar Meurers. 2011. On deriv-
ing semantic representations from dependencies: A
615
practical approach for evaluating meaning in learner
corpora. In Proceedings of the Intern. Confer-
ence on Dependency Linguistics (DEPLING 2011),
pages 94?103, Barcelona. http://purl.org/
dm/papers/hahn-meurers-11.html.
Michael Hahn and Detmar Meurers. 2012. Evaluat-
ing the meaning of answers to reading comprehen-
sion questions: A semantics-based approach. In Pro-
ceedings of the 7th Workshop on Innovative Use of
NLP for Building Educational Applications (BEA-7) at
NAACL-HLT 2012, pages 94?103, Montreal. http:
//aclweb.org/anthology/W12-2039.pdf.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten. 2009.
The WEKA data mining software: An update. In The
SIGKDD Explorations, volume 11, pages 10?18.
Michael Halliday. 1967. Notes on Transitivity and Theme
in English. Part 1 and 2. Journal of Linguistics, 3:37?
81, 199?244.
Manfred Krifka. 2007. Basic notions of information struc-
ture. In Caroline Fery, Gisbert Fanselow, and Manfred
Krifka, editors, The notions of information structure,
volume 6 of Interdisciplinary Studies on Information
Structure (ISIS). Universita?tsverlag Potsdam, Potsdam.
Ivana Kruijff-Korbayova? and Mark Steedman. 2003. Dis-
course and information structure. Journal of Logic,
Language and Information (Introduction to the Special
Issue), 12(3):249?259.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710.
Detmar Meurers, Ramon Ziai, Niels Ott, and Stacey Bai-
ley. 2011a. Integrating parallel analysis modules to
evaluate the meaning of answers to reading comprehen-
sion questions. IJCEELL. Special Issue on Automatic
Free-text Evaluation, 21(4):355?369. http://purl.
org/dm/papers/meurers-ea-11.html.
Detmar Meurers, Ramon Ziai, Niels Ott, and Janina Kopp.
2011b. Evaluating answers to reading comprehen-
sion questions in context: Results for German and
the role of information structure. In Proceedings of
the TextInfer 2011 Workshop on Textual Entailment,
pages 1?9, Edinburgh, Scotland, UK, July. http:
//aclweb.org/anthology/W11-2401.pdf.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natural
Language Engineering, 7(3):207?233.
Joakim Nivre, Jens Nilsson, Johan Hall, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(1):1?41.
Niels Ott, Ramon Ziai, and Detmar Meurers. 2012. Cre-
ation and analysis of a reading comprehension exercise
corpus: Towards evaluating meaning in context. In
Thomas Schmidt and Kai Wo?rner, editors, Multilin-
gual Corpora and Multilingual Corpus Analysis, Ham-
burg Studies in Multilingualism (HSM), pages 47?69.
Benjamins, Amsterdam. http://purl.org/dm/
papers/ott-ziai-meurers-12.html.
John C. Platt. 1998. Sequential minimal optimization:
A fast algorithm for training support vector machines.
Technical Report MSR-TR-98-14, Microsoft Research.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2002. Numerical Recipes
in C++. Cambridge University Press, Cambridge, UK.
J.R. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann Publishers.
Frank Richter and Manfred Sailer. 2004. Basic concepts
of lexical resource semantics. In Arnold Beckmann and
Norbert Preining, editors, European Summer School in
Logic, Language and Information 2003. Course Mate-
rial I, volume 5 of Collegium Logicum, pages 87?143.
Publication Series of the Kurt Go?del Society, Wien.
Robert C. Russell. 1918. US patent number 1.261.167, 4.
Gerard Salton and Michael J. McGill. 1983. Introduction
to modern information retrieval. McGraw-Hill, New
York.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49, Manchester, UK.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and found:
Resources and empirical evaluations in native language
identification. In Proceedings of the 24th International
Conference on Computational Linguistics (COLING),
pages 2585?2602, Mumbai, India.
Peter Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the Twelfth European Conference on Machine Learn-
ing (ECML-2001), pages 491?502, Freiburg, Germany.
Klaus von Heusinger. 1999. Intonation and Information
Structure. The Representation of Focus in Phonology
and Semantics. Habilitationssschrift, Universita?t Kon-
stanz, Konstanz, Germany.
David H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5(2):241?259.
Ramon Ziai, Niels Ott, and Detmar Meurers. 2012. Short
answer assessment: Establishing links between re-
search strands. In Joel Tetreault, Jill Burstein, and
Claudial Leacock, editors, Proceedings of the 7th Work-
shop on Innovative Use of NLP for Building Edu-
cational Applications (BEA-7) at NAACL-HLT 2012,
pages 190?200, Montreal, June. http://aclweb.
org/anthology/W12-2022.pdf.
616
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 98?106,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Emotional Perception of Fairy Tales:
Achieving Agreement in Emotion Annotation of Text
Ekaterina P. Volkova1,2, Betty J. Mohler2, Detmar Meurers1, Dale Gerdemann1, Heinrich H. Bu?lthoff2
1 Universita?t Tu?bingen, Seminar fu?r Sprachwissenschaft
19 Wilchelmstr., Tu?bingen, 72074, Germany
2 Max Planck Institute for Biological Cybernetics
38 Spemannstr., Tu?bingen, 72076, Germany
Abstract
Emotion analysis (EA) is a rapidly developing
area in computational linguistics. An EA
system can be extremely useful in fields such
as information retrieval and emotion-driven
computer animation. For most EA systems,
the number of emotion classes is very limited
and the text units the classes are assigned
to are discrete and predefined. The question
we address in this paper is whether the set
of emotion categories can be enriched and
whether the units to which the categories
are assigned can be more flexibly defined.
We present an experiment showing how an
annotation task can be set up so that untrained
participants can perform emotion analysis
with high agreement even when not restricted
to a predetermined annotation unit and using
a rich set of emotion categories. As such it
sets the stage for the development of more
complex EA systems which are closer to the
actual human emotional perception of text.
1 Introduction
As a first step towards developing an emotion
analysis (EA) system simulating human emotional
perception of text, it is important to research the
nature of the emotion analysis performed by humans
and examine whether they can reliably perform
the task. To investigate these issues, we conducted
an experiment to find out the strategies people
use to annotate selected folk fairy tale texts for
emotions. The participants had to choose from a set
of fifteen emotion categories, a significantly larger
set than typically used in EA, and assign them to an
unrestricted range of text.
To explore whether human annotators can reliably
perform a task, inter-annotator agreement (IAA)
(Artstein and Poesio, 2008) is the relevant measure.
This measure can be calculated between every two
individual annotations in order to find pairs or even
teams of annotators whose strategies seem to be
consistent and coherent enough so that they can be
used further as the gold-standard annotation suited
to train a machine learning approach for automatic
EA analysis. A resulting EA system, capable of
simulating human emotional perception of text,
would be useful for information retrieval and many
other fields.
There are two main aspects of the resulting anno-
tations to be researched. First, how consistently can
people perceive and locate the emotional aspect of
fairy tale texts? Second, how do they express their
perception of text by means of annotation strategies?
In the next sections, we address these questions and
provide details of an experiment we conducted to
empirically advance our understanding of the issues.
2 Motivation and Aimed Application
Most existing EA systems are implemented for and
used in specific predefined areas. The application
field could be anything from extracting appraisal
expressions (Whitelaw et al, 2005) to opinion
mining of customer feedback (Lee et al, 2008).
In our case, the intended application of the EA
system predominantly is emotion enhancement of
human-computer interaction, especially in virtual
or augmented reality. Emotion enhancement of
98
computer animation, especially when it deals with
spoken or written text, is primarily done through
manual annotation of text, even if a rich database
of perceptually guided animations for behavioral
scripts compilation is available (Cunningham and
Wallraven, 2009). The resulting system of our
project is meant to be a bridge between unprocessed
input text (generated or provided) and visual and
auditory information, coming from the virtual
character, like generated speech, facial expressions
and body language. In this way a virtual character
would be able to simulate emotional perception and
production of text in story telling scenarios.
3 Related Work
Although EA is often referred to as a developing
field, the amount of work carried out during the last
decades is phenomenal. This section is not meant as
a full overview of the related research as that scope
is too great for the length of this paper. To contextu-
alize the research presented in this paper we focus on
the projects that inspired us and fostered the ideas.
The work done by Alm (Alm and Sproat, 2005;
Alm et al, 2005; Alm, 2008) is close to our
project in its sprit and goals. Alm, (2008) aims at
implementing affective text-to-speech system for
storytelling scenarios. An EA system, detecting
sentences with emotions expressed in written text
is a crucial element for achieving this goal. The
annotated corpus was composed of three sets of
children?s stories written by Beatrix Potter, H. C.
Andersen, and the Brothers Grimm.
Like Liu et al (2003), Alm (2008) uses sev-
eral emotional categories, while most research in
automatic EA works with pure polarities. The set
of emotion categories used is essentially the list of
basic emotions (Ekman, 1993), which has a justified
preference for negative emotion categories. Ek-
mann?s list of basic emotions was extended by Alm,
since the emotion of surprise is validly taken as am-
bivalent and was thus split into positive surprise and
negative surprise. The EA system described in Alm
et al (2005) is machine learning based, where the
EA problem is defined as multi-class classification
problem, with sentences as classification units.
Liu et al (2003) have combined an emotion
lexicon and handcrafted rules, which allowed them
to create affect models and thus form a representa-
tion of the emotional affinity of a sentence. Their
annotation scheme is also sentence-based. The
EA system was tested on short user-composed text
emails describing emotionally colored events.
In the research on recognizing contextual polarity
done by Wilson et al (2009) a rich prior-polarity
lexicon and dependency parsing technique were
employed to detect and analyze subjectivity on
phrasal level, taking into account all the power of
context, captured through such features as negation,
polarity modification and polarity shifters. The
work presents auspicious results of high accuracy
scores for classification between neutrality and
polarized private states and between negative and
positive subjective phrases. A detailed account
of several ML algorithms performance tests is
discussed in thought-provoking manner. This work
encouraged us to build a lexicon of subjective clues
and use sentence structure information for future
feature extraction and ML architecture training.
Another thought-provoking work by Polanyj
(2006) shows the influence of the context on subjec-
tive clues. This is relevant to our project since we
are collecting lexicons of subjective clues and the
mechanisms of contextual influence may prove to
be of value for future automatic EA system training.
Bethard et at. (2004) provide valuable informa-
tion about corpus annotation for EA means and give
accounts on the performance of various existing ML
algorithms. They provide excellent analysis of au-
tomatic extraction of opinion proposition and their
holders. For feature extraction, the authors employ
such well-known resources as WordNet (Miller et
al., 1990), PropBank (Kingsbury et al, 2002) and
FrameNet (Baker et al, 1998). Several types of
classification tasks involve evaluation on the level
of documents. For example, detecting subjective
sentences, expressions, and other opinionated items
in documents representing certain press categories
(Wiebe et al, 2004) and measuring strength of
subjective clauses (Wilson et al, 2004). All these
and many more helped us to decide upon our own
strategies, provided many examples of corpus col-
lection and annotation, feature extraction and ML
techniques usage in ways specific for the EA task.
99
4 Experimental Setup
Having established the research context, we now
turn to the questions we investigate in this paper:
the use of an enriched category set and the flexible
annotation units, and their influence on annotation
quality. We describe the experiment we conducted
and its main results. Each participant performed
several tasks for each session. The first task always
was a cognitive task on emotion categories taken
outside the fairy tales context. The results are dis-
cussed in Sections 4.1 and 4.2. The next assignment
discussed in Section 4.3 was to annotate a list of
words for their inherent polarities. The third task
was to read the text out loud to the experimenter.
This allowed the participant to feel immersed into
the story telling scenario and also get used to the
text of the story they were about to annotate for
the full set of emotion categories. The annotation
process is described in Section 4.4. The last exercise
was to read the full fairy tale text out loud again,
with the difference that this time their voice and
face were recorded by means of a microphone and a
camera. The potential importance of the extra data
sources like speech melody and facial expressions
are further discussed in Section 8 as future work.
Ten German native speakers voluntarily partic-
ipated in the experiment. The participants were
divided into two groups and each participant worked
on five of the eight texts. The fairy tale sets for each
group overlapped in two texts, which allowed us to
achieve a high number of individual annotations in a
short amount of time and compare the performance
of people working on different sets of texts (see
Table 1). Each participant annotated their texts in
five sessions, dealing with only one text per session.
The fatigue effect was avoided as no annotator had
more than one session a day.
4.1 Determining Emotion Categories
First, we needed to define the set of emotions to
be used in the experiment. Based on the current
emotion theories from comparative literature and
cognitive psychology (Ekman, 1993; Auracher,
2007; Fontaine et al, 2007), we compiled a set of
fifteen emotion categories: seven positive, seven
negative, and neutral (see Table 2). We chose an
equal number of negative and positive emotions,
User Fairy Tale ID
JG D R BR FH DS BM SJ
A1 ? ? ? ? ?
A2 ? ? ? ? ?
A3 ? ? ? ? ?
A4 ? ? ? ? ?
A5 ? ? ? ? ?
A6 ? ? ? ? ?
A7 ? ? ? ? ?
A8 ? ? ? ? ?
A9 ? ? ? ? ?
A10 ? ? ? ? ?
Table 1: Annotation Sets
Positive Negative
Entspannung (relief) Unruhe (disturbance)
Freude (joy) Trauer (sadness)
Hoffnung (hope) Verzweiflung ( despair)
Interesse (interest) Ekel (disgust)
Mitgefu?hl (compassion) Hass (hatred)
U?berraschung (surprise) Angst (fear)
Zustimmung (approval) A?rger (anger)
Table 2: Emotion Categories Used in the Experiment
since in our experiment the main focus is on the
freedom and equality of choice of emotion cate-
gories. We aimed at the set to be comprehensive and
we also expected the participants to be able to detect
each of the emotions in the text as well as express
them through speech melody and facial expressions.
The polarity of each category was determined
experimentally. Participants were asked to decide
on the underlying polarity of each emotion category
and then to evaluate each emotion on an intensity
scale [1:5], ?5? marking extreme polarization, ?1?
being close to neutral. All participants were in full
agreement concerning the underlying polarity of
the emotions in the set, while the numerical values
varied. It is important to note, that the category
U?berraschung (surprise) was stably estimated as
positive. In English the word surprise is reported
to be ambivalent (Alm and Sproat, 2005), but we
found that in German its most common translation
is clearly positive.
4.2 Emotion Categories Clustering
In the second part of the experiment we asked partic-
ipants to organize the fifteen emotions into clusters.
Each cluster was to represent a situation in which
100
Cluster Polarity
{relief, hope, joy} positive
{joy, surprise} positive
{joy, approval} positive
{approval, interest} positive
{disgust, anger, hatred} negative
{fear, despair, disturbance} negative
{fear, disturbance, sadness} negative
{sadness, compassion} mixed
Table 3: Emotion Clusters
several emotions were equally likely to co-occur,
e.g. a situation formulated by a participant as ?When
a friend gives me a nicely wrapped birthday present
and I am about to open it.? was reported to involve
such emotions as joy, interest and surprise. On
average, each participant has formed 5 clusters with
3?4 items per cluster. The clusters were encoded as
sets on unordered pairs of items. Pairs were filtered
out if they were indicated by fewer than seven par-
ticipants. As the result, the following eight clusters
were obtained (see Table 3). For most clusters, the
categories composing them share one polarity. The
{sadness, compassion} cluster is the only exception.
It is important to note that the clusters were
determined through this cognitive task, indepen-
dently of the annotations. Since the annotators
agree well on clustering the emotions, employing
this information captures conceptual agreement
between individual annotations even if the specific
emotion categories for the same stretch of text do
not coincide. However, we intend to keep the full
set of emotions for the future corpus expansions.
4.3 Word list Annotation
For each text, we compiled its word list by taking the
set of words contained in the text, normalizing each
word to its lemma and filtering the set for most com-
mon German stop words (function words, pronouns,
auxiliaries). Like full story texts, word lists were
divided into two annotation sets. At each session,
before seeing the full text of the fairy tale, the partic-
ipant was to annotate each item of the corresponding
word list for its inherent polarity. All the words were
taken out their contexts and were neutral by default.
The annotator?s task was to label only those words
that had the potential to change the polarity of the
context in which they could occur. We purposefully
German Title English Title Abbr.
Arme Junge im Grab Poor Boy in Grave JG
Bremer Stadtmusikanten Bremen Musicians BM
Dornro?schen Little Briar-Rose BR
Eselein Donkey D
Frau Holle Mother Hulda FH
Heilige Joseph im Walde St. Joseph in Forest SJ
Hund und Sperling Dog and Sparrow DS
Ra?tsel Riddle R
Table 4: Stories Used (the titles are shortened)
did not limit the task to the words occurring in all
texts in order to be able to investigate the stability
of participants? decisions. Every annotator worked
with five word lists, one for each fairy tale text. The
total number of unique items for the first annotation
set was 893 words and 823 words long for the
second set; 267 and 236 words correspondingly
occurred in more than one word list. These words
could potentially be marked with different polarity
categories, but in fact only about 15% of those
words (4% from the total number of items on each
of the word lists) were ?unstable?, namely, labeled
with different polarities by the same annotator. The
labels received in these cases were either {positive,
neutral} or {negative, neutral}. These words were
further ?stabilized? by either choosing the most
frequent label or the neutral label if the unstable
word had received only two label instances. The
results show that such annotation tasks could be
used further for subjective clues lexicon collection.
4.4 Text Annotation
For the third and main part of the experiment, we
selected eight Grimm?s fairy tales, each 1200 ? 1400
words long and written in Standard German (see
Table 4). The texts were chosen based on their
genre, for in spite of the depth of all the hidden
and open references to human psyche and national
traditions that were shown in works of (von Franz,
1996; Propp and Dundes, 1977), folk fairy tales
are relatively uncomplicated in the plot-line and
the characters? personalities. Due to this relative
simplicity of the content, we expect the participants?
emotional reactions to folk fairy tale texts to be more
coherent than to other texts of fiction literature.
The task for the participants was to locate and
mark stretches of text where an emotion was to be
101
conveyed through the speech melody and/or facial
expressions if the participant was to read the text
out loud. To make the annotation process and its
further analysis time-efficient and convenient for
both, annotators and experimenters, a simple tool
was developed. We created the Manual Emotion
Annotation Tool (MEAT) which allows the user
to annotate text for emotion by selecting stretches
of text and labeling it with one of fifteen emotion
categories. The application also has a special mode
for word list annotation, where only the three
polarity categories are available: positive, negative
and neutral. The user can always undo their labels
or change them until they are satisfied with the
annotation and can submit the results. The main
part of the experiment resulted in fifty individual
annotations which produced 150 annotation pairs.
5 Analyzing Inter-annotator Agreement
For each of the 150 pairs (two texts annotated
by ten annotators, six texts annotated by five
annotators), the IAA rate was calculated. However,
the calculation of IAA is not as straightforward
in this situation as it might seem. In many types
of corpus annotation, e.g., in POS tagging, there
are previously identified discrete elements. In this
experiment we intentionally have no predefined
units, even if this makes the IAA calculation more
difficult. Consider the following examples:
(1) A1: ?. . . [the evil wolf]X ate the girl?
A2: ?. . . the [evil wolf ate the girl]X?
(2) A1: ?. . . [the evil wolf]X ate the girl?
A2: ?. . . [the evil wolf]Y ate the girl?
(3) A1: ?. . . [the evil wolf]X ate the girl?
A2: ?. . . the evil wolf ate [the girl]X?
(4) A1: ?. . . [the evil wolf]X ate [the girl]Z?
A2: ?. . . [the evil wolf ate the girl]X?
In example (1) both annotators marked certain
stretches of text with the same category X, but the
annotations do not completely coincide, there is
only an overlap. This situation is similar to that in
syntactic annotation, where one needs to distinguish
between bracketing and labeling of the constituent
and measures such as Parseval (Carroll et al, 2002)
have been much debated.
Both annotators in example (1) recognize evil
wolf as marked for X and thus this example should
be counted towards agreement, while examples (2)
and (3) should not. A second type of evaluation
arises if the emotion clusters are taken into account.
According to this evaluation type, example (2) is
counted towards agreement if the categories X and
Y belong to the same cluster.
Example (4) provides an illustration of how IAA
is accounted for in a more complex case. Annotator
A1 has marked two stretches of text with two
different emotion categories, while annotator A2
has united both stretches under the same emotion
category. Both annotators agree that the evil wolf is
marked for X, but disagree on the emotion category
for the girl. In order to avoid the crossing brackets
problem (Carroll et al, 2002), we treat the evil
wolf ate as agreement, and the girl as disagree-
ment. Although ate was left unmarked by one of
the annotators, it is counted towards agreement
because it is next to a stretch of text on which both
annotators agree. Stretches of text the annotators
agree or disagree upon also receive weight values:
the higher the number of words that belong to open
word classes in a stretch, the higher its weight.
The general calculation formulae for the IAA
measure are taken from (Artstein and Poesio, 2008):
? =
Ao ?Ae
1?Ae
Ao =
1
i
?
i?I
argi
Ae =
1
I2
?
k?K
nc1knc2k
Ao is the observed agreement, Ae is the expected
agreement, I is the number of annotation items, K
is the set of all categories used by both annotators,
nck is the number of items assigned by annotator c
to category k.
6 Analyzing Annotation Strategies
Analysis of IAA, presented in Section 5 can answer
the first question we aim to investigate: How consis-
tently do people perceive and locate the emotional
aspect of fairy tale texts? The second issue nec-
essary for investigation is the annotation strategies
people use to express their emotional perception
of text. In our experiment conditions, the resulting
strategies can be investigated via three aspects:
a) length of user-defined flexible units b) emotional
102
0%?1%?
2%?3%?
4%?5%?
6%?7%?
8%?9%?
10%?
1? 3? 5? 7? 9? 11? 13? 15? 17? 19? 21? 23? 25? 27? 29?
Unit?
lengt
h?Fre
quen
cy?(%
)?
Unit?Length?(in?word?tokens)?
Figure 1: Annotator Defined Unit Length Rating
composition of fairy tales c) emotional flow of the
fairy tales. In this section we give a brief account of
our findings concerning the given aspects.
The participants were always free to select text
stretches of the length they considered to be appro-
priate for a specific emotional category label. The
only guideline they received was to mark the entire
stretch of text which, according to their judgement,
was marked by the chosen emotion category and,
if read without the surrounding context, would
still allow one to clearly perceive the applied
emotion category label. As Figure 1 shows, the
most frequent unit length consists of four to seven
word tokens, which corresponds to short phrases,
e.g., a verb phrase with a noun phrase argument.
We consider the findings to be encouraging, since
this observation could be used favorably for the
automatic EA system training.
Emotional composition of a fairy tale helps to re-
veal the overall character of the text and establish
if the story is abundant with various emotions or is
overloaded with only a few. For our overall research
goal, we would prefer the former kind of stories,
since they would build a rich training corpus. Fig-
ures 2 and 3 give an overview on the average shares
various emotion categories hold over the eight texts.
It is important to note that 65%? 75% of the text was
left neutral. The results show that most stories are
rich in positive rather than negative emotions, with
two exceptions we would like to elaborate upon. The
stories The Poor Boy in the Grave and The Dog
and the Sparrow belonged to different annotation
sets and thus no annotator dealt with both stories.
These texts were selected partially for their potential
0%?
5%?
10%?
15%?
20%?
25%?
JG? DS? BR? BT? SJ? D? FH? R?
approval?compassion?hope?interest?joy?relief?surprise?
Figure 2: Distribution of Positive Emotion Categories in Texts
0%?
5%?
10%?
15%?
20%?
25%?
JG? DS? BR? BT? SJ? D? FH? R?
anger?despair?disgust?disturbance?fear?hatered?sadness?
Figure 3: Distribution of Negative Emotion Categories in Texts
overcharge with negative emotions. The hypothesis
proved to be true, since the annotators have labeled
on average 20% of text with negative emotions, like
hatred and sadness. The only positive emotion cate-
gory salient for the The Poor Boy in the Grave story
is compassion, which is also mostly triggered by sad
events happening to a positive character.
The emotional flow in the fairy tales is illustrated
by the graph presented in Figure 4. In order to build
it, we used the numerical evaluations obtained in
the first part of the experiment and described in
section 4.1. For each fairy tale text, each word token
was mapped to the absolute value of the average
numerical evaluation of its emotional categories
assigned by all participants. The word tokens also
received its relative position in the text, where the
first word was at position 0.0 and the last at 1.0.
Thus, the emotional trajectories of all texts were
correlated despite the fact that their actual lengths
differed. The polynomial fit graph, taken over thus
acquired emotional flow common for all fairy tale
texts has a wave-shaped form and is similar to the
103
0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 00 . 00 . 20 . 40 . 6
0 . 81 . 01 . 21 . 4
1 . 6
S t o r y  p r o g r e s s  [ r u ]Emotional r
esponse [ru]
Figure 4: Emotional Trajectory over all Stories
emotional trajectory reported by Alm and Sproat
(2005). The emotional charge increases and falls
steeply in the beginning of the fairy tale, then cycles
though rise and fall phases (which do not exceed
in their intensity the average rate of 0.6) and then
ascents steeply at the end of the story. We agree with
the explanation of such a trajectory, given by Propp
and Dundes (1977) and also elaborated by Alm and
Sproat (2005) ? the first emotional intensity peak
in the story line corresponds to the rising action,
after the main characters have been introduced and
the plot develops through a usually unexpected
event. At the end of the story the intensity is high-
est, regardless whether the denouement is a happy
ending or a tragedy. The fact that the fairy tale texts
we chose for the experiment are relatively short is
probably responsible for the steep peak of intensity
in the very beginning of the story ? the stories are
too short to include a proper exposition. However,
we need to investigate further how much of this is a
property of texts themselves and how much ? the
perception (and thus annotation) of emotions.
7 Results
The IAA scores were calculated using the emotion
clusters information, for according to the results,
participants would often stably use different emo-
tions from same clusters at the same stretch of text.
Four out of ten participants, two from each
group (marked gray in Table 1), had very low IAA
scores (? < 0.40 average per participant), a high
proportion of unmarked text, and they used few
emotion categories ( < 7 categories average per
participant), so for the evaluation part their data was
discarded. The final IAA evaluation was calculated
on all the annotation pairs obtained from the six
remaining participants (marked black in table 1),
whose average agreement score in the original set
of participants was originally higher than 0.50. The
total number of annotation pairs amounted to 48:
two texts annotated by all the six annotators, six
texts annotated by three annotators for each of the
two annotation sets.
According to the interpretation of ? by (Landis
and Koch, 1977), the annotator agreement was mod-
erate on average (0.53), and some pairs approached
the almost perfect IAA rate (0.83). The IAA rates,
calculated on the full set of fifteen emotions, with-
out taking the emotion clusters into consideration,
gave a moderate IAA rate on average (0.34) and
reached substantial level (0.62) at maximum. The
? rates are considerably high for the hard task and
are comparable with the results presented in (Alm
and Sproat, 2005). The word lists have a somewhat
lower ? IAA (0.45 on average, 0.72 at maximum),
which is due to the low number of categories and
the heavy bias towards the neutral category. The
observed agreement on word lists is considerably
high: 0.81 on average, reaching 0.91 at maximum.
While our approach may seem very similar to
the one of Alm (2005), there are some important
differences. We gave the participants the freedom of
using flexible annotation units, which allowed the
annotators to define the source of emotion more pre-
cisely and mark several emotions in one sentence. In
fact, in 39% of all annotated sentences represented a
mixture of the neutral category and ?polarized? cat-
egories, 20% of which included more than one ?po-
larized? categories. Another difference is the rich set
of emotion categories, with equal number of positive
and negative items. The results show that people can
successfully use the large set to express their emo-
tional perception of text (e.g., see Figures 3 and 2).
Other important findings include the fact that
short phrases are the naturally preferred annotation
unit among our participants and that the emotional
trajectory of a general story line corresponds to the
one proposed by Propp and Dundes (1977).
104
8 Future Work
8.1 Corpus Expansion
In the near future, we will expand the collections
of annotated text in order to compile a substantially
large training corpus. We plan to work further
with three annotators that have formed a natural
team, since their group has always attained the
highest annotation scores for their annotation set,
exceeding the highest scores in the other annotation
set. The task defined for the three annotators is
similar to the experiment described in the paper,
with several differences. For the corpus expansion
we chose 85 stories by the Grimm Brothers 1400
? 4500 tokens long. We expect that longer texts
have more potential space for an emotionally rich
plot. Each text will be annotated by two people,
the third annotator will tie-break disagreements by
choosing the most appropriate of the conflicting
categories, similar to the method described by (Alm
and Sproat, 2005). It is also probable that a basic
annotation unit will be defined and imposed on the
annotators, for, as the studies discussed in Section 6
show, short phrases are a language unit most often
naturally chosen by annotators.
Each of the annotators will also work with a sin-
gle word list, compiled from all texts and filtered for
the most common stop-words. Each of the words on
the word list should be annotated with its inherent
polarity (positive, negative or neutral). Since each
word on the list is free of its context, the lists
provide valuable information about the word and its
context interaction in full texts, which can be further
used for machine learning architecture training.
We also plan to keep the fifteen emotion cat-
egories and their clustering, since it gives the
annotator more freedom of expression and simulta-
neously allows the researches to find the common
cognitive ground behind the labels if they vary
within one cluster
8.2 Feature Extraction and Machine Learning
Architecture Training
When the corpus is large enough, the relevant
features will be extracted automatically by means
of existing NLP tools, followed by training a ma-
chine learning architecture, most probably TiMBL
(Daelemans et al, 2004), to map textual units to
the emotion categories. It is yet to be determined
which features to use, one compulsory parameter
is that all the features should be available through
automatic processing tools. This is crucial, since
the resulting EA system has to be fully automated
with no manual work involved.
8.3 Extra Information Sources and their
Potential Contribution
We also plan to collect data from other information
sources, like video and audio recordings, by inviting
amateur actors for story-telling sessions. This will
allow emotion retrieval from the speech melody,
facial expressions and body language. The manual
annotation and the extra data sources can be aligned
by means of Text and Speech Aligner (Rapp, 1995),
which allows to track correspondences between
them. This alignment would most certainly ben-
efit the facial and body animation of the virtual
characters, since there is no clear understanding
of time correlation between emotions labeled in
written text and the ones expressed through speech
and facial clues in a story telling scenario. An EA
system could also be perfected through a careful
analysis of recorded speech and video of story
telling sessions ? regular recurrence of subjectivity
of certain contexts will be even more significant
if the transmission of the emotions from the story
teller to the listener via mentioned information
sources is successful.
9 Conclusions
In this paper, we reported on an experiment inves-
tigating the inter-annotator agreement levels which
can be achieved by untrained human annotators per-
forming emotion analysis of variable units of text.
While EA is a very difficult task, our experiment
shows that even untrained annotators can have high
agreement rates, even given considerable freedom
in expressing their emotional perception of text. To
the best of our knowledge, this is the first attempt at
emotion analysis that operates on flexible, annotator
defined units and uses a relatively rich inventory of
emotion categories. We consider the resulting IAA
rates to be high enough to accept the annotations
as suitable for gold-standard corpus compilation in
the frame of this research. As such, we view this
work as the first step towards the development of a
more complex EA system, which aims to simulate
the actual human emotional perception of text.
105
References
C.O. Alm and R. Sproat. 2005. Emotional sequencing
and development in fairy tales. In Proceedings of the
First International Conference on Affective Computing
and Intelligent Interaction (ACII05). Springer.
C.O. Alm, D. Roth, and R. Sproat. 2005. Emotions from
text: Machine learning for text-based emotion predic-
tion. In Proceedings of HLT/EMNLP, volume 2005.
C.O. Alm. 2008. Affect in Text and Speech.
lrc.cornell.edu.
R. Artstein and M. Poesio. 2008. Inter-coder agreement
for computational linguistics. Computational Linguis-
tics, 34(4):555?596.
Jan Auracher. 2007. ... wie auf den allma?chtigen Schlag
einer magischen Rute. Psychophysiologische Messun-
gen zur Textwirkung. Ars poetica ; 3. Dt. Wiss.-Verl.
C.F. Baker, C.J. Fillmore, and J.B. Lowe. 1998.
The berkeley framenet project. In Proceedings of
the 17th international conference on Computational
linguistics-Volume 1, pages 86?90. Association for
Computational Linguistics Morristown, NJ, USA.
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2004. Automatic
extraction of opinion propositions and their holders. In
2004 AAAI Spring Symposium on Exploring Attitude
and Affect in Text, page 2224.
J. Carroll, A. Frank, D. Lin, D. Prescher, and H. Uszkor-
eit. 2002. Beyond Parseval-Towards improved evalua-
tion measures for parsing systems. In Workshop at the
3rd International Conference on Language Resources
and Evaluation LREC-02., Las Palmas.
D. W. Cunningham and C. Wallraven. 2009. Dynamic
information for the recognition of conversational ex-
pressions. Journal of Vision, 9(13:7):1?17, 12.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2004. Timbl: Tilburg memory
based learner, version 5.1, reference guide. ilk techni-
cal report 04-02. Technical report.
P. Ekman. 1993. Facial Expression and Emotion. Amer-
ican Psychologist, 48(4):384?392.
JR Fontaine, KR Scherer, EB Roesch, and PC Ellsworth.
2007. The world of emotions is not two-dimensional.
Psychological science: a journal of the American Psy-
chological Society/APS, 18(12):1050.
P. Kingsbury, M. Palmer, and M. Marcus. 2002. Adding
semantic annotation to the Penn Treebank. In Pro-
ceedings of the Human Language Technology Confer-
ence, pages 252?256. Citeseer.
J.R. Landis and G.G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159?174.
D. Lee, O.R. Jeong, and S. Lee. 2008. Opinion min-
ing of customer feedback data on the web. In Pro-
ceedings of the 2nd international conference on Ubiq-
uitous information management and communication,
page 230235, New York, New York, USA. ACM.
Hugo Liu, Henry Lieberman, and Ted Selker. 2003.
A model of textual affect sensing using real-world
knowledge. In Proceedings of the 8th international
conference on Intelligent user interfaces - IUI ?03,
page 125, New York, New York, USA. ACM Press.
G.A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K.J. Miller. 1990. Introduction to Wordnet: An on-
line lexical database*. International Journal of lexi-
cography, 3(4):235.
L. Polanyi and A. Zaenen. 2006. Contextual valence
shifters. Computing Attitude and Affect in Text: The-
ory and Applications, page 110.
V.I.A. Propp and A. Dundes. 1977. Morphology of the
Folktale. University of Texas Press.
S. Rapp. 1995. Automatic phonemic transcription and
linguistic annotation from known text with Hidden
Markov Models. In Proceedings of ELSNET Goes
East and IMACS Workshop. Citeseer.
M.L. von Franz. 1996. The interpretation of fairy tales.
Shambhala Publications.
C. Whitelaw, N. Garg, and S. Argamon. 2005. Using ap-
praisal groups for sentiment analysis. In Proceedings
of the 14th ACM international conference on Informa-
tion and knowledge management, page 631. ACM.
J. Wiebe, T. Wilson, R. Bruce, M. Bell, and M. Martin.
2004. Learning subjective language. Computational
linguistics, 30(3):277?308.
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad
are you? Finding strong and weak opinion clauses. In
Proceedings of the National Conference on Artificial
Intelligence, pages 761?769. Menlo Park, CA; Cam-
bridge, MA; London; AAAI Press; MIT Press; 1999.
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recogniz-
ing Contextual Polarity: an exploration of features for
phrase-level sentiment analysis. Computational Lin-
guistics, 35(3):399433, September.
106
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 10?18,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Enhancing Authentic Web Pages for Language Learners
Detmar Meurers1, Ramon Ziai1,
Luiz Amaral2, Adriane Boyd3, Aleksandar Dimitrov1, Vanessa Metcalf3, Niels Ott1
1 Universita?t Tu?bingen
2 University of Massachusetts Amherst
3 The Ohio State University
Abstract
Second language acquisition research since
the 90s has emphasized the importance of
supporting awareness of language categories
and forms, and input enhancement techniques
have been proposed to make target language
features more salient for the learner.
We present an NLP architecture and web-
based implementation providing automatic vi-
sual input enhancement for web pages. Learn-
ers freely choose the web pages they want to
read and the system displays an enhanced ver-
sion of the pages. The current system supports
visual input enhancement for several language
patterns known to be problematic for English
language learners, as well as fill-in-the-blank
and clickable versions of such pages support-
ing some learner interaction.
1 Introduction
A significant body of research into the effectiveness
of meaning-focused communicative approaches to
foreign language teaching has shown that input
alone is not sufficient to acquire a foreign lan-
guage, especially for older learners (cf., e.g., Light-
bown and Spada, 1999). Recognizing the important
role of consciousness in second-language learning
(Schmidt, 1990), learners have been argued to ben-
efit from (Long, 1991) or even require (Lightbown,
1998) a so-called focus on form to overcome incom-
plete or incorrect knowledge of specific forms or
regularities. Focus on form is understood to be ?an
occasional shift of attention to linguistic code fea-
tures? (Long and Robinson, 1998, p. 23).
In an effort to combine communicative and struc-
turalist approaches to second language teaching,
Rutherford and Sharwood Smith (1985) argued for
the use of consciousness raising strategies drawing
the learner?s attention to specific language proper-
ties. Sharwood Smith (1993, p. 176) coined the term
input enhancement to refer to strategies highlighting
the salience of language categories and forms.
Building on this foundational research in second
language acquisition and foreign language teaching,
in this paper we present an NLP architecture and a
system for automatic visual input enhancement of
web pages freely selected by language learners. We
focus on learners of English as a Second Language
(ESL), and the language patterns enhanced by the
system include some of the well-established diffi-
culties: determiners and prepositions, the distinction
between gerunds and to-infinitives, wh-question for-
mation, tense in conditionals, and phrasal verbs.
In our approach, learners can choose any web
page they like, either by using an ordinary search-
engine interface to search for one or by entering the
URL of the page they want to enhance. In contrast to
textbooks and other pre-prepared materials, allow-
ing the learner to choose up-to-date web pages on
any topic they are interested in and enhancing the
page while keeping it intact (with its links, multi-
media, and other components working) clearly has
a positive effect on learner motivation. Input en-
hanced web pages also are attractive for people out-
side a traditional school setting, such as in the vol-
untary, self-motivated pursuit of knowledge often
referred to as lifelong learning. The latter can be
particularly relevant for adult immigrants, who are
10
already functionally living in the second language
environment, but often stagnate in their second lan-
guage acquisition and lack access or motivation to
engage in language classes or other explicit lan-
guage learning activities. Nevertheless, they do use
the web to obtain information that is language-based
and thus can be enhanced to also support language
acquisition while satisfying information needs.
In terms of paper organization, in section 2 we
first present the system architecture and in 2.1 the
language phenomena handled, before considering
the issues involved in evaluating the approach in 2.2.
The context of our work and related approaches are
discussed in section 3, and we conclude and discuss
several avenues for future research in section 4.
2 The Approach
The WERTi system (Working with English Real
Texts interactively) we developed follows a client-
server paradigm where the server is responsible for
fetching the web page and enriching it with annota-
tions, and the client then receives the annotated web
page and transforms it into an enhanced version.
The client here is a standard web browser, so on the
learner?s side no additional software is needed.
The system currently supports three types of input
enhancement: i) color highlighting of the pattern or
selected parts thereof, ii) a version of the page sup-
porting identification of the pattern through clicking
and automatic color feedback, and iii) a version sup-
porting practice, such as a fill-in-the-blank version
of the page with automatic color feedback.
The overall architecture is shown in Figure 1.
Essentially, the automated input enhancement pro-
cess consists of the following steps:
1. Fetch the page.
2. Find the natural language text portions in it.
3. Identify the targeted language pattern.
4. Annotate the web page, marking up the lan-
guage patterns identified in the previous step.
5. Transform the annotated web page into the out-
put by visually enhancing the targeted pattern
or by generating interaction possibilities.
Steps 1?4 take place on the server side, whereas step
5 happens in the learner?s browser.1 As NLP is only
involved in step 3, we here focus on that step.
1As an alternative to the server-based fetching of web pages,
Server
UIMA                     
Browser                                         
URL Fetching
HTML Annotation
Identifying text in HTML page
Tokenization
Sentence Boundary Detection
POS Tagging
Pattern-specific NLP
Colorize Click Practice
Figure 1: Overall WERTi architecture. Grey components
are the same for all patterns and activities, cf. section 2.1.
While the first prototype of the WERTi system2
presented at CALICO (Amaral, Metcalf and Meur-
ers, 2006) and EUROCALL (Metcalf and Meurers,
2006) was implemented in Python, the current sys-
tem is Java-based, with all NLP being integrated in
the UIMA framework (Ferrucci and Lally, 2004).
UIMA is an architecture for the management and
analysis of unstructured information such as text,
which is built on the idea of referential annotation
and can be seen as an NLP analysis counterpart
to current stand-off encoding standards for anno-
tated corpora (cf., e.g., Ide et al 2000). The input
we are developing a Firefox plugin, leaving only the NLP up to
the server. This increases compatibility with web pages using
dynamically generated contents and special session handling.
2http://purl.org/icall/werti-v1
11
can be monotonically enriched while passing from
one NLP component to the next, using a flexible
data repository common to all components (Go?tz
and Suhre, 2004). Such annotation-based processing
is particularly useful in the WERTi context, where
keeping the original text intact is essential for dis-
playing it in enhanced form.
A second benefit of using the UIMA framework is
that it supports a flexible combination of individual
NLP components into larger processing pipelines.
To obtain a flexible approach to input enhancement
in WERTi, we need to be able to identify and an-
alyze phenomena from different levels of linguistic
analysis. For example, lexical classes can be iden-
tified by a POS tagger, whereas other patterns to be
enhanced require at least shallow syntactic chunk-
ing. The more diverse the set of phenomena, the
less feasible it is to handle all of them within a
single processing strategy or formalism. Using the
UIMA framework, we can re-use the same basic
processing (e.g., tokenizing, POS tagging) for all
phenomena and still be able to branch into pattern-
specific NLP in a demand-driven way. Given that
NLP components in UIMA include self-describing
meta-information, the processing pipeline to be run
can dynamically be obtained from the module con-
figuration instead of being hard-wired into the core
system. The resulting extensible, plugin-like archi-
tecture seems particularly well-suited for the task of
visual input enhancement of a wide range of hetero-
geneous language properties.
Complementing the above arguments for the
UIMA-based architecture of the current WERTi sys-
tem, a detailed discussion of the advantages of an
annotation-based, demand-driven NLP architecture
for Intelligent Computer-Assisted Language Learn-
ing can be found in Amaral, Meurers, and Ziai (To
Appear), where it is employed in an Intelligent Lan-
guage Tutoring System.
2.1 Implemented Modules
The modules implemented in the current system
handle a number of phenomena commonly judged
as difficult for second language learners of English.
In the following we briefly characterize each mod-
ule, describing the nature of the language pattern,
the required NLP, and the input enhancement results,
which will be referred to as activities.
Lexical classes
Lexical classes are the most basic kind of linguis-
tic category we use for input enhancement. The in-
ventory of lexical categories to be used and which
ones to focus on should be informed by second
language acquisition research and foreign language
teaching needs. The current system focuses on func-
tional elements such as prepositions and determiners
given that they are considered to be particularly dif-
ficult for learners of English (cf. De Felice, 2008 and
references therein).
We identify these functional elements using the
LingPipe POS tagger (http://alias-i.com/
lingpipe) employing the Brown tagset (Francis
and Kucera, 1979). As we show in section 2.2, the
tagger reliably identifies prepositions and determin-
ers in native English texts such as those expected for
input enhancement.
The input enhancement used for lexical classes is
the default set of activities provided by WERTi. In
the simplest case, Color, all automatically identified
instances in the web page are highlighted by color-
ing them; no learner interaction is required. This is
illustrated by Figure 2, which shows the result of en-
hancing prepositions in a web page from the British
Figure 2: Screenshot of color activity for prepositions, cf.
http://purl.org/icall/werti-color-ex
12
newspaper The Guardian.3
In this and the following screenshots, links al-
ready present in the original web page appear in light
blue (e.g., Vauban in Germany). This raises an im-
portant issue for future research, namely how to de-
termine the best visual input enhancement for a par-
ticular linguistic pattern given a specific web page
with its existing visual design features (e.g., bold-
facing in the text or particular colors used to indicate
links), which includes the option of removing or al-
tering some of those original visual design features.
A more interactive activity type is Click, where
the learner during reading can attempt to identify in-
stances of the targeted language form by clicking on
it. Correctly identified instances are colored green
by the system, incorrect guesses red.
Thirdly, input can be turned into Practice activi-
ties, where in its simplest form, WERTi turns web
pages into fill-in-the-blank activities and provides
immediate color coded feedback for the forms en-
tered by the learner. The system currently accepts
only the form used in the original text as correct.
In principle, alternatives (e.g., other prepositions)
can also be grammatical and appropriate. The ques-
tion for which cases equivalence classes of target an-
swers can automatically be determined is an interest-
ing question for future research.4
Gerunds vs. to-infinitives
Deciding when a verb is required to be realized as
a to-infinitive and when as a gerund -ing form can be
difficult for ESL learners. Current school grammars
teach students to look for certain lexical clues that
reliably indicate which form to choose. Examples
of such clues are prepositions such as after and of,
which can only be followed by a gerund.
In our NLP approach to this language pattern, we
use Constraint Grammar rules (Karlsson et al, 1995)
on top of POS tagging, which allow for straightfor-
ward formulation of local disambiguation rules such
as: ?If an -ing form immediately follows the prepo-
sition by, select the gerund reading.? Standard POS
3Given the nature of the input enhancement using colors, the
highlighting in the figure is only visible in a color printout.
4The issue bears some resemblance to the task of identify-
ing paraphrases (Androutsopoulos and Malakasiotis, 2009) or
classes of learner answers which differ in form but are equiva-
lent in terms of meaning (Bailey and Meurers, 2008).
tagsets for English contain a single tag for all -ing
forms. In order to identify gerunds only, we in-
troduce all possible readings for all -ing forms and
wrote 101 CG rules to locally disambiguate them.
The to-infinitives, on the other hand, are relatively
easy to identify based on the surface form and re-
quire almost no disambiguation.
For the implementation of the Constraint Gram-
mar rules, we used the freely available CG3 system.5
While simple local disambiguation rules are suffi-
cient for the pattern discussed here, through iterative
application of rules, Constraint Grammar can iden-
tify a wide range of phenomena without the need to
provide a full grammatical analysis.
The Color activity resulting from input enhance-
ment is similar to that for lexical classes described
above, but the system here enhances both verb forms
and clue phrases. Figure 3 shows the system high-
lighting gerunds in orange, infinitives in purple, and
clue phrases in blue.
Figure 3: Color activity for gerunds vs. to-infinitives, cf.
http://purl.org/icall/werti-color-ex2
For the Click activity, the web page is shown
with colored gerund and to-infinitival forms and the
learner can click on the corresponding clue phrases.
For the Practice activity, the learner is presented
with a fill-in-the-black version of the web page, as
in the screenshot in Figure 4. For each blank, the
learner needs to enter the gerund or to-infinitival
form of the base form shown in parentheses.
Wh-questions
Question formation in English, with its particu-
lar word order, constitutes a well-known challenge
for second language learners and has received sig-
nificant attention in the second language acquisi-
5http://beta.visl.sdu.dk/cg3.html
13
Figure 4: Practice activity for gerunds vs. to-infinitives,
cf. http://purl.org/icall/werti-cloze-ex
tion literature (cf., e.g., White et al, 1991; Spada
and Lightbown, 1993). Example (1) illustrates the
use of do-support and subject-aux inversion in wh-
questions as two aspects challenging learners.
(1) What do you think it takes to be successful?
In order to identify the wh-question patterns, we
employ a set of 126 hand-written Constraint Gram-
mar rules. The respective wh-word acts as the lex-
ical clue to the question as a whole, and the rules
then identify the subject and verb phrase based on
the POS and lexical information of the local context.
Aside from the Color activity highlighting the rel-
evant parts of a wh-question, we adapted the other
activity types to this more complex language pattern.
The Click activity prompts learners to click on either
the subject or the verb phrase of the question. The
Practice activity presents the words of a wh-question
in random order and requires the learner to rearrange
them into the correct one.
Conditionals
English has five types of conditionals that are used
for discussing hypothetical situations and possible
outcomes. The tenses used in the different condi-
tional types vary with respect to the certainty of the
outcome as expressed by the speaker/writer. For ex-
ample, one class of conditionals expresses high cer-
tainty and uses present tense in the if -clause and fu-
ture in the main clause, as in example (2).
(2) If the rain continues, we will return home.
The recognition of conditionals is approached us-
ing a combination of shallow and deep methods. We
first look for lexical triggers of a conditional, such as
the word if at the beginning of a sentence. This first
pass serves as a filter to the next, more expensive
processing step, full parsing of the candidate sen-
tences using Bikel?s statistical parser (Bikel, 2002).
The parse trees are then traversed to identify and
mark the verb forms and the trigger word.
For the input enhancement, we color all relevant
parts of a conditional, namely the trigger and the
verb forms. The Click activity for conditionals re-
quires the learner to click on exactly these parts. The
Practice activity prompts users to classify the condi-
tional instances into the different classes.
Phrasal verbs
Another challenging pattern for English language
learners are phrasal verbs consisting of a verb and
either a preposition, an adverb or both. The meaning
of a phrasal verb often differs considerably from that
of the underlying verb, as in (3) compared to (4).
(3) He switched the glasses without her noticing.
(4) He switched off the light before he went to bed.
This distinction is difficult for ESL learners, who
often confuse phrasal and non-phrasal uses.
Since this is a lexical phenomenon, we ap-
proached the identification of phrasal verbs via a
database lookup in a large online collection of verbs
known to occur in phrasal form.6 In order to find out
about noun phrases and modifying adverbs possibly
occurring in between the verb and its particles, we
run a chunker and use this information in specifying
a filter for such intervening elements.
The visual input enhancement activities targeting
phrasal verbs are the same as for lexical classes, with
the difference that for the Practice activity, learners
have to fill in only the particle, not the particle and
the main verb, since otherwise the missing contents
may be too difficult to reconstruct. Moreover, we
want the activity to focus on distinguishing phrasal
from non-phrasal uses, not verb meaning in general.
2.2 Evaluation issues
The success of a visual input enhancement approach
such as the one presented in this paper depends on
a number of factors, each of which can in principle
6http://www.usingenglish.com/reference/
phrasal-verbs
14
be evaluated. The fundamental but as far as we are
aware unanswered question in second language ac-
quisition research is for which language categories,
forms, and patterns input enhancement can be effec-
tive. As Lee and Huang (2008) show, the study of
visual input enhancement sorely needs more experi-
mental studies. With the help of the WERTi system,
which systematically produces visual input enhance-
ment for a range of language properties, it becomes
possible to conduct experiments in a real-life foreign
language teaching setting to test learning outcomes7
with and without visual input enhancement under a
wide range of parameters. Relevant parameters in-
clude the linguistic nature of the language property
to be enhanced as well as the nature of the input en-
hancement to be used, be it highlighting through col-
ors or fonts, engagement in different types of activi-
ties such as clicking, entering fill-in-the-blank infor-
mation, reordering language material, etc.
A factor closely related to our focus in this pa-
per is the impact of the quality of the NLP analysis.8
For a quantitative evaluation of the NLP, one signif-
icant problem is the mismatch between the phenom-
ena focused on in second language learning and the
available gold standards where these phenomena are
actually annotated. For example, standard corpora
such as the Penn Treebank contain almost no ques-
tions and thus do not constitute a useful gold stan-
dard for wh-question identification. Another prob-
lem is that some grammatical distinctions taught to
language learners are disputed in the linguistic liter-
ature. For example, Huddleston and Pullum (2002,
p. 1120) eliminate the distinction between gerunds
and present participles, combining them into a class
called ?gerund-participle?. And in corpus annota-
tion practice, gerunds are not identified as a class by
the tagsets used to annotate large corpora, making it
unclear what gold standard our gerund identification
component should be evaluated against.
While the lack of available gold standards means
that a quantitative evaluation of all WERTi mod-
ules is beyond the scope of this paper, the deter-
miner and preposition classes focused on in the lex-
ical classes module can be identified using the stan-
7Naturally, online measures of noticing, such as eye tracking
or Event-Related Potentials (ERP) would also be relevant.
8The processing time for the NLP analysis as other relevant
aspect is negligible for most of the activities presented here.
dard CLAWS-7 or Brown tagsets, for which gold-
standard corpora are available. We thus decided
to evaluate this WERTi module against the BNC
Sampler Corpus (Burnard, 1999), which contains
a variety of genres, making it particularly appro-
priate for evaluating a tool such as WERTi, which
learners are expected to use with a wide range of
web pages as input. The BNC Sampler corpus is
annotated with the fine-grained CLAWS-7 tagset9
where, e.g., prepositions are distinguished from sub-
ordinating conjunctions. By mapping the relevant
POS tags from the CLAWS-7 tagset to the Brown
tagset used by the LingPipe tagger as integrated in
WERTi, it becomes possible to evaluate WERTi?s
performance for the specific lexical classes focused
on for input enhancement, prepositions and deter-
miners. For prepositions, precision was 95.07% and
recall 90.52% while for determiners, precision was
97.06% with a recall of 94.07%.
The performance of the POS tagger on this refer-
ence corpus thus seems to be sufficient as basis for
visual input enhancement, but the crucial question
naturally remains whether identification of the target
patterns is reliable in the web pages that language
learners happen to choose. For a more precise quan-
titative study, it will thus be important to try the sys-
tem out with real-life users in order to identify a set
of web pages which can constitute an adequate test
set. Interestingly, which web pages the users choose
depends on the search engine front-end we provide
for them. As discussed under outlook in section 4,
we are exploring the option to implicitly guide them
towards web pages containing enough instances of
the relevant language patterns in text at the appro-
priate reading difficulty.
3 Context and related work
Contextualizing our work, one can view the auto-
matic visual input enhancement approach presented
here as an enrichment of Data-Driven Learning
(DDL). Where DDL has been characterized as an
?attempt to cut out the middleman [the teacher] as
far as possible and to give the learner direct access
to the data? (Boulton 2009, p. 82, citing Tim Johns),
in visual input enhancement the learner stays in con-
9http://www.natcorp.ox.ac.uk/docs/
c7spec.html
15
trol, but the NLP uses ?teacher knowledge? about rel-
evant and difficult language properties to make those
more prominent and noticeable for the learner.
In the context of Intelligent Computer-Assisted
Language Learning (ICALL), NLP has received
most attention in connection with Intelligent Lan-
guage Tutoring Systems, where NLP is used to ana-
lyze learner data and provide individual feedback on
that basis (cf. Heift and Schulze, 2007). Demands
on such NLP are high given that it needs to be able
to handle learner language and provide high-quality
feedback for any sentence entered by the learner.
In contrast, visual input enhancement makes use
of NLP analysis of authentic, native-speaker text and
thus applies the tools to the native language they
were originally designed and optimized for. Such
NLP use, which we will refer to as Authentic Text
ICALL (ATICALL), also does not need to be able
to correctly identify and manipulate all instances of
a language pattern for which input enhancement is
intended. Success can be incremental in the sense
that any visual input enhancement can be beneficial,
so that one can focus on enhancing those instances
which can be reliably identified in a text. In other
words, for ATICALL, precision of the NLP tools is
more important than recall. It is not necessary to
identify and enhance all instances of a given pattern
as long as the instances we do identify are in fact
correct, i.e., true positives. As the point of our sys-
tem is to enhance the reading experience by raising
language awareness, pattern occurrences we do not
identify are not harmful to the overall goal.10
We next turn to a discussion of some interest-
ing approaches in two closely related fields, exercise
generation and reading support tools.
3.1 Exercise Generation
Exercise generation is widely studied in CALL re-
search and some of the work relates directly to the
input enhancement approach presented in this paper.
For instance, Antoniadis et al (2004) describe the
plans of the MIRTO project to support ?gap-filling?
and ?lexical spotting? exercises in combination with
a corpus database. However, MIRTO seems to fo-
10While identifying all instances of a pattern indeed is not
crucial in this context, representativeness remains relevant to
some degree. Where only a skewed subset of a pattern is high-
lighted, learners may not properly conceptualize the pattern.
cus on a general architecture supporting instructor-
determined activity design. Visual input enhance-
ment or language awareness are not mentioned. The
VISL project (Bick, 2005) offers games and visual
presentations in order to foster knowledge of syntac-
tic forms and rules, and its KillerFiller tool can cre-
ate slot-filler exercises from texts. However, Killer-
Filler uses corpora and databases as the text base and
it presents sentences in isolation in a testing setup.
In contrast to such exercise generation systems, we
aim at enhancing the reader?s second language input
using the described web-based mash-up approach.
3.2 Reading Support Tools
Another branch of related approaches consists of
tools supporting the reading of texts in a foreign lan-
guage. For example, the Glosser-RuG project (Ner-
bonne et al, 1998) supports reading of French texts
for Dutch learners with an online, context-dependent
dictionary, as well as morphological analysis and ex-
amples of word use in corpora. A similar system,
focusing on multi-word lexemes, was developed in
the COMPASS project (Breidt and Feldweg, 1997).
More recently, the ALPHEIOS project11 has pro-
duced a system that can look up words in a lexi-
con and provide aligned translations. While such
lexicon-based tools are certainly useful to learners,
they rely on the learner asking for help instead of
enhancing specific structures from the start and thus
clearly differ from our approach.
Finally, the REAP project12 supports learners in
searching for texts that are well-suited for provid-
ing vocabulary and reading practice (Heilman et al,
2008). While it differs in focus from the visual input
enhancement paradigm underlying our approach, it
shares with it the emphasis on providing the learner
with authentic text in support of language learning.
4 Conclusion and Outlook
In this paper we presented an NLP architecture and
a concrete system for the enhancement of authen-
tic web pages in order to support language aware-
ness in ESL learners. The NLP architecture is flexi-
ble enough to integrate any processing approach that
lends itself to the treatment of the language phe-
11http://alpheios.net
12http://reap.cs.cmu.edu
16
nomenon in question, without confining the devel-
oper to a particular formalism. The WERTi system
illustrates this with five language patterns typically
considered difficult for ESL learners: lexical classes,
gerunds vs. to-infinitives, wh-questions, condition-
als and phrasal verbs.
Looking ahead, we already mentioned the funda-
mental open question where input enhancement can
be effective in section 2.2. A system such as WERTi,
systematically producing visual input enhancement,
can help explore this question under a wide range of
parameters in a real-life language teaching setting.
A more specific future research issue is the auto-
matic computation of equivalence classes of target
forms sketched in section 2.1. Not yet mentioned
but readily apparent is the goal to integrate more
language patterns known to be difficult for language
learners into WERTi (e.g., active/passive, tense and
aspect distinctions, relative clauses), and to explore
the approach for other languages, such as German.
A final important avenue for future research con-
cerns the starting point of the system, the step where
learners search for a web page they are interested
in and select it for presentation with input enhance-
ment. Enhancing of patterns presupposes that the
pages contain instances of the pattern. The less
frequent the pattern, the less likely we are to find
enough instances of it in web pages returned by the
standard web search engines typically used by learn-
ers to find pages of interest to them. The issue is re-
lated to research on providing learners with texts at
the right level of reading difficulty (Petersen, 2007;
Miltsakaki and Troutt, 2008), but the focus for us
is on ensuring that texts which include instances of
the specific language pattern targeted by a given in-
put enhancement are ranked high in the search re-
sults. Ott (2009) presents a search engine prototype
which, in addition to the content-focused document-
term information and traditional readability mea-
sures, supports indexing based on a more general no-
tion of a text model into which the patterns relevant
to input enhancement can be integrated ? an idea we
are exploring further (Ott and Meurers, Submitted).
Acknowledgments
We benefited from the feedback we received at
CALICO 06, EUROCALL 06, and the ICALL
course13 at ESSLLI 09, where we discussed our
work on the Python-based WERTi prototype. We
would like to thank Chris Hill and Kathy Corl
for their enthusiasm and encouragement. We are
grateful to Magdalena Leshtanska, Emma Li, Iliana
Simova, Maria Tchalakova and Tatiana Vodolazova
for their good ideas and WERTi module contribu-
tions in the context of a seminar at the University of
Tu?bingen in Summer 2008. Last but not least, the
paper benefited from two helpful workshop reviews.
References
Luiz Amaral, Vanessa Metcalf, and Detmar Meur-
ers. 2006. Language awareness through re-use
of NLP technology. Presentation at the CALICO
Workshop on NLP in CALL ? Computational and
Linguistic Challenges, May 17, 2006. University
of Hawaii. http://purl.org/dm/handouts/
calico06-amaral-metcalf-meurers.pdf.
Luiz Amaral, Detmar Meurers, and Ramon Ziai.
To Appear. Analyzing learner language: To-
wards a flexible NLP architecture for intelligent
language tutors. Computer-Assisted Language
Learning. http://purl.org/dm/papers/
amaral-meurers-ziai-10.html.
Ion Androutsopoulos and Prodromos Malakasiotis.
2009. A survey of paraphrasing and textual entailment
methods. Technical report, NLP Group, Informatics
Dept., Athens University of Economics and Business,
Greece. http://arxiv.org/abs/0912.3747.
Georges Antoniadis, Sandra Echinard, Olivier Kraif,
Thomas Lebarbe?, Mathieux Loiseau, and Claude Pon-
ton. 2004. NLP-based scripting for CALL activities.
In Proceedings of the COLING Workshop on eLearn-
ing for CL and CL for eLearning, Geneva.
Stacey Bailey and Detmar Meurers. 2008. Diagnosing
meaning errors in short answers to reading compre-
hension questions. In (Tetreault et al, 2008), pages
107?115.
Eckhard Bick. 2005. Grammar for fun: IT-based gram-
mar learning with VISL. In P. Juel, editor, CALL for
the Nordic Languages, pages 49?64. Samfundslitter-
atur, Copenhagen.
Daniel M. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In Pro-
ceedings of the Second Int. Conference on Human
Language Technology Research, San Francisco.
Alex Boulton. 2009. Data-driven learning: Reasonable
fears and rational reassurance. Indian Journal of Ap-
plied Linguistics, 35(1):81?106.
13http://purl.org/dm/09/esslli/
17
Elisabeth Breidt and Helmut Feldweg. 1997. Accessing
foreign languages with COMPASS. Machine Transla-
tion, 12(1?2):153?174.
L. Burnard, 1999. Users Reference Guide for the BNC
Sampler. Available on the BNC Sampler CD.
Rachele De Felice. 2008. Automatic Error Detection in
Non-native English. Ph.D. thesis, St Catherine?s Col-
lege, University of Oxford.
Catherine Doughty and J. Williams, editors. 1998. Fo-
cus on form in classroom second language acquisition.
Cambridge University Press, Cambridge.
David Ferrucci and Adam Lally. 2004. UIMA: an ar-
chitectural approach to unstructured information pro-
cessing in the corporate research environment. Natu-
ral Language Engineering, 10(3?4):327?348.
W. Nelson Francis and Henry Kucera, 1979. Brown cor-
pus manual. Dept. of Linguistics, Brown University.
Thilo Go?tz and Oliver Suhre. 2004. Design and im-
plementation of the UIMA Common Analysis System.
IBM Systems Journal, 43(3):476?489.
Trude Heift and Mathias Schulze. 2007. Errors and In-
telligence in Computer-Assisted Language Learning:
Parsers and Pedagogues. Routledge.
Michael Heilman, Le Zhao, Juan Pino, and Maxine Eske-
nazi. 2008. Retrieval of reading materials for vocab-
ulary and reading practice. In (Tetreault et al, 2008),
pages 80?88.
Rodney Huddleston and Geoffrey K. Pullum. 2002. The
Cambridge Grammar of the English Language. Cam-
bridge University Press.
Nancy Ide, Patrice Bonhomme, and Laurent Romary.
2000. XCES: An XML-based encoding standard for
linguistic corpora. In Proceedings of the 2nd Int. Con-
ference on Language Resources and Evaluation.
Tim Johns. 1994. From printout to handout: Grammar
and vocabulary teaching in the context of data-driven
learning. In T. Odlin, editor, Perspectives on Pedagog-
ical Grammar, pages 293?313. CUP, Cambridge.
Fred Karlsson, Atro Voutilainen, Juha Heikkila?, and
Arto Anttila, editors. 1995. Constraint Grammar:
A Language-Independent System for Parsing Unre-
stricted Text. Mouton de Gruyter, Berlin, New York.
Sang-Ki Lee and Hung-Tzu Huang. 2008. Visual in-
put enhancement and grammar learning: A meta-
analytic review. Studies in Second Language Acqui-
sition, 30:307?331.
Patsy M. Lightbown and Nina Spada. 1999. How lan-
guages are learned. Oxford University Press, Oxford.
Patsy M. Lightbown. 1998. The importance of timing
in focus on form. In (Doughty and Williams, 1998),
pages 177?196.
Michael H. Long and Peter Robinson. 1998. Focus on
form: Theory, research, and practice. In (Doughty and
Williams, 1998), pages 15?41.
M. H. Long. 1991. Focus on form: A design feature
in language teaching methodology. In K. De Bot,
C. Kramsch, and R. Ginsberg, editors, Foreign lan-
guage research in cross-cultural perspective, pages
39?52. John Benjamins, Amsterdam.
Vanessa Metcalf and Detmar Meurers. 2006.
Generating web-based English preposition
exercises from real-world texts. Presenta-
tion at EUROCALL, Sept. 7, 2006. Granada,
Spain. http://purl.org/dm/handouts/
eurocall06-metcalf-meurers.pdf.
Eleni Miltsakaki and Audrey Troutt. 2008. Real time
web text classification and analysis of reading diffi-
culty. In (Tetreault et al, 2008), pages 89?97.
John Nerbonne, Duco Dokter, and Petra Smit. 1998.
Morphological processing and computer-assisted lan-
guage learning. Computer Assisted Language Learn-
ing, 11(5):543?559.
Niels Ott and Detmar Meurers. Submitted. Information
retrieval for education: Making search engines lan-
guage aware. http://purl.org/dm/papers/
ott-meurers-10.html.
Niels Ott. 2009. Information retrieval for language learn-
ing: An exploration of text difficulty measures. Mas-
ter?s thesis, International Studies in Computational
Linguistics, University of Tu?bingen.
Sarah E. Petersen. 2007. Natural Language Processing
Tools for Reading Level Assessment and Text Simplifi-
cation for Bilingual Education. Ph.D. thesis, Univer-
sity of Washington.
William E. Rutherford and Michael Sharwood Smith.
1985. Consciousness-raising and universal grammar.
Applied Linguistics, 6(2):274?282.
Richard W. Schmidt. 1990. The role of conscious-
ness in second language learning. Applied Linguistics,
11:206?226.
Michael Sharwood Smith. 1993. Input enhancement in
instructed SLA: Theoretical bases. Studies in Second
Language Acquisition, 15:165?179.
Nina Spada and Patsy M. Lightbown. 1993. Instruction
and the development of questions in l2 classrooms.
Studies in Second Language Acquisition, 15:205?224.
Joel Tetreault, Jill Burstein, and Rachele De Felice, ed-
itors. 2008. Proceedings of the Third Workshop on
Innovative Use of NLP for Building Educational Ap-
plications. ACL, Columbus, Ohio, June.
Lydia White, Nina Spada, Patsy M. Lightbown, and Leila
Ranta. 1991. Input enhancement and L2 question for-
mation. Applied Linguistics, 12(4):416?432.
18
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 111?117,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Automatic?Sentiment?Classification?of?Product?Reviews?Using?Maximal?
Phrases?Based?Analysis
Maria?Tchalakova
Textkernel?BV
Nieuwendammerkade?
28A?17
NL?1022?AB?Amsterdam
The?Netherlands
maria.tchalakova@gm
ail.com
Dale?Gerdemann
University?of?T?bingen
Wilhelmstr.?19?23
72074?T?bingen
Germany
dale.gerdemann@googl
email.com
Detmar?Meurers
University?of?T?bingen
Wilhelmstr.?19?23
72074?T?bingen?
Germany
dm@sfs.uni-
tuebingen.de
Abstract
In ? this ? paper ? we ? explore ? the ? use ? of ? phrases?
occurring ? maximally ? in ? text ? as ? features ? for?
sentiment?classification?of?product?reviews.?The?
goal?is?to?find?in?a?statistical?way?representative?
words ? and ? phrases ? used ? typically ? in ? positive?
and ?negative ? reviews. ?The?approach ?does ?not?
rely?on?predefined?sentiment?lexicons,?and?the?
motivation ? for ? this ? is ? that ? potentially ? every?
word ? could ? be ? considered ? as ? expressing?
something?positive?and/or?negative?in?different?
situations,?and?that?the?context?and?the?personal?
attitude?of?the?opinion?holder?should?be?taken?
into?account?when?determining?the?polarity?of?
the ? phrase, ? instead ? of ? doing ? this ? out ? of?
particular?context.
1 Introduction
As?human?beings?we?use?different?ways?to?express?
opinions ? or ? sentiments. ? The ? field ? of ? sentiment?
analysis?tries?to?identify?the?ways,?in?which?people?
express?opinions?or?sentiments?towards?a?particular?
target ? or ? entity. ? The ? entities ? could ? be ? persons,?
products,?events,?etc.?With?the?development?of?the?
Internet?technologies?and?robust?search?engines?in?
the ? last ? decade, ? people ? nowadays ? have ? a ? huge?
amount?of?free?information.?Because?of?this?huge?
amount, ? however, ? the ? data ? needs ? to ? be ? first?
effectively?processed?so?that?it?could?be?used?in?a?
helpful ? way. ? The ? automatic ? identification ? of?
sentiments?would?make?possible?the?processing?of?
large?amounts?of?such?opinionated?data.?? 
???The?focus?of?this?paper?is?sentiment?classification?
at?document?level,?namely?classification?of?product?
reviews ? in ? the ? categories ? positive ? polarity ? or?
negative?polarity.?Training?and?testing?data?for?our?
experiments ? is ? the ? Multi?Domain ? Sentiment?
Dataset ? (Blitzer ? et ? al., ? 2007), ? which ? consists ? of?
product?reviews?of?different?domains,?downloaded?
from ? Amazon1. ? We ? explore ? the ? use ? of ? phrases?
occurring ? maximally ? in ? text ? as ? features ? for?
sentiment ? classification ? of ? product ? reviews. ?In?
contrast ? to ? many ? related ? works ? on ? sentiment?
classification?of?documents,?we?do?not?use?general?
polarity ? lexicons, ? which ? contain ? predefined?
positive?and?negative?words.?Very?often?the?same?
word?or?phrase?could?express?something?positive?in?
one?situation?and?something?negative? in ?another.?
We?identify?words?and?phrases,?which?are?typically?
used?in?positive?and?negative?documents?of?some?
specific?domains,?based?on?the?frequencies?of?the?
words?and?phrases?in?the?domain?specific?corpora.?
After ? that ?we ?use ? these ?phrases ? to ? classify ?new?
sentiment ? documents ? from ? the ? same ? type ? of?
documents,?from?which?the?phrases?are?extracted.
1http://www.amazon.com/
111
2 Phrase?Extraction
In?order?to?extract?distinctive?phrases?we?use?the?
approach?of?Burek?and?Gerdemann?(2009),?who?try?
to?identify?phrases,?which?are?distinctive?for?each?
of ? the ? four ?different ? categories ?of ? documents ? in?
their ? medical ? data. ? With ? distinctive ? they ? mean?
phrases, ? which ? occur ? predominantly ? in ? one?
category ? of ? the ? documents ? or ? another. ? The?
algorithm?extracts?phrases?of?any?length.?The?idea?
is ? that ? if ? a ? phrase ? is ? distinctive ? for ? a ? particular?
category,?it?does?not?matter?how?long?the?phrase?is.?
The?algorithm?looks?for?repeats?of?phrases?of?any?
length, ? and ? could ? also ? count ? different ? types ? of?
occurrences ? of ? phrases, ? e.g. ? maximal, ? left?
maximal,?or?right?maximal.?Considering?such?types?
of?occurrences,?it?is?possible?to?restrict?the?use?of?
certain ? phrases, ? which ? might ? not ? be ? much?
distinctive ? and ? therefore ? might ? not ? be?
representative?for?a?category.?Similar?to?Burek?and?
Gerdemann?(2009)?we?experiment?with?using?all?
types ?of ?occurrences ?of ? a ? phrase ? as ? long ?as ? the?
phrase?occurs?maximally?at? least?one?time?in?the?
text.
2.1 Distinctiveness?of?Phrases
Distinctive ? phrases ? are ? phrases, ? which?
predominantly ? occur ? in ? one ? particular ? type ? of?
documents ? (Burek ? and ? Gerdemann, ? 2009). ? The?
presence?of?such?phrases?in?a?document?is?a?good?
indicator?of?the?category?(or?type)?of?the?document.?
The?general?rule,?as?Burek?and?Gerdemann?(2009)?
point ? out, ? is ? that ? if ? some ?phrases ? are ?uniformly?
distributed ? in ? a ? set ? of ? documents ? with ? different?
categories, ? then ? these ?phrases ?are ?not ?distinctive?
for?any?of?the?categories?in?the?collection.?On?the?
other?hand,?if?particular?phrases?appear?more?often?
in?one?category?of?documents?than?in?another,?they?
are?good?representatives?for?the?documents?of?this?
type,?and?consequently?are?said?to?be?distinctive2.??
??There?are?different?weighting?schemes,?which?one?
can?use?to?determine?the?importance?of?a?term?for?
the ? semantics ? of ? a ? document. ? Burek ? and?
Gerdemann ? (2009) ? implement ? their ? own ? scoring?
2If ? the?number?of ?occurrences?of ? such?phrase? in ? the?whole?
collection?of?documents?is?very?small,?however,?the?clustering?
of?the?phrase?in?some?documents?of?a?specific?category,?may?
be?purely?accidental.?(Burek?and?Gerdemann,?2009)
function?for?weighting?the?extracted?phrases.?One?
of?their?reasons?not?to?use?the?standard?weighting?
function?tf?idf?is?that?the?idf?measure?does?not?take?
into?account?what?the?category?of?the?documents?is,?
in?which?the?term?occurs.?This?is?important?in?their?
case,?because?their?data?consist?of?four?categories,?
which ? could ? be ? grouped ? in ? two ? main ? classes,?
namely ?excellent? and ?good? on?the?one?hand,?and?
fair? and?poor?on?the?other?hand.?A?problem?when?
using?tf?idf?will ?appear, ? if ? there?is?a?rare?phrase,?
which ? occurs ? in ? a ? small ? number ? of ? documents,?
however, ? it ? clusters ? in ?documents ? from ? the ? two?
different?classes,?for?example,?in?excellent?and?fair,?
or ? in ?good? and ?poor. ? This ? will ? not ? be ? a ? good?
distinctive ? phrase ? for ? this ? categorization ? of ? the?
data. ? Another ? motivation ? to ? develop ? their ? own?
scoring ? function ? is ? to ?cope ?with ? the ?problem?of?
burstiness?(see?section?2.2.1).
2.2 Extraction?of?Phrases
This?section?describes?the?algorithm?of?Burek?and?
Gerdemann ? (2009) ? for ? extracting ? distinctive?
phrases?and?how?we?have?modified?and?used?it?in?
the?context ?of ?our?work. ?We?first ?show?how?the?
phrases?are?ranked,?so?that?one?knows?what?phrases?
are?more?or?less?distinctive?than?others.
2.2.1 The?Scoring?Algorithm
The ? extracted ? phrases ? are ? represented ? by?
occurrence ? vectors. ? These ? vectors ? have ? two?
elements???one?for?the?number?of?documents?with?
category ?positive ? polarity, ? and ? another ? for ? the?
negative?polarity.?Each?element?of?the?vector?stores?
the ?number ? of ? distinct ? documents, ? in ?which ? the?
phrase?occurs.?For?example,?if?a?phrase?occurs?in?
10 ? positive ? reviews, ? and ? 1 ? negative ? review, ? the?
occurrence?vector?of?this?phrase?is?<10,?1>?.?This?
shows?that?for?the?representation?of?the?phrases?we?
take? into?account? the?document?frequency?of? the?
phrase,?and?not?its?term?frequency.?The?motivation?
behind?this?choice?is?to?cope?with?the?problem?of?
burstiness?of?terms.?Madsen?et?al.?(2005)?explain?
burstiness ? in ? the ? following ? way: ?The ? term?
burstiness ?(Church?and?Gale, ?1995;?Katz, ?1996) ?
describes?the?behavior?of?a?rare?word?appearing?
many?times?in?a?single?document.?Because?of?the ?
large ?number ?of ?possible ?words, ?most ?words ?do?
not ?appear ? in ?a ?given ?document. ?However, ? if ?a ?
112
word?does?appear?once,?it?is?much?more?likely?to ?
appear?again,?i.e.?words?appear?in?bursts.
? ?We ?assign ? a ? score ? to ? a ? phrase ? by ?giving ? the?
phrase ? one ? point, ? if ? the ? phrase ? occurs ? in ? a?
document?with?positive?polarity?and?zero?points,?if?
it?occurs?in?a?document?with?negative?polarity.
? ? ?Let?us?take?again?the?occurrence?vector?of?<10,?
1>?.?According?to?the?way?the?points?are?given,?the?
vector?will?be?assigned?a?score?of?10?((1?point?*?
10)? ?+?(0?points?*?1)?=?10).?Is?this?a?good?score,?
which ? indicates ? that ? the ?phrase ? is ?distinctive ? for?
documents?of?category?positive?polarity??We?can?
answer ? this ? question, ? if ? we ? randomly ? choose?
another?phrase,?which?occurs?in?11?documents,?and?
see?what?the?probability?is,?that?this?phrase?would?
have?a?score,?which?is?higher?than?or?equally?high?
to?the?score?of?the?phrase?in?question?(Burek?and?
Gerdemann, ? 2009). ? In ? order ? to ? calculate ? this?
probability, ? the ? scoring ? method ? performs ? a?
simulation, ? in ? which ? occurrence ? vectors ? for?
randomly?chosen?phrases?are?created.?Let?us?pick?
randomly?one?phrase,?which?hypothetically?occurs?
in?11?reviews.?Let?also,?have?a?data?of?600?positive?
reviews?and?600?negative?reviews.?The?probability?
then, ? that ? the ? random?phrase ?would ?occur ? ? in ?a?
positive?or?a?negative?review?is?0.5.?Based?on?these?
probabilities, ? the ? simulation ? process ? constructs?
random?vectors?for?the?random?phrase,?indicating?
whether ? the ? phrase ? occurs ? in ? a ? positive ? or ? in?
negative ? review. ? For ? example, ? if ? in ? a ? particular?
run, ? the ? simulation ? says ? that ? the ? random ?phrase?
occurs?in?a?positive?review,?then?we?have?a?random?
vector?of?<1,?0>.?Otherwise,?<0,?1>?for?a?negative?
review. ?The?program?calculates ?as ?many?random?
vectors ? as ? the ?number ?of ? reviews, ? in ?which ? the?
random?phrase?is?said?to?occur.?In?this?example,?the?
number?of?documents?is?11.?Therefore,?11?random?
vectors ?will ?be ?constructed. ?They?may? look? like?
this:?<1,?0>,?<1,?0>,?<0,?1>?,?<1,?0>,?<1,?0>,?<0,?
1>,?<0,?1>,?<0,?1>,?<1,?0>,?<1,?0>,?<0,?1>.?These?
vectors?are?then?summed?up,?and?the?result?vector?
<6, ? 5> ? is ? the ? random? occurrence ?vector ? for ? the?
random ? phrase. ? It ? tells ? us ? that ? the ? phrase,?
hypothetically, ? occurs ? in ? 6 ? positive ? and ? in ? 5?
negative?reviews.?The?score?for?the?random?phrase?
is?now?calculated?in?the?same?way?as?for?the?non?
random ? phrases: ? 1 ? point ? is ? given ? for ? each?
occurrence?of?the?phrase?in?a?positive?review,?and?0?
points?otherwise.?So,?the?score?for?this?phrase?is?6?(?
((1?point?*?6)??+?(0?points?*?5)?=?6)?).?This?process?
is ?performed?a ?certain ?number ?of ? times. ?For ? the?
experiments?presented? in?section?3.2, ?we?run?the?
simulation?10,000?times?for?each?extracted?phrase.?
This?means?that?10,000?random?vectors?per?phrase?
are?created.
? ?The ? last ? step ? is ? to ? compare ? the ? scores ?of ? the?
random?phrase?with?the?score?of?the?actual?phrase,?
and?to?see?how?many?of?the?10,000?random?vectors?
give ? a ? score ? higher ? than ?or ? equally ? high ? to ? the?
score?of?the?actual?phrase.?If?the?number?of?random?
vectors,?which?give?a?higher?than?or?equally?high?
score ? to ? the ? actual ? phrase, ? is ? bigger ? than ? the?
number ? of ? random ? vectors, ? which ? give ? a ? score?
lower?than?the?actual?phrase,?then?the?actual?phrase?
is?assigned?a?positive?score,?and?the?value?of?this?
score ? is ? the ? approximate ? number ? of ? random?
vectors, ? from?which ?higher ? than?or ?equally ?high?
scores?to?the?actual?phrase?score?are?calculated.?If?
the ?number ? is ? lower, ? the ?phrase ? is ? assigned ? the?
approximate ? number ? of ? random ? vectors, ? from?
which?lower?scores?than?the?actual?phrase?score?are?
calculated, ? and ? a ? minus ? sign ? is ? attached ? to ? the?
number,?making?the?score?negative.
2.2.2 The?Phrase?Extraction?Algorithm
The?main?idea?of?the?algorithm?is?that?if?a?phrase?is?
distinctive ? for ? a ? particular ? category, ? it ? does ? not?
matter?how?long?the?phrase?is???as?long?as?it?helps?
for ? distinguishing ? one ? type ? of ? document ? from?
another,?it?should?be?extracted. ?In?order?to?extract?
phrases ? in ? this ? way, ? the ? whole ? collection ? of?
documents?is?represented?as?one?long?string.?Each?
phrase?is?then?a?substring?of?this?string.?It?will?be?
very?expensive?to?compute?statistics?(i.e.?tf?and?df)?
and?to?run? the?simulation?process?(see?2.2.1) ?for?
each?substring?in?the?text. ?The?reason?is?that ?the?
amount?of?substrings?might?be?huge???there?are?a?
total ? of ? N(N ? + ? 1) ? / ? 2 ? substrings ? in ? a ? corpus?
(Yamamoto ? and ? Church, ? 2001). ? Yamamoto ? and?
Church ? (2001) ? show ? how ? this ? problem ? can ? be?
overcome ? by ? grouping ? the ? substrings ? into?
equivalence?classes?and?performing?operations?(i.e.?
computing?statistics)?on?these?classes?instead?of?on?
the?individual?elements?of?the?classes.?They?use?for?
this?the?suffix?array?data?structure.?The?number?of?
the?classes?is?at?most?2N???1.
113
2.2.3 Maximal?Occurrence?of?a?Phrase
The ? suffix ? array ? data ? structure ? allows ? for ? easy?
manipulation?of?the?strings.?The?algorithm?extracts?
phrases ? if ? they? repeat ? in ? text, ?and?if ? the ?phrases?
occur?maximally?at ? least ?once? in ? the? text. ? If ? the?
phrase?do?not?occur?maximally?at?least?one?time,?
then? it ?may?not ?be?a ?good?linguistic?unit, ?which?
could? stand?on? its ?own. ?Example?of ?such ?words?
might ? be ? the ? different ? parts ? of ? certain ? named?
entities. ? For ? instance, ? the ? name ?Bugs ? Bunny. ? If?
Bugs?or?Bunny?never?appear?apart?from?each?other?
in? the ? text, ? then? this ? imply? that ? they?comprise?a?
single ? entity ? and ? they ? should ? always ? appear?
together?in?the?text.?In?this?case?it?does?not?make?
sense, ? for ? example, ? to ? count ? only ?Bugs? or ? only?
Bunny? and ? calculate ? statistics ? (e.g. ? tf ? or ? df) ? for?
each?of?them.?They?should?be?grouped?instead?into?
a?class.???
? Burek ? and ? Gerdemann ? (2009) ? mention ? three?
different ? types ? of ? occurrences ? of ? a ? phrase: ? left?
maximal, ? right ? maximal, ? and ? maximal. ? A ? left?
maximal?occurrence?of?a?phrase?S[i,j]?means?that?
the ? longer ?phrase ?S[i?1,j] ?does ?not ? repeat ? in ? the?
corpus ? (Burek ? and ? Gerdemann, ? 2009). ? For?
example, ? in ? the ? sentences ? below, ? the ? phrase?
recommend? is?not?left?maximal,?because?it?can?be?
extended?to?the?left?with?the?word?highly:
I?highly?recommend?the?book.?
You?highly?recommend?this?camera.
? ??On?the?other?hand?the?phrase?highly?recommend?
is?left?maximal.?
? ?In?a?similar ?way?we?define? the?notion?of?right?
maximal ? occurrence ? of ? a ? phrase. ? A ? maximal?
occurrence?of?a?phrase?is?when?the?occurrence?of?
the?phrase?is?both?left?maximal?and?right?maximal?
(Burek?and?Gerdemann,?2009).?The?phrase?highly ?
recommend? in?the?example?sentences?above?is?in?
this?sense?maximal.
??It?is?not?clear?a?priori?which?of?these?types?should?
be?taken?into?account?for?the?successful?realization?
of?a?given?application.?One?could?consider?only?the?
left ?maximal, ?only ? the ? ? right ?maximal, ?only ? the?
maximal ? occurrences ? of ? the ? phrases, ? or ? all?
occurrences. ? We ? experimented ? with ?all?
occurrences. ? Our ? motivation ? is ? that ? using ? all?
phrases ?would ?give ?us ? a ? big ? enough ?number ?of?
distinctive?phrases?and?we?will?most?probably?not?
have?a?problem?with?data?sparseness.
3 Sentiment ? Classification ? of ? Product?
Reviews
For ? the ? experiments ? presented ?below?we ?used ? a?
supervised ? machine ? learning ? approach, ? and?
different ? sets ? of ? features. ? Reviews ? from ? two?
domains,?books?and?cameras?&?photos,?are?used?as?
training?and?testing?data.
3.1 Choosing ? Distinctive ? Phrases ? for?
Classification
Once ? the ? phrases ? with ? which ? we ? would ? like ? to?
represent?the?documents?are?extracted,?we?need?to?
consider?two?things?in?the?very?beginning.?On?the?
one ? hand, ? the ? phrases ? should ? be ? as ? much?
distinctive ? as ? possible. ?On ? the ?other ? hand, ? even?
though ? a ? phrase ? might ? occur ? predominantly ? in?
negative ? reviews, ? it ? occurs ? very ? often ? also ? in?
positive ?reviews? (once?or ?at ? least ? several ? times),?
and?vice?versa.?Should?we?consider?such?phrases??
If?yes,?what?would?be?the?least?acceptable?number?
of?occurrences?of?the?phrases?in?the?opposite?type?
of ? reviews? ? We ? might ? choose ? as ? distinctive?
phrases?those?which?occur?only?in?positive?or?only?
in?negative?reviews,?however,?these?phrases?will?be?
very?few,?and?we?might?have?the?problem?of?data?
sparseness.?On?the?other?hand,?using?all?extracted?
phrases?might?bring?a?lot?of?noise,?because?many?of?
the?phrases?will?not?be?very?good?characteristics?of?
the?data. ?We?experimented?with?several ?different?
subsets?of?the?set?of?all?extracted?phrases.?
? ? In ? order ? to ? decide ? what ? subsets ? of ? extracted?
phrases?to?use,?we?analyzed?the?set?of?all?extracted?
phrases ?paying ?attention? to ? their ?vectors ?and ? the?
scores,?trying?to?find?a?trade?off?between?the?two?
mentioned?considerations?above.
3.2 Experiments?
??SVM?is?used?as?a?machine?learning?algorithm?for?
the?experiments?(the?implementation?of?the?SVM?
package?LibSVM3?in?GATE4).
3Libsvm:?a?library?for?support?vector?machines,?2001.?software?
available?at?http://www.csie.ntu.edu.tw/?cjlin/libsvm.
4http://gate.ac.uk/
114
? ?For?each?experiment?we?first?divide?the?reviews?
of?each?domain?into?training?and?testing?data?with?
ratio?two?to?one.?From?this?training?data?we?extract?
the ? distinctive ? phrases, ? which ? are ? later ? used ? as?
features ? to ? the ? learning ?algorithm. ?As?evaluation?
method?we?apply?the?k?fold?cross?validation?test,?
with?k=10.?For?all?experiments?we?used?the?default?
tf?idf?weight?for?the?n?grams.?For?each?domain?we?
conduct?five?different?experiments,?each?time?using?
different ? subsets ? of ? distinctive ? phrases. ? ? All?
experiments?were?performed?with?GATE.
??For?each?domain?the?training?data?from?which?the?
phrases ? are ? extracted ? consists ? of ? about ? 665?
negative?and?665?positive?reviews.?The?testing?data?
consists?of?333?negative?and?333?positive?reviews.?
? ?It?is?interesting?to?notice?that?although?the?results?
of ? the ? experiments ? are ? different, ? they ? are ? very?
close?to?each?other,?regardless?of?the?big?difference?
in ? the ? number ? of ? phrases ? used ? as ? features.?
Therefore, ? we ? decided ? to ? experiment ? with ? all?
extracted?phrases.?It?turned?out?that?the?results?of?
that?experiment?are?the?best.?This?would?imply?that?
the ? bigger ? number ? of ? phrases ? is ? helpful ? and ? it?
compensates ? for ? the ?use ?of ? phrases ? that ? are ?not?
much?distinctive.
? ?The?results?of?all?experiments?for?domain ?books?
are ? summarized ? in ? Table ? 1. ? The ? best ? achieved?
results?of?81%?precision,?recall,?and?F?measure?are?
given ? in ? bold. ? The ? rightmost ? column ? gives ? the?
number?of?negative?(n.)?and?positive?(p.)? ?phrases?
used?in?each?experiment.
Experiment Reviews P R F?m Phrases?used
Exp1 Negative
Positive
Overall
0.77
0.80
0.78
0.80
0.77
0.78
0.79
0.78
0.78
1685?n.
1116?p.
Exp2 Negative
Positive
Overall
0.75
0.80
0.77
0.80
0.74
0.77
0.77
0.76
0.77
924?n.
568?p.
Exp3 Negative
Positive
Overall
0.76
0.78
0.77
0.78
0.76
0.77
0.77
0.77
0.77
349?n.?
178?p.
Exp4 Negative
Positive
Overall
0.77
0.79
0.78
0.79
0.77
0.78
0.78
0.78
0.78
10552?n.?
9084?p.
Exp5 Negative
Positive
Overall
0.80
0.81
0.81
0.81
0.80
0.81
0.80
0.80
0.81
All:
24107?n.?
21149?p.
Table?1:?Domain?books
? ? Table ? 2 ? summarizes ? the ? results ? for ? domain?
camera&photos, ?showing?the?best?results?of?86%?
precision,?recall,?and?F?measure?in?bold.
? ? Similar ? to ? the ? experiments ? with ? reviews ? of?
domain?books,?the?results?for?camera&photos?in?all?
five ? experiments ? are ? very ? close. ? Again ? the ? best?
results?are?obtained?when?all?extracted?distinctive?
phrases?are?considered.?
Experiment Reviews P R F?m Phrases?
used
Exp1 Negative
Positive
Overall
0.85
0.83
0.84
0.83
0.85
0.84
0.84
0.84
0.84
1746n.?1883?
p.
Exp2 Negative
Positive
Overall
0.84
0.81
0.83
0.81
0.85
0.83
0.82
0.83
0.83
1013n.?1053?
p.
Exp3 Negative
Positive
Overall
0.86
0.83
0.85
0.83
0.87
0.85
0.85
0.85
0.85
384?n.?
432?p.
Exp4 Negative
Positive
Overall
0.85
0.83
0.84
0.83
0.86
0.84
0.84
0.84
0.84
7572?n.?
9821?p.
Exp5 Negative
Positive
Overall
0.86
0.85
0.86
0.85
0.87
0.86
0.86
0.86
0.86
All:
16378?n.?
17951?p.
Table?2:?Domain?camera&photos.
In ?order ? to ?evaluate ?how?well ? the ? results ?of ? the?
experiments ? are ? we ? performed ? several ? more?
experiments, ? in?which?the?texts?were?represented?
with?unigrams?(1?grams) ?and?bigrams?(2?grams).?
Pang?and ?Lee ? (2008) ?note ? that: ?whether ?higher?
order?n?grams?are?useful?features?appears?to?be?a?
matter?of?some?debate.?For?example,?Pang?et?al. ?
(2002) ?report ? that ?unigrams?outperform?bigrams?
when ? classifying ? movie ? reviews ? by ? sentiment ?
polarity,?but?Dave?et?al.?(2003)?find?that?in?some?
settings, ? bigrams ? and ? trigrams ? yield ? better ?
product?review?polarity?classification.?Bekkerman?
and ?Allan ? (2004) ? review? the ? results ? of ? different?
experiments ? on ? text ? categorization ? in ? which ? n?
gram?approaches?were?used,?and?conclude?that?the?
use?of?bigrams?for?the?representation?of?texts?does?
not ? show ? general ? improvement ? (Burek ? and?
Gerdemann, ? 2009). ? It ? seems ? intuitive ? that ? when?
bigrams ? are ? used, ? we ? would ? have ? a ? better?
representation ? of ? the ? texts, ? because ? we ? would?
know?what?words?combine?with?what?other?words?
in? the ? texts. ?However, ? there ? is ?a ?data?sparseness?
problem.??
? ? It ? seems ? interesting ? to ? compare ? the ? results?
obtained ? by ? representing ? the ? texts ? as ? unigrams,?
bigrams, ? and ? distinctive ? (maximally ? occurring)?
phrases,?because?the?model?based?on?phrases?might?
use?both?unigrams?and?bigrams,?and?it?allows?also?
any ? other ? higher ? n?grams, ? that ? is, ? more ? context?
115
(and?semantics)?of?the?text?is?preserved.?
? ? Tables ? 3 ? and ? 4 ? present ? the ? results ? of ? the?
experiments?using?bag?of?tokens?(1?gram)?models,?
while?Tables?5?and?6?present?the?experiments?with?
the?2?gram?models.?GATE?was?used?as?a?working?
environment,?and?SVM?as?learning?algorithm.
Reviews Precision Recall F?measure
Negative? 0.77 0.82 0.79
Positive? 0.82 0.75 0.78
Overall 0.79 0.79 0.79
Table?3:?Domain?books,?1?gram.
Reviews Precision Recall F?measure
Negative 0.86 0.84 0.85
Positive 0.84 0.86 0.85
Overall 0.85 0.85 0.85
Table?4:?Domain?camera&photos,?1?gram.
Reviews Precision Recall F?measure
Negative 0.72 0.80 0.75
Positive 0.78 0.69 0.73
Overall 0.75 0.75 0.75
Table?5:?Domain?books,?2?gram.
Reviews Precision Recall F?measure
Negative 0.84 0.83 0.83
Positive 0.83 0.84 0.83
Overall 0.83 0.83 0.83
Table?6:?Domain?camera&photos,?2?gram.
Features Precision Recall F?measure
All?phrases 0.81 0.81 0.81
1?gram 0.79 0.79 0.79
2?gram 0.75 0.75 0.75
Table?7:?Comparison,?Domain?books.
Features Precision Recall F?measure
All?phrases 0.86 0.86 0.86
1?gram 0.85 0.85 0.85
2?gram 0.83 0.83 0.83
Table?8:?Comparison,?Domain?camera&photos.
Tables?7?and?8?summarize?the?overall?results?using?
1?gram?and?2?gram?models?and?a?model?based?on?
distinctive ? phrases ? for ? the ? representation ? of ? the?
texts. ? For ? both ? domains ? the ? best ? results ? are?
achieved ? with ? the ? model ? based ? on ? phrases ? (all?
phrases). ? For ? the ? domain ?books? the ? overall?
precision, ? recall ? and ? F?measure ? results ? achieved?
with ? that ? model ? (81%) ? are ? 2% ? higher ? than ? the?
results?obtained?using?the?1?gram?model,?and?6%?
higher?than?the?results?obtained?using?the?2?gram?
model. ? For ? domain ?cameras ? & ? photos, ? an?
improvement?of?1%?and?3%?is?achieved?with?the?
phrase?model?in?comparison?with?the?1?gram?and?
2?gram?models,?respectively.
4 Related?Work
Close?to?our?work?seems?to?be?Funk?et?al.?(2008).?
They?classify?product ?and?company?reviews? into?
one?of?the?1?star?to?5?star?categories.?The?features?
to? the ? learning?algorithm?(also?SVM)?are?simple?
linguistic ? features ? of ? single ? tokens. ? They ? report?
best ? results ? with ? the ? combinations ?root ? &?
orthography, ? and ?only ? root. ? Another ? interesting?
related?work?is?that?of?Turney?(2002).?He?uses?an?
unsupervised ? learning ? algorithm ? to ? classify ? a?
review?as?recommended?or?not?recommended.?The?
algorithm ? extracts ? phrases ? from ? a ?given ? review,?
and?determines?their?pointwise?mutual?information?
with?the?words ?excellent? and ?poor.?Turney?(2002)?
points ?out ? that ? the ?contexual ? information? is ?very?
often?necessary?for?the?correct?determination?of?the?
sentiment?polarity?of?a?certain?word.
5 Conclusion
This ? paper ? presented ? different ? experiments ? on?
classifying?product?reviews?of?domains ?books? and?
cameras ?& ?photos? under ? the ? categories ?positive?
polarity? and ?negative ? polarity? using ? distinctive?
(maximally ? occurring) ? phrases ? as ? features. ? For?
both?domains?best?results ?were?achieved?with?all?
extracted ? distinctive ? phrases ? as ? features. ? This?
approach?outperforms?slightly ? the?1?gram?and?2?
gram?experiments?on?this?data?and?shows?that?the?
use?of?phrases?occurring?maximally?in?text?could?
be ? successfully ? applied ? in ? the ? classification ? of?
sentiment?data?and?that?it?is?worth?experimenting?
with?classifying?sentiment?data?without?necessarily?
relying?on?general?predefined?sentiment?lexicons.
116
References
Ron?Bekkerman?and?James?Allan.?2004.?Using?bigrams?
in ? text ? categorization. ?Technical ? Report ? IR?408,?
Center ? of ? Intelligent ? Information ? Retrieval, ? UMass?
Amherst.?
John?Blitzer,?Mark?Dredze,?and?Fernando?Pereira.?2007.?
Biographies, ? bollywood, ? boomboxes ? and ? blenders:?
Domain ? adaptation ? for ? sentiment ? classification. ? In?
Proceedings ? of ? the ? 45th ? Annual ? Meeting ? of ? the ?
Association ? of ? Computational ? Linguistics, ? pp.?
440?447, ? Prague, ? Czech ? Republic. ? Association ? for?
Computational?Linguistics.
Gaston ? Burek ? and ? Dale ? Gerdemann. ? 2009. ? Maximal?
phrases ? based ? analysis ? for ? prototyping ? online?
discussion ? forums ? postings. ? In ?Proceedings ? of ? the?
workshop?on?Adaptation?of?Language?Resources?and ?
Technologies ? to ? New ? Domains ? (AdaptLRTtoND),?
Borovets,?Bulgaria.
Kenneth ? W. ? Church ? and ? William ? A. ? Gale. ? 1995.?
Poisson ? mixtures. ?Natural ? Language ? Engineering,?
1:163?190.
Kushal?Dave,?Steve?Lawrence?and?David?M.?Pennock.?
2003.?Mining?the?peanut?gallery: ?Opinion?extraction?
and ? semantic ? classification ? of ? product ? reviews. ? In?
Proceedings?of?WWW,?pp.?519?528.
Adam ? Funk, ? Yaoyong ? Li, ? Horacio ? Saggion, ? Kalina?
Bontcheva, ? and ? Christian ? Leibold. ? 2008. ? Opinion?
analysis ? for ? business ? intelligence ? applications. ? In?
Proceedings ? of ?First ? International ? Workshop ? on?
Ontology?supported?Business ? Intelligence ?(OBI2008)?
at ? the ?7th ? International ? Semantic ? Web ? Conference ?
(ISWC),?Karlsruhe,?Germany.
Slava?M.?Katz.?1996.??Distribution?of?content?words?and?
phrases ? in ? text ? and ? language ? modelling. ?Natural?
Language?Engineering,?2(1):15?59.
Rasmus?E.?Madsen,?David?Kauchak,?and?Charles?Elkan.?
2005. ? Modeling ? word ? burstiness ? sing ? the ? dirichlet?
distribution.?In?Proceedings?of?the?22nd?International ?
Conference?on?Machine?Learning,?pp.?545?552.
Bo?Pang?and ?Lillian ?Lee. ?2008. ?Opinion ?mining ?and?
sentiment ? analysis. ?Foundations ? and ? Trends ? in ?
Information?Retrieval,?Vol.?2,?Nos.?1?2?(2008)?1?135.
Bo?Pang,?Lillian?Lee.,?and?Shivakumar?Vaithyanathan.?
2002. ? Thumbs ? up? ? Sentiment ? classification ? using?
machine ? learning ? techniques. ? In ?Proceedings ?of ? the?
Conference ? on ? Empirical ? Methods ? in ? Natural ?
Language?Processing?(EMNLP),?pp.?79?86.
Peter ?D. ?Turney. ?2002.?Thumbs?up?or ? thumbs?down??
Semantic ? orientation ? applied ? to ? unsupervised?
classification?of?reviews.?In ?Proceedings?of?the?40th?
Annual ? Meeting ? on ? Association ? for ? Computational ?
Linguistics,?pp.?417?424.
Mikio?Yamamoto?and?Kenneth?W.?Church.?2001.?Using?
suffix ? arrays ? to ? compute ? term ? frequency ? and?
document?frequency?for?all?substrings?in?a?corpus.?In?
Computational??Linguistics,?27(1):1?30.
117
Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 1?9,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Evaluating Answers to Reading Comprehension Questions in Context:
Results for German and the Role of Information Structure
Detmar Meurers Ramon Ziai Niels Ott Janina Kopp
Seminar fu?r Sprachwissenschaft / SFB 833
Universita?t Tu?bingen
Wilhelmstra?e 19 / Nauklerstra?e 35
72074 Tu?bingen, Germany
{dm,rziai,nott,jkopp}@sfs.uni-tuebingen.de
Abstract
Reading comprehension activities are an au-
thentic task including a rich, language-based
context, which makes them an interesting real-
life challenge for research into automatic con-
tent analysis. For textual entailment research,
content assessment of reading comprehension
exercises provides an interesting opportunity
for extrinsic, real-purpose evaluation, which
also supports the integration of context and
task information into the analysis.
In this paper, we discuss the first results for
content assessment of reading comprehension
activities for German and present results which
are competitive with the current state of the
art for English. Diving deeper into the results,
we provide an analysis in terms of the differ-
ent question types and the ways in which the
information asked for is encoded in the text.
We then turn to analyzing the role of the ques-
tion and argue that the surface-based account
of information that is given in the question
should be replaced with a more sophisticated,
linguistically informed analysis of the informa-
tion structuring of the answer in the context of
the question that it is a response to.
1 Introduction
Reading comprehension exercises offer a real-life
challenge for the automatic analysis of meaning.
Given a text and a question, the content assessment
task is to determine whether the answer given to a
reading comprehension question actually answers
the question or not. Such reading comprehension
exercises are a common activity in foreign language
teaching, making it possible to use activities which
are authentic and for which the language teachers
provide the gold standard judgements.
Apart from the availability of authentic exercises
and independently motivated gold standard judge-
ments, there are two further reasons for putting read-
ing comprehension tasks into the spotlight for au-
tomatic meaning analysis. Firstly, such activities
include a text as an explicit context on the basis of
which the questions are asked. Secondly, answers to
reading comprehension questions in foreign language
teaching typically are between a couple of words and
several sentences in length ? too short to rely purely
on the distribution of lexical material (as, e.g., in
LSA, Landauer et al, 1998). The answers also ex-
hibit a significant variation in form, including a high
number of form errors, which makes it necessary to
develop an approach which is robust enough to de-
termine meaning correspondences in the presence of
errors yet flexible enough to support the rich vari-
ation in form which language offers for expressing
related meanings.
There is relatively little research on content assess-
ment for reading comprehension tasks and it so far
has focused exclusively on English, including both
reading comprehension questions answered by na-
tive speakers (Leacock and Chodorow, 2003; Nielsen
et al, 2009) and by language learners (Bailey and
Meurers, 2008). The task is related to the increas-
ingly popular strand of research on Recognizing Tex-
tual Entailment (RTE, Dagan et al, 2009) and the
Answer Validation Exercise (AVE, Rodrigo et al,
2009), which both have also generally targeted En-
glish.
1
The RTE challenge abstracts away from concrete
tasks to emphasize the generic semantic inference
component and it has significantly advanced the field
under this perspective. At the same time, an inves-
tigation of the role of the context under which an
inference holds requires concrete tasks, for which
content assessment of reading comprehension tasks
seems particularly well-suited. Borrowing the ter-
minology Spa?rck Jones (2007) coined in the context
of evaluating automatic summarization systems, one
can say that we pursue an extrinsic, full-purpose eval-
uation of aspects of textual inference. The content
assessment task provides two distinct opportunities
to investigate textual entailment: On the one hand,
one can conceptualize it as a textual inference task
of deciding whether a given text T supports a partic-
ular student answer H . On the other hand, if target
answers are provided by the teachers, the task can be
seen as a special bi-directional case of textual entail-
ment, namely a paraphrase recognition task compar-
ing the student answers to the teacher target answers.
In this paper, we focus on this second approach.
The aim of this paper is twofold. On the one hand,
we want to present the first content assessment ap-
proach for reading comprehension activities focusing
on German. In the discussion of the results, we will
highlight the impact of the question types and the
way in which the information asked for is encoded
in the text. On the other hand, we want to discuss
the importance of the explicit language-based context
and how an analysis of the question and the way a
text encodes the information being asked for can help
advance research on automatic content assessment.
Overall, the paper can be understood as a step in the
long-term agenda of exploring the role and impact
of the task and the context on the automatic analysis
and interpretation of natural language.
2 Data
The experiments described in this paper are based
on the Corpus of Reading comprehension Exercises
in German (CREG), which is being collected in col-
laboration with two large German programs in the
US, at Kansas University (Prof. Nina Vyatkina) and
at The Ohio State University (Prof. Kathryn Corl).
German teachers are using the WEb-based Learner
COrpus MachinE (WELCOME, Meurers et al, 2010)
interface to enter the regular, authentic reading com-
prehension exercises used in class, which are thereby
submitted to a central corpus repository. These exer-
cises consist of texts, questions, target answers, and
corresponding student answers. Each student answer
is transcribed from the hand-written submission by
two independent annotators. These two annotators
then assess the contents of the answers with respect
to meaning: Did the student provide a meaningful
answer to the question? In this binary content as-
sessment one thus distinguishes answers which are
appropriate from those which are inappropriate in
terms of meaning, independent of whether the an-
swers are grammatically well-formed or not.
From the collected data, we selected an even dis-
tribution of unique appropriate and inappropriate stu-
dent answers in order to obtain a 50% random base-
line for our system. Table 1 lists how many questions,
target answers and student answers each of the two
data sets contains. The data used for this paper is
made freely available upon request under a standard
Creative Commons by-nc-sa licence.1
KU data set OSU data set
Target Answers 136 87
Questions 117 60
Student Answers 610 422
# of Students 141 175
avg. Token # 9.71 15.00
Table 1: The reading comprehension data sets used
3 Approach
Our work builds on the English content assessment
approach of Bailey and Meurers (2008), who pro-
pose a Content Assessment Module (CAM) which
automatically compares student answers to target re-
sponses specified by foreign language teachers. As a
first step we reimplemented this approach for English
in a system we called CoMiC (Comparing Mean-
ing in Context) which is discussed in Meurers et al
(2011). This reimplementation was then adapted
for German, resulting in the CoMiC-DE system pre-
sented in this paper.
The comparison of student answers and target an-
swer is based on an alignment of tokens, chunks, and
1http://creativecommons.org/licenses/by-nc-sa/3.0/
2
dependency triples between the student and the target
answer at different levels of abstraction. Figure 1
shows a simple example including token-level and
chunk-level alignments between the target answer
(TA) and the student answer (SA).
Figure 1: Basic example for alignment approach
As the example suggests, it is not sufficient to align
only identical surface forms given that significant lex-
ical and syntactic variation occurs in typical student
answers. Alignment thus is supported at different
levels of abstraction. For example, the token units
are enriched with lemma and synonym information
using standard NLP tools. Table 2 gives an overview
of which NLP tools we use for which task in CoMiC-
DE. In general, the components are very similar to
those used in the English system, with different sta-
tistical models and parameters where necessary.
Annotation Task NLP Component
Sentence Detection OpenNLP
http://incubator.apache.org/opennlp
Tokenization OpenNLP
Lemmatization TreeTagger (Schmid, 1994)
Spell Checking Edit distance (Levenshtein, 1966),
igerman98 word list
http://www.j3e.de/ispell/igerman98
Part-of-speech Tagging TreeTagger (Schmid, 1994)
Noun Phrase Chunking OpenNLP
Lexical Relations GermaNet (Hamp and Feldweg, 1997)
Similarity Scores PMI-IR (Turney, 2001)
Dependency Relations MaltParser (Nivre et al, 2007)
Table 2: NLP tools used in the German system
Integrating the multitude of units and their rep-
resentations at different levels of abstraction poses
significant challenges to the system architecture.
Among other requirements, different representations
of the same surface string need to be stored without
interfering with each other, and various NLP tools
need to collaborate in order to produce the final rich
data structures used for answer comparison. To meet
these requirements, we chose to implement our sys-
tem in the Unstructured Information Management
Architecture (UIMA, cf. Ferrucci and Lally, 2004).
UIMA allows automatic analysis modules to access
layers of stand-off annotation, and hence allows for
the coexistence of both independent and interdepen-
dent annotations, unlike traditional pipeline-style ar-
chitectures, where the output of each component re-
places its input. The use of UIMA in recent success-
ful large-scale projects such as DeepQA (Ferrucci
et al, 2010) confirms that UIMA is a good candi-
date for complex language processing tasks where
integration of various representations is required.
In order to determine the global alignment con-
figuration, all local alignment options are computed
for every mappable unit. These local candidates are
then used as input for the Traditional Marriage Al-
gorithm (Gale and Shapley, 1962) which computes a
global alignment solution where each mappable unit
is aligned to at most one unit in the other response,
such as the one we saw in Figure 1.
On the basis of the resulting global alignment con-
figuration, the system performs the binary content
assessment by evaluating whether the meaning of the
learner and the target answer are sufficiently similar.
For this purpose, it extracts features which encode
the numbers and types of alignment and feeds them
to the memory-based classifier TiMBL (Daelemans
et al, 2007). The features used are listed in Table 3.
Features Description
1. Keyword Overlap Percent of keywords aligned
(relative to target)
2./3. Token Overlap Percent of aligned target/learner tokens
4./5. Chunk Overlap Percent of aligned target/learner chunks
6./7. Triple Overlap Percent of aligned target/learner triples
8. Token Match Percent of token alignments
that were token-identical
9. Similarity Match Percent of token alignments
that were similarity-resolved
10. Type Match Percent of token alignments
that were type-resolved
11. Lemma Match Percent of token alignments
that were lemma-resolved
12. Synonym Match Percent of token alignments
that were synonym-resolved
13. Variety of Match Number of kinds of token-level
(0-5) alignments
Table 3: Features used for the memory-based classifier
3
4 Content Assessment Experiment
4.1 Setup
We ran our content assessment experiment using
the two data sets introduced in section 2, one from
Kansas University and the other from The Ohio State
University. Both of these contain only records where
both annotators agreed on the binary assessment (ap-
propriate/inappropriate meaning). Each set is bal-
anced, i.e., they contain the same number of appro-
priate and inappropriate student answers.
In training and testing the TiMBL-based classi-
fier, we followed the methodology of Bailey (2008,
p. 240), where seven classifiers are trained using the
different available distance metrics (Overlap, Leven-
shtein, Numeric Overlap, Modified value difference,
Jeffrey divergence, Dot product, Cosine). Training
and testing was performed using the leave-one-out
scheme (Weiss and Kulikowski, 1991) and for each
item the output of the seven classifiers was combined
via majority voting.
4.2 Results
The classification accuracy for both data sets is sum-
marized in Table 4. We report accuracy and the total
number of answers for each data set.
KU data set OSU data set
# of answers 610 422
Accuracy 84.6% 84.6%
Table 4: Classification accuracy for the two data sets
The 84.6% accuracy figure obtained for both data
sets shows that CoMiC-DE is quite successful in
performing content assessment for the German data
collected so far, a result which is competitive with
the one for English obtained by Bailey and Meurers
(2008), who report an accuracy of 78% for the binary
assessment task on a balanced English data set.
A remarkable feature is the identity of the scores
for the two data sets, considering that the data was
collected at different universities from different stu-
dents in different classes run by different teachers.
Moreover, there was no overlap in exercise material
between the two data sets. This indicates that there
is some characteristic uniformity of the learner re-
sponses in authentic reading comprehension tasks,
suggesting that the course setting and task type effec-
tively constrains the degree of syntactic and lexical
variation in the student answers. This includes the
stage of the learners in this foreign language teaching
setting, which limits their exposure to linguistic con-
structions, as well as the presence of explicit reading
texts that the questions are about, which may lead
learners to use the lexical material provided instead
of rephrasing content in other words. We intend to ex-
plore these issues in our future work to obtain a more
explicit picture of the contextual and task properties
involved.
Another aspect which should be kept in mind is
that the scores we obtained are based on a data set
for which the two human annotators had agreed on
their assessment. We expect automatic classification
results to degrade given more controversial data about
which human annotators disagree, especially since
such data will presumably contain more ambiguous
cues, giving rise to multiple interpretations.
4.3 Evaluation by question type
The overall results include many different question
types which pose different kinds of challenges to
our system. To develop an understanding of those
challenges, we performed a more fine-grained evalu-
ation by question types. To distinguish relevant sub-
cases, we applied the question classification scheme
introduced by Day and Park (2005). This scheme is
more suitable here than other common answer-typing
schemata such as the one in Li and Roth (2002),
which tend to focus on questions asking for factual
knowledge.
Day and Park (2005) distinguish five different
question forms: yes/no (question to be answered
with either yes or no), alternative (two or more
yes/no questions connected with or), true or false
(a statement to be classified as true or false),
who/what/when/where/how/why (wh-question con-
taining the respective question word), and multiple
choice (choice between several answers presented
with a question, of any other question type). In addi-
tion, they introduce a second dimension distinguish-
ing the types of comprehension involved, i.e., how
the information asked for by the question can be ob-
tained from the text: literal (questions that can be an-
swered directly and explicitly from the text), reorga-
nization (questions where information from various
4
parts of the text must be combined), inference (ques-
tions where literal information and world knowledge
must be combined), prediction (prediction of how
a story might continue), evaluation (comprehensive
judgement about aspects of the text) and personal
response (personal opinion or feelings about the text
or the subject).
Out of the five different forms of question, our
data contains questions of all forms except for the
multiple choice category and the true or false cate-
gory given that we are explicitly targeting free text
responses. To obtain a more detailed picture of the
wh-question category, we decided to split that cat-
egory into its respective wh-words and added one
more category to it, for which. Also, we added the
type ?several? for questions which contain more than
one question presented to the student at a time. Of the
six comprehension types, our data contained literal,
reorganization and inference questions.
Table 5 reports the accuracy results by question
forms and comprehension types for the combined
OSU and KU data set. The counts encode the num-
ber of student answers for which accuracy is reported
(micro-averages). The numbers in brackets specify
the number of distinct questions and the correspond-
ing accuracy measures are computed by grouping
answers by their question (macro-averages). Com-
paring answer-based (micro-average) accuracy with
question-based (macro-average) accuracy allows us
to see whether the results for questions with a high
number of answers outweigh questions with a small
number of answers. In general the micro- and macro-
averages reported are very similar and the overall
accuracy is the same (84.6%). Overall, the results
thus do not seem to be biased towards a specific, fre-
quently answered question instance. Where larger
differences between micro- and macro-averages do
arise, as for alternative, when, and where questions,
these are cases with few overall instances in the data
set, cautioning us against overinterpreting results for
such small subsets. The 4.2% gap for the relatively
frequent ?several? question type underlines the het-
erogeneous nature of this class, which may warrant
more specific subclasses in the future.
Overall, the accuracy of content assessment for
wh-questions that can be answered with a concrete
piece of information from the text are highest, with
92.6% for ?which? questions, and results in the upper
80s for five other wh-questions. Interestingly, ?who?
questions fare comparatively badly, pointing to a rel-
atively high variability in the expression of subjects,
which would warrant the integration of a dedicated
approach to coreference resolution. Such a direct so-
lution is not available for ?why? questions, which at
79.3% is the worst wh-question type. The high vari-
ability of those answers is rooted in the fact that they
ask for a cause or reason, which can be expressed in
a multitude of ways, especially for comprehension
types involving inferences or reorganization of the
information given in the text.
This drop between comprehension types, from lit-
eral (86.0%) to inference (81.5%) and reorganization
(78.0%), can also be observed throughout and is ex-
pected given that the CoMiC-DE system makes use
of surface-based alignments where it can find them.
For the system to improve on the non-literal com-
prehension types, features encoding a richer set of
abstractions (e.g., to capture distributional similarity
at the chunk level or global linguistic phenomena
such as negation) need to be introduced.
Just as in the discussion of the micro- and macro-
averages above, the ?several? question type again
rears its ugly heads in terms of a low overall accuracy
(77.7%). This supports the conclusion that it requires
a dedicated approach. Based on an analysis of the
nature and sequence of the component questions, in
future work we plan to determine how such combi-
nations constrain the space of variation in acceptable
answers.
Finally, while there are few instances for the ?al-
ternative? question type, the fact that it resulted in
the lowest accuracy (57.1%) warrants some attention.
The analysis indeed revealed a general issue, which
is discussed in the next section.
5 From eliminating repeated elements to
analyzing information structure
Bailey (2008, sec. 5.3.12) observed that answers fre-
quently repeat words given in the question. In her cor-
pus example (1), the first answer repeats ?the moral
question raised by the Clinton incident? from the
question, whereas the second one reformulates this
given material. But both sentences essentially answer
the question in the same way.2
2Independent of the issue discussed here, note the presuppo-
5
Comprehension type
Literal Reorganization Inference Total
Question type Acc. # Acc. # Acc. # Acc. #
Alternative 0 1 (1) ? 0 66.7 (58.3) 6 (3) 57.1 (43.8) 7 (4)
How 85.7 (83.3) 126 (25) 83.3 (77.8) 12 (3) 100 7 (1) 86.2 (83.3) 145 (29)
What 87.0 (87.6) 247 (40) 74.2 (71.7) 31 (4) 83.3 (83.3) 6 (1) 85.6 (86.1) 284 (45)
When 85.7 (93.3) 7 (3) ? 0 ? 0 85.7 (93.3) 7 (3)
Where 88.9 (94.4) 9 (3) ? 0 ? 0 88.9 (94.4) 9 (3)
Which 92.3 (90.7) 183 (29) 100.0 14 (5) 83.3 (83.3) 6 (2) 92.6 (91.6) 203 (36)
Who 73.9 (80.2) 23 (9) 94.4 (88.9) 18 (3) ? 0 82.9 (82.4) 41 (12)
Why 80.5 (83.3) 128 (17) 57.1 (57.9) 14 (3) 84.4 (81.1) 32 (4) 79.3 (79.7) 174 (24)
Yes/No ? 0 100.0 5 (1) ? 0 100.0 5 (1)
Several 82.1 (85.6) 95 (13) 68.4 (75.1) 38 (5) 75 (74.3) 24 (2) 77.7 (81.9) 157 (20)
Total 86.0 (86) 819 (140) 78.0 (80.7) 132 (24) 81.5 (76.8) 81 (13) 84.6 (84.6) 1032 (177)
Table 5: Accuracy by question form and comprehension types following Day and Park (2005). Counts denoting number
of student answers, in brackets: number of questions and macro-average accuracy computed by grouping by questions.
(1) What was the major moral question raised by
the Clinton incident?
a. The moral question raised by the Clinton
incident was whether a politician?s person
life is relevant to their job performance.
b. A basic question for the media is whether
a politician?s personal life is relevant to his
or her performance in the job.
The issue arising from the occurrence of such
given material for a content assessment approach
based on alignment is that all alignments are counted,
yet those for given material do not actually con-
tribute to answering the question, as illustrated by
the (non)answer containing only given material ?The
moral question raised by the Clinton incident was
whatever.? Bailey (2008) concludes that an answer
should not be rewarded (or punished) for repeating
material that is given in the question and her imple-
mentation thus removes all words from the answers
which are given in the question.
While such an approach successfully eliminates
any contribution from these given words, it has the un-
fortunate consequence that any NLP processes requir-
ing well-formed complete sentences (such as, e.g.,
dependency parsers) perform poorly on sentences
from which the given words have been removed. In
our reimplementation of the approach, we therefore
kept the sentences as such intact and instead made
sition failure arising for this authentic reading comprehension
question ? as far as we see, there was no ?major moral question
raised by the Clinton incident?.
use of the UIMA architecture to add a givenness
annotation to those words of the answer which are
repeated from the question. Such given tokens and
any representations derived from them are ignored
when the local alignment possibilities are computed.
While successfully replicating the givenness filter
of Bailey (2008) without the negative consequences
on other NLP analysis, targeting given words in this
way is problematic, which becomes particularly ap-
parent when considering examples for the ?alterna-
tive? question type. In this question type, exemplified
in Figure 2 by an example from the KU data set, the
answer has to select one of the options from an ex-
plicitly given set of alternatives.
Q: Ist die Wohnung in einem Neubau oder einem Altbau?
?Is the flat in a new building or in an old building??
TA: Die
The
Wohnung
flat
ist
is
in
in
einem
a
Neubau
new building
.
SA: Die
The
Wohnung
flat
ist
is
in
in
einem
a
Neubau
new building
Figure 2: ?Alternative? question with answers consisting
entirely of given words, resulting in no alignments.
The question asks whether the apartment is in a
new or in an old building, and both alternatives are
explicitly given in the question. The student picked
the same alternative as the one that was selected in
the target answer. Indeed, the two answers are iden-
tical, but the givenness filter excludes all material
from alignment and hence the content assessment
6
classification fails to identify the student answer as
appropriate. This clearly is incorrect and essentially
constitutes an opportunity to rethink the givenness
filter.
The givenness filter is based on a characterization
of the material we want to ignore, which was moti-
vated by the fact that it is easy to identify the material
that is repeated from the question. On the other hand,
if we analyze the reading comprehension questions
more closely, it becomes possible to connect this
issue to research in formal pragmatics which inves-
tigates the information structure (cf. Krifka, 2007)
imposed on a sentence in a discourse addressing
an explicit (or implicit) question under discussion
(Roberts, 1996). Instead of removing given elements
from an answer, under this perspective we want to
identify which part of an answer constitutes the so-
called focus answering the question.3
The advantage of linking our issue to the more
general investigation of information structure in lin-
guistics is readily apparent if we consider the signif-
icant complexity involved (cf., e.g., Bu?ring, 2007).
The issue of asking what constitutes the focus of a
sentence is distinct from asking what new informa-
tion is included in a sentence. New information can
be contained in the topic of a sentence. On the other
hand, the focus can also contain given information.
In (2a), for example, the focus of the answer is ?a
green apple?, even though apples are explicitly given
in the question and only the fact that a green one will
be bought is new.
(2) You?ve looked at the apples long enough now,
what do you want to buy?
a. I want to buy a green apple.
In some situations the focus can even consist en-
tirely of given information. This is one way of in-
terpreting what goes on in the case of the alternative
questions discussed at the end of the last section.
This question type explicitly mentions all alternatives
as part of the question, so that the focus of the an-
swer selecting one of those alternatives will typically
3The information structure literature naturally also provides
a more sophisticated account of givenness. For example, for
Schwarzschild (1999), givenness also occurs between hypernyms
and coreferent expressions, which would not be detected by the
simple surface-based givenness filter included in the current
CoMiC-DE.
consist entirely of given information.
As a next step we plan to build on the notion of
focus characterized in (a coherent subset of) the infor-
mation structure literature by developing an approach
which identifies the part of an answer which consti-
tutes the focus so that we can limit the alignment
procedure on which content assessment is based to
the focus of each answer.
6 Related Work
There are few systems targeting the short answer eval-
uation tasks. Most prominent among them is C-Rater
(Leacock and Chodorow, 2003), a short answer scor-
ing system for English meant for deployment in Intel-
ligent Tutoring Systems (ITS). The authors highlight
the fact that C-Rater is not simply a string matching
program but instead uses more sophisticated NLP
such as shallow parsing and synonym matching. C-
Rater reportedly achieved an accuracy of 84% in two
different studies, which is remarkably similar to the
scores we report in this paper although clearly the
setting and target language differ from ours.
More recently in the ITS field, Nielsen et al (2009)
developed an approach focusing on recognizing tex-
tual entailment in student answers. To that end, a
corpus of questions and answers was manually an-
notated with word-word relations, so-called ?facets?,
which represent individual semantic propositions in a
particular answer. By learning how to recognize and
classify these facets in student answers, the system
is then able to give a more differentiated rating of
a student answer than ?right? or ?wrong?. We find
that this is a promising move in the fields of answer
scoring and textual entailment since it also breaks
down the complex entailment problem into a set of
sub-problems.
7 Conclusion
We presented CoMiC-DE, the first content assess-
ment system for German. For the data used in evalu-
ation so far, CoMiC-DE performs on a competitive
level when compared to previous work on English,
with accuracy at 84.6%. In addition to these results,
we make our reading comprehension corpus freely
available for research purposes in order to encourage
more work on content assessment and related areas.
In a more detailed evaluation by question and com-
7
prehension type, we gained new insights into how
question types influence the content assessment tasks.
Specifically, our system had more difficulty classify-
ing answers to ?why?-questions than other question
forms, which we attribute to the fact that causal re-
lations exhibit more form variation than other types
of answer material. Also, the comprehension type
?reorganization?, which requires the reader to collect
and combine information from different places in the
text, posed more problems to our system than the
?literal? type.
Related to the properties of questions, we showed
by example that simply marking given material on
a surface level is insufficient and a partitioning into
focused and background material is needed instead.
This is especially relevant for alternative questions,
where the exclusion of all given material renders the
alignment process useless. Future work will therefore
include focus detection in answers and its use in the
alignment process. For example, given a weighting
scheme for individual alignments, focused material
could be weighted more prominently in alignment in
order to reflect its importance in assessing the answer.
Acknowledgements
We would like to thank two anonymous TextInfer
reviewers for their helpful comments.
References
Stacey Bailey, 2008. Content Assessment in Intelli-
gent Computer-Aided Language Learning: Mean-
ing Error Diagnosis for English as a Second Lan-
guage. Ph.D. thesis, The Ohio State University.
http://osu.worldcat.org/oclc/243467551.
Stacey Bailey and Detmar Meurers, 2008. Diagnos-
ing Meaning Errors in Short Answers to Read-
ing Comprehension Questions. In Proceedings
of the 3rd Workshop on Innovative Use of NLP
for Building Educational Applications (BEA-3)
at ACL?08. Columbus, Ohio, pp. 107?115. http:
//aclweb.org/anthology/W08-0913.
Daniel Bu?ring, 2007. Intonation, Semantics and In-
formation Structure. In Gillian Ramchand and
Charles Reiss (eds.), The Oxford Handbook of Lin-
guistic Interfaces, Oxford University Press.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot
and Antal van den Bosch, 2007. TiMBL: Tilburg
Memory-Based Learner Reference Guide, ILK
Technical Report ILK 07-03. Version 6.0. Tilburg
University.
Ido Dagan, Bill Dolan, Bernardo Magnini and Dan
Roth, 2009. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Natural Language
Engineering, 15(4):i?xvii.
Richard R. Day and Jeong-Suk Park, 2005. Develop-
ing Reading Comprehension Questions. Reading
in a Foreign Language, 17(1):60?73.
David Ferrucci, Eric Brown et al, 2010. Building
Watson: An Overview of the DeepQA Project. AI
Magazine, 31(3):59?79.
David Ferrucci and Adam Lally, 2004. UIMA: An
Architectural Approach to Unstructured Informa-
tion Processing in the Corporate Research Envi-
ronment. Natural Language Engineering, 10(3?
4):327?348.
David Gale and Lloyd S. Shapley, 1962. College Ad-
missions and the Stability of Marriage. American
Mathematical Monthly, 69:9?15.
Birgit Hamp and Helmut Feldweg, 1997. GermaNet
? a Lexical-Semantic Net for German. In Pro-
ceedings of ACL workshop Automatic Informa-
tion Extraction and Building of Lexical Semantic
Resources for NLP Applications. Madrid. http:
//aclweb.org/anthology/W97-0802.
Manfred Krifka, 2007. Basic Notions of Information
Structure. In Caroline Fery, Gisbert Fanselow and
Manfred Krifka (eds.), The Notions of Information
Structure, Universita?tsverlag Potsdam, Potsdam,
volume 6 of Interdisciplinary Studies on Informa-
tion Structure (ISIS).
Thomas Landauer, Peter Foltz and Darrell Laham,
1998. An Introduction to Latent Semantic Analysis.
Discourse Processes, 25:259?284.
Claudia Leacock and Martin Chodorow, 2003. C-
rater: Automated Scoring of Short-Answer Ques-
tions. Computers and the Humanities, 37:389?
405.
Vladimir I. Levenshtein, 1966. Binary Codes Capa-
ble of Correcting Deletions, Insertions, and Rever-
sals. Soviet Physics Doklady, 10(8):707?710.
Xin Li and Dan Roth, 2002. Learning Question Clas-
sifiers. In Proceedings of the 19th International
8
Conference on Computational Linguistics (COL-
ING 2002). Taipei, Taiwan, pp. 1?7.
Detmar Meurers, Niels Ott and Ramon Ziai, 2010.
Compiling a Task-Based Corpus for the Analysis
of Learner Language in Context. In Proceedings of
Linguistic Evidence. Tu?bingen, pp. 214?217. http:
//purl.org/dm/papers/meurers-ott-ziai-10.html.
Detmar Meurers, Ramon Ziai, Niels Ott and Stacey
Bailey, 2011. Integrating Parallel Analysis Mod-
ules to Evaluate the Meaning of Answers to
Reading Comprehension Questions. IJCEELL.
Special Issue on Automatic Free-text Evalua-
tion, 21(4):355?369. http://purl.org/dm/papers/
meurers-ziai-ott-bailey-11.html.
Rodney D. Nielsen, Wayne Ward and James H. Mar-
tin, 2009. Recognizing entailment in intelligent
tutoring systems. Natural Language Engineering,
15(4):479?501.
Joakim Nivre, Jens Nilsson, Johan Hall, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov and Erwin Marsi, 2007. MaltParser: A
Language-Independent System for Data-Driven
Dependency Parsing. Natural Language Engineer-
ing, 13(1):1?41.
Craige Roberts, 1996. Information Structure in Dis-
course: Towards an Integrated Formal Theory of
Pragmatics. In Jae-Hak Yoon and Andreas Kathol
(eds.), OSU Working Papers in Linguistics No. 49:
Papers in Semantics, The Ohio State University.
A?lvaro Rodrigo, Anselmo Pen?as and Felisa Verdejo,
2009. Overview of the Answer Validation Exercise
2008. In Carol Peters, Thomas Deselaers, Nicola
Ferro, Julio Gonzalo, Gareth Jones, Mikko Ku-
rimo, Thomas Mandl, Anselmo Pen?as and Vivien
Petras (eds.), Evaluating Systems for Multilin-
gual and Multimodal Information Access, Springer
Berlin / Heidelberg, volume 5706 of Lecture Notes
in Computer Science, pp. 296?313.
Helmut Schmid, 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
the International Conference on New Methods in
Language Processing. Manchester, UK, pp. 44?49.
Roger Schwarzschild, 1999. GIVENness, AvoidF
and other Constraints on the Placement of Accent.
Natural Language Semantics, 7(2):141?177.
Karen Spa?rck Jones, 2007. Automatic Summarising:
The State of the Art. Information Processing and
Management, 43:1449?1481.
Peter Turney, 2001. Mining the Web for Synonyms:
PMI-IR Versus LSA on TOEFL. In Proceedings
of the Twelfth European Conference on Machine
Learning (ECML-2001). Freiburg, Germany, pp.
491?502.
Sholom M. Weiss and Casimir A. Kulikowski, 1991.
Computer Systems That Learn: Classification and
Prediction Methods from Statistics, Neural Nets,
Machine Learning, and Expert Systems. Morgan
Kaufmann, San Mateo, CA.
9
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 163?173,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
On Improving the Accuracy of Readability Classification
using Insights from Second Language Acquisition
Sowmya Vajjala
Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
sowmya@sfs.uni-tuebingen.de
Detmar Meurers
Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
dm@sfs.uni-tuebingen.de
Abstract
We investigate the problem of readability as-
sessment using a range of lexical and syntac-
tic features and study their impact on predict-
ing the grade level of texts. As empirical ba-
sis, we combined two web-based text sources,
Weekly Reader and BBC Bitesize, targeting
different age groups, to cover a broad range
of school grades. On the conceptual side, we
explore the use of lexical and syntactic mea-
sures originally designed to measure language
development in the production of second lan-
guage learners. We show that the develop-
mental measures from Second Language Ac-
quisition (SLA) research when combined with
traditional readability features such as word
length and sentence length provide a good
indication of text readability across different
grades. The resulting classifiers significantly
outperform the previous approaches on read-
ability classification, reaching a classification
accuracy of 93.3%.
1 Introduction
Reading plays an important role in the development
of first and second language skills, and it is one of
the most important means of obtaining information
about any subject, in and outside of school. How-
ever, teachers often find it difficult to obtain texts
appropriate to the reading level of their students, on
a given topic. In many cases, they end up modifying
or creating texts, which takes significant time and ef-
fort. In addition to such a traditional school setting,
finding texts at the appropriate reading level is also
important in a wide range of real-life contexts in-
volving people with intellectual disabilities, dyslex-
ics, immigrant populations, and second or foreign
language learners.
Readability-based text classification, when used
as a ranking parameter in a search engine, can help
in retrieving texts that suit a particular target reading
level for a given query topic. In the context of lan-
guage learning, a language aware search engine (Ott
and Meurers, 2010) that includes readability classi-
fication can facilitate the selection of texts from the
web that are appropriate for the students in terms of
form and content. This is one of the main motiva-
tions underlying our research.
Readability assessment has a long history
(DuBay, 2006). Traditionally, only a limited set of
surface features such as word length and sentence
length were considered to derive a formula for read-
ability. More recently, advances in computational
linguistics made it possible to automatically extract
a wider range of language features from text. This
facilitated building machine learning models that es-
timate the reading level of a text. On the other hand,
there has also been an on-going stream of research
on reading and text complexity in other areas such as
Second Language Acquisition (SLA) research and
psycholinguistics.
In SLA research, a range of measures have been
proposed to study the development of complexity
in the language produced by learners. These mea-
sures are used to evaluate the oral or written pro-
duction abilities of language learners. The aim of
readability classification, on the other hand, is to re-
trieve texts to be comprehended by readers at a par-
163
ticular level. Since we want to classify and retrieve
texts for learners of different age groups, we hypoth-
esized that these SLA-based complexity measures of
learner production, when used as features for read-
ability classification will improve the performance
of the classifiers. In this paper, we show that this
approach indeed results in a significant performance
improvement compared to previous research.
We used the WeeklyReader website1 as one of
the text used in previous research. We combined it
with texts crawled from the BBC-Bitesize website2,
which provides texts for a different age group. The
combined corpus, WeeBit, covers a comparatively
larger range of ages than covered before.
To summarize, the contributions of this paper are:
? We adapt measures from second language ac-
quisition research to readability classification
and show that the overall classification accu-
racies of an approach including these features
significantly outperforms previous approaches.
? We extend the most widely used WeeklyReader
corpus by combining it with another corpus that
is graded for a different age-group, thereby cre-
ating a larger and more diverse corpus as basis
for future research.
The paper is organized as follows: Section 2 de-
scribes related work on reading level classification
to put our work in context. Section 3 introduces the
corpora we used. Section 4 describes the features
we considered in detail. Section 5 presents the ap-
proach and discusses the results. Section 6 provides
a summary and points to future work.
2 Related Work
The traditional readability formulae made use of a
limited number of surface features, such as the aver-
age sentence length and the average word length in
characters or syllables (Kincaid et al, 1975; Cole-
man and Liau, 1975). Some works also made use
of lists of ?difficult? words, typically based on fre-
quency counts, to estimate readability of texts (Dale
and Chall, 1948; Chall and Dale, 1995; Stenner,
1http://www.weeklyreader.com
2http://www.bbc.co.uk/bitesize
1996). Dubay (2006) provides a broad survey of tra-
ditional approaches to readability assessment. Al-
though the features considered appear shallow in
terms of linguistic modeling, they have been popular
for many years and are widely used.
More recently, the developments in computational
linguistics made it possible to consider various lex-
ical and syntactic features to automatically model
readability. In some of the early works on statis-
tical readability assessment, Si and Callan (2001)
and Collins-Thompson and Callan (2004) reported
the impact of using unigram language models to es-
timate the grade level of a given text. The models
were built on a United States text book corpus.
Heilman et al (2007; 2008b; 2008a) extended
this approach and worked towards retrieving rele-
vant reading materials for language learners in the
REAP3 project. They extended the above mentioned
approach to include a set of manually and later au-
tomatically extracted grammatical features.
Schwarm and Ostendorf (2005) and Petersen and
Ostendorf (2009) report on classification experi-
ments with WeeklyReader data, considering statisti-
cal language models, traditional formulae, as well as
certain basic parse tree features in building an SVM-
based statistical model. Feng et al (2010) and Feng
(2010) went beyond lexical and syntactic features
and studied the impact of several discourse-based
features, comparing their performance on the Week-
lyReader corpus.
While the vast majority of approaches have tar-
geted English texts, some work on other languages
such as German, Portuguese, French and Italian (vor
der Bru?ck et al, 2008; Aluisio et al, 2010; Fran-
cois and Watrin, 2011; Dell?Orletta et al, 2011) is
starting to emerge. Parse-tree-based features have
also been used to measure the complexity of spoken
Swedish (Roll et al, 2007).
The process of text comprehension and the effect
of factors such as the coherence of texts have also
been intensively studied (e.g., Crossley et al, 2007a;
2007b; Graesser et al, 2004) and measures to ana-
lyze the text under this perspective have been imple-
mented in the CohMetrix project.4
The DARPA Machine Reading program created
3http://reap.cs.cmu.edu
4http://cohmetrix.memphis.edu
164
a corpus of general text readability containing var-
ious forms of human and machine generated texts
(Strassel et al, 2010).5 The aim of this program is to
transform natural language texts into a format suit-
able for automatic processing by machines and to
filter out poorly written documents based on the text
quality. Kate et al (2010) used this data set to build
a coarse grained model of text readability.
While in this paper we focus on comparing com-
putational linguistic approaches to readability as-
sessment and improving the state of the art on a tra-
ditional and available data set, Nelson et al (2012)
compared several research and commercially avail-
able text difficulty assessment systems in support of
the Common Core Standards? goal of providing stu-
dents with texts at the appropriate level of difficulty
throughout their schooling.6
Independent of the research on readability, the
complexity of the texts produced by language learn-
ers has been extensively investigated in Second
Language Acquisition (SLA) research (Housen and
Kuiken, 2009). Recent approaches have automated
and compared a number of such complexity mea-
sures for learner language, specifically in English as
Second Language learner narratives (Lu, 2010; Lu,
2011b). So far, there is hardly any work on using
such insights in computational linguistics, though,
with the notable exception of Chen and Zechner
(2011) using SLA features to evaluate spontaneous
non-native speech. Given that graded corpora are
also intended to be used by incremental age groups,
we started to investigate whether the insights from
SLA research can fruitfully be applied to readability
classification.
3 Corpora
We used a combined corpus of WeeklyReader and
BBC-Bitesize to develop a statistical model that
classifies texts into five grade levels, based on the
age groups.
WeeklyReader7 is an educational newspaper, with
articles targeted at four grade levels (Level 2, Level
3, Level 4, and Senior), corresponding to children
5The corpus is apparently intended to be available for public
use, but does not yet seem to be so; we so far were unsuccessful
in obtaining more information from the authors.
6http://www.corestandards.org
7http://www.weeklyreader.com
between ages 7?8, 8?9, 9?10, and 9?12 years. The
articles cover a wide range of non-fiction topics,
from science to current affairs, written according to
the grade level of the readers. The exact criterion
of graded writing is not published by the magazine.
We obtained permission to use the graded magazine
articles and downloaded the archives in 11/2011.8
Though we used the same WeeklyReader text
base as the previous works, the corpus is not identi-
cal since we downloaded our version more recently.
Thus the archive contained more articles per level
and some preprocessing may differ. The Week-
lyReader magazine issues in addition to the actual
articles include teacher guides, student quizzes, im-
ages and brain teaser games, which we did not in-
clude in the corpus. The distribution of articles after
this preprocessing is shown in Table 1.
Grade Age Number of Avg. Number of
Level in Years Articles Sentences/Article
Level 2 7?8 629 23.41
Level 3 8?9 801 23.28
Level 4 9?10 814 28.12
Senior 10-12 1325 31.21
Table 1: The Weekly Reader corpus
BBC-Bitesize9 is a website with articles classi-
fied into four grade levels (KS1, KS2, KS3 and
GCSE), corresponding to children between ages 5?
7, 8?11, 11?14 and 14?16 years. The Bitesize cor-
pus is freely available on the web, and we crawled it
in 2009. Most of the articles at KS1 consisted of im-
ages and flash files and other audio-visual material,
with little text. Hence, we did not include KS1 in
our corpus. We also excluded pages that contained
only images, audio, or video files without text.
To cover a broad range of non-overlapping age
groups, we used Level 2, Level 3 and Level 4 from
WeeklyReader and KS3 and GCSE from Bitesize
data respectively and built a combined corpus cover-
ing learners aged 7 to 16 years. Note that while KS2
covers the age group of 8?11 years, Levels 2, 3, and
8A license to use the texts on the website for research can be
obtained for a small fee from support@weeklyreader.com. To
support comparable research, we will share the exact corpus we
used with other researchers who have obtained a license to use
the WeeklyReader materials.
9http://www.bbc.co.uk/bitesize
165
4 together cover ages 7?10 years. Similarly, the Se-
nior Level overlaps with Level 4 and KS3. Hence,
we excluded KS2 and Senior from the combined
corpus. We will refer to the combined five-level cor-
pus we created in this way as WeeBit. The distribu-
tion of articles in the combined WeeBit corpus after
preprocessing and removing the overlapping grade
levels, is shown in Table 2.
Grade Age Number of Avg. Number of
Level in Years Articles Sentences/Article
Level 2 7?8 629 23.41
Level 3 8?9 801 23.28
Level 4 9?10 814 28.12
KS3 11?14 644 22.71
GCSE 14?16 3500 27.85
Table 2: The WeeBit corpus
To avoid a classification bias towards a class with
more training examples during, for each level in the
WeeBit corpus, 500 documents were taken as train-
ing set and 125 documents were taken as test set.
In total, we trained on a set of 2500 documents and
used a test set of 625 documents, spanning across
five grade levels.
4 Features
To build our classification models, we combined
features used in previous research with other parse
tree features as well as lexical richness and syntactic
complexity features from SLA research. We group
the features into three broad categories: lexical, syn-
tactic and traditional features.
4.1 Lexical Features
Word n-grams have been frequently used as lexical
features in the previous research (Collins-Thompson
and Callan, 2004; Schwarm and Ostendorf, 2005).10
POS n-grams as well as POS-tag ratio features have
also been used in some of the later works (Feng et
al., 2010; Petersen and Ostendorf, 2009).
In the SLA context, independent of the readability
research, Lu (2011a) studied the relationship of lexi-
cal richness to the quality of English as Second Lan-
guage (ESL) learners? oral narratives and analyzed
10In the readability literature, n-grams are traditionally dis-
cussed as lexical features. N-grams beyond unigrams naturally
also encode aspects of syntax.
the distribution of three dimensions of lexical rich-
ness (lexical density, sophistication and variation) in
them using various metrics proposed in the language
acquisition literature. Those measures were used to
analyze a large scale corpus of Chinese learners of
English. We adapted some of the metrics from this
research as our lexical features:
Type-Token Ratio (TTR) is the ratio of number
of word types (T) to total number word tokens in
a text (N). It has been widely used as a measure
of lexical diversity or lexical variation in language
acquisition studies. However, since it is depen-
dent on the text size, various alternative transfor-
mations of TTR came into existence. We consid-
ered Root TTR (T/
?
N ), Corrected TTR (T/
?
2N ),
Bilogarithmic TTR (Log T/Log N) and Uber Index
(Log2T/Log(N/T )).
Another recent TTR variant we considered, which
is not a part of Lu (2011a), is the Measure of Textual
Lexical Diversity (MTLD; McCarthy and Jarvis,
2010). It is a TTR-based approach that is not af-
fected by text length. It is evaluated sequentially, as
the mean length of string sequences that maintain a
default Type-Token Ratio value. That is, the TTR
is calculated at each word. When the default TTR
value is reached, the MTLD count increases by one
and TTR evaluations are again reset. McCarthy and
Jarvis (2010) considered the default TTR as 0.72 and
we continued with the same default.
Considering nouns, adjectives, non-modal and
non-auxiliary verbs and adverbs as lexical items,
Lu (2011a) studied various syntactic category based
word ratio measures. Lexical variation is defined
as the ratio of the number of lexical types to lexi-
cal tokens. Other variants of lexical variation stud-
ied in Lu (2011a) included noun, adjective, modi-
fier, adverb and verb variations, which represent the
proportion of the words of the respective categories
compared to all lexical words in the document. Al-
ternative measures of verb variation, namely Verb
Variation-1 (Tverb/Nverb), Squared Verb Variation-
1 (T 2verb/Nverb) and Corrected Verb Variation-1
(Tverb/
?
2Nverb) are also studied in the literature.
We considered all these measures of lexical varia-
tion as a part of our lexical features. We have also
included Lexical Density, which is the ratio of the
number of lexical items in relation to the total num-
ber of words in a text.
166
In addition to these measures from the SLA lit-
erature, in our lexical features we included the aver-
age number of syllables per word (NumSyll) and the
average number of characters per word (NumChar),
which are used as word-level indicators of text com-
plexity in various traditional formulae (Kincaid et
al., 1975; Coleman and Liau, 1975).
Finally, we included the proportion of words in
the text which are found on the Academic Word List
as another lexical feature. It refers to the word list
created by Coxhead (2000), which contains a list of
most frequent words found in the academic texts.11
The list does not include the most frequent words in
the English language as such. The words in this list
are specific to academic contexts. It was intended to
be used both by teachers and students as a measure
of vocabulary acquisition. We use it as an additional
lexical feature in our work ? and it turned out to be
one of the most predictive features.
All the lexical features we considered in this work
are listed in Table 3. The SLA based lexical features
are referred to as SLALEX in the table. Of these,
Lexical Features from SLA research (SLALEX)
? Lexical Density (LD)
? Type-Token Ratio (TTR)
? Corrected TTR (CTTR)
? Root TTR (RTTR)
? Bilogarithmic TTR (LogTTR)
? Uber Index (Uber)
? Lexical Word Variation (LV)
? Verb Variation-1 (VV1)
? Squared VV1 (SVV1)
? Corrected VV1 (CVV1)
? Verb Variation 2 (VV2)
? Noun Variation (NV)
? Adjective Variation (AdjV)
? Adverb Variation (AdvV)
? Modifier Variation (ModV)
? Mean Textual Lexical Density (MTLD)
Other Lexical Features
? Proportion of words in AWL (AWL)
? Avg. Num. Characters per word (NumChar)
? Avg. Num. Syllables per word (NumSyll)
Table 3: Lexical Features (LEXFEATURES)
11http://en.wikipedia.org/wiki/Academic_Word_List
six features CTTR, RTTR, SVV1, CVV1, AdvV, ModV
were shown by Lu (2011b) to correlate best with the
learner data. We will refer to them as BESTLEX-
SLA, highlighted in italics in the table.
4.2 Syntactic Features
Schwarm and Ostendorf (2005) implemented four
parse tree features (average parse tree height, aver-
age number of SBARs, NPs per sentence and VPs
per sentence) in their work. Feng (2010) considered
more syntactic features, adding the average lengths
of phrases (NP, VP and PP) per sentence in words
and characters, and the total number of respective
phrases in the document. In our work, we started
with reconsidering the above mentioned syntactic
features.
In addition, we included measures of syntactic
complexity from the SLA literature. Lu (2010) se-
lected 14 measures from a large set of measures used
to monitor the syntactic development in language
learners. He then used these measures in the analysis
of syntactic complexity in second language writing
and showed that some of them correlate well with
the syntactic development of adult Chinese learners
of English. They are grouped into five broad cate-
gories:
The first set consists of three measures of syn-
tactic complexity based on the length of a unit at
the sentential, clausal and T-unit level respectively.
The definitions for sentence, clause and T-unit were
adapted from the SLA literature. While a sentence
is considered to be a group of words delimited with
punctuation mark, a clause is any structure with a
subject and a finite verb. Finally, a T-unit is char-
acterized as one main clause plus any subordinate
clause or non-clausal structure that is attached to or
embedded in it.
The second type of measure targets sentence com-
plexity. Clauses per sentence is considered as a sen-
tence complexity measure.
The third set of measures reflect the amount of
subordination in the sentence. They include clauses
per T-unit, complex T-units per T-unit, dependent
clauses per clause and dependent clauses per T-unit.
A complex T-unit is considered as any T-unit that
contains a dependent clause.
The fourth type of measures measured the amount
of co-ordination in a sentence. They consist of co-
167
ordinate phrases per clause and co-ordinate phases
per T-unit. Any adjective, verb, adverb or noun
phrase that dominates a co-ordinating conjunction is
considered a co-ordinate phrase.
The fifth type of measures represented the rela-
tionship between specific syntactic structures and
larger production units. They include complex nom-
inals per clause, complex nominals per T-unit and
verb phrases per T-unit. Complex nominals are com-
prised of a) nouns plus adjective, possessive, prepo-
sitional phrase, relative clause, participle or appos-
itive, b) nominal clauses, c) gerunds and infinitives
in subject positions.
We implemented these 14 syntactic measures as
features in building our classification models, in ad-
dition to existing features. Eight of these features
(MLC, MLT, CP/C, CP/T, CN/C, CN/T, MLS, VP/T)
were argued to correlate best with language develop-
ment. We refer to this subset of eight as BESTSYN-
SLA, shown in italics in Table 4. We will see in sec-
tion 5 that a set including those features also holds
good predictive power for classifying graded texts.
We also included the number of dependent
clauses, complex T-units, and co-ordinate phrases
per sentence as additional syntactic features. Table 4
summarizes the syntactic features used in this paper.
4.3 ?Traditional? Features
The average number of characters per word (Num-
Char), the average number of syllables per word
(NumSyll), and the average sentence length in words
(MLS) have been used to derive formulae for read-
ability in the past. We refer to them as Traditional
Features below. We included MLS in the syntactic
features and NumChar, and NumSyll in the Lexi-
cal features. We also included two popular readabil-
ity formulae, Flesch-Kincaid score (Kincaid et al,
1975) and Coleman-Liau readability formula (Cole-
man and Liau, 1975), as additional features. The
latter will be referred as Coleman below, and both
formulas together as Traditional Formulae.
5 Experiments and Evaluation
We used the Berkeley Parser (Petrov and Klein,
2007) with the standard model they provide for
building syntactic parse trees and defined the pat-
terns for extracting various syntactic features from
Syntactic features from SLA research (SLASYN)
? Mean length of clause (MLC)
? Mean length of a sentence (MLS)
? Mean length of T-unit (MLT)
? Num. of Clauses per Sentence (C/S)
? Num. of T-Units per sentence (T/S)
? Num. of Clauses per T-unit (C/T)
? Num. of Complex-T-Units per T-unit (CT/T)
? Dependent Clause to Clause Ratio (DC/C)
? Dependent Clause to T-unit Ratio (DC/T)
? Co-ordinate Phrases per Clause (CP/C)
? Co-ordinate Phrases per T-unit (CP/T)
? Complex Nominals per Clause (CN/C)
? Complex Nominals per T-unit (CN/T)
? Verb phrases per T-unit (VP/T)
Other Syntactic features
? Num. NPs per sentence (NumNP)
? Num. VPs per sentence (NumVP)
? Num. PPs per sentence (NumPP))
? Avg. length of a NP (NPSize)
? Avg. length of a VP (VPSize)
? Avg. length of a PP (PPSize)
? Num. Dependent Clauses per sentence (NumDC)
? Num. Complex-T units per sentence (NumCT)
? Num. Co-ordinate Phrases per sentence (CoOrd)
? Num. SBARs per sentence (NumSBAR)
? Avg. Parse Tree Height (TreeHeight)
Table 4: Syntactic features (SYNFEATURES)
the trees using the Tregex pattern matcher (Levy and
Andrew, 2006). More details about the patterns from
the SLA literature and their definitions can be found
in Lu (2010). We used the OpenNLP12 tagger to
get POS tag information and calculate Lexical Rich-
ness features. We used the WEKA (Hall et al, 2009)
toolkit for our classification experiments. We ex-
plored different classification algorithms such as De-
cision Trees, Support Vector Machines, and Logis-
tic Regression. The Multi-Layer Perceptron (MLP)-
classifier performed best with various combinations
of features, so we focus on reporting the results for
that algorithm.
12http://opennlp.apache.org
168
Feature set # Features Classifier Performance
Accuracy RMSE
Traditional Formulae 2 38.8% 0.36
Traditional Features 3 70.3% 0.25
Trad. Features + Trad. formulae 5 72.3% 0.32
SLALEX 16 68.1% 0.29
SLASYN 14 71.2% 0.28
SLALEX + SLASYN 30 82.3% 0.23
BEST10SYN 10 69.9% 0.28
All Syntactic Features 25 75.3% 0.27
BEST10LEX 10 82.4% 0.22
All Lexical Features 19 86.7% 0.20
BEST10ALL 10 89.7% 0.18
All features 46 93.3% 0.15
Table 5: Classification results for WeeBit Corpus
5.1 Evaluation Metrics
We report our results in terms of classification accu-
racy and root mean square error.
Classification accuracy refers to the percentage of
instances in the test set that are classified correctly.
The correct classifications include both true posi-
tives and true negatives. However, accuracy does
not reflect how close the prediction is to the actual
value. A difference between expected and predicted
values of one grade level is treated the same way as
the difference of, e.g., four grade levels.
Root mean square error (RMSE) is a measure
which gives a better picture of this difference.
RMSE is the square root of empirical mean of the
squared prediction errors. It is frequently used as
a measure to estimate the deviation of an observed
value from the expected value. In readability assess-
ment, it can be understood as the average difference
between the predicted grade level and the expected
grade level.
5.2 Feature Combinations
Complementing our experiments comparing the dif-
ferent lexical and syntactic features and their com-
bination, we also used WEKA?s information-gain-
based feature selection algorithm, and selected the
Top-10 best features using the ranker method.
When all features were considered, the top 10
most predictive features were found to be: (Num-
Char, NumSyll, MLS, AWL, ModVar, CoOrd, Cole-
man, DC/C, CN/C,and AdvVar), which are referred
to as BEST10ALL in the table.
Considering the 25 syntactic features alone, the
10 most predictive features were: (MLS, CoOrd,
DC/C, CN/C, CP/C, NumPP, VPSize, C/T, CN/T and
NumVP), referred to as BEST10SYN in the table.
The 10 most predictive features amongst all the
lexical features were: (NumChar, NumSyll, AWL,
ModV, AdvV, AdjV, LV, VV1, NV and SVV1). They
are referred to as BEST10LEX in the table.
Although the traditionally used features (Num-
Char, NumSyll, MLS) seem to be the most predictive,
it can be seen from the other top ranked features,
that there is significant overlap between the best fea-
tures identified by WEKA and the features which
Lu (2010; 2011b) identified as correlating best with
language development (shown in italics in Table 3
and Table 4), which supports our hypothesis that the
SLA-based measures are useful features for read-
ability classification of non-learner text too.
5.3 Results
Table 5 shows the results of our classification ex-
periments using WEKA?s Multi-Layer Perceptron
algorithm with different combinations of features.
Combining all features results in the best accuracy
of 93.3%, which is a large improvement over the
current state of the art in readability classification
reported on the WeeklyReader corpus (74.01% by
Feng et al, 2010). It should, however, be kept
169
# Features Highest reported accuracy
Previous work (on WeeklyReader)
(Feng et al, 2010) 122 74.01%
(Petersen and Ostendorf, 2009) 25 63.18%
Syntactic features only (Petersen and Ostendorf, 2009) 4 50.91%
Our Results (on WeeklyReader alone)
Syntactic features from (Petersen and Ostendorf, 2009) 4 50.68%
All our Syntactic Features 25 64.3%
All our Lexical Features 19 84.1%
All our Features 46 91.3%
Our Results (on WeeBit)
All our Syntactic Features 25 75.3%
All our Lexical Features 19 86.7%
All our Features 46 93.3%
Table 6: Overall Results and Comparison with Previous Work
in mind that the improvement is achieved on the
WeeBit corpus which is an extension of the Week-
lyReader corpus previously used. Interestingly, the
result of 89.7% for BEST10ALL, the top 10 features
chosen by the WEKA ranker, are quite close to our
best result, with a very small number of features.
Lexical features seem to perform better than syn-
tactic features when considered separately. How-
ever, this better performance of lexical features was
mainly due to the addition of the traditionally used
features NumChar and NumSyll. So it is no won-
der that these shallow features have been used in
the traditional readability formulae for such a long
time; but the predictive power of the traditional for-
mulae as features by themselves is poor (38.8%), in
line with the conclusions drawn in previous research
(Schwarm and Ostendorf, 2005; Feng et al, 2010)
about the Flesch-Kincaid and Dale-Chall formulae.
Interestingly, Coleman, which was not considered
in those previous approaches, was ranked among
the Top-10 most predictive features by the WEKA
ranker. So it holds a good predictive power when
used as one of the features for the classifier.
We also studied the impact of SLA based fea-
tures alone on readability classification. The perfor-
mance of the SLA based lexical features (SLALEX)
and syntactic features (SLASYN) when considered
separately are still in a comparable range with the
previously reported results on readability classifi-
cation (68.1% and 71.2% respectively). However,
combining both of them resulted in an accuracy of
82.3%, which is a considerable improvement over
previously reported results. It again adds weight to
the initial hypothesis that SLA based features can be
useful for readability classification.
5.4 Comparison with previous work
Table 6 provides an overall comparison of the accu-
racies obtained for the key features sets in our work
with the best results reported in the literature for the
WeeklyReader corpus. However, since our classi-
fication experiments were carried out with a newly
compiled corpus extending the WeeklyReader data,
such a direct comparison is not particularly mean-
ingful by itself. To address this issue, we explored
two avenues.
Firstly, we ran additional experiments, training
and testing on the WeeklyReader data only, includ-
ing the four levels used in previous work on that cor-
pus. A summary of the results can be seen in Table
6. Our approach with 46 features results in 91.3%
accuracy on the WeeklyReader corpus, compared to
74.01% as the best previous WeeklyReader result,
reported by Feng et al (2010) for their much larger
feature set (122 features).
In order to verify the impact of our choice of fea-
tures, we also did a replication of the parsed syntac-
tic feature measures reported by (Schwarm and Os-
tendorf, 2005) on the WeeklyReader corpus and ob-
tained essentially the same accuracy as the one pub-
170
lished (50.7% vs. 50.91%), supporting the compa-
rability of the WeeklyReader data used. The signif-
icant performance increase we reported thus seems
to be due to the new features we integrated from the
SLA literature.
Secondly, we were interested in the impact of the
training size on the results. We therefore investi-
gated how good our best approach (using all fea-
tures) is on a training corpus that is comparable to
the WeeklyReader corpus used in previous work in
terms of the number of documents per class. When
we took 1400 WeeklyReader documents distributed
into four classes as described in Feng et al (2010),
we obtained an accuracy of 84.2%, compared to the
74.01% they reported as best result. Using 2500
documents distributed into four classes as in Pe-
tersen and Ostendorf (2009) we obtained 88.4%,
compared to their best result of 63.18%. Given that
the original corpora used are not available, these
WeeklyReader corpora with the same source, num-
ber of documents, and size of classes are as close
as we can get to a direct comparison. In the future,
the availability of the WeeBit corpus will support a
more direct comparison of approaches.
In sum, the above experiments seem to indicate
that the set of features and classifier used in our ap-
proach play an important role in the resulting signif-
icant increase in accuracy.
6 Conclusion and Discussion
We created a new corpus, WeeBit, by combining
texts from two graded web sources WeeklyReader
and BBC Bitesize. The resulting text corpus is
larger and covers more grade levels, spanning the
age group between 7 and 16 years. We hope that
the availability of this graded corpus will be useful
as an empirical basis for future studies in automatic
readability assessment.13
We studied the impact of various lexical and syn-
tactic features and explored their performance in
combination with features encoding syntactic com-
plexity and lexical richness that were inspired by
Second Language Acquisition research. Our experi-
ments show that not only the full set of features, but
13As mentioned above, we will make the WeeBit corpus
available to all researchers who have obtained the inexpensive
research license from WeeklyReader.
also specific manually or automatically selected sub-
sets of features provide results significantly improv-
ing on the previously published state of the art in
automatic readability assessment. There also seems
to be a clear correlation between the good predictors
according to SLA research on language learning and
those that performed well in text classification.
Although the exact criteria based on which the
individual corpora (WeeklyReader, BBC-Bitesize)
were created is not known, it is possible that they
were created with the well-known, traditional read-
ability formulae in mind. It would be surprising if
the two corpora, compiled in the US and Britain by
different companies, were created with the same set
of measures in mind, so the WeeBit corpus should
be less affected. Still, it is possible that the rea-
son the traditional features NumChar, NumSyll and
MLS held such a strong predictive power is that
these measures were considered when the texts were
written. But removing these traditional features only
strengthens the role of the other features and thereby
the main point of the paper arguing for the usefuless
of SLA developmental measures for readability clas-
sification.
As a part of our future work, we intend to revisit
and study the impact of further classes of features
employed in psycholinguistics and cognitive sci-
ence research, such as those studied in Coh-Metrix
(Graesser et al, 2004) or in the context of retrieving
texts for specific groups of readers (Feng, 2010).
In terms of our overall application goal, we are
currently studying the ability of the classification
models we built to generalize to web data. We then
plan to add the classification model to a language
aware search engine (Ott and Meurers, 2010). Such
a search engine may then also be able to integrate
user feedback on the readability levels of webpages,
to build a dynamic, online model of readability.
7 Acknowledgements
We thank the anonymous reviewers and workshop
organizers for their feedback on the paper. The re-
search leading to these results has received funding
from the European Commission?s 7th Framework
Program under grant agreement number 238405
(CLARA).14
14http://clara.uib.no
171
References
Sandra Aluisio, Lucia Specia, Caroline Gasperin, and
Carolina Scarton. 2010. Readability assessment for
text simplification. In Proceedings of the NAACL HLT
2010 Fifth Workshop on Innovative Use of NLP for
Building Educational Applications, pages 1?9, Los
Angeles, California, June.
Jeanne S. Chall and Edgar Dale. 1995. Readability
Revisted: The New Dale-Chall Readability Formula.
Brookline Books.
Maio Chen and Klaus Zechner. 2011. Computing and
evaluating syntactic complexity features for automated
scoring of spontaneous non-native speech. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 722?731,
Portland, Oregon, June.
Meri Coleman and T.L. Liau. 1975. A computer read-
ability formula designed for machine scoring. Journal
of Applied Psychology, 60:283?284.
Kevyn Collins-Thompson and Jamie Callan. 2004. A
language modeling approach to predicting reading dif-
ficulty. In Proceedings of HLT/NAACL 2004, Boston,
USA.
Averil Coxhead. 2000. A new academic word list.
Teachers of English to Speakers of Other Languages,
34(2):213?238.
Scott A. Crossley, David F. Dufty, Philip M. McCarthy,
and Danielle S. McNamara. 2007a. Toward a new
readability: A mixed model approach. In Danielle S.
McNamara and Greg Trafton, editors, Proceedings of
the 29th annual conference of the Cognitive Science
Society. Cognitive Science Society.
Scott A. Crossley, Max M. Louwerse, Philip M. Mc-
Carthy, and Danielle S. McNamara. 2007b. A lin-
guistic analysis of simplified and authentic texts. The
Modern Language Journal, 91(1):15?30.
Edgar Dale and Jeanne S. Chall. 1948. A formula for
predicting readability. Educational research bulletin;
organ of the College of Education, 27(1):11?28.
Felice Dell?Orletta, Simonetta Montemagni, and Giulia
Venturi. 2011. Read-it: Assessing readability of ital-
ian texts with a view to text simplification. In Proceed-
ings of the 2nd Workshop on Speech and Language
Processing for Assistive Technologies, pages 73?83.
William H. DuBay. 2006. The Classic Readability Stud-
ies. Impact Information, Costa Mesa, California.
Lijun Feng, Martin Jansche, Matt Huenerfauth, and
Noe?mie Elhadad. 2010. A comparison of features for
automatic readability assessment. In In Proceedings of
the 23rd International Conference on Computational
Linguistics (COLING 2010), Beijing, China.
Lijun Feng. 2010. Automatic Readability Assessment.
Ph.D. thesis, City University of New York (CUNY).
Thomas Francois and Patrick Watrin. 2011. On the con-
tribution of mwe-based features to a readability for-
mula for french as a foreign language. In Proceedings
of Recent Advances in Natural Language Processing,
pages 441?447.
Arthur C. Graesser, Danielle S. McNamara, Max M.
Louweerse, and Zhiqiang Cai. 2004. Coh-metrix:
Analysis of text on cohesion and language. Behav-
ior Research Methods, Instruments and Computers,
36:193?202.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
The SIGKDD Explorations, 11(1).
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2007. Combining
lexical and grammatical features to improve readabil-
ity measures for first and second language texts. In
Human Language Technologies 2007: The Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (HLT-NAACL?07),
pages 460?467, Rochester, New York.
Michael Heilman, Kevyn Collins-Thompson, and Max-
ine Eskenazi. 2008a. An analysis of statistical mod-
els and features for reading difficulty prediction. In
Proceedings of the 3rd Workshop on Innovative Use of
NLP for Building Educational Applications, Colum-
bus, Ohio.
Michael Heilman, Le Zhao, Juan Pino, and Maxine Eske-
nazi. 2008b. Retrieval of reading materials for vocab-
ulary and reading practice. In Proceedings of the Third
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications (BEA-3) at ACL?08, pages 80?
88, Columbus, Ohio.
Alex Housen and Folkert Kuiken. 2009. Complexity,
accuracy, and fluency in second language acquisition.
Applied Linguistics, 30(4):461?473.
Rohit J. Kate, Xiaoqiang Luo, Siddharth Patwardhan,
Martin Franz, Radu Florian, Raymond J. Mooney,
Salim Roukos, and Chris Welty. 2010. Learning to
predict readability using diverse linguistic features. In
23rd International Conference on Computational Lin-
guistics (COLING 2010).
J. P. Kincaid, R. P. Jr. Fishburne, R. L. Rogers, and B. S
Chissom. 1975. Derivation of new readability for-
mulas (Automated Readability Index, Fog Count and
Flesch Reading Ease formula) for Navy enlisted per-
sonnel. Research Branch Report 8-75, Naval Techni-
cal Training Command, Millington, TN.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In 5th International Conference on Lan-
guage Resources and Evaluation, Genoa, Italy.
172
Xiaofei Lu. 2010. Automatic analysis of syntactic
complexity in second language writing. International
Journal of Corpus Linguistics, 15(4):474?496.
Xiaofei Lu. 2011a. A corpus-based evaluation of syn-
tactic complexity measures as indices of college-level
esl writers? language development. TESOL Quarterly,
45(1):36?62, March.
Xiaofei Lu. 2011b. The relationship of lexical richness
to the quality of esl learners? oral narratives. The Mod-
ern Languages Journal. in press.
Philip McCarthy and Scott Jarvis. 2010. Mtld, vocd-
d, and hd-d: A validation study of sophisticated ap-
proaches to lexical diversity assessment. Behavior Re-
search Methods, 42(2):381?392.
J. Nelson, C. Perfetti, D. Liben, and M. Liben. 2012.
Measures of text difficulty: Testing their predictive
value for grade levels and student performance. Tech-
nical report, The Council of Chief State School Offi-
cers.
Niels Ott and Detmar Meurers. 2010. Information re-
trieval for education: Making search engines language
aware. Themes in Science and Technology Education.
Special issue on computer-aided language analysis,
teaching and learning: Approaches, perspectives and
applications, 3(1?2):9?30.
Sarah E. Petersen and Mari Ostendorf. 2009. A machine
learning approach to reading level assessment. Com-
puter Speech and Language, 23:86?106.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference, pages 404?
411, Rochester, New York, April.
Mikael Roll, Johan Frid, and Merie Horne. 2007. Mea-
suring syntactic complexity in spontaneous spoken
swedish. Language and Speech, 50(2).
Sarah Schwarm and Mari Ostendorf. 2005. Reading
level assessment using support vector machines and
statistical language models. In Proceedings of the
43rd Annual Meeting of the Association for Computa-
tional Linguistics (ACL?05), pages 523?530, Ann Ar-
bor, Michigan.
Luo Si and Jamie Callan. 2001. A statistical model for
scientific readability. In Proceedings of the 10th Inter-
national Conference on Information and Knowledge
Management (CIKM), pages 574?576. ACM.
A. Jackson Stenner. 1996. Measuring reading compre-
hension with the lexile framework. In Fourth North
American Conference on Adolescent/Adult Literacy.
Stephanie Strassel, Dan Adams, Henry Goldberg,
Jonathan Herr, Ron Keesing, Daniel Oblinger, Heather
Simpson, Robert Schrag, and Jonathan Wright. 2010.
The darpa machine reading program - encouraging lin-
guistic and reasoning research with a series of read-
ing tasks. In Language Resources and Evaluation
(LREC), Malta.
Tim vor der Bru?ck, Sven Hartrumpf, and Hermann Hel-
big. 2008. A readability checker with supervised
learning using deep syntactic and semantic indicators.
Informatica, 32(4):429?-435.
173
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 190?200,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Short Answer Assessment: Establishing Links Between Research Strands
Ramon Ziai Niels Ott Detmar Meurers
SFB 833 / Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
{rziai,nott,dm}@sfs.uni-tuebingen.de
Abstract
A number of different research subfields are
concerned with the automatic assessment of
student answers to comprehension questions,
from language learning contexts to computer
science exams. They share the need to evaluate
free-text answers but differ in task setting and
grading/evaluation criteria, among others.
This paper has the intention of fostering
synergy between the different research strands.
It discusses the different research strands,
details the crucial differences, and explores
under which circumstances systems can be
compared given publicly available data. To that
end, we present results with the CoMiC-EN
Content Assessment system (Meurers et al,
2011a) on the dataset published by Mohler
et al (2011) and outline what was necessary
to perform this comparison. We conclude
with a general discussion on comparability and
evaluation of short answer assessment systems.
1 Introduction
Short answer assessment systems compare students?
responses to questions with manually defined target
responses or answer keys in order to judge the
appropriateness of the responses, or in order to
automatically assign a grade. A number of
approaches have emerged in recent years, each of
them with different aims and different backgrounds.
In this paper, we will draw a map of the short answer
assessment landscape, highlighting the similarities
and differences between approaches and the data used
for evaluation. We will provide an overview of 12
systems and sketch their attributes. Subsequently,
we will zoom into the comparison of two of them,
namely CoMiC-EN (Meurers et al, 2011a) and the
one which we call the Texas system (Mohler et al,
2011) and discuss the issues that arise with this
endeavor. Returning to the bigger picture, we will
explore how such systems could be compared in
general, in the belief that meaningful comparison
of approaches across research strands will be an
important ingredient in advancing this relatively new
research field.
2 The short answer assessment landscape
2.1 General aspects
Researchers from all directions have settled in the
landscape of short answer assessment, each of them
with different backgrounds and different goals. In
this section, we aim at providing an overview of
these research villages, also hoping to construct a
road network that may connect them.
Most approaches to short answer assessment are
situated in an educational context. Some focus on
GCSE1 tests, others aim at university assessment
tests in the medical domain. Another strand
of approaches focuses on language teaching and
learning. All of these approaches share one theme:
they assess short texts written by students. These
may be answers to questions that ask for knowledge
acquired in a course, e.g., in computer science, or to
reading comprehension questions in second language
1The General Certificate of Secondary Education (GCSE) is
an academic qualification in the United Kingdom, usually taken
at the age of 14?16.
190
learning. While thematically related, short answer
assessment is different from essay grading. Short
answers are formulated by students in a much more
controlled setting. Not only are they short, they
usually are supposed to contain only a few facts that
answer only one question.
Another common theme of these approaches is
that they compare the student answers to one or more
previously defined correct answers that are either
given in natural language as target answers or as a list
of concepts in an answer key. The ways of technically
conducting these comparisons vary widely, as we
discuss below in Section 2.2.
There also are conceptual differences between
the approaches. Some systems focus on assessing
whether or not the student has properly answered
the question. They put the spot on comparing the
meaning of target answers and student answers; they
aim at being tolerant of form errors such as spelling
or grammar errors. Others aim at giving a grade as
accurate as possible, therefore not only assessing
meaning but also performing grading similar to
human teachers. This can also include modules that
take into account form errors.
These two views on a similar task are also reflected
in the annotation of the data used in experiments:
Systems performing meaning comparison usually
operate with labels specifying the relations between
target answers and student answers. Grading systems
naturally aim at producing numerical grades. Since
labels are on a nominal scale, and grades are on
an ordinal scale (or even treated as being on an
interval scale), the difference between meaning
comparison and grading results in a whole string
of other differences in methodology.
Researchers also enter the short answer landscape
from different home countries: Some projects are
interested in the strategies and mechanics of meaning
comparison, others aim at reducing the load and costs
of large-scale assessment tests, and yet others aim
at improving intelligent tutoring systems, requiring
additional components that provide useful feedback
to students using these systems.
2.2 Approaches
Table 1 summarizes the features of the short answer
assessment systems discussed hereafter.
One of the earlier systems is WebLAS, presented
by Bachman et al (2002). A human task creator feeds
the system with scores for model answers. Regular
expressions are then created automatically from these
model answers. Since each regular expression is
associated with a score, matching the expression
against a student answer yields a score for that answer.
Bachman et al (2002) do not provide an evaluation
study based on data.
Another earlier system is CarmelTC by Rose? et
al. (2003). It has been designed as a component
in the Why2 tutorial dialogue system (VanLehn et
al., 2002). Even though Rose? et al (2003) position
CarmelTC in the context of essay grading, it may be
considered to deal with short answers: in their data,
the average length of a student response is approx.
48 words. Their system is designed to perform
text classification on single sentences in the student
responses, where each class of text represents one
possible model response, plus an additional class for
?no match?. They combine decision trees operating
on an automatic syntactic analysis, a Naive Bayes
text classifier, and a bag-of-words approach. In a
50-fold cross validation experiment with one physics
question, six classes and 126 student responses,
hand-tagged by two annotators, CarmelTC reaches
an F-measure value of 0.85. They do not report on a
baseline. Concerning the quality of the gold standard,
they report that conflicts in the annotation have been
resolved.
C-Rater (Leacock and Chodorow, 2003) is based
on a paraphrase recognition approach. It employs
correct answer models consisting of essential points
formulated in natural language. C-Rater aims at
automatic scoring and focuses on meaning, thus
tolerating form errors. Leacock and Chodorow
(2003) present two pilot studies, one of them dealing
with reading comprehension. From 16,625 student
answers with an average length of 43 words, they
drew a random sample of 100 answers to each of
the seven questions. This sample was scored by
one human judge using a three-way scoring system
(full credit, partial credit, no credit). Their system
achieved 84% agreement with the gold standard.
Information about the distribution of the scoring
categories is given indirectly: A baseline system that
assigns scores randomly would have achieved 47%
accuracy.
191
System Goal Technique Domain Lang.
WebLAS (Bachman et al, 2002) Assessment of
language ability
Auto-generated regular
expressions
Foreign language
teaching
EN
CarmelTC (Rose? et al, 2003) Automatic grading Text classification Physics EN
C-Rater (Leacock and Chodorow,
2003)
Assessment test Paraphrase recognition Mathematics,
Reading comp.
EN
IAT (Mitchell et al, 2003) Assessment,
Automatic grading
Information extraction
w/ handwritten patterns
Medical EN
Oxford (Pulman and Sukkarieh,
2005)
Assessment,
automatic grading
Information extraction
w/ handwritten patterns
GCSE exams EN
Atenea (Pe?rez et al, 2005) Automatic grading N-gram overlap, Latent
Semantic Analysis
Computer science ES
Logic-based System (Makatchev
and VanLehn, 2007)
Meaning comparison First-order logic,
machine learning
Physics EN
CAM (Bailey and Meurers, 2008),
CoMiC-EN (Meurers et al, 2011a)
Meaning comparison Alignment, machine
learning
Reading comp. in
foreign language
EN
Facets System (Nielsen et al, 2009) Meaning comparison
& tutoring systems
Alignment of facets,
machine learning
Elementary
school science
classes
EN
Texas (Mohler et al, 2011) Automatic grading Graph alignment,
semantic similarity
Computer science EN
CoMiC-DE (Meurers et al, 2011b) Meaning comparison Alignment, machine
learning
Reading comp. in
foreign language
DE
CoSeC-DE (Hahn and Meurers,
2012)
Meaning comparison Alignment via
Lexical-Resource
Semantics
Reading comp. in
foreign language
DE
Table 1: Short Answer Assessment systems and their Features
Information extraction templates form the core of
the Intelligent Assessment Technologies system (IAT,
Mitchell et al 2003). These templates are created
manually in a special-purpose authoring tool by
exploring sample responses. They allow for syntactic
variation, e.g., filling the subject slot in a sentence
with different equivalent concepts. The templates
corresponding to a question are then matched against
the student answer. Unlike other systems, IAT
additionally features templates for explicitly invalid
answers. They tested their approach with a progress
test that has to be taken by medicine students.
Approximately 800 students each plowed through
270 test items. The automatically graded responses
then were moderated: Human judges streamlined the
answers to achieve a more consistent grading. This
step already had been done before with tests graded
by humans. Mitchell et al (2003) state that their
system reaches 99.4% accuracy on the full dataset
after the manual adjustment of the templates via
the moderation process. Summarizing, they report
an error of ?between 5 and 5.5%? in inter-grader
agreement and an error of 5.8% in automatic grading
without the moderation step, though it is not entirely
clear which data these statistics correspond to. No
information on the distribution of grades or a random
baseline is provided.
The Oxford system (Pulman and Sukkarieh, 2005)
is another one to employ an information extraction
approach. Again, templates are constructed manually.
Motivated by the necessary robustness to process
language with grammar mistakes and spelling errors,
they use shallow analyses in their pre-processing.
In order to overcome the hassle of manually con-
structing templates, they also investigated machine
learning techniques. However, the automatically
generated templates were outperformed by the
manually created ones. Furthermore, they state that
manually created templates can be equipped with
messages provided to the student as feedback in a
tutoring system. For evaluating their system, they
used factual science questions and the corresponding
192
student answers from GCSE tests. 200 graded
answers for each of nine questions served as a
training set, while another 60 answers served as a
test set. They report that their system achieves an
accuracy of 84%. With inconsistencies in the human
grading removed, it achieves 93%. However, they do
not report on the level of inter-grader agreement or
on a random baseline.
Pe?rez et al (2005) present the Atenea system,
a combined approach that makes use of Latent
Semantic Analysis (LSA, Landauer et al 1998) and
n-gram overlap. While n-gram overlap supports
comparing target responses and student responses
with differing word order, it does not deal with
synonyms and related terms. Hence, they use LSA to
add a component that deals with semantic relatedness
in the comparison step. As a test corpus, they
collected nine different questions from computer
science exams. A tenth question ?[consists] of a
set of definitions of ?Operating System? obtained
from the Internet.? Altogether, they gathered 924
student responses and 44 target responses written
by teachers. Since their LSA module had been
trained on English but their data were in Spanish,
they chose to use Altavista Babelfish to translate the
data into English. They do not provide information
about the distribution of scores and about inter-grader
agreement. Atenea achieves a Pearson?s correlation
of r = 0.554 with the scores in the gold standard.
The approach by Makatchev and VanLehn (2007),
which we refer to as the Logic-based System,
enters the landscape from the direction of artificial
intelligence. It is related to CarmelTC and its
dataset, but follows a different route: target
responses are manually encoded in first-order
predicate language. Similar logic representations
are constructed automatically for student answers.
They explore various strategies for matching these
two logic representation on the basis of 16 semantic
classes. In an evaluation experiment, they tested the
system on 293 ?natural language utterances? with
ten-fold cross validation. The test data are skewed
towards the ?empty? label that indicates that none
of the 16 semantic labels could be attached. They
do not report on other properties of the dataset such
as number of annotators or number of questions to
which the student answers were given. Their winning
configuration yields a F-measure value of 0.4974.
While Makatchev and VanLehn (2007) position their
approach in the context of the Why2 tutorial dialogue
system, their use of semantic classes seems to make
them more related to meaning comparison than to
grading.
The Content Assessment Module (CAM) pre-
sented in Bailey (2008) and Bailey and Meurers
(2008) utilizes an approach that is different from
the systems discussed so far: Following a three-step
strategy, the system first automatically generates
linguistic annotations for questions, target responses
and student responses. In an alignment phase, these
annotations are then used to map from elements
(words, lemmas, chunks, dependency triples) in
the student responses to elements in the target
responses. Finally, a machine learning classifier
judges on the basis of this alignment, whether
or not the student has answered the question
correctly. The data used for evaluation was made
available as the Corpus of Reading Comprehension
Exercises in English (CREE, Meurers et al 2011a).
This corpus consists of 566 responses produced
by intermediate ESL learners at The Ohio State
University as part of their regular assignments.
Students had access to their textbooks and typically
answered questions in one to three sentences. All
responses were labelled as either appropriate or
inappropriate by two independent annotators, along
with a detailed diagnosis code specifying the nature
of the inappropriateness (missing concept, extra
concept, blend, non-answer). In leave-one-out
evaluation on the development set containing 311
responses to 47 different questions, CAM achieved
87% accuracy on the binary judgment (response
correct/incorrect). For the test set containing 255
responses to 28 questions, the approach achieved
88%. However, the distribution of categories in the
data is heavily skewed with 71% of the responses
marked as correct in the development set and 84% in
the test set. The best result obtained on a balanced
set with leave-one-out-testing is 78%. Meurers et
al. (2011a) present a re-implementation of CAM
called CoMiC-EN (Comparing Meaning in Context
in English), achieving an accuracy of 87.6% on the
CREE development set and 88.4% on the test set.
With their Facets System, Nielsen et al (2009)
establish a connection to the field of Recognizing
Textual Entailment (RTE, Dagan et al 2009). In
193
a number of friendly challenges, RTE research has
spawned numerous systems that try to automatically
answer the following question: Given a text and a
hypothesis, is the hypothesis entailed by the text?
Short answers assessment can be seen as a RTE task
in which the target response corresponds to the text
and the student response to the hypothesis. Nielsen et
al. (2009) base their system on what they call facets.
These facets are meaning representations of parts
of sentences. They are constructed automatically
from dependency and semantic parses of the target
responses. Each facet in the target response is then
looked up in the corresponding student response
and equipped with one of five labels2 ranging from
unaddressed (the student did not mention the fact
in this facet) to expressed (the student named the
fact). This step is taken via machine learning.
From a tutoring system in real-life operation, they
gathered responses from third- to sixth-grade students
answering questions for science classes. Two
annotators worked on these data, producing 142,151
facets. Furthermore, all facets were looked up in
the corresponding student responses and annotated
accordingly, using the mentioned set of labels. The
best result of the Facets System is 75.5% accuracy on
one of the held-out test sets. With ten-fold cross
validation on the training set, it achieves 77.1%
accuracy. The majority label baselines are 51.1% and
54.6% respectively. Providing this more fine-grained
analysis of facets that are searched for in student
responses, Nielsen et al (2009) claim to ?enable
more intelligent dialogue control? in tutoring systems.
From the point of view of grading vs. meaning
comparison, their approach can be counted towards
the latter, since their labels can be conflated to
produce a single yes/no decision.
Another recent approach is described by Mohler et
al. (2011), hereafter referred to as the Texas system.
Student responses and target responses are annotated
using a dependency parser. Thereupon, subgraphs of
the dependency structures are constructed in order to
map one response to the other. These alignments
are generated using machine learning. Dealing
with subgraphs allows for variation in word order
between the two responses that are to be compared.
2In human annotation, they use eight labels, which are
grouped into five broader categories as used by their system.
In order to account for meaning, they combine
lexical semantic similarity with the aforementioned
alignment. They make use of several WordNet-based
measures and two corpus-based measures, namely
Latent Semantic Analysis and Explicit Semantic
Analysis (ESA, Gabrilovich and Markovitch 2007).
For evaluating their system, Mohler et al (2011)
collected student responses from an online learning
environment. 80 questions from ten introductory
computer science assignments spread across two
exams were gathered together with 2,273 student
responses. These responses were graded by two
human judges on a scale from zero to five. The
judges fully agreed in 57% of all cases, their
Pearson correlation computes to r = 0.586. The
gold standard has been created by computing the
arithmetic mean of the two judgments for each
response. The Texas system achieves r = 0.518 and
a Root Mean Square Error of 0.978 as its best result.
Mohler et al (2011) mention that ?[t]he dataset is
biased towards correct answers?. Data are publicly
available. We used these in an evaluation experiment
with the CoMiC-EN system, discussed in Section 3.
While almost all short answer assessment research
has targeted answers written in English, there are
two recent approaches dealing with German answers.
The CoMiC-EN reimplementation of CAM discussed
above was motivated by the need for a modular
architecture supporting a transfer of the system to
German, resulting in its counterpart named CoMiC-
DE (Meurers et al, 2011b). The German system
utilizes the same strategies as the English one,
but with language-dependent processing modules
being replaced. Meurers et al (2011b) evaluated
CoMiC-DE on a subset of the Corpus of Reading
Comprehension Questions in German (CREG, Ott et
al. 2012), collected in collaboration with the German
programs at The Ohio State University and the
University of Kansas. Like in CREE, all responses
are rated by two annotators with both binary and
detailed diagnosis codes.3 The aforementioned
subset contains 1,032 learner responses and 223
target responses to 177 questions. Furthermore, it
features an even distribution of correct and incorrect
answers according to the judgement of two human
3In CREG, correct answers as well as incorrect ones can be
labelled with missing concept, extra concept, or blend.
194
annotators. On that subset, CoMiC-DE achieved an
accuracy of 84.6% in the binary classification task.
CREG is freely available for research purposes under
a Creative Commons by-nc-sa license.
Hahn and Meurers (2012) present the CoSeC-DE
approach based on Lexical Resource Semantics
(LRS, Richter and Sailer 2003). In a first step,
they create LRS representations from POS-tagged
and dependency-parsed data. These underspecified
LRS representations of student responses and target
responses are then aligned. Using A* as heuristic
search algorithm, a best alignment is computed and
equipped with a numeric score representing the
quality of the alignment of the formulae. If this
best alignment scores higher than a threshold, the
system judges student response and target response
to convey the same meaning. The alignment
and comparison mechanism does not utilize any
linguistic representations other than the LRS
semantic formulae. These semantic representations
abstract away from surface features, e.g., by treating
active and passive voice equally. Hahn and Meurers
(2012) claim that that ?[semantic representations]
more clearly expose those distinction which do make
a difference in meaning.? They evaluate the approach
on the above-mentioned subset of CREG containing
1,032 learner responses and report an accuracy of
86.3%.
3 A concrete system comparison
After discussing the broad landscape of Short Answer
Evaluation systems, the main characteristics and
differences, we now turn to a comparison of two
concrete systems, namely CoMiC-EN (Meurers
et al, 2011a) and the Texas system Mohler et
al. (2011), to explore what is involved in such a
concrete comparison of two systems from different
contexts. While CoMiC-EN was developed with
meaning comparison in mind, the purpose of the
Texas system is answer grading. We pick these
two systems because they constitute recent and
interesting instances of their respective fields and
the corresponding data are freely available.
3.1 Data
In evaluating the Texas system, Mohler et al (2011)
used a corpus of ten assignments and two exams from
an introductory computer science class. In total, the
Texas corpus consists of 2,442 responses, which were
collected using an online learning platform. Each
response is rated by two annotators with a numerical
grade on a 0?5 scale. Annotators were not given any
specific instructions besides the scale itself, which
resulted in an exact agreement of 57.7%. In order to
arrive at a gold standard rating, the numerical average
of the two ratings was computed. The data exist in
raw, sentence-segmented and parsed versions and are
freely available for research use. Table 2 presents
a breakdown of the score counts and distribution
statistics of the Texas corpus. A bias towards correct
answers can be observed, which is also mentioned by
Mohler et al (2011).
Score #
0.000 24
0.500 3
1.000 23
1.500 46
1.750 1
2.000 93
2.250 2
2.500 125
3.000 164
Score #
3.250 1
3.500 187
3.625 1
3.750 1
4.000 220
4.125 2
4.500 310
4.750 1
5.000 1238
x = 4.19, s = 1.11
Table 2: Details on the gold standard scores in the Texas
corpus. Non-integer scores result from averaging between
raters and normalization onto the 0?5 scale.
3.2 Approaches
CoMiC-EN uses a three-step approach to meaning
comparison. Annotation uses NLP to enrich the
student and target answers, as well as the question
text, with linguistic information on different levels
(words, chunks, dependency triples) and types of
abstraction (tokens, lemmas, distributional vectors,
etc.). Alignment maps elements of the learner answer
to elements of the target response using annotation.
The global alignment solution is computed using the
Traditional Marriage Algorithm (Gale and Shapley,
1962). Finally, Classification analyzes the possible
alignments and labels the learner response with a
binary or detailed diagnosis code. The features used
in the classification step are shown in Table 3.
For the Texas system, Mohler et al (2011) used a
combination of bag-of-words (BOW) features and
195
Features Description
1. Keyword Overlap Percent of keywords aligned
(relative to target)
2./3. Token Overlap Percent of aligned
target/learner tokens
4./5. Chunk Overlap Percent of aligned
target/learner chunks
6./7. Triple Overlap Percent of aligned
target/learner triples
8. Token Match Percent of token alignments
that were token-identical
9. Similarity Match Percent of token alignments
that were similarity-resolved
10. Type Match Percent of token alignments
that were type-resolved
11. Lemma Match Percent of token alignments
that were lemma-resolved
12. Synonym Match Percent of token alignments
that were synonym-resolved
13. Variety of Match Number of kinds of
(0-5) token-level alignments
Table 3: Features used in the CoMiC-EN system
dependency graph alignment in connection with
two different machine learning approaches. Among
the BOW features are WordNet-based similarity
measures such as the one by Lesk (1986) and vector
space measures such as tf ? idf (Salton and McGill,
1983) and the more advanced LSA (Landauer et al,
1998). The dependency graph alignment approach
builds on a node-to-node matching stage which
computes a score for each possible match between
nodes of the student and target response. In the next
stage, the optimal graph alignment is computed based
on the node-to-node scores using the Hungarian
algorithm.
Mohler et al (2011) also employ a technique
they call ?question demoting?, which refers to the
exclusion of words from the alignment process
if they already appeared in the question string.
Incidentally, the technique is also used in the earlier
CAM system (Bailey and Meurers, 2008), but called
?Givenness filter? there, following the long tradition
of research on givenness (Schwarzschild, 1999) as a
notion of information structure investigated in formal
pragmatics.
To produce the final system score, the Texas
system uses two machine learning techniques based
on Support Vector Machines (SVMs), SVMRank and
Support Vector Regression (SVR). Both techniques
are trained with several combinations of the
dependency alignment and BOW features. While
with SVR one trains a function to produce a score on
the 0?5 scale itself, SVMRank produces a ranking of
student answers which does not produce a 0?5 grade.
Therefore, Mohler et al (2011) employ isotonic
regression to map the ranking to the 0?5 scale.
In terms of performance, Mohler et al (2011)
report that the SVMRank system produces a better
correlation measure (r = 0.518) while the SVR
system yields a better RMSE (0.978).
3.3 Evaluation
We now turn to the evaluation of CoMiC-EN on the
Texas corpus as it is a publicly available dataset. As
mentioned before, CoMiC-EN performs meaning
comparison based on a system of categories while
the Texas system is a scoring approach, trying to
predict a grade. While the former is a classification
task, the latter is better characterized as a regression
problem because of the desired numerical outcome.
Of course, one could simply pretend that individual
grades are classes and treat scoring as a classification
task. However, a classification approach has no
knowledge of numerical relationships, i.e., it does
not ?know? that 4 is a higher grade than 3 and a
much higher grade than 1 (assuming a 0?5 scale).
As a result, if an evaluation metric such as Pearson
correlation is used, classification systems are at a
disadvantage because some misclassifications are
punished more than others. We discuss this point
further in Section 4.
For these reasons, to obtain a more interesting
comparison, we modified CoMiC-EN to perform
scoring instead of meaning comparison. This means
that the memory-based learning approach CoMiC-
EN had employed so far was no longer applicable and
had to be replaced with a regression-capable learning
strategy. We chose Support Vector Regression (SVR)
using libSVM4 since that is one of the methods
employed by Mohler et al (2011). However, all other
parts of CoMiC-EN such as the processing pipeline
and the alignment approach and the extracted features
remained the same.
4http://www.csie.ntu.edu.tw/?cjlin/
libsvm
196
The evaluation procedure was carried out as a
12-fold cross-validation due to the 12 assignments
in the Texas corpus. For each fold, one complete
assignment was held out as test set. Parameters for
the SVR were determined using a grid search using
the tools provided with libSVM. As kernel function,
we used a linear kernel as it was also used in the
evaluation of the Texas system and thus constitutes
a vital part of the evaluation setup. In general, we
designed to evaluation procedure to be as close as
possible to the Texas one.
Table 4 presents detailed results on the 12 folds
as well as the overall results and a baseline which
always predicts the median value 5.
Assignment # responses r RMSE
1 203 0.416 0.958
2 210 0.349 1.221
3 217 0.335 0.969
4 210 0.338 1.212
5 112 0.010 1.030
6 182 0.646 0.702
7 182 0.265 0.991
8 189 0.521 0.942
9 189 0.220 0.942
10 168 0.699 0.990
11 (exam) 300 0.436 1.076
12 (exam) 280 0.619 1.165
Median Baseline 2442 ? 1.375
Overall 2442 0.405 1.016
Table 4: Detailed results of CoMiC-EN on Texas corpus
The CoMiC-EN system on the Texas data set does
not quite reach the level achieved by the Texas system
on their data set. We obtained a Pearson correlation
of r = 0.405 and an RMSE of 1.016 over all 12 folds.
However, let us keep in mind the objective of this
experiment as exemplifying the process needed to
directly compare two systems from different research
strands on the same dataset.
4 Comparability of approaches & datasets
It seems clear that for systems to be comparable
and results to be reproducible, datasets must be
publicly available, as is the case with the Texas
corpus. However, data availability alone does not
ensure meaningful comparison. Depending on the
context the corpus was drawn from, datasets will
differ just like the corresponding systems:
? Data source: Reading comprehension task in
language learning setting, language tutoring
context, automated grading of short answer
exams
? Language properties: Native vs. learner
language, domain-specific language (e.g., com-
puter science)
? Assessment scheme: nominal vs. interval scale
Especially the last point deserves some further
discussion. Depending on the kind of assessment
scheme, which in turn is motivated by the task,
different evaluation methods may be chosen. Scoring
systems are often evaluated using a correlation metric
in order to capture the systems? tendency to assign
similar but not necessary equal grades as the human
raters. Conversely, with category-based schemes one
usually reports accuracy, which expresses how many
items were classified correctly.
The question that arises is how a system coming
from one paradigm can be compared to one from
the other paradigm in a meaningful way. One might
argue that the tasks are simply too different: scoring
might take form errors into account while meaning
comparison by definition does not. Moreover,
while classification labels say something explicit and
absolute about a piece of data, grades by definition
are relative to the scale they come from. It thus seems
impossible to somehow unify the two schemes as they
express fundamentally different ideas.
However, the strategies systems use to tackle
scoring or meaning comparison are undoubtedly
similar and should be comparable, as we argue in this
paper. So in order for researchers to learn from other
approaches and also compare their results to those of
other systems which tackle a different task, changes
to systems seem necessary and should be preferred
over changes to the gold standard data. In the case
presented here, a meaning comparison system was
turned into a scoring system by changing the machine
learning component from classification to regression,
which requires a certain level of system modularity.
Having compared the two systems using Pearson
correlation and RMSE, it also makes sense to
consider the relevance of these evaluation metrics.
For example, it is the case that pairwise correlation
assumes a normal distribution whereas datasets like
197
the Texas corpus are heavily skewed towards correct
answers (see Table 2). Mohler et al (2011) also note
that in distributions with zero variance, correlation is
undefined, which is not a problem as such but limits
the use of correlation as evaluation metric. Mohler
et al (2011) propose that RMSE is better suited to
the task since it captures the relative error a system
makes when trying to predict scores. However,
RMSE is scale-dependent and thus RMSE values
across different studies cannot be compared. We
can only suggest that in order to sufficiently describe
a system?s performance, several metrics need to be
reported.
Finally, an important point concerns the quality
of gold standards. Given the relatively low inter-
annotator agreement in the Texas corpus (r =
0.586, RMSE = 0.659) it seems fair to ask whether
answers without perfect agreement should be used in
training and testing systems at all. In the CREE
and CREG corpora, answers with disagreement
among the annotators have either been excluded
from experiments or resolved by an additional judge.
This approach is also supported by recent literature
(cf., e.g., Beigman and Beigman Klebanov 2009;
Beigman Klebanov and Beigman 2009). However,
for the Texas corpus, Mohler et al (2011) have opted
to use the arithmetic mean of the two graders as gold
standard. While mathematically a viable solution,
it seems questionable whether the mean is reliable
with only two graders, especially if they have not
operated on the grounds of explicit guidelines. It
would be interesting to see whether in this case, a
system trained on more, singly annotated data would
perform better than one on less, doubly annotated
data, as argued for by Dligach et al (2010). In any
case, if many disagreements occur, one should ask
the question whether the annotation task is defined
well enough and whether machines should really be
expected to perform it consistently if humans have
trouble doing so.
5 Conclusion
We discussed several issues in the comparison of
short answer evaluation systems. To that end, we
gave an overview of the existing systems and picked
two for a concrete comparison on the same data, the
CoMiC-EN system (Meurers et al, 2011a) and the
Texas system (Mohler et al, 2011). In comparing
the two, it was necessary to turn CoMiC-EN into
a scoring system because the Texas corpus as
the chosen gold standard contains numeric scores
assigned by humans. Taking a step back from
the concrete comparison, we gave a more general
description of what is necessary to compare short
answer evaluation systems. We observed that more
datasets need to be publicly available in order for
performance comparisons to have meaning, a point
also made earlier by Pulman and Sukkarieh (2005).
Moreover, we noted how datasets differ in similar
aspects as systems do, such as task context and
assessment scheme. We then criticized the use of
correlation measures as evaluation metrics for short
answer scoring. Finally, we discussed the importance
of gold standard quality.
We conclude that it is interesting and relevant
to compare short answer evaluation systems even
if the concrete task they tackle, such as grading or
meaning comparison, is not the same. However, the
availability and quality of the datasets will decide
to what extent systems can sensibly be compared.
For progress to be made in this area, more publicly
available datasets and systems are needed. The
upcoming SemEval-2013 task on ?Textual entailment
and paraphrasing for student input assessment?5
will hopefully become one important step into this
direction (see also Dzikovska et al 2012).
Acknowledgements
We are grateful to the three anonymous BEA
reviewers for their detailed and helpful comments.
References
Lyle Bachman, Nathan Carr, Greg Kamei, Mikyung Kim,
Michael Pan, Chris Salvador, and Yasuyo Sawaki.
2002. A reliable approach to automatic assessment of
short answer free responses. In Proceedings of the 19th
International Conference on Computational Linguistics
(COLING 2002), pages 1?4.
Stacey Bailey and Detmar Meurers. 2008. Diagnosing
meaning errors in short answers to reading compre-
hension questions. In Joel Tetreault, Jill Burstein,
and Rachele De Felice, editors, Proceedings of the
5http://www.cs.york.ac.uk/semeval-2013/
task4/
198
3rd Workshop on Innovative Use of NLP for Building
Educational Applications (BEA-3) at ACL?08, pages
107?115, Columbus, Ohio.
Stacey Bailey. 2008. Content Assessment in Intelligent
Computer-Aided Language Learning: Meaning Error
Diagnosis for English as a Second Language. Ph.D.
thesis, The Ohio State University.
Eyal Beigman and Beata Beigman Klebanov. 2009.
Learning with annotation noise. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP, volume 1, pages
280?287. Association for Computational Linguistics.
Beata Beigman Klebanov and Eyal Beigman. 2009. From
annotator agreement to noise models. Computational
Linguistics, 35(4):495?503.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: Rational,
evaluation and approaches. Natural Language
Engineering, 15(4):i?xvii, 10.
Dmitriy Dligach, Rodney D. Nielsen, and Martha Palmer.
2010. To annotate more accurately or to annotate more.
In Proceedings of the Fourth Linguistic Annotation
Workshop, LAW IV ?10, pages 64?72, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback
for explanation questions: A dataset and baselines.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computational
Linguistics: Human Language Technologies.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-based
explicit semantic analysis. In Proceedings of the
20th International Joint Conference on Artificial
Intelligence, pages 6?12.
David Gale and Lloyd S. Shapley. 1962. College
admissions and the stability of marriage. American
Mathematical Monthly, 69:9?15.
Michael Hahn and Detmar Meurers. 2012. Evaluating
the meaning of answers to reading comprehension
questions: A semantics-based approach. In Pro-
ceedings of the 7th Workshop on Innovative Use of
NLP for Building Educational Applications (BEA-7) at
NAACL-HLT 2012, Montreal.
Thomas Landauer, Peter Foltz, and Darrell Laham. 1998.
An introduction to latent semantic analysis. Discourse
Processes, 25:259?284.
Claudia Leacock and Martin Chodorow. 2003. C-
rater: Automated scoring of short-answer questions.
Computers and the Humanities, 37:389?405.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a
pine cone from an ice cream cone. In Proceedings
of the 5th annual international conference on Systems
documentation, pages 24?26, Toronto, Ontario,
Canada.
Maxim Makatchev and Kurt VanLehn. 2007. Combining
baysian networks and formal reasoning for semantic
classification of student utterances. In Proceedings
of the International Conference on AI in Education
(AIED), Los Angeles, July.
Detmar Meurers, Ramon Ziai, Niels Ott, and Stacey
Bailey. 2011a. Integrating parallel analysis modules
to evaluate the meaning of answers to reading
comprehension questions. IJCEELL. Special Issue on
Automatic Free-text Evaluation, 21(4):355?369.
Detmar Meurers, Ramon Ziai, Niels Ott, and Janina Kopp.
2011b. Evaluating answers to reading comprehension
questions in context: Results for german and the
role of information structure. In Proceedings of the
TextInfer 2011 Workshop on Textual Entailment, pages
1?9, Edinburgh, Scotland, UK, July. Association for
Computational Linguistics.
Tom Mitchell, Nicola Aldrige, and Peter Broomhead.
2003. Computerized marking of short-answer
free-text responses. Paper presented at the 29th
annual conference of the International Association for
Educational Assessment (IAEA), Manchester, UK.
Michael Mohler, Razvan Bunescu, and Rada Mihalcea.
2011. Learning to grade short answer questions
using semantic similarity measures and dependency
graph alignments. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
752?762, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2009. Recognizing entailment in intelligent tutoring
systems. Natural Language Engineering, 15(4):479?
501.
Niels Ott, Ramon Ziai, and Detmar Meurers. 2012.
Creation and analysis of a reading comprehension
exercise corpus: Towards evaluating meaning in
context. In Thomas Schmidt and Kai Wo?rner,
editors, Multilingual Corpora and Multilingual Corpus
Analysis, Hamburg Studies in Multilingualism (HSM).
Benjamins, Amsterdam. to appear.
Diana Pe?rez, Enrique Alfonseca, Pilar Rodr??guez, Alfio
Gliozzo, Carlo Strapparava, and Bernardo Magnini.
2005. About the effects of combining latent semantic
analysis with natural language processing techniques
for free-text assessment. Revista signos, 38(59):325?
343.
Stephen G. Pulman and Jana Z. Sukkarieh. 2005.
Automatic short answer marking. In Jill Burstein and
Claudia Leacock, editors, Proceedings of the Second
199
Workshop on Building Educational Applications Using
NLP, pages 9?16, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Frank Richter and Manfred Sailer. 2003. Basic concepts
of lexical resource semantics. In Arnold Beckmann
and Norbert Preining, editors, ESSLLI 2003 ? Course
Material I, volume 5 of Collegium Logicum, pages
87?143, Wien. Kurt Go?del Society.
Carolyn Penstein Rose?, Antonio Roque, Dumisizwe
Bhembe, and Kurt VanLehn. 2003. A hybrid
approach to content analysis for automatic essay
grading. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology: companion volume of the Proceedings
of HLT-NAACL 2003?short papers - Volume 2,
NAACL-Short ?03, pages 88?90, Edmonton, Canada.
Association for Computational Linguistics.
Gerard Salton and Michael J. McGill. 1983. Introduction
to modern information retrieval. McGraw-Hill, New
York.
Roger Schwarzschild. 1999. GIVENness, AvoidF and
other constraints on the placement of accent. Natural
Language Semantics, 7(2):141?177.
Kurt VanLehn, Pamela W. Jordan, Carolyn Penstein Rose?,
Dumisizwe Bhembe, Michael Boettner, Andy Gaydos,
Maxim Makatchev, Umarani Pappuswamy, Micheal
Ringenberg, Antonio Roque, Stephanie Siler, and
Ramesh Srivastava. 2002. The architecture of why2-
atlas: A coach for qualitative physics essay writing.
In Proceedings of the 6th International Conference
on Intelligent Tutoring Systems, volume 2363, pages
158?167, Biarritz, France and San Sebastian, Spain,
June 2-7. Springer LNCS.
200
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 208?215,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Informing Determiner and Preposition Error Correction with Word Clusters
Adriane Boyd Marion Zepf Detmar Meurers
Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
{adriane,mzepf,dm}@sfs.uni-tuebingen.de
Abstract
We extend our n-gram-based data-driven pre-
diction approach from the Helping Our Own
(HOO) 2011 Shared Task (Boyd and Meur-
ers, 2011) to identify determiner and preposi-
tion errors in non-native English essays from
the Cambridge Learner Corpus FCE Dataset
(Yannakoudakis et al, 2011) as part of the
HOO 2012 Shared Task. Our system focuses
on three error categories: missing determiner,
incorrect determiner, and incorrect preposi-
tion. Approximately two-thirds of the errors
annotated in HOO 2012 training and test data
fall into these three categories. To improve
our approach, we developed a missing deter-
miner detector and incorporated word cluster-
ing (Brown et al, 1992) into the n-gram pre-
diction approach.
1 Introduction
We extend our n-gram-based prediction approach
(Boyd and Meurers, 2011) from the HOO 2011
Shared Task (Dale and Kilgarriff, 2011) for the HOO
2012 Shared Task. This approach is an extension
of the preposition prediction approach presented
in Elghafari, Meurers and Wunsch (2010), which
uses a surface-based approach to predict preposi-
tions in English using frequency information from
web searches to choose the most likely preposition
in a given context. For each preposition in the text,
the prediction algorithm considers up to three words
of context on each side of the preposition, building
a 7-gram with a preposition slot in the middle:
rather a question the scales falling
For each prediction task, a cohort of queries is con-
structed with each of the candidate prepositions in
the slot to be predicted:
1. rather a question of the scales falling
2. rather a question to the scales falling
3. rather a question in the scales falling
. . .
9. rather a question on the scales falling
In Elghafari, Meurers and Wunsch (2010), the
queries are submitted to the Yahoo search engine
and in Boyd and Meurers (2011), the search engine
is replaced with the ACL Anthology Reference Cor-
pus (ARC, Bird et al, 2008), which contains texts of
the same genre as the HOO 2011 data. If no hits are
found for any of the 7-gram queries, shorter over-
lapping n-grams are used to approximate the 7-gram
query. For instance, a 7-gram may be approximated
by two overlapping 6-grams:
[rather a question of the scales falling]
?
[rather a question of the scales]
[a question of the scales falling]
If there are still no hits, the overlap backoff will
continue reducing the n-gram length until it reaches
3-grams with one word of context on each side of
the candidate correction. If no hits are found at
the 3-gram level, the Boyd and Meurers (2011) ap-
proach predicts the original token, effectively mak-
ing no modifications to the original text. The ap-
proach from Elghafari, Meurers and Wunsch (2010),
addressing a prediction task rather than a correction
task (i.e., the original token is masked), predicted the
most frequent preposition of if no hits were found.
208
Elghafari, Meurers and Wunsch (2010) showed
this surface-based approach to be competitive with
published state-of-the-art machine learning ap-
proaches using complex feature sets (Gamon et al,
2008; De Felice, 2008; Tetreault and Chodorow,
2008; Bergsma et al, 2009). For a set of nine fre-
quent prepositions (of, to, in, for, on, with, at, by,
from), they accurately predicted 76.5% on native
data from section J of the British National Corpus.
For these nine prepositions, De Felice (2008) iden-
tified a baseline of 27% for the task of choosing
a preposition in a slot (choose of ) and her system
achieved 70.1% accuracy. Humans performing the
same task agree 89% of the time (De Felice, 2008).
For the academic texts in the HOO 2011 Shared
Task, Boyd and Meurers (2011) detected 67% of de-
terminer and preposition substitution errors (equiva-
lent to detection recall in the current task) and pro-
vided the appropriate correction for approximately
half of the detected cases. We achieved a detection
F-score of approximately 80% and a correction F-
score of 44% for the four function word prediction
tasks we considered (determiners, prepositions, con-
junctions, and quantifiers).
2 Our Approach
For the 2012 shared task corpus, we do not have
the advantage of access to a genre-specific reference
corpus such as the ARC used for the first challenge,
so we instead use the Google Web 1T 5-gram Cor-
pus (Web1T5, Brants and Franz, 2006), which con-
tains 1-gram to 5-gram counts for a web corpus with
approximately 1 trillion tokens and 95 billion sen-
tences. Compared to our earlier approach, using the
Web1T5 corpus reduces the size of available context
by going from 7-grams to 5-grams, but we are inten-
tionally keeping the corpus resources and algorithm
simple. We are particularly interested in exploring
the space between surface forms and abstractions
by incorporating information from word clustering,
an issue which is independent from the choice of a
more sophisticated learning algorithm.
Rozovskaya and Roth (2011) compared a range of
learning algorithms for the task of correcting errors
made by non-native writers, including an averaged
perceptron algorithm (Rizzolo and Roth, 2007) and
an n-gram count-based approach (Bergsma et al,
2009), which is similar to our approach. They found
that the count-based approach performs nearly as
well as the averaged perceptron approach when
trained with ten times as much data. Without access
to a large multi-genre corpus even a tenth the size
of the Web1T5 corpus, we chose to use Web1T5.
Our longest queries thus are 5-grams with at least
one word of context on each side of the candidate
function word and the shortest are 3-grams with
one word of context on each side. A large multi-
genre corpus would improve the results by support-
ing access to longer n-grams, and it would also make
deeper linguistic analysis such as part-of-speech tag-
ging feasible.
Table 1 shows the sets of determiners and prepo-
sitions for each of the three categories addressed by
our system: missing determiner (MD), incorrect de-
terminer (RD), and incorrect preposition (RT). The
function word lists are compiled from all single-
word corrections of these types in the training data.
The counts show the frequency of the error types in
the test data, along with the total frequency of func-
tion word candidates.
The following sections describe the main exten-
sions to our system for the 2012 shared task: a sim-
ple correction probability model, a missing deter-
miner detector, and the addition of hierarchical word
clustering to the prediction approach.
2.1 Correction Probability Model
To adapt the system for the CLC FCE learner data,
we added a simple correction probability model to
the n-gram predictor that multiplies the counts for
each n-gram by the probability of a particular re-
placement in the training data. The model includes
both correct and incorrect occurrences of each can-
didate, ignoring any corrections that make up less
than 0.5% of the corrections for a particular token.
For instance, the word among has the following cor-
rection probabilities: among 0.7895, from 0.1053,
between 0.0526. Even such a simplistic probability
model has a noticeable effect on the system perfor-
mance, improving the overall correction F-score by
approximately 3%. The preposition substitution er-
ror detection F-score alone improves by 9%.
Prior to creating the probability model, we exper-
imented with the addition of a bias toward the origi-
nal token, which we hoped would reduce the number
209
Category # Errors Candidate Corrections # Occurrences
Original Revised
MD 125 131 a, an, another, any, her, his, its, my, our, that,
the, their, these, this, those, which, your
-
RD 39 37 a, an, another, any, her, his, its, my, our, that,
the, their, these, this, those, which, your
1924
RT 136 148 about, after, against, along, among, around, as,
at, before, behind, below, between, by,
concerning, considering, during, for, from, in,
into, like, near, of, off, on, onto, out, outside,
over, regarding, since, through, throughout, till,
to, toward, towards, under, until, via, with,
within, without
2202
Table 1: Single-Word Prepositions and Determiners with Error and Overall Frequency in Test Data
of overcorrections generated by our system. With-
out the probability model, a bias toward the original
token improves the results, however, with the prob-
ability model, the bias is no longer useful.
2.2 Word Clustering
In the 2011 shared task, we observed that data spar-
sity issues are magnified in non-native texts because
the n-gram context may contain additional errors
or other infrequent or unusual n-gram sequences.
We found that abstracting to part-of-speech tags
and lemmas in certain contexts leads to small im-
provements in system performance. For the 2012
shared task, we explore the effects of abstracting to
word clusters derived from co-occurrence informa-
tion (Brown et al, 1992), another type of abstraction
relevant to our n-gram prediction approach. We hy-
pothesize that replacing tokens in the n-gram context
in our prediction tasks with clusters will reduce the
data sparsity for non-native text.
Clusters derived from co-occurrence frequencies
offer an attractive type of abstraction that occupy
a middle ground between relatively coarse-grained
morphosyntactic abstractions such as part-of-speech
tags and fine-grained abstractions such as lemmas.
For determiner and preposition prediction, part-of-
speech tags clearly retain too few distinctions. For
example, the choice of a/an before a noun phrase de-
pends on the onset of the first word in the phrase, in-
formation which is not preserved by part-of-speech
tagging. Likewise, preposition selection may be de-
pendent on lexical specifications (e.g., phrasal verbs
such as depend on) or on semantic or world knowl-
edge (cf. Wechsler, 1994).
Brown et al (1992) present a hierarchical word
clustering algorithm that can handle a large num-
ber of classes and a large vocabulary. The algorithm
clusters a vocabulary into C clusters given a corpus
to estimate the parameters of an n-gram language
model. Summarized briefly, the algorithm first cre-
ates C clusters for the C most frequent words in
the corpus. Then, a cluster is added containing the
next most frequent word. After the new cluster is
added, the pair of clusters is merged for which the
loss in average mutual information is smallest, re-
turning the number of clusters to C. The remaining
words in the vocabulary are added one by one and
pairs of clusters are merged in the same fashion un-
til all words have been divided into C clusters.
Using the implementation from Liang (2005),1
we generate word clusters for the most frequent
100,000 tokens in the ukWaC corpus (Baroni et al,
2009). We convert all tokens to lower case, replace
all lower frequency words with a single unique to-
ken, and omit from the clustering the candidate cor-
rections from Table 1 along with the low frequency
tokens. Our corpus is the first 18 million sentences
from ukWaC.2 After converting all tokens to lower-
case and omitting the candidate function words, a
total of 75,333 tokens are clustered.
We create three sets of clusters with sizes 500,
1000, and 2000. Due to time constraints, we did not
yet explore larger sizes. Brown et al (1992) report
that the words in a cluster appear to share syntac-
tic or semantic features. The clusters we obtained
appear to be overwhelmingly semantic in nature.
1Available at http://cs.stanford.edu/?pliang/software
2Those sentences in the file ukwac dep parsed 01.
210
Cluster ID Selected Cluster Members
(1) 00100 was..., woz, wasn?t, was, wasnt
(2) 0111110111101 definetly, definatly, assuredly, definately, undoubtedly, certainly, definitely
(3) 1001110100 extremely, very, incredibly, inordinately, exceedingly, awfully
(4) 1110010001 john, richard, peter, michael, andrew, david, stephen
(5) 11101001001 12.30pm, 7am, 2.00pm, 4.00pm, weekday, tuesdays
Table 2: Sample Clusters from ukWaC with 2000 Clusters
Table 2 shows examples from the set of 2000 clus-
ters. Examples (1) and (2) show how tokens with
errors in tokenization or misspellings are clustered
with tokens with standard spelling and standard tok-
enization. Such clusters may be useful for the shared
task by allowing the system to abstract away from
spelling errors in the learner essays. Examples (3)?
(5) show semantically similar clusters.
An excerpt of the hierarchical cluster tree for the
cluster ID from example (3) is shown in Figure 1.
The tree shows a subset of the clusters for cluster
IDs beginning with the sequence 1001110. Each bi-
nary branch appends a 0 or 1 to the cluster ID as
shown in the edge labels. The cluster 1001110100
(extremely, very) is found in the left-most leaf of
the right branch. A few of the most frequent clus-
ter members are shown for each leaf of the tree.
In our submissions to the shared task, we included
five different cluster settings: 1) using the original
word-based approach with no clusters, 2) using only
2000 clusters, 3) using the word-based approach ini-
tially and backing off to 2000 clusters if no hits are
found, 4) backing off to 1000 clusters, and 5) back-
ing off to 500 clusters. The detailed results will be
presented in section 3.
2.3 Missing Determiner Detector
We newly developed a missing determiner detector
to identify those places in the learner text where
a determiner is missing. Since determiners mostly
occur in noun phrases, we extract all noun phrases
from the text and put them through a two-stage clas-
sifier. For a single-stage classifier, always predict-
ing ?no error? leads to a very high baseline accu-
racy of 98%. Therefore, we first filter out those
noun phrases which already contain a determiner, a
possessive pronoun, another possessive token (e.g.,
?s), or an existential there, or whose head is a pro-
noun. This prefiltering reduces the baseline accu-
racy to 93.6%, but also filters out 10% of learner er-
rors (false negatives), which thus cannot be detected
in stage two.
In the second stage, a decision tree classifier de-
cides for every remaining noun phrase whether a de-
terminer is missing. From the 203 features we orig-
inally extracted to inform the classification, the chi
squared algorithm selected 30. Almost all of the se-
lected features capture properties of either the head
of the noun phrase, its first word, or the token im-
mediately preceding the noun phrase. We follow
Minnen et al (2000) in defining the head of a noun
phrase as the rightmost noun, or if there is no noun,
the rightmost token. As suggested by Han et al
(2004), the classifier considers the parts of speech
of these three words, while the features that record
the respective literal word were discarded.
We also experimented with using the entire noun
phrase and its part-of-speech tag sequence as fea-
tures (Han et al, 2004), which proved not to be
helpful due to the limited size of the training data.
We replaced the part-of-speech tag sequence with a
number of boolean features that each indicate equiv-
alence with a particular sequence. Of these features
only the one that checks whether the whole noun
phrase consists of a single common noun in the sin-
gular was included in the final feature set. Addi-
tionally, the selected features include countability
information from noun countability lists generated
by Baldwin and Bond (2003), which assign nouns
to one or more countability classes: countable, un-
countable/mass noun, bipartite, or plural only.
The majority of the 30 selected features refer to
the position of one of the three tokens (head, first
word, and preceding token) in the cluster hierarchy
described in section 2.2. The set of 500 clusters
proved not to be fine-grained enough, so we used
211
1001110
10011101
100111011
1001110111
. . .
1001110110
slightly
significantly
0 1
100111010
1001110101
10011101011
. . .
10011101010
terribly
quite
0 1
1001110100
extremely
very
0 1
0 1
10011100
100111001
more
100111000
fewer
less
0 1
0 1
Figure 1: Hierarchical Clustering Subtree for Cluster Prefix 1001110
the set of 1000 clusters. To take full advantage of the
hierarchical nature of the cluster IDs, we extract pre-
fixes of all possible lengths (1?18 characters) from
the cluster ID of the respective token. For the head
and the first word, prefixes of length 3?14 were se-
lected by the attribute selector, in addition to a prefix
of length 6 for the preceding token?s cluster ID.
Among the discarded features are many extracted
from the context surrounding the noun phrase, in-
cluding the parts of speech and cluster membership
of three words to the left and right of the noun
phrase, excluding the immediately preceding token.
Features referring to possible sister conjuncts of the
noun phrase, the next 3rd person pronoun in a fol-
lowing sentence, or previous occurrences of the head
in the text also turned out not to be useful. The per-
formance of the classifier was only marginally af-
fected by the reduction in the number of features.
We conclude from this that missing determiner de-
tection is sufficiently informed by local features.
In order to increase the robustness of the classifier,
we generated additional data from the written por-
tion of the BNC by removing a determiner in 20% of
all sentences. The resulting rate of errors is roughly
equal to the rate of errors in the learner texts and the
addition of the BNC data increases the amount of
training data by a factor of 13. We trained a classifier
on both datasets (referred to as HOO-BNC below).
It achieves an F-score of 46.7% when evaluated on
30% of the shared task training data, which was held
out from the classifier training data. On the revised
test data, it reaches an F-score of 44.5%.
3 Results
The following two sections discuss our overall re-
sults for the shared task and our performance on the
three error types targeted by our system.
3.1 Overall
Figure 2 shows the overall recognition and correc-
tion F-score for the cluster settings described in
section 2.2. With the missing determiner detec-
tor HOO-BNC described in section 2.3, these cor-
respond to runs #5?9 submitted to the shared task.
For the unrevised data, Run #6 (2000 clusters only)
gives our best result for overall detection F-score
(30.26%) and Run #7 (2000 cluster backoff) for cor-
rection F-score (18.44%). For the revised data, Run
212
  0
  5
  10
  15
  20
  25
  30
N
o 
Cl
us
te
rs
2,
00
0 
Cl
us
te
rs
2,
00
0 
Ba
ck
of
f
1,
00
0 
Ba
ck
of
f
50
0 
Ba
ck
of
f
F?
Sc
or
e
Cluster Settings
Recognition
Correction
Figure 2: Recognition and Correction F-Score with Clustering
#7 (2000 cluster backoff) has our best overall detec-
tion F-score (32.21%) and Run #5 (no clusters) has
our best overall correction F-score (22.46%).
Runs using clusters give the best results in two
other metrics reported in the shared task results for
the revised data. Run #6 (2000 clusters only) gives
the best results for determiner correction F-score and
Run #2 (2000 cluster backoff), which differs only
from Run #7 in the choice of missing determiner de-
tector, gives the best results for preposition detection
and recognition F-scores.
The detailed results for Runs #5?9 with the re-
vised data are shown in Figure 2. This graph shows
that the differences between the systems with and
without clusters are very small. The recognition F-
score is best with 2000 cluster backoff and the cor-
rection F-score is best with no clusters. In both
cases, the difference between the top two results is
less than 0.01. There is, however, a noticeable in-
crease in performance as the number of clusters in-
creases, which indicates that a larger number of clus-
ters may improve results further. The set of 2000
clusters may still retain too few distinctions for this
task.
3.2 Targeted Error Types
Our system handles three of the six error types in the
shared task: missing determiner (MD), incorrect de-
terminer (RD), and incorrect preposition (RT). The
recognition and correction F-scores for our best-
forming run for each type are shown in Figure 3.
  0
  5
  10
  15
  20
  25
  30
  35
  40
  45
M
D
R
D R
T
F?
Sc
or
e
Error Type
Recognition
Correction
Figure 3: Recognition and Correction F-Score for the
Targeted Error Types
In a comparison of performance on individual er-
ror types in the shared task, our system does best
on the task for which it was originally developed,
213
preposition prediction. We place 4th in recognition
and 3rd in correction F-score for this error type. For
missing determiner (MD) and incorrect determiner
(RD) errors, our system is ranked similarly as in our
overall performance (4th?6th).
For the sake of replicability, as the HOO 2012 test
data is not publicly available, we include our results
on the HOO training data for the preposition and de-
terminer substitution errors in Table 3.
Error No Clusters
Type Recognition Correction
Prec Rec Prec Rec
RT 32.69 29.94 24.85 22.77
RD 10.63 18.56 8.37 14.61
Error 2000 Backoff
Type Recognition Correction
Prec Rec Prec Rec
RT 25.87 35.60 18.26 25.13
RD 9.71 23.65 7.48 18.23
Table 3: Results for HOO 2012 Training Data
Results are reported for the no cluster and 2000
cluster backoff settings, which show that incorpo-
rating the cluster backoff improves recall at the ex-
pense of precision. Missing determiner errors are
not reported directly as the missing determiner de-
tector was trained on the training data, but see the
evaluation at the end of section 2.3.
4 Discussion and Conclusion
The n-gram prediction approach with the new miss-
ing determiner detector performed well in the HOO
2012 Shared Task, placing 6th in terms of detection
and 5th in terms of correction out of fourteen teams
participating in the shared task. In our best sub-
missions evaluated using the revised test data, we
achieved a detection F-score of 32.71%, a recogni-
tion F-score of 29.21% and a correction F-score of
22.73%. For the three error types addressed by our
approach, our correction F-scores are 39.17% for
missing determiners, 9.23% for incorrect determin-
ers, and 30.12% for incorrect prepositions. Informa-
tion from hierarchical word clustering (Brown et al,
1992) extended the types of abstractions available
to our n-gram prediction approach and improved the
performance of the missing determiner detector.
For the n-gram prediction approach, word clusters
IDs from the hierarchical word clustering replace to-
kens in the surrounding context in order to improve
recall for learner texts which may contain errors
or infrequent token sequences. The use of cluster-
based contexts with 2000 clusters as a backoff from
the word-based approach leads to a very small im-
provement in the overall recognition F-score for the
HOO 2012 Shared Task, but our best overall correc-
tion F-score was obtained using our original word-
based approach. The differences between the word-
based and cluster-based approaches are quite small,
so we did not see as much improvement from the
word cluster abstractions as we had hoped. We
experimented with sets of clusters of several sizes
(500, 1000, 2000) and found that as the number
of clusters becomes smaller, the performance de-
creases, suggesting that a larger number of clusters
may lead to more improvement for this task.
Information from the word cluster hierarchy was
also integrated into our new missing determiner de-
tector, which uses a decision tree classifier to decide
whether a determiner should be inserted in front of
a determiner-less NP. Lexical information from the
extracted noun phrases and surrounding context are
not as useful for the classifier as information about
the position of the tokens in the word cluster hier-
archy. In particular, cluster information appears to
help compensate for lexical sparsity given a rela-
tively small amount of training data.
In future work, we plan to explore additional clus-
tering approaches and to determine when the use of
word cluster abstractions is helpful for the task of
predicting determiners, prepositions, and other func-
tion words. An approach that refers to word clus-
ters in certain contexts or in a customized fashion
for each candidate correction may lead to improved
performance for the task of detecting and correcting
such errors in texts by non-native writers.
References
Timothy Baldwin and Francis Bond, 2003. Learn-
ing the countability of English nouns from corpus
data. In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics
(ACL). pp. 463?470.
214
M. Baroni, S. Bernardini, A. Ferraresi and
E. Zanchetta, 2009. The WaCky Wide Web: A
Collection of Very Large Linguistically Processed
Web-Crawled Corpora. Language Resources and
Evaluation, 43(3):209?226.
Shane Bergsma, Dekang Lin and Randy Goebel,
2009. Web-scale N-gram models for lexical dis-
ambiguation. In Proceedings of the 21st interna-
tional jont conference on Artifical intelligence (IJ-
CAI?09). Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
Steven Bird, Robert Dale et al, 2008. The ACL An-
thology Reference Corpus. In Proceedings of the
6th International Conference on Language Re-
sources and Evaluation (LREC). Marrakesh, Mo-
rocco.
Adriane Boyd and Detmar Meurers, 2011. Data-
Driven Correction of Function Words in Non-
Native English. In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Genera-
tion ? Helping Our Own (HOO) Challenge. As-
sociation for Computational Linguistics, Nancy,
France.
Thorsten Brants and Alex Franz, 2006. Web 1T
5-gram Version 1. Linguistic Data Consortium.
Philadelphia.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, T. J. Watson, Vincent J. Della Pietra and
Jenifer C. Lai, 1992. Class-Based n-gram Models
of Natural Language. Computational Linguistics,
18(4):467?479.
Robert Dale and Adam Kilgarriff, 2011. Helping
Our Own: The HOO 2011 Pilot Shared Task. In
Proceedings of the 13th European Workshop on
Natural Language Generation. Nancy, France.
Rachele De Felice, 2008. Automatic Error Detection
in Non-native English. Ph.D. thesis, Oxford.
Anas Elghafari, Detmar Meurers and Holger Wun-
sch, 2010. Exploring the Data-Driven Prediction
of Prepositions in English. In Proceedings of the
23rd International Conference on Computational
Linguistics (COLING). Beijing.
Michael Gamon, Jianfeng Gao et al, 2008. Us-
ing Contextual Speller Techniques and Language
Modeling for ESL Error Correction. In Proceed-
ings of the Third International Joint Conference
on Natural Language Processing. Hyderabad.
Na-Rae Han, Martin Chodorow and Claudia Lea-
cock, 2004. Detecting Errors in English Arti-
cle Usage with a Maximum Entropy Classifier
Trained on a Large, Diverse Corpus. In Proceed-
ings of the 4th International Conference on Lan-
guage Resources and Evaluation (LREC). Lisbon.
Percy Liang, 2005. Semi-Supervised Learning for
Natural Language. Master?s thesis, Massachusetts
Institute of Technology.
Guido Minnen, Francis Bond and Ann Copestake,
2000. Memory-based learning for article gener-
ation. In Proceedings of the 2nd Workshop on
Learning Language in Logic and the 4th Confer-
ence on Computational Natural Language Learn-
ing. volume 7, pp. 43?48.
Nick Rizzolo and Dan Roth, 2007. Modeling Dis-
criminative Global Inference. In Proceedings of
the First International Conference on Semantic
Computing (ICSC). IEEE, Irvine, California, pp.
597?604.
Alla Rozovskaya and Dan Roth, 2011. Algorithm
Selection and Model Adaptation for ESL Cor-
rection Tasks. In Proceedings of the 49th An-
nual Meeting of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies (ACL-HLT). Portland, Oregon.
Joel Tetreault and Martin Chodorow, 2008. Native
Judgments of Non-Native Usage: Experiments in
Preposition Error Detection. In Proceedings of the
22nd International Conference on Computational
Linguistics (COLING). Manchester.
Stephen Wechsler, 1994. Preposition Selection Out-
side the Lexicon. In Raul Aranovich, William
Byrne, Susanne Preuss and Martha Senturia
(eds.), Proceedings of the Thirteenth West Coast
Conference on Formal Linguistics. CSLI Publica-
tions, Stanford, California, pp. 416?431.
H. Yannakoudakis, T. Briscoe and B. Medlock,
2011. A new dataset and method for automati-
cally grading ESOL texts. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Tech-
nologies (ACL-HLT).
215
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 326?336,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Evaluating the Meaning of Answers to Reading Comprehension Questions
A Semantics-Based Approach
Michael Hahn Detmar Meurers
SFB 833 / Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
{mhahn,dm}@sfs.uni-tuebingen.de
Abstract
There is a rise in interest in the evaluation of
meaning in real-life applications, e.g., for as-
sessing the content of short answers. The ap-
proaches typically use a combination of shal-
low and deep representations, but little use is
made of the semantic formalisms created by
theoretical linguists to represent meaning.
In this paper, we explore the use of the un-
derspecified semantic formalism LRS, which
combines the capability of precisely repre-
senting semantic distinctions with the ro-
bustness and modularity needed to represent
meaning in real-life applications.
We show that a content-assessment approach
built on LRS outperforms a previous approach
on the CREG data set, a freely available cor-
pus of answers to reading comprehension ex-
ercises by learners of German. The use of such
a formalism also readily supports the integra-
tion of notions building on semantic distinc-
tions, such as the information structuring in
discourse, which we show to be useful for con-
tent assessment.
1 Introduction
There is range of systems for the evaluation of short
answers. While the task is essentially about eval-
uating sentences based on their meaning, the ap-
proaches typically use a combination of shallow and
deep representations, but little use is made of the se-
mantic formalisms created by theoretical linguists to
represent meaning. One of the reasons for this is that
semantic structures are difficult to derive because of
the complex compositionality of natural language.
Another difficulty is that form errors in the input cre-
ate problems for deep processing, which is required
for extracting semantic representations.
On the other hand, semantic representations have
the significant advantage that they on the one hand
abstract away from variation in the syntactic real-
ization of the same meaning and on the other hand
clearly expose those distinctions which do make a
difference in meaning. For example, the difference
between dog bites man and man bites dog is still
present in deeper syntactic or semantic representa-
tions, while semantic representations abstract way
from meaning-preserving form variation, such as the
active-passive alternation (dog bites man ? man was
bitten by dog). This suggests that sufficiently robust
approaches using appropriate semantic formalisms
can be useful for the evaluation of short answers.
In this paper, we explore the use of Lexical Re-
source Semantics (Richter and Sailer, 2003), one
of the underspecified semantic formalisms combin-
ing the capability of precisely representing seman-
tic distinctions with the robustness and modularity
needed to represent meaning in real-life applica-
tions. Specifically, we address the task of evaluating
the meaning of answers to reading comprehension
exercises.
We will base our experiments on the freely avail-
able data set used for the evaluation of the CoMiC-
DE system (Meurers et al, 2011), which does not
use semantic representations. The data consists of
answers to reading comprehension exercise written
by learners of German together with questions and
corresponding target answers.
326
2 Related Work
There are several systems which assess the content
of short answers. Mitchell et al (2002) use hand-
crafted patterns which indicate correct answers to a
question. Similarly, Nielsen et al (2009) use manu-
ally annotated word-word relations or ?facets?. Pul-
man and Sukkarieh (2005) use machine learning
to automatically find such patterns. Other systems
evaluate the correctness of answers by comparing
them to one or more manually annotated target an-
swers. C-Rater (Leacock and Chodorow, 2003) and
the system of Mohler et al (2011) compare the syn-
tactic parse to the parse of target answers. A com-
parison of a range of content assessment approaches
can be found in Ziai et al (2012).
The work in this paper is most similar to a line
of work started by Bailey and Meurers (2008), who
present a system for automatically assessing an-
swers to reading comprehension questions written
by learners of English. The basic idea is to align
the student answers to a target answer using a par-
allel approach with several levels on which words
or chunks can be matched to each other. Classifica-
tion is done by a machine learning component. The
CoMiC-DE system for German is also based on this
approach (Meurers et al, 2011).
In terms of broader context, the task is related
to the research on Recognizing Textual Entailment
(RTE) (Dagan et al, 2006). In particular, align-
ment (e.g., MacCartney et al, 2008, Sammons et al,
2009) and graph matching approaches (Haghighi et
al., 2005, Rus et al, 2007) are broadly similar to our
approach.
3 General Setup
3.1 Empirical challenge: CREG
Our experiments are based on the freely available
Corpus of Reading comprehension Exercises in Ger-
man (CREG, Ott et al, 2012) . It consists of texts,
questions, target answers, and corresponding student
answers written by learners of German. For each
student answer, two independent annotators evalu-
ated whether it correctly answers the question. An-
swers were only assessed with respect to meaning;
the assessment is in principle intended to be inde-
pendent of grammaticality and orthography. The
task of our system is to decide which answers cor-
rectly answer the given question and which do not.
3.2 Formal basis: Lexical Resource Semantics
Lexical Resource Semantics (LRS) (Richter and
Sailer, 2003) is an underspecified semantic formal-
ism which embeds model-theoretic semantic lan-
guages like IL or Ty2 into constraint-based typed
feature structure formalisms as used in HPSG. It
is formalized in the Relational Speciate Reentrancy
Language (RSRL) (Richter, 2000).
While classical formal semantics uses fully ex-
plicit logical formulae, the idea of underspecified
formalisms such as LRS is to derive semantic rep-
resentations which are not completely specified and
subsume a set of possible resolved expressions, thus
abstracting away from ambiguities, in particular, but
not exclusively, scope ambiguities.
As an example for the representations, consider
the ambiguous example (1) from the CREG corpus.
(1) Alle
all
Zimmer
rooms
haben
have
nicht
not
eine
a
Dusche.
shower
?Not every room has a shower.?
?No room has a shower.?
The LRS representation of (1) is shown in Figure
1, where INCONT (INTERNAL CONTENT) encodes
the core semantic contribution of the head, EXCONT
(EXTERNAL CONTENT) the semantic representation
of the sentence, and PARTS is a list containing the
subterms of the representation.
?
?
?
?
?
?
?
?
?
?
?
INCONT haben(e)
EXCONT A
PARTS
?A, haben(e), ?x1(B? C),
zimmer(x1), ?x2 (D ? E), ? F,
dusche(x2), subj(e,x1), obj(e,x2)
?e(haben(e) ? subj(e,x1) ? obj(e,x2)
?
?
?
?
?
?
?
?
?
?
?
?
Ex2(D & E)
(haben(e) & subj(e,x1) & obj(e,x2))
    F
Ax1(B    C)
zimmer(x1) dusche(x2) Ee 
    A
Figure 1: LRS and dominance graph for (1)
The representation also includes a set of subterm
constraints, visualized as a dominance graph at the
327
bottom of the figure. The example (1) has several
readings, which is reflected in the fact that the rel-
ative scope of the two quantifiers and the negation
is not specified. The different readings of the sen-
tence can be obtained by identifying each of the
meta-variables A, . . . , F with one of the subformu-
las. Meta-variables are labels that indicate where a
formula can be plugged in; they are only part of the
underspecified representation and do not occur in the
resolved representation.
This illustrates the main strengths of an under-
specified semantic formalism such as LRS for prac-
tical applications. All elements of the semantic rep-
resentation are explicitly available on the PARTS list,
with dominance constraints and variable bindings
providing separate control over the structure of the
representation. The underspecified nature of LRS
also supports partial analyses for severely ill-formed
input or fragments, which is problematic for clas-
sical approaches to semantic compositionality such
as Montague semantics (Montague, 1973). Another
advantage of LRS as an underspecified formalism
is that it abstracts away from the computationally
costly combinatorial explosion of possible readings
of ambiguous sentences, yet it also is able to rep-
resent fine-grained semantic distinctions which are
difficult for shallow semantic methods to capture.
3.3 Our general approach
In a first step, LRS representations for the student
answer, the target answer, and the question are auto-
matically derived on the basis of the part-of-speech
tags assigned by TreeTagger (Schmid, 1994) and the
dependency parses by MaltParser (Nivre and Hall,
2005) in the way discussed in Hahn and Meurers
(2011). In this approach, LRS structures are de-
rived in two steps. First, surface representations
are mapped to syntax-semantics-interface represen-
tations, which abstract away from some form vari-
ation at the surface. In the second step, rules map
these interface representations to LRS representa-
tions. The approach is robust in that it always results
in an LRS structure, even for ill-formed sentences.
Our system then aligns the LRS representations
of the target answer and the student answer to each
other and also to the representation of the ques-
tion. Alignment takes into account both local crite-
ria, in particular semantic similarity, and global cri-
teria, which measure the extent to which the align-
ment preserves structure on the level of variables and
dominance constraints.
The alignments between answers and the question
are used to determine which elements of the seman-
tic representations are focused in the sense of In-
formation Structure (von Heusinger, 1999; Kruijff-
Korbayova? and Steedman, 2003; Krifka, 2008), an
active field of research in linguistics addressing the
question how the information in sentences is pack-
aged and integrated into discourse.
Overall meaning comparison in our approach is
then done based on a set of numerical scores com-
puted from potential alignments and their quality.
Given its LRS basis, we will call the system CoSeC-
DE (Comparing Semantics in Context).
4 Aligning Meaning Representations
The alignment is done on the level of the PARTS lists,
on which all elements of the semantic representation
are available:
Definition 1. An alignment a between two LRS
representations S and T with PARTS lists pn1 and
qm1 is an injective partial function from {1,...,n} to
{1,...,m}.
Requiring a to be injective ensures that every ele-
ment of one representation can be aligned to at most
one element of the other representation. Note that
this definition is symmetrical in the sense that the
direction can be inverted simply by inverting the in-
jective alignment function.
To automatically derive alignments, we define a
maximization criterion which combines three fac-
tors measuring different aspects of alignment qual-
ity. In addition to i) the similarity of the align-
ment links, the quality Q of the alignment a takes
into account the structural correspondence between
aligned elements by evaluating the consistency of
alignments ii) with respect to the induced variable
bindings ? and, and iii) with respect to dominance
constraints:
Q(a, ?|S, T ) = linksScore(a|S, T )
? variableScore(?)
? dominanceScore(a|S, T )
(1)
The approach thus uses a deep representation ab-
stracting away from the surface, but the meaning
328
comparison approach on this deep level is flat, yet
at the same time is able to take into account struc-
tural criteria. In consequence, the approach is mod-
ular because it uses the minimal building blocks of
semantic representations, but is able to make use of
the full expressive power of the semantic formalism.
4.1 Evaluating the Quality of Alignment Links
The quality of an alignment link between two ex-
pressions is evaluated by recursively evaluating the
similarity of their components. In the base case,
variables can be matched with any variable of the
same semantic type:
sim(x? , y? ) = 1
Meta-variables can be matched with any meta-
variable of the same semantic type:
sim(A? ,B? ) = 1
For predicates with arguments, both the predicate
name and the arguments are compared:
sim(P1(a
k
1), P2(b
k
1)) =
sim(P1, P2) ?
k?
i=1
sim(ai, bi)
(2)
If the predicates have different numbers of argu-
ments, similarity is zero. Linguistically well-known
phenomena where the number of arguments of se-
mantically similar predicates differ do not cause a
problem for this definition, because semantic roles
are linked to the verbal predicate via grammatical
function terms such as subj and obj predicating over
a Davidsonian event variable, as in Figure 1.1
For formulas with generalized quantifiers, the
quantifiers, the variables, the scopes and the restric-
tors are compared:
sim(Q1x1(? ? ?), Q2x2(? ? ?)) =
sim(Q1, Q2) ? sim(x1, x2)
?sim(?, ?) ? sim(?, ?)
(3)
Lambda abstraction is dealt with analogously.
The similarity sim(P1, P2) of names of predicates
and generalized quantifiers takes into account sev-
eral sources of evidence and is estimated as the max-
imum of the following quantities:
1In this paper, we simply use grammatical function names
in place of semantic role labels in the formulas. A more sophis-
ticated, real mapping from syntactic functions to semantic roles
could usefully be incorporated.
As a basic similarity, the Levenshtein distance
normalized to the interval [0,1] (with 1 denoting
identity and 0 total dissimilarity) is used. This ac-
counts for the high frequency of spelling errors in
learner language.
Synonyms in GermaNet (Hamp and Feldweg,
1997) receive the score 1.
For numbers, the (normalized) difference
|n1?n2|
max(n1,n2)
is used.
For certain pairs of dissimilar elements which be-
long to the same category, constant costs are de-
fined. This encourages the system to align these el-
ements, unless the structural factors, i.e., the quality
of the unifier and the consistency with dominance
constraints, discourage this. Such constants are de-
fined for pairs of grammatical function terms. Other
constants are defined for pairs of numerical terms
and for pairs of terms encoding affirmative and neg-
ative natural language expressions and logical nega-
tion.
Having defined how to compute the quality for
single alignment links, we still need to define how to
compute the combined score of the alignment links,
which we define to be the sum of the qualities of the
links:
linksScore(a|pn1 , q
m
1 ) =
n?
k=1
{
sim(pk, qa(k)) if a(k) is defined,
?NULL else.
(4)
The quality of a given overall alignment thus is
determined by the quality of the alignment links of
the PARTS elements which are aligned. For those
PARTS elements not aligned, a constant cost ?NULL
must be paid, which, however, may be smaller than
a costly alignment link in another overall alignment.
4.2 Evaluating Unifiers
Alignments between structurally corresponding se-
mantic elements should be preferred. For situations
in which they structurally do not correspond, this
may have the effect of dispreferring the pairing of
elements which in terms of the words on the surface
are identical or very similar. Consider the sentence
pair in (2), where Frau in (2a) syntactically corre-
sponds to Mann in (2b).
329
(2) a. Eine
a
Frau
woman
sieht
sees
einen
a
Mann
man
?A woman sees a man.?
b. Ein
a
Mann
man
sieht
sees
eine
a
Frau
woman
?A man sees a woman.?
On the level of the semantic representation, this
is reflected in the correspondence between the vari-
ables x1 and y1, both of which occur as arguments
of subj, as shown in Figure 2.
Ex2x(Dx &&&&&&)&habensujD&&& be
,1oDxF&&& be
Ex2x(Dx 
Au((D&&& bB&&&&&&)&h
abB
,1oDxF&&& bB
&&&&&&C&zaiBnsujD&&& iB
Ej1oDxF&&& iB
&&&&&&C&zaieAu((D&&& ie
Ej1oDxF&&& ie
Figure 2: An excerpt of an alignment between the PARTS
lists of (2a) on the left and (2b) on the right. Dotted align-
ment links are the ones only plausible on the surface.
Our solution to capture this distinction is to use
the concept of a unifier, well-known from logic pro-
gramming. A unifier for terms ?, ? is a substitu-
tion ? such that ?? = ??. Every alignment in-
duces a unifier, which unifies all variables which are
matched by the alignment.
The alignment in Figure 2 (without the dotted
links) induces the unifier
?1 = [(x1, y1) 7? z1; (x2, y2) 7? z2].
If links between the matching predicates mann and
frau, respectively, are added, one also has to unify x1
with y2 and x2 with y1 and thus obtains the unifier
?2 = [(x1, x2, y1, y2) 7? z].
Intuitively, a good unifier unifies only variables
which correspond to the same places in the seman-
tic structures to be aligned. In the case of Figure 2,
choosing an alignment including the dotted links re-
sults in the unifier ?2 which unifies x1 and x2 ? yet
they are structurally different, with one belonging to
the subject and the other one to the object.
In general, it can be expected that an alignment
which preserves the structure will not unify two dis-
tinct variables from the same LRS representation,
since they are known to be structurally distinct. So
we want to capture the information loss resulting
from unification. This intuition is captured by (5),
which answers the following question: Given some
variable z in a unified expression, how many addi-
tional bits do we need on average2 to encode the
original pair of variables x, y in the PARTS lists p
and q, respectively?
H(?) =
1
Zp,q
?
z?Ran(?)
W?(z) log(W?(z)) (5)
where W?(z) = |{x ? V ar(p)|x? = z}|
? |{y ? V ar(q)|y? = z}|
(6)
Zp,q = |V ar(p)| ? |V ar(q)| (7)
The value of a unifier ? is then defined as follows:
variableScore(?) =
(
1?
H(?)
H?
)k
(8)
where k is a numerical parameter with 0 ? k ? 1
and H? is a (tight) upper bound on H(?) obtained
by evaluating the worst unifier, i.e., the unifier that
unifies all variables H? = log(Zp,q).
4.3 Evaluating consistency with dominance
constraints
While evaluating unifiers ensures that alignments
preserve the structure on the level of variables, it is
also important to evaluate their consistency with the
dominance structure of the underspecified semantic
representations, such as the one we saw in Figure 1.
Consider the following pair:
(3) a. Peter
Peter
kommt
comes
und
and
Hans
Hans
kommt
comes
nicht.
not
?Peter comes and Hans does not come.?
b. Peter
Peter
kommt
comes
nicht
not
und
and
Hans
Hans
kommt.
comes
?Peter does not come and Hans comes.?
While the words and also the PARTS lists of the
sentences are identical, they clearly differ in mean-
ing. Figure 3 on the next page shows the LRS domi-
nance graphs for the two sentences together with an
2For simplicity, it is assumed that every combination in
V ar(p)? V ar(q) occurs the same number of times.
330
alignment between them. The semantic difference
between the two sentences is reflected in the posi-
tion of the negation in the dominance graph: while
it dominates kommen(e2) ? subj(e2,hans) in (3a), it
dominates kommen(f1) ? subj(f1,peter) in (3b).
To account for this issue, we evaluate the consis-
tency of the alignment with respect to dominance
constraints. An alignment a is optimally consistent
with respect to dominance structure if it defines an
isomorphism between its range and its domain with
respect to the relation / ?is dominated by?.
Figure 3 shows an alignment which aligns all
matching elements in (3b) and (3a). The link be-
tween the negations violates the isomorphism re-
quirement: the negation dominates kommen(e2) ?
subj(e2,hans) in (3a), while it does not dominate the
corresponding elements in (3b). An optimally con-
sistent alignment will thus leave the negations un-
aligned. Unaligned negations can later be used in
the overall meaning comparison as strong evidence
that the sentences do not mean the same.
dominanceScore measures how ?close? a is to
defining an isomorphism. We use the following sim-
ple score, which is equal to 1 if and only if a defines
an isomorphism:
dominanceScore(a|S, T ) =
1
1 +
?
i,j?Dom(a) ?
?
?
?
?
pi / pj ,
pi . pj ,
qa(i) / qa(j),
qa(i) . qa(j)
?
?
?
?
(9)
where ? is a function taking four truth values as its
arguments. It measures the extent to which the iso-
morphism requirement is violated by an alignment.
?(t1, t2, t1, t2) is defined as 0 because there is no
violation if the dominance relation between pi and
pj is equal to that between the elements they are
aligned with, qa(i) and qa(j). For other combinations
of truth values, ? should be set to values greater than
zero, empirically determined on a development set.
4.4 Finding the best alignment
Because of the use of non-local criteria in the max-
imization criterion Q(a, ?|S, T ) defined in equation
(1), an efficient method is needed to find the align-
ment maximizing the criterion. We exploit the struc-
ture inherent in the set of possible alignments to ap-
ply the A* algorithm (Russel and Norvig, 2010). We
first generalize the notion of an alignment.
Definition 2. A partial alignment of order i is an
index i together with an alignment which does not
have alignment links for any pj with j > i.
A partial alignment can be interpreted as a class
of alignments which agree on the first i elements.
Definition 3. The refinements ?(a) of the partial
alignment a (of order i) are the partial alignments b
such that (1) b is of order i+1, and (2) a and b agree
on {1, ..., i}.
Intuitively, refinements of an alignment of order i
are obtained by deciding how to align element i+1.
? induces a tree over the set of partial alignments,
whose leaves are exactly the complete alignments.
A simple optimistic estimate for the value of all
complete descendants of an alignment a of order i is
given by the following expression:
optimistic(a, ?|S, T ) = variableScore(?)
?dominanceScore(a, S, T )
?(linksScorei(a, ?|p, q)+
n?
k=i+1
heuristic(k, a, pn1 , q
m
1 ))
(10)
where linksScorei is the sum in (4) restricted
to 1 ? k ? i, and heuristic(k, a, pn1 , q
m
1 ) is
0 if pk is aligned and a simple, optimistic esti-
mate for the quality of the best possible align-
ment link containing pk if pk is unaligned. It
is estimated as the maximum of ?NULL and
max{sim(pk, qj) | qj unaligned}.
The estimate in (10) is optimistic in the sense
that it provides an upper bound on the values of all
complete alignments below a. It defines a mono-
tone heuristic and thus allows complete and optimal
search using the A* algorithm. To obtain an efficient
implementation, additional issues such as the order
of elements in the PARTS lists were taken care of. As
they do not play a role for the conceptualization of
our approach, they are not discussed here.
The crucial part at this point of the discussion
is that the A* search can determine the best align-
ment between two PARTS lists. As mentioned in
the overview in section 3.3, we compute three such
331
Ex2(2D &)h
Ea2(2b &)e
nsuu)jE)e ,1oFE)eAB)C)z 
nsuu)jE)h ,1oFE)hAi mj, 
r2(2d
&
Ec2(2 &h
E 2(2! &e
nsuu)jEe ,1oFEeAB)C)z 
nsuu)jEh ,1oFEhAimj, 
"2(2#
$
Figure 3: Alignment between the dominance graphs of (3a) and (3b). The red dotted link violates isomorphism.
alignments: between the student and the target an-
swer, between the question and the student answer,
and between the question and the target answer.
5 From Alignment to Meaning Comparison
Based on the three alignments computed using the
just discussed algorithm, we now explore different
options for computing whether the student answer
is correct or not. We discuss several alternatives,
all involving the computation of a numerical score
based on the alignments. For each of these scores, a
threshold is empirically determined, over which the
student answer is considered to be correct.
Basic Scores The simplest score, ALIGN, is com-
puted by dividing the alignment quality Q between
the student answer and the target answer as defined
in equation (1) by the number of elements in the
smaller PARTS list. Two other scores are computed
based on the number of alignment links between
student and target answer, which for the EQUAL-
Student score is divided by the number of elements
of the PARTS list of the student answer, and for the
EQUAL-Target score by those of the target answer.
For dealing with functional elements, i.e., predi-
cates like subj, obj, quantifiers and the lambda op-
erator, we tried out three options. The straight case
is the one mentioned above, treating all elements on
the PARTS list equally (EQUAL). As a second op-
tion, to see how important the semantic relations be-
tween words are, and how much is just the effect of
the elements themselves, we defined a score which
ignores functional elements (IGNORE). A third pos-
sibility is to weight elements so that functional and
non-functional ones differ in impact (WEIGHTED).
Each of the three scores (EQUAL, IGNORE,
WEIGHTED) is either divided by the number of el-
ements of the PARTS list of the student answer or
the target, resulting in six scores. In addition, three
more scores result from computing the average of
the student and target answer scores.
Information Structure Scores Basing meaning
comparison on actual semantic representation also
allows us to directly take into account Information
Structure as a structuring of the meaning of a sen-
tence in relation to the discourse. Bailey and Meur-
ers (2008), Meurers et al (2011), and Mohler et al
(2011) showed that excluding those parts of the an-
swer which are mentioned (given) in the question
greatly improves classification accuracy. Meurers
et al (2011) argue that the relevant linguistic as-
pect is not whether the material was mentioned in
the question, but the distinction between focus and
background in Information Structure (Krifka, 2008).
The focus essentially is the information in the an-
swer which selects between the set of alternatives
that the question raises.
This issue becomes relevant, e.g., in the case of
?or? questions, where the focused information de-
termining whether the answer is correct is explicitly
given in the question. This is illustrated by the ques-
tion in (4) with target answer (5a) and student an-
swer (5b), from the CREG corpus. While all words
in the answers are mentioned in the question, the
part of the answers which actually answer the ques-
tion are the focused elements shown in boldface.
(4) Ist
is
die
the
Wohnung
flat
in
in
einem
a
Altbau
old building
oder
or
Neubau?
new building
(5) a. Die
the
Wohnung
flat
ist
is
in
in
einem
a
Altbau.
old.building
b. Die
the
Wohnung
flat
ist
is
in
in
einem
a
Neubau.
new.building
332
To realize a focus-based approach, one naturally
needs a component which automatically identifies
the focus of an answer in a question-answer pair. As
a first approximation, this currently is implemented
by a module which marks the elements of the PARTS
lists of the answers for information structure. El-
ements which are not aligned to the question are
marked as focused. Furthermore, in answers to ?or?
questions, it marks as focused all elements which
are aligned to the semantic contribution of a word
belonging to one of the alternatives. ?Or? questions
are recognized by the presence of oder (?or?) and the
absence of a wh-word.
While previous systems simply ignored all words
given in the question during classification, our sys-
tem aligns all elements and recognizes givenness
based on the alignments. Therefore, givenness is
still recognized if the surface realization is differ-
ent. Furthermore, material which incidentally is also
found in the question, but which is structurally dif-
ferent, is not assumed to be given.
Scores using information structure were obtained
in the way of the BASIC scores but counting only
those elements which are recognized as focused
(FOCUS). For comparison, we also used the same
scores with givenness detection instead of focus de-
tection, i.e., in these scores, all elements aligned to
the question were excluded (GIVEN).
Annotating semantic rather than surface represen-
tations for information structure has the advantage
that the approach can be extended to cover focus-
ing of relations in addition to focusing of entities.
The general comparison approach also is compat-
ible with more sophisticated focus detection tech-
niques capable of integrating a range of cues, in-
cluding syntactic cues and specialized constructions
such as clefts, or prosodic information for spoken
language answers ? an avenue we intend to pursue
in future research.
Dissimilar score We also explored one special-
ized score paying particular attention to dissimi-
lar aligned elements, as mentioned in section 4.1.
Where a focused number is aligned to a different
number, or a focused polarity expression is aligned
to the opposite polarity, or a logical negation is not
aligned, then 0 is returned as score, i.e., the student
answer is false. In all other cases, the DISSIMILAR
score is identical to the WEIGHTED-Average FOCUS
score, i.e., the score based on the average of the stu-
dent and target scores with weighting and focus de-
tection.
6 Experiments
6.1 Corpus
We base the experiments on the 1032 answers from
the CREG corpus which are used in the evaluation
of the CoMiC-DE system reported by Meurers et al
(2011). The corpus is balanced, i.e., the numbers of
correct and of incorrect answers are the same. It con-
tains only answers where the two human annotators
agreed on the binary label.
6.2 Setup
The alignment algorithm contains a set of numeri-
cal parameters which need to be determined empir-
ically, such as ?NULL and the function ?. In a first
step, we optimized these parameters and the weights
used in the WEIGHTED scores using grid search on
a development set of 379 answers. These answers
are from CREG, but do not belong to the 1032 an-
swers used for testing. We used the accuracy of the
DISSIMILAR score as performance metric.
In our experiment, we explored each score sep-
arately to predict which answers are correct and
which not. For each score, classification is based
on a threshold which is estimated as the arithmetic
mean of the average score of correct and the average
score of incorrect answers. Training and testing was
performed using the leave-one-out scheme (Weiss
and Kulikowski, 1991). When testing on a particular
answer, student answers answering the same ques-
tion were excluded from training.
6.3 Results
Figure 4 shows the accuracy results obtained in our
experiments together with the result of CoMiC-DE
on the same dataset. With an accuracy of up to
86.3%, the WEIGHTED-Average FOCUS score out-
perform the 84.6% reported for CoMiC-DE (Meur-
ers et al, 2011) on the same dataset. This is remark-
able given that CoMiC-DE uses several (but com-
parably shallow) levels of linguistic abstraction for
finding alignment links, whereas our approach is ex-
clusively based on the semantic representations.
333
Score BASIC GIVEN FOCUS
ALIGN 77.1
EQUAL
Student 69.8 75.3 75.2
Target 70.0 75.5 75.2
Average 76.6 80.8 80.7
IGNORE
Student 75.8 80.1 80.3
Target 77.2 82.2 82.3
Average 79.8 84.7 84.9
WEIGHTED
Student 75.0 80.6 80.7
Target 76.1 83.3 83.3
Average 80.9 86.1 86.3
DISSIMILAR 85.9
CoMiC-DE 84.6
Figure 4: Classification accuracy of CoSeC-DE
The fact that WEIGHTED-Average outperforms
the IGNORE-Average scores shows that the inclu-
sion of functional element (i.e., predicates like subj,
obj), which are not available to approaches based
on aligning surface strings, improves the accuracy.3
On the other hand, the lower performance of EQUAL
shows that functional elements should be treated dif-
ferently from content-bearing elements.
Of the 13.7% answers misclassified by
WEIGHTED-Average FOCUS, 53.5% are false
negatives and 46.5% are false positives.
We also investigated the impact of grammaticality
on the result by manually annotating a sample of 220
student answers for grammatical well-formedness,
66% of which were ungrammatical. On this sam-
ple, grammatical and ungrammatical student an-
swers were evaluated with essentially the same ac-
curacy (83% for ungrammatical answers, 81% for
grammatical answers).
The decrease in accuracy of the COMBINED score
over the best score can be traced to some yes-no-
questions which have an unaligned negation but are
correct. On the other hand, testing only on answers
with focused numbers results in an accuracy of 97%.
The performance of GIVEN and FOCUS scores
3We also evaluated IGNORE scores using parameter values
optimized for these scores, but their performance was still be-
low those of the corresponding WEIGHTED-Average scores.
compared to BASIC confirms that information struc-
turing helps in targeting the relevant parts of the an-
swers. Since CoMiC-DE also demotes given mate-
rial, the better GIVEN results of our approach must
result from other aspects than the information struc-
ture awareness. Unlike previous approaches, the FO-
CUS scores support reference to the material focused
in the answers. However, since currently the FOCUS
scores only differs from the GIVEN scores for alter-
native questions, and the test corpus only contains
seven answers to such ?or? questions, we see no se-
rious quantitative difference in accuracy between the
FOCUS and GIVENNESS results.
While the somewhat lower accuracy of the score
ALIGN shows that the alignment scores are not suf-
ficient for classification, the best-performing scores
do not require much additional computation and do
not need any information that is not in the align-
ments or the automatic focus annotation.
7 Future Work
The alert reader will have noticed that our ap-
proach currently does not support many-to-many
alignments. As is known, e.g., from phrase-based
machine translation, this is an interesting avenue for
dealing with non-compositional expressions, which
we intend to explore in future work. The align-
ment approach can be adapted to such alignments
by adding a factor measuring the quality of many-to-
many links to linkScore (4) and optimistic (10).
8 Conclusion
We presented the CoSeC-DE system for evaluating
the content of answers to reading comprehension
questions. Unlike previous content assessment sys-
tems, it is based on formal semantics, using a novel
approach for aligning underspecified semantic rep-
resentations. The approach readily supports the in-
tegration of important information structural differ-
ences in a way that is closely related to the informa-
tion structure research in formal semantics and prag-
matics. Our experiments showed the system to out-
perform our shallower multi-level system CoMiC-
DE on the same CREG-1032 data set, suggesting
that formal semantic representations can indeed be
useful for content assessment in real-world contexts.
334
Acknowledgements
We are grateful to the three anonymous BEA re-
viewers for their very encouraging and helpful com-
ments.
References
Stacey Bailey and Detmar Meurers. 2008. Diagnos-
ing meaning errors in short answers to reading com-
prehension questions. In Joel Tetreault, Jill Burstein,
and Rachele De Felice, editors, Proceedings of the 3rd
Workshop on Innovative Use of NLP for Building Edu-
cational Applications (BEA-3) at ACL?08, pages 107?
115, Columbus, Ohio.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entail-
ment Challenge. In J. Quionero-Candela, I. Dagan,
B. Magnini, and F. d?Alch Buc, editors, Machine
Learning Challenges, volume 3944 of Lecture Notes
in Computer Science, pages 177?190. Springer.
Aria D. Haghighi, Andrew Y. Ng, and Christopher D.
Manning. 2005. Robust textual inference via graph
matching. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods in
Natural Language Processing, pages 387?394. Asso-
ciation for Computational Linguistics.
Michael Hahn and Detmar Meurers. 2011. On deriv-
ing semantic representations from dependencies: A
practical approach for evaluating meaning in learner
corpora. In Kim Gerdes, Eva Hajicov, and Leo Wan-
ner, editors, Depling 2011 Proceedings, pages 94?103,
Barcelona.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet - a
Lexical-Semantic Net for German. In Proceedings of
ACL workshop Automatic Information Extraction and
Building of Lexical Semantic Resources for NLP Ap-
plications, pages 9?15.
Manfred Krifka. 2008. Basic notions of information
structure. Acta Linguistica Hungarica, 55(3):243?
276.
Ivana Kruijff-Korbayova? and Mark Steedman. 2003.
Discourse and information structure. Journal of Logic,
Language and Information (Introduction to the Special
Issue), pages 249?259.
Claudia Leacock and Martin Chodorow. 2003. C-rater:
Automated scoring of short-answer questions. Com-
puters and the Humanities, 37(4):389?405.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model for
natural language inference. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 802?811. Association for Compu-
tational Linguistics.
Detmar Meurers, Ramon Ziai, Niels Ott, and Janina
Kopp. 2011. Evaluating answers to reading compre-
hension questions in context: Results for German and
the role of information structure. In Proceedings of the
TextInfer 2011 Workshop on Textual Entailment, pages
1?9, Edinburgh, Scotland, UK, July. Association for
Computational Linguistics.
Tom Mitchell, Terry Russell, Peter Broomhead, and
Nicola Aldridge. 2002. Towards robust computerised
marking of free-text responses. In Proceedings of
the 6th International Computer Assisted Assessment
(CAA) Conference.
Michael Mohler, Razvan Bunescu, and Rada Mihalcea.
2011. Learning to grade short answer questions using
semantic similarity measures and dependency graph
alignments. In Proceedings of the 49th Annual Meet-
ing of the Association for Comnputational Linguistics,
pages 752?762.
Richard Montague. 1973. The Proper Treatment of Qun-
tification in Ordinary English. In Jaakko Hintikka,
Julius Moravcsik, and Patrick Suppes, editors, Ap-
proaches to Natural Language, pages 221?242. Rei-
del, Dordrecht.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2009. Recognizing entailment in intelligent tutoring
systems. Natural Language Engineering, 15(4):479?
501.
Joakim Nivre and Johan Hall. 2005. Maltparser: A
language-independent system for data-driven depen-
dency parsing. In Proceedings of the Fourth Workshop
on Treebanks and Linguistic Theories, pages 13?95.
Niels Ott, Ramon Ziai, and Detmar Meurers. 2012. Cre-
ation and analysis of a reading comprehension exercise
corpus: Towards evaluating meaning in context. In
Thomas Schmidt and Kai Wrner, editors, Multilingual
Corpora and Multilingual Corpus Analysis, Hamburg
Studies in Multilingualism (HSM). Benjamins, Ams-
terdam.
Stephen G. Pulman and Jana Z. Sukkarieh. 2005. Au-
tomatic short answer marking. In Proceedings of the
2nd Workshop on Building Educational Applications
Using NLP, pages 9?16.
Frank Richter and Manfred Sailer. 2003. Basic Concepts
of Lexical Resource Semantics. In Arnold Beckmann
and Norbert Preining, editors, ESSLLI 2003 ? Course
Material I, volume 5 of Collegium Logicum, pages 87?
143, Wien. Kurt Go?del Society.
Frank Richter. 2000. A Mathematical Formalism
for Linguistic Theories with an Application in Head-
Driven Phrase Structure Grammar. Phil. dissertation,
Eberhard-Karls-Universita?t Tu?ingen.
Vasile Rus, Arthur Graesser, and Kirtan Desai. 2007.
Lexico-syntactic subsumption for textual entailment.
335
Recent Advances in Natural Language Processing IV:
Selected Papers frp, RANLP 2005, pages 187?196.
Stuart Russel and Peter Norvig. 2010. Artificial Intelli-
gence. A Modern Approach. Pearson, 2nd edition.
Mark Sammons, V.G.Vinod Vydiswaran, Tim Vieira,
Nikhil Johri, Ming-Wei Chang, Dan Goldwasser,
Vivek Srikumar, Gourab Kundu, Yuancheng Tu, Kevin
Small, Joshua Rule, Quang Do, and Dan Roth. 2009.
Relation Alignment for Textual Entailment Recogni-
tion. In Text Analysis Conference (TAC).
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49.
Klaus von Heusinger. 1999. Intonation and Information
Structure. The Representation of Focus in Phonology
and Semantics. Habilitationssschrift, Universita?t Kon-
stanz, Konstanz, Germany.
Sholom M. Weiss and Casimir A. Kulikowski. 1991.
Computer systems that learn. Morgan Kaufmann, San
Mateo, CA.
Ramon Ziai, Niels Ott, and Detmar Meurers. 2012.
Short answer assessment: Establishing links between
research strands. In Proceedings of the 7th Workshop
on Innovative Use of NLP for Building Educational
Applications (BEA-7) at NAACL-HLT 2012, Montreal.
336
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 197?206,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Combining Shallow and Linguistically Motivated Features in
Native Language Identification
Serhiy Bykh Sowmya Vajjala Julia Krivanek Detmar Meurers
Seminar fu?r Sprachwissenschaft, Universita?t Tu?bingen
{sbykh, sowmya, krivanek, dm}@sfs.uni-tuebingen.de
Abstract
We explore a range of features and ensembles
for the task of Native Language Identification
as part of the NLI Shared Task (Tetreault et al,
2013). Starting with recurring word-based n-
grams (Bykh and Meurers, 2012), we tested
different linguistic abstractions such as part-
of-speech, dependencies, and syntactic trees
as features for NLI. We also experimented
with features encoding morphological proper-
ties, the nature of the realizations of particu-
lar lemmas, and several measures of complex-
ity developed for proficiency and readabil-
ity classification (Vajjala and Meurers, 2012).
Employing an ensemble classifier incorporat-
ing all of our features we achieved an ac-
curacy of 82.2% (rank 5) in the closed task
and 83.5% (rank 1) in the open-2 task. In
the open-1 task, the word-based recurring n-
grams outperformed the ensemble, yielding
38.5% (rank 2). Overall, across all three tasks,
our best accuracy of 83.5% for the standard
TOEFL11 test set came in second place.
1 Introduction
Native Language Identification (NLI) tackles the
problem of determining the native language of an
author based on a text the author has written in a
second language. With Tomokiyo and Jones (2001),
Jarvis et al (2004), and Koppel et al (2005) as first
publications on NLI, the research focus in computa-
tional linguistics is relatively young. But with over
a dozen new publications in the last two years, it is
gaining significant momentum.
In Bykh and Meurers (2012), we explored a data-
driven approach using recurring n-grams with three
levels of abstraction using parts-of-speech (POS). In
the present work, we continue exploring the contri-
bution and usefulness of more linguistically moti-
vated features in the context of the NLI Shared Task
(Tetreault et al, 2013), where our approach is in-
cluded under the team name ?Tu?bingen?.
2 Corpora used
T11: TOEFL11 (Blanchard et al, 2013) This is the
main corpus of the NLI Shared Task 2013. It con-
sists of essays written by English learners with 11
native language (L1) backgrounds (Arabic, Chinese,
French, German, Hindi, Italian, Japanese, Korean,
Spanish, Telugu, Turkish), and from three different
proficiency levels (low, medium, high). Each L1 is
represented by a set of 1100 essays (train: 900, dev:
100, test: 100). The labels for the train and dev sets
were given from the start, the labels for the test set
were provided after the results were submitted.
ICLE: International Corpus of Learner English
(Granger et al, 2009) The ICLEv2 corpus consists
of 6085 essays written by English learners of 16 dif-
ferent L1 backgrounds. They are at a similar level of
English proficiency, namely higher intermediate to
advanced and of about the same age. For the cross-
corpus tasks we used the essays for the seven L1s in
the intersection with T11, i.e., Chinese (982 essays),
French (311), German (431), Italian (391), Japanese
(366), Spanish (248), and Turkish (276).
FCE: First Certificate in English Corpus (Yan-
nakoudakis et al, 2011) The FCE dataset consists
of 1238 scripts produced by learners taking the First
Certificate in English exam, assessing English at an
197
upper-intermediate level. For the cross-corpus tasks,
we used the essays by learners of the eight L1s in
the intersection with T11, i.e., Chinese (66 essays),
French (145), German (69), Italian (76), Japanese
(81), Korean (84), Spanish (198), and Turkish (73).
BALC: BUiD (British University in Dubai) Arab
Learner Corpus (Randall and Groom, 2009) The
BALC corpus consists of 1865 English learner texts
written by students with an Arabic L1 background
from the last year of secondary school and the first
year of university. The texts were scored and as-
signed to six proficiency levels. For the cross-corpus
NLI tasks, we used the data from the levels 3?5
amounting to overall 846 texts. We excluded the two
lowest and the highest, sixth level based on pretests
with the full BALC data.
ICNALE: International Corpus Network of
Asian Learners of English (Ishikawa, 2011) The
version of the ICNALE corpus we used consists of
5600 essays written by college students in ten coun-
tries and areas in Asia as well as by English na-
tive speakers. The learner essays are assigned to
four proficiency levels following the CEFR guide-
lines (A2, B1, B2, B2+). For the cross-corpus tasks,
we used the essays written by learners from Korea
(600 essays) and from Pakistan (400).1 Without ac-
cess to a corpus with Hindi as L1, we decided to la-
bel the essays written by Pakistani students as Hindi.
Most of the languages spoken in Pakistan, including
the official language Urdu, belong to the same Indo-
Aryan/-Iranian language family as Hindi. Our main
focus here was on avoiding overlap with Telugu, the
other Indian language in this shared task, which be-
longs to the Dravidian language family.
TU?TEL-NLI: Tu?bingen Telugu NLI Corpus We
collected 200 English texts written by Telugu native
speakers from bilingual (English-Telugu) blogs, lit-
erary articles, news and movie review websites.
NT11: NON-TOEFL11 We combined the ICLE,
FCE, ICNALE, BALC and TU?TEL-NLI sources
discussed above in the NT11 corpus consisting of
overall 5843 essays for 11 L1s, as shown in Table 1.
1We did not include ICNALE data for more L1s to avoid
overrepresentation of already well-represented Asian L1s.
Corpora
L1 ICLE FCE BALC ICNALE TU?TEL #
ARA - - 846 - - 846
CHI 982 66 - - - 1048
FRE 311 145 - - - 456
GER 431 69 - - - 500
HIN - - - 400 - 400
ITA 391 76 - - - 467
JPN 366 81 - - - 447
KOR - 84 - 600 - 684
SPA 248 198 - - - 446
TEL - - - - 200 200
TUR 276 73 - - - 349
# 3005 792 846 1000 200 5843
Table 1: Distribution of essays for the 11 L1s in NT11
3 Features
Recurring word-based n-grams (rc. word ng.)
Following, Bykh and Meurers (2012), we used all
word-based n-grams occurring in at least two texts
of the training set. We focused on recurring uni-
grams and bigrams, which in our previous work and
in T11 testing with the dev set worked best. For the
larger T11 train ? NT11 set, recurring n-grams up
to length five were best, but for uniformity we only
used word-based unigrams and bigrams for all tasks.
As in our previous work, we used a binary feature
representation encoding the presence or absence of
the n-gram in a given essay.
Recurring OCPOS-based n-grams (rc. OCPOS
ng.) All OCPOS n-grams occurring in at least two
texts of the training set were obtained as described
in Bykh and Meurers (2012). OCPOS means that
the open class words (nouns, verbs, adjectives and
cardinal numbers) are replaced by the corresponding
POS tags. For POS tagging we used the OpenNLP
toolkit (http://opennlp.apache.org).
In Bykh and Meurers (2012), recurring OCPOS
n-grams up to length three performed best. How-
ever, for T11 we found that including four- and five-
grams was beneficial. This confirms our assumption
that longer n-grams can be sufficiently common to
be useful (Bykh and Meurers, 2012, p. 433). Thus
we used the recurring OCPOS n-grams up to length
five for the experiments in this paper. We again used
a binary feature representation.
198
Recurring word-based dependencies (rc. word
dep.) Extending the perspective on recurring pieces
of data to other data types, we explored a new fea-
ture: recurring word-based dependencies. A feature
of this type consists of a head and all its immediate
dependents. The dependencies were obtained using
the MATE parser (Bohnet, 2010). The words in each
n-tuple are recorded in lowercase and listed in the or-
der in which they occur in the text; heads thus are not
singled out in this encoding. For example, the sen-
tence John gave Mary an interesting book yields the
following two potential features (john, gave, mary,
book) and (an, interesting, book). As with recur-
ring n-grams we utilized only features occurring in
at least two texts of the training set, and we used a
binary feature representation.
Recurring function-based dependencies (rc.
func. dep.) The recurring function-based depen-
dencies are a variant of the recurring word-based
dependencies described above, where each depen-
dent is represented by its grammatical function. The
above example sentence thus yields the two features
(sbj, gave, obj, obj) and (nmod, nmod, book).
Complexity Given that the proficiency level of a
learner was shown to play a role in NLI (Tetreault
et al, 2012), we implemented all the text com-
plexity features from Vajjala and Meurers (2012),
who used measures of learner language complex-
ity from SLA research for readability classification.
These features consist of lexical richness and syn-
tactic complexity measures from SLA research (Lu,
2010; 2012) as well as other syntactic parse tree
properties and traditionally used readability formu-
lae. The parse trees were built using the Berke-
ley parser (Petrov and Klein, 2007) and the syntac-
tic complexity measures were estimated using the
Tregex package (Levy and Andrew, 2006).
In addition, we included morphological and POS
features from the CELEX Lexical Database (Baayen
et al, 1995). The morphological properties of words
in CELEX include information about the deriva-
tional, inflectional and compositional features of
the words along with information about their mor-
phological origins and complexity. POS properties
of the words in CELEX describe the various at-
tributes of a word depending on its parts of speech.
We included all the non-frequency based and non-
word-string attributes from the English Morphology
Lemma (EML) and English Syntax Lemma (ESL)
files of the CELEX database. We also defined Age
of Acquisition features based on the psycholinguis-
tic database compiled by Kuperman et al (2012). Fi-
nally, we included the ratios of various POS tags to
the total number of words as POS density features,
using the POS tags from the Berkeley parser output.
Suffix features The use of different derivational
and inflectional suffixes may contain information
regarding the L1 ? either through L1 transfer, or
in terms of what suffixes are taught, e.g., for
nominalization. In a very basic approximation of
morphological analysis, we used the porter stem-
mer implementation of MorphAdorner (http://
morphadorner.northwestern.edu). For each
word in a learner text, we removed the stem
it identified from the word, and if a suffix re-
mained, we matched it against the Wiktionary list of
English suffixes (http://en.wiktionary.org/
wiki/Appendix:Suffixes:English). For each
valid suffix thus identified, we defined a binary fea-
ture (suffix, bin.) recording the presence/absence
and a feature counting the number of occurrences
(suffix, cnt.) in a given learner text.
Stem-suffix features We also wondered whether
the subset of morphologically complex unigrams
may be more indicative than considering all uni-
grams as features. As a simple approximation of this
idea, we used the stemmer plus suffix-list approach
mentioned above and used all words for which a suf-
fix was identified as features, both binary (stemsuf-
fix, bin.) and count-based (stemsuffix, cnt.).
Local trees Based on the syntactic trees assigned
by the Berkeley Parser (Petrov and Klein, 2007), we
extracted all local trees, i.e., trees of depth one. For
example, for the sentence I have a tree, the parser
output is: (ROOT (S (NP (PRP I)) (VP (VBP have)
(NP (DT a) (NN tree))) (. .))) for which the local
trees are (S NP VP .), (NP PRP), (NP DT NN), (VP
VBP NP), (ROOT S). Count-based features are used.
Stanford dependencies Tetreault et al (2012) ex-
plored the utility of basic dependencies as features
for NLI. In our approach, we extracted all Stanford
199
dependencies (de Marneffe et al, 2006) using the
trees assigned by the Berkeley Parser. We consid-
ered lemmatized typed dependencies (type dep. lm.)
such as nsubj(work,human) and POS tagged ones
(type dep. POS) such as nsubj(VB,NN) for our fea-
tures. We used count-based features for those typed
dependencies.
Dependency number (dep. num.) We encoded the
number of dependents realized by a verb lemma,
normalized by this lemma?s count. For example, if
the lemma take occurred ten times in a document,
three times with two dependents and seven times
with three dependents, we get the features take:2-
dependents = 3/10 and take:3-dependents = 7/10.
Dependency variability (dep. var.) These fea-
tures count possible dependent-POS combinations
for a verb lemma, normalized by this verb lemma?s
count. If in the example above, the lemma take
occurred three times with two dependents JJ-NN,
two times with three dependents JJ-NN-VB, and five
times with three dependents NN-NN-VB, we ob-
tain take:JJ-NN = 3/10, take:JJ-NN-VB = 2/10, and
take:NN-NN-VB = 5/10.
Dependency POS (dep. POS) These features are
derived from the dep. var. features and encode how
frequent which kind of category was a dependent for
a given verb lemma. Continuing the example above,
take takes dependents of three different categories:
JJ, NN and VB. For each category, we create a fea-
ture, the value of which is the category count divided
by the number of dependents of the given lemma,
normalized by the lemma?s count in the document.
In the example, we obtain take:JJ = (1/2 + 1/3)/10,
take:NN = (1/2 + 1/3 + 2/3)/10, and take:VB = (1/3
+ 1/3)/10.
Lemma realization matrix (lm. realiz.) We spec-
ified a set of features that is calculated for each dis-
tinct lemma and three feature sets generalizing over
all lemmas of the same category:
1. Distinct lemma counts of a specific category
normalized by the total count of this category
in a document. For example, if the lemma can
is found in a document two times as a verb and
five times as a noun, and the document contains
30 verbs and 50 nouns, we obtain the two fea-
tures can:VB = 2/30 and can:NN = 5/50.
2. Type-Lemma ratio: lemmas of same category
normalized by total lemma count
3. Type-Token ratio: tokens of same category nor-
malized by total token count
4. Lemma-Token Ratio: lemmas of same category
normalized by tokens of same category
Proficiency and prompt features Finally, for some
settings in the closed task we also included two nom-
inal features to encode the proficiency (low, medium,
high) and the prompt (P1?P8) features provided as
meta-data along with the T11 corpus.
4 Results
4.1 Evaluation Setup
We developed our approach with a focus on the
closed task, training the models on the T11 train set
and testing them on the T11 dev set. For the
closed task, we report the accuracies on the dev set
for all models (single feature type models and en-
sembles as introduced in sections 4.2 and 4.3),
before presenting the accuracies on the submitted
test set models, which were trained on the T11 train
? dev set. In addition, for the submitted models
we report the accuracies obtained via 10-fold cross-
validation on the T11 train ? dev set using the folds
specification provided by the organizers of the NLI
Shared Task 2013.
The results for the open-1 task are obtained by
training the models on the NT11 set, and the results
for the open-2 task are obtained by training the mod-
els on the T11 train ? dev set ? NT11 set. For the
open-1 and open-2 tasks, we report the basic single
feature type results on the T11 dev set and two sets
of results on the T11 test set: the results for the ac-
tual submitted systems and the results for the com-
plete systems, i.e., including the features used in the
closed task submissions that for the open tasks were
only computed after the submission deadline (given
our focus on the closed task and finite computational
infrastructure). We include the figures for the com-
plete systems to allow a proper comparison of the
performance of our models across the tasks.
Below we provide a description of the various ac-
curacies (%) we report for the different tasks:
200
? Acctest: Accuracy on the T11 test set after
training the model on:
? closed: T11 train ? dev set
? open-1: NT11 set
? open-2: T11 train ? dev set ? NT11 set
? Accdev: Accuracy on the T11 dev set after
training the model on:
? closed: T11 train set
? open-1: NT11 set
? open-2: T11 train set ? NT11 set
? Acc10train?dev: Accuracy on the T11 train ? dev
set obtained via 10-fold cross-validation using
the data split information provided by the orga-
nizers, applicable only for the closed task.
In terms of the tools used for classification, we
employed LIBLINEAR (Fan et al, 2008) using
L2-regularized logistic regression, LIBSVM (Chang
and Lin, 2011) using C-SVC with the RBF kernel
and WEKA SMO (Platt, 1998; Hall et al, 2009) fit-
ting logistic models to SVM outputs (the -M option).
Which classifier was used where is discussed below.
4.2 Single Feature Type Classifier Results
First we evaluated the performance of each fea-
ture separately for the closed task by computing the
Accdev values. These results constituted the basis
for the ensembles discussed in section 4.3. We also
report the corresponding results for the open-1 and
open-2 tasks, which were partly obtained after the
system submission and thus were not used for de-
veloping the approach. As classifier, we generally
used LIBLINEAR, except for complexity and lm.
realiz., where SMO performed consistently better.
The summary of the single feature type performance
is shown in Table 2.
The results reveal some first interesting insights
into the employed feature sets. The figures show
that the recurring word-based n-grams (rc. word ng.)
taken from Bykh and Meurers (2012) are the best
performing single feature type in our set yielding an
Accdev value of 81.3%. This finding is in line with
the previous research on different data sets showing
that lexical information seems to be highly relevant
for the task of NLI (Brooke and Hirst, 2011; Bykh
and Meurers, 2012; Jarvis et al, 2012; Jarvis and
Paquot, 2012; Tetreault et al, 2012). But also the
more abstract linguistic features, such as complexity
Accdev
Feature type closed open-1 open-2
1. rc. word ng. 81.3 42.0 80.3
2. rc. OCPOS ng. 67.6 26.6 64.8
3. rc. word dep. 67.7 30.9 69.4
4. rc. func. dep. 62.4 28.2 61.3
5. complexity 37.6 19.7 36.5
6. stemsuffix, bin. 50.3 21.4 48.8
7. stemsuffix, cnt. 48.2 19.3 47.1
8. suffix, bin. 20.4 9.1 17.5
9. suffix, cnt. 19.0 13.0 17.7
10. type dep. lm. 67.3 25.7 67.5
11. type dep. POS 46.6 27.8 27.6
12. local trees 49.1 26.2 25.7
13. dep. num. 39.7 19.6 41.8
14. dep. var. 41.5 18.6 40.1
15. dep. POS 47.8 21.5 47.4
16. lm. realiz. 70.3 30.3 66.9
Table 2: Single feature type results on T11 dev set
measures, local trees, or dependency variation mea-
sures seem to contribute relevant information, con-
sidering the random baseline of 9% for this task.
Having explored the performance of the single
feature type models, the interesting question was,
whether it is possible to obtain a higher accuracy
than yielded by the recurring word-based n-grams
by combining multiple feature types into a single
model. We thus investigated different combinations,
with a primary focus on the closed task.
4.3 Combining Feature Types
We followed Tetreault et al (2012) in exploring two
options: On the one hand, we combined the differ-
ent feature types directly in a single vector. On the
other hand, we used an ensemble classifier. The en-
semble setup used combines the probability distribu-
tions provided by the individual classifier for each
of the incorporated feature type models. The indi-
vidual classifiers were trained as discussed above,
and ensembles were trained and tested using LIB-
SVM, which in our tests performed better for this
purpose than LIBLINEAR. To obtain the ensemble
training files, we performed 10-fold cross-validation
for each feature model on the T11 train set (for in-
ternal evaluation) and on the T11 train ? dev set (for
201
submission) and took the corresponding probability
estimate distributions. For the ensemble test files,
we took the probability estimate distribution yielded
by each feature model trained on the T11 train set
and tested on the T11 dev set (for internal evalua-
tion), as well as by each feature model trained on
the T11 train ? dev set and tested on the T11 test set
(for submission).
In our tests, the ensemble classifier always outper-
formed the single vector combination, which is in
line with the findings of Tetreault et al (2012). We
thus focused on ensemble classification for combin-
ing the different feature types.
4.4 Closed Task (Main) Results
We submitted the predictions for the systems listed
in Table 3, which we chose in order to test all fea-
ture types together, the best performing single fea-
ture type, everything except for the best single fea-
ture type, and two subsets, with the latter primarily
including more abstract linguistic features.
id system description system type
1 overall system ensemble
2 rc. word ng. single model
3 #1 minus rc. word ng. ensemble
4 well performing subset ensemble
5 ?linguistic subset? ensemble
Table 3: Submitted systems for all three tasks
The results for the submitted systems are shown in
Table 4. Here and in the following result tables, the
system ids in the table headers correspond to the ids
in Table 3, the best result on the test set is shown in
bold, and the symbols have the following meaning:
? x = feature type used
? - = feature type not used
? -* = feature type ready after submission
We report theAcctest,Accdev andAcc10train?dev ac-
curacies introduced in section 4.1. The Accdev re-
sults are consistently better than the Acctest results,
highlighting that relying on a single development
set can be problematic. The cross-validation results
are more closely aligned with the ultimate test set
performance.
systems
Feature type 1 2 3 4 5
1. rc. word ng. x x - x -
2. rc. OCPOS ng. x - x x -
3. rc. word dep. x - x x -
4. rc. func. dep. x - x x -
5. complexity x - x x x
6. stemsuffix, bin. x - x x x
7. stemsuffix, cnt. x - x - x
8. suffix, bin. x - x x x
9. suffix, cnt. x - x - x
10. type dep. lm. x - x - x
11. type dep. POS x - x - x
12. local trees x - x - x
13. dep. num. x - x x -
14. dep. var. x - x x -
15. dep. POS x - x x -
16. lm. realiz. x - x x -
proficiency x - x x -
prompt x - x x -
Acctest 82.2 79.6 81.0 81.5 74.7
Accdev 85.4 81.3 83.5 84.9 76.3
Acc10train?dev 82.4 78.9 80.7 81.7 74.1
Table 4: Results for the closed task
Overall, comparing the results for the different
systems shows the following main points (with the
system ids in the discussion shown in parentheses):
? The overall system performed better than any
single feature type alone (cf. Tables 2 and 4).
The ensemble thus is successful in combining
the strengths of the different feature types.
? The rc. word ng. feature type alone (2) per-
formed very well, but the overall system with-
out that feature type (3) still outperformed it.
Thus apparently the different properties ac-
cessed by more elaborate linguistic modelling
contribute some information not provided by
the surface-based n-gram feature.
? A system incorporating a subset of the differ-
ent feature types (4) performed still reasonably
well. Hence, it is conceivable that a subsys-
tem consisting of some selected feature types
would perform equally well (eliminating only
information present in multiple feature types)
or even outperform the overall system (by re-
moving some noise). This point will be inves-
tigated in detail in our future work.
202
? System 5, combining a subset of feature types,
where each one incorporates some degree
of linguistic abstraction (in contrast to pure
surface-based feature types such as word-based
n-grams), performed at a reasonably high level,
supporting the assumption that incorporating
more linguistic knowledge into the system de-
sign has something to contribute.
Putting our results into the context of the NLI
Shared Task 2013, with our best Acctest value of
82.2% for closed as the main task, we ranked fifth
out of 29 participating teams. The best result in
the competition, obtained by the team ?Jarvis?, is
83.6%. According to the significance test results
provided by the shared task organizers, the differ-
ence of 1.4% is not statistically significant (0.124
for pairwise comparison using McNemar?s test).
4.5 Open-1 Task Results
The Accdev values for the single feature type models
for the open-1 task were included in Table 2. The
results for the test set are presented in Table 5. We
report two different Acctest values: the accuracy for
the actual submitted systems (Acctest) and for the
corresponding complete systems (Acctest with ?) as
discussed in section 4.1.
systems
Feature type 1 2 3 4 5
1. rc. word ng. x x - x -
2. rc. OCPOS ng. x - x x -
3. rc. word dep. x - x x -
4. rc. func. dep. x - x x -
5. complexity x - x x x
6. stemsuffix, bin. x - x x x
7. stemsuffix, cnt. x - x - x
8. suffix, bin. x - x x x
9. suffix, cnt. x - x - x
10. type dep. lm. -? - -? - -?
11. type dep. POS -? - -? - -?
12. local trees -? - -? - -?
13. dep. num. x - x x -
14. dep. var. x - x x -
15. dep. POS x - x x -
16. lm. realiz. x - x x -
Acctest 36.4 38.5 33.2 37.8 21.2
Acctest with ? 37.0 n/a 35.4 n/a 29.9
Table 5: Results for the open-1 task
Conceptually, the open-1 task is a cross-corpus
task, where we used the NT11 data for training and
T11 data for testing. It is more challenging for sev-
eral reasons. First, the models are trained on data
that is likely to be different from the one of the
test set in a number of respects, including possible
differences in genre, task and topic, or proficiency
level. Second, the amount of data we were able to
obtain to train our model is far below what was pro-
vided for the closed task. Thus a drop in accuracy is
to be expected.
Particularly interesting is the fact that our best re-
sult for the open-1 task (38.5%) was obtained using
the rc. word ng. feature type alone. Thus adding
the more abstract features did not improve the accu-
racy. The reason for that may be the smaller train-
ing corpus size, the uneven distribution of the texts
among the different L1s in the NT11 corpus, or the
mentioned potential differences between NT11 and
T11 in genre, task and topic, and learner proficiency.
Also interesting is the fact that the system combining
a subset of feature types outperformed the overall
system. This finding supports the assumption men-
tioned in section 4.4 that the ensemble classifier can
be optimized by informed, selective model combina-
tion instead of combining all available information.
To put our results into the context of the NLI
Shared Task 2013, our best Acctest value of 38.5%
for the open-1 task achieved rank two out of three
participating teams. The best accuracy of 56.5% was
obtained by the team ?Toronto?. While the open-
1 task results in general are much lower than the
closed task results, highlighting an important chal-
lenge for future NLI work, they nevertheless are
meaningful steps forward considering the random
baseline of 9%.
4.6 Open-2 Task Results
For the open-2 task we provide the same information
as for open-1. The Accdev values for the single fea-
ture type models are shown in Table 2, and the two
Acctest values, i.e., the accuracy for the actual sub-
mitted systems (Acctest) and for the complete sys-
tems (Acctest with ?) can be found in Table 6.
For the open-2 task, we put the T11 train ?
dev and NT11 sets together to train our models. The
interesting question behind this task is, whether it is
possible to improve the accuracy of NLI by adding
203
systems
Feature type 1 2 3 4 5
1. rc. word ng. x x - x -
2. rc. OCPOS ng. x - x x -
3. rc. word dep. -? - -? -? -
4. rc. func. dep. x - x x -
5. complexity x - x x x
6. stemsuffix, bin. x - x x x
7. stemsuffix, cnt. x - x - x
8. suffix, bin. x - x x x
9. suffix, cnt. x - x - x
10. type dep. lm. -? - -? - -?
11. type dep. POS x - x - x
12. local trees x - x - x
13. dep. num. x - x x -
14. dep. var. x - x x -
15. dep. POS x - x x -
16. lm. realiz. x - x x -
Acctest 83.5 81.0 79.3 82.5 64.8
Acctest with ? 84.5 n/a 83.3 82.9 79.8
Table 6: Results for the open-2 task
data from corpora other than the one used for test-
ing. This is far from obvious, especially considering
the low results obtained for the open-1 task pointing
to significant differences between the T11 and the
NT11 corpora.
Overall, when using all feature types, our results
for the open-2 task (84.5%) are better than those we
obtained for the closed task (82.2%). So adding data
from a different domain improves the results, which
is encouraging since it indicates that something gen-
eral about the language used is being learned, not
(just) something specific to the T11 corpus. Essen-
tially, the open-2 task also is closest to the real-world
scenario of using whatever resources are available to
obtain the best result possible.
Putting the results into the context of the NLI
Shared Task 2013, our best Acctest value of 83.5%
(84.5%) is the highest accuracy for the open-2 task,
i.e, first rank out of four participating teams.
5 Conclusions
We explored the task of Native Language Identifi-
cation using a range of different feature types in the
context of the NLI Shared Task 2013. We consid-
ered surface features such as recurring word-based
n-grams system as our basis. We then explored
the contribution and usefulness of some more elab-
orate, linguistically motivated feature types for the
given task. Using an ensemble model combining
features based on POS, dependency, parse trees as
well as lemma realization, complexity and suffix in-
formation features, we were able to outperform the
high accuracy achieved by the surface-based recur-
ring n-grams features alone. The exploration of
linguistically-informed features thus is not just of
analytic interest but can also make a quantitative dif-
ference for obtaining state-of-the-art performance.
In terms of future work, we have started exploring
the various feature types in depth to better under-
stand the causalities and correlations behind the re-
sults obtained. We also intend to explore more com-
plex linguistically motivated features further, such
as features based on syntactic alternations as used in
Krivanek (2012). Studying such variation of linguis-
tic properties, instead of recording their presence as
we mostly did in this exploration, also stands to pro-
vide a more directly interpretable perspective on the
feature space identified as effective for NLI.
Acknowledgments
We thank Dr. Shin?ichiro Ishikawa and Dr. Mick
Randall for providing access to the ICNALE corpus
and the BALC corpus respectively. We also thank
the shared task organizers for organizing this inter-
esting competition and sharing the TOEFL11 cor-
pus. Our research is partially funded through the Eu-
ropean Commission?s 7th Framework Program un-
der grant agreement number 238405 (CLARA).
References
R. H. Baayen, R. Piepenbrock, and L. Gulikers. 1995.
The CELEX lexical database (cd-rom). CDROM,
http://www.ldc.upenn.edu/Catalog/
readme_files/celex.readme.html.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
corpus of non-native english. Technical report, Edu-
cational Testing Service.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
24th International Conference on Computational Lin-
guistics (COLING), pages 89?97.
Julian Brooke and Graeme Hirst. 2011. Native lan-
guage detection with ?cheap? learner corpora. In
204
Learner Corpus Research 2011 (LCR 2011), Louvain-
la-Neuve.
Serhiy Bykh and Detmar Meurers. 2012. Native lan-
guage identification using recurring n-grams ? in-
vestigating abstraction and domain dependence. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING), pages 425?
440, Mumbay, India.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27. Software available at http://www.csie.
ntu.edu.tw/?cjlin/libsvm.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher Manning. 2006. Generating typed depen-
dency parses from phrase structure parses. In Proceed-
ings of the 5th International Conference on Language
Resources and Evaluation (LREC-2006), Genoa, Italy,
May 24-26.
R.E. Fan, K.W. Chang, C.J. Hsieh, X.R. Wang, and C.J.
Lin. 2008. Liblinear: A library for large linear classi-
fication. The Journal of Machine Learning Research,
9:1871?1874. Software available at http://www.
csie.ntu.edu.tw/?cjlin/liblinear.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier, and
Magali Paquot, 2009. International Corpus of Learner
English, Version 2. Presses Universitaires de Louvain,
Louvain-la-Neuve.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update. In
The SIGKDD Explorations, volume 11, pages 10?18.
Shin?ichiro Ishikawa. 2011. A new horizon in learner
corpus studies: The aim of the ICNALE projects. In
G. Weir, S. Ishikawa, and K. Poonpon, editors, Cor-
pora and language technologies in teaching, learning
and research, pages 3?11. University of Strathclyde
Publishing, Glasgow, UK. http://language.
sakura.ne.jp/icnale/index.html.
Scott Jarvis and Magali Paquot. 2012. Exploring the
role of n-grams in L1-identification. In Scott Jarvis
and Scott A. Crossley, editors, Approaching Language
Transfer through Text Classification: Explorations in
the Detection-based Approach, pages 71?105. Multi-
lingual Matters.
Scott Jarvis, Gabriela Castan?eda-Jime?nez, and Rasmus
Nielsen. 2004. Investigating L1 lexical transfer
through learners? wordprints. Presented at the 2004
Second Language Research Forum. State College,
Pennsylvania, USA.
Scott Jarvis, Gabriela Castan?eda-Jime?nez, and Rasmus
Nielsen. 2012. Detecting L2 writers? L1s on the
basis of their lexical styles. In Scott Jarvis and
Scott A. Crossley, editors, Approaching Language
Transfer through Text Classification: Explorations in
the Detection-based Approach, pages 34?70. Multilin-
gual Matters.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by mining a
text for errors. In Proceedings of the eleventh ACM
SIGKDD international conference on Knowledge dis-
covery in data mining (KDD ?05), pages 624?628,
New York.
Julia Krivanek. 2012. Investigating syntactic alternations
as characteristic features of learner language. Master?s
thesis, University of Tu?bingen, April.
Victor Kuperman, Hans Stadthagen-Gonzalez, and Marc
Brysbaert. 2012. Age-of-acquisition ratings for
30,000 english words. Behavior Research Methods,
44(4):978?990.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In 5th International Conference on Lan-
guage Resources and Evaluation, Genoa, Italy.
Xiaofei Lu. 2010. Automatic analysis of syntactic
complexity in second language writing. International
Journal of Corpus Linguistics, 15(4):474?496.
Xiaofei Lu. 2012. The relationship of lexical richness
to the quality of ESL learners? oral narratives. The
Modern Languages Journal.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference, pages 404?
411, Rochester, New York, April.
John C. Platt. 1998. Sequential minimal optimiza-
tion: A fast algorithm for training support vector ma-
chines. Technical Report MSR-TR-98-14, Microsoft
Research.
Mick Randall and Nicholas Groom. 2009. The BUiD
Arab learner corpus: a resource for studying the ac-
quisition of L2 english spelling. In Proceedings of the
Corpus Linguistics Conference (CL), Liverpool, UK.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and found:
Resources and empirical evaluations in native lan-
guage identification. In Proceedings of the 24th In-
ternational Conference on Computational Linguistics
(COLING), pages 2585?2602, Mumbai, India.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A report on the first native language identification
shared task. In Proceedings of the Eighth Workshop
on Building Educational Applications Using NLP, At-
lanta, GA, USA, June. Association for Computational
Linguistics.
205
Laura Mayfield Tomokiyo and Rosie Jones. 2001.
You?re not from round here, are you? naive bayes de-
tection of non-native utterance text. In Proceedings of
the 2nd Meeting of the North American Chapter of the
Association for Computational Linguistics (NAACL),
pages 239?246.
Sowmya Vajjala and Detmar Meurers. 2012. On im-
proving the accuracy of readability classification us-
ing insights from second language acquisition. In Joel
Tetreault, Jill Burstein, and Claudial Leacock, editors,
Proceedings of the 7th Workshop on Innovative Use
of NLP for Building Educational Applications (BEA7)
at NAACL-HLT, pages 163?-173, Montre?al, Canada,
June. Association for Computational Linguistics.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automati-
cally grading ESOL texts. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
- Volume 1, HLT ?11, pages 180?189, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Corpus available from http://ilexir.co.uk/
applications/clc-fce-dataset.
206
Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 59?68,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
On The Applicability of Readability Models to Web Texts
Sowmya Vajjala Detmar Meurers
Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
{sowmya,dm}@sfs.uni-tuebingen.de
Abstract
An increasing range of features is being
used for automatic readability classifica-
tion. The impact of the features typically
is evaluated using reference corpora con-
taining graded reading material. But how
do the readability models and the features
they are based on perform on real-world
web texts? In this paper, we want to take a
step towards understanding this aspect on
the basis of a broad range of lexical and
syntactic features and several web datasets
we collected.
Applying our models to web search re-
sults, we find that the average reading level
of the retrieved web documents is rela-
tively high. At the same time, documents
at a wide range of reading levels are iden-
tified and even among the Top-10 search
results one finds documents at the lower
levels, supporting the potential usefulness
of readability ranking for the web. Finally,
we report on generalization experiments
showing that the features we used gener-
alize well across different web sources.
1 Introduction
The web is a vast source of information on a broad
range of topics. While modern search engines
make use of a range of features for identifying and
ranking search results, the question whether a web
page presents its information in a form that is ac-
cessible to a given reader is only starting to receive
attention. Researching the use of readability as-
sessment as a ranking parameter for web search
can be a relevant step in that direction.
Readability assessment has a long history span-
ning various fields of research from Educational
Psychology to Computer Science. At the same
time, the question which features generalize to dif-
ferent types of documents and whether the read-
ability models are appropriate for real-life appli-
cations has only received little attention.
Against this backdrop, we want to see how well
a state-of-the-art readability assessment approach
using a broad range of features performs when ap-
plied to web data. Based on the approach intro-
duced in Vajjala and Meurers (2012), we thus set
out to explore the following two questions in this
paper:
? Which reading levels can be identified in a
systematic sample of web texts?
? How well do the features used generalize to
different web sources?
The paper is organized as follows: Section 2
surveys related work. Section 3 introduces the cor-
pus and the features we used. Section 4 describes
our readability models. Section 5 discusses our ex-
periments investigating the applicability of these
models to web texts. Section 6 reports on a second
set of experiments conducted to test the generaliz-
ability of the features used. Section 7 concludes
the paper with a discussion of our results.
2 Related Work
2.1 Readability Assessment
The need for assessing the readability of a piece
of text has been explored in the educational re-
search community for over eight decades. DuBay
(2006) provides an overview of early readability
formulae, which were based on relatively shallow
features and wordlists. Some of the formulae are
still being used in practice, as exemplified by the
Flesch-Kincaid Grade Level (Kincaid et al, 1975)
available in Microsoft Word.
More recent computational linguistic ap-
proaches view readability assessment as a
59
classification problem and explore different
types of features. Statistical language modeling
has been a popular approach (Si and Callan,
2001; Collins-Thompson and Callan, 2004),
with the hypothesis that the word usage patterns
across grade levels are distinctive enough. Heil-
man et. al. (2007; 2008) extended this approach
by combining language models with manually
and automatically extracted grammatical features.
The relation of text coherence and cohesion
to readability is well explored in the CohMetrix
project (McNamara et al, 2002). Ma et al (2012a;
2012b) approached readability assessment as a
ranking problem and also compared human versus
automatic feature extraction for the task of label-
ing children?s literature.
The WeeklyReader1, an American educational
newspaper with graded readers has been a pop-
ular source of data for readability classification
research in the recent past. Petersen and Osten-
dorf (2009), Feng et al (2009) and Feng (2010)
used it to build readability models with a range
of lexical, syntactic, language modeling and dis-
course features. In Vajjala and Meurers (2012)
we created a larger corpus, WeeBit, by combining
WeeklyReader with graded reading material from
the BBCBitesize website.2 We adapted measures
of lexical richness and syntactic complexity from
Second Language Acquisition (SLA) research as
features for readability classification and showed
that such measures of proficiency can successfully
be used as features for readability assessment.
2.2 Readability Assessment of Web Texts
Despite the significant body of research on read-
ability assessment, applying it to retrieve relevant
texts from the web has elicited interest only in the
recent past. While Benno?hr (2005) and Newbold
et al (2010) created new readability formulae for
this purpose, Ott and Meurers (2010) and Tan et
al. (2012) used existing readability formulae to fil-
ter search engine results. The READ-X project
(Miltsakaki and Troutt, 2008; Miltsakaki, 2009)
combined standard readability formulae with topic
classification to retrieve relevant texts for users.
The REAP Project3 supports the lexical acqui-
sition of individual learners by retrieving texts that
suit a given learner level. Kidwell et al (2011) also
1http://weeklyreader.com
2http://www.bbc.co.uk/bitesize
3http://reap.cs.cmu.edu
used a word-acquisition model for readability pre-
diction. Collins-Thompson et al (2011) and Kim
et al (2012) employed word distribution based
readability models for personalized search and for
creating entity profiles respectively. Nakatani et
al. (2010) followed a language modeling approach
to rank search results to take user comprehension
into account. Google also has an option to filter
search results based on reading level, apparently
using a language modeling approach.4 Kanungo
and Orr (2009) used search result snippet based
features to predict the readability of short web-
summaries.
All the above approaches primarily restrict
themselves to traditional formulae or statistical
language models encoding the distribution of
words. The effect of lexical and syntactic features
as used in recent research on readability thus re-
mains to be studied in a web context. Furthermore,
the generalizability of the features used to other
data sets also remains to be explored. These are
the primary issues we address in this paper.
3 Corpus and Features
Let us turn to answering our first question: Which
reading levels can be identified in a systematic
sample of web texts? To address this question, we
first need to introduce the features we used, the
graded corpus we used to train the model, and the
nature of the readability model.
Since the goal of this paper is not to present
new features but to explore the application of a
readability approach to the web, we here simply
adopt the feature and corpus setup introduced in
Vajjala and Meurers (2012). The WeeBit corpus
used is a corpus of texts belonging to five reading
levels, corresponding to children of age group 7?
16 years. It consists of 625 documents per reading
level. The articles cover a range of fiction and non-
fiction topics. Each article is labeled as belong-
ing to one of five reading levels: Level 2, Level 3,
Level 4, KS3 and GCSE.
We adapted both the lexical and syntactic fea-
tures of Vajjala and Meurers (2012) to build read-
ability models on the basis of the WeeBit corpus
and then studied their applicability to real-world
documents retrieved from the web as well as the
applicability of those features across different web
sources.
4http://goo.gl/aVy93
60
Lexical features (LEXFEATURES) The lexical
features are motivated by the lexical richness mea-
sures used to estimate the quality of language
learners? oral narratives (Lu, 2012). We included
several type-token ratio variants used in SLA re-
search: generic type token ratio, root TTR, cor-
rected TTR, bilogarithmic TTR and Uber Index.
In addition, there are lexical variation measures
used to estimate the distribution of various parts
of speech in the given text. They include the
noun variation, adjective variation, modifier vari-
ation, adverb variation and verb variation, which
represent the proportion of words of the respec-
tive part of speech categories compared to all lex-
ical words in the document. Alternative measures
for verb variation, namely, Squared Verb Variation
and Corrected Verb Variation are also included.
Apart from these, we also added the traditionally
used measures of average number of characters
per word, average number of syllables per word,
and two readability formulae, the Flesch-Kincaid
score (Kincaid et al, 1975) and the Coleman-Liau
score (Coleman and Liau, 1975). Finally, we in-
cluded the percentage of words from the Aca-
demic Word List5. It is a list created by Coxhead
(2000) which consists of words that are more com-
monly found in academic texts.
Syntactic features (SYNFEATURES) These
features are adapted from the syntactic complexity
measures used to analyze second language writing
(Lu, 2010). They are calculated based on the
parser output of the BerkeleyParser (Petrov and
Klein, 2007), using the Tregex (Levy and Andrew,
2006) pattern matcher. They include: mean
lengths of various production units (sentence,
clause and t-unit); clauses per sentence and t-unit;
t-units per sentence; complex-t units per t-unit
and per sentence; dependent clauses per clause,
t-unit and sentence; co-ordinate phrases per
clause, t-unit and sentence; complex nominals per
clause and t-unit; noun phrases, verb phrases and
preposition phrases per sentence; average length
of NP, VP and PP; verb phrases per t-unit; SBARs
per sentence and average parse tree height.
We refer to the feature subset containing all
the traditionally used features (# char. per word,
# syll. per word and # words per sentence) as
TRADFEATURES in this paper.
5http://simple.wiktionary.org/wiki/
Wiktionary:Academic_word_list
4 The Readability Model
In computational linguistics, readability assess-
ment is generally approached as a classification
problem. To our knowledge, only Heilman et al
(2008) and Ma et al (2012a) experimented with
other kinds of statistical models.
We approach readability assessment as a regres-
sion problem. This produces a model which pro-
vides a continuous estimate of the reading level,
enabling us to see if there are documents that fall
between two levels or above the maximal level
found in the training data. We used the WEKA
implementation of linear regression for this pur-
pose. Since linear regression assumes that the data
falls on an interval scale with evenly spaced read-
ing levels, we used numeric values from 1?5 as
reading levels instead of the original class names
in the WeeBit corpus. Table 1 shows the mapping
from WeeBit classes to numeric values, along with
the age groups per class.
WeeBit class Age (years) Reading level
Level 2 7?8 1
Level 3 8?9 2
Level 4 9?10 3
KS3 11?14 4
GCSE 14?16 5
Table 1: WeeBit Reading Levels for Regression
We report Pearson?s correlation coefficient and
Root Mean Square Error (RMSE) as our evalua-
tion metrics. Correlation coefficient measures the
extent of linear relationship between two random
variables. In readability assessment, a high corre-
lation indicates that the texts at a higher difficulty
level are more likely to receive a higher level pre-
diction from the model and those at lower diffi-
culty level would more likely receive a lower pre-
diction. RMSE can be interpreted as the aver-
age deviation in grade levels between the predicted
and the actual values.
We trained four regression models with the fea-
ture subsets introduced in section 3: LEXFEA-
TURES, SYNFEATURES, TRADFEATURES and
ALLFEATURES. While the criterion used in cre-
ating the graded texts in WeeBit is not known, it
is likely that they were created with the traditional
measures in mind. Indeed, the traditional features
also were among the most predictive features in
Vajjala and Meurers (2012). Hence, apart from
61
training the above mentioned four regression mod-
els, we also trained a fifth model excluding the tra-
ditional features and formulae. This experiment
was performed to verify if the traditional features
are creating a skewed model that relies too heavily
on those well-known and thus easily manipulated
features in making decisions on test data. We refer
to this fifth feature group as NOTRAD.
Table 2 shows the result of our regression ex-
periments using 10-fold cross-validation on the
WeeBit corpus, employing the different feature
subsets and the complete feature set.
Feature Set # Features Corr. RMSE
LEXFEATURES 17 0.84 0.78
SYNFEATURES 25 0.88 0.64
TRADFEATURES 3 0.66 1.06
ALLFEATURES 42 0.92 0.54
NOTRAD 37 0.89 0.63
Table 2: Linear Regression Results for WeeBit
The best correlation of 0.92 was achieved with
the complete feature set. 0.92 is considered a
strong correlation and coupled with an RMSE of
0.54, we can conclude that our regression model
is a good model. In comparison, in Vajjala and
Meurers (2012), where we tackle readability as-
sessment as a classification problem, we obtained
93.3% accuracy on this dataset using all features.
Looking at the feature subsets, there also is a
good correlation between the model predictions
and the actual results in the other cases, except
for the model considering only traditional features.
While traditional features often are among the
most predictive features in readability research,
we also found that a model which does not include
them can perform at a comparable level (0.89).
Comparing these results with previous research
using regression modeling for readability assess-
ment is not particularly meaningful because of the
differences in the corpus and the levels used. For
example, while Heilman et al (2008) used a cor-
pus of 289 texts across 12 reading levels achieving
a correlation of 0.77, we used the WeeBit corpus
containing 3125 texts across 5 reading levels.6
We took the two best models of Table 2,
MODALL using ALLFEATURES and MODNO-
TRAD using the NOTRAD feature set, and set out
to answer our first guiding question, about the
6Direct comparisons on the same data set would be most
indicative, but many datasets, such as the corpus used in Heil-
man et al (2008), are not accessible due to copyright issues.
reading levels which such models can identify in a
systematic sample of web texts.
5 Applying readability models to web texts
To investigate the effect of the two readability
models for real-world web texts, we studied their
performance on two types of web data:
? web documents we crawled from specific
web sites that offer the same type of material
for two groups of readers differing in their
reading skills
? web documents identified by a web search
engine for a sample of web queries selected
from a public query log
5.1 Readability of web data drawn from
characteristic web sites
5.1.1 Web test sets used
Following the approach of Collins-Thompson and
Callan (2005) and Sato et al (2008), who eval-
uated readability models using independent web-
based test sets, we compiled three sets of web doc-
uments that given their origin can be classified into
two classes each:
Wiki ? SimpleWiki: Wikipedia7, along with its
manually simplified version Simple Wikipedia8 is
increasingly used in two-class readability classi-
fication tasks and text simplification approaches
(Napoles and Dredze, 2010; Zhu et al, 2010;
Coster and Kauchak, 2011). We use a collection
of 2000 randomly selected parallel articles from
each of the two websites, which in the following
is referred to as WIKI and SIMPLEWIKI.
Time ? Time for Kids: Time for Kids9 is a divi-
sion of the TIME magazine10, which produces ar-
ticles exclusively for children and is used widely
in classrooms. We took a sample of 2000 docu-
ments each from Time and from Time for Kids for
our experiments and refer them TIME and TFK.
NormalNews ? ChildrensNews: We crawled
websites that contain news articles written for chil-
dren (e.g., http://www.firstnews.co.uk) and
categorized them as CHILDRENSNEWS. We also
crawled freely accessible articles from popular
news websites such as BBC or The Guardian and
7http://en.wikipedia.org
8http://simple.wikipedia.org
9http://www.timeforkids.com
10http://www.time.com
62
categorized them as NORMALNEWS. We took
10K documents from each of these two categories
for our experiments.
These three corpus pairs collected as test cases
differ in several aspects. For example, Sim-
pleWikipedia is not targeting children as such,
whereas Time for Kids and ChildrensNews are.
And SimpleWikipedia ? Wikipedia covers paral-
lel articles in two versions, whereas this is not
the case for the the two Time and the two News
corpora. However, as far as we see these differ-
ences are orthogonal to the issue we are research-
ing here, namely their use as real-life test cases to
study the effect of the classification model learned
on the WeeBit data.
We applied the two regression models which
had performed best on the WeeBit corpus (cf. Ta-
ble 2 in section 4) to these web datasets. The aver-
age reading levels of the different datasets accord-
ing to these two models are reported in Table 3.
Data Set MODALL MODNOTRAD
SIMPLEWIKI 3.86 2.67
TFK 4.15 2.72
CHILDRENSNEWS 4.19 2.39
WIKI 4.21 3.33
TIME 5.04 4.07
NORMALNEWS 5.58 4.42
Table 3: Applying the WeeBit regression model to
the six web datasets
The table shows that both MODALL and MOD-
NOTRAD place the documents from the children
websites (SIMPLEWIKI, TFK and CHILDREN-
SNEWS) at lower reading levels than those from
 0
 10
 20
 30
 40
 50
 60
1 2 3 4 5 higher
% o
f do
cum
ent
s be
long
ing 
to a
 rea
ding
 lev
el
Reading level
Distribution of reading levels across web texts with traditional features
SimpleWikiTFKChildrensNewsWikiTimeNormalNews
Figure 1: Reading levels assigned by MODALL
the regular websites for adults (TIME, WIKI and
NORMALNEWS). However, there is an interesting
difference in the predictions made by the two mod-
els. The MODALL model including the traditional
features consistently assigns a higher reading level
to all the documents, and it also fails to separate
CHILDRENSNEWS (4.19) from WIKI (4.20).
To be able to inspect this in detail, we plot-
ted the class-wise reading level distribution of our
regression models. Figure 1 shows the distribu-
tion of reading levels for these web datasets using
MODALL. As we already knew from the averages,
the model assigns somewhat higher reading levels
to all documents, and the figure confirms that the
texts for children (SIMPLEWIKI, TFK and CHIL-
DRENSNEWS) are only marginally distinguished
from the corresponding websites targeting adult
readers (TIME,WIKI and NORMALNEWS). The
NORMALNEWS dataset alo seems to be placed
in a much higher distribution compared to all the
other test sets, with more than 50% of the docu-
ments getting a prediction of ?higher? (the label
used for documents placed at level 6 or higher).
Figure 2 shows the distribution of reading levels
across the test sets according to MODNOTRAD,
the model without traditional features. The model
provides a broader coverage across all reading lev-
els, with documents from children web sites and
SimpleWikipedia clearly being placed at the lower
end of the spectrum and web pages targeting adults
at the higher end. NORMALNEWS documents are
again placed the highest, but less than 10% fall
outside the range established by WeeBit. TIME
shows the highest diversity, with around 20% for
each reading level above the lowest one.
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
1 2 3 4 5 higher
% o
f do
cum
ent
s be
long
ing 
to a
 rea
ding
 lev
el
Reading level
Distribution of reading levels across web texts without traditional features
SimpleWikiTFKChildrensNewsWikiTimeNormalNews
Figure 2: Reading levels using MODNOTRAD
63
The first set of experiments shows that the
readability models which were successful on the
WeeBit reference corpus seem to be able to iden-
tify a corresponding broad range among web doc-
uments that we selected top-down by relying on
prototypical websites targeting ?adult? and ?child?
readers, which are likely to feature more difficult
and easier web documents, respectively. While
we cannot evaluate the difference between the two
models quantitatively, given the lack of an external
gold standard classification of the crawled data,
the MODNOTRAD conceptually seems to do a bet-
ter job at distinguishing the two classes of web-
sites in line with the top-down expectations.
5.2 Readability of search results
Complementing the first set of experiments, estab-
lishing that the readability models are capable of
placing web documents in line with the top-down
classification of the sites they originate from, in
the second set of experiments we want to investi-
gate bottom-up whether for some random topics of
interest, the web offers texts at different readabil-
ity levels. This also is of practical relevance, since
ranking web search results by readability is only
useful if there actually are documents at different
reading levels for a given query.
For this investigation, we took the MOD-
NOTRAD model and used it to estimate the
reading level of web search results. For
web searching, we used the BING search
API (http://datamarket.azure.com/dataset/
bing/search) and computed the reading levels
of the Top-100 search results for a sample of 50
test queries, selected from a publicly accessible
database (Lu and Callan, 2003).
Figure 3 characterizes the data obtained through
the web searches in terms of the percentage of doc-
Figure 3: Documents retrieved per reading level
uments belonging to a given reading level, accord-
ing to the MODNOTRAD model. In the Top-100
search results obtained for each of the 50 queries,
the model identifies documents at all reading lev-
els, with a peak at reading level 4 (corresponding
to KS3 in the original WeeBit dataset).
To determine how much individual queries dif-
fer in terms of the readability of the documents
they retrieve, we also looked at the results for each
query separately. Figure 4 shows the mean read-
ing level of the Top-100 results for each of the 50
search queries. From query to query, the aver-
age readability of the documents retrieved seems
to differ relatively little, with most results falling
into the higher reading levels (4 or above).
Figure 4: Average reading level of search results
Returning to the question whether there are
documents of different reading levels for a given
query, we need to check how much variation exists
around the observed, rather similar averages. Ta-
ble 4 provides the individual reading levels of the
Top-10 search results for a sample of 10 queries
from our experiment, along with the average read-
ing level of the Top-100 results for that query. The
results in Table 4 indicate that indeed there are
documents at a broad range of reading levels even
among the most relevant search results returned by
the BING web search engine.
Looking at the individual query results, we
found that although a lot of news documents
tended towards a higher reading level, it is in-
deed possible to find some texts at lower read-
ing levels even within Top-10 results (indicated in
bold). However, we found that even for queries
that we would expect to result in hits from web-
sites targeting child readers, those sites often did
not make it into the Top-10 results. The same was
true for sites offering ?simple? language, such as
Simple Wikipedia, which was not among the top
64
Result Rank? 1 2 3 4 5 6 7 8 9 10 AvgTop100
Query
local anaesthetic 3.18 4.57 5.35 3.09 4.24 4.6 3.95 4.74 2.72 4.73 3.78
copyright copy law 1.77 4.59 1.43 2.67 4.63 6.2 2.69 1.1 3.87 5.61 4.57
halley comet 1.69 4.47 4.54 4.24 2.37 4.1 4.86 3.56 4.21 3.56 4.04
public offer 4.4 4.35 5.06 5.03 4.36 5.16 4.13 4.67 3.81 1.1 4.39
optic sensor 2.67 3.38 4.5 3.17 2.54 4.19 4.84 1.47 2.2 3.31 3.83
europe union politics 3.61 4.9 6.3 4.02 2.17 4.5 1.47 1.58 4.88 6.33 4.33
presidential poll 4.98 5.38 1.77 6.1 4.76 3.82 1.05 5.11 3.92 4.25 3.95
shakespeare 2.39 2.9 4.2 4.74 4.76 3.89 1.47 2.13 2.6 4.06 3.58
air pollution 1.17 4.93 3.7 2.3 4.36 3.73 3.71 3.49 2.22 2.67 4.21
euclidean geometry 3.88 4.71 4.7 4.3 4.45 4.63 4.04 4.1 3.48 2.58 3.18
Table 4: Reading levels of individual search results
results even when it contained pages directly rel-
evant to the query. To provide access to those
pages, reranking the search results based on read-
ability would thus be of value.
While we do not want to jump to conclusions
based on our sample of 50 queries, the results
of our experiments seem to support the idea that
readability-based re-ranking of web search results
can help users in accessing web documents that
also are at the right level for the given user. Re-
turning to the first overall question that lead us
here, our experiments support the answer that in-
deed there are documents spread across different
reading levels on the web with a tendency towards
higher reading levels.
6 Generalizability of the Feature Set
We can now turn to the second question raised in
the introduction: How well do the features gener-
alize across different classes of web documents?
We saw in section 5.1 that the predictions of the
two models we used varied quite a bit, solely
based on whether the traditional readability fea-
tures were included in the model or not. This con-
firms the need to investigate how generally appli-
cable which types of features are across datasets.
As far as we know, such an experiment vali-
dating the generalizability of features was not yet
performed in this domain. As there are no pub-
licly available graded web datasets to build new
readability models with the same feature set, we
used the datasets we introduced in section 5.1.1 for
creating two-class readability classification mod-
els. Since there are no clear age-group annota-
tions with all these datasets, we decided to use a
broad two-level classification instead of more fine
grained grade levels.
The difference between this experiment and the
previous one lies in the primary question it at-
tempts to answer. Here, the focus is on veri-
fying if the features are capable of building ac-
curate classification models on different training
sets. In the previous experiment, it was on check-
ing if a given classification model (which in that
experiment was trained on the WeeBit corpus) can
successfully discriminate reading levels for docu-
ments from various real-world texts.
We observed in Section 5.1 that with traditional
features, the WeeBit based readability model as-
signed higher reading levels to all the documents
from our web datasets. So, it would perhaps be
a natural step to train these binary classification
models excluding the traditional features. How-
ever, the traditional features may still be useful
(with different weights) for constructing classifi-
cation models with other training data. So, we
trained two sets of models per training set ? one
with ALLFEATURES and another excluding tradi-
tional features (NOTRAD).
We trained binary classification models using
the following training sets:
? TIME ? TFK texts
? WIKI ? SIMPLEWIKI texts
? NORMALNEWS ? KIDSNEWS texts
? TIME+WIKI ? TFK+SIMPLEWIKI texts
We used the Sequential Minimal Optimization
(SMO) algorithm implementation in the WEKA
tool kit to train these classifiers. The choice of
the algorithm here was motivated by the fact that
training is quick and that SMO has successfully
65
been used in previous research on readability as-
sessment (Feng, 2010; Hancke et al, 2012).
Table 5 summarizes the classification accura-
cies obtained with the four models using 10-fold
cross validation for the four web corpora.
Training Set Accuracy-All Accuracy-NoTrad
TIME ? TFK 95.11% 89.52%
WIKI ? SIMPLEWIKI 92.32% 88.81%
NORMALNEWS ? KIDSNEWS 97.93% 92.54%
TIME+WIKI ? TFK+SIMPLEWIKI 93.38% 89.72%
Table 5: Cross-validation accuracies for binary
classification on different web corpora
The results in the table show that the same set
of features consistently result in creating accu-
rate classification models for all four web corpora.
Each of the two-class classification models per-
formed well, despite the fact that the documents
were created by different people and most likely
with different instructions on how to write sim-
ple texts or simplify already existing texts. It was
interesting to note the role of traditional features
in improving the accuracy of these binary classi-
fication models. But, in the previous experiment,
the model with traditional features consistently put
all the documents into higher reading levels. It is
possible that the role of traditional features in the
WeeBit corpus may be skewed as it is likely that it
was prepared with traditional readability measures
in mind. Contrasting the results of these two ex-
periments raises the question of what features hold
more weight in what dataset, which is an interest-
ing issue to explore in the future.
In sum, this experiment provides some clear
evidence for affirmatively answering the second
question about the generalizability of the feature
set we used. The features seem to be sufficiently
general for them to be useful in performing read-
ability assessment of real-world documents.
7 Conclusion and Discussion
In this paper, we set out to investigate the appli-
cability and generalizability of readability models
for real-world web texts. We started with build-
ing readability models using linear regression, on
a 5-level readability corpus with a range of lexi-
cal and syntactic features (section 4). We applied
the two best models thus obtained to several web
datasets we compiled from websites targeting chil-
dren and others designed for adults (section 5.1)
and on the Top-100 results obtained using a stan-
dard web search engine (section 5.2).
We observed that the models identified texts
across a broad range of reading levels in the web
corpora. Our pilot study of the reading levels of
the search results confirmed that readability mod-
els could be useful as re-ranking or filtering pa-
rameters that prioritize relevant results which are
at the right level for a given user. At the same
time, we observed in both these experiments that
the average reading level of general web articles
is relatively high according to our models. Apart
from result ranking, this also calls for the construc-
tion of efficient text simplification systems which
pick up the difficult texts and attempt to simplify
them to a given reading level.
We then proceeded to investigate how well
the features used to build these readability mod-
els generalize across different corpora. For this,
we reused the corpora with articles for children
and adult readers from prototypical websites (sec-
tion 5.1.1) and built four binary classification
models with all of the readability features (sec-
tion 6). Each of the models achieved good clas-
sification accuracies, supporting that the broad
feature set used generalizes well across corpora.
Whether or not to use traditional readability fea-
tures is somewhat difficult to answer since those
formulae are often taken into account when writ-
ing materials, so high classification accuracy on
such corpora may be superficial in that it is not
necessarily indicative of the spectrum of texts
found on the web (section 5.1). This also raises
the more general question which features work
best for which kind of dataset. A systematic ex-
ploration of the effect of the individual features
along with the impact of document topic and genre
on readability would be interesting and relevant to
pursue in the future.
In our future work, we also intend to explore
further features for this task and improve our un-
derstanding of the correlations between the differ-
ent features. Finally, we are considering reformu-
lating readability assessment as ordinal regression
or preference ranking.
Acknowledgements
We would like to thank the anonymous reviewers
for their detailed, useful comments on the paper.
This research was funded by the European Com-
mission?s 7th Framework Program under grant
agreement number 238405 (CLARA).
66
References
Jasmine Benno?hr. 2005. A web-based personalised
textfinder for language learners. Master?s thesis,
School of Informatics, University of Edinburgh.
Meri Coleman and T. L. Liau. 1975. A computer read-
ability formula designed for machine scoring. Jour-
nal of Applied Psychology, 60:283?284.
Kevyn Collins-Thompson and Jamie Callan. 2004.
A language modeling approach to predicting read-
ing difficulty. In Proceedings of HLT/NAACL 2004,
Boston, USA.
Kevyn Collins-Thompson and Jamie Callan. 2005.
Predicting reading difficulty with statistical lan-
guage models. Journal of the American Society for
Information Science and Technology, 56(13):1448?
1462.
K. Collins-Thompson, P. N. Bennett, R. W. White,
S. de la Chica, and D. Sontag. 2011. Personaliz-
ing web search results by reading level. In Proceed-
ings of the Twentieth ACM International Conference
on Information and Knowledge Management (CIKM
2011).
William Coster and David Kauchak. 2011. Simple en-
glish wikipedia: A new text simplification task. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 665?669, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Averil Coxhead. 2000. A new academic word list.
Teachers of English to Speakers of Other Languages,
34(2):213?238.
William H. DuBay. 2006. The Classic Readability
Studies. Impact Information, Costa Mesa, Califor-
nia.
Lijun Feng, Nomie Elhadad, and Matt Huenerfauth.
2009. Cognitively motivated features for readability
assessment. In Proceedings of the 12th Conference
of the European Chapter of the ACL (EACL 2009),
pages 229?237, Athens, Greece, March. Association
for Computational Linguistics.
Lijun Feng. 2010. Automatic Readability Assessment.
Ph.D. thesis, City University of New York (CUNY).
Julia Hancke, Detmar Meurers, and Sowmya Vajjala.
2012. Readability classification for german using
lexical, syntactic, and morphological features. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING), pages 1063?
1080, Mumbay, India.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2007. Combin-
ing lexical and grammatical features to improve
readability measures for first and second language
texts. In Human Language Technologies 2007:
The Conference of the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL-07), pages 460?467, Rochester, New York.
Michael Heilman, Kevyn Collins-Thompson, and
Maxine Eskenazi. 2008. An analysis of statistical
models and features for reading difficulty prediction.
In Proceedings of the 3rd Workshop on Innovative
Use of NLP for Building Educational Applications
at ACL-08, Columbus, Ohio.
Tapas Kanungo and David Orr. 2009. Predicting the
readability of short web summaries. In Proceed-
ings of the Second ACM International Conference
on Web Search and Data Mining, WSDM ?09, pages
202?211, New York, NY, USA. ACM.
P. Kidwell, G. Lebanon, and K. Collins-Thompson.
2011. Statistical estimation of word acquisition with
application to readability prediction. In Journal of
the American Statistical Association. 106(493):21-
30.
Jin Young Kim, Kevyn Collins-Thompson, Paul N.
Bennett, and Susan T. Dumais. 2012. Characteriz-
ing web content, user interests, and search behavior
by reading level and topic. In Proceedings of the
fifth ACM international conference on Web search
and data mining, WSDM ?12, pages 213?222, New
York, NY, USA. ACM.
J. P. Kincaid, R. P. Jr. Fishburne, R. L. Rogers, and
B. S Chissom. 1975. Derivation of new readability
formulas (Automated Readability Index, Fog Count
and Flesch Reading Ease formula) for Navy enlisted
personnel. Research Branch Report 8-75, Naval
Technical Training Command, Millington, TN.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In 5th International Conference on Lan-
guage Resources and Evaluation, Genoa, Italy.
Jie Lu and Jamie Callan. 2003. Content-based retrieval
in hybrid peer-to-peer networks. In Proceedings of
the Twelfth International Conference on Information
and Knowledge Management (CIKM?03).
Xiaofei Lu. 2010. Automatic analysis of syntac-
tic complexity in second language writing. Inter-
national Journal of Corpus Linguistics, 15(4):474?
496.
Xiaofei Lu. 2012. The relationship of lexical richness
to the quality of ESL learners? oral narratives. The
Modern Languages Journal, pages 190?208.
Yi Ma, Eric Fosler-Lussier, and Robert Lofthus. 2012a.
Ranking-based readability assessment for early pri-
mary children?s literature. In Proceedings of the
2012 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies, NAACL HLT ?12,
pages 548?552, Stroudsburg, PA, USA. Association
for Computational Linguistics.
67
Yi Ma, Ritu Singh, Eric Fosler-Lussier, and Robert
Lofthus. 2012b. Comparing human versus au-
tomatic feature extraction for fine-grained elemen-
tary readability assessment. In Proceedings of the
First Workshop on Predicting and Improving Text
Readability for target reader populations, PITR ?12,
pages 58?64, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Danielle S. McNamara, Max M. Louwerse, and
Arthur C. Graesser. 2002. Coh-metrix: Auto-
mated cohesion and coherence scores to predict text
readability and facilitate comprehension. Proposal
of Project funded by the Office of Educational Re-
search and Improvement, Reading Program.
Eleni Miltsakaki and Audrey Troutt. 2008. Real time
web text classification and analysis of reading dif-
ficulty. In Proceedings of the Third Workshop on
Innovative Use of NLP for Building Educational Ap-
plications (BEA-3) at ACL?08, pages 89?97, Colum-
bus, Ohio. Association for Computational Linguis-
tics.
Eleni Miltsakaki. 2009. Matching readers? prefer-
ences and reading skills with appropriate web texts.
In Proceedings of the 12th Conference of the Eu-
ropean Chapter of the Association for Computa-
tional Linguistics: Demonstrations Session, EACL
?09, pages 49?52, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Makoto Nakatani, Adam Jatowt, and Katsumi Tanaka.
2010. Adaptive ranking of search results by consid-
ering user?s comprehension. In Proceedings of the
4th International Conference on Ubiquitous Infor-
mation Management and Communication (ICUIMC
2010), pages 182?192. ACM Press, Suwon, Korea.
Courtney Napoles and Mark Dredze. 2010. Learn-
ing simple wikipedia: a cogitation in ascertaining
abecedarian language. In Proceedings of the NAACL
HLT 2010 Workshop on Computational Linguistics
and Writing: Writing Processes and Authoring Aids,
CL&W ?10, pages 42?50, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Neil Newbold, Harry McLaughlin, and Lee Gillam.
2010. Rank by readability: Document weighting for
information retrieval. In Hamish Cunningham, Al-
lan Hanbury, and Stefan Ru?ger, editors, Advances in
Multidisciplinary Retrieval, volume 6107 of Lecture
Notes in Computer Science, pages 20?30. Springer
Berlin / Heidelberg.
Niels Ott and Detmar Meurers. 2010. Information re-
trieval for education: Making search engines lan-
guage aware. Themes in Science and Technology
Education. Special issue on computer-aided lan-
guage analysis, teaching and learning: Approaches,
perspectives and applications, 3(1?2):9?30.
Sarah E. Petersen and Mari Ostendorf. 2009. A ma-
chine learning approach to reading level assessment.
Computer Speech and Language, 23:86?106.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Satoshi Sato, Suguru Matsuyoshi, and Yohsuke Kon-
doh. 2008. Automatic assessment of japanese text
readability based on a textbook corpus. In LREC?08.
Luo Si and Jamie Callan. 2001. A statistical model for
scientific readability. In Proceedings of the 10th In-
ternational Conference on Information and Knowl-
edge Management (CIKM), pages 574?576. ACM.
Chenhao Tan, Evgeniy Gabrilovich, and Bo Pang.
2012. To each his own: Personalized content se-
lection based on text comprehensibility. In In Pro-
ceedings of WSDM.
Sowmya Vajjala and Detmar Meurers. 2012. On im-
proving the accuracy of readability classification us-
ing insights from second language acquisition. In
Joel Tetreault, Jill Burstein, and Claudial Leacock,
editors, Proceedings of the 7th Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions (BEA7) at NAACL-HLT, pages 163?173, Mon-
tral, Canada, June. Association for Computational
Linguistics.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of The
23rd International Conference on Computational
Linguistics (COLING), August 2010. Beijing, China.
68
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 21?29,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Exploring Measures of ?Readability? for Spoken Language:
Analyzing linguistic features of subtitles
to identify age-specific TV programs
Sowmya Vajjala and Detmar Meurers
LEAD Graduate School, Department of Linguistics
University of T?ubingen
{sowmya,dm}@sfs.uni-tuebingen.de
Abstract
We investigate whether measures of read-
ability can be used to identify age-specific
TV programs. Based on a corpus of BBC
TV subtitles, we employ a range of lin-
guistic readability features motivated by
Second Language Acquisition and Psy-
cholinguistics research.
Our hypothesis that such readability fea-
tures can successfully distinguish between
spoken language targeting different age
groups is fully confirmed. The classifiers
we trained on the basis of these readability
features achieve a classification accuracy
of 95.9%. Investigating several feature
subsets, we show that the authentic mate-
rial targeting specific age groups exhibits
a broad range of linguistics and psycholin-
guistic characteristics that are indicative of
the complexity of the language used.
1 Introduction
Reading, listening, and watching television pro-
grams are all ways to obtain information partly en-
coded in language. Just like books are written for
different target groups, current TV programs target
particular audiences, which differ in their interests
and ability to understand language. For books and
text in general, a wide range of readability mea-
sures have been developed to determine for which
audience the information encoded in the language
used is accessible. Different audiences are com-
monly distinguished in terms of the age or school
level targeted by a given text.
While for TV programs the nature of the inter-
action between the audio-visual presentation and
the language used is a relevant factor, in this pa-
per we want to explore whether the language by
itself is equally characteristic of the particular age
groups targeted by a given TV program. We thus
focused on the language content of the program
as encoded in TV subtitles and explored the role
of text complexity in predicting the intended age
group of the different programs.
The paper is organized as follows. Section 2
introduces the corpus we used, and section 3 the
readability features employed and their motiva-
tion. Section 4 discusses the experimental setup,
the experiments we conducted and their results.
Section 5 puts our research into the context of re-
lated work, before section 6 concludes and pro-
vides pointers to future research directions.
2 Corpus
The BBC started subtitling all the scheduled pro-
grams on its main channels in 2008, implement-
ing UK regulations designed to help the hearing
impaired. Van Heuven et al. (2014) constructed a
corpus of subtitles from the programs run by nine
TV channels of the BBC, collected over a period
of three years, January 2010 to December 2012.
They used this corpus to compile an English word
frequencies database SUBTLEX-UK
1
, as a part of
the British Lexicon Project (Keuleers et al., 2012).
The subtitles of four channels (CBeebies, CBBC,
BBC News and BBC Parliament) were annotated
with the channel names.
While CBeebies targets children aged under 6
years, CBBC telecasts programs for children 6?12
years old. The other two channels (News, Parlia-
ment) are not assigned to a specific age-group, but
it seems safe to assume that they target a broader,
adult audience. In sum, we used the BBC subtitle
corpus with a three-way categorization: CBeebies,
CBBC, Adults.
Table 1 shows the basic statistics for the overall
corpus. For our machine learning experiments, we
use a balanced subcorpus with 3776 instances for
each class. As shown in the table, the programs for
1
http://crr.ugent.be/archives/1423
21
Program Category Age group # texts avg. tokens avg. sentence length
per text (in words)
CBEEBIES < 6 years 4846 1144 4.9
CBBC 6?12 years 4840 2710 6.7
Adults (News + Parliament) > 12 years 3776 4182 12.9
Table 1: BBC Subtitles Corpus Description
the older age-groups tend to be longer (i.e., more
words per text) and have longer sentences. While
text length and sentence length seem to constitute
informative features for predicting the age-group,
we hypothesized that other linguistic properties of
the language used may be at least as informative as
those superficial (and easily manipulated) proper-
ties. Hence, we explored a broad linguistic feature
set encoding various aspects of complexity.
3 Features
The feature set we experimented with consists of
152 lexical and syntactic features that are primar-
ily derived from the research on text complexity
in Second Language Acquisition (SLA) and Psy-
cholinguistics. There are four types of features:
Lexical richness features (LEX): This group
consists of various part-of-speech (POS) tag den-
sities, lexical richness features from SLA research,
and the average number of senses per word.
Concretely, the POS tag features are: the pro-
portion of words belonging to different parts of
speech (nouns, proper nouns, pronouns, determin-
ers, adjectives, verbs, adverbs, conjunctions, in-
terjections, and prepositions) and different verb
forms (VBG, VBD, VBN, VBP in the Penn Tree-
bank tagset; Santorini 1990) per document.
The SLA-based lexical richness features we
used are: type-token ratio and corrected type-
token ratio, lexical density, ratio of nouns, verbs,
adjectives and adverbs to the number of lexical
words in a document, as described in Lu (2012).
The POS information required to extract these
features was obtained using Stanford Tagger
(Toutanova et al., 2003). The average number of
senses for a non-function word was obtained by
using the MIT WordNet API
2
(Finlayson, 2014).
Syntactic complexity features (SYNTAX): This
group of features encodes the syntactic complex-
ity of a text derived from the constituent struc-
ture of the sentences. Some of these features are
2
http://projects.csail.mit.edu/jwi
derived from SLA research (Lu, 2010), specif-
ically: mean lengths of production units (sen-
tence, clause, t-unit), sentence complexity ratio
(# clauses/sentence), subordination in a sentence
(# clauses per t-unit, # complex t-units per t-unit,
# dependent clauses per clause and t-unit), co-
ordination in a sentence (# co-ordinate phrases
per clause and t-unit, # t-units/sentence), and spe-
cific syntactic structures (# complex nominals per
clause and t-unit, # VP per t-unit). Other syntactic
complexity features we made use of are the num-
ber of NPs, VPs, PPs, and SBARs per sentence
and their average length (in terms of # words), the
average parse tree height and the average number
of constituents per sub-tree.
All of these features were extracted using the
Berkeley Parser (Petrov and Klein, 2007) and the
Tregex pattern matcher (Levy and Andrew, 2006).
While the selection of features for these two
classes is based on Vajjala and Meurers (2012), for
the following two sets of features, we explored fur-
ther information available through psycholinguis-
tic resources.
Psycholinguistic features (PSYCH): This group
of features includes an encoding of the average
Age-of-acquisition (AoA) of words according to
different norms as provided by Kuperman et al.
(2012), including their own AoA rating obtained
through crowd sourcing. It also includes mea-
sures of word familiarity, concreteness, imageabil-
ity, meaningfulness and AoA as assigned in the
MRC Psycholinguistic database
3
(Wilson, 1988).
For each feature, the value per text we computed
is the average of the values for all the words in the
text that had an entry in the database.
While these measures were not developed with
readability analysis in mind, we came across one
paper using such features as measures of word
difficulty in an approach to lexical simplification
(Jauhar and Specia, 2012).
3
http://www.psych.rl.ac.uk/
22
Celex features (CELEX): The Celex lexical
database (Baayen et al., 1995) for English con-
sists of annotations for the morphological, syntac-
tic, orthographic and phonological properties for
more than 50k words and lemmas. We included
all the morphological and syntactic properties that
were encoded using character or numeric codes in
our feature set. We did not use frequency informa-
tion from this database.
In all, this feature set consists of 35 morpholog-
ical and 49 syntactic properties per lemma. The
set includes: proportion of morphologically com-
plex words, attributive nouns, predicative adjec-
tives, etc. in the text. A detailed description of
all the properties of the words and lemmas in this
database can be found in the Celex English Lin-
guistic Guide
4
.
For both the PSYCH and CELEX features,
we encode the average value for a given text.
Words which were not included in the respec-
tive databases were ignored for this computation.
On average, around 40% of the words from texts
for covered by CELEX, 75% by Kuperman et al.
(2012) and 77% by the MRC database.
We do not use any features encoding the occur-
rence or frequency of specific words or n-grams in
a document.
4 Experiments and Results
4.1 Experimental Setup
We used the WEKA toolkit (Hall et al., 2009) to
perform our classification experiments and evalu-
ated the classification accuracy using 10-fold cross
validation. As classification algorithm, we used
the Sequential Minimal Optimization (SMO) im-
plementation in WEKA, which marginally outper-
formed (1?1.5%) some other classification algo-
rithms (J48 Decision tree, Logistic Regression and
Random Forest) we tried in initial experiments.
4.2 Classification accuracy with various
feature groups
We discussed in the context of Table 1 that sen-
tence length may be a good surface indicator of
the age-group. So, we first constructed a classifi-
cation model with only one feature. This yielded
a classification accuracy of 71.4%, which we con-
sider as our baseline (instead of a basic random
baseline of 33%).
4
http://catalog.ldc.upenn.edu/docs/
LDC96L14/eug_a4.pdf
We then constructed a model with all the fea-
tures we introduced in section 3. This model
achieves a classification accuracy of 95.9%, which
is a 23.7% improvement over the sentence length
baseline in terms of classification accuracy.
In order to understand what features contribute
the most to classification accuracy, we applied fea-
ture selection on the entire set, using two algo-
rithms available in WEKA, which differ in the way
they select feature subsets:
? InfoGainAttributeEval evaluates the features
individually based on their Information Gain
(IG) with respect to the class.
? CfsSubsetEval (Hall, 1999) chooses a feature
subset considering the correlations between
features in addition to their predictive power.
Both feature selection algorithms use methods
that are independent of the classification algorithm
as such to select the feature subsets.
Information Gain-based feature selection re-
sults in a ranked list of features, which are inde-
pendent of each other. The Top-10 features ac-
cording to this algorithm are listed in Table 2.
Feature Group
avg. AoA (Kuperman et al., 2012) PSYCH
avg. # PPs in a sentence SYNTAX
avg. # instances where the lemma
has stem and affix
CELEX
? avg. parse tree height SYNTAX
? avg. # NPs in a sentence SYNTAX
avg. # instances of affix substitution CELEX
? avg. # prep. in a sentence LEX
avg. # instances where a lemma is
not a count noun
CELEX
avg. # clauses per sentence SYNTAX
? sentence length SYNTAX
Table 2: Ranked list of Top-10 features using IG
As is clear from their description, all Top-10
features encode different linguistic aspects of a
text. While there are more syntactic features fol-
lowed by Celex features in these Top-10 features,
the most predictive feature is a psycholinguistic
feature encoding the average age of acquisition of
words. A classifier using only the Top-10 IG fea-
tures achieves an accuracy of 84.5%.
Applying CfsSubsetEval to these Top-10 fea-
tures set selects the six features not prefixed by a
23
hyphen in the table, indicating that these features
do not correlate with each other (much). A clas-
sifier using only this subset of 6 features achieves
an accuracy of 84.1%.
We also explored the use of CfsSubsetEval fea-
ture selection on the entire feature set instead of
using only the Top 10 features. From the total of
152 features, CfsSubsetEval selected a set of 41
features. Building a classification model with only
these features resulted in a classification accuracy
of 93.9% which is only 2% less than the model
including all the features.
Table 3 shows the specific feature subset se-
lected by the CfsSubsetEval method, including
# preposition phrases
# t-units
# co-ordinate phrases per t-unit
# lexical words in total words
# interjections
# conjunctive phrases
# word senses
# verbs
# verbs, past participle (VBN)
# proper nouns
# plural nouns
avg. corrected type-token ratio
avg. AoA acc. to ratings of Kuperman et al. (2012)
avg. AoA acc. to ratings of Cortese and Khanna (2008)
avg. word imageability rating (MRC)
avg. AoA according to MRC
# morph. complex words (e.g., sandbank)
# morph. conversion (e.g., abandon)
# morph. irrelevant (e.g., meow)
# morph. obscure (e.g., dedicate)
# morph. may include root (e.g., imprimatur)
# foreign words (e.g., eureka)
# words with multiple analyses (e.g., treasurer)
# noun verb affix compounds (e.g., stockholder)
# lemmas with stem and affix (e.g., abundant=abound+ant)
# flectional forms (e.g., bagpipes)
# clipping allomorphy (e.g., phone vs. telephone)
# deriv. allomorphy (e.g., clarify?clarification)
# flectional allomorphy (e.g., verb bear 7? adjective born)
# conversion allomorphy (e.g., halve?half )
# lemmas with affix substitution (e.g., active=action+ive)
# words with reversion (e.g., downpour)
# uncountable nouns
# collective, countable nouns
# collective, uncountable nouns
# post positive nouns.
# verb, expression (e.g., bell the cat)
# adverb, expression (e.g., run amok)
# reflexive pronouns
# wh pronouns
# determinative pronouns
Table 3: CfsSubsetEval feature subset
some examples illustrating the morphological fea-
tures. The method does not provide a ranked list,
so the features here simply appear in the order in
which they are included in the feature vector.
All of these features except for the psycholinguis-
tic features encode the number of occurrences av-
eraged across the text (e.g., average number of
prepositions/sentence in a text) unless explicitly
stated otherwise. The psycholinguistic features
encode the average ratings of words for a given
property (e.g., average AoA of words in a text).
Table 4 summarizes the classification accura-
cies with the different feature subsets seen so far,
with the feature count shown in parentheses.
Feature Subset (#) Accuracy SD
All Features (152) 95.9% 0.37
Cfs on all features (41) 93.9% 0.59
Top-10 IG features (10) 84.5% 0.70
Cfs on IG (6) 84.1% 0.55
Table 4: Accuracy with various feature subsets
We performed statistical significance tests be-
tween the feature subsets using the Paired T-tester
(corrected), provided with WEKA and all the dif-
ferences in accuracy were found to be statistically
significant at p < 0.001. We also provide the Stan-
dard Deviation (SD) of the test set accuracy in the
10 folds of CV per dataset, to make it possible to
compare these experiments with future research on
this dataset in terms of statistical significance.
Table 5 presents the classification accuracies of
individual features from the Top-10 features list
(introduced in Table 2).
Feature Accuracy
AoA Kup Lem 82.4%
# pp 74.0%
# stem & affix 77.7%
avg. parse tree height 73.4%
# np 73.0%
# substitution 74.3%
# prep 72.0%
# uncountable nouns 68.3%
# clauses 72.5%
sentence length 71.4%
Table 5: Accuracies of Top-10 individual features
The table shows that all but one of the features
individually achieves a classification accuracy
above 70%. The first feature (AoA Kup Lem)
24
alone resulted in an accuracy of 82.4%, which is
quite close to the accuracy obtained by all the Top-
10 features together (84.5%).
To obtain a fuller picture of the impact of dif-
ferent feature groups, we also performed ablation
tests removing some groups of features at a time.
Table 6 shows the results of these tests along with
the SD of the 10 fold CV. All the results that are
statistically different at p < 0.001 from the model
with all features (95.9% accuracy, 0.37 SD) are in-
dicated with a *.
Features Acc. SD
All ? AoA Kup Lem 95.9% 0.37
All ? All AoA Features 95.6% 0.58
All ? PSYCH 95.8% 0.31
All ? CELEX 94.7%* 0.51
All ? CELEX?PSYCH 93.6%* 0.66
All ? CELEX?PSYCH?LEX
(= SYNTAX only) 77.5%* 0.99
LEX 93.1%* 0.70
CELEX 90.0%* 0.79
PSYCH 84.5%* 1.12
Table 6: Ablation test accuracies
Interestingly, removing the most predictive in-
dividual feature (AoA Kup Lem) from the feature
set did not change the overall classification accu-
racy at all. Removing all of the AoA features or
all of the psycholinguistic features also resulted in
only a very small drop. The combination of the
linguistic features, covering lexical and syntactic
characteristics as well as the morphological, syn-
tactic, orthographic, and phonological properties
from Celex, thus seem to be equally characteristic
of the texts targeting different age-groups as the
psycholinguistic properties, even though the fea-
tures are quite different in nature.
In terms of separate groups of features, syntac-
tic features alone performed the worst (77.5%) and
lexical richness features the best (93.1%).
To investigate which classes were mixed up by
the classifier, consider Table 7 showing the con-
fusion matrix for the model with all features on a
10-fold CV experiment.
We find that CBeebies is more often con-
fused with the CBBC program for older chil-
dren (156+214) and very rarely with the program
for adults (1+2). The older children programs
(CBBC) are more commonly confused with pro-
grams for adults (36+58) compared to CBeebies
classified as? CBeebies CBBC Adults
CBeebies (0?6) 3619 156 1
CBBC (6?12) 214 3526 36
Adults (12+) 2 58 3716
Table 7: Confusion Matrix
(1+2), which is expected given that the CBBC au-
dience is closer in age to adults than the CBeebies
audience.
Summing up, we can conclude from these ex-
periments that the classification of transcripts into
age groups can be informed by a wide range of lin-
guistics and psycholinguistic features. While for
some practical tasks a few features may be enough
to obtain a classification of sufficient accuracy, the
more general take-home message is that authentic
texts targeting specific age groups exhibit a broad
range of linguistics characteristics that are indica-
tive of the complexity of the language used.
4.3 Effect of text size and training data size
When we first introduced the properties of the cor-
pus in Table 1, it appeared that sentence length
and the overall text length could be important pre-
dictors of the target age-groups. However, the list
of Top-10 features based on information gain was
dominated by more linguistically oriented syntac-
tic and psycholinguistic features.
Sentence length was only the tenth best feature
by information gain and did not figure at all in the
43 features chosen by the CfsSubsetEval method
selecting features that are highly correlated with
the class prediction while having low correlation
between themselves. As mentioned above, sen-
tence length as an individual feature only achieved
a classification accuracy of 71.4%.
The text length is not a part of any feature set we
used, but considering the global corpus properties
we wanted to verify how well it would perform
and thus trained a model with only text length
(#sentences per text) as a feature. This achieved
a classification accuracy of only 56.7%.
The corpus consists of transcripts of whole TV
programs and hence an individual transcript text
typically is longer than the texts commonly used in
readability classification experiments. This raises
the question whether the high classification accu-
racies we obtained are the consequences of the
larger text size.
As a second issue, the training size available for
the 10-fold cross-validation experiments is com-
25
paratively large, given the 3776 text per level
available in the overall corpus. We thus also
wanted to study the impact of the training size on
the classification accuracy achieved.
Pulling these threads together, we compared
the classification accuracy against text length and
training set size to better understand their impact.
For this, we trained models with different text
sizes (by considering the first 25%, 50%, 75% or
100% of the sentences from each text) and with
different training set sizes (from 10% to 100%).
Figure 1 presents the resulting classification ac-
curacy in relation to training set size for the dif-
ferent text sizes. All models were trained with the
full feature set (152 features), using 10-fold cross-
validation as before.
 90
 91
 92
 93
 94
 95
 96
 10  20  30  40  50  60  70  80  90  100
clas
sific
atio
n ac
cura
cy (
in p
erce
nt)
training set size (in percent)
Variation of Classification Accuracy with training set size and text sample size
25% text size50% text size75% text size100% text size
Figure 1: Classification accuracy for different text
sizes and training set sizes
As expected, both the training set size and the
text size affect the classification accuracy. How-
ever, the classification accuracy even for the small-
est text and training set size is always above 90%,
which means that the unusually large text and
training size is not the main factor behind the very
high accuracy rates.
In all four cases of text size, there was a small
effect of training set size on the classification ac-
curacy. But the effect reduced as the text size in-
creased. At 25% text size, for example, the clas-
sification accuracy ranged 90?93% (mean 92.1%,
SD 0.9) as the training set size increased from 10%
to 100%. However, at 100% text size, the range
was only 94.8?96% (mean 95.6%, SD 0.4).
Comparing the results in terms of text size
alone, larger text size resulted in better classifica-
tion accuracy in all cases, irrespective of the train-
ing set size. A longer text will simply provide
more information for the various linguistic fea-
tures, enabling the model to deliver better judg-
ments about the text. However, despite the text
length being reduced to one fourth of its size, the
models built with our feature set always collect
enough information to ensure a classification ac-
curacy of at least 90%.
In the above experiments, we varied the text size
from 10% to 100%. But since these are percent-
ages, texts from CBBC and Adults on average still
are longer than CBEEBIES texts. While this re-
flects the fact that TV transcripts in real life are of
different length, we also wanted to see what hap-
pens when we eliminate such length differences.
We thus trained classification models fixing the
length of all documents to a concrete absolute
length, starting from 100 words (rounded off to the
nearest sentence boundary) increasing the text size
until we achieve the best overall performance. Fig-
ure 2 displays the classification accuracy we ob-
tained for the different (maximum) text sizes, for
all features and feature subsets.
 65
 70
 75
 80
 85
 90
 95
 100
 100  200  300  400  500  600  700  800  900
clas
sific
atio
n ac
cura
cy (
in p
erce
nt)
max. text size (in number of words)
Variation of Classification Accuracy with text sample size in words
All FeaturesPSYCHLEXSYNCELEX
Figure 2: Classification accuracy for different ab-
solute text sizes (in words)
The plot shows that the classification accuracy
already reaches 80% accuracy for short texts, 100
words in length, for the model with all features. It
rises to above 90% for texts which are 300 words
long and reaches the best overall accuracy of al-
most 96% for texts which are 900 words in length.
All the feature subsets too follow the same trend,
with varying degrees of accuracy that is always
lower than the model with all features.
While in this paper, we focus on documents,
the issue whether the data can be reduced further
26
to perform readability at the sentence level is dis-
cussed in Vajjala and Meurers (2014a).
5 Related Work
Analyzing the complexity of written texts and
choosing suitable texts for various target groups
including children is widely studied in computa-
tional linguistics. Some of the popular approaches
include the use of language models and machine
learning approaches (e.g., Collins-Thompson and
Callan, 2005; Feng, 2010). Web-based tools such
as REAP
5
and TextEvaluator
6
are some examples
of real-life applications for selecting English texts
by grade level.
In terms of analyzing spoken language, research
in language assessment has analyzed spoken tran-
scripts in terms of syntactic complexity (Chen and
Zechner, 2011) and other textual characteristics
(Crossley and McNamara, 2013).
In the domain of readability assessment,
the Common Core Standards (http://www.
corestandards.org) guideline texts were
used as a standard test set in the recent past (Nel-
son et al., 2012; Flor et al., 2013). This test set
contains some transcribed speech. However, to
the best of our knowledge, the process of select-
ing suitable TV programs for children as explored
in this paper has not been considered as a case of
readability assessment of spoken language before.
Subtitle corpora have been created and used
in computational linguistics for various pur-
poses. Some of them include video classifica-
tion (Katsiouli et al., 2007), machine translation
(Petukhova et al., 2012), and simplification for
deaf people (Daelemans et al., 2004). But, we are
not aware of any such subtitle research studying
the problem of automatically identifying TV pro-
grams for various age-groups.
This paper thus can be seen as connecting sev-
eral threads of research, from the analysis of text
complexity and readability, via the research on
measuring SLA proficiency that many of the lin-
guistic features we used stem from, to the com-
putational analysis of speech as encoded in subti-
tles. The range of linguistic characteristics which
turn out to be relevant and the very high preci-
sion with which the age-group classification can
be performed, even when restricting the input to
5
http://reap.cs.cmu.edu
6
https://texteval-pilot.ets.org/
TextEvaluator
artificially shortened transcripts, confirm the use-
fulness of connecting these research threads.
6 Conclusions
In this paper, we described a classification ap-
proach identifying TV programs for different
age-groups based on a range of linguistically-
motivated features derived from research on text
readability, proficiency in SLA, and psycholin-
guistic research. Using a collection of subtitle
documents classified into three groups based on
the targeted age-group, we explored different clas-
sification models with our feature set.
The experiments showed that our linguistically
motivated features perform very well, achieving
a classification accuracy of 95.9% (section 4.2).
Apart from the entire feature set, we also exper-
imented with small groups of features by apply-
ing feature selection algorithms. As it turns out,
the single most predictive feature was the age-
of-acquisition feature of Kuperman et al. (2012),
with an accuracy of 82.4%. Yet when this fea-
ture is removed from the overall feature set, the
classification accuracy is not reduced, highlighting
that such age-group classification is informed by a
range of different characteristics, not just a single,
dominating one. Authentic texts targeting specific
age groups exhibit a broad range of linguistics and
psycholinguistic characteristics that are indicative
of the complexity of the language used.
While an information gain-based feature subset
consisting of 10 features resulted in an accuracy of
84.5%, a feature set chosen using the CfsSubsetE-
val method in WEKA gave an accuracy of 93.9%.
Any of the feature groups we tested exceeded the
random baseline (33%) and a baseline using the
popular sentence length feature (71.4%) by a large
margin. Individual feature groups also performed
well at over 90% accurately in most of the cases.
The analysis thus supports multiple, equally valid
perspectives on a given text, each view encoding a
different linguistic aspect.
Apart from the features explored, we also stud-
ied the effect of the training set size and the length
of the text considered for feature extraction on
classification accuracy (Section 4.3). The size of
training set mattered more when the text size was
smaller. Text size, which did not work well as an
individual feature, clearly influences classification
accuracy by providing more information for model
building and testing.
27
In terms of the practical relevance of the re-
sults, one question that needs some attention is
how well the features and trained models gener-
alize across different type of TV programs or lan-
guages. While we have not yet investigated this
for TV subtitles, in experiments investigating the
cross-corpus performance of a model using the
same feature set, we found that the approach per-
forms well for a range of corpora composed of
reading materials for language learners (Vajjala
and Meurers, 2014b). The very high classification
accuracies of the experiments we presented in the
current paper thus seem to support the assumption
that the approach can be useful in practice for au-
tomatically identifying TV programs for viewers
of different age groups.
Regarding the three class distinctions and the
classifier setup we used in this paper, the approach
can also be generalized to other scales and a re-
gression setup (Vajjala and Meurers, 2013).
6.1 Outlook
The current work focused mostly on modeling and
studying different feature groups in terms of their
classification accuracy. Performing error analysis
and looking at the texts where the approach failed
may yield further insights into the problem. Some
aspects of the text that we did not consider in-
clude discourse coherence or topic effects. Study-
ing these two aspects can provide more insights
into the nature of the language used in TV pro-
grams directed at viewers of different ages. A
cross-genre evaluation between written and spo-
ken language complexity across age-groups could
also be insightful.
On the technical side, it would also be useful
to explore the possibility of using a parser tuned
to spoken language, to check if this helps improve
the classification accuracy of syntactic features.
While in this paper we focused on English, a
related readability model also performed well for
German (Hancke et al., 2012) so that we expect
the general approach to be applicable to other lan-
guages, subject to the availability of the relevant
resources and tools.
Acknowledgements
We would like to thank Marc Brysbaert and his
colleagues for making their excellent resources
available to the research community. We also
thank the anonymous reviewers for their useful
feedback. This research was funded by LEAD
Graduate School (GSC 1028, http://purl.org/
lead), a project of the Excellence Initiative of the
German federal and state governments.
References
Harald R. Baayen, Richard Piepenbrock, and Leon Gu-
likers. 1995. The CELEX lexical database. http:
//catalog.ldc.upenn.edu/LDC96L14.
Maio Chen and Klaus Zechner. 2011. Computing and
evaluating syntactic complexity features for auto-
mated scoring of spontaneous non-native speech. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
722?731, Portland, Oregon, June.
Kevyn Collins-Thompson and Jamie Callan. 2005.
Predicting reading difficulty with statistical lan-
guage models. Journal of the American Society for
Information Science and Technology, 56(13):1448?
1462.
Michael J. Cortese and Maya M. Khanna. 2008. Age
of acquisition ratings for 3,000 monosyllabic words.
Behavior Research Methods, 43:791?794.
Scott Crossley and Danielle McNamara. 2013. Ap-
plications of text analysis tools for spoken re-
sponse grading. Language Learning & Technology,
17:171?192.
Walter Daelemans, Anja Hoethker, and Erik F.
Tjong Kim Sang. 2004. Automatic sentence sim-
plification for subtitling in Dutch and English. In
Fourth International Conference on Language Re-
sources And Evaluation (LREC), pages 1045?1048.
Lijun Feng. 2010. Automatic Readability Assessment.
Ph.D. thesis, City University of New York (CUNY).
Mark Alan Finlayson. 2014. Java libraries for access-
ing the princeton wordnet: Comparison and evalua-
tion. In Proceedings of the 7th Global Wordnet Con-
ference, pages 78?85.
Michael Flor, Beata Beigman Klebanov, and Kath-
leen M. Sheehan. 2013. Lexical tightness and text
complexity. In Proceedings of the Second Workshop
on Natural Language Processing for Improving Tex-
tual Accessibility (PITR) held at ACL, pages 29?38,
Sofia, Bulgaria. ACL.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
The SIGKDD Explorations, 11:10?18.
Mark A. Hall. 1999. Correlation-based Feature Selec-
tion for Machine Learning. Ph.D. thesis, The Uni-
versity of Waikato, Hamilton, NewZealand.
28
Julia Hancke, Detmar Meurers, and Sowmya Vajjala.
2012. Readability classification for german using
lexical, syntactic, and morphological features. In
Proceedings of the 24th International Conference
on Computational Linguistics (COLING): Technical
Papers, pages 1063?1080, Mumbai, India.
Sujay Kumar Jauhar and Lucia Specia. 2012. Uow-
shef: Simplex ? lexical simplicity ranking based on
contextual and psycholinguistic features. In In pro-
ceedings of the First Joint Conference on Lexical
and Computational Semantics (SEM).
Polyxeni Katsiouli, Vassileios Tsetsos, and Stathes
Hadjiefthymiades. 2007. Semantic video classifi-
cation based on subtitles and domain terminologies.
In Proceedings of the 1st International Workshop
on Knowledge Acquisition from Multimedia Content
(KAMC).
Emmanuel Keuleers, Paula Lacey, Kathleen Rastle,
and Marc Brysbaert. 2012. The british lexicon
project: Lexical decision data for 28,730 monosyl-
labic and disyllabic english words. Behavior Re-
search Methods, 44:287?304.
Victor Kuperman, Hans Stadthagen-Gonzalez, and
Marc Brysbaert. 2012. Age-of-acquisition ratings
for 30,000 english words. Behavior Research Meth-
ods, 44(4):978?990.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In 5th International Conference on Lan-
guage Resources and Evaluation, pages 2231?2234,
Genoa, Italy. European Language Resources Asso-
ciation (ELRA).
Xiaofei Lu. 2010. Automatic analysis of syntac-
tic complexity in second language writing. Inter-
national Journal of Corpus Linguistics, 15(4):474?
496.
Xiaofei Lu. 2012. The relationship of lexical richness
to the quality of ESL learners? oral narratives. The
Modern Languages Journal, pages 190?208.
Jessica Nelson, Charles Perfetti, David Liben, and
Meredith Liben. 2012. Measures of text difficulty:
Testing their predictive value for grade levels and
student performance. Technical report, The Coun-
cil of Chief State School Officers.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Volha Petukhova, Rodrigo Agerri, Mark Fishel, Yota
Georgakopoulou, Sergio Penkale, Arantza del Pozo,
Mirjam Sepesy Maucec, Martin Volk, and Andy
Way. 2012. Sumat: Data collection and parallel
corpus compilation for machine translation of sub-
titles. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation
(LREC-2012), pages 21?28, Istanbul, Turkey. Euro-
pean Language Resources Association (ELRA).
Beatrice Santorini. 1990. Part-of-speech tagging
guidelines for the Penn Treebank, 3rd revision, 2nd
printing. Technical report, Department of Computer
Science, University of Pennsylvania.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-
ofspeech tagging with a cyclic dependency net-
work. In HLT-NAACL, pages 252?259, Edmonton,
Canada.
Sowmya Vajjala and Detmar Meurers. 2012. On im-
proving the accuracy of readability classification us-
ing insights from second language acquisition. In
In Proceedings of the 7th Workshop on Innovative
Use of NLP for Building Educational Applications
(BEA) at NAACL-HLT, pages 163?-173, Montr?eal,
Canada. ACL.
Sowmya Vajjala and Detmar Meurers. 2013. On
the applicability of readability models to web texts.
In Proceedings of the Second Workshop on Natural
Language Processing for Improving Textual Acces-
sibility (PITR) held at ACL, pages 59?-68, Sofia,
Bulgaria. ACL.
Sowmya Vajjala and Detmar Meurers. 2014a. Assess-
ing the relative reading level of sentence pairs for
text simplification. In Proceedings of the 14th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL). ACL.
Sowmya Vajjala and Detmar Meurers. 2014b. Read-
ability assessment for text simplification: From an-
alyzing documents to identifying sentential simplifi-
cations. International Journal of Applied Linguis-
tics, Special Issue on Current Research in Read-
ability and Text Simplification, edited by Thomas
Franc?ois and Delphine Bernhard.
Walter J.B. Van Heuven, Pawel Mandera, Emmanuel
Keuleers, and Marc Brysbaert. 2014. Subtlex-UK:
A new and improved word frequency database for
British English. The Quarterly Journal of Experi-
mental Psychology, pages 1?15.
Michael D. Wilson. 1988. The mrc psycholinguis-
tic database: Machine readable dictionary, version
2. Behavioural Research Methods, Instruments and
Computers, 20(1):6?11.
29
LAW VIII - The 8th Linguistic Annotation Workshop, pages 159?168,
Dublin, Ireland, August 23-24 2014.
Focus Annotation in Reading Comprehension Data
Ramon Ziai Detmar Meurers
Sonderforschungsbereich 833
Eberhard Karls Universit?at T?ubingen
{rziai,dm}@sfs.uni-tuebingen.de
Abstract
When characterizing the information structure of sentences, the so-called focus identifies the part
of a sentence addressing the current question under discussion in the discourse. While this notion
is precisely defined in formal semantics and potentially very useful in theoretical and practical
terms, it has turned out to be difficult to reliably annotate focus in corpus data.
We present a new focus annotation effort designed to overcome this problem. On the one hand, it
is based on a task-based corpus providing more explicit context. The annotation study is based
on the CREG corpus (Ott et al., 2012), which consists of answers to explicitly given reading
comprehension questions. On the other hand, we operationalize focus annotation as an incremental
process including several substeps which provide guidance, such as explicit answer typing.
We evaluate the focus annotation both intrinsically by calculating agreement between annotators
and extrinsically by showing that the focus information substantially improves the automatic
meaning assessment of answers in the CoMiC system (Meurers et al., 2011).
1 Introduction
This paper discusses the interplay of linguistic and computational linguistic aspects in the analysis of
focus as a core notion of information structure. Empirically, our work focuses on analyzing the responses
to reading comprehension questions. In computational linguistics, automatic meaning assessment deter-
mining whether a response appropriately answers a given question about a given text has developed into
an active field of research. Short Answer Assessment recently was also highlighted by the Joint Student
Response Analysis and Textual Entailment Challenge (Dzikovska et al., 2013). Some research in this
domain has pointed out the relevance of identifying which parts of a response are given by the question
(Bailey and Meurers, 2008; Mohler et al., 2011), with recent work pointing out that the relevant notion
here is that of focus as discussed in formal pragmatics (Meurers et al., 2011; Hahn and Meurers, 2012).
Figure 1 provides an example of answer comparison for meaning assessment, where the focus (marked
by square brackets) can effectively be used to zoom in on the information that is relevant for comparing a
target answer (TA) with a student answer (SA) given a question (Q).
Figure 1: Answer comparison with the help of focus
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
159
To support this line of research, one needs to be able to identify the focus in a response. As a first step,
we have designed an annotation scheme and manual annotation process for identifying the focus in a
corpus of reading comprehension responses. Focus here is understood in the sense of Krifka (2007) as
indicating the presence of alternatives in the context and being a direct answer to the Question Under
Discussion (QUD, Roberts 1996). This semantic view of focus is essentially language-independent.
Some attempts at systematically identifying focus in authentic data have been made in the past (Dipper
et al., 2007; Calhoun et al., 2010). However, most approaches either capture a notion of focus more
closely related to particular language features, such as the Topic-Focus Articulation and its relation to the
word order in Czech (Bur?a?nov?a et al., 2000), or the approaches were not rewarded with much success
(Ritz et al., 2008). The latter have tried to identify focus in newspaper text or other data types where no
explicit questions are available, making the task of determining the QUD, and thus reliably annotating
focus, very hard. In contrast, in the research presented here, we work with responses to explicitly given
questions that are asked about an explicitly given text. Thus, we can make use of the characteristics of the
questions and text to obtain reliable focus annotation for the responses.
Theoretical linguists have discussed the notion of focus for decades, cf., e.g., Jackendoff (1972),
Stechow (1981), Rooth (1992), Schwarzschild (1999) and B?uring (2007). However, for insights and
arguments from theoretical work to be applicable in computational linguistics, they need to be linked
to thorough empirical work ? an area where some work remains to be done (cf., e.g., De Kuthy and
Meurers, 2012), with some recent research making significant headway (Riester and Baumann, 2013).
As it stands, computational linguists have not yet been able to fully profit from the theoretical debate on
focus. An important reason complementing the one just mentioned is the fact that the context in which
the text to be analyzed is produced has rarely been explicitly taken into account and encoded. Yet, many
of the natural tasks in which focus annotation would be relevant actually do contain explicit task and
context information of relevance to determining focus. To move things forward, this paper builds on
the availability and relevance of task-based language data and presents an annotation study of focus on
authentic reading comprehension data. As a second component of our proposal, we operationalize the
focus annotation in terms of several incremental steps, such as explicit answer typing, which provide
relevant information guiding the focus annotation as such.
Overall, the paper tries to accomplish two goals, which are also reflected in the way the annotation
is evaluated: i) to present an effective focus annotation scheme and to evaluate how consistently it can
be applied, and ii) to explore the possible impact of focus annotation on Short Answer Assessment.
Establishing a focus annotation scheme for question-response pairs from authentic reading comprehension
data involves sharpening and linking the concepts and tests from theoretical linguistic with the wide range
of properties realized in the authentic reading comprehension data. The work thus stands to contribute
both to an empirical evaluation and enrichment of the linguistic concepts as well as to the development of
automatic focus annotation approaches in computational linguistics.
The paper is organized as follows: Section 2 presents the corpus data on which we base the annotation
effort and the annotation process. Section 3 introduces the scheme we developed for annotating the
reading comprehension data. Section 4 then launches into both intrinsic and extrinsic evaluation of the
manual annotation, before section 5 concludes the paper.
2 Data and Annotation Setup
We base our work on the CREG corpus (Ott et al., 2012), a task-based corpus consisting of answers to
reading comprehension questions written by learners of German at the university level. The overall corpus
includes 164 reading texts, 1,517 reading comprehension questions, 2,057 target answers provided by the
teachers, and 36,335 learner answers. We use the CREG-1032 data subset (Meurers et al., 2011) for the
present annotation work in order to enable comparison to previously published results on that data set
(Meurers et al., 2011; Hahn and Meurers, 2012; Horbach et al., 2013). The CREG-1032 data set consists
of two sub-corpora, which correspond to the sites they were collected at, Kansas University (KU) and
Ohio State University (OSU). For the present work, we limited ourselves to the OSU portion of the data
because it contains longer answers and more answers per question.
160
The OSU subset consists of 422 student answers to 60 questions, for which 87 target answers are
available. The student answers were produced by 175 intermediate learners of German in the US, who on
average wrote about 15 tokens per answer. All student answers were rated by two annotators with respect
to whether they answer the question or not. The subset is balanced, i.e. it contains the same number of
correct and incorrect answers, and both annotators agreed on the meaning assessment.
To obtain a gold-standard focus annotation for this data set, we set out to manually annotate both
target answers and student answers with focus. We also annotated the question forms in the question.
The annotation was performed by two graduate research assistants in linguistics using the brat
1
rapid
annotation tool directly on the token level. Each annotator was given a separate directory containing
identical source files to annotate. In order to sharpen distinctions and refine the annotation scheme to its
current state, we drew a random sample of 100 questions, target answers and student answers from each
sub-corpus of CREG and trained our two annotators on them. During this piloting process, the first author
met with the annotators to discuss difficult cases and decide how the scheme would accommodate them.
Figure 2 shows a sample screenshot of the brat tool. The question asks for a person, namely the one
?wandering through the dark outskirts?. The target response provides an answer with an appropriate focus.
The student response instead appears to answer a question about the reason for this person?s action, such
as ?Why did he wander through the dark outskirts??.
Q: ?Who wandered through the dark outskirts??
TA: ?The child?s father wandered through the dark outskirts.?
SA: ?He searched for wood.?
Figure 2: Example with a who-question and a different QUD for the student answer
3 Annotation Scheme
In this section, we introduce the annotation scheme we developed. An important characteristic of our
annotation scheme is that it is applied incrementally: annotators first look at the surface question form, then
determine the set of alternatives (Krifka, 2007, sec. 3), and finally they mark instances of the alternative
set in answers. The rich task context of reading comprehension data with its explicit questions allows
us to circumvent the problem of guessing an implicit QUD, except in the cases where students answer a
different question (which we account for separately, see below). In the following, we present the three
types of categories our scheme is built on.
Question Form is used to mark the surface form of a question, where we distinguish wh-questions,
polarity questions, alternative questions, imperatives and noun phrase questions. In themselves, question
forms do not encode any semantics, but merely act as an explicit marker of the surface question form.
Table 1 gives an overview and examples of this dimension.
Focus is used to mark the focused words or phrases in an answer. We do not distinguish between
contrastive and new information focus, as this is not relevant for assessing an answer. Multiple foci can be
encoded and in fact do occur in the data.
1
http://brat.nlplab.org
161
Category Example Translation
WhPhrase ?Warum hatte Schorlemmer zu Beginn Angst?? ?Why was Schorlemmer afraid in the beginning??
YesNo ?Muss man deutscher Staatsb?urger sein?? ?Does one have to be a German citizen??
Alternative ?Ist er f?ur oder gegen das EU-Gesetz?? ?Is he for or against the EU law??
Imperative ?Begr?unden Sie diesen anderen Spitznamen.? ?Give reasons for this other nickname.?
NounPhrase ?Wohnort?? ?Place of residence??
Table 1: Question Forms in the annotation scheme
The starting point of our focus annotation is Krifka (2007)?s understanding of focus as the part of an
utterance that indicates the presence of alternatives relevant to the interpretation. We operationalize this
by testing whether a given part of the utterance is needed to distinguish between alternatives in the QUD.
Concretely, we train annotators to perform substitution tests in which they compare two potential extents
of the focus to identify whether the difference in the extent of the focus also selects a different valid
alternative in the sense of discriminating between alternatives in the QUD. For instance, consider the
example in (1), where the focus is made explicit by the square brackets.
(1) Where does Heike live?
She lives [[in Berlin.]]
F
Here ?in? needs to be part of the focus because exchanging it for another word with the same POS
changes the meaning of the phrase in a way picking another alternative, as in ?She lives near Berlin?.
Consider the same answer to a slightly different question in (2). Here the set of alternatives is more
constrained and hence ?in? is not focused.
(2) In what city does Heike live?
She lives in [[Berlin]]
F
.
Other criteria we defined to guide focus annotation include the following:
? Coordination: If several foci are coordinated, each should be marked separately.
? Givenness: Avoid marking given material except where needed to distinguish between alternatives.
? Each sentence is assumed to include at least one focus. If it does not answer the explicit question, it
must be annotated with a different QUD (discussed below).
? Focus never crosses sentence boundaries.
? Focus does not apply to sub-lexical units, such as syllables.
? Punctuation at focus boundaries is to be excluded.
In addition to marking focus, we annotate the relation between the explicitly given question and the
Question Under Discussion actually answered by a given response. In the most straightforward case, the
QUD is identical to the explicit question given, which in the annotation scheme is encoded as question
answered. In cases where the QUD differs from the explicitly given question, we distinguish three cases:
In the cases related to the implicit moves discussed in B?uring (2003, p. 525) exemplified by (3), the QUD
answered can be a subquestion of the explicit question, which we encode as question narrowed down.
When it addresses a more general QUD, as in (4), the response is annotated as question generalized.
(3) What did the pop stars wear?
The female pop stars wore caftans.
(4) Would you like a Coke or a Sprite?
I?d like a beer.
Finally, we also mark complete failures of question answer congruence with question ignored. In all
cases where the QUD being answered differs from the question explicitly given, the annotator is required
to specify the QUD apparently being answered.
162
Answer Type expresses the semantic category of the focus in relation to the question form. It further
describes the nature of the question-answer congruence by specifying the semantic class of the set of
alternatives. The answer types discussed in the computational linguistic literature generally are specific to
particular content domains, so that we developed our own taxonomy. Examples include Time/Date,
Location, Entity, and Reason. In addition to semantically restricting the focus to a specific type,
answer types can also provide syntactic cues restricting focus marking. For example, an Entity will
typically be encoded as a nominal expression. For annotation, the advantage of answer types is that they
force annotators to make an explicit commitment to the semantic nature of the focus they are annotating,
leading to potentially higher consistency and reliability of annotation. On the conceptual side, the semantic
restriction encoded in the answer type bears an interesting resemblance to what in a Structured Meaning
approach to focus (Krifka, 1992) is referred to as restriction of the question (Krifka, 2001, p. 3).
Category Description Example (translated)
Time Date time/date expression, usually incl. preposition The movie starts at 5:50
Living Being individual, animal or plant The father of the child padded through the dark
outskirts.
Thing concrete object which is not alive For the Spaniards toilet and stove are more
important than the internet.
Abstract Entity entity that is not concrete The applicant needs a completed vocational
training as a cook.
Report reported incident or statement The speaker says ?We ask all youths to have
their passports ready.?
Reason reason or cause for a statement The maintenance of a raised garden bed is
easier because one does not need to stoop.
Location place or relative location She is from Berlin.
Action activity or happening. In the vegetable garden one needs to hoe and
water.
Property attribute of something Reputation and money are important for Til.
Yes No polar answer, including whole statement
if not elliptic
The mermaid does not marry the prince.
Manner way in which something is done The word is used ironically in this story.
Quantity/Duration countable amount of something The company seeks 75 employees.
State state something is in, or result of some action If he works hard now, he won?t have to work
in the future.
Table 2: Answer Types with examples
4 Evaluation
The approach is evaluated in two ways. First, the consistency with which the focus annotation scheme
was applied is evaluated in section 4.1 by calculating inter-annotator agreement. In section 4.2 we then
explore the effect of focus annotation on Short Answer Assessment. For both evaluations, we provide a
qualitative discussion of characteristic examples.
4.1 Intrinsic Evaluation
4.1.1 Quantitative Results
Having carried out the manual annotation experiment, the question arises how to compare and calculate
agreement of spans of tokens in focus annotation. While comparing individual spans and calculating some
kind of overlap measure is certainly possible, it is hard to interpret the meaning of such numbers. We
therefore decided to make as few assumptions as possible and treat each token as a markable for which
the annotator needs to make a decision. On that basis, we then follow standard evaluation procedures in
calculating percentage agreement and Cohen?s Kappa (Artstein and Poesio, 2009).
Table 3 summarizes the agreement results. For both student and target answers, we report the granularity
of the distinction being made (focus/background vs. all answer types), the number of tokens the distinction
applies to, and finally percentage and Kappa agreement.
163
Type of distinction Type of answers # tokens % ?
Binary Student 6329 82.8 .65
(focus/background) Target 6983 84.9 .69
Detailed Student 5198 72.6 .61
(13 Answer Types + background) Target 6839 76.5 .67
Table 3: Inter-annotator agreement on student and target answers
The results show that all numbers are in the area of substantial agreement (? > .6). This is a noticeably
improvement over the results obtained by Ritz et al. (2008), who report ? = .51 on tokens in questionnaire
data, and it is on a par with the results reported by Calhoun et al. (2010). Annotation was easier on the
more well-formed target answers than on the often ungrammatical student answers. Moving from the
binary focus/background distinction to the one involving all Answer Types, we still obtain relatively good
agreement. This indicates that the semantic characterization of foci via Answer Types works quite well,
with the gap between student and target answers being even more apparent here.
In order to assess the effect of answer length, we also computed macro-average versions of percentage
agreement and ? for the binary focus distinction, following Ott et al. (2012, p. 55) but averaging over
answers. We obtained 84.0% and ? = .67 for student answers, and 87.4% and ? = .74 for target answers.
A few longer answers which are harder to annotate thus noticeably affected the agreement results of
Table 3 negatively.
4.1.2 Examples
To explore the nature of the disagreements, we showcase two characteristic issues here based on examples
from the corpus. Consider the following case where the annotators disagreed on the annotation of a
student answer:
Q: Warum nennt der Autor Hamburg das ?Tor zur Welt der Wissenschaft??
?Why does the author call Hamburg the ?gate to the world of science???
SA: [[Hamburg hat viel renommierte Universit
?
aten]]
F
(annotator 1)
Hamburg hat [[viel renommierte Universit
?
aten]]
F
(annotator 2)
?Hamburg has many renowned universities?
Figure 3: Disagreement involving given material
Whereas annotator 1 marks the whole answer on the grounds that the focus is of Answer Type Reason
and needs to include the whole proposition, annotator 2 excludes material given in the question. Both can
in theory be justified, but annotator 1 is closer to our guidelines here, taking into account that ?Hamburg?
indeed discriminates between alternatives (one could give reasons that do not include ?Hamburg?) and
thus needs to be part of the focus.
The second example illustrates the issue of deciding where the boundary of a focus is:
Q: Wof?ur ist der Aufsichtsrat verantwortlich?
?What is the supervisory board responsible for??
SA: Der Aufsichtsrat ist f ?ur [[die Bestellung]]
F
verantwortlich. (annotator 1)
Der Aufsichtsrat ist [[f ?ur die Bestellung]]
F
verantwortlich. (annotator 2)
?The supervisory board is responsible for the appointment.?
Figure 4: Disagreement on a preposition
Annotator 1 correctly excluded ?f?ur? (?for?) from the focus, only marking ?die Bestellung? (?the
appointment?) given that ?f?ur? is only needed for reasons of well-formedness. Annotator 2 apparently
thought that ?f?ur? makes a semantic difference here, but it is hard to construct a grammatical example
with a different preposition that changes the meaning of the focused expression.
164
4.2 Extrinsic Evaluation
It has been pointed out that evaluating an expert annotation of a theoretical linguistic notion only
intrinsically is problematic because there is no non-theoretical grounding involved (Riezler, 2014).
Therefore, besides calculating agreement measures, we also evaluated the resulting annotation in a larger
computational task, the automatic meaning assessment of answers to reading comprehension questions.
We used the CoMiC system (Comparing Meaning in Context, Meurers et al., 2011) as a testbed for our
experiment. CoMiC is an alignment-based system operating in three stages:
1. Annotating linguistic units (words, chunks and dependencies) in student and target answer on various
levels of abstraction
2. Finding alignments of linguistic units between student and target answer based on annotation
3. Classifying the student answer based on number and type of alignments, using a supervised machine
learning setup with 13 features in total
In stage 2, CoMiC integrates a simplistic approach to givenness, excluding all words from alignment
that are mentioned in the question. We transferred the underlying method to the notion of focus and
implemented a component that excludes all non-focused words from alignment, resulting in alignments
between focused parts of answers only. We only used the foci where students did not ignore the question
according to the annotators.
For the present evaluation, we experimented with three different settings involving the basic givenness
filter and our focus annotations: i) using the givenness filter by itself as a baseline, ii) aligning only
focused tokens as described above and iii) combining both by producing a givenness and a focus version
of each classification feature. All three settings were tried out for annotator 1 and 2.
4.2.1 Quantitative Results
Table 4 summarizes the quantitative results. It shows that focus beats the basic givenness baseline of
84.6% on its own, pushing the classification accuracy to 86.7% for annotator 1 and 87.2% for annotator 2.
Annotator 1 Annotator 2
Basic givenness only 84.6
Focus only 86.7 87.2
Focus + givenness 90.3 89.3
Table 4: Answer classification accuracy with the CoMiC system
While this is an encouraging result already, the combination of basic givenness and focus performs
substantially better, reaching 90.3% accuracy for annotator 1 and 89.3% for annotator 2.
In terms of the conceptual notions of formal pragmatics, this is an interesting result. While the notion
of givenness implemented here is surface-based and mechanistic and thus could be improved, the results
support the idea that both of the commonly discussed dimensions, focus/background and new/given, are
useful and informative information-structural dimensions that complement each other in assessing the
meaning of answers.
Interestingly, the focus annotation of annotator 2 on its own performed better than that of annotator 1,
but worse when combined with basic givenness. We suspect that annotator 2?s understanding of focus
relied more on the concept of givenness than annotator 1?s, causing the combination of the two to be less
informative than for annotator 1.
4.2.2 Alignment Example
The possible benefits of using focus to constrain alignment can take different forms: focus can lead us to
exclude extra, irrelevant material, but it can also uncover the fact that the relevant piece of information has
in fact not been included, as in the following corpus example:
165
Q: Was machen sie, um die Brunnen im Winter zu sch?utzen?
?What do they do to protect the wells in winter??
TA: Zw
?
olf der 47 Brunnen werden im Winter aus Schutz vor dem Frost und
Witterungssch
?
aden [[eingehaust]]
F
?Twelve of the 47 wells are encased in winter for protection from freezing and damage from
weather conditions?
SA: im Winter gibt es Frost und Witterungssch
?
aden
?in winter there is freezing and damage from weather conditions?
Figure 5: No alignments because the student answer ignores the question
The question asks what is being done to protect the wells in winter, for which the text states that twelve
of wells are encased for protection (technically, this is an answer to a sub-question since nothing is asserted
about the other wells). Additional new information such as ?vor dem Frost und Witterungssch?aden? does
not distinguish between alternatives to the question ?Was machen sie. . . ??, which clearly asks for an
Action. The target and student answer have high token overlap due to the presence of such extra
information, but only the target answer contains the relevant focus ?eingehaust?. Without the focus filter,
CoMiC wrongly classifies this answer as correct, but with the added focus information, it has the means
to judge this answer adequately.
5 Conclusion and Outlook
We presented a focus annotation study based on reading comprehension data, which we view as a
contribution to the general goal of analyzing and annotating focus. Motivated by the limited success of
approaches trying to tackle focus annotation from a general conceptual level, we aim to proceed from
the concrete task to the more general setting. This allows us to separate a) identifying the QUD and b)
determining the location and extent of the focus in the language material, where a) is informed and greatly
simplified by the explicit question.
Using this approach in combination with semantically motivated annotation guidelines, we showed that
focus annotation can be carried out systematically with Kappa values in the range of .61 to .69, depending
on the well-formedness of the language and the number of classes distinguished.
With respect to the practical goal of improving automatic assessment of short student answers, we
showed that information structural distinctions are relevant and able to quantitatively improve the results,
as demonstrated by an increase from 84.6% to 90.3% accuracy in a binary classification task on a balanced
data set.
While the manual annotation showcases the relevance and impact of focus annotation, we see the
design of an automatic focus/background classification system on the basis of our annotated data as the
logical next step. As such a system cannot perform the kind of introspective language analysis our human
annotators employed, we will have to approximate focus through surface criteria such as word order,
syntactic categories and focus sensitive particles. It remains to be seen how much of the potential benefit
of focus annotation can be reached by automatic focus annotation using machine learning.
Finally, in order to obtain more human-annotated data, we are planning to turn focus annotation of
answers to questions into a feasible crowd-sourcing task.
Acknowledgements
We are grateful to Heike Cardoso and Stefanie Wolf for carrying out the manual annotation and providing
valuable feedback. We also would like to thank Kordula De Kuthy, Verena Henrich, Niels Ott and the
three anonymous reviewers for their helpful comments.
166
References
Ron Artstein and Massimo Poesio. 2009. Survey article: Inter-coder agreement for computational linguistics.
Computational Linguistics, 34(4):1?42.
Stacey Bailey and Detmar Meurers. 2008. Diagnosing meaning errors in short answers to reading comprehension
questions. In Joel Tetreault, Jill Burstein, and Rachele De Felice, editors, Proceedings of the 3rd Workshop on
Innovative Use of NLP for Building Educational Applications (BEA-3) at ACL?08, pages 107?115, Columbus,
Ohio.
Eva Bur?a?nov?a, Eva Haji?cov?a, and Petr Sgall. 2000. Tagging of very large corpora: topic-focus articulation. In
Proceedings of the 18th conference on Computational linguistics - Volume 1, COLING ?00, pages 139?144,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Daniel B?uring. 2003. On d-trees, beans, and b-accents. Linguistics and Philosophy, 26(5):511?545.
Daniel B?uring. 2007. Intonation, semantics and information structure. In Gillian Ramchand and Charles Reiss,
editors, The Oxford Handbook of Linguistic Interfaces. Oxford University Press.
Sasha Calhoun, Jean Carletta, Jason Brenier, Neil Mayo, Dan Jurafsky, Mark Steedman, and David Beaver. 2010.
The NXT-format switchboard corpus: A rich resource for investigating the syntax, semantics, pragmatics and
prosody of dialogue. Language Resources and Evaluation, 44:387?419.
Kordula De Kuthy and Detmar Meurers. 2012. Focus projection between theory and evidence. In Sam Featherston
and Britta Stolterfoth, editors, Empirical Approaches to Linguistic Theory ? Studies in Meaning and Structure,
volume 111 of Studies in Generative Grammar, pages 207?240. De Gruyter.
Stefanie Dipper, Michael G?otze, and Stavros Skopeteas, editors. 2007. Information Structure in Cross-Linguistic
Corpora: Annotation Guidelines for Phonology, Morphology, Syntax, Semantics and Information Structure,
volume 7 of Interdisciplinary Studies on Information Structure. Universit?atsverlag Potsdam, Potsdam, Germany.
Myroslava Dzikovska, Rodney Nielsen, Chris Brew, Claudia Leacock, Danilo Giampiccolo, Luisa Bentivogli,
Peter Clark, Ido Dagan, and Hoa Trang Dang. 2013. Semeval-2013 task 7: The joint student response analysis
and 8th recognizing textual entailment challenge. In Second Joint Conference on Lexical and Computational
Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation
(SemEval 2013), pages 263?274, Atlanta, Georgia, USA, June. Association for Computational Linguistics.
Michael Hahn and Detmar Meurers. 2012. Evaluating the meaning of answers to reading comprehension ques-
tions: A semantics-based approach. In Proceedings of the 7th Workshop on Innovative Use of NLP for Building
Educational Applications (BEA-7) at NAACL-HLT 2012, pages 94?103, Montreal.
Andrea Horbach, Alexis Palmer, and Manfred Pinkal. 2013. Using the text to evaluate short answers for reading
comprehension exercises. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Vol-
ume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, pages 286?295,
Atlanta, Georgia, USA, June. Association for Computational Linguistics.
Ray Jackendoff. 1972. Semantic Interpretation in Generative Grammar. MIT Press, Cambridge, MA.
Manfred Krifka. 1992. A compositional semantics for multiple focus constructions. In Joachim Jacobs, editor,
Informationsstruktur und Grammatik, pages 17?54. Westdeutscher Verlag, Opladen.
Manfred Krifka. 2001. For a structured meaning account of questions and answers. In C. Fery and W. Sternefeld,
editors, Audiatur Vox Sapientia. A Festschrift for Arnim von Stechow, volume 52 of studia grammatica, pages
287?319. Akademie Verlag, Berlin.
Manfred Krifka. 2007. Basic notions of information structure. In Caroline Fery, Gisbert Fanselow, and Manfred
Krifka, editors, The notions of information structure, volume 6 of Interdisciplinary Studies on Information
Structure (ISIS), pages 13?55. Universit?atsverlag Potsdam, Potsdam.
Detmar Meurers, Ramon Ziai, Niels Ott, and Janina Kopp. 2011. Evaluating answers to reading comprehension
questions in context: Results for german and the role of information structure. In Proceedings of the TextInfer
2011 Workshop on Textual Entailment, pages 1?9, Edinburgh, Scotland, UK, July. Association for Computa-
tional Linguistics.
Michael Mohler, Razvan Bunescu, and Rada Mihalcea. 2011. Learning to grade short answer questions using
semantic similarity measures and dependency graph alignments. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies, pages 752?762, Portland,
Oregon, USA, June. Association for Computational Linguistics.
167
Niels Ott, Ramon Ziai, and Detmar Meurers. 2012. Creation and analysis of a reading comprehension exercise
corpus: Towards evaluating meaning in context. In Thomas Schmidt and Kai W?orner, editors, Multilingual Cor-
pora and Multilingual Corpus Analysis, Hamburg Studies in Multilingualism (HSM), pages 47?69. Benjamins,
Amsterdam.
Arndt Riester and Stefan Baumann. 2013. Focus triggers and focus types from a corpus perspective. Dialogue &
Discourse, 4(2):215?248.
Stefan Riezler. 2014. On the problem of theoretical terms in empirical computational linguistics. Computational
Linguistics, 40(1):235?245.
Julia Ritz, Stefanie Dipper, and Michael G?otze. 2008. Annotation of information structure: An evaluation across
different types of texts. In Proceedings of the 6th International Conference on Language Resources and Evalu-
ation, pages 2137?2142, Marrakech, Morocco.
Craige Roberts. 1996. Information structure in discourse: Towards an integrated formal theory of pragmatics. In
Jae-Hak Yoon and Andreas Kathol, editors, OSU Working Papers in Linguistics No. 49: Papers in Semantics.
The Ohio State University.
Mats Rooth. 1992. A theory of focus interpretation. Natural Language Semantics, 1(1):75?116.
Roger Schwarzschild. 1999. GIVENness, AvoidF and other constraints on the placement of accent. Natural
Language Semantics, 7(2):141?177.
Arnim von Stechow. 1981. Topic, focus, and local relevance. In Wolfgang Klein and W. Levelt, editors, Crossing
the Boundaries in Linguistics, pages 95?130. Reidel, Dordrecht.
168
