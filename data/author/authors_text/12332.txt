Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 45?52,
Athens, Greece, 31 March, 2009. c?2009 Association for Computational Linguistics
Automatic Treebank-Based Acquisition of Arabic LFG Dependency
Structures
Lamia Tounsi Mohammed Attia
NCLT, School of Computing, Dublin City University, Ireland
{lamia.tounsi, mattia, josef}@computing.dcu.ie
Josef van Genabith
Abstract
A number of papers have reported on meth-
ods for the automatic acquisition of large-scale,
probabilistic LFG-based grammatical resources
from treebanks for English (Cahill and al., 2002),
(Cahill and al., 2004), German (Cahill and al.,
2003), Chinese (Burke, 2004), (Guo and al.,
2007), Spanish (O?Donovan, 2004), (Chrupala
and van Genabith, 2006) and French (Schluter
and van Genabith, 2008). Here, we extend the
LFG grammar acquisition approach to Arabic and
the Penn Arabic Treebank (ATB) (Maamouri and
Bies, 2004), adapting and extending the methodol-
ogy of (Cahill and al., 2004) originally developed
for English. Arabic is challenging because of its
morphological richness and syntactic complexity.
Currently 98% of ATB trees (without FRAG and
X) produce a covering and connected f-structure.
We conduct a qualitative evaluation of our annota-
tion against a gold standard and achieve an f-score
of 95%.
1 Introduction
Treebank-based statistical parsers tend to achieve
greater coverage and robustness compared to ap-
proaches using handcrafted grammars. However,
they are criticised for being too shallow to mark
important syntactic and semantic dependencies
needed for meaning-sensitive applications (Ka-
plan, 2004). To treat this deficiency, a number
of researchers have concentrated on enriching
shallow parsers with deep dependency informa-
tion. (Cahill and al., 2002), (Cahill and al., 2004)
outlined an approach which exploits information
encoded in the Penn-II Treebank (PTB) trees to
automatically annotate each node in each tree
with LFG f-structure equations representing deep
predicate-argument structure relations. From this
LFG annotated treebank, large-scale unification
grammar resources were automatically extracted
and used in parsing (Cahill and al., 2008) and
generation (Cahill and van Genabith, 2006).
This approach was subsequently extended to
other languages including German (Cahill and
al., 2003), Chinese (Burke, 2004), (Guo and al.,
2007), Spanish (O?Donovan, 2004), (Chrupala
and van Genabith, 2006) and French (Schluter
and van Genabith, 2008).
Arabic is a semitic language and is well-known
for its morphological richness and syntactic
complexity. In this paper we describe the porting
of the LFG annotation methodology to Arabic in
order to induce LFG f-structures from the Penn
Arabic Treebank (ATB) (Bies, 2003), (Maamouri
and Bies, 2004). We evaluate both the coverage
and quality of the automatic f-structure annotation
of the ATB. Ultimately, our goal is to use the f-
structure annotated ATB to derive wide-coverage
resources for parsing and generating unrestricted
Arabic text. In this paper we concentrate on the
annotation algorithm.
The paper first provides a brief overview of
Lexical Functional Grammar, and the Penn
Arabic Treebank (ATB). The next section presents
the architecture of the f-structure annotation
algorithm for the acquisition of f-structures from
the Arabic treebank. The last section provides
an evaluation of the quality and coverage of the
annotation algorithm.
1.1 Lexical Functional Grammar
Lexical-Functional Grammar (LFG) (Kaplan and
Bresnan, 1982); (Bresnan, 2001), (Falk, 2001)
2001, (Sells, 1985) is a constraint-based theory
of grammar. LFG rejects concepts of configura-
tionality and movement familiar from generative
grammar, and provides a non-derivational alterna-
tive of parallel structures of which phrase structure
trees are only one component.
LFG involves two basic, parallel forms of
45
knowledge representation: c(onstituent)-structure,
which is represented by (f-structure annotated)
phrase structure trees; and f(unctional)-structure,
represented by a matrix of attribute-value pairs.
While c-structure accounts for language-specific
lexical idiosyncrasies, syntactic surface config-
urations and word order variations, f-structure
provides a more abstract level of representation
(grammatical functions/ labeled dependencies),
abstracting from some cross-linguistic syntactic
differences. Languages may differ typologically
as regards surface structural representations, but
may still encode similar syntactic functions (such
as, subject, object, adjunct, etc.). For a recent
overview on LFG-based analyses of Arabic see
(Attia, 2008) who presents a hand-crafted Arabic
LFG parser using the XLE (Xerox Linguistics En-
vironment).
1.2 The Penn Arabic Treebank (ATB)
The Penn Arabic Treebank project started in
2001 with the aim of describing written Modern
Standard Arabic newswire. The Treebank consists
of 23611 sentences (Bies, 2003), (Maamouri and
Bies, 2004) .
Arabic is a subject pro-drop language: a null
category (pro) is allowed in the subject position
of a finite clause if the agreement features on
the verb are rich enough to enable content to be
recovered (Baptista, 1995), (Chomsky, 1981).
This is represented in the ATB annotation by an
empty node after the verb marked with a -SBJ
functional tag. The ATB annotation, following
the Penn-II Treebank, utilises the concept of
empty nodes and traces to mark long distance
dependencies, as in relative clauses and questions.
The default word order in Arabic is VSO. When
the subject precedes the verb (SVO), the con-
struction is considered as topicalized. Modern
Standard Arabic also allows VOS word order
under certain conditions, e.g. when the object is
a pronoun. The ATB annotation scheme involves
24 basic POS-tags (497 different tags with mor-
phological information ), 22 phrasal tags, and 20
individual functional tags (52 different combined
tags).
The relatively free word order of Arabic means
that phrase structural position is not an indicator
of grammatical function, a feature of English
which was heavily exploited in the automatic LFG
annotation of the Penn-II Treebank (Cahill and
al., 2002). Instead, in the ATB functional tags are
used to mark the subject as well as the object.
The syntactic annotation style of the ATB follows,
as much as possible, the methodologies and
bracketing guidelines already used for the English
Penn-II Treebank. For example, in the Penn
English Treebank (PTB) (Marcus, 1994), small
clauses are considered sentences composed of
a subject and a predicate, without traces for an
omitted verb or any sort of control relationship, as
in example (1) for the sentence ?I consider Kris a
fool?.
(1) (S (NP-SBJ I)
(VP consider
(S (NP-SBJ Kris)
(NP-PRD a fool))))
The team working on the ATB found this
approach very convenient for copula construc-
tions in Arabic, which are mainly verbless
(Maamouri and Bies, 2004). Therefore they used
a similar analysis without assuming a deleted
copula verb or control relationship, as in (2).
(2) (S (NP-SBJ Al-mas>alatu

??

A??
?
@)
(ADJ-PRD basiyTatuN

??J


?fl
.
))

??J


?fl
.

??

A??
?
@
Al-mas>alatu basiyTatuN
the-question simple
?The question is simple.?
2 Architecture of the Arabic Automatic
Annotation Algorithm
The annotation algorithm for Arabic is based on
and substantially revises the methodology used for
English.
For English, f-structure annotation is very much
driven by configurational information: e.g. the
leftmost NP sister of a VP is likely to be a direct
object and hence annotated ? OBJ =?. This infor-
mation is captured in the format of left-right anno-
tation matrices, which specify annotations for left
or right sisters relative to a local head.
By contrast, Arabic is a lot less configurational and
has much richer morphology. In addition, com-
pared to the Penn-II treebank, the ATB features a
larger functional tag set. This is reflected in the de-
sign of the Arabic f-structure annotation algorithm
46
(Figure 1), where left-right annotation matrices
play a much smaller role than for English. The
annotation algorithm recursively traverses trees in
the ATB. It exploits ATB morpho-syntactic fea-
tures, ATB functional tags, and (some) configura-
tional information in the local subtrees.
We first mask (conflate) some of the complex
morphological information available in the pre-
terminal nodes to be able to state generalisations
for some of the annotation components. We then
head-lexicalise ATB trees identifying local heads.
Lexical macros exploit the full morphological an-
notations available in the ATB and map them to
corresponding f-structure equations. We then ex-
ploit ATB functional tags mapping them to SUBJ,
OBJ, OBL, OBJ2, TOPIC and ADJUNCT etc.
grammatical functions. The remaining functions
(COMP, XCOMP, SPEC etc.) as well as some
cases of SUBJ, OBJ, OBL, OBJ2, TOPIC and AD-
JUNCT, which could not be identified by ATB
tags, are treated in terms of left-right context anno-
tation matrices. Coordination is treated in a sepa-
rate component to keep the other components sim-
ple. Catch-all & Clean-Up corrects overgenerali-
sations in the previous modules and uses defaults
for remaining unannotated nodes. Finally, non-
local dependencies are handled by a Traces com-
ponent.
The next sub-sections describe the main modules
of the annotation algorithm.
2.1 Conflation
ATB preterminals are very fine-grained, encod-
ing extensive morpho-syntactic details in addi-
tion to POS information. For example, the word
	
?

?
	
J? sanaqifu ?[we will] stand? is tagged as
(FUT+IV1P+IV+IVSUFF MOOD:I) denoting an
imperfective (I) verb (V) in the future tense (FUT),
and is first person (1) plural (P) with indicative
mood (IVSUFF MOOD:I). In total there are over
460 preterminal types in the treebank. This level
of fine-grainedness is an important issue for the
annotation as we cannot state grammatical func-
tion (dependency) generalizations about heads and
left and right contexts for such a large tag set. To
deal with this problem, for some of the annotation
algorithm components we masked the morpho-
syntactic details in preterminals, thereby conflat-
ing them into more generic POS tags. For exam-
ple, the above-mentioned tag will be conflated as
VERB.
Figure 1: Architecture of the Arabic annotation al-
gorithm
2.2 Lexical Macros
Lexical macros, by contrast, utilise the de-
tailed morpho-syntactic information encoded in
the preterminal nodes of the Penn Arabic Tree-
bank trees and provide the required functional an-
notations accordingly. These tags usually include
information related to person, number, gender,
definiteness, case, tense, aspect, mood, etc.
Table 1 lists common tags for nouns and verbs and
shows the LFG functional annotation assigned to
each tag.
2.3 Functional Tags
In addition to monadic POS categories, the ATB
treebank contains a set of labels (called functional
tags or functional labels) associated with func-
tional information, such as -SBJ for ?subject? and
-OBJ for ?object?. The functional tags module
translates these functional labels into LFG func-
tional equations, e.g. -OBJ is assigned the anno-
tation ?OBJ=?. An f-structure equation look-up
table assigns default f-structure equations to each
functional label in the ATB (Table 2).
A particular treatment is applied for the tag -PRD
(predicate). This functional tag is used with cop-
ula complements, as in (3) and the correspond-
ing c-structure in Figure 2. Copula complements
47
Tag Annotation
Nouns
MASC ? GEND = masc (masculine)
FEM ? GEND = fem (feminine)
SG ? NUM = sg (singular)
DU ? NUM = dual
PL ? NUM = pl (plural)
ACC ? CASE = acc (accusative)
NOM ? CASE = nom (nominative)
GEN ? CASE = gen (genitive)
Verbs
1 ? PERS = 1
2 ? PERS = 2
3 ? PERS = 3
S ? NUM = sg
D ? NUM = dual
P ? NUM = pl
F ? GEND = masc
M ? GEND = fem
Table 1: Morpho-syntactic tags and their functional anno-
tations
Functional Label Annotation
-SBJ (subject) ? SUBJ = ?
-OBJ (object) ? OBJ = ?
-DTV (dative), ? OBJ2 =?
-BNF (Benefactive)
-TPC (topicalized) ? TOPIC=?
-CLR (clearly related) ? OBL =?
-LOC (locative),
-MNR (manner),
-DIR (direction), ??? ADJUNCT
-TMP (temporal),
-ADV (adverbial)
-PRP (purpose),
Table 2: Functional tags used in the ATP Treebank and their
default annotations
correspond to the open complement grammatical
function XCOMP in LFG and the ATB tag -PRD
is associated with the annotation in (4) in order to
produce the f-structure in Figure 3. The resulting
analysis includes a main predicator ?null be? and
specifies the control relationship through a func-
tional equation stating that the main subject is co-
indexed with the subject of the XCOMP.
(3)

?

K


P?Q?
	
?

?
	
KY?? @
Al-hudonapu Daruwriy?apN
the-truce necessary
?The truce is necessary.?
(4) ? PRED = ?null be?
? XCOMP = ?
? SUBJ= ? SUBJ
S
NP-SBJ
N
Alhudonapu
NP-PRD
N
DaruwriyapN
Figure 2: C-structure for example (3)
2
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
PRED ?null be
D
SUBJ , XCOMP
E
?
SUBJ
2
6
6
6
6
6
4
PRED ?Al-hudonapu?
NUM sg
GEND fem
DEF +
CASE nom
3
7
7
7
7
7
5
1
XCOMP
2
6
6
6
6
6
6
6
6
4
PRED ?Daruwriy?apN?
NUM sg
GEND fem
DEF -
CASE nom
SUBJ
h i
1
3
7
7
7
7
7
7
7
7
5
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
Figure 3: F-structure for example (3)
2.4 Left-Right Context Rules
The left-right context annotation module is based
on a tripartite division of local subtrees into a left-
hand-side context (LHS) followed by a head (H)
followed by a right-hand-side context (RHS). We
developed our own head finding, or head lexical-
ization, rules based on a variety of heuristics and
manual inspection of the PS rules.
Initially, we extracted 45785 Phrase Structure (PS)
rules from the treebank. The reason for the rela-
tively large number of PS rules is the fine-grained
nature of the tags encoding morphological infor-
mation for pre-terminal nodes. When we conflate
pre-terminals containing morphological informa-
tion to basic POS tags, the set of PS rules is re-
duced to 9731.
Treebanks grammars follow the Zipfian law: for
each category, there is a small number of highly
frequent rules expanding that category, followed
by a large number of rules with a very low fre-
quency. Therefore, for each LHS category we se-
lect the most frequent rules which together give
85% coverage. This results is a reduced set of 339
(most frequent) PS rules. These rules are manu-
ally examined and used to construct left-right LFG
f-structure annotation matrices for the treebank.
The annotation matrices encode information about
48
the left and right context of a rule?s head and state
generalisations about the functional annotation of
constituents to the left and right of the local head.
Consider sentence (5), where an NP is expanded
as NP NP ADJP. The first NP is considered the
head and is given the annotation ?=?. The second
NP and the ADJP are located to the left (Arabic
reading) of the head (LHS). The left-right context
matrix for NP constituents analyses these phrases
as adjuncts and assigns them the annotation ? ? ?
ADJUNCT.
(5)

?

J


??m
.
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 223?229,
Dublin, Ireland, August 23-24, 2014.
DCU: Aspect-based Polarity Classification for SemEval Task 4
Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab Barman
Dasha Bogdanova, Jennifer Foster and Lamia Tounsi
CNGL Centre for Global Intelligent Content
National Centre for Language Technology
School of Computing
Dublin City University
Dublin, Ireland
{jwagner,parora,scortes,ubarman}@computing.dcu.ie
{dbogdanova,jfoster,ltounsi}@computing.dcu.ie
Abstract
We describe the work carried out by DCU
on the Aspect Based Sentiment Analysis
task at SemEval 2014. Our team submit-
ted one constrained run for the restaurant
domain and one for the laptop domain for
sub-task B (aspect term polarity predic-
tion), ranking highest out of 36 systems on
the restaurant test set and joint highest out
of 32 systems on the laptop test set.
1 Introduction
This paper describes DCU?s participation in the
Aspect Term Polarity sub-task of the Aspect Based
Sentiment Analysis task at SemEval 2014, which
focuses on predicting the sentiment polarity of as-
pect terms for a restaurant and a laptop dataset.
Given, for example, the sentence I have had so
many problems with the computer and the aspect
term the computer, the task is to predict whether
the sentiment expressed towards the aspect term is
positive, negative, neutral or conflict.
Our polarity classification system uses super-
vised machine learning with support vector ma-
chines (SVM) (Boser et al., 1992) to classify an
aspect term into one of the four classes. The fea-
tures we employ are word n-grams (with n rang-
ing from 1 to 5) in a window around the aspect
term, as well as features derived from scores as-
signed by a sentiment lexicon. Furthermore, to
reduce data sparsity, we experiment with replacing
sentiment-bearing words in our n-gram feature set
with their polarity scores according to the lexicon
and/or their part-of-speech tag.
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
The paper is organised as follows: in Section 2,
we describe the sentiment lexicons used in this
work and detail the process by which they are
combined, filtered and extended; in Section 3, we
describe our baseline method, a heuristic approach
which makes use of the sentiment lexicon, fol-
lowed by our machine learning method which in-
corporates the rule-based method as features in ad-
dition to word n-gram features; in Section 4, we
present the results of both methods on the training
and test data, and perform an error analysis on the
test set; in Section 5, we compare our approach to
previous research in sentiment classification; Sec-
tion 6 discusses efficiency of our system and on-
going work to improve its speed; finally, in Sec-
tion 7, we conclude and provide suggestions as to
how this research could be fruitfully extended.
2 Sentiment Lexicons
The following four lexicons are employed:
1. MPQA
1
(Wilson et al., 2005) classifies a
word or a stem and its part of speech tag
into positive, negative, both or neutral with
a strong or weak subjectivity.
2. SentiWordNet
2
(Baccianella et al., 2010)
specifies the positive, negative and objective
scores of a synset and its part of speech tag.
3. General Inquirer
3
indicates whether a word
expresses positive or negative sentiment.
4. Bing Liu?s Opinion Lexicon
4
(Hu and Liu,
1
http://mpqa.cs.pitt.edu/lexicons/
subj_lexicon/
2
http://sentiwordnet.isti.cnr.it/
3
http://www.wjh.harvard.edu/
?
inquirer/
inqtabs.txt
4
http://www.cs.uic.edu/
?
liub/FBS/
sentiment-analysis.html#lexicon
223
2004) indicates whether a word expresses
positive or negative sentiment.
2.1 Lexicon Combination
Since the four lexicons differ in their level of detail
and in how they present information, it is neces-
sary, when combining them, to consolidate the in-
formation and present it in a uniform manner. Our
combination strategy assigns a sentiment score to
a word as follows:
? MPQA: 1 for strong positive subjectivity, -1
for strong negative subjectivity, 0.5 for weak
positive subjectivity, -0.5 for weak negative
subjectivity, and 0 otherwise
? SentiWordNet: The positive score if the pos-
itive score is greater than the negative and ob-
jective scores, the negative score if the nega-
tive score is greater than the positive and the
objective scores, and 0 otherwise
? General Inquirer and Bing Liu?s Opinion
Lexicon: 1 for positive and -1 for negative
The above four scores are summed to arrive at a
final score between -4 and 4 for a word.
5
2.2 Lexicon Filtering
Initial experiments with our sentiment lexicon and
the training data led us to believe that there were
many irrelevant entries that, although capable of
conveying sentiment in some other context, were
not contributing to the sentiment of aspect terms
in the two domains of the task. Therefore, these
words are manually filtered from the lexicon. Ex-
amples of deleted words are just, clearly, indi-
rectly, really and back.
2.3 Adding Domain-Specific Words
A manual inspection of the training data revealed
words missing from the merged sentiment lexicon
but which do express sentiment in these domains.
Examples are mouthwatering, watery and better-
configured. We add these to the lexicon with a
score of either 1 or -1 (depending on their polarity
in the training data). We also add words (e.g. zesty,
acrid) from an online list of culinary terms.
6
5
We also tried to vote over the four lexicon scores but this
did not improve over summing.
6
http://world-food-and-wine.com/
describing-food
2.4 Handling Variation
In order to ensure that all inflected forms of a
word are covered, we lemmatise the words in the
training data using the IMS TreeTagger (Schmid,
1994) and we construct new possibilities using a
suffix list. To correct misspelled words, we con-
sider the corrected form of a misspelled word to be
the form with the highest frequency in a reference
corpus
7
among all the forms within an edit dis-
tance of 1 and 2 from the misspelled word (Norvig,
2012). Multi-word expressions of the form x-y
are added with the polarity of xy or x, as in laid-
back/laidback and well-shaped/well. Expressions
x y, are added with the polarity of x-y, as in so
so/so-so.
3 Methodology
We first build a rule-based system which classi-
fies the polarity of an aspect term based solely on
the scores assigned by the sentiment lexicon. We
then explore different ways of converting the rule-
based system into features which can then be com-
bined with bag-of-n-gram features in a supervised
machine learning set-up.
3.1 Rule-Based Approach
In order to predict the polarity of an aspect term,
we sum the polarity scores of all the words in the
surrounding sentence according to our sentiment
lexicon. Since not all the sentiment words occur-
ring in a sentence influence the polarity of the as-
pect term to the same extent, it is important to
weight the score of each sentiment word by its dis-
tance to the aspect term. Therefore, for each word
in the sentence which is found in our lexicon we
take the score from the lexicon and divide it by its
distance to the aspect term. The distance is calcu-
lated using the sum of the following three distance
functions:
? Token Distance: This function calculates the
difference in the position of the sentiment
word and the aspect term by counting the to-
kens between them.
7
The reference corpus consists of about a million
words retrieved from several public domain books from
Project Gutenberg (http://www.gutenberg.org/),
lists of most frequent words from Wiktionary (http:
//en.wiktionary.org/wiki/Wiktionary:
Frequency_lists) and the British National Corpus
(http://www.kilgarriff.co.uk/bnc-readme.
html) and two thousand laptop reviews crawled from CNET
(http://www.cnet.com/).
224
? Discourse Chunk Distance: This function
counts the discourse chunks that must be
crossed in order to get from the sentiment
word to the aspect term. If the sentiment
word and the aspect term are in the same
discourse chunk, then the distance is zero.
We use the discourse segmenter described in
(Tofiloski et al., 2009).
? Dependency Path Distance: This function
calculates the shortest path between the sen-
timent word and the aspect term in a syntac-
tic dependency graph for the sentence, pro-
duced by parsing the sentence with a PCFG-
LA parser (Attia et al., 2010) trained on con-
sumer review data (Le Roux et al., 2012)
8
,
and converting the resulting phrase-structure
tree into a dependency graph using the Stan-
ford converter (de Marneffe and Manning,
2008) (version 3.3.1).
Since our lexicon also contains multi-word ex-
pressions such as finger licking, we also look up
bigrams and trigrams from the input sentence in
our lexicon. Negation is handled by reversing the
polarity of sentiment words that appear within a
window of three words of the following negators:
not, n?t, no and never.
For each aspect term, we use the distance-
weighted sum of the polarity scores to predict one
of the three classes positive, negative and neutral.
9
After experimenting with various thresholds we
settled on the following simple strategy: if the po-
larity score for an aspect term is greater than zero
then it is classified as positive, if the score is less
than zero, then it is classified as negative, other-
wise it is classified as neutral.
3.2 Machine Learning Approach
We train a four-way SVM classifier for each do-
main (laptop and restaurant), using Weka?s SMO
implementation (Platt, 1998; Hall et al., 2009).
10
8
To facilitate parsing, the data was normalised using the
process described in (Le Roux et al., 2012) with minor mod-
ifications, e. g. treatment of non-breakable space characters,
abbreviations and emoticons. The normalised version of the
data was used for all experiments.
9
We also experimented with classifying aspect terms as
conflict when the individual scores for positive and negative
sentiment were both relatively high. However, this proved
unsuccessful.
10
We also experimented with logistic regression, random
forests, k-nearest neighbour, naive Bayes and multi-layer per-
ceptron in Weka, but did not match performance of an SVM
trained with default parameters.
Transf. n c n-gram Freq.
-L? 2 2 cord with 1
AL? 2 2 <aspect> with 56
ALS? 1 4 <negu080> 595
ALSR- 1 4 <negu080> 502
AL? 2 4 and skip 1
ALSR- 2 4 and <negu080> 25
ALSRP 1 4 <negu080>/vb 308
Table 1: 7 of the 2,640 bag-of-n-gram features
extracted for the aspect term cord from the lap-
top training sentence I charge it at night and skip
taking the cord with me because of the good bat-
tery life. The last column shows the frequency of
the feature in the training data. Transformations:
A=aspect, L=lowercase, S=score, R=restricted to
certain POS, P=POS annotation
Our system submission uses bag-of-n-gram fea-
tures and features derived from the rule-based ap-
proach. Decisions about parameters are made in 5-
fold cross-validation on the training data provided
for the task.
3.2.1 Bag-of-N-gram Features
We extract features encoding the presence of spe-
cific lower-cased n-grams (L) (n = 1, ..., 5) in
the context of the aspect term to be classified (c
words to the left and c words to the right with
c = 1, ..., 5, inf) for 10 combinations of trans-
formations: replacement of the aspect term with
<ASPECT> (A), replacement of sentiment words
with a discretised score (S), restriction (R) of the
sentiment word replacement to certain parts-of-
speech, and annotation of the discretised score
with the POS (P) of the sentiment word. An ex-
ample is shown in Table 1.
3.2.2 Adding Rule-Based Score Features
We explore two approaches for incorporating in-
formation from the rule-based approach (Sec-
tion 3.1) into our SVM classifier. The first ap-
proach is to encode polarity scores directly as the
following four features:
1. distance-weighted sum of scores of positive
words in the sentence
2. distance-weighted sum of scores of negative
words in the sentence
3. number of positive words in the sentence
225
4. number of negative words in the sentence
The second approach is less direct: for each do-
main, we train J48 decision trees with minimum
leaf size 60 using the four rule-based features de-
scribed above. We then use the decision rules
and the conjunctions leading from the root node
to each leaf node to binarise the above four basic
score features, producing 122 features. Further-
more, we add normalised absolute values, rank of
values and interval indicators, producing 48 fea-
tures.
3.2.3 Submitted Runs
We eliminate features that have redundant value
columns for the training data, and we apply fre-
quency thresholds (13, 18, 25 and 35) to further
reduce the number of features. We perform a grid-
search to optimise the parameters C and ? of the
SVM RBF kernel. We choose the system to sub-
mit based on average cross-validation accuracy.
We experiment with combinations of the three fea-
ture sets described above. We choose the bina-
rised features over the raw rule-based scores be-
cause cross-validation results are inferior for the
rule-based scores in initial experiments with fea-
ture frequency threshold 35: 70.26 vs. 71.36 for
laptop and 72.06 vs. 72.15 for restaurant. There-
fore, we decide to focus on systems with binarised
score features for lower feature frequency thresh-
olds, which are more CPU-intensive to train. For
both domains, the system we end up submitting
is a combination of the n-gram features and the
binarised features with parameters C = 3.981,
? = 0.003311 for the laptop data, C = 1.445,
? = 0.003311 for the restaurant data, and a fre-
quency threshold of 13.
4 Results and Analysis
Table 2 shows the training and test accuracy of
the task baseline system (Pontiki et al., 2014), a
majority baseline classifying everything as posi-
tive, our rule-based system and our submitted sys-
tem. The restaurant domain has a higher accuracy
than the laptop domain for all systems, the SVM
system outperforms the rule-based system on both
domains, and the test accuracy is higher than the
training accuracy for all systems in the restaurant
domain.
We observe that the majority of our systems? er-
rors fall into the following categories:
Dataset System Training Test
Laptop Baseline ? 51.1%
Laptop All positive 41.9% 52.1%
Laptop Rule-based 65.4% 67.7%
Laptop SVM 72.3% 70.5%
Restaurant Baseline ? 64.3%
Restaurant All positive 58.6% 64.2%
Restaurant Rule-based 69.5% 77.8%
Restaurant SVM 72.7% 81.0%
Table 2: Accuracy of the task baseline system, a
system classifying everything as positive, our rule-
based system and our submitted SVM-based sys-
tem on train (5-fold cross-validation) and test sets
? Sentiment not expressed explicitly: The
sentiment cannot be inferred from local lexi-
cal and syntactic information, e. g. The sushi
is cut in blocks bigger than my cell phone.
? Non-obvious expression of negation: For
example, The Management was less than ac-
comodating [sic]. The rule-based approach
does not capture such cases and there are
not enough similar training examples for the
SVM to learn to correctly classify them.
? Conflict cases: The training data contains
too few examples of conflict sentences for the
system to learn to detect them.
11
For the restaurant domain, there are more than
fifty cases where the rule-based approach fails to
detect sentiment, but the machine learning ap-
proach classifies it correctly. Most of these cases
contain no sentiment lexicon words, thus the rule-
based system marks them as being neutral. How-
ever, the machine learning system was able to fig-
ure out the correct polarity. Examples of such
cases include Try the rose roll (not on menu) and
The gnocchi literally melts in your mouth!. Fur-
thermore, in the laptop domain, a number of the
errors made by the rule-based system arise from
the ambiguous nature of some lexicon words. For
example, the sentence Only 2 usb ports ... seems
kind of ... limited is misclassified because the
word kind is considered to be positive.
There are a few cases where the rule-based sys-
tem outperforms the machine learning one. It hap-
pens when a sentence contains a rare word with
strong polarity, e. g. the word heavenly in The
11
We only classify one test instance as conflict.
226
chocolate raspberry cake is heavenly - not too
sweet, but full of flavor.
5 Related Work
The use of supervised machine learning with bag-
of-word or bag-of-n-gram feature sets has been
a standard approach to the problem of sentiment
polarity classification since the seminal work by
Pang et al. (2002) on movie review polarity pre-
diction. Heuristic methods which rely on a lexi-
con of sentiment words have also been widespread
and much of the research in this area has been
devoted to the unsupervised induction of good
quality sentiment indicators (see, for example,
Hatzivassiloglou and McKeown (1997) and Tur-
ney (2002), and Liu (2010) for an overview). The
integration of sentiment lexicon scores as fea-
tures in supervised machine learning to supple-
ment standard bag-of-n-gram features has also
been employed before (see, for example, Bak-
liwal et al. (2013)). The replacement of train-
ing/test words with scores/labels from sentiment
lexicons has also been used by Baccianella et
al. (2009), who supplement n-grams such as hor-
rible location with generalised expressions such
as NEGATIVE location. Linguistic features which
capture generalisations at the level of syntax (Mat-
sumoto et al., 2005), semantics (Johansson and
Moschitti, 2010) and discourse (Lazaridou et al.,
2013) have also been widely applied. In using bi-
narised features derived from the nodes of a deci-
sion tree, we are following our recent work which
uses the same technique in a different task: quality
estimation for machine translation (Rubino et al.,
2012; Rubino et al., 2013).
The main novelty in our system lies not in the
individual techniques but rather in they way they
are combined and integrated. For example, our
combination of token/chunk/dependency path dis-
tance used to weight the relationship between a
sentiment word and the aspect term has ? to the
best of our knowledge ? not been applied before.
6 Efficiency
Building a system for a shared task, we focus
solely on the accuracy of the system in all our deci-
sions. For example, we parse all training and test
data multiple times using different grammars to
increase sentence coverage from 99.87% to 100%.
To offer a more practical system, we work on
implementing a simplified, fully automated sys-
tem that is more efficient. So far, we replaced
time-consuming parsing with POS tagging. The
system accepts as input and generates as output
valid SemEval ABSA XML documents.
12
After
extracting the text and the aspect terms from the
input, the text is normalised using the process de-
scribed in Footnote 8. The feature extraction is
performed as described in Section 3 with the fol-
lowing modifications:
? The POS information used by the n-gram
feature extractor is obtained using the IMS
TreeTagger (Schmid, 1994) instead of using
the PCFG-LA parser (Attia et al., 2010).
? The distance used by the rule-based approach
is the token distance only, instead of a com-
bination of three distance functions.
The sentiment lexicon and the classification mod-
els used are described in Sections 2 and 3 respec-
tively.
The test sets containing 800 sentences are POS
tagged in less than half a second each. Surpris-
ingly, accuracy of aspect term polarity prediction
increases to 71.4% (from 70.5% for the submitted
system) on the laptop test set, using the same SVM
parameters as for the submitted system. However,
we see a degradation to 78.8% (from 81.0% for the
submitted system) for the restaurant test set. This
is an encouraging result as the SVM parameters
are not yet fully optimised for the slightly different
information and as the remaining modifications to
be implemented should not change accuracy any
further.
The next bottleneck that needs to be addressed
before the system can be used in applications re-
quiring quick responses is the current implementa-
tion of the n-gram feature extractor: It enumerates
all n-grams (for all context window sizes and n-
gram transformations) only to then intersect these
features with the list of selected features. For the
shared task, this made sense as we initially need
all features to make our selection of features, and
as we only need to run the feature extractor a few
times. For a practical system that has to process
new test sets frequently, however, it will be more
efficient to check for each selected feature whether
the respective event occurs in the input.
12
We validate documents using the XML schema defini-
tion provided on the shared task website.
227
7 Conclusion
We have described our aspect term polarity predic-
tion system, which employs supervised machine
learning using a combination of n-grams and sen-
timent lexicon features. Although our submitted
system performs very well, it is interesting to note
that our rule-based system is not that far behind.
This suggests that a state-of-the-art system can be
build without machine learning and that careful
design of the other system components is impor-
tant. However, the very good performance of our
machine-learning-based system also suggests that
word n-gram features do provide useful informa-
tion that is missed by a sentiment lexicon alone,
and that it is always worthwhile to perform careful
parameter tuning to eke out as much as possible
from such an approach.
Future work should investigate how much each
system component contributes to the overall per-
formance, e. g. lexicon combination, lemmatisa-
tion, spelling correction, other normalisations,
negation handling, distance function and n-gram
feature transformations. There is also room for
improvements in most of these components, e. g.
our handling of complex negations. Detection of
conflicts also needs more attention. Features in-
dicating the presence of trigger words for negation
and conflicts that are currently used only internally
in the rule-based component could be added to the
SVM feature set. It would also be interesting to
see how the compositional approach described by
Socher et al. (2013) handles these difficult cases.
The score features could be easily augmented by
breaking down scores by the four employed lexi-
cons. This way, the SVM can choose to combine
the information from these scores differently than
just summing them, allowing it to learn more com-
plex relations. Lexicon filtering and addition of
domain-specific entries could be automated to re-
duce the time needed to adjust to a new domain.
Finally, machine learning methods that can effi-
ciently handle large feature sets such as logistic
regression should be tried with the full feature set
(not applying frequency thresholds).
Acknowledgements
This research is supported by the Science Foun-
dation Ireland (Grant 12/CE/I2267) as part of
CNGL (www.cngl.ie) at Dublin City University.
The authors wish to acknowledge the DJEI/DES/
SFI/HEA Irish Centre for High-End Computing
(ICHEC) for the provision of computational facil-
ities and support. We are grateful to Qun Liu and
Josef van Genabith for their helpful comments.
References
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statis-
tical latent-variable parsing models for arabic, en-
glish and french. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 67?75.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2009. Multi-facet rating of product reviews.
In Proceedings of ECIR, pages 461?472.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of the Seventh Conference
on International Language Resources and Evalua-
tion (LREC?10).
Akshat Bakliwal, Jennifer Foster, Jennifer van der Puil,
Ron O?Brien, Lamia Tounsi, and Mark Hughes.
2013. Sentiment analysis of political tweets: To-
wards an accurate classifier. In Proceedings of the
NAACL Workshop on Language Analysis in Social
Media, pages 49?58.
Bernhard E. Boser, Isabelle M. Guyon, and
Vladimir N. Vapnik. 1992. A training algo-
rithm for optimal margin classifiers. In Proceedings
of the Fifth Annual Workshop on Computational
Learning Theory, pages 144?152.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies rep-
resentation. In COLING 2008 Workshop on Cross-
framework and Cross-domain Parser Evaluation.,
pages 1?8.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
ACM SIGKDD Explorations Newsletter, 11(1):10?
18.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 35th Annual Meeting
of the ACL and the 8th Conference of the European
Chapter of the ACL, pages 174?181.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177.
228
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion ex-
pression detection. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 67?76.
Angeliki Lazaridou, Ivan Titov, and Caroline
Sporleder. 2013. A bayesian model for joint
unsupervised induction of sentiment, aspect and
discourse representations. In Proceedings of
the 51th Annual Meeting of the Association for
Computational Linguistics, pages 1630?1639.
Joseph Le Roux, Jennifer Foster, Joachim Wagner, Ra-
sul Samad Zadeh Kaljahi, and Anton Bryl. 2012.
DCU-Paris13 systems for the SANCL 2012 shared
task. Notes of the First Workshop on Syntactic
Analysis of Non-Canonical Language (SANCL).
Bing Liu. 2010. Sentiment analysis and subjectivity.
In Handbook of Natural Language Processing.
Shotaro Matsumoto, Hiroya Takamura, and Manubu
Okumura, 2005. Advances in Knowledge Discovery
and Data Mining, volume 3518 of Lecture Notes in
Computer Science, chapter Sentiment Classification
Using Word Sub-sequences and Dependency Sub-
trees, pages 301?311.
Peter Norvig. 2012. How to write a spelling corrector.
http://norvig.com/spell-correct.
html. [Online; accessed 2014-03-19].
Po Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79?86.
John C. Platt. 1998. Fast training of support vec-
tor machines using sequential minimal optimization.
In B. Schoelkopf, C. Burges, and A. Smola, edi-
tors, Advances in Kernel Methods - Support Vector
Learning, pages 185?208.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval).
Raphael Rubino, Jennifer Foster, Joachim Wagner, Jo-
hann Roturier, Rasul Samad Zadeh Kaljahi, and Fred
Hollowood. 2012. Dcu-symantec submission for
the wmt 2012 quality estimation task. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, pages 138?144.
Raphael Rubino, Joachim Wagner, Jennifer Foster, Jo-
hann Roturier, Rasoul Samad Zadeh Kaljahi, and
Fred Hollowood. 2013. DCU-Symantec at the
WMT 2013 quality estimation shared task. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 392?397.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49.
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP, pages 1631?
1642.
Milan Tofiloski, Julian Brooke, and Maite Taboada.
2009. A syntactic and lexical-based discourse seg-
menter. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, ACLShort ?09, pages 77?
80.
Peter Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classi-
cation of reviews. In Proceedings of the ACL, pages
417?424.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ?05, pages 347?354.
229
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 1?12,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Statistical Parsing of Morphologically Rich Languages (SPMRL)
What, How and Whither
Reut Tsarfaty
Uppsala Universitet
Djame? Seddah
Alpage (Inria/Univ. Paris-Sorbonne)
Yoav Goldberg
Ben Gurion University
Sandra Ku?bler
Indiana University
Marie Candito
Alpage (Inria/Univ. Paris 7)
Jennifer Foster
NCLT, Dublin City University
Yannick Versley
Universita?t Tu?bingen
Ines Rehbein
Universita?t Saarbru?cken
Lamia Tounsi
NCLT, Dublin City University
Abstract
The term Morphologically Rich Languages
(MRLs) refers to languages in which signif-
icant information concerning syntactic units
and relations is expressed at word-level. There
is ample evidence that the application of read-
ily available statistical parsing models to such
languages is susceptible to serious perfor-
mance degradation. The first workshop on sta-
tistical parsing of MRLs hosts a variety of con-
tributions which show that despite language-
specific idiosyncrasies, the problems associ-
ated with parsing MRLs cut across languages
and parsing frameworks. In this paper we re-
view the current state-of-affairs with respect
to parsing MRLs and point out central chal-
lenges. We synthesize the contributions of re-
searchers working on parsing Arabic, Basque,
French, German, Hebrew, Hindi and Korean
to point out shared solutions across languages.
The overarching analysis suggests itself as a
source of directions for future investigations.
1 Introduction
The availability of large syntactically annotated cor-
pora led to an explosion of interest in automati-
cally inducing models for syntactic analysis and dis-
ambiguation called statistical parsers. The devel-
opment of successful statistical parsing models for
English focused on the Wall Street Journal Penn
Treebank (PTB, (Marcus et al, 1993)) as the pri-
mary, and sometimes only, resource. Since the ini-
tial release of the Penn Treebank (PTB Marcus et
al. (1993)), many different constituent-based parsing
models have been developed in the context of pars-
ing English (e.g. (Magerman, 1995; Collins, 1997;
Charniak, 2000; Chiang, 2000; Bod, 2003; Char-
niak and Johnson, 2005; Petrov et al, 2006; Huang,
2008; Finkel et al, 2008; Carreras et al, 2008)).
At their time, each of these models improved the
state-of-the-art, bringing parsing performance on the
standard test set of the Wall-Street-Journal to a per-
formance ceiling of 92% F1-score using the PARS-
EVAL evaluation metrics (Black et al, 1991). Some
of these parsers have been adapted to other lan-
guage/treebank pairs, but many of these adaptations
have been shown to be considerably less successful.
Among the arguments that have been proposed
to explain this performance gap are the impact of
small data sets, differences in treebanks? annotation
schemes, and inadequacy of the widely used PARS-
EVAL evaluation metrics. None of these aspects in
isolation can account for the systematic performance
deterioration, but observed from a wider, cross-
linguistic perspective, a picture begins to emerge ?
that the morphologically rich nature of some of the
languages makes them inherently more susceptible
to such performance degradation. Linguistic factors
associated with MRLs, such as a large inventory of
word-forms, higher degrees of word order freedom,
and the use of morphological information in indi-
cating syntactic relations, makes them substantially
harder to parse with models and techniques that have
been developed with English data in mind.
1
In addition to these technical and linguistic fac-
tors, the prominence of English parsing in the litera-
ture reduces the visibility of research aiming to solve
problems particular to MRLs. The lack of stream-
lined communication among researchers working
on different MRLs often leads to a reinventing the
wheel syndrome. To circumvent this, the first work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010) offers a platform for
this growing community to share their views of the
different problems and oftentimes similar solutions.
We identify three main types of challenges, each
of which raises many questions. Many of the ques-
tions are yet to be conclusively answered. The first
type of challenges has to do with the architectural
setup of parsing MRLs: What is the nature of the in-
put? Can words be represented abstractly to reflect
shared morphological aspects? How can we cope
with morphological segmentation errors propagated
through the pipeline? The second type concerns the
representation of morphological information inside
the articulated syntactic model: Should morpholog-
ical information be encoded at the level of PoS tags?
On dependency relations? On top of non-terminals
symbols? How should the integrated representations
be learned and used? A final genuine challenge
has to do with sound estimation for lexical probabil-
ities: Given the finite, and often rather small, set of
data, and the large number of morphological analy-
ses licensed by rich inflectional systems, how can we
analyze words unseen in the training data?
Many of the challenges reported here are mostly
irrelevant when parsing Section 23 of the PTB but
they are of primordial importance in other tasks, in-
cluding out-of-domain parsing, statistical machine
translation, and parsing resource-poor languages.
By synthesizing the contributions to the workshop
and bringing it to the forefront, we hope to advance
the state of the art of statistical parsing in general.
In this paper we therefore take the opportunity
to analyze the knowledge that has been acquired in
the different investigations for the purpose of iden-
tifying main bottlenecks and pointing out promising
research directions. In section 2, we define MRLs
and identify syntactic characteristics associated with
them. We then discuss work on parsing MRLs in
both the dependency-based and constituency-based
setup. In section 3, we review the types of chal-
lenges associated with parsing MRLs across frame-
works. In section 4, we focus on the contributions to
the SPMRL workshop and identify recurring trends
in the empirical results and conceptual solutions. In
section 5, we analyze the emerging picture from a
bird?s eye view, and conclude that many challenges
could be more faithfully addressed in the context of
parsing morphologically ambiguous input.
2 Background
2.1 What are MRLs?
The term Morphologically Rich Languages (MRLs)
is used in the CL/NLP literature to refer to languages
in which substantial grammatical information, i.e.,
information concerning the arrangement of words
into syntactic units or cues to syntactic relations, is
expressed at word level.
The common linguistic and typological wisdom is
that ?morphology competes with syntax? (Bresnan,
2001). In effect, this means that rich morphology
goes hand in hand with a host of nonconfigurational
syntactic phenomena of the kind discussed by Hale
(1983). Because information about the relations be-
tween syntactic elements is indicated in the form of
words, these words can freely change their positions
in the sentence. This is referred to as free word or-
der (Mithun, 1992). Information about the group-
ing of elements together can further be expressed by
reference to their morphological form. Such logical
groupings of disparate elements are often called dis-
continuous constituents. In dependency structures,
such discontinuities impose nonprojectivity. Finally,
rich morphological information is found in abun-
dance in conjunction with so-called pro-drop or zero
anaphora. In such cases, rich morphological infor-
mation in the head (or co-head) of the clause of-
ten makes it possible to omit an overt subject which
would be semantically impoverished.
English, the most heavily studied language within
the CL/NLP community, is not an MRL. Even
though a handful of syntactic features (such as per-
son and number) are reflected in the form of words,
morphological information is often secondary to
other syntactic factors, such as the position of words
and their arrangement into phrases. German, an
Indo-European language closely related to English,
already exhibits some of the properties that make
2
parsing MRLs problematic. The Semitic languages
Arabic and Hebrew show an even more extreme case
in terms of the richness of their morphological forms
and the flexibility in their syntactic ordering.
2.2 Parsing MRLs
Pushing the envelope of constituency parsing:
The Head-Driven models of the type proposed
by Collins (1997) have been ported to parsing
many MRLs, often via the implementation of Bikel
(2002). For Czech, the adaptation by Collins et al
(1999) culminated in an 80 F1-score.
German has become almost an archetype of the
problems caused by MRLs; even though German
has a moderately rich morphology and a moder-
ately free word order, parsing results are far from
those for English (see (Ku?bler, 2008) and references
therein). Dubey (2005) showed that, for German
parsing, adding case and morphology information
together with smoothed markovization and an ade-
quate unknown-word model is more important than
lexicalization (Dubey and Keller, 2003).
For Modern Hebrew, Tsarfaty and Sima?an (2007)
show that a simple treebank PCFG augmented with
parent annotation and morphological information as
state-splits significantly outperforms Head-Driven
markovized models of the kind made popular by
Klein and Manning (2003). Results for parsing
Modern Standard Arabic using Bikel?s implemen-
tation on gold-standard tagging and segmentation
have not improved substantially since the initial re-
lease of the treebank (Maamouri et al, 2004; Kulick
et al, 2006; Maamouri et al, 2008).
For Italian, Corazza et al (2004) used the Stan-
ford parser and Bikel?s parser emulation of Collins?
model 2 (Collins, 1997) on the ISST treebank, and
obtained significantly lower results compared to En-
glish. It is notable that these models were ap-
plied without adding morphological signatures, us-
ing gold lemmas instead. Corazza et al (2004) fur-
ther tried different refinements including parent an-
notation and horizontal markovization, but none of
them obtained the desired improvement.
For French, Crabbe? and Candito (2008) and Sed-
dah et al (2010) show that, given a corpus compara-
ble in size and properties (i.e. the number of tokens
and grammar size), the performance level, both for
Charniak?s parser (Charniak, 2000) and the Berke-
ley parser (Petrov et al, 2006) was higher for pars-
ing the PTB than it was for French. The split-merge-
smooth implementation of (Petrov et al, 2006) con-
sistently outperform various lexicalized and unlexi-
calized models for French (Seddah et al, 2009) and
for many other languages (Petrov and Klein, 2007).
In this respect, (Petrov et al, 2006) is considered
MRL-friendly, due to its language agnostic design.
The rise of dependency parsing: It is commonly
assumed that dependency structures are better suited
for representing the syntactic structures of free word
order, morphologically rich, languages, because this
representation format does not rely crucially on the
position of words and the internal grouping of sur-
face chunks (Mel?c?uk, 1988). It is an entirely differ-
ent question, however, whether dependency parsers
are in fact better suited for parsing such languages.
The CoNLL shared tasks on multilingual depen-
dency parsing in 2006 and 2007 (Buchholz and
Marsi, 2006; Nivre et al, 2007a) demonstrated that
dependency parsing for MRLs is quite challenging.
While dependency parsers are adaptable to many
languages, as reflected in the multiplicity of the lan-
guages covered,1 the analysis by Nivre et al (2007b)
shows that the best result was obtained for English,
followed by Catalan, and that the most difficult lan-
guages to parse were Arabic, Basque, and Greek.
Nivre et al (2007a) drew a somewhat typological
conclusion, that languages with rich morphology
and free word order are the hardest to parse. This
was shown to be the case for both MaltParser (Nivre
et al, 2007c) and MST (McDonald et al, 2005), two
of the best performing parsers on the whole.
Annotation and evaluation matter: An emerg-
ing question is therefore whether models that have
been so successful in parsing English are necessar-
ily appropriate for parsing MRLs ? but associated
with this question are important questions concern-
ing the annotation scheme of the related treebanks.
Obviously, when annotating structures for languages
with characteristics different than English one has to
face different annotation decisions, and it comes as
no surprise that the annotated structures for MRLs
often differ from those employed in the PTB.
1The shared tasks involved 18 languages, including many
MRLs such as Arabic, Basque, Czech, Hungarian, and Turkish.
3
For Spanish and French, it was shown by Cowan
and Collins (2005) and in (Arun and Keller, 2005;
Schluter and van Genabith, 2007), that restructuring
the treebanks? native annotation scheme to match
the PTB annotation style led to a significant gain in
parsing performance of Head-Driven models of the
kind proposed in (Collins, 1997). For German, a
language with four different treebanks and two sub-
stantially different annotation schemes, it has been
shown that a PCFG parser is sensitive to the kind of
representation employed in the treebank.
Dubey and Keller (2003), for example, showed
that a simple PCFG parser outperformed an emula-
tion of Collins? model 1 on NEGRA. They showed
that using sister-head dependencies instead of head-
head dependencies improved parsing performance,
and hypothesized that it is due to the flatness of
phrasal annotation. Ku?bler et al (2006) showed con-
siderably lower PARSEVAL scores on NEGRA (Skut
et al, 1998) relative to the more hierarchically struc-
tured Tu?Ba-D/Z (Hinrichs et al, 2005), again, hy-
pothesizing that this is due to annotation differences.
Related to such comparisons is the question of the
relevance of the PARSEVAL metrics for evaluating
parsing results across languages and treebanks. Re-
hbein and van Genabith (2007) showed that PARS-
EVAL measures are sensitive to annotation scheme
particularities (e.g. the internal node ratio). It was
further shown that different metrics (i.e. the Leaf-
ancestor path (Sampson and Babarczy, 2003) and
dependency based ones in (Lin, 1995)) can lead to
different performance ranking. This was confirmed
also for French by Seddah et al (2009).
The questions of how to annotate treebanks for
MRLs and how to evaluate the performance of the
different parsers on these different treebanks is cru-
cial. For the MRL parsing community to be able to
assess the difficulty of improving parsing results for
French, German, Arabic, Korean, Basque, Hindi or
Hebrew, we ought to first address fundamental ques-
tions including: Is the treebank sufficiently large
to allow for proper grammar induction? Does the
annotation scheme fit the language characteristics?
Does the use of PTB annotation variants for other
languages influence parsing results? Does the space-
delimited tokenization allow for phrase boundary
detection? Do the results for a specific approach
generalize to more than one language?
3 Primary Research Questions
It is firmly established in theoretical linguistics that
morphology and syntax closely interact through pat-
terns of case marking, agreement, clitics and various
types of compounds. Because of such close interac-
tions, we expect morphological cues to help parsing
performance. But in practice, when trying to incor-
porate morphological information into parsing mod-
els, three types of challenges present themselves:
Architecture and Setup: When attempting to
parse complex word-forms that encapsulate both
lexical and functional information, important archi-
tectural questions emerge, namely, what is the na-
ture of the input that is given to the parsing system?
Does the system attempt to parse sequences of words
or does it aim to assign structures to sequences of
morphological segments? If the former is the case,
how can we represent words abstractly so as to re-
flect shared morphological aspects between them?
If the latter is the case, how can we arrive at a good
enough morphological segmentation for the purpose
of statistical parsing, given raw input texts?
When working with morphologically rich lan-
guages such as Hebrew or Arabic, affixes may have
syntactically independent functions. Many parsing
models assume segmentation of the syntactically in-
dependent parts, such as prepositions or pronominal
clitics, prior to parsing. But morphological segmen-
tation requires disambiguation which is non-trivial,
due to case syncretism and high morphological am-
biguity exhibited by rich inflectional systems. The
question is then when should we disambiguate the
morphological analyses of input forms? Should we
do that prior to parsing or perhaps jointly with it?2
Representation and Modeling: Assuming that
the input to our system reflects morphological infor-
mation, one way or another, which types of morpho-
2Most studies on parsing MRLs nowadays assume the gold
standard segmentation and disambiguated morphological infor-
mation as input. This is the case, for instance, for the Arabic
parsing at CoNLL 2007 (Nivre et al, 2007a). This practice de-
ludes the community as to the validity of the parsing results
reported for MRLs in shared tasks. Goldberg et al (2009), for
instance, show a gap of up to 6pt F1-score between performance
on gold standard segmentation vs. raw text. One way to over-
come this is to devise joint morphological and syntactic disam-
biguation frameworks (cf. (Goldberg and Tsarfaty, 2008)).
4
logical information should we include in the parsing
model? Inflectional and/or derivational? Case infor-
mation and/or agreement features? How can valency
requirements reflected in derivational morphology
affect the overall syntactic structure? In tandem with
the decision concerning the morphological informa-
tion to include, we face genuine challenges concern-
ing how to represent such information in the syntac-
tic model, be it constituency-based or dependency-
based. Should we encode morphological informa-
tion at the level of PoS tags and/or on top of syn-
tactic elements? Should we decorate non-terminals
nodes and/or dependency arcs or both?
Incorporating morphology in the statistical model
is often even more challenging than the sum of
these bare decisions, because of the nonconfigu-
rational structures (free word order, discontinuous
constituents) for rich markings are crucial (Hale,
1983). The parsing models designed for English of-
ten focus on learning rigid word order, and they do
not take morphological information into account (cf.
developing parsers for German (Dubey and Keller,
2003; Ku?bler et al, 2006)). The more complex ques-
tion is therefore: what type of parsing model should
we use for parsing MRLs? shall we use a general
purpose implementation and attempt to amend it?
how? or perhaps we should devise a new model from
first principles, to address nonconfigurational phe-
nomena effectively? using what form of representa-
tion? is it possible to find a single model that can
effectively cope with different kinds of languages?
Estimation and Smoothing: Compared to En-
glish, MRLs tend to have a greater number of word
forms and higher out-of-vocabulary (OOV) rates,
due to the many feature combinations licensed by
the inflectional system. A typical problem associ-
ated with parsing MRLs is substantial lexical data
sparseness due to high morphological variation in
surface forms. The question is therefore, given our
finite, and often fairly small, annotated sets of data,
how can we guess the morphological analyses, in-
cluding the PoS tag assignment and various features,
of an OOV word? How can we learn the probabil-
ities of such assignments? In a more general setup,
this problem is akin to handling out-of-vocabulary
or rare words for robust statistical parsing, and tech-
niques for domain adaptation via lexicon enhance-
Constituency-Based Dependency-Based
Arabic (Attia et al, 2010) (Marton et al, 2010)?
Basque - (Bengoetxea and Gojenola, 2010)
English (Attia et al, 2010) -
French (Attia et al, 2010)
(Seddah et al, 2010)
(Candito and Seddah, 2010)? -
German (Maier, 2010) -
Hebrew (Tsarfaty and Sima?an, 2010) (Goldberg and Elhadad, 2010)?
Hindi - (Ambati et al, 2010a)?
(Ambati et al, 2010b)
Korean (Chung et al, 2010) -
Table 1: An overview of SPMRL contributions. (? report
results also for non-gold standard input)
ment (also explored for English and other morpho-
logically impoverished languages).
So, in fact, incorporating morphological informa-
tion inside the syntactic model for the purpose of
statistical parsing is anything but trivial. In the next
section we review the various approaches taken in
the individual contributions of the SPMRL work-
shop for addressing such challenges.
4 Parsing MRLs: Recurring Trends
The first workshop on parsing MRLs features 11
contributions for a variety of languages with a
range of different parsing frameworks. Table 1 lists
the individual contributions within a cross-language
cross-framework grid. In this section, we focus on
trends that occur among the different contributions.
This may be a biased view since some of the prob-
lems that exist for parsing MRLs may have not been
at all present, but it is a synopsis of where we stand
with respect to problems that are being addressed.
4.1 Architecture and Setup: Gold vs. Predicted
Morphological Information
While morphological information can be very infor-
mative for syntactic analysis, morphological anal-
ysis of surface forms is ambiguous in many ways.
In German, for instance, case syncretism (i.e. a sin-
gle surface form corresponding to different cases) is
pervasive, and in Hebrew and Arabic, the lack of vo-
calization patterns in written texts leads to multiple
morphological analyses for each space-delimited to-
ken. In real world situations, gold morphological in-
formation is not available prior to parsing. Can pars-
ing systems make effective use of morphology even
when gold morphological information is absent?
5
Several papers address this challenge by present-
ing results for both the gold and the automatically
predicted PoS and morphological information (Am-
bati et al, 2010a; Marton et al, 2010; Goldberg and
Elhadad, 2010; Seddah et al, 2010). Not very sur-
prisingly, all evaluated systems show a drop in pars-
ing accuracy in the non-gold settings.
An interesting trend is that in many cases, us-
ing noisy morphological information is worse than
not using any at all. For Arabic Dependency pars-
ing, using predicted CASE causes a substantial drop
in accuracy while it greatly improves performance
in the gold setting (Marton et al, 2010). For
Hindi Dependency Parsing, using chunk-internal
cues (i.e. marking non-recursive phrases) is benefi-
cial when gold chunk-boundaries are available, but
suboptimal when they are automatically predicted
(Ambati et al, 2010a). For Hebrew Dependency
Parsing with the MST parser, using gold morpholog-
ical features shows no benefit over not using them,
while using automatically predicted morphological
features causes a big drop in accuracy compared to
not using them (Goldberg and Elhadad, 2010). For
French Constituency Parsing, Seddah et al (2010)
and Candito and Seddah (2010) show that while
gold information for the part-of-speech and lemma
of each word form results in a significant improve-
ment, the gain is low when switching to predicted
information. Reassuringly, Ambati et al (2010a),
Marton et al (2010), and Goldberg and Elhadad
(2010) demonstrate that some morphological infor-
mation can indeed be beneficial for parsing even in
the automatic setting. Ensuring that this is indeed
so, appears to be in turn linked to the question of
how morphology is represented and incorporated in
the parsing model.
The same effect in a different guise appears in
the contribution of Chung et al (2010) concerning
parsing Korean. Chung et al (2010) show a sig-
nificant improvement in parsing accuracy when in-
cluding traces of null anaphors (a.k.a. pro-drop) in
the input to the parser. Just like overt morphology,
traces and null elements encapsulate functional in-
formation about relational entities in the sentence
(the subject, the object, etc.), and including them at
the input level provides helpful disambiguating cues
for the overall structure that represents such rela-
tions. However, assuming that such traces are given
prior to parsing is, for all practical purposes, infeasi-
ble. This leads to an interesting question: will iden-
tifying such functional elements (marked as traces,
overt morphology, etc) during parsing, while com-
plicating that task itself, be on the whole justified?
Closely linked to the inclusion of morphological
information in the input is the choice of PoS tag set
to use. The generally accepted view is that fine-
grained PoS tags are morphologically more informa-
tive but may be harder to statistically learn and parse
with, in particular in the non-gold scenario. Mar-
ton et al (2010) demonstrate that a fine-grained tag
set provides the best results for Arabic dependency
parsing when gold tags are known, while a much
smaller tag set is preferred in the automatic setting.
4.2 Representation and Modeling:
Incorporating Morphological Information
Many of the studies presented here explore the use
of feature representation of morphological informa-
tion for the purpose of syntactic parsing (Ambati et
al., 2010a; Ambati et al, 2010b; Bengoetxea and
Gojenola, 2010; Goldberg and Elhadad, 2010; Mar-
ton et al, 2010; Tsarfaty and Sima?an, 2010). Clear
trends among the contributions emerge concerning
the kind of morphological information that helps sta-
tistical parsing. Morphological CASE is shown to be
beneficial across the board. It is shown to help for
parsing Basque, Hebrew, Hindi and to some extent
Arabic.3 Morphological DEFINITENESS and STATE
are beneficial for Hebrew and Arabic when explic-
itly represented in the model. STATE, ASPECT and
MOOD are beneficial for Hindi, but only marginally
beneficial for Arabic. CASE and SUBORDINATION-
TYPE are the most beneficial features for Basque
transition-based dependency parsing.
A closer view into the results mentioned in the
previous paragraph suggests that, beyond the kind
of information that is being used, the way in which
morphological information is represented and used
by the model has substantial ramification as to
whether or not it leads to performance improve-
ments. The so-called ?agreement features? GEN-
DER, NUMBER, PERSON, provide for an interesting
case study in this respect. When included directly as
3For Arabic, CASE is useful when gold morphology infor-
mation is available, but substantially hurt results when it is not.
6
machine learning features, agreement features ben-
efit dependency parsing for Arabic (Marton et al,
2010), but not Hindi (dependency) (Ambati et al,
2010a; Ambati et al, 2010b) or Hebrew (Goldberg
and Elhadad, 2010). When represented as simple
splits of non-terminal symbols, agreement informa-
tion does not help constituency-based parsing per-
formance for Hebrew (Tsarfaty and Sima?an, 2010).
However, when agreement patterns are directly rep-
resented on dependency arcs, they contribute an im-
provement for Hebrew dependency parsing (Gold-
berg and Elhadad, 2010). When agreement is en-
coded at the realization level inside a Relational-
Realizational model (Tsarfaty and Sima?an, 2008),
agreement features improve the state-of-the-art for
Hebrew parsing (Tsarfaty and Sima?an, 2010).
One of the advantages of the latter study is that
morphological information which is expressed at the
level of words gets interpreted elsewhere, on func-
tional elements higher up the constituency tree. In
dependency parsing, similar cases may arise, that
is, morphological information might not be as use-
ful on the form on which it is expressed, but would
be more useful at a different position where it could
influence the correct attachment of the main verb
to other elements. Interesting patterns of that sort
occur in Basque, where the SUBORDINATIONTYPE
morpheme attaches to the auxiliary verb, though it
mainly influences attachments to the main verb.
Bengoetxea and Gojenola (2010) attempted two
different ways to address this, one using a trans-
formation segmenting the relevant morpheme and
attaching it to the main verb instead, and another
by propagating the morpheme along arcs, through
a ?stacking? process, to where it is relevant. Both
ways led to performance improvements. The idea of
a segmentation transformation imposes non-trivial
pre-processing, but it may be that automatically
learning the propagation of morphological features
is a promising direction for future investigation.
Another, albeit indirect, way to include morpho-
logical information in the parsing model is using
so-called latent information or some mechanism
of clustering. The general idea is the following:
when morphological information is added to stan-
dard terminal or non-terminal symbols, it imposes
restrictions on the distribution of these no-longer-
equivalent elements. Learning latent informa-
tion does not represent morphological information
directly, but presumably, the distributional restric-
tions can be automatically learned along with the
splits of labels symbols in models such as (Petrov
et al, 2006). For Korean (Chung et al, 2010),
latent information contributes significant improve-
ments. One can further do the opposite, namely,
merging terminals symbols for the purpose of ob-
taining an abstraction over morphological features.
When such clustering uses a morphological signa-
ture of some sort, it is shown to significantly im-
prove constituency-based parsing for French (Can-
dito and Seddah, 2010).
4.3 Representation and Modeling: Free Word
Order and Flexible Constituency Structure
Off-the-shelf parsing tools are found in abundance
for English. One problematic aspect of using them
to parse MRLs lies in the fact that these tools fo-
cus on the statistical modeling of configurational
information. These models often condition on the
position of words relative to one another (e.g. in
transition-based dependency parsing) or on the dis-
tance between words inside constituents (e.g. in
Head-Driven parsing). Many of the contributions to
the workshop show that working around existing im-
plementations may be insufficient, and we may have
to come up with more radical solutions.
Several studies present results that support the
conjecture that when free word-order is explicitly
taken into account, morphological information is
more likely to contribute to parsing accuracy. The
Relational-Realizational model used in (Tsarfaty
and Sima?an, 2010) allows for reordering of con-
stituents at a configuration layer, which is indepen-
dent of the realization patterns learned from the data
(vis-a`-vis case marking and agreement). The easy-
first algorithm of (Goldberg and Elhadad, 2010)
which allows for significant flexibility in the order of
attachment, allows the model to benefit from agree-
ment patterns over dependency arcs that are easier
to detect and attach first. The use of larger subtrees
in (Chung et al, 2010) for parsing Korean, within a
Bayesian framework, allows the model to learn dis-
tributions that take more elements into account, and
thus learn the different distributions associated with
morphologically marked elements in constituency
structures, to improve performance.
7
In addition to free word order, MRLs show higher
degree of freedom in extraposition. Both of these
phenomena can result in discontinuous structures.
In constituency-based treebanks, this is either an-
notated as additional information which has to be
recovered somehow (traces in the case of the PTB,
complex edge labels in the German Tu?Ba-D/Z), or
as discontinuous phrase structures, which cannot be
handled with current PCFG models. Maier (2010)
suggests the use of Linear Context-Free Rewriting
Systems (LCFRSs) in order to make discontinuous
structure transparent to the parsing process and yet
preserve familiar notions from constituency.
Dependency representation uses non-projective
dependencies to reflect discontinuities, which is
problematic to parse with models that assume pro-
jectivity. Different ways have been proposed to deal
with non-projectivity (Nivre and Nilsson, 2005; Mc-
Donald et al, 2005; McDonald and Pereira, 2006;
Nivre, 2009). Bengoetxea and Gojenola (2010)
discuss non-projective dependencies in Basque and
show that the pseudo-projective transformation of
(Nivre and Nilsson, 2005) improves accuracy for de-
pendency parsing of Basque. Moreover, they show
that in combination with other transformations, it
improves the utility of these other ones, too.
4.4 Estimation and Smoothing: Coping with
Lexical Sparsity
Morphological word form variation augments the
vocabulary size and thus worsens the problem of lex-
ical data sparseness. Words occurring with medium-
frequency receive less reliable estimates, and the
number of rare/unknown words is increased. One
way to cope with the one of both aspects of this
problem is through clustering, that is, providing an
abstract representation over word forms that reflects
their shared morphological and morphosyntactic as-
pects. This was done, for instance, in previous work
on parsing German. Versley and Rehbein (2009)
cluster words according to linear context features.
These clusters include valency information added to
verbs and morphological features such as case and
number added to pre-terminal nodes. The clusters
are then integrated as features in a discriminative
parsing model to cope with unknown words. Their
discriminative model thus obtains state-of-the-art re-
sults on parsing German.
Several contribution address similar challenges.
For constituency-based generative parsers, the sim-
ple technique of replacing word forms with more
abstract symbols is investigated by (Seddah et al,
2010; Candito and Seddah, 2010). For French, re-
placing each word form by its predicted part-of-
speech and lemma pair results in a slight perfor-
mance improvement (Seddah et al, 2010). When
words are clustered, even according to a very local
linear-context similarity measure, measured over a
large raw corpus, and when word clusters are used in
place of word forms, the gain in performance is even
higher (Candito and Seddah, 2010). In both cases,
the technique provides more reliable estimates for
in-vocabulary words, since a given lemma or cluster
appear more frequently. It also increases the known
vocabulary. For instance, if a plural form is un-
seen in the training set but the corresponding singu-
lar form is known, then in a setting of using lemmas
in terminal symbols, both forms are known.
For dependency parsing, Marton et al (2010) in-
vestigates the use of morphological features that in-
volve some semantic abstraction over Arabic forms.
The use of undiacritized lemmas is shown to im-
prove performance. Attia et al (2010) specifically
address the handling of unknown words in the latent-
variable parsing model. Here again, the technique
that is investigated is to project unknown words to
more general symbols using morphological clues. A
study on three languages, English, French and Ara-
bic, shows that this method helps in all cases, but
that the greatest improvement is obtained for Arabic,
which has the richest morphology among three.
5 Where we?re at
It is clear from the present overview that we are
yet to obtain a complete understanding concerning
which models effectively parse MRLs, how to an-
notate treebanks for MRLs and, importantly, how
to evaluate parsing performance across types of lan-
guages and treebanks. These foundational issues are
crucial for deriving more conclusive recommenda-
tions as to the kind of models and morphological
features that can lead to advancing the state-of-the-
art for parsing MRLs. One way to target such an
understanding would be to encourage the investiga-
tion of particular tasks, individually or in the context
8
of shared tasks, that are tailored to treat those prob-
lematic aspects of MRLs that we surveyed here.
So far, constituency-based parsers have been as-
sessed based on their performance on the PTB (and
to some extent, across German treebanks (Ku?bler,
2008)) whereas comparison across languages was
rendered opaque due to data set differences and
representation idiosyncrasies. It would be interest-
ing to investigate such a cross-linguistic compari-
son of parsers in the context of a shared task on
constituency-based statistical parsing, in additional
to dependency-based ones as reported in (Nivre et
al., 2007a). Standardizing data sets for a large
number of languages with different characteristics,
would require us, as a community, to aim for
constituency-representation guidelines that can rep-
resent the shared aspects of structures in different
languages, while at the same time allowing differ-
ences between them to be reflected in the model.
Furthermore, it would be a good idea to intro-
duce parsing tasks, for either constituent-based or
dependency-based setups, which consider raw text
as input, rather than morphologically segmented
and analyzed text. Addressing the parsing prob-
lem while facing the morphological disambiguation
challenge in its full-blown complexity would be il-
luminating and educating for at least two reasons:
firstly, it would give us a better idea of what is the
state-of-the-art for parsing MRLs in realistic scenar-
ios. Secondly, it might lead to profound insights
about the potentially successful ways to use mor-
phology inside a parser, which may differ from the
insights concerning the use of morphology in the
less realistic parsing scenarios, where gold morpho-
logical information is given.
Finally, to be able to perceive where we stand
with respect to parsing MRLs and how models fare
against one another across languages, it would be
crucial to arrive at evaluation metrics that capture
information that is shared among the different repre-
sentations, for instance, functional information con-
cerning predicate-argument relations. Using the dif-
ferent kinds of measures in the context of cross-
framework tasks will help us understand the util-
ity of the different evaluation metrics that have been
proposed and to arrive at a clearer picture of what it
is that we wish to compare, and how we can faith-
fully do so across models, languages and treebanks.
6 Conclusion
This paper presents the synthesis of 11 contributions
to the first workshop on statistical parsing for mor-
phologically rich languages. We have shown that
architectural, representational, and estimation issues
associated with parsing MRLs are found to be chal-
lenging across languages and parsing frameworks.
The use of morphological information in the non
gold-tagged input scenario is found to cause sub-
stantial differences in parsing performance, and in
the kind of morphological features that lead to per-
formance improvements.
Whether or not morphological features help pars-
ing also depends on the kind of model in which
they are embedded, and the different ways they are
treated within. Furthermore, sound statistical esti-
mation methods for morphologically rich, complex
lexica, turn out to be crucial for obtaining good pars-
ing accuracy when using general-purpose models
and algorithms. In the future we hope to gain better
understanding of the common pitfalls in, and novel
solutions for, parsing morphologically ambiguous
input, and to arrive at principled guidelines for se-
lecting the model and features to include when pars-
ing different kinds of languages. Such insights may
be gained, among other things, in the context of
more morphologically-aware shared parsing tasks.
Acknowledgements
The program committee would like to thank
NAACL for hosting the workshop and SIGPARSE
for their sponsorship. We further thank INRIA Al-
page team for their generous sponsorship. We are
finally grateful to our reviewers and authors for their
dedicated work and individual contributions.
References
Bharat Ram Ambati, Samar Husain, Sambhav Jain,
Dipti Misra Sharma, and Rajeev Sangal. 2010a. Two
methods to incorporate local morphosyntactic features
in Hindi dependency parsing. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010b. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
9
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
French. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
306?313, Ann Arbor, MI.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Applica-
tion of different techniques to dependency parsing of
Basque. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Daniel M. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In Pro-
ceedings of the Second International Conference on
Human Language Technology Research, pages 178?
182. Morgan Kaufmann Publishers Inc. San Francisco,
CA, USA.
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage
of English grammars. In Proceedings of the DARPA
Speech and Natural Language Workshop, pages 306?
311, San Mateo (CA). Morgan Kaufman.
Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proceedings of the tenth conference
on European chapter of the Association for Computa-
tional Linguistics, pages 19?26, Budapest, Hungary.
Joan Bresnan. 2001. Lexical-Functional Syntax. Black-
well, Oxford.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Language Learning (CoNLL), pages 149?164,
New York, NY.
Marie Candito and Djame? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 9?16, Manchester,
UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
ACL, pages 173?180, Barcelona, Spain, June.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Annual Meeting of the
North American Chapter of the ACL (NAACL), Seattle.
David Chiang. 2000. Statistical parsing with an
automatically-extracted Tree Adjoining Grammar. In
Proceedings of the 38th Annual Meeting on Associ-
ation for Computational Linguistics, pages 456?463,
Hong Kong. Association for Computational Linguis-
tics Morristown, NJ, USA.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Michael Collins, Jan Hajic?, Lance Ramshaw, and
Christoph Tillmann. 1999. A statistical parser for
Czech. In Proceedings of the 37th Annual Meeting
of the ACL, volume 37, pages 505?512, College Park,
MD.
Michael Collins. 1997. Three Generative, Lexicalized
Models for Statistical Parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics, pages 16?23, Madrid, Spain.
Anna Corazza, Alberto Lavelli, Giogio Satta, and
Roberto Zanoli. 2004. Analyzing an Italian treebank
with state-of-the-art statistical parsers. In Proceedings
of the Third Third Workshop on Treebanks and Lin-
guistic Theories (TLT 2004), Tu?bingen, Germany.
Brooke Cowan and Michael Collins. 2005. Morphology
and reranking for the statistical parsing of Spanish. In
in Proceedins of EMNLP.
Benoit Crabbe? and Marie Candito. 2008. Expe?riences
d?analyse syntaxique statistique du franc?ais. In Actes
de la 15e`me Confe?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
Amit Dubey and Frank Keller. 2003. Probabilistic pars-
ing for German using sister-head dependencies. In In
Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 96?103,
Ann Arbor, MI.
Amit Dubey. 2005. What to do when lexicalization fails:
parsing German with suffix analysis and smoothing.
In 43rd Annual Meeting of the Association for Compu-
tational Linguistics.
10
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL.
Yoav Goldberg and Michael Elhadad. 2010. Easy-
first dependency parsing of Modern Hebrew. In Pro-
ceedings of the NAACL/HLT Workshop on Statistical
Parsing of Morphologically Rich Languages (SPMRL
2010), Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of the 46nd Annual Meet-
ing of the Association for Computational Linguistics.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and em-hmm-based lexical probabilities. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 327?335.
Kenneth L. Hale. 1983. Warlpiri and the grammar of
non-configurational languages. Natural Language and
Linguistic Theory, 1(1).
Erhard W. Hinrichs, Sandra Ku?bler, and Karin Naumann.
2005. A unified representation for morphological,
syntactic, semantic, and referential annotations. In
Proceedings of the ACL Workshop on Frontiers in Cor-
pus Annotation II: Pie in the Sky, pages 13?20, Ann
Arbor, MI.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430.
Sandra Ku?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German?
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 111?
119, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Sandra Ku?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63. Association for Com-
putational Linguistics.
Seth Kulick, Ryan Gabbard, and Mitchell Marcus. 2006.
Parsing the Arabic treebank: Analysis and improve-
ments. In Proceedings of TLT.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In International
Joint Conference on Artificial Intelligence, pages
1420?1425, Montreal.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic treebank:
Building a large-scale annotated Arabic corpus. In
Proceedings of NEMLAR International Conference on
Arabic Language Resources and Tools.
Mohamed Maamouri, Ann Bies, and Seth Kulick. 2008.
Enhanced annotation and parsing of the Arabic tree-
bank. In Proceedings of INFOS.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 276?283, Cambridge, MA.
Wolfgang Maier. 2010. Direct parsing of discontin-
uous constituents in german. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Yuval Marton, Nizar Habash, and Owen Rambow. 2010.
Improving Arabic dependency parsing with lexical and
inflectional morphological features. In Proceedings of
the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Ryan T. McDonald and Fernando C. N. Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In Proc. of EACL?06.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proc. of ACL?05, Ann Arbor, USA.
Igor Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Marianne Mithun. 1992. Is basic word order universal?
In Doris L. Payne, editor, Pragmatics of Word Order
Flexibility. John Benjamins, Amsterdam.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL), Ann Arbor, MI.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007a. The CoNLL 2007 shared task on depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, pages 915?932,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007b. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 915?932.
11
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?ls?en Eryig?it, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007c. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Ines Rehbein and Josef van Genabith. 2007. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), Prague, Czech Republic.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Djame? Seddah, Marie Candito, and Benoit Crabbe?. 2009.
Cross parser evaluation and tagset variation: A French
Treebank study. In Proceedings of the 11th Interna-
tion Conference on Parsing Technologies (IWPT?09),
pages 150?161, Paris, France, October. Association
for Computational Linguistics.
Djame? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Wojciech Skut, Thorsten Brants, Brigitte Krenn, and
Hans Uszkoreit. 1998. A linguistically interpreted
corpus of German newspaper texts. In ESSLLI
Workshop on Recent Advances in Corpus Annotation,
Saarbru?cken, Germany.
Reut Tsarfaty and Khalil Sima?an. 2007. Three-
dimensional parametrization for parsing morphologi-
cally rich languages. In Proceedings of the 10th Inter-
national Conference on Parsing Technologies (IWPT),
pages 156?167.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
Realizational parsing. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics,
pages 889?896.
Reut Tsarfaty and Khalil Sima?an. 2010. Model-
ing morphosyntactic agreement in constituency-based
parsing of Modern Hebrew. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Yannick Versley and Ines Rehbein. 2009. Scalable dis-
criminative parsing for german. In Proceedings of the
11th International Conference on Parsing Technolo-
gies (IWPT?09), pages 134?137, Paris, France, Octo-
ber. Association for Computational Linguistics.
12
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 67?75,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Handling Unknown Words in Statistical Latent-Variable Parsing Models for
Arabic, English and French
Mohammed Attia, Jennifer Foster, Deirdre Hogan, Joseph Le Roux, Lamia Tounsi,
Josef van Genabith?
National Centre for Language Technology
School of Computing, Dublin City University
{mattia,jfoster,dhogan,jleroux,ltounsi,josef}@computing.dcu.ie
Abstract
This paper presents a study of the impact
of using simple and complex morphological
clues to improve the classification of rare and
unknown words for parsing. We compare
this approach to a language-independent tech-
nique often used in parsers which is based
solely on word frequencies. This study is ap-
plied to three languages that exhibit different
levels of morphological expressiveness: Ara-
bic, French and English. We integrate infor-
mation about Arabic affixes and morphotac-
tics into a PCFG-LA parser and obtain state-
of-the-art accuracy. We also show that these
morphological clues can be learnt automati-
cally from an annotated corpus.
1 Introduction
For a parser to do a reasonable job of analysing free
text, it must have a strategy for assigning part-of-
speech tags to words which are not in its lexicon.
This problem, also known as the problem of un-
known words, has received relatively little attention
in the vast literature on Wall-Street-Journal (WSJ)
statistical parsing. This is likely due to the fact that
the proportion of unknown words in the standard
English test set, Section 23 of the WSJ section of
Penn Treebank, is quite small. The problem mani-
fests itself when the text to be analysed comes from
a different domain to the text upon which the parser
has been trained, when the treebank upon which the
parser has been trained is limited in size and when
?Author names are listed in alphabetical order. For further
correspondence, contact L. Tounsi, D. Hogan or J. Foster.
the language to be parsed is heavily inflected. We
concentrate on the latter case, and examine the prob-
lem of unknown words for two languages which lie
on opposite ends of the spectrum of morphologi-
cal expressiveness and for one language which lies
somewhere in between: Arabic, English and French.
In our experiments we use a Berkeley-style latent-
variable PCFG parser and we contrast two tech-
niques for handling unknown words within the gen-
erative parsing model: one in which no language-
specific information is employed and one in which
morphological clues (or signatures) are exploited.
We find that the improvement accrued from look-
ing at a word?s morphology is greater for Arabic
and French than for English. The morphological
clues we use for English are taken directly from the
Berkeley parser (Petrov et al, 2006) and those for
French from recent work on French statistical pars-
ing with the Berkeley parser (Crabbe? and Candito,
2008; Candito et al, 2009). For Arabic, we present
our own set of heuristics to extract these signatures
and demonstrate a statistically significant improve-
ment of 3.25% over the baseline model which does
not employ morphological information.
We next try to establish to what extent these clues
can be learnt automatically by extracting affixes
from the words in the training data and ranking these
using information gain. We show that this automatic
method performs quite well for all three languages.
The paper is organised as follows: In Section 2
we describe latent variable PCFG parsing models.
This is followed in Section 3 by a description of our
three datasets, including statistics on the extent of
the unknown word problem in each. In Section 4, we
67
present results on applying a version of the parser
which uses a simple, language-agnostic, unknown-
word handling technique to our three languages. In
Section 5, we show how this technique is extended
to include morphological information and present
parsing results for English and French. In Section 6,
we describe the Arabic morphological system and
explain how we used heuristic rules to cluster words
into word-classes or signatures. We present parsing
results for the version of the parser which uses this
information. In Section 7, we describe our attempts
to automatically determine the signatures for a lan-
guage and present parsing results for the three lan-
guages. Finally, in Section 8, we discuss how this
work might be fruitfully extended.
2 Latent Variable PCFG Parsing
Johnson (1998) showed that refining treebank cate-
gories with parent information leads to more accu-
rate grammars. This was followed by a collection of
linguistically motivated propositions for manual or
semi-automatic modifications of categories in tree-
banks (Klein and Manning, 2003). In PCFG-LAs,
first introduced by Matsuzaki et al (2005), the re-
fined categories are learnt from the treebank us-
ing unsupervised techniques. Each base category
? and this includes part-of-speech tags ? is aug-
mented with an annotation that refines its distribu-
tional properties.
Following Petrov et al (2006) latent annotations
and probabilities for the associated rules are learnt
incrementally following an iterative process consist-
ing of the repetition of three steps.
1. Split each annotation of each symbol into n
(usually 2) new annotations and create rules
with the new annotated symbols. Estimate1 the
probabilities of the newly created rules.
2. Evaluate the impact of the newly created anno-
tations and discard the least useful ones. Re-
estimate probabilities with the new set of anno-
tations.
3. Smooth the probabilities to prevent overfitting.
We use our own parser which trains a PCFG-LA us-
ing the above procedure and parses using the max-
1Estimation of the parameters is performed by running Ex-
pectation/Maximisation on the training corpus.
rule parsing algorithm (Petrov et al, 2006; Petrov
and Klein, 2007). PCFG-LA parsing is relatively
language-independent but has been shown to be very
effective on several languages (Petrov, 2009). For
our experiments, we set the number of iterations to
be 5 and we test on sentences less than or equal to
40 words in length. All our experiments, apart from
the final one, are carried out on the development sets
of our three languages.
3 The Datasets
Arabic We use the the Penn Arabic Treebank
(ATB) (Bies and Maamouri, 2003; Maamouri and
Bies., 2004). The ATB describes written Modern
Standard Arabic newswire and follows the style and
guidelines of the English Penn-II treebank. We use
the part-of-speech tagset defined by Bikel and Bies
(Bikel, 2004). We employ the usual treebank split
(80% training, 10% development and 10% test).
English We use the Wall Street Journal section of
the Penn-II Treebank (Marcus et al, 1994). We train
our parser on sections 2-21 and use section 22 con-
catenated with section 24 as our development set.
Final testing is carried out on Section 23.
French We use the French Treebank (Abeille? et
al., 2003) and divide it into 80% for training, 10%
for development and 10% for final results. We fol-
low the methodology defined by Crabbe? and Can-
dito (2008): compound words are merged and the
tagset consists of base categories augmented with
morphological information in some cases2.
Table 1 gives basic unknown word statistics for
our three datasets. We calculate the proportion of
words in our development sets which are unknown
or rare (specified by the cutoff value) in the corre-
sponding training set. To control for training set
size, we also provide statistics when the English
training set is reduced to the size of the Arabic and
French training sets and when the Arabic training set
is reduced to the size of the French training set. In an
ideal world where training set sizes are the same for
all languages, the problem of unknown words will
be greatest for Arabic and smallest for English. It is
2This is called the CC tagset: base categories with verbal
moods and extraction features
68
language cutoff #train #dev #unk %unk language #train #dev #unk %unk
Arabic 0 594,683 70,188 3794 5.40 Reduced English 597,999 72,970 2627 3.60
- 1 - - 6023 8.58 (Arabic Size) - - 3849 5.27
- 5 - - 11,347 16.17 - - - 6700 9.18
- 10 - - 15,035 21.42 - - - 9083 12.45
English 0 950,028 72,970 2062 2.83 Reduced Arabic 266,132 70,188 7027 10.01
- 1 - - 2983 4.09 (French Size) - - 10,208 14.54
- 5 - - 5306 7.27 - - - 16,977 24.19
- 10 - - 7230 9.91 - - - 21,434 30.54
French 0 268,842 35,374 2116 5.98 Reduced English 265,464 72,970 4188 5.74
- 1 - - 3136 8.89 (French Size) - - 5894 8.08
- 5 - - 5697 16.11 - - - 10,105 13.85
- 10 - - 7584 21.44 - - - 13,053 17.89
Table 1: Basic Unknown Word Statistics for Arabic, French and English
reasonable to assume that the levels of inflectional
richness have a role to play in these differences.
4 A Simple Lexical Probability Model
The simplest method for handling unknown words
within a generative probabilistic parsing/tagging
model is to reserve a proportion of the lexical rule
probability mass for such cases. This is done by
mapping rare words in the training data to a spe-
cial UNKNOWN terminal symbol and estimating rule
probabilities in the usual way. We illustrate the pro-
cess with the toy unannotated PCFG in Figures 1
and 2. The lexical rules in Fig. 1 are the original
rules and the ones in Fig. 2 are the result of apply-
ing the rare-word-to-unknown-symbol transforma-
tion. Given the input sentence The shares recovered,
the word recovered is mapped to the UNKNOWN to-
ken and the three edges corresponding to the rules
NNS ? UNKNOWN, V BD ? UNKNOWN and
JJ ? UNKNOWN are added to the chart at this posi-
tion. The disadvantage of this simple approach is ob-
vious: all unknown words are treated equally and the
tag whose probability distribution is most dominated
by rare words in the training will be deemed the
most likely (JJ for this example), regardless of the
characteristics of the individual word. Apart from
its ease of implementation, its main advantage is its
language-independence - it can be used off-the-shelf
for any language for which a PCFG is available.3
One parameter along which the simple lexical
3Our simple lexical model is equivalent to the Berkeley sim-
pleLexicon option.
probability model can vary is the threshold used to
decide whether a word in the training data is rare or
?unknown?. When the threshold is set to n, a word
in the training data is considered to be unknown if it
occurs n or fewer times. We experiment with three
thresholds: 1, 5 and 10. The result of this experi-
ment for our three languages is shown in Table 2.
The general trend we see in Table 2 is that the
number of training set words considered to be un-
known should be minimized. For all three lan-
guages, the worst performing grammar is the one
obtained when the threshold is increased to 10. This
result is not unexpected. With this simple lexical
probability model, there is a trade-off between ob-
taining good guesses for words which do not occur
in the training data and obtaining reliable statistics
for words which do. The greater the proportion of
the probability mass that we reserve for the unknown
word section of the grammar, the more performance
suffers on the known yet rare words since these are
the words which are mapped to the UNKNOWN sym-
bol. For example, assume the word restructuring oc-
curs 10 times in the training data, always tagged as
a VBG. If the unknown threshold is less than ten and
if the word occurs in the sentence to be parsed, a
VBG edge will be added to the chart at this word?s
position with the probability 10/#VBG. If, however,
the threshold is set to 10, the word (in the training set
and the input sentence) will be mapped to UNKNOWN
and more possibilities will be explored (an edge for
each TAG ? UNKNOWN rule in the grammar). We
can see from Table 1 that at threshold 10, one fifth
69
VBD -> fell 50/153
VBD -> reoriented 2/153
VBD -> went 100/153
VBD -> latched 1/153
NNS -> photofinishers 1/201
NNS -> shares 200/201
JJ -> financial 20/24
JJ -> centrist 4/24
DT -> the 170/170
Figure 1: The original toy PCFG
VBD -> fell 50/153
VBD -> UNKNOWN 3/153
VBD -> went 100/153
NNS -> UNKNOWN 1/201
NNS -> shares 200/201
JJ -> financial 20/24
JJ -> UNKNOWN 4/24
DT -> the 170/170
Figure 2: Rare ? UNKNOWN
VBD -> fell 50/153
VBD -> UNK-ed 3/153
VBD -> went 100/153
NNS -> UNK-s 1/201
NNS -> shares 200/201
JJ -> financial 20/24
JJ -> UNK-ist 4/24
DT -> the 170/170
Figure 3: Rare ? UN-
KNOWN+SIGNATURE
Unknown Threshold Recall Precision F-Score Tagging Accuracy
Arabic
1 78.60 80.49 79.53 94.03
5 77.17 79.81 78.47 91.16
10 75.32 78.69 76.97 89.06
English
1 89.20 89.73 89.47 95.60
5 88.91 89.74 89.33 94.66
10 88.00 88.97 88.48 93.61
French
1 83.60 84.17 83.88 94.90
5 82.31 83.10 82.70 92.99
10 80.87 82.05 81.45 91.56
Table 2: Varying the Unknown Threshold with the Simple Lexical Probability Model
of the words in the Arabic and French development
sets are unknown, and this is reflected in the drop in
parsing performance at these thresholds.
5 Making use of Morphology
Unknown words are not all the same. We exploit this
fact by examining the effect on parsing accuracy of
clustering rare training set words using cues from
the word?s morphological structure. Affixes have
been shown to be useful in part-of-speech tagging
(Schmid, 1994; Tseng et al, 2005) and have been
used in the Charniak (Charniak, 2000), Stanford
(Klein and Manning, 2003) and Berkeley (Petrov et
al., 2006) parsers. In this section, we contrast the
effect on parsing accuracy of making use of such in-
formation for our three languages of interest.
Returning to our toy English example in Figures 1
and 2, and given the input sentence The shares re-
covered, we would like to use the fact that the un-
known word recovered ends with the past tense
suffix -ed to boost the probability of the lexical
rule V BD ? UNKNOWN. If we specialise the
UNKNOWN terminal using information from English
morphology, we can do just that, resulting in the
grammar in Figure 3. Now the word recovered is
mapped to the symbol UNK-ed and the only edge
which is added to the chart at this position is the one
corresponding to the rule V BD ? UNK-ed.
For our English experiments we use the unknown
word classes (or signatures) which are used in the
Berkeley parser. A signature indicates whether a
words contains a digit or a hyphen, if a word starts
with a capital letter or ends with one of the following
English suffixes (both derivational and inflectional):
-s, -ed, -ing, -ion, -er, -est, -ly, -ity, -y and -al.
For our French experiments we employ the same
signature list as Crabbe? and Candito (2008), which
itself was adapted from Arun and Keller (2005).
This list consists of (a) conjugation suffixes of regu-
70
lar verbs for common tenses (eg. -ons, -ez, -ent. . . )
and (b) derivational suffixes for nouns, adverbs and
adjectives (eg. -tion, -ment, -able. . . ).
The result of employing signature information
for French and English is shown in Table 3. Be-
side each f-score the absolute improvement over the
UNKNOWN baseline (Table 2) is given. For both
languages there is an improvement at all unknown
thresholds. The improvement for English is statis-
tically significant at unknown thresholds 1 and 10.4
The improvement is more marked for French and is
statistically significant at all levels.
In the next section, we experiment with signature
lists for Arabic.5
6 Arabic Signatures
In order to use morphological clues for Arabic we
go further than just looking at suffixes. We exploit
all the richness of the morphology of this language
which can be expressed through morphotactics.
6.1 Handling Arabic Morphotactics
Morphotactics refers to the way morphemes com-
bine together to form words (Beesley, 1998; Beesley
and Karttunen, 2003). Generally speaking, morpho-
tactics can be concatenative, with morphemes either
prefixed or suffixed to stems, or non-concatenative,
with stems undergoing internal alternations to con-
vey morphosyntactic information. Arabic is consid-
ered a typical example of a language that employs
non-concatenative morphotactics.
Arabic words are traditionally classified into three
types: verbs, nouns and particles. Adjectives take
almost all the morphological forms of, and share the
same templatic structures with, nouns. Adjectives,
for example, can be definite, and are inflected for
case, number and gender.
There are a number of indicators that tell us
whether the word is a verb or a noun. Among
4Statistical significance was determined using the strati-
fied shuffling method. The software used to perform the test
was downloaded from http://www.cis.upenn.edu/
?
dbikel/software.html.
5An inspection of the Berkeley Arabic grammar (available
at http://code.google.com/p/berkeleyparser/
downloads/list) shows that no Arabic-specific signatures
were employed. The Stanford parser uses 9 signatures for Ara-
bic, designed for use with unvocalised text. An immediate fu-
ture goal is to test this signature list with our parser.
these indicators are prefixes, suffixes and word tem-
plates. A template (Beesley and Karttunen, 2003) is
a kind of vocalization mould in which a word fits. In
derivational morphology Arabic words are formed
through the amalgamation of two tiers, namely, root
and template. A root is a sequence of three (rarely
two or four) consonants which are called radicals,
and the template is a pattern of vowels, or a com-
bination of consonants and vowels, with slots into
which the radicals of the root are inserted.
For the purpose of detection we use the reverse
of this information. Given that we have a word, we
try to extract the stem, by removing prefixes and suf-
fixes, and match the word against a number of verbal
and nominal templates. We found that most Ara-
bic templatic structures are in complementary dis-
tribution, i.e. they are either restricted to nominal
or verbal usage, and with simple regular expression
matching we can decide whether a word form is a
noun or a verb.
6.2 Noun Indicators
In order to detect that a word form is a noun (or ad-
jective), we employ heuristic rules related to Arabic
prefixes/suffixes and if none of these rules apply we
attempt to match the word against templatic struc-
tures. Using this methodology, we are able to detect
95% of ATB nouns.6
We define a list of 42 noun templates which are
used to indicate active/passive participle nouns, ver-
bal nouns, nouns of instrument and broken plural
nouns (see Table 4 for some examples). Note that
templates ending with taa marboutah ?ap? or start-
ing with meem madmoumah ?mu? are not consid-
ered since they are covered by our suffix/prefix rules,
which are as follows:
1- The definite article prefix ?  or in Buckwalter
transliteration ?Al?.
2- The tanween suffix

, 

,

 or ?N?, ?F?, ?K?, ?AF?.
3- The feminine plural suffix HA, or ?+At?.
4- The taa marboutah ending ? or ?ap? whether as a
6The heuristics we developed are designed to work on dia-
critized texts. Although diacritics are generally ignored in mod-
ern writing, the issue of restoring diacritics has been satisfac-
torily addressed by different researchers. For example, Nelken
and Shieber (2005) presented an algorithm for restoring diacrit-
ics to undiacritized MSA texts with an accuracy of over 90%
and Habasah et al (2009) reported on a freely-available toolkit
(MADA-TOKAN) an accuracy of over 96%.
71
Unknown Threshold Recall Precision F-Score Tagging Accuracy
Arabic
1 80.67 82.19 *81.42 (+ 1.89) 96.32
5 80.66 82.81 *81.72 (+ 3.25) 95.15
10 79.86 82.49 *81.15 (+ 4.18) 94.38
English
1 ***89.64 89.95 89.79 (+ 0.32) 96.44
5 89.16 89.80 89.48 (+ 0.15) 96.32
10 89.14 89.78 **89.46 (+ 0.98) 96.21
French
1 85.15 85.77 *85.46 (+ 1.58) 96.13
5 84.08 84.80 *84.44 (+ 1.74) 95.54
10 84.21 84.78 *84.49 (+ 3.04) 94.68
Table 3: Baseline Signatures for Arabic, French and English
statistically significant with *:p < 10?4, **: p < 10?3, ***: p < 0.004,
Template Name Regular Specification
Arabic Buckwalter Expression
?A

?
	
?

	
K 

{inofiEAl {ino.i.A. verbal noun (masdar)
?A

?
	
??

mifoEAl mi.o.A. noun instrument
??

	
?


J?

? musotafoEil musota.o.i. noun participle
?J


?

A

	
?

? mafAEiyl ma.A.iy. noun plural
?

?
	
?


J?

{isotafoEal {isota.o.a. verb
??

?

	
? fuwEil .uw.i. verb passive
Table 4: Sample Arabic Templatic Structures for Nouns and Verbs
feminine marker suffix or part of the word.
5- The genitive case marking kasrah 

, or ?+i?.
6- Words of length of at least five characters ending
with doubled yaa ?


or ?y??.
7- Words of length of at least six characters ending
with alif mamdoudah and hamzah Z  or ?A??.
8- Words of length of at least seven characters start-
ing with meem madmoumah ? or ?mu?.
6.3 Verb Indicators
In the same way, we define a list of 16 templates and
we combine them with heuristic rules related to Ara-
bic prefixes/suffixes to detect whether a word form
is exclusively a verb. The prefix/suffix heuristics are
as follows:
9-The plural marker suffix  ? or ?uwA? indicates a
verb.
10- The prefixes H , ?


,
	
?
,



,

? or ?sa?, ?>a?,
?>u?, ?na?, ?nu?, ?ya?, ?yu?, ?ta?, ?tu? indicate im-
prefective verb.
The verbal templates are less in number than the
noun templates yet they are no less effective in de-
tecting the word class (see Table 4 for examples).
Using these heuristics we are able to detect 85% of
ATB verbs.
6.4 Arabic Signatures
We map the 72 noun/verb classes that are identi-
fied using our hand-crafted heuristics into sets of
signatures of varying sizes: 4, 6, 14, 21, 25, 28
and 72. The very coarse-grained set considers just
4 signatures UNK-noun, UNK-verb, UNK-num,
and UNK and the most fine-grained set of 72 signa-
tures associates one signature per heuristic. In ad-
dition, we have evaluated the effect of reordering
rules and templates and also the effect of collating
all signatures satisfying an unknown word. The re-
sults of using these various signatures sets in parsing
72
UNK
NUM NOUN VERB
digits (see section 6.2) (see section 6.3)
Al definiteness tashkil At suffix ap suffix imperfect
rule 1 rules 2 and 5 rule 3 rule 4 rule 10
y? suffix A? suffix mu prefix verbal noun templates suffixes
rule 6 rule 7 rule 8 3 groupings dual/plural suffixes
plural templates participle active templates participle passive templates instrument templates passive templates
4 groupings
other templates verbal templates
5 groupings
Table 6: Arabic signatures
Cutoff 1 5 10
4 80.78 80.71 80.09
6 81.14 81.16 81.06
14 80.88 81.45 81.19
14 reorder 81.39 81.01 80.81
21 81.38 81.55 81.35
21 reorder 81.20 81.13 80.58
21 collect 80.94 80.56 79.63
25 81.18 81.25 81.26
28 81.42 81.72 (+ 3.25) 81.15
72 79.64 78.87 77.58
Table 5: Baseline Signatures for Arabic
our Arabic development set are presented in Table 5.
We achieve our best labeled bracketing f-score using
28 signatures with an unknown threshold of five. In
fact we get an improvement of 3.25% over using no
signatures at all (see Table 2). Table 3 describes in
more detail the scores obtained using the 28 signa-
tures present in Table 6. Apart from the set contain-
ing 72 signatures, all of the baseline signature sets in
Table 5 yield a statistically significant improvement
over the generic UNKNOWN results (p < 10?4).
7 Using Information Gain to Determine
Signatures
It is clear that dividing the UNKNOWN terminal into
more fine-grained categories based on morpholog-
ical information helps parsing for our three lan-
guages. In this section we explore whether useful
morphological clues can be learnt automatically. If
they can, it means that a latent-variable PCFG parser
can be adapted to any language without knowledge
of the language in question since the only language-
specific component in such a parser is the unknown-
signature specification.
In a nutshell, we extract affix features from train-
ing set words7 and then use information gain to rank
these features in terms of their predictive power in a
POS-tagging task. The features deemed most dis-
criminative are then used as signatures, replacing
our baseline signatures described in Sections 5 and
6. We are not going as far as actual POS-tagging,
but rather seeing whether the affixes that make good
features for a part-of-speech tagger also make good
unknown word signatures.
We experiment with English and French suffixes
of length 1-3 and Arabic prefixes and suffixes of var-
ious lengths as well as stem prefixes and suffixes of
length 2, 4 and 6. For each of our languages we
experiment with several information gain thresholds
on our development sets and we fix on an English
signature list containing 24 suffixes, a French list
containing 48 suffixes and an Arabic list containing
38 prefixes and suffixes.
Our development set results are presented in Ta-
ble 7. For all three languages, the information gain
signatures perform at a comparable level to the base-
line hand-crafted signatures (Table 3). For each
of the three unknown-word handling techniques, no
signature (UNKNOWN), hand-crafted signatures and
information gain signatures, we select the best un-
known threshold for each language?s development
set and apply these grammars to our test sets. The
f-scores are presented in Table 8, along with the up-
per bounds obtained by parsing with these grammars
in gold-tag mode. For French, the effect of tagging
accuracy on overall parse accuracy is striking. The
improvements that we get from using morphological
signatures are greatest for Arabic8 and smallest for
7We omit all function words and high frequency words be-
cause we are interested in the behaviour of words which are
likely to be similar to rare words.
8Bikel?s parser trained on the same Arabic data and tested
on the same input achieves an f-score of 76.50%. We trained
a 5-split-merge-iteration Berkeley grammar and parsed with the
73
Unknown Threshold Recall Precision F-Score Tagging Accuracy
Arabic IG
1 80.10 82.15 *81.11 (+ 1.58) 96.53
5 80.03 82.49 *81.32 (+ 2.85) 95.30
10 80.17 82.40 *81.27 (+ 4.3) 94.66
English IG
1 89.38 89.87 89.63 (+ 0.16) 96.45
5 89.54 90.22 ***89.88 (+ 0.55) 96.41
10 89.22 90.05 *89.63 (+ 1.15) 96.19
French IG
1 84.78 85.36 *85.07 (+ 1.19) 96.17
5 84.63 85.24 **84.93 (+ 2.23) 95.30
10 84.18 84.80 *84.49 (+ 3.09) 94.68
Table 7: Information Gain Signature Results
statistically significant with *:p < 10?4, **: p < 2 ? 10?4, ***: p < 0.005
Language No Sig Baseline Sig IG Sig
Arabic 78.34 *81.59 *81.33
Arabic Gold Tag 81.46 82.43 81.90
English 89.48 89.65 89.77
English Gold Tag 89.94 90.10 90.23
French 83.74 *85.77 **85.55
French Gold Tag 88.82 88.41 88.86
statistically significant with *: p < 10?4, **: p < 10?3
Table 8: F-Scores on Test Sets
English. The results for the information gain signa-
tures are promising and warrant further exploration.
8 Conclusion
We experiment with two unknown-word-handling
techniques in a statistical generative parsing model,
applying them to Arabic, French and English. One
technique is language-agnostic and the other makes
use of some morphological information (signatures)
in assigning part-of-speech tags to unknown words.
The performance differences from the two tech-
niques are smallest for English, the language with
the sparsest morphology of the three and the small-
est proportion of unknown words in its development
set. As a result of carrying out these experiments,
we have developed a list of Arabic signatures which
can be used with any statistical parser which does
Berkeley parser, achieving an f-score of 75.28%. We trained the
Berkeley parser with the -treebank SINGLEFILE option so that
English signatures were not employed.
its own tagging. We also present results which show
that signatures can be learnt automatically.
Our experiments have been carried out using gold
tokens. Tokenisation is an issue particularly for Ara-
bic, but also for French (since the treebank contains
merged compounds) and to a much lesser extent for
English (unedited text with missing apostrophes). It
is important that the experiments in this paper are re-
peated on untokenised text using automatic tokeni-
sation methods (e.g. MADA-TOKAN).
The performance improvements that we demon-
strate for Arabic unknown-word handling are obvi-
ously just the tip of the iceberg in terms of what can
be done to improve performance on a morpholog-
ically rich language. The simple generative lexical
probability model we use can be improved by adopt-
ing a more sophisticated approach in which known
and unknown word counts are combined when esti-
mating lexical rule probabilities for rare words (see
Huang and Harper (2009) and the Berkeley sophis-
ticatedLexicon training option). Further work will
also include making use of a lexical resource exter-
nal to the treebank (Goldberg et al, 2009; Habash,
2008) and investigating clustering techniques to re-
duce data sparseness (Candito and Crabbe?, 2009).
Acknowledgements
This research is funded by Enterprise Ireland
(CFTD/07/229 and PC/09/037) and the Irish Re-
search Council for Science Engineering and Tech-
nology (IRCSET). We thank Marie Candito and our
three reviewers for their very helpful suggestions.
74
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel,
2003. Treebanks: Building and Using Parsed
Corpora, chapter Building a Treebank for French.
Kluwer, Dordrecht.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
French. In ACL. The Association for Computer Lin-
guistics.
Kenneth R. Beesley and Lauri Karttunen. 2003. Finite
State Morphology. CSLI studies in computational lin-
guistics.
Kenneth R. Beesley. 1998. Arabic morphology using
only finite-state operations. In The Workshop on Com-
putational Approaches to Semitic Languages.
Ann Bies and Mohammed Maamouri. 2003. Penn Ara-
bic Treebank guidelines. Technical Report TB-1-28-
03.
Dan Bikel. 2004. On the Parameter Space of Generative
Lexicalized Parsing Models. Ph.D. thesis, University
of Pennslyvania.
Marie Candito and Benoit Crabbe?. 2009. Improving gen-
erative statistical parsing with semi-supervised word
clustering. In Proceedings of IWPT?09.
Marie Candito, Beno??t Crabbe?, and Djame? Seddah. 2009.
On statistical parsing of French with supervised and
semi-supervised strategies. In Proceedings of the
EACL 2009 Workshop on Computational Linguis-
tic Aspects of Grammatical Inference, pages 49?57,
Athens, Greece, March.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the Annual Meeting of the
North American Association for Computational Lin-
guistics (NAACL-00), pages 132?139, Seattle, Wash-
ington.
Beno??t Crabbe? and Marie Candito. 2008. Expe?riences
d?analyse syntaxique statistique du franc?ais. In Actes
de TALN.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities.
In EACL, pages 327?335. The Association for Com-
puter Linguistics.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
Mada+tokan: A toolkit for Arabic tokenization, di-
acritization, morphological disambiguation, pos tag-
ging, stemming and lemmatization. In Proceedings of
the 2nd International Conference on Arabic Language
Resources and Tools (MEDAR).
Nizar Habash. 2008. Four techniques for online handling
of out-of-vocabulary words in arabic-english statistical
machine translation. In Proceedings of Association for
Computational Linguistics, pages 57?60.
Zhongqiang Huang and Mary Harper. 2009. Self-
training pcfg grammars with latent annotations across
languages. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
Singapore, August.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Dan Klein and Chris Manning. 2003. Accurate unlex-
icalised parsing. In Proceedings of the 41st Annual
Meeting of the ACL.
Mohammed Maamouri and Ann Bies. 2004. Developing
an Arabic Treebank: Methods, guidelines, procedures,
and tools. In Workshop on Computational Approaches
to Arabic Script-based Languages, COLING.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure. In
Proceedings of the 1994 ARPA Speech and Natural
Language Workshop, pages 114?119, Princeton, New
Jersey.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting of the ACL,
pages 75?82, Ann Arbor, June.
Rani Nelken and Stuart M. Shieber. 2005. Arabic dia-
critization using weighted finite-state transducers. In
ACL-05 Workshop on Computational Approaches to
Semitic Languages.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL, Rochester, NY, April.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the ACL, Sydney,
Australia, July.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Berkeley, Berkeley, CA, USA.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing (NeMLaP-1), pages 44?49.
Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005. Morphological features help POS tagging
of unknown words across language varieties. In Pro-
ceedings of the Fourth SIGHAN Workshop on Chinese
Language Processing.
75
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 19?27,
Beijing, August 2010
Automatic Extraction of Arabic Multiword Expressions
Mohammed Attia, Antonio Toral, Lamia Tounsi, Pavel Pecina and Josef van Genabith
School of Computing, Dublin City University
{mattia,atoral,ltounsi,ppecina,josef}@computing.dcu.ie
Abstract
In this paper we investigate the automatic
acquisition of Arabic Multiword Expres-
sions (MWE). We propose three com-
plementary approaches to extract MWEs
from available data resources. The first
approach relies on the correspondence
asymmetries between Arabic Wikipedia
titles and titles in 21 different languages.
The second approach collects English
MWEs from Princeton WordNet 3.0,
translates the collection into Arabic us-
ing Google Translate, and utilizes differ-
ent search engines to validate the output.
The third uses lexical association mea-
sures to extract MWEs from a large unan-
notated corpus. We experimentally ex-
plore the feasibility of each approach and
measure the quality and coverage of the
output against gold standards.
1 Introduction
A lexicon of multiword expressions (MWEs) has
a significant importance as a linguistic resource
because MWEs cannot usually be analyzed lit-
erally, or word-for-word. In this paper we ap-
ply three approaches to the extraction of Arabic
MWEs from multilingual, bilingual, and monolin-
gual data sources. We rely on linguistic informa-
tion, frequency counts, and statistical measures to
create a refined list of candidates. We validate the
results with manual and automatic testing.
The paper is organized as follows: in this intro-
duction we describe MWEs and provide a sum-
mary of previous related research. Section 2 gives
a brief description of the data sources used. Sec-
tion 3 presents the three approaches used in our
experiments, and each approach is tested and eval-
uated in its relevant sub-section. In Section 4 we
discuss the results of the experiments. Finally, we
conclude in Section 5.
1.1 What Are Multiword Expressions?
Multiword expressions (MWEs) are defined
as idiosyncratic interpretations that cross word
boundaries or spaces (Sag et al, 2002). The exact
meaning of an MWE is not directly obtained from
its component parts. Accommodating MWEs in
NLP applications has been reported to improve
tasks, such as text mining (SanJuan and Ibekwe-
SanJuan, 2006), syntactic parsing (Nivre and Nils-
son, 2004; Attia, 2006), and Machine Translation
(Deksne, 2008).
There are two basic criteria for identifying
MWEs: first, component words exhibit statisti-
cally significant co-occurrence, and second, they
show a certain level of semantic opaqueness or
non-compositionality. Statistically significant co-
occurrence can give a good indication of how
likely a sequence of words is to form an MWE.
This is particularly interesting for statistical tech-
niques which utilize the fact that a large number
of MWEs are composed of words that co-occur to-
gether more often than can be expected by chance.
The compositionality, or decomposabil-
ity (Villavicencio et al 2004), of MWEs is also
a core issue that presents a challenge for NLP ap-
plications because the meaning of the expression
is not directly predicted from the meaning of the
component words. In this respect, composition-
alily varies between phrases that are highly com-
19
positional, such as,       	  
 	   qfla-
?idatun ?askariyyatun, ?military base?, and those
that show a degree of idiomaticity, such as,       

   Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 49?58,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Sentiment Analysis of Political Tweets: Towards an Accurate Classifier
Akshat Bakliwal1, Jennifer Foster2, Jennifer van der Puil3?,
Ron O?Brien4, Lamia Tounsi2 and Mark Hughes5
1Search and Information Extraction Lab, IIIT-Hyderabad, India
2NCLT/CNGL, School of Computing, Dublin City University, Ireland
3Department of Computer Science and Statistics, Trinity College, Ireland
4Quiddity, Dublin, Ireland
5CLARITY, School of Computing, Dublin City University, Ireland
1akshat.bakliwal@research.iiit.ac.in
2,5{jfoster,ltounsi,mhughes}@computing.dcu.ie
3jvanderp@tcd.ie
4ron@quiddity.ie
Abstract
We perform a series of 3-class sentiment clas-
sification experiments on a set of 2,624 tweets
produced during the run-up to the Irish Gen-
eral Elections in February 2011. Even though
tweets that have been labelled as sarcastic
have been omitted from this set, it still rep-
resents a difficult test set and the highest
accuracy we achieve is 61.6% using super-
vised learning and a feature set consisting
of subjectivity-lexicon-based scores, Twitter-
specific features and the top 1,000 most dis-
criminative words. This is superior to various
naive unsupervised approaches which use sub-
jectivity lexicons to compute an overall senti-
ment score for a <tweet,political party> pair.
1 Introduction
Supervised machine learning using minimal feature
engineering has been shown to work well in binary
positive/negative sentiment classification tasks on
well-behaved datasets such as movie reviews (Pang
et al, 2002). In this paper we describe sentiment
analysis experiments in a more complicated setup:
the task is three-class positive/negative/neutral clas-
sification, the sentiment being classified is not at the
general document level but rather directed towards a
topic, the documents are tweets, and the topic is poli-
tics, specifically the Irish General Election of Febru-
ary 2011.
?Akshat Bakliwal and Jennifer van der Puil carried out their
part of this work while employed as summer interns at the Cen-
tre for Next Generation Localisation(CNGL) in the School of
Computing, DCU.
The dataset used in the experiments contains
tweets which were collected in the run up to the elec-
tion and which were subsequently doubly annotated
as positive, negative or neutral towards a particular
political party or party leader. The annotators also
marked a tweet as sarcastic if its literal sentiment
was different to its actual sentiment. Before explor-
ing the thorny issue of sentiment classification in the
face of sarcasm, we simplify the problem by first try-
ing to establish some sentiment analysis baselines
for those tweets which were not deemed to be sar-
castic.
We first explore a naive approach in which a sub-
jectivity lexicon is used as the primary source of in-
formation in determining whether sentiment towards
a political party or party leader is positive, negative
or neutral. The best version of this method achieves
an accuracy of 58.9, an absolute improvement of 4.9
points over the majority baseline (54%) in which all
tweets are classified as neutral. When these lexi-
con scores are combined with bag-of-word features
and some Twitter-specific features in a supervised
machine learning setup, this accuracy increases to
61.6%.
The paper is organised as follows: related work
is described in Section 2, followed by a brief dis-
cussion of the 2011 Irish General Election in Sec-
tion 3, a description of the dataset in Section 4
and a description of the natural language processing
tools and resources employed in Section 5. In Sec-
tion 6, the unsupervised lexicon-based approach is
presented and its limitations discussed. Section 7 de-
scribes the machine-learning-based experiments and
Section 8 concludes and provides hints towards fu-
49
ture work with this new dataset.
2 Previous Work
The related work can be divided into two groups,
general sentiment analysis research and research
which is devoted specifically to the political domain.
2.1 General Sentiment Analysis
Research in the area of sentiment mining started
with product (Turney, 2002) and movie (Pang et al,
2002) reviews. Turney (2002) used Pointwise Mu-
tual Information (PMI) to estimate the sentiment ori-
entation of phrases. Pang et al (2002) employed
supervised learning with various set of n-gram fea-
tures, achieving an accuracy of almost 83% with un-
igram presence features on the task of document-
level binary sentiment classification. Research on
other domains and genres including blogs (Chesley,
2006) and news (Godbole et al, 2007) followed.
Early sentiment analysis research focused on
longer documents such as movie reviews and blogs.
Microtext on the other hand restricts the writer to a
more concise expression of opinion. Smeaton and
Bermingham (2010) tested the hypothesis that it is
easier to classify sentiment in microtext as compared
to longer documents. They experimented with mi-
crotext from Twitter, microreviews from blippr, blog
posts and movie reviews and concluded that it is eas-
ier to identify sentiment from microtext. However,
as they move from contextually sparse unigrams to
higher n-grams, it becomes more difficult to improve
the performance of microtext sentiment classifica-
tion, whereas higher-order information makes it eas-
ier to perform classification of longer documents.
There has been some research on the use of pos-
itive and negative emoticons and hashtags in tweets
as a proxy for sentiment labels (Go et al, 2009; Pak
and Paroubek, 2010; Davidov et al, 2010; Bora,
2012). Bakliwal et al (2012) emphasized the im-
portance of preprocessing and proposed a set of
features to extract maximum sentiment information
from tweets. They used unigram and bigram fea-
tures along with features which are more associated
with tweets such as emoticons, hashtags, URLs, etc.
and showed that combining linguistic and Twitter-
specific features can boost the classification accu-
racy.
2.2 Political Sentiment Analysis
In recent years, there has been growing interest in
mining online political sentiment in order to pre-
dict the outcome of elections. One of the most in-
fluential papers is that of Tumasjan et al (2010)
who focused on the 2009 German federal election
and investigated whether Twitter can be used to pre-
dict election outcomes. Over one hundred thousand
tweets dating from August 13 to September 19, 2009
containing the names of the six parties represented
in the German parliament were collected. LIWC
2007 (Pennebaker et al, 2007) was then used to ex-
tract sentiment from the tweets. LIWC is a text anal-
ysis software developed to assess emotional, cog-
nitive and structural components of text samples
using a psychometrically validated internal dictio-
nary. Tumasjan et al concluded that the number of
tweets/mentions of a party is directly proportional to
the probability of winning the elections.
O?Connor et al (2010) investigated the extent to
which public opinion polls were correlated with po-
litical sentiment expressed in tweets. Using the Sub-
jectivity Lexicon (Wilson et al, 2005), they estimate
the daily sentiment scores for each entity. A tweet is
defined as positive if it contains a positive word and
vice versa. A sentiment score for that day is calcu-
lated as the ratio of the positive count over the neg-
ative count. They find that their sentiment scores
were correlated with opinion polls on presidential
job approval but less strongly with polls on electoral
outcome.
Choy et al (2011) discuss the application of on-
line sentiment detection to predict the vote percent-
age for each of the candidates in the Singapore pres-
idential election of 2011. They devise a formula to
calculate the percentage vote each candidate will re-
ceive using census information on variables such as
age group, sex, location, etc. They combine this
with a sentiment-lexicon-based sentiment analysis
engine which calculates the sentiment in each tweet
and aggregates the positive and negative sentiment
for each candidate. Their model was able to predict
the narrow margin between the top two candidates
but failed to predict the correct winner.
Wang et al (2012) proposed a real-time sentiment
analysis system for political tweets which was based
on the U.S. presidential election of 2012. They col-
50
lected over 36 million tweets and collected the sen-
timent annotations using Amazon Mechanical Turk.
Using a Naive Bayes model with unigram features,
their system achieved 59% accuracy on the four-
category classification.
Bermingham and Smeaton (2011) are also con-
cerned with predicting electoral outcome, in partic-
ular, the outcome of the Irish General Election of
2011 (the same election that we focused on). They
analyse political sentiment in tweets by means of su-
pervised classification with unigram features and an
annotated dataset different to and larger than the one
we present, achieving 65% accuracy on the task of
positive/negative/neutral classification. They con-
clude that volume is a stronger indicator of election
outcome than sentiment, but that sentiment still has
a role to play.
Gayo-Avello (2012) calls into question the use of
Twitter for election outcome prediction. Previous
works which report positive results on this task using
data from Twitter are surveyed and shortcomings in
their methodology and/or assumptions noted. In this
paper, our focus is not the (non-) predictive nature of
political tweets but rather the accurate identification
of any sentiment expressed in the tweets. If the ac-
curacy of sentiment analysis of political tweets can
be improved (or its limitations at least better under-
stood) then this will likely have a positive effect on
its usefulness as an alternative or complement to tra-
ditional opinion polling.
3 #ge11: The Irish General Election 2011
The Irish general elections were held on February
25, 2011. 165 representatives were elected across 43
constituencies for the Da?il, the main house of parlia-
ment. Eight parties nominated their candidates for
election and a coalition (Fine Gael and Labour) gov-
ernment was formed. The parties in the outgoing
coalition government, Fianna Fa?il and the Greens,
suffered disastrous defeats, the worst defeat of a sit-
ting government since the foundatation of the State
in 1922.
Gallagher and Marsh (2011, chapter 5) discuss the
use of social media by parties, candidates and vot-
ers in the 2011 election and conclude that it had a
much more important role to play in this election
than in the previous one in 2007. On the role of Twit-
ter in particular, they report that ?Twitter was less
widespread among candidates [than Facebook], but
it offered the most diverse source of citizen coverage
during the election, and it has been integrated into
several mainstream media?. They estimated that 7%
of the Irish population had a Twitter account at the
time of the election.
4 Dataset
We compiled a corpus of tweets using the Twitter
search API between 20th and the 25th of January
2011 (one month before the election). We selected
the main political entities (the five biggest politi-
cal parties ? Fianna Fa?il, Fine Gael, Labour, Sinn
Fe?in and the Greens ? and their leaders) and per-
form query-based search to collect the tweets relat-
ing to these entities. The resulting dataset contains
7,916 tweets of which 4,710 are retweets or dupli-
cates, leaving a total of 3,206 tweets.
The tweets were annotated by two Irish annota-
tors with a knowledge of the Irish political land-
scape. Disagreements between the two annotators
were studied and resolved by a third annotator. The
annotators were asked to identify the sentiment as-
sociated with the topic (or entity) of the tweet. An-
notation was performed using the following 6 labels:
? pos: Tweets which carry positive sentiment to-
wards the topic
? neg: Tweets which carry negative sentiment to-
wards the topic
? mix: Tweets which carry both positive and neg-
ative sentiment towards the topic
? neu: Tweets which do not carry any sentiment
towards the topic
? nen: Tweets which were written in languages
other than English.
? non: Tweets which do not have any mention
or relation to the topic. These represent search
errors.
In addition to the above six classes, annotators were
asked to flag whether a tweet was sarcastic.
The dataset which we use for the experiments
described in this paper contains only those tweets
51
Positive Tweets 256 9.75%
Negative Tweets 950 36.22%
Neutral Tweets 1418 54.03%
Total Tweets 2624
Table 1: Class Distribution
that have been labelled as either positive, negative
or neutral, i.e. non-relevant, mixed-sentiment and
non-English tweets are discarded. We also simplify
our task by omitting those tweets which have been
flagged as sarcastic by one or both of the annotators,
leaving a set of 2,624 tweets with a class distribution
as shown in Table 1.
5 Tools and Resources
In the course of our experiments, we use two differ-
ent subjectivity lexicons, one part-of-speech tagger
and one parser. For part-of-speech tagging we use
a tagger (Gimpel et al, 2011) designed specifically
for tweets. For parsing, we use the Stanford parser
(Klein and Manning, 2003). To identify the senti-
ment polarity of a word we use:
1. Subjectivity Lexicon (SL) (Wilson et al,
2005): This lexicon contains 8,221 words
(6,878 unique forms) of which 3,249 are adjec-
tives, 330 are adverbs, 1,325 are verbs, 2,170
are nouns and remaining (1,147) words are
marked as anypos. There are many words
which occur with two or more different part-of-
speech tags. We extend SL with 341 domain-
specific words to produce an extended SL.
2. SentiWordNet 3.0 (SWN) (Baccianella et al,
2010): With over 100+ thousand words, SWN
is far larger than SL but is likely to be noisier
since it has been built semi-automatically. Each
word in the lexicon is associated with both a
positive and negative score, and an objective
score given by (1), i.e. the positive, negative
and objective score sum to 1.
ObjScore = 1?PosScore?NegScore (1)
6 Naive Lexicon-based Classification
In this section we describe a naive approach to sen-
timent classification which does not make use of la-
belled training data but rather uses the information
in a sentiment lexicon to deduce the sentiment ori-
entation towards a political party in a tweet (see
Liu (2010) for an overview of this unsupervised
lexicon-based approach). In Section 6.1, we present
the basic method along with some variants which
improve on the basic method by making use of infor-
mation about part-of-speech, negation and distance
from the topic. In Section 6.2, we examine some
of the cases which remain misclassified by our best
lexicon-based method. In Section 6.3, we discuss
briefly those tweets that have been labelled as sar-
castic.
6.1 Method and Results
Our baseline lexicon-based approach is as follows:
we look up each word in our sentiment lexicon and
sum up the scores to corresponding scalars. The re-
sults are shown in Table 2. Note that the most likely
estimated class prediction is neutral with a probabil-
ity of .5403 (1418/2624).
6.1.1 Which Subjectivity Lexicon?
The first column shows the results that we obtain
when the lexicon we use is our extended version of
the SL lexicon. The results in the second column
are those that result from using SWN. In the third
column, we combine the two lexicons. We define
a combination pattern of Extended-SL and SWN in
which we prioritize Extended-SL because it is man-
ually checked and some domain-specific words are
added. For the words which were missing from
Extended-SL (SWN), we assign them the polarity of
SWN (Extended-SL). Table 3 explains exactly how
the scores from the two lexicons are combined. Al-
though SWN slightly outperforms Extended-SL for
the baseline lexicon-based approach (first row of Ta-
ble 2), it is outperformed by Extended-SL and the
combinaton of the two lexicons for all the variants.
We can conclude from the full set of results in Ta-
ble 2 that SWN is less useful than Extended-SL or
the combination of SWN and Extended-SL.
6.1.2 Filtering by Part-of-Speech
The results in the first row of Table 2 represent
our baseline experiment in which each word in the
tweet is looked up in the sentiment lexicon and
its sentiment score added to a running total. We
achieve a classification accuracy of 52.44% with the
52
Method Extended-SL SWN Combined
3-Class Classification (Pos vs
Neg vs Neu)
Correct Accuracy Correct Accuracy Correct Accuracy
Baseline 1376 52.44% 1379 52.55% 1288 49.09%
Baseline + Adj 1457 55.53% 1449 55.22% 1445 55.07%
Baseline + Adj + S 1480 56.40% 1459 55.60% 1481 56.44%
Baseline + Adj + S + Neg 1495 56.97% 1462 55.72% 1496 57.01%
Baseline + Adj + S + Neg +
Phrases
1511 57.58% 1479 56.36% 1509 57.51%
Baseline + Adj + S + Neg +
Phrases + Than
1533 58.42% 1502 57.24% 1533 58.42%
Distance Based Scoring:
Baseline + Adj + S + Neg +
Phrases + Than
1545 58.88% 1506 57.39% 1547 58.96%
Sarcastic Tweets 87/344 25.29% 81/344 23.55% 87/344 25.29%
Table 2: 3-class classification using the naive lexicon-based approach. The majority baseline is 54.03%.
Extended-
SL
polarity
SWN
Polarity
Combination
Polarity
-1 -1 -2
-1 0 -1
-1 1 -1
0 -1 -0.5
0 0 0
0 1 0.5
1 -1 1
1 0 1
1 1 2
Table 3: Combination Scheme of extended-SL and SWN.
Here 0 represents either a neutral word or a word missing
from the lexicon.
Extended-SL lexicon. We speculate that this low
accuracy is occurring because too many words that
appear in the sentiment lexicon are included in the
overall sentiment score without actually contribut-
ing to the sentiment towards the topic. To refine our
approach one step further, we use part-of-speech in-
formation and consider only adjectives for the clas-
sification of tweets since adjectives are strong in-
dicators of sentiment (Hatzivassiloglou and Wiebe,
2000). We achieve an accuracy improvement of ap-
proximately three absolute points, and this improve-
ment holds true for both sentiment lexicons. This
supports our hypothesis that we are using irrelevant
information for classification in the baseline system.
Our next improvement (third row of Table 2)
comes from mapping all inflected forms to their
stems (using the Porter stemmer). Examples of in-
flected forms that are reduced to their stems are de-
lighted or delightful. Using stemming with adjec-
tives over the baseline, we achieve an accuracy of
56.40% with Extended-SL.
6.1.3 Negation
?Negation is a very common linguistic construc-
tion that affects polarity and, therefore, needs to
be taken into consideration in sentiment analysis?
(Councill et al, 2010). We perform negation han-
dling in tweets using two different approaches. In
the first approach, we first identify negation words
53
and reverse the polarity of sentiment-bearing words
within a window of three words. In the second ap-
proach, we try to resolve the scope of the negation
using syntactic parsing. The Stanford dependency
scheme (de Marneffe and Manning, 2008) has a spe-
cial relation (neg) to indicate negation. We reverse
the sentiment polarity of a word marked via the neg
relation as being in the scope of a negation. Using
the first approach, we see an improvement of 0.6%
in the classification accuracy with the Extended-SL
lexicon. Using the second approach, we see an
improvement of 0.5%. Since there appears to be
very little difference between the two approaches to
negation-handling and in order to reduce the compu-
tational burden of running the Stanford parser each
time to obtain the dependencies, we continue further
experiments with the first method only. Using base-
line + stemming + adjectives + neg we achieve an
accuracy of 56.97% with the Extended-SL lexicon.
6.1.4 Domain-specific idioms
In the context of political tweets we see many
sentiment-bearing idioms and fixed expressions, e.g.
god save us, X for Taoiseach1, wolf in sheep?s cloth-
ing, etc. In our study, we had a total of 89 phrases.
When we directly account for these phrases, we
achieve an accuracy of 57.58% (an absolute im-
provement of 0.6 points over the last step).
6.1.5 Comparative Expressions
Another form of expressing an opinion towards
an entity is by comparing the entity with some other
entity. For example consider the tweet:
Fast Food sounds like a better vote than Fianna Fail.
(2)
In this tweet, an indirect negative sentiment is ex-
pressed towards the political party Fianna Fa?il. In
order to take into account such constructions, the
following procedure is applied: we divide the tweet
into two parts, left and right. The left part contains
the text which comes before the than and the right
part contains the text which comes after than, e.g.
Tweet: ?X is better than Y?
Left: ?X is better?
Right: ?Y?.
1The term Taoiseach refers to the Irish Prime Minister.
We then use the following strategy to calculate the
polarity of the tweet oriented towards the entity:
S left = sentiment score of Left.
S right = sentiment score of Right.
Ent pos left = if entity is left of
?than?, then 1, otherwise ? 1.
Ent pos right = if entity is right of
?than?, then 1, otherwise ? 1.
S(tweet) = Ent pos left ? S left +
Ent pos right ? S right. (3)
So in (2) above the entity, Fianna Fa?il, is to the
right of than meaning that its Ent pos right value
is 1 and its Ent pos left value is -1. This has the
effect of flipping the polarity of the positive word
better. By including the ?than? comparison, we see
an improvement of absolute 0.8% (third last row of
Table 2).
6.1.6 Distance Scoring
To emphasize the topic-oriented nature of our sen-
timent classification, we also define a distance-based
scoring function where we define the overall score
of the tweet as given in (4). Here dis(word) is de-
fined as number of words between the topic (i.e. the
political entity) and the sentiment word.
S(tweet) =
n?
i=1
S(wordi)/dis(wordi). (4)
The addition of the distance information further en-
hanced our system accuracy by 0.45%, taking it to
58.88% (second last row of Table 2). Our highest
overall accuracy (58.96) is achieved in this setting
using the combined lexicon.
It should be noted that this lexicon-based ap-
proach is overfitting to our dataset since the list of
domain-specific phrases and the form of the com-
parative constructions have been obtained from the
dataset itself. This means that we are making a
strong assumption about the representativeness of
this dataset and accuracy on a held-out test set is
likely to be lower.
6.2 Error Analysis
In this section we discuss pitfalls of the naive
lexicon-based approach with the help of some exam-
ples (see Table 4). Consider the first example from
54
the table, @username and u believe people in fianna
fail . What are you a numbskull or a journalist ?
In this tweet, we see that negative sentiment is im-
parted by the question part of the tweet, but actually
there are no sentiment adjectives. The word numb-
skull is contributing to the sentiment but is tagged as
a noun and not as an adjective. This tweet is tagged
as negative by our annotators and as neutral by our
lexicon-based classifier.
Consider the second example from Table 4,
@username LOL . A guy called to our house tonight
selling GAA tickets . His first words were : I?m
not from Fianna Fail . This is misclassified because
there are no sentiment bearing words according to
the sentiment lexicon. The last tweet in the table rep-
resents another example of the same problem. Note
however that the emoticon :/ in the last tweet and the
web acronym LOL in the second tweet are providing
hints which our system is not making use of.
In the third example from Table 4, @username
Such scary words .? Sinn Fein could top the poll ?
in certain constituencies . I feel sick at the thought
of it . ? In this example, we have three sentiment
bearing words: scary, top and sick. Two of the three
words are negative and one word is positive. The
word scary is stemmed incorrectly as scari which
means that it is out of the scope of our lexicons.
If we just count the number of sentiment words re-
maining, then this tweet is labelled as neutral but ac-
tually is negative with respect to the party Sinn Fe?in.
We proposed the use of distance as a measure of re-
latedness to the topic and we observed a minor im-
provement in classification accuracy. However, for
this example, the distance-based approach does not
work. The word top is just two words away from the
topic and thus contributes the maximum, resulting in
the whole tweet being misclassified as positive.
6.3 Sarcastic Tweets
?Political discouse is plagued with humor, double
entendres, and sarcasm; this makes determining po-
litical preference of users hard and inferring voting
intention even harder.?(Gayo-Avello, 2012)
As part of the annotation process, annotators were
asked to indicate whether they thought a tweet ex-
hibited sarcasm. Some examples of tweets that were
annotated as sarcastic are shown in Table 5.
We made the decision to omit these tweets from
the main sentiment classification experiments under
the assumption that they constituted a special case
which would be better handled by a different clas-
sifier. This decision is vindicated by the results in
the last row of Table 2 which show what happens
when we apply our best classifier (Distance-based
Scoring: Baseline+Adj+S+Neg+Phrases+Than) to
the sarcastic tweets ? only a quarter of them are cor-
rectly classified. Even with a very large and highly
domain-tuned lexicon, the lexicon-based approach
on its own will struggle to be of use for cases such
as these, but the situation might be improved were
the lexicon to be used in conjunction with possible
sarcasm indicators such as exclamation marks.
7 Supervised Machine Learning
Although our dataset is small, we investigate
whether we can improve over the lexicon-based ap-
proach by using supervised machine learning. As
our learning algorithm, we employ support vector
machines in a 5-fold cross validation setup. The tool
we use is SVMLight (Joachims, 1999).
We explore two sets of features. The first are the
tried-and-tested unigram presence features which
have been used extensively not only in sentiment
analysis but in other text classification tasks. As we
have only 2,624 training samples, we performed fea-
ture selection by ranking the features using the Chi-
squared metric.
The second feature set consists of 25 features
which are inspired by the work on lexicon-based
classification described in the previous section.
These are the counts of positive, negative, objec-
tive words according to each of the three lexicons
and the corresponding sentiment scores for the over-
all tweets. In total there are 19 such features. We
also employ six Twitter-related presence features:
positive emoticons, negative emoticons, URLs, pos-
itive hashtags, negative hashtags and neutral hash-
tags. For further reference we call this second set of
features our ?hand-crafted? features.
The results are shown in Table 6. We can see
that using the hand-crafted features alone barely im-
proves over the majority baseline of 54.03 but it does
improve over our baseline lexicon-based approach
(see first row of Table 2). Encouragingly, we see
some benefit from using these features in conjunc-
55
Tweet Topic
Manual
Polar-
ity
Calculated
Polarity
Reason for
misclassifica-
tion
@username and u believe people in fianna fail .
What are you a numbskull or a journalist ?
Fianna
Fa?il
neg neu
Focus only on
adjectives
@username LOL . A guy called to our house
tonight selling GAA tickets . His first words were :
I?m not from Fianna Fail .
Fianna
Fa?il
neg neu
No sentiment
words
@username Such scary words .? Sinn Fein could
top the poll ? in certain constituencies . I feel sick
at the thought of it .
Sinn
Fe?in
neg pos
Stemming
and word
distance order
@username more RTE censorship . Why are they
so afraid to let Sinn Fein put their position across .
Certainly couldn?t be worse than ff
Sinn
Fe?in
pos neg
contribution
of afraid
Based on this programme the winners will be Sinn
Fein & Gilmore for not being there #rtefl
Sinn
Fe?in
pos neu
Focus only on
adjectives
#thefrontline pearce Doherty is a spoofer ! Vote
sinn fein and we loose more jobs
Sinn
Fe?in
neg pos
Focus only on
adjectives &
contribution
of phrase Vote
X
@username Tread carefully Conor . BNP
endorsing Sinn Fin etc . etc .
Sinn
Fe?in
neg neu
No sentiment
words
@username ah dude . You made me go to the fine
gael web site ! :/
Fine
Gael
neg neu
No sentiment
words
Table 4: Misclassification Examples
Feature Set # Features Accuracy
# samples = 2624 SVM Light
Hand-crafted 25 54.76
Unigram
7418 55.22
Top 1000 58.92
Top 100 56.86
Unigram + Hand-crafted
7444 54.73
Top 1000 61.62
Top 100 59.53
Table 6: Results of 3-Class Classification using Super-
vised Machine Learning
tion with the unigram features. Our best overall re-
sult of 61.62% is achieved by using the Top 1000 un-
igram features together with these hand-crafted fea-
tures. This result seems to suggest that, even with
only a few thousand training instances, employing
supervised machine learning is still worthwhile.
8 Conclusion
We have introduced a new dataset of political tweets
which will be made available for use by other re-
searchers. Each tweet in this set has been annotated
for sentiment towards a political entity, as well as
for the presence of sarcasm. Omitting the sarcastic
tweets from our experiments, we show that we can
classify a tweet as being positive, negative or neutral
towards a particular political party or party leader
with an accuracy of almost 59% using a simple ap-
proach based on lexicon lookup. This improves over
the majority baseline by almost 5 absolute percent-
age points but as the classifier uses information from
the test set itself, the result is likely to be lower on
a held-out test set. The accuracy increases slightly
when the lexicon-based information is encoded as
features and employed together with bag-of-word
features in a supervised machine learning setup.
Future work involves carrying out further exper-
56
Sarcastic Tweets
Ah bless Brian Cowen?s little cotton socks! He?s staying on as leader of FF because its better for the
country. How selfless!
So now Brian Cowen is now Minister for foreign affairs and Taoiseach? Thats exactly what he needs
more responsibilities http://bbc.in/hJI0hb
Mary Harney is going. Surprise surprise! Brian Cowen is going to be extremely busy with all these
portfolios to administer. Super hero!
Now in its darkest hour Fianna Fail needs. . . Ivor!
Labour and Fine Gael have brought the election forward by 16 days Crisis over Ireland is SAVED!! #vinb
@username Maybe one of those nice Sinn Fein issue boiler suits? #rtefl
I WILL vote for Fine Gael if they pledge to dress James O?Reilly as a leprechaun and send him
to the White House for Paddy?s Day.
Table 5: Examples of tweets which have been flagged as sarcastic
iments on those tweets that have been annotated as
sarcastic, exploring the use of syntactic dependency
paths in the computation of distance between a word
and the topic, examining the role of training set class
bias on the supervised machine learning results and
exploring the use of distant supervision to obtain
more training data for this domain.
Acknowledgements
Thanks to Emmet O Briain, Lesley Ni Bhriain and
the anonymous reviewers for their helpful com-
ments. This research has been supported by En-
terprise Ireland (CFTD/2007/229) and by Science
Foundation Ireland (Grant 07/CE/ I1142) as part of
the CNGL (www.cngl.ie) at the School of Comput-
ing, DCU.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10).
Akshat Bakliwal, Piyush Arora, Senthil Madhappan,
Nikhil Kapre, Mukesh Singh, and Vasudeva Varma.
2012. Mining sentiments from tweets. In Proceedings
of the WASSA?12 in conjunction with ACL?12.
Adam Bermingham and Alan F. Smeaton. 2010. Clas-
sifying sentiment in microblogs: is brevity an advan-
tage? In Proceedings of the 19th ACM international
conference on Information and Knowledge Manage-
ment.
Adam Bermingham and Alan Smeaton. 2011. On using
Twitter to monitor political sentiment and predict elec-
tion results. In Proceedings of the Workshop on Sen-
timent Analysis where AI meets Psychology (SAAIP
2011).
Nibir Nayan Bora. 2012. Summarizing public opinions
in tweets. In Journal Proceedings of CICLing 2012.
Paula Chesley. 2006. Using verbs and adjectives to au-
tomatically classify blog sentiment. In Proceedings
of AAAI-CAAW-06, the Spring Symposia on Computa-
tional Approaches.
Murphy Choy, Michelle L. F. Cheong, Ma Nang Laik,
and Koo Ping Shung. 2011. A sentiment analysis
of Singapore Presidential Election 2011 using Twitter
data with census correction. CoRR, abs/1108.5520.
Isaac G. Councill, Ryan McDonald, and Leonid Ve-
likovich. 2010. What?s great and what?s not: learn-
ing to classify the scope of negation for improved sen-
timent analysis. In Proceedings of the Workshop on
Negation and Speculation in Natural Language Pro-
cessing, NeSp-NLP ?10.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using Twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the COLING Workshop
on Cross-Framework and Cross-Domain Parser Eval-
uation.
Michael Gallagher and Michael Marsh. 2011. How Ire-
land Voted 2011: The Full Story of Ireland?s Earth-
quake Election. Palgrave Macmillan.
Daniel Gayo-Avello. 2012. ?I wanted to predict elec-
tions with Twitter and all I got was this lousy paper?.
57
A balanced survey on election prediction using Twitter
data. CoRR, abs/1204.6441.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
Twitter: annotation, features, and experiments. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies: short papers - Volume 2.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter
sentiment classification using distant supervision. In
CS224N Project Report, Stanford University.
Namrata Godbole, Manjunath Srinivasaiah, and Steven
Skiena. 2007. Large-scale sentiment analysis for
news and blogs. In Proceedings of the International
Conference on Weblogs and Social Media (ICWSM).
Vasileios Hatzivassiloglou and Janyce M. Wiebe. 2000.
Effects of adjective orientation and gradability on sen-
tence subjectivity. In Proceedings of COLING.
Thorsten Joachims. 1999. Advances in kernel meth-
ods. chapter Making large-scale support vector ma-
chine learning practical, pages 169?184. MIT Press,
Cambridge, MA, USA.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1.
Bing Liu. 2010. Handbook of natural language pro-
cessing. chapter Sentiment Analysis and Subjectivity.
Chapman and Hall.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010. From
tweets to polls: Linking text sentiment to public opin-
ion time series. In Proceedings of the International
Conference on Weblogs and Social Media (ICWSM).
Alexander Pak and Patrick Paroubek. 2010. Twitter as a
corpus for sentiment analysis and opinion mining. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10).
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the con-
ference on Empirical Methods in Natural Language
Processing - Volume 10.
James W. Pennebaker, Cindy K. Chung, Molly Ireland,
Amy Gonzales, and Roger J. Booth. 2007. The de-
velopment and psychometric properties of liwc2007.
Technical report, Austin,Texas.
Andranik Tumasjan, Timm Oliver Sprenger, Philipp G.
Sandner, and Isabell M. Welpe. 2010. Predicting elec-
tions with Twitter: What 140 characters reveal about
political sentiment. In Proceedings of the Interna-
tional Conference on Weblogs and Social Media.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics.
Hao Wang, Dogan Can, Abe Kazemzadeh, Franc?ois Bar,
and Shrikanth Narayanan. 2012. A system for real-
time Twitter sentiment analysis of 2012 U.S. presiden-
tial election cycle. In ACL (System Demonstrations).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing.
58
Proceedings of the First Celtic Language Technology Workshop, pages 41?49,
Dublin, Ireland, August 23 2014.
Cross-lingual Transfer Parsing for Low-Resourced Languages: An Irish
Case Study
Teresa Lynn
1,2
, Jennifer Foster
1
, Mark Dras
2
and Lamia Tounsi
1
1
CNGL, School of Computing, Dublin City University, Ireland
2
Department of Computing, Macquarie University, Sydney, Australia
1
{tlynn,jfoster,ltounsi}@computing.dcu.ie
2
{teresa.lynn,mark.dras}@mq.edu.au
Abstract
We present a study of cross-lingual direct transfer parsing for the Irish language. Firstly we
discuss mapping of the annotation scheme of the Irish Dependency Treebank to a universal de-
pendency scheme. We explain our dependency label mapping choices and the structural changes
required in the Irish Dependency Treebank. We then experiment with the universally annotated
treebanks of ten languages from four language family groups to assess which languages are the
most useful for cross-lingual parsing of Irish by using these treebanks to train delexicalised pars-
ing models which are then applied to sentences from the Irish Dependency Treebank. The best
results are achieved when using Indonesian, a language from the Austronesian language family.
1 Introduction
Considerable efforts have been made over the past decade to develop natural language processing re-
sources for the Irish language (U?? Dhonnchadha et al., 2003; U?? Dhonnchadha and van Genabith, 2006;
U?? Dhonnchadha, 2009; Lynn et al., 2012a; Lynn et al., 2012b; Lynn et al., 2013). One such resource
is the Irish Dependency Treebank (Lynn et al., 2012a) which contains just over 1000 gold standard de-
pendency parse trees. These trees are labelled with deep syntactic information, marking grammatical
roles such as subject, object, modifier, and coordinator. While a valuable resource, the treebank does not
compare in size to similar resources of other languages.
1
The small size of the treebank affects the accu-
racy of any statistical parsing models learned from this treebank. Therefore, we would like to investigate
whether training data from other languages can be successfully utilised to improve Irish parsing.
Cross-lingual transfer parsing involves training a parser on one language, and parsing data of another
language. McDonald et al. (2011) describe two types of cross-lingual parsing, direct transfer parsing in
which a delexicalised version of the source language treebank is used to train a parsing model which
is then used to parse the target language, and a more complicated projected transfer approach in which
the direct transfer approach is used to seed a parsing model which is then trained to obey source-target
constraints learned from a parallel corpus. These experiments revealed that languages that were typo-
logically similar were not necessarily the best source-target pairs, sometimes due to variations between
their language-specific annotation schemes. In more recent work, however, McDonald et al. (2013) re-
ported improved results on cross-lingual direct transfer parsing using a universal annotation scheme, to
which six chosen treebanks are mapped for uniformity purposes. Underlying the experiments with this
new annotation scheme is the universal part-of-speech (POS) tagset designed by Petrov et al. (2012).
While their results confirm that parsers trained on data from languages in the same language group (e.g.
Romance and Germanic) show the most accurate results, they also show that training data taken across
language-groups also produces promising results. We attempt to apply the direct transfer approach with
Irish as the target language.
The Irish language belongs to the Celtic branch of the Indo-European language family. The natural
first step in cross-lingual parsing for Irish would be to look to those languages of the Celtic language
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
For example, the Danish dependency treebank has 5,540 trees (Kromann, 2003); the Finnish dependency treebank has
15,126 trees (Haverinen et al., 2013)
41
group, i.e. Welsh, Scots Gaelic, Manx, Breton and Cornish, as a source of training data. However,
these languages are just as, if not further, under-resourced. Thus, we attempt to use the languages of the
universal dependency treebanks (McDonald et al., 2013).
The paper is organised as follows. In Section 2, we give an overview of the status of the Irish lan-
guage and the Irish Dependency Treebank. Section 3 describes the mapping of the Irish Dependency
Treebank?s POS tagset (U?? Dhonnchadha and van Genabith, 2006) to that of Petrov et al. (2012), and
the Irish Dependency Treebank annotation scheme (Lynn et al. (2012b)) to the Universal Dependency
Scheme. Following that, in Section 4 we carry out cross-lingual direct transfer parsing experiments with
ten harmonised treebanks to assess whether any of these languages are suitable for such parsing transfer
for Irish. Section 5 summarises our work.
2 Irish Language and Treebank
Irish, a minority EU language, is the national and official language of Ireland. Despite this status, Irish
is only spoken on a daily basis by a minority. As a Celtic language, Irish shares specific linguistic
features with other Celtic languages, such as a VSO (verb-subject-object) word order and interesting
morphological features such as inflected prepositions and initial mutations, for example.
Compared to other EU-official languages, Irish language technology is under-resourced, as highlighted
by a recent study (Judge et al., 2012). In the area of morpho-syntactic processing, recent years have seen
the development of a part-of-speech tagger (U?? Dhonnchadha and van Genabith, 2006), a morphological
analyser (U?? Dhonnchadha et al., 2003), a shallow chunker (U?? Dhonnchadha, 2009), a dependency tree-
bank (Lynn et al., 2012a; Lynn et al., 2012b) and statistical dependency parsing models for MaltParser
(Nivre et al., 2006) and Mate parser (Bohnet, 2010) trained on this treebank (Lynn et al., 2013).
The annotation scheme for the Irish Dependency Treebank (Lynn et al., 2012b) was inspired by Lexical
Functional Grammar (Bresnan, 2001) and has its roots in the dependency annotation scheme described
by C?etino?glu et al. (2010). It was extended and adapted to suit the linguistic characterisics of the Irish
language. The final label set consists of 47 dependency labels, defining grammatical and functional
relations between the words in a sentence. The label set is hierarchical in nature with labels such as
vparticle (verb particle) and vocparticle (vocative particle), for example, representing more
fine-grained versions of the particle label.
3 A universal dependency scheme for the Irish Dependency Treebank
In this section, we describe how a ?universal? version of the Irish Dependency Treebank was created by
mapping the original POS tags to universal POS tags and mapping the original dependency scheme to the
universal dependency scheme. The result of this effort is an alternative version of the Irish Dependency
Treebank which will be made available to the research community along with the original.
3.1 Mapping the Irish POS tagset to the Universal POS tagset
The Universal POS tagset (Petrov et al., 2012) has been designed to facilitate unsupervised and cross-
lingual part-of-speech tagging and parsing research, by simplifying POS tagsets and unifying them across
languages. The Irish Dependency Treebank was built upon a POS-tagged corpus developed by U?? Dhon-
nchadha and van Genabith (2006). The treebank?s tagset contains both coarse- and fine-grained POS tags
which we map to the Universal POS tags (e.g. Prop Noun? NOUN). Table 1 shows the mappings.
Most of the POS mappings made from the Irish POS tagset to the universal tagset are intuitive. How-
ever, some decisions require explanation.
Cop ? VERB There are two verbs ?to be? in Irish: the substantive verb b?? and the copula is. For
that reason, the Irish POS tagset differentiates the copula by using the POS tag Cop. In Irish syntax
literature, there is some discussion over its syntactic role, whether it is a verb or a linking particle. The
role normally played is that of a linking element between a subject and a predicate. However, Lynn et al.
(2012a)?s syntactic analysis of the copula is in line with that of Stenson (1981), regarding it as a verb. In
addition, because the copula is often labelled in the Irish annotation scheme as the syntactic head of the
matrix clause, we have chosen VERB as the most suitable mapping for this part of speech.
42
Part-of-speech (POS) mappings
Universal Irish Universal Irish
NOUN
Noun Noun, Pron Ref,
Subst Subst, Verbal Noun,
Prop Noun
ADP
Prep Deg, Prep Det, Prep Pron,
Prep Simp, Prep Poss,
Prep CmpdNoGen, Prep Cmpd,
Prep Art, Pron Prep
PRON
Pron Pers, Pron Idf, Pron Q,
Pron Dem
ADV
Adv Temp, Adv Loc, Adv Dir,
Adv Q, Adv Its, Adv Gn
VERB
Cop Cop, Verb PastInd, Verb PresInd,
Verb PresImp, Verb VI, Verb VT,
Verb VTI, Verb PastImp, Verb Cond,
Verb FutInd, Verb VD, Verb Imper
PRT
Part Vb, Part Sup, Part Inf, Part Pat,
Part Voc, Part Ad, Part Deg, Part Comp,
Part Rel, Part Num, Part Cp,
DET Art Art, Det Det NUM Num Num
ADJ Prop Adj, Verbal Adj, Adj Adj X
Item Item, Abr Abr, CM CM, CU CU,
CC CC, Unknown Unknown,
Guess Abr, Itj Itj, Foreign Foreign,
CONJ Conj Coord, Conj Subord . . . ... ... ? ? ! ! : : ? . Punct Punct
Table 1: Mapping of Irish Coarse and Fine-grained POS pairs (coarse fine) to Universal POS tagset.
Pron Prep?ADP Pron Prep is the Irish POS tag for pronominal prepositions, which are also referred
to as prepositional pronouns. Characteristic of Celtic languages, they are prepositions inflected with their
pronominal objects ? compare, for example, le mo chara ?with my friend? with leis ?with him?. While
the Irish POS labelling scheme labels them as pronouns in the first instance, our dependency labelling
scheme treats the relationship between them and their syntactic heads as obl (obliques) or padjunct
(prepositional adjuncts). Therefore, we map them to ADP (adpositions).
3.2 Mapping the Irish Dependency Scheme to the Universal Dependency Scheme
The departure point for the design of the Universal Dependency Annotation Scheme (McDonald et
al., 2013) was the Stanford typed dependency scheme (de Marneffe and Manning, 2008), which was
adapted based on a cross-lingual analysis of six languages: English, French, German, Korean, Spanish
and Swedish. Existing English and Swedish treebanks were automatically mapped to the new universal
scheme. The rest of the treebanks were developed manually to ensure consistency in annotation. The
study also reports some structural changes (e.g. Swedish treebank coordination structures).
2
There are 41 dependency relation labels to choose from in the universal annotation scheme
3
. McDon-
ald et al. (2013) use all labels in the annotation of the German and English treebanks. The remaining
languages use varying subsets of the label set. In our study we map the Irish dependency annotation
scheme to 30 of the universal labels. The mappings are given in Table 2.
As with the POS mapping discussed in Section 3.1, mapping the Irish dependency scheme to the
universal scheme was relatively straightforward, due in part, perhaps, to a similar level of granularity
suggested by the similar label set sizes (Irish 47; standard universal 41). That said, there were significant
considerations made in the mapping process, which involved some structural change in the treebank and
the introduction of more specific analyses in the labelling scheme. These are discussed below.
3.2.1 Structural Differences
The following structural changes were made manually before the dependency labels were mapped to the
universal scheme.
coordination The most significant structural change made to the treebank was an adjustment to the
analysis of coordination. The original Irish Dependency Treebank subscribes to the LFG coordination
analysis, where the coordinating conjunction (e.g. agus ?and?) is the head, with the coordinates as its
dependents, labelled coord (see Figure 1). The Universal Dependency Annotation scheme, on the
2
There are two versions of the annotation scheme: the standard version (where copulas and adpositions are syntactic heads),
and the content-head version which treats content words as syntactic heads. We are using the standard version for our study.
3
The vmod label is used only in the content-head version.
43
Dependency Label Mappings
Universal Irish Universal Irish
root top csubj csubj
acomp adjpred, advpred, ppred dep for
adpcomp N/A det det, det2, dem
adpmod padjunct, obl, obl2, obl ag dobj obj, vnobj, obj q
adpobj pobj mark subadjunct
advcl N/A nmod addr, nadjunct
advmod
adjunct, advadjunct, quant,
advadjunct q
nsubj subj, subj q
amod adjadjunct num N/A
appos app p punctuation
attr npred parataxis N/A
aux toinfinitive poss poss
cc N/A prt
particle, vparticle, nparticle, advparticle,
vocparticle, particlehead, cleftparticle,
qparticle, aug
ccomp comp rcmod relmod
compmod nadjunct rel relparticle
conj coord xcomp xcomp
Table 2: Mapping of Irish Dependency Annotation Scheme to Universal Dependency Annotation Scheme
other hand, uses right-adjunction, where the first coordinate is the head of the coordination, and the
rest of the phrase is adjoined to the right, labelling coordinating conjunctions as cc and the following
coordinates as conj (Figure 2).
coord det subj advpred top coord det subj advpred obl det pobj
Bh?? an l?a an-te agus bh?? gach duine sti?ugtha leis an tart
Be-PAST the day very-hot and be-PAST every person parched with the thirst
?The day was very hot and everyone was parched with the thirst?
Figure 1: LFG-style coordination of original Irish Dependency Treebank
top det subj advpred cc conj det subj advpred obl det pobj
Bh?? an l?a an-te agus bh?? gach duine sti?ugtha leis an tart
Be-PAST the day very-hot and be-PAST every person parched with the thirst
?The day was very hot and everyone was parched with the thirst?
Figure 2: Stanford-style coordination changes to original Irish Dependency Treebank
subordinate clauses In the original Irish Dependency Treebank, the link between a matrix clause and
its subordinate clause is similar to that of LFG: the subordinating conjunction (e.g. mar ?because?, nuair
?when?) is a subadjunct dependent of the matrix verb, and the head of the subordinate clause is a
comp dependent of the subordinating conjunction (Figure 3). In contrast, the universal scheme is in
line with the Stanford analysis of subordinate clauses, where the head of the clause is dependent on the
matrix verb, and the subordinating conjunction is a dependent of the clause head (Figure 4).
3.2.2 Differences between dependency types
We found that the original Irish scheme makes distinctions that the universal scheme does not ? this
finer-grained information takes the form of the following Irish-specific dependency types: advpred,
44
top subj xcomp obl det pobj adjadjunct subadjunct comp subj ppred pobj num
Caithfidh t?u brath ar na himreoir?? ?aiti?ula nuair at?a t?u i Roinn 1
Have-to-FUT you rely on the players local when REL-be-PRES you in Division 1
?You have to rely on the local players when you are in Division 1?
Figure 3: LFG-style subordinate clause analysis (with original Irish Dependency labels)
top subj xcomp obl det pobj adjadjunct subadjunct comp subj ppred pobj num
Caithfidh t?u brath ar na himreoir?? ?aiti?ula nuair at?a t?u i Roinn 1
Have-to-FUT you rely on the players local when REL-be-PRES you in Division 1
?You have to rely on the local players when you are in Division 1?
Figure 4: Stanford-style subordinate clause analysis (with original Irish Dependency labels)
ppred, subj q, obj q, advadjunct q, obl, obl2. In producing the universal version of the tree-
bank, these Irish-specific dependency types are mapped to less informative universal ones (see Table 2).
Conversely, we found that the universal scheme makes distinctions that the Irish scheme does not. Some
of these dependency types are not needed for Irish. For example, there is no indirect object iobj in Irish,
nor is there a passive construction that would require nsubjpass, csubjpass or auxpass. Also, in
the Irish Dependency Treebank, the copula is usually the root (top) or the head of a subordinate clause
(e.g. comp) which renders the universal type cop redundant. Others that are not used are adp, expl,
infmod, mwe, neg, partmod. However, we did identify some dependency relationships in the univer-
sal scheme that we introduce to the universal Irish Dependency Treebank (adpcomp, adposition,
advcl, num, parataxis). These are explained below.
comp? adpcomp, advcl, parataxis, ccomp The following new mappings were previously subsumed
by the Irish dependency label comp (complement clause). The mapping for comp has thus been split
between adpcomp, advcl, parataxis and ccomp.
? adpcomp is a clausal complement of an adposition. An example from the English data is ?some
understanding of what the company?s long-term horizon should begin to look like?, where ?begin?,
as the head of the clause, is a dependent of the preposition ?of?. An example of how we use this
label in Irish is: an l??ne l?antosach is m?o cl?u a th?ainig as Ciarra?? ?o bh?? aimsir Sheehy ann ?the most
renowned forward line to come out of Kerry since Sheehy?s time? (lit. ?from it was Sheehy?s time?).
The verb bh?? ?was?, head of the dependent clause, is an adcomp dependent of the preposition ?o.
? advcl is used to identify adverbial clause modifiers. In the English data, they are often introduced
by subordinating conjunctions such as ?when?, ?because?, ?although?, ?after?, ?however?, etc. An
example is ?However, because the guaranteed circulation base is being lowered, ad rates will be
higher?. Here, ?lowered? is a advcl dependent of ?will?. An example of usage is: T?a truailli?u m?or
san ?ait mar nach bhfuil c?oras s?earachais ann ?There is a lot of pollution in the area because there
is no sewerage system?, where bhfuil ?is? is an advcl dependent of T?a ?is?.
45
? parataxis labels clausal structures that are separated from the previous clause with punctuation
such as ? ... : () ; and so on. Examples in Irish Is l?eir go bhfuil ag ?eir?? le feachtas an IDA ?
meastar gur in
?
Eirinn a lonna??tear timpeall 30% de na hionaid ?It is clear that the IDA campaign is
succeeding ? it is believed that 30% of the centres are based in Ireland?. Here, meastar ?is believed?
is a parataxis dependent of Is ?is?.
? ccomp covers all other types of clausal complements. For example, in English, ?Mr. Amos says the
Show-Crier team will probably do two live interviews a day?. The head of the complement clause
here is ?do?, which is a comp dependent of the matrix verb ?says?. A similar Irish example is: D?uirt
siad nach bhfeiceann siad an cine?al seo chomh minic ?They said that they don?t see this type as
often?. Here, bhfeiceann ?see? is the head of the complement clause, which is a comp dependent of
the verb D?uirt ?Said?.
quant? num, advmod The Irish Dependency Scheme uses one dependency label (quant) to cover
all types of numerals and quantifiers. We now use the universal scheme to differentiate between quanti-
fiers such as m?or?an ?many? and numerals such as fiche ?twenty?.
nadjunct? nmod, compmod The Irish dependency label nadjunct accounts for all nominal mod-
ifiers. However, in order to map to the universal scheme, we discriminate two kinds: (i) nouns that mod-
ify nouns (usually genitive case in Irish) are mapped to compmod (e.g. plean marga??ochta ?marketing
plan?) and (ii) nouns that modify clauses are mapped to nmod (e.g. bliain ?o shin ?a year ago?).
4 Parsing Experiments
We now describe how we extend the direct transfer experiments described in McDonald et al. (2013)
to Irish. In Section 4.1, we describe the datasets used in our experiments and explain the experimental
design. In Section 4.2, we present the results, which we then discuss in Section 4.3.
4.1 Data and Experimental Setup
We present the datasets used in our experiments and explain how they are used. Irish is the target
language for all our parsing experiments.
Universal Irish Dependency Treebank This is the universal version of the Irish Dependency Treebank
which contains 1020 gold-standard trees, which have been mapped to the Universal POS tagset and
Universal Dependency Annotation Scheme, as described in Section 3. In order to establish a monolingual
baseline against which to compare our cross-lingual results, we perform a five-fold cross-validation by
dividing the full data set into five non-overlapping training/test sets. We also test our cross-lingual models
on an delexicalised version of this treebank.
Transfer source training data For our direct transfer cross-lingual parsing experiments, we use 10 of
the standard version harmonised training data sets
4
made available by McDonald et al. (2013): Brazilian
Portuguese (PT-BR), English (EN), French (FR), German (DE), Indonesian (ID), Italian (IT), Japanese
(JA), Korean (KO), Spanish (ES) and Swedish (SV). For the purposes of uniformity, we select the first
4447 trees from each treebank ? to match the number of trees in the smallest data set (Swedish). We
delexicalise all treebanks and use the universal POS tags as both the coarse- and fine-grained values.
5
We train a parser on all 10 source data sets outlined and use each induced parsing model to parse and test
on a delexicalised version of the Universal Irish Dependency Treebank.
Largest transfer source training data - Universal English Dependency Treebank English has the
largest source training data set (sections 2-21 of the Wall Street Journal data in the Penn Treebank (Mar-
cus et al., 1993) contains 39, 832 trees). As with the smaller transfer datasets, we delexicalise this dataset
and use the universal POS tag values only. We experiment with this larger training set in order to establish
whether more training data helps in a cross-lingual setting.
4
Version 2 data sets downloaded from https://code.google.com/p/uni-dep-tb/
5
Note that the downloaded treebanks had some fine-grained POS tags that were not used across all languages: e.g. VERB-
VPRT (Spanish), CD (English).
46
Parser and Evaluation Metrics We use a transition-based dependency parsing system, MaltParser
(Nivre et al., 2006) for all of our experiments. All our models are trained using the stacklazy algorithm,
which can handle the non-projective trees present in the Irish data. In each case we report Labelled
Attachment Score (LAS) and Unlabelled Attachment Score (UAS).
6
4.2 Results
All cross-lingual results are presented in Table 3. Note that when we train and test on Irish (our mono-
lingual baseline), we achieve an average accuracy of 78.54% (UAS) and 71.59% (LAS) over the five
cross-validation runs. The cross-lingual results are substantially lower than this baseline. The LAS
results range from 0.84 (JA) to 43.88 (ID) and the UAS from 16.74 (JA) to 61.69 (ID).
SingleT MultiT LargestT
Training EN FR DE ID IT JA KO PT-BR ES SV All EN
UAS 51.72 56.84 49.21 61.69 50.98 16.74 18.02 57.31 57.00 49.95 57.69 51.59
LAS 35.03 37.91 33.04 43.88 37.98 0.84 9.35 42.13 41.94 34.02 41.38 33.97
Experiment SingleT-30 MultiT-30 LargestT-30
Training EN FR DE ID IT JA KO PT-BR ES SV All EN
Avg sent len 23 24 16 21 21 9 11 24 26 14 19 23
UAS 55.97 60.98 53.42 64.86 54.47 16.88 19.27 60.47 60.53 54.40 61.40 55.54
LAS 38.42 41.44 36.24 46.45 40.56 1.19 10.08 45.04 45.23 37.76 44.63 37.08
Table 3: Multi-lingual transfer parsing results
A closer look at the single-source transfer parsing evaluation results (SingleT) shows that some lan-
guage sources are particularly strong for parsing accuracy of certain labels. For example, ROOT (for
Indonesian), adpobj (for French) and amod (for Spanish). In response to these varied results, we ex-
plore the possibility of combining the strengths of all the source languages (multi-source direct transfer
(MultiT) ? also implemented by McDonald et al. (2011)). A parser is trained on a concatenation of
all the delexicalised source data described in Section 4.1 and tested on the full delexicalised Universal
Irish Dependency Treebank. Combining all source data produces parsing results of 57.69% (UAS) and
41.38% (LAS), which is outperformed by the best individual source language model.
Parsing with the large English training set (LargestT) yielded results of 51.59 (UAS) and 33.97 (LAS)
compared to a UAS/LAS of 51.72/35.05 for the smaller English training set. We investigated more
closely why the larger training set did not improve performance by incrementally adding training sen-
tences to the smaller set ? none of these increments reveal any higher scores, suggesting that English is
not a suitable source training language for Irish.
It is well known that sentence length has a negative effect on parsing accuracy. As noted in earlier
experiments (Lynn et al., 2012b), the Irish Dependency Treebank contains some very long difficult-to-
parse sentences (some legal text exceeds 300 tokens in length). The average sentence length is 27 tokens.
By placing a 30-token limit on the Universal Irish Dependency Treebank we are left with 778 sentences,
with an average sentence length of 14. We use this new 30-token-limit version of the Irish Dependency
Treebank data to test our parsing models. The results are shown in the lower half of Table 3. Not
surprisingly, the results rise substantially for all models.
4.3 Discussion
McDonald et al. (2013)?s single-source transfer parsing results show that languages within the same
language groups make good source-target pairs. They also show reasonable accuracy of source-target
pairing across language groups. For instance, the baseline when parsing French is 81.44 (UAS) and 73.37
(LAS), while the transfer results obtained using an English treebank are 70.14 (UAS) and 58.20(LAS).
Our baseline parser for Irish yields results of 78.54 (UAS) and 71.59 (LAS), while Indonesian-Irish
transfer results are 61.69 (UAS) and 43.88 (LAS).
The lowest scoring source language is Japanese. This parsing model?s output shows less than 3%
accuracy when identifying the ROOT label. This suggests the effect that the divergent word orders have
6
All scores are micro-averaged.
47
on this type of cross-lingual parsing ? VSO (Irish) vs SOV (Japanese). Another factor that is likely to be
playing a role is the size of the Japanese sentences. The average sentence length in the Japanese training
data is only 9 words, which means that this dataset is comparatively smaller than the others. It is also
worth noting that the universal Japanese treebank uses only 15 of the 41 universal labels (the universal
Irish treebank uses 30 of these labels).
As our best performing model (Indonesian) is an Austronesian language, we investigate why this
language does better when compared to Indo-European languages. We compare the results obtained by
the Indonesian parser with those of the English parser (SingleT). Firstly, we note that the Indonesian
parser captures nominal modification much better than English, resulting in an increased precision-recall
score of 60/67 on compmod. This highlights that the similarities in noun-noun modification between
Irish and Indonesian helps cross-lingual parsing. In both languages the modifying noun directly follows
the head noun, e.g. ?the statue of the hero? translates in Irish as dealbh an laoich (lit. statue the hero);
in Indonesian as patung palawan (lit. statue hero). Secondly, our analysis shows that the English parser
does not capture long-distance dependencies as well as the Indonesian parser. For example, we have
observed an increased difference in precision-recall of 44%-44% on mark, 12%-17.88% on cc and
4%-23.17% on rcmod when training on Indonesian. Similar differences have also been observed when
we compare with the French and English (LargestT) parsers. The Irish language allows for the use
of multiple conjoined structures within a sentence and it appears that long-distance dependencies can
affect cross-lingual parsing. Indeed, excluding very long sentences from the test set reveals substantial
increases in precision-recall scores for labels such as advcl, cc, conj and ccomp ? all of which are
labels associated with long-distance dependencies.
With this study, we had hoped that we would be able to identify a way to bootstrap the development
of the Irish Dependency Treebank and parser through the use of delexicalised treebanks annotated with
the Universal Annotation Scheme. While the current treebank data might capture certain linguistic phe-
nomena well, we expected that some cross-linguistic regularities could be taken advantage of. Although
the best cross-lingual model failed to outperform the monolingual model, perhaps it might be possible to
combine the strengths of the Indonesian and Irish treebanks? We performed 5-fold cross-validation on
the combined Indonesian and Irish data sets. The results did not improve over the Irish model. We then
analysed the extent of their complementarity by counting the number of sentences where the Indonesian
model outperformed the Irish model. This happened in only 20 cases, suggesting that there is no benefit
in using the Indonesian data over the Irish data nor in combining them at the sentence-level.
5 Conclusion and Future Work
In this paper, we have reported an implementation of cross-lingual direct transfer parsing of the Irish
language. We have also presented and explained our mapping of the Irish Dependency Treebank to the
Universal POS tagset and Universal Annotation Scheme. Our parsing results show that an Austronesian
language surpasses Indo-European languages as source data for cross-lingual Irish parsing.
In extending this research, there are many interesting avenues which could be explored including
the use of Irish as a source language for another Celtic language and experimenting with the projected
transfer approach of McDonald et al. (2011).
Acknowledgements
This research is supported by the Science Foundation Ireland (Grant 12/CE/I2267) as part of the CNGL
(www.cngl.ie) at Dublin City University. We thank the three anonymous reviewers for their helpful
feedback. We also thank Elaine U?? Dhonnchadha (Trinity College Dublin) and Brian
?
O Raghallaigh
(Fiontar, Dublin City University) for their linguistic advice.
References
Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of COL-
ING?10.
48
Joan Bresnan. 2001. Lexical Functional Syntax. Oxford: Blackwell.
?
Ozlem C?etino?glu, Jennifer Foster, Joakim Nivre, Deirdre Hogan, Aoife Cahill, and Josef van Genabith. 2010. LFG
without C-structures. In Proceedings of the 9th International Workshop on Treebanks and Linguistic Theories.
Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representa-
tion. In Workshop on Crossframework and Cross-domain Parser Evaluation (COLING2008).
Katri Haverinen, Jenna Nyblom, Timo Viljanen, Veronika Laippala, Samuel Kohonen, Anna Missil?a, Stina Ojala,
Tapio Salakoski, and Filip Ginter. 2013. Building the essential resources for Finnish: the Turku dependency
treebank. Language Resources and Evaluation, pages 1?39.
John Judge, Ailbhe N?? Chasaide, Rose N?? Dhubhda, Kevin P. Scannell, and Elaine U?? Dhonnchadha. 2012. The
Irish Language in the Digital Age. Springer Publishing Company, Incorporated.
Matthias Kromann. 2003. The Danish Dependency Treebank and the DTAG Treebank Tool. In Proceedings of
the Second Workshop on Treebanks and Linguistic Theories (TLT2003).
Teresa Lynn,
?
Ozlem C?etino?glu, Jennifer Foster, Elaine U?? Dhonnchadha, Mark Dras, and Josef van Genabith.
2012a. Irish treebanking and parsing: A preliminary evaluation. In Proceedings of the Eight International
Conference on Language Resources and Evaluation (LREC?12), pages 1939?1946.
Teresa Lynn, Jennifer Foster, Mark Dras, and Elaine U?? Dhonnchadha. 2012b. Active learning and the Irish
treebank. In Proceedings of the Australasian Language Technology Workshop (ALTA), pages 23?32.
Teresa Lynn, Jennifer Foster, and Mark Dras. 2013. Working with a small dataset ? semi-supervised depen-
dency parsing for Irish. In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich
Languages, pages 1?11, Seattle, Washington, USA, October. Association for Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of
english: The Penn treebank. COMPUTATIONAL LINGUISTICS, 19(2):313?330.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-source transfer of delexicalized dependency parsers.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?11, pages
62?72, Stroudsburg, PA, USA. Association for Computational Linguistics.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev,
Keith Hall, Slav Petrov, Hao Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria Bertomeu, and Castell?o Jungmee
Lee. 2013. Universal dependency annotation for multilingual parsing. In Proceedings of ACL ?13.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for depen-
dency parsing. In Proceedings of the Fifth International Conference on Language Resources and Evaluation
(LREC2006).
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of the
Eight International Conference on Language Resources and Evaluation (LREC?12).
Nancy Stenson. 1981. Studies in Irish Syntax. T?ubingen: Gunter Narr Verlag.
Elaine U?? Dhonnchadha and Josef van Genabith. 2006. A part-of-speech tagger for Irish using finite-state morphol-
ogy and constraint grammar disambiguation. In Proceedings of the 5th International Conference on Language
Resources and Evaluation (LREC 2006).
Elaine U?? Dhonnchadha, Caoilfhionn Nic Ph?aid??n, and Josef van Genabith. 2003. Design, implementation and
evaluation of an inflectional morphology finite state transducer for Irish. Machine Translation, 18:173?193.
Elaine U?? Dhonnchadha. 2009. Part-of-Speech Tagging and Partial Parsing for Irish using Finite-State Transduc-
ers and Constraint Grammar. Ph.D. thesis, Dublin City University.
49
