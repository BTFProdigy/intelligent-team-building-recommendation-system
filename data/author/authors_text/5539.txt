Efficient Confirmation Strategy for Large-scale Text Retrieval
Systems with Spoken Dialogue Interface
Kazunori Komatani Teruhisa Misu Tatsuya Kawahara Hiroshi G. Okuno
Graduate School of Informatics
Kyoto University
Yoshida-Hommachi, Sakyo, Kyoto 606-8501, Japan
{komatani,kawahara,okuno}@i.kyoto-u.ac.jp
Abstract
Adequate confirmation for keywords is in-
dispensable in spoken dialogue systems
to eliminate misunderstandings caused by
speech recognition errors. Spoken lan-
guage also inherently includes out-of-
domain phrases and redundant expressions
such as disfluency, which do not contribute
to task achievement. It is necessary to
appropriately make confirmation for im-
portant portions. However, a set of key-
words necessary to achieve the tasks can-
not be predefined in retrieval for a large-
scale knowledge base unlike conventional
database query tasks. In this paper, we
describe two statistical measures for iden-
tifying portions to be confirmed. A rele-
vance score represents the matching degree
with the target knowledge base. A sig-
nificance score detects portions that conse-
quently affect the retrieval results. These
measures are defined based on information
that is automatically derived from the tar-
get knowledge base. An experimental eval-
uation shows that our method improved the
success rate of retrieval by generating con-
firmation more efficiently than using a con-
ventional confidence measure.
1 Introduction
Information retrieval systems with spoken lan-
guage have been studied (Harabagiu et al, 2002;
Hori et al, 2003). They require both automatic
speech recognition (ASR) and information re-
trieval (IR) technologies. As a straight mani-
festation to create these systems, we can think
of using ASR results as an input for IR systems
that retrieve a text knowledge base (KB). How-
ever, two problems occur in the characteristics
of speech.
1. Speech recognition errors
2. Redundancy included in spoken language
expressions
One is an ASR error, which is basically in-
evitable in speech communications. Therefore,
an adequate confirmation is indispensable in
spoken dialogue systems to eliminate the mis-
understandings caused by ASR errors.
If keywords to be confirmed are defined, the
system can confirm them using confidence mea-
sures (Komatani and Kawahara, 2000; Hazen
et al, 2000) to manage the errors. In con-
ventional tasks for spoken dialogue systems in
which their target of retrieval was well-defined,
such as the relational database, keywords that
are important to achieve the tasks correspond
to items in the relational database. Most spo-
ken dialogue systems that have been developed,
such as airline information systems (Levin et al,
2000; Potamianos et al, 2000; San-Segundo et
al., 2000) and train information systems (Allen
et al, 1996; Sturm et al, 1999; Lamel et al,
1999), are categorized here. However, it is not
feasible to define such keywords in retrieval for
operation manuals (Komatani et al, 2002) or
WWW pages, where the target of retrieval is
not organized and is written as natural language
text.
Another problem is that a user?s utterances
may include redundant expressions or out-of-
domain phrases. A speech interface has been
said to have the advantage of ease of input. This
means that redundant expressions, such as dis-
fluency and irrelevant phrases, are easily input.
These do not directly contribute to task achieve-
ment and might even be harmful. ASR results
that may include such redundant portions are
not adequate for an input of IR systems.
A novel method is described in this paper
that automatically detects necessary portions
for task achievement from the ASR results of
a user?s utterances; that is, the system deter-
mines if each part of the ASR results is neces-
sary for the retrieval. We introduce two mea-
sures for each portion of the results. One is a
relevance score (RS) with the target document
 
[HOWTO] Use Speech Recognition in
Windows XP
The information in this article applies to:
? Microsoft Windows XP Professional
? Microsoft Windows XP Home Edition
Summary: This article describes how to use
speech recognition in Windows XP. If you
installed speech recognition with Microsoft
Office XP, or if you purchased a new com-
puter that has Office XP installed, you can
use speech recognition in all Office pro-
grams as well as other programs for which
it is enabled.
Detail information: Speech recognition en-
ables the operating system to convert spo-
ken words to written text. An internal
driver, called a speech recognition engine,
recognizes words and converts them to
text. The speech recognition engine ...
 
Figure 1: Example of one article in database
set. The score is computed with a document
language model and is used for making confir-
mation prior to the retrieval. The other is a sig-
nificance score (SS) in the document matching.
It is computed after the retrieval using N-best
results and is used for prompting the user for
post-selection if necessary. Information neces-
sary to define these two measures, such as a doc-
ument language model and retrieval results for
N-best candidates of the ASR, can be automat-
ically derived from the target knowledge base.
Therefore, the system can detect the portions
necessary for the retrieval and make the confir-
mation efficiently without defining the keywords
manually.
2 Text Retrieval System for
Large-scale Knowledge Base
Our task involves text retrieval for a large-
scale knowledge base. As the target domain,
we adopted a software support knowledge base
provided by the Microsoft Corporation. The
knowledge base consists of the following three
components: glossary, frequently asked ques-
tions (FAQ), and a database of support articles.
Figure 1 is an example of the database. The
knowledge base is very large-scale, as shown in
Table 1.
The Dialog Navigator (Kiyota et al, 2002)
was developed in the University of Tokyo as a
Table 1: Document set (Knowledge base)
Text collection # of texts # of characters
Glossary 4,707 700,000
FAQ 11,306 6,000,000
Support articles 23,323 22,000,000
text retrieval system for this knowledge base.
The system accepts a typed-text input as ques-
tions from users and outputs a result of the re-
trieval. The system interprets input sentences
taking a syntactic dependency and synonymous
expression into consideration for matching it
with the knowledge base. The system can also
navigate for the user when he/she makes vague
questions based on scenarios (dialog card) that
were described manually beforehand. Hundreds
of the dialog cards have been prepared to ask
questions back to the users. If a user question
matches its input part, the system generates a
question based on its description.
We adopted the Dialog Navigator as a back-
end system and constructed a text retrieval sys-
tem with a spoken dialogue interface. We then
investigated a confirmation strategy to interpret
the user?s utterances robustly by taking into ac-
count the problems that are characteristic of
spoken language, as previously described.
3 Confirmation Strategy using
Relevance Score and Significance
Score
Making confirmations for every portion that has
the possibility to be an ASR error is tedious.
This is because every erroneous portion does
not necessarily affect the retrieval results. We
therefore take the influence of recognition er-
rors for retrieval into consideration, and control
generation of confirmation.
We make use of N-best results of the ASR
for the query and test if a significant difference
is caused among N-best sets of retrieved can-
didates. If there actually is, we then make a
confirmation on the portion that makes the dif-
ference. This is regarded as a posterior confir-
mation. On the other hand, if a critical error
occurs in the ASR result, such as those in the
product name in software support, the follow-
ing retrieval would make no sense. Therefore,
we also introduce a confirmation prior to the
retrieval for critical words.
The system flow including the confirmation is
summarized below.
1. Recognize a user?s utterance.
Speech input
System User
ASR
(N-best candidates)
Calculation of
relevance score
Language model
for ASR
Language model
trained with KB
Confirmation using
relevance score
Critical
words
Confirmation for
influential words
Reply or rephrase
Matching with KB with
weighting by relevance score
retrieval
result
retrieval
result
retrieval
result
Confirmation using significance score
Final result
Display the result
Confirmation for difference
between candidates
Reply or rephrase
Dialog
Navigator
(text retrieval)
Target text
KB TFIDF
Figure 2: System flow
2. Calculate a relevance score for each phrase
of ASR results.
3. Make a confirmation for critical words with
a low relevance score.
4. Retrieve the knowledge base using the Dia-
log Navigator for N-best candidates of the
ASR.
5. Calculate significance scores and generate
a confirmation based on them.
6. Show the retrieval results to the user.
This flow is also shown in Figure 2 and ex-
plained in the following subsections in detail.
3.1 Definition of Relevance Score
We use test-set perplexity for each portion of
the ASR results as one of the criteria in deter-
mining whether the portion is influential or not
for the retrieval. The language model to cal-
culate the perplexity was trained only with the
target knowledge base. It is different from that
used in the ASR.
The perplexity is defined as an exponential of
entropy per word, and it represents the average
number of the next words when we observe a
word sequence. The perplexity can be denoted
as the following equation because we assume an
ergodicity on language and use a trigram as a
language model.
log PP = ? 1
n
?
k
log P (wk|wk?1, wk?2)
This perplexity PP represents the degree that
a given word sequence, w1w2 ? ? ?wn, matches
the knowledge base with which the language
model P (wn|wn?1, wn?2) was trained. If the
perplexity is small, it indicates the sequence ap-
pears frequently in the knowledge base. On the
contrary, the perplexity for a portion including
the ASR errors increases because it is contex-
tually less frequent. The perplexity for out-of-
domain phrases similarly increases because they
scarcely appear in the knowledge base. It en-
ables us to detect a portion that is not influen-
tial for retrieval or those portions that include
ASR errors. Here, a phrase, called bunsetsu1
in Japanese, is adopted as a portion for which
the perplexity is calculated. We use a syntac-
tic parser KNP (Kurohashi and Nagao, 1994)
to divide the ASR results into the phrases2.
1Bunsetsu is a commonly used linguistic unit in
Japanese, which consists of one or more content words
and zero or more functional words that follow.
2As the parser was designed for written language, the
division often fails for portions including ASR errors.
The division error, however, does not affect the whole
system?s performance because the perplexity for the er-
roneous portions increases, indicating they are irrelevant.
 
User utterance:
?Atarashiku katta XP no pasokon de fax kinou
wo tsukau niha doushitara iidesu ka??
(Please tell me how to use the facsimile func-
tion in the personal computer with Windows
XP that I recently bought.)
Speech recognition result:
?Atarashiku katta XP no pasokon de fax kinou
wo tsukau ni sono e ikou??
[The underlined part was incorrectly recognized
here.]
Division into phrases (bunsetsu):
?Atarashiku / katta / XP no / pasokon de / fax
kinou wo / tsukau ni / sono / e / ikou??
Calculation of perplexity:
phrases (their context) PP RS
(<S>) Atarashiku (katta) 499.57 0.86
(atarashiku) katta (XP) 2079.83 0.47
(katta) XP no (pasokon) 105.64 0.99
(no) pasokon de (FAX) 185.92 0.95
(de) FAX kinou wo (tsukau) 236.23 0.89
(wo) tsukau ni (sono) 98.40 0.99
(ni) sono (e) 1378.72 0.62
(sono) e (ikou) 144.58 0.96
(e) ikou (</S>) 27150.00 0.00
<S>, </S> denote the beginning and end of a sen-
tence.
 
Figure 3: Example of calculating perplexity
(PP ) and relevance score (RS)
We then calculate the perplexity for the
phrases (bunsetsu) to which the preceding and
following words are attached. We then define
the relevance score by applying a sigmoid-like
transform to the perplexity using the following
equation. Thus, the score ranges between 0 and
1 by the transform and can be used as a weight
for each bunsetsu.
RS =
1
1 + exp(? ? (log PP ? ?))
Here, ? and ? are constants and are empiri-
cally set to 2.0 and 11.0. An example of calcu-
lating the relevance score is shown in Figure 3.
In this sample, a portion, ?Atarashiku katta (=
that I bought recently)?, that appeared in the
beginning of the utterance does not contribute
to any retrieval. A portion at the end of the sen-
tence was incorrectly recognized because it may
have been pronounced weakly. The perplexity
for these portions increases as a result, and the
relevance score correspondingly decreases.
3.2 Confirmation for Critical Words
using Relevance Score
Critical words should be confirmed before the
retrieval. This is because a retrieval result will
be severely damaged if the portions are not cor-
rectly recognized. We define a set of words that
are potentially critical using tf?idf values, which
are often used in information retrieval. They
can be derived from the target knowledge base
automatically. We regard a word with the max-
imum tf?idf values in each document as being
its representative, and the words that are rep-
resentative in more documents are regarded as
being more important. When the amount of
documents represented by the more important
words exceeds 10% out of the whole number of
documents, we define a set of the words as being
critical. As a result, 35 words were selected as
potentially critical ones in the knowledge base,
such as ?set up?, ?printer?, and ?(Microsoft) Of-
fice?.
We use the relevance score to determine
whether we should make a confirmation for the
critical words. If a critical word is contained
in a phrase whose relevance score is lower than
threshold ?, the system makes a confirmation.
We set threshold ? through the preliminary ex-
periment. The confirmation is done by present-
ing the recognition results to the user. Users can
either confirm or discard or correct the phrase
before passing it to the following matching mod-
ule.
3.3 Weighted Matching using
Relevance Score
A phrase that has a low relevance score is likely
to be an ASR error or a portion that does not
contribute to retrieval. We therefore use the rel-
evance score RS as a weight for phrases during
the matching with the knowledge base. This re-
lieves damage to the retrieval by ASR errors or
redundant expressions.
3.4 Significance Score using Retrieval
Results
The significance score is defined by using plural
retrieval results corresponding to N-best candi-
dates of the ASR. Ambiguous portions during
the ASR appear as the differences between the
N-best candidates. The score represents the de-
gree to which the portions are influential.
The significance score is calculated for por-
tions that are different among N-best candi-
dates. We define the significance score SS(n,m)
as the difference between the retrieval results of
n-th and m-th candidates. The value is defined
by the equation,
SS(n,m) = 1 ? |res(n) ? res(m)|
2
|res(n)||res(m)| .
Here, res(n) denotes a set of retrieval results
for the n-th candidate, and |res(n)| denotes the
number of elements in the set. That is, the sig-
nificance score decreases if the retrieval results
have a large common part.
Figure 4 has an example of calculating the
significance score. In this sample, the portions
of ?suuzi (numerals)? and ?suushiki (numeral
expressions)? differ between the first and sec-
ond candidates of the ASR. As the retrieval re-
sults for each candidate, 14 and 15 items are
obtained, respectively. The number of com-
mon items between the two retrieval results is
8. Then, the significance score for the portion
is 0.70 by the above equation.
3.5 Confirmation using Significance
Score
The confirmation is also made for the portions
detected by the significance score. If the score
is higher than a threshold, the system makes
the confirmation by presenting the difference to
users3. Here, we set the number of N-best can-
didates of the ASR to 3, and the threshold for
the score is set to 0.5.
In the confirmation phrase, if a user selects
from the list, the system displays the corre-
sponding retrieval results. If the score is lower
than the threshold, the system does not make
the confirmation and presents retrieval results
of the first candidate of the ASR. If a user
judges all candidates as inappropriate, the sys-
tem rejects the current candidates and prompts
him/her to utter the query again.
4 Experimental Evaluation
We implemented and evaluated our method as
a front-end of Dialog Navigator. The front-end
works on a Web browser, Internet Explorer 6.0.
Julius (Lee et al, 2001) for SAPI4 was used as a
speech recognizer on PCs. The system presents
a confirmation to users on the display. He or she
replies to the confirmation by selecting choices
with the mouse.
3Confirmation will be generated practically if one of
the significance scores between the first candidate and
others exceeds the threshold.
4http://julius.sourceforge.jp/sapi/
 
[#1 candidate of ASR]
?WORD2002 de suuzi wo nyuryoku suru
houhou wo oshiete kudasai.? (Please tell me
the way to input numerals in WORD 2002.)
Retrieval results (# of the results was 14.)
1. Input the present date and time in Word
2. WORD: Add a space between Japanese and
alphanumeric characters
3. WORD: Check the form of inputted char-
acters
4. WORD: Input a handwritten signature
5. WORD: Put watermark characters into the
background of a character
6. ...
[#2 candidate of ASR]
?WORD2002 de suushiki wo nyuryoku suru
houhou wo oshiete kudasai.? (Please tell
me the way to input numerical expressions in
WORD 2002.)
Retrieval results (# of the results was 15.)
1. Insert numerical expressions in Word
2. Input the present date and time in Word
3. Input numerical expressions in Spreadsheet
4. Input numerical expressions in PowerPoint
5. Input numerical expressions in Excel
6. ...
Significance score
SS(1, 2) = 1 ? 8214?15 = 0.70
(# of common items in the retrieval results
was 8.)
 
Figure 4: Example of calculating significance
score
We collected the test data by 30 subjects who
had not used our system. Each subject was re-
quested to retrieve support information for 14
tasks, which consisted of 11 prepared scenarios
(query sentences are not given) and 3 sponta-
neous queries. Subjects were allowed to utter
the sentence again up to 3 times per task if a rel-
evant retrieval result was not obtained. We ob-
tained 651 utterances for 420 tasks in total. The
average word accuracy of the ASR was 76.8%.
4.1 Evaluation of Success Rate of
Retrieval
We calculated the success rates of retrieval for
the collected speech data. We regarded the re-
trieval as having succeeded when the retrieval
results contained an answer for the user?s initial
question. We set three experimental conditions:
Table 2: Comparison of success rate of retrieval with method using only ASR results
# utterances Transcription ASR results Our method
651 520 421 457
(79.9%) (64.7%) (70.2%)
1. Transcription: A correct transcription of
user utterances, which was made manually,
was used as an input to the Dialog Naviga-
tor. This condition corresponds to a case of
100% ASR accuracy, indicating an utmost
performance obtained by improvements in
the ASR and our dialogue strategy.
2. ASR results: The first candidate of the
ASR was used as an input (baseline).
3. Our method: The N-best candidates of the
ASR were used as an input, and confirma-
tion was generated based on our method
using both the relevance and significance
scores. It was assumed that the users
responded appropriately to the generated
confirmation.
Table 2 lists the success rate. The rate when
the transcription was used as the input was
79.9%. The remaining errors included those
caused by irrelevant user utterances and those
in the text retrieval system. Our method at-
tained a better success rate than the condition
where the first candidate of the ASR was used.
Improvement of 36 cases (5.5%) was obtained by
our method, including 30 by the confirmations
and 14 by weighting during the matching using
the relevance score, though the retrieval failed
eight times as side effects of the weighting.
We further investigated the results shown in
Table 2. Table 3 lists the relations between the
success rate of the retrieval and the accuracy
of the ASR per utterance. The improvement
rate out of the number of utterances was rather
high between 40% and 60%. This means that
our method was effective not only for utterances
with high ASR accuracy but also for those with
around 50% accuracy. That is, an appropriate
confirmation was generated even for utterances
whose ASR accuracy was not very high.
4.2 Evaluation of Confirmation
Efficiency
We also evaluated our method from the number
of generated confirmations. Our method gener-
ated 221 confirmations. This means that con-
firmations were generated once every three ut-
terances on the average. The 221 confirmations
consisted of 66 prior to the retrieval using the
relevance score and 155 posterior to the retrieval
using the significance score.
We compared our method with a conventional
one, which used a confidence measure (CM)
based on N-best candidates of the ASR (Ko-
matani and Kawahara, 2000)5. In this method,
the system generated confirmation only for con-
tent words with a confidence measure lower
than ?1. The thresholds to generate confirma-
tion (?1) were set to 0.4, 0.6, and 0.8. If a con-
tent word that was confirmed was rejected by
the user, the retrieval was executed after remov-
ing a phrase that included it.
The number of confirmations and retrieval
successes are shown in Table 4. Our method
achieved a higher success rate with a less num-
ber of confirmations (less than half) compared
with the case of ?1 = 0.8 in the conventional
method. Thus, the generated confirmations
based on the two scores were more efficient.
The confidence measure used in the conven-
tional method only reflects the acoustic and
linguistic likelihood of the ASR results. Our
method, however, reflects the domain knowl-
edge because the two scores are derived by ei-
ther a language model trained with the target
knowledge base or by retrieval results for the
N-best candidates. The domain knowledge can
be introduced without any manual deliberation.
The experimental results show that the scores
are appropriate to determine whether a confir-
mation should be generated or not.
5 Conclusion
We described an appropriate confirmation strat-
egy for large-scale text retrieval systems with a
spoken dialogue interface. We introduced two
measures, relevance score and significance score,
for ASR results. The measures are useful to con-
trol confirmation efficiently for portions includ-
ing either ASR errors or redundant expressions.
The portions to be confirmed are determined
5We used a word-level CM only because defining se-
mantic categories for content words is required to cal-
culate the concept-level CM. Because the semantic cate-
gory corresponded to items in a relational database, we
cannot use the concept-level CM in this task.
Table 3: Success rate of retrieval per ASR accuracy
ASR accuracy (%) # utterances ASR results Our method # improvement
?40 37 9 11 2 ( 5.4%)
?60 73 33 42 9 (12.3%)
?80 194 116 129 13 ( 6.7%)
?100 347 263 275 12 ( 3.5%)
Total 651 421 457 36 ( 5.5%)
Table 4: Comparison with method using confidence measure (CM)
Our method CM (?1 = 0.4) CM (?1 = 0.6) CM (?1 = 0.8)
# confirmation 221 77 254 484
# success (success rate) 457 (70.2%) 427 (65.6%) 435 (66.8%) 445 (68.4%)
using information that is automatically derived
from the target knowledge base, such as a statis-
tical language model, tf?idf values, and retrieval
results. An experimental evaluation shows that
our method can efficiently generate confirma-
tions for better task achievement compared with
that using a conventional confidence measure of
the ASR. Our method is not dependent on the
software support task, and expected to be ap-
plicable to general text retrieval tasks.
6 Acknowledgments
The authors are grateful to Prof. Kurohashi and
Mr. Kiyota at the University of Tokyo and Ms.
Kido at Microsoft Corporation for their helpful
advice.
References
J. F. Allen, B. W. Miller, E. K. Ringger, and
T. Sikorski. 1996. A robust system for natu-
ral spoken dialogue. In Proc. of the 34th An-
nual Meeting of the ACL, pages 62?70.
S. Harabagiu, D. Moldovan, and J. Picone.
2002. Open-domain voice-activated question
answering. In Proc. COLING, pages 502?508.
T. J. Hazen, T. Burianek, J. Polifroni, and
S. Seneff. 2000. Integrating recognition con-
fidence scoring with language understanding
and dialogue modeling. In Proc. ICSLP.
C. Hori, T. Hori, H. Isozaki, E. Maeda, S. Kata-
giri, and S. Furui. 2003. Deriving disambigu-
ous queries in a spoken interactive ODQA
system. In Proc. IEEE-ICASSP.
Y. Kiyota, S. Kurohashi, and F. Kido. 2002.
?Dialog Navigator?: A question answering
system based on large text knowledge base.
In Proc. COLING, pages 460?466.
K. Komatani and T. Kawahara. 2000. Flexible
mixed-initiative dialogue management using
concept-level confidence measures of speech
recognizer output. In Proc. COLING, pages
467?473.
K. Komatani, T. Kawahara, R. Ito, and H. G.
Okuno. 2002. Efficient dialogue strategy to
find users? intended items from information
query results. In Proc. COLING, pages 481?
487.
S. Kurohashi and M. Nagao. 1994. A syntactic
analysis method of long Japanese sentences
based on the detection of conjunctive struc-
tures. Computational Linguistics, 20(4):507?
534.
L. F. Lamel, S. Rosset, J-L. S. Gauvain, and
S. K. Bennacef. 1999. The LIMSI ARISE
system for train travel information. In Proc.
IEEE-ICASSP.
A. Lee, T. Kawahara, and K. Shikano. 2001.
Julius ? an open source real-time large vo-
cabulary recognition engine. In Proc. EU-
ROSPEECH, pages 1691?1694.
E. Levin, S. Narayanan, R. Pieraccini, K. Bia-
tov, E. Bocchieri, G. Di Fabbrizio, W. Eck-
ert, S. Lee, A. Pokrovsky, M. Rahim, P. Rus-
citti, and M. Walker. 2000. The AT&T-
DARPA communicator mixed-initiative spo-
ken dialogue system. In Proc. ICSLP.
A. Potamianos, E. Ammicht, and H.-K. J. Kuo.
2000. Dialogue management in the Bell labs
communicator system. In Proc. ICSLP.
R. San-Segundo, B. Pellom, W. Ward, and
J. Pardo. 2000. Confidence measures for dia-
logue management in the CU communicator
system. In Proc. IEEE-ICASSP.
J. Sturm, E. Os, and L. Boves. 1999. Issues in
spoken dialogue systems: Experiences with
the Dutch ARISE system. In Proc. ESCA
workshop on Interactive Dialogue in Multi-
Modal Systems.
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 1003?1010, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Speech-based Information Retrieval System
with Clarification Dialogue Strategy
Teruhisa Misu Tatsuya Kawahara
School of informatics
Kyoto University
Sakyo-ku, Kyoto, Japan
misu@ar.media.kyoto-u.ac.jp
Abstract
This paper addresses a dialogue strategy
to clarify and constrain the queries for
speech-driven document retrieval systems.
In spoken dialogue interfaces, users often
make utterances before the query is com-
pletely generated in their mind; thus input
queries are often vague or fragmental. As
a result, usually many items are matched.
We propose an efficient dialogue frame-
work, where the system dynamically se-
lects an optimal question based on infor-
mation gain (IG), which represents reduc-
tion of matched items. A set of possible
questions is prepared using various knowl-
edge sources. As a bottom-up knowl-
edge source, we extract a list of words
that can take a number of objects and po-
tentially causes ambiguity, using a depen-
dency structure analysis of the document
texts. This is complemented by top-down
knowledge sources of metadata and hand-
crafted questions. An experimental evalu-
ation showed that the method significantly
improved the success rate of retrieval, and
all categories of the prepared questions
contributed to the improvement.
1 Introduction
The target of spoken dialogue systems is being ex-
tended from simple databases such as flight informa-
tion (Levin et al, 2000; Potamianos et al, 2000) to
general documents (Fujii and Itou, 2003) including
newspaper articles (Chang et al, 2002; Hori et al,
2003). In such systems, the automatic speech recog-
nition (ASR) result of the user utterance is matched
against a set of target documents using the vector
space model, and documents with high matching
scores are presented to the user.
In this kind of document retrieval systems, user
queries must include sufficient information to iden-
tify the desired documents. In conventional doc-
ument query tasks with typed-text input, such as
TREC QA Track (NIST and DARPA, 2003), queries
are (supposed to be) definite and specific. However,
this is not the case when speech input is adopted.
The speech interface makes input easier. However,
this also means that users can start utterances before
queries are thoroughly formed in their mind. There-
fore, input queries are often vague or fragmental,
and sentences may be ill-formed or ungrammatical.
Moreover, important information may be lost due to
ASR errors. In such cases, an enormous list of possi-
ble relevant documents is usually obtained because
there is very limited information that can be used
as clues for retrieval. Therefore, it is necessary to
narrow down the documents by clarifying the user?s
intention through a dialogue.
There have been several studies on the follow-up
dialogue, and most of these studies assume that the
target knowledge base has a well-defined structure.
For example, Denecke (Denecke and Waibel, 1997)
addressed a method to generate guiding questions
based on a tree structure constructed by unifying
pre-defined keywords and semantic slots. However,
these approaches are not applicable to general docu-
1003
Query utterance
System User
Automatic speech
recognition (ASR)
Confirmation
for robust
retrieval 
Confirmation
Reply
Matching
with
Knowledge
base (KB)
Knowledge
base
(KB)
Dialogue to 
narrow down
retrieved
documents
Question
Reply
Final resultpresented in this paper
Figure 1: System overview
ment sets without such structures.
In this paper, we propose a dialogue strategy to
clarify the user?s query and constrain the retrieval
for a large-scale text knowledge base, which does
not have a structure nor any semantic slots. In the
proposed scheme, the system dynamically selects an
optimal question, which can reduce the number of
matched items most efficiently. As a criterion of
efficiency of the questions, information gain (IG)
is defined. A set of possible questions is prepared
using bottom-up and top-down knowledge sources.
As a bottom-up knowledge source, we conduct de-
pendency structure analysis of the document texts,
and extract a list of words that can take a number
of objects, thus potentially causing ambiguity. This
is combined with top-down knowledge sources of
metadata and hand-crafted questions. The system
then updates the query sentence using the user?s re-
ply to the question, so as to generate a confirmation
to the user.
2 Document retrieval system for
large-scale knowledge base
2.1 System overview
We have studied a dialogue framework to overcome
the problems in speech-based document retrieval
systems. In the framework, the system can han-
dle three types of problems caused by speech input:
ASR errors, redundancy in spoken language expres-
sion, and vagueness of queries. First, the system re-
alizes robust retrieval against ASR errors and redun-
Table 1: Document set (Knowledge Base: KB)
Text collection # documents text size(byte)
glossary 4,707 1.4M
FAQ 11,306 12M
DB of support articles 23,323 44M
dancies by detecting and confirming them. Then, the
system makes questions to clarify the user?s query
and narrow down the retrieved documents.
The system flow of these processes is summarized
below and also shown in Figure 1.
1. Recognize the user?s query utterance.
2. Make confirmation for phrases which may in-
clude critical ASR errors.
3. Retrieve from knowledge base (KB).
4. Ask possible questions to the user and narrow
down the matched documents.
5. Output the retrieval results.
In this paper, we focus on the latter stage of the
proposed framework, and present a clarification dia-
logue strategy to narrow down documents.
2.2 Task and back-end retrieval system
Our task involves text retrieval from a large-scale
knowledge base. For the target domain, we adopt a
software support knowledge base (KB) provided by
Microsoft Corporation. The knowledge base con-
sists of the following three kinds: glossary, fre-
quently asked questions (FAQ), and support articles.
The specification is listed in Table 1, and there are
about 40K documents in total. An example of sup-
port article is shown in Figure 2.
Dialog Navigator (Kiyota et al, 2002) has been
developed at University of Tokyo as a retrieval sys-
tem for this KB. The system accepts a typed-text in-
put from users and outputs a result of the retrieval.
The system interprets an input sentence by taking
syntactic dependency and synonymous expression
into consideration for matching it with the KB. The
target of the matching is the summaries and detail
information in the support articles, and the titles of
the Glossary and FAQ. The retrieved result is dis-
played to the user as the list of documents like Web
1004
? ?
HOWTO:
Use Speech Recognition in Windows XP
The information in this article applies to:
? Microsoft Windows XP Professional
? Microsoft Windows XP Home Edition
Summary: This article describes how to use speech
recognition in Windows XP. If you installed speech
recognition with Microsoft Office XP, or if you pur-
chased a new computer that has Office XP installed,
you can use speech recognition in all Office pro-
grams as well as other programs for which it is en-
abled.
Detail information: Speech recognition enables the op-
erating system to convert spoken words to written
text. An internal driver, called a speech recognition
engine, recognizes words and converts them to text.
The speech recognition engine ...
? ?
Figure 2: Example of software support article
search engines. Since the user has to read detail
information of the retrieved documents by clicking
their icons one by one, the number of items in the
final result is restricted to about 15.
In this work, we adopt Dialog Navigator as a
back-end system and construct a spoken dialogue in-
terface.
3 Dialogue strategy to clarify user?s vague
queries
3.1 Dialogue strategy based on information
gain (IG)
In the proposed clarification dialogue strategy, the
system asks optimal questions to constrain the given
retrieval results and help users find the intended
ones. Questions are dynamically generated by se-
lecting from a pool of possible candidates that sat-
isfy the precondition. The information gain (IG)
is defined as a criterion for the selection. The IG
represents a reduction of entropy, or how many re-
trieved documents can be eliminated by incorpo-
rating additional information (a reply to a question
in this case). Its computation is straightforward if
the question classifies the document set in a com-
pletely disjointed manner. However, the retrieved
documents may belong to two or more categories for
some questions, or may not belong to any category.
For example, some documents in our KB are related
with multiple versions of MS-Office, but others may
be irrelevant to any of them. Moreover, the match-
ing score of the retrieved documents should be taken
into account in this computation. Therefore, we de-
fine IG H(S) for a candidate question S by the fol-
lowing equations.
H(S) = ?
n
?
i=0
P (i) ? log P (i)
P (i) =
|C
i
|
?
n
i=0
|C
i
|
|C
i
| =
?
D
k
?i
CM(D
k
)
Here, D
k
denotes the k-th retrieved document by
matching the query to the KB, and CM(D) denotes
the matching score of document D. Thus, C
i
rep-
resents the number of documents classified into cat-
egory i by candidate question S, which is weighted
with the matching score. The documents that are not
related to any category are classified as category 0.
The system flow incorporating this strategy is
summarized below and also shown in Figure 3.
1. For a query sentence, retrieve from KB.
2. Calculate IG for all possible candidate ques-
tions which satisfy precondition.
3. Select the question with the largest IG (larger
than a threshold), and ask the question to the
user. Otherwise, output the current retrieval re-
sult.
4. Update the query sentence using the user?s re-
ply to the question.
5. Return to 1.
This procedure is explained in detail in the fol-
lowing sections.
3.2 Question generation based on bottom-up
and top-down knowledge sources
We prepare a pool of questions using three methods
based on bottom-up knowledge together with top-
down knowledge of KB. For a bottom-up knowledge
1005
System User
Matching with 
knowledge base (KB)
Knowledge base
(KB)
Any question 
with large IG? NO
Retrieval result
YES
Select question with 
largest IG for clarification
Question
Update 
query sentence Reply
Question pool
Figure 3: Overview of query clarification
Table 2: Examples of candidate questions (Dependency structure analysis: method 1)
Question Precondition Ratio of IG
applicable doc.
What did you delete? Query sentence includes ?delete? 2.15 (%) 7.44
What did you install? Query sentence includes ?install? 3.17 (%) 6.00
What did you insert? Query sentence includes ?insert? 1.12 (%) 7.12
What did you save? Query sentence includes ?save? 1.81 (%) 6.89
What is the file type? Query sentence includes ?file? 0.94 (%) 6.00
What did you setup? Query sentence includes ?setup? 0.69 (%) 6.45
source, we conducted a dependency structure anal-
ysis on KB. As for top-down knowledge, we make
use of metadata included in KB and human knowl-
edge.
3.2.1 Questions based on dependency structure
analysis (method 1)
This type of question is intended to clarify the
modifier or object of some words, based on de-
pendency structure analysis, when they are uncer-
tain. For instance, the verb ?delete? can have var-
ious objects such as ?application program? or ?ad-
dress book?. Therefore, the query can be clarified by
identifying such objects if they are missing. How-
ever, not all words need to be confirmed because the
modifier or object can be identified almost uniquely
for some words. For instance, the object of the
word ?shutdown? is ?computer? in most cases in this
task domain. It is tedious to identify the object of
such words. We therefore determine the words to be
confirmed by calculating entropy for modifier-head
pairs from the text corpus. The procedure is as fol-
lows.
1. Extract all modifier-head pairs from the text of
KB and query sentences (typed input) to an-
other retrieval system1 provided by Microsoft
Japan.
2. Calculate entropy H(m) for every word based
on probability P (i). This P (i) is calculated
with the occurrence count N(m) of word m
that appears in the text corpus and the count
N(i, m) of word m whose modifier is i.
H(m) = ?
?
i
P (i) ? log P (i)
P (i) =
N(i, m)
N(m)
1http://www.microsoft.com/japan/enable/nlsearch/
1006
Table 3: Examples of candidate questions (Metadata: method 2)
Question Precondition Ratio of IG
applicable doc.
What is the version None 30.03 (%) 2.63
of your Windows?
What is your application? None 30.28 (%) 2.31
What is the version Query sentence includes ?Word? 3.76 (%) 2.71
of your Word?
What is the version Query sentence includes ?Excel? 4.13 (%) 2.44
of your Excel?
Table 4: List of candidate questions (Human knowledge: method 3)
Question Precondition Ratio of IG
applicable doc.
When did the symptom occur? None 15.40 (%) 8.08
Tell me the error message. Query sentence includes ?error? 2.63 (%) 8.61
What do you concretely None 6.98 (%) 8.04
want to do?
As a result, we selected 40 words that have a large
value of entropy. Question sentences for these words
were generated with a template of ?What did you
...?? and unnatural ones were corrected manually.
Categories for IG calculation are defined by objects
of these words included in matched documents. The
system can make question using this method when
these words are included in the user?s query. Ta-
ble 2 lists examples of candidate questions using this
method. In this table, ratio of applicable document
corresponds to the ratio of documents that include
the words selected above, and IG is calculated using
applicable documents.
3.2.2 Questions based on metadata included in
KB (method 2)
We also prepare candidate questions using the
metadata attached to the KB. In general large-scale
KBs, metadata is usually attached to manage them
efficiently. For example, category information is at-
tached to newspaper articles and books in libraries.
In our target KB, a number of documents include
metadata of product names to which the document
applies. The system can generate question to which
the user?s query corresponds using this metadata.
However, some documents are related with multiple
versions, or may not belong to any category. There-
fore, the performance of these questions greatly de-
pends on the characteristics of the metadata.
Fourteen candidate questions are prepared using
this method. Example of candidate questions are
listed in Table 3. Ratio of applicable document cor-
responds to the ratio of documents that have meta-
data of target products.
3.2.3 Questions based on human knowledge
(method 3)
Software support is conventionally provided by
operators at call centers. We therefore prepare can-
didate questions based on the human knowledge that
has been accumulated there. This time, three kinds
of questions are hand-crafted. For instance, the
question ?When did the symptom occur?? tries to
capture key information to identify relevant docu-
ments. The categories for IG caluclation are defined
using hand-crafted rules by focusing on key-phrases
such as ?after ...? or ?during ...?. Candidate ques-
tions are listed in Table 4.
An example dialogue where the system asks ques-
tions based on IG is in Figure 4.
3.3 Update of retrieval query sentence
Through the dialogue to clarify the user?s query,
the system updates the query sentence using the
user?s reply to the question. Our backend informa-
tion retrieval system does not adopt simple ?bag-
1007
S1: What is your problem?
U1: Too garbled to read.
(Retrieval results):
? ?
1. Close button and maximize button are garbled.
2. Characters are garbled in Outlook Today.
3. Characters are garbled while inserting Japanese
text.
4. VB application is garbled to read.
? ? ?
? ?
(Calculate IG)
? Candidate question 1:
What is garbled to read? ? IG 5.27
? Candidate question 2:
What is the version of your Windows? ? IG 1.43
? Candidate question 3:
When did the symptom occur? ? IG 2.47
? ? ?
S2: (Select question with largest IG)
What is garbled to read?
U2: Characters on window button.
S3: (Update query sentence)
Retrieving with ?Characters on window button are too garbled
to read?.
Figure 4: Example dialogue
of-words? model, but conducts a more precise de-
pendency structure analysis for matching; therefore
forming an appropriate query sentence is desirable
rather than simply adding keywords. Moreover, it is
more comprehensible to the user to present the up-
dated query sentence than to show the sequence of
ASR results. Here, the update rules of the query sen-
tence are prepared as follows.
1. Questions based on dependency structure anal-
ysis
The user?s reply is added immediately before
of after the word that is the reply?s modifying
head. For instance, the reply to the question
?What did you delete?? is inserted right after
the word ?delete? in the query sentence.
2. Questions based on metadata of KB
Phrases ?In {Product name} {version name}?
are added to the query sentence.
3. Questions based on human knowledge
The position where the user?s reply is inserted
is specified beforehand for each question can-
didate. For instance, the reply to the question
?Tell me the error message.? is inserted right
after the word ?error? in the query sentence.
A dialogue example where the system updates the
user?s query is shown in Figure 5. In the exam-
ple, the systemmakes confirmation ?Retrieving with
?When I try to open it in explorer, I cannot open Ex-
cel 2002 file? ? at the end of the dialogue before pre-
senting the actual retrieval result.
3.4 Experimental evaluation
We implemented and evaluated the proposed
method. We collected a test data by 14 subjects who
had not used our system Each subject was requested
to retrieve support articles for 14 tasks, which con-
sisted of prepared scenarios (query sentences were
not given). The subjects were allowed to utter a
query again up to twice per task if they thought an
adequate retrieval result was not obtained. As a re-
sult, we collected 238 utterances for 196 (=14 ? 14)
tasks in total. An example of scenario and user ut-
terances are shown in Figure 6. The average word
accuracy of ASR was 82.9%. The threshold value in
IG that the system makes a question is set to 1.0 ini-
1008
S1: What is your problem?
U1: I cannot open the file.
S2: What is the file type? (method 1)
U2: Excel file.
(Update query sentence): ?I cannot open Excel file.?
S3: What is the version of your Excel? (method 2)
U3: My Excel is version 2002.
(Update query sentence): ?I cannot open Excel 2002 file.?
S4: When did the symptom occur? (method 3)
U4: Tried to open it in explorer.
S5: Retrieving with ?When I try to open it in explorer, I cannot
open Excel 2002 file?.
Figure 5: Query sentence update using user?s reply
? ?
? An example of scenario
You are looking for restaurant in Kyoto using
WWW. You have found a nice restaurant and tried
to print out an image of the map showing the restau-
rant. However, it is not printed out. (Your browser
is IE 6.0)
? Examples of users? utterance
? I want to print an image of map.
? I can?t print out.
? I failed to print a picture in homepage using
IE.
? Please tell me how to print out an image.
? ?
Figure 6: Example of scenario and user utterances
tially, and incremented by 0.3 every time the system
generates a question through a dialogue session.
First, we evaluated the success rate of retrieval.
We regarded a retrieval as successful when the re-
trieval result contained a correct document entry for
the scenario. We compared the following cases.
1. Transcript: A correct transcript of the user ut-
terance, prepared manually, was used as an in-
put.
2. ASR result (baseline): The ASR result was
used as an input.
3. Proposed method (log data): The system gener-
ated questions based on the proposed method,
and the user replied to them as he/she thought
appropriate.
We also evaluated the proposed method by simu-
lation in order to confirm its theoretical effect. Var-
ious factors of the entire system might influence the
performance in real dialogue which is evaluated by
the log data. Specifically, the users might not have
answered the questions appropriately, or the replies
might not have been correctly recognized. There-
fore, we also evaluated with the following condition.
4. Proposed method (simulation): The system
generated questions based on the proposed
method, and appropriate answers were given
manually.
Table 5 lists the retrieval success rate and the rank of
the correct document in the retrieval result, by these
cases. The proposed method achieved a better suc-
cess rate than when the ASR result was used. An
improvement of 12.6% was achieved in the simula-
tion case, and 7.7% by the log data. These figures
demonstrate the effectiveness of the proposed ap-
proach. The success rate of the retrieval was about
5% higher in the simulation case than the log data.
This difference is considered to be caused by follow-
ing factors.
1. ASR errors in user?s uttered replies
In the proposed strategy, the retrieval sentence
is updated using the user?s reply to the question
regardless of ASR errors. Even when the user
notices the ASR errors, he/she cannot correct
them. Although it is possible to confirm them
using ASR confidence measures, it makes di-
alogue more complicated. Hence, it was not
implemented this time.
2. User?s misunderstanding of the system?s ques-
tions
Users sometimes misunderstood the system?s
questions. For instance, to the system question
?When did the symptom occur??, some user
1009
Table 5: Success rate and average rank of correct
document in retrieval
Success Rank of
rate correct doc.
Transcript 76.1% 7.20
ASR result (baseline) 70.7% 7.45
Proposed method 78.4% 4.40(log data)
Proposed method 83.3% 3.85(simulation)
Table 6: Comparison of question methods
Success # generated
rate questions(per dialogue)
ASR result (baseline) 70.7% ?
Dependency structure 74.5% 0.38analysis (method 1)
Metadata (method 2) 75.7% 0.89
Human knowledge 74.5% 0.97(method 3)
All methods 83.3% 2.24(method 1-3)
replied simply ?just now? instead of key infor-
mation for the retrieval. To this problem, it may
be necessary to make more specific questions
or to display reply examples.
We also evaluated the efficiency of the individual
methods. In this experiment, each of the three meth-
ods was used to generate questions. The results are
in Table 6. The improvement rate by the three meth-
ods did not differ very much, and most significant
improvement was obtained by using the three meth-
ods together. While the questions based on human
knowledge are rather general and were used more
often, the questions based on the dependency struc-
ture analysis are specific, and thus more effective
when applicable. Hence, the questions based on the
dependency structure analysis (method 1) obtained
a relatively high improvement rate per question.
4 Conclusion
We proposed a dialogue strategy to clarify user?
queries for document retrieval tasks. Candidate
questions are prepared based on the dependency
structure analysis of the KB together with KB meta-
data and human knowledge. The system selects an
optimal question based on information gain (IG).
Then, the query sentence is updated using the user?s
reply. An experimental evaluation showed that the
proposed method significantly improved the success
rate of retrieval, and all categories of the prepared
questions contributed to the improvement.
The proposed approach is intended for restricted
domains, where all KB documents and several
knowledge sources are available, and it is not ap-
plicable to open-domain information retrieval such
as Web search. We believe, however, that there are
many targets of information retrieval in restricted
domains, for example, manuals of electric appli-
ances and medical documents for expert systems.
The methodology proposed here is not so dependent
on the domains, thus applicable to many other tasks
of this category.
5 Acknowledgements
The authors are grateful to Prof. Kurohashi and Dr.
Kiyota at University of Tokyo and Dr. Komatani at
Kyoto University for their helpful advice.
References
E. Chang, F. Seide, H. M. Meng, Z. Chen, Y. Shi, and Y. C. Li.
2002. A system for spoken query information retrieval on
mobile devices. IEEE Trans. on Speech and Audio Process-
ing, 10(8):531?541.
M. Denecke and A. Waibel. 1997. Dialogue strategies guid-
ing users to their communicative goals. In Proc. EU-
ROSPEECH.
A. Fujii and K. Itou. 2003. Building a test collection for
speech-driven Web retrieval. In Proc. EUROSPEECH.
C. Hori, T. Hori, H. Isozaki, E. Maeda, S. Katagiri, and S. Furui.
2003. Deriving disambiguous queries in a spoken interactive
ODQA system. In Proc. IEEE-ICASSP.
Y. Kiyota, S. Kurohashi, and F. Kido. 2002. ?Dialog Nav-
igator?: A question answering system based on large text
knowledge base. In Proc. COLING, pages 460?466.
E. Levin, S. Narayanan, R. Pieraccini, K. Biatov, E. Bocchieri,
G. Di Fabbrizio, W. Eckert, S. Lee, A. Pokrovsky, M. Rahim,
P. Ruscitti, and M. Walker. 2000. The AT&T-DARPA Com-
municator mixed-initiative spoken dialogue system. In Proc.
ICSLP.
NIST and DARPA. 2003. The twelfth Text REtrieval Confer-
ence (TREC 2003). In NIST Special Publication SP 500?
255.
A. Potamianos, E. Ammicht, and H.-K. J. Kuo. 2000. Dia-
logue management in the Bell labs Communicator system.
In Proc. ICSLP.
1010
Dialog Navigator : A Spoken Dialog Q-A System
based on Large Text Knowledge Base
Yoji Kiyota, Sadao Kurohashi (The University of Tokyo)
kiyota,kuro@kc.t.u-tokyo.ac.jp
Teruhisa Misu, Kazunori Komatani, Tatsuya Kawahara (Kyoto University)
misu,komatani,kawahara@kuis.kyoto-u.ac.jp
Fuyuko Kido (Microsoft Co., Ltd.)
fkido@microsoft.com
Abstract
This paper describes a spoken dialog Q-
A system as a substitution for call centers.
The system is capable of making dialogs
for both fixing speech recognition errors
and for clarifying vague questions, based
on only large text knowledge base. We in-
troduce two measures to make dialogs for
fixing recognition errors. An experimental
evaluation shows the advantages of these
measures.
1 Introduction
When we use personal computers, we often en-
counter troubles. We usually consult large manu-
als, experts, or call centers to solve such troubles.
However, these solutions have problems: it is diffi-
cult for beginners to retrieve a proper item in large
manuals; experts are not always near us; and call
centers are not always available. Furthermore, op-
eration cost of call centers is a big problem for en-
terprises. Therefore, we proposed a spoken dialog
Q-A system which substitute for call centers, based
on only large text knowledge base.
If we consult a call center, an operator will help
us through a dialog. The substitutable system also
needs to make a dialog. First, asking backs for fixing
speech recognition errors are needed. Note that too
many asking backs make the dialog inefficient. Sec-
ondly, asking backs for clarifying users? problems
are also needed, because they often do not know
their own problems so clearly.
To realize such asking backs, we developed a sys-
tem as shown in Figure 1. The features of our system
are as follows:
 Precise text retrieval.
The system precisely retrieves texts from large
confirmation using
confidence in recognition
confirmation using
significance for retrieval
automatic speech recognizer
(Julius)
speech input
confirmation for
significant parts
user?s selection
N-best candidates
(or reject all)
user?s selection
N-best candidates of speech recognition
asking back(s)
with dialog cards
description extraction
choices in
dialog cards
user?s selection
final result
retrieval result text
retrieval
systemuser
text
knowledge
base
dialog for clarifying
vague questions
dialog
cards
dialog for fixing
speech recognition
errors
Figure 1: Architecture.
text knowledge base provided by Microsoft
Corporation (Table 1), using question types,
products, synonymous expressions, and syntac-
tic information. Dialog cards which can cope
with very vague questions are also retrieved.
 Dialog for fixing speech recognition errors.
When accepting speech input, recognition er-
rors are inevitable. However, it is not obvi-
ous which portions of the utterance the sys-
tem should confirm by asking back to the user.
A great number of spoken dialog systems for
particular task domains, such as (Levin et al,
2000), solved this problem by defining slots,
but it is not applicable to large text knowledge
base. Therefore, we introduce two measures
of confidence in recognition and significance
for retrieval to make dialogs for fixing speech
recognition errors.
 Dialog for clarifying vague questions.
When a user asks a vague question such as
?An error has occurred?, the system navigates
him/her to the desired answer, asking him/her
back using both dialog cards and extraction of
Table 1: Text collections.
# of # of matching
text collection texts characters target
Glossary 4,707 700,000 entries
Help texts 11,306 6,000,000 titles
Support KB 23,323 22,000,000 entire texts
summaries that makes differences between re-
trieved texts more clear.
Our system makes asking backs by showing them
on a display, and users respond them by selecting
the displayed buttons by mouses.
Initially, we developed the system as a keyboard
based Q-A system, and started its service in April
2002 at the web site of Microsoft Corporation. The
extension for speech input was done based on the
one-year operation. Our system uses Julius (Lee et
al., 2001) as a Japanese speech recognizer, and it
uses language model acquired from the text knowl-
edge base of Microsoft Corporation.
In this paper, we describe the above three features
in Section 2, 3, and 4. After that, we show experi-
mental evaluation, and then conclude this paper.
2 Precise Text Retrieval
It is critical for a Q-A system to retrieve relevant
texts for a question precisely. In this section, we
describe the score calculation method, giving large
points to modifier-head relations between bunsetsu1
based on the parse results of KNP (Kurohashi and
Nagao, 1994), to improve precision of text retrieval.
Our system also uses question types, product names,
and synonymous expression dictionary as described
in (Kiyota et al, 2002).
First, scores of all sentences in each text are calcu-
lated as shown in Figure 2. Sentence score is the to-
tal points of matching keywords and modifier-head
relations. We give 1 point to a matching of a key-
word, and 2 points to a matching of a modifier-head
relation (these parameters were set experimentally).
Then sentence score is normalized by the maximum
matching score (MMS) of both sentences as follows
(the MMS is the sentence score with itself):
sentence score

the MMS of a
user question



the MMS of a
text sentence

1Bunsetsu is a commonly used linguistic unit in Japanese,
consisting of one or more adjoining content words and zero or
more following functional words.
Outlook
?Outlook?
tsukau
?use?
meru
?mail?
jushin
?receive?
Outlook wo tsukatte
meru wo jushin dekinai.
?I cannot receive mails using Outlook.?
Outlook
?Outlook?
?mail?
jushin
?receive?
error
?error?
Outlook de meru wo jushin
suru sai no error.
?An error while receiving mails
 using Outlook.?
+1
+1
+1
MMS8 10
user question text sentence
meru
+2
sentence score
= 5
	
  



 
Figure 2: Score calculation.
vague
concrete
Error ga hassei shita.
?An error has occurred.?
Komatte imasu.
?I have a problem.?
Windows 98 de kidouji ni
error ga hassei shita.
?An error has occurred
while booting Windows 98.?
text knowledge base
clarifying questions
using dialog cards
text retrieval &
description extraction
user
questions
Figure 3: User navigation.
Finally, the sentence that has the largest score in
each text is selected as the representative sentence of
the text. Then, the score of the sentence is regarded
as the score of the text.
3 Dialog Strategy for Clarifying Questions
In most cases, users? questions are vague. To cope
with such vagueness, our system uses the following
two methods: asking backs using dialog cards and
extraction of summaries that makes difference be-
tween retrieved texts more clear (Figure 3).
3.1 Dialog cards
If a question is very vague, it matches many texts,
so users have to pay their labor on finding a rele-
vant one. Our system navigates users to the desired
answer using dialog cards as shown in Figure 3.
We made about three hundred of dialog cards
to throw questions back to users. Figure 4 shows
two dialog cards. UQ (User Question) is fol-
lowed by a typical vague user question. If a user
question matches it, the dialog manager asks the
back question after SYS, showing choices be-
[Error]
UQ Error ga hassei suru
?An error occurs?
SYSError wa itsu hassei shimasuka?
?When does the error occurs??
SELECT
Windows kidou ji goto [Error/Booting Windows]
?while booting Windows?
in?satsu ji goto [Error/Printing Out]
?while printing out?
application kidou ji goto [Error/Launching Applications]
?while launching applications?
/SELECT
[Error/Booting Windows]
UQ Windows wo kidou ji ni error ga hassei suru
?An error occurs while booting Windows?
SYSAnata ga otsukai no Windows wo erande kudasai.
?Choose your Windows.?
SELECT
Windows 95 retrieve Windows 95 wo kidou ji ni error ga hassei suru
?An error occurs while booting Windows 95?
Windows 98 retrieve Windows 98 wo kidou ji ni error ga hassei suru
?An error occurs while booting Windows 98?
Windows ME retrieve Windows ME wo kidou ji ni error ga hassei suru
?An error occurs while booting Windows ME?
/SELECT
Figure 4: Dialog cards.
tween SELECT and /SELECT. Every choice is
followed by goto or retrieve. goto means that the
system follow the another dialog cards if this choice
is selected. retrieve means that the system retrieve
texts using the query specified there.
3.2 Description extraction from retrieved texts
In most cases, the neighborhood of the part that
matches the user question describes specific symp-
toms and conditions of the problem users encounter.
Our system extracts such descriptions from the re-
trieved texts as the summaries of them. The algo-
rithm is described in (Kiyota et al, 2002).
4 Dialog Strategy for Speech Input
It is necessary for a spoken dialog system to deter-
mine which portions of the speech input should be
confirmed. Moreover, criteria for judging whether
it should make confirmation or not are needed, be-
cause too many confirmations make the dialog inef-
ficient. Therefore, we introduce two criteria of con-
fidence in recognition and significance for retrieval.
Our system makes two types of asking backs for
fixing recognition errors (Figure 1). First, Julius out-
puts  -best candidates of speech recognition. Then,
the system makes confirmation for significant parts
based on confidence in recognition. After that, the
system retrieves relevant texts in the text knowledge
base using each candidate, and makes confirmation
based on significance for retrieval.
4.1 Confidence in recognition
We define the confidence in recognition for each
phrase in order to reject partial recognition errors. It
is calculated based on word perplexity, which is of-
ten used in order to evaluate suitability of language
models for test-set sentences. We adopt word per-
plexity because of the following reasons: incorrectly
recognized parts are often unnatural in context, and
words that are unnatural in context have high per-
plexity values.
As Julius uses trigram as its language model, the
word perplexity  is calculated as follows:
  




 



 


 s are summed up in each bunsetsu (phrases).
As a result, the system assigned the sum of  s
to each bunsetsu as the criterion for confidence in
recognition.
We preliminarily defined the set of product names
as significant phrases2. If the sums of  s for any
significant phrases are beyond the threshold (now,
we set it 50), the system makes confirmation for
these phrases.
4.2 Significance for retrieval
The system calculates significance for retrieval us-
ing  -best candidates of speech recognition. Be-
cause slight speech recognition errors are not harm-
ful for retrieval results, we regard a difference that
affects its retrieval result as significant. Namely,
when the difference between retrieval results for
each recognition candidate is large, we regard that
the difference is significant.
Significance for retrieval is defined as a rate
of disagreement of five high-scored retrieved texts
among  recognition candidates. For example, if
there is a substituted part in two recognition candi-
dates, and only one text is commonly retrieved out
of five high-scored texts by both candidates, the sig-
nificance for retrieval for the substituted part is 0.8
(   ).
The system makes confirmation which candidate
should be used, if significance for retrieval is beyond
the threshold (now, we set it 0.5).
2We are now developing a method to define the set of sig-
nificant phrases semi-automatically.
Table 2: Number of successful retrieval for each speaker.
speaker # of ASR transcription speech recognition with confidence with significance with both
ID utterances corr. (1) results (2) in recognition (3) for retrieval (4) measures (5)
A 13 87.8% 10/13 8/13 8/13 10/13 10/13
B 14 55.0% 6/14 3/14 5/14 3/14 5/14
C 13 61.7% 5/13 5/13 5/13 5/13 5/13
D 13 83.1% 11/13 9/13 9/13 9/13 9/13
total 53 68.5% 32/53 25/53 27/53 27/53 29/53
U: OS ? ME ???????????????????????????
????????????????????????????????
?????????????
?I am using (Windows) ME as OS(operating system), then ...?
ASR: OS ? IME ???????????????????????????
???????????????????????????
(?ME? was incorrectly recognized as ?IME?.)
S: The underlined product name seems to be uncorrectly recognized. May I
remove it, or leave it?
U: (selected ?remove?)
S: The following texts are retrieved.
1. [AC97]????????????? [???] ??????????
2. [??????]? [???]??????????
3. [WinMe] TrueType??????????????? (a correct answer)
U: Word 2002 ? ?? ???????????????
?Please tell me the way to input formulas in Word 2002.?
ASR: 1. Word 2002 ? ?? (numbers) ???????????????
2. Word 2002 ? ?? (formulas) ???????????????
3. Word 2002 ? ?? (values) ???????????????
S: Please select the most correct recognition result from the above candidates.
U: (selected No. 2)
S: The following texts are retrieved.
1. Word ???????? (a correct answer)
2. Word ??????????????
3. ????????????????
Figure 5: Dialogs for fixing speech recognition er-
rors.
(U: user, S: system, ASR: automatic speech recognition)
5 Experimental Evaluation
We evaluated the system performance experimen-
tally. For the experiments, we had 4 subjects, who
were accustomed to using computers. They made
utterances by following given 10 scenarios and also
made several utterances freely. In total, 53 utter-
ances were recorded. Figure 5 shows two successful
dialogs by confirmation using confidence in recog-
nition and by that using significance for retrieval.
We experimented on the system using the 53
recorded utterances by the following methods:
(1) Using correct transcription of recorded utter-
ance, including fillers.
(2) Using speech recognition results from which
only fillers were removed.
(3) Using speech recognition results and making
confirmation by confidence in recognition.
(4) Using  -best candidates of speech recognition
and making confirmation by significance for re-
trieval. Here,   .
(5) Using  -best candidates of speech recognition
and both measures in (3) and (4).
In these experiments, we assumed that users al-
ways correctly answer system?s asking backs. We
regarded a retrieval as a successful one if a relevant
text was contained in ten high-scored retrieval texts.
Table 2 shows the result. It indicates that our
confirmation methods for fixing speech recognition
errors improve the success rate. Furthermore, the
success rate with both measures gets close to that
with the transcriptions. Considering that the speech
recognition correctness is about 70%, the proposed
dialog strategy is effective.
6 Conclusion
We proposed a spoken dialog Q-A system in which
asking backs for fixing speech recognition errors and
those for clarifying vague questions are integrated.
To realize dialog for fixing recognition errors based
on large text knowledge base, we introduced two
measures of confidence in recognition and signif-
icance for retrieval. The experimental evaluation
shows the advantages of these measures.
References
Yoji Kiyota, Sadao Kurohashi, and Fuyuko Kido. 2002.
?Dialog Navigator? : A Question Answering System
based on Large Text Knowledge Base. In Proceedings
of COLING 2002, pages 460?466.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic
analysis method of long Japanese sentences based on
the detection of conjunctive structures. Computational
Linguistics, 20(4).
A. Lee, T. Kawahara, and K. Shikano. 2001. Julius ? an
open source real-time large vocabulary recognition en-
gine. In Proceedings of European Conf. Speech Com-
mun. & Tech. (EUROSPEECH), pages 1691?1694.
E. Levin, S. Narayanan, R. Pieraccini, K. Biatov,
E. Bocchieri, G. Di Fabbrizio, W. Eckert, S. Lee,
A. Pokrovsky, M. Rahim, P. Ruscitti, and M. Walker.
2000. The AT&T-DARPA communicator mixed-
initiative spoken dialogue system. In Proceedings of
Int?l Conf. Spoken Language Processing (ICSLP).
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 32?39,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Annotating Dialogue Acts to Construct Dialogue Systems for Consulting
Kiyonori Ohtake Teruhisa Misu Chiori Hori Hideki Kashioka Satoshi Nakamura
MASTAR Project, National Institute of Information and Communications Technology
Hikaridai, Keihanna Science City, JAPAN
kiyonori.ohtake (at) nict.go.jp
Abstract
This paper introduces a new corpus of con-
sulting dialogues, which is designed for
training a dialogue manager that can han-
dle consulting dialogues through sponta-
neous interactions from the tagged dia-
logue corpus. We have collected 130 h
of consulting dialogues in the tourist guid-
ance domain. This paper outlines our tax-
onomy of dialogue act annotation that can
describe two aspects of an utterances: the
communicative function (speech act), and
the semantic content of the utterance. We
provide an overview of the Kyoto tour
guide dialogue corpus and a preliminary
analysis using the dialogue act tags.
1 Introduction
This paper introduces a new dialogue corpus for
consulting in the tourist guidance domain. The
corpus consists of speech, transcripts, speech act
tags, morphological analysis results, dependency
analysis results, and semantic content tags. In this
paper, we describe the current status of a dialogue
corpus that is being developed by our research
group, focusing on two types of tags: speech act
tags and semantic content tags. These speech act
and semantic content tags were designed to ex-
press the dialogue act of each utterance.
Many studies have focused on developing spo-
ken dialogue systems. Their typical task do-
mains included the retrieval of information from
databases or making reservations, such as airline
information e.g., DARPA Communicator (Walker
et al, 2001) and train information e.g., ARISE
(Bouwman et al, 1999) and MASK (Lamel et al,
2002). Most studies assumed a definite and con-
sistent user objective, and the dialogue strategy
was usually designed to minimize the cost of in-
formation access. Other target tasks include tutor-
ing and trouble-shooting dialogues (Boye, 2007).
In such tasks, dialogue scenarios or agendas are
usually described using a (dynamic) tree structure,
and the objective is to satisfy all requirements.
In this paper, we introduce our corpus, which is
being developed as part of a project to construct
consulting dialogue systems, that helps the user in
making a decision. So far, several projects have
been organized to construct speech corpora such
as CSJ (Maekawa et al, 2000) for Japanese. The
size of CSJ is very large, and a great part of the
corpus consists of monologues. Although, CSJ
includes some dialogues, the size of dialogues is
not enough to construct a dialogue system via re-
cent statistical techniques. In addition, relatively
to consulting dialogues, the existing large dialogue
corpora covered very clear tasks in limited do-
mains.
However, consulting is a frequently used and
very natural form of human interaction. We of-
ten consult with a sales clerk while shopping or
with staff at a concierge desk in a hotel. Such dia-
logues usually form part of a series of information
retrieval dialogues that have been investigated in
many previous studies. They also contains various
exchanges, such as clarifications and explanations.
The user may explain his/her preferences vaguely
by listing examples. The server would then sense
the user?s preferences from his/her utterances, pro-
vide some information, and then request a deci-
sion.
It is almost impossible to handcraft a scenario
that can handle such spontaneous consulting dia-
logues; thus, the dialogue strategy should be boot-
strapped from a dialogue corpus. If an extensive
dialogue corpus is available, we can model the
dialogue using machine learning techniques such
as partially observable Markov decision processes
(POMDPs) (Thomson et al, 2008). Hori et al
(2008) have also proposed an efficient approach to
organize a dialogue system using weighted finite-
state transducers (WFSTs); the system obtains the
32
Table 2: Overview of Kyoto tour guide dialogue
corpus
dialogue type F2F WOZ TEL
# of dialogues 114 80 62
# of guides 3 2 2
avg. # of utterance 365.4 165.2 324.5/ dialogue (guide)
avg. # of utterance 301.7 112.9 373.5/ dialogue (tourist)
structure of the transducers and the weight for
each state transitions from an annotated corpus.
Thus, the corpus must be sufficiently rich in in-
formation to describe the consulting dialogue to
construct the statistical dialogue manager via such
techniques.
In addition, a detailed description would be
preferable when developing modules that focus
on spoken language understanding and generation
modules. In this study, we adopt dialogue acts
(DAs) (Bunt, 2000; Shriberg et al, 2004; Banga-
lore et al, 2006; Rodriguez et al, 2007; Levin et
al., 2002) for this information and annotate DAs in
the corpus.
In this paper, we describe the design of the Ky-
oto tour guide dialogue corpus in Section 2. Our
design of the DA annotation is described in Sec-
tion 3. Sections 4 and 5 respectively describe two
types of the tag sets, namely, the speech act tag
and the semantic content tag.
2 Kyoto Tour Guide Dialogue Corpus
We are currently developing a dialogue corpus
based on tourist guidance for Kyoto City as the tar-
get domain. Thus far, we have collected itinerary
planning dialogues in Japanese, in which users
plan a one-day visit to Kyoto City. There are
three types of dialogues in the corpus: face-to-
face (F2F), Wizard of OZ (WOZ), and telephonic
(TEL) dialogues. The corpus consists of 114 face-
to-face dialogues, 80 dialogues using the WOZ
system, and 62 dialogues obtained from telephone
conversations with the interface of the WOZ sys-
tem.
The overview of these three types of dialogues
is shown in Table 2. Each dialogue lasts for almost
30 min. Most of all the dialogues have been man-
ually transcribed. Table 2 also shows the average
number of utterances per a dialogue.
Each face-to-face dialogue involved a profes-
sional tour guide and a tourist. Three guides, one
male and two females, were employed to collect
the dialogues. All three guides were involved in
almost the same number of dialogues. The guides
used maps, guidebooks, and a PC connected to the
internet.
In the WOZ dialogues, two female guides were
employed. Each of them was participated in 40
dialogues. The WOZ system consists of two in-
ternet browsers, speech synthesis program, and
an integration program for the collaborative work.
Collaboration was required because in addition to
the guide, operators were employed to operate the
WOZ system and support the guide. Each of the
guide and operators used own computer connected
each other, and they collaboratively operate the
WOZ system to serve a user (tourist).
In the telephone dialogues, two female guides
who are the same for the WOZ dialogues were
employed. In these dialogues, we used the WOZ
system, but we did not need the speech synthesis
program. The guide and a tourist shared the same
interface in different rooms, and they could talk to
each other through the hands-free headset.
Dialogues to plan a one-day visit consist of sev-
eral conversations for choosing places to visit. The
conversations usually included sequences of re-
quests from the users and provision of information
by the guides as well as consultation in the form of
explanation and evaluation. It should be noted that
in this study, enabling the user to access informa-
tion is not an objective in itself, unlike information
kiosk systems such as those developed in (Lamel
et al, 2002) or (Thomson et al, 2008). The objec-
tive is similar to the problem-solving dialogue of
the study by Ferguson and Allen (1998), in other
words, accessing information is just an aspect of
consulting dialogues.
An example of dialogue via face-to-face com-
munication is shown in Table 1. This dialogue is
a part of a consultation to decide on a sightseeing
spot to visit. The user asks about the location of a
spot, and the guide answers it. Then, the user pro-
vides a follow-up by evaluating the answer. The
task is challenging because there are many utter-
ances that affect the flow of the dialogue during a
consultation. The utterances are listed in the order
of their start times with the utterance ids (UID).
From the column ?Time? in the table, it is easy to
see that there are many overlaps.
33
Table 1: Example dialogue from the Kyoto tour guide dialogue corpus
UID Time (ms) Speaker Transcript Speech act tag** Semantic content tag
56 76669?78819 User
Ato (And,)
WH?Question Where
null
Ohara ga (Ohara is) (activity),location
dono henni (where) (activity),(demonstrative),interr
narimasuka (I?d like to know) (activity),predicate
57 80788?81358 Guide kono (here) State Answer?56 (demonstrative),kosoahendesune (is around) (demonstrative),noun
58 81358?81841 Guide Ohara ha (Ohara) State Inversion location
59 81386?82736 User Chotto (a bit) State Evaluation?57 (transp),(cost),(distance),adverb-phrasehanaresugitemasune (is too far) (transp),(cost),(distance),predicate
60 83116?83316 Guide A (Yeah,) Pause Grabber null
61 83136?85023 User
Kore demo (it)
Y/N?Question
null
ichinichi dewa (in a day) (activity),(planning),duration
doudeshou (Do you think I can do) (activity),(planning),(demonstrative),interr
62 83386?84396 Guide Soudesune (right.) State Acknowledgment?59 null
63 85206?87076 Guide
Ichinichi (One day)
State AffirmativeAnswer?61
(activity),(planning),(entity),day-window
areba (is) (activity),(planning),predicate
jubuN (enough) (consulting),(activity),adverb-phrase
ikemasu (to enjoy it.) (consulting),(activity),action
64 88392?90072 Guide
Oharamo (Ohara is)
State Opinion
(activity),location
sugoku (very) (recommendation),(activity),adverb-phrase
kireidesuyo (a beautiful spot) (recommendation),(activity),predicate
65 89889?90759 User Iidesune (that would be nice.) State Acknowledgment?64 (consulting),(activity),predicateEvaluation?64
* Tags are concatenated using a delimiter ? ? and omitting null values.
The number following the ??? symbol denotes the target utterance of the function.
3 Annotation of Communicative
Function and Semantic Content in DA
We annotate DAs in the corpus in order to de-
scribe a user?s intention and a system?s (or the tour
guide?s) action. Recently, several studies have ad-
dressed multilevel annotation of dialogues (Levin
et al, 2002; Bangalore et al, 2006; Rodriguez et
al., 2007); in our study, we focus on the two as-
pects of a DA indicated by Bunt (2000). One is the
communicative function that corresponds to how
the content should be used in order to update the
context, and the other is a semantic content that
corresponds to what the act is about. We consider
both of them important information to handle the
consulting dialogue. We designed two different
tag sets to annotate DAs in the corpus. The speech
act tag is used to capture the communicative func-
tions of an utterance using domain-independent
multiple function layers. The semantic content tag
is used to describe the semantic contents of an ut-
terance using domain-specific hierarchical seman-
tic classes.
4 Speech Act Tags
In this section, we introduce the speech act (SA)
tag set that describes communicative functions of
utterances. As the base units for tag annotation,
we adopt clauses that are detected by applying
the clause boundary annotation program (Kash-
ioka and Maruyama, 2004) to the transcript of the
dialogue. Thus, in the following discussions, ?ut-
terance? denotes a clause.
4.1 Tag Specifications
There are two major policies in SA annotation.
One is to select exactly one label from the tag set
(e.g., the AMI corpus1). The other is to annotate
with as many labels as required. MRDA (Shriberg
et al, 2004) and DIT++ (Bunt, 2000) are defined
on the basis of the second policy. We believe that
utterances are generally multifunctional and this
multifunctionality is an important aspect for man-
aging consulting dialogues through spontaneous
interactions. Therefore, we have adopted the latter
policy.
By extending the MRDA tag set and DIT++, we
defined our speech act tag set that consists of six
layers to describe six groups of function: Gen-
eral, Response, Check, Constrain, ActionDiscus-
sion, and Others. A list of the tag sets (excluding
the Others layer is shown in Table 3. The General
layer has two sublayers under the labels, Pause
and WH-Question, respectively. The two sublay-
ers are used to elaborate on the two labels, respec-
tively. A tag of the General layer must be labeled
to an utterance, but the other layer?s tags are op-
tional, in other words, layers other than the Gen-
eral layer can take null values when there is no tag
which is appropriate to the utterance. In the practi-
cal annotation, the most appropriate tag is selected
from each layer, without taking into account any
of the other layers.
The descriptions of the layers are as follows:
General: It is used to represent the basic form
1http://corpus.amiproject.org
34
Table 3: List of speech act tags and their occurrence in the experiment
Tag Percentage(%) Tag Percentage(%) Tag Percentage(%) Tag Percentage(%)User Guide User Guide User Guide User Guide
(General) (Response) (ActionDiscussion) (Constrain)
Statement 45.25 44.53 Acknowledgment 19.13 5.45 Opinion 0.52 2.12 Reason 0.64 2.52
Pause 12.99 15.05 Accept 4.68 6.25 Wish 1.23 0.05 Condition 0.61 3.09
Backchannel 26.05 9.09 PartialAccept 0.02 0.10 Request 0.22 0.19 Elaboration 0.28 4.00
Y/N-Question 3.61 2.19 AffirmativeAnswer 0.08 0.20 Suggestion 0.16 1.12 Evaluation 1.35 2.01
WH-Question 1.13 0.40 Reject 0.25 0.11 Commitment 1.15 0.29 (Check)
Open-Question 0.32 0.32 PartialReject 0.04 0.03 RepetitionRequest 0.07 0.03
OR?after-Y/N 0.05 0.02 NegativeAnswer 0.10 0.10 UnderstandingCheck 0.19 0.20
OR-Question 0.05 0.03 Answer 1.16 2.57 DoubleCheck 0.36 0.15
Statement== 9.91 27.79 ApprovalRequest 2.01 1.07
of the unit. Most of the tags in this layer
are used to describe forward-looking func-
tions. The tags are classified into three large
groups: ?Question,? ?Fragment,? and ?State-
ment.? ?Statement==? denotes the continua-
tion of the utterance.
Response: It is used to label responses directed
to a specific previous utterance made by the
addressee.
Check: It is used to label confirmations that are
along a certain expected response.
Constrain: It is used to label utterances that re-
strict or complement the target of the utter-
ance.
ActionDiscussion: It is used to label utterances
that pertain to a future action.
Others: It is used to describe various functions of
the utterance, e.g., Greeting, SelfTalk, Wel-
come, Apology, etc.
In the General layer, there are two sublayers:? (1)
the Pause sublayer that consists of Hold, Grabber,
Holder, and Releaser and (2) the WH sublayer that
labels the WH-Question type.
It should be noted that this taxonomy is in-
tended to be used for training spoken dialogue sys-
tems. Consequently, it contains detailed descrip-
tions to elaborate on the decision-making process.
For example, checks are classified into four cat-
egories because they should be treated in various
ways in a dialogue system. UnderstandingCheck
is often used to describe clarifications; thus, it
should be taken into account when creating a di-
alogue scenario. In contrast, RepetitionRequest,
which is used to request that the missed portions
of the previous utterance be repeated, is not con-
cerned with the overall dialogue flow.
An example of an annotation is shown in Table
1. Since the Response and Constrain layers are not
necessarily directed to the immediately preceding
utterance, the target utterance ID is specified.
4.2 Evaluation
We performed a preliminary annotation of the
speech act tags in the corpus. Thirty dialogues
(900 min, 23,169 utterances) were annotated by
three labellers. When annotating the dialogues, we
took into account textual information, audio infor-
mation, and contextual information The result was
cross-checked by another labeller.
4.2.1 Distributional Statistics
The frequencies of the tags, expressed as a per-
centages, are shown in Table 3. In the General
layer, nearly half of the utterances were Statement.
This bias is acceptable because 66% of the utter-
ances had tag(s) of other layers.
The percentages of tags in the Constrain layer
are relatively higher than those of tags in the other
layers. They are also higher than the percentages
of the corresponding tags of MRDA (Shriberg
et al, 2004) and SWBD-DAMSL(Jurafsky et al,
1997).
These statistics characterize the consulting dia-
logue of sightseeing planning, where explanations
and evaluations play an important role during the
decision process.
4.2.2 Reliability
We investigated the reliability of the annotation.
Another two dialogues (2,087 utterances) were an-
notated by three labelers and the agreement among
them was examined. These results are listed in Ta-
ble 4. The agreement ratio is the average of all the
combinations of the three individual agreements.
In the same way, we also computed the average
Kappa statistic, which is often used to measure the
agreement by considering the chance rate.
A high concordance rate was obtained for the
General layer. When the specific layers and sub-
layers are taken into account, Kappa statistic was
35
Table 4: Agreement among labellers
General layer All layers
Agreement ratio 86.7% 74.2%
Kappa statistic 0.74 0.68
0.68, which is considered a good result for this
type of task. (cf. (Shriberg et al, 2004) etc.)
4.2.3 Analysis of Occurrence Tendency
during Progress of Episode
We then investigated the tendencies of tag occur-
rence through a dialogue to clarify how consult-
ing is conducted in the corpus. We annotated the
boundaries of episodes that determined the spots
to visit in order to carefully investigate the struc-
ture of the decision-making processes. In our cor-
pus, users were asked to write down their itinerary
for a practical one day tour. Thus, the beginning
and ending of an episode can be determined on the
basis of this itinerary.
As a result, we found 192 episodes. We selected
122 episodes that had more than 50 utterances,
and analyzed the tendency of tag occurrence. The
episodes were divided into five segments so that
each segment had an equal number of utterances.
The tendency of tag occurrence is shown in Figure
1. The relative occurrence rate denotes the number
of times the tags appeared in each segment divided
by the total number of occurrences throughout the
dialogues. We found three patterns in the tendency
of occurrence. The tags corresponding to the first
pattern frequently appear in the early part of an
episode; this typically applies to Open-Question,
WH-Question, and Wish. The tags of the sec-
ond pattern frequently appear in the later part, this
typically applies to Evaluation, Commitment, and
Opinion. The tags of the third pattern appear uni-
formly over an episode, e.g., Y/N-Question, Ac-
cept, and Elaboration. These statistics characterize
the dialogue flow of sightseeing planning, where
the guide and the user first clarify the latter?s in-
terests (Open, WH-Questions), list and evaluate
candidates (Evaluation), and then the user makes
a decision (Commitment).
This progression indicates that a session (or di-
alogue phase) management is required within an
episode to manage the consulting dialogue, al-
though the test-set perplexity2 , which was calcu-
2The perplexity was calculated by 10-fold cross validation
of the 30 dialogues.







    
	






	



	



	

	
Coling 2008: Companion volume ? Posters and Demonstrations, pages 59?62
Manchester, August 2008
Bayes Risk-based Dialogue Management
for Document Retrieval System with Speech Interface
Teruhisa Misu ? ?
?School of Informatics,
Kyoto University
Sakyo-ku, Kyoto, Japan
Tatsuya Kawahara?
?National Institute of Information
and Communications Technology
Hikari-dai, Seika-cho, Soraku-gun,
Kyoto, Japan
Abstract
We propose an efficient dialogue manage-
ment for an information navigation sys-
tem based on a document knowledge base
with a spoken dialogue interface. In order
to perform robustly for fragmental speech
input and erroneous output of an auto-
matic speech recognition (ASR), the sys-
tem should selectively use N-best hypothe-
ses of ASR and contextual information.
The system also has several choices in gen-
erating responses or confirmations. In this
work, we formulate the optimization of
the choices based on a unified criterion:
Bayes risk, which is defined based on re-
ward for correct information presentation
and penalty for redundant turns. We have
evaluated this strategy with a spoken di-
alogue system which also has question-
answering capability. Effectiveness of the
proposed framework was confirmed in the
success rate of retrieval and the average
number of turns.
1 Introduction
In the past years, a great number of spoken dia-
logue systems have been developed. Their typi-
cal task domains include airline information (ATIS
& DARPA Communicator) and bus location tasks.
Although the above systems can handle simple
database retrieval or transactions with constrained
dialogue flows, they are expected to handle more
c
? 2008. Teruhisa Misu and Tatsuya Kawahara,
Licensed under the Creative Commons Attribution-
Noncommercial-Share Alike 3.0 Unported license
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some
rights reserved.
complex tasks. Meanwhile, more and more elec-
tronic text resources are recently being accumu-
lated. Since most documents are indexed (e.g., via
Web search engines), we are potentially capable of
accessing these documents. Reflecting such a situ-
ation, in recent years, the target of spoken dialogue
systems has been extended to retrieval of general
documents (Chang et al, 2002).
There are quite a few choices for handling user
utterances and generating responses in the spo-
ken dialogue systems that require parameter tun-
ing. Since a subtle change in these choices may
affect the behavior the entire system, they are usu-
ally tuned by hand by an expert. It is also the
case in speech-baed document retrieval systems.
We can make use of N-best hypotheses to realize
robust retrieval against errors in automatic speech
recognition (ASR). Input queries are often vague
or fragmented in speech interfaces, thus concate-
nation of contextual information is important to
make meaningful retrieval. Such decisions tend to
be optimized module by module, but they should
be done in an integrated way. For example, we
could make more appropriate retrieval by rescoring
the N-best ASR hypotheses by the information re-
trieval scores. Even if the target document is iden-
tified, the system has several choices for generat-
ing responses. Confirmation is needed to elimi-
nate any misunderstandings caused by ASR errors,
but users easily become irritated with so many re-
dundant confirmations. Although there are several
works dealing with dialogue management in call
routing systems (Levin and Pieraccini, 2006), they
cannot handle the complex decision making pro-
cesses in information guidance tasks.
Therefore, we address the extension of conven-
tional optimization methods of dialogue manage-
ment to be applicable to general document retrieval
59
tasks. In particular, we propose a dialogue man-
agement that optimizes the choices in response
generation by minimizing Bayes risk. The Bayes
risk is defined based on reward for correct informa-
tion presentation and penalty for redundant turns
as well as the score of document retrieval and an-
swer extraction.
2 Task and Knowledge Base (KB)
As the target domain, we adopt a sightseeing
guide for Kyoto city. The KBs of this system are
Wikipedia documents concerning Kyoto and the
official tourist information of Kyoto city (810 doc-
uments, 220K words in total).
?Dialogue Navigator for Kyoto City? is a doc-
ument retrieval system with a spoken dialogue in-
terface. The system can retrieve information from
the above-mentioned document set. This system is
also capable of handling user?s specific question,
such as ?Who built this shrine?? using the QA
technique.
3 Dialogue Management and Response
Generation in Document Retrieval
System
3.1 Choices in Generating Responses
We analyzed the dialogue sessions collected in the
field trial of the ?Dialogue Navigator for Kyoto
City?, and found that we could achieve a higher
success rate by dealing with following issues.
1. Use of N-best hypotheses of ASR
There have been many studies that have used
the N-best hypotheses (or word graph) of ASR
for making robust interpretations of user utter-
ances in relational database query tasks (Ray-
mond et al, 2003). We also improved retrieval
by using all of the nouns in the 3-best hypothe-
ses(Misu and Kawahara, 2007). However, the
analysis also showed that some retrieval fail-
ures were caused by some extraneous nouns in-
cluded in erroneous hypotheses, and a higher
success rate could be achieved by selecting an
optimal hypothesis.
2. Incorporation of contextual information
In interactive query systems, users tend to make
queries that include anaphoric expressions. In
these cases, it is impossible to extract the cor-
rect answer using only the current query. For
example, ?When was it built?? makes no sense
when used by itself. We deal with this problem
by concatenating the contextual information or
keywords from the user?s previous utterances to
generate a query. However, this may include in-
appropriate context when the user changes the
topic.
3. Choices in generating responses or confirma-
tions
An indispensable part of the process to avoid
presenting inappropriate documents is confir-
mation, especially when the score of retrieval
is low. This decision is also affected by points 1
and 2 mentioned above. The presentation of the
entire document may also be ?safer? than pre-
senting the specific answer to the user?s ques-
tion, when the score of answer extraction is low.
3.2 Generation of Response Candidates
The manners of response for a document d con-
sist of the following three actions. One is the pre-
sentation (Pres(d)) of the document d, which is
made by summarizing it. Second is making a con-
firmation (Conf(d)) for presenting the document
d. The last is answering (Ans(d)) the user?s spe-
cific question, which is generated by extracting one
specific sentence from the document d.
For these response candidates, we define the
Bayes risk based on the reward for success, the
penalty for a failure, and the probability of suc-
cess. Then, we select the candidate with the mini-
mal Bayes risk. The system flow of these processes
is summarized below.
1. Make search queries W
i
(i = 1, . . . , 8) using the
1st, 2nd, and 3rd hypothesis of ASR, and all of
them, with/without contextual information.
2. For each query W
i
, retrieve from the KB and
obtain a candidate document d
i
and its likeli-
hood p(d
i
).
3. For each document d
i
, generate presentation
Pres(d
i
), confirmation Conf(d
i
), and answer-
ing Ans(d
i
) response candidates.
4. Calculate the Bayes risk for 25 response can-
didates, which are the combination of 4 (N-best
hypotheses)? 2 (use of contextual information)
? 3 (choice in response generation) + 1 (rejec-
tion).
5. Select the optimal response candidate that has
the minimal Bayes risk.
60
3.3 Definition of Bayes Risk for Candidate
Response
For these response candidates, we define the Bayes
risk based on the reward for success, the penalty
for a failure, and the probability of success (ap-
proximated by the confidence measure). That
is, a reward is given according to the manner
of response (Rwd
Ret
or Rwd
QA
) when the sys-
tem presents an appropriate response. On the
other hand, a penalty is given based on extrane-
ous time, which is approximated by the number
of sentences before obtaining the appropriate in-
formation when the system presents an incorrect
response. For example, the penalty for a confir-
mation is 2 {system?s confirmation + user?s ap-
proval}, and that of a rejection is 1 {system?s re-
jection}. When the system presents incorrect in-
formation, the penalty for a failure FailureRisk
(FR) is calculated, which consists of the improper
presentation, the user?s correction, and the sys-
tem?s request for a rephrasal. Additional sentences
for the completion of a task (AddSent) are also
given as extraneous time before accessing the ap-
propriate document when the user rephrases the
query/question. The value of AddSent is calcu-
lated as an expected number of risks assuming the
probability of success by rephrasal was p1.
The Bayes risk for the response candidates
is formulated as follows using the likelihood
of retrieval p(d), likelihood of answer extrac-
tion p
QA
(d), and the reward pair (Rwd
Ret
and
Rwd
QA
; Rwd
Ret
< Rwd
QA
) for successful pre-
sentations as well as the FR for inappropriate pre-
sentations.
? Presentation of document d (without confir-
mation)
Risk(Pres(d)) = ?Rwd
Ret
? p(d)
+(FR + AddSent) ? (1 ? p(d))
? Confirmation for presenting document d
Risk(Conf(d)) = (?Rwd
Ret
+ 2) ? p(d)
+(2 + AddSent) ? (1 ? p(d))
? Answering user?s question using document d
Risk(Ans(d)) = ?Rwd
QA
? p
QA
(d) ? p(d)
+(FR + AddSent) ? (1 ? p
QA
(d) ? p(d))
? Rejection
Risk(Rej) = 1 + AddSent
1In the experiment, we use the success rate of the field trial
presented in (Misu and Kawahara, 2007).
? ?
User utterance: When did the shogun order to
build the temple?
(Previous query:) Tell me about the Silver
Pavilion.
Response candidates:
* With context:
? p(Silver Pavilion history) = 0.4
? p
QA
(Silver Pavilion history) = 0.2 : In 1485
- Risk(Pres(Silver Pavilion history)) = 6.4
- Risk(Conf(Silver Pavilion history))= 4.8
-Risk(Ans(Silver Pavilion history; In1485)) = 9.7
. . .
* Rejection
- Risk(Rej) = 9.0
?
Response: Conf (Silver Pavilion history)
?Do you want to know the history of the Silver
Pavilion??
? ?
Figure 1: Example of calculating Bayes risk
Figure 1 shows an example of calculating a
Bayes risk (where FR = 6, Rwd
Ret
= 5,
Rwd
QA
= 40). In this example, an appropriate
document is retrieved by incorporating the previ-
ous user query. However, since the answer to the
user?s question does not exist in the knowledge
base, the score of answer extraction is low. There-
fore, the system chooses a confirmation before pre-
senting the entire document.
4 Experimental Evaluation by Cross
Validation
We have evaluated the proposed method using the
user utterances collected in the ?Dialogue Navi-
gator for Kyoto City? field trial. We transcribed
in-domain 1,416 utterances (1,084 queries and
332 questions) and labeled their correct docu-
ments/NEs by hand.
The evaluation measures we used were the suc-
cess rate and the average number of sentences for
information access. We regard a retrieval as suc-
cessful if the system presents (or confirms) the ap-
propriate document/NE for the query. The num-
ber of sentences for information access is used as
an approximation of extraneous time before ac-
cessing the document/NE. That is, it is 1 {user
utterance} if the system presents the requested
document without a confirmation. If the system
makes a confirmation before presentation, it is 3
61
{user utterance + system?s confirmation + user?s
approval}, and that for presenting an incorrect doc-
ument is 15 {user utterance + improper presenta-
tion (3 = # presented sentences) + user?s correction
+ system?s apology + request for rephrasing + ad-
ditional sentences for task completion} (FR = 6 &
AddSent = 8), which are determined based on the
typical recovery pattern observed in the field trial.
We determined the value of the parameters by a
2-fold cross validation by splitting the test set into
two (set-1 & set-2), that is, set-1 was used as a de-
velopment set to estimate FR and Rwd for eval-
uating set-2, and vice versa. The parameters were
tuned to minimize the total number of sentences
for information access in the development set. We
compared the proposed method with the following
conventional methods. Note that method 1 is the
baseline method and method 2 was adopted in the
original ?Dialogue Navigator for Kyoto City? and
used in the field trial.
Method 1 (baseline)
? Make a search query using the 1st hypothesis
of ASR.
? Incorporate the contextual information related
to the current topic.
? Make a confirmation when the ASR confi-
dence of the pre-defined topic word is low.
? Answer the question when the user query is
judged a question.
Method 2 (original system)
? Make a search query using all nouns in the 1st-
3rd hypotheses of ASR.
? The other conditions are the same as in method
1.
The comparisons to these conventional methods
are shown in Table 1. The improvement compared
with that in baseline method 1 is 6.4% in the re-
sponse success rate and 0.78 of a sentence in the
number of sentences for information access.
A breakdown of the selected response candi-
dates by the proposed method is shown in Table
2. Many of the responses were generated using a
single hypothesis from the N-best list of ASR. The
result confirms that the correct hypothesis may not
be the first one, and the proposed method selects
the appropriate one by considering the likelihood
of retrieval. Most of the confirmations were gen-
erated using the 1st hypothesis of ASR. The An-
swers to questions were often generated from the
search queries with contextual information. This
Table 1: Comparison with conventional methods
Success rate # sentences
for presentation
Method 1 (baseline) 59.2% 5.49
Method 2 63.4% 4.98
Proposed method 65.6% 4.71
Table 2: Breakdown of selected candidates
w/o context with context
Pres Conf Ans Pres Conf Ans
1st hyp. 233 134 65 2 151 2
2nd hyp. 140 43 28 2 2 6
3rd hyp. 209 50 46 1 6 5
merge all 75 11 3 18 0 91
rejection 111
result suggests that when users used anaphoric ex-
pressions, the appropriate contextual information
was incorporated into the question.
5 Conclusion
We have proposed a dialogue framework to gener-
ate an optimal response. Specifically, the choices
in response generation are optimized as a mini-
mization of the Bayes risk based on the reward for
a correct information presentation and a penalty
for redundant time. Experimental evaluations us-
ing real user utterances were used to demonstrate
that the proposed method achieved a higher suc-
cess rate for information access with a reduced
number of sentences. Although we implemented
only a simple confirmation using the likelihood of
retrieval, the proposed method is expected to han-
dle more complex dialogue management such as
the confirmation considering the impact for the re-
trieval (Misu and Kawahara, 2006).
References
Chang, E., F. Seide, H. Meng, Z. Chen, Y. Shi, and Y. Li.
2002. A System for Spoken Query Information Retrieval
on Mobile Devices. IEEE Trans. on Speech and Audio
Processing, 10(8):531?541.
Levin, E. and R. Pieraccini. 2006. Value-based Optimal De-
cision for Dialog Systems. In Proc. Spoken Laguage Tech-
nology Workshop (SLT), pages 198?201.
Misu, T. and T. Kawahara. 2006. Dialogue strategy to clarify
user?s queries for document retrieval system with speech
interface. Speech Communication, 48(9):1137?1150.
Misu, T. and T. Kawahara. 2007. Speech-based interactive
information guidance system using question-answering
technique. In Proc. ICASSP.
Raymond, C., Y. Esteve, F. Bechet, R. De Mori, and
G. Damnati. 2003. Belief Confirmation in Spoken Di-
alog Systems using Confidence Measures. In Proc. Au-
tomatic Speech Recognition and Understanding Workshop
(ASRU).
62
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 221?224,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Modeling Spoken Decision Making Dialogue
and Optimization of its Dialogue Strategy
Teruhisa Misu, Komei Sugiura, Kiyonori Ohtake,
Chiori Hori, Hideki Kashioka, Hisashi Kawai and Satoshi Nakamura
MASTAR Project, NICT
Kyoto, Japan.
teruhisa.misu@nict.go.jp
Abstract
This paper presents a spoken dialogue frame-
work that helps users in making decisions.
Users often do not have a definite goal or cri-
teria for selecting from a list of alternatives.
Thus the system has to bridge this knowledge
gap and also provide the users with an appro-
priate alternative together with the reason for
this recommendation through dialogue. We
present a dialogue state model for such deci-
sion making dialogue. To evaluate this model,
we implement a trial sightseeing guidance sys-
tem and collect dialogue data. Then, we opti-
mize the dialogue strategy based on the state
model through reinforcement learning with a
natural policy gradient approach using a user
simulator trained on the collected dialogue
corpus.
1 Introduction
In many situations where spoken dialogue interfaces
are used, information access by the user is not a goal in
itself, but a means for decision making (Polifroni and
Walker, 2008). For example, in a restaurant retrieval
system, the user?s goal may not be the extraction of
price information but to make a decision on candidate
restaurants based on the retrieved information.
This work focuses on how to assist a user who is
using the system for his/her decision making, when
he/she does not have enough knowledge about the tar-
get domain. In such a situation, users are often un-
aware of not only what kind of information the sys-
tem can provide but also their own preference or fac-
tors that they should emphasize. The system, too, has
little knowledge about the user, or where his/her inter-
ests lie. Thus, the system has to bridge such gaps by
sensing (potential) preferences of the user and recom-
mend information that the user would be interested in,
considering a trade-off with the length of the dialogue.
We propose a model of dialogue state that consid-
ers the user?s preferences as well as his/her knowledge
about the domain changing through a decision making
dialogue. A user simulator is trained on data collected
with a trial sightseeing system. Next, we optimize
the dialogue strategy of the system via reinforcement
learning (RL) with a natural policy gradient approach.
2 Spoken decision making dialogue
We assume a situation where a user selects from a given
set of alternatives. This is highly likely in real world
situations; for example, the situation wherein a user se-
lects one restaurant from a list of candidates presented
Choose the optimal spot
1. Cherry 
Blossoms
2. Japanese
Garden
3. Easy
Access
Kinkakuji-
Temple
Ryoanji-
Temple
Nanzenji-
Temple
?
?????
Goal
Criteria
Alternatives
(choices)
?????
p1 p2 p3
v11 v12 v13
? ?
Figure 1: Hierarchy structure for sightseeing guidance
dialogue
by a car navigation system. In this work, we deal with
a sightseeing planning task where the user determines
the sightseeing spot to visit, with little prior knowledge
about the target domain. The study of (Ohtake et al,
2009), which investigated human-human dialogue in
such a task, reported that such consulting usually con-
sists of a sequence of information requests from the
user, presentation and elaboration of information about
certain spots by the guide followed by the user?s evalu-
ation. We thus focus on these interactions.
Several studies have featured decision support sys-
tems in the operations research field, and the typical
method that has been employed is the Analytic Hierar-
chy Process (Saaty, 1980) (AHP). In AHP, the problem
is modeled as a hierarchy that consists of the decision
goal, the alternatives for achieving it, and the criteria
for evaluating these alternatives. An example hierarchy
using these criteria is shown in Figure 1.
For the user, the problem of making an optimal de-
cision can be solved by fixing a weight vector P
user
=
(p
1
, p
2
, . . . , p
M
) for criteria and local weight matrix
V
user
= (v
11
, v
12
, . . . , v
1M
, . . . , v
NM
) for alterna-
tives in terms of the criteria. The optimal alternative
is then identified by selecting the spot k with the maxi-
mum priority of
?
M
m=1
p
m
v
km
. In typical AHP meth-
ods, the procedure of fixing these weights is often con-
ducted through pairwise comparisons for all the possi-
ble combinations of criteria and spots in terms of the
criteria, followed by weight tuning based on the re-
sults of these comparisons (Saaty, 1980). However, this
methodology cannot be directly applied to spoken dia-
logue systems. The information about the spot in terms
of the criteria is not known to the users, but is obtained
only via navigating through the system?s information.
In addition, spoken dialogue systems usually handle
several candidates and criteria, making pairwise com-
parison a costly affair.
We thus consider a spoken dialogue framework that
estimates the weights for the user?s preference (po-
tential preferences) as well as the user?s knowledge
221
about the domain through interactions of information
retrieval and navigation.
3 Decision support system with spoken
dialogue interface
The dialogue system we built has two functions: an-
swering users? information requests and recommend-
ing information to them. When the system is requested
to explain about the spots or their determinants, it ex-
plains the sightseeing spots in terms of the requested
determinant. After satisfying the user?s request, the
system then provides information that would be helpful
in making a decision (e.g., instructing what the system
can explain, recommending detailed information of the
current topic that the user might be interested in, etc.).
Note that the latter is optimized via RL (see Section 4).
3.1 Knowledge base
Our back-end DB consists of 15 sightseeing spots as al-
ternatives and 10 determinants described for each spot.
We select determinants that frequently appear in the di-
alogue corpus of (Ohtake et al, 2009) (e.g. cherry blos-
soms, fall foliage). The spots are annotated in terms of
these determinants if they apply to them. The value of
the evaluation e
nm
is ?1? when the spot n applies to the
determinant m and ?0? when it does not.
3.2 System initiative recommendation
The content of the recommendation is determined
based on one of the following six methods:
1. Recommendation of determinants based on the
currently focused spot (Method 1)
This method is structured on the basis of the user?s
current focus on a particular spot. Specifically, the
system selects several determinants related to the
current spot whose evaluation is ?1? and presents
them to the user.
2. Recommendation of spots based on the cur-
rently focused determinant (Method 2)
This method functions on the basis of the focus on
a certain specific determinant.
3. Open prompt (Method 3)
The system does not make a recommendation, and
presents an open prompt.
4. Listing of determinants 1 (Method 4)
This method lists several determinants to the user in
ascending order from the low level user knowledge
K
sys
(that the system estimates). (K
sys
, P
sys
, p
m
and Pr(p
m
= 1) are defined and explained in Sec-
tion 4.2.)
5. Listing of determinants 2 (Method 5)
This method also lists the determinants, but the or-
der is based on the user?s high preference P
sys
(that
the system estimates).
6. Recommendation of user?s possibly preferred
spot (Method 6)
The system recommends a spot as well as the de-
terminants that the users would be interested in
based on the estimated preference P
sys
. The sys-
tem selects one spot k with a maximum value of
?
M
m=1
Pr(p
m
= 1) ? e
k,m
. This idea is based
on collaborative filtering which is often used for
recommender systems (Breese et al, 1998). This
method will be helpful to users if the system suc-
cessfully estimates the user?s preference; however,
it will be irrelevant if the system does not.
We will represent these recommendations
through a dialogue act expression, (ca
sys
{sc
sys
}),
which consists of a communicative act ca
sys
and the semantic content sc
sys
. (For exam-
ple Method1{(Spot
5
), (Det
3
,Det
4
,Det5)},
Method3{NULL,NULL}, etc.)
4 Optimization of dialogue strategy
4.1 Models for simulating a user
We introduce a user model that consists of a tuple of
knowledge vector K
user
, preference vector P
user
, and
local weight matrix V
user
. In this paper, for simplic-
ity, a user?s preference vector or weight for determi-
nants P
user
= (p
1
, p
2
, . . . , p
M
) is assumed to con-
sist of binary parameters. That is, if the user is in-
terested in (or potentially interested in) the determi-
nant m and emphasizes it when making a decision,
the preference p
m
is set to ?1?. Otherwise, it is set
to ?0?. In order to represent a state that the user has
potential preference, we introduce a knowledge param-
eter K
user
= (k
1
, k
2
, . . . , k
M
) that shows if the user
has the perception that the system is able to handle or
he/she is interested in the determinants. k
m
is set to
?1? if the user knows (or is listed by system?s recom-
mendations) that the system can handle determinant m
and ?0? when he/she does not. For example, the state
that the determinant m is the potential preference of a
user (but he/she is unaware of that) is represented by
(k
m
= 0, p
m
= 1). This idea is in contrast to previous
research which assumes some fixed goal observable by
the user from the beginning of the dialogue (Schatz-
mann et al, 2007). A user?s local weight v
nm
for spot
n in terms of determinant m is set to ?1?, when the
system lets the user know that the evaluation of spots is
?1? through recommendation Methods 1, 2 and 6.
We constructed a user simulator that is based on
the statistics calculated through an experiment with the
trial system (Misu et al, 2010) as well as the knowl-
edge and preference of the user. That is, the user?s com-
municative act cat
user
and the semantic content sct
user
for the system?s recommendation at
sys
are generated
based on the following equation:
Pr(cat
user
, sct
user
|cat
sys
, sct
sys
,K
user
,P
user
)
= Pr(cat
user
|cat
sys
)
?Pr(sct
user
|K
user
,P
user
, cat
user
, cat
sys
, sct
sys
)
This means that the user?s communicative act ca
user
is sampled based on the conditional probability of
Pr(cat
user
|cat
sys
) in (Misu et al, 2010). The seman-
tic content sc
user
is selected based on the user?s pref-
erence P
user
under current knowledge about the de-
terminants K
user
. That is, the sc is sampled from the
determinants within the user?s knowledge (k
m
= 1)
based on the probability that the user requests the de-
terminant of his/her preference/non-preference, which
is also calculated from the dialogue data of the trial sys-
tem.
4.2 Dialogue state expression
We defined the state expression of the user in the pre-
vious section. However the problem is that for the
system, the state (P
user
,K
user
,V
user
) is not observ-
able, but is only estimated from the interactions with
the user. Thus, this model is a partially observable
Markov decision process (POMDP) problem. In or-
der to estimate unobservable properties of a POMDP
222
 
Priors of the estimated state:
- Knowledge: K
sys
= (0.22, 0.01, 0.02, 0.18, . . . )
- Preference: P
sys
= (0.37, 0.19, 0.48, 0.38, . . . )
Interactions (observation):
- System recommendation:
a
sys
= Method1{(Spot
5
), (Det
1
, Det
3
, Det4)}
- User query:
a
user
= Accept{(Spot
5
), (Det
3
)}
Posterior of the estimated state:
- Knowledge: K
sys
= (1.00, 0.01, 1.00, 1.00, . . . )
- Preference: P
sys
= (0.26, 0.19, 0.65, 0.22, . . . )
User?s knowledge acquisition:
- Knowledge: K
user
? {k
1
= 1, k
3
= 1, k
4
= 1}
- Local weight: V
user
? {v
51
= 1, v
53
= 1, v
54
=
1}
 
Figure 2: Example of state update
and handle the problem as an MDP, we introduce
the system?s inferential user knowledge vector K
sys
or probability distribution (estimate value) K
sys
=
(Pr(k
1
= 1), P r(k
2
= 1), . . . , P r(k
M
= 1)) and
that of preference P
sys
= (Pr(p
1
= 1), P r(p
2
=
1), . . . , P r(p
M
= 1)).
The dialogue state DSt+1 or estimated user?s dia-
logue state of the step t+1 is assumed to be dependent
only on the previous state DSt, as well as the interac-
tions It = (at
sys
, at
user
).
The estimated user?s state is represented as a prob-
ability distribution and is updated by each interac-
tion. This corresponds to representing the user types
as a probability distribution, whereas the work of (Ko-
matani et al, 2005) classifies users to several discrete
user types. The estimated user?s preference P
sys
is up-
dated when the system observes the interaction It. The
update is conducted based on the following Bayes? the-
orem using the previous state DSt as a prior.
Pr(p
m
= 1|It) =
Pr(I
t
|p
m
=1)Pr(p
m
=1)
Pr(I
t
|p
m
=1)Pr(p
m
=1)+Pr(I
t
|(p
m
=0))Pr(1?Pr(p
m
=1))
Here, Pr(It|p
m
= 1), P r(It|(p
m
= 0) to the right
side was obtained from the dialogue corpus of (Misu et
al., 2010). This posterior is then used as a prior in the
next state update using interaction It+1. An example
of this update is illustrated in Figure 2.
4.3 Reward function
The reward function that we use is based on the num-
ber of agreed attributes between the user preference
and the decided spot. Users are assumed to determine
the spot based on their preference P
user
under their
knowledge K
user
(and local weight for spots V
user
)
at that time, and select the spot k with the maximum
priority of
?
m
k
k
? p
k
? v
km
. The reward R is then
calculated based on the improvement in the number of
agreed attributes between the user?s actual (potential)
preferences and the decided spot k over the expected
agreement by random spot selection.
R =
M
?
m=1
p
m
? e
k,m
?
1
N
N
?
n=1
M
?
m=1
p
m
? e
n,m
For example, if the decided spot satisfies three prefer-
ences and the average agreement of the agreement by
random selection is 1.3, then the reward is 1.7.
4.4 Optimization by reinforcement learning
The problem of system recommendation generation is
optimized through RL. The MDP (S, A, R) is defined
as follows. The state parameter S = (s
1
, s
2
, . . . , s
I
) is
generated by extracting the features of the current dia-
logue state DSt. We use the following 29 features 1.
1. Parameters that indicate the # of interactions from
the beginning of the dialogue. This is approximated by
five parameters using triangular functions. 2. User?s
previous communicative act (1 if at?1
user
= x
i
, other-
wise 0). 3. System?s previous communicative act (1 if
at?1
sys
= y
j
, otherwise 0). 4. Sum of the estimated user
knowledge about determinants (?N
n=1
Pr(k
n
= 1)).
5. Number of presented spot information. 6. Expecta-
tion of the probability that the user emphasizes the de-
terminant in the current state (Pr(k
n
= 1)? Pr(p
n
=
1)) (10 parameters). The action set A consists of the
six recommendation methods shown in subsection 3.2.
Reward R is given by the reward function of subsection
4.3.
A system action a
sys
(ca
sys
) is sampled based on the
following soft-max (Boltzmann) policy.
?(a
sys
= k|S) = Pr(a
sys
= k|S,?)
=
exp(
?
I
i=1
s
i
? ?
ki
)
?
J
j=1
exp(
?
I
i=1
s
i
? ?
ji
)
Here, ? = (?
11
, ?
12
, . . . ?
1I
, . . . , ?
JI
) consists of J (#
actions) ? I (# features) parameters. The parameter
?
ji
works as a weight for the i-th feature of the ac-
tion j and determines the likelihood that the action j
is selected. This ? is the target of optimization by RL.
We adopt the Natural Actor Critic (NAC) (Peters and
Schaal, 2008), which adopts a natural policy gradient
method as the policy optimization method.
4.5 Experiment by dialogue simulation
For each simulated dialogue session, a simulated user
(P
user
,K
user
,V
user
) is sampled. A preference vector
P
user
of the user is generated so that he/she has four
preferences. As a result, four parameters in P
user
are
?1? and the others are ?0?. This vector is fixed through-
out the dialogue episode. This sampling is conducted
based on the rate proportional to the percentage of users
who emphasize it for making decisions (Misu et al,
2010). The user?s knowledge K
user
is also set based
on the statistics of the ?percentage of users who stated
the determinants before system recommendation?. For
each determinant, we sample a random valuable r that
ranges from ?0? to ?1?, and k
m
is set to ?1? if r is
smaller than the percentage. All the parameters of
local weights V
user
are initialized to ?0?, assuming
that users have no prior knowledge about the candi-
date spots. As for system parameters, the estimated
user?s preference P
sys
and knowledge K
sys
are ini-
tialized based on the statistics of our trial system (Misu
et al, 2010).
We assumed that the system does not misunderstand
the user?s action. Users are assumed to continue a di-
alogue session for 20 turns2, and episodes are sampled
using the policy ? at that time and the user simulator
1Note that about half of them are continuous variables and
that the value function cannot be denoted by a lookup table.
2In practice, users may make a decision at any point once
they are satisfied collecting information. And this is the rea-
son why we list the rewards in the early dialogue stage in
223
Table 1: Comparison of reward with baseline methods
Reward (?std)
Policy T = 5 T = 10 T = 15 T = 20
NAC 0.96 (0.53) 1.04 (0.51) 1.12 (0.50) 1.19 (0.48)
B1 0.02 (0.42) 0.13 (0.54) 0.29 (0.59) 0.34 (0.59)
B2 0.46 (0.67) 0.68 (0.65) 0.80 (0.61) 0.92 (0.56)
Table 2: Comparison of reward with discrete dialogue
state expression
Reward (?std)
State T = 5 T = 10 T = 15 T = 20
PDs 0.96 (0.53) 1.04 (0.51) 1.12 (0.50) 1.19 (0.48)
Discrete 0.89 (0.60) 0.97 (0.56) 1.03 (0.54) 1.10 (0.52)
Table 3: Effect of estimated preference and knowledge
Reward (?std)
Policy T = 5 T = 10 T = 15 T = 20
Pref+Know0.96 (0.53) 1.04 (0.51) 1.12 (0.50) 1.19 (0.48)
Pref only 0.94 (0.57) 0.96 (0.55) 1.02 (0.55) 1.09 (0.53)
Know only 0.96 (0.59) 1.00 (0.56) 1.08 (0.53) 1.15 (0.51)
No Pref or
Know
0.93 (0.57) 0.96 (0.55) 1.02 (0.53) 1.08 (0.52)
of subsection 4.1. In each turn, the system is rewarded
using the reward function of subsection 4.3. The pol-
icy (parameter ?) is updated using NAC in every 2,000
dialogues.
4.6 Experimental result
The policy was fixed at about 30,000 dialogue
episodes. We analyzed the learned dialogue policy by
examining the value of weight parameter ?. We com-
pared the parameters of the trained policy between ac-
tions3. The weight of the parameters that represent the
early stage of the dialogue was large in Methods 4 and
5. On the other hand, the weight of the parameters that
represent the latter stage of the dialogue was large in
Methods 2 and 6. This suggests that in the trained pol-
icy, the system first bridges the knowledge gap between
the user, estimates the user?s preference, and then, rec-
ommends specific information that would be useful to
the user.
Next, we compared the trained policy with the fol-
lowing baseline methods.
1. No recommendation (B1)
The system only provides the requested informa-
tion and does not generate any recommendations.
2. Random recommendation (B2)
The system randomly chooses a recommendation
from six methods.
The comparison of the average reward between the
baseline methods is listed in Table 1. Note that the ora-
cle average reward that can be obtained only when the
user knows all knowledge about the knowledge base
(it requires at least 50 turns) was 1.45. The reward by
the strategy optimized by NAC was significantly better
than that of baseline methods (n = 500, p < .01).
We then compared the proposed method with the
case where estimated user?s knowledge and preference
are represented as discrete binary parameters instead of
probability distributions (PDs). That is, the estimated
user?s preference p
m
of determinant m is set to ?1?
when the user requested the determinant, otherwise it
is ?0?. The estimated user?s knowledge k
m
is set to
the following subsections. In our trial system, the dialogue
length was 16.3 turns with a standard deviation of 7.0 turns.
3The parameters can be interpreted as the size of the con-
tribution for selecting the action.
?1? when the system lets the user know the determi-
nant, otherwise it is ?0?. Another dialogue strategy was
trained using this dialogue state expression. This result
is shown in Table 2. The proposed method that rep-
resents the dialogue state as a probability distribution
outperformed (p < .01 (T=15,20)) the method using a
discrete state expression.
We also compared the proposed method with the
case where either one of estimated preference or
knowledge was used as a feature for dialogue state in
order to carefully investigate the effect of these factors.
In the proposed method, expectation of the probabil-
ity that the user emphasizes the determinant (Pr(k
n
=
1) ? Pr(p
n
= 1)) was used as a feature of dialogue
state. We evaluated the performance of the cases where
the estimated knowledge Pr(k
n
= 1) or estimated
preference Pr(p
n
= 1) was used instead of the expec-
tation of the probability that the user emphasizes the
determinant. We also compared with the case where
no preference/knowledge feature was used. This result
is shown in Table 3. We confirmed that significant im-
provement (p < .01 (T=15,20)) was obtained by taking
into account the estimated knowledge of the user.
5 Conclusion
In this paper, we presented a spoken dialogue frame-
work that helps users select an alternative from a list of
alternatives. We proposed a model of dialogue state for
spoken decision making dialogue that considers knowl-
edge as well as preference of the user and the system,
and its dialogue strategy was trained by RL. We con-
firmed that the learned policy achieved a better recom-
mendation strategy over several baseline methods.
Although we dealt with a simple recommendation
strategy with a fixed number of recommendation com-
ponents, there are many possible extensions to this
model. The system is expected to handle a more com-
plex planning of natural language generation. We also
need to consider errors in speech recognition and un-
derstanding when simulating dialogue.
References
J. Breese, D. Heckerman, and C. Kadie. 1998. ?empirical
analysis of predictive algorithms for collaborative filter-
ing?. In ?Proc. the 14th Annual Conference on Uncer-
tainty in Artificial Intelligence?, pages 43?52.
K. Komatani, S. Ueno, T. Kawahara, and H. Okuno. 2005.
User Modeling in Spoken Dialogue Systems to Generate
Flexible Guidance. User Modeling and User-Adapted In-
teraction, 15(1):169?183.
T. Misu, K. Ohtake, C. Hori, H. Kashioka, H. Kawai, and
S. Nakamura. 2010. Construction and Experiment of a
Spoken Consulting Dialogue System. In Proc. IWSDS.
K. Ohtake, T. Misu, C. Hori, H. Kashioka, and S. Nakamura.
2009. Annotating Dialogue Acts to Construct Dialogue
Systems for Consulting. In Proc. The 7th Workshop on
Asian Language Resources, pages 32?39.
J. Peters and S. Schaal. 2008. Natural Actor-Critic. Neuro-
computing, 71(7-9):1180?1190.
J. Polifroni and M. Walker. 2008. Intensional Summaries
as Cooperative Responses in Dialogue: Automation and
Evaluation. In Proc. ACL/HLT, pages 479?487.
T. Saaty. 1980. The Analytic Hierarchy Process: Planning,
Priority Setting, Resource Allocation. Mcgraw-Hill.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and
S. Young. 2007. Agenda-based User Simulation for
Bootstrapping a POMDP Dialogue System. In Proc.
HLT/NAACL.
224
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 259?265,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Toward Construction of Spoken Dialogue System
that Evokes Users? Spontaneous Backchannels
Teruhisa Misu, Etsuo Mizukami, Yoshinori Shiga, Shinichi Kawamoto?,
Hisashi Kawai and Satoshi Nakamura
National Institute of Information and Communications Technology (NICT), Kyoto, Japan.
teruhisa.misu@nict.go.jp
Abstract
This paper addresses a first step toward a
spoken dialogue system that evokes user?s
spontaneous backchannels. We construct
an HMM-based dialogue-style text-to-speech
(TTS) system that generates human-like cues
that evoke users? backchannels. A spoken
dialogue system for information navigation
was implemented and the TTS was evaluated
in terms of evoked user backchannels. We
conducted user experiments and demonstrated
that the user backchannels evoked by our TTS
are more informative for the system in detect-
ing users? feelings than those by conventional
reading-style TTS.
1 Introduction
One of the most enduring problems in spoken di-
alogue systems research is realizing a natural dia-
logue in a human-human form. One direction re-
searchers have been utilizing spontaneous nonverbal
and paralinguistic information. For example,
This paper focuses on backchannels, one of the
most common forms of para-linguistic information
in human-human dialogue. In particular, we focus
on users? verbal feedback, such as ?uh-huh? (called
Aizuchi in Japanese), and non-verbal feedback in the
form of nods. Such backchannels are very com-
mon phenomena, and considered to be used to fa-
cilitate smooth human-human communications. In
this regard, Maynard (Maynard, 1986) indicated that
such backchannels are listener?s signals to let the
speaker continue speaking (continuer), to indicate
that the listener understands and consents. It was
also hypothesized that humans detect feelings ex-
pressed via backchannels, and the correlation be-
tween backchannel patterns and user interests was
examined (Kawahara et al, 2008). These studies in-
dicate that detection of spontaneous user backchan-
? currently with Japan Advanced Institute of Science and
Technology (JAIST)
nels can benefit spoken dialogue systems by provid-
ing informative cues that reflect the user?s situation.
For instance, if a spoken dialogue system can detect
user?s backchannels, it can facilitate smooth turn-
taking. The system can also detect user?s feelings
and judge if it should continue the current topic or
change it.
Despite these previous studies and decades of
analysis on backchannels, few practical dialogue
systems have made use of them. This is proba-
bly due to the fact that users do not react as spon-
taneously to dialogue systems as they do to other
humans. We presume one of the reasons for this
is the unnatural intonation of synthesized speech.
That is, conventional speech synthesizers do not pro-
vide users with signs to elicit backchannels; an ap-
propriate set of lexical, acoustic and prosodic cues
(or backchannel-inviting cues (A. Gravano and J.
Hirschberg, 2009)), which tends to precede the lis-
tener?s backchannels in human-human communica-
tion. Though recorded human speech can provide
such cues, it is costly to re-record system?s speech
every time system scripts are updated. In this work,
we therefore tackle the challenge of constructing
dialogue-style text-to-speech (TTS) system that in-
spires users to make spontaneous backchannels un-
der the hypothesis of:
People will give more spontaneous backchannels to
a spoken dialogue system that makes more spontaneous
backchannel-inviting cues than a spoken dialogue system
that makes less spontaneous ones.
which is derived from the Media Equation (Reeves
and Nass, 1996).
2 Related Works
A number of studies have aimed at improving
the naturalness of TTS. Though most of these
have focused on means of realizing a clear and
easy-to-listen-to reading-style speech, some at-
tempts have been made at spontaneous conversa-
tional speech. Andersson (Andersson et al, 2010)
and Marge (Marge et al, 2010) focused on lexi-
259
cal phenomena such as lexical filler and acknowl-
edgments in spontaneous speech, and showed that
inserting them improves the naturalness of human-
computer dialogues. In this work, we tackle con-
structing a natural dialogue-style TTS system focus-
ing on prosodic phenomena such as intonation and
phoneme duration.
In the field of conversation analysis, many studies
analyzed backchannels in human-human dialogue
focusing on lexical and non-verbal cues (Koiso et
al., 1998; Ward and Tsukahara, 2000; A. Gravano
and J. Hirschberg, 2009). For instance these cues
were examined in preceding utterances, such as in
part-of-speech tags, length of pause, power contour
pattern, and F
0
contour pattern around the end of
the Inter-Pausal Units (IPUs). (A. Gravano and J.
Hirschberg, 2009) showed that when several of the
above cues occur simultaneously, the likelihood of
occurrence of a backchannel will increase.
Several studies also utilized the above findings
for spoken dialogue systems. Okato (Okato et al,
1996) and Fujie (Fujie et al, 2005) trained models to
predict backchannels, and implemented spoken di-
alogue systems that make backchannels. Our goal
differs in that it is to inspire users to give backchan-
nels.
3 Construction of Spoken Dialogue TTS
3.1 Spoken Dialogue Data collection for TTS
In order to make spontaneous dialogue-style TTS
that can evoke backchannels, we construct a spon-
taneous dialogue-style speech corpus that contains
backchannel-inviting cues, and then train an HMM
acoustic model for synthesis.
We collected our training data by dubbing a script
of our Kyoto Sightseeing Guidance Spoken Dia-
logue Corpus (Misu et al, 2009), a set of itinerary-
planning dialogues in Japanese. In the dialogue
task, the expert guide has made recommendations on
sightseeing spots and restaurants until has decided
on a plan for the day. With the guide?s recommen-
dations, many users give spontaneous backchannels.
We made a set of dialogue scripts from the corpus,
and asked voice actors to act them out.
When preparing the dialogue script for dubbing,
we first removed fillers and backchannels from the
transcripts of the dialogue corpus. We then anno-
tated the guide?s end of the IPUs, where the the
user made backchannels, with #. A sample dialogue
script is shown in Figure 6. We asked two profes-
sional voice actresses to duplicate the spoken dia-
logue of the script, with playing the role of the tour
guide, and the other as the tourist, sitting face-to-
face. During the recording, we asked the tour guide
role to read the scenario with intonation so that the
tourist role would spontaneously make backchan-
nels at the points marked with #. The tourist was
allowed to make backchannels at will at any pause
segments the guide made. We recorded 12 dialogue
sessions in total. The speech data was manually la-
beled, and 239.3 minutes of tour guide utterances,
which are used to train our HMM for the TTS sys-
tem, were collected. The training data is comple-
mented by the ATR 503 phonetically balanced sen-
tence set (Abe et al, 1990), so as to cover deficien-
cies in the phoneme sequence. The sentence set is
collected from news articles, and data consists of
43.1 minutes of reading-style speech.
3.2 Analysis of Collected Speech Data
Before training the HMM, we analyzed the collected
spoken dialogue data to confirm if the recorded di-
alogue speech data contained backchannel-inviting
prosodic cues. We compared prosodic features of
the dialogue speech data with those of the reading-
style speech data (phonetically balanced sentences
that we collected). Following the findings of a pre-
vious study (Koiso et al, 1998), we investigated the
duration, F
0
contour pattern and power contour pat-
tern of the final phoneme of the IPUs1.
In conversation analysis of Japanese, the F
0
con-
tour pattern label of the final phoneme is often used.
While the contour pattern is usually manually la-
beled, we roughly determined the patterns based on
the following procedure. We first normalized the log
F
0
scale using all utterances so that it has zero mean
and one standard deviation (z-score: z = (x??)/?).
We then divided each final phoneme of the IPU into
former and latter parts, and calculated the F
0
slope
of each segment by linear regression. By combina-
tion of following three patterns, we defined nine F
0
contour patterns for the final phonemes of the IPUs.
The pattern of the segment was judged as rise if the
slope was larger than a threshold ?. If the slope was
less than the threshold??, the pattern was judged as
fall. Otherwise, it was judged as flat. Here, ? was
empirically set to 5.0. The power contour patterns
of the IPUs were estimated by a similar procedure.
We analyzed 3,311 IPUs that were not followed
1For this study, we define an IPU as a maximal sequence
of words surrounded by silence longer than 200 ms. This unit
usually coincides with one Japanese phrasal unit.
260
Table 1: Prosodic analysis of final phonemes of IPUs
(dialogue script vs. newsarticle script)
dialogue newsarticle
dur. phoneme [msec] 177.1 (? 83.6) 119.4 (? 31.3)
average (? standard deviation)
F
0
power
pattern dialogue news dialogue news
rise-rise 3.7 % 10.4 % 0.0 % 0.0 %
rise-flat 2.6 % 2.1 % 0.0 % 0.0 %
rise-fall 18.8 % 3.2 % 0.0 % 0.0 %
flat-rise 4.8 % 11.5 % 0.0 % 0.0 %
flat-flat 3.5 % 1.8 % 0.0 % 9.2 %
flat-fall 12.6 % 2.7 % 13.6 % 0.1 %
fall-rise 29.2 % 47.0 % 0.0 % 0.0 %
fall-flat 7.7 % 9.0 % 86.0 % 90.7 %
fall-fall 17.1 % 12.3 % 0.0 % 0.0 %
by a turn-switch in the dialogue-style speech data
and 645 non-sentence-end IPUs in the reading-
style speech data. The prosodic features of final
phonemes of these IPUs are listed in Table 1.
According to a study (Koiso et al, 1998), in which
prosodic features of IPUs followed by a turn-hold
with backchannel, without backchannel and turn-
switch were compared, a long duration in the final
phoneme is a speaker?s typical sign to keep floor.
The same study also reported that the flat-fall and
rise-fall pattern of F
0
and power are more likely
to be followed by a backchannel than a turn-hold
without a backchannel and turn-switch. In our col-
lected speech corpus, there were actually signifi-
cant (p < 0.01) differences in the duration of the
final phoneme between that in the dialogue-style
speech and in reading-style speech. There was
also significant (p < 0.01) difference in the oc-
currence probability of the above two prosodic pat-
terns between dialogue-style speech and reading-
style speech data. These figures indicate that
as a whole the collected dialogue-style data con-
tains more backchannel-inviting cues than collected
reading-style speech data.
We trained HMM for our TTS system Ximera
using the HMM-based Speech Synthesis System
(HTS) (Zen et al, 2007). We adopted mel log spec-
trum approximation (MLSA) filter-based vocod-
ing (SPTK, 2011), a quint-phone-based phoneme
set and five state HMM-based acoustic modeling.
All training data including reading-style speech data
were used for model training.
4 User Experiment
4.1 Dialogue System used for Experiment
To evaluate our TTS system based on users? reac-
tions, a sightseeing guidance spoken dialogue sys-
Figure 1: Screen shot of the dialogue system
tem that assist users in making decision was im-
plemented. The system can explain six sightseeing
spots in Kyoto. The system provides responses to
user requests for explanation about a certain spot.
Each descriptive text on a sightseeing spot consists
of 500 (?1%) characters, 30 phrases. The text is
synthesized using section 3 TTS2. We set the speech
rate of our TTS as nine phoneme per second.
A display is used to present photos of the tar-
get sightseeing spot and an animated 3D desktop
avatar named Hanna. Figure 1 shows the GUI
the user sees. The avatar can express its status
through several motions. For example, when the
user begins speaking, it can express the state of
listening using the listener?s motion, as shown in
the figure. A sample dialogue with the system is
shown in Table 7. A video (with English subtitles)
of an sample dialogue with a user can be seen at
http://mastarpj.nict.go.jp/?xtmisu/video/TTS.wmv.
To compare the effectiveness of our TTS in
evoking users? spontaneous backchannels, we con-
structed a comparison system that adopts a conven-
tional reading-style TTS system. An HMM model
was trained using 10-hour reading-style speech by
another professional female narrator. Other settings,
such as the descriptive text and avatar agent, were
the same as those of the base system.
4.2 Comparison of Prosodic Features of the
Synthesized Speech
Prior to the experiments, we investigated the
prosodic features of the final phoneme of IPUs in
the synthesized explanations on six spots to confirm
if they contain backchannel-inviting cues. The re-
sults are given in Table 2.
Tendencies in the duration of the final phoneme
and prosody pattern distribution of the synthesized
2The descriptive texts are not included in the training data.
261
Table 2: Prosodic analysis of final phonemes of IPUs
(dialogue-style TTS vs. reading-style TTS)
dialogue synth. reading synth.
dur. phoneme [msec] 172.9 (? 29.6) 126.1 (? 19.1)
average (? standard deviation)
F
0
power
pattern dialogue reading dialogue reading
rise-rise 5.4 % 0.0 % 0.0 % 0.0 %
rise-flat 2.0 % 0.0 % 1.7 % 0.0 %
rise-fall 23.5 % 0.0 % 46.3 % 5.3 %
flat-rise 5.0 % 0.0 % 0.0 % 0.0 %
flat-flat 1.7 % 0.0 % 4.0 % 9.2 %
flat-fall 15.8 % 0.0 % 22.8 % 18.1 %
fall-rise 15.8 % 0.0 % 0.7 % 0.0 %
fall-flat 3.4 % 0.0 % 7.0 % 0.0 %
fall-fall 27.5 % 100.0 % 17.4 % 76.5 %
speech by the dialogue-style TTS system were simi-
lar to that of recorded dialogue speech, suggests that
the constructed dialogue-style TTS system can du-
plicate the backchannel-inviting cues of the recorded
original speech. The synthesized dialogue-style
speech also contained much more rise-fall and flat-
fall patterns in F
0
and power than that generated by
the reading-style TTS system. The average dura-
tion of the final phoneme was also longer. Consider-
ing the fact that the speech data was generated from
the same script, this indicates that the synthesized
speech by the dialogue-style TTS system contains
more backchannel-inviting features than that by the
reading-style TTS system.
4.3 Experimental Setup
We evaluated the TTS systems using 30 subjects
who had not previously used spoken dialogue sys-
tems. Subjects were asked to use the dialogue sys-
tem in two settings; dialogue-style TTS system and
reading-style TTS system. The experiment was con-
ducted in a small (about 2 m2) soundproof room
with no one else present.
We instructed the subjects to speak with the avatar
agent Hanna (not with the system). We also told
them that the avatar agent was listening to their
speech at all times using the microphone, and was
observing their reactions using the camera above the
display3. Subjects were given the task of acquiring
information about three candidate sightseeing spots
in Kyoto shown on the display and then selecting
one that they liked. An example dialogue with the
system is shown in Table 7. A video (with English
subtitles) showing a real user dialogue can be seen
at http://mastarpj.nict.go.jp/?xtmisu/video/exp.avi.
3The system did not actually sense the subjects? reactions.
Table 3: Questionnaire items
1. Overall, which speech was better?
2. Which speech had easier-to-understand explanations?
3. For which speech did you feel compelled to give
backchannels?
4. Which speech was more appropriate for this system?
5. Which speech had more human-like explanation?
(a) both
(b) dialogue style
(c) reading style
(d) neither
#5#4
#3#2
#1
Figure 2: Questionnaire results
After the subject selected from candidate spots,
we changed the TTS system settings and instructed
the user to have another dialogue session selecting
one of another three spots. Considering the effects of
the order, the subjects were divided into four groups;
the first group (Group 1) used the system in the order
of ?Spot list A with dialogue-style speech ? Spot
list B with reading-style speech,? the second group
(Group 2) worked in reverse order. Groups 3 and 4
used a system alternating the order of the spot sets.
5 Experimental Results
5.1 Questionnaire Results
After the experiments, subjects were asked to fill in
a questionnaire about the system. Table 3 shows the
questionnaire items. The subjects selected (a) both
are good, (b) dialogue-style speech was better, (c)
reading-style speech was better, or (d) neither were
good. Figure 2 shows the results.
The dialogue-style speech generally earned
higher ratings, but reading-style was slightly higher
in items #2 and #5. This tendency is likely at-
tributable to the fact that the dialogue-style speech
had worse clarity and naturalness than reading-style.
The mean opinion score (MOS), which is often used
to measure clarity and naturalness of TTS, of the
dialogue-style TTS was in fact 2.79, worse than 3.74
for the reading-style.
5.2 Analysis of Frequency of Backchannels
We analyzed the number of backchannels that users
made during the dialogue session. We manually
annotated subjects? verbal feedbacks, such as ?uh-
huh? and nodding of the head using the recorded
video. Out of 30 subjects, 26 gave some form of
262
Table 4: Percentages and average number of users who made backchannels
TTS % users made BCs # average BCs taken
Group 1: (Dialogue? Reading) Dialogue-style 100.0% (50.0%, 100.0%) 30.4 (1.8, 28.6)
(Spot list A? Spot list B) Reading-style 100.0% (50.0%, 87.5%) 26.1 (3.1, 23.0)
Group 2: (Reading? Dialogue) Dialogue-style 75.0% (25.0%, 62.5%) 12.7 (0.5, 12.2)
(Spot list A? Spot list B) Reading-style 75.0% (25.0%, 62.5%) 12.9 (1.3, 11.6)
Group 3: (Dialogue? Reading) Dialogue-style 100.0% (28.6%, 100.0%) 14.0 (0.4, 13.6)
(Spot list B? Spot list A) Reading-style 100.0% (0%, 100.0%) 19.3 (0, 19.3)
Group 4: (Reading? Dialogue) Dialogue-style 87.5% (42.9%, 87.5%) 28.2 (4.7, 23,5)
(Spot list B? Spot list A) Reading-style 100.0% (71.4%, 87.5%) 24.8 (6.5, 18.3)
All: Dialogue-style 86.7% (36.7%, 86.7%) 21.1 (1.7, 19.4)
Reading-style 90.0% (40.0%, 83.3%) 20.6 (2.4, 18.2)
Total backchannel (verbal feedback [Aizuchi], nodding)
backchannel to the system. Table 4 shows the per-
centages and average number of times subjects gave
backchannels. Many users made more backchannels
using the dialogue-style TTS system. Despite the
significant difference in questionnaire item #3, there
were no significant differences in the average num-
ber of users? backchannels.
5.3 Informativeness of Backchannels
We then evaluated the TTS in terms of the informa-
tiveness of evoked backchannels. The spontaneous
prosodic pattern of the backchannels is expected
to suggest positive/negative feelings on regarding
the recommended candidate. One promising use
of backchannels in our application is for detecting
users? feelings about the currently focused on spot,
and choosing to continue the explanation on the cur-
rent topic if the user seems interested, or otherwise
change the topic. We therefore label backchannels
made during the systems explanation of the spot
that the user finally selected as ?positive? and those
made during the explanations of the other two spots
as ?negative? and consider distinguishing between
them. In human-human dialogues, it was confirmed
that when a user responds promptly, the majority of
responses are positive, and more backchannels also
suggest positive responses (Kawahara et al, 2008).
We investigated the informativeness of the
backchannels based on their classification rate, or
whether the system can distinguish positive and neg-
ative backchannels, using 10-fold cross-validation.
That is, the backchannels evoked by the dialogue-
style TTS system were divided into 10 groups and
nine were used for training and the other for classi-
fication tests. We trained decision trees using J4.8
algorithm using timing, frequency, total frequency
throughout the session and type of backchannel (ver-
bal feedback or nod) as the feature set. The classifi-
cation error cost of the positive sample was set to (#
negative samples / # positive samples) considering
the difference in the number of positive and nega-
tive samples. Ten trials were conducted by chang-
ing the test set and the average classification rate
was calculated. The classification rate of backchan-
nels evoked by the system with dialogue-style TTS
was 71.4%, The confusion matrix of the classifi-
cation is shown below. We obtained precisions of
62.8% in the classification of the positive backchan-
nels, and 73.2% in that of the negative backchan-
nels. The rates are significantly higher than chance
rates of 33.5% and 66.5%. This result indicates
the backchannels evoked by the dialogue-style TTS
were informative for the system.
Table 5: Confusion matrix of classification
? classified as positive negative
? label
positive 76 141
negative 45 386
The classification rate of the reading-style TTS
system was calculated in the same way. The av-
erage classification rate of backchannels evoked by
reading-style TTS was a significantly lower 47.4%,
meaning they were not informative at all.
These results suggest that our dialogue-style TTS
system can evoke more spontaneous and informative
backchannels that reflects users? intentions than the
conventional reading-style one. This classification
rate is not completely satisfactory, but we expect that
users? feeling can be detected after observing several
backchannels. We also believe that we can estimate
users? interest more precisely by combining verbal
information of dialogue acts (Misu et al, 2010).
6 Conclusions
This paper presented our first steps toward a spoken
dialogue system that evokes users? spontaneous lis-
tener?s reactions. We constructed a dialogue-style
TTS and confirmed that by generating human-like
backchannel-inviting cues, the system can evoke
user?s spontaneous backchannels, which are infor-
mative for the system.
263
References
A. Gravano and J. Hirschberg. 2009. Backchannel-
inviting cues in task-oriented dialogue. In Proc. In-
terspeech, pages 1019?1022.
M. Abe, Y. Sagisaka, T. Umeda, and H. Kuwabara. 1990.
Speech Database User?s Manual. ATR Technical Re-
port TR-I-0166.
S. Andersson, K. Georgila, D. Traum, and R. Clark
M. Aylett. 2010. Prediction and Realisation of Con-
versational Characteristics by Utilising Spontaneous
Speech for Unit Selection. In Proc. Speech Prosody.
S. Fujie, K. Fukushima, and T. Kobayashi. 2005. Back-
channel feedback generation using linguistic and non-
linguistic information and its application to spoken di-
alogue system. In Proc. Interspeech, pages 889?892.
T. Kawahara, M. Toyokura, T. Misu, and C. Hori. 2008.
Detection of Feeling Through Back-Channels in Spo-
ken Dialogue. In Proc. Interspeech, pages 1696?1696.
H. Koiso, Y. Horiuchi, S. Tutiya, A. Ichikawa, and
Y. Den. 1998. An Analysis of Turn-Taking and
Backchannels based on Prosodic and Syntactic Fea-
tures in Japanese Map Task Dialogue. Language and
Speech, 41(3-4):295?322.
M. Marge, J. Miranda, A. Black, and A. I. Rudnicky.
2010. Towards Improving the Naturalness of Social
Conversations with Dialogue Systems. In Proc. SIG-
DIAL, pages 91?94.
S. Maynard. 1986. On back-channel behavior in
japanese and english casual conversation. Linguistics,
24(6):1079?1108.
T. Misu, K. Ohtake, C. Hori, H. Kashioka, and S. Naka-
mura. 2009. Annotating Communicative Function
and Semantic Content in Dialogue Act for Construc-
tion of Consulting Dialogue Systems. In Proc. Inter-
speech.
Teruhisa Misu, Komei Sugiura, Kiyonori Ohtake, Chiori
Hori, Hideki Kashioka, Hisashi Kawai, and Satoshi
Nakamura. 2010. Dialogue Strategy Optimization
to Assist User?s Decision for Spoken Consulting Di-
alogue Systems. In Proc. IEEE-SLT, pages 342?347.
Y. Okato, K. Kato, M. Yamamoto, and S. Itahashi. 1996.
Insertion of interjectory response based on prosodic
information. In Proc. of IEEE Workshop Interac-
tive Voice Technology for Telecommunication Applica-
tions, pages 85?88.
B. Reeves and C. Nass. 1996. The Media Equation: How
People Treat Computers, Television, and New Media
Like Real People and Places. Cambridge University
Press.
SPTK, 2011. Speech Signal Processing Toolkit (SPTK).
http://sp-tk.sourceforge.net/.
N. Ward and W. Tsukahara. 2000. Prosodic fea-
tures which cue backchannel responses in English and
Japanese. Journal of Pragmatics, 32(8):1177?1207.
H. Zen, T. Nose, J. Yamagishi, S. Sako, T. Masuko, A.W.
Black, and K. Tokuda. 2007. The HMM-based speech
synthesis system version 2.0. In Proc. ISCA SSW6.
A Appendix
264
Table 6: Example dialogue scenario used for recording (translation of Japanese)
Guide And Kyomizu-dera.
Guide Yeah, it is very famous for.
Guide There is this like wooden balcony#, and you can?t see the whole city#, but you can see a
good part of the city from there. And it is a beautiful view.
User It is nice. You have been there?
Guide Yes, many a times.
Guide Unfortunately, in winter it is not very green#, and there are not any flowers.
User So if there is snow will they close the stairs or will they - we are just going to be careful
walking the steps?
Guide You have to be careful going up.
Guide Basically, it?s a slope it?s a slope#, so if it?s very icy - if there is, you know, like black
ice#, you might want to rethink hiking up that slope. But usually it is okay# in the winter.
I mean, Kyomizu-dera and this whole area, like every tourist eventually makes their way
here#, whether they are Japanese or foreign.
Guide So, I am sure the paths should be clear.
Guide Yeah, that is a good spot too.
Table 7: Example dialogue with the system (translation of Japanese)
System Hello, my name is Hanna, a tour guide for sightseeing in Kyoto. Nice to meet you. Please
say ?Okay? when you have decided which spot you would like to know about first.
User Okay.
System Which spot would you like to know about?
User Tell me about Shinnyo-do temple.
System Sure, I?ll explain Shinnyo-do temple.
System Shinnyo-do is a Buddhist Tendai Temple in Sakyo-ku, Kyoto City and the formal name is
Shinsho Gokuraku-ji. You may take the bus directly from Kyoto Station for about thirty
minutes, and the fee is two hundred twenty yen. The admission fee for the temple is five
hundred yen. The temple was established in nine hundred eighty-four A.D., when Amida
Nyorai-zo; a principle image from Mt. Hiei was placed in the Imperial villa of this area.
System Shall I continue?
User Sure.
System The temple is famous, especially for its coloring of autumn leaves; Red Maple stand on both
sides of the stone-paved approach, and it appears as though the temple is placed inside a
maple forest. . . .
. . .
System That?s all about this spot.
System Which spot would you like to know about next?
User Daitoku-ji please.
. . .
265
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 84?93,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Reinforcement Learning of Question-Answering Dialogue Policies
for Virtual Museum Guides
Teruhisa Misu1?, Kallirroi Georgila2, Anton Leuski2, David Traum2
1National Institute of Information and Communications Technology (NICT), Kyoto, Japan
2USC Institute for Creative Technologies, Playa Vista, CA, USA
teruhisa.misu@nict.go.jp, {kgeorgila,leuski,traum}@ict.usc.edu
Abstract
We use Reinforcement Learning (RL) to learn
question-answering dialogue policies for a
real-world application. We analyze a corpus
of interactions of museum visitors with two
virtual characters that serve as guides at the
Museum of Science in Boston, in order to
build a realistic model of user behavior when
interacting with these characters. A simulated
user is built based on this model and used
for learning the dialogue policy of the virtual
characters using RL. Our learned policy out-
performs two baselines (including the original
dialogue policy that was used for collecting
the corpus) in a simulation setting.
1 Introduction
In the last 10 years Reinforcement Learning (RL)
has attracted much attention in the dialogue commu-
nity, to the extent that we can now consider RL as the
state-of-the-art in statistical dialogue management.
RL is used in the framework of Markov Decision
Processes (MDPs) or Partially Observable Markov
Decision Processes (POMDPs). In this paradigm
dialogue moves transition between dialogue states
and rewards are given at the end of a successful dia-
logue. The goal of RL is to learn a dialogue policy,
i.e. the optimal action that the system should take at
each possible dialogue state. Typically rewards de-
pend on the domain and can include factors such as
task completion, dialogue length, and user satisfac-
tion. Traditional RL algorithms require on the order
? This work was done when the first author was a visiting
researcher at USC/ICT.
of thousands of dialogues to achieve good perfor-
mance. Because it is very difficult to collect such a
large number of dialogues with real users, instead,
simulated users (SUs), i.e. models that simulate the
behavior of real users, are employed (Georgila et al,
2006). Through the interaction between the system
and the SUs thousands of dialogues can be gener-
ated and used for learning. A good SU should be
able to replicate the behavior of a real user in the
same dialogue context (Ai and Litman, 2008).
Most research in RL for dialogue management
has been done in the framework of slot-filling appli-
cations (Georgila et al, 2010; Thomson and Young,
2010), largely ignoring other types of dialogue. In
this paper we focus on the problem of learning di-
alogue policies for question-answering characters.
With question-answering systems (or characters),
the natural language understanding task is to retrieve
the best response to a user initiative, and the main
dialogue policy decision is whether to provide this
best response or some other kind of move (e.g. a re-
quest for repair, clarification, or topic change), when
the best answer does not seem to be good enough.
Note that often in the literature the term question-
answering is used for slot-filling dialogue systems
as well, in the sense that the user asks some ques-
tions, for example, about restaurants in a particular
area, and the system answers by providing a list of
options, for example, restaurants. We use the term
?question-answering? for systems where user ques-
tions can be independent of one another (follow-
up questions are possible though) and do not have
the objective of reducing the search space and re-
trieving results from a database of e.g. restaurants,
flights, etc. Thus examples of question-answering
84
characters can be virtual interviewees (that can an-
swer questions, e.g. about an incident), virtual scien-
tists (that can answer general science-related ques-
tions), and so forth.
For our experiments we use a corpus (Aggarwal
et al, 2012) of interactions of real users with two
virtual characters, the Twins, that serve as guides at
the Museum of Science in Boston (Swartout et al,
2010). The role of these virtual characters is to en-
tertain and educate the museum visitors. They can
answer queries about themselves and their technol-
ogy, generally about science, as well as questions
related to the exhibits of the museum. An example
interaction between a museum visitor and the Twins
is shown in Figure 1. The dialogue policy of the
Twins was arbitrarily hand-crafted (see section 7 for
details) and many other policies are possible (includ-
ing Baseline 2, presented in section 7, and taking
more advantage of question topics and context). We
propose to use RL for optimizing the system?s re-
sponse generation. This is a real-world application
for which RL appears to be an appropriate method.
Although there are similarities between question-
answering and slot-filling dialogues there are also a
number of differences, such as the reward function
and the behavior of the users. As discussed later in
detail, in question-answering the users have a num-
ber of questions that they are planning to ask (stock
of queries), which can be increased or decreased de-
pending not only on whether they received the in-
formation that they wanted but also on how satisfied
they are with the interaction. The system has to plan
ahead in order to maximize the number of success-
ful responses that it provides to user queries. At the
same time it needs to avoid providing incorrect or
incoherent responses so that the user does not give
up the interaction.
One of the challenges of our task is to define an
appropriate reward function. Unlike slot-filling dia-
logues, it is not clear what makes an interaction with
a question-answering system successful. A second
challenge is that in a museum setting it is not clear
what constitutes a dialogue session. Often two or
more users alternate in asking questions, which fur-
ther complicates the problem of defining a good re-
ward function. A third challenge is that the domain
is not well defined, i.e. users do not know in advance
what the system is capable of (what kind of ques-
tions the characters can answer). Moreover, there
User: What are your names? (ASR: what are
your names)
Ada: My name?s Ada.
Grace: And I?m Grace. We?re your Virtual Mu-
seum Guides. With your help, we can suggest ex-
hibits that will get you thinking! Or answer ques-
tions about things you may have seen here.
Ada: What do you want to learn about?
User: Artificial intelligence. (ASR: is
artificial intelligence)
Grace: One example of AI, or Artificial Intelli-
gence, is 20Q, an online computer activity here at
Computer Place that asks you questions to guess
what you?re thinking.
Ada: I wish we?d been programmed to do that.
Nah. . . on second thought, I prefer just answering
your questions.
Grace: That takes AI too.
Figure 1: Example dialogue between the Twins virtual
characters and a museum visitor.
are many cases of ?junk? user questions (e.g. ?are
you stupid??) or even user prompts in languages
other than English (e.g. ?hola?).
We first analyze our corpus in order to build a re-
alistic model of user behavior when interacting with
the virtual characters. A SU is built based on this
model and used for learning the dialogue policy of
the virtual characters using RL. Then we compare
our learned policy with two baselines, one of which
is the dialogue policy of the original system that was
used for collecting our corpus and that is currently
installed at the Museum of Science in Boston. Our
learned policy outperforms both baselines in a sim-
ulation setting.
To our knowledge this is the first study that uses
RL for learning this type of question-answering dia-
logue policy. Furthermore, unlike most studies that
use data collected by having paid subjects interact
with the system, we use data collected from real
users, in our case museum visitors.1 We also com-
pare our learned dialogue policy with the dialogue
policy of the original system that is currently in-
stalled at the Museum of Science in Boston.
The structure of the paper is as follows. In sec-
1Note that the CMU ?Let?s Go!? corpus is another case of
using real user data for learning dialogue policies for the Spoken
Dialogue Challenge.
85
tion 2 we present related work. Section 3 provides a
brief introduction to RL and section 4 describes our
corpus. Then in section 5 we explain how we built
our SU from the corpus, and in section 6 we describe
our learning methodology. Section 7 presents our
evaluation results. Finally section 8 presents some
discussion and ideas for future work together with
our conclusion.
2 Related Work
To date, RL has mainly been used for learning di-
alogue policies for slot-filling applications such as
restaurant recommendations (Jurc???c?ek et al, 2012),
sightseeing recommendations (Misu et al, 2010),
appointment scheduling (Georgila et al, 2010), etc.,
largely ignoring other types of dialogue. Recently
there have been some experiments on applying RL
to the more difficult problem of learning negotia-
tion policies (Heeman, 2009; Georgila and Traum,
2011a; Georgila and Traum, 2011b). Also, RL has
been applied to tutoring domains (Tetreault and Lit-
man, 2008; Chi et al, 2011).
There has been a lot of work on developing
question-answering systems with dialogue capabil-
ities, e.g. (Jo?nsson et al, 2004; op den Akker et al,
2005; Varges et al, 2009). Most of these systems are
designed for information extraction from structured
or unstructured databases in closed or open domains.
One could think of them as adding dialogue capa-
bilities to standard question-answering systems such
as the ones used in the TREC question-answering
track (Voorhees, 2001). Other work has focused on
a different type of question-answering dialogue, i.e.
question-answering dialogues that follow the form
of an interview and that can be used, for example,
for training purposes (Leuski et al, 2006; Gandhe et
al., 2009). But none of these systems uses RL.
To our knowledge no one has used RL for learning
policies for question-answering systems as defined
in section 1. Note that Rieser and Lemon (2009)
used RL for question-answering, but in their case,
question-answering refers to asking for information
about songs and artists in an mp3 database, which
is very much like a slot-filling task, i.e. the system
has to fill a number of slots (e.g. name of band, etc.)
in order to query a database of songs and present
the right information to the user. As discussed in
section 1 our task is rather different.
3 Reinforcement Learning
A dialogue policy is a function from contexts to
(possibly probabilistic) decisions that the dialogue
system will make in those contexts. Reinforcement
Learning (RL) is a machine learning technique used
to learn the policy of the system. For an RL-based
dialogue system the objective is to maximize the re-
ward it gets during an interaction. RL is used in the
framework of Markov Decision Processes (MDPs)
or Partially Observable Markov Decision Processes
(POMDPs).
In this paper we follow a POMDP-based ap-
proach. A POMDP is defined as a tuple (S, A, P , R,
O, Z, ?, b0) where S is the set of states (representing
different contexts) which the system may be in (the
system?s world),A is the set of actions of the system,
P : S ? A ? P (S, A) is the set of transition prob-
abilities between states after taking an action, R : S
? A ?< is the reward function, O is a set of obser-
vations that the system can receive about the world,
Z is a set of observation probabilities Z : S ? A
? Z(S, A), and ? a discount factor weighting long-
term rewards. At any given time step i the world
is in some unobserved state si ? S. Because si is
not known exactly, we keep a distribution over states
called a belief state b, thus b(si) is the probability of
being in state si, with initial belief state b0. When
the system performs an action ?i ? A based on b,
following a policy pi : S ? A, it receives a reward
ri(si, ?i) ? < and transitions to state si+1 accord-
ing to P (si+1|si, ?i) ? P . The system then receives
an observation oi+1 according to P (oi+1|si+1, ?i).
The quality of the policy pi followed by the agent is
measured by the expected future reward also called
Q-function, Qpi : S ? A ? <.
There are several algorithms for learning the opti-
mal dialogue policy and we use Natural Actor Critic
(NAC) (Peters and Schaal, 2008), which adopts a
natural policy gradient method for policy optimiza-
tion, also used by (Thomson and Young, 2010;
Jurc???c?ek et al, 2012). Policy gradient methods do
not directly update the value of state S orQ-function
(expected future reward). Instead, the policy pi (or
parameter?, see below) is directly updated so as to
increase the reward of dialogue episodes generated
by the previous policy.
A system action asys is sampled based on the fol-
lowing soft-max (Boltzmann) policy:
86
pi(asys = k|?) = Pr(asys = k|?,?)
= exp(
?I
i=1 ?i ? ?ki)?J
j=1 exp(
?I
i=1 ?i ? ?ji)
Here, ? = (?1, ?2, . . . , ?I) is a basis func-
tion, which is a vector function of the belief state.
? = (?11, ?12, . . . ?1I , . . . , ?JI ) consists of J (# ac-
tions) ? I (# features) parameters. The parameter
?ji works as a weight for the i-th feature of the ac-
tion j and determines the likelihood that the action j
is selected. ? is the target of optimization by RL.
During training, RL algorithms require thousands
of interactions between the system and the user
to achieve good performance. For this reason we
need to build a simulated user (SU) (Georgila et al,
2006), that will behave similarly to a real user, and
will interact with the policy for thousands of itera-
tions to generate data in order to explore the search
space and thus facilitate learning.
Topic Example user question/prompt
introduction Hello.
personal Who are you named after?
school Where do you go to school?
technology What is artificial intelligence?
interfaces What is a virtual human?
exhibition What can I do at Robot Park?
Table 1: Topics of user questions/prompts.
4 The Twins Corpus
As mentioned in section 1 the Twins corpus (Aggar-
wal et al, 2012) was collected at the Museum of Sci-
ence in Boston (Swartout et al, 2010). The Twins
can answer a number of user questions/prompts in
several topics, i.e. about themselves and their tech-
nology, about science in general, and about exhibits
in the museum. We have divided these topics in six
categories shown in Table 1 together with an exam-
ple for each category.
An example interaction between a museum vis-
itor and the Twins is shown in Figure 1. We can
also see the output of the speech recognizer. In the
part of the corpus that we use for our experiment
automatic speech recognition (ASR) was performed
by Otosense, an ASR engine developed by the USC
SAIL lab. Natural language understanding and di-
alogue management are both performed as a single
task by the NPCEditor (Leuski and Traum, 2010),
a text classification system that classifies the user?s
query to a system?s answer using cross-language in-
formation retrieval techniques. When the system
fails to understand the user?s query it can prompt her
to do one of the following:
? rephrase her query (from now on referred to
as off-topic response 1, OT1), e.g. ?please
rephrase your question?;
? prompt the user to ask a particular question that
the system knows that it can handle (from now
on referred to as off-topic response 2, OT2),
e.g. ?you may ask us about our hobbies?;
? cease the dialogue and check out the ?behind
the scenes? exhibit which explains how the vir-
tual characters work (from now on referred to
as off-topic response 3, OT3).
The Twins corpus contains about 200,000 spoken
utterances from museum visitors (primarily chil-
dren) and members of staff or volunteers. For the
purposes of this paper we used 1,178 dialogue ses-
sions (11,074 pairs of user and system utterances)
collected during March to May 2011. This subset
of the corpus contains manual transcriptions of user
queries, system responses, and correct responses to
user queries (the responses that the system should
give when ASR is perfect).
5 User Simulation Model
In order to build a model of user behavior we per-
form an analysis of the corpus. One of our chal-
lenges is that the boundaries between dialogue ses-
sions are hard to define, i.e. it is very hard to auto-
matically calculate whether the same or a new user
speaks to the system, unless complex voice iden-
tification techniques are employed. We make the
reasonable assumption that a new dialogue session
starts when there are no questions to the system for
a time interval greater than 120 sec.
From each session we extract 30 features. A full
list is shown in Table 7 in the Appendix. Our goal
is to measure the contribution of each feature to
the user?s decision with respect to two issues: (1)
whether the user will cease the dialogue or not, and
(2) what kind of query the user will make next, based
87
on what has happened in the dialogue so far. To do
that we use the Chi-squared test, which is commonly
used for feature selection.
So to measure the contribution of each feature to
whether the user will cease the dialogue or not, we
give a binary label to each user query in our corpus,
i.e. 1 when the query is the last user query in the di-
alogue session and 0 otherwise. Then we calculate
the contribution of each feature for estimating this
label. In Table 8, column 1, in the Appendix, we can
see the 10 features that contribute the most to pre-
dicting whether the user will cease the dialogue. As
we can see the dominant features are not whether
the system correctly responded to the user?s query,
but mostly features based on the dialogue history
(e.g. the number of the system?s off-topic responses
so far) and user type information. Indeed, a further
analysis of the corpus showed that children tend to
have longer dialogue sessions than adults.
Our next step is the estimation of the contribution
of each feature for predicting the user?s next query.
The label we predict here is the topic of the user?s
utterance (personal, exhibition, etc., see Table 1).
We can see the 10 most predictive features in Ta-
ble 8, column 2, in the Appendix. The contribution
of the most recent user?s utterance (previous topic
category) is larger than that of dialogue history fea-
tures. This tendency is the same when we ignore re-
peated user queries, e.g. when the system makes an
error and the user rephrases her query (see Table 8,
column 3, in the Appendix). The user type is impor-
tant for predicting the next user query. In Figure 2
we can see the percentages of user queries per user
type and topic.
Based on the above analysis we build a simulated
user (SU). The SU simulates the following:
? User type (child, male, female): a child user
is sampled with a probability of 51.1%, a male
with 31.1%, and a female with 17.8%. These
probabilities are estimated from the corpus.
? Number of questions the user is planning to
ask (stock of queries): We assume here that
the user is planning to ask a number of ques-
tions. This number may increase or decrease.
For example, it can increase when the system
prompts the user to ask about a particular topic
(OT2 prompt), and it may decrease when the
user decides to cease the dialogue immediately.
Figure 2: Percentages of user queries per user type and
topic.
The number of questions is sampled from a
user type dependent Zipf distribution (strictly
speaking the continuous version of the distri-
bution; Parato distribution) the parameter of
which is estimated from the corpus using the
maximum likelihood criterion. We chose Zipf
because it is a long-tail distribution that fits our
data (users are not expected to ask a large num-
ber of questions). According to this distribution
a child user is more likely to have a larger stock
of queries than a male or female adult.
? User?s reaction: The user has to decide on
one of the following. Go to the next topic
(Go-on); cease the dialogue if there are no
more questions in the stock of queries (Out-of-
stock); rephrase the previous query (Rephrase);
abandon the dialogue (Give-up) regardless of
the remaining questions in the stock; gener-
ate a query based on a system recommenda-
tion, OT2 prompt (Refill). We calculate the
user type dependent probability for these ac-
tions from the corpus. But the problem here
is that it is not possible to distinguish be-
tween the case in which the user asked all the
questions in the stock of queries (i.e. all the
questions she intended to ask) and left, from
the case in which she gave up and abandoned
the dialogue. We estimate the percentage of
?Give-up? as the difference between the ratio of
?Cease? after an incorrect response and the ra-
88
tio of ?Cease? after a correct response, assum-
ing a similar percentage of ?Out-of-stock? for
both correct and incorrect responses. Likewise,
the difference in ?Go-on? for OT2 and other re-
sponses is attributed to ?Refill?. The probabil-
ity of ?Rephrase? is estimated from the corpus.
For example the probability that a child will
rephrase after an OT1 system prompt is 54%,
after an erroneous system prompt 38%, etc.
? Topic for next user query (e.g. introduction,
personal, etc.): The SU selects a new topic
based on user type dependent topic transition
bigram probabilities estimated from the corpus.
? User utterance: The SU selects a user utter-
ance from the corpus that matches the current
user type and topic. We have split the corpus
in groups of user utterances based on user type
and topic and we sample accordingly.
? Utterance timing: We simulate utterance tim-
ing (duration of pause between system utter-
ance and next user query) per user type and
user change. The utterance timing is sampled
based on a Gaussian distribution the parameters
of which are set based on the corpus statistics.
For example, the average duration of a session
until the user changes is 62.7 sec with a stan-
dard deviation of 71.2 sec.
6 Learning Question-Answering Policies
Our goal is to use RL in order to optimize the sys-
tem?s response generation. As we saw in the previ-
ous section the SU generates a user utterance from
our corpus. We do not currently use ASR error sim-
ulation but instead a real ASR engine. So the au-
dio file that corresponds to the selected user utter-
ance is forwarded to 3 ASR systems, with child,
male, and female acoustic models (AMs) respec-
tively. Then these recognition results are forwarded
to the NPCEditor that produces an N-best list of pos-
sible system responses (retrieval results). That is,
as mentioned in section 4, the NPCEditor classifies
each ASR result to a system answer using cross-
language information retrieval techniques. The pol-
icy can choose one of the NPCEditor retrieval re-
sults or reject them and instead present one of the
three off-topic prompts (OT1, OT2, or OT3). So the
system has 10 possible actions to choose between:
? use the response with the best or the second
best score retrieved from the NPCEditor based
on a child AM (2 actions);
? use the response with the best or the second
best score retrieved from the NPCEditor based
on a male AM (2 actions);
? use the response with the best or the second
best score retrieved from the NPCEditor based
on a female AM (2 actions);
? use the response with the best of the 6 afore-
mentioned scores of the NPCEditor;
? use off-topic prompt OT1;
? use off-topic prompt OT2;
? use off-topic prompt OT3.
We use the following features to optimize our di-
alogue policy (see section 3). We use the 6 retrieval
scores of the NPCEditor (the 2 best scores for each
user type ASR result), the previous system action,
the ASR confidence scores, the voting scores (calcu-
lated by adding the scores of the results that agree),
the system?s belief on the user type and user change,
and the system?s belief on the user?s previous topic.
So we need to learn a POMDP-based policy using
these 42 features.
Unlike slot-filling dialogues, defining the reward
function is not a simple task (e.g. reward the system
for filled and confirmed slots). So in order to define
the reward function and thus measure the quality of
the dialogue we set up a questionnaire. We asked
5 people to rate 10 dialogues in a 5-Likert scale.
Each dialogue session included 5 question-answer
pairs. Then we used regression analysis to set the
reward for each of the question-answer pair cate-
gories shown in Table 2. So for example, responding
correctly to an in-domain user question is rewarded
(+23.2) whereas providing an erroneous response to
a junk question, i.e. treating junk questions as if they
were in-domain questions, is penalized (-14.7).
One limitation of this reward function (Reward
function 1) is that it does not take into account
whether the user has previously experienced an off-
topic system prompt. To account for that we define
Reward function 2. Here we consider the number
of off-topic responses in the two most recent system
prompts. Reward function 2 is shown in Table 3.
89
QA Pair Reward
in-domain ? correct 23.2
in-domain ? error -12.2
in-domain ? OT1 -5.4
in-domain ? OT2 -8.4
in-domain ? OT3 -9.6
junk question ? error -14.7
junk question ? OT1 4.8
junk question ? OT2 10.2
junk question ? OT3 6.1
give up -16.9
Table 2: Reward function 1.
QA Pair Reward
in-domain ? correct 16.9
in-domain ? error -2.0
in-domain ? OT1 13.9
in-domain ? OT1(2) 7.3
in-domain ? OT2 -7.9
in-domain ? OT2(2) 4.2
in-domain ? OT3 -15.8
in-domain ? OT3(2) -8.3
junk question ? error -4.6
junk question ? OT1 4.1
junk question ? OT1(2) 4.1
junk question ? OT2 43.4
junk question ? OT2(2) -33.1
junk question ? OT3 3.1
junk question ? OT3(2) 6.1
give up -19.5
Table 3: Reward function 2.
As we can see, providing an OT2 as the first off-
topic response is a poor action (-7.9); it is preferable
to ask the user to rephrase her question (OT1) as a
first attempt to recover from the error (+13.9). On
the other hand, providing an OT2 prompt, after an
off-topic prompt has occured in the previous system
prompt, is a reasonable action (+4.2).
7 Evaluation
We compare our learned policy with two baselines.
The first baseline, Baseline 1, is the dialogue pol-
icy that is used by our system that is currently in-
stalled at the Museum of Science in Boston. Base-
line 1 selects the best ASR result (i.e. the result
with the highest confidence score) out of the results
with the 3 different AMs (child, male, and female),
and forwards this result to the NPCEditor to retrieve
the system?s response. If the NPCEditor score is
higher than an emprically set pre-defined threshold
(see (Leuski and Traum, 2010) for details), then the
system presents the retrieved response, otherwise it
presents an off-topic prompt. The system presents
these off-topic prompts in a fixed order. First, OT1,
then OT2, and then OT3.
We also have Baseline 2, which forwards all 3
ASR results to the NPCEditor (using child, male,
and female AMs). Then the NPCEditor retrieves 3
results, one for each one of the 3 ASR results, and
selects the retrieved result with the highest score.
Again if this score is higher than a threshold, the sys-
tem will present this result, otherwise it will present
an off-topic prompt.
Each policy interacts with the SU for 10,000 di-
alogue sessions and we calculate the average accu-
mulated reward for each dialogue. In Tables 4 and 5
we can see our results for Reward functions 1 and 2
respectively. In both cases the learned policy outper-
forms both baselines. For both reward functions the
most predictive feature is the ASR confidence score
when combined with the NPCEditor?s retrieval score
and the previous system action. Also, for both re-
ward functions the second best feature is ?voting?
when combined with the retrieval score and the pre-
vious system action.
In Table 6 we can see how often the learned pol-
icy, which is based on Reward function 1 using all
features, selects each one of the 10 system actions
(200,000 system turns in total).
Policy Avg Reward
Baseline 1 24.76 (19.29)
Baseline 2 51.63 (49.84)
Learned Policy - Features
Retrieval score
+ system action (*) 46.74
(*) + ASR confidence score 61.59
(*) + User type probability 47.28
(*) + Estimated previous topic 47.87
(*) + Voting 59.94
All features 60.93
Table 4: Results with reward function 1. The values in
parentheses for Baselines 1 and 2 are the rewards when
the NPCEditor does not use the pre-defined threshold.
90
Policy Avg Reward
Baseline 1 39.40 (38.51)
Baseline 2 55.45 (54.49)
Learned Policy - Features
Retrieval score
+ system action (*) 49.15
(*) + ASR confidence score 69.51
(*) + User type probability 50.15
(*) + Estimated previous topic 49.84
(*) + Voting 69.06
All features 73.59
Table 5: Results with reward function 2. The values in
parentheses for Baselines 1 and 2 are the rewards when
the NPCEditor does not use the pre-defined threshold.
System Action Frequency
Child + 1st best score 10.33%
Child + 2nd best score 2.70%
Male + 1st best score 13.72%
Male + 2nd best score 1.03%
Female + 1st best score 39.73%
Female + 2nd best score 0.79%
Best of scores 1-6 2.38%
OT1 11.01%
OT2 6.86%
OT3 11.45%
Table 6: Frequency of the system actions of the learned
policy that is based on Reward function 1 using all fea-
tures.
8 Discussion and Conclusion
We showed that RL is a promising technique for
learning question-answering policies. Currently we
use the same SU for both training and testing the
policies. One could argue that this favors the learned
policy over the baselines. Because our SU is based
on general corpus statistics (probability that the user
is child or male or female, number of questions the
user is planning to ask, probability of moving to the
next topic or ceasing the dialogue, utterance timing
statistics) rather than sequential information we be-
lieve that this is acceptable. We only use sequential
information when we calculate the next topic that
the user will choose. That is, due to the way the
SU is built and its randomness, we believe that it is
very unlikely that the same patterns that were gener-
ated during training will be generated during testing.
Thus we do not anticipate that our results would be
different if for testing we used a SU trained on a dif-
ferent part of the corpus, or that the learned policy is
favored over the baselines. However, this is some-
thing to verify experimentally in future work.
For future work we would also like to do the fol-
lowing. First of all, currently we are in the process of
analyzing user satisfaction questionnaires from mu-
seum visitors in order to define a better reward func-
tion. Second, we would like to use voice identifi-
cation techniques to automatically estimate from the
corpus the statistics of having more than one user
or alternating users in the same session. Third, and
most important, we would like to incorporate the
learned policy into the system that is currently in-
stalled in the museum and evaluate it with real users.
Fourth, currently our SU is based on only some of
our findings from the analysis of the corpus. We in-
tend to build a more complex and hopefully more
realistic SU based on our full corpus analysis. Fi-
nally, we will also experiment with learning policies
directly from the data (Li et al, 2009).
To conclude, we analyzed a corpus of interactions
of museum visitors with two virtual characters that
serve as guides at the Museum of Science in Boston,
in order to build a realistic model of user behavior
when interacting with these characters. Based on
this analysis, we built a SU and used it for learning
the dialogue policy of the virtual characters using
RL. We compared our learned policy with two base-
lines, one of which was the dialogue policy of the
original system that was used for collecting the cor-
pus and that is currently installed at the Museum of
Science in Boston. Our learned policy outperformed
both baselines which shows that RL is a promising
technique for learning question-answering dialogue
policies.
Acknowledgments
This work was funded by the NSF grant #1117313.
The Twins corpus collection was supported by the
NSF grant #0813541.
References
Priti Aggarwal, Ron Artstein, Jillian Gerten, Athanasios
Katsamanis, Shrikanth Narayanan, Angela Nazarian,
and David Traum. 2012. The Twins corpus of mu-
91
seum visitor questions. In Proc. of the Language
Resources and Evaluation Conference (LREC), pages
2355?2361, Istanbul, Turkey.
Hua Ai and Diane Litman. 2008. Assessing dialog sys-
tem user simulation evaluation measures using human
judges. In Proc. of the Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL-HLT), pages 622?629, Columbus,
OH, USA.
Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jor-
dan. 2011. Empirically evaluating the application
of reinforcement learning to the induction of effective
and adaptive pedagogical strategies. User Modeling
and User-Adapted Interaction, 21(1-2):137?180.
Sudeep Gandhe, Nicolle Whitman, David Traum, and
Ron Artstein. 2009. An integrated authoring tool
for tactical questioning dialogue systems. In Proc. of
the IJCAI Workshop on Knowledge and Reasoning in
Practical Dialogue Systems, Pasadena, CA, USA.
Kallirroi Georgila and David Traum. 2011a. Learn-
ing culture-specific dialogue models from non culture-
specific data. In Proc. of HCI International, Lecture
Notes in Computer Science Vol. 6766, pages 440?449,
Orlando, FL, USA.
Kallirroi Georgila and David Traum. 2011b. Reinforce-
ment learning of argumentation dialogue policies in
negotiation. In Proc. of Interspeech, pages 2073?
2076, Florence, Italy.
Kallirroi Georgila, James Henderson, and Oliver Lemon.
2006. User simulation for spoken dialogue systems:
Learning and evaluation. In Proc. of Interspeech,
pages 1065?1068, Pittsburgh, PA, USA.
Kallirroi Georgila, Maria K. Wolters, and Johanna D.
Moore. 2010. Learning dialogue strategies from older
and younger simulated users. In Proc. of the Annual
SIGdial Meeting on Discourse and Dialogue (SIG-
dial), pages 103?106, Tokyo, Japan.
Peter A. Heeman. 2009. Representing the reinforcement
learning state in a negotiation dialogue. In Proc. of the
IEEE Automatic Speech Recognition and Understand-
ing Workshop (ASRU), Merano, Italy.
Arne Jo?nsson, Frida Ande?n, Lars Degerstedt, Annika
Flycht-Eriksson, Magnus Merkel, and Sara Norberg.
2004. Experiences from combining dialogue system
development with information access techniques. In
New Directions in Question Answering, Mark T. May-
bury (Ed), pages 153?164. AAAI/MIT Press.
Filip Jurc???c?ek, Blaise Thomson, and Steve Young. 2012.
Reinforcement learning for parameter estimation in
statistical spoken dialogue systems. Computer Speech
and Language, 26(3):168?192.
Anton Leuski and David Traum. 2010. Practical lan-
guage processing for virtual humans. In Proc. of the
22nd Annual Conference on Innovative Applications
of Artificial Intelligence (IAAI), Atlanta, GA, USA.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective question
answering characters. In Proc. of the Annual SIGdial
Meeting on Discourse and Dialogue (SIGdial), pages
18?27, Sydney, Australia.
Lihong Li, Jason D. Williams, and Suhrid Balakrishnan.
2009. Reinforcement learning for dialog management
using least-squares policy iteration and fast feature se-
lection. In Proc. of Interspeech, pages 2475?2478,
Brighton, United Kingdom.
Teruhisa Misu, Komei Sugiura, Kiyonori Ohtake, Chiori
Hori, Hideki Kashioka, Hisashi Kawai, and Satoshi
Nakamura. 2010. Modeling spoken decision making
dialogue and optimization of its dialogue strategy. In
Proc. of the Annual SIGdial Meeting on Discourse and
Dialogue (SIGdial), pages 221?224, Tokyo, Japan.
Rieks op den Akker, Harry Bunt, Simon Keizer, and
Boris van Schooten. 2005. From question answering
to spoken dialogue: Towards an information search as-
sistant for interactive multimodal information extrac-
tion. In Proc. of Interspeech, pages 2793?2796, Lis-
bon, Portugal.
Jan Peters and Stefan Schaal. 2008. Natural actor-critic.
Neurocomputing, 71(7-9):1180?1190.
Verena Rieser and Oliver Lemon. 2009. Does this list
contain what you were searching for? Learning adap-
tive dialogue strategies for interactive question an-
swering. Natural Language Engineering, 15(1):55?
72.
William Swartout, David Traum, Ron Artstein, Dan
Noren, Paul Debevec, Kerry Bronnenkant, Josh
Williams, Anton Leuski, Shrikanth Narayanan, Diane
Piepol, Chad Lane, Jacquelyn Morie, Priti Aggarwal,
Matt Liewer, Jen-Yuan Chiang, Jillian Gerten, Selina
Chu, and Kyle White. 2010. Ada and Grace: Toward
realistic and engaging virtual museum guides. In Proc.
of the International Conference on Intelligent Virtual
Agents (IVA), pages 286?300, Philadelphia, PA, USA.
Joel R. Tetreault and Diane J. Litman. 2008. A reinforce-
ment learning approach to evaluating state representa-
tions in spoken dialogue systems. Speech Communi-
cation, 50(8-9):683?696.
Blaise Thomson and Steve Young. 2010. Bayesian up-
date of dialogue state: A POMDP framework for spo-
ken dialogue systems. Computer Speech and Lan-
guage, 24(4):562?588.
Sebastian Varges, Fuliang Weng, and Heather Pon-Barry.
2009. Interactive question answering and constraint
relexation in spoken dialogue systems. Natural Lan-
guage Engineering, 15(1):9?30.
EllenM. Voorhees. 2001. The TREC question answering
track. Natural Language Engineering, 7(4):361?378.
92
Appendix
Features Features
average ASR accuracy of user queries if system correctly answered current user query
# user queries if system responded with off-topic prompt
to current user query
# correct system responses # times user repeated current query
# incorrect system responses # successive incorrect system responses
# off-topic system prompts # successive off-topic system prompts
% correct system responses # user queries for topic ?introduction?
% incorrect system responses # user queries for topic ?personal?
user type (child, male, female) # user queries for topic ?school?
if user asks example query 1 # user queries for topic ?technology?
if user asks example query 2 # user queries for topic ?interfaces?
if user asks example query 3 # user queries for topic ?exhibition?
if user asks example query 4 # user queries for other topics
if system correctly responds to example query 1 if system correctly responds to example query 3
if system correctly responds to example query 2 if system correctly responds to example query 4
# junk user queries previous topic category
Table 7: List of features used in predicting when the user will cease a session (Cease Dialogue), what the user will say
next (Say Next 1), and what the user will say next after removing repeated user queries (Say Next 2). Example query
1 is ?who are you named after??; example query 2 is ?are you a computer??; example query 3 is ?what do you like to
do for fun??; example query 4 is ?what is artificial intelligence??.
Cease Dialogue Say Next 1 Say Next 2
average ASR accuracy of previous topic category previous topic category
user queries
user type (child, male, female) # user queries for topic ?personal? # junk user queries
# off-topic system prompts # user queries # successive incorrect system
responses
# successive off-topic system # junk user queries if system correctly answered
prompts current user query
# incorrect system responses % correct system responses user type (child, male, female)
# user queries % incorrect system responses % incorrect system responses
# junk user queries # incorrect system responses % correct system responses
# user queries for other topics # user queries for other topics # incorrect system responses
if system responded with off-topic # correct system responses # off-topic system prompts
prompt to current user query
% correct system responses user type (child, male, female) # user queries
Table 8: List of the 10 most dominant features (in order of importance) in predicting when the user will cease a session
(Cease Dialogue), what the user will say next (Say Next 1), and what the user will say next after removing repeated
user queries (Say Next 2).
93
Proceedings of the SIGDIAL 2014 Conference, pages 22?31,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Situated Language Understanding at 25 Miles per Hour
Teruhisa Misu, Antoine Raux
?
, Rakesh Gupta
Honda Research Institute USA
425 National Avenue
Mountain View, CA 94040
tmisu@hra.com
Ian Lane
Carnegie Mellon University
NASA Ames Research Park
Moffett Field, CA 93085
Abstract
In this paper, we address issues in situ-
ated language understanding in a rapidly
changing environment ? a moving car.
Specifically, we propose methods for un-
derstanding user queries about specific tar-
get buildings in their surroundings. Unlike
previous studies on physically situated in-
teractions such as interaction with mobile
robots, the task is very sensitive to tim-
ing because the spatial relation between
the car and the target is changing while
the user is speaking. We collected situated
utterances from drivers using our research
system, Townsurfer, which is embedded
in a real vehicle. Based on this data, we
analyze the timing of user queries, spa-
tial relationships between the car and tar-
gets, head pose of the user, and linguis-
tic cues. Optimized on the data, our al-
gorithms improved the target identification
rate by 24.1% absolute.
1 Introduction
Recent advances in sensing technologies have en-
abled researchers to explore applications that re-
quire a clear awareness of the systems? dynamic
context and physical surroundings. Such appli-
cations include multi-participant conversation sys-
tems (Bohus and Horvitz, 2009) and human-robot
interaction (Tellex et al., 2011; Sugiura et al.,
2011). The general problem of understanding and
interacting with human users in such environments
is referred to as situated interaction.
We address yet another environment, where sit-
uated interactions takes place ? a moving car. In
the previous work, we collected over 60 hours of
in-car human-human interactions, where drivers
interact with an expert co-pilot sitting next to them
in the vehicle (Cohen et al., 2014). One of the
?
Currently with Lenovo.
insights from the analysis on this corpus is that
drivers frequently use referring expressions about
their surroundings. (e.g. What is that big building
on the right?) Based on this insight, we have de-
veloped Townsurfer (Lane et al., 2012; Misu et
al., 2013), a situated in-car intelligent assistant.
Using geo-location information, the system can
answer user queries/questions that contain object
references about points-of-interest (POIs) in their
surroundings. We use driver (user) face orienta-
tion to understand their queries and provide the re-
quested information about the POI they are look-
ing at. We have previously demonstrated and eval-
uated the system in a simulated environment (Lane
et al., 2012). In this paper, we evaluate its utility
in real driving situations.
Compared to conventional situated dialog tasks,
query understanding in our task is expected to be
more time sensitive, due to the rapidly changing
environment while driving. Typically, a car will
move 10 meters in one second while driving at 25
mi/h. So timing can be a crucial factor. In addi-
tion, it is not well understood what kind of linguis-
tic cues are naturally provided by drivers, and their
contributions to situated language understanding
in such an environment. To the best of our knowl-
edge, this is the first study that tackles the issue of
situated language understanding in rapidly moving
vehicles.
In this paper, we first present an overview of the
Townsurfer in-car spoken dialog system (Section
2). Based on our data collection using the sys-
tem, we analyze user behavior while using the sys-
tem focusing on language understanding (Section
3). Specifically, we answer the following research
questions about the task and the system through
data collection and analysis:
1. Is timing an important factor of situated lan-
guage understanding?
2. Does head pose play an important role in lan-
guage understanding? Or is spatial distance
information enough?
22
Speech recognition
Natural language understanding
Gaze (Head-pose) estimation
3) POI Posterior calculation            by Belief tracking
1) (Candidate)     POI look-up
Microphone
Depth sensor
Sensors Sensor signal understanding POI identification (situated understanding)
Speech
Gaze
2) POI score (prior) calculation
Understanding result (POI with maximum posterior)
Geo-location estimation Semantic         geo-spatial database
GPS
IMU
Geo-location
Figure 1: System overview of Townsurfer
Table 1: Example dialog with Townsurfer
U1: What is that place. (POI in gaze)
S1: This is Specialty Cafe, a mid-scale coffee
shop that serves sandwiches.
U2: What is its (POI in dialog history) rating.
S2: The rating of Specialty Cafe is above av-
erage.
U3: How about that one on the left.
(POI located on the left)
S3: This is Roger?s Deli, a low-priced restau-
rant that serves American food.
3. What is the role of linguistic cues in this task?
What kinds of linguistic cues do drivers nat-
urally provide?
Based on the hypothesis obtained from the analy-
sis for these questions, we propose methods to im-
prove situated language understanding (Section 4),
and analyze their contributions based on the col-
lected data (Sections 5 and 6). We then clarify our
research contributions through discussion (Section
7) and comparison with related studies (Section 8).
2 Architecture and Hardware of
Townsurfer
The system uses three main input modalities,
speech, geo-location, and head pose. Speech is
the main input modality of the system. It is used to
trigger interactions with the system. User speech
is recognized, then requested concepts/values are
extracted. Geo-location and head pose informa-
tion are used to understand the target POI of the
user query. An overview of the system with a pro-
cess flow is illustrated in Figure 1 and an exam-
ple dialog with the system is shown in Table 1. A
video of an example dialog is also attached.
In this paper, we address issues in identify-
ing user intended POI, which is a form of ref-
erence resolution using multi-modal information
sources
1
. The POI identification process consists
of the following three steps (cf. Figure 1). This
is similar to but different from our previous work
on landmark-based destination setting (Ma et al.,
2012).
1) The system lists candidate POIs based on geo-
location at the timing of a driver query. Rela-
tive positions of POIs to the car are also cal-
culated based on geo-location and the head-
ing of the car.
2). Based on spatial linguistic cues in the user
utterance (e.g. to my right, on the left), a
2D scoring function is selected to identify ar-
eas where the target POI is likely to be. This
function takes into account the position of the
POI relative to the car, as well as driver head
pose. Scores for all candidate POIs are cal-
culated.
3) Posterior probabilities of each POI are cal-
culated using the score of step 2 as prior,
and non-spatial linguistic information (e.g.
POI categories, building properties) as obser-
vations. This posterior calculation is com-
puted using our Bayesian belief tracker called
DPOT (Raux and Ma, 2011).
The details are explained in Section 4.
System hardware consists of a 3D depth sen-
sor (Primesense Carmine 1.09), a USB GPS (BU-
353S4), an IMU sensor (3DM-GX3-25) and a
close talk microphone (plantronics Voyage Leg-
1
We do not deal with issues in language understanding
related to dialog history and query type. (e.g. General infor-
mation request such as U1 vs request about specific property
of POI such as U2 in Table 1)
23
end UC). These consumer grade sensors are in-
stalled in our Honda Pilot experiment car. We
use Point Cloud Library (PCL) for the face direc-
tion estimation. Geo-location is estimated based
on Extended Kalman filter-based algorithm using
GPS and gyro information as input at 1.5 Hz. The
system is implemented based on the Robot Oper-
ating System ROS (Quigley et al., 2009). Each
component is implemented as a node of ROS, and
communications between the nodes are performed
using the standard message passing mechanisms
in ROS.
3 Data Collection and Analysis
3.1 Collection Setting
We collected data using a test route. The route
passes through downtown Mountain View
2
and
residential area around Honda Research Institute.
We manually constructed our database containing
250 POIs (businesses such as restaurants, compa-
nies) in this area. Each database entry (POI) has
name, geo-location, category and property infor-
mation explained in Section 3.4. POI geo-location
is represented as a latitude-longitude pair (e.g.
37.4010,-122.0539). Size and shape of buildings
are not taken into account. It takes about 30 min-
utes to drive the route. The major difference be-
tween residential area and downtown is the POI
density. While each POI in downtown has on aver-
age 7.2 other POIs within 50 meters, in residential
area POIs have only 1.9 neighbors. Speed limits
also differ between the two (35 mi/h vs 25 mi/h).
We collected data from 14 subjects. They were
asked to drive the test route and make queries
about surrounding businesses. We showed a demo
video
3
of the system to the users before starting the
data collection. We also told them that the objec-
tive is a data collection for a situated spoken dia-
log system, rather than the evaluation of the whole
system. We asked subjects to include the full de-
scription of the target POI within a single utterance
to avoid queries whose understanding requires di-
alog history information
4
. Although the system
answered based on the baseline strategy explained
in Section 4.1, we asked subjects to ignore the sys-
tem responses.
As a result, we collected 399 queries with a
valid target POI. Queries about businesses that do
2
We assumed that a POI is in downtown when it is located
within the rectangle by geo-location coordinates (37.3902, -
122.0827) and (37.3954, -122.0760).
3
not the attached one.
4
Understanding including dialog history information is
our future work.
POI
x
y ?
face direction
target 
direction
Heading 
direction
Figure 2: Parameters used to calculate POI score
(prior)
?  :   rightX:   left+:   no cue
Distance (m)y
RightLeft Distance (m)
x
Figure 3: Target POI positions
not exist on our database (typically a vacant store)
were excluded. The data contains 171 queries in
downtown and 228 in residential area. The queries
were transcribed and the user-intended POIs were
manually annotated by confirming the intended
target POI with the subjects after the data collec-
tion based on a video taken during the drive.
3.2 Analysis of Spatial Relation of POI and
Head Pose
We first analyze the spatial relation between posi-
tion cues (right/left) and the position of the user-
intended target POIs Out of the collected 399
queries, 237 (59.4%) of them contain either right
or left position cue (e.g. What is that on the left?).
The relation between the position cues (cf. Figure
2) and POI positions at start-of-speech timing
5
is
plotted in Figure 3. The X-axis is a lateral distance
(a distance in the direction orthogonal to the head-
ing; a positive value means the right direction) and
the Y-axis is an axial distance (a distance in the
heading direction; a negative value means the POI
is in back of the car. ). The most obvious finding
from the scatter plot is that right and left are pow-
5
Specifically, the latest GPS and face direction informa-
tion at that timing is used.
24
Table 2: Comparison of average and standard deviation of distance (in meter) of POI form the car
ASR result timing Start-of-speech timing
Position cue Site Ave dist. Std dist. Ave dist. Std dist.
Right/left Downtown 17.5 31.0 31.9 28.3
Residential 22.0 36.3 45.2 36.5
No right/left Downtown 17.4 27.8 31.1 26.5
cue Residential 38.3 45.9 52.3 43.4
Distance (m)y
?angular difference (degree)
Figure 4: Relation between POI positions and
head pose
erful cues for the system to identify target POIs.
We can also see that the POI position distribution
has a large standard deviation. This is partly be-
cause the route has multiple sites from downtown
and residential area. Interestingly, while the aver-
age distance to the target POI in downtown is 37.0
meters, that of residential area is 57.4 meters.
We also analyze the relation between face di-
rection and POI positions. Figure 4 plots the re-
lation between the axial distance and the angular
difference ? (between the user face direction and
the target POI direction) (cf. Figure 2). The scat-
ter plot suggests that the angular differences for
distant target POIs is often small. For close target
POIs the angular differences are larger and have a
large variance
6
.
3.3 Analysis of Timing
Referring expressions such as ?the building on the
right? must be resolved with respect to the context
in which the user intended. However, in a moving
car, such a context (i.e. the position of the car and
the situation in the surroundings) can be very dif-
ferent between the time when the user starts speak-
ing the sentence and the time they finish speaking
it. Therefore, situated understanding must be very
time sensitive.
To confirm and investigate this issue, we ana-
lyze the difference in the POI positions between
the time the ASR result is output vs the time the
user actually started speaking. The hypothesis is
6
We will discuss the reason for this in Section 6.2.
Table 3: User-provided linguistic cues
Category of linguistic cue Percentage
used (%)
Relative position to the car (right/left) 59.4
Business category (e.g. restaurant, cafe) 31.8
Color of the POI (e.g. green, yellow) 12.8
Cuisine (e.g. Chinese, Japanese, Mexican) 8.3
Equipments (e.g. awning, outside seating) 7.2
Relative position to the road (e.g. corner) 6.5
that the latter yields a more accurate context in
which to interpret the user sentence. In contrast,
our baseline system uses the more straightforward
approach of resolving expressions using the con-
text at the time of resolution, i.e. whenever the
ASR/NLU has finished processing an utterance
(hereafter ?ASR results timing?).
Specifically, we compare the average axial dis-
tance to the target POIs and its standard deviation
between these two timings. Table 2 lists these fig-
ures broken down by position cue types and sites.
The average axial distance from the car to the tar-
get POIs is often small at the ASR result timing,
but the standard deviation is generally small at the
start-of-speech timing. This indicates that the tar-
get POI positions at the start-of-speech timing is
more consistent across users and sentence lengths
than that at the ASR result timing. This result indi-
cates the presence of a better POI likelihood func-
tion using the context (i.e. car position and orien-
tation) at the start-of-speech timing than using the
ASR result timing.
3.4 Analysis of Linguistic Cues
We then analyze the linguistic cues provided by
the users. Here, we focus on objective and sta-
ble cues. We exclude subjective cues (e.g. big,
beautiful, colorful) and cues that might change in
a short period of time (e.g. with a woman dressed
in green in front). We have categorized the linguis-
tic cues used to describe the target POIs. Table 3
lists the cue types and the percentage of user utter-
ances containing each cue type.
The cues that the users most often provided con-
cern POI position related to the car (right and left).
Nearly 60% of queries included this type of cue
and every subject provided it at least once. The
second most frequent cue is category of business,
especially in downtown. Users also provided col-
25
ors of POIs. Other cues include cuisine, equip-
ments, relative position to the road (e.g. on the
corner).
Another interesting finding from the analysis is
that the users provided more linguistic cues with
increasing candidate POIs in their field of view.
Actually, the users provided 1.51 categories in av-
erage per query in downtown, while they provided
1.03 categories in residential area. (cf. POI den-
sity in Section 3.2: 7.2 vs 1.9) This indicates that
users provide cues considering environment com-
plexity.
4 Methods for Situated Language
Understanding
4.1 Baseline Strategy
We use our previous version (Misu et al., 2013)
as the baseline system for situated language un-
derstanding. The baseline strategy consists of the
following three paragraphs, which correspond to
the process 1)-3) in Section 2 and Figure 1.
The system makes a POI look-up based on the
geo-location information at the time ASR result
is obtained. The search range of candidate POIs
is within the range (relative geo-location of POIs
against the car location) of -50 to 200 meters in
the travelling direction and 100 meters to the left
and 100 meters to the right in the lateral direction.
The ASR result timing is also used to measure the
distances to the candidate POIs.
POI priors are calculated based on the distance
from the car (= axial distance) based on ?the closer
to the car the likely? principle. We use a likelihood
function inversely proportional to the distance. We
use position cues simply to remove POIs from a
list of candidates. For example ?right? position
cue is used to remove candidate POIs that are lo-
cated on< 0 position in the lateral distance. When
no right/left cue is provided, POIs outside of 45
degrees from the face direction are removed from
the list of candidates.
No linguistic cues except right/left are used to
calculate POI posterior probabilities. So, the sys-
tem selects the POI with the highest prior (POI
score) as the language understanding result.
4.2 Strategies Toward Better Situated
Language Understanding
To achieve better situated language understanding
(POI identification) based on the findings of the
analysis in Section 3, we modify steps 1)-3) as fol-
lows:
1. Using start-of-speech timing for the POI
prior calculation
Distance (m)y
?  :   rightX:   left
RightLeft Distance (m)x
Figure 5: Example GMM fitting
2. Gaussian mixture model (GMM)-based POI
probability (prior) calculation
3. Linguistic cues for the posterior calculation.
We use the start-of-speech timing instead of the
time ASR result is output. Because the standard
deviations of the POI distances are small (cf. Sec-
tion 3.2), we expect that a better POI probability
score estimation with the POI positions at this tim-
ing in the subsequent processes than the positions
at the ASR result timing. The POI look-up range
is the same as the baseline.
We apply Gaussian mixture model (GMM) with
diagonal covariance matrices over the input pa-
rameter space. The POI probability (prior) is cal-
culated based on these Gaussians. We use two in-
put parameters of the lateral and axial distances for
queries with right/left cue, and three parameters of
the lateral and axial distances and the difference
in degree between the target and head pose direc-
tions for queries without right/left cue. (The effect
of the parameters is discussed later in Section 6.2.)
We empirically set the number of Gaussian com-
ponents to 2. An example GMM fitting to the POI
positions for queries with right and left cues is il-
lustrated in Figures 5. The center of ellipse is the
mean of the Gaussian.
We use the five linguistic cue categories of Sec-
tion 3.4 for the posterior calculation by the belief
tracker. In the following experiments, we use ei-
ther 1 or 0 as a likelihood of natural language un-
derstanding (NLU) observation. The likelihood
for the category value is 1 if a user query (NLU
result) contains the target value, otherwise 0. This
corresponds to a strategy of simply removing can-
didate POIs that do not have the category values
specified by the user. Here, we assume a clean POI
database with all their properties annotated manu-
ally.
26
Table 4: Comparison of POI identification rate
Method Success
rate (%)
right/left linguistic cues,
the-closer-the-likely likelihood, 43.1
ASR result timing) (Baseline)
1) Start-of-speech timing 42.9
2) GMM-based likelihood 47.9
3) Linguistic cues 54.6
1) + 2) 50.6
1) + 3) 54.4
2) + 3) 62.2
1) + 2) + 3) 67.2
5 Experiments
We use manual transcriptions and natural language
understanding results of the user queries to focus
our evaluations on the issues listed in Section 1.
We evaluate the situated language understanding
(POI identification) performance based on cross
validation. We use the data from 13 users to train
GMM parameters and to define a set of possible
linguistic values, and the data from the remaining
user for evaluation. We train the model parameters
of the GMM using the EM algorithm. Knowledge
about the sites (downtown or residential area) is
not used in the training
7
.
We do not set a threshold for the presentation.
We judge the system successfully understands a
user query when the posterior of the target (user-
intended) POI is the highest. The chance rate,
given by the average of the inverse number of can-
didate POIs in the POI look-up is 10.0%.
6 Analysis of the Results
We first analyze the effect of our three methods
described in Section 4.2. The results are listed in
Table 4.
Simply using the POI positions at the start-of-
speech timing instead of those of the ASR result
timing did not lead to an improvement. This re-
sult is reasonable because the distances to target
POIs are often smaller at the ASR result timing
as we showed in Table 2. However, we achieved
a better improvement (7.5% over the baseline) by
combining it with the GMM-based likelihood cal-
culation. The results supports our Section 3.3 hy-
pothesis that the POI position is less dependent
on users/scenes at the start-of-speech timing. The
linguistic cues were the most powerful informa-
7
The performance was better when the knowledge was not
used.
Confusion
Linguistic cue
Localization error
User error
Figure 6: Breakdown of error causes
tion for this task. The improvement over the base-
line was 11.5%. By using these three methods to-
gether, we obtained more than additive improve-
ment of 24.1% in the POI identification rate over
the baseline
8
. The success rates per site were
60.8% in downtown and 71.9% in residential area.
6.1 Error Analysis
To analyze the causes of the remaining errors, we
have categorized the errors into the following four
categories:
1. Ambiguous references: There were multi-
ple POIs that matched the user query. (e.g.
another yellow building sat next to the target)
2. Linguistic cue: The driver used undefined
linguistic cues such subjective expressions or
dynamic references objects (e.g. optometrist,
across the street, colorful)
3. Localization error: Errors in estimating
geo-location or heading of the car.
4. User error: There were errors in the user
descriptions (e.g. user misunderstood the
neighbor POI?s outside seating as the tar-
get?s)
The distribution of error causes is illustrated in
Figure 6. More than half of the errors are due
to reference ambiguity. These errors are expected
to be resolved through clarification dialogs. (e.g.
asking user ?Did you mean the one in front or
back??) Linguistic errors might be partly resolved
by using a better database with detailed category
information. For dynamic references and subjec-
tive cues, use of image processing techniques will
help. Localization errors can be solved by using
high-quality GPS and IMU sensors. User errors
were rare and only made in downtown.
6.2 Breakdown of Effect of the Spatial
Distance and Head Pose
We then evaluate the features used for the POI
prior calculation to investigate the effect of the in-
put parameters of the lateral and axial distances
8
For reference, the performances of ?1) + 2) + 3)? were
62.9%, 67.2%, 66.1%, 67.2%, and 66.2% when the number
of Gaussian components were 1, 2, 3, 4, and 5.
27
Table 5: Relation between the parameters used for
the POI identification and success rates (%)
query type
parameters used right/left no cue
lateral (x) distance 58.6 51.2
axial (y) distance 59.5 53.7
face direction 43.3 44.4
lateral + axial (x+ y) 73.8 54.3
lateral (x) + face direction 57.8 48.1
axial (y) + face direction 59.1 54.9
lateral + axial + face 68.4 57.4
and the difference in degree between the target
and user face direction angles. Table 5 lists the
relationship between the parameters used for the
GMM-based likelihood calculation and the POI
identification performances
9
.
The results indicate that the axial distance is
the most important parameter. We got a slight
improvement by using the face direction informa-
tion for the queries without right/left cue, but the
improvement was not significant. On the other
hand, use of face direction information for the
right/left queries clearly degraded the POI iden-
tification performance. We think this is because
the users finished looking at the POI and returned
the face to the front when they started speaking,
thus they explicitly provided right/left information
to the system. However, we believe that using a
long-term trajectory of the user face direction will
contribute to an improve in the POI identification
performance.
6.3 Breakdown of the Effect of Linguistic
Cues
We then evaluate the effect of the linguistic cues
per category. Table 6 lists the relationship between
the categories used for the posterior calculation
and the success rates. There is a strong correlation
between the frequency of the cues used (cf. Table
3) and their contributions to the improvement in
success rate. For example, business category in-
formation contributed the most, boosting the per-
formance by 8.5%.
Another point we note is that the contribution of
business category and cuisine categories is large.
Because other categories (e.g. color) are not read-
ily available in a public POI database (e.g. Google
Places API, Yelp API), we can obtain reasonable
performance without using a special database or
9
Note that, we first determine the function to calculate
POI scores (priors) based on the position cues, then calculate
scores with the selected function.
Table 6: Effect of linguistic cues
linguistic cue Success
category used rate (%)
No linguistic cues (*) 50.6
(*) + Business category (e.g. cafe) 59.1
(*) + Color of the POI (e.g. green) 57.6
(*) + Cuisine (e.g. Chinese) 54.1
(*) + Equipments (e.g. awning) 53.9
(*) + Relative position (e.g. corner) 51.4
image processing.
We also found that linguistic cues were espe-
cially effective in downtown. Actually, while the
improvement
10
was 20.0% in downtown that for
residential area was 14.4%. This mainly would be
because the users provided more linguistic cues in
downtown considering the difficulty of the task.
6.4 Using Speech Recognition Results
We evaluate the degradation by using automatic
speech recognition (ASR) results. We use Google
ASR
11
and Julius (Kawahara et al., 2004) speech
recognition system with a language model trained
from 38K example sentences generated from a
grammar. An acoustic model trained from the
WSJ speech corpus is used. Note that they are
not necessarily the best system for this domain.
Google ASR uses a general language model for
dictation and Julius uses a mismatched acoustic
model in terms of the noise condition.
The query success rate was 56.3% for Julius and
60.3% for Google ASR. We got ASR accuracies
of 77.9% and 80.4% respectively. We believe the
performance will improve when N-best hypothe-
ses with confidence scores are used in the posterior
calculating using the belief tracker.
7 Discussion
The main limitation of this work comes from the
small amount of data that we were able to collect.
It is not clear how the results obtained here would
generalize to other sites, POI density, velocities
and sensor performances. Also, results might de-
pend on experimental conditions, such as weather,
hour, season. Hyper-parameters such as the opti-
mal number of Gaussian components might have
to be adapted to different situations. We there-
fore acknowledge that the scenes we experimented
are only a limited cases of daily driving activities.
10
1) + 2) vs 1) + 2) + 3).
11
Although it is not realistic to use cloud-based speech
recognition system considering the current latency, we use
this as a reference system.
28
However, the methods we propose are general and
our findings should be verifiable without loss of
generality by collecting more data and using more
input parameters (e.g. velocity) for the POI prior
calculation.
In addition, much future work remains to realize
a natural interaction with the system, such as tak-
ing into account dialog history and selecting opti-
mal system responses. On the other hand, we be-
lieve this is one of the best platform to investigate
situated interactions. The major topics that we are
going to tackle are:
1. Dialog strategy: Dialog strategy and system
prompt generation for situated environments
are important research topics, especially to
clarify the target when there is ambiguity as
mentioned in Section 6.1. The topic will in-
clude an adaptation of system utterances (en-
trainment) to the user (Hu et al., 2014).
2. Eye tracker: Although we believe head pose
is good enough to estimate user intentions be-
cause we are trained to move the head in driv-
ing schools to look around to confirm safety,
we would like to confirm the difference in
this task between face direction and eye-gaze.
3. POI identification using face direction trajec-
tory: Our analysis showed that the use of face
direction sometimes degrades the POI identi-
fication performance. However, we believe
that using a trajectory of face direction will
change the result.
4. Database: We assumed a clean and perfect
database but we are going to evaluate the per-
formance when noisy database is used. (e.g.
A database based on image recognition re-
sults or user dialog log.)
5. Feedback: Koller et al. (2012) demonstrated
referential resolution is enhanced by giving
gaze information feedback to the user. We
would like to analyze the effect of feedback
with an automotive augmented reality envi-
ronment using our 3D head-up display (Ng-
Thow-Hing et al., 2013).
8 Related Work
The related studies include a landmark-based nav-
igation that handles landmarks as information for
a dialog. Similar system concepts have been
provided for pedestrian navigation situations (Ja-
narthanam et al., 2013; Hu et al., 2014), they do
not handle a rapidly changing environment.
Several works have used timing to enhance
natural interaction with systems. Rose and
Horvitz (2003) and Raux and Eskenazi (2009)
used timing information to detect user barge-ins.
Studies on incremental speech understanding and
generation (Skantze and Hjalmarsson, 2010; Deth-
lefs et al., 2012) have proved that real-time feed-
back actions have potential benefits for users.
Komatani et al. (2012) used user speech timing
against user?s previous and system?s utterances
to understand the intentions of user utterances.
While the above studies have handled timing fo-
cusing on (para-)linguistic aspect, our work han-
dles timing issues in relation to the user?s physical
surroundings.
Recent advancements in gaze and face direction
estimation have led to better user behavior under-
standing. There are a number of studies that have
analyzed relationship between gaze and user in-
tention, such as user focus (Yonetani et al., 2010),
preference (Kayama et al., 2010), and reference
expression understanding (Koller et al., 2012), be-
tween gaze and turn-taking (Jokinen et al., 2010;
Kawahara, 2012). Nakano et al. (2013) used face
direction for addressee identification. The previ-
ous studies most related to ours are reference res-
olution methods by Chai and Prasov (2010), Iida
et al. (2011) and Kennington et al. (2013). They
confirmed that the system?s reference resolution
performance is enhanced by taking the user?s eye
fixation into account. However, their results are
not directly applied to an interaction in a rapidly
changing environment while driving, where eye
fixations are unusual activities.
Marge and Rudnicky (2010) analyzed the effect
of space and distance for spatial language under-
standing for a human-robot communication. Our
task differs with this because we handle a rapidly
changing environment. We believe we can im-
prove our understanding performance based on
their findings.
9 Conclusion
We addressed situated language understanding in
a moving car. We focused on issues in understand-
ing user language of timing, spatial distance, and
linguistic cues. Based on the analysis of the col-
lected user utterances, we proposed methods of us-
ing start-of-speech timing for the POI prior calcu-
lation, GMM-based POI probability (prior) calcu-
lation, and linguistic cues for the posterior calcula-
tion to improve the accuracy of situated language
understanding. The effectiveness of the proposed
methods was confirmed by achieving a significant
improvement in a POI identification task.
29
10 Acknowledgments
The authors would like to thank Yi Ma at Ohio
State University for his contributions to the devel-
opment of HRItk.
References
D. Bohus and E. Horvitz. 2009. Models for Multi-
party Engagement in Open-World Dialog. In Proc.
SIGDIAL, pages 225?234.
J. Chai and Z. Prasov. 2010. Fusing eye gaze with
speech recognition hypotheses to resolve exophoric
reference in situated dialogue. In Proc. EMNLP.
D. Cohen, A. Chandrashekaran, I. Lane, and A. Raux.
2014. The hri-cmu corpus of situated in-car interac-
tions. In Proc. IWSDS, pages 201?212.
N. Dethlefs, H. Hastie, V. Rieser, and O. Lemon. 2012.
Optimising incremental dialogue decisions using in-
formation density for interactive systems. In Proc.
EMNLP, pages 82?93.
Z. Hu, G. Halberg, C. Jimenez, and M. Walker. 2014.
Entrainment in pedestrian direction giving: How
many kinds of entrainment? In Proc. IWSDS, pages
90?101.
R. Iida, M. Yasuhara, and T. Tokunaga. 2011. Multi-
modal reference resolution in situated dialogue by
integrating linguistic and extra-linguistic clues. In
Proc. IJCNLP, pages 84?92.
S. Janarthanam, O. Lemon, X. Liu, P. Bartie, W. Mack-
aness, and T. Dalmas. 2013. A multithreaded con-
versational interface for pedestrian navigation and
question answering. In Proc. SIGDIAL, pages 151?
153.
K. Jokinen, M. Nishida, and S. Yamamoto. 2010. On
eye-gaze and turn-taking. In Proc. EGIHMI.
T. Kawahara, A. Lee, K. Takeda, K. Itou, and
K. Shikano. 2004. Recent Progress of Open-Source
LVCSR Engine Julius and Japanese Model Reposi-
tory. In Proc. ICSLP, volume IV.
T. Kawahara. 2012. Multi-modal sensing and analysis
of poster conversations toward smart posterboard. In
Proc. SIGDIAL.
K. Kayama, A. Kobayashi, E. Mizukami, T. Misu,
H. Kashioka, H. Kawai, and S. Nakamura. 2010.
Spoken Dialog System on Plasma Display Panel Es-
timating User?s Interest by Image Processing. In
Proc. 1st International Workshop on Human-Centric
Interfaces for Ambient Intelligence (HCIAmi).
C. Kennington, S. Kousidis, and D. Schlangen. 2013.
Interpreting situated dialogue utterances: an update
model that uses speech, gaze, and gesture informa-
tion. In Proc. SIGDIAL.
A. Koller, K. Garoufi, M. Staudte, and M. Crocker.
2012. Enhancing referential success by tracking
hearer gaze. In Proc. SIGDIAL, pages 30?39.
K. Komatani, A. Hirano, and M. Nakano. 2012. De-
tecting system-directed utterances using dialogue-
level features. In Proc. Interspeech.
I. Lane, Y. Ma, and A. Raux. 2012. AIDAS - Immer-
sive Interaction within Vehicles. In Proc. SLT.
Y. Ma, A. Raux, D. Ramachandran, and R. Gupta.
2012. Landmark-based location belief tracking in
a spoken dialog system. In Proc. SIGDIAL, pages
169?178.
M. Marge and A. Rudnicky. 2010. Comparing Spo-
ken Language Route Instructions for Robots across
Environment Representations. In Proc. SIGDIAL,
pages 157?164.
T. Misu, A. Raux, I. Lane, J. Devassy, and R. Gupta.
2013. Situated multi-modal dialog system in vehi-
cles. In Proc. Gaze in Multimodal Interaction, pages
25?28.
Y. Nakano, N. Baba, H. Huang, and Y. Hayashi.
2013. Implementation and evaluation of a multi-
modal addressee identification mechanism for mul-
tiparty conversation systems. In Proc. ICMI, pages
35?42.
V. Ng-Thow-Hing, K. Bark, L. Beckwith, C. Tran,
R. Bhandari, and S. Sridhar. 2013. User-centered
perspectives for automotive augmented reality. In
Proc. ISMAR.
M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote,
J. Leibs, R. Wheeler, and A. Ng. 2009. ROS:
an open-source Robot Operating System. In Proc.
ICRA Workshop on Open Source Software.
A. Raux and M. Eskenazi. 2009. A Finite-state Turn-
taking Model for Spoken Dialog Systems. In Proc.
HLT/NAACL, pages 629?637.
A. Raux and Y. Ma. 2011. Efficient probabilistic track-
ing of user goal and dialog history for spoken dialog
systems. In Proc. Interspeech, pages 801?804.
R. Rose and H. Kim. 2003. A hybrid barge-in proce-
dure for more reliable turn-taking in human-machine
dialog systems. In Proc. Automatic Speech Recog-
nition and Understanding Workshop (ASRU), pages
198?203.
G. Skantze and A. Hjalmarsson. 2010. Towards incre-
mental speech generation in dialogue systems. In
Proc. SIGDIAL, pages 1?8.
K. Sugiura, N. Iwahashi, H. Kawai, and S. Nakamura.
2011. Situated spoken dialogue with robots using
active learning. Advance Robotics, 25(17):2207?
2232.
30
Table 7: Example user utterances
- What is that blue restaurant on the right?
- How about this building to my right with outside seating?
- What is that Chinese restaurant on the left?
- Orange building to my right.
- What kind of the restaurant is that on the corner?
- The building on my right at the corner of the street.
- What about the building on my right with woman with a jacket in front
- Do you know how good is this restaurant to the left?
- Townsurfer, there is an interesting bakery what is that?
- Is this restaurant on the right any good?
S. Tellex, T. Kollar, S. Dickerson, M. Walter, A. Baner-
jee, S. Teller, and N. Roy. 2011. Understanding nat-
ural language commands for robotic navigation and
mobile manipulation. In Proc. AAAI.
R. Yonetani, H. Kawashima, T. Hirayama, and T. Mat-
suyama. 2010. Gaze probing: Event-based estima-
tion of objects being focused on. In Proc. ICPR,
pages 101?104.
11 Appendix
Test route:
https://www.google.com/maps/
preview/dir/Honda+Research+
Institute,+425+National+Ave+
%23100,+Mountain+View,+CA+
94043/37.4009909,-122.0518957/
37.4052337,-122.0565795/37.
3973374,-122.0595982/37.4004787,
-122.0730021/Wells+Fargo/37.
4001639,-122.0729708/37.3959193,
-122.0539449/37.4009821,-122.
0540093/@37.3999836,-122.
0792529,14z/data=!4m21!4m20!
1m5!1m1!1s0x808fb713c225003d:
0xcf989a0bb230e5c0!2m2!
1d-122.054006!2d37.401016!
1m0!1m0!1m0!1m0!1m5!1m1!1s0x0:
0x86ca9ba8a2f15150!2m2!1d-122.
082546!2d37.388722!1m0!1m0!1m0!
3e0
31
