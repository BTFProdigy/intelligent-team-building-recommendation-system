An Integrated Method for Chinese Unknown Word Extraction1 
LUO Zhiyong 
College of Computer Science
Beijing University of 
Technology 
Beijing, PRC 100022 
Center for Language 
Information Processing 
Beijing Language and Culture 
University 
Beijing, PRC 100083 
luo_zy@blcu.edu.cn 
SONG Rou 
Center for Language 
Information Processing 
Beijing Language and Culture 
University 
Beijing, PRC 100083 
songrou@blcu.edu.cn 
                                                           
1 This paper is supported by NSFC (60272055) and 863 Project (2001AA114111) 
 
Abstract 
Unknown word recognition is an important 
problem in Chinese word segmentation systems. 
In this paper, we propose an integrated method 
for Chinese unknown word extraction for off-
line corpus processing, in which both context-
entropy (on each side) and frequency ratio 
against background corpus are introduced to 
evaluate the candidate words. Both of the meas-
ures are computed efficiently on Suffix array 
with much less space overhead. Our method can 
also be reinforced when combined with a basic 
Segmentor by boundary-verification and arbi-
trary n-gram words can be extracted by our 
method. We test our method on Chinese novel 
Xiao Ao Jiang Hu, and obtain satisfactory 
achievements compared to traditional criteria 
such as Likelihood Ratio. 
1 Introduction 
The unique feature of Chinese writing system is 
that it is character-based, not word-based. The fact 
that there are no delimiters between words poses the 
well-known problem of word segmentation. Any 
Chinese Information Processing (CIP) systems be-
yond character level, such as information retrieval, 
automatic proofreading, text classification, text-to-
speech conversion, syntactic parser, information ex-
traction and machine translation, etc. should have a 
built-in word segmentation block. Currently, dic-
tionary-based method is the basic and efficient one 
for word segmentation. A fixed Chinese electronic 
dictionary is required for most CIP systems. Yet 
there are many unknown words (out of the fixed dic-
tionary) coming into being all the time. The un-
known words are diverse, including proper nouns 
(person names, place names, organization names, 
etc.), domain-specific terminological nouns and ab-
breviations, even author-coined terms, etc. and they 
appear frequently in real text. This may cause ambi-
guity in Chinese word segmentation and lead to er-
rors in the applications. Presently, many systems 
(Tan et al 1999), (Liu, 2000), (Song, 1993), (Luo et 
al, 2001) focus on online recognition of proper 
nouns, and have achieved inspiring results in news-
corpus but will be deteriorated in special text, such 
as spoken corpus, novels. As to the rests of unknown 
words types, it is still the obstacle of application sys-
tems, although they are really important for specific 
collections of texts.  
For instance, according to our count on Chinese 
novel Xiao Ao Jiang Hu (??????) (JIN Yong 
(??), 1967), there are almost 515 unknown word 
types (out of our 243,539-item general dictionary) of 
total 39,404 occurrences and total 112,654 charac-
ters, and there are 983,134 characters overall in this 
novel (that is, about 11.46% characters of the whole 
novel are occupied by unknown words.). And most 
of them, such as ??????(person name), ???
?? ?(normal noun), ????? ?(organization 
name), etc. can?t be recognized by most current CIP 
systems. It is important to note that without efficient 
unknown word extraction method, most CIP systems 
can?t obtain satisfactory results. 
2 Relative research works 
Offline unknown word extraction can be treated as 
a special kind of Automatic Term Extraction (ATE). 
There are many research works on ATE. And most 
successful systems are based on statistics. Many sta-
tistical metrics have been proposed, including point-
wise mutual information (MI) (Church et al 1990), 
mean and variance, hypothesis testing (t-test, chi-
square test, etc.), log-likelihood ratio (LR) (Dunning, 
1993), statistic language model (Tomokiyo, et al 
2003), and so on. Point-wise MI is often used to find 
interesting bigrams (collocations). However, MI is 
actually better to think of it as a measure of inde-
pendence than of dependence (Manning et al 1999). 
LR is one of the most stable methods for ATE so far, 
and more appropriate for sparse data than other met-
rics. However, LR is still biased to two frequent 
words that are rarely adjacent, such as the pair (the, 
the) (Pantel et al 2001). On the other aspect, MI and 
LR metrics are difficult to extend to extract multi-
word terms.  
Relative frequency ratio (RFR) of terms between 
two different corpora can also be used to discover 
domain-oriented multi-word terms that are charac-
teristic of a corpus when compared with another 
(Damerau, 1993). In this paper, RFR values between 
source corpus and background one will be used to 
rank the final candidate-list. 
There are also many hybrid methods combined 
statistical metrics with linguistic knowledge, such as 
Part-of-Speech filters (Smadja, 1994). But POS fil-
ters are not appropriate for Chinese term extraction. 
Since all the terms extraction approaches need to 
access all the possible patterns and find their fre-
quency of occurrence, a highly efficient data struc-
ture based on PAT-tree (Chien, 1997), (Chien, 1998) 
and (Thian et al 1999) has been used popularly for 
this purpose. However, PAT-tree still has much 
space overhead, and is very expensive for construc-
tion. Now, we introduce an alternative data structure 
as Suffix array, with much less space overhead, to 
commit this task. 
In this paper, we propose a four-phase offline un-
known word extraction method: (a) Construct the 
Suffix arrays of source text and background corpus. 
In this phase, Suffix arrays, sorted on both left and 
right sides context for each occurrence of Chinese 
character, are constructed. We call them Left-index 
and Right-index respectively; (b) Extract frequent n-
gram candidate terms. In this phase, firstly we ex-
tract n-grams, appearing more than one time in dif-
ferent contexts according to Left-index and Right-
index of source text, into Left-list and Right-list re-
spectively. Then, we combine Left-list with Right-
list, and extract n-grams which appear in both of 
them as candidates (C-list, for short). We also com-
pute frequency, context-entropy and relative fre-
quency ratio against background corpus for each 
candidate in this phase; (c) Filter candidates in C-list 
with context-entropy and boundary-verification cou-
pled with General Purpose Word Segmentation Sys-
tem (GPWS) (Lou et al 2001). In this phase, we 
segment each sentence, where each candidate ap-
pears, in the source text with GPWS and eliminate 
the candidates cross word boundary; (d) Output the 
final terms on relative frequency ratios. 
The remainder of our paper is organized as fol-
lows: Section 2 describes the candidate terms extrac-
tion approach on Suffix array. Section 3 describes 
the candidates? filter approach on context-entropy 
and boundary-verification coupled with GPWS. Sec-
tion 4 describes the relative frequency ratios and 
output of the final list. Section 5 gives our experi-
mental result and Section 6 gives conclusion and 
future work. 
3 Candidates extraction on Suffix array 
Suffix array (also known as String PAT-
array)(Manber et al 1993) is a compact data struc-
ture to handle arbitrary-length strings and performs 
much powerful on-line string search operations such 
as the ones supported by PAT-tree, but has less 
space overhead.  
Definition 1. Let X = x0x1x2..xn-1xn as a string of 
length n. For the sake of left and right context sort-
ing, we have extended X by inserting two unique 
terminators ($, less than all of the characters) as sen-
tinel symbols at both ends of it, i.e. x0 = xn = $ in X. 
Let LSi = xixi-1..x0 (RSi = xixi+1..xn) as the left (right) 
suffix of X that starts at position i. 
The Suffix array Left-index[0..n] (Right-
index[0..n]) is an array of indexes of LSi (RSi), 
where LSLeft-index[i] < LSLeft-index[j] (RSRight-index[i] < 
RSRight-index[j]), i<j, in lexicological order. 
Let LLCP[i] (RLCP[i]), i=0..n-1, as the length of 
Longest Common Prefix (LCP) between two adja-
cent suffix strings, LSLeft-index[i] and           LSLeft-
index[i+1] (RSRight-index[i] and RSRight-index[i+1]). These ar-
rays on both sides are assistant data structures for 
speeding string search. 
Figure 1 shows a simple Suffix array sorted on left 
and right context, coupled with the LCP arrays re-
spectively.  
We apply the sort-algorithm proposed by (Manber 
et al 1993), which takes O(nlogn) in worst cases 
performance, to construct the Suffix arrays, and sort 
all the suffix strings in UNICODE order. 
Figure 2 shows fragments of Suffix arrays of test 
corpus Xiao Ao Jiang Hu in readable style.  
Sorted suffix arrays have clustered all similar n-
grams (of arbitrary length) into continuous blocks 
and the frequent string patterns, as the longest com-
mon prefix (LCP) of adjacent strings, can be ex-
tracted by scanning through the suffix arrays sorted 
on left context and right respectively.  
 
 
 
 
 
 
 
 
 
 
 
 
String ?tobeornottobe? 
# 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 
String $ t o b e o r n o t t o b e $ 
 
Suffix array 
# 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 
Left-index 0 14 3 12 4 13 7 5 8 2 11 6 1 9 10 
Right-index 0 14 12 3 13 4 7 11 2 5 8 6 10 1 9 
 
LCP arrays on both sides 
# 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 
LLCP 0 0 3 0 4 0 0 1 1 2 0 0 1 1 / 
RLCP 0 0 2 0 1 0 0 3 1 1 0 0 4 1 / 
Figure 1: Suffix array example 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
*Left part is fragment of Left Suffix array, starts at position of Chinese character ??? 
*Right part is fragment of Right Suffix array, starts at position of Chinese character ??? 
 
Figure 2: Fragments of Suffix array of Xiao Ao Jiang Hu 
 
?????????????????????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
?????????????????????? 
?????????????????????? 
?????????????????????? 
?????????????????????? 
?????????????????????? 
?????????????????????? 
?????????????????????? 
?????????????????????? 
?????????????????????? 
?? ???????????????????? 
?????????????????????? 
?????????????????????? 
?????????????????????? 
?????????????????????? 
?????????????????????? 
?????????????????????? 
?????????????????????? 
??????????????????????? 
?????????????????????? 
?????????????????????? 
As show in Figure 2, on right sorted part which 
starts at the position of Chinese Character ???, we 
can extract the repeated n-grams, such as ????
???, ???????, ???????, ?????
??, ????????, ??????, etc., in turn and 
skip many substrings, such as ????, ?????, 
etc., because they are not the LCP of adjacent suffix 
strings and only appear in the upper string ????
?? for their all occurrences. We can apply the same 
skill on left sorted part which start at the position of 
Chinese character ???, and extract ???????, 
???????, ???????, ????????
??, ????????, ??????, etc., as re-
peated n-grams and skip many substrings, such as 
????, ?????, etc., for the same reasons.  
To extract candidate terms, we can scan through 
both left and right Suffix arrays and select all re-
peated n-grams into Left-list and Right-list respec-
tively. The terms, which appear in both lists, can be 
treated as candidates (denoted by C-list). Extraction 
procedure can be done efficiently by coupled with 
the arrays of length of LCP on both sides via stack 
operations. The length and frequency of candidates 
can also be computed in this procedure. 
For example in Figure 2, term ?????? should 
appear in both Left-list and Right-list, and it is a 
good candidate. Yet n-grams ??????? is not a 
candidate because even though ??????? does 
appear in Right-list, it does not exist in our final 
Left-list (It always appears as a substring of direct 
upper string ???????? according to right 
part of Figure 2). 
Term TC 
Left 
Con-
text-
entropy 
Right 
Con-
text-
entropy 
RFR 
??? 5922 6.6804 4.9900 22743.7
?? 1267 4.7974 3.8534 0.9 
??? 1184 5.9656 4.8688 10104.9
?? 1123 4.8512 4.1473 1.0 
?? 1053 5.5446 4.7758 89.8 
??? 929 5.7310 4.7623 7928.6
??? 919 5.5887 4.5220 7843.2
?? 532 0.0930 4.4570 170.2 
?? 528 5.5960 0.0412 1013.9
??? 525 5.5891 4.4294 4480.6
???? 320 4.6805 4.8253 2731.0
??? 284 4.0897 0.0585 2423.8
???? 281 4.0624 3.7344 2398.2
??? 176 4.3386 4.0105 1502.0
???? 156 1.7374 2.0613 1331.3
???? 153 4.6941 4.4650 1305.7
??? 103 4.3266 3.4258 879.0 
??? 97 4.2815 3.1410 827.8 
??? 80 3.0207 2.7821 682.7 
???? 73 3.6620 3.9186 623.0 
Table 1: Examples of candidates order by TC 
Table 1 lists many examples of candidates ex-
tracted from Xiao Ao Jiang Hu, order by term count 
(TC).  
4 Filter candidate terms 
As what show in Table 1, not all the terms in C-
list extracted in Section 3 can be treated as signifi-
cant terms because of their incomplete lexical 
boundaries. There two kinds of incomplete-
boundary terms: (1) terms as substring of significant 
terms; (2) terms overlapping the boundaries of adja-
cent significant terms. In this section, we will take 
measures, including Context-entropy test and 
boundary-verification with common Segmentor 
(GPWS) with general lexicon, to eliminate these 
invalid candidates respectively. 
4.1 Measure on Context-entropy 
According to our investigation, significant terms 
in specific collection of texts can be used frequently 
and in different contexts. On the other hand sub-
string of significant term almost locates in its corre-
sponding upper string (that is, in fixed context) even 
through it occur frequently. In this part, we propose 
a metric Context-entropy as a measure of this fea-
ture to filter out substrings of significant terms.  
Definition 2. Assume ?  as a candidate term 
which appears n times in corpus X, ? = 
{a1,a2,?,as}(?= {b1,b2,?,bt}) as a set of left (right) 
side contexts of ? in X.  
Left and right Context-entropy of ? in X can be 
define as: 
?
?
=
?
???
ia
i
i n
aCaC
n
LCE ),(log),(1)(
,
?
?
=
?
???
ib
i
i n
bCbC
n
RCE ),(log),(1)(
 
where ??
??
==
??
??
ii b
i
a
i bCaCn ),(),( ,   
C(ai,?) (C(?,bi)) is count of concurrence of ai and 
? (? and bi) in X. 
Significant terms, which can be used in different 
context, will get high values of Context-entropy on 
both sides. And the substrings, which almost 
emerge because of their upper strings, will get com-
parative low values. The 3rd and 4th columns of 
Table 1 show the values of Context-entropy on both 
sides of a list of candidate terms. Many candidates, 
which almost emerge because of their direct upper 
strings, such as ????(in ?????(person name)), 
????(in ?????(person name)) , ?????(in 
??????(organization name)), appear in rela-
tively fixed contexts and should get much lower 
value(s) of one or both sides of Context-entropy.  
4.2 Boundary-verification with GPWS 
The candidate list of terms includes all of the n-
grams, which appear in different context on both 
sides more than ones. The unique feature of Chinese 
writing system is that there are no delimiters be-
tween words poses a big problem: Many of candi-
date terms are invalid because of the overlapped 
factual words? boundary, i.e. these candidates in-
clude several fragments of adjacent words, such as 
????(overlapping the boundary of common word 
????(Hua Mountain)), ?????(overlapping the 
boundary of common word ????(Sir)), etc. listed 
in Table 2. We eliminate these candidates by verify-
ing boundaries of them with a common Segmentor 
(GPWS (Lou et al 2001)) and a general lexicon 
(with 243,539 words).  
GPWS was built as shared framework undertak-
ing different CIP applications. It has achieved very 
good performance and great adaptability across dif-
ferent application domains in disambiguation, 
identification of proper nouns (including Chinese 
names, Chinese place names, translated names of 
foreigners, organization and company names, etc.), 
identification of high-frequency suffix phrases and 
numbers. In this part, we ONLY use the utilities of 
GPWS to perform the Maximum Match (MM) to 
find the boundaries of words in lexicon, and all of 
the unknown words (out of our lexicon) will be seg-
mented into pieces. Coupled with GPWS, we 
propose a voting mechanism for boundary-
verification as follows: 
For each candidate term in C-list as term 
Begin 
Declare falseNum as integer for the number of invalid 
boundary-check of term; 
Declare trueNum as integer for the number of valid 
boundary-check of term; 
falseNum = 0; 
trueNum = 0; 
For each sentence, in which term appears, in fore-
ground corpus, as sent 
Begin 
Segment sent with GPWS; 
Compare the term?s position in sent with the 
segment result of GPWS; 
If term crosses the adjacent words boundary 
Set falseNum = falseNum+1; 
Else 
Set trueNum = trueNum+1; 
End 
If  falseNum > trueNum 
Set boundary-verification flag of term to FALSE; 
Else 
Set boundary-verification flag of term to TRUE; 
End 
Assistant with the segmentor, we eliminate 
38,697 items of total 117,807 in C-list in 96.85% of 
precision. Table 2 shows many examples of candi-
dates eliminated by sides-verification with GPWS. 
Candidate 
term 
Segment result of GPWS for 
one sentence, in which term 
appears 
?? ??/??/?/?/??/????/? 
??? ??/??/??/?/?/??/??/?/? 
??? ??/??/?/?/??/?/??/?/?/???/?/?/??/? 
?? ?/??/??/??/?/?/? 
Table 2: Examples of candidates eliminated by 
GPWS 
5 Relative frequency ratio against background 
corpus 
Relative frequency ratio (RFR) is a useful method 
to be used to discover characteristic linguistic phe-
nomena of a corpus when compared with another 
(Damerau, 1993). RFR of term ? in corpus X com-
pared with another corpus Y, RFR(?;X,Y), simply 
compares the frequency of ?  in X (denoted as 
f(?,X)) to ? in Y (denoted as f(?,Y)): 
RFR(?;X,Y) = f(?,X)/f(?,Y) 
RFR of term is based upon the fact that the sig-
nificant terms will appear frequently in specific 
collection of text (treated as foreground corpus) but 
rarely or even not in other quite different corpus 
(treated as background corpus). The higher of RFR 
values of the terms, the more informative of the 
terms will be in foreground corpus than in back-
ground one. 
However, selection of background corpus is an 
important problem. Degree of difference between 
foreground and background corpus is rather difficult 
to measure and it will affect the values of RFR of 
terms. Commonly, large and general corpora will be 
treated as background corpus for comparison. In 
this paper, for our foreground corpus (Xiao Ao Ji-
ang Hu), we experientially select a group of novels 
of the same author excluding Xiao Ao Jiang Hu as 
compared background corpus for some reasons as 
follows: 
(a) Same author wrote all of the novels, including 
foreground and background. The unique n-
grams in writing style of the author will not 
emerge on RFR values. 
(b) All of the novels are in the same category. The 
specific n-grams for this category will not 
emerge on RFR values. 
So, most of the candidate terms with higher RFR 
values will be more informative and be more sig-
nificant for the source novel. 
On the final phase, we will sort all of the filtered 
candidate terms on RFR values in desc-order so that 
the forepart of the final list will get high precision 
for extraction.  
The last column of Table 1 shows the RFR values 
of many candidates compared with our background 
corpus. Many candidates, such as ????, ????, 
which are frequent in both foreground and back-
ground corpus, will get much lower RFR values and 
will be eliminated from our final top list. 
6 Experimental result 
We use novel Xiao Ao Jiang Hu as foreground 
corpus compared with the rest of novels of Mr. JIN 
Yong as background corpus. The total characters of 
foreground and background corpus are 983,134 and 
7,551,555 respectively. We read through the novel 
Xiao Ao Jiang Hu and 5 graduates manually se-
lected 515 new terms (out of our lexicon) with exact 
meaning in the novel as follows for the final test: 
(a) Proper nouns, such as person names: ?????, 
??????, ??????, place names: ???
? ?, ???? ?, ????? ?, organization 
names: ??????, ?????? etc. 
(b) Normal nouns, such as ??????, ????
??, etc. 
(c) Others, such as ????, ????, etc. 
By our method, we extract 117,807 candidates in 
this novel. Table 3 shows the result after filtering 
with Context-entropy on both sides and boundary-
verification on different total extracted numbers; 
We also compared our integrated method to tradi-
tional measure LR. On lower total number levels, 
LR will overrun our method in unknown-word re-
call, and in turn overrun by us on higher levels. As 
to precision, our method always keeps ahead. 
We also notice that both of the methods have 
much low precision in extraction. To retrieve terms 
with much certain, we rank the entire final list on 
RFR values in final phase. Most significant terms 
will comes in the front of ranked list. 
Table 3 shows that our method Table 4 shows the 
top 12 of final list, and Figure 3 shows the perform-
ance of our method on different top levels when 
ranks the final list on RFR values.  
7 Conclusion 
Unknown word recognition is an important prob-
lem in CIP systems. Suffix array based method is an 
efficient method for exact arbitrary-length frequent 
terms. And most of substring of significant terms, 
which almost appear in fixed contexts, can be 
eliminated by Context-entropy values. Large lexi-
con can help to verify the unknown word doundaris 
and filter incomplete-boundary n-grams. Most sig-
nificant informative candidates list on the top of 
final list according to RFR values for subsequent 
manual confirmation, and on the other aspect, RFR 
also reflects the internal character of the extracted 
terms. 
Total Number 
Extracted 
Word 
in 
Dict
Unknown 
Words Precision 
Unknown-
words 
Recall 
Our 
method 306 57 0.68 0.11 534 
LR 222 103 0.61 0.20 
Our 
method 668 126 0.60 0.24 1325
LR 421 171 0.49 0.33 
Our 
method 1411 225 0.55 0.44 2996
LR 888 287 0.39 0.56 
Our 
method 2877 346 0.50 0.67 6498
LR 1608 366 0.30 0.71 
Our 
method 4,643 512 0.44 0.99 11684
LR 2,428 427 0.24 0.83 
Table 3: Result of our method compared to LR 
 
Term TF RFR 
Left Con-
text-
entropy 
Right 
Context-
entropy
??? 5922 22743.7 6.6804 4.9900 
??? 1184 10104.9 5.9656 4.8688 
??? 929 7928.6 5.7310 4.7623 
??? 919 7843.2 5.5887 4.5220 
???
? 915 7809.1 5.5789 4.2271 
?? 729 6221.6 5.5360 4.4128 
??? 722 6161.9 5.5751 4.7080 
??? 553 4719.6 4.7371 3.8601 
??? 525 4480.6 5.5891 4.4294 
??? 516 4403.8 5.4427 4.1689 
??? 482 4113.6 5.3223 4.7837 
?? 414 3533.3 5.2607 2.6043 
Table 4: Top 12 terms of final list order by RFR 
 References 
Chien, L-F. 1997, PAT-Tree-Based Keyword Extraction 
for Chinese Information Retrieval. Proceedings of the 
1997 ACM SIGIR, Philadelphia, PA, USA, pp. 50-58. 
Chien, L-F. 1998, PAT-Tree-Based Adaptive Key phrase 
Extraction for Intelligent Chinese Information Retrieval. 
In special issue on Information Retrieval with Asian 
Languages, Information Processing and Management, 
Elsevier Press. 
Christopher D. Manning, Hinrich Schutze. 1999. Founda-
tions of Statistical Natural Language Processing, MIT 
Press. 
Dekai Wu and Pascale Fung. 1994. Improving Chinese 
tokenization with linguistic filters on statistical lexical 
acquisition. In Proceedings of the Fourth ACL Confer-
ence on Applied Natural Language Processing 
(ANLP94), Stuttgart, Germany. 
Frank Z. Smadja. 1994. Retrieving collocations from text: 
Xtract. Computational Linguistics, 19(1): 143-177. 
Fred J. Damerau. 1993. Generating and evaluating do-
main-oriented multi-word terms from texts. Information 
Processing and Management, 29(4): 433?447. 
H.Y. Tan, J.H. Zheng, K.Y. Liu. 1999. A Study on the 
Automatic Recognition of Chinese Place Names, 
Proceedings of the 5th Joint Conference on 
Computational Linguistics 99, Tsinghua University 
Press. Kenneth W. Church and Patrick Hanks. 1990. Word asso-
ciation norms, mutual information, and lexicography. 
Computational Linguistics, volume 16. 
Kunihiko Sadakane. 1998. A fast algorithm for making 
suffix arrays and for Burrows-Wheeler transformation, 
Proceedings of the ieee Data Compression Conference, 
pp. 129?138. 
K.Y. Liu. 2000. Automatic Segmentation and Tagging for 
Chinese Text, Commercial Press. 
 
 
 
 
Manber, U. and Myers, G. 1993. Suffix Arrays: A New 
Method for On-Line String Searches. SIAM Journal on 
Computing 22, 935-948. 
R. Song. 1993. Recognition of Personal Names Based on 
Corpus and Rules, Journal of Computational Linguis-
tics: Research and Applications, Beijing Language In-
stitute Press. 
R. Song. 1998. The Geometric Structures of Chinese 
Words and Phrases, International Conference on Chi-
nese Grammers 98, Beijing. 
Patrick Pantel and Dekang Lin. 2001. A statistical corpus-
based term extractor. In E. Stroulia and S. Matwin, edi-
tors, Lecture Notes in Artificial Intelligence, pages 36?
46. Springer-Verlag. 
Ted E. Dunning. 1993. Accurate methods for the statistics 
of surprise and coincidence. Computational Linguistics, 
19 (1): 61?74. 
Thian-Huat Ong, Hsinchun Chen. 1999. Updateable PAT-
Tree Approach to Chinese Key Phrase Extraction using 
Mutual Information: A Linguistic Foundation for 
Knowledge Management, Proceedings of the Second 
Asian Digital Library Conference, November 8-9, pp. 
63-84. 
T. Lou, R. Song, W.L. Li, and Z.Y. Luo. 2001. The de-
sign and Implementation of a Modern General Purpose 
Segmentation System, Journal of Chinese Information 
Processing, Issue No. 5. 
T. Tomokiyo and M. Hurst. 2003. A Language Model 
Approach to Keyphrase Extraction.  ACL-2003 Work-
shop on Multiword Expressions: Analysis, Acquisition 
and Treatment. 
Z.Y. Luo, R. Song. 2001. Integrated and Fast Recogni-
tion of Proper Noun in Modern Chinese Word Segmen-
tation, ICCC, Singapore. 
Figure 3: Test result on different top levels
0
10
20
30
40
50
60
70
80
90
100
5 20 80 120 160 200 240 280 320 360 400 440 480 520 560 600 640 680
Top levels
Pe
rc
en
t(
%)
Precision
Recall
On Generalized-Topic-Based Chinese Discourse Structure * 
 
Song Rou1 Jiang Yuru2,4 Wang Jingyi3 
Beijing Language and Culture University1 
Beijing University of Polytechnic Technology2 
Beijing Forest University3 
Beijing University of Information Science and technology4 
Abstract: Due to the lack of external formal marks, components in Chinese discourse can hardly be 
categorized into the traditional syntactic system. In fact, Chinese is a typical topic-prominent language, so 
it should rather be analyzed from the point of topic. This paper, targeting at computer processing, raises the 
concepts of punctuation clause, generalized topic, discourse structure and topic clause, and reveals the 
properties of Chinese discourse structure based on generalized topic. The applicability of this theory has 
been validated in an initial experiment. 
Keywords: Punctuation Clause, Generalized Topic, Discourse Structure, Topic Clause. 
 
1. Punctuation Clause, Generalized Topic and Discourse Structure 
The traditional study on syntax is based on individual sentences and the formal marks of syntactic 
components. But due to the lack of external formal marks, the concept of sentence in Chinese is not clear 
and the boundary of sentences is difficult to be defined. What?s more, there are no formal means to 
discriminate variant types of syntactic structures. Therefore, the traditional parsing often meets difficulty 
when it comes to Chinese. This paper does not intend to provide a comprehensive analysis of the 
achievements and deficiency of the work done by the scholars in this field before. The study is rather 
based on the factual language phenomena in Chinese and oriented to computer processing of the language. 
In this paper, some concepts, including punctuation clause, generalized topic, discourse structure and topic 
clause, are defined, and some properties of Chinese discourse structure are raised, and initial verification 
done in practical application. 
The basic unit of a Chinese discourse is punctuation clause (PClause). A PClause is a string of words 
separated by comma, semicolon, period, exclamation mark, question mark or quotation marks. Since 
PClauses can be identified with formal marks, and their internal structure and their relations with each 
other are restrained, therefore the basic conditions of processing them with computers are satisfied.  
E.g. 1.1. (Adopted from newspaper news) 
?????????????????????????????????????????
??????????????? 
(?Suddenly??he heard the sound of water in the washroom.?the police officers and the special 
policemen kicked the door open??wrestled the man in the washroom on the floor and handcuffed him?
?after identifying??was nobody but Ye Chengjian.1 ) 
This is a discourse fragment composed of 6 punctuation clauses. 
E.g. 1.2. (Adopted from newspaper news) 
?????????????????????????????????????????
???????????????????? 
(?Ye Chengjian confessed murders in Zhuhai, seducing and blackmailing a Taiwan businessman 
named You, ? and the four armed robbery in Macao?? and identified the places where he illegally hid 
                                                        
* This study is supported by National Natural Science Foundation of China, subject No. 60872121 
1 With a purpose to show the structure of PClauses in Chinese, the translation of Chinese works may not appear very 
standard in English. The same applies hereinafter. 
the guns.) 
This is a discourse fragment composed of 3 punctuation clauses. 
The discourse structure in Chinese is a kind of syntactic structure of a PClause sequence, which is 
composed of a generalized topic and a number of comments. Generalized topic refers to a syntactic 
component of a PClause. The subsequent parts of the punctuation clause after it and the neighbor PClauses 
may be comments about it. Usually a generalized topic is nominal, functioning as the subject, object or 
attributive in the clause in traditional grammar. In this case, the comments answer ?what? and ?how? about 
the topic. The generalized topic can also be verbal, playing the part of the central component of a verb 
phrase. In some cases, the generalized topic can even be adverbial or an individual preposition. That?s why 
the word ?generalized? is adopted. For sake of simplicity, generalized topic will simply be referred to as 
topic in later sections.  
E.g. 1.3. (Adopted from A Tale of Old Man Xing and His Dog, by Zhang Xianliang) 
?????????????????????????????????????????
?????????????????? 
(She collected her needlework, went into the house, swept the kang2, got on it and sat down, lowered 
her head, let her hands dangle between her knees and waited quietly like a prisoner in the hearing room.) 
In this example, each of the seven PClauses has the topic ??(she)? as appears in the first PClause, 
and make comment about ??(she)?, answering the questions about her behavior and what she is like. 
They compose a discourse structure. The first PClause is composed of one topic and one comment, while 
the rest have comment only but no topic. This discourse structure can be expressed below. 
{?[?????????            {She [collected her needlework,  
?????                         went into the house, 
??????                       swept the kang, 
????????                   got on it and sat down, 
?????                         lowered her head, 
?????????                 let her hand dangle between her knees, 
????????????????]}  waited quietly like a prisoner in the hearing room.]} 
For sake of visual cognition, the PClauses are put in different lines and are indented after the topic 
that they comment. This way of expression is called indented new-line representation. What is quoted by 
the ?[]?marks is some comments, the left of which is the topic. And what is quoted by the ?{}?marks is the 
discourse structure.  
E.g. 1.4. (Adopted from Fortress Besieged by Chien Chung-Shu) 
{?[{??[??????          {She [{was wearing only [ a scarlet top, 
????????]}]}                        and navy blue, skin-tight shorts, ]}]} 
These two PClauses both comment on what ????(she was wearing only)?. ???(was wearing 
only)? is one topic, and ??????(a scarlet top)?, and ????????(navy blue, skin-tight shorts)? 
are two comments, answering the question of what was being worn only. The topic and its two comments, 
when combined together, constitute a discourse structure, which is in turn the comment of ??(she)?, 
answering the question of what she was like. In other words, this discourse structure and ?she? constitute 
an external discourse structure.  
E.g. 1.5. (Adopted from Fortress Besieged by Chien Chung-Shu) 
{??{??[????????? 
???????]}} 
                                                        
2 a kind of bed in some parts of China 
{Hung-chien[{was so horrified that[his forehead nearly shrank into his eyebrows,  
(as) his eyebrows rose up to his hairline,]}]} 
These two PClauses both comment on the extent of his being horrified. The verbal structure of verb + 
auxiliary???3?was so horrified??is the topic. The topic and its two comments constitute a discourse 
structure, which is in turn the comment of ???(Hung-chien)?.  
E.g. 1.6. (Adopted from A Tale of Old Man Xing and His Dog, by Zhang Xianliang) 
{???????[{?[??????  {More than 300 people of the team [{all [need feeding, 
??????]}]}                                  need clothing.]}]} 
The two PClauses comment on what ????????(more than 300 people all)?. The generalized 
topic ??(all)? has two comments ??????(need feeding)? and ??????(need clothing)?. They 
both answer ?all? what. ??(all)? and the two comments constitutes a discourse structure, commenting on 
what ????????(more than 300 people all)? were like. The two form external discourse structure. 
E.g. 1.7. (Adopted from Preamble of CONSTITUTION OF THE PEOPLE'S REPUBLIC OF 
CHINA) 
{???[{??????[??????????????? 
????????????????]} 
???????? 
??????????]} 
{This Constitution, [{in legal form, [affirms the achievements of the struggles of the Chinese people 
of all nationalities,  
(and) defines the basic system and basic tasks of the state;]}  
(it) is the fundamental law of the state,  
(and) has supreme legal authority. ]}  
The adverbial ???????(in legal form)? in the first PClause is the generalized topic. The 
section after it ???????????????(affirms the achievements of the struggles of the 
Chinese people of all nationalities)? and the second PClause ????????????????
(defines the basic system and basic tasks of the state)? are its two comments, answering what is done ?in 
legal form?. These three constitute a discourse structure. This structure, together with the third and the 
fourth PClauses, are all comments on the subject of the first PClause ????(this Constitution)? , 
answering what ????(this Constitution)? is about. These three comments, together with ????(this 
Constitution)?  form the external discourse structure.  
E.g. 1.8. (Adopted from Fortress Besieged by Chien Chung-Shu) 
{??[{?[???????        {The students[{took[grades as too cheap, 
???????]}]}                      courses as too easy]}]} 
The preposition ??4? in the first PClause is the generalized topic. ???????(took grades as too 
cheap)?and ????????(took courses as too easy)? comment on what and its result. These three 
then constitute a discourse structure, making comments on ???(the students)?. They form the external 
discourse structure.  
E.g. 1.9. (Adopted from Royal Tramp (Lu Ding Ji) by Louis Cha) 
????????{????? 
[?????????????????]} 
Gu Yanwu bought at the town{a piece of court bulletin,  
                                                        
3 the word ??? in Chinese is an auxiliary, indicating result. 
4 the word ??? in Chinese is a preposition. It is used in transitive structure, introducing the object. 
[(it) listed in detail the names of the criminals accused in the case of Ming Dynasty history.]} 
The discourse structures in other examples of this section are embedding, while this example is of 
overlapping type. The first PClause ?????????????(Gu Yanwu bought at the town a piece 
of court bulletin)? is a discourse structure. The object ?????(a piece of court bulletin)? is not the 
topic in this PClause, but it is the topic of the second PClause ?????????????????(it 
listed in detail the names of the criminals accused in the case of Ming Dynasty history)? and the two form 
another discourse structure. The two structures are overlapping, they share one component ?????(a 
court bulletin)?. 
 
2. The static property of Chinese discourse structure 
From the examples in the previous section, we can notice the characteristics of Chinese discourse 
structure: 
(1) A generalized topic and a comment group constitute a discourse structure. A comment group is 
composed of a number of comments. 
(2) A comment can be the part of a PClause that follows the topic, or a whole PClause, or another 
discourse structure. Therefore, the discourse structure is embedded in a recursive way to the 
right. 
Using Context-Free Grammar, the rules are 
? DiscourseStructure?GeneralizedTopic CommentGroup 
? CommentGroup?Comment 
? CommentGroup?Comment CommentGroup 
? Comment?PClauseTail 
? Comment?PClause 
? Comment?DiscourseStructure 
? GeneralizedTopic? 
? PClauseTail? 
? PClause? 
Here PClauseTail is the tail of the PClause where the generalized topic appears. In these rules, ?-? 
are generating rules for discourse structure, comment group and comment respectively. ??? are the 
generating rules for generalized topic, PClause tail and PClauses. The right part of these rules is related to 
terminal symbols and is not listed here. 
Statistics on the corpora show that in genuine Chinese texts, there are a large number of PClauses 
whose subject is missing. This phenomenon is regarded as zero anaphora or elision in traditional language 
study. But as a matter of fact, the nature of this phenomenon is that there is more than one comment that 
corresponds to a topic. Since it is a topic, it is natural that there are a lot of comments. There are pauses 
between the comments and the result is that several PClauses are formed. Neither is this phenomenon zero 
anaphora nor ellipses, but topic sharing. 
Take 1.8 as an example. The following is its generating process (the numbers following the arrow are 
rule ID).  
DiscourseStructure 
??GeneralizedTopic CommentGroup 
????? CommentGroup 
????? Comment CommentGroup 
????? DiscourseStructure CommentGroup 
????? GeneralizedTopic CommentGroup CommentGroup 
????? ?????? CommentGroup CommentGroup 
????? ?????? Comment CommentGroup CommentGroup 
????? ?????? PClauseTail CommentGroup CommentGroup 
????? ?????? ???????????????CommentGroup CommentGroup 
????? ?????? ???????????????Comment CommenrGroup 
????? ?????? ???????????????PClause CommenrGroup 
?????  ??????  ?????????????????????????           
??????CommentGroup 
?????  ??????  ?????????????????????????           
??????Comment CommentGroup 
?????  ??????  ?????????????????????????           
??????PClause CommentGroup 
?????  ??????  ?????????????????????????           
??????????????CommentGroup 
?????  ??????  ?????????????????????????           
??????????????Comment 
?????  ??????  ?????????????????????????           
??????????????PClause 
?????  ??????  ?????????????????????????           
???????????????????????? 
This nature describes the internal relations of a discourse structure. Therefore it is termed static 
nature. 
This nature can cover most examples in the preceding section except example 1.9. This is because the 
overlapping type of the discourse structure in the example 1.9 can not be represented by Context-Free 
Grammar. 
 
3. Dynamic Property of Chinese Topic Clause 
3.1. Topic Structure and Topic Clause 
 In this paper, the structure formed by a comment and its topic is called a topic structure. A topic 
structure as comment can be combined with an external topic and form an external topic structure. If the 
topic of a comment is the outmost layer of a discourse structure, it is then called the topic clause. In most 
cases, every PClause corresponds to a topic clause.  
E.g.3.1. (Adopted from the Biology Section of China Encyclopedia) 
c1?????????, (the spawning season of neoceratodus forsteri is quiet long) 
c2              ??? 9?10 ?????(usually September and October are most 
productive period) 
c3        ???(eggs are big) 
c4        ?? 6?7??, (eggs are 6-7 mm in diameter) 
c5          ????, (have gelatinous membrane) 
c6          ????(are not sticky) 
c7        ????????(the eggs are laid among plants) 
c8          ????????(some sink deep in the water) 
Here, the outmost topic is ?????(neoceratodus forsteri)?. 
The topic clause of c1 is c1 itself. The comment is ??????(the spawning season of is quiet 
long)?. 
c2???? 9?10 ????(usually September and October are most productive period)?is a 
comment, and its topic is????(the spawning season)?. The topic structure composed of the two is the 
comment on ?????(neoceratodus forsteri)??therefore ??????????? 9?10????? 
is the topic clause of c2? 
c3???(eggs are big)?is the comment on the topic ?????(neoceratodus forsteri)?. The topic 
clause of c3 is ????????. 
c4??? 6?7 ??(eggs are 6-7 mm in diameter)? is the comment on the topic ?????
(neoceratodus forsteri)?. The topic clause of c4 is ??????? 6?7???? 
c5?????(have gelatinous membrane)?is the comment on the topic ??(eggs)?. The topic structure 
??????(eggs have gelatinous membrane)? composed of the two is the comment on ?????
(neoceratodus forsteri)?. And the topic clause of c5 is ???????????? 
c6????(are not sticky)? is the comment on the topic ??(eggs)?. The topic structure ?????
(eggs are not sticky)? composed of the two is the comment on ?????(neoceratodus forsteri)?. And the 
topic clause of c6 is ??????????? 
c7????????(the eggs are laid among plants)? is the comment on the topic ?????
(neoceratodus forsteri)?. The topic clause of c7 is ?????????????? 
c8????????(some sink deep in the water)?is the comment on the topic ??(eggs)?? The 
topic structure ?????????(eggs some sink deep in the water)? composed of the two is the 
comment on the topic ?????(neoceratodus forsteri)?. And the topic clause of c8 is ????????
??????? 
The purpose of analyzing a PClause sequence is to find out its discourse structure. If the topic clause 
of every PClause is constructed, the topic of each comment at every layer is then found out, and 
consequently the entire discourse structure will be clear. The next section provides an approach to finding 
out the topic clause of PClauses. 
3.2. Stack Model of Dynamic Generation of the Topic Clause  
The topic clause of PClause ci of Ex.3.1 is marked as ci?. They are listed below. 
c1?.?????????, (the spawning season of neoceratodus forsteri is quiet long) 
c2?.?????????? 9?10 ?????(the spawning season of neoceratodus forsteri 
usually September and October are most productive period) 
c3?.???????(neoceratodus forsteri?s eggs are big) 
c4?.?????? 6?7??, (neoceratodus forsteri?s eggs are 6-7mm in diameter) 
c5?.?????????, (neoceratodus forsteri?s eggs have gelatinous membrane) 
c6?.?????????(neoceratodus forsteri?s eggs are not sticky) 
c7?.????????????(neoceratodus forsteri?s eggs are laid among plants) 
c8?.?????????????(some eggs of neoceratodus forsteri sink deep in the water) 
The generation of each ci? is exemplified below. 
c1?=c1? 
The topic of c2 is ????(the spawning season)? in c1?. Delete the part of c1? right to the topic and 
replace it with c2?and we will have c2?? 
The topic of c3 is ?????(neoceratodus forsteri?)? in c2?. Delete the part of c2? right to the topic 
and replace it with c3?and we will have c3?. 
The topic of c4 is ?????(neoceratodus forsteri?)? in c3?. Delete the part of c3? right to the topic 
and replace it with c4?and we will have c4?. 
The topic of c5 is ??(eggs)? in c4?. Delete the part of c4? right to the topic and replace it with c5?and 
we will have c5?. 
The topic of c6 is ??(eggs)? in c5?. Delete the part of c5? right to the topic and replace it with c6?and 
we will have c6?. 
The topic of c7 is ?????(neoceratodus forsteri?)? in c6?. Delete the part of c6? right to the topic 
and replace it with c7?and we will have c7?. 
The topic of c8 is ??(eggs)? in c7?. Delete the part of c7? right to the topic and replace it with c8?and 
we will have c8?. 
Generally, given a PClause sequence },,{ 1 ncc ? , if the first PClause is a complete structure of 
topic-comment, then 
(1) the topic clause of the first PClause is the PClause itself; 
(2) if the topic of a subsequent PClause is missing, then the topic should be in the topic clause of its 
previous PClause; 
(3) the topic clause of every subsequent PClause can be generated recursively by stack operation. 
Note the topic clause of ic  as ic ??and the topic clause of 1+ic  as 1+ic ?, 
?3.1?if the topic of 1+ic  is missing?and ic ?= ??A ?where A is the topic of 1+ic , then 
1+ic ?= 1+iAc? ? 
?3.2?if the topic of 1+ic  is not missing, then 1+ic ?= 1+ic . 
If we regard the beginning and the end of a topic clause as the bottom and the top of a stack 
respectively, then the removal and connection of the components in the generation process of topic clause 
are typical stack operations. Therefore the recursive law of such generation can be called the stack model 
The stack model can not only applied to embedded discourse structure, but also some overlapping 
structures such as instance 1.4. Details are not given here. Our investigation into corpora (about 340,000 
Chinese characters) of different registers shows that more than 95% PClauses meet the model. 
From the stack model, it can be seen that the key to generate the topic clause of a PClause is to 
identifying which component of the topic clause of the previous PClause is its topic. This would require to 
uncover the constraints for forming the discourse structure.  
 
4. Constraints on Discourse Structure 
4.1. Acceptability and completeness of Topic Clause 
A topic structure is composed of a topic and its comments. Therefore mostly it is acceptable. A topic 
clause is not only acceptable, but also complete with necessary syntactic and semantic components. Taking 
advantage of this nature, the filtering of topic-seeking for a PClause can be boiled down to the judgment of 
the acceptability and completeness of a single clause. For example, the topic clause of PClause 7 in 
example 3.1 is: 
c7?.????????????(neoceratodus forsteri?s eggs are laid among plants) 
and PClause c8 is  
????????(some sink deep in the water) 
According to the stack model, the options for the topic clause of c8 are: 
?1? ????????(some sink deep in the water) 
?suppose that the topic of c8 is not missing? 
?2? ????????????(neoceratodus forsteri some sink deep in the water) 
?suppose that the topic of c8 is ?????(neoceratodus forsteri)?? 
?3? ?????????????(neoceratodus forsteri?eggs some sink deep in the water) 
?suppose that the topic of c8 is ??(eggs)?? 
?4? ???????????????(neoceratodus forsteri?eggs some are laid sink deep in 
the water) 
?suppose that the topic of c8 is ???(be laid)?? 
?5? ?????????????????(neoceratodus forsteri?eggs some are laid plant 
sink deep in the water) 
?suppose that the topic of c8 is ???(plant)?? 
?6? ???????????????????(neoceratodus forsteri?eggs some are laid 
among plant sink deep in the water) 
?suppose that the topic of c8 is ???(middle)?? 
Chinese intuition tells us that (1) is not complete, and (4)(5)(6) are not acceptable, so the candidates 
are (2) and (3) only. We see that if we can formalize our intuition, we can considerably narrow down the 
scope of options.  
The topic and the comment of a topic clause are often from different PClauses, and the components in 
a topic clause that have discourse functions (such as discourse conjunctions) can affect the acceptability of 
the topic clause. This problem needs to be addressed in separate study.  
4.2. Semantic Constraints 
E.g. 4.1.??????????????(He bought a wallet, (it) is a brand product.) 
The topic of the second PClause could be ??(he)? or ???(a wallet)?. We can eliminate the first 
possibility by using semantic constraints, because a person can not be a product. 
4.3. Syntactic Constraints 
An investigation into corpora shows that the syntactic relations of the topic and the comments are of 
the following types: 
 (1) If the relation of a topic and its comment in the same PClause is subject-predicate, then the same 
relation is true of it with its comments in other PClauses (see example 1.3); 
 (2) If the relation of a topic and its comment in the same PClause is predicate-object, 
preposition-object or attribute-central, then the relation of it and its comment in other PClauses is of the 
same type or subject-predicate type (see example 1.4 and 1.8). 
 (3) If the relation of a topic and its comment in the same PClause is adverbial-central or 
predicate-complement, then its relation with its comment in other PClauses is the same (see example 1.5, 
1.6 and 1.7). 
 (4) If a component is not the topic of the PClause where it is appears, but is the topic of other 
PClauses, then it must be the object or attribute in the PClause where it appears and its relation with the 
comments in other PClauses is subject-predicate (see example 1.9). 
In addition, adjectives, numbers in partition in respect of quantity and some adverbs (such a adverbs 
indicating degree) cannot function as general topics. 
4.4 Context Constraints 
E.g. 4.2.??????????(He has a friend, (who is)very generous with money.) 
The topic of the second PClause could be ??(he)? or ????(a friend)?. Whether it is ?he is 
generous with money? or ?his friend is generous with money?, it will present no problem either 
semantically or syntactically. However, abundant instances and analyses show that if  
(1) the structure of the topic clause of the previous PClause is SVO; 
(2) the core verb of the topic clause of the previous PClause has a sense of ?owing? or ?introducing?; 
and 
(3) the second PClause is an adjective phrase but does not fall into the category of mental state 
then the topic of the second PClause is the object rather than the subject of the topic clause. 
According to this constraint, the topic of the second PClause is ????(a friend)? 
4.5. Cognition Constraints 
Theoretically, there is no limit to the size of a discourse structure. Countless layers could be 
embedded or overlapped. For example, we could have the following discourse structure. 
E.g. 4.3???????? 3? 
???????? 1? 
                  ??? 4? 
                          ???? 1? 
                                    ???? 5? 
?? 
(The circumference ratio?s  
integer part is 3, 
 the first number in the fraction part is 1,  
           followed by 4, 
             followed by 1, 
                                                   followed by 5  
?) 
Here ????(circumference ratio)?, ?1?, ?4?, ?1?, ?5? all are topics. They could go on with no limit.  
But the study on factual corpora have discovered that the maximum layer of embedding or 
overlapping is 5, and if we shall return from the deeper layers, the maximum number of the layers that can 
be jumped back is 3. This has much to do with people?s cognition ability. The following is an example of 5 
layers of embedding and overlapping with 3 layers of maximum return. The underlined words are the 
generalized topics. The numbers in the brackets to the right of the PClauses indicating the depth of the 
embedding and overlapping. PClause ?????(release them)? reaches the fifth layer in depth, but the 
next PClause ???????(when the authorities started investigating)? returns to layer2, retreating 3 
layers.  
Ex. 4.4. (Adopted from Royal Tramp by Louis Cha)  
???????????????0?(Cheng Weifan, on the long boat journey from Hangzhou to Nanxun) 
  ??????1?(thought things over) 
  ???????1?(had come up with a good plan.) 
  ????????????????1?(thought the book had already been in circulation for some time) 
      ?????????2?(It  was therefore  too  late  for  concealment) 
      ?????????????2?(the  only expedient  left was to play a trick) 
          ????????????3?(on one hand, send people to go to the bookshops all over the country) 
                 ??????????????4? (buy back and then destroy all copies of the book)  
          ????????3?(on the other hand, work day and night) 
              ??????4?(make a new printing mould) 
              ??????????4?(remove all the offensive bits) 
              ??????4?(reprint the book) 
                      ??????5?(release them)  
      ????????2?(when the authorities started investigating) 
          ???????????3?(inspect the new edition of Ming History) 
          ???????????3?(find Wu?s charges to be groundless) 
      ??????????2?(can avert a hideous disaster) 
 
5. Initial Application of Discourse Structure based on General Topic 
5.1. Discourse Structure in Encyclopedia 
The herein discussed Chinese discourse structure based on general topic has been initially applied and 
tested in the analyses of encyclopedia texts. 
The entries in encyclopedia are expository, covering people, places, species, events, devices and 
terms etc. in various subjects. Because the different aspects of an object must be exposed, the leading role 
of the topic is very obvious. It frequently occurs that many PClauses are used to comment on one topic, 
and the comments on different aspects of an object are often presented as embedded or overlapping 
structures. In order to mine the information of the object described, it is necessary to analyze the governing 
scope of a topic. In other words, it is necessary to locate the object commented by every PClause. 
Therefore the discourse structure must be analyzed. Take 3.1 for example, we must be clear about for 
?what? September and October are the active period, the eggs of ?what? are big, the eggs of ?what? are 
6-7 mm in diameter, ?what? has gelatinous membranes and so on. 
5.2. An Experiment on Discourse Structure in Encyclopedia 
The experiment object of the paper is the entries about various fishes in the biology volume of China 
Encyclopedia. The objective is the find the topic clause of every Pclause. 
There are 224 entries about fishes in this volume, each one with a title, viz. the name of the order, 
family, genera and species of a fish. The first PClause in the text does not mention the name, but 
introduces the genera information of it. The name is not necessarily mentioned in later Pclauses. For 
example,   
???? 
Neoceratodus forsteri; Queensland lungfish 
?????????????? 1 ?(????????),???????????????
125 ??,?? 10 ???????????????????? 
(A member of the family Ceratodontidae and order Ceratodontiformes (see picture of Neoceratodus 
forsteri). (It) is the biggest extant lungfish species in the world. (Its)Body length (is) about 125 centimeters, 
(it) weighs as much as 10 kilograms. (Its) Body is elongated, covered with big and thin round scales ?) 
In the experiment, the entry names (both in Chinese and English) and bracketed information are 
deleted. But the entry title is added to the left of the first PClause, connected by a ??(is) ?. For example, 
the first PClause of the above example of neoceratodus forsteri is changed into ???????????
????????? 1???, the rest remains unchanged. 
The experiment selected 3999 PClauses of 86 entries as training data, and 577 PCauses of 13 entries 
as open-test data. The input of the experiment is the topic clause of a PClause and its next PClause, and the 
output is the topic clause of the second PClause. In other words, the target of the experiment is to decide 
the topic of the PClause within a limited scope under the scheme of stack model.  
For the training data, each PClause is replenished manually into a topic clause, and then the words are 
segmented. In this way, the training topic clause set G is obtained. The principle of testing is described 
below. For each tested PClause c and the topic clause d of its proceeding PClause, word segmentation is 
done separately. String d is cut at different places, the tails are replaced with c every time. Thus a number 
of candidate topic clauses of c are obtained. Then the similarity reckoning is made about the candidate 
topic clauses and the topic clauses in G. The one with the maximum similarity is chosen as the result for 
output. 
In order to solve the problem of data sparse in the calculation, semantic generalization is made about 
related words. The semantic categories employed are?subjects of fishes ( e.g. neoceratodus forsteri, 
alopias), part (e.g. head, scale, fin), position (e.g. back, abdomen), location ( e.g. front, upper), shape (e.g. 
fusiformis, cylindrical), size (e.g. big, short), color (e.g. red, light blue). environment (e.g. pond, near sea), 
geographical region(e.g. the Pacific, Huanghai), season(e.g. early spring, autumn), number (e.g. 3, 1-3) etc. 
Verbs are rarely generalized.  
The result of the initial experiment showed the accuracy rate for open test was 78%. If add the title of 
a text to the beginning of every PClause in the text, 66% accuracy rate can be got as a baseline. The result 
of the experiment is not high indeed and there is room for improvement. Since the experiment principle 
was the similarity of the topic clauses, in essence only the stack model and the acceptability of topic clause 
are used. Semantic constraints, syntactic constraints, context constraints and cognitive constraints are not 
employed. In addition, word segmentation is not entirely correct, and the semantic generalization is quite 
rough. 78% accuracy rate of under such rough conditions has initially proved the applicability of the 
theoretical system. 
 
6. Discussion 
 This paper employs discourse structure of topic-comment in analyzing Chinese, takes PClauses as the 
basic discourse unit, and extends the concept of topic to generalized topic. As a result, the properties of 
Chinese discourse structure are proposed. Investigations into large amount of language data have proved 
that this theoretical system is natural and applicable to Chinese, which is also backed up by initial 
experiment. Of course, the theory need to be improved, and the various types of constraints under the 
theory framework need to be further uncovered. More and detailed study needs to be done along this path. 
 
References 
[1] CHEN Ping?1987??Discouse Analysis of Zero anaphora in Chinese?Zhongguo Yuwen? No.5,1987. 
[2] CHU Chauncey C.?1998??A Discourse Grammar of Mandarin Chinese?Peter Lang Publication Inc. 
New York. 
[3] HUANG He yan, CHEN Zhao xiong?2002??The Hybrid Strategy Processing Approach of Complex 
Long Sentence?Journal of Chinese Information Processing?Vol.16, No.3. 
[4] HOU min, SUN Jian-jun?2005??Zero Anaphora in Chinese and How to Process it in 
Chinese-English MT?Journal of Chinese Information Processing?Vol.19, No.3. 
[5] HUANG Jian-cuan?SONG Rou?2008?,A Research on the Annotation of Punctuated Clauses?
Frontiers of Content Computing?Edited by SUN Mao-song and CHEN Qun-xiu? Tsinghua 
University Press, Beijing. 
[6] LI Xing; ZONG Cheng-qing?2006??A Hierarchical Parsing Approach with Punctuation Processing 
for Long Chinese Sentences?Journal of Chinese Information Processing?Vol.20, No.4. 
[7] MAO Qi, LIAN Le-xin, ZHOU Wen-cui, YUAN Chun-fang?2007??Chinese Syntactic Parsing 
Algorithm Based on Segmentation of Punctuation?Journal of Chinese Information Processing?
Vol.21, No.2. 
[8] SONG Rou?1992??The Delesion of the Fronts of Clauses in Chinese Narratives?Journal of Chinese 
Information Processing?Vol.6, No.3. 
[9] SONG Rou?2008??Research on Properties of Syntactic RelationBetween P-Clauses in Modern 
Chinese?Chinese Teaching in the World, No.2, 2008. 
[10] SONG Rou, WANG Jingyi?2008??Syntactic Relation Between P-Clauses in Modern Chinese and 
Annotated Corpus, CCID & Lancaster University Joint Workshop on Corpus Linguistics & Machine 
Translation Applications, 2008. Beijing. 
[11] XING Fu-yi?1997??Chinese Gramma?Northeast Normal University Press?Changchun. 
[12] XU Yu-long?2004??Towards a Functional-Pragmatic Model of Discourse Anaphora Resolution?
Shaihai Foreign Language Education Press?Shaihai. 
 
